<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  RadVLM A Multitask Conversational Vision-Language Model for Radiology">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-85fa0021576d440b004aac793a0dbd4c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-07-æ›´æ–°"><a href="#2025-02-07-æ›´æ–°" class="headerlink" title="2025-02-07 æ›´æ–°"></a>2025-02-07 æ›´æ–°</h1><h2 id="RadVLM-A-Multitask-Conversational-Vision-Language-Model-for-Radiology"><a href="#RadVLM-A-Multitask-Conversational-Vision-Language-Model-for-Radiology" class="headerlink" title="RadVLM: A Multitask Conversational Vision-Language Model for Radiology"></a>RadVLM: A Multitask Conversational Vision-Language Model for Radiology</h2><p><strong>Authors:Nicolas Deperrois, Hidetoshi Matsuo, Samuel RuipÃ©rez-Campillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser, Koji Fujimoto, Mizuho Nishio, Thomas M. Sutter, Julia E. Vogt, Jonas Kluckert, Thomas Frauenfelder, Christian BlÃ¼thgen, Farhad Nooralahzadeh, Michael Krauthammer</strong></p>
<p>The widespread use of chest X-rays (CXRs), coupled with a shortage of radiologists, has driven growing interest in automated CXR analysis and AI-assisted reporting. While existing vision-language models (VLMs) show promise in specific tasks such as report generation or abnormality detection, they often lack support for interactive diagnostic capabilities. In this work we present RadVLM, a compact, multitask conversational foundation model designed for CXR interpretation. To this end, we curate a large-scale instruction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks â€“ such as report generation, abnormality classification, and visual grounding â€“ and multi-turn, multi-task conversational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. Our results show that RadVLM achieves state-of-the-art performance in conversational capabilities and visual grounding while remaining competitive in other radiology tasks. Ablation studies further highlight the benefit of joint training across multiple tasks, particularly for scenarios with limited annotated data. Together, these findings highlight the potential of RadVLM as a clinically relevant AI assistant, providing structured CXR interpretation and conversational capabilities to support more effective and accessible diagnostic workflows. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰çš„å¹¿æ³›åº”ç”¨ä»¥åŠæ”¾å°„ç§‘åŒ»ç”Ÿçš„çŸ­ç¼ºï¼Œæ¨åŠ¨äº†è‡ªåŠ¨CXRåˆ†æå’ŒAIè¾…åŠ©æŠ¥å‘ŠåŠŸèƒ½çš„ä¸æ–­å¢é•¿éœ€æ±‚ã€‚è™½ç„¶ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æŠ¥å‘Šç”Ÿæˆæˆ–å¼‚å¸¸æ£€æµ‹ç­‰ç‰¹å®šä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹æ”¯æŒäº¤äº’å¼è¯Šæ–­çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RadVLMï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘çš„å¤šä»»åŠ¡å¯¹è¯åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºCXRè§£é‡Šè®¾è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡100ä¸‡å¼ å›¾åƒå’ŒæŒ‡ä»¤å¯¹ï¼Œå…¶ä¸­åŒ…æ‹¬å•å›åˆä»»åŠ¡ï¼Œå¦‚æŠ¥å‘Šç”Ÿæˆã€å¼‚å¸¸åˆ†ç±»å’Œè§†è§‰å®šä½ï¼Œä»¥åŠå¤šå›åˆã€å¤šä»»åŠ¡å¯¹è¯äº¤äº’ã€‚é€šè¿‡å¯¹RadVLMè¿›è¡Œå¾®è°ƒååœ¨æ­¤æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä»¥åŠé‡æ–°å®ç°åŸºçº¿VLMsçš„ä¸åŒä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒRadVLMåœ¨å¯¹è¯èƒ½åŠ›å’Œè§†è§‰å®šä½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶åœ¨å…¶ä»–æ”¾å°„å­¦ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†è”åˆè®­ç»ƒå¤šä¸ªä»»åŠ¡çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚è¿™äº›å‘ç°å…±åŒçªå‡ºäº†RadVLMä½œä¸ºä¸´åºŠç›¸å…³çš„AIåŠ©ç†çš„æ½œåŠ›ï¼Œæä¾›ç»“æ„åŒ–CXRè§£é‡Šå’Œå¯¹è¯èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´æœ‰æ•ˆå’Œå¯è®¿é—®çš„è¯Šæ–­å·¥ä½œæµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03333v1">PDF</a> 21 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”±äºèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰çš„å¹¿æ³›åº”ç”¨å’Œæ”¾å°„ç§‘åŒ»ç”ŸçŸ­ç¼ºï¼Œå¯¹è‡ªåŠ¨åŒ–CXRåˆ†æå’ŒAIè¾…åŠ©æŠ¥å‘Šçš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚å°½ç®¡ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æŠ¥å‘Šç”Ÿæˆæˆ–å¼‚å¸¸æ£€æµ‹ç­‰ç‰¹å®šä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹äº¤äº’å¼è¯Šæ–­åŠŸèƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç´§å‡‘ã€å¤šä»»åŠ¡å¯¹è¯åŸºç¡€æ¨¡å‹RadVLMï¼Œç”¨äºCXRè§£è¯»ã€‚ä¸ºæ­¤ç›®çš„ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1ç™¾ä¸‡ä¸ªå›¾åƒæŒ‡ä»¤å¯¹ï¼ŒåŒ…æ‹¬æŠ¥å‘Šç”Ÿæˆã€å¼‚å¸¸åˆ†ç±»å’Œè§†è§‰å®šä½ç­‰å•ä»»åŠ¡ä»¥åŠå¤šä»»åŠ¡å¤šè½®å¯¹è¯äº¤äº’ã€‚ç»è¿‡åœ¨æ­¤æŒ‡ä»¤æ•°æ®é›†ä¸Šå¾®è°ƒåï¼ŒRadVLMåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºé‡æ–°å®ç°çš„åŸºçº¿VLMã€‚ç»“æœè¯æ˜RadVLMåœ¨å¯¹è¯èƒ½åŠ›å’Œè§†è§‰å®šä½æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶åœ¨å…¶ä»–æ”¾å°„å­¦ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰çš„å¹¿æ³›åº”ç”¨å’Œæ”¾å°„ç§‘åŒ»ç”ŸçŸ­ç¼ºï¼Œå¯¼è‡´å¯¹è‡ªåŠ¨åŒ–CXRåˆ†æå’ŒAIè¾…åŠ©æŠ¥å‘Šçš„éœ€æ±‚å¢é•¿ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹äº¤äº’å¼è¯Šæ–­åŠŸèƒ½ã€‚</li>
<li>RadVLMæ˜¯ä¸€ä¸ªç´§å‡‘ã€å¤šä»»åŠ¡å¯¹è¯åŸºç¡€æ¨¡å‹ï¼Œç”¨äºCXRè§£è¯»ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒæŒ‡ä»¤å¯¹ï¼Œæ¶µç›–å•ä»»åŠ¡å’Œå¤šä»»åŠ¡å¯¹è¯äº¤äº’ã€‚</li>
<li>RadVLMåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿VLMï¼Œå…·æœ‰å…ˆè¿›çš„å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚</li>
<li>RadVLMåœ¨å¯¹è¯èƒ½åŠ›å’Œè§†è§‰å®šä½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å¯èƒ½æˆä¸ºä¸´åºŠç›¸å…³çš„AIåŠ©æ‰‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-becd96094b2b3edb13587cb9ceb1c69f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fae4f41bf333e9021a1c1b8a8ad2ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b771b99a3e6845b3b3ddc606f4b6322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c28bd666449c504462fe6af93fac79b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAP-Image-Recovery-with-Guarantees-using-Locally-Convex-Multi-Scale-Energy-LC-MUSE-Model"><a href="#MAP-Image-Recovery-with-Guarantees-using-Locally-Convex-Multi-Scale-Energy-LC-MUSE-Model" class="headerlink" title="MAP Image Recovery with Guarantees using Locally Convex Multi-Scale   Energy (LC-MUSE) Model"></a>MAP Image Recovery with Guarantees using Locally Convex Multi-Scale   Energy (LC-MUSE) Model</h2><p><strong>Authors:Jyothi Rikhab Chand, Mathews Jacob</strong></p>
<p>We propose a multi-scale deep energy model that is strongly convex in the local neighbourhood around the data manifold to represent its probability density, with application in inverse problems. In particular, we represent the negative log-prior as a multi-scale energy model parameterized by a Convolutional Neural Network (CNN). We restrict the gradient of the CNN to be locally monotone, which constrains the model as a Locally Convex Multi-Scale Energy (LC-MuSE). We use the learned energy model in image-based inverse problems, where the formulation offers several desirable properties: i) uniqueness of the solution, ii) convergence guarantees to a minimum of the inverse problem, and iii) robustness to input perturbations. In the context of parallel Magnetic Resonance (MR) image reconstruction, we show that the proposed method performs better than the state-of-the-art convex regularizers, while the performance is comparable to plug-and-play regularizers and end-to-end trained methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå°ºåº¦æ·±åº¦èƒ½é‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ•°æ®æµå½¢é™„è¿‘çš„å±€éƒ¨é‚»åŸŸå†…å¼ºçƒˆå‡¸ï¼Œç”¨äºè¡¨ç¤ºå…¶æ¦‚ç‡å¯†åº¦ï¼Œå¹¶åº”ç”¨äºåé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è´Ÿå¯¹æ•°å…ˆéªŒè¡¨ç¤ºä¸ºä¸€ä¸ªå¤šå°ºåº¦èƒ½é‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å‚æ•°åŒ–ã€‚æˆ‘ä»¬å°†CNNçš„æ¢¯åº¦é™åˆ¶ä¸ºå±€éƒ¨å•è°ƒï¼Œä»è€Œå°†æ¨¡å‹çº¦æŸä¸ºå±€éƒ¨å‡¸å¤šå°ºåº¦èƒ½é‡ï¼ˆLC-MuSEï¼‰ã€‚æˆ‘ä»¬åœ¨åŸºäºå›¾åƒçš„åé—®é¢˜ä¸­ä½¿ç”¨å­¦åˆ°çš„èƒ½é‡æ¨¡å‹ï¼Œå…¶å…¬å¼å…·æœ‰å‡ ä¸ªç†æƒ³çš„ç‰¹æ€§ï¼šiï¼‰è§£çš„å”¯ä¸€æ€§ï¼Œiiï¼‰å¯¹åé—®é¢˜çš„æœ€å°æ”¶æ•›ä¿è¯ï¼Œä»¥åŠiiiï¼‰å¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ã€‚åœ¨å¹¶è¡Œç£å…±æŒ¯ï¼ˆMRï¼‰å›¾åƒé‡å»ºçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„å‡¸æ­£åˆ™åŒ–å™¨ï¼ŒåŒæ—¶å…¶æ€§èƒ½ä¸å³æ’å³ç”¨æ­£åˆ™åŒ–å™¨å’Œç«¯åˆ°ç«¯è®­ç»ƒçš„æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æå‡ºä¸€ç§å¤šå°ºåº¦æ·±åº¦èƒ½é‡æ¨¡å‹ï¼Œé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å‚æ•°åŒ–è¡¨ç¤ºè´Ÿå¯¹æ•°å…ˆéªŒä½œä¸ºå¤šå°ºåº¦èƒ½é‡æ¨¡å‹ï¼Œå¹¶é™åˆ¶CNNçš„æ¢¯åº¦åœ¨å±€éƒ¨å•è°ƒï¼Œå°†å…¶çº¦æŸä¸ºå±€éƒ¨å‡¸å¤šå°ºåº¦èƒ½é‡ï¼ˆLC-MuSEï¼‰ã€‚è¯¥æ¨¡å‹åº”ç”¨äºå›¾åƒé€†é—®é¢˜ï¼Œå…·æœ‰å”¯ä¸€è§£ã€å‘é€†é—®é¢˜æœ€å°å€¼çš„æ”¶æ•›ä¿è¯ä»¥åŠå¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ã€‚åœ¨å¹¶è¡Œç£å…±æŒ¯ï¼ˆMRï¼‰å›¾åƒé‡å»ºçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„å‡¸æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ€§èƒ½ä¸å³æ’å³ç”¨æ­£åˆ™åŒ–æ–¹æ³•å’Œç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å¤šå°ºåº¦æ·±åº¦èƒ½é‡æ¨¡å‹ï¼Œç”¨äºè¡¨ç¤ºæ¦‚ç‡å¯†åº¦ï¼Œå¹¶åº”ç”¨äºé€†é—®é¢˜ã€‚</li>
<li>å°†è´Ÿå¯¹æ•°å…ˆéªŒè¡¨ç¤ºä¸ºå¤šå°ºåº¦èƒ½é‡æ¨¡å‹ï¼Œé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ã€‚</li>
<li>æ¨¡å‹çš„æ¢¯åº¦åœ¨å±€éƒ¨è¢«é™åˆ¶ä¸ºå•è°ƒï¼Œå½¢æˆå±€éƒ¨å‡¸å¤šå°ºåº¦èƒ½é‡ï¼ˆLC-MuSEï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å›¾åƒé€†é—®é¢˜ä¸­è¡¨ç°å‡ºä¼˜è‰¯æ€§èƒ½ï¼Œå…·æœ‰å”¯ä¸€è§£ã€æ”¶æ•›ä¿è¯å’Œå¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ã€‚</li>
<li>åœ¨å¹¶è¡Œç£å…±æŒ¯å›¾åƒé‡å»ºä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºè®¸å¤šå…ˆè¿›çš„å‡¸æ­£åˆ™åŒ–æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æ€§èƒ½ä¸å³æ’å³ç”¨æ­£åˆ™åŒ–æ–¹æ³•å’Œç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33cf5e0a5e4137e77bb376483d8572aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9940091350311d8aaeb13d1671538d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85fa0021576d440b004aac793a0dbd4c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Unified-Framework-for-Semi-Supervised-Image-Segmentation-and-Registration"><a href="#A-Unified-Framework-for-Semi-Supervised-Image-Segmentation-and-Registration" class="headerlink" title="A Unified Framework for Semi-Supervised Image Segmentation and   Registration"></a>A Unified Framework for Semi-Supervised Image Segmentation and   Registration</h2><p><strong>Authors:Ruizhe Li, Grazziela Figueredo, Dorothee Auer, Rob Dineen, Paul Morgan, Xin Chen</strong></p>
<p>Semi-supervised learning, which leverages both annotated and unannotated data, is an efficient approach for medical image segmentation, where obtaining annotations for the whole dataset is time-consuming and costly. Traditional semi-supervised methods primarily focus on extracting features and learning data distributions from unannotated data to enhance model training. In this paper, we introduce a novel approach incorporating an image registration model to generate pseudo-labels for the unannotated data, producing more geometrically correct pseudo-labels to improve the model training. Our method was evaluated on a 2D brain data set, showing excellent performance even using only 1% of the annotated data. The results show that our approach outperforms conventional semi-supervised segmentation methods (e.g. teacher-student model), particularly in a low percentage of annotation scenario. GitHub: <a target="_blank" rel="noopener" href="https://github.com/ruizhe-l/UniSegReg">https://github.com/ruizhe-l/UniSegReg</a>. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ç»“åˆäº†æ ‡æ³¨æ•°æ®å’Œéæ ‡æ³¨æ•°æ®ï¼Œæ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸€ç§é«˜æ•ˆæ–¹æ³•ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡æ³¨æ˜¯éå¸¸è€—æ—¶å’Œæ˜‚è´µçš„ã€‚ä¼ ç»Ÿçš„åŠç›‘ç£æ–¹æ³•ä¸»è¦å…³æ³¨ä»éæ ‡æ³¨æ•°æ®ä¸­æå–ç‰¹å¾å’Œå­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œä»¥æé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆæœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç»“åˆå›¾åƒé…å‡†æ¨¡å‹ç”Ÿæˆéæ ‡æ³¨æ•°æ®çš„ä¼ªæ ‡ç­¾ï¼Œç”Ÿæˆæ›´å‡ ä½•æ­£ç¡®çš„ä¼ªæ ‡ç­¾ï¼Œä»¥æé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨2Dè„‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå³ä½¿åœ¨ä»…ä½¿ç”¨1%çš„æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„åŠç›‘ç£åˆ†å‰²æ–¹æ³•ï¼ˆå¦‚å¸ˆå¾’æ¨¡å‹ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡æ³¨æ¯”ä¾‹è¾ƒä½çš„æƒ…å†µä¸‹ã€‚GitHubåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/ruizhe-l/UniSegReg">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03229v1">PDF</a> Accepted for publication at IEEE International Symposium on   Biomedical Imaging (ISBI) 2025</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾åƒæ³¨å†Œæ¨¡å‹ç”Ÿæˆæœªæ ‡æ³¨æ•°æ®çš„ä¼ªæ ‡ç­¾çš„æ–°æ–¹æ³•ï¼Œä»¥æ”¹å–„æ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨ä½æ ‡æ³¨åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨äºŒç»´è„‘æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œä»…ä½¿ç”¨1%çš„æ ‡æ³¨æ•°æ®å³å¯å®ç°è‰¯å¥½è¡¨ç°ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„åŠç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œå¦‚å¸ˆå¾’æ¨¡å‹ç­‰ï¼Œè¯¥æ–¹æ³•æ›´ä¸ºå‡ºè‰²ã€‚å¯ä»¥é€šè¿‡GitHubè®¿é—®è¯¦ç»†ä¿¡æ¯å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/ruizhe-l/UniSegReg">https://github.com/ruizhe-l/UniSegReg</a>ã€‚</p>
<p><strong>å…³é”®æ”¶è·ç‚¹</strong></p>
<ol>
<li><p>åˆ©ç”¨åŠç›‘ç£å­¦ä¹ ï¼ˆåˆ©ç”¨æ ‡æ³¨å’Œæœªæ ‡æ³¨æ•°æ®ï¼‰æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²æ•ˆç‡ã€‚è¿™æ˜¯å› ä¸ºæ ‡æ³¨æ•´ä¸ªæ•°æ®é›†çš„æ—¶é—´æˆæœ¬é«˜ä¸”æˆæœ¬é«˜æ˜‚ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»æœªæ ‡æ³¨æ•°æ®ä¸­æå–ç‰¹å¾å’Œè¿›è¡Œæ•°æ®åˆ†å¸ƒå­¦ä¹ ä»¥æé«˜æ¨¡å‹è®­ç»ƒã€‚æœ¬ç ”ç©¶é‡‡ç”¨ä¸€ç§æ–°æ–¹æ³•ç»“åˆå›¾åƒæ³¨å†Œæ¨¡å‹ã€‚</p>
</li>
<li><p>åˆ©ç”¨å›¾åƒæ³¨å†Œæ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾çš„æŠ€æœ¯ä»¥æ”¹è¿›æ¨¡å‹è®­ç»ƒï¼Œè¿™ç§æ–¹å¼ç›¸è¾ƒäºä¼ ç»ŸåŠç›‘ç£åˆ†å‰²æ–¹æ³•èƒ½æ›´å¥½åœ°ä½¿ç”¨æœªæ ‡æ³¨æ•°æ®çš„ä¿¡æ¯ã€‚è¿™ä¸ºæå‡æ¨¡å‹çš„ç²¾åº¦æä¾›äº†æ–°çš„å¯èƒ½ã€‚è€Œä¸”å…¶ä¼˜åŠ¿åœ¨äºå³ä½¿åœ¨ä»…ä½¿ç”¨æå°‘æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿä¾ç„¶æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚åŒæ—¶å®ƒè¿˜å¸¦æ¥äº†å¦ä¸€ä¸ªå¥½å¤„ï¼šæ— éœ€åœ¨æ˜‚è´µçš„ç¡¬ä»¶è®¾å¤‡æˆ–è®­ç»ƒè¿‡ç¨‹ä¸­æŠ•å…¥å¤§é‡èµ„æºï¼Œå°±å¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4256b28562e5d42aa119ef635604ebb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bde9c7ec1d87c349e696097bbfb8a198.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c59f613aa426555eadd8dcd9139534.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Tell2Reg-Establishing-spatial-correspondence-between-images-by-the-same-language-prompts"><a href="#Tell2Reg-Establishing-spatial-correspondence-between-images-by-the-same-language-prompts" class="headerlink" title="Tell2Reg: Establishing spatial correspondence between images by the same   language prompts"></a>Tell2Reg: Establishing spatial correspondence between images by the same   language prompts</h2><p><strong>Authors:Wen Yan, Qianye Yang, Shiqi Huang, Yipei Wang, Shonit Punwani, Mark Emberton, Vasilis Stavrinides, Yipeng Hu, Dean Barratt</strong></p>
<p>Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yanwenCi/Tell2Reg.git">https://github.com/yanwenCi/Tell2Reg.git</a>. </p>
<blockquote>
<p>ç©ºé—´å¯¹åº”å…³ç³»å¯ä»¥é€šè¿‡æˆå¯¹çš„åˆ†å‰²åŒºåŸŸæ¥è¡¨ç¤ºï¼Œå›¾åƒæ³¨å†Œç½‘ç»œçš„ç›®æ ‡æ˜¯åˆ†å‰²å¯¹åº”åŒºåŸŸï¼Œè€Œä¸æ˜¯é¢„æµ‹ä½ç§»åœºæˆ–è½¬æ¢å‚æ•°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡åŸºäºGroundingDINOå’ŒSAMçš„é¢„è®­ç»ƒå¤šæ¨¡å¼æ¨¡å‹ï¼Œä½¿ç”¨åŒä¸€è¯­è¨€æç¤ºé¢„æµ‹ä¸¤ä¸ªä¸åŒå›¾åƒä¸Šçš„å¯¹åº”åŒºåŸŸå¯¹ã€‚è¿™å®ç°äº†å…¨è‡ªåŠ¨ã€æ— éœ€è®­ç»ƒçš„å›¾è±¡æ³¨å†Œç®—æ³•ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§å›¾åƒæ³¨å†Œä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¹‹ä¸€â€”â€”è·¨å—è¯•è€…å‰åˆ—è…ºMRå›¾åƒæ³¨å†Œçš„å®éªŒç»“æœï¼Œè¯¥ä»»åŠ¡æ¶‰åŠæ‚£è€…ä¹‹é—´å¼ºåº¦å’Œå½¢æ€çš„å¾ˆå¤§å˜åŒ–ã€‚Tell2Regæ— éœ€è®­ç»ƒï¼Œæ¶ˆé™¤äº†ä»¥å‰å¯¹æ­¤æ³¨å†Œä»»åŠ¡æ‰€éœ€çš„é«˜æˆæœ¬å’Œæ—¶é—´å¯†é›†çš„æ•°æ®æ•´ç†å’Œæ ‡è®°çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•ä¼˜äºç»è¿‡æµ‹è¯•çš„æ— ç›‘ç£å­¦ä¹ åŸºç¡€çš„æ³¨å†Œæ–¹æ³•ï¼Œå…¶æ€§èƒ½ä¸å¼±ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†é¢å¤–çš„å®šæ€§ç»“æœï¼Œå»ºè®®é¦–æ¬¡å‘ç°è¯­è¨€è¯­ä¹‰ä¸ç©ºé—´å¯¹åº”å…³ç³»ä¹‹é—´å­˜åœ¨æ½œåœ¨å…³è”ï¼ŒåŒ…æ‹¬è¯­è¨€æç¤ºåŒºåŸŸçš„ç©ºç®€ä¸å˜æ€§ä»¥åŠè·å¾—å±€éƒ¨å’Œå…¨å±€å¯¹åº”å…³ç³»ä¹‹é—´è¯­è¨€æç¤ºçš„å·®å¼‚ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/yanwenCi/Tell2Reg.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yanwenCi/Tell2Reg.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03118v1">PDF</a> 5 pages, 3 figures, conference paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å…¨æ–°çš„å›¾åƒæ³¨å†Œæ–¹æ³•ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ¨¡å‹ï¼Œåˆ©ç”¨ç›¸åŒçš„è¯­è¨€æç¤ºé¢„æµ‹ä¸¤ä¸ªä¸åŒå›¾åƒä¸­çš„å¯¹åº”åŒºåŸŸå¯¹ï¼Œå®ç°å…¨è‡ªåŠ¨ä¸”æ— éœ€è®­ç»ƒçš„å›¾è±¡æ³¨å†Œç®—æ³•ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§å›¾åƒæ³¨å†Œä»»åŠ¡ï¼Œå¹¶åœ¨ç—…äººé—´å‰åˆ—è…ºMRå›¾åƒæ³¨å†Œè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æ­¤æ–¹æ³•æ— éœ€è€—æ—¶è€—åŠ›çš„æ•°æ®æ•´ç†å’Œæ ‡æ³¨ï¼Œæ€§èƒ½ä¸å¼±ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚ç ”ç©¶å‘ç°è¯­è¨€è¯­ä¹‰ä¸ç©ºé—´å¯¹åº”ä¹‹é—´å¯èƒ½å­˜åœ¨æ½œåœ¨å…³è”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ³¨å†Œç½‘ç»œé€šè¿‡é¢„æµ‹å¯¹åº”åŒºåŸŸå¯¹è¡¨ç¤ºç©ºé—´å¯¹åº”å…³ç³»ï¼Œè€Œéé¢„æµ‹ä½ç§»åœºæˆ–è½¬æ¢å‚æ•°ã€‚</li>
<li>åŒä¸€è¯­è¨€æç¤ºå¯ç”¨äºé¢„æµ‹ä¸¤ä¸ªä¸åŒå›¾åƒä¸­çš„å¯¹åº”åŒºåŸŸå¯¹ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„è·¨æ¨¡æ€æ¨¡å‹ï¼ŒåŸºäºGroundingDINOå’ŒSAMï¼Œå®ç°æ— éœ€è®­ç»ƒçš„å›¾è±¡æ³¨å†Œç®—æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§å›¾åƒæ³¨å†Œä»»åŠ¡ï¼Œå¹¶åœ¨å‰åˆ—è…ºMRå›¾åƒæ³¨å†Œè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ˜‚è´µå’Œè€—æ—¶çš„æ•°æ®æ•´ç†å’Œæ ‡æ³¨çš„éœ€æ±‚ã€‚</li>
<li>æ­¤æ–¹æ³•æ€§èƒ½ä¸å¼±ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fbeb22ec28dc7dde08b039a6508f10e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c3924ca3ed7be23ae58030af5f6f785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68a1009c1662eeeb4237b95363a8ee82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b467c78441f3bdc1717c5441f67ce2b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da50a6b90c200df7957db97c4d8d451e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a84ac35bed468088b579f2ef1267c7dd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Generalizable-Features-for-Tibial-Plateau-Fracture-Segmentation-Using-Masked-Autoencoder-and-Limited-Annotations"><a href="#Learning-Generalizable-Features-for-Tibial-Plateau-Fracture-Segmentation-Using-Masked-Autoencoder-and-Limited-Annotations" class="headerlink" title="Learning Generalizable Features for Tibial Plateau Fracture Segmentation   Using Masked Autoencoder and Limited Annotations"></a>Learning Generalizable Features for Tibial Plateau Fracture Segmentation   Using Masked Autoencoder and Limited Annotations</h2><p><strong>Authors:Peiyan Yue, Die Cai, Chu Guo, Mengxing Liu, Jun Xia, Yi Wang</strong></p>
<p>Accurate automated segmentation of tibial plateau fractures (TPF) from computed tomography (CT) requires large amounts of annotated data to train deep learning models, but obtaining such annotations presents unique challenges. The process demands expert knowledge to identify diverse fracture patterns, assess severity, and account for individual anatomical variations, making the annotation process highly time-consuming and expensive. Although semi-supervised learning methods can utilize unlabeled data, existing approaches often struggle with the complexity and variability of fracture morphologies, as well as limited generalizability across datasets. To tackle these issues, we propose an effective training strategy based on masked autoencoder (MAE) for the accurate TPF segmentation in CT. Our method leverages MAE pretraining to capture global skeletal structures and fine-grained fracture details from unlabeled data, followed by fine-tuning with a small set of labeled data. This strategy reduces the dependence on extensive annotations while enhancing the modelâ€™s ability to learn generalizable and transferable features. The proposed method is evaluated on an in-house dataset containing 180 CT scans with TPF. Experimental results demonstrate that our method consistently outperforms semi-supervised methods, achieving an average Dice similarity coefficient (DSC) of 95.81%, average symmetric surface distance (ASSD) of 1.91mm, and Hausdorff distance (95HD) of 9.42mm with only 20 annotated cases. Moreover, our method exhibits strong transferability when applying to another public pelvic CT dataset with hip fractures, highlighting its potential for broader applications in fracture segmentation tasks. </p>
<blockquote>
<p>å¯¹èƒ«éª¨å¹³å°éª¨æŠ˜ï¼ˆTPFï¼‰è¿›è¡Œå‡†ç¡®çš„è‡ªåŠ¨åˆ†å‰²ï¼Œéœ€è¦ä»è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­è¯†åˆ«å‡ºå¤§é‡çš„æ ‡æ³¨æ•°æ®æ¥è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œè·å–è¿™äº›æ ‡æ³¨æ•°æ®é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚è¿™ä¸€è¿‡ç¨‹éœ€è¦ä¸“ä¸šçŸ¥è¯†æ¥è¯†åˆ«å¤šç§éª¨æŠ˜æ¨¡å¼ã€è¯„ä¼°ä¸¥é‡ç¨‹åº¦ï¼Œå¹¶è€ƒè™‘ä¸ªä½“è§£å‰–ç»“æ„å·®å¼‚ï¼Œè¿™ä½¿å¾—æ ‡æ³¨è¿‡ç¨‹éå¸¸è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚è™½ç„¶åŠç›‘ç£å­¦ä¹ æ–¹æ³•å¯ä»¥åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹éª¨æŠ˜å½¢æ€çš„å¤æ‚æ€§å’Œå·®å¼‚æ€§ï¼Œä»¥åŠæ•°æ®é›†ä¹‹é—´æœ‰é™çš„é€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„æœ‰æ•ˆè®­ç»ƒç­–ç•¥ï¼Œç”¨äºCTå›¾åƒä¸­TPFçš„å‡†ç¡®åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨MAEè¿›è¡Œé¢„è®­ç»ƒï¼Œä»æœªæ ‡æ³¨æ•°æ®ä¸­æ•è·å…¨å±€éª¨éª¼ç»“æ„å’Œç²¾ç»†çš„éª¨æŠ˜ç»†èŠ‚ï¼Œç„¶åä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æ­¤ç­–ç•¥å‡å°‘äº†å¯¹å¤§é‡æ ‡æ³¨çš„ä¾èµ–ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹å­¦ä¹ é€šç”¨å’Œå¯è¿ç§»ç‰¹å¾çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨åŒ…å«180ä¾‹TPFçš„CTæ‰«æå†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…ä½¿ç”¨20ä¸ªæ ‡æ³¨ç—…ä¾‹çš„æƒ…å†µä¸‹ï¼Œå¹³å‡Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰è¾¾åˆ°95.81%ï¼Œå¹³å‡å¯¹ç§°è¡¨é¢è·ç¦»ï¼ˆASSDï¼‰ä¸º1.91æ¯«ç±³ï¼ŒHausdorffè·ç¦»ï¼ˆ95HDï¼‰ä¸º9.42æ¯«ç±³ï¼Œæ€§èƒ½æŒç»­ä¼˜äºåŠç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤„ç†å¦ä¸€å…¬å…±éª¨ç›†CTæ•°æ®é›†ï¼ˆåŒ…å«é«‹å…³èŠ‚éª¨æŠ˜ï¼‰æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„å¯è¿ç§»æ€§ï¼Œçªæ˜¾å…¶åœ¨éª¨æŠ˜åˆ†å‰²ä»»åŠ¡ä¸­æ›´å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02862v1">PDF</a> 5 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„æœ‰æ•ˆè®­ç»ƒç­–ç•¥ï¼Œç”¨äºå‡†ç¡®åœ°å¯¹CTä¸­çš„èƒ«éª¨å¹³å°éª¨æŠ˜ï¼ˆTPFï¼‰è¿›è¡Œè‡ªåŠ¨åˆ†å‰²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨MAEçš„é¢„è®­ç»ƒåŠŸèƒ½ï¼Œä»éæ ‡è®°æ•°æ®ä¸­æ•è·å…¨å±€éª¨éª¼ç»“æ„å’Œç²¾ç»†éª¨æŠ˜ç»†èŠ‚ï¼Œç„¶åç”¨å°‘é‡æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚è¿™ç§ç­–ç•¥å‡å°‘äº†å¤§é‡æ³¨é‡Šçš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹å­¦ä¹ é€šç”¨å’Œå¯è½¬ç§»ç‰¹å¾çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŠç›‘ç£æ–¹æ³•ï¼Œå¹¶åœ¨å¦ä¸€ä¸ªå…¬å…±éª¨ç›†CTæ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„å¯è½¬ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®è‡ªåŠ¨åˆ†å‰²èƒ«éª¨å¹³å°éª¨æŠ˜å¯¹äºæ²»ç–—è®¡åˆ’å’Œé¢„åè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>è·å–æ ‡æ³¨æ•°æ®æ˜¯è¯¥é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦ä¸“ä¸šçŸ¥è¯†æ¥è¯†åˆ«å¤šæ ·çš„éª¨æŠ˜æ¨¡å¼ã€è¯„ä¼°ä¸¥é‡ç¨‹åº¦ä»¥åŠè€ƒè™‘ä¸ªä½“è§£å‰–ç»“æ„å·®å¼‚ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ æ–¹æ³•è™½å¯åˆ©ç”¨æœªæ ‡è®°æ•°æ®ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†éª¨æŠ˜å½¢æ€çš„å¤æ‚æ€§å’Œå¯å˜æ€§ï¼Œä¸”åœ¨è·¨æ•°æ®é›†æ–¹é¢çš„é€šç”¨æ€§æœ‰é™ã€‚</li>
<li>æå‡ºçš„åŸºäºæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„è®­ç»ƒç­–ç•¥èƒ½å¤Ÿåˆ©ç”¨æœªæ ‡è®°æ•°æ®æ•æ‰å…¨å±€éª¨éª¼ç»“æ„å’Œç²¾ç»†éª¨æŠ˜ç»†èŠ‚ï¼Œå¹¶é€šè¿‡å¾®è°ƒå°‘é‡æ ‡è®°æ•°æ®è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>è¯¥ç­–ç•¥å‡å°‘äº†å¤§é‡æ³¨é‡Šçš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç‰¹å¾è¿ç§»èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸åŠç›‘ç£æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„Diceç›¸ä¼¼ç³»æ•°ã€æ›´ä½çš„å¯¹ç§°è¡¨é¢è·ç¦»å’ŒHausdorffè·ç¦»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d125e73a529c414b27f146ea38b3beb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce0c9c1548c4d04baddd5f79913aad8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a7bfb94ea0e98948ee407c45d27b23e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97bd82905dba87816ee93d9e667ea553.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="When-are-Diffusion-Priors-Helpful-in-Sparse-Reconstruction-A-Study-with-Sparse-view-CT"><a href="#When-are-Diffusion-Priors-Helpful-in-Sparse-Reconstruction-A-Study-with-Sparse-view-CT" class="headerlink" title="When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with   Sparse-view CT"></a>When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with   Sparse-view CT</h2><p><strong>Authors:Matt Y. Cheung, Sophia Zorek, Tucker J. Netherton, Laurence E. Court, Sadeer Al-Kindi, Ashok Veeraraghavan, Guha Balakrishnan</strong></p>
<p>Diffusion models demonstrate state-of-the-art performance on image generation, and are gaining traction for sparse medical image reconstruction tasks. However, compared to classical reconstruction algorithms relying on simple analytical priors, diffusion models have the dangerous property of producing realistic looking results \emph{even when incorrect}, particularly with few observations. We investigate the utility of diffusion models as priors for image reconstruction by varying the number of observations and comparing their performance to classical priors (sparse and Tikhonov regularization) using pixel-based, structural, and downstream metrics. We make comparisons on low-dose chest wall computed tomography (CT) for fat mass quantification. First, we find that classical priors are superior to diffusion priors when the number of projections is &#96;&#96;sufficientâ€™â€™. Second, we find that diffusion priors can capture a large amount of detail with very few observations, significantly outperforming classical priors. However, they fall short of capturing all details, even with many observations. Finally, we find that the performance of diffusion priors plateau after extremely few ($\approx$10-15) projections. Ultimately, our work highlights potential issues with diffusion-based sparse reconstruction and underscores the importance of further investigation, particularly in high-stakes clinical settings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶ä¸”æ­£é€æ¸åº”ç”¨äºç¨€ç–åŒ»å­¦å›¾åƒé‡å»ºä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸ä¾èµ–äºç®€å•åˆ†æå…ˆéªŒçŸ¥è¯†çš„ä¼ ç»Ÿé‡å»ºç®—æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹å…·æœ‰ä¸€ä¸ªå±é™©çš„ç‰¹æ€§ï¼Œå³å³ä½¿ç»“æœä¸æ­£ç¡®ï¼Œä¹Ÿèƒ½äº§ç”Ÿé€¼çœŸçš„å›¾åƒï¼Œç‰¹åˆ«æ˜¯åœ¨è§‚æµ‹æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬é€šè¿‡æ”¹å˜è§‚æµ‹æ•°æ®çš„æ•°é‡ï¼Œç ”ç©¶æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒé‡å»ºå…ˆéªŒçŸ¥è¯†çš„å®ç”¨æ€§ï¼Œå¹¶åˆ©ç”¨åƒç´ çº§ã€ç»“æ„çº§å’Œä¸‹æ¸¸æŒ‡æ ‡ä¸ç»å…¸å…ˆéªŒçŸ¥è¯†ï¼ˆç¨€ç–å’ŒTikhonovæ­£åˆ™åŒ–ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¯¹ä½å‰‚é‡èƒ¸éƒ¨å£è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰è¿›è¡Œè„‚è‚ªè´¨é‡å®šé‡æ¯”è¾ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‘ç°å½“æŠ•å½±æ•°é‡â€œè¶³å¤Ÿâ€æ—¶ï¼Œç»å…¸å…ˆéªŒçŸ¥è¯†ä¼˜äºæ‰©æ•£å…ˆéªŒçŸ¥è¯†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£å…ˆéªŒçŸ¥è¯†èƒ½å¤Ÿåœ¨å¾ˆå°‘çš„è§‚æµ‹æ•°æ®ä¸‹æ•æ‰åˆ°å¤§é‡ç»†èŠ‚ï¼Œæ˜¾è‘—ä¼˜äºç»å…¸å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å¾ˆå¤šè§‚æµ‹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¹Ÿæ— æ³•æ•æ‰åˆ°æ‰€æœ‰ç»†èŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£å…ˆéªŒçŸ¥è¯†çš„æ€§èƒ½åœ¨æå°‘çš„ï¼ˆçº¦10-15ä¸ªï¼‰æŠ•å½±åè¾¾åˆ°å¹³ç¨³ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–é‡å»ºä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†è¿›ä¸€æ­¥è°ƒæŸ¥çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02771v1">PDF</a> Accepted at IEEE ISBI 2025, 5 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨ç¨€ç–åŒ»å­¦å›¾åƒé‡å»ºä»»åŠ¡ä¸­è·å¾—å…³æ³¨ã€‚æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒé‡å»ºå…ˆéªŒçš„æ•ˆç”¨ï¼Œé€šè¿‡æ”¹å˜è§‚æµ‹æ•°é‡ï¼Œä¸ç»å…¸å…ˆéªŒï¼ˆç¨€ç–å’ŒTikhonovæ­£åˆ™åŒ–ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶åœ¨ä½å‰‚é‡èƒ¸è…”è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­è¿›è¡Œè„‚è‚ªè´¨é‡é‡åŒ–çš„è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è§‚æµ‹æ•°é‡å……è¶³æ—¶ï¼Œç»å…¸å…ˆéªŒä¼˜äºæ‰©æ•£å…ˆéªŒï¼›æ‰©æ•£å…ˆéªŒåœ¨è§‚æµ‹æ•°é‡æå°‘æ—¶å³å¯æ•æ‰å¤§é‡ç»†èŠ‚ï¼Œæ˜¾è‘—ä¼˜äºç»å…¸å…ˆéªŒï¼Œä½†åœ¨æ•æ‰æ‰€æœ‰ç»†èŠ‚æ–¹é¢ä»æœ‰ä¸è¶³ï¼›æ­¤å¤–ï¼Œæ‰©æ•£å…ˆéªŒåœ¨æå°‘æŠ•å½±åçš„æ€§èƒ½è¶‹äºç¨³å®šã€‚æœ¬æ–‡å¼ºè°ƒäº†æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–é‡å»ºä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†åœ¨é«˜é£é™©ä¸´åºŠç¯å¢ƒä¸­è¿›ä¸€æ­¥è°ƒæŸ¥çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å“è¶Šï¼Œå¹¶åœ¨åŒ»å­¦å›¾åƒé‡å»ºä¸­è·å¾—å…³æ³¨ã€‚</li>
<li>ç›¸æ¯”ç»å…¸å…ˆéªŒï¼Œæ‰©æ•£æ¨¡å‹åœ¨è§‚æµ‹æ•°é‡æå°‘æ—¶ä»èƒ½æ•æ‰å¤§é‡ç»†èŠ‚ã€‚</li>
<li>åœ¨å……è¶³è§‚æµ‹ä¸‹ï¼Œç»å…¸å…ˆéªŒè¡¨ç°ä¼˜äºæ‰©æ•£å…ˆéªŒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰æ‰€æœ‰ç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æ‰©æ•£å…ˆéªŒçš„æ€§èƒ½åœ¨æå°‘æŠ•å½±åè¶‹äºç¨³å®šã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–é‡å»ºä¸­å­˜åœ¨æ½œåœ¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88e8c3f6d18bae8ad9a199ebfef865f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-278810dc85640d2ab66d703cf421a6e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-054f5ae381b60c7b61857cd1ba0a099e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b90e06d4e86f5c7b3bbe3e8dc569ddb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Adaptive-Voxel-Weighted-Loss-Using-L1-Norms-in-Deep-Neural-Networks-for-Detection-and-Segmentation-of-Prostate-Cancer-Lesions-in-PET-CT-Images"><a href="#Adaptive-Voxel-Weighted-Loss-Using-L1-Norms-in-Deep-Neural-Networks-for-Detection-and-Segmentation-of-Prostate-Cancer-Lesions-in-PET-CT-Images" class="headerlink" title="Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for   Detection and Segmentation of Prostate Cancer Lesions in PET&#x2F;CT Images"></a>Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for   Detection and Segmentation of Prostate Cancer Lesions in PET&#x2F;CT Images</h2><p><strong>Authors:Obed Korshie Dzikunu, Shadab Ahamed, Amirhossein Toosi, Xiaoxiao Li, Arman Rahmim</strong></p>
<p>This study proposes a new loss function for deep neural networks, L1-weighted Dice Focal Loss (L1DFL), that leverages L1 norms for adaptive weighting of voxels based on their classification difficulty, towards automated detection and segmentation of metastatic prostate cancer lesions in PET&#x2F;CT scans. We obtained 380 PSMA [18-F] DCFPyL PET&#x2F;CT scans of patients diagnosed with biochemical recurrence metastatic prostate cancer. We trained two 3D convolutional neural networks, Attention U-Net and SegResNet, and concatenated the PET and CT volumes channel-wise as input. The performance of our custom loss function was evaluated against the Dice and Dice Focal Loss functions. For clinical significance, we considered a detected region of interest (ROI) as a true positive if at least the voxel with the maximum standardized uptake value falls within the ROI. We assessed the modelsâ€™ performance based on the number of lesions in an image, tumour volume, activity, and extent of spread. The L1DFL outperformed the comparative loss functions by at least 13% on the test set. In addition, the F1 scores of the Dice Loss and the Dice Focal Loss were lower than that of L1DFL by at least 6% and 34%, respectively. The Dice Focal Loss yielded more false positives, whereas the Dice Loss was more sensitive to smaller volumes and struggled to segment larger lesions accurately. They also exhibited network-specific variations and yielded declines in segmentation accuracy with increased tumour spread. Our results demonstrate the potential of L1DFL to yield robust segmentation of metastatic prostate cancer lesions in PSMA PET&#x2F;CT images. The results further highlight potential complexities arising from the variations in lesion characteristics that may influence automated prostate cancer tumour detection and segmentation. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ObedDzik/pca_segment.git">https://github.com/ObedDzik/pca_segment.git</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œæå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œå³L1åŠ æƒDice Focal Lossï¼ˆL1DFLï¼‰ã€‚å®ƒåˆ©ç”¨L1èŒƒæ•°æ ¹æ®åˆ†ç±»éš¾åº¦å¯¹ä½“ç´ è¿›è¡Œè‡ªé€‚åº”åŠ æƒï¼Œæ—¨åœ¨å®ç°PET&#x2F;CTæ‰«æä¸­è½¬ç§»æ€§å‰åˆ—è…ºç™Œç—…å˜çš„è‡ªåŠ¨æ£€æµ‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬è·å¾—äº†380ä¾‹è¢«è¯Šæ–­ä¸ºç”ŸåŒ–å¤å‘è½¬ç§»æ€§å‰åˆ—è…ºç™Œæ‚£è€…çš„PSMA [18-F] DCFPyL PET&#x2F;CTæ‰«æã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ª3Då·ç§¯ç¥ç»ç½‘ç»œï¼Œå³Attention U-Netå’ŒSegResNetï¼Œå¹¶å°†PETå’ŒCTä½“ç§¯æŒ‰é€šé“åˆå¹¶ä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°çš„æ€§èƒ½ä¸Diceå’ŒDice Focal Losså‡½æ•°è¿›è¡Œäº†è¯„ä¼°ã€‚å¯¹äºä¸´åºŠæ„ä¹‰ï¼Œå¦‚æœæœ€å¤§æ ‡å‡†åŒ–æ‘„å–å€¼çš„ä½“ç´ ä½äºæ„Ÿå…´è¶£åŒºåŸŸå†…ï¼Œåˆ™æˆ‘ä»¬å°†å…¶è§†ä¸ºçœŸæ­£çš„é˜³æ€§åŒºåŸŸã€‚æˆ‘ä»¬æ ¹æ®å›¾åƒä¸­çš„ç—…å˜æ•°é‡ã€è‚¿ç˜¤ä½“ç§¯ã€æ´»åŠ¨æ€§ä»¥åŠæ‰©æ•£ç¨‹åº¦è¯„ä¼°äº†æ¨¡å‹æ€§èƒ½ã€‚åœ¨æµ‹è¯•é›†ä¸Šï¼ŒL1DFLè‡³å°‘æ¯”å¯¹æ¯”æŸå¤±å‡½æ•°é«˜å‡º13%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDice Losså’ŒDice Focal Lossçš„F1åˆ†æ•°è‡³å°‘æ¯”L1DFLä½6%å’Œ34%ã€‚Dice Focal Lossäº§ç”Ÿäº†æ›´å¤šçš„å‡é˜³æ€§ï¼Œè€ŒDice Losså¯¹è¾ƒå°çš„ä½“ç§¯æ›´æ•æ„Ÿï¼Œå¹¶ä¸”åœ¨åˆ†å‰²è¾ƒå¤§çš„ç—…å˜æ—¶å‡†ç¡®æ€§è¾ƒå·®ã€‚å®ƒä»¬è¿˜è¡¨ç°å‡ºç½‘ç»œç‰¹å®šçš„å˜åŒ–ï¼Œå¹¶éšç€è‚¿ç˜¤æ‰©æ•£çš„å¢åŠ è€Œå¯¼è‡´åˆ†å‰²ç²¾åº¦ä¸‹é™ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†L1DFLåœ¨PSMA PET&#x2F;CTå›¾åƒä¸­ç¨³å¥åˆ†å‰²è½¬ç§»æ€§å‰åˆ—è…ºç™Œç—…å˜çš„æ½œåŠ›ã€‚ç»“æœè¿˜è¿›ä¸€æ­¥å¼ºè°ƒäº†ç”±äºç—…å˜ç‰¹å¾çš„å·®å¼‚æ‰€äº§ç”Ÿçš„æ½œåœ¨å¤æ‚æ€§ï¼Œè¿™å¯èƒ½ä¼šå½±å“è‡ªåŠ¨åŒ–å‰åˆ—è…ºç™Œè‚¿ç˜¤æ£€æµ‹å’Œåˆ†å‰²ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ObedDzik/pca_segment.git%E3%80%82">https://github.com/ObedDzik/pca_segment.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02756v1">PDF</a> 29 pages, 7 figures, 1 table</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æŸå¤±å‡½æ•°ï¼Œåä¸ºL1èŒƒæ•°åŠ æƒçš„Dice Focal Lossï¼ˆL1DFLï¼‰ï¼Œç”¨äºè‡ªé€‚åº”åŠ æƒåˆ†ç±»éš¾åº¦çš„ä½“ç´ ã€‚æ­¤å‡½æ•°åº”ç”¨äºPET&#x2F;CTæ‰«æä¸­è½¬ç§»æ€§å‰åˆ—è…ºç™Œç—…å˜çš„è‡ªåŠ¨æ£€æµ‹å’Œåˆ†å‰²ã€‚ç ”ç©¶ä½¿ç”¨äº†380ä¾‹PSMA [18-F] DCFPyL PET&#x2F;CTæ‰«æçš„ç—…æ‚£æ•°æ®ï¼Œå¹¶è®­ç»ƒäº†ä¸¤ç§3Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆAttention U-Netå’ŒSegResNetï¼‰ã€‚è¯¥ç ”ç©¶å°†PETå’ŒCTä½“ç§¯æŒ‰é€šé“æ–¹å¼ç»“åˆä½œä¸ºè¾“å…¥ï¼Œå¹¶è¯„ä¼°äº†è‡ªå®šä¹‰æŸå¤±å‡½æ•°ä¸Diceå’ŒDice FocalæŸå¤±å‡½æ•°çš„æ€§èƒ½ã€‚åœ¨ä¸´åºŠæ„ä¹‰æ–¹é¢ï¼Œå¦‚æœæœ€å¤§æ ‡å‡†åŒ–æ‘„å–å€¼çš„ä½“ç´ ä½äºæ„Ÿå…´è¶£åŒºåŸŸå†…ï¼Œåˆ™å°†å…¶è§†ä¸ºçœŸæ­£çš„é˜³æ€§ã€‚æ ¹æ®å›¾åƒä¸­çš„ç—…å˜æ•°é‡ã€è‚¿ç˜¤ä½“ç§¯ã€æ´»åŠ¨æ€§å’Œæ‰©æ•£ç¨‹åº¦è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚L1DFLåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è‡³å°‘æ¯”å¯¹æ¯”æŸå¤±å‡½æ•°é«˜å‡º13%ã€‚æ­¤å¤–ï¼ŒDice Losså’ŒDice Focal Lossçš„F1åˆ†æ•°è‡³å°‘æ¯”L1DFLä½6%å’Œ34%ã€‚Dice Focal Lossäº§ç”Ÿæ›´å¤šçš„å‡é˜³æ€§ï¼Œè€ŒDice Losså¯¹è¾ƒå°çš„ä½“ç§¯æ›´æ•æ„Ÿï¼Œå¹¶ä¸”éš¾ä»¥å‡†ç¡®åˆ†å‰²è¾ƒå¤§çš„ç—…å˜ã€‚ä»–ä»¬çš„ç½‘ç»œç‰¹å®šå˜åŒ–éšç€è‚¿ç˜¤æ‰©æ•£çš„å¢åŠ ï¼Œåˆ†å‰²ç²¾åº¦ä¸‹é™ã€‚ç ”ç©¶ç»“æœå±•ç¤ºäº†L1DFLåœ¨PSMA PET&#x2F;CTå›¾åƒä¸­ç¨³å¥åˆ†å‰²è½¬ç§»æ€§å‰åˆ—è…ºç™Œç—…å˜çš„æ½œåŠ›ï¼Œå¹¶çªå‡ºäº†ç—…å˜ç‰¹å¾å˜åŒ–å¯¹è‡ªåŠ¨å‰åˆ—è…ºç™Œè‚¿ç˜¤æ£€æµ‹å’Œåˆ†å‰²çš„å½±å“ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/ObedDzik/pca_segment.git%E3%80%82">https://github.com/ObedDzik/pca_segment.gitã€‚</a></p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°L1DFLï¼Œç”¨äºæ·±åº¦ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«å…³æ³¨äºè‡ªé€‚åº”åŠ æƒä½“ç´ åœ¨è½¬ç§»æ€§å‰åˆ—è…ºç™Œç—…å˜æ£€æµ‹ä¸åˆ†å‰²ä¸­çš„åº”ç”¨ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨PSMA [18-F] DCFPyL PET&#x2F;CTæ‰«ææ•°æ®ï¼Œè®­ç»ƒäº†ä¸¤ç§3Då·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</li>
<li>L1DFLåœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„Dice Losså’ŒDice Focal Lossï¼Œè‡³å°‘é«˜å‡º13%ã€‚</li>
<li>Dice Focal Lossäº§ç”Ÿè¾ƒå¤šå‡é˜³æ€§ï¼Œè€ŒDice Losså¯¹å°ä½“ç§¯æ›´æ•æ„Ÿï¼Œå¯¹å¤§å‹ç—…å˜çš„åˆ†å‰²å‡†ç¡®æ€§è¾ƒå·®ã€‚</li>
<li>éšç€è‚¿ç˜¤æ‰©æ•£çš„å¢åŠ ï¼Œæ‰€æœ‰æŸå¤±å‡½æ•°éƒ½è¡¨ç°å‡ºç½‘ç»œç‰¹å®šçš„å˜åŒ–åŠåˆ†å‰²ç²¾åº¦çš„ä¸‹é™ã€‚</li>
<li>L1DFLå…·æœ‰åœ¨PSMA PET&#x2F;CTå›¾åƒä¸­ç¨³å¥åˆ†å‰²è½¬ç§»æ€§å‰åˆ—è…ºç™Œç—…å˜çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4a81242b3d3d3e10016008ff122c6e9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RFMedSAM-2-Automatic-Prompt-Refinement-for-Enhanced-Volumetric-Medical-Image-Segmentation-with-SAM-2"><a href="#RFMedSAM-2-Automatic-Prompt-Refinement-for-Enhanced-Volumetric-Medical-Image-Segmentation-with-SAM-2" class="headerlink" title="RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical   Image Segmentation with SAM 2"></a>RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical   Image Segmentation with SAM 2</h2><p><strong>Authors:Bin Xie, Hao Tang, Yan Yan, Gady Agam</strong></p>
<p>Segment Anything Model 2 (SAM 2), a prompt-driven foundation model extending SAM to both image and video domains, has shown superior zero-shot performance compared to its predecessor. Building on SAMâ€™s success in medical image segmentation, SAM 2 presents significant potential for further advancement. However, similar to SAM, SAM 2 is limited by its output of binary masks, inability to infer semantic labels, and dependence on precise prompts for the target object area. Additionally, direct application of SAM and SAM 2 to medical image segmentation tasks yields suboptimal results. In this paper, we explore the upper performance limit of SAM 2 using custom fine-tuning adapters, achieving a Dice Similarity Coefficient (DSC) of 92.30% on the BTCV dataset, surpassing the state-of-the-art nnUNet by 12%. Following this, we address the prompt dependency by investigating various prompt generators. We introduce a UNet to autonomously generate predicted masks and bounding boxes, which serve as input to SAM 2. Subsequent dual-stage refinements by SAM 2 further enhance performance. Extensive experiments show that our method achieves state-of-the-art results on the AMOS2022 dataset, with a Dice improvement of 2.9% compared to nnUNet, and outperforms nnUNet by 6.4% on the BTCV dataset. </p>
<blockquote>
<p>Segment Anything Model 2ï¼ˆSAM 2ï¼‰æ˜¯ä¸€ä¸ªæç¤ºé©±åŠ¨çš„åŸºç¡€æ¨¡å‹ï¼Œå®ƒå°†SAMæ‰©å±•åˆ°å›¾åƒå’Œè§†é¢‘é¢†åŸŸï¼Œå¹¶æ˜¾ç¤ºå‡ºæ¯”å…¶å‰èº«æ›´å‡ºè‰²çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚å»ºç«‹åœ¨SAMæˆåŠŸè¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²çš„åŸºç¡€ä¸Šï¼ŒSAM 2å‘ˆç°å‡ºå·¨å¤§çš„å‘å±•æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸SAMç±»ä¼¼ï¼ŒSAM 2å—é™äºå…¶è¾“å‡ºäºŒè¿›åˆ¶è’™ç‰ˆã€æ— æ³•æ¨æ–­è¯­ä¹‰æ ‡ç­¾ä»¥åŠå¯¹ç›®æ ‡å¯¹è±¡åŒºåŸŸç²¾ç¡®æç¤ºçš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œç›´æ¥å°†SAMå’ŒSAM 2åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¼šäº§ç”Ÿä¸ç†æƒ³çš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰å¾®è°ƒé€‚é…å™¨æ¢ç´¢äº†SAM 2çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨BTCVæ•°æ®é›†ä¸Šå®ç°äº†92.30%çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ï¼Œè¶…è¿‡äº†ç°å­˜çš„nnUNetæ¨¡å‹ï¼Œæé«˜äº†12%ã€‚ä¹‹åï¼Œæˆ‘ä»¬é€šè¿‡ç ”ç©¶å„ç§æç¤ºç”Ÿæˆå™¨æ¥è§£å†³æç¤ºä¾èµ–é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªUNetæ¥è‡ªä¸»ç”Ÿæˆé¢„æµ‹è’™ç‰ˆå’Œè¾¹ç•Œæ¡†ï¼Œä½œä¸ºSAM 2çš„è¾“å…¥ã€‚éšåï¼ŒSAM 2çš„ä¸¤é˜¶æ®µç²¾ç»†åŒ–è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AMOS2022æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸nnUNetç›¸æ¯”ï¼ŒDiceç³»æ•°æé«˜äº†2.9%ï¼Œåœ¨BTCVæ•°æ®é›†ä¸Šè¶…è¿‡äº†nnUNet 6.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02741v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAM 2æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ— è®­ç»ƒæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡é€šè¿‡è‡ªå®šä¹‰å¾®è°ƒé€‚é…å™¨å’Œè‡ªä¸»ç”Ÿæˆçš„æç¤ºæ©è†œå’Œè¾¹ç•Œæ¡†ï¼Œæé«˜äº†SAM 2çš„æ€§èƒ½ï¼Œå®ç°äº†åœ¨BTCVæ•°æ®é›†ä¸Šçš„DSCè¾¾åˆ°92.3%ï¼Œå¹¶è¶…è¶Šäº†æœ€æ–°æ–¹æ³•çš„æ€§èƒ½ã€‚é€šè¿‡åŒé˜¶æ®µç²¾ç»†è°ƒæ•´ï¼Œè¯¥æ–¹æ³•åœ¨AMOS2022æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAM 2æ¨¡å‹æ‰©å±•äº†SAMæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘é¢†åŸŸçš„åº”ç”¨ï¼Œå±•ç°å‡ºå“è¶Šçš„æ— è®­ç»ƒæ€§èƒ½ã€‚</li>
<li>SAM 2åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†å—é™äºè¾“å‡ºä¸ºäºŒè¿›åˆ¶æ©ç ã€æ— æ³•æ¨æ–­è¯­ä¹‰æ ‡ç­¾ä»¥åŠä¾èµ–ç²¾ç¡®çš„ç›®æ ‡å¯¹è±¡åŒºåŸŸæç¤ºã€‚</li>
<li>é€šè¿‡è‡ªå®šä¹‰çš„å¾®è°ƒé€‚é…å™¨ï¼ŒSAM 2çš„æ€§èƒ½å¾—åˆ°äº†æå‡ï¼Œå®ç°äº†åœ¨BTCVæ•°æ®é›†ä¸Šçš„DSCé«˜è¾¾92.3%ï¼Œè¶…è¶Šäº†nnUNetçš„æ–¹æ³•ã€‚</li>
<li>å¼•å…¥UNetè‡ªä¸»ç”Ÿæˆé¢„æµ‹æ©ç å’Œè¾¹ç•Œæ¡†ï¼Œä½œä¸ºSAM 2çš„è¾“å…¥ï¼Œè§£å†³äº†å¯¹æç¤ºçš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡SAM 2çš„åŒé˜¶æ®µç²¾ç»†è°ƒæ•´ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚</li>
<li>åœ¨AMOS2022æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€ä½³ç»“æœï¼Œç›¸è¾ƒäºnnUNetï¼ŒDiceç³»æ•°æé«˜äº†2.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a7ce02659f5c16477699b6cdf88f620.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27256fc48e633e6d3e1635478fbc8136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e086ec7ba21c67513ac8d2144cbf2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c88f0f3f89dfa0959e8ea0451292fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e1cd54a8cbfd7dd25c7761e60e50bb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedRAX-Medical-Reasoning-Agent-for-Chest-X-ray"><a href="#MedRAX-Medical-Reasoning-Agent-for-Chest-X-ray" class="headerlink" title="MedRAX: Medical Reasoning Agent for Chest X-ray"></a>MedRAX: Medical Reasoning Agent for Chest X-ray</h2><p><strong>Authors:Adibvafa Fallahpour, Jun Ma, Alif Munim, Hongwei Lyu, Bo Wang</strong></p>
<p>Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at <a target="_blank" rel="noopener" href="https://github.com/bowang-lab/MedRAX">https://github.com/bowang-lab/MedRAX</a> </p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRsï¼‰åœ¨ç–¾ç—…ç®¡ç†å’Œæ‚£è€…æŠ¤ç†çš„å…³é”®å†³ç­–ä¸­æ‰®æ¼”ç€ä¸å¯æˆ–ç¼ºçš„è§’è‰²ã€‚è™½ç„¶æœ€è¿‘çš„åˆ›æ–°å·²ç»å¯¼è‡´å„ç§CXRè§£é‡Šä»»åŠ¡çš„ä¸“ç”¨æ¨¡å‹çš„å‡ºç°ï¼Œä½†è¿™äº›è§£å†³æ–¹æ¡ˆé€šå¸¸ç‹¬ç«‹è¿è¡Œï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠå®è·µä¸­çš„å®ç”¨æ•ˆç”¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†MedRAXï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šç”¨çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œå®ƒå°†æœ€å…ˆè¿›çš„CXRåˆ†æå·¥å…·å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚MedRAXåŠ¨æ€åˆ©ç”¨è¿™äº›æ¨¡å‹æ¥è§£å†³å¤æ‚çš„åŒ»ç–—æŸ¥è¯¢ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ä¸ºäº†ä¸¥æ ¼è¯„ä¼°å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ChestAgentBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2500ä¸ªå¤æ‚åŒ»ç–—æŸ¥è¯¢çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–7ä¸ªä¸åŒçš„ç±»åˆ«ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸å¼€æºå’Œä¸“æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒMedRAXå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿™ä»£è¡¨äº†åœ¨å®é™…éƒ¨ç½²è‡ªåŠ¨åŒ–CXRè§£é‡Šç³»ç»Ÿæ–¹é¢å–å¾—äº†é‡è¦çš„ä¸€æ­¥ã€‚æ•°æ®å’Œä»£ç å·²å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/bowang-lab/MedRAX">https://github.com/bowang-lab/MedRAX</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02673v1">PDF</a> 11 pages, 4 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†MedRAXç³»ç»ŸåŠå…¶åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä»·å€¼ã€‚ä½œä¸ºé¦–ä¸ªæ•´åˆäº†å…ˆè¿›è®¡ç®—æœºåŒ–Xå…‰å½±åƒåˆ†æå·¥å…·å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼ŒMedRAXèƒ½å¤ŸåŠ¨æ€åˆ©ç”¨è¿™äº›æ¨¡å‹è§£å†³å¤æ‚çš„åŒ»å­¦é—®é¢˜ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ç”¨äºè¯„ä¼°å…¶æ€§èƒ½çš„ChestAgentBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œå±•ç¤ºäº†MedRAXç›¸è¾ƒäºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹åœ¨æ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MedRAXæ˜¯ä¸€ä¸ªé›†æˆäº†å…ˆè¿›CXRåˆ†æå·¥å…·å’Œå¤šç§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿè§£å†³å¤æ‚çš„åŒ»å­¦é—®é¢˜ã€‚</li>
<li>MedRAXæ— éœ€é¢å¤–è®­ç»ƒå³å¯åŠ¨æ€åˆ©ç”¨å¤šç§æ¨¡å‹è¿›è¡Œè¯Šæ–­ã€‚</li>
<li>MedRAXé€šè¿‡ChestAgentBenchåŸºå‡†æµ‹è¯•å¹³å°è¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>MedRAXå®ç°äº†ç›¸è¾ƒäºå…¶ä»–å¼€æºå’Œä¸“æœ‰æ¨¡å‹åœ¨CXRè§£è¯»é¢†åŸŸçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>MedRAXç³»ç»Ÿçš„æ•°æ®ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºå…±äº«å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8eb618b10a5ef8729180cba83e8f3008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4163c9424d383c2999ff567b7073cbe6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b69ac3ec8cd611a0966fc928b7a361d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98bfe0e61646743bad5ee8e179c5bce9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Graph-Structure-Learning-for-Tumor-Microenvironment-with-Cell-Type-Annotation-from-non-spatial-scRNA-seq-data"><a href="#Graph-Structure-Learning-for-Tumor-Microenvironment-with-Cell-Type-Annotation-from-non-spatial-scRNA-seq-data" class="headerlink" title="Graph Structure Learning for Tumor Microenvironment with Cell Type   Annotation from non-spatial scRNA-seq data"></a>Graph Structure Learning for Tumor Microenvironment with Cell Type   Annotation from non-spatial scRNA-seq data</h2><p><strong>Authors:Yu-An Huang, Yue-Chao Li, Hai-Ru You, Jie Pan, Xiyue Cao, Xinyuan Li, Zhi-An Huang, Zhu-Hong You</strong></p>
<p>The exploration of cellular heterogeneity within the tumor microenvironment (TME) via single-cell RNA sequencing (scRNA-seq) is essential for understanding cancer progression and response to therapy. Current scRNA-seq approaches, however, lack spatial context and rely on incomplete datasets of ligand-receptor interactions (LRIs), limiting accurate cell type annotation and cell-cell communication (CCC) inference. This study addresses these challenges using a novel graph neural network (GNN) model that enhances cell type prediction and cell interaction analysis. Our study utilized a dataset consisting of 49,020 cells from 19 patients across three cancer types: Leukemia, Breast Invasive Carcinoma, and Colorectal Cancer. The proposed scGSL model demonstrated robust performance, achieving an average accuracy of 84.83%, precision of 86.23%, recall of 81.51%, and an F1 score of 80.92% across all datasets. These metrics represent a significant enhancement over existing methods, which typically exhibit lower performance metrics. Additionally, by reviewing existing literature on gene interactions within the TME, the scGSL model proves to robustly identify biologically meaningful gene interactions in an unsupervised manner, validated by significant expression differences in key gene pairs across various cancers. The source code and data used in this paper can be found in <a target="_blank" rel="noopener" href="https://github.com/LiYuechao1998/scGSL">https://github.com/LiYuechao1998/scGSL</a>. </p>
<blockquote>
<p>é€šè¿‡å•ç»†èƒRNAæµ‹åºï¼ˆscRNA-seqï¼‰æ¢ç´¢è‚¿ç˜¤å¾®ç¯å¢ƒï¼ˆTMEï¼‰ä¸­çš„ç»†èƒå¼‚è´¨æ€§å¯¹äºç†è§£ç™Œç—‡è¿›å±•å’Œæ²»ç–—æ•ˆæœè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„scRNA-seqæ–¹æ³•ç¼ºä¹ç©ºé—´èƒŒæ™¯ï¼Œä¾èµ–äºä¸å®Œå…¨çš„é…ä½“-å—ä½“ç›¸äº’ä½œç”¨ï¼ˆLRIï¼‰æ•°æ®é›†ï¼Œé™åˆ¶äº†å‡†ç¡®çš„ç»†èƒç±»å‹æ³¨é‡Šå’Œç»†èƒé—´é€šä¿¡ï¼ˆCCCï¼‰æ¨æ–­ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ–°å‹å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨¡å‹æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æ¨¡å‹å¯æé«˜ç»†èƒç±»å‹é¢„æµ‹å’Œç»†èƒç›¸äº’ä½œç”¨åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ä½¿ç”¨äº†åŒ…å«æ¥è‡ªä¸‰ç§ç™Œç—‡ç±»å‹ï¼ˆç™½è¡€ç—…ã€ä¾µè¢­æ€§ä¹³è…ºç™Œå’Œç»“è‚ ç™Œï¼‰çš„19åæ‚£è€…çš„49,020ä¸ªç»†èƒçš„æ•°æ®åº“ã€‚æ‰€æå‡ºçš„scGSLæ¨¡å‹è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®åº¦ä¸º84.83%ï¼Œç²¾ç¡®åº¦ä¸º86.23%ï¼Œå¬å›ç‡ä¸º81.51%ï¼ŒF1åˆ†æ•°ä¸º80.92%ã€‚è¿™äº›æŒ‡æ ‡ç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰äº†æ˜¾è‘—çš„æå‡ï¼Œé€šå¸¸ç°æœ‰æ–¹æ³•çš„æ€§èƒ½æŒ‡æ ‡è¾ƒä½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹è‚¿ç˜¤å¾®ç¯å¢ƒä¸­åŸºå› ç›¸äº’ä½œç”¨çš„ç°æœ‰æ–‡çŒ®è¿›è¡Œå›é¡¾ï¼ŒscGSLæ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°è¯†åˆ«å‡ºç”Ÿç‰©å­¦ä¸Šæœ‰æ„ä¹‰çš„åŸºå› ç›¸äº’ä½œç”¨ï¼Œè¿™æ˜¯ä»¥æ— ç›‘ç£çš„æ–¹å¼è¿›è¡Œçš„ï¼Œå¹¶é€šè¿‡å„ç§ç™Œç—‡ä¸­å…³é”®åŸºå› å¯¹è¡¨è¾¾å·®å¼‚çš„æ˜¾è‘—æ€§å¾—åˆ°äº†éªŒè¯ã€‚æœ¬æ–‡ä½¿ç”¨çš„æºä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LiYuechao1998/scGSL">https://github.com/LiYuechao1998/scGSL</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02629v1">PDF</a> 29 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå•ç»†èƒRNAæµ‹åºï¼ˆscRNA-seqï¼‰æŠ€æœ¯ï¼Œæ¢ç´¢è‚¿ç˜¤å¾®ç¯å¢ƒï¼ˆTMEï¼‰ä¸­çš„ç»†èƒå¼‚è´¨æ€§å¯¹äºç†è§£ç™Œç—‡è¿›å±•å’Œæ²»ç–—ååº”è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ–°å‹å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨¡å‹åº”å¯¹ç°æœ‰scRNA-seqæ–¹æ³•ç¼ºä¹ç©ºé—´ä¸Šä¸‹æ–‡å’Œä¸å®Œå…¨çš„é…ä½“-å—ä½“ç›¸äº’ä½œç”¨æ•°æ®é›†çš„é—®é¢˜ï¼Œæå‡äº†ç»†èƒç±»å‹é¢„æµ‹å’Œç»†èƒäº¤äº’åˆ†æçš„èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨åŒ…å«æ¥è‡ªç™½è¡€ç—…ã€ä¹³è…ºæµ¸æ¶¦æ€§ç™Œå’Œç»“ç›´è‚ ç™Œçš„19åæ‚£è€…å…±49ï¼Œ020ä¸ªç»†èƒçš„æ•°æ®åº“ï¼Œæ‰€æscGSLæ¨¡å‹å±•ç°å‡ºç¨³å¥æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†84.83%ï¼Œç²¾åº¦ä¸º86.23%ï¼Œå¬å›ç‡ä¸º81.51%ï¼ŒF1åˆ†æ•°ä¸º80.92%ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ€§èƒ½æœ‰æ‰€æå‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹è‚¿ç˜¤å¾®ç¯å¢ƒä¸­åŸºå› äº¤äº’çš„ç°æœ‰æ–‡çŒ®è¿›è¡Œå›é¡¾ï¼ŒscGSLæ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°è¯†åˆ«å‡ºç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› äº¤äº’ï¼Œä¸”åœ¨å¤šç§ç™Œç—‡ä¸­å…³é”®åŸºå› å¯¹çš„è¡¨è¾¾å·®å¼‚æ˜¾è‘—ã€‚ç›¸å…³ç ”ç©¶ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LiYuechao1998/scGSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LiYuechao1998/scGSLæ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä½¿ç”¨å•ç»†èƒRNAæµ‹åºæŠ€æœ¯æ¢ç´¢è‚¿ç˜¤å¾®ç¯å¢ƒä¸­çš„ç»†èƒå¼‚è´¨æ€§å¯¹ç†è§£ç™Œç—‡è¿›å±•å’Œæ²»ç–—ååº”è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰scRNA-seqæ–¹æ³•å­˜åœ¨ç©ºé—´ä¸Šä¸‹æ–‡ç¼ºå¤±å’Œé…ä½“-å—ä½“äº’åŠ¨æ•°æ®é›†ä¸å®Œå…¨çš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨æ–°å‹å›¾ç¥ç»ç½‘ç»œæ¨¡å‹scGSLï¼Œæå‡äº†ç»†èƒç±»å‹é¢„æµ‹å’Œç»†èƒäº¤äº’åˆ†æçš„å‡†ç¡®æ€§ã€‚</li>
<li>scGSLæ¨¡å‹åœ¨å¤šç§ç™Œç—‡ç»†èƒæ•°æ®é›†ä¸­å±•ç°å‡ºç¨³å¥æ€§èƒ½ï¼Œå‡†ç¡®ç‡ç­‰æŒ‡æ ‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>scGSLæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«ç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› äº¤äº’ï¼Œä¸”åœ¨å¤šç§ç™Œç—‡ä¸­å…³é”®åŸºå› å¯¹è¡¨è¾¾å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>æ‰€ææ¨¡å‹åœ¨è¯†åˆ«ç»†èƒç±»å‹å’Œæå‡ç»†èƒäº¤äº’åˆ†ææ–¹é¢å…·æœ‰é‡è¦çš„åŒ»å­¦ç ”ç©¶å’Œæ²»ç–—åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00d109705f4a520422a86ffc53d75e76.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AAD-DCE-An-Aggregated-Multimodal-Attention-Mechanism-for-Early-and-Late-Dynamic-Contrast-Enhanced-Prostate-MRI-Synthesis"><a href="#AAD-DCE-An-Aggregated-Multimodal-Attention-Mechanism-for-Early-and-Late-Dynamic-Contrast-Enhanced-Prostate-MRI-Synthesis" class="headerlink" title="AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis"></a>AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis</h2><p><strong>Authors:Divya Bharti, Sriprabha Ramanarayanan, Sadhana S, Kishore Kumar M, Keerthi Ram, Harsh Agarwal, Ramesh Venkatesan, Mohanasankar Sivaprakasam</strong></p>
<p>Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical imaging technique that plays a crucial role in the detailed visualization and identification of tissue perfusion in abnormal lesions and radiological suggestions for biopsy. However, DCE-MRI involves the administration of a Gadolinium based (Gad) contrast agent, which is associated with a risk of toxicity in the body. Previous deep learning approaches that synthesize DCE-MR images employ unimodal non-contrast or low-dose contrast MRI images lacking focus on the local perfusion information within the anatomy of interest. We propose AAD-DCE, a generative adversarial network (GAN) with an aggregated attention discriminator module consisting of global and local discriminators. The discriminators provide a spatial embedded attention map to drive the generator to synthesize early and late response DCE-MRI images. Our method employs multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient (ADC), and T1 pre-contrast for image synthesis. Extensive comparative and ablation studies on the ProstateX dataset show that our model (i) is agnostic to various generator benchmarks and (ii) outperforms other DCE-MRI synthesis approaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE for early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late response, and (ii) emphasize the importance of attention ensembling. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/bhartidivya/AAD-DCE">https://github.com/bhartidivya/AAD-DCE</a>. </p>
<blockquote>
<p>åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰æ˜¯ä¸€ç§åŒ»å­¦æˆåƒæŠ€æœ¯ï¼Œåœ¨å¼‚å¸¸ç—…å˜çš„è¡€æµçŒæ³¨è¯¦ç»†å¯è§†åŒ–ä»¥åŠé’ˆå¯¹æ´»æ£€çš„æ”¾å°„å­¦å»ºè®®ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒDCE-MRIæ¶‰åŠä½¿ç”¨åŸºäºé’†ï¼ˆGadï¼‰çš„é€ å½±å‰‚ï¼Œè¿™å¯èƒ½ä¸ä½“å†…æ¯’æ€§é£é™©ç›¸å…³ã€‚ä»¥å‰åˆæˆDCE-MRå›¾åƒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•é‡‡ç”¨éå¯¹æ¯”æˆ–ä½å‰‚é‡å¯¹æ¯”MRIå›¾åƒä½œä¸ºå•æ¨¡æ€è¾“å…¥ï¼Œç¼ºä¹å…³æ³¨è§£å‰–ç»“æ„å†…çš„å±€éƒ¨çŒæ³¨ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†AAD-DCEï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼ŒåŒ…å«ä¸€ä¸ªèšåˆæ³¨æ„åŠ›åˆ¤åˆ«å™¨æ¨¡å—ï¼Œå®ƒç”±å…¨å±€å’Œå±€éƒ¨åˆ¤åˆ«å™¨ç»„æˆã€‚åˆ¤åˆ«å™¨æä¾›ç©ºé—´åµŒå…¥æ³¨æ„åŠ›å›¾ï¼Œä»¥é©±åŠ¨ç”Ÿæˆå™¨åˆæˆæ—©æœŸå’Œæ™šæœŸå“åº”DCE-MRIå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬T2åŠ æƒï¼ˆT2Wï¼‰ã€è¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰å’ŒT1é¢„å¯¹æ¯”ç”¨äºå›¾åƒåˆæˆã€‚åœ¨ProstateXæ•°æ®é›†ä¸Šçš„å¹¿æ³›æ¯”è¾ƒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ï¼ˆiï¼‰å¯¹å„ç§ç”Ÿæˆå™¨æŒ‡æ ‡è¡¨ç°ä¸­ç«‹ï¼›ï¼ˆiiï¼‰ä¸å…¶ä»–DCE-MRIåˆæˆæ–¹æ³•ç›¸æ¯”è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œæ—©æœŸå“åº”çš„PSNRæé«˜0.64dBã€SSIMæé«˜0.0518ã€MAEé™ä½0.015ï¼Œæ™šæœŸå“åº”çš„PSNRæé«˜0.1dBã€SSIMæé«˜0.0424ã€MAEé™ä½0.021ï¼›ï¼ˆiiiï¼‰å¼ºè°ƒäº†æ³¨æ„åŠ›é›†æˆçš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bhartidivya/AAD-DCE%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/bhartidivya/AAD-DCEä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02555v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æŠ€æœ¯ï¼Œæå‡ºä¸€ç§åä¸ºAAD-DCEçš„DCE-MRIå›¾åƒåˆæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨èšé›†æ³¨æ„åŠ›é‰´åˆ«å™¨æ¨¡å—ï¼Œç»“åˆå…¨å±€å’Œå±€éƒ¨é‰´åˆ«å™¨ï¼Œæä¾›ç©ºé—´åµŒå…¥æ³¨æ„åŠ›å›¾ï¼Œé©±åŠ¨ç”Ÿæˆå™¨åˆæˆæ—©æœŸå’Œæ™šæœŸå“åº”çš„DCE-MRIå›¾åƒã€‚ä½¿ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ˆT2åŠ æƒã€è¡¨è§‚æ‰©æ•£ç³»æ•°å’ŒT1é¢„å¯¹æ¯”å›¾åƒï¼‰è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶åœ¨ProstateXæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å¯¹æ¯”å’Œæ¶ˆèç ”ç©¶ï¼Œè¯æ˜è¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCE-MRIåœ¨å¼‚å¸¸ç—…å˜ç»„ç»‡çŒæ³¨çš„è¯¦ç»†å¯è§†åŒ–å’Œè¯†åˆ«ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†ä½¿ç”¨åŸºäºé’†çš„å¯¹æ¯”å‰‚å­˜åœ¨æ¯’æ€§é£é™©ã€‚</li>
<li>ä»¥å¾€çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åˆæˆDCE-MRIå›¾åƒæ—¶ï¼Œä¸»è¦ä¾èµ–éå¯¹æ¯”æˆ–ä½å‰‚é‡å¯¹æ¯”MRIå›¾åƒï¼Œå¿½ç•¥äº†å±€éƒ¨çŒæ³¨ä¿¡æ¯ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºAAD-DCEçš„GANæ–¹æ³•ï¼Œä½¿ç”¨èšé›†æ³¨æ„åŠ›é‰´åˆ«å™¨æ¨¡å—ï¼ŒåŒ…å«å…¨å±€å’Œå±€éƒ¨é‰´åˆ«å™¨ã€‚</li>
<li>é‰´åˆ«å™¨æä¾›ç©ºé—´åµŒå…¥æ³¨æ„åŠ›å›¾ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨åˆæˆæ—©æœŸå’Œæ™šæœŸå“åº”çš„DCE-MRIå›¾åƒã€‚</li>
<li>AAD-DCEæ–¹æ³•é‡‡ç”¨å¤šæ¨¡æ€è¾“å…¥ï¼ˆT2åŠ æƒã€ADCå’ŒT1é¢„å¯¹æ¯”å›¾åƒï¼‰è¿›è¡Œå›¾åƒåˆæˆã€‚</li>
<li>åœ¨ProstateXæ•°æ®é›†ä¸Šçš„ç ”ç©¶è¡¨æ˜ï¼ŒAAD-DCEæ–¹æ³•åœ¨å„ç§ç”Ÿæˆå™¨æŒ‡æ ‡ä¸Šè¡¨ç°ä¸æ•æ„Ÿï¼Œå¹¶ä¼˜äºå…¶ä»–DCE-MRIåˆæˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b4602d4ba034d511f9660f2d09e9b37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10923e0595e1d3960897a4f7d6c07a84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8f697724877fc4e629c9434f5612c2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0dc7540a7dcd2bb35fa94a97dc61e18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bb2725ff790ad8b53b472e16b72185b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-450b6799898d335223e789206f61cc60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01b678f867eb0bdcc0aa62ec3ed15a84.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PixelShuffler-A-Simple-Image-Translation-Through-Pixel-Rearrangement"><a href="#PixelShuffler-A-Simple-Image-Translation-Through-Pixel-Rearrangement" class="headerlink" title="PixelShuffler: A Simple Image Translation Through Pixel Rearrangement"></a>PixelShuffler: A Simple Image Translation Through Pixel Rearrangement</h2><p><strong>Authors:Omar Zamzam</strong></p>
<p>Image-to-image translation is a topic in computer vision that has a vast range of use cases ranging from medical image translation, such as converting MRI scans to CT scans or to other MRI contrasts, to image colorization, super-resolution, domain adaptation, and generating photorealistic images from sketches or semantic maps. Image style transfer is also a widely researched application of image-to-image translation, where the goal is to synthesize an image that combines the content of one image with the style of another. Existing state-of-the-art methods often rely on complex neural networks, including diffusion models and language models, to achieve high-quality style transfer, but these methods can be computationally expensive and intricate to implement. In this paper, we propose a novel pixel shuffle method that addresses the image-to-image translation problem generally with a specific demonstrative application in style transfer. The proposed method approaches style transfer by shuffling the pixels of the style image such that the mutual information between the shuffled image and the content image is maximized. This approach inherently preserves the colors of the style image while ensuring that the structural details of the content image are retained in the stylized output. We demonstrate that this simple and straightforward method produces results that are comparable to state-of-the-art techniques, as measured by the Learned Perceptual Image Patch Similarity (LPIPS) loss for content preservation and the Fr&#39;echet Inception Distance (FID) score for style similarity. Our experiments validate that the proposed pixel shuffle method achieves competitive performance with significantly reduced complexity, offering a promising alternative for efficient image style transfer, as well as a promise in usability of the method in general image-to-image translation tasks. </p>
<blockquote>
<p>å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªä¸»é¢˜ï¼Œå…·æœ‰å¹¿æ³›çš„ç”¨ä¾‹ï¼Œä»åŒ»å­¦å›¾åƒè½¬æ¢ï¼ˆä¾‹å¦‚å°†MRIæ‰«æè½¬æ¢ä¸ºCTæ‰«ææˆ–å…¶ä»–MRIå¯¹æ¯”å‰‚ï¼‰åˆ°å›¾åƒå½©è‰²åŒ–ã€è¶…åˆ†è¾¨ç‡ã€åŸŸé€‚åº”ä»¥åŠä»è‰å›¾æˆ–è¯­ä¹‰åœ°å›¾ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚å›¾åƒé£æ ¼è¿ç§»ä¹Ÿæ˜¯å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„ä¸€ä¸ªå¹¿æ³›ç ”ç©¶çš„åº”ç”¨ï¼Œå…¶ç›®æ ‡æ˜¯å°†ä¸€ä¸ªå›¾åƒçš„å†…å®¹ä¸å¦ä¸€ä¸ªå›¾åƒçš„é£æ ¼ç»“åˆèµ·æ¥åˆæˆä¸€ä¸ªæ–°çš„å›¾åƒã€‚ç°æœ‰çš„å…ˆè¿›æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ï¼Œæ¥å®ç°é«˜è´¨é‡çš„é£æ ¼è¿ç§»ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—é‡å¤§ä¸”å®æ–½å¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åƒç´ é‡æ’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸€èˆ¬å¯ä»¥è§£å†³å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢é—®é¢˜ï¼Œå¹¶åœ¨é£æ ¼è¿ç§»ä¸­æœ‰ç‰¹å®šçš„æ¼”ç¤ºåº”ç”¨ã€‚æ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡é‡æ’é£æ ¼å›¾åƒçš„åƒç´ ï¼Œä½¿é‡æ’å›¾åƒä¸å†…å®¹å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯æœ€å¤§åŒ–æ¥å®ç°é£æ ¼è¿ç§»ã€‚è¿™ç§æ–¹æ³•æœ¬è´¨ä¸Šä¿ç•™äº†é£æ ¼å›¾åƒçš„é¢œè‰²ï¼ŒåŒæ—¶ç¡®ä¿å†…å®¹å›¾åƒçš„ç»“æ„ç»†èŠ‚ä¿ç•™åœ¨é£æ ¼åŒ–çš„è¾“å‡ºä¸­ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™ç§ç®€å•ç›´æ¥çš„æ–¹æ³•äº§ç”Ÿçš„ç»“æœä¸å…ˆè¿›æŠ€æœ¯çš„ç»“æœç›¸å½“ï¼Œé€šè¿‡Learned Perceptual Image Patch Similarity (LPIPS)æŸå¤±æ¥è¡¡é‡å†…å®¹ä¿ç•™æƒ…å†µï¼Œä»¥åŠFrÃ©chet Inception Distance (FID)åˆ†æ•°æ¥è¡¡é‡é£æ ¼ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„åƒç´ é‡æ’æ–¹æ³•åœ¨æ˜¾è‘—é™ä½å¤æ‚åº¦çš„åŒæ—¶ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆçš„å›¾åƒé£æ ¼è¿ç§»æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼ŒåŒæ—¶ä¹Ÿä¸ºè¯¥æ–¹æ³•åœ¨ä¸€èˆ¬çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„å¯ç”¨æ€§æä¾›äº†æ‰¿è¯ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03021v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åƒç´ çº§å›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œä»¥å›¾åƒé£æ ¼è½¬ç§»ä¸ºå…·ä½“åº”ç”¨åœºæ™¯ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡æ–°æ’åˆ—é£æ ¼å›¾åƒçš„åƒç´ ï¼Œä½¿é‡æ–°æ’åˆ—åçš„å›¾åƒä¸å†…å®¹å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯æœ€å¤§åŒ–ï¼Œä»è€Œåœ¨ä¿æŒé£æ ¼å›¾åƒé¢œè‰²çš„åŒæ—¶ä¿ç•™å†…å®¹å›¾åƒçš„ç»†èŠ‚ä¿¡æ¯ã€‚è¯¥æ–¹æ³•ä¸ç°æœ‰çš„å¤æ‚ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œåœ¨å¤æ‚æ€§å’Œæ•ˆæœä¸Šå…·æœ‰ç«äº‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨é£æ ¼è¿ç§»æ–¹é¢å°¤ä¸ºæ˜¾è‘—ã€‚æœ‰æœ›æˆä¸ºå›¾åƒé£æ ¼è½¬ç§»çš„ä¸€ç§æœ‰æ•ˆé€‰æ‹©ï¼Œä¸”åœ¨æ›´å¹¿æ³›çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­äº¦æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚<br>     â€‹        â€‹â€‹â€‹â€‹ </p>
<p><strong>Key Takeaways</strong>ï¼š â€‹â€‹<br>â€‹â€‹</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4249be70728c40f81581b9234f0ef149.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d27b29046c535f08538ff383720761ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce03319fdca1ee60bc62215e26679ccf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LNQ-2023-challenge-Benchmark-of-weakly-supervised-techniques-for-mediastinal-lymph-node-quantification"><a href="#LNQ-2023-challenge-Benchmark-of-weakly-supervised-techniques-for-mediastinal-lymph-node-quantification" class="headerlink" title="LNQ 2023 challenge: Benchmark of weakly-supervised techniques for   mediastinal lymph node quantification"></a>LNQ 2023 challenge: Benchmark of weakly-supervised techniques for   mediastinal lymph node quantification</h2><p><strong>Authors:Reuben Dorent, Roya Khajavi, Tagwa Idris, Erik Ziegler, Bhanusupriya Somarouthu, Heather Jacene, Ann LaCasce, Jonathan Deissler, Jan Ehrhardt, Sofija Engelson, Stefan M. Fischer, Yun Gu, Heinz Handels, Satoshi Kasai, Satoshi Kondo, Klaus Maier-Hein, Julia A. Schnabel, Guotai Wang, Litingyu Wang, Tassilo Wald, Guang-Zhong Yang, Hanxiao Zhang, Minghui Zhang, Steve Pieper, Gordon Harris, Ron Kikinis, Tina Kapur</strong></p>
<p>Accurate assessment of lymph node size in 3D CT scans is crucial for cancer staging, therapeutic management, and monitoring treatment response. Existing state-of-the-art segmentation frameworks in medical imaging often rely on fully annotated datasets. However, for lymph node segmentation, these datasets are typically small due to the extensive time and expertise required to annotate the numerous lymph nodes in 3D CT scans. Weakly-supervised learning, which leverages incomplete or noisy annotations, has recently gained interest in the medical imaging community as a potential solution. Despite the variety of weakly-supervised techniques proposed, most have been validated only on private datasets or small publicly available datasets. To address this limitation, the Mediastinal Lymph Node Quantification (LNQ) challenge was organized in conjunction with the 26th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2023). This challenge aimed to advance weakly-supervised segmentation methods by providing a new, partially annotated dataset and a robust evaluation framework. A total of 16 teams from 5 countries submitted predictions to the validation leaderboard, and 6 teams from 3 countries participated in the evaluation phase. The results highlighted both the potential and the current limitations of weakly-supervised approaches. On one hand, weakly-supervised approaches obtained relatively good performance with a median Dice score of $61.0%$. On the other hand, top-ranked teams, with a median Dice score exceeding $70%$, boosted their performance by leveraging smaller but fully annotated datasets to combine weak supervision and full supervision. This highlights both the promise of weakly-supervised methods and the ongoing need for high-quality, fully annotated data to achieve higher segmentation performance. </p>
<blockquote>
<p>å¯¹3D CTæ‰«æä¸­çš„æ·‹å·´ç»“å¤§å°è¿›è¡Œå‡†ç¡®è¯„ä¼°å¯¹äºç™Œç—‡åˆ†æœŸã€æ²»ç–—ç®¡ç†ä»¥åŠç›‘æµ‹æ²»ç–—ååº”è‡³å…³é‡è¦ã€‚åŒ»å­¦æˆåƒé¢†åŸŸæœ€å…ˆè¿›çš„åˆ†å‰²æ¡†æ¶é€šå¸¸ä¾èµ–äºå®Œå…¨æ³¨é‡Šçš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå¯¹äºæ·‹å·´ç»“åˆ†å‰²ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸å¾ˆå°ï¼Œå› ä¸ºå¯¹3D CTæ‰«æä¸­çš„å¤§é‡æ·‹å·´ç»“è¿›è¡Œæ³¨é‡Šéœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘ï¼Œå¼±ç›‘ç£å­¦ä¹ å¼•èµ·äº†åŒ»å­¦æˆåƒç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ï¼Œå®ƒåˆ©ç”¨ä¸å®Œæ•´æˆ–å˜ˆæ‚çš„æ³¨é‡Šä½œä¸ºæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡å·²ç»æå‡ºäº†å¤šç§å¼±ç›‘ç£æŠ€æœ¯ï¼Œä½†å¤§å¤šæ•°æŠ€æœ¯ä»…åœ¨ç§æœ‰æ•°æ®é›†æˆ–å°å‹å…¬å¼€æ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œçºµéš”æ·‹å·´ç»“é‡åŒ–ï¼ˆLNQï¼‰æŒ‘æˆ˜èµ›ä¸ç¬¬26å±ŠåŒ»å­¦å›¾åƒè®¡ç®—å’Œè®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ï¼ˆMICCAI 2023ï¼‰è”åˆä¸¾åŠã€‚è¯¥æŒ‘æˆ˜èµ›æ—¨åœ¨é€šè¿‡æä¾›æ–°çš„éƒ¨åˆ†æ³¨é‡Šæ•°æ®é›†å’Œç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•çš„å‘å±•ã€‚æ€»å…±æœ‰æ¥è‡ª5ä¸ªå›½å®¶çš„16ä¸ªå›¢é˜Ÿæäº¤äº†éªŒè¯æ’è¡Œæ¦œçš„é¢„æµ‹ï¼Œæœ‰æ¥è‡ª3ä¸ªå›½å®¶çš„6ä¸ªå›¢é˜Ÿå‚åŠ äº†è¯„ä¼°é˜¶æ®µã€‚ç»“æœæ—¢çªå‡ºäº†å¼±ç›‘ç£æ–¹æ³•çš„æ½œåŠ›ï¼Œä¹ŸæŒ‡å‡ºäº†å…¶å½“å‰å±€é™æ€§ã€‚ä¸€æ–¹é¢ï¼Œå¼±ç›‘ç£æ–¹æ³•å–å¾—äº†ç›¸å¯¹è¾ƒå¥½çš„æ€§èƒ½ï¼Œä¸­ä½æ•°Diceå¾—åˆ†ä¸º61.0%ã€‚å¦ä¸€æ–¹é¢ï¼Œæ’åé å‰çš„å›¢é˜Ÿçš„ä¸­ä½æ•°Diceå¾—åˆ†è¶…è¿‡70%ï¼Œä»–ä»¬é€šè¿‡ç»“åˆå¼±ç›‘ç£å’Œå…¨ç›‘ç£ã€åˆ©ç”¨è¾ƒå°ä½†å®Œå…¨æ³¨é‡Šçš„æ•°æ®é›†æ¥æå‡æ€§èƒ½ã€‚è¿™æ—¢çªæ˜¾äº†å¼±ç›‘ç£æ–¹æ³•çš„æ½œåŠ›ï¼Œä¹Ÿè¡¨æ˜ä¸ºäº†å®ç°æ›´é«˜çš„åˆ†å‰²æ€§èƒ½ï¼Œä»éœ€è¦é«˜è´¨é‡ã€å®Œå…¨æ³¨é‡Šçš„æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10069v2">PDF</a> Submitted to MELBA; Accepted for publication at the Journal of   Machine Learning for Biomedical Imaging (MELBA)   <a target="_blank" rel="noopener" href="https://melba-journal.org/2025:001">https://melba-journal.org/2025:001</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·‹å·´èŠ‚ç‚¹åœ¨ä¸‰ç»´CTæ‰«æä¸­çš„ç²¾ç¡®è¯„ä¼°å¯¹ç™Œç—‡åˆ†æœŸã€æ²»ç–—ç®¡ç†ä»¥åŠç›‘æµ‹æ²»ç–—æ•ˆæœçš„é‡è¦æ€§ã€‚ç°æœ‰å…ˆè¿›çš„åŒ»å­¦æˆåƒåˆ†å‰²æ¡†æ¶é€šå¸¸ä¾èµ–äºå®Œå…¨æ ‡æ³¨çš„æ•°æ®é›†ï¼Œä½†æ·‹å·´èŠ‚ç‚¹åˆ†å‰²æ•°æ®é›†é€šå¸¸è¾ƒå°ï¼Œå› ä¸ºå¯¹ä¸‰ç»´CTæ‰«æä¸­çš„å¤šä¸ªæ·‹å·´èŠ‚ç‚¹è¿›è¡Œæ ‡æ³¨éœ€è¦è€—è´¹å¤§é‡æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚æœ€è¿‘ï¼ŒåŒ»å­¦æˆåƒç¤¾åŒºå¯¹åˆ©ç”¨ä¸å®Œå…¨æˆ–å˜ˆæ‚æ ‡æ³¨çš„å¼±ç›‘ç£å­¦ä¹ äº§ç”Ÿäº†å…´è¶£ï¼Œä½œä¸ºä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡æå‡ºäº†å¤šç§å¼±ç›‘ç£æŠ€æœ¯ï¼Œä½†å¤§å¤šæ•°æŠ€æœ¯ä»…åœ¨ç§æœ‰æ•°æ®é›†æˆ–å°å‹å…¬å¼€æ•°æ®é›†ä¸Šå¾—åˆ°éªŒè¯ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œåª’ä½“æ—æ·‹å·´ç»“é‡åŒ–ï¼ˆLNQï¼‰æŒ‘æˆ˜ä¸ç¬¬26å±ŠåŒ»å­¦å›¾åƒè®¡ç®—å’Œè®¡ç®—æœºè¾…åŠ©å¹²é¢„å›½é™…ä¼šè®®ï¼ˆMICCAI 2023ï¼‰ç»“åˆä¸¾åŠã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨é€šè¿‡æä¾›æ–°çš„éƒ¨åˆ†æ ‡æ³¨æ•°æ®é›†å’Œç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•çš„å‘å±•ã€‚ç»“æœæ˜¾ç¤ºå¼±ç›‘ç£æ–¹æ³•çš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ã€‚ä¸€æ–¹é¢ï¼Œå¼±ç›‘ç£æ–¹æ³•å–å¾—äº†ç›¸å¯¹è¾ƒå¥½çš„æ€§èƒ½ï¼Œä¸­ä½Diceç³»æ•°ä¸º61.0%ã€‚å¦ä¸€æ–¹é¢ï¼Œæ’åé å‰çš„å›¢é˜Ÿé€šè¿‡ç»“åˆå¼±ç›‘ç£å’Œå…¨ç›‘ç£çš„å°è§„æ¨¡å®Œå…¨æ ‡æ³¨æ•°æ®é›†ï¼Œå–å¾—äº†è¶…è¿‡70%çš„ä¸­ä½Diceç³»æ•°ï¼Œè¿™çªæ˜¾äº†å¼±ç›‘ç£æ–¹æ³•çš„æ½œåŠ›ä»¥åŠå®ç°æ›´é«˜åˆ†å‰²æ€§èƒ½å¯¹é«˜è´¨é‡å®Œå…¨æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·‹å·´èŠ‚ç‚¹åœ¨ä¸‰ç»´CTæ‰«æä¸­çš„ç²¾ç¡®è¯„ä¼°å¯¹ç™Œç—‡æ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŒ»å­¦æˆåƒåˆ†å‰²æ¡†æ¶é€šå¸¸ä¾èµ–å®Œå…¨æ ‡æ³¨çš„æ•°æ®é›†ï¼Œä½†æ·‹å·´èŠ‚ç‚¹åˆ†å‰²æ•°æ®é›†å› æ ‡æ³¨éš¾åº¦å¤§è€Œè¾ƒå°ã€‚</li>
<li>å¼±ç›‘ç£å­¦ä¹ å·²æˆä¸ºè§£å†³è¿™ä¸€é—®é¢˜çš„æ½œåœ¨æ–¹æ³•ã€‚</li>
<li>LNQæŒ‘æˆ˜æä¾›äº†æ–°çš„éƒ¨åˆ†æ ‡æ³¨æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ¨åŠ¨å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•çš„å‘å±•ã€‚</li>
<li>å¼±ç›‘ç£æ–¹æ³•å–å¾—è¾ƒå¥½æ€§èƒ½ï¼Œä¸­ä½Diceç³»æ•°è¾¾åˆ°61.0%ã€‚</li>
<li>é«˜æ€§èƒ½å›¢é˜Ÿç»“åˆå¼±ç›‘ç£å’Œå…¨ç›‘ç£æ–¹æ³•ï¼Œåˆ©ç”¨å°è§„æ¨¡å®Œå…¨æ ‡æ³¨æ•°æ®é›†æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d72adcae66ece76f63154ec22625eddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12e7d57eeefe7ad4035bed9dea215823.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bce89181e0d4bd2f783202eb78495feb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Invertible-Diffusion-Models-for-Compressed-Sensing"><a href="#Invertible-Diffusion-Models-for-Compressed-Sensing" class="headerlink" title="Invertible Diffusion Models for Compressed Sensing"></a>Invertible Diffusion Models for Compressed Sensing</h2><p><strong>Authors:Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</strong></p>
<p>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and fine-tunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end fine-tuning, we propose a novel two-level invertible design to transform both (1) multi-step sampling process and (2) noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Guaishou74851/IDM">https://github.com/Guaishou74851/IDM</a>. </p>
<blockquote>
<p>è™½ç„¶æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰é€šè¿‡æé«˜é‡å»ºè´¨é‡æ¥æ˜¾è‘—æ¨è¿›å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰çš„å‘å±•ï¼Œä½†å½“å‰éœ€è¦ä»é›¶å¼€å§‹è®­ç»ƒCSç¥ç»ç½‘ç»œçš„éœ€æ±‚åˆ¶çº¦äº†å…¶æœ‰æ•ˆæ€§å¹¶é˜»ç¢äº†å¿«é€Ÿéƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†å®ƒä»¬é¢ä¸´æ¨ç†é€Ÿåº¦æ…¢ä»¥åŠå¯¹CSé€‚åº”æ€§æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹é«˜æ•ˆç«¯åˆ°ç«¯CSæ–¹æ³•ã€‚IDMå°†å¤§è§„æ¨¡çš„æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œç›´æ¥ä»CSæµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚ä¸ºäº†å®ç°è¿™ç§å†…å­˜å¯†é›†å‹çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œå°†ï¼ˆ1ï¼‰å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œï¼ˆ2ï¼‰æ¯ä¸€æ­¥ä¸­çš„å™ªå£°ä¼°è®¡U-Netè½¬æ¢ä¸ºå¯é€†ç½‘ç»œã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¸…é™¤äº†å¤§å¤šæ•°ä¸­é—´ç‰¹å¾ï¼Œä»¥å‡å°‘é«˜è¾¾93.8%çš„GPUå†…å­˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒIDMåœ¨PSNRä¸Šæ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„CSç½‘ç»œé«˜å‡ºé«˜è¾¾2.64dBã€‚ä¸æœ€è¿‘çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•DDNMç›¸æ¯”ï¼Œæˆ‘ä»¬çš„IDMåœ¨PSNRä¸Šå®ç°äº†é«˜è¾¾10.09dBçš„å¢ç›Šï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†14.54å€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Guaishou74851/IDM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Guaishou74851/IDMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.17006v2">PDF</a> Accepted for publication in IEEE Transactions on Pattern Analysis and   Machine Intelligence (TPAMI)</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ•ˆçš„ç«¯åˆ°ç«¯æ‰©æ•£æ¨¡å‹â€”â€”å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ã€‚é€šè¿‡å¯¹å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹è¿›è¡Œé‡æ„æ¨¡å‹çš„é‡æ„å’Œåˆ©ç”¨ç«¯å¯¹ç«¯å¾®è°ƒï¼Œç›´æ¥å¯¹åŸå§‹å›¾åƒè¿›è¡Œæ¢å¤ï¼Œæ‰“ç ´äº†ä¼ ç»Ÿå™ªå£°ä¼°è®¡å­¦ä¹ çš„ä¸€æ­¥å¤„ç†èŒƒå¼ã€‚é‡‡ç”¨å¯é€†è®¾è®¡å®ç°äº†å†…å­˜å¯†é›†å‹çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œå‡å°‘äº†GPUå†…å­˜ä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼ŒIDMåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„CSç½‘ç»œï¼Œç›¸æ¯”æœ€æ–°çš„æ‰©æ•£æ¨¡å‹DDNMæœ‰æ›´é«˜çš„æ€§èƒ½æå‡é€Ÿåº¦ã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥è®ºæ–‡æå‡ºäº†æ–°å‹å›¾åƒå‹ç¼©æ„ŸçŸ¥æŠ€æœ¯å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ã€‚</li>
<li>IDMä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•æ¨ç†é€Ÿåº¦æ…¢å’Œé€‚åº”æ€§å·®çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å¯é€†è®¾è®¡å®ç°äº†å†…å­˜å¯†é›†å‹çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œæ˜¾è‘—å‡å°‘äº†GPUå†…å­˜ä½¿ç”¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIDMåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸Šä¼˜äºå…¶ä»–CSç½‘ç»œï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>ä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹DDNMç›¸æ¯”ï¼ŒIDMå®ç°äº†æ›´é«˜çš„æ€§èƒ½æå‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.17006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e647544169bc28be72965b455e42105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6866aa196008958648b0f477807b33f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a0aa905de9a5f0aa6fbe840dadf3f8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e76267811ff6d5ebe42bef37cec6d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6806b5a1f932503e54ec26504f32bb8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4743078b57f464191e4cf6a6a307af0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-Pixels-Enhancing-LIME-with-Hierarchical-Features-and-Segmentation-Foundation-Models"><a href="#Beyond-Pixels-Enhancing-LIME-with-Hierarchical-Features-and-Segmentation-Foundation-Models" class="headerlink" title="Beyond Pixels: Enhancing LIME with Hierarchical Features and   Segmentation Foundation Models"></a>Beyond Pixels: Enhancing LIME with Hierarchical Features and   Segmentation Foundation Models</h2><p><strong>Authors:Patrick Knab, Sascha Marton, Christian Bartelt</strong></p>
<p>LIME (Local Interpretable Model-agnostic Explanations) is a popular XAI framework for unraveling decision-making processes in vision machine-learning models. The technique utilizes image segmentation methods to identify fixed regions for calculating feature importance scores as explanations. Therefore, poor segmentation can weaken the explanation and reduce the importance of segments, ultimately affecting the overall clarity of interpretation. To address these challenges, we introduce the DSEG-LIME (Data-Driven Segmentation LIME) framework, featuring: i) a data-driven segmentation for human-recognized feature generation by foundation model integration, and ii) a user-steered granularity in the hierarchical segmentation procedure through composition. Our findings demonstrate that DSEG outperforms on several XAI metrics on pre-trained ImageNet models and improves the alignment of explanations with human-recognized concepts. The code is available under: <a target="_blank" rel="noopener" href="https://github/">https://github</a>. com&#x2F;patrick-knab&#x2F;DSEG-LIME </p>
<blockquote>
<p>LIMEï¼ˆæœ¬åœ°å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Šï¼‰æ˜¯ä¸€ä¸ªæµè¡Œçš„XAIæ¡†æ¶ï¼Œç”¨äºæ­ç¤ºè§†è§‰æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„å†³ç­–åˆ¶å®šè¿‡ç¨‹ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨å›¾åƒåˆ†å‰²æ–¹æ³•æ¥è¯†åˆ«å›ºå®šåŒºåŸŸï¼Œä»¥è®¡ç®—ç‰¹å¾é‡è¦æ€§åˆ†æ•°ä½œä¸ºè§£é‡Šã€‚å› æ­¤ï¼Œåˆ†å‰²ä¸ä½³å¯èƒ½ä¼šå‰Šå¼±è§£é‡Šï¼Œé™ä½ç‰‡æ®µçš„é‡è¦æ€§ï¼Œæœ€ç»ˆå½±å“æ•´ä½“è§£é‡Šçš„æ¸…æ™°åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DSEG-LIMEï¼ˆæ•°æ®é©±åŠ¨åˆ†å‰²LIMEï¼‰æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼ši)é€šè¿‡åŸºç¡€æ¨¡å‹é›†æˆè¿›è¡Œæ•°æ®é©±åŠ¨åˆ†å‰²ï¼Œä»¥äº§ç”Ÿäººç±»è¯†åˆ«çš„ç‰¹å¾ï¼›ii)é€šè¿‡ç»„åˆå®ç°å±‚æ¬¡åˆ†å‰²è¿‡ç¨‹ä¸­çš„ç”¨æˆ·å¯¼å‘ç²’åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDSEGåœ¨é¢„è®­ç»ƒçš„ImageNetæ¨¡å‹ä¸Šçš„å¤šä¸ªXAIæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶æé«˜äº†ä¸äººç±»è¯†åˆ«æ¦‚å¿µçš„è§£é‡Šå¯¹é½ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/patrick-knab/DSEG-LIME">https://github.com/patrick-knab/DSEG-LIME</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07733v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LIMEæ¡†æ¶åœ¨æ­ç¤ºæœºå™¨è§†è§‰æ¨¡å‹å†³ç­–è¿‡ç¨‹ä¸­å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶åœ¨å›¾åƒåˆ†å‰²æ–¹é¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥DSEG-LIMEæ¡†æ¶ï¼Œé€šè¿‡é›†æˆåŸºç¡€æ¨¡å‹å®ç°æ•°æ®é©±åŠ¨åˆ†å‰²ï¼Œå¹¶å…è®¸ç”¨æˆ·è°ƒæ•´å±‚æ¬¡åˆ†å‰²è¿‡ç¨‹çš„ç²’åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDSEGåœ¨å¤šä¸ªå¯è§£é‡Šæ€§æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†å¯¹é¢„è®­ç»ƒImageNetæ¨¡å‹çš„è§£é‡Šä¸äººç±»è®¤çŸ¥æ¦‚å¿µçš„å¥‘åˆåº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LIMEæ˜¯æµè¡Œçš„XAIæ¡†æ¶ï¼Œç”¨äºæ­ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œå°¤å…¶åœ¨å›¾åƒé¢†åŸŸã€‚</li>
<li>LIMEåˆ©ç”¨å›¾åƒåˆ†å‰²æ–¹æ³•è¯†åˆ«å›ºå®šåŒºåŸŸæ¥è®¡ç®—ç‰¹å¾é‡è¦æ€§åˆ†æ•°ä½œä¸ºè§£é‡Šã€‚</li>
<li>è¾ƒå·®çš„åˆ†å‰²ä¼šå‰Šå¼±è§£é‡Šå¹¶é™ä½æ®µçš„é‡è¦æ€§ï¼Œå½±å“æ•´ä½“è§£é‡Šæ¸…æ™°åº¦ã€‚</li>
<li>DSEG-LIMEæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨åˆ†å‰²ç”Ÿæˆäººç±»è¯†åˆ«ç‰¹å¾ï¼Œå¹¶å…è®¸ç”¨æˆ·è°ƒæ•´å±‚æ¬¡åˆ†å‰²è¿‡ç¨‹çš„ç²’åº¦ã€‚</li>
<li>DSEGåœ¨å¤šä¸ªå¯è§£é‡Šæ€§æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºä¼ ç»ŸLIMEã€‚</li>
<li>DSEGæé«˜äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„è§£é‡Šä¸äººç±»è®¤çŸ¥æ¦‚å¿µçš„å¥‘åˆåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.07733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc2eaecb7ea5d58211fb1f5adab157aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d67cd22adaadf6e52848e02f4cd5d7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e18d4b6321e42cabc088edf9a033a842.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Full-Automation-of-Geometry-Extraction-for-Biomechanical-Analysis-of-Abdominal-Aortic-Aneurysm-Neural-Network-Based-versus-Classical-Methodologies"><a href="#Towards-Full-Automation-of-Geometry-Extraction-for-Biomechanical-Analysis-of-Abdominal-Aortic-Aneurysm-Neural-Network-Based-versus-Classical-Methodologies" class="headerlink" title="Towards Full Automation of Geometry Extraction for Biomechanical   Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical   Methodologies"></a>Towards Full Automation of Geometry Extraction for Biomechanical   Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical   Methodologies</h2><p><strong>Authors:Farah Alkhatib, Mostafa Jamshidian, Donatien Le Liepvre, Florian Bernard, Ludovic Minvielle, Antoine FondanÃ¨che, Elke R. Gizewski, Eva Gassner, Alexander Loizides, Maximilian Lutz, Florian Enzmann, Hozan Mufty, Inge Fourneau, Adam Wittek, Karol Miller</strong></p>
<p>Background: For the clinical adoption of stress-based rupture risk estimation in abdominal aortic aneurysms (AAAs), a fully automated pipeline, from clinical imaging to biomechanical stress computation, is essential. To this end, we investigated the impact of AI-based image segmentation methods on stress computation results in the walls of AAAs. We compared wall stress distributions and magnitudes calculated from geometry models obtained from classical semi-automated segmentation versus automated neural network-based segmentation. Method: 16 different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking between 15 and 40 minutes of human effort per patient, depending on image quality. The same images were automatically segmented using PRAEVAorta2 commercial software by NUREA (<a target="_blank" rel="noopener" href="https://www.nurea-soft.com/">https://www.nurea-soft.com/</a>), developed based on artificial intelligence (AI) algorithms, and automatically post-processed with an in-house MATLAB code, requiring only 1-2 minutes of computer time per patient. Aneurysm wall stress calculations were automatically performed using the BioPARR software (<a target="_blank" rel="noopener" href="https://bioparr.mech.uwa.edu.au/">https://bioparr.mech.uwa.edu.au/</a>). Results: Compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values. However, our statistical analysis indicated that the differences in AAA wall stress obtained using the two segmentation methods are not statistically significant and fall well within the typical range of inter-analyst and intra-analyst variability. Conclusions: Our findings are a steppingstone toward a fully automated pipeline for biomechanical analysis of AAAs, starting with CT scans and concluding with wall stress assessment. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šä¸ºäº†åœ¨è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰çš„ä¸´åºŠæ²»ç–—ä¸­é‡‡ç”¨åŸºäºåº”åŠ›çš„ç ´è£‚é£é™©è¯„ä¼°ï¼Œå¼€å‘ä¸€ä¸ªä»ä¸´åºŠæˆåƒåˆ°ç”Ÿç‰©åŠ›å­¦åº”åŠ›è®¡ç®—çš„å…¨è‡ªåŠ¨åŒ–æµç¨‹è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºäººå·¥æ™ºèƒ½çš„å›¾åƒåˆ†å‰²æ–¹æ³•å¯¹AAAå£åº”åŠ›è®¡ç®—ç»“æœçš„å½±å“ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ç»å…¸åŠè‡ªåŠ¨åˆ†å‰²å’ŒåŸºäºè‡ªåŠ¨åŒ–ç¥ç»ç½‘ç»œåˆ†å‰²æ‰€å¾—åˆ°çš„å‡ ä½•æ¨¡å‹è®¡ç®—çš„å£åº”åŠ›åˆ†å¸ƒå’Œå¤§å°ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šå¯¹16ä¾‹ä¸åŒAAAæ‚£è€…çš„å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒè¿›è¡ŒåŠè‡ªåŠ¨åˆ†æï¼Œåˆ†æå‘˜æ ¹æ®å›¾åƒè´¨é‡éœ€è¦èŠ±è´¹15è‡³40åˆ†é’Ÿçš„æ—¶é—´å¤„ç†æ¯ä½æ‚£è€…ã€‚ä½¿ç”¨NUREAå¼€å‘çš„PRAEVAorta2å•†ä¸šè½¯ä»¶ï¼ˆ<a target="_blank" rel="noopener" href="https://www.nurea-soft.com/%EF%BC%89%E5%AF%B9%E7%9B%B8%E5%90%8C%E5%9B%BE%E5%83%8F%E8%BF%9B%E8%A1%8C%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95%E7%9A%84%E8%87%AA%E5%8A%A8%E5%88%86%E5%89%B2%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%86%85%E9%83%A8MATLAB%E4%BB%A3%E7%A0%81%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%90%8E%E5%A4%84%E7%90%86%EF%BC%8C%E6%AF%8F%E4%BD%8D%E6%82%A3%E8%80%85%E4%BB%85%E9%9C%80%E8%8A%B1%E8%B4%B91/~2%E5%88%86%E9%92%9F%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E3%80%82%E5%8A%A8%E8%84%89%E7%98%A4%E5%A3%81%E5%BA%94%E5%8A%9B%E8%AE%A1%E7%AE%97%E8%87%AA%E5%8A%A8%E9%80%9A%E8%BF%87BioPARR%E8%BD%AF%E4%BB%B6%EF%BC%88https://bioparr.mech.uwa.edu.au/%EF%BC%89%E8%BF%9B%E8%A1%8C%E3%80%82">https://www.nurea-soft.com/ï¼‰å¯¹ç›¸åŒå›¾åƒè¿›è¡ŒåŸºäºäººå·¥æ™ºèƒ½ç®—æ³•çš„è‡ªåŠ¨åˆ†å‰²ï¼Œå¹¶ä½¿ç”¨å†…éƒ¨MATLABä»£ç è¿›è¡Œè‡ªåŠ¨åå¤„ç†ï¼Œæ¯ä½æ‚£è€…ä»…éœ€èŠ±è´¹1\~2åˆ†é’Ÿçš„è®¡ç®—æœºå¤„ç†æ—¶é—´ã€‚åŠ¨è„‰ç˜¤å£åº”åŠ›è®¡ç®—è‡ªåŠ¨é€šè¿‡BioPARRè½¯ä»¶ï¼ˆhttps://bioparr.mech.uwa.edu.au/ï¼‰è¿›è¡Œã€‚</a></p>
<p>ç»“æœï¼šä¸ç»å…¸åŠè‡ªåŠ¨åˆ†å‰²ç›¸æ¯”ï¼ŒåŸºäºè‡ªåŠ¨ç¥ç»ç½‘ç»œçš„åˆ†å‰²å¯¼è‡´åº”åŠ›åˆ†å¸ƒç›¸å½“ï¼Œå³°å€¼å’Œ99thç™¾åˆ†ä½æœ€å¤§ä¸»åº”åŠ›å€¼ç•¥é«˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»Ÿè®¡åˆ†æè¡¨æ˜ï¼Œä½¿ç”¨è¿™ä¸¤ç§åˆ†å‰²æ–¹æ³•è·å¾—çš„AAAå£åº”åŠ›ä¹‹é—´çš„å·®å¼‚åœ¨ç»Ÿè®¡å­¦ä¸Šå¹¶ä¸æ˜¾è‘—ï¼Œå¹¶ä¸”è¿œè¿œä½äºåˆ†æå‘˜é—´å’Œåˆ†æå‘˜å†…éƒ¨çš„å˜å¼‚æ€§å…¸å‹èŒƒå›´ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07238v3">PDF</a> 43 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸´åºŠå½±åƒæ•°æ®çš„è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰ç ´è£‚é£é™©è¯„ä¼°åœ¨ä¸´åºŠåº”ç”¨è¿‡ç¨‹ä¸­éœ€è¦å…¨è‡ªåŠ¨åŒ–å¤„ç†æµç¨‹ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºäººå·¥æ™ºèƒ½çš„å›¾åƒåˆ†å‰²æŠ€æœ¯å¯¹AAAå£åº”åŠ›è®¡ç®—çš„å½±å“ï¼Œå¹¶ä¸ä¼ ç»ŸåŠè‡ªåŠ¨åˆ†å‰²æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼Œè‡ªåŠ¨ç¥ç»ç½‘ç»œåˆ†å‰²æ–¹æ³•ä¸ä¼ ç»ŸåŠè‡ªåŠ¨åˆ†å‰²æ–¹æ³•å¾—åˆ°çš„åº”åŠ›åˆ†å¸ƒç›¸å½“ï¼Œå³°å€¼å’Œ99thç™¾åˆ†ä½æœ€å¤§ä¸»åº”åŠ›å€¼ç•¥é«˜ï¼Œä½†äºŒè€…åœ¨ç»Ÿè®¡å­¦ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ï¼Œå¹¶è½åœ¨åˆ†æå¸ˆé—´å’Œåˆ†æå¸ˆå†…éƒ¨çš„å˜å¼‚èŒƒå›´å†…ã€‚è¿™ä¸ºè¿›ä¸€æ­¥å¼€å‘å…¨è‡ªåŠ¨çš„AAAç”Ÿç‰©åŠ›å­¦åˆ†ææµç¨‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¸´åºŠé‡‡ç”¨åŸºäºåº”åŠ›æ–¹æ³•çš„è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰ç ´è£‚é£é™©è¯„ä¼°éœ€è¦å…¨è‡ªåŠ¨å¤„ç†æµç¨‹ã€‚</li>
<li>å¯¹æ¯”äº†åŸºäºäººå·¥æ™ºèƒ½çš„å›¾åƒåˆ†å‰²æ–¹æ³•å’Œä¼ ç»ŸåŠè‡ªåŠ¨åˆ†å‰²æ–¹æ³•åœ¨AAAå£åº”åŠ›è®¡ç®—ä¸­çš„åº”ç”¨ã€‚</li>
<li>è‡ªåŠ¨ç¥ç»ç½‘ç»œåˆ†å‰²å¾—åˆ°çš„åº”åŠ›åˆ†å¸ƒä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“ï¼Œå³°å€¼å’Œç‰¹å®šç™¾åˆ†ä½åº”åŠ›å€¼ç•¥é«˜ã€‚</li>
<li>ä¸¤ç§åˆ†å‰²æ–¹æ³•åœ¨ç»Ÿè®¡å­¦ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ï¼Œå¹¶å¤„äºåˆ†æå¸ˆé—´çš„å˜å¼‚èŒƒå›´å†…ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.07238">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31582a126d9d80047a16caa6f5274152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7796c9b62016b5cd8633312bbea3400.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="One-Model-to-Rule-them-All-Towards-Universal-Segmentation-for-Medical-Images-with-Text-Prompts"><a href="#One-Model-to-Rule-them-All-Towards-Universal-Segmentation-for-Medical-Images-with-Text-Prompts" class="headerlink" title="One Model to Rule them All: Towards Universal Segmentation for Medical   Images with Text Prompts"></a>One Model to Rule them All: Towards Universal Segmentation for Medical   Images with Text Prompts</h2><p><strong>Authors:Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>In this study, we aim to build up a model that can Segment Anything in radiology scans, driven by medical terminologies as Text prompts, termed as SAT. Our main contributions are three folds: (i) for dataset construction, we construct the first multi-modal knowledge tree on human anatomy, including 6502 anatomical terminologies; Then, we build up the largest and most comprehensive segmentation dataset for training, by collecting over 22K 3D medical image scans from72 segmentation datasets, across 497 classes, with careful standardization on both image scans and label space; (ii) for architecture design, we propose to inject medical knowledge into a text encoder via contrastive learning, and then formulate a universal segmentation model, that can be prompted by feeding in medical terminologies in text form; (iii) As a result, we have trained SAT-Nano (110M parameters) and SAT-Pro (447M parameters), demonstrating superior or comparable performance to 72 specialist models, i.e., nnU-Nets, U-Mamba or SwinUNETR, trained on each dataset&#x2F;subsets. We validate SAT as a foundational segmentation model, with better generalization on external (cross-center) datasets, and can be further improved on specific tasks after fine-tuning adaptation. Comparing with state-of-the-art interactive segmentation model MedSAM, SAT demonstrate superior performance, scalability and robustness. We further compare SAT with BiomedParse, and observe SAT is significantly superior in both internal and external evaluation. Through extensive ablation study, we validate the benefit of domain knowledge on universal segmentation, especially on tail categories. As a use case, we demonstrate that SAT can act as a powerful out-of-the-box agent for large language models, enabling visual grounding in versatile application scenarios. All the data, codes, and models in this work have been released. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ”¾å°„å­¦æ‰«æä¸­åˆ†å‰²ä»»ä½•ä¸œè¥¿ï¼Œä»¥åŒ»å­¦æœ¯è¯­ä½œä¸ºæ–‡æœ¬æç¤ºè¿›è¡Œé©±åŠ¨ï¼Œç§°ä¸ºSATã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š(i)å¯¹äºæ•°æ®é›†æ„å»ºï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€æ£µå…³äºäººä½“è§£å‰–å­¦çš„å¤šæ¨¡æ€çŸ¥è¯†æ ‘ï¼ŒåŒ…å«6502ä¸ªè§£å‰–å­¦æœ¯è¯­ï¼›ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æ”¶é›†æ¥è‡ª72ä¸ªåˆ†å‰²æ•°æ®é›†çš„è¶…è¿‡2.2ä¸‡ä»½3DåŒ»å­¦å›¾åƒæ‰«ææ•°æ®ï¼Œæ„å»ºäº†ç”¨äºè®­ç»ƒçš„æœ€å¤§ä¸”æœ€å…¨é¢çš„åˆ†å‰²æ•°æ®é›†ï¼Œæ¶µç›–497ç±»ï¼Œå¹¶å¯¹å›¾åƒæ‰«æå’Œæ ‡ç­¾ç©ºé—´è¿›è¡Œäº†ä¸¥è°¨çš„æ ‡å‡†åŒ–ï¼›(ii)åœ¨æ¶æ„è®¾è®¡æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†åŒ»å­¦çŸ¥è¯†æ³¨å…¥æ–‡æœ¬ç¼–ç å™¨ï¼Œç„¶ååˆ¶å®šä¸€ä¸ªé€šç”¨åˆ†å‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€šè¿‡è¾“å…¥åŒ»å­¦æœ¯è¯­çš„æ–‡æœ¬å½¢å¼è¿›è¡Œæç¤ºï¼›(iii)å› æ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒäº†SAT-Nanoï¼ˆ1.1äº¿å‚æ•°ï¼‰å’ŒSAT-Proï¼ˆ4.47äº¿å‚æ•°ï¼‰ï¼Œå…¶æ€§èƒ½ä¼˜äºæˆ–ä¸åœ¨æ¯ä¸ªæ•°æ®é›†&#x2F;å­é›†ä¸Šè®­ç»ƒçš„72ä¸ªä¸“å®¶æ¨¡å‹ï¼ˆå³nnU-Netã€U-Mambaæˆ–SwinUNETRï¼‰ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬éªŒè¯äº†SATä½œä¸ºä¸€ä¸ªåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œåœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒé€‚åº”åå¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä¸æœ€å…ˆè¿›çš„äº¤äº’å¼åˆ†å‰²æ¨¡å‹MedSAMç›¸æ¯”ï¼ŒSATåœ¨æ€§èƒ½ã€å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚æˆ‘ä»¬å°†SATä¸BiomedParseè¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°SATåœ¨å†…éƒ¨å’Œå¤–éƒ¨è¯„ä¼°ä¸­éƒ½æ˜¾è‘—ä¼˜è¶Šã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬éªŒè¯äº†é¢†åŸŸçŸ¥è¯†å¯¹é€šç”¨åˆ†å‰²ï¼ˆç‰¹åˆ«æ˜¯åœ¨å°¾éƒ¨ç±»åˆ«ä¸­ï¼‰çš„ç›Šå¤„ã€‚ä½œä¸ºä¸€ä¸ªç”¨ä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SATå¯ä»¥ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰åŠ›å·¥å…·ï¼Œåœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­å®ç°è§†è§‰å®šä½ã€‚æœ¬å·¥ä½œä¸­çš„æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œæ¨¡å‹å‡å·²å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.17183v4">PDF</a> 69 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ”¾å°„æ‰«æä¸­å®ç°ä»»æ„åˆ†å‰²ï¼Œä»¥åŒ»å­¦æœ¯è¯­ä½œä¸ºæ–‡æœ¬æç¤ºè¿›è¡Œé©±åŠ¨ï¼Œè¢«ç§°ä¸ºSATã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šä¸€ã€åœ¨æ•°æ®é›†æ„å»ºæ–¹é¢ï¼Œæ„å»ºäº†é¦–ä¸ªäººä½“è§£å‰–å­¦çš„å¤šæ¨¡æ€çŸ¥è¯†æ ‘ï¼ŒåŒ…å«6502ä¸ªè§£å‰–å­¦æœ¯è¯­ï¼›å¹¶é€šè¿‡æ”¶é›†è¶…è¿‡22Kä¸ªä¸‰ç»´åŒ»å­¦å›¾åƒæ‰«æå’Œæ ‡å‡†åŒ–å›¾åƒæ‰«æå’Œæ ‡ç­¾ç©ºé—´ï¼Œå»ºç«‹äº†æœ€å¤§çš„ç»¼åˆåˆ†å‰²æ•°æ®é›†ï¼›äºŒã€åœ¨æ¶æ„è®¾è®¡æ–¹é¢ï¼Œæå‡ºé€šè¿‡å¯¹æ¯”å­¦ä¹ å°†åŒ»å­¦çŸ¥è¯†æ³¨å…¥æ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶åˆ¶å®šé€šç”¨åˆ†å‰²æ¨¡å‹ï¼Œå¯é€šè¿‡è¾“å…¥åŒ»å­¦æœ¯è¯­è¿›è¡Œæç¤ºï¼›ä¸‰ã€è®­ç»ƒäº†SAT-Nanoï¼ˆ1.1äº¿å‚æ•°ï¼‰å’ŒSAT-Proï¼ˆ4.47äº¿å‚æ•°ï¼‰ï¼Œç›¸è¾ƒäºåœ¨æ¯ä¸ªæ•°æ®é›†&#x2F;å­é›†ä¸Šè®­ç»ƒçš„nnU-Netã€U-Mambaæˆ–SwinUNETRç­‰72ç§ä¸“ä¸šæ¨¡å‹è¡¨ç°å‡ºä¼˜è¶Šæˆ–ç›¸å½“çš„æ€§èƒ½ã€‚éªŒè¯äº†SATä½œä¸ºåŸºç¡€åˆ†å‰²æ¨¡å‹çš„ä¼˜è¶Šæ€§ï¼Œåœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šç»è¿‡å¾®è°ƒåèƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚ç›¸è¾ƒäºå…ˆè¿›çš„äº¤äº’å¼åˆ†å‰²æ¨¡å‹MedSAMï¼ŒSATå±•ç°å‡ºæ€§èƒ½ã€å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§çš„ä¼˜åŠ¿ã€‚é€šè¿‡ä¸BiomedParseçš„æ¯”è¾ƒï¼Œè¯æ˜äº†SATåœ¨å†…éƒ¨å’Œå¤–éƒ¨è¯„ä¼°ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼ŒéªŒè¯äº†é¢†åŸŸçŸ¥è¯†å¯¹é€šç”¨åˆ†å‰²çš„ç›Šå¤„ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å°¾éƒ¨ç±»åˆ«ã€‚ä½œä¸ºä¸€ä¸ªåº”ç”¨åœºæ™¯ï¼Œå±•ç¤ºäº†SATå¯ä»¥ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§å¤–ç›’ä»£ç†ï¼Œåœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­å®ç°è§†è§‰å®šä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»ºç«‹äº†ä¸€ä¸ªåä¸ºSATçš„æ¨¡å‹ï¼Œå¯åœ¨æ”¾å°„æ‰«æä¸­å®ç°ä»»æ„åˆ†å‰²ï¼Œä»¥åŒ»å­¦æœ¯è¯­ä½œä¸ºæ–‡æœ¬æç¤ºè¿›è¡Œé©±åŠ¨ã€‚</li>
<li>æ„å»ºäº†é¦–ä¸ªäººä½“è§£å‰–å­¦çš„å¤šæ¨¡æ€çŸ¥è¯†æ ‘å’Œæœ€å¤§çš„ç»¼åˆåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å­¦ä¹ å°†åŒ»å­¦çŸ¥è¯†æ³¨å…¥æ–‡æœ¬ç¼–ç å™¨ï¼Œæå‡ºé€šç”¨åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è®­ç»ƒäº†SAT-Nanoå’ŒSAT-Proä¸¤ç§æ¨¡å‹ï¼Œè¡¨ç°å‡ºä¼˜è¶Šæˆ–ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>SATå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šç»è¿‡å¾®è°ƒåå¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>SATç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ¨¡å‹å±•ç°å‡ºæ€§èƒ½ã€å¯æ‰©å±•æ€§å’Œç¨³å¥æ€§çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.17183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fb3a18e4a301896e494f4ab14726173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4c2d137e473c6ec17da86ab0e698f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2b9bbbce681da0487c29baaa1e8731d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ReFusion-Learning-Image-Fusion-from-Reconstruction-with-Learnable-Loss-via-Meta-Learning"><a href="#ReFusion-Learning-Image-Fusion-from-Reconstruction-with-Learnable-Loss-via-Meta-Learning" class="headerlink" title="ReFusion: Learning Image Fusion from Reconstruction with Learnable Loss   via Meta-Learning"></a>ReFusion: Learning Image Fusion from Reconstruction with Learnable Loss   via Meta-Learning</h2><p><strong>Authors:Haowen Bai, Zixiang Zhao, Jiangshe Zhang, Yichen Wu, Lilun Deng, Yukun Cui, Shuang Xu, Baisong Jiang</strong></p>
<p>Image fusion aims to combine information from multiple source images into a single one with more comprehensive informational content. Deep learning-based image fusion algorithms face significant challenges, including the lack of a definitive ground truth and the corresponding distance measurement. Additionally, current manually defined loss functions limit the modelâ€™s flexibility and generalizability for various fusion tasks. To address these limitations, we propose ReFusion, a unified meta-learning based image fusion framework that dynamically optimizes the fusion loss for various tasks through source image reconstruction. Compared to existing methods, ReFusion employs a parameterized loss function, that allows the training framework to be dynamically adapted according to the specific fusion scenario and task. ReFusion consists of three key components: a fusion module, a source reconstruction module, and a loss proposal module. We employ a meta-learning strategy to train the loss proposal module using the reconstruction loss. This strategy forces the fused image to be more conducive to reconstruct source images, allowing the loss proposal module to generate a adaptive fusion loss that preserves the optimal information from the source images. The update of the fusion module relies on the learnable fusion loss proposed by the loss proposal module. The three modules update alternately, enhancing each other to optimize the fusion loss for different tasks and consistently achieve satisfactory results. Extensive experiments demonstrate that ReFusion is capable of adapting to various tasks, including infrared-visible, medical, multi-focus, and multi-exposure image fusion. </p>
<blockquote>
<p>å›¾åƒèåˆæ—¨åœ¨å°†æ¥è‡ªå¤šä¸ªæºå›¾åƒçš„ä¿¡æ¯åˆå¹¶åˆ°ä¸€ä¸ªå•ä¸€å›¾åƒä¸­ï¼Œä»¥è·å–æ›´å…¨é¢çš„ä¿¡æ¯å†…å®¹ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒèåˆç®—æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹æ˜ç¡®çš„çœŸå®æ ‡ç­¾å’Œç›¸åº”çš„è·ç¦»åº¦é‡ã€‚æ­¤å¤–ï¼Œå½“å‰æ‰‹åŠ¨å®šä¹‰çš„æŸå¤±å‡½æ•°é™åˆ¶äº†æ¨¡å‹å¯¹å„ç§èåˆä»»åŠ¡çš„çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ReFusionï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºäºå…ƒå­¦ä¹ çš„å›¾åƒèåˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡æºå›¾åƒé‡å»ºåŠ¨æ€ä¼˜åŒ–èåˆæŸå¤±ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒReFusioné‡‡ç”¨å‚æ•°åŒ–æŸå¤±å‡½æ•°ï¼Œå…è®¸è®­ç»ƒæ¡†æ¶æ ¹æ®ç‰¹å®šçš„èåˆåœºæ™¯å’Œä»»åŠ¡è¿›è¡ŒåŠ¨æ€é€‚åº”ã€‚ReFusionç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šèåˆæ¨¡å—ã€æºé‡å»ºæ¨¡å—å’ŒæŸå¤±ææ¡ˆæ¨¡å—ã€‚æˆ‘ä»¬é‡‡ç”¨å…ƒå­¦ä¹ ç­–ç•¥æ¥è®­ç»ƒæŸå¤±ææ¡ˆæ¨¡å—ï¼Œä½¿ç”¨é‡å»ºæŸå¤±ã€‚è¿™ä¸€ç­–ç•¥å¼ºåˆ¶èåˆå›¾åƒæ›´æœ‰åŠ©äºé‡å»ºæºå›¾åƒï¼Œä½¿æŸå¤±ææ¡ˆæ¨¡å—èƒ½å¤Ÿç”Ÿæˆè‡ªé€‚åº”èåˆæŸå¤±ï¼Œä¿ç•™æºå›¾åƒä¸­çš„æœ€ä½³ä¿¡æ¯ã€‚èåˆæ¨¡å—çš„æ›´æ–°ä¾èµ–äºç”±æŸå¤±ææ¡ˆæ¨¡å—æå‡ºçš„å¯å­¦ä¹ èåˆæŸå¤±ã€‚è¿™ä¸‰ä¸ªæ¨¡å—äº¤æ›¿æ›´æ–°ï¼Œç›¸äº’å¢å¼ºï¼Œé’ˆå¯¹å„ç§ä»»åŠ¡ä¼˜åŒ–èåˆæŸå¤±ï¼Œå¹¶å§‹ç»ˆå–å¾—ä»¤äººæ»¡æ„çš„ç»“æœã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReFusionèƒ½å¤Ÿé€‚åº”å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬çº¢å¤–å¯è§å…‰ã€åŒ»ç–—ã€å¤šç„¦ç‚¹å’Œå¤šæ›å…‰å›¾åƒèåˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.07943v3">PDF</a> This article is published in International Journal of Computer Vision   (IJCV), 2024</p>
<p><strong>Summary</strong><br>    åŸºäºæ·±åº¦å­¦ä¹ å’Œå…ƒå­¦ä¹ çš„å›¾åƒèåˆæ¡†æ¶ReFusionï¼Œé€šè¿‡æºå›¾åƒé‡å»ºåŠ¨æ€ä¼˜åŒ–èåˆæŸå¤±ï¼Œé€‚ç”¨äºå¤šç§èåˆä»»åŠ¡ï¼Œæé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒèåˆæ—¨åœ¨å°†å¤šä¸ªæºå›¾åƒçš„ä¿¡æ¯åˆå¹¶åˆ°ä¸€ä¸ªæ›´å…¨é¢çš„å•ä¸€å›¾åƒä¸­ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å›¾åƒèåˆä¸­é¢ä¸´ç¼ºä¹æ˜ç¡®çš„åœ°é¢çœŸå®å’Œç›¸åº”çš„è·ç¦»æµ‹é‡çš„æŒ‘æˆ˜ã€‚</li>
<li>ReFusionæ˜¯ä¸€ä¸ªåŸºäºå…ƒå­¦ä¹ çš„å›¾åƒèåˆæ¡†æ¶ï¼Œé€šè¿‡æºå›¾åƒé‡å»ºåŠ¨æ€ä¼˜åŒ–èåˆæŸå¤±ã€‚</li>
<li>ReFusioné‡‡ç”¨å‚æ•°åŒ–çš„æŸå¤±å‡½æ•°ï¼Œæ ¹æ®ç‰¹å®šçš„èåˆæƒ…æ™¯å’Œä»»åŠ¡åŠ¨æ€è°ƒæ•´è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>ReFusionåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šèåˆæ¨¡å—ã€æºé‡å»ºæ¨¡å—å’ŒæŸå¤±ææ¡ˆæ¨¡å—ã€‚</li>
<li>é‡‡ç”¨å…ƒå­¦ä¹ ç­–ç•¥è®­ç»ƒæŸå¤±ææ¡ˆæ¨¡å—ï¼Œä½¿ç”¨é‡å»ºæŸå¤±è¿«ä½¿èåˆå›¾åƒæ›´æœ‰åŠ©äºé‡å»ºæºå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.07943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-405d27fafc27911eba7461bce5dd42ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9134b3b30d24f2c8952f5c67592e371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a79f93ac0e4b67a2ed09c8613626850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d79ef413d15b324adb5f10dfd9556e4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SelfFed-Self-Supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-Medical-Images"><a href="#SelfFed-Self-Supervised-Federated-Learning-for-Data-Heterogeneity-and-Label-Scarcity-in-Medical-Images" class="headerlink" title="SelfFed: Self-Supervised Federated Learning for Data Heterogeneity and   Label Scarcity in Medical Images"></a>SelfFed: Self-Supervised Federated Learning for Data Heterogeneity and   Label Scarcity in Medical Images</h2><p><strong>Authors:Sunder Ali Khowaja, Kapal Dev, Syed Muhammad Anwar, Marius George Linguraru</strong></p>
<p>Self-supervised learning in the federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for medical images to overcome data heterogeneity and label scarcity issues. The first phase of the SelfFed framework helps to overcome the data heterogeneity issue by leveraging the pre-training paradigm that performs augmentative modeling using Swin Transformer-based encoder in a decentralized manner. The label scarcity issue is addressed by fine-tuning paradigm that introduces a contrastive network and a novel aggregation strategy. We perform our experimental analysis on publicly available medical imaging datasets to show that SelfFed performs better when compared to existing baselines and works. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID datasets. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances. </p>
<blockquote>
<p>åœ¨è”é‚¦å­¦ä¹ èŒƒå¼ä¸­ï¼Œè‡ªç›‘ç£å­¦ä¹ å› å…¶åœ¨æ— æ ‡ç­¾ä½†å­¤ç«‹æ•°æ®ä¸Šçš„åä½œå­¦ä¹ èƒ½åŠ›è€Œå—åˆ°ä¸šç•Œå’Œç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼ŒåŸºäºè‡ªç›‘ç£çš„è”é‚¦å­¦ä¹ ç­–ç•¥ç”±äºæ ‡ç­¾ç¨€ç¼ºå’Œå¤šæ ·åŒ–çš„æ•°æ®åˆ†å¸ƒï¼ˆå³æ•°æ®å¼‚è´¨æ€§ï¼‰è€Œé¢ä¸´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒ»å­¦å›¾åƒçš„SelfFedæ¡†æ¶ï¼Œä»¥è§£å†³æ•°æ®å¼‚è´¨æ€§å’Œæ ‡ç­¾ç¨€ç¼ºé—®é¢˜ã€‚SelfFedæ¡†æ¶çš„ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒèŒƒå¼ï¼Œä»¥åˆ†å¸ƒå¼æ–¹å¼ä½¿ç”¨åŸºäºSwin Transformerçš„ç¼–ç å™¨è¿›è¡Œå¢å¼ºå»ºæ¨¡ï¼Œæœ‰åŠ©äºå…‹æœæ•°æ®å¼‚è´¨æ€§é—®é¢˜ã€‚é€šè¿‡å¾®è°ƒèŒƒå¼å¼•å…¥å¯¹æ¯”ç½‘ç»œå’Œä¸€ç§æ–°çš„èšåˆç­–ç•¥æ¥è§£å†³æ ‡ç­¾ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒåˆ†æï¼Œä»¥è¯æ˜SelfFedä¸ç°æœ‰åŸºå‡†ç›¸æ¯”è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨Retinaå’ŒCOVID-FLæ•°æ®é›†ä¸Šçš„éIIDæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†æœ€é«˜è¾¾8.8%å’Œ4.1%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨å°‘æ•°ï¼ˆ10%ï¼‰æ ‡è®°å®ä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¹Ÿä¼˜äºç°æœ‰åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.01514v3">PDF</a> 22 pages, 10 figures, 2 tables</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„è”é‚¦å­¦ä¹ é¢ä¸´æ•°æ®å¼‚è´¨æ€§å’Œæ ‡ç­¾ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºçš„SelfFedæ¡†æ¶é€šè¿‡é¢„è®­ç»ƒé˜¶æ®µåˆ©ç”¨åŸºäºSwin Transformerçš„ç¼–ç å™¨è¿›è¡Œå¢å¼ºå»ºæ¨¡ï¼Œè§£å†³æ•°æ®å¼‚è´¨æ€§ï¼Œå¹¶é€šè¿‡å¾®è°ƒé˜¶æ®µå¼•å…¥å¯¹æ¯”ç½‘ç»œå’Œæ–°å‹èšåˆç­–ç•¥åº”å¯¹æ ‡ç­¾ç¨€ç¼ºé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSelfFedç›¸è¾ƒäºç°æœ‰åŸºçº¿åœ¨éIIDæ•°æ®é›†ä¸Šæœ€å¤§æå‡è¾¾8.8%å’Œ4.1%ï¼Œåœ¨ä»…ä½¿ç”¨å°‘é‡ï¼ˆ10%ï¼‰æ ‡è®°å®ä¾‹çš„æƒ…å†µä¸‹ä¾ç„¶ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ä¸­çš„è‡ªç›‘ç£å­¦ä¹ é¢ä¸´æ•°æ®å¼‚è´¨æ€§å’Œæ ‡ç­¾ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>SelfFedæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒé˜¶æ®µè§£å†³æ•°æ®å¼‚è´¨æ€§ï¼Œé€šè¿‡åŸºäºSwin Transformerçš„ç¼–ç å™¨è¿›è¡Œå¢å¼ºå»ºæ¨¡ã€‚</li>
<li>SelfFedé€šè¿‡å¾®è°ƒé˜¶æ®µå¼•å…¥å¯¹æ¯”ç½‘ç»œå’Œæ–°å‹èšåˆç­–ç•¥æ¥åº”å¯¹æ ‡ç­¾ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>åœ¨å…¬å¼€åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSelfFedç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>SelfFedåœ¨éIIDæ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œæœ€å¤§æå‡è¾¾8.8%å’Œ4.1%ã€‚</li>
<li>SelfFedåœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡è®°å®ä¾‹çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.01514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62a6e4a424d00290e398bb118e274774.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28937bce630394c8e6eb5107f54db695.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2ee7e65f21517a54c08e7d9144f4a342.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  Metis A Foundation Speech Generation Model with Masked Generative   Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3a643477f2b327f2b2f7f7e067d41e25.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  Dress-1-to-3 Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30762.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
