<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  Dress-1-to-3 Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3a643477f2b327f2b2f7f7e067d41e25.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-07-æ›´æ–°"><a href="#2025-02-07-æ›´æ–°" class="headerlink" title="2025-02-07 æ›´æ–°"></a>2025-02-07 æ›´æ–°</h1><h2 id="Dress-1-to-3-Single-Image-to-Simulation-Ready-3D-Outfit-with-Diffusion-Prior-and-Differentiable-Physics"><a href="#Dress-1-to-3-Single-Image-to-Simulation-Ready-3D-Outfit-with-Diffusion-Prior-and-Differentiable-Physics" class="headerlink" title="Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics"></a>Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics</h2><p><strong>Authors:Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang</strong></p>
<p>Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: <a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a> </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹æ¨¡å‹çš„è¿›å±•æå¤§åœ°æ¨åŠ¨äº†å›¾åƒåˆ°3Dé‡å»ºçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„æ¨¡å‹é€šå¸¸è¢«èåˆæˆå•ä¸ªéƒ¨åˆ†ï¼Œè¿™åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨å—åˆ°äº†é™åˆ¶ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨3Dæœè£…ç”Ÿæˆï¼Œè¿™æ˜¯è™šæ‹Ÿè¯•ç©¿ç­‰åº”ç”¨çš„å…³é”®é¢†åŸŸï¼Œè¦æ±‚æœè£…å¯åˆ†ç¦»ä¸”é€‚åˆæ¨¡æ‹Ÿã€‚æˆ‘ä»¬ä»‹ç»äº†Dress-1-to-3ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç®¡é“ï¼Œå®ƒå¯ä»¥ä»é‡ç”Ÿå›¾åƒä¸­é‡å»ºç‰©ç†å¯è¡Œçš„ã€é€‚åˆæ¨¡æ‹Ÿçš„åˆ†ç¦»æœè£…ï¼ŒåŒ…æ‹¬ç¼çº«å›¾æ¡ˆå’Œäººç±»ã€‚ä»å›¾åƒå¼€å§‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°ç¼çº«å›¾æ¡ˆç”Ÿæˆæ¨¡å‹ï¼Œä»¥åˆ›å»ºç²—ç³™çš„ç¼çº«å›¾æ¡ˆï¼Œä»¥åŠä¸é¢„è®­ç»ƒçš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œç”Ÿæˆå¤šè§†å›¾å›¾åƒã€‚ç¼çº«å›¾æ¡ˆè¿›ä¸€æ­¥ä½¿ç”¨åŸºäºç”Ÿæˆçš„å¤šè§†å›¾å›¾åƒçš„å¯å¾®åˆ†æœè£…æ¨¡æ‹Ÿå™¨è¿›è¡Œç»†åŒ–ã€‚å¤šç§å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ³•æå¤§åœ°æé«˜äº†é‡å»ºçš„3Dæœè£…å’Œäººä½“ä¸è¾“å…¥å›¾åƒçš„å‡ ä½•å¯¹é½ã€‚æ­¤å¤–ï¼Œé€šè¿‡é›†æˆçº¹ç†ç”Ÿæˆæ¨¡å—å’Œäººç±»è¿åŠ¨ç”Ÿæˆæ¨¡å—ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å®šåˆ¶çš„ã€ç‰©ç†å¯è¡Œçš„é€¼çœŸåŠ¨æ€æœè£…æ¼”ç¤ºã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03449v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://dress-1-to-3.github.io/">https://dress-1-to-3.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Dress-1-to-3é¡¹ç›®ï¼Œè¯¥é¡¹ç›®æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒé‡å»ºç‰©ç†å¯è¡Œæ€§é«˜ã€æ¨¡æ‹Ÿå°±ç»ªçš„åˆ†ç¦»è¡£ç‰©çš„æ–°æ–¹æ³•ã€‚é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„å›¾åƒåˆ°ç¼çº«æ¨¡å¼ç”Ÿæˆæ¨¡å‹å’Œé¢„è®­ç»ƒçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œè¯¥é¡¹ç›®èƒ½å¤Ÿç”Ÿæˆå¤šè§†è§’å›¾åƒå¹¶ä¼˜åŒ–ç¼çº«æ¨¡å¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯å¾®åˆ†çš„æœè£…æ¨¡æ‹Ÿå™¨ï¼Œè¯¥é¡¹ç›®èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„åŠ¨æ€æœè£…æ¼”ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dress-1-to-3é¡¹ç›®èƒ½å¤Ÿä»å•å¼ å›¾åƒé‡å»ºç‰©ç†å¯è¡Œæ€§é«˜çš„åˆ†ç¦»è¡£ç‰©ã€‚</li>
<li>è¯¥é¡¹ç›®ä½¿ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒåˆ°ç¼çº«æ¨¡å¼ç”Ÿæˆæ¨¡å‹å’Œé¢„è®­ç»ƒçš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤šè§†è§’å›¾åƒç”Ÿæˆå’Œç¼çº«æ¨¡å¼ä¼˜åŒ–ï¼Œæé«˜äº†é‡å»ºè¡£ç‰©çš„å‡ ä½•å¯¹é½åº¦ã€‚</li>
<li>é¡¹ç›®é‡‡ç”¨äº†å¯å¾®åˆ†çš„æœè£…æ¨¡æ‹Ÿå™¨æ¥ä¼˜åŒ–ç¼çº«æ¨¡å¼ã€‚</li>
<li>è¯¥é¡¹ç›®èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„åŠ¨æ€æœè£…æ¼”ç¤ºã€‚</li>
<li>Dress-1-to-3é¡¹ç›®åœ¨è™šæ‹Ÿè¯•ç©¿ç­‰åº”ç”¨ä¸Šå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-818c7b25b3d90db9955d41866047fc67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a3d193962d7f79ff5c601cbc5452695.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Masked-Autoencoders-Are-Effective-Tokenizers-for-Diffusion-Models"><a href="#Masked-Autoencoders-Are-Effective-Tokenizers-for-Diffusion-Models" class="headerlink" title="Masked Autoencoders Are Effective Tokenizers for Diffusion Models"></a>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</h2><p><strong>Authors:Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj</strong></p>
<p>Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥è¯æ˜äº†å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´å¥½åœ°å­¦ä¹ å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œæ¥è‡ªtokenizerçš„æ½œåœ¨ç©ºé—´å±æ€§ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚åœ¨ç†è®ºå’Œå®è·µä¸Šï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆè´¨é‡çš„æé«˜ä¸å…·æœ‰æ›´å¥½ç»“æ„çš„æ½œåœ¨åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ï¼Œä¾‹å¦‚å…·æœ‰æ›´å°‘çš„é«˜æ–¯æ··åˆæ¨¡å¼å’Œæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚å—è¿™äº›è§è§£çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†MAETokï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ©è†œå»ºæ¨¡çš„è‡ªç¼–ç å™¨ï¼ˆAEï¼‰ï¼Œä»¥å­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬åˆ†æçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å˜åˆ†è‡ªç¼–ç å™¨å½¢å¼å¹¶éå¿…éœ€ï¼Œä»…ä½¿ç”¨AEçš„åˆ¤åˆ«æ€§æ½œåœ¨ç©ºé—´å°±èƒ½åœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…ä½¿ç”¨128ä¸ªä»¤ç‰Œã€‚MAETokå®ç°äº†é‡å¤§å®é™…æ”¹è¿›ï¼Œåœ¨512x512ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†1.69çš„gFIDï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†76å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†31å€ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºæœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹æ¥è¯´ï¼Œæ½œåœ¨ç©ºé—´çš„ç»“æ„æ¯”å˜åˆ†çº¦æŸæ›´ä¸ºé‡è¦ã€‚ä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03444v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ½œæ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å·²è¯æ˜å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•åˆ©ç”¨tokenizerçš„æ½œåœ¨ç©ºé—´ä»¥æ›´å¥½åœ°å­¦ä¹ å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œä»å­˜åœ¨è®¸å¤šå¾…æ¢ç´¢ä¹‹å¤„ã€‚ç ”ç©¶å‘ç°ï¼Œæ›´å¥½çš„ç”Ÿæˆè´¨é‡ä¸å…·æœ‰æ›´å¥½ç»“æ„çš„æ½œåœ¨åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ï¼Œå¦‚å…·æœ‰æ›´å°‘çš„é«˜æ–¯æ··åˆæ¨¡å¼å’Œæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†MAETokï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ©æ¨¡å»ºæ¨¡çš„è‡ªç¼–ç å™¨ï¼ˆAEï¼‰ï¼Œç”¨äºå­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚å®éªŒéªŒè¯æˆ‘ä»¬çš„åˆ†æï¼Œè¯æ˜å˜åˆ†è‡ªç¼–ç å™¨å½¢å¼å¹¶éå¿…éœ€ï¼Œä»…ä½¿ç”¨AEçš„åˆ¤åˆ«æ€§æ½œåœ¨ç©ºé—´å³å¯åœ¨ImageNetç”Ÿæˆä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œä½¿ç”¨ä»…128ä¸ªä»¤ç‰Œã€‚MAETokå®ç°äº†å®é™…æ”¹è¿›ï¼Œåœ¨512x512ç”Ÿæˆçš„æƒ…å†µä¸‹ï¼ŒgFIDè¾¾åˆ°1.69ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜76å€ï¼Œæ¨ç†é€Ÿåº¦æé«˜31å€ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ½œåœ¨ç©ºé—´çš„ç»“æ„å¯¹äºæœ‰æ•ˆçš„æ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ï¼Œè€Œä¸æ˜¯å˜åˆ†çº¦æŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²è¯æ˜å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ½œåœ¨ç©ºé—´çš„ç»“æ„å¯¹äºæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ä½¿ç”¨tokenizerçš„æ½œåœ¨ç©ºé—´è¿›è¡Œæ›´å¥½çš„å­¦ä¹ å’Œç”Ÿæˆå¾…æ¢ç´¢ã€‚</li>
<li>æ›´å¥½çš„ç”Ÿæˆè´¨é‡ä¸å…·æœ‰æ›´å¥½ç»“æ„çš„æ½œåœ¨åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ã€‚</li>
<li>MAETokæ˜¯ä¸€ç§åˆ©ç”¨æ©æ¨¡å»ºæ¨¡çš„è‡ªç¼–ç å™¨ï¼ˆAEï¼‰ï¼Œæ—¨åœ¨å­¦ä¹ è¯­ä¹‰ä¸°å¯Œçš„æ½œåœ¨ç©ºé—´å¹¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚</li>
<li>MAETokå®ç°äº†æ˜¾è‘—çš„å®é™…æ”¹è¿›ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36e84e25574c050be01dca395a616864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6af970ceb4fadf4d4f763f3df007acbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-051db8762afab001fb125e21ad49c732.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11184faa79582ac610996d064ada7da3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5efb32d9c5a89b17baa60d89c857ff8a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Mixture-Based-Framework-for-Guiding-Diffusion-Models"><a href="#A-Mixture-Based-Framework-for-Guiding-Diffusion-Models" class="headerlink" title="A Mixture-Based Framework for Guiding Diffusion Models"></a>A Mixture-Based Framework for Guiding Diffusion Models</h2><p><strong>Authors:Yazid Janati, Badr Moufad, Mehdi Abou El Qassime, Alain Durmus, Eric Moulines, Jimmy Olsson</strong></p>
<p>Denoising diffusion models have driven significant progress in the field of Bayesian inverse problems. Recent approaches use pre-trained diffusion models as priors to solve a wide range of such problems, only leveraging inference-time compute and thereby eliminating the need to retrain task-specific models on the same dataset. To approximate the posterior of a Bayesian inverse problem, a diffusion model samples from a sequence of intermediate posterior distributions, each with an intractable likelihood function. This work proposes a novel mixture approximation of these intermediate distributions. Since direct gradient-based sampling of these mixtures is infeasible due to intractable terms, we propose a practical method based on Gibbs sampling. We validate our approach through extensive experiments on image inverse problems, utilizing both pixel- and latent-space diffusion priors, as well as on source separation with an audio diffusion model. The code is available at <a target="_blank" rel="noopener" href="https://www.github.com/badr-moufad/mgdm">https://www.github.com/badr-moufad/mgdm</a> </p>
<blockquote>
<p>é™å™ªæ‰©æ•£æ¨¡å‹åœ¨è´å¶æ–¯åé—®é¢˜é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ€è¿‘çš„æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥è§£å†³ä¸€ç³»åˆ—é—®é¢˜ï¼Œä»…åˆ©ç”¨æ¨ç†æ—¶é—´è®¡ç®—ï¼Œä»è€Œæ— éœ€åœ¨åŒä¸€æ•°æ®é›†ä¸Šå¯¹ç‰¹å®šä»»åŠ¡æ¨¡å‹è¿›è¡Œé‡æ–°è®­ç»ƒã€‚ä¸ºäº†è¿‘ä¼¼è´å¶æ–¯åé—®é¢˜çš„åéªŒæ¦‚ç‡ï¼Œæ‰©æ•£æ¨¡å‹ä»ä¸€ç³»åˆ—ä¸­é—´åéªŒåˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œæ¯ä¸ªåˆ†å¸ƒéƒ½æœ‰ä¸€ä¸ªéš¾ä»¥å¤„ç†çš„å¯èƒ½æ€§å‡½æ•°ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†è¿™äº›ä¸­é—´åˆ†å¸ƒçš„ä¸€ç§æ–°å‹æ··åˆè¿‘ä¼¼æ–¹æ³•ã€‚ç”±äºè¿™äº›æ··åˆçš„ç›´æ¥åŸºäºæ¢¯åº¦çš„é‡‡æ ·ç”±äºéš¾ä»¥å¤„ç†çš„é¡¹ç›®è€Œä¸å¯è¡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‰å¸ƒæ–¯é‡‡æ ·çš„å®ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å›¾åƒåé—®é¢˜çš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åˆ©ç”¨äº†åƒç´ å’Œæ½œåœ¨ç©ºé—´æ‰©æ•£å…ˆéªŒï¼Œä»¥åŠä½¿ç”¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹çš„æºåˆ†ç¦»ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://www.github.com/badr-moufad/mgdm%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://www.github.com/badr-moufad/mgdmä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03332v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å»å™ªæ‰©æ•£æ¨¡å‹åœ¨è´å¶æ–¯åé—®é¢˜é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ€æ–°æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥è§£å†³å„ç§é—®é¢˜ï¼Œä»…åˆ©ç”¨æ¨ç†æ—¶é—´çš„è®¡ç®—ï¼Œä»è€Œæ— éœ€åœ¨ç›¸åŒæ•°æ®é›†ä¸Šé’ˆå¯¹ç‰¹å®šä»»åŠ¡é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸­é—´åˆ†å¸ƒçš„æ–°å‹æ··åˆè¿‘ä¼¼æ–¹æ³•ï¼Œç”¨äºè¿‘ä¼¼è´å¶æ–¯åé—®é¢˜çš„åéªŒåˆ†å¸ƒã€‚ç”±äºè¿™äº›æ··åˆç‰©çš„ç›´æ¥åŸºäºæ¢¯åº¦çš„é‡‡æ ·æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‰å¸ƒæ–¯é‡‡æ ·çš„å®ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å›¾åƒåé—®é¢˜å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›å®éªŒåˆ©ç”¨äº†åƒç´ å’Œæ½œåœ¨ç©ºé—´çš„æ‰©æ•£å…ˆéªŒï¼Œä»¥åŠéŸ³é¢‘æ‰©æ•£æ¨¡å‹çš„æºåˆ†ç¦»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»å™ªæ‰©æ•£æ¨¡å‹åœ¨è´å¶æ–¯åé—®é¢˜é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚</li>
<li>é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¢«ç”¨ä½œè§£å†³å„ç§é—®é¢˜çš„å…ˆéªŒï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆè¿‘ä¼¼æ–¹æ³•ï¼Œç”¨äºè¿‘ä¼¼è´å¶æ–¯åé—®é¢˜çš„åéªŒåˆ†å¸ƒã€‚</li>
<li>ç›´æ¥åŸºäºæ¢¯åº¦çš„é‡‡æ ·ç”±äºå…·æœ‰ä¸å¯çŸ¥çš„å¯èƒ½æ€§è€Œä¸åˆ‡å®é™…ã€‚</li>
<li>é‡‡ç”¨å‰å¸ƒæ–¯é‡‡æ ·æ–¹æ³•ä½œä¸ºå®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åœ¨å›¾åƒåé—®é¢˜å’ŒéŸ³é¢‘æ‰©æ•£æ¨¡å‹çš„æºåˆ†ç¦»ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31d6c289459e869c577256a263a86c7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0253ce4e9742bb0fe66fa154a3489add.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-934f826f5b27d07fcc8e5b34fd3356a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86cf14668d95f2dde1eb30037dde6481.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="When-are-Diffusion-Priors-Helpful-in-Sparse-Reconstruction-A-Study-with-Sparse-view-CT"><a href="#When-are-Diffusion-Priors-Helpful-in-Sparse-Reconstruction-A-Study-with-Sparse-view-CT" class="headerlink" title="When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with   Sparse-view CT"></a>When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with   Sparse-view CT</h2><p><strong>Authors:Matt Y. Cheung, Sophia Zorek, Tucker J. Netherton, Laurence E. Court, Sadeer Al-Kindi, Ashok Veeraraghavan, Guha Balakrishnan</strong></p>
<p>Diffusion models demonstrate state-of-the-art performance on image generation, and are gaining traction for sparse medical image reconstruction tasks. However, compared to classical reconstruction algorithms relying on simple analytical priors, diffusion models have the dangerous property of producing realistic looking results \emph{even when incorrect}, particularly with few observations. We investigate the utility of diffusion models as priors for image reconstruction by varying the number of observations and comparing their performance to classical priors (sparse and Tikhonov regularization) using pixel-based, structural, and downstream metrics. We make comparisons on low-dose chest wall computed tomography (CT) for fat mass quantification. First, we find that classical priors are superior to diffusion priors when the number of projections is &#96;&#96;sufficientâ€™â€™. Second, we find that diffusion priors can capture a large amount of detail with very few observations, significantly outperforming classical priors. However, they fall short of capturing all details, even with many observations. Finally, we find that the performance of diffusion priors plateau after extremely few ($\approx$10-15) projections. Ultimately, our work highlights potential issues with diffusion-based sparse reconstruction and underscores the importance of further investigation, particularly in high-stakes clinical settings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç¨€ç–åŒ»å­¦å›¾åƒé‡å»ºä»»åŠ¡ä¸­è·å¾—äº†ç‰µå¼•ã€‚ç„¶è€Œï¼Œä¸ä¾èµ–äºç®€å•åˆ†æå…ˆéªŒçŸ¥è¯†çš„ä¼ ç»Ÿé‡å»ºç®—æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹å…·æœ‰äº§ç”Ÿé€¼çœŸç»“æœçš„é£é™©ï¼Œå³ä½¿ç»“æœæ˜¯é”™è¯¯çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨è§‚æµ‹æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬é€šè¿‡æ”¹å˜è§‚æµ‹æ•°é‡ï¼Œä½¿ç”¨åƒç´ ã€ç»“æ„å’Œä¸‹æ¸¸æŒ‡æ ‡æ¥æ¯”è¾ƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒé‡å»ºå…ˆéªŒçš„å®ç”¨æ€§ï¼Œå¹¶ä¸ç¨€ç–å…ˆéªŒå’ŒTikhonovæ­£åˆ™åŒ–ç­‰ä¼ ç»Ÿå…ˆéªŒè¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨ä½å‰‚é‡èƒ¸è…”å£è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„è„‚è‚ªè´¨é‡å®šé‡è¯„ä¼°ä¸­è¿›è¡Œäº†æ¯”è¾ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‘ç°å½“æŠ•å½±æ•°é‡â€œè¶³å¤Ÿâ€æ—¶ï¼Œç»å…¸å…ˆéªŒä¼˜äºæ‰©æ•£å…ˆéªŒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£å…ˆéªŒå¯ä»¥åœ¨éå¸¸å°‘çš„è§‚æµ‹å€¼ä¸­æ•æ‰åˆ°å¤§é‡ç»†èŠ‚ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå…ˆéªŒã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å¾ˆå¤šè§‚æµ‹å€¼çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¹Ÿæ— æ³•æ•æ‰åˆ°æ‰€æœ‰ç»†èŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£å…ˆéªŒåœ¨æå°‘çš„æŠ•å½±ï¼ˆçº¦10-15ä¸ªï¼‰ä¹‹åæ€§èƒ½è¾¾åˆ°ç¨³å®šã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å·¥ä½œçªå‡ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–é‡å»ºä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†è¿›ä¸€æ­¥è°ƒæŸ¥çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02771v1">PDF</a> Accepted at IEEE ISBI 2025, 5 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒé‡å»ºå…ˆéªŒçš„å®ç”¨æ€§ï¼Œä¸ç»å…¸å…ˆéªŒæ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨è§‚æµ‹æ•°é‡è¶³å¤Ÿæ—¶ï¼Œç»å…¸å…ˆéªŒè¡¨ç°ä¼˜äºæ‰©æ•£å…ˆéªŒï¼›ä½†åœ¨è§‚æµ‹æ•°é‡è¾ƒå°‘æ—¶ï¼Œæ‰©æ•£å…ˆéªŒèƒ½å¤Ÿæ•æ‰å¤§é‡ç»†èŠ‚ï¼Œæ˜¾è‘—ä¼˜äºç»å…¸å…ˆéªŒã€‚ç„¶è€Œï¼Œå³ä½¿è§‚æµ‹æ•°é‡å¢å¤šï¼Œæ‰©æ•£å…ˆéªŒä¹Ÿæ— æ³•å®Œå…¨æ•æ‰æ‰€æœ‰ç»†èŠ‚ã€‚æœ€ç»ˆï¼Œæœ¬æ–‡å¼ºè°ƒäº†æ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–é‡å»ºä¸­çš„æ½œåœ¨é—®é¢˜ï¼Œå¹¶å¼ºè°ƒäº†å¯¹é«˜é£é™©çš„åŒ»ç–—ä¸´åºŠç¯å¢ƒè¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶å¼€å§‹ç”¨äºç¨€ç–åŒ»å­¦å›¾åƒé‡å»ºä»»åŠ¡ã€‚</li>
<li>ä¸ä¾èµ–ç®€å•åˆ†æå…ˆéªŒçš„ç»å…¸é‡å»ºç®—æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé€¼çœŸçš„ç»“æœï¼Œå³ä½¿ç»“æœä¸æ­£ç¡®ã€‚</li>
<li>åœ¨è§‚æµ‹æ•°é‡å……è¶³æ—¶ï¼Œç»å…¸å…ˆéªŒè¡¨ç°ä¼˜äºæ‰©æ•£å…ˆéªŒã€‚</li>
<li>åœ¨è§‚æµ‹æ•°é‡è¾ƒå°‘æ—¶ï¼Œæ‰©æ•£å…ˆéªŒèƒ½å¤Ÿæ•æ‰å¤§é‡ç»†èŠ‚ï¼Œæ˜¾è‘—ä¼˜äºç»å…¸å…ˆéªŒã€‚</li>
<li>æ‰©æ•£å…ˆéªŒæ— æ³•å®Œå…¨æ•æ‰æ‰€æœ‰ç»†èŠ‚ï¼Œå³ä½¿åœ¨è§‚æµ‹æ•°é‡è¾ƒå¤šçš„æƒ…å†µä¸‹ã€‚</li>
<li>æ‰©æ•£åŸºäºçš„ç¨€ç–é‡å»ºæ€§èƒ½åœ¨æå°‘æ•°æŠ•å½±åè¾¾åˆ°ç¨³å®šçŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88e8c3f6d18bae8ad9a199ebfef865f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-278810dc85640d2ab66d703cf421a6e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-054f5ae381b60c7b61857cd1ba0a099e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b90e06d4e86f5c7b3bbe3e8dc569ddb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an \textbf{online} algorithm capable of collecting data during runtime and supporting a \textbf{black-box} objective function. Moreover, the \textbf{query efficiency} of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, \textbf{Fast Direct}, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>å¼•å¯¼å¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–¹å‘å¯¹äºå®šåˆ¶é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä»¥è§£å†³ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç°æœ‰çš„å¼•å¯¼å¼æ‰©æ•£æ¨¡å‹è¦ä¹ˆä¾èµ–äºä½¿ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†å¯¹å¼•å¯¼æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¦ä¹ˆéœ€è¦ç›®æ ‡å‡½æ•°å¯å¾®ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä»»åŠ¡è€Œè¨€ï¼Œç¦»çº¿æ•°æ®é›†é€šå¸¸ä¸å¯ç”¨ï¼Œä¸”å…¶ç›®æ ‡å‡½æ•°é€šå¸¸ä¸å¯å¾®åˆ†ï¼Œä¾‹å¦‚æ ¹æ®äººç±»åå¥½è¿›è¡Œå›¾åƒç”Ÿæˆã€ä¸ºè¯ç‰©å‘ç°ç”Ÿæˆåˆ†å­ä»¥åŠææ–™è®¾è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç®±ç›®æ ‡å‡½æ•°çš„<strong>åœ¨çº¿</strong>ç®—æ³•ã€‚æ­¤å¤–ï¼Œç®—æ³•çš„<strong>æŸ¥è¯¢æ•ˆç‡</strong>ä¹Ÿè‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç›®æ ‡æŸ¥è¯¢è¯„ä¼°å¾€å¾€æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œç®€å•çš„ç®—æ³•â€”â€”<strong>Fast Direct</strong>ï¼Œç”¨äºé«˜æ•ˆæŸ¥è¯¢åœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆã€‚æˆ‘ä»¬çš„Fast Directåœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œè¿™æœ‰æœ›å®ç°é«˜æ•ˆçš„æŸ¥è¯¢å¼•å¯¼ç”Ÿæˆã€‚åœ¨åäºŒä¸ªé«˜åˆ†è¾¨ç‡ï¼ˆ$\small {1024 \times 1024}$ï¼‰å›¾åƒç›®æ ‡ç”Ÿæˆä»»åŠ¡å’Œå…­ä¸ª3Dåˆ†å­ç›®æ ‡ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼ŒæŸ¥è¯¢æ•ˆç‡æé«˜äº†$\textbf{6}\times$åˆ°$\textbf{10}\times$ï¼Œä»¥åŠåˆ†åˆ«æé«˜äº†$\textbf{11}\times$åˆ°$\textbf{44}\times$ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹å‘â€”â€”åœ¨çº¿æ•°æ®æŒ‡å¯¼ä¸‹çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆã€‚ç°æœ‰çš„æ¨¡å‹å¤§å¤šä¾èµ–ç¦»çº¿æ•°æ®é›†å’Œå¯å¾®çš„ç›®æ ‡å‡½æ•°è¿›è¡Œè®­ç»ƒå’ŒæŒ‡å¯¼ã€‚ä½†å®é™…åº”ç”¨ä¸­ï¼Œè®¸å¤šä»»åŠ¡æ— æ³•ä½¿ç”¨ç¦»çº¿æ•°æ®é›†æˆ–ç›®æ ‡å‡½æ•°ä¸å¯å¾®ï¼Œå¦‚å›¾åƒç”Ÿæˆã€è¯ç‰©å‘ç°çš„åˆ†å­ç”Ÿæˆå’Œææ–™è®¾è®¡ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§åœ¨çº¿ç®—æ³•ï¼Œèƒ½åœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç®±ç›®æ ‡å‡½æ•°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„ç®—æ³•â€”â€”Fast Directï¼Œç”¨äºåœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆã€‚è¯¥ç®—æ³•åœ¨å›¾åƒå’Œåˆ†å­ç”Ÿæˆä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜ç§€çš„æŸ¥è¯¢æ•ˆç‡å’Œç”Ÿæˆæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ­£åœ¨æœç€å®šåˆ¶åŒ–æ–¹å‘å‘å±•ï¼Œä»¥åº”å¯¹ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¾èµ–äºç¦»çº¿æ•°æ®é›†å’Œå¯å¾®çš„ç›®æ ‡å‡½æ•°è¿›è¡Œè®­ç»ƒå’ŒæŒ‡å¯¼ï¼Œä½†å®é™…åº”ç”¨ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Fast Directç®—æ³•é€šè¿‡åœ¨çº¿æ•°æ®æŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæ”¯æŒé»‘ç®±ç›®æ ‡å‡½æ•°ï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„æŸ¥è¯¢æ•ˆç‡ã€‚</li>
<li>Fast Directç®—æ³•åœ¨å›¾åƒå’Œåˆ†å­ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç®—æ³•é€šè¿‡æ„å»ºä¼ªç›®æ ‡åœ¨æ•°æ®æµå½¢ä¸Šè¿›è¡Œæ›´æ–°ï¼Œä»¥é€šç”¨æ–¹å‘å¼•å¯¼å™ªå£°åºåˆ—çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨å¤šä¸ªé«˜æ¸…æ™°åº¦å›¾åƒç”Ÿæˆä»»åŠ¡å’Œåˆ†å­ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-029ea006bc87307a71aacf8abdad7c74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8e6b3245a8fea1168ea1a33d9883dd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b02e30e6862644daeb69fdee46ecbd0f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="One-Prompt-One-Story-Free-Lunch-Consistent-Text-to-Image-Generation-Using-a-Single-Prompt"><a href="#One-Prompt-One-Story-Free-Lunch-Consistent-Text-to-Image-Generation-Using-a-Single-Prompt" class="headerlink" title="One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation   Using a Single Prompt"></a>One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation   Using a Single Prompt</h2><p><strong>Authors:Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency, we propose a novel training-free method for consistent text-to-image (T2I) generation, termed â€œOne-Prompt-One-Storyâ€ (1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-Value Reweighting and Identity-Preserving Cross-Attention, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness through quantitative metrics and qualitative assessments. Code is available at <a target="_blank" rel="noopener" href="https://github.com/byliutao/1Prompt1Story">https://github.com/byliutao/1Prompt1Story</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å¯ä»¥ä»è¾“å…¥æç¤ºä¸­åˆ›å»ºé«˜è´¨é‡çš„å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ”¯æŒæ•…äº‹å™è¿°ä¸­çš„èº«ä»½ä¸€è‡´æ€§ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜çš„ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡è®­ç»ƒæˆ–å¯¹åŸå§‹æ¨¡å‹æ¶æ„è¿›è¡Œé¢å¤–ä¿®æ”¹ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒé¢†åŸŸå’Œå¤šæ ·åŒ–æ‰©æ•£æ¨¡å‹é…ç½®ä¸­çš„åº”ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°è¯­è¨€æ¨¡å‹çš„å›ºæœ‰èƒ½åŠ›ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Œé€šè¿‡å•ä¸ªæç¤ºç†è§£èº«ä»½ã€‚ä»å›ºæœ‰çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— éœ€è®­ç»ƒçš„ä¸€è‡´æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºâ€œOne-Prompt-One-Storyâ€ï¼ˆ1Prompt1Storyï¼‰ã€‚æˆ‘ä»¬çš„1Prompt1Storyæ–¹æ³•å°†æ‰€æœ‰æç¤ºåˆå¹¶ä¸ºä¸€ä¸ªè¾“å…¥ï¼Œç”¨äºT2Iæ‰©æ•£æ¨¡å‹ï¼Œåˆæ­¥ä¿ç•™è§’è‰²èº«ä»½ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨ä¸¤ç§æ–°æŠ€æœ¯å®Œå–„ç”Ÿæˆè¿‡ç¨‹ï¼šå¥‡å¼‚å€¼å†æƒå€¼å’Œèº«ä»½ä¿æŒäº¤å‰æ³¨æ„åŠ›ï¼Œç¡®ä¿ä¸æ¯ä¸ªå¸§çš„è¾“å…¥æè¿°æ›´å¥½å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸å„ç§ç°æœ‰çš„ä¸€è‡´T2Iç”Ÿæˆæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œé€šè¿‡å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°æ¥è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/byliutao/1Prompt1Story%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/byliutao/1Prompt1Storyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13554v3">PDF</a> 28 pages, 22 figures, ICLR2025 conference</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿä»è¾“å…¥æç¤ºåˆ›å»ºé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ”¯æŒèº«ä»½ä¿æŒçš„æ•…äº‹å™è¿°çš„è¿ç»­ç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•é€šå¸¸éœ€è¦åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡è®­ç»ƒæˆ–å¯¹åŸå§‹æ¨¡å‹æ¶æ„è¿›è¡Œé¢å¤–ä¿®æ”¹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒé¢†åŸŸå’Œå¤šæ ·åŒ–æ‰©æ•£æ¨¡å‹é…ç½®ä¸­çš„åº”ç”¨æ€§ã€‚æœ¬æ–‡é¦–å…ˆè§‚å¯Ÿåˆ°è¯­è¨€æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ï¼Œç§°ä¹‹ä¸ºä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿé€šè¿‡å•ä¸ªæç¤ºç†è§£èº«ä»½ã€‚å—å†…åœ¨ä¸Šä¸‹æ–‡ä¸€è‡´æ€§çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— éœ€è®­ç»ƒçš„ä¸€è‡´æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºâ€œOne-Prompt-One-Storyâ€ï¼ˆ1Prompt1Storyï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¤šä¸ªæç¤ºåˆå¹¶ä¸ºä¸€ä¸ªå•ç‹¬çš„è¾“å…¥ï¼Œç”¨äºT2Iæ‰©æ•£æ¨¡å‹ï¼Œæœ€åˆä¿ç•™è§’è‰²èº«ä»½ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ç§æ–°æŠ€æœ¯å®Œå–„ç”Ÿæˆè¿‡ç¨‹ï¼šå¥‡å¼‚å€¼é‡åŠ æƒå’Œèº«ä»½ä¿æŒäº¤å‰æ³¨æ„åŠ›ï¼Œç¡®ä¿ä¸æ¯ä¸ªå¸§çš„è¾“å…¥æè¿°æ›´å¥½åœ°å¯¹é½ã€‚é€šè¿‡å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/byliutao/1Prompt1Story">https://github.com/byliutao/1Prompt1Story</a> æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨æ”¯æŒèº«ä»½ä¿æŒçš„æ•…äº‹å™è¿°çš„è¿ç»­ç”Ÿæˆä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è§£å†³æ­¤é—®é¢˜é€šå¸¸éœ€è¦å¤§é‡è®­ç»ƒå’Œæ¨¡å‹æ¶æ„ä¿®æ”¹ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å†…åœ¨ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— éœ€è®­ç»ƒçš„ä¸€è‡´æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•â€”â€”â€œOne-Prompt-One-Storyâ€ï¼ˆ1Prompt1Storyï¼‰ã€‚</li>
<li>1Prompt1Storyæ–¹æ³•é€šè¿‡å°†å¤šä¸ªæç¤ºåˆå¹¶ä¸ºä¸€ä¸ªè¾“å…¥ï¼Œç”¨äºT2Iæ‰©æ•£æ¨¡å‹ï¼Œä»è€Œä¿ç•™è§’è‰²èº«ä»½ã€‚</li>
<li>é€šè¿‡å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡ŒæŠ€æœ¯å®Œå–„ï¼Œå¦‚å¥‡å¼‚å€¼é‡åŠ æƒå’Œèº«ä»½ä¿æŒäº¤å‰æ³¨æ„åŠ›ï¼Œæé«˜äº†ä¸è¾“å…¥æè¿°çš„å¯¹é½ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2006bee3f4502eed4d699e57e8005a73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93c2477fc0dccec2691c2870e73f16e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5e3e5aa4668aa86223a068e38f36e9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cd7fb00db1ab57c4fde180195accb15.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="IterComp-Iterative-Composition-Aware-Feedback-Learning-from-Model-Gallery-for-Text-to-Image-Generation"><a href="#IterComp-Iterative-Composition-Aware-Feedback-Learning-from-Model-Gallery-for-Text-to-Image-Generation" class="headerlink" title="IterComp: Iterative Composition-Aware Feedback Learning from Model   Gallery for Text-to-Image Generation"></a>IterComp: Iterative Composition-Aware Feedback Learning from Model   Gallery for Text-to-Image Generation</h2><p><strong>Authors:Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui</strong></p>
<p>Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: <a target="_blank" rel="noopener" href="https://github.com/YangLing0818/IterComp">https://github.com/YangLing0818/IterComp</a> </p>
<blockquote>
<p>å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚RPGã€Stable Diffusion 3å’ŒFLUXï¼Œåœ¨ç»„åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åœ¨ç»„åˆç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå„è‡ªç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œä¸€äº›æ¨¡å‹åœ¨å±æ€§ç»‘å®šæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™åœ¨ç©ºé—´å…³ç³»æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚è¿™ç§å·®å¼‚çªæ˜¾äº†éœ€è¦ä¸€ç§èƒ½å¤Ÿåˆ©ç”¨å„ç§æ¨¡å‹çš„äº’è¡¥ä¼˜åŠ¿æ¥å…¨é¢æ”¹è¿›ç»„åˆèƒ½åŠ›çš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†IterCompï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒä»å¤šä¸ªæ¨¡å‹ä¸­èšåˆç»„åˆæ„ŸçŸ¥æ¨¡å‹åå¥½ï¼Œå¹¶é‡‡ç”¨è¿­ä»£åé¦ˆå­¦ä¹ æ–¹æ³•æ¥æé«˜ç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç²¾é€‰äº†å…­ä¸ªå¼ºå¤§çš„å¼€æºæ‰©æ•£æ¨¡å‹ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬çš„ä¸‰ä¸ªå…³é”®ç»„åˆæŒ‡æ ‡ï¼šå±æ€§ç»‘å®šã€ç©ºé—´å…³ç³»å’Œéç©ºé—´å…³ç³»ã€‚åŸºäºè¿™äº›æŒ‡æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒ…å«ä¼—å¤šå›¾åƒæ’åå¯¹çš„ç»„åˆæ„ŸçŸ¥æ¨¡å‹åå¥½æ•°æ®é›†ï¼Œä»¥è®­ç»ƒç»„åˆæ„ŸçŸ¥å¥–åŠ±æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿­ä»£åé¦ˆå­¦ä¹ æ–¹æ³•ï¼Œä»¥é—­ç¯æ–¹å¼æé«˜ç»„åˆæ€§ï¼Œä½¿åŸºç¡€æ‰©æ•£æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ¬¡è¿­ä»£ä¸­ä¸æ–­è¿›æ­¥å’Œè‡ªæˆ‘å®Œå–„ã€‚ç†è®ºè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼ˆå¦‚Omostå’ŒFLUXï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨å¤šç±»åˆ«å¯¹è±¡ç»„åˆå’Œå¤æ‚è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚IterCompä¸ºæ‰©æ•£æ¨¡å‹çš„å¥–åŠ±åé¦ˆå­¦ä¹ å’Œç»„åˆç”Ÿæˆç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/YangLing0818/IterComp">https://github.com/YangLing0818/IterComp</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07171v2">PDF</a> ICLR 2025. Project: <a target="_blank" rel="noopener" href="https://github.com/YangLing0818/IterComp">https://github.com/YangLing0818/IterComp</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶IterCompã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ±‡èšå¤šä¸ªæ¨¡å‹çš„ç»„åˆæ„ŸçŸ¥æ¨¡å‹åå¥½ï¼Œå¹¶é€šè¿‡è¿­ä»£åé¦ˆå­¦ä¹ æ–¹æ³•å¢å¼ºç»„åˆç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡æ„å»ºåŒ…å«å›¾åƒæ’åºå¯¹çš„ç»„åˆæ„ŸçŸ¥æ¨¡å‹åå¥½æ•°æ®é›†ï¼Œå¹¶å¼•å…¥è¿­ä»£åé¦ˆå­¦ä¹ æ–¹æ³•ï¼Œå®ç°åŸºç¡€æ‰©æ•£æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„é—­ç¯ä¼˜åŒ–å’Œè‡ªæˆ‘ç²¾è¿›ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒIterCompåœ¨å¤šç±»åˆ«å¯¹è±¡ç»„åˆå’Œå¤æ‚è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæ‰©æ•£æ¨¡å‹çš„å¥–åŠ±åé¦ˆå­¦ä¹ å’Œç»„åˆç”Ÿæˆç ”ç©¶å¼€å¯äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¦‚RPGã€Stable Diffusion 3å’ŒFLUXåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»„åˆç”Ÿæˆæ–¹é¢ä»å­˜åœ¨äº’è¡¥æ€§ã€‚</li>
<li>IterCompæ¡†æ¶æ±‡èšå¤šä¸ªæ¨¡å‹çš„ç»„åˆæ„ŸçŸ¥æ¨¡å‹åå¥½ï¼Œæé«˜å…¨é¢æ”¹è¿›ç»„åˆèƒ½åŠ›ã€‚</li>
<li>æ„å»ºåŒ…å«å›¾åƒæ’åºå¯¹çš„ç»„åˆæ„ŸçŸ¥æ¨¡å‹åå¥½æ•°æ®é›†ä»¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>å¼•å…¥è¿­ä»£åé¦ˆå­¦ä¹ æ–¹æ³•å®ç°åŸºç¡€æ‰©æ•£æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„é—­ç¯ä¼˜åŒ–å’Œè‡ªæˆ‘ç²¾è¿›ã€‚</li>
<li>IterCompåœ¨å¤šç±»åˆ«å¯¹è±¡ç»„åˆå’Œå¤æ‚è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>IterCompä¸ºæ‰©æ•£æ¨¡å‹çš„å¥–åŠ±åé¦ˆå­¦ä¹ å’Œç»„åˆç”Ÿæˆç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8dc2175ace97d6444ed2dff61e17dc92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f509a475bef72aae99014fde9a5461.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a643477f2b327f2b2f7f7e067d41e25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a6ed40e370b80bccbea6db90401b45c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PixelShuffler-A-Simple-Image-Translation-Through-Pixel-Rearrangement"><a href="#PixelShuffler-A-Simple-Image-Translation-Through-Pixel-Rearrangement" class="headerlink" title="PixelShuffler: A Simple Image Translation Through Pixel Rearrangement"></a>PixelShuffler: A Simple Image Translation Through Pixel Rearrangement</h2><p><strong>Authors:Omar Zamzam</strong></p>
<p>Image-to-image translation is a topic in computer vision that has a vast range of use cases ranging from medical image translation, such as converting MRI scans to CT scans or to other MRI contrasts, to image colorization, super-resolution, domain adaptation, and generating photorealistic images from sketches or semantic maps. Image style transfer is also a widely researched application of image-to-image translation, where the goal is to synthesize an image that combines the content of one image with the style of another. Existing state-of-the-art methods often rely on complex neural networks, including diffusion models and language models, to achieve high-quality style transfer, but these methods can be computationally expensive and intricate to implement. In this paper, we propose a novel pixel shuffle method that addresses the image-to-image translation problem generally with a specific demonstrative application in style transfer. The proposed method approaches style transfer by shuffling the pixels of the style image such that the mutual information between the shuffled image and the content image is maximized. This approach inherently preserves the colors of the style image while ensuring that the structural details of the content image are retained in the stylized output. We demonstrate that this simple and straightforward method produces results that are comparable to state-of-the-art techniques, as measured by the Learned Perceptual Image Patch Similarity (LPIPS) loss for content preservation and the Fr&#39;echet Inception Distance (FID) score for style similarity. Our experiments validate that the proposed pixel shuffle method achieves competitive performance with significantly reduced complexity, offering a promising alternative for efficient image style transfer, as well as a promise in usability of the method in general image-to-image translation tasks. </p>
<blockquote>
<p>å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªä¸»é¢˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œä»åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆä¾‹å¦‚å°†MRIæ‰«æè½¬æ¢ä¸ºCTæ‰«ææˆ–å…¶ä»–MRIå¯¹æ¯”å‰‚ï¼‰åˆ°å›¾åƒä¸Šè‰²ã€è¶…åˆ†è¾¨ç‡ã€åŸŸé€‚åº”ä»¥åŠä»è‰å›¾æˆ–è¯­ä¹‰åœ°å›¾ç”Ÿæˆé€¼çœŸçš„å›¾åƒç­‰ã€‚å›¾åƒé£æ ¼è¿ç§»ä¹Ÿæ˜¯å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„ä¸€ä¸ªå¹¿æ³›åº”ç”¨ï¼Œç›®æ ‡æ˜¯å°†ä¸€å¼ å›¾åƒçš„å†…å®¹ä¸å¦ä¸€å¼ çš„é£æ ¼åˆæˆåœ¨ä¸€èµ·ã€‚ç°æœ‰çš„å…ˆè¿›æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ï¼Œä»¥å®ç°é«˜è´¨é‡çš„é£æ ¼è¿ç§»ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å®æ–½å¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åƒç´ é‡æ’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸€èˆ¬æ€§åœ°è§£å†³äº†å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘é—®é¢˜ï¼Œå¹¶ä»¥é£æ ¼è¿ç§»ä½œä¸ºå…·ä½“æ¼”ç¤ºåº”ç”¨ã€‚æ‰€æå‡ºçš„æ–¹æ³•é€šè¿‡é‡æ’é£æ ¼å›¾åƒçš„åƒç´ æ¥æ¥è¿‘é£æ ¼è¿ç§»ï¼Œä½¿å¾—é‡æ’å›¾åƒå’Œå†…å®¹å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯æœ€å¤§åŒ–ã€‚è¿™ç§æ–¹æ³•æœ¬è´¨ä¸Šä¿ç•™äº†é£æ ¼å›¾åƒçš„é¢œè‰²ï¼ŒåŒæ—¶ç¡®ä¿å†…å®¹å›¾åƒçš„ç»“æ„ç»†èŠ‚ä¿ç•™åœ¨é£æ ¼åŒ–çš„è¾“å‡ºä¸­ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¿™ç§ç®€å•ç›´æ¥çš„æ–¹æ³•äº§ç”Ÿçš„ç»“æœä¸å…ˆè¿›æŠ€æœ¯äº§ç”Ÿçš„ç»“æœç›¸å½“ï¼Œé€šè¿‡ä¿ç•™å†…å®¹çš„æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰æŸå¤±å’Œé£æ ¼ç›¸ä¼¼åº¦çš„FrÃ©chet Inceptionè·ç¦»ï¼ˆFIDï¼‰åˆ†æ•°æ¥è¡¡é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼Œæ‰€æå‡ºçš„åƒç´ é‡æ’æ–¹æ³•åœ¨æ˜¾è‘—é™ä½å¤æ‚æ€§çš„åŒæ—¶å®ç°äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºé«˜æ•ˆçš„å›¾åƒé£æ ¼è¿ç§»æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼ŒåŒæ—¶ä¹Ÿä¸ºä¸€èˆ¬å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­è¯¥æ–¹æ³•çš„ä½¿ç”¨æä¾›äº†å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03021v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘é—®é¢˜çš„æ–°å‹åƒç´ é‡æ’æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é£æ ¼è½¬ç§»ä¸­çš„åº”ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡æ’é£æ ¼å›¾åƒçš„åƒç´ ï¼Œæœ€å¤§åŒ–é‡æ’å›¾åƒä¸å†…å®¹å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œä»è€Œåœ¨ä¿ç•™é£æ ¼å›¾åƒé¢œè‰²çš„åŒæ—¶ï¼Œç¡®ä¿å†…å®¹å›¾åƒçš„ç»“æ„ç»†èŠ‚åœ¨é£æ ¼åŒ–è¾“å‡ºä¸­å¾—åˆ°ä¿ç•™ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•çš„ç»“æœä¸å…ˆè¿›æŠ€æœ¯ç›¸å½“ï¼Œå¹¶åœ¨å†…å®¹ä¿ç•™å’Œé£æ ¼ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å›¾åƒåˆ°å›¾åƒç¿»è¯‘åœ¨è®¡ç®—æœºè§†è§‰ä¸­æ˜¯ä¸€ä¸ªå¹¿æ³›ç ”ç©¶çš„ä¸»é¢˜ï¼Œæ¶µç›–äº†å¤šç§åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒç¿»è¯‘ã€å›¾åƒè‰²å½©åŒ–ã€è¶…åˆ†è¾¨ç‡ã€åŸŸé€‚åº”ä»¥åŠä»è‰å›¾æˆ–è¯­ä¹‰åœ°å›¾ç”ŸæˆçœŸå®æ„Ÿå›¾åƒã€‚</li>
<li>é£æ ¼è½¬ç§»æ˜¯å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„ä¸€ä¸ªåº”ç”¨ï¼Œæ—¨åœ¨åˆæˆä¸€ä¸ªç»“åˆäº†ä¸€å¼ å›¾åƒçš„å†…å®¹å’Œå¦ä¸€å¼ å›¾åƒçš„é£æ ¼çš„å›¾åƒã€‚</li>
<li>å½“å‰å…ˆè¿›æŠ€æœ¯å¸¸ä¾èµ–å¤æ‚çš„ç¥ç»ç½‘ç»œï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹ï¼Œæ¥å®ç°é«˜è´¨é‡çš„é£æ ¼è½¬ç§»ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å®æ–½å¤æ‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åƒç´ é‡æ’æ–¹æ³•ï¼Œé€šè¿‡æœ€å¤§åŒ–é‡æ’é£æ ¼å›¾åƒä¸å†…å®¹å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯æ¥è§£å†³å›¾åƒåˆ°å›¾åƒç¿»è¯‘é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿ç•™é£æ ¼å›¾åƒé¢œè‰²çš„åŒæ—¶ï¼Œç¡®ä¿å†…å®¹å›¾åƒçš„ç»“æ„ç»†èŠ‚åœ¨é£æ ¼åŒ–è¾“å‡ºä¸­å¾—åˆ°ä¿ç•™ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•çš„ç»“æœä¸å…ˆè¿›æŠ€æœ¯ç›¸å½“ï¼Œå¹¶åœ¨å†…å®¹ä¿ç•™å’Œé£æ ¼ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¸”å®ç°å¤æ‚åº¦æ˜¾è‘—é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4249be70728c40f81581b9234f0ef149.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d27b29046c535f08538ff383720761ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce03319fdca1ee60bc62215e26679ccf.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Realistic-Data-Generation-for-Real-World-Super-Resolution"><a href="#Towards-Realistic-Data-Generation-for-Real-World-Super-Resolution" class="headerlink" title="Towards Realistic Data Generation for Real-World Super-Resolution"></a>Towards Realistic Data Generation for Real-World Super-Resolution</h2><p><strong>Authors:Long Peng, Wenbo Li, Renjing Pei, Jingjing Ren, Jiaqi Xu, Yang Wang, Yang Cao, Zheng-Jun Zha</strong></p>
<p>Existing image super-resolution (SR) techniques often fail to generalize effectively in complex real-world settings due to the significant divergence between training data and practical scenarios. To address this challenge, previous efforts have either manually simulated intricate physical-based degradations or utilized learning-based techniques, yet these approaches remain inadequate for producing large-scale, realistic, and diverse data simultaneously. In this paper, we introduce a novel Realistic Decoupled Data Generator (RealDGen), an unsupervised learning data generation framework designed for real-world super-resolution. We meticulously develop content and degradation extraction strategies, which are integrated into a novel content-degradation decoupled diffusion model to create realistic low-resolution images from unpaired real LR and HR images. Extensive experiments demonstrate that RealDGen excels in generating large-scale, high-quality paired data that mirrors real-world degradations, significantly advancing the performance of popular SR models on various real-world benchmarks. </p>
<blockquote>
<p>ç°æœ‰çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯ç”±äºåœ¨è®­ç»ƒæ•°æ®å’Œå®é™…åœºæ™¯ä¹‹é—´å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œå¾€å¾€ä¸èƒ½åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æœ‰æ•ˆåœ°æ¨å¹¿ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä¹‹å‰çš„ç ”ç©¶è¦ä¹ˆæ‰‹åŠ¨æ¨¡æ‹Ÿå¤æ‚çš„åŸºäºç‰©ç†çš„é€€åŒ–ï¼Œè¦ä¹ˆåˆ©ç”¨åŸºäºå­¦ä¹ çš„æŠ€æœ¯ï¼Œä½†è¿™äº›æ–¹æ³•ä»ç„¶ä¸è¶³ä»¥åŒæ—¶ç”Ÿæˆå¤§è§„æ¨¡ã€ç°å®å’Œå¤šæ ·åŒ–çš„æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„Realistic Decoupled Data Generatorï¼ˆRealDGenï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç°å®ä¸–ç•Œè¶…åˆ†è¾¨ç‡è®¾è®¡çš„æ— ç›‘ç£å­¦ä¹ æ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬ç²¾å¿ƒåˆ¶å®šäº†å†…å®¹å’Œé€€åŒ–æå–ç­–ç•¥ï¼Œå¹¶å°†å…¶é›†æˆåˆ°ä¸€ç§æ–°å‹çš„å†…å®¹ä¸é€€åŒ–è§£è€¦æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥ä»æœªé…å¯¹çš„çœŸå®ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒä¸­åˆ›å»ºé€¼çœŸçš„ä½åˆ†è¾¨ç‡å›¾åƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRealDGenåœ¨ç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡é…å¯¹æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™äº›æ•°æ®åæ˜ äº†ç°å®ä¸–ç•Œä¸­çš„é€€åŒ–æƒ…å†µï¼Œå¹¶åœ¨å„ç§çœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æµè¡ŒSRæ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07255v4">PDF</a> accepted by ICLR 2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„ç°å®è§£è€¦æ•°æ®ç”Ÿæˆå™¨ï¼ˆRealDGenï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç°å®ä¸–ç•Œçš„è¶…åˆ†è¾¨ç‡é—®é¢˜çš„æ— ç›‘ç£å­¦ä¹ æ•°æ®ç”Ÿæˆæ¡†æ¶ã€‚å®ƒé‡‡ç”¨å†…å®¹å’Œé™è§£æå–ç­–ç•¥ï¼Œç»“åˆå†…å®¹é™è§£è§£è€¦æ‰©æ•£æ¨¡å‹ï¼Œä»éé…å¯¹çš„ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒä¸­ç”Ÿæˆé€¼çœŸçš„ä½åˆ†è¾¨ç‡å›¾åƒã€‚å®éªŒè¯æ˜ï¼ŒRealDGenèƒ½å¤Ÿç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€ç¬¦åˆç°å®é€€åŒ–æƒ…å†µçš„æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å„ç§ç°å®åŸºå‡†æµ‹è¯•ä¸­çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯åœ¨å¤æ‚ç°å®åœºæ™¯ä¸­æ³›åŒ–æ•ˆæœä¸ä½³ã€‚</li>
<li>æ­¤å‰çš„æ–¹æ³•åŒ…æ‹¬æ‰‹åŠ¨æ¨¡æ‹Ÿç‰©ç†é™è§£å’Œä½¿ç”¨å­¦ä¹ æŠ€æœ¯ï¼Œä½†æ— æ³•åŒæ—¶äº§ç”Ÿå¤§è§„æ¨¡ã€ç°å®å’Œå¤šæ ·åŒ–çš„æ•°æ®ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç°å®è§£è€¦æ•°æ®ç”Ÿæˆå™¨ï¼ˆRealDGenï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºç°å®ä¸–ç•Œçš„è¶…åˆ†è¾¨ç‡é—®é¢˜è®¾è®¡ã€‚</li>
<li>RealDGené‡‡ç”¨å†…å®¹å’Œé™è§£æå–ç­–ç•¥ï¼Œç»“åˆå†…å®¹é™è§£è§£è€¦æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä»éé…å¯¹çš„ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒä¸­ç”Ÿæˆé€¼çœŸçš„ä½åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRealDGenèƒ½ç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€åæ˜ ç°å®é€€åŒ–æƒ…å†µçš„æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d08035cd470594ebf0ade67c6f6fdd71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8267b36be76bc2ea43912b2453a1a402.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd9f9da21e02fce016a4720cb9f8f07b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f09d43b9123ab3a1ee6fd9cf3ed6106.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Diffusion-Bridge-Implicit-Models"><a href="#Diffusion-Bridge-Implicit-Models" class="headerlink" title="Diffusion Bridge Implicit Models"></a>Diffusion Bridge Implicit Models</h2><p><strong>Authors:Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu</strong></p>
<p>Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/thu-ml/DiffusionBridge">https://github.com/thu-ml/DiffusionBridge</a>. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¡¥æ¨¡å‹ï¼ˆDDBMsï¼‰æ˜¯åœ¨ç»™å®šçš„ä¸¤ä¸ªä»»æ„é…å¯¹åˆ†å¸ƒç«¯ç‚¹ä¹‹é—´è¿›è¡Œæ’å€¼çš„æ‰©æ•£æ¨¡å‹çš„ä¸€ç§å¼ºå¤§å˜ä½“ã€‚å°½ç®¡å®ƒä»¬åœ¨å›¾åƒç¿»è¯‘ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰å‰é€”çš„æ€§èƒ½ï¼Œä½†DDBMséœ€è¦è®¡ç®—å¯†é›†é‡‡æ ·è¿‡ç¨‹ï¼Œè¿™æ¶‰åŠé€šè¿‡æ•°ç™¾æ¬¡ç½‘ç»œè¯„ä¼°æ¨¡æ‹Ÿï¼ˆéšæœºï¼‰å¾®åˆ†æ–¹ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡å–äº†åŠ å¿«DDBMsé‡‡æ ·çš„ç¬¬ä¸€æ­¥ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œå—åˆ°æ‰©æ•£æ¨¡å‹çš„æˆç†Ÿé£Ÿè°±çš„å¯å‘ã€‚æˆ‘ä»¬é€šè¿‡å®šä¹‰ç¦»æ•£æ—¶é—´æ­¥é•¿ä¸Šçš„éé©¬å°”å¯å¤«æ‰©æ•£æ¡¥å¯¹DDBMsè¿›è¡Œæ¦‚æ‹¬ï¼Œè¿™äº›æ¡¥ä¸é‡‡æ ·ç›¸å…³ï¼Œå…·æœ‰ç›¸åŒçš„è¾¹ç¼˜åˆ†å¸ƒå’Œè®­ç»ƒç›®æ ‡ï¼Œäº§ç”Ÿä»éšæœºåˆ°ç¡®å®šæ€§çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶å½¢æˆæ‰©æ•£æ¡¥éšæ¨¡å‹ï¼ˆDBIMsï¼‰ã€‚DBIMsä¸ä»…æ¯”æ™®é€šDDBMsé‡‡æ ·å™¨å¿«25å€ï¼Œè€Œä¸”å¼•å…¥äº†ä¸€ç§æ–°çš„ã€ç®€å•ä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œæ¿€å‘äº†é«˜é˜¶æ•°å€¼æ±‚è§£å™¨çš„çµæ„Ÿã€‚æ­¤å¤–ï¼ŒDBIMsä»¥ç‹¬ç‰¹çš„æ–¹å¼ä¿æŒäº†ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œé€šè¿‡åœ¨åˆå§‹é‡‡æ ·æ­¥éª¤ä¸­ä½¿ç”¨å¼•å¯¼å™ªå£°ï¼Œè¿™ä½¿å…¶åœ¨å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­å®ç°äº†å¿ å®çš„ç¼–ç ã€é‡å»ºå’Œè¯­ä¹‰æ’å€¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thu-ml/DiffusionBridge%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-ml/DiffusionBridgeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15885v4">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>DDBMsï¼ˆå»å™ªæ‰©æ•£æ¡¥æ¢æ¨¡å‹ï¼‰æ˜¯ç”¨äºåœ¨ä¸¤ä¸ªä»»æ„é…å¯¹åˆ†å¸ƒä¹‹é—´è¿›è¡Œæ’å€¼çš„å¼ºå¤§æ‰©æ•£æ¨¡å‹å˜ä½“ã€‚ä¸ºåŠ å¿«DDBMsçš„é‡‡æ ·è¿‡ç¨‹ï¼Œæœ¬ç ”ç©¶é€šè¿‡éé©¬å°”å¯å¤«æ‰©æ•£æ¡¥æ¢å¯¹DDBMsè¿›è¡Œæ¨å¹¿ï¼Œå½¢æˆæ‰©æ•£æ¡¥æ¢éšæ¨¡å‹ï¼ˆDBIMsï¼‰ã€‚DBIMsä¸ä»…é€Ÿåº¦é«˜è¾¾åŸDDBMsçš„25å€ï¼Œè¿˜å¯å‘ä½¿ç”¨é«˜é˜¶æ•°å€¼æ±‚è§£å™¨ã€‚æ­¤å¤–ï¼ŒDBIMsé€šè¿‡åˆå§‹é‡‡æ ·æ­¥éª¤ä¸­çš„å¼•å¯¼å™ªå£°ä¿æŒäº†ç”Ÿæˆå¤šæ ·æ€§ï¼Œä½¿ç¼–ç ã€é‡å»ºå’Œå›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­çš„è¯­ä¹‰æ’å€¼æ›´åŠ å‡†ç¡®ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Denoising Diffusion Bridge Models (DDBMs) å…è®¸åœ¨ä¸¤ä¸ªä»»æ„é…å¯¹åˆ†å¸ƒé—´è¿›è¡Œæ’å€¼ã€‚</li>
<li>é‡‡æ ·è¿‡ç¨‹è®¡ç®—å¯†é›†ï¼Œæ¶‰åŠæ¨¡æ‹Ÿéšæœºå¾®åˆ†æ–¹ç¨‹å’Œæ•°ç™¾æ¬¡ç½‘ç»œè¯„ä¼°ã€‚</li>
<li>æœ¬ç ”ç©¶æ¨å¹¿äº†DDBMsï¼Œå½¢æˆæ‰©æ•£æ¡¥æ¢éšæ¨¡å‹ï¼ˆDBIMsï¼‰ï¼Œä»¥åŠ å¿«é‡‡æ ·é€Ÿåº¦ã€‚</li>
<li>DBIMsé€šè¿‡éé©¬å°”å¯å¤«æ‰©æ•£æ¡¥æ¢å®šä¹‰ï¼Œå…·æœ‰ç›¸åŒçš„è¾¹é™…åˆ†å¸ƒå’ŒåŸ¹è®­ç›®æ ‡ã€‚</li>
<li>DBIMsä¸ä»…ä½¿é‡‡æ ·é€Ÿåº¦æé«˜äº†é«˜è¾¾25å€ï¼Œè¿˜å¯å‘äº†é«˜é˜¶æ•°å€¼æ±‚è§£å™¨çš„ä½¿ç”¨ã€‚</li>
<li>DBIMsé€šè¿‡åˆå§‹é‡‡æ ·æ­¥éª¤ä¸­çš„å¼•å¯¼å™ªå£°ä¿æŒç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f5f24a1f22f9712a5729e657f0cd12cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2168847ad73418759c0cf49bea42be57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c68df5f067cc793625a158171eb849a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5742d0aa04c7dfd7f20275519faf3475.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas"><a href="#Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas" class="headerlink" title="Images that Sound: Composing Images and Sounds on a Single Canvas"></a>Images that Sound: Composing Images and Sounds on a Single Canvas</h2><p><strong>Authors:Ziyang Chen, Daniel Geng, Andrew Owens</strong></p>
<p>Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these visual spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: <a target="_blank" rel="noopener" href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a> </p>
<blockquote>
<p>å…‰è°±å›¾ï¼ˆSpectrogramsï¼‰æ˜¯å£°éŸ³çš„ä¸€ç§äºŒç»´è¡¨ç°å½¢å¼ï¼Œå®ƒä»¬ä¸æˆ‘ä»¬è§†è§‰ä¸–ç•Œä¸­çš„å›¾åƒæˆªç„¶ä¸åŒã€‚å½“è‡ªç„¶å›¾åƒè¢«ç”¨ä½œå…‰è°±å›¾æ—¶ï¼Œä¼šäº§ç”Ÿä¸è‡ªç„¶çš„å£°éŸ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¯ä»¥åˆæˆåŒæ—¶çœ‹èµ·æ¥åƒè‡ªç„¶å›¾åƒå¹¶å¬èµ·æ¥åƒè‡ªç„¶éŸ³é¢‘çš„å…‰è°±å›¾ã€‚æˆ‘ä»¬ç§°è¿™äº›å…‰è°±å›¾ä¸ºâ€œå›¾åƒå£°éŸ³â€ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç®€å•ä¸”é›¶å¯åŠ¨ï¼Œå®ƒåˆ©ç”¨åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œçš„é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å…‰è°±å›¾æ‰©æ•£æ¨¡å‹ã€‚åœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¹¶è¡Œä½¿ç”¨éŸ³é¢‘å’Œå›¾åƒæ‰©æ•£æ¨¡å‹å¯¹å™ªå£°æ½œåœ¨æ•°æ®è¿›è¡Œå»å™ªå¤„ç†ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå¯èƒ½åœ¨ä¸¤ä¸ªæ¨¡å‹ä¸‹éƒ½å­˜åœ¨çš„æ ·æœ¬ã€‚é€šè¿‡å®šé‡è¯„ä¼°å’Œæ„ŸçŸ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°ç”Ÿæˆäº†ä¸æœŸæœ›çš„éŸ³é¢‘æç¤ºå¯¹é½çš„å…‰è°±å›¾ï¼ŒåŒæ—¶é‡‡ç”¨äº†æœŸæœ›çš„å›¾åƒæç¤ºçš„è§†è§‰å¤–è§‚ã€‚è§†é¢‘ç»“æœè¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12221v3">PDF</a> Accepted to NeurIPS 2024. Project site:   <a target="_blank" rel="noopener" href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•å°†é¢‘è°±å›¾ï¼ˆsoundçš„äºŒç»´è¡¨ç¤ºï¼‰ä¸è‡ªç„¶å›¾åƒå’ŒéŸ³é¢‘ç›¸ç»“åˆï¼Œåˆæˆå‡ºæ—¢åƒè‡ªç„¶å›¾åƒåˆå‘å‡ºè‡ªç„¶éŸ³é¢‘çš„é¢‘è°±å›¾ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°é¢‘è°±å›¾çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åå‘è¿‡ç¨‹åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­ç”ŸæˆåŒæ—¶ç¬¦åˆéŸ³é¢‘å’Œå›¾åƒæç¤ºçš„æ ·æœ¬ã€‚è¯¥ç ”ç©¶ä¸ºåˆæˆè§†å¬å†…å®¹å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>è®ºæ–‡å±•ç¤ºäº†ä¸€ç§èƒ½å¤Ÿåˆæˆå…¼å…·è‡ªç„¶å›¾åƒå¤–è§‚å’Œè‡ªç„¶éŸ³é¢‘æ•ˆæœçš„é¢‘è°±å›¾çš„æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°é¢‘è°±å›¾çš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚</li>
<li>é€šè¿‡åå‘è¿‡ç¨‹ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨éŸ³é¢‘å’Œå›¾åƒæ‰©æ•£æ¨¡å‹å¹¶è¡Œå»å™ªæ½œåœ¨å™ªå£°ï¼Œç”ŸæˆåŒæ—¶ç¬¦åˆä¸¤ä¸ªæ¨¡å‹çš„æ ·æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•æˆåŠŸç”Ÿæˆä¸éŸ³é¢‘æç¤ºå¯¹é½çš„é¢‘è°±å›¾ï¼ŒåŒæ—¶ä¿ç•™è§†è§‰æç¤ºçš„å¤–è§‚ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å®šé‡è¯„ä¼°å’Œæ„ŸçŸ¥ç ”ç©¶éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ºè§†å¬å†…å®¹çš„åˆæˆå¼€è¾Ÿäº†æ–°çš„æ–¹å‘ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.12221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81e75a8e9569874f786cb3bba0171baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cfefe2e544f83e69f340499ef02fdde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ae259fc0c8ba64e0a7b6d944dfbf97f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76cb0deeb7cd43ba2e9bded4665e86c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5e1e238f84316b7df6adf955acd175.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GazeHTA-End-to-end-Gaze-Target-Detection-with-Head-Target-Association"><a href="#GazeHTA-End-to-end-Gaze-Target-Detection-with-Head-Target-Association" class="headerlink" title="GazeHTA: End-to-end Gaze Target Detection with Head-Target Association"></a>GazeHTA: End-to-end Gaze Target Detection with Head-Target Association</h2><p><strong>Authors:Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang</strong></p>
<p>Precisely detecting which object a person is paying attention to is critical for human-robot interaction since it provides important cues for the next action from the human user. We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets. </p>
<blockquote>
<p>ç²¾ç¡®æ£€æµ‹äººä»¬æ­£åœ¨å…³æ³¨çš„å¯¹è±¡å¯¹äºäººæœºäº¤äº’è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä¸ºæ¥è‡ªäººç±»ç”¨æˆ·çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œæä¾›äº†é‡è¦çº¿ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ³¨è§†ç›®æ ‡æ£€æµ‹æ–¹æ³•æ¥é¢„æµ‹ä¸ªä½“ä¸ä»–ä»¬æ‰€å…³æ³¨çš„å›¾åƒåŒºåŸŸä¹‹é—´çš„å¤´ç›®æ ‡å…³è”ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä½¿ç”¨ç‹¬ç«‹ç»„ä»¶ï¼Œå¦‚ç°æˆçš„å¤´éƒ¨æ£€æµ‹å™¨ï¼Œæˆ–è€…åœ¨å»ºç«‹å¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´å…³è”æ—¶å­˜åœ¨é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…·æœ‰å¤´éƒ¨å’Œç›®æ ‡å…³è”ï¼ˆGazeHTAï¼‰çš„ç«¯åˆ°ç«¯å¤šäººæ³¨è§†ç›®æ ‡æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»…åŸºäºè¾“å…¥åœºæ™¯å›¾åƒé¢„æµ‹å¤šä¸ªå¤´éƒ¨ç›®æ ‡å®ä¾‹ã€‚GazeHTAé€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³æ³¨è§†ç›®æ ‡æ£€æµ‹ä¸­çš„æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾ä»¥è¿›è¡Œä¸°å¯Œçš„è¯­ä¹‰ç†è§£ï¼›ï¼ˆ2ï¼‰é‡æ–°æ³¨å…¥å¤´éƒ¨ç‰¹å¾ä»¥å¢å¼ºå¤´éƒ¨å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜å¤´éƒ¨ç†è§£ï¼›ï¼ˆ3ï¼‰å­¦ä¹ è¿æ¥å›¾ä½œä¸ºå¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„æ˜ç¡®è§†è§‰å…³è”ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒGazeHTAåœ¨ä¸¤é¡¹æ ‡å‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„æ³¨è§†ç›®æ ‡æ£€æµ‹æ–¹æ³•ä»¥åŠä¸¤é¡¹ç»è¿‡é€‚åº”çš„æ‰©æ•£åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10718v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯å¤šäººæ³¨è§†ç›®æ ‡æ£€æµ‹æ¡†æ¶GazeHTAè¢«æå‡ºï¼Œå®ƒèƒ½å¤Ÿé¢„æµ‹ä¸ªä½“ä¸æ³¨è§†ç›®æ ‡å›¾åƒåŒºåŸŸä¹‹é—´çš„å¤´ç›®æ ‡è¿æ¥ã€‚å®ƒå€ŸåŠ©é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾è¿›è¡Œä¸°å¯Œçš„è¯­ä¹‰ç†è§£ï¼Œå¢å¼ºå¤´éƒ¨ç‰¹å¾ä»¥æ”¹å–„å¤´éƒ¨ç†è§£ï¼Œå¹¶å­¦ä¹ è¿æ¥å›¾ä½œä¸ºå¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„æ˜ç¡®è§†è§‰å…³è”ã€‚åœ¨ä¸¤é¡¹æ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGazeHTAåœ¨æ³¨è§†ç›®æ ‡æ£€æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ä»¥åŠä¸¤ä¸ªåŸºäºæ‰©æ•£çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GazeHTAæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šäººæ³¨è§†ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé¢„æµ‹ä¸ªä½“ä¸æ³¨è§†ç›®æ ‡ä¹‹é—´çš„è¿æ¥ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æå–åœºæ™¯ç‰¹å¾ï¼Œå®ç°ä¸°å¯Œçš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>GazeHTAé€šè¿‡å¢å¼ºå¤´éƒ¨ç‰¹å¾æ¥æ”¹å–„å¯¹å¤´éƒ¨çš„ç†è§£ã€‚</li>
<li>å­¦ä¹ è¿æ¥å›¾ä½œä¸ºå¤´éƒ¨å’Œæ³¨è§†ç›®æ ‡ä¹‹é—´çš„æ˜ç¡®è§†è§‰å…³è”æ˜¯GazeHTAçš„å…³é”®ç‰¹ç‚¹ã€‚</li>
<li>GazeHTAåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•åŠåŸºäºæ‰©æ•£çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜åŒ…æ‹¬ä½¿ç”¨ç‹¬ç«‹ç»„ä»¶ã€å¤´æ£€æµ‹å™¨çš„é—®é¢˜ä»¥åŠå»ºç«‹å¤´ä¸æ³¨è§†ç›®æ ‡ä¹‹é—´å…³è”çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01a82ed905a833eab68c0a6bfb98bba4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e807b84f381fa61002ac69784c434396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2132d27ba5369e4602466113f6a60072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed5309089b22118083cd10ba2b9f7aff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86dd0a1e61b579662f80fc69280514fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-85fa0021576d440b004aac793a0dbd4c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  RadVLM A Multitask Conversational Vision-Language Model for Radiology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-23949113b12518532383866eae9f593a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  SiLVR Scalable Lidar-Visual Radiance Field Reconstruction with   Uncertainty Quantification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
