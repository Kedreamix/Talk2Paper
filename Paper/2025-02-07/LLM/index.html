<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  Do Large Language Model Benchmarks Test Reliability?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c2aedf490ff7ee0cbb3ff7b1063b933f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-07-æ›´æ–°"><a href="#2025-02-07-æ›´æ–°" class="headerlink" title="2025-02-07 æ›´æ–°"></a>2025-02-07 æ›´æ–°</h1><h2 id="Do-Large-Language-Model-Benchmarks-Test-Reliability"><a href="#Do-Large-Language-Model-Benchmarks-Test-Reliability" class="headerlink" title="Do Large Language Model Benchmarks Test Reliability?"></a>Do Large Language Model Benchmarks Test Reliability?</h2><p><strong>Authors:Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry</strong></p>
<p>When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also reliable. Many benchmarks have been created to track LLMsâ€™ growing capabilities, however there has been no similar focus on measuring their reliability. To understand the potential ramifications of this gap, we investigate how well current benchmarks quantify model reliability. We find that pervasive label errors can compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.   Motivated by this gap in the evaluation of reliability, we then propose the concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that, indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures further reveals previously unidentified patterns of problems on which frontier models consistently struggle. We provide code at <a target="_blank" rel="noopener" href="https://github.com/MadryLab/platinum-benchmarks">https://github.com/MadryLab/platinum-benchmarks</a> </p>
<blockquote>
<p>åœ¨éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œç¡®ä¿è¿™äº›æ¨¡å‹ä¸ä»…å…·å¤‡èƒ½åŠ›ï¼Œè€Œä¸”å¯é æ˜¯éå¸¸é‡è¦çš„ã€‚è™½ç„¶å·²ç»åˆ›å»ºäº†è®¸å¤šåŸºå‡†æµ‹è¯•æ¥è·Ÿè¸ªLLMä¸æ–­å¢é•¿çš„èƒ½åŠ›ï¼Œä½†å¯¹äºè¡¡é‡å®ƒä»¬çš„å¯é æ€§å¹¶æ²¡æœ‰ç±»ä¼¼çš„å…³æ³¨ã€‚ä¸ºäº†äº†è§£è¿™ä¸€å·®è·çš„æ½œåœ¨å½±å“ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å½“å‰åŸºå‡†æµ‹è¯•å¯¹æ¨¡å‹å¯é æ€§é‡åŒ–çš„ç¨‹åº¦ã€‚æˆ‘ä»¬å‘ç°æ™®éå­˜åœ¨çš„æ ‡ç­¾é”™è¯¯å¯èƒ½ä¼šæŸå®³è¿™äº›è¯„ä¼°ç»“æœï¼Œæ©ç›–æ¨¡å‹æŒä¹…çš„å¤±è´¥å’Œä¸å¯é çš„è¡Œä¸ºã€‚å—è¿™ç§å¯é æ€§è¯„ä¼°å·®è·çš„é©±åŠ¨ï¼Œæˆ‘ä»¬éšåæå‡ºäº†æ‰€è°“çš„é“‚é‡‘åŸºå‡†æµ‹è¯•çš„æ¦‚å¿µï¼Œå³ç²¾å¿ƒç­–åˆ’çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥å°½é‡å‡å°‘æ ‡ç­¾é”™è¯¯å’Œæ¨¡ç³Šæ€§ã€‚ä½œä¸ºæ„å»ºæ­¤ç±»åŸºå‡†æµ‹è¯•çš„åˆæ­¥å°è¯•ï¼Œæˆ‘ä»¬ä¿®è®¢äº†æ¥è‡ªåäº”ä¸ªç°æœ‰æµè¡ŒåŸºå‡†æµ‹è¯•çš„ç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨è¿™äº›é“‚é‡‘åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å¹¿æ³›çš„æ¨¡å‹ï¼Œå¹¶å‘ç°å‰æ²¿çš„LLMç¡®å®åœ¨ç®€å•çš„ä»»åŠ¡ï¼ˆå¦‚å°å­¦æ°´å¹³çš„æ•°å­¦åº”ç”¨é¢˜ï¼‰ä¸Šè¡¨ç°å‡ºå¤±è´¥ã€‚è¿›ä¸€æ­¥åˆ†æè¿™äº›å¤±è´¥æ­ç¤ºäº†ä¹‹å‰æœªå‘ç°çš„æ¨¡å¼é—®é¢˜ï¼Œå‰æ²¿æ¨¡å‹åœ¨è¿™äº›é—®é¢˜ä¸Šä¸€ç›´è¡¨ç°å›°éš¾ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/MadryLab/platinum-benchmarks%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/MadryLab/platinum-benchmarksä¸Šæä¾›äº†ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03461v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éƒ¨ç½²éœ€å…¼é¡¾èƒ½åŠ›å’Œå¯é æ€§ã€‚å½“å‰ä¸»è¦å…³æ³¨æ¨¡å‹çš„èƒ½åŠ›è¯„ä¼°ï¼Œè€Œç¼ºä¹å¯¹å…¶å¯é æ€§çš„è¡¡é‡ã€‚ç ”ç©¶å‘ç°ï¼Œæ™®éå­˜åœ¨çš„æ ‡ç­¾é”™è¯¯ä¼šå½±å“æ¨¡å‹è¯„ä¼°ï¼Œæ©ç›–æ¨¡å‹å¤±è´¥å’Œä¸å¯é è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œæå‡ºâ€œç™½é‡‘åŸºå‡†æµ‹è¯•â€æ¦‚å¿µï¼Œç²¾å¿ƒç¼–åˆ¶ä»¥æœ€å°åŒ–æ ‡ç­¾é”™è¯¯å’Œæ¨¡ç³Šæ€§ã€‚ä¿®è®¢ç°æœ‰15ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸­çš„ä¾‹å­ï¼Œå‘ç°å‰æ²¿LLMåœ¨ç®€å•ä»»åŠ¡ä¸Šä»æœ‰å¤±è´¥ï¼Œå¦‚å°å­¦æ•°å­¦é¢˜ã€‚åˆ†æè¿™äº›å¤±è´¥æ­ç¤ºä¹‹å‰æœªè¯†åˆ«çš„é—®é¢˜æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMéƒ¨ç½²éœ€åŒæ—¶å…³æ³¨èƒ½åŠ›å’Œå¯é æ€§ã€‚</li>
<li>å½“å‰ä¸»è¦å…³æ³¨æ¨¡å‹çš„èƒ½åŠ›è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œç¼ºä¹å¯¹å…¶å¯é æ€§çš„è¡¡é‡ã€‚</li>
<li>æ ‡ç­¾é”™è¯¯ä¼šå½±å“æ¨¡å‹è¯„ä¼°ï¼Œæ©ç›–æ¨¡å‹å¤±è´¥å’Œä¸å¯é è¡Œä¸ºã€‚</li>
<li>æå‡ºâ€œç™½é‡‘åŸºå‡†æµ‹è¯•â€æ¦‚å¿µï¼Œä»¥æœ€å°åŒ–æ ‡ç­¾é”™è¯¯å’Œæ¨¡ç³Šæ€§ã€‚</li>
<li>ä¿®è®¢ç°æœ‰åŸºå‡†æµ‹è¯•ä¾‹å­ï¼Œå‘ç°å‰æ²¿LLMåœ¨ç®€å•ä»»åŠ¡ä¸Šçš„å¤±è´¥ã€‚</li>
<li>è¿™äº›å¤±è´¥æ­ç¤ºä¹‹å‰æœªè¯†åˆ«çš„é—®é¢˜æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03461">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02c143e97b457502d93eba3732600a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6635c42adae2e8c67fea908e9d43dfea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8e4920e9679b4252856a6208939d673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-193e50fd8256ddb4820ce68cf7646172.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adapt-Pruner-Adaptive-Structural-Pruning-for-Efficient-Small-Language-Model-Training"><a href="#Adapt-Pruner-Adaptive-Structural-Pruning-for-Efficient-Small-Language-Model-Training" class="headerlink" title="Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language   Model Training"></a>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language   Model Training</h2><p><strong>Authors:Boyao Wang, Rui Pan, Shizhe Diao, Xingyuan Pan, Jipeng Zhang, Renjie Pi, Tong Zhang</strong></p>
<p>Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress&#x2F;prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. </p>
<blockquote>
<p>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å› å…¶è¾¹ç¼˜è®¾å¤‡åº”ç”¨çš„å¹¿æ³›æ€§è€Œå¼•èµ·äº†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„å¹¿æ³›å…³æ³¨ã€‚ä¸ºäº†è·å¾—æ€§èƒ½å¼ºå¤§çš„SLMï¼Œä¼ ç»Ÿçš„æ–¹æ³•è¦ä¹ˆä»å¤´å¼€å§‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œè¦ä¹ˆå‹ç¼©æˆ–ä¿®å‰ªç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†è¿™ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸”æ— æ³•ä¸é¢„è®­ç»ƒç›¸æŠ—è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ¶‰åŠç»“æ„åŒ–ä¿®å‰ªå’Œæ¨¡å‹è®­ç»ƒç›¸ç»“åˆçš„åŠ é€Ÿæ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼š1ï¼‰é€å±‚è‡ªé€‚åº”ä¿®å‰ªï¼ˆAdapt-Prunerï¼‰åœ¨LLMä¸­æä¸ºæœ‰æ•ˆï¼Œå¯¹ç°æœ‰ä¿®å‰ªæŠ€æœ¯äº§ç”Ÿäº†é‡å¤§æ”¹è¿›ï¼›2ï¼‰é…å¤‡è¿›ä¸€æ­¥è®­ç»ƒçš„è‡ªé€‚åº”ä¿®å‰ªäº§ç”Ÿçš„æ¨¡å‹ä¸ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ï¼›3ï¼‰å¢é‡ä¿®å‰ªé€šè¿‡äº¤æ›¿è¿›è¡Œä¿®å‰ªå’ŒåŸ¹è®­ï¼Œæ¯æ¬¡åªç§»é™¤ä¸€å°éƒ¨åˆ†ç¥ç»å…ƒï¼ˆ~5%ï¼‰ï¼Œå¸¦æ¥äº†ä¸å°çš„æ€§èƒ½æå‡ã€‚åœ¨LLaMA-3.1-8Bä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAdapt-Pruneråœ¨å¸¸è¯†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡å¹³å‡ä¼˜äºä¼ ç»Ÿçš„ä¿®å‰ªæ–¹æ³•ï¼ˆå¦‚LLM-Prunerã€FLAPå’ŒSliceGPTï¼‰1%-7%ã€‚æ­¤å¤–ï¼ŒAdapt-Pruneré€šè¿‡ä¿®å‰ªæ›´å¤§çš„æ¨¡å‹ï¼Œå°†MobileLLM-125Mçš„æ€§èƒ½æ¢å¤åˆ°600Mçš„çº§åˆ«ï¼ˆMMLUåŸºå‡†æµ‹è¯•ï¼‰ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªæ–°çš„1Bæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†LLaMA-3.2-1Bã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03460v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ¶‰åŠç»“æ„å‰ªæå’Œæ¨¡å‹è®­ç»ƒçš„åŠ é€Ÿæ–¹æ³•ï¼Œç ”ç©¶äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å¼ºå¤§æ€§èƒ½ã€‚å‘ç°å±‚é€‚åº”å‰ªæï¼ˆAdapt-Prunerï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æä¸ºæœ‰æ•ˆï¼Œå¯¹ç°æœ‰å‰ªææŠ€æœ¯æœ‰æ˜¾è‘—æ”¹å–„ã€‚ç»“åˆè¿›ä¸€æ­¥è®­ç»ƒçš„é€‚åº”å‰ªæå¯ç”Ÿæˆä¸ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚å¢é‡å‰ªæé€šè¿‡äº¤æ›¿è¿›è¡Œå‰ªæå’Œè®­ç»ƒï¼Œæ¯æ¬¡ä»…ç§»é™¤ä¸€å°éƒ¨åˆ†ç¥ç»å…ƒï¼ˆçº¦5%ï¼‰ï¼Œå¸¦æ¥äº†ä¸ä¿—çš„æ€§èƒ½æå‡ã€‚åœ¨LLaMA-3.1-8Bä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAdapt-Pruneråœ¨å¸¸è¯†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®åº¦å¹³å‡é«˜å‡ºä¼ ç»Ÿå‰ªææ–¹æ³•ï¼ˆå¦‚LLM-Prunerã€FLAPå’ŒSliceGPTï¼‰1%-7%ã€‚æ­¤å¤–ï¼ŒAdapt-Pruneré€šè¿‡ä»å¤§æ¨¡å‹ä¸­å‰ªæï¼Œå°†MobileLLM-125Mçš„æ€§èƒ½æ¢å¤åˆ°MMLUåŸºå‡†æµ‹è¯•çš„600Mæ°´å¹³ï¼Œå¹¶å‘ç°ä¸€ä¸ªæ–°çš„å°å‹æ¨¡å‹ï¼ˆä»…1Bï¼‰ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†LLaMA-3.2-1Bã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å‰ªææŠ€æœ¯åœ¨å°å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåŠ›ä¸ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å±‚é€‚åº”å‰ªæï¼ˆAdapt-Prunerï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—æ”¹è¿›äº†ç°æœ‰å‰ªææŠ€æœ¯ã€‚</li>
<li>ç»“åˆè¿›ä¸€æ­¥è®­ç»ƒçš„é€‚åº”å‰ªæèƒ½å¤Ÿç”Ÿæˆä¸ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>å¢é‡å‰ªæé€šè¿‡äº¤æ›¿è¿›è¡Œå‰ªæå’Œè®­ç»ƒï¼Œå¯æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Adapt-Pruneråœ¨å¸¸è¯†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®åº¦å¹³å‡é«˜å‡ºå…¶ä»–ä¼ ç»Ÿå‰ªææ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä»å¤§æ¨¡å‹ä¸­å‰ªæï¼ŒAdapt-Prunerå¯å°†å°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡è‡³æ›´é«˜æ°´å¹³ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†ä¸€ä¸ªæ–°çš„å°å‹è¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4478257977c8e79f78a8693bb8a08c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41703ac77f409c9fa150751beb979e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2edbe29ee9b76449412ffefb0ed79351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c63cc636d3f3ad6a7375dc525d7a20e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e187a35f025a0db12b7f0c9aaf866a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da27b7237cb63769a620f4933e70239a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Designing-LLM-simulated-Immersive-Spaces-to-Enhance-Autistic-Childrenâ€™s-Social-Affordances-Understanding"><a href="#Designing-LLM-simulated-Immersive-Spaces-to-Enhance-Autistic-Childrenâ€™s-Social-Affordances-Understanding" class="headerlink" title="Designing LLM-simulated Immersive Spaces to Enhance Autistic Childrenâ€™s   Social Affordances Understanding"></a>Designing LLM-simulated Immersive Spaces to Enhance Autistic Childrenâ€™s   Social Affordances Understanding</h2><p><strong>Authors:Yancheng Cao, Yangyang HE, Yonglin Chen, Menghan Chen, Shanhe You, Yulin Qiu, Min Liu, Chuan Luo, Chen Zheng, Xin Tong, Jing Liang, Jiangtao Gong</strong></p>
<p>One of the key challenges faced by autistic children is understanding social affordances in complex environments, which further impacts their ability to respond appropriately to social signals. In traffic scenarios, this impairment can even lead to safety concerns. In this paper, we introduce an LLM-simulated immersive projection environment designed to improve this ability in autistic children while ensuring their safety. We first propose 17 design considerations across four major categories, derived from a comprehensive review of previous research. Next, we developed a system called AIroad, which leverages LLMs to simulate drivers with varying social intents, expressed through explicit multimodal social signals. AIroad helps autistic children bridge the gap in recognizing the intentions behind behaviors and learning appropriate responses through various stimuli. A user study involving 14 participants demonstrated that this technology effectively engages autistic children and leads to significant improvements in their comprehension of social affordances in traffic scenarios. Additionally, parents reported high perceived usability of the system. These findings highlight the potential of combining LLM technology with immersive environments for the functional rehabilitation of autistic children in the future. </p>
<blockquote>
<p>è‡ªé—­ç—‡å„¿ç«¥é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç†è§£å¤æ‚ç¯å¢ƒä¸­çš„ç¤¾ä¼šè§„èŒƒï¼Œè¿™è¿›ä¸€æ­¥å½±å“äº†ä»–ä»¬å¯¹ç¤¾ä¼šä¿¡å·çš„é€‚å½“ååº”èƒ½åŠ›ã€‚åœ¨äº¤é€šåœºæ™¯ä¸­ï¼Œè¿™ç§éšœç¢ç”šè‡³ä¼šå¯¼è‡´å®‰å…¨é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿçš„æ²‰æµ¸å¼æŠ•å½±ç¯å¢ƒï¼Œæ—¨åœ¨æé«˜è‡ªé—­ç—‡å„¿ç«¥åœ¨è¿™ä¸€æ–¹é¢çš„èƒ½åŠ›ï¼ŒåŒæ—¶ç¡®ä¿ä»–ä»¬çš„å®‰å…¨ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºå››ä¸ªä¸»è¦ç±»åˆ«çš„17ä¸ªè®¾è®¡è€ƒé‡ï¼Œè¿™äº›è€ƒé‡æ˜¯åŸºäºå¯¹å…ˆå‰ç ”ç©¶çš„å…¨é¢å›é¡¾è€Œå¾—å‡ºçš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåä¸ºAIroadçš„ç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå…·æœ‰ä¸åŒç¤¾ä¼šæ„å›¾çš„é©¾é©¶å‘˜ï¼Œè¿™äº›æ„å›¾é€šè¿‡æ˜ç¡®çš„å¤šæ¨¡å¼ç¤¾ä¼šä¿¡å·æ¥è¡¨è¾¾ã€‚AIroadå¸®åŠ©è‡ªé—­ç—‡å„¿ç«¥å¼¥è¯†åˆ«äººç±»è¡Œä¸ºèƒŒåçš„æ„å›¾å·®è·ï¼Œå¹¶é€šè¿‡å„ç§åˆºæ¿€å­¦ä¹ é€‚å½“çš„ååº”ã€‚ä¸€é¡¹æ¶‰åŠ14åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æŠ€æœ¯æœ‰æ•ˆåœ°å¸å¼•äº†è‡ªé—­ç—‡å„¿ç«¥çš„å‚ä¸ï¼Œå¹¶åœ¨ä»–ä»¬å¯¹äº¤é€šåœºæ™¯ä¸­çš„ç¤¾ä¼šè§„èŒƒçš„ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æ­¤å¤–ï¼Œçˆ¶æ¯æŠ¥å‘Šäº†ç³»ç»Ÿçš„é«˜å¯ç”¨æ€§æ„ŸçŸ¥ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å°†å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ä¸æ²‰æµ¸å¼ç¯å¢ƒç›¸ç»“åˆåœ¨æœªæ¥å¯¹è‡ªé—­ç—‡å„¿ç«¥è¿›è¡ŒåŠŸèƒ½åº·å¤çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03447v1">PDF</a> iui2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªé—­ç—‡æ‚£è€…éš¾ä»¥ç†è§£å¤æ‚ç¯å¢ƒä¸­çš„ç¤¾äº¤è§„åˆ™å’Œæƒ¯ä¾‹çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨¡æ‹Ÿæ²‰æµ¸å¼æŠ•å½±ç¯å¢ƒä»¥æé«˜è‡ªé—­ç—‡å„¿ç«¥ç†è§£èƒ½åŠ›çš„æŠ€æœ¯ç³»ç»ŸAIroadã€‚AIroadå€ŸåŠ©LLMæŠ€æœ¯æ¨¡æ‹Ÿé©¾é©¶è€…çš„ç¤¾äº¤æ„å›¾ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€ç¤¾äº¤ä¿¡å·è¡¨è¾¾è¿™äº›æ„å›¾ã€‚é€šè¿‡ç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜AIroadèƒ½å¤Ÿå¸®åŠ©è‡ªé—­ç—‡å„¿ç«¥è¯†åˆ«è¡Œä¸ºèƒŒåçš„æ„å›¾å¹¶å­¦ä¹ é€‚å½“ååº”ã€‚çˆ¶æ¯æŠ¥å‘Šç§°ç³»ç»Ÿçš„é«˜å¯ç”¨æ€§ï¼Œæ˜¾ç¤ºè¯¥æŠ€æœ¯å…·æœ‰æ½œåŠ›ä¸ºè‡ªé—­ç—‡å„¿ç«¥çš„åŠŸèƒ½åº·å¤æä¾›å¸®åŠ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªé—­ç—‡å„¿ç«¥é¢ä¸´ç†è§£å¤æ‚ç¯å¢ƒä¸­ç¤¾äº¤è§„åˆ™çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ä»–ä»¬éš¾ä»¥å¯¹ç¤¾ä¼šä¿¡å·åšå‡ºé€‚å½“ååº”ã€‚</li>
<li>é’ˆå¯¹è‡ªé—­ç—‡å„¿ç«¥çš„ç†è§£èƒ½åŠ›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ©ç”¨LLMæŠ€æœ¯çš„æ²‰æµ¸å¼æŠ•å½±ç¯å¢ƒè®¾è®¡ï¼Œæ—¨åœ¨æé«˜ä»–ä»¬çš„ç¤¾äº¤ç†è§£èƒ½åŠ›ã€‚</li>
<li>ç³»ç»ŸAIroadæ¨¡æ‹Ÿä¸åŒé©¾é©¶è€…çš„ç¤¾äº¤æ„å›¾ï¼Œé€šè¿‡å¤šæ¨¡æ€ç¤¾äº¤ä¿¡å·å¸®åŠ©è‡ªé—­ç—‡å„¿ç«¥è¯†åˆ«å’Œç†è§£è¿™äº›æ„å›¾ã€‚</li>
<li>é€šè¿‡ç”¨æˆ·ç ”ç©¶éªŒè¯äº†AIroadçš„æœ‰æ•ˆæ€§ï¼Œå‚ä¸è€…è¡¨ç°å‡ºå¯¹ç¤¾äº¤è§„åˆ™çš„æ›´å¥½ç†è§£ã€‚</li>
<li>çˆ¶æ¯å¯¹AIroadç³»ç»Ÿçš„å¯ç”¨æ€§ç»™äºˆäº†é«˜åº¦è¯„ä»·ã€‚</li>
<li>AIroadæŠ€æœ¯ä¸ºè‡ªé—­ç—‡å„¿ç«¥çš„åŠŸèƒ½åº·å¤æä¾›äº†æ½œåœ¨çš„å¸®åŠ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec926a00760170f2db52d2fe276b53c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12a8e09245036d7ef04f9e94397b21bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-538464511a23a7efb85fcf59aabcd065.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BFS-Prover-Scalable-Best-First-Tree-Search-for-LLM-based-Automatic-Theorem-Proving"><a href="#BFS-Prover-Scalable-Best-First-Tree-Search-for-LLM-based-Automatic-Theorem-Proving" class="headerlink" title="BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic   Theorem Proving"></a>BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic   Theorem Proving</h2><p><strong>Authors:Ran Xin, Chenguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, Kai Shen</strong></p>
<p>Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLMâ€™s policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \texttt{BFS-Prover} achieves a score of $71.31$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¿€å‘äº†ä½¿ç”¨Lean4è¿›è¡Œè‡ªåŠ¨å®šç†è¯æ˜çš„å…´è¶£ï¼Œå…¶ä¸­æœ‰æ•ˆçš„æ ‘æœç´¢æ–¹æ³•åœ¨è¯æ˜æœç´¢ç©ºé—´ä¸­å¯¼èˆªè‡³å…³é‡è¦ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå€¼å‡½æ•°å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œä½†æ›´ç®€å•æ–¹æ³•å¦‚æœ€ä½³ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶BFSåœ¨å¤§è§„æ¨¡å®šç†è¯æ˜ä»»åŠ¡ä¸­æ˜¯å¦èƒ½å¤Ÿå®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†å¯æ‰©å±•çš„ä¸“å®¶è¿­ä»£æ¡†æ¶<code>BFS-Prover&#39;ï¼Œå®ƒå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªä¸“å®¶è¿­ä»£è½®æ¬¡å®ç°æˆ˜ç•¥æ•°æ®è¿‡æ»¤ï¼Œæ’é™¤å¯é€šè¿‡å…‰æŸæœç´¢èŠ‚ç‚¹æ‰©å±•è§£å†³çš„é—®é¢˜ï¼Œä¸“æ³¨äºæ›´å¤æ‚çš„æ¡ˆä¾‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨äºçŠ¶æ€æˆ˜æœ¯å¯¹çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ”¹è¿›äº†BFSçš„æ ·æœ¬æ•ˆç‡ï¼ŒçŠ¶æ€æˆ˜æœ¯å¯¹ä¼šè‡ªåŠ¨æ³¨é‡Šç¼–è¯‘å™¨é”™è¯¯åé¦ˆï¼Œä»¥ä¼˜åŒ–LLMçš„ç­–ç•¥ï¼Œä½¿å…¶ä¼˜å…ˆè¿›è¡Œæœ‰ç”Ÿäº§ä»·å€¼çš„æ‰©å±•ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨BFSä¸­é‡‡ç”¨é•¿åº¦å½’ä¸€åŒ–ï¼Œä»¥é¼“åŠ±æ›´æ·±å…¥çš„è¯æ˜è·¯å¾„çš„æ¢ç´¢ã€‚</code>BFS-Proverâ€™åœ¨MiniF2Fæµ‹è¯•é›†ä¸Šå–å¾—äº†71.31çš„åˆ†æ•°ï¼Œä»è€Œè´¨ç–‘äº†å¤æ‚æ ‘æœç´¢æ–¹æ³•çš„å¿…è¦æ€§ï¼Œè¯æ˜å½“é€‚å½“æ‰©å±•æ—¶ï¼ŒBFSå¯ä»¥å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03438v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ¿€å‘äº†ä½¿ç”¨Lean4è¿›è¡Œè‡ªåŠ¨å®šç†è¯æ˜çš„å…´è¶£ï¼Œå…¶ä¸­æœ‰æ•ˆçš„æ ‘æœç´¢æ–¹æ³•åœ¨è¯æ˜æœç´¢ç©ºé—´ä¸­è‡³å…³é‡è¦ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå€¼å‡½æ•°å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œä½†æ›´ç®€å•çš„æ–¹æ³•å¦‚æœ€ä½³ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶BFSåœ¨å¤§è§„æ¨¡å®šç†è¯æ˜ä»»åŠ¡ä¸­æ˜¯å¦å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†å¯æ‰©å±•çš„ä¸“å®¶è¿­ä»£æ¡†æ¶BFS-Proverï¼Œå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªä¸“å®¶è¿­ä»£è½®æ¬¡ä¸­å®ç°æˆ˜ç•¥æ•°æ®è¿‡æ»¤ï¼Œæ’é™¤å¯ä»¥é€šè¿‡æŸæœç´¢èŠ‚ç‚¹æ‰©å±•è§£å†³çš„é—®é¢˜ï¼Œä¸“æ³¨äºæ›´å¤æ‚çš„æƒ…å†µã€‚å…¶æ¬¡ï¼Œé€šè¿‡åº”ç”¨äºçŠ¶æ€æˆ˜æœ¯å¯¹è‡ªåŠ¨æ³¨é‡Šçš„ç¼–è¯‘å™¨é”™è¯¯åé¦ˆçš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œæé«˜äº†BFSçš„æ ·æœ¬æ•ˆç‡ï¼Œä¼˜åŒ–äº†LLMçš„ç­–ç•¥ä»¥ä¼˜å…ˆè€ƒè™‘æœ‰æ•ˆçš„æ‰©å±•ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨BFSä¸­é‡‡ç”¨é•¿åº¦å½’ä¸€åŒ–ï¼Œä»¥é¼“åŠ±æ›´æ·±å…¥çš„è¯æ˜è·¯å¾„çš„æ¢ç´¢ã€‚BFS-Proveråœ¨MiniF2Fæµ‹è¯•é›†ä¸Šå–å¾—äº†71.31çš„è¯„åˆ†ï¼Œä»è€ŒæŒ‘æˆ˜äº†å¤æ‚æ ‘æœç´¢æ–¹æ³•çš„å¿…è¦æ€§ï¼Œè¡¨æ˜é€‚å½“æ‰©å±•çš„BFSå¯ä»¥å®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨å®šç†è¯æ˜é¢†åŸŸçš„åº”ç”¨æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå…¶ä¸­æœ‰æ•ˆçš„æ ‘æœç´¢æ–¹æ³•è‡³å…³é‡è¦ã€‚</li>
<li>è™½ç„¶è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ˜¯å½“å‰ä¸»è¦æ–¹æ³•ï¼Œä½†æœ€ä½³ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„BFS-Proverä¸“å®¶è¿­ä»£æ¡†æ¶å…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼ŒåŒ…æ‹¬æˆ˜ç•¥æ•°æ®è¿‡æ»¤ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œé•¿åº¦å½’ä¸€åŒ–ã€‚</li>
<li>é€šè¿‡æˆ˜ç•¥æ•°æ®è¿‡æ»¤ï¼ŒBFS-Proverèƒ½å¤Ÿä¸“æ³¨äºæ›´å¤æ‚çš„è¯æ˜é—®é¢˜ã€‚</li>
<li>DPOæé«˜äº†BFSçš„æ ·æœ¬æ•ˆç‡ï¼Œé€šè¿‡ä¼˜åŒ–LLMçš„ç­–ç•¥ä»¥ä¼˜å…ˆè€ƒè™‘æœ‰æ•ˆçš„æ‰©å±•ã€‚</li>
<li>é•¿åº¦å½’ä¸€åŒ–é¼“åŠ±BFSæ¢ç´¢æ›´æ·±å…¥çš„è¯æ˜è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bda1588ee31f18bbf098ae5fd3b1add8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c896fdb100bb3ed3e0d7f2ea8075c0be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-033b9ecbb7b11989231334ce649b48ed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SPRI-Aligning-Large-Language-Models-with-Context-Situated-Principles"><a href="#SPRI-Aligning-Large-Language-Models-with-Context-Situated-Principles" class="headerlink" title="SPRI: Aligning Large Language Models with Context-Situated Principles"></a>SPRI: Aligning Large Language Models with Context-Situated Principles</h2><p><strong>Authors:Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin</strong></p>
<p>Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness. We release our code and model generations at <a target="_blank" rel="noopener" href="https://github.com/honglizhan/SPRI-public">https://github.com/honglizhan/SPRI-public</a>. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚ç›¸ç»“åˆå¹¶åæ˜ å‡ºæ¥ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç»†äººç±»ç›‘ç£çš„ä»»åŠ¡ä¸­ï¼Œæ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œå› ä¸ºä¾èµ–äººç±»ä¸“å®¶æä¾›ç‰¹å®šæƒ…å¢ƒçš„æŒ‡å¯¼æ˜¯èµ„æºå¯†é›†å‹å’Œè€—æ—¶çš„ã€‚æ—©æœŸçš„å·¥ä½œå·²ç»åˆ©ç”¨é¢„å…ˆå®šä¹‰çš„ä¸€ç»„è§„åˆ™æˆ–åŸåˆ™æ¥å¼•å¯¼æ¨¡å‹çš„è¡Œä¸ºï¼ˆBaiç­‰äººï¼Œ2022å¹´ï¼›Sunç­‰äººï¼Œ2023å¹´ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›åŸåˆ™å¾€å¾€æ˜¯é€šç”¨çš„ï¼Œéš¾ä»¥é€‚åº”æ¯ä¸ªå•ç‹¬çš„è¾“å…¥æŸ¥è¯¢æˆ–ä¸Šä¸‹æ–‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æƒ…å¢ƒåŒ–åŸåˆ™ï¼ˆSPRIï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦æœ€å°æˆ–æ— éœ€äººå·¥å‚ä¸çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®æ—¶ä¸ºæ¯ä¸ªè¾“å…¥æŸ¥è¯¢è‡ªåŠ¨ç”ŸæˆæŒ‡å¯¼åŸåˆ™ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥å¯¹é½æ¯ä¸ªå“åº”ã€‚æˆ‘ä»¬åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸Šè¯„ä¼°äº†SPRIï¼Œå¹¶è¯æ˜ï¼š1ï¼‰SPRIå¯ä»¥åœ¨å¤æ‚çš„ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­æ¨å¯¼å‡ºåŸåˆ™ï¼Œå…¶æ€§èƒ½ä¸ä¸“å®¶åˆ¶å®šçš„åŸåˆ™ç›¸å½“ï¼›2ï¼‰SPRIç”Ÿæˆçš„åŸåˆ™å¯¼è‡´å®ä¾‹ç‰¹å®šçš„è§„åˆ™ï¼Œä¼˜äºå…ˆå‰çš„LLMä½œä¸ºæ³•å®˜çš„æ¡†æ¶ï¼›3ï¼‰ä½¿ç”¨SPRIç”ŸæˆåˆæˆSFTæ•°æ®å¯¼è‡´çœŸå®æ€§æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/honglizhan/SPRI-public%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/honglizhan/SPRI-publicä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ç”Ÿæˆçš„æ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03397v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹é›†æˆå’Œåæ˜ äººç±»ä»·å€¼è§‚çš„æŒ‘æˆ˜æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSituated-PRInciplesï¼ˆSPRIï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®æ—¶ä¸ºæ¯ä¸€ä¸ªè¾“å…¥æŸ¥è¯¢è‡ªåŠ¨ç”ŸæˆæŒ‡å¯¼åŸåˆ™ï¼Œå¹¶åˆ©ç”¨è¿™äº›åŸåˆ™å¯¹é½æ¯ä¸€ä¸ªå“åº”ï¼Œæ— éœ€æˆ–åªéœ€æå°‘çš„äººå·¥å‚ä¸ã€‚å®éªŒè¡¨æ˜ï¼ŒSPRIèƒ½åœ¨å¤æ‚é¢†åŸŸä»»åŠ¡ä¸­æ¨å¯¼å‡ºä¸ä¸“å®¶åˆ¶å®šçš„åŸåˆ™ç›¸å½“çš„åŸåˆ™ï¼Œä¸”åœ¨å®ä¾‹ç‰¹å®šçš„è¯„åˆ¤æ ‡å‡†ä¸Šä¼˜äºå…ˆå‰çš„LLMåˆ¤æ–­æ¡†æ¶ã€‚æ­¤å¤–ï¼Œä½¿ç”¨SPRIç”Ÿæˆåˆæˆæ•°æ®å¯æ˜¾è‘—æé«˜çœŸå®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆå’Œåæ˜ äººç±»ä»·å€¼è§‚æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
<li>SPRIæ¡†æ¶å¯ä»¥å®æ—¶ä¸ºæ¯ä¸€ä¸ªè¾“å…¥æŸ¥è¯¢è‡ªåŠ¨ç”ŸæˆæŒ‡å¯¼åŸåˆ™ï¼Œå®ç°æ¨¡å‹çš„è‡ªåŠ¨å¯¹é½ã€‚</li>
<li>SPRIæ¡†æ¶å¯ä»¥å‡å°‘å¯¹äººå·¥ç»éªŒçš„ä¾èµ–ã€‚</li>
<li>SPRIåœ¨å¤æ‚é¢†åŸŸä»»åŠ¡ä¸­æ¨å¯¼å‡ºçš„åŸåˆ™ä¸ä¸“å®¶åˆ¶å®šçš„åŸåˆ™ç›¸å½“ã€‚</li>
<li>SPRIåœ¨å®ä¾‹ç‰¹å®šçš„è¯„åˆ¤æ ‡å‡†ä¸Šä¼˜äºå…ˆå‰çš„LLMåˆ¤æ–­æ¡†æ¶ã€‚</li>
<li>ä½¿ç”¨SPRIç”Ÿæˆåˆæˆæ•°æ®å¯ä»¥æ˜¾è‘—æé«˜çœŸå®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-82bbec3519395052dbbd84be1ff54992.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4393692d8103b655ea27ee7ff87d035a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9c76d76fe25228621f4737d7c1fb4cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44f84305e20eaa94022d78d642a9e91b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LIMO-Less-is-More-for-Reasoning"><a href="#LIMO-Less-is-More-for-Reasoning" class="headerlink" title="LIMO: Less is More for Reasoning"></a>LIMO: Less is More for Reasoning</h2><p><strong>Authors:Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu</strong></p>
<p>We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (&gt;100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based modelsâ€™ 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the modelâ€™s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as â€œcognitive templatesâ€ that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/LIMO">https://github.com/GAIR-NLP/LIMO</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°å‘ç°ï¼Œè¿™é¡¹å‘ç°å¯¹æˆ‘ä»¬ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æ¨ç†å¦‚ä½•äº§ç”Ÿæå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚è™½ç„¶ä¼ ç»Ÿæ™ºæ…§è¡¨æ˜ï¼Œå¤æ‚çš„æ¨ç†ä»»åŠ¡éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼ˆ&gt;100,000ä¸ªæ ·æœ¬ï¼‰ï¼Œä½†æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨å‡ºäººæ„æ–™çš„å°‘é‡æ ·æœ¬å¯ä»¥æœ‰æ•ˆåœ°æ¿€å‘å¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬æå‡ºçš„LIMOæ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚ä»…ä½¿ç”¨817ä¸ªç²¾é€‰çš„è®­ç»ƒæ ·æœ¬ï¼ŒLIMOåœ¨AIMEä¸Šè¾¾åˆ°äº†57.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨MATHä¸Šè¾¾åˆ°äº†94.8%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºä¹‹å‰çš„åŸºäºSFTçš„æ¨¡å‹åˆ†åˆ«æé«˜äº†6.5%å’Œ59.2%ï¼ŒåŒæ—¶ä»…ä½¿ç”¨äº†ä¹‹å‰æ–¹æ³•æ‰€éœ€è®­ç»ƒæ•°æ®çš„1%ã€‚LIMOå±•ç°å‡ºå“è¶Šçš„éåˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨10ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†40.5%çš„ç»å¯¹æ”¹è¿›ï¼Œè¶…è¶Šäº†ä½¿ç”¨100å€æ•°æ®çš„æ¨¡å‹ï¼Œè¿™æŒ‘æˆ˜äº†SFTå¯¼è‡´è®°å¿†è€Œéæ³›åŒ–çš„è§‚å¿µã€‚åŸºäºè¿™äº›ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†â€œå°‘å³æ˜¯å¤šâ€çš„æ¨ç†å‡è®¾ï¼ˆLIMOå‡è®¾ï¼‰ï¼šåœ¨å·²å…¨é¢ç¼–ç é¢†åŸŸçŸ¥è¯†çš„åŸºçŸ³æ¨¡å‹ä¸­ï¼Œé€šè¿‡å°‘é‡ä½†ç²¾å¿ƒç¼–æ’çš„è®¤çŸ¥è¿‡ç¨‹æ¼”ç¤ºï¼Œå¯ä»¥äº§ç”Ÿå¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥å‡è®¾è®¤ä¸ºï¼Œå¤æ‚æ¨ç†çš„æ¿€å‘é˜ˆå€¼ç”±ä¸¤ä¸ªå…³é”®å› ç´ å†³å®šï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒæœŸé—´æ¨¡å‹ä¸­ç¼–ç çš„çŸ¥è¯†åŸºç¡€çš„å®Œæ•´æ€§ï¼Œï¼ˆ2ï¼‰ä½œä¸ºâ€œè®¤çŸ¥æ¨¡æ¿â€çš„åæœŸè®­ç»ƒæ ·æœ¬çš„æœ‰æ•ˆæ€§ï¼Œè¿™äº›æ¨¡æ¿å‘æ¨¡å‹å±•ç¤ºå¦‚ä½•åˆ©ç”¨å…¶çŸ¥è¯†åº“æ¥è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ä¸ºäº†ä¿ƒè¿›æ•°æ®é«˜æ•ˆæ¨ç†çš„å†ç°æ€§å’Œæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/LIMO%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86LIMO%E4%BD%9C%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%85%A8%E9%9D%A2%E7%9A%84%E5%BC%80%E6%BA%90%E5%A5%97%E4%BB%B6%E3%80%82">https://github.com/GAIR-NLP/LIMOä¸Šå‘å¸ƒäº†LIMOä½œä¸ºä¸€ä¸ªå…¨é¢çš„å¼€æºå¥—ä»¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03387v1">PDF</a> 17 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å‘ç°æŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æ¨ç†èƒ½åŠ›å¦‚ä½•å‡ºç°çš„ç†è§£ã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºå¤æ‚çš„æ¨ç†ä»»åŠ¡éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼ˆ&gt; 10ä¸‡ä¸ªä¾‹å­ï¼‰ï¼Œä½†æœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨å°‘æ•°æ„å¤–æƒŠäººçš„ä¾‹å­å¯ä»¥æœ‰æ•ˆæ¿€å‘å¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæ‰€æå‡ºçš„LIMOæ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å±•ç°å‡ºå‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚ä»…ä½¿ç”¨817ä¸ªç²¾é€‰çš„è®­ç»ƒæ ·æœ¬ï¼ŒLIMOåœ¨AIMEä¸Šè¾¾åˆ°57.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨MATHä¸Šè¾¾åˆ°94.8%ï¼Œç›¸è¾ƒäºä¹‹å‰çš„åŸºäºSFTçš„æ¨¡å‹åˆ†åˆ«æé«˜äº†6.5%å’Œ35.6%ã€‚LIMOå±•ç°å‡ºå“è¶Šçš„éåˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨10ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†40.5%çš„ç»å¯¹æ”¹è¿›ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨100å€æ•°æ®çš„æ¨¡å‹ï¼Œè´¨ç–‘äº†SFTå¯èƒ½å¯¼è‡´è®°å¿†è€Œéæ³›åŒ–çš„è§‚ç‚¹ã€‚ç ”ç©¶ç»“æœå¼•å‡ºäº†ä¸€ç§æ–°çš„å‡è®¾â€”â€”å°‘å³æ˜¯å¤šæ¨ç†å‡è®¾ï¼ˆLIMOå‡è®¾ï¼‰ï¼šåœ¨é¢„è®­ç»ƒé˜¶æ®µå…¨é¢ç¼–ç é¢†åŸŸçŸ¥è¯†çš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æœ€å°‘çš„ã€ç²¾å¿ƒç»„ç»‡åçš„è®¤çŸ¥è¿‡ç¨‹æ¼”ç¤ºæ¥æ¿€å‘å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚å‡è®¾çš„å…³é”®åœ¨äºé¢„è®­ç»ƒæœŸé—´æ¨¡å‹çŸ¥è¯†åŸºç¡€çš„å®Œæ•´æ€§ä»¥åŠåè®­ç»ƒç¤ºä¾‹ä½œä¸ºâ€œè®¤çŸ¥æ¨¡æ¿â€çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ä¿ƒè¿›æ•°æ®é«˜æ•ˆæ¨ç†çš„å†ç°æ€§å’Œæœªæ¥ç ”ç©¶ï¼ŒLIMOä½œä¸ºä¸€ä¸ªå…¨é¢çš„å¼€æºå¥—ä»¶å·²ç»å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡ä½¿ç”¨å°‘é‡ç²¾é€‰çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œæœ‰æ•ˆæ¿€å‘ã€‚</li>
<li>æ–°çš„æ¨¡å‹LIMOä»…ä½¿ç”¨å¾ˆå°‘çš„è®­ç»ƒæ ·æœ¬å°±èƒ½åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>LIMOå±•ç°å‡ºå“è¶Šçš„éåˆ†å¸ƒæ³›åŒ–èƒ½åŠ›ï¼Œè´¨ç–‘äº†SFTå¯èƒ½å¯¼è‡´è®°å¿†è€Œéæ³›åŒ–çš„è§‚ç‚¹ã€‚</li>
<li>LIMOå‡è®¾æå‡ºï¼Œå¤æ‚æ¨ç†èƒ½åŠ›çš„å‡ºç°å–å†³äºæ¨¡å‹é¢„è®­ç»ƒçŸ¥è¯†åŸºç¡€çš„å®Œæ•´æ€§å’Œåè®­ç»ƒç¤ºä¾‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¼ ç»Ÿè§‚ç‚¹å…³äºå¤§é‡è®­ç»ƒæ•°æ®çš„éœ€è¦å—åˆ°æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å°‘é‡ç²¾å‡†çš„è®­ç»ƒç¤ºä¾‹æ¼”ç¤ºå¯ä»¥å¼•å¯¼æ¨¡å‹è§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4f411c6477783f8950071b01a8f35ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91d4c5656e12a56c01261fd3d2728ea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd7cf74f617d72ef2026a03d7e5d5210.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Demystifying-Long-Chain-of-Thought-Reasoning-in-LLMs"><a href="#Demystifying-Long-Chain-of-Thought-Reasoning-in-LLMs" class="headerlink" title="Demystifying Long Chain-of-Thought Reasoning in LLMs"></a>Demystifying Long Chain-of-Thought Reasoning in LLMs</h2><p><strong>Authors:Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue</strong></p>
<p>Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/eddycmu/demystify-long-cot">https://github.com/eddycmu/demystify-long-cot</a>. </p>
<blockquote>
<p>æ‰©å±•æ¨ç†è®¡ç®—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé•¿æœŸçš„æ€ç»´é“¾ï¼ˆCoTï¼‰ä½¿å›æº¯å’Œé”™è¯¯ä¿®æ­£ç­‰ç­–ç•¥å¾—ä»¥å®æ–½ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºå‘å±•è¿™äº›èƒ½åŠ›çš„é‡è¦æ–¹æ³•ï¼Œç„¶è€Œé•¿æœŸCoTå‡ºç°çš„æ¡ä»¶ä»ä¸æ¸…æ¥šï¼Œè€Œä¸”RLè®­ç»ƒéœ€è¦ä»”ç»†çš„è®¾è®¡é€‰æ‹©ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†é•¿æœŸCoTæ¨ç†çš„æœºåˆ¶ï¼Œç¡®å®šäº†ä½¿æ¨¡å‹ç”Ÿæˆé•¿æœŸCoTè½¨è¿¹çš„å…³é”®å› ç´ ã€‚é€šè¿‡å¹¿æ³›çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLå®éªŒï¼Œæˆ‘ä»¬æå‡ºäº†å››ä¸ªä¸»è¦å‘ç°ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰è™½ç„¶SFTå¹¶ä¸æ˜¯ç»å¯¹å¿…è¦çš„ï¼Œä½†å®ƒç®€åŒ–äº†è®­ç»ƒå¹¶æé«˜äº†æ•ˆç‡ï¼›<br>ï¼ˆ2ï¼‰æ¨ç†èƒ½åŠ›å¾€å¾€éšç€è®­ç»ƒè®¡ç®—çš„å¢åŠ è€Œæ¶Œç°ï¼Œä½†å…¶å‘å±•å¹¶éå¿…ç„¶ï¼Œè¿™ä½¿å¾—å¥–åŠ±å¡‘é€ å¯¹äºç¨³å®šCoTé•¿åº¦å¢é•¿è‡³å…³é‡è¦ï¼›<br>ï¼ˆ3ï¼‰æ‰©å¤§å¯éªŒè¯çš„å¥–åŠ±ä¿¡å·å¯¹RLè‡³å…³é‡è¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œåˆ©ç”¨å¸¦æœ‰è¿‡æ»¤æœºåˆ¶çš„å˜ˆæ‚çš„ç½‘é¡µæå–è§£å†³æ–¹æ¡ˆå…·æœ‰å¾ˆå¼ºçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒSTEMæ¨ç†ä¹‹ç±»çš„ç¦»åˆ†å¸ƒä»»åŠ¡ï¼›<br>ï¼ˆ4ï¼‰åŸºç¡€æ¨¡å‹æœ¬èº«å°±å…·æœ‰é”™è¯¯ä¿®æ­£ç­‰æ ¸å¿ƒèƒ½åŠ›ï¼Œä½†è¦é€šè¿‡RLæœ‰æ•ˆåœ°æ¿€åŠ±è¿™äº›æŠ€èƒ½æ¥å®Œæˆå¤æ‚ä»»åŠ¡éœ€è¦å¤§é‡çš„è®¡ç®—ï¼Œè€Œä¸”è¡¡é‡å®ƒä»¬çš„å‡ºç°éœ€è¦ä¸€ç§ç»†è‡´çš„æ–¹æ³•ã€‚è¿™äº›è§è§£ä¸ºä¼˜åŒ–è®­ç»ƒç­–ç•¥ä»¥å¢å¼ºLLMä¸­çš„é•¿æœŸCoTæ¨ç†æä¾›äº†å®é™…æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/eddycmu/demystify-long-cot">https://github.com/eddycmu/demystify-long-cot</a>ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03373v1">PDF</a> Preprint, under review</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ¨ç†è®¡ç®—æ‰©å±•æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œå…¶ä¸­é•¿æ€è€ƒé“¾ï¼ˆCoTsï¼‰ç­–ç•¥å¦‚å›æº¯å’Œé”™è¯¯æ ¡æ­£å°¤ä¸ºé‡è¦ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯å¼€å‘è¿™äº›èƒ½åŠ›çš„é‡è¦æ–¹æ³•ï¼Œä½†é•¿æ€è€ƒé“¾å‡ºç°çš„æ¡ä»¶å°šä¸æ¸…æ¥šï¼Œä¸”RLè®­ç»ƒéœ€è¦è°¨æ…è®¾è®¡é€‰æ‹©ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿçš„è°ƒæŸ¥é•¿æ€è€ƒé“¾æ¨ç†çš„æœºåˆ¶ï¼Œç¡®å®šäº†ä¿ƒè¿›æ¨¡å‹ç”Ÿæˆé•¿æ€è€ƒé“¾è½¨è¿¹çš„å…³é”®å› ç´ ã€‚é€šè¿‡å¹¿æ³›çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLå®éªŒï¼Œæˆ‘ä»¬æå‡ºå››ç‚¹ä¸»è¦å‘ç°ï¼šSFTå¹¶éå¿…éœ€ï¼Œä½†ç®€åŒ–è®­ç»ƒå¹¶æé«˜æ•ˆç‡ï¼›æ¨ç†èƒ½åŠ›å¾€å¾€éšç€è®­ç»ƒè®¡ç®—çš„å¢åŠ è€Œå‘å±•ï¼Œä½†å…¶å‘å±•å¹¶éå¿…ç„¶ï¼Œå¥–åŠ±å¡‘å½¢å¯¹ç¨³å®šæ€è€ƒé•¿åº¦å¢é•¿è‡³å…³é‡è¦ï¼›éªŒè¯å¥–åŠ±ä¿¡å·çš„æ‰©å±•å¯¹RLè‡³å…³é‡è¦ï¼›åŸºäºè¿‡æ»¤æœºåˆ¶çš„å˜ˆæ‚çš„ç½‘é¡µæå–è§£å†³æ–¹æ¡ˆå¯¹äºå‡ºåˆ†å¸ƒä»»åŠ¡ï¼ˆå¦‚STEMæ¨ç†ï¼‰å…·æœ‰å·¨å¤§æ½œåŠ›ï¼›åŸºç¡€æ¨¡å‹æœ¬èº«å°±å…·å¤‡çº é”™ç­‰æ ¸å¿ƒèƒ½åŠ›ï¼Œä½†é€šè¿‡RLæ¿€åŠ±è¿™äº›æŠ€èƒ½è¿›è¡Œå¤æ‚ä»»åŠ¡éœ€è¦å¤§é‡çš„è®¡ç®—ï¼Œå¹¶ä¸”è¡¡é‡å®ƒä»¬çš„å‡ºç°éœ€è¦ç»†è‡´çš„æ–¹æ³•ã€‚è¿™äº›è§è§£ä¸ºä¼˜åŒ–è®­ç»ƒç­–ç•¥ä»¥æé«˜LLMçš„é•¿æ€è€ƒé“¾æ¨ç†èƒ½åŠ›æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è™½ç„¶ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†å®ƒå¯ä»¥ç®€åŒ–è®­ç»ƒå¹¶æé«˜æ•ˆç‡ã€‚</li>
<li>æ¨ç†èƒ½åŠ›ä¼šéšç€è®­ç»ƒè®¡ç®—çš„å¢åŠ è€Œå‘å±•ï¼Œä½†è¿™ä¸€è¿‡ç¨‹å¹¶ä¸ç¨³å®šï¼Œå¥–åŠ±å¡‘å½¢æ˜¯å…³é”®ã€‚</li>
<li>æ‰©å±•éªŒè¯å¥–åŠ±ä¿¡å·å¯¹å¼ºåŒ–å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>åˆ©ç”¨å¸¦æœ‰è¿‡æ»¤æœºåˆ¶çš„å˜ˆæ‚ç½‘é¡µæå–è§£å†³æ–¹æ¡ˆå¯¹äºå¤„ç†å‡ºåˆ†å¸ƒä»»åŠ¡å…·æœ‰æ½œåŠ›ã€‚</li>
<li>åŸºç¡€æ¨¡å‹å·²ç»å…·å¤‡ä¸€äº›æ ¸å¿ƒèƒ½åŠ›å¦‚é”™è¯¯æ ¡æ­£ï¼Œä½†è¦é€šè¿‡å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆæ¿€åŠ±è¿™äº›æŠ€èƒ½éœ€è¦å¤§é‡è®¡ç®—ã€‚</li>
<li>æµ‹é‡è¿™äº›èƒ½åŠ›çš„å‡ºç°éœ€è¦ç»†è‡´çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af10f0a7b7cc78df96784fd158c86ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6360c7a588ae7009d3d3e7922862ef54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e6895250d51fe4181ef19af287058c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-119ccae63ac71ad8b35ef892b14169a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e44880da0561de8f301fe12e79d1323e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7474b4beb9b100aeaa7cc3bbcf22453.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Simplifying-Formal-Proof-Generating-Models-with-ChatGPT-and-Basic-Searching-Techniques"><a href="#Simplifying-Formal-Proof-Generating-Models-with-ChatGPT-and-Basic-Searching-Techniques" class="headerlink" title="Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques"></a>Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques</h2><p><strong>Authors:Sangjun Han, Taeil Hur, Youngmi Hur, Kathy Sangkyung Lee, Myungyoon Lee, Hyojae Lim</strong></p>
<p>The challenge of formal proof generation has a rich history, but with modern techniques, we may finally be at the stage of making actual progress in real-life mathematical problems. This paper explores the integration of ChatGPT and basic searching techniques to simplify generating formal proofs, with a particular focus on the miniF2F dataset. We demonstrate how combining a large language model like ChatGPT with a formal language such as Lean, which has the added advantage of being verifiable, enhances the efficiency and accessibility of formal proof generation. Despite its simplicity, our best-performing Lean-based model surpasses all known benchmarks with a 31.15% pass rate. We extend our experiments to include other datasets and employ alternative language models, showcasing our modelsâ€™ comparable performance in diverse settings and allowing for a more nuanced analysis of our results. Our findings offer insights into AI-assisted formal proof generation, suggesting a promising direction for future research in formal mathematical proof. </p>
<blockquote>
<p>å½¢å¼åŒ–è¯æ˜ç”Ÿæˆè¿™ä¸€æŒ‘æˆ˜æœ‰ç€æ‚ ä¹…çš„å†å²èƒŒæ™¯ï¼Œä½†éšç€ç°ä»£æŠ€æœ¯çš„å‘å±•ï¼Œæˆ‘ä»¬å¯èƒ½ç»ˆäºå¤„äºåœ¨å®é™…æ•°å­¦é—®é¢˜ä¸Šå–å¾—çœŸæ­£è¿›å±•çš„é˜¶æ®µã€‚æœ¬æ–‡æ¢è®¨äº†å°†ChatGPTå’ŒåŸºæœ¬æœç´¢æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥ç®€åŒ–å½¢å¼åŒ–è¯æ˜ç”Ÿæˆçš„è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯ä¾§é‡äºminiF2Fæ•°æ®é›†çš„åº”ç”¨ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸Leanç­‰å¯éªŒè¯çš„å½¢å¼åŒ–è¯­è¨€ç›¸ç»“åˆï¼Œä»¥æé«˜å½¢å¼åŒ–è¯æ˜çš„æ•ˆç‡å’Œå¯åŠæ€§ã€‚å°½ç®¡æˆ‘ä»¬çš„åŸºäºLeançš„æœ€ä½³æ¨¡å‹éå¸¸ç®€å•ï¼Œä½†å…¶é€šè¿‡ç‡è¾¾åˆ°äº†31.15%ï¼Œè¶…è¿‡äº†æ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±•å®éªŒï¼ŒåŒ…æ‹¬ä½¿ç”¨å…¶ä»–æ•°æ®é›†å’Œæ›¿ä»£è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸­çš„å¯æ¯”æ€§èƒ½ï¼Œå¹¶å…è®¸å¯¹ç»“æœè¿›è¡Œæ›´å¾®å¦™çš„åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæä¾›äº†è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å½¢å¼åŒ–æ•°å­¦è¯æ˜çš„ç ”ç©¶æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03321v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç°ä»£æŠ€æœ¯å¯èƒ½ä½¿æˆ‘ä»¬åœ¨å®é™…çš„æ•°å­¦é—®é¢˜ä¸Šå–å¾—è¿›å±•ã€‚æœ¬æ–‡æ¢è®¨äº†å°†ChatGPTå’ŒåŸºæœ¬æœç´¢æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥ç®€åŒ–ç”Ÿæˆå½¢å¼è¯æ˜çš„è¿‡ç¨‹ï¼Œç‰¹åˆ«å…³æ³¨miniF2Fæ•°æ®é›†ã€‚é€šè¿‡ä¸å½¢å¼è¯­è¨€ï¼ˆå¦‚Leanï¼‰ç›¸ç»“åˆï¼Œè¿™ç§æŠ€æœ¯å¯ä»¥æé«˜å½¢å¼è¯æ˜ç”Ÿæˆçš„æ•ˆç‡å’Œå¯åŠæ€§ã€‚æœ€ä½³æ€§èƒ½çš„Leanæ¨¡å‹è¶…è¶Šäº†æ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ï¼Œè¾¾åˆ°31.15%çš„é€šè¿‡ç‡ã€‚å®éªŒè¿˜æ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†å¹¶ä½¿ç”¨å…¶ä»–è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨å„ç§ç¯å¢ƒä¸­çš„æ€§èƒ½å¯æ¯”æ€§ï¼Œå¹¶ä¸ºç»“æœæä¾›äº†æ›´å¾®å¦™çš„åˆ†æã€‚è¿™ä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©å½¢å¼è¯æ˜ç”Ÿæˆæä¾›äº†è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å½¢å¼æ•°å­¦è¯æ˜çš„ç ”ç©¶æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æŠ€æœ¯æœ‰åŠ©äºåœ¨å®é™…æ•°å­¦é—®é¢˜ä¸Šçš„å½¢å¼è¯æ˜ç”Ÿæˆå–å¾—è¿›å±•ã€‚</li>
<li>ChatGPTä¸åŸºæœ¬æœç´¢æŠ€æœ¯çš„ç»“åˆç®€åŒ–äº†å½¢å¼è¯æ˜ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>ä¸å½¢å¼è¯­è¨€Leançš„ç»“åˆæé«˜äº†å½¢å¼è¯æ˜ç”Ÿæˆçš„æ•ˆç‡å’Œå¯åŠæ€§ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„Leanæ¨¡å‹åœ¨miniF2Fæ•°æ®é›†ä¸Šçš„é€šè¿‡ç‡è¾¾åˆ°31.15%ï¼Œè¶…è¶Šç°æœ‰åŸºå‡†ã€‚</li>
<li>å®éªŒæ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†å’Œè¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†æ¨¡å‹çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
<li>ä¸åŒç¯å¢ƒä¸‹çš„å®éªŒç»“æœä¸ºåˆ†ææä¾›äº†æ›´å¾®å¦™çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a97b025fe8cbd1632352adceaa0875c2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Intent-Representation-Learning-with-Large-Language-Model-for-Recommendation"><a href="#Intent-Representation-Learning-with-Large-Language-Model-for-Recommendation" class="headerlink" title="Intent Representation Learning with Large Language Model for   Recommendation"></a>Intent Representation Learning with Large Language Model for   Recommendation</h2><p><strong>Authors:Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang</strong></p>
<p>Intent-based recommender systems have garnered significant attention for uncovering latent fine-grained preferences. Intents, as underlying factors of interactions, are crucial for improving recommendation interpretability. Most methods define intents as learnable parameters updated alongside interactions. However, existing frameworks often overlook textual information (e.g., user reviews, item descriptions), which is crucial for alleviating the sparsity of interaction intents. Exploring these multimodal intents, especially the inherent differences in representation spaces, poses two key challenges: i) How to align multimodal intents and effectively mitigate noise issues; ii) How to extract and match latent key intents across modalities. To tackle these challenges, we propose a model-agnostic framework, Intent Representation Learning with Large Language Model (IRLLRec), which leverages large language models (LLMs) to construct multimodal intents and enhance recommendations. Specifically, IRLLRec employs a dual-tower architecture to learn multimodal intent representations. Next, we propose pairwise and translation alignment to eliminate inter-modal differences and enhance robustness against noisy input features. Finally, to better match textual and interaction-based intents, we employ momentum distillation to perform teacher-student learning on fused intent representations. Empirical evaluations on three datasets show that our IRLLRec framework outperforms baselines. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/wangyu0627/IRLLRec">https://github.com/wangyu0627/IRLLRec</a>. </p>
<blockquote>
<p>åŸºäºæ„å›¾çš„æ¨èç³»ç»Ÿå·²ç»å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæ­ç¤ºæ½œåœ¨çš„ç²¾ç»†åå¥½ã€‚ä½œä¸ºäº¤äº’çš„åŸºç¡€å› ç´ ï¼Œæ„å›¾å¯¹äºæé«˜æ¨èçš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°æ–¹æ³•å°†æ„å›¾å®šä¹‰ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå¹¶éšç€äº¤äº’è¿›è¡Œæ›´æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰æ¡†æ¶å¾€å¾€å¿½è§†äº†æ–‡æœ¬ä¿¡æ¯ï¼ˆå¦‚ç”¨æˆ·è¯„è®ºã€å•†å“æè¿°ï¼‰ï¼Œè¿™å¯¹äºç¼“è§£äº¤äº’æ„å›¾çš„ç¨€ç–æ€§è‡³å…³é‡è¦ã€‚æ¢ç´¢è¿™äº›å¤šæ¨¡å¼æ„å›¾ï¼Œå°¤å…¶æ˜¯è¡¨ç¤ºç©ºé—´ä¸­çš„å†…åœ¨å·®å¼‚ï¼Œæå‡ºäº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯å¦‚ä½•å¯¹é½å¤šæ¨¡å¼æ„å›¾å¹¶æœ‰æ•ˆåœ°ç¼“è§£å™ªå£°é—®é¢˜ï¼›äºŒæ˜¯å¦‚ä½•è·¨æ¨¡æ€æå–å’ŒåŒ¹é…æ½œåœ¨çš„å…³é”®æ„å›¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¨¡å‹æ— å…³æ¡†æ¶â€”â€”åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ„å›¾è¡¨ç¤ºå­¦ä¹ ï¼ˆIRLLRecï¼‰ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºå¤šæ¨¡å¼æ„å›¾å¹¶æé«˜æ¨èæ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼ŒIRLLRecé‡‡ç”¨åŒå¡”æ¶æ„æ¥å­¦ä¹ å¤šæ¨¡å¼æ„å›¾è¡¨ç¤ºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºé…å¯¹å’Œç¿»è¯‘å¯¹é½æ¥æ¶ˆé™¤æ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶æé«˜å¯¹æŠ—å™ªå£°è¾“å…¥ç‰¹å¾çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œä¸ºäº†æ›´å¥½åœ°åŒ¹é…æ–‡æœ¬å’ŒåŸºäºäº¤äº’çš„æ„å›¾ï¼Œæˆ‘ä»¬åœ¨èåˆçš„æ„å›¾è¡¨ç¤ºä¸Šé‡‡ç”¨åŠ¨é‡è’¸é¦æ³•è¿›è¡Œå¸ˆå¾’å­¦ä¹ ã€‚åœ¨ä¸‰å¥—æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„IRLLRecæ¡†æ¶ä¼˜äºåŸºå‡†çº¿ã€‚ç›¸å…³å®ç°å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/wangyu0627/IRLLRec">https://github.com/wangyu0627/IRLLRec</a> äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03307v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong>ï¼š<br>æ„å›¾æ„ŸçŸ¥æ¨èç³»ç»Ÿé€šè¿‡æŒ–æ˜æ½œåœ¨ç²¾ç»†åå¥½å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æ„å›¾ä½œä¸ºäº¤äº’çš„åŸºæœ¬å› ç´ ï¼Œå¯¹äºæé«˜æ¨èçš„å¯è§£é‡Šæ€§è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°æ–¹æ³•å°†æ„å›¾å®šä¹‰ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œä¸äº¤äº’ä¸€èµ·æ›´æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰æ¡†æ¶å¾€å¾€å¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯ï¼ˆå¦‚ç”¨æˆ·è¯„è®ºã€å•†å“æè¿°ï¼‰ï¼Œè¿™å¯¹äºç¼“è§£äº¤äº’æ„å›¾çš„ç¨€ç–æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹å¦‚ä½•å¯¹é½å¤šæ¨¡å¼æ„å›¾å¹¶æœ‰æ•ˆç¼“è§£å™ªå£°é—®é¢˜ä»¥åŠå¦‚ä½•åœ¨ä¸åŒæ¨¡å¼ä¸‹æå–å’ŒåŒ¹é…æ½œåœ¨å…³é”®æ„å›¾è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œåä¸ºâ€œåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ„å›¾è¡¨ç¤ºå­¦ä¹ â€ï¼ˆIRLLRecï¼‰ã€‚IRLLRecåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºå¤šæ¨¡å¼æ„å›¾å¹¶å¢å¼ºæ¨èã€‚å…·ä½“è€Œè¨€ï¼ŒIRLLRecé‡‡ç”¨åŒå¡”æ¶æ„æ¥å­¦ä¹ å¤šæ¨¡å¼æ„å›¾è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºé…å¯¹å’Œç¿»è¯‘å¯¹é½æ¥æ¶ˆé™¤è·¨æ¨¡å¼çš„å·®å¼‚å¹¶æé«˜å¯¹ä¸åŒè¾“å…¥ç‰¹å¾çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œä¸ºäº†æ›´å¥½åœ°åŒ¹é…æ–‡æœ¬å’ŒåŸºäºäº¤äº’çš„æ„å›¾ï¼Œæˆ‘ä»¬é‡‡ç”¨åŠ¨é‡è’¸é¦æ³•å¯¹èåˆçš„æ„å›¾è¡¨ç¤ºè¿›è¡Œå¸ˆå¾’å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„IRLLRecæ¡†æ¶ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å…·ä½“å®ç°è¯·å‚è€ƒ<a target="_blank" rel="noopener" href="https://github.com/wangyu0627/IRLLRec%E3%80%82">https://github.com/wangyu0627/IRLLRecã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ„å›¾æ„ŸçŸ¥æ¨èç³»ç»ŸæŒ–æ˜ç”¨æˆ·æ½œåœ¨ç²¾ç»†åå¥½ï¼Œä»¥æé«˜æ¨èçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç°æœ‰æ¨èç³»ç»Ÿæ–¹æ³•å¤šå¿½ç•¥æ–‡æœ¬ä¿¡æ¯ï¼Œå¦‚ç”¨æˆ·è¯„è®ºå’Œå•†å“æè¿°ï¼Œè¿™å½±å“äº†äº¤äº’æ„å›¾çš„ä¸°å¯Œæ€§ã€‚</li>
<li>å¤„ç†å¤šæ¨¡å¼æ„å›¾é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¯¹é½ä¸åŒæ¨¡å¼çš„æ„å›¾å¹¶æ¶ˆé™¤å™ªå£°ï¼Œä»¥åŠåœ¨ä¸åŒæ¨¡å¼ä¸‹æå–å’ŒåŒ¹é…å…³é”®æ½œåœ¨æ„å›¾ã€‚</li>
<li>IRLLRecæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºå¤šæ¨¡å¼æ„å›¾è¡¨ç¤ºï¼Œé‡‡ç”¨åŒå¡”æ¶æ„æ¥å­¦ä¹ è¿™äº›è¡¨ç¤ºã€‚</li>
<li>IRLLRecé€šè¿‡é…å¯¹å’Œç¿»è¯‘å¯¹é½æ¥å¢å¼ºå¯¹ä¸åŒè¾“å…¥ç‰¹å¾çš„ç¨³å¥æ€§ï¼Œå¹¶æ¶ˆé™¤è·¨æ¨¡å¼çš„å·®å¼‚ã€‚</li>
<li>åŠ¨é‡è’¸é¦æ³•ç”¨äºåŒ¹é…æ–‡æœ¬å’ŒåŸºäºäº¤äº’çš„æ„å›¾ï¼Œå¯¹èåˆçš„æ„å›¾è¡¨ç¤ºè¿›è¡Œå¸ˆå¾’å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9923901b4e8c4ff40b8fd25cbf06f500.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-998984628d008124c7a6985733c7cfed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-402dbcf4b6113d182ce6b93000f76eed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9790131be3fb3a6e0e12315e142c0efd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77cc34d61c83c761d668a37746b5c7b0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Harmony-in-Divergence-Towards-Fast-Accurate-and-Memory-efficient-Zeroth-order-LLM-Fine-tuning"><a href="#Harmony-in-Divergence-Towards-Fast-Accurate-and-Memory-efficient-Zeroth-order-LLM-Fine-tuning" class="headerlink" title="Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient   Zeroth-order LLM Fine-tuning"></a>Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient   Zeroth-order LLM Fine-tuning</h2><p><strong>Authors:Qitao Tan, Jun Liu, Zheng Zhan, Caiwei Ding, Yanzhi Wang, Jin Lu, Geng Yuan</strong></p>
<p>Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ ‡å‡†çš„ä¸€é˜¶ï¼ˆFOï¼‰å¾®è°ƒéœ€è¦å¤§é‡çš„å†…å­˜ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ€è¿‘ï¼Œé›¶é˜¶ï¼ˆZOï¼‰ä¼˜åŒ–ä½œä¸ºä¸€ç§å…·æœ‰å‰æ™¯çš„å†…å­˜é«˜æ•ˆè®­ç»ƒèŒƒå¼è„±é¢–è€Œå‡ºï¼Œå®ƒé¿å…äº†åå‘ä¼ é€’å¹¶åªä¾èµ–æ­£å‘ä¼ é€’è¿›è¡Œæ¢¯åº¦ä¼°è®¡ï¼Œå› æ­¤å¯¹äºèµ„æºå—é™çš„åœºæ™¯å…·æœ‰å¾ˆå¼ºçš„å¸å¼•åŠ›ã€‚ç„¶è€Œï¼ŒZOæ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¿œè¿œè½åäºFOæ–¹æ³•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹é€å±‚å‘æ•£åˆ†æï¼Œæ­ç¤ºäº†FOå’ŒZOä¼˜åŒ–çš„ä¸åŒæ›´æ–°æ¨¡å¼ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¨¡ä»¿FOæ–¹æ³•çš„å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†<strong>DiZO</strong>ï¼ˆå‘æ•£é©±åŠ¨é›¶é˜¶ä¼˜åŒ–ï¼‰ã€‚DiZOé€šè¿‡ç»“åˆæŠ•å½±åˆ°ZOæ›´æ–°æ¥è¿›è¡Œå‘æ•£é©±åŠ¨çš„å±‚é€‚åº”ï¼Œç”Ÿæˆç²¾ç¡®ç¼©æ”¾è‡³æ¯å±‚ä¸ªåˆ«ä¼˜åŒ–éœ€æ±‚çš„æ›´æ–°å¹…åº¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDiZOåœ¨æ— éœ€ç‰ºç‰²ååé‡çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„æ”¶æ•›è¿­ä»£æ¬¡æ•°ï¼Œåœ¨å„ç§æ•°æ®é›†ä¸Šå°†è®­ç»ƒGPUå°æ—¶æ•°å‡å°‘äº†é«˜è¾¾48%ã€‚æ­¤å¤–ï¼ŒDiZOåœ¨å¾®è°ƒRoBERTaå¤§å‹æ¨¡å‹ã€OPTç³»åˆ—å’ŒLlamaç³»åˆ—æ—¶ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºä»£è¡¨æ€§çš„ZOåŸºçº¿ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†å†…å­˜å¯†é›†å‹çš„FOå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03304v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ ‡å‡†ä¸€é˜¶å¾®è°ƒéœ€è¦å¤§é‡å†…å­˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚é›¶é˜¶ä¼˜åŒ–ä½œä¸ºä¸€ç§å†…å­˜æ•ˆç‡é«˜çš„è®­ç»ƒèŒƒå¼å¤‡å—å…³æ³¨ï¼Œå®ƒé¿å…äº†åå‘ä¼ æ’­å¹¶åªä¾èµ–æ­£å‘ä¼ æ’­è¿›è¡Œæ¢¯åº¦ä¼°è®¡ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„åœºæ™¯ã€‚ç„¶è€Œï¼Œé›¶é˜¶æ–¹æ³•åœ¨æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢è¿œè¿œè½åäºä¸€é˜¶æ–¹æ³•ã€‚ä¸ºäº†ç¼©å°å·®è·ï¼Œæœ¬æ–‡è¿›è¡Œäº†é€å±‚å‘æ•£åˆ†æï¼Œæ­ç¤ºäº†ä¸€é˜¶å’Œé›¶é˜¶ä¼˜åŒ–çš„ä¸åŒæ›´æ–°æ¨¡å¼ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†å‘æ•£é©±åŠ¨é›¶é˜¶ä¼˜åŒ–ï¼ˆDiZOï¼‰ã€‚DiZOé€šè¿‡ç»“åˆæŠ•å½±åˆ°é›¶é˜¶æ›´æ–°æ¥è¿›è¡Œå‘æ•£é©±åŠ¨å±‚é€‚é…ï¼Œç”Ÿæˆç²¾ç¡®ç¼©æ”¾ã€å¹…åº¦å„å¼‚çš„æ›´æ–°ï¼Œä»¥æ»¡è¶³å„å±‚çš„ä¸ªåˆ«ä¼˜åŒ–éœ€æ±‚ã€‚ç»“æœè¯æ˜ï¼ŒDiZOåœ¨ä¸å½±å“ååé‡çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘äº†æ”¶æ•›æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ï¼Œåœ¨å¤šç§æ•°æ®é›†ä¸Šå°†GPUè®­ç»ƒæ—¶é—´å‡å°‘äº†é«˜è¾¾48%ã€‚æ­¤å¤–ï¼ŒDiZOåœ¨å¾®è°ƒRoBERTa-largeã€OPTç³»åˆ—å’ŒLlamaç³»åˆ—æ—¶ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºä»£è¡¨æ€§çš„é›¶é˜¶åŸºçº¿ï¼Œç”šè‡³æœ‰äº›æƒ…å†µä¸‹è¶…è¿‡äº†å†…å­˜å¯†é›†çš„ä¸€é˜¶å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ ‡å‡†ä¸€é˜¶å¾®è°ƒéœ€è¦å¤§é‡å†…å­˜ã€‚</li>
<li>é›¶é˜¶ä¼˜åŒ–æ˜¯ä¸€ç§å†…å­˜æ•ˆç‡é«˜çš„è®­ç»ƒèŒƒå¼ï¼Œä½†å…¶åœ¨æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>é€šè¿‡é€å±‚å‘æ•£åˆ†æï¼Œå‘ç°ä¸€é˜¶å’Œé›¶é˜¶ä¼˜åŒ–çš„ä¸åŒæ›´æ–°æ¨¡å¼ã€‚</li>
<li>æå‡ºäº†å‘æ•£é©±åŠ¨é›¶é˜¶ä¼˜åŒ–ï¼ˆDiZOï¼‰ï¼Œé€šè¿‡ç»“åˆæŠ•å½±åˆ°é›¶é˜¶æ›´æ–°æ¥è¿›è¡Œå±‚é€‚é…ã€‚</li>
<li>DiZOæ˜¾è‘—å‡å°‘äº†æ”¶æ•›æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>DiZOåœ¨å¤šç§æ•°æ®é›†ä¸Šå°†GPUè®­ç»ƒæ—¶é—´å‡å°‘äº†é«˜è¾¾48%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d93c6992016fd8f27244041c9415ab74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-281e2e4dfc509b316c15de0f406054b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54701d8ad0a77c6d11a7488d0e261e45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a8d73dc5f4bb92828ce643f749a5daf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b69c1701869ec7d09338a196c35435c1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Token-Assorted-Mixing-Latent-and-Text-Tokens-for-Improved-Language-Model-Reasoning"><a href="#Token-Assorted-Mixing-Latent-and-Text-Tokens-for-Improved-Language-Model-Reasoning" class="headerlink" title="Token Assorted: Mixing Latent and Text Tokens for Improved Language   Model Reasoning"></a>Token Assorted: Mixing Latent and Text Tokens for Improved Language   Model Reasoning</h2><p><strong>Authors:DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, Qinqing Zheng</strong></p>
<p>Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®è®­ç»ƒæ—¶ï¼Œæ“…é•¿æ¨ç†å’Œè§„åˆ’ã€‚åœ¨è¿™ç§æ•°æ®ä¸­ï¼Œæ–‡æœ¬æ ‡è®°ä¼šæ˜ç¡®æ¦‚è¿°é€æ­¥çš„æ€ç»´æ–¹å¼ã€‚ç„¶è€Œï¼Œè¿™ä¼šå¯¼è‡´å†—é•¿çš„è¾“å…¥ï¼Œå…¶ä¸­è®¸å¤šè¯è¯­æ”¯æŒæ–‡æœ¬è¿è´¯æ€§è€Œéæ ¸å¿ƒæ¨ç†ä¿¡æ¯ï¼Œå¤„ç†è¿™äº›è¾“å…¥ä¼šæ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨ç†è¿‡ç¨‹çš„æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨VQ-VAEç”Ÿæˆçš„æ½œåœ¨ç¦»æ•£æ ‡è®°æ¥éƒ¨åˆ†æŠ½è±¡æœ€åˆçš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ¨ç†ç—•è¿¹çš„é•¿åº¦ã€‚æˆ‘ä»¬æ¢ç´¢äº†æ½œåœ¨ç—•è¿¹æŠ½è±¡åœ¨ä¸¤ç§åœºæ™¯ä¸­çš„åº”ç”¨ï¼š1ï¼‰ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ä»¥è§£å†³æ‰¾é’¥åŒ™è¿·å®«é—®é¢˜ï¼›2ï¼‰åœ¨æ­¤æ··åˆæ•°æ®ä¸Šå¾®è°ƒLLMï¼Œæ‰©å±•è¯æ±‡è¡¨ï¼ŒåŒ…æ‹¬æœªè§è¿‡çš„æ½œåœ¨æ ‡è®°ï¼Œç”¨äºé€»è¾‘å’Œæ•°å­¦æ¨ç†é—®é¢˜ã€‚ä¸ºäº†ä¿ƒè¿›æœ‰æ•ˆå­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•çš„è®­ç»ƒç¨‹åºï¼Œè¯¥ç¨‹åºéšæœºæ··åˆæ½œåœ¨æ ‡è®°å’Œæ–‡æœ¬æ ‡è®°ï¼Œè¿™å¯ä»¥å®ç°å¯¹æ–°æ½œåœ¨æ ‡è®°çš„å¿«é€Ÿé€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03275v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é“¾çŠ¶æ€ç»´ï¼ˆCoTï¼‰æ•°æ®è®­ç»ƒæ—¶æ“…é•¿æ¨ç†å’Œè§„åˆ’ï¼Œä½†è¿™ç§æ–¹å¼å¯¼è‡´è¾“å…¥æ–‡æœ¬å†—é•¿ï¼Œè®¸å¤šè¯æ±‡æ˜¯ä¸ºäº†ä¿æŒæ–‡æœ¬è¿è´¯æ€§è€Œéæ ¸å¿ƒæ¨ç†ä¿¡æ¯ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆçš„æ¨ç†è¿‡ç¨‹è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡VQ-VAEç”Ÿæˆçš„æ½œåœ¨ç¦»æ•£ä»¤ç‰Œéƒ¨åˆ†æŠ½è±¡æœ€åˆçš„æ¨ç†æ­¥éª¤ï¼Œæ˜¾è‘—å‡å°‘æ¨ç†è½¨è¿¹çš„é•¿åº¦ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ½œåœ¨è½¨è¿¹æŠ½è±¡åœ¨ä¸¤ç§åœºæ™¯ä¸­çš„åº”ç”¨ï¼šä¸€æ˜¯åœ¨è¿·å®«å¯»é’¥é—®é¢˜ä¸­ä»å¤´è®­ç»ƒæ¨¡å‹ï¼›äºŒæ˜¯åœ¨é€»è¾‘å’Œæ•°å­¦æ¨ç†é—®é¢˜ä¸Šå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œæ‰©å±•è¯æ±‡è¡¨åŒ…å«æœªè§è¿‡çš„æ½œåœ¨ä»¤ç‰Œã€‚ä¸ºæœ‰æ•ˆå­¦ä¹ ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§ç®€å•çš„è®­ç»ƒç¨‹åºï¼Œå¯éšæœºæ··åˆæ½œåœ¨ä»¤ç‰Œå’Œæ–‡æœ¬ä»¤ç‰Œï¼Œä½¿æ¨¡å‹å¿«é€Ÿé€‚åº”æ–°ä»¤ç‰Œã€‚è¯¥æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨é“¾çŠ¶æ€ç»´ï¼ˆCoTï¼‰æ•°æ®è®­ç»ƒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸­è¾“å…¥æ–‡æœ¬å†—é•¿ï¼ŒåŒ…å«å¤§é‡éæ ¸å¿ƒæ¨ç†ä¿¡æ¯çš„è¯æ±‡ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡æ½œåœ¨ç¦»æ•£ä»¤ç‰Œéƒ¨åˆ†æŠ½è±¡æœ€åˆçš„æ¨ç†æ­¥éª¤ã€‚</li>
<li>æ½œåœ¨è½¨è¿¹æŠ½è±¡åº”ç”¨äºä¸¤ç§åœºæ™¯ï¼šä»å¤´è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒLLMã€‚</li>
<li>æ‰©å±•çš„è¯æ±‡è¡¨åŒ…å«æœªè§è¿‡çš„æ½œåœ¨ä»¤ç‰Œï¼Œç”¨äºé€»è¾‘å’Œæ•°å­¦æ¨ç†é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ç®€å•çš„è®­ç»ƒç¨‹åºï¼Œéšæœºæ··åˆæ½œåœ¨å’Œæ–‡æœ¬ä»¤ç‰Œï¼Œä¿ƒè¿›æ¨¡å‹å¯¹æ–°ä»¤ç‰Œçš„å¿«é€Ÿé€‚åº”ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f68659523e9080b51a98bd9930392032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abd703178d587d46b2eff258a276339f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085519fea854f5b3a9a5d3650ff0cc94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d472546ec1e061cc70a41fe834f67b49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdb7e1e6d8946499fdf61c6d46d98210.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Scalable-In-Context-Learning-on-Tabular-Data-via-Retrieval-Augmented-Large-Language-Models"><a href="#Scalable-In-Context-Learning-on-Tabular-Data-via-Retrieval-Augmented-Large-Language-Models" class="headerlink" title="Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models"></a>Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models</h2><p><strong>Authors:Xumeng Wen, Shun Zheng, Zhen Xu, Yiming Sun, Jiang Bian</strong></p>
<p>Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse data schemas and different task domains. However, existing LLM-based TabICL approaches are constrained to few-shot scenarios due to the sequence length limitations of LLMs, as tabular instances represented in plain text consume substantial tokens. To address this limitation and enable scalable TabICL for any data size, we propose retrieval-augmented LLMs tailored to tabular data. Our approach incorporates a customized retrieval module, combined with retrieval-guided instruction-tuning for LLMs. This enables LLMs to effectively leverage larger datasets, achieving significantly improved performance across 69 widely recognized datasets and demonstrating promising scaling behavior. Extensive comparisons with state-of-the-art tabular models reveal that, while LLM-based TabICL still lags behind well-tuned numeric models in overall performance, it uncovers powerful algorithms under limited contexts, enhances ensemble diversity, and excels on specific datasets. These unique properties underscore the potential of language as a universal and accessible interface for scalable tabular data learning. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é’ˆå¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œåè®­ç»ƒå®šåˆ¶åï¼Œå¯ä»¥è·å¾—é€šç”¨çš„ä¸Šä¸‹æ–‡è¡¨æ ¼å­¦ä¹ ï¼ˆTabICLï¼‰èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®æ¨¡å¼å’Œä»»åŠ¡é¢†åŸŸä¸­å®ç°æœ‰æ•ˆçš„è¿ç§»ã€‚ç„¶è€Œï¼Œç”±äºLLMçš„åºåˆ—é•¿åº¦é™åˆ¶ï¼Œç°æœ‰çš„åŸºäºLLMçš„TabICLæ–¹æ³•ä»…é™äºå°æ ·æœ¬åœºæ™¯ï¼Œä»¥çº¯æ–‡æœ¬å½¢å¼è¡¨ç¤ºçš„è¡¨æ ¼å®ä¾‹ä¼šæ¶ˆè€—å¤§é‡ä»¤ç‰Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œå®ç°ä»»ä½•æ•°æ®è§„æ¨¡çš„å¯æ‰©å±•TabICLï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹è¡¨æ ¼æ•°æ®å¢å¼ºçš„æ£€ç´¢è¾…åŠ©LLMã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸€ä¸ªå®šåˆ¶çš„æ£€ç´¢æ¨¡å—ï¼Œä»¥åŠä¸æ£€ç´¢å¼•å¯¼çš„LLMæŒ‡ä»¤è°ƒæ•´ç›¸ç»“åˆã€‚è¿™ä½¿å¾—LLMèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼Œåœ¨69ä¸ªå¹¿æ³›è®¤å¯çš„æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„å¯æ‰©å±•è¡Œä¸ºã€‚ä¸æœ€å…ˆè¿›è¡¨æ ¼æ¨¡å‹çš„å¹¿æ³›æ¯”è¾ƒè¡¨æ˜ï¼Œè™½ç„¶åŸºäºLLMçš„TabICLåœ¨æ€»ä½“æ€§èƒ½ä¸Šä»è½åäºç»è¿‡è‰¯å¥½è°ƒæ•´çš„æ•°å€¼æ¨¡å‹ï¼Œä½†å®ƒåœ¨æœ‰é™ç¯å¢ƒä¸‹å‘ç°äº†å¼ºå¤§çš„ç®—æ³•ï¼Œå¢å¼ºäº†ç»„åˆå¤šæ ·æ€§ï¼Œå¹¶åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¿™äº›ç‹¬ç‰¹ç‰¹æ€§çªæ˜¾äº†è¯­è¨€ä½œä¸ºé€šç”¨å’Œå¯è®¿é—®æ¥å£åœ¨å¯æ‰©å±•è¡¨æ ¼æ•°æ®å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03147v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é’ˆå¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œåè®­ç»ƒï¼Œå¯ä»¥è·å–é€šç”¨çš„è¡¨æ ¼ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆTabICLï¼‰ã€‚ç„¶è€Œï¼Œç”±äºè¡¨æ ¼å®ä¾‹ä»¥çº¯æ–‡æœ¬å½¢å¼è¡¨ç¤ºä¼šæ¶ˆè€—å¤§é‡ä»¤ç‰Œï¼Œç°æœ‰LLMçš„åºåˆ—é•¿åº¦é™åˆ¶ä½¿å…¶ä»…é™äºå°æ ·ä¾‹åœºæ™¯ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶å¹¶å®ç°ä»»ä½•æ•°æ®å¤§å°çš„å¯æ‰©å±•TabICLï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„æ£€ç´¢å¢å¼ºå‹LLMã€‚è¯¥æ–¹æ³•ç»“åˆäº†å®šåˆ¶åŒ–çš„æ£€ç´¢æ¨¡å—å’Œæ£€ç´¢æŒ‡å¯¼çš„LLMæŒ‡ä»¤å¾®è°ƒæŠ€æœ¯ï¼Œä½¿LLMèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼Œåœ¨69ä¸ªå¹¿æ³›è®¤å¯çš„æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç°å‡ºæœ‰å‰æ™¯çš„æ‰©å±•æ€§ã€‚å°½ç®¡LLMçš„TabICLåœ¨æ€»ä½“æ€§èƒ½ä¸Šä»è½åäºç»è¿‡è‰¯å¥½è°ƒæ•´çš„æ•°å€¼æ¨¡å‹ï¼Œä½†å®ƒèƒ½åœ¨æœ‰é™ä¸Šä¸‹æ–‡ä¸­æ­ç¤ºå¼ºå¤§çš„ç®—æ³•ï¼Œæé«˜é›†æˆå¤šæ ·æ€§ï¼Œå¹¶åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚è¿™å‡¸æ˜¾äº†è¯­è¨€ä½œä¸ºå¯æ‰©å±•è¡¨æ ¼æ•°æ®å­¦ä¹ çš„é€šç”¨å’Œå¯è®¿é—®æ¥å£çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡é€šè¿‡åå¤©è®­ç»ƒå­¦ä¹ TabICLçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿè·¨ä¸åŒæ•°æ®æ¨¡å¼å’Œä»»åŠ¡åŸŸè¿›è¡Œæœ‰æ•ˆè¿ç§»ã€‚</li>
<li>LLMåœ¨è¡¨æ ¼æ•°æ®å¤„ç†ä¸Šå—é™äºåºåˆ—é•¿åº¦ï¼Œä¸»è¦é€‚ç”¨äºå°æ ·ä¾‹åœºæ™¯ã€‚</li>
<li>æ£€ç´¢å¢å¼ºå‹LLMé€šè¿‡ç»“åˆæ£€ç´¢æ¨¡å—å’ŒæŒ‡ä»¤å¾®è°ƒæŠ€æœ¯è§£å†³äº†åºåˆ—é•¿åº¦é™åˆ¶é—®é¢˜ï¼Œå®ç°äº†å¯æ‰©å±•çš„TabICLã€‚</li>
<li>æ£€ç´¢å¢å¼ºå‹LLMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>LLM-based TabICLåœ¨æ€»ä½“æ€§èƒ½ä¸Šè™½ä¸åŠæ•°å€¼æ¨¡å‹ï¼Œä½†åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹æ­ç¤ºå¼ºå¤§ç®—æ³•ï¼Œæé«˜é›†æˆå¤šæ ·æ€§ã€‚</li>
<li>è¯­è¨€ä½œä¸ºè¡¨æ ¼æ•°æ®å­¦ä¹ çš„æ¥å£å…·æœ‰é€šç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-794fdada3b907cf49bc40bb60210cafc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f81fb15aeafda82dad10e1b9acd52ea0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e90f30928366d0a06b083e5cdfb68385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0eed5782f6af179339c23167d7abc94.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FuXi-Î±-Scaling-Recommendation-Model-with-Feature-Interaction-Enhanced-Transformer"><a href="#FuXi-Î±-Scaling-Recommendation-Model-with-Feature-Interaction-Enhanced-Transformer" class="headerlink" title="FuXi-$Î±$: Scaling Recommendation Model with Feature Interaction   Enhanced Transformer"></a>FuXi-$Î±$: Scaling Recommendation Model with Feature Interaction   Enhanced Transformer</h2><p><strong>Authors:Yufei Ye, Wei Guo, Jin Yao Chin, Hao Wang, Hong Zhu, Xi Lin, Yuyang Ye, Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen</strong></p>
<p>Inspired by scaling laws and large language models, research on large-scale recommendation models has gained significant attention. Recent advancements have shown that expanding sequential recommendation models to large-scale recommendation models can be an effective strategy. Current state-of-the-art sequential recommendation models primarily use self-attention mechanisms for explicit feature interactions among items, while implicit interactions are managed through Feed-Forward Networks (FFNs). However, these models often inadequately integrate temporal and positional information, either by adding them to attention weights or by blending them with latent representations, which limits their expressive power. A recent model, HSTU, further reduces the focus on implicit feature interactions, constraining its performance. We propose a new model called FuXi-$\alpha$ to address these issues. This model introduces an Adaptive Multi-channel Self-attention mechanism that distinctly models temporal, positional, and semantic features, along with a Multi-stage FFN to enhance implicit feature interactions. Our offline experiments demonstrate that our model outperforms existing models, with its performance continuously improving as the model size increases. Additionally, we conducted an online A&#x2F;B test within the Huawei Music app, which showed a $4.76%$ increase in the average number of songs played per user and a $5.10%$ increase in the average listening duration per user. Our code has been released at <a target="_blank" rel="noopener" href="https://github.com/USTC-StarTeam/FuXi-alpha">https://github.com/USTC-StarTeam/FuXi-alpha</a>. </p>
<blockquote>
<p>å—ç¼©æ”¾å®šå¾‹å’Œå¤§è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œå¯¹å¤§è§„æ¨¡æ¨èæ¨¡å‹çš„ç ”ç©¶å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œå°†åºåˆ—æ¨èæ¨¡å‹æ‰©å±•åˆ°å¤§è§„æ¨¡æ¨èæ¨¡å‹å¯ä»¥æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ã€‚å½“å‰æœ€å…ˆè¿›çš„åºåˆ—æ¨èæ¨¡å‹ä¸»è¦ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œé¡¹ç›®ä¹‹é—´çš„æ˜¾å¼ç‰¹å¾äº¤äº’ï¼Œè€Œéšå¼äº¤äº’åˆ™é€šè¿‡å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰è¿›è¡Œç®¡ç†ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¸èƒ½å……åˆ†åœ°æ•´åˆæ—¶é—´å’Œä½ç½®ä¿¡æ¯ï¼Œæ— è®ºæ˜¯é€šè¿‡å°†å®ƒä»¬æ·»åŠ åˆ°æ³¨æ„åŠ›æƒé‡ä¸­ï¼Œè¿˜æ˜¯å°†å®ƒä»¬ä¸æ½œåœ¨è¡¨ç¤ºæ··åˆï¼Œè¿™éƒ½é™åˆ¶äº†å®ƒä»¬çš„è¡¨è¾¾èƒ½åŠ›ã€‚æœ€è¿‘çš„ä¸€ä¸ªæ¨¡å‹HSTUè¿›ä¸€æ­¥å‡å°‘äº†éšå¼ç‰¹å¾äº¤äº’çš„å…³æ³¨åº¦ï¼Œåˆ¶çº¦äº†å…¶æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹â€”â€”FuXi-$\alpha$ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å¤šé€šé“è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿç‹¬ç‰¹åœ°å»ºæ¨¡æ—¶é—´ã€ä½ç½®å’Œè¯­ä¹‰ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªå¤šé˜¶æ®µFFNï¼Œä»¥å¢å¼ºéšå¼ç‰¹å¾äº¤äº’ã€‚æˆ‘ä»¬çš„ç¦»çº¿å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå…¶æ€§èƒ½ä¸æ–­æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨åä¸ºéŸ³ä¹åº”ç”¨ç¨‹åºä¸­è¿›è¡Œäº†åœ¨çº¿A&#x2F;Bæµ‹è¯•ï¼Œå¹³å‡æ¯ç”¨æˆ·æ’­æ”¾æ­Œæ›²æ•°é‡å¢åŠ äº†4.76%ï¼Œå¹³å‡æ¯ç”¨æˆ·æ”¶å¬æ—¶é•¿å¢åŠ äº†5.10%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/USTC-StarTeam/FuXi-alpha%E3%80%82">https://github.com/USTC-StarTeam/FuXi-alphaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03036v1">PDF</a> Accepted by WWW2025</p>
<p><strong>Summary</strong><br>     å—å°ºåº¦å®šå¾‹å’Œå¤§è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œå¤§è§„æ¨¡æ¨èæ¨¡å‹ç ”ç©¶å¤‡å—å…³æ³¨ã€‚æ–°çš„æ¨¡å‹FuXi-$\alpha$é€šè¿‡å¼•å…¥è‡ªé€‚åº”å¤šé€šé“è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œåˆ†é˜¶æ®µå‰é¦ˆç½‘ç»œè§£å†³äº†ç°æœ‰æ¨¡å‹çš„ä¸è¶³ï¼ŒåŒ…æ‹¬å»ºæ¨¡æ—¶é—´ã€ä½ç½®å’Œè¯­ä¹‰ç‰¹å¾çš„åˆ†ç¦»ä»¥åŠå¼ºåŒ–éšæ€§ç‰¹å¾äº¤äº’çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨ä¸åŒè§„æ¨¡çš„å®éªŒä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ¨¡å‹æ€§èƒ½çš„ä¼˜åŠ¿ï¼Œå¹¶åœ¨åä¸ºéŸ³ä¹åº”ç”¨çš„åœ¨çº¿A&#x2F;Bæµ‹è¯•ä¸­å®ç°äº†å¹³å‡æ¯ç”¨æˆ·æ’­æ”¾æ­Œæ›²æ•°å’Œå¹³å‡æ”¶å¬æ—¶é—´çš„å¢é•¿ã€‚ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ¨èæ¨¡å‹ç ”ç©¶å› å°ºåº¦å®šå¾‹å’Œå¤§è¯­è¨€æ¨¡å‹çš„å¯å‘è€Œå¤‡å—å…³æ³¨ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„é¡ºåºæ¨èæ¨¡å‹ä¸»è¦åˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œé¡¹ç›®é—´çš„æ˜¾å¼ç‰¹å¾äº¤äº’ï¼Œè€Œéšæ€§äº¤äº’åˆ™é€šè¿‡å‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰ç®¡ç†ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é›†æˆæ—¶é—´å’Œä½ç½®ä¿¡æ¯æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>FuXi-$\alpha$æ¨¡å‹é€šè¿‡å¼•å…¥è‡ªé€‚åº”å¤šé€šé“è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥åˆ†åˆ«å»ºæ¨¡æ—¶é—´ã€ä½ç½®å’Œè¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>FuXi-$\alpha$æ¨¡å‹é‡‡ç”¨åˆ†é˜¶æ®µå‰é¦ˆç½‘ç»œä»¥å¢å¼ºéšæ€§ç‰¹å¾äº¤äº’ã€‚</li>
<li>çº¿ä¸‹å®éªŒè¡¨æ˜ï¼ŒFuXi-$\alpha$æ¨¡å‹ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸”æ€§èƒ½éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œä¸æ–­æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f0945d89016737a2fe9fd0c968ff293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec6d237746e4a3c0346b352c39c552b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d1a96882e407621d836d8835a3f864c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afccc27f44437798310abb516ddde1bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e5a68aaea49f51a8d0dcc19a1895cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c0f8498e0f62e5a7df1e197b9d4e4b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Mol-LLM-Generalist-Molecular-LLM-with-Improved-Graph-Utilization"><a href="#Mol-LLM-Generalist-Molecular-LLM-with-Improved-Graph-Utilization" class="headerlink" title="Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization"></a>Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization</h2><p><strong>Authors:Chanhui Lee, Yuheon Song, YongJun Jeong, Hanbum Ko, Rodrigo Hormazabal, Sehui Han, Kyunghoon Bae, Sungbin Lim, Sungwoong Kim</strong></p>
<p>Recent advances in Large Language Models (LLMs) have motivated the development of general LLMs for molecular tasks. While several studies have demonstrated that fine-tuned LLMs can achieve impressive benchmark performances, they are far from genuine generalist molecular LLMs due to a lack of fundamental understanding of molecular structure. Specifically, when given molecular task instructions, LLMs trained with naive next-token prediction training assign similar likelihood scores to both original and negatively corrupted molecules, revealing their lack of molecular structure understanding that is crucial for reliable and general molecular LLMs. To overcome this limitation and obtain a true generalist molecular LLM, we introduce a novel multi-modal training method based on a thorough multi-modal instruction tuning as well as a molecular structure preference optimization between chosen and rejected graphs. On various molecular benchmarks, the proposed generalist molecular LLM, called Mol-LLM, achieves state-of-the-art performances among generalist LLMs on most tasks, at the same time, surpassing or comparable to state-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior generalization performances in reaction prediction tasks, demonstrating the effect of the molecular structure understanding for generalization perspective. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†é€šç”¨LLMåœ¨åˆ†å­ä»»åŠ¡é¢†åŸŸçš„å‘å±•ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„LLMå¯ä»¥åœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹å¯¹åˆ†å­ç»“æ„çš„åŸºæœ¬ç†è§£ï¼Œå®ƒä»¬è·ç¦»çœŸæ­£çš„é€šç”¨åˆ†å­LLMè¿˜å¾ˆè¿œã€‚å…·ä½“æ¥è¯´ï¼Œå½“æ¥å—åˆ†å­ä»»åŠ¡æŒ‡ä»¤æ—¶ï¼Œç”¨ç®€å•çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è®­ç»ƒè€Œè·å¾—çš„LLMä¼šå¯¹åŸå§‹åˆ†å­å’Œè´Ÿé¢å¹²æ‰°åˆ†å­åˆ†é…ç›¸ä¼¼çš„å¯èƒ½æ€§åˆ†æ•°ï¼Œè¿™æ­ç¤ºäº†å®ƒä»¬ç¼ºä¹å¯¹äºåˆ†å­ç»“æ„çš„ç†è§£ï¼Œè¿™å¯¹äºå¯é å’Œé€šç”¨çš„åˆ†å­LLMè‡³å…³é‡è¦ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§å¹¶è·å¾—çœŸæ­£çš„é€šç”¨åˆ†å­LLMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå…¨é¢å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ä»¥åŠæ‰€é€‰å’Œæ‹’ç»å›¾å½¢ä¹‹é—´åˆ†å­ç»“æ„åå¥½ä¼˜åŒ–çš„æ–°å‹å¤šæ¨¡å¼è®­ç»ƒæ–¹æ³•ã€‚åœ¨å„ç§åˆ†å­åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„é€šç”¨åˆ†å­LLMï¼ˆç§°ä¸ºMol-LLMï¼‰åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è¶…è¶Šæˆ–ç›¸å½“äºæœ€æ–°ä¸“ä¸šLLMã€‚æ­¤å¤–ï¼ŒMol-LLMåœ¨ååº”é¢„æµ‹ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†ä»æ³›åŒ–è§’åº¦ç†è§£åˆ†å­ç»“æ„çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02810v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†é€šç”¨LLMåœ¨åˆ†å­ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å°½ç®¡å·²æœ‰ç ”ç©¶è¡¨æ˜ç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨åŸºå‡†æµ‹è¯•ä¸­å–å¾—ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹å¯¹åˆ†å­ç»“æ„çš„åŸºæœ¬ç†è§£ï¼Œå®ƒä»¬è·ç¦»çœŸæ­£çš„é€šç”¨åˆ†å­LLMè¿˜å¾ˆè¿œã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå…¨é¢å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ä»¥åŠæ‰€é€‰å’Œæ‹’ç»å›¾å½¢ä¹‹é—´çš„åˆ†å­ç»“æ„åå¥½ä¼˜åŒ–çš„æ–°å‹å¤šæ¨¡å¼è®­ç»ƒæ–¹æ³•ã€‚åœ¨å„ç§åˆ†å­åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„åä¸ºMol-LLMçš„é€šç”¨åˆ†å­LLMåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æŸäº›ä»»åŠ¡ä¸Šè¶…è¶Šäº†æˆ–ç›¸å½“äºæœ€æ–°æŠ€æœ¯æ°´å¹³çš„ä¸“å®¶LLMã€‚æ­¤å¤–ï¼ŒMol-LLMåœ¨ååº”é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå“è¶Šæ³›åŒ–æ€§èƒ½ï¼Œè¯æ˜äº†ç†è§£åˆ†å­ç»“æ„å¯¹æ³›åŒ–æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£è¢«åº”ç”¨äºåˆ†å­ä»»åŠ¡ï¼Œä½†ä»ç¼ºä¹çœŸæ­£çš„é€šç”¨åˆ†å­ç†è§£ã€‚</li>
<li>å½“å‰LLMåœ¨åˆ†å­ä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™äºå¯¹åˆ†å­ç»“æ„çš„åŸºæœ¬ç†è§£ç¼ºå¤±ã€‚</li>
<li>é’ˆå¯¹è¿™ä¸€ç¼ºé™·ï¼Œæå‡ºäº†åä¸ºMol-LLMçš„æ–°å‹å¤šæ¨¡å¼è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>Mol-LLMåœ¨å¤šç§åˆ†å­åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œéƒ¨åˆ†ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†ä¸“å®¶LLMçš„æ€§èƒ½ã€‚</li>
<li>Mol-LLMåœ¨ååº”é¢„æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç†è§£å’ŒæŒæ¡åˆ†å­ç»“æ„å¯¹äºæé«˜LLMåœ¨åˆ†å­ä»»åŠ¡ä¸­çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bd8a434896d4d92e0920bd324c0013c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943f894cbcef3443aa9a7191f7e577be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b29052515b30c0b2267d58b6e4f345.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a047d61427d92bf4af2e603cf20c3e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5890f4c8f21821f7d6bea1c9728c5498.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-982b5a46e9b81f121f6429a87a86e446.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transformers-Boost-the-Performance-of-Decision-Trees-on-Tabular-Data-across-Sample-Sizes"><a href="#Transformers-Boost-the-Performance-of-Decision-Trees-on-Tabular-Data-across-Sample-Sizes" class="headerlink" title="Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes"></a>Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes</h2><p><strong>Authors:Mayuka Jayawardhana, Renbo Tu, Samuel Dooley, Valeriia Cherepanova, Andrew Gordon Wilson, Frank Hutter, Colin White, Tom Goldstein, Micah Goldblum</strong></p>
<p>Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on numerous tables for in-context learning, has demonstrated excellent performance for dataset sizes up to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically trained from scratch on each dataset without benefiting from pretraining data and must learn the relationships between columns from their entries alone since they lack natural language understanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is essential, yet they are not competitive with GBDTs on medium or large datasets, since their context lengths are limited. In this paper, we propose a simple and lightweight approach for fusing large language models and TabPFN with gradient-boosted decision trees, which allows scalable GBDTs to benefit from the natural language capabilities and pretraining of transformers. We name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at sufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on a wide range of dataset sizes in between. We demonstrate state-of-the-art performance against numerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best average performance among all methods we test for all but very small dataset sizes. We release our code at <a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°éå¸¸å‡ºè‰²ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä»æè¿°ç‰¹å¾å’Œæ ‡ç­¾çš„è‡ªç„¶è¯­è¨€åˆ—æ ‡é¢˜ä¸­æå–æ„ä¹‰ã€‚ä¸æ­¤ç±»ä¼¼ï¼ŒTabPFNæ˜¯ä¸€ä¸ªæœ€è¿‘çš„éLLMè½¬æ¢å™¨ï¼Œå®ƒåœ¨ä¼—å¤šè¡¨æ ¼ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶å·²æ˜¾ç¤ºå‡ºåœ¨æœ€å¤šè¾¾ä¸€åƒä¸ªæ ·æœ¬çš„æ•°æ®é›†ä¸Šçš„å‡ºè‰²æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTsï¼‰é€šå¸¸ä¼šåœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šä»å¤´å¼€å§‹è®­ç»ƒï¼Œæ— æ³•ä»é¢„è®­ç»ƒæ•°æ®ä¸­è·ç›Šï¼Œå¹¶ä¸”ç”±äºå®ƒä»¬ç¼ºä¹è‡ªç„¶è¯­è¨€ç†è§£ï¼Œå¿…é¡»ä»…ä»æ¡ç›®æœ¬èº«å­¦ä¹ åˆ—ä¹‹é—´çš„å…³ç³»ã€‚LLMså’ŒTabPFNåœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­å¼ºçƒˆçš„å…ˆéªŒçŸ¥è¯†è‡³å…³é‡è¦ï¼Œä½†åœ¨ä¸­ç­‰æˆ–å¤§å‹æ•°æ®é›†ä¸Šï¼Œå®ƒä»¬ä¸GBDTsä¸å…·æœ‰ç«äº‰åŠ›ï¼Œå› ä¸ºå®ƒä»¬çš„ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œè½»é‡çº§çš„èåˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒTabPFNä¸æ¢¯åº¦æå‡å†³ç­–æ ‘çš„æ–¹æ³•ï¼Œè¿™å…è®¸å¯æ‰©å±•çš„GBDTå—ç›Šäºè½¬æ¢å™¨çš„è‡ªç„¶è¯­è¨€èƒ½åŠ›å’Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„èåˆæ–¹æ³•åˆ†åˆ«å‘½åä¸ºLLM-Boostå’ŒPFN-Boostã€‚å°½ç®¡å®ƒä»¬åœ¨è¶³å¤Ÿå°çš„æ•°æ®é›†å¤§å°ä¸Šå¯ä»¥ä¸è½¬æ¢å™¨çš„æ€§èƒ½ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œå¹¶ä¸”åœ¨è¶³å¤Ÿå¤§çš„å°ºå¯¸ä¸Šå¯ä»¥ä¸GBDTsçš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œä½†LLM-Boostå’ŒPFN-Booståœ¨ä¸­é—´èŒƒå›´çš„å¤šç§æ•°æ®é›†å¤§å°ä¸Šéƒ½è¡¨ç°å‡ºä¼˜äºä¸¤ä¸ªç‹¬ç«‹ç»„ä»¶çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä¸ä¼—å¤šåŸºå‡†çº¿å’Œé›†æˆç®—æ³•ç›¸æ¯”ï¼Œå±•ç°äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œé™¤éå¸¸å°çš„æ•°æ®é›†å¤§å°å¤–ï¼ŒPFN-Booståœ¨æ‰€æœ‰æ–¹æ³•ä¸­è·å¾—äº†æœ€ä½³çš„å¹³å‡æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02672v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ä»è‡ªç„¶è¯­è¨€æè¿°çš„åˆ—æ ‡é¢˜ä¸­æå–ç‰¹å¾å’Œæ ‡ç­¾çš„æ„ä¹‰ã€‚è€Œé¢„è®­ç»ƒåœ¨å¤§é‡è¡¨æ ¼ä¸Šçš„éLLMæ¨¡å‹TabPFNï¼Œå¯¹äºåƒæ ·æœ¬è§„æ¨¡ä»¥ä¸‹çš„æ•°æ®é›†ä¹Ÿå±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼Œæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTsï¼‰é€šå¸¸ä»æ•°æ®é›†ä¸­ç›´æ¥è®­ç»ƒï¼Œç¼ºä¹è‡ªç„¶è¯­è¨€ç†è§£çš„èƒ½åŠ›ã€‚åœ¨å°è§„æ¨¡è¡¨æ ¼æ•°æ®é›†ä¸­ï¼ŒLLMså’ŒTabPFNå‡­å€Ÿå¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸­å¤§è§„æ¨¡æ•°æ®é›†ä¸Šåˆ™ä¸å¦‚GBDTsã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è½»é‡çº§çš„èåˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒTabPFNä¸æ¢¯åº¦æå‡å†³ç­–æ ‘çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºLLM-Boostå’ŒPFN-Boostã€‚è¿™ä¸¤ç§èåˆæ–¹æ³•èƒ½åœ¨ä¸åŒæ•°æ®é›†å¤§å°ä¹‹é—´å®ç°ä¼˜äºå•ä¸€ç»„ä»¶çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ç­‰è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’ŒTabPFNèƒ½ä»è‡ªç„¶è¯­è¨€æè¿°çš„åˆ—æ ‡é¢˜ä¸­æå–ç‰¹å¾å’Œæ ‡ç­¾æ„ä¹‰ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>GBDTsé€šå¸¸ä»æ•°æ®é›†ä¸­ç›´æ¥è®­ç»ƒï¼Œç¼ºä¹è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œåœ¨ä¸­å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>LLM-Boostå’ŒPFN-Boostèåˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒTabPFNä¸æ¢¯åº¦æå‡å†³ç­–æ ‘çš„æ–¹æ³•èƒ½åœ¨ä¸åŒæ•°æ®é›†å¤§å°ä¹‹é—´å®ç°ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>LLM-Boostå’ŒPFN-Boostç›¸è¾ƒäºå•ä¸€ç»„ä»¶å’ŒåŸºçº¿ç®—æ³•ï¼Œå±•ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>PFN-Booståœ¨é™¤æå°è§„æ¨¡æ•°æ®é›†å¤–çš„æ‰€æœ‰æ•°æ®é›†å¤§å°ä¸Šéƒ½å–å¾—äº†æœ€ä½³å¹³å‡æ€§èƒ½ã€‚</li>
<li>èåˆæ–¹æ³•ç»“åˆäº†LLMså’ŒGBDTsçš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ•°æ®é›†å¤§å°ä¸Šå®ç°å‡è¡¡çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-155ab9ea96b16aa1bf38a99036944a13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-557f160475fc9ac7bb65d414f94dda85.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PolarQuant-Quantizing-KV-Caches-with-Polar-Transformation"><a href="#PolarQuant-Quantizing-KV-Caches-with-Polar-Transformation" class="headerlink" title="PolarQuant: Quantizing KV Caches with Polar Transformation"></a>PolarQuant: Quantizing KV Caches with Polar Transformation</h2><p><strong>Authors:Insu Han, Praneeth Kacham, Amin Karbasi, Vahab Mirrokni, Amir Zandieh</strong></p>
<p>Large language models (LLMs) require significant memory to store Key-Value (KV) embeddings in their KV cache, especially when handling long-range contexts. Quantization of these KV embeddings is a common technique to reduce memory consumption. This work introduces PolarQuant, a novel quantization method employing random preconditioning and polar transformation. Our method transforms the KV embeddings into polar coordinates using an efficient recursive algorithm and then quantizes resulting angles. Our key insight is that, after random preconditioning, the angles in the polar representation exhibit a tightly bounded and highly concentrated distribution with an analytically computable form. This nice distribution eliminates the need for explicit normalization, a step required by traditional quantization methods which introduces significant memory overhead because quantization parameters (e.g., zero point and scale) must be stored in full precision per each data block. PolarQuant bypasses this normalization step, enabling substantial memory savings. The long-context evaluation demonstrates that PolarQuant compresses the KV cache by over x4.2 while achieving the best quality scores compared to the state-of-the-art methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦åœ¨å…¶KVç¼“å­˜ä¸­å­˜å‚¨å¤§é‡çš„é”®å€¼ï¼ˆKVï¼‰åµŒå…¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿èŒƒå›´ä¸Šä¸‹æ–‡æ—¶ã€‚ä¸ºäº†å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œé‡åŒ–è¿™äº›KVåµŒå…¥æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†PolarQuantï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨éšæœºé¢„å¤„ç†å’Œæåæ ‡å˜æ¢çš„æ–°å‹é‡åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é«˜æ•ˆçš„é€’å½’ç®—æ³•å°†KVåµŒå…¥è½¬æ¢ä¸ºæåæ ‡ï¼Œç„¶åå¯¹æ‰€å¾—çš„è§’è¿›è¡Œé‡åŒ–ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œç»è¿‡éšæœºé¢„å¤„ç†åï¼Œæåæ ‡è¡¨ç¤ºä¸­çš„è§’åº¦å‘ˆç°å‡ºç´§å¯†ä¸”é«˜åº¦é›†ä¸­çš„åˆ†å¸ƒï¼Œå…·æœ‰å¯åˆ†æè®¡ç®—çš„å½¢å¼ã€‚è¿™ç§ç¾è§‚çš„åˆ†å¸ƒä¸éœ€è¦é¢å¤–çš„æ ‡å‡†åŒ–æ­¥éª¤ï¼Œè¿™æ˜¯ä¼ ç»Ÿé‡åŒ–æ–¹æ³•æ‰€è¦æ±‚çš„ï¼Œè€Œè¿™äº›é‡åŒ–æ–¹æ³•ä¸­çš„æ¯ä¸ªæ•°æ®å—éƒ½å¿…é¡»ä»¥å…¨ç²¾åº¦å­˜å‚¨é‡åŒ–å‚æ•°ï¼ˆä¾‹å¦‚é›¶ç‚¹å’Œå°ºåº¦ï¼‰ï¼Œè¿™ä¼šå¼•å…¥é¢å¤–çš„å†…å­˜å¼€é”€ã€‚PolarQuanté€šè¿‡ç»•è¿‡æ ‡å‡†åŒ–æ­¥éª¤å®ç°äº†æ˜¾è‘—çš„å†…å­˜èŠ‚çœã€‚é•¿ä¸Šä¸‹æ–‡è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPolarQuantå‹ç¼©äº†KVç¼“å­˜è¶…è¿‡x4.2å€ï¼ŒåŒæ—¶å–å¾—äº†æœ€å¥½çš„è´¨é‡åˆ†æ•°ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”æœ‰æ‰€è¶…è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é”®å€¼ï¼ˆKVï¼‰åµŒå…¥éœ€è¦å ç”¨å¤§é‡å†…å­˜æ¥å­˜å‚¨ï¼Œå¤„ç†é•¿èŒƒå›´ä¸Šä¸‹æ–‡æ—¶å°¤å…¶å¦‚æ­¤ã€‚ä¸ºå‡å°‘å†…å­˜æ¶ˆè€—ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºPolarQuantçš„æ–°å‹é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨éšæœºé¢„å¤„ç†å’Œæåæ ‡å˜æ¢ã€‚è¯¥æ–¹æ³•é€šè¿‡é«˜æ•ˆçš„é€’å½’ç®—æ³•å°†KVåµŒå…¥è½¬æ¢ä¸ºæåæ ‡ï¼Œç„¶åé‡åŒ–ç»“æœè§’åº¦ã€‚å…³é”®è§è§£æ˜¯ï¼Œéšæœºé¢„å¤„ç†åï¼Œæåæ ‡ä¸­çš„è§’åº¦å‘ˆç°å‡ºç•Œé™ç´§å¯†ã€é«˜åº¦é›†ä¸­çš„åˆ†å¸ƒï¼Œå…·æœ‰å¯åˆ†æçš„è®¡ç®—å½¢å¼ã€‚è¿™ç§åˆ†å¸ƒæ— éœ€æ‰§è¡Œä¼ ç»Ÿé‡åŒ–æ–¹æ³•æ‰€éœ€çš„å½’ä¸€åŒ–æ­¥éª¤ï¼Œä»è€Œé¿å…äº†å› å­˜å‚¨é‡åŒ–å‚æ•°ï¼ˆä¾‹å¦‚é›¶ç‚¹å’Œæ¯”ä¾‹ï¼‰è€Œäº§ç”Ÿçš„å†…å­˜å¼€é”€ã€‚PolarQuanté€šè¿‡ç»•è¿‡æ­¤å½’ä¸€åŒ–æ­¥éª¤ï¼Œå®ç°äº†å¯è§‚çš„å†…å­˜èŠ‚çœã€‚é•¿ä¸Šä¸‹æ–‡è¯„ä¼°è¡¨æ˜ï¼ŒPolarQuantå°†KVç¼“å­˜å‹ç¼©äº†è¶…è¿‡4.2å€ï¼ŒåŒæ—¶è¾¾åˆ°äº†ç›¸æ¯”æœ€æ–°æŠ€æœ¯æœ€ä½³çš„è´¨é‡è¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„KVåµŒå…¥éœ€è¦å¤§å†…å­˜æ¥å­˜å‚¨ã€‚</li>
<li>æåæ ‡é‡åŒ–æ–¹æ³•PolarQuantè¢«å¼•å…¥ï¼Œç”¨äºå‡å°‘å†…å­˜æ¶ˆè€—ã€‚</li>
<li>PolarQuantä½¿ç”¨éšæœºé¢„å¤„ç†å’Œæåæ ‡è½¬æ¢ã€‚</li>
<li>éšæœºé¢„å¤„ç†åçš„è§’åº¦åˆ†å¸ƒç´§å¯†ä¸”é›†ä¸­ï¼Œæ— éœ€å½’ä¸€åŒ–æ­¥éª¤ã€‚</li>
<li>ä¼ ç»Ÿé‡åŒ–æ–¹æ³•éœ€è¦å­˜å‚¨é‡åŒ–å‚æ•°ï¼Œå¢åŠ äº†å†…å­˜å¼€é”€ã€‚</li>
<li>PolarQuantæ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨ï¼Œå‹ç¼©äº†KVç¼“å­˜è¶…è¿‡4.2å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b4c468ec281a0d64ad182feccd9c4d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9f5f17ab5ea6602bc444ff5fdda19b8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation"><a href="#BARE-Combining-Base-and-Instruction-Tuned-Language-Models-for-Better-Synthetic-Data-Generation" class="headerlink" title="BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation"></a>BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation</h2><p><strong>Authors:Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Boris Hanin, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia</strong></p>
<p>As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. </p>
<blockquote>
<p>éšç€æ¨¡å‹è®­ç»ƒä¸­å¯¹é«˜è´¨é‡æ•°æ®çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆåˆæˆæ•°æ®æ¥è°ƒæ•´å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å…³äºåˆæˆæ•°æ®çš„å¸¸è§å‡è®¾æ˜¯ï¼Œä»æŒ‡ä»¤è°ƒæ•´è¿‡çš„æ¨¡å‹ä¸­é‡‡æ ·å°±è¶³å¤Ÿäº†ï¼›ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨äº§ç”Ÿå¤šæ ·åŒ–è¾“å‡ºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™æ˜¯æ³›åŒ–çš„ä¸€ä¸ªå…³é”®è¦æ±‚ã€‚å°½ç®¡æœ‰å„ç§æç¤ºæ–¹æ³•ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ä»æŒ‡ä»¤è°ƒæ•´è¿‡çš„æ¨¡å‹ä¸­å®ç°æœ‰æ„ä¹‰çš„å¤šæ ·æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å‘ç°æ²¡æœ‰ç»è¿‡åè®­ç»ƒçš„åŸºå‡†æ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„å¤šæ ·æ€§ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›è¾ƒå¼±ï¼Œå› æ­¤è´¨é‡è¾ƒä½ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Base-Refineï¼ˆBAREï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç»“åˆåŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤è°ƒæ•´è¿‡çš„æ¨¡å‹çš„è´¨é‡ã€‚é€šè¿‡å°‘é‡çš„ç¤ºä¾‹å’Œç­›é€‰ï¼ŒBAREç”Ÿæˆäº†å¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨BAREç”Ÿæˆæ ·æœ¬ä»…1000ä¸ªè¿›è¡Œå¾®è°ƒå³å¯åœ¨LiveCodeBenchä»»åŠ¡ä¸Šè¾¾åˆ°ä¸æœ€ä½³ç›¸ä¼¼è§„æ¨¡æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆçš„æ•°æ®è¿›è¡Œå¾®è°ƒåœ¨GSM8Kä¸Šå®ç°äº†æ¯”ä»…ä½¿ç”¨æŒ‡ä»¤æ•°æ®æé«˜101%ï¼Œåœ¨RAFTä¸Šæ¯”ç°æœ‰æœ€ä½³æ–¹æ³•æé«˜äº†18.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01697v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆé«˜è´¨é‡æ¨¡å‹è®­ç»ƒæ•°æ®éœ€æ±‚çš„æå‡ï¼Œç ”ç©¶è€…åˆ©ç”¨åˆæˆæ•°æ®ä¼˜åŒ–ä¸è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è™½ç„¶å¯¹åˆæˆæ•°æ®çš„å¸¸è§„å‡è®¾æ˜¯é€šè¿‡æŒ‡ä»¤å¾®è°ƒæ¨¡å‹é‡‡æ ·è¶³å¤Ÿçš„æ•°æ®å³å¯ï¼Œä½†è¿™äº›æ¨¡å‹éš¾ä»¥ç”Ÿæˆå¤šæ ·åŒ–è¾“å‡ºï¼Œè¿™å¯¹äºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚å°½ç®¡æœ‰å„ç§æç¤ºæ–¹æ³•ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­å‘ç°ä»æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å®ç°æœ‰æ„ä¹‰å¤šæ ·æ€§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸åï¼Œç ”ç©¶å‘ç°æœªç»è®­ç»ƒçš„åŸºå‡†æ¨¡å‹å±•ç°å‡ºæ›´å¤§çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›è¾ƒä½ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œç ”ç©¶æå‡ºäº†ç»“åˆåŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è´¨é‡çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•â€”â€”Base-Refineï¼ˆBAREï¼‰ã€‚é€šè¿‡å°‘é‡çš„ç¤ºä¾‹å’Œç²¾å¿ƒç­›é€‰çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼ŒBAREèƒ½ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨1,000ä¸ªBAREç”Ÿæˆæ ·æœ¬è¿›è¡Œå¾®è°ƒå³å¯åœ¨LiveCodeBenchä»»åŠ¡ä¸Šè¾¾åˆ°ä¸æœ€ä½³ç›¸ä¼¼è§„æ¨¡æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨BAREç”Ÿæˆæ•°æ®è¿›è¡Œå¾®è°ƒåœ¨GSM8Kä¸Šå®ç°äº†å¯¹æŒ‡ä»¤ä»…æ•°æ®çš„æ”¹è¿›æå‡äº†101%ï¼Œå¹¶ä¸”åœ¨RAFTä¸Šç›¸å¯¹äºç°æœ‰æ–¹æ³•æé«˜äº†18.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜æ­£åœ¨åˆ©ç”¨åˆæˆæ•°æ®ä¼˜åŒ–å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>è™½ç„¶æŒ‡ä»¤å¾®è°ƒæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ•°æ®ï¼Œä½†å®ƒä»¬éš¾ä»¥ç”Ÿæˆå¤šæ ·åŒ–è¾“å‡ºã€‚</li>
<li>æœªç»è®­ç»ƒçš„åŸºå‡†æ¨¡å‹å±•ç°å‡ºæ›´å¤§çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›è¾ƒä½ã€‚</li>
<li>æå‡ºçš„Base-Refineï¼ˆBAREï¼‰æ–¹æ³•ç»“åˆäº†åŸºå‡†æ¨¡å‹çš„å¤šæ ·æ€§å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„è´¨é‡ã€‚</li>
<li>BAREé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæé«˜äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å°‘é‡BAREç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œå¾®è°ƒå³å¯è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d21c6b0ee4ae4a31251f77c5ca955e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2aedf490ff7ee0cbb3ff7b1063b933f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecc96ecda174238e70bc680753559e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d93849a1a7b4bb94a319e97c5973c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a2a1fce24198ceb9f7f7f2a168d9850.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Efficient-Prompt-Compression-with-Evaluator-Heads-for-Long-Context-Transformer-Inference"><a href="#Efficient-Prompt-Compression-with-Evaluator-Heads-for-Long-Context-Transformer-Inference" class="headerlink" title="Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference"></a>Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference</h2><p><strong>Authors:Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han</strong></p>
<p>Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly â€œskim throughâ€ input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks. </p>
<blockquote>
<p>è™½ç„¶æ¶‰åŠé•¿ä¸Šä¸‹æ–‡è¾“å…¥çš„åº”ç”¨å¯¹äºæœ‰æ•ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šå¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ å’Œæ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆã€æ— éœ€è®­ç»ƒæç¤ºå‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å‹ç¼©æç¤ºä¸­ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬ç¡®å®šäº†åŸºäºtransformerçš„LLMä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼Œå°†å…¶æŒ‡å®šä¸ºè¯„ä¼°å¤´ï¼Œè¿™äº›è¯„ä¼°å¤´èƒ½å¤Ÿé€‰æ‹©é•¿è¾“å…¥ä¸­æœ€æœ‰åˆ©äºæ¨ç†çš„æ ‡è®°ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†EHPCï¼Œä¸€ç§åŸºäºè¯„ä¼°å¤´çš„æç¤ºå‹ç¼©æ–¹æ³•ï¼Œå®ƒä½¿LLMèƒ½å¤Ÿå€ŸåŠ©é¢„å¡«å……é˜¶æ®µä»…åˆ©ç”¨å‰å‡ å±‚ä¸­çš„è¯„ä¼°å¤´å¿«é€Ÿâ€œæµè§ˆâ€è¾“å…¥æç¤ºï¼Œéšååªå°†é‡è¦æ ‡è®°ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚EHPCåœ¨ä¸¤ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ï¼šæç¤ºå‹ç¼©å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿæ–¹é¢å–å¾—äº†æœ€æ–°ç»“æœã€‚å› æ­¤ï¼Œå®ƒæœ‰æ•ˆåœ°é™ä½äº†å•†ä¸šAPIè°ƒç”¨çš„å¤æ‚æ€§å’Œæˆæœ¬ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼ŒEHPCä¸åŸºäºé”®å€¼ç¼“å­˜çš„åŠ é€Ÿæ–¹æ³•ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œä»è€Œçªæ˜¾äº†å…¶åœ¨æé«˜LLMé•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12959v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥æ—¶é¢ä¸´è®¡ç®—æˆæœ¬å¢åŠ å’Œæ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒå…è´¹æç¤ºå‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å‹ç¼©æç¤ºçš„åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬ç¡®å®šäº†åŸºäºè½¬æ¢å™¨çš„LLMä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼Œç§°ä¸ºè¯„ä¼°å¤´ï¼Œè¿™äº›å¤´èƒ½å¤Ÿé€‰æ‹©é•¿è¾“å…¥ä¸­å¯¹æ¨ç†æœ€é‡è¦çš„æ ‡è®°ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬å¼€å‘äº†EHPCï¼ˆåŸºäºè¯„ä¼°å¤´çš„æç¤ºå‹ç¼©æ–¹æ³•ï¼‰ï¼Œä½¿LLMèƒ½å¤Ÿè¿…é€Ÿæµè§ˆè¾“å…¥æç¤ºï¼Œä»…åœ¨é¢„å¡«å……é˜¶æ®µä½¿ç”¨å‰å‡ å±‚å¸¦æœ‰è¯„ä¼°å¤´ï¼Œç„¶åä»…å°†é‡è¦æ ‡è®°ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚EHPCåœ¨ä¸¤ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œå³æç¤ºå‹ç¼©å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿã€‚è¿™æœ‰æ•ˆé™ä½äº†å•†ä¸šAPIè°ƒç”¨çš„å¤æ‚æ€§å’Œæˆæœ¬ï¼Œå¹¶ä¸åŸºäºé”®å€¼ç¼“å­˜çš„åŠ é€Ÿæ–¹æ³•å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚è¿™æ˜¾ç¤ºäº†EHPCåœ¨æé«˜LLMåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„æ•ˆç‡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡è¾“å…¥æ—¶é¢ä¸´è®¡ç®—æˆæœ¬å¢åŠ å’Œæ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è®­ç»ƒå…è´¹çš„æç¤ºå‹ç¼©æ–¹æ³•ï¼Œèƒ½å¤Ÿå‹ç¼©æç¤ºå¹¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ç¡®å®šç‰¹å®šçš„æ³¨æ„åŠ›å¤´â€”â€”è¯„ä¼°å¤´ï¼Œèƒ½å¤Ÿåœ¨LLMä¸­ç­›é€‰å‡ºå¯¹æ¨ç†æœ€é‡è¦çš„æ ‡è®°ã€‚</li>
<li>å¼€å‘äº†EHPCæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨è¯„ä¼°å¤´åœ¨å‰å‡ å±‚ä¸­è¿›è¡Œé¢„å¡«å……ï¼ŒåŠ é€Ÿäº†é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚</li>
<li>EHPCåœ¨æç¤ºå‹ç¼©å’Œé•¿ä¸Šä¸‹æ–‡æ¨ç†åŠ é€Ÿæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚</li>
<li>EHPCæœ‰æ•ˆé™ä½äº†å•†ä¸šAPIè°ƒç”¨çš„å¤æ‚æ€§åŠæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-06cd36deadccb545de4c6996e69bfe0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eba1403dcbbfb351ee57e35e73a30e87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9572ae14dec302578bcbbe122c02a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65dbbceecfa1aa6a382d18adcb6960dc.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="How-Developers-Interact-with-AI-A-Taxonomy-of-Human-AI-Collaboration-in-Software-Engineering"><a href="#How-Developers-Interact-with-AI-A-Taxonomy-of-Human-AI-Collaboration-in-Software-Engineering" class="headerlink" title="How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in   Software Engineering"></a>How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in   Software Engineering</h2><p><strong>Authors:Christoph Treude, Marco A. Gerosa</strong></p>
<p>Artificial intelligence (AI), including large language models and generative AI, is emerging as a significant force in software development, offering developers powerful tools that span the entire development lifecycle. Although software engineering research has extensively studied AI tools in software development, the specific types of interactions between developers and these AI-powered tools have only recently begun to receive attention. Understanding and improving these interactions has the potential to enhance productivity, trust, and efficiency in AI-driven workflows. In this paper, we propose a taxonomy of interaction types between developers and AI tools, identifying eleven distinct interaction types, such as auto-complete code suggestions, command-driven actions, and conversational assistance. Building on this taxonomy, we outline a research agenda focused on optimizing AI interactions, improving developer control, and addressing trust and usability challenges in AI-assisted development. By establishing a structured foundation for studying developer-AI interactions, this paper aims to stimulate research on creating more effective, adaptive AI tools for software development. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼AIï¼Œæ­£åœ¨è½¯ä»¶å¼€å‘é¢†åŸŸå´­éœ²å¤´è§’ï¼Œä¸ºå¼€å‘è€…æä¾›è´¯ç©¿æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å¼ºå¤§å·¥å…·ã€‚å°½ç®¡è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·²ç»å¯¹è½¯ä»¶å¼€å‘ä¸­çš„AIå·¥å…·è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¼€å‘è€…ä¸è¿™äº›AIå·¥å…·ä¹‹é—´çš„ç‰¹å®šäº¤äº’ç±»å‹æœ€è¿‘æ‰å¼€å§‹å—åˆ°å…³æ³¨ã€‚ç†è§£å’Œæ”¹è¿›è¿™äº›äº¤äº’æœ‰å¯èƒ½æé«˜AIé©±åŠ¨å·¥ä½œæµç¨‹ä¸­çš„ç”Ÿäº§åŠ›ã€ä¿¡ä»»å’Œæ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼€å‘è€…ä¸AIå·¥å…·ä¹‹é—´äº¤äº’ç±»å‹çš„åˆ†ç±»ï¼Œè¯†åˆ«äº†åä¸€ç§ä¸åŒçš„äº¤äº’ç±»å‹ï¼Œå¦‚è‡ªåŠ¨å®Œæˆä»£ç å»ºè®®ã€å‘½ä»¤é©±åŠ¨æ“ä½œå’Œå¯¹è¯è¾…åŠ©ã€‚åŸºäºè¿™ç§åˆ†ç±»ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸€é¡¹ä»¥ä¼˜åŒ–AIäº¤äº’ã€æ”¹å–„å¼€å‘è€…æ§åˆ¶å’Œè§£å†³AIè¾…åŠ©å¼€å‘ä¸­çš„ä¿¡ä»»å’Œå¯ç”¨æ€§æŒ‘æˆ˜ä¸ºé‡ç‚¹çš„ç ”ç©¶è®®ç¨‹ã€‚é€šè¿‡ä¸ºç ”ç©¶å¼€å‘è€…ä¸AIä¹‹é—´çš„äº¤äº’å»ºç«‹ç»“æ„åŒ–çš„åŸºç¡€ï¼Œæœ¬æ–‡æ—¨åœ¨åˆºæ¿€å…³äºåˆ›å»ºæ›´æœ‰æ•ˆã€è‡ªé€‚åº”çš„AIå·¥å…·ç”¨äºè½¯ä»¶å¼€å‘çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08774v2">PDF</a> Accepted at 2nd ACM International Conference on AI Foundation Models   and Software Engineering (FORGE 2025)</p>
<p><strong>Summary</strong>ï¼šäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼AIï¼Œæ­£åœ¨è½¯ä»¶å¼€å‘é¢†åŸŸå´­éœ²å¤´è§’ï¼Œä¸ºå¼€å‘è€…æä¾›è´¯ç©¿æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å¼ºå¤§å·¥å…·ã€‚å°½ç®¡è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·²å¯¹AIå·¥å…·åœ¨è½¯ä»¶å¼€å‘ä¸­çš„åº”ç”¨è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ï¼Œä½†å¼€å‘è€…ä¸è¿™äº›AIå·¥å…·ä¹‹é—´çš„ç‰¹å®šäº¤äº’ç±»å‹æœ€è¿‘æ‰å¼€å§‹å—åˆ°å…³æ³¨ã€‚ç†è§£å’Œæ”¹è¿›è¿™äº›äº¤äº’æœ‰æœ›æé«˜AIé©±åŠ¨å·¥ä½œæµç¨‹ä¸­çš„ç”Ÿäº§åŠ›ã€ä¿¡ä»»å’Œæ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†å¼€å‘è€…ä¸AIå·¥å…·ä¹‹é—´äº¤äº’ç±»å‹çš„åˆ†ç±»ï¼Œç¡®å®šäº†åŒ…æ‹¬è‡ªåŠ¨å®Œæˆä»£ç å»ºè®®ã€å‘½ä»¤é©±åŠ¨æ“ä½œå’Œå¯¹è¯è¾…åŠ©ç­‰åä¸€ç§ä¸åŒçš„äº¤äº’ç±»å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¼˜åŒ–AIäº¤äº’ã€æ”¹å–„å¼€å‘è€…æ§åˆ¶å’Œè§£å†³AIè¾…åŠ©å¼€å‘ä¸­çš„ä¿¡ä»»å’Œå¯ç”¨æ€§æŒ‘æˆ˜ä¸ºé‡ç‚¹çš„ç ”ç©¶è®®ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ä¸ºå¼€å‘è€…ä¸AIä¹‹é—´çš„äº¤äº’å»ºç«‹ç»“æ„åŒ–åŸºç¡€ï¼Œåˆºæ¿€å…³äºåˆ›å»ºæ›´æœ‰æ•ˆã€é€‚åº”æ€§æ›´å¼ºçš„AIå·¥å…·çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AIåœ¨è½¯ä»¶å¼€å‘ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæä¾›è´¯ç©¿æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å¼ºå¤§å·¥å…·ã€‚</li>
<li>å¼€å‘è€…ä¸AIå·¥å…·çš„äº¤äº’ç±»å‹å¼€å§‹å—åˆ°å…³æ³¨ï¼Œè¿™å¯¹æé«˜AIå·¥ä½œæµç¨‹çš„ç”Ÿäº§åŠ›ã€ä¿¡ä»»å’Œæ•ˆç‡æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€å‘è€…ä¸AIå·¥å…·ä¹‹é—´äº¤äº’ç±»å‹çš„åˆ†ç±»ï¼ŒåŒ…æ‹¬åä¸€ç§ä¸åŒçš„äº¤äº’ç±»å‹ï¼Œå¦‚è‡ªåŠ¨å®Œæˆä»£ç å»ºè®®ã€å‘½ä»¤é©±åŠ¨æ“ä½œå’Œå¯¹è¯è¾…åŠ©ç­‰ã€‚</li>
<li>éœ€è¦ä¼˜åŒ–AIäº¤äº’ï¼Œæ”¹å–„å¼€å‘è€…å¯¹AIå·¥å…·çš„æ§åˆ¶ã€‚</li>
<li>è§£å†³AIè¾…åŠ©å¼€å‘ä¸­çš„ä¿¡ä»»å’Œå¯ç”¨æ€§æŒ‘æˆ˜æ˜¯ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨å»ºç«‹ç ”ç©¶å¼€å‘è€…ä¸AIä¹‹é—´äº¤äº’çš„ç»“æ„åŒ–åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b90fbc63425f1ea41f2c4aca41608c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c06179a5a585d53087def5aad5e7c27.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Simple-Is-Effective-The-Roles-of-Graphs-and-Large-Language-Models-in-Knowledge-Graph-Based-Retrieval-Augmented-Generation"><a href="#Simple-Is-Effective-The-Roles-of-Graphs-and-Large-Language-Models-in-Knowledge-Graph-Based-Retrieval-Augmented-Generation" class="headerlink" title="Simple Is Effective: The Roles of Graphs and Large Language Models in   Knowledge-Graph-Based Retrieval-Augmented Generation"></a>Simple Is Effective: The Roles of Graphs and Large Language Models in   Knowledge-Graph-Based Retrieval-Augmented Generation</h2><p><strong>Authors:Mufei Li, Siqi Miao, Pan Li</strong></p>
<p>Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the queryâ€™s need and the downstream LLMâ€™s capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines â€“ all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAGâ€™s strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨è¯¸å¦‚å¹»è§‰å’ŒçŸ¥è¯†å‚¨å¤‡è¿‡æ—¶ç­‰å±€é™æ€§ã€‚åŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å°†LLMè¾“å‡ºæ ¹æ¤äºçŸ¥è¯†å›¾è°±ä¸­çš„ç»“æ„åŒ–å¤–éƒ¨çŸ¥è¯†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºçŸ¥è¯†å›¾è°±çš„RAGæ¡†æ¶åœ¨ä¼˜åŒ–æ£€ç´¢æœ‰æ•ˆæ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡æ—¶ï¼Œä»ç„¶éš¾ä»¥ç¡®å®šé€‚åˆLLMæ¶ˆåŒ–çš„å¤§é‡ç›¸å…³å›¾ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥äº†å­å›¾RAGï¼Œæ‰©å±•äº†åŸºäºçŸ¥è¯†å›¾è°±çš„RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ£€ç´¢å­å›¾å¹¶åˆ©ç”¨LLMè¿›è¡Œæ¨ç†å’Œç­”æ¡ˆé¢„æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›æ–°åœ°æ•´åˆäº†è½»é‡çº§å¤šå±‚æ„ŸçŸ¥å™¨å’Œå¹¶è¡Œä¸‰å…ƒè¯„åˆ†æœºåˆ¶ï¼Œä»¥å®ç°é«˜æ•ˆä¸”çµæ´»çš„å­å›¾æ£€ç´¢ï¼ŒåŒæ—¶ç¼–ç æ–¹å‘ç»“æ„è·ç¦»ä»¥æé«˜æ£€ç´¢æ•ˆç‡ã€‚æ£€ç´¢åˆ°çš„å­å›¾å¤§å°å¯ä»¥æ ¹æ®æŸ¥è¯¢çš„éœ€è¦å’Œä¸‹æ¸¸LLMçš„èƒ½åŠ›è¿›è¡Œçµæ´»è°ƒæ•´ã€‚è¿™ç§è®¾è®¡åœ¨æ¨¡å‹å¤æ‚åº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œå®ç°äº†å¯æ‰©å±•ä¸”é€šç”¨çš„æ£€ç´¢è¿‡ç¨‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºæˆ‘ä»¬æ£€ç´¢åˆ°çš„å­å›¾ï¼Œè¾ƒå°çš„LLMï¼ˆå¦‚Llama3.1-8B-Instructï¼‰èƒ½å¤Ÿå±•ç°å‡ºå…·æœ‰è§£é‡Šæ€§çš„æ¨ç†ç»“æœï¼Œè€Œè¾ƒå¤§çš„æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰åˆ™è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„å‡†ç¡®æ€§â€”â€”æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½ä¸éœ€è¦è¿›è¡Œå¾®è°ƒã€‚åœ¨WebQSPå’ŒCWQåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œçªæ˜¾äº†å­å›¾RAGåœ¨æ•ˆç‡ã€å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå®ƒå‡å°‘äº†å¹»è§‰å¹¶æ”¹å–„äº†å“åº”ä¾æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20724v4">PDF</a> Accepted by ICLR 2025; Code available at   <a target="_blank" rel="noopener" href="https://github.com/Graph-COM/SubgraphRAG">https://github.com/Graph-COM/SubgraphRAG</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¹»è±¡å’ŒçŸ¥è¯†è¿‡æ—¶ç­‰é—®é¢˜ã€‚åŸºäºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡ç»“åˆå¤–éƒ¨ç»“æ„åŒ–çŸ¥è¯†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰KG-based RAGæ¡†æ¶åœ¨ä¼˜åŒ–æ£€ç´¢æœ‰æ•ˆæ€§ä¸æ•ˆç‡ä¹‹é—´ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºSubgraphRAGï¼Œæ‰©å±•äº†KG-based RAGæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å­å›¾å¹¶åˆ©ç”¨LLMè¿›è¡Œæ¨ç†å’Œç­”æ¡ˆé¢„æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºå’Œå¹¶è¡Œä¸‰å…ƒç»„è¯„åˆ†æœºåˆ¶å®ç°é«˜æ•ˆçµæ´»çš„å­å›¾æ£€ç´¢ï¼ŒåŒæ—¶ç¼–ç æ–¹å‘ç»“æ„è·ç¦»ä»¥å¢å¼ºæ£€ç´¢æ•ˆæœã€‚å­å›¾å¤§å°å¯æ ¹æ®æŸ¥è¯¢éœ€æ±‚å’Œä¸‹æ¸¸LLMèƒ½åŠ›çµæ´»è°ƒæ•´ã€‚è¿™ä¸€è®¾è®¡åœ¨æ¨¡å‹å¤æ‚åº¦å’Œæ¨ç†èƒ½åŠ›ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œå®ç°äº†å¯ä¼¸ç¼©å’Œé€šç”¨çš„æ£€ç´¢è¿‡ç¨‹ã€‚åœ¨WebQSPå’ŒCWQåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSubgraphRAGåœ¨æ•ˆç‡ã€å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå‡å°‘äº†å¹»è±¡ï¼Œæé«˜äº†å“åº”çš„æ¥åœ°æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¹»è±¡å’ŒçŸ¥è¯†è¿‡æ—¶é—®é¢˜ã€‚</li>
<li>KG-based RAGæ–¹æ³•é€šè¿‡ç»“åˆå¤–éƒ¨ç»“æ„åŒ–çŸ¥è¯†è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ç°æœ‰KG-based RAGæ¡†æ¶åœ¨ä¼˜åŒ–æ£€ç´¢æœ‰æ•ˆæ€§ä¸æ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SubgraphRAGé€šè¿‡æ£€ç´¢å­å›¾å¹¶åˆ©ç”¨LLMè¿›è¡Œæ¨ç†å’Œç­”æ¡ˆé¢„æµ‹æ¥æ‰©å±•KG-based RAGæ¡†æ¶ã€‚</li>
<li>SubgraphRAGå®ç°äº†é«˜æ•ˆçµæ´»çš„å­å›¾æ£€ç´¢ï¼Œé€šè¿‡è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºå’Œå¹¶è¡Œä¸‰å…ƒç»„è¯„åˆ†æœºåˆ¶ã€‚</li>
<li>SubgraphRAGé€šè¿‡ç¼–ç æ–¹å‘ç»“æ„è·ç¦»ä»¥å¢å¼ºæ£€ç´¢æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ac5c4ef762854f026e8b14179792cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-839141b697461c1858cff237274c519f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-455ab4c204fdce1aeec82d4c5085fb36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cd3e0faa10ca71d48512eed53e9ead0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f6ac0aaa65f9ad8a5c420dfa34607a0b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  ReachAgent Enhancing Mobile Agent via Page Reaching and Operation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-06/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8403c7fc132779a8f63d5454c54bb394.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-06  CASIM Composite Aware Semantic Injection for Text to Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15230.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
