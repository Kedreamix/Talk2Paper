<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Training-Free Text-Guided Color Editing with Multi-Modal Diffusion   Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-803ce6d4e16748d387890020f2bd54c5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-14-æ›´æ–°"><a href="#2025-08-14-æ›´æ–°" class="headerlink" title="2025-08-14 æ›´æ–°"></a>2025-08-14 æ›´æ–°</h1><h2 id="Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer"><a href="#Training-Free-Text-Guided-Color-Editing-with-Multi-Modal-Diffusion-Transformer" class="headerlink" title="Training-Free Text-Guided Color Editing with Multi-Modal Diffusion   Transformer"></a>Training-Free Text-Guided Color Editing with Multi-Modal Diffusion   Transformer</h2><p><strong>Authors:Zixin Yin, Xili Dai, Ling-Hao Chen, Deyu Zhou, Jianan Wang, Duomin Wang, Gang Yu, Lionel M. Ni, Heung-Yeung Shum</strong></p>
<p>Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility. </p>
<blockquote>
<p>æ–‡æœ¬ä¸­çš„å›¾åƒå’Œè§†é¢‘å¯¼å‘è‰²å½©ç¼–è¾‘æ˜¯ä¸€ä¸ªåŸºç¡€ä½†å°šæœªè§£å†³çš„é—®é¢˜ï¼Œå®ƒè¦æ±‚å¯¹è‰²å½©å±æ€§è¿›è¡Œç²¾ç»†çš„æ“æ§ï¼ŒåŒ…æ‹¬è¡¨é¢äº®åº¦ã€å…‰æºé¢œè‰²å’Œå‘¨å›´ç¯å¢ƒç…§æ˜ï¼ŒåŒæ—¶ä¿æŒå‡ ä½•ç»“æ„ã€æè´¨ç‰¹æ€§å’Œå…‰çº¿ç‰©è´¨äº¤äº’çš„ç‰©ç†ä¸€è‡´æ€§ã€‚ç°æœ‰çš„æ— è®­ç»ƒæ–¹æ³•åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œä½†åœ¨ç²¾ç¡®è‰²å½©æ§åˆ¶æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”åœ¨ç¼–è¾‘å’Œéç¼–è¾‘åŒºåŸŸéƒ½å¼•å…¥äº†è§†è§‰ä¸ä¸€è‡´æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ColorCtrlï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒçš„è‰²å½©ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç°ä»£å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMM-DiTï¼‰çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§åœ°æ“ä½œæ³¨æ„åŠ›å›¾å’Œå€¼ä»¤ç‰Œæ¥åˆ†ç¦»ç»“æ„å’Œè‰²å½©ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ç²¾ç¡®ä¸”ä¸€è‡´çš„é¢œè‰²ç¼–è¾‘ï¼Œä»¥åŠå±æ€§å¼ºåº¦çš„è¯æ±‡çº§æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…ä¿®æ”¹æç¤ºæŒ‡å®šçš„æ„å›¾åŒºåŸŸï¼Œè€Œä¸å½±å“æ— å…³åŒºåŸŸã€‚åœ¨SD3å’ŒFLUX.1-devä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒColorCtrlä¼˜äºç°æœ‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åœ¨ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†å¼ºå¤§çš„å•†ä¸šæ¨¡å‹ï¼Œå¦‚FLUX.1 Kontext Maxå’ŒGPT-4oå›¾åƒç”Ÿæˆã€‚å½“æ‰©å±•åˆ°è§†é¢‘æ¨¡å‹ï¼ˆå¦‚CogVideoXï¼‰æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæ›´å¤§çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒæ—¶é—´è¿è´¯æ€§å’Œç¼–è¾‘ç¨³å®šæ€§æ–¹é¢ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜é€‚ç”¨äºåŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Step1X-Editå’ŒFLUX.1 Kontext devï¼‰ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†å…¶é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ— è®­ç»ƒçš„å¤šæ¨¡æ€æ‰©æ•£Transformerï¼ˆMM-DiTï¼‰çš„ColorCtrlé¢œè‰²ç¼–è¾‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ“çºµæ³¨æ„åŠ›å›¾å’Œå€¼ä»¤ç‰Œæ¥åˆ†ç¦»ç»“æ„å’Œé¢œè‰²ï¼Œå®ç°äº†ç²¾ç¡®ä¸”ä¸€è‡´çš„é¢œè‰²ç¼–è¾‘ï¼Œå¹¶å…·å¤‡è¯è¯­çº§åˆ«çš„å±æ€§å¼ºåº¦æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒColorCtrlåœ¨ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢è¶…è¶Šç°æœ‰æ— è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åœ¨è§†é¢‘æ¨¡å‹å¦‚CogVideoXä¸Šå±•ç°å‡ºæ›´å¤§çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒæ—¶é—´è¿è´¯æ€§å’Œç¼–è¾‘ç¨³å®šæ€§æ–¹é¢ã€‚æ­¤å¤–ï¼ŒColorCtrlè¿˜é€‚ç”¨äºåŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æ‰©æ•£æ¨¡å‹ï¼Œå¦‚Step1X-Editå’ŒFLUX.1 Kontext devï¼Œæ˜¾ç¤ºå‡ºå…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ColorCtrlæ˜¯ä¸€ç§åŸºäºæ— è®­ç»ƒçš„å¤šæ¨¡æ€æ‰©æ•£Transformerï¼ˆMM-DiTï¼‰çš„é¢œè‰²ç¼–è¾‘æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ“çºµæ³¨æ„åŠ›å›¾å’Œå€¼ä»¤ç‰Œï¼ŒColorCtrlå®ç°äº†ç»“æ„å’Œé¢œè‰²çš„åˆ†ç¦»ã€‚</li>
<li>ColorCtrlå®ç°äº†ç²¾ç¡®ä¸”ä¸€è‡´çš„é¢œè‰²ç¼–è¾‘ï¼Œå…·å¤‡è¯è¯­çº§åˆ«çš„å±æ€§å¼ºåº¦æ§åˆ¶ã€‚</li>
<li>ä¸ç°æœ‰æ— è®­ç»ƒæ–¹æ³•å’Œå•†ä¸šæ¨¡å‹ç›¸æ¯”ï¼ŒColorCtrlåœ¨ç¼–è¾‘è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ColorCtrlåœ¨è§†é¢‘æ¨¡å‹ä¸Šçš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿä¿æŒæ—¶é—´è¿è´¯æ€§å’Œç¼–è¾‘ç¨³å®šæ€§ã€‚</li>
<li>ColorCtrlé€‚ç”¨äºåŸºäºæŒ‡ä»¤çš„ç¼–è¾‘æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cea5a83541288423f15a331d31abbf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31da892fc85202f43fd5a84187814e33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86d63eeff8a1416ee8398d83186d35ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6728fba4c29fa5a198d9475b12861b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1c09f1f1dab7f975e4bed96207a54a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd7955e4a8ea4cc42168b55e09840676.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models"><a href="#TARA-Token-Aware-LoRA-for-Composable-Personalization-in-Diffusion-Models" class="headerlink" title="TARA: Token-Aware LoRA for Composable Personalization in Diffusion   Models"></a>TARA: Token-Aware LoRA for Composable Personalization in Diffusion   Models</h2><p><strong>Authors:Yuqi Peng, Lingtao Zheng, Yufeng Yang, Yi Huang, Mingfu Yan, Jianzhuang Liu, Shifeng Chen</strong></p>
<p>Personalized text-to-image generation aims to synthesize novel images of a specific subject or style using only a few reference images. Recent methods based on Low-Rank Adaptation (LoRA) enable efficient single-concept customization by injecting lightweight, concept-specific adapters into pre-trained diffusion models. However, combining multiple LoRA modules for multi-concept generation often leads to identity missing and visual feature leakage. In this work, we identify two key issues behind these failures: (1) token-wise interference among different LoRA modules, and (2) spatial misalignment between the attention map of a rare token and its corresponding concept-specific region. To address these issues, we propose Token-Aware LoRA (TARA), which introduces a token mask to explicitly constrain each module to focus on its associated rare token to avoid interference, and a training objective that encourages the spatial attention of a rare token to align with its concept region. Our method enables training-free multi-concept composition by directly injecting multiple independently trained TARA modules at inference time. Experimental results demonstrate that TARA enables efficient multi-concept inference and effectively preserving the visual identity of each concept by avoiding mutual interference between LoRA modules. The code and models are available at <a target="_blank" rel="noopener" href="https://github.com/YuqiPeng77/TARA">https://github.com/YuqiPeng77/TARA</a>. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ—¨åœ¨ä½¿ç”¨å°‘é‡å‚è€ƒå›¾åƒåˆæˆç‰¹å®šä¸»é¢˜æˆ–é£æ ¼çš„å…¨æ–°å›¾åƒã€‚æœ€è¿‘åŸºäºä½ç§©é€‚é…ï¼ˆLoRAï¼‰çš„æ–¹æ³•é€šè¿‡å‘é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ³¨å…¥è½»é‡çº§ã€æ¦‚å¿µç‰¹å®šçš„é€‚é…å™¨ï¼Œå®ç°äº†é«˜æ•ˆçš„å•æ¦‚å¿µå®šåˆ¶ã€‚ç„¶è€Œï¼Œä¸ºå¤šæ¦‚å¿µç”Ÿæˆç»„åˆå¤šä¸ªLoRAæ¨¡å—é€šå¸¸ä¼šå¯¼è‡´èº«ä»½ç¼ºå¤±å’Œè§†è§‰ç‰¹å¾æ³„éœ²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºè¿™äº›å¤±è´¥èƒŒåçš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰ä¸åŒLoRAæ¨¡å—ä¹‹é—´çš„æ ‡è®°çº§å¹²æ‰°ï¼›ï¼ˆ2ï¼‰ç¨€æœ‰æ ‡è®°çš„æ³¨æ„åŠ›å›¾ä¸å…¶å¯¹åº”çš„æ¦‚å¿µç‰¹å®šåŒºåŸŸä¹‹é—´çš„ç©ºé—´ä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tokenæ„ŸçŸ¥LoRAï¼ˆTARAï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªæ ‡è®°æ©ç æ¥æ˜¾å¼çº¦æŸæ¯ä¸ªæ¨¡å—ä¸“æ³¨äºå…¶ç›¸å…³çš„ç¨€æœ‰æ ‡è®°ï¼Œä»¥é¿å…å¹²æ‰°ï¼Œä»¥åŠä¸€ä¸ªè®­ç»ƒç›®æ ‡ï¼Œé¼“åŠ±ç¨€æœ‰æ ‡è®°çš„ç©ºé—´æ³¨æ„åŠ›ä¸å…¶æ¦‚å¿µåŒºåŸŸå¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨æ¨ç†æ—¶é—´ç›´æ¥æ³¨å…¥å¤šä¸ªç‹¬ç«‹è®­ç»ƒçš„TARAæ¨¡å—ï¼Œå®ç°äº†æ— éœ€è®­ç»ƒçš„å¤šæ¦‚å¿µç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTARAèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„å¤šæ¦‚å¿µæ¨ç†ï¼Œå¹¶æœ‰æ•ˆåœ°ä¿ç•™æ¯ä¸ªæ¦‚å¿µçš„èº«ä»½ç‰¹å¾ï¼Œé¿å…äº†LoRAæ¨¡å—ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YuqiPeng77/TARA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YuqiPeng77/TARAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08812v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬ç”Ÿæˆçš„ä¸ªæ€§åŒ–å›¾åƒåˆæˆæ–¹æ³•é€šè¿‡ä½¿ç”¨å°‘é‡å‚è€ƒå›¾åƒåˆæˆç‰¹å®šä¸»é¢˜æˆ–é£æ ¼çš„å›¾åƒã€‚è™½ç„¶ç°æœ‰çš„åŸºäºä½ç§©é€‚é…ï¼ˆLoRAï¼‰çš„æ–¹æ³•å¯ä»¥å®ç°å•ä¸€æ¦‚å¿µå®šåˆ¶ï¼Œä½†åœ¨å¤šæ¦‚å¿µç”Ÿæˆæ—¶ä¼šå‡ºç°èº«ä»½ç¼ºå¤±å’Œè§†è§‰ç‰¹å¾æ³„éœ²çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶é’ˆå¯¹è¿™äº›é—®é¢˜æå‡ºäº†Token-Aware LoRAï¼ˆTARAï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥tokenæ©ç å’Œè®­ç»ƒç›®æ ‡æ¥è§£å†³æ¨¡å—é—´çš„å¹²æ‰°å’Œç©ºé—´ä¸åŒ¹é…é—®é¢˜ã€‚TARAæ–¹æ³•å®ç°äº†æ— éœ€è®­ç»ƒçš„å¤šæ¦‚å¿µç»„åˆï¼Œé€šè¿‡ç›´æ¥åœ¨æ¨ç†æ—¶é—´æ³¨å…¥å¤šä¸ªç‹¬ç«‹è®­ç»ƒçš„TARAæ¨¡å—ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šæ¦‚å¿µæ¨ç†ï¼Œå¹¶æœ‰æ•ˆä¿ç•™äº†æ¯ä¸ªæ¦‚å¿µçš„è§†è§‰èº«ä»½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ç”Ÿæˆçš„ä¸ªæ€§åŒ–å›¾åƒåˆæˆæ–¹æ³•ä½¿ç”¨å°‘é‡å‚è€ƒå›¾åƒåˆæˆç‰¹å®šä¸»é¢˜æˆ–é£æ ¼çš„å›¾åƒã€‚</li>
<li>åŸºäºä½ç§©é€‚é…ï¼ˆLoRAï¼‰çš„æ–¹æ³•å¯å®ç°å•ä¸€æ¦‚å¿µå®šåˆ¶ï¼Œä½†åœ¨å¤šæ¦‚å¿µç”Ÿæˆæ—¶å­˜åœ¨é—®é¢˜ã€‚</li>
<li>å¤šæ¦‚å¿µç”Ÿæˆå¤±è´¥çš„ä¸»è¦åŸå› åŒ…æ‹¬ä¸åŒLoRAæ¨¡å—é—´çš„token-wiseå¹²æ‰°å’Œç©ºé—´ä¸åŒ¹é…ã€‚</li>
<li>TARAæ–¹æ³•é€šè¿‡å¼•å…¥tokenæ©ç å’Œè®­ç»ƒç›®æ ‡æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>TARAå®ç°äº†æ— éœ€è®­ç»ƒçš„å¤šæ¦‚å¿µç»„åˆï¼Œæé«˜äº†å¤šæ¦‚å¿µæ¨ç†çš„æ•ˆç‡ã€‚</li>
<li>TARAèƒ½å¤Ÿä¿ç•™æ¯ä¸ªæ¦‚å¿µçš„è§†è§‰èº«ä»½ï¼Œé¿å…äº†ç›¸äº’å¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4a8a43b288e2bd45e68481fe1f17966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c421c148ef3d4c9f5ce7c8599b9361b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa985ccbd9281f2d1778cd3b1a10035b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2f74aab8afcb06efe227e22f0742f51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42eac3d383c736ed5995609cdb02ca27.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Patient-Adaptive-Focused-Transmit-Beamforming-using-Cognitive-Ultrasound"><a href="#Patient-Adaptive-Focused-Transmit-Beamforming-using-Cognitive-Ultrasound" class="headerlink" title="Patient-Adaptive Focused Transmit Beamforming using Cognitive Ultrasound"></a>Patient-Adaptive Focused Transmit Beamforming using Cognitive Ultrasound</h2><p><strong>Authors:Wessel L. van Nierop, OisÃ­n Nolan, Tristan S. W. Stevens, Ruud J. G. van Sloun</strong></p>
<p>Focused transmit beamforming is the most commonly used acquisition scheme for echocardiograms, but suffers from relatively low frame rates, and in 3D, even lower volume rates. Fast imaging based on unfocused transmits has disadvantages such as motion decorrelation and limited harmonic imaging capabilities. This work introduces a patient-adaptive focused transmit scheme that has the ability to drastically reduce the number of transmits needed to produce a high-quality ultrasound image. The method relies on posterior sampling with a temporal diffusion model to perceive and reconstruct the anatomy based on partial observations, while subsequently taking an action to acquire the most informative transmits. This active perception modality outperforms random and equispaced subsampling on the 2D EchoNet-Dynamic dataset and a 3D Philips dataset, where we actively select focused elevation planes. Furthermore, we show it achieves better performance in terms of generalized contrast-to-noise ratio when compared to the same number of diverging waves transmits on three in-house echocardiograms. Additionally, we can estimate ejection fraction using only 2% of the total transmits and show that the method is robust to outlier patients. Finally, our method can be run in real-time on GPU accelerators from 2023. The code is publicly available at <a target="_blank" rel="noopener" href="https://tue-bmd.github.io/ulsa/">https://tue-bmd.github.io/ulsa/</a> </p>
<blockquote>
<p>èšç„¦å‘å°„æ³¢æŸå½¢æˆæ˜¯è¶…å£°å¿ƒåŠ¨å›¾æœ€å¸¸ç”¨çš„é‡‡é›†æ–¹æ¡ˆï¼Œä½†å…¶å¸§ç‡è¾ƒä½ï¼Œåœ¨ä¸‰ç»´æƒ…å†µä¸‹ä½“ç§¯é€Ÿç‡æ›´ä½ã€‚åŸºäºéèšç„¦å‘å°„çš„å¿«é€Ÿæˆåƒå­˜åœ¨è¿åŠ¨å»ç›¸å…³å’Œè°æ³¢æˆåƒèƒ½åŠ›æœ‰é™ç­‰ç¼ºç‚¹ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§ç—…äººè‡ªé€‚åº”çš„èšç„¦å‘å°„æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¤§å¹…åº¦å‡å°‘äº§ç”Ÿé«˜è´¨é‡è¶…å£°å›¾åƒæ‰€éœ€è¦çš„å‘å°„æ¬¡æ•°ã€‚è¯¥æ–¹æ³•ä¾èµ–äºåé‡‡æ ·å’Œæ—¶åºæ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºéƒ¨åˆ†è§‚å¯Ÿæ¥æ„ŸçŸ¥å’Œé‡å»ºç»“æ„ï¼Œç„¶åé‡‡å–è¡ŒåŠ¨è·å–æœ€å…·ä¿¡æ¯é‡çš„å‘å°„ã€‚è¿™ç§ä¸»åŠ¨æ„ŸçŸ¥æ¨¡å¼åœ¨EchoNet-DynamicäºŒç»´æ•°æ®é›†å’Œé£åˆ©æµ¦ä¸‰ç»´æ•°æ®é›†ä¸Šä¼˜äºéšæœºå’Œç­‰è·å­é‡‡æ ·ï¼Œæˆ‘ä»¬ä¸»åŠ¨é€‰æ‹©èšç„¦çš„ä»°è§’å¹³é¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºå…¶åœ¨å¹¿ä¹‰å¯¹æ¯”å™ªå£°æ¯”æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œåœ¨ä¸ä¸‰ç§å†…éƒ¨è¶…å£°å¿ƒåŠ¨å›¾çš„å‘æ•£æ³¢å‘å°„ç›¸åŒæ•°é‡çš„æƒ…å†µä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨æ€»å‘å°„æ¬¡æ•°çš„2%å³å¯ä¼°ç®—å°„è¡€åˆ†æ•°ï¼Œå¹¶è¯æ˜è¯¥æ–¹æ³•å¯¹å¼‚å¸¸æ‚£è€…å…·æœ‰ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨2023å¹´çš„GPUåŠ é€Ÿå™¨ä¸Šå®æ—¶è¿è¡Œã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://tue-bmd.github.io/ulsa/">https://tue-bmd.github.io/ulsa/</a>ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç—…äººè‡ªé€‚åº”çš„èšç„¦å‘å°„æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿå¤§å¤§å‡å°‘äº§ç”Ÿé«˜è´¨é‡è¶…å£°å›¾åƒæ‰€éœ€çš„å‘å°„æ¬¡æ•°ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åé‡‡æ ·å’Œä¸´æ—¶æ‰©æ•£æ¨¡å‹æ¥æ„ŸçŸ¥å’Œé‡å»ºç»“æ„ï¼Œç„¶åé‡‡å–åŠ¨ä½œè·å–æœ€å…·ä¿¡æ¯é‡çš„å‘å°„ã€‚æ­¤ä¸»åŠ¨æ„ŸçŸ¥æ¨¡å¼åœ¨2D EchoNet-Dynamicæ•°æ®é›†å’Œ3Dé£åˆ©æµ¦æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºéšæœºå’Œç­‰è·å­é‡‡æ ·çš„æ€§èƒ½ï¼Œå¹¶èƒ½å®ç°æ›´å¥½çš„å¹¿ä¹‰å¯¹æ¯”å™ªå£°æ¯”ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨æ€»å‘å°„é‡çš„2%å³å¯ä¼°è®¡å°„è¡€åˆ†æ•°ï¼Œä¸”å¯¹å¼‚å¸¸æ‚£è€…å…·æœ‰ç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•å¯åœ¨2023å¹´çš„GPUåŠ é€Ÿå™¨ä¸Šå®æ—¶è¿è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç—…äººè‡ªé€‚åº”èšç„¦å‘å°„æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¤§å¹…åº¦å‡å°‘äº§ç”Ÿé«˜è´¨é‡è¶…å£°å›¾åƒæ‰€éœ€çš„å‘å°„æ¬¡æ•°ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåˆ©ç”¨åé‡‡æ ·å’Œä¸´æ—¶æ‰©æ•£æ¨¡å‹æ¥æ„ŸçŸ¥å’Œé‡å»ºç»“æ„ã€‚</li>
<li>ä¸»åŠ¨æ„ŸçŸ¥æ¨¡å¼åœ¨2Då’Œ3Dæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºéšæœºå’Œç­‰è·å­é‡‡æ ·çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨å¹¿ä¹‰å¯¹æ¯”å™ªå£°æ¯”æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>ä»…ä½¿ç”¨å°‘é‡å‘å°„å³å¯ä¼°è®¡å°„è¡€åˆ†æ•°ã€‚</li>
<li>è¯¥æ–¹æ¡ˆå¯¹å¼‚å¸¸æ‚£è€…å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f08d3e524f6970c80b4d52f6b447a70c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1611e8a3d0ef078cf5243c3e2728ce8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2fd0d1e21f7201bcd7cc82cc9044ab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-709afebca3f564696cd1b40b1a324814.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-Palette-based-Color-Guidance-in-Diffusion-Models"><a href="#Exploring-Palette-based-Color-Guidance-in-Diffusion-Models" class="headerlink" title="Exploring Palette based Color Guidance in Diffusion Models"></a>Exploring Palette based Color Guidance in Diffusion Models</h2><p><strong>Authors:Qianru Qiu, Jiafeng Mao, Xueting Wang</strong></p>
<p>With the advent of diffusion models, Text-to-Image (T2I) generation has seen substantial advancements. Current T2I models allow users to specify object colors using linguistic color names, and some methods aim to personalize color-object association through prompt learning. However, existing models struggle to provide comprehensive control over the color schemes of an entire image, especially for background elements and less prominent objects not explicitly mentioned in prompts. This paper proposes a novel approach to enhance color scheme control by integrating color palettes as a separate guidance mechanism alongside prompt instructions. We investigate the effectiveness of palette guidance by exploring various palette representation methods within a diffusion-based image colorization framework. To facilitate this exploration, we construct specialized palette-text-image datasets and conduct extensive quantitative and qualitative analyses. Our results demonstrate that incorporating palette guidance significantly improves the modelâ€™s ability to generate images with desired color schemes, enabling a more controlled and refined colorization process. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„ç”ŸæˆæŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ã€‚å½“å‰çš„T2Iæ¨¡å‹å…è®¸ç”¨æˆ·ä½¿ç”¨è¯­è¨€ä¸­çš„é¢œè‰²åç§°æ¥æŒ‡å®šå¯¹è±¡é¢œè‰²ï¼Œä¸€äº›æ–¹æ³•æ—¨åœ¨é€šè¿‡æç¤ºå­¦ä¹ æ¥å®ç°é¢œè‰²ä¸å¯¹è±¡çš„å…³è”ä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ§åˆ¶æ•´ä¸ªå›¾åƒçš„é¢œè‰²æ–¹æ¡ˆæ—¶é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯å¯¹äºèƒŒæ™¯å…ƒç´ å’Œæœªæ˜ç¡®æåŠçš„æ¬¡è¦å¯¹è±¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡é›†æˆè°ƒè‰²æ¿ä½œä¸ºç‹¬ç«‹äºæç¤ºæŒ‡ä»¤ä¹‹å¤–çš„æŒ‡å¯¼æœºåˆ¶æ¥æé«˜é¢œè‰²æ–¹æ¡ˆæ§åˆ¶çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨åŸºäºæ‰©æ•£çš„å›¾åƒå½©è‰²åŒ–æ¡†æ¶å†…æ¢ç´¢äº†å„ç§è°ƒè‰²æ¿è¡¨ç¤ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€æ¢ç´¢ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸“é—¨çš„è°ƒè‰²æ¿-æ–‡æœ¬-å›¾åƒæ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆè°ƒè‰²æ¿æŒ‡å¯¼å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹ç”Ÿæˆå…·æœ‰æ‰€éœ€é¢œè‰²æ–¹æ¡ˆå›¾åƒçš„èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å¯æ§å’Œæ›´ç²¾ç»†çš„å½©è‰²åŒ–è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08754v1">PDF</a> Accepted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚å½“å‰æ¨¡å‹å…è®¸ç”¨æˆ·ä½¿ç”¨è¯­è¨€è‰²å½©åç§°æŒ‡å®šå¯¹è±¡é¢œè‰²ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨æ§åˆ¶æ•´ä¸ªå›¾åƒçš„è‰²å½©æ–¹æ¡ˆä¸Šä»å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒŒæ™¯å…ƒç´ å’Œæœªæ˜ç¡®æåŠçš„æ¬¡è¦å¯¹è±¡ä¸Šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡é›†æˆè‰²å½©æ–¹æ¡ˆä½œä¸ºç‹¬ç«‹äºæç¤ºæŒ‡ä»¤çš„å¼•å¯¼æœºåˆ¶æ¥æé«˜è‰²å½©æ–¹æ¡ˆæ§åˆ¶æ•ˆæœçš„æ–°æ–¹æ³•ã€‚é€šè¿‡æ¢ç´¢åŸºäºæ‰©æ•£çš„å›¾åƒç€è‰²æ¡†æ¶ä¸­çš„ä¸åŒè‰²å½©æ–¹æ¡ˆè¡¨ç¤ºæ–¹æ³•ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†è‰²å½©æ–¹æ¡ˆæŒ‡å¯¼çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€æ¢ç´¢ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸“é—¨çš„è‰²å½©æ–¹æ¡ˆæ–‡æœ¬å›¾åƒæ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå¼•å…¥è‰²å½©æ–¹æ¡ˆæŒ‡å¯¼å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹ç”Ÿæˆå…·æœ‰æ‰€éœ€è‰²å½©æ–¹æ¡ˆçš„å›¾åƒçš„èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´å¯æ§å’Œç²¾ç»†çš„ç€è‰²è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸéšç€æ‰©æ•£æ¨¡å‹çš„å‘å±•å–å¾—äº†é‡å¤§è¿›å±•ã€‚</li>
<li>å½“å‰æ¨¡å‹å…è®¸ç”¨æˆ·ä½¿ç”¨è¯­è¨€æŒ‡å®šå¯¹è±¡é¢œè‰²ï¼Œä½†æ§åˆ¶æ•´ä¸ªå›¾åƒçš„è‰²å½©æ–¹æ¡ˆä»å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡é›†æˆè‰²å½©æ–¹æ¡ˆä½œä¸ºç‹¬ç«‹äºæç¤ºæŒ‡ä»¤çš„å¼•å¯¼æœºåˆ¶çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜è‰²å½©æ–¹æ¡ˆçš„æ§åˆ¶æ•ˆæœã€‚</li>
<li>é€šè¿‡åœ¨åŸºäºæ‰©æ•£çš„å›¾åƒç€è‰²æ¡†æ¶ä¸­æ¢ç´¢ä¸åŒçš„è‰²å½©æ–¹æ¡ˆè¡¨ç¤ºæ–¹æ³•ï¼Œè°ƒæŸ¥äº†è‰²å½©æ–¹æ¡ˆæŒ‡å¯¼çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ºäº†ä¿ƒè¿›æ¢ç´¢ï¼Œæ„å»ºäº†ä¸“é—¨çš„è‰²å½©æ–¹æ¡ˆæ–‡æœ¬å›¾åƒæ•°æ®é›†ã€‚</li>
<li>å¼•å…¥è‰²å½©æ–¹æ¡ˆæŒ‡å¯¼å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹ç”Ÿæˆå…·æœ‰æ‰€éœ€è‰²å½©æ–¹æ¡ˆçš„å›¾åƒçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-390f289c733641996363008d463ab675.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70fe7c85fce6d2a7e86bb26114de6991.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4170cbb011ae1f3e4e8875faa283ff8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bc5302a3a14478f11d0ad31dbc6d284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b691600cfbff158c112d8aff6fa1fb7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c140861bd1f9fdfd76c4c2b5bfed384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793d07a725af15c31c262162d7e0a63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de91184e268fb93fd9755ff2a8bb2b57.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration"><a href="#Unlocking-the-Potential-of-Diffusion-Priors-in-Blind-Face-Restoration" class="headerlink" title="Unlocking the Potential of Diffusion Priors in Blind Face Restoration"></a>Unlocking the Potential of Diffusion Priors in Blind Face Restoration</h2><p><strong>Authors:Yunqi Miao, Zhiyu Qu, Mingqi Gao, Changrui Chen, Jifei Song, Jungong Han, Jiankang Deng</strong></p>
<p>Although diffusion prior is rising as a powerful solution for blind face restoration (BFR), the inherent gap between the vanilla diffusion model and BFR settings hinders its seamless adaptation. The gap mainly stems from the discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2) synthesized and real-world images. The vanilla diffusion model is trained on images with no or less degradations, whereas BFR handles moderately to severely degraded images. Additionally, LQ images used for training are synthesized by a naive degradation model with limited degradation patterns, which fails to simulate complex and unknown degradations in real-world scenarios. In this work, we use a unified network FLIPNET that switches between two modes to resolve specific gaps. In Restoration mode, the model gradually integrates BFR-oriented features and face embeddings from LQ images to achieve authentic and faithful face restoration. In Degradation mode, the model synthesizes real-world like degraded images based on the knowledge learned from real-world degradation datasets. Extensive evaluations on benchmark datasets show that our model 1) outperforms previous diffusion prior based BFR methods in terms of authenticity and fidelity, and 2) outperforms the naive degradation model in modeling the real-world degradations. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£å…ˆéªŒï¼ˆdiffusion priorï¼‰å·²ç»æˆä¸ºç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰çš„ä¸€ç§å¼ºå¤§è§£å†³æ–¹æ¡ˆï¼Œä½†æ™®é€šæ‰©æ•£æ¨¡å‹ä¸BFRè®¾ç½®ä¹‹é—´çš„å›ºæœ‰å·®è·é˜»ç¢äº†å…¶æ— ç¼é€‚åº”ã€‚è¿™ä¸€å·®è·ä¸»è¦æºäºä»¥ä¸‹ä¸¤ç‚¹çš„ä¸ä¸€è‡´ï¼š1ï¼‰é«˜è´¨é‡ï¼ˆHQï¼‰ä¸ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒä¹‹é—´ï¼›2ï¼‰åˆæˆå›¾åƒä¸çœŸå®ä¸–ç•Œå›¾åƒä¹‹é—´ã€‚æ™®é€šæ‰©æ•£æ¨¡å‹æ˜¯åœ¨æ²¡æœ‰æˆ–è¾ƒå°‘é€€åŒ–çš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè€ŒBFRåˆ™å¤„ç†ä¸­åº¦è‡³é‡åº¦é€€åŒ–çš„å›¾åƒã€‚æ­¤å¤–ï¼Œç”¨äºè®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ˜¯ç”±ç®€å•çš„é€€åŒ–æ¨¡å‹åˆæˆçš„ï¼Œå…·æœ‰æœ‰é™çš„é€€åŒ–æ¨¡å¼ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„å¤æ‚å’ŒæœªçŸ¥é€€åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç»Ÿä¸€ç½‘ç»œFLIPNETï¼Œè¯¥ç½‘ç»œå¯ä»¥åœ¨ä¸¤ç§æ¨¡å¼ä¹‹é—´è¿›è¡Œåˆ‡æ¢ä»¥è§£å†³ç‰¹å®šçš„å·®è·ã€‚åœ¨ä¿®å¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹é€æ¸æ•´åˆé¢å‘BFRçš„ç‰¹å¾å’Œäººè„¸åµŒå…¥ï¼Œä»ä½è´¨é‡å›¾åƒä¸­å®ç°çœŸå®å’Œå¿ è¯šçš„äººè„¸ä¿®å¤ã€‚åœ¨é€€åŒ–æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹åŸºäºä»çœŸå®ä¸–ç•Œé€€åŒ–æ•°æ®é›†ä¸­å­¦ä¹ çš„çŸ¥è¯†ï¼Œåˆæˆç±»ä¼¼çœŸå®ä¸–ç•Œçš„é€€åŒ–å›¾åƒã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹1ï¼‰åœ¨çœŸå®æ€§å’Œä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„åŸºäºæ‰©æ•£å…ˆéªŒçš„BFRæ–¹æ³•ï¼›2ï¼‰åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œé€€åŒ–æ–¹é¢ä¼˜äºç®€å•çš„é€€åŒ–æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08556v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£å…ˆéªŒè™½ç„¶åœ¨ç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„è§£å†³æ–¹æ¡ˆæ½œåŠ›ï¼Œä½†æ™®é€šæ‰©æ•£æ¨¡å‹ä¸BFRè®¾ç½®ä¹‹é—´å­˜åœ¨å›ºæœ‰å·®è·ï¼Œé˜»ç¢äº†å…¶æ— ç¼é€‚åº”ã€‚å·®è·ä¸»è¦æºäºé«˜è´¨é‡ï¼ˆHQï¼‰ä¸ä½è´¨é‡ï¼ˆLQï¼‰å›¾åƒã€åˆæˆå›¾åƒä¸çœŸå®ä¸–ç•Œå›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚æ™®é€šæ‰©æ•£æ¨¡å‹è®­ç»ƒäºæ— é€€åŒ–æˆ–è¾ƒå°‘é€€åŒ–çš„å›¾åƒï¼Œè€ŒBFRå¤„ç†ä¸­åº¦è‡³é‡åº¦é€€åŒ–çš„å›¾åƒã€‚æ­¤å¤–ï¼Œç”¨äºè®­ç»ƒçš„LQå›¾åƒç”±ç®€å•çš„é€€åŒ–æ¨¡å‹åˆæˆï¼Œæ¨¡æ‹Ÿçš„é€€åŒ–æ¨¡å¼æœ‰é™ï¼Œæ— æ³•æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„å¤æ‚å’ŒæœªçŸ¥é€€åŒ–ã€‚æœ¬ç ”ç©¶ä½¿ç”¨ç»Ÿä¸€ç½‘ç»œFLIPNETï¼Œå¯åœ¨ä¸¤ç§æ¨¡å¼ä¹‹é—´åˆ‡æ¢ä»¥è§£å†³ç‰¹å®šå·®è·ã€‚ä¿®å¤æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹é€æ¸èå…¥é¢å‘BFRçš„ç‰¹å¾å’Œäººè„¸åµŒå…¥è‡ªLQå›¾åƒï¼Œå®ç°çœŸå®å¯ä¿¡çš„äººè„¸ä¿®å¤ã€‚é€€åŒ–æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹åŸºäºä»çœŸå®ä¸–ç•Œé€€åŒ–æ•°æ®é›†ä¸­è·å¾—çš„çŸ¥è¯†ï¼Œåˆæˆç±»ä¼¼çœŸå®ä¸–ç•Œçš„é€€åŒ–å›¾åƒã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹1ï¼‰åœ¨çœŸå®æ€§å’Œä¿çœŸåº¦æ–¹é¢ä¼˜äºå…ˆå‰çš„æ‰©æ•£å…ˆéªŒåŸºäºBFRçš„æ–¹æ³•ï¼›2ï¼‰åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œé€€åŒ–æ–¹é¢ä¼˜äºç®€å•çš„é€€åŒ–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å…ˆéªŒåœ¨ç›²è„¸ä¿®å¤ï¼ˆBFRï¼‰ä¸­å…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œä½†å­˜åœ¨ä¸å®é™…åº”ç”¨åœºæ™¯çš„å·®è·ã€‚</li>
<li>å·®è·ä¸»è¦æºäºé«˜è´¨é‡ä¸ä½è´¨é‡å›¾åƒã€åˆæˆä¸çœŸå®å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>æ™®é€šæ‰©æ•£æ¨¡å‹ä¸»è¦å¤„ç†æ— é€€åŒ–æˆ–è¾ƒå°‘é€€åŒ–çš„å›¾åƒï¼Œè€ŒBFRé¢ä¸´ä¸­åº¦è‡³é‡åº¦é€€åŒ–å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰é€€åŒ–æ¨¡å‹æ— æ³•å……åˆ†æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„å¤æ‚å’ŒæœªçŸ¥é€€åŒ–ã€‚</li>
<li>FLIPNETç½‘ç»œé€šè¿‡ä¸¤ç§æ¨¡å¼è§£å†³ä¸Šè¿°é—®é¢˜ï¼šä¿®å¤æ¨¡å¼å®ç°çœŸå®å¯ä¿¡çš„äººè„¸ä¿®å¤ï¼Œé€€åŒ–æ¨¡å¼åˆæˆç±»ä¼¼çœŸå®ä¸–ç•Œçš„é€€åŒ–å›¾åƒã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒFLIPNETåœ¨çœŸå®æ€§å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6f6319c365dc52ac81b826216c901c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c11f202d2fe08fa0002240304dbdc20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d228b441648bf7f31bf3cfdcadc1c415.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Spatiotemporally-Consistent-Indoor-Lighting-Estimation-with-Diffusion-Priors"><a href="#Spatiotemporally-Consistent-Indoor-Lighting-Estimation-with-Diffusion-Priors" class="headerlink" title="Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion   Priors"></a>Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion   Priors</h2><p><strong>Authors:Mutian Tong, Rundi Wu, Changxi Zheng</strong></p>
<p>Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works. </p>
<blockquote>
<p>ä»å•å¼ å›¾åƒæˆ–è§†é¢‘ä¸­ä¼°è®¡å®¤å†…ç…§æ˜ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå…¶é«˜åº¦ä¸é€‚å®šçš„æ€§è´¨ï¼Œå°¤å…¶æ˜¯å½“åœºæ™¯çš„å…‰çº¿æ¡ä»¶åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šå˜åŒ–æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥ä»è¾“å…¥è§†é¢‘ä¸­ä¼°è®¡ä¸€ä¸ªè¿ç»­çš„ç¯å…‰åœºï¼Œæè¿°åœºæ™¯çš„ç©ºé—´å’Œæ—¶é—´å˜åŒ–ç…§æ˜ã€‚æˆ‘ä»¬åˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒæ¥ä¼˜åŒ–è¡¨ç¤ºä¸ºå¤šå±‚æ„ŸçŸ¥æœºçš„å…‰åœºã€‚ä¸ºäº†å®ç°å¯¹è‡ªç„¶åœºæ™¯çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡è”åˆå¡«å……å¤šä¸ªä½œä¸ºå…‰æ¢é’ˆçš„é•€é“¬çƒæ¥å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥é¢„æµ‹å¤šä¸ªä½ç½®çš„ç…§æ˜ã€‚æˆ‘ä»¬åœ¨ä»å•å¼ å›¾åƒæˆ–è§†é¢‘ä¼°è®¡å®¤å†…ç…§æ˜æ–¹é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¼˜äºå¯¹æ¯”åŸºå‡†çš„æ€§èƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ä»è‡ªç„¶è§†é¢‘ä¸­ä¼°è®¡æ—¶ç©ºä¸€è‡´ç…§æ˜çš„ç»“æœï¼Œè¿™åœ¨ä»¥å‰çš„å·¥ä½œä¸­å¾ˆå°‘å±•ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08384v1">PDF</a> 11 pages. Accepted by SIGGRAPH 2025 as Conference Paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»è¾“å…¥è§†é¢‘ä¸­ä¼°è®¡åœºæ™¯æ—¶ç©ºå˜åŒ–ç…§æ˜çš„æ–¹æ³•ã€‚åˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒä¼˜åŒ–è¡¨ç¤ºä¸ºå¤šå±‚æ„ŸçŸ¥æœºçš„å…‰åœºï¼Œå¹¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥è”åˆå¡«å……å¤šä¸ªé“¬çƒä½œä¸ºå…‰æ¢é’ˆï¼Œå®ç°å¯¹åœºæ™¯ä¸­å¤šä¸ªä½ç½®çš„ç…§æ˜é¢„æµ‹ã€‚åœ¨å®¤å†…å¤–ç…§æ˜ä¼°è®¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹é‡å¤–è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ç…§æ˜ä¼°è®¡æ–¹é¢ï¼Œä¼˜äºç°æœ‰å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡è§£å†³äº†ä»å•ä¸€å›¾åƒæˆ–è§†é¢‘ä¸­ä¼°è®¡å®¤å†…ç…§æ˜çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“åœºæ™¯ç…§æ˜åœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šå˜åŒ–æ—¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¿ç»­å…‰åœºä¼°è®¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»è¾“å…¥è§†é¢‘ä¸­æè¿°åœºæ™¯çš„æ—¶ç©ºå˜åŒ–ç…§æ˜ã€‚</li>
<li>åˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒä¼˜åŒ–å…‰åœºçš„è¡¨ç¤ºï¼Œè¯¥å…‰åœºè¢«è¡¨ç¤ºä¸ºå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°å¯¹å¤šä¸ªä½ç½®çš„ç…§æ˜é¢„æµ‹ã€‚</li>
<li>é€šè¿‡è”åˆå¡«å……å¤šä¸ªé“¬çƒä½œä¸ºå…‰æ¢é’ˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>åœ¨å®¤å†…å¤–ç…§æ˜ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹é‡å¤–è§†é¢‘çš„ç…§æ˜ä¼°è®¡æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5bb65c51ac4d0b14d5b88e484b7dcdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2923ca4073cc9568c1cc224425958883.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe97eaaf1213e0a6f7b48b7df7f23aba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e48505090a0e05de25e357d1014b79ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b044c2d8ee639888f28abc45179025bf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-Reusing-to-Forecasting-Accelerating-Diffusion-Models-with-TaylorSeers"><a href="#From-Reusing-to-Forecasting-Accelerating-Diffusion-Models-with-TaylorSeers" class="headerlink" title="From Reusing to Forecasting: Accelerating Diffusion Models with   TaylorSeers"></a>From Reusing to Forecasting: Accelerating Diffusion Models with   TaylorSeers</h2><p><strong>Authors:Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang</strong></p>
<p>Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$$\times$ acceleration. %Our code is provided in the supplementary materials and will be made publicly available on GitHub. Our codes have been released in Github:<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/TaylorSeer">https://github.com/Shenyi-Z/TaylorSeer</a> </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²ç»å®ç°äº†é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆçš„é©å‘½æ€§è¿›å±•ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚ä»ç„¶å¯¹äºå®æ—¶åº”ç”¨æ¥è¯´æ˜¯å·¨å¤§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ç‰¹å¾ç¼“å­˜æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç¼“å­˜å‰é¢æ—¶é—´æ­¥çš„ç‰¹å¾å¹¶åœ¨åç»­æ—¶é—´æ­¥ä¸­é‡å¤ä½¿ç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œåœ¨é—´éš”æ—¶é—´è¾ƒé•¿çš„æ—¶é—´æ­¥ä¸­ï¼Œæ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å¾ç›¸ä¼¼æ€§ä¼šå¤§å¹…ä¸‹é™ï¼Œå¯¼è‡´ç”±ç‰¹å¾ç¼“å­˜å¼•å…¥çš„é”™è¯¯æ˜¾è‘—å¢åŠ ï¼Œä»è€Œä¸¥é‡æŸå®³ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TaylorSeerã€‚å®ƒé¦–å…ˆè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨æœªæ¥æ—¶é—´æ­¥çš„ç‰¹å¾å¯ä»¥åŸºäºå®ƒä»¬åœ¨ä»¥å‰æ—¶é—´æ­¥çš„å€¼è¿›è¡Œé¢„æµ‹ã€‚åŸºäºç‰¹å¾éšæ—¶é—´æ­¥å˜åŒ–ç¼“æ…¢ä¸”è¿ç»­çš„äº‹å®ï¼ŒTaylorSeeré‡‡ç”¨å¾®åˆ†æ–¹æ³•è¿‘ä¼¼ç‰¹å¾çš„é«˜é˜¶å¯¼æ•°ï¼Œå¹¶ä½¿ç”¨æ³°å‹’çº§æ•°å±•å¼€é¢„æµ‹æœªæ¥æ—¶é—´æ­¥çš„ç‰¹å¾ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œå®ƒåœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åŠ é€Ÿæ¯”çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨FLUXå’ŒHunyuanVideoä¸Šå®ç°äº†è¿‘ä¹æ— æŸçš„4.99Ã—å’Œ5.00Ã—çš„åŠ é€Ÿï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚åœ¨DiTä¸Šï¼Œä¸ä¹‹å‰çš„æœ€ä½³æ€§èƒ½ç›¸æ¯”ï¼Œå®ƒåœ¨åŠ é€Ÿ4.53Ã—çš„æƒ…å†µä¸‹å®ç°äº†3.41æ›´ä½çš„FIDã€‚æˆ‘ä»¬çš„ä»£ç å·²ä½œä¸ºè¡¥å……ææ–™æä¾›ï¼Œå¹¶å°†å…¬å¼€åœ¨GitHubä¸Šå‘å¸ƒã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/TaylorSeer">https://github.com/Shenyi-Z/TaylorSeer</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06923v2">PDF</a> 15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature   caching for diffusion transformers acceleration</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusion Transformersï¼ˆDiTï¼‰åœ¨é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆä¸­çš„é©å‘½æ€§è¿›å±•ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚ä»ç„¶å¾ˆå¤§ï¼Œä¸é€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ç‰¹å¾ç¼“å­˜æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨æ—¶é—´é—´éš”è¾ƒå¤§çš„æ—¶åˆ»ï¼Œç‰¹å¾ç›¸ä¼¼æ€§ä¼šé™ä½ï¼Œå¯¼è‡´ç‰¹å¾ç¼“å­˜å¼•å…¥çš„é”™è¯¯å¢åŠ ï¼Œä¸¥é‡å½±å“ç”Ÿæˆè´¨é‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TaylorSeeræ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹åœ¨å…ˆå‰æ—¶åˆ»çš„ç‰¹å¾å€¼æ¥é¢„æµ‹æœªæ¥æ—¶åˆ»çš„ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨æ³°å‹’çº§æ•°å±•å¼€å¼è¿›è¡Œé¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸­æ•ˆæœæ˜¾è‘—ï¼Œå°¤å…¶åœ¨é«˜é€ŸåŠ é€Ÿä¸‹è¡¨ç°æ›´ä¼˜ç§€ã€‚ä¾‹å¦‚ï¼Œåœ¨FLUXå’ŒHunyuanVideoä¸Šå®ç°äº†è¿‘ä¹æ— æŸçš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨DiTä¸Šå®ç°äº†è¾ƒä½çš„FIDå¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformers (DiT) å·²å®ç°é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆçš„é‡å¤§çªç ´ã€‚</li>
<li>ç‰¹å¾ç¼“å­˜è¢«æå‡ºä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨æ—¶é—´é—´éš”å¤§çš„æ—¶åˆ»ä¼šå‡ºç°é—®é¢˜ã€‚</li>
<li>TaylorSeeræ–¹æ³•åˆ©ç”¨å…ˆå‰æ—¶åˆ»çš„ç‰¹å¾é¢„æµ‹æœªæ¥æ—¶åˆ»çš„ç‰¹å¾ã€‚</li>
<li>TaylorSeeré‡‡ç”¨å¾®åˆ†æ³•è®¡ç®—ç‰¹å¾çš„é«˜é˜¶å¯¼æ•°è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>å®éªŒè¯æ˜TaylorSeeråœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸­æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>TaylorSeeråœ¨é«˜é€ŸåŠ é€Ÿä¸‹è¡¨ç°ä¼˜ç§€ï¼Œå®ç°äº†è¿‘ä¹æ— æŸçš„åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7a00b60253b5b2f1909f2641a2ffadb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fe8027d197db4ee9029904b740c7104.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0180ba33397c9d8e180af98c73aec09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d8a62bbd5a73b6769fd5d66a60debde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69e3935cf91c1e70b1e3bb80e5e7f7f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fancy123-One-Image-to-High-Quality-3D-Mesh-Generation-via-Plug-and-Play-Deformation"><a href="#Fancy123-One-Image-to-High-Quality-3D-Mesh-Generation-via-Plug-and-Play-Deformation" class="headerlink" title="Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play   Deformation"></a>Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play   Deformation</h2><p><strong>Authors:Qiao Yu, Xianzhi Li, Yuan Tang, Xu Han, Long Hu, Yixue Hao, Min Chen</strong></p>
<p>Generating 3D meshes from a single image is an important but ill-posed task. Existing methods mainly adopt 2D multiview diffusion models to generate intermediate multiview images, and use the Large Reconstruction Model (LRM) to create the final meshes. However, the multiview images exhibit local inconsistencies, and the meshes often lack fidelity to the input image or look blurry. We propose Fancy123, featuring two enhancement modules and an unprojection operation to address the above three issues, respectively. The appearance enhancement module deforms the 2D multiview images to realign misaligned pixels for better multiview consistency. The fidelity enhancement module deforms the 3D mesh to match the input image. The unprojection of the input image and deformed multiview images onto LRMâ€™s generated mesh ensures high clarity, discarding LRMâ€™s predicted blurry-looking mesh colors. Extensive qualitative and quantitative experiments verify Fancy123â€™s SoTA performance with significant improvement. Also, the two enhancement modules are plug-and-play and work at inference time, allowing seamless integration into various existing single-image-to-3D methods. Code at: <a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/Fancy123">https://github.com/YuQiao0303/Fancy123</a> </p>
<blockquote>
<p>ä»å•ä¸€å›¾åƒç”Ÿæˆ3Dç½‘æ ¼æ˜¯ä¸€é¡¹é‡è¦ä½†ä¸é€‚å®šçš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸­é—´å¤šè§†è§’å›¾åƒï¼Œå¹¶ä½¿ç”¨å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆLRMï¼‰åˆ›å»ºæœ€ç»ˆçš„ç½‘æ ¼ã€‚ç„¶è€Œï¼Œå¤šè§†è§’å›¾åƒå­˜åœ¨å±€éƒ¨ä¸ä¸€è‡´æ€§ï¼Œç½‘æ ¼å¾€å¾€å¯¹è¾“å…¥å›¾åƒçš„ä¿çœŸåº¦ä¸è¶³æˆ–çœ‹èµ·æ¥æ¨¡ç³Šã€‚æˆ‘ä»¬æå‡ºFancy123ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå¢å¼ºæ¨¡å—å’Œä¸€ä¸ªåæŠ•å½±æ“ä½œï¼Œåˆ†åˆ«è§£å†³ä¸Šè¿°ä¸‰ä¸ªé—®é¢˜ã€‚å¤–è§‚å¢å¼ºæ¨¡å—å¯¹2Då¤šè§†è§’å›¾åƒè¿›è¡Œå˜å½¢ï¼Œä»¥å¯¹é½é”™ä½åƒç´ ï¼Œå®ç°æ›´å¥½çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚ä¿çœŸåº¦å¢å¼ºæ¨¡å—å¯¹3Dç½‘æ ¼è¿›è¡Œå˜å½¢ï¼Œä»¥åŒ¹é…è¾“å…¥å›¾åƒã€‚å°†è¾“å…¥å›¾åƒå’ŒåæŠ•å½±åçš„å¤šè§†è§’å›¾åƒæŠ•å½±åˆ°LRMç”Ÿæˆçš„ç½‘æ ¼ä¸Šï¼Œç¡®ä¿é«˜æ¸…æ™°åº¦ï¼Œæ‘’å¼ƒLRMé¢„æµ‹çš„æ¨¡ç³Šç½‘æ ¼é¢œè‰²ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†Fancy123çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªå¢å¼ºæ¨¡å—å³æ’å³ç”¨ï¼Œå¯åœ¨æ¨ç†é˜¶æ®µå·¥ä½œï¼Œå¯æ— ç¼é›†æˆåˆ°å„ç§ç°æœ‰çš„å•å›¾åƒåˆ°3Dçš„æ–¹æ³•ä¸­ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/Fancy123">https://github.com/YuQiao0303/Fancy123</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16185v2">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰æŠ€æœ¯ç”Ÿæˆå•å¹…å›¾åƒçš„ä¸‰ç»´ç½‘æ ¼æ˜¯ä¸€é¡¹é‡è¦ä½†éš¾ä»¥è§£å†³çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡ç”¨äºŒç»´å¤šè§†è§’æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸­é—´å¤šè§†è§’å›¾åƒï¼Œå¹¶ä½¿ç”¨å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆLRMï¼‰åˆ›å»ºæœ€ç»ˆç½‘æ ¼ã€‚ç„¶è€Œï¼Œå¤šè§†è§’å›¾åƒå­˜åœ¨å±€éƒ¨ä¸ä¸€è‡´æ€§ï¼Œç½‘æ ¼å¾€å¾€ç¼ºä¹ä¸è¾“å…¥å›¾åƒçš„ä¿çœŸåº¦æˆ–çœ‹èµ·æ¥æ¨¡ç³Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Fancy123æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå¢å¼ºæ¨¡å—å’Œä¸€ä¸ªåå‘æŠ•å½±æ“ä½œã€‚å¤–è§‚å¢å¼ºæ¨¡å—å¯¹äºŒç»´å¤šè§†è§’å›¾åƒè¿›è¡Œå˜å½¢ï¼Œä»¥å¯¹é½é”™ä½åƒç´ ï¼Œæé«˜å¤šè§†è§’ä¸€è‡´æ€§ã€‚ä¿çœŸåº¦å¢å¼ºæ¨¡å—å¯¹ä¸‰ç»´ç½‘æ ¼è¿›è¡Œå˜å½¢ï¼Œä»¥åŒ¹é…è¾“å…¥å›¾åƒã€‚å°†è¾“å…¥å›¾åƒå’Œå˜å½¢çš„å¤šè§†è§’å›¾åƒæŠ•å½±åˆ°LRMç”Ÿæˆçš„ç½‘æ ¼ä¸Šï¼Œç¡®ä¿é«˜æ¸…æ™°åº¦ï¼Œå¹¶æ‘’å¼ƒLRMé¢„æµ‹çš„æ¨¡ç³Šç½‘æ ¼é¢œè‰²ã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†Fancy123çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œä¸¤ä¸ªå¢å¼ºæ¨¡å—å³æ’å³ç”¨ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— ç¼é›†æˆåˆ°å„ç§ç°æœ‰çš„å•å›¾åƒåˆ°ä¸‰ç»´è½¬æ¢æ–¹æ³•ä¸­ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå•å¹…å›¾åƒçš„ä¸‰ç»´ç½‘æ ¼æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–äºŒç»´å¤šè§†è§’æ‰©æ•£æ¨¡å‹å’Œå¤§å‹é‡å»ºæ¨¡å‹ï¼ˆLRMï¼‰ã€‚</li>
<li>å¤šè§†è§’å›¾åƒå­˜åœ¨å±€éƒ¨ä¸ä¸€è‡´æ€§ï¼Œä¸”ç”Ÿæˆçš„ç½‘æ ¼å¸¸å¸¸æ¨¡ç³Šæˆ–ä¸è¾“å…¥å›¾åƒä¸åŒ¹é…ã€‚</li>
<li>Fancy123é€šè¿‡å¼•å…¥ä¸¤ä¸ªå¢å¼ºæ¨¡å—å’Œä¸€ä¸ªåå‘æŠ•å½±æ“ä½œæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å¤–è§‚å¢å¼ºæ¨¡å—æé«˜å¤šè§†è§’ä¸€è‡´æ€§ï¼Œè€Œä¿çœŸåº¦å¢å¼ºæ¨¡å—ç¡®ä¿ç½‘æ ¼ä¸è¾“å…¥å›¾åƒåŒ¹é…ã€‚</li>
<li>é€šè¿‡å°†å›¾åƒå’Œå˜å½¢åçš„å¤šè§†è§’å›¾åƒæŠ•å½±åˆ°LRMç½‘æ ¼ä¸Šï¼Œå®ç°é«˜æ¸…æ™°åº¦è¾“å‡ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b189b42f204f307b67cd27cf6763d84b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d8155974705b950c109482434609232.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f91081ca9a675e590b48c4ff238986f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d258c1b673d0b172f534a6f5685f7b8a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REDUCIO-Generating-1K-Video-within-16-Seconds-using-Extremely-Compressed-Motion-Latents"><a href="#REDUCIO-Generating-1K-Video-within-16-Seconds-using-Extremely-Compressed-Motion-Latents" class="headerlink" title="REDUCIO! Generating 1K Video within 16 Seconds using Extremely   Compressed Motion Latents"></a>REDUCIO! Generating 1K Video within 16 Seconds using Extremely   Compressed Motion Latents</h2><p><strong>Authors:Rui Tian, Qi Dai, Jianmin Bao, Kai Qiu, Yifan Yang, Chong Luo, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access. One crucial obstacle for large-scale applications is the expensive training and inference cost. In this paper, we argue that videos contain significantly more redundant information than images, allowing them to be encoded with very few motion latents. Towards this goal, we design an image-conditioned VAE that projects videos into extremely compressed latent space and decode them based on content images. This magic Reducio charm enables 64x reduction of latents compared to a common 2D VAE, without sacrificing the quality. Building upon Reducio-VAE, we can train diffusion models for high-resolution video generation efficiently. Specifically, we adopt a two-stage generation paradigm, first generating a condition image via text-to-image generation, followed by text-image-to-video generation with the proposed Reducio-DiT. Extensive experiments show that our model achieves strong performance in evaluation. More importantly, our method significantly boosts the training and inference efficiency of video LDMs. Reducio-DiT is trained in just 3.2K A100 GPU hours in total and can generate a 16-frame 1024$\times$1024 video clip within 15.5 seconds on a single A100 GPU. Code released at <a target="_blank" rel="noopener" href="https://github.com/microsoft/Reducio-VAE">https://github.com/microsoft/Reducio-VAE</a> . </p>
<blockquote>
<p>å•†ä¸šè§†é¢‘ç”Ÿæˆæ¨¡å‹å·²ç»å±•ç°å‡ºé€¼çœŸã€é«˜ä¿çœŸçš„ç»“æœï¼Œä½†ä»ç„¶é™äºæœ‰é™è®¿é—®ã€‚å¤§è§„æ¨¡åº”ç”¨çš„ä¸€ä¸ªå…³é”®éšœç¢æ˜¯è®­ç»ƒå’Œæ¨ç†æˆæœ¬é«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè§†é¢‘åŒ…å«æ¯”å›¾åƒæ›´å¤šçš„å†—ä½™ä¿¡æ¯ï¼Œå› æ­¤å¯ä»¥ç”¨å¾ˆå°‘çš„åŠ¨æ½œç¼–ç ã€‚ä¸ºäº†è¾¾æˆè¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå›¾åƒæ¡ä»¶çš„VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰ï¼Œå®ƒå°†è§†é¢‘æŠ•å½±åˆ°æå‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶æ ¹æ®å†…å®¹å›¾åƒè¿›è¡Œè§£ç ã€‚è¿™ä¸ªç¥å¥‡çš„Reducioå’’è¯­å®ç°äº†ä¸å¸¸è§2D VAEç›¸æ¯”çš„64å€æ½œç©ºé—´å‡å°‘ï¼Œè€Œä¸ä¼šç‰ºç‰²è´¨é‡ã€‚åŸºäºReducio-VAEï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°è®­ç»ƒç”¨äºé«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µç”ŸæˆèŒƒå¼ï¼Œé¦–å…ˆé€šè¿‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡ä»¶å›¾åƒï¼Œç„¶åé€šè¿‡æå‡ºçš„Reducio-DiTè¿›è¡Œæ–‡æœ¬å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯„ä¼°ä¸­è¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è§†é¢‘LDMçš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚Reducio-DiTæ€»å…±åªéœ€è¦3.2Kä¸ªA100 GPUå°æ—¶è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å•ä¸ªA100 GPUä¸Šå¯ä»¥åœ¨15.5ç§’å†…ç”Ÿæˆä¸€ä¸ª16å¸§çš„1024x1024è§†é¢‘ç‰‡æ®µã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/Reducio-VAE%E3%80%82">https://github.com/microsoft/Reducio-VAEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13552v3">PDF</a> Accepted to ICCV2025. Code available at   <a target="_blank" rel="noopener" href="https://github.com/microsoft/Reducio-VAE">https://github.com/microsoft/Reducio-VAE</a></p>
<p><strong>Summary</strong><br>è§†é¢‘ç”Ÿæˆæ¨¡å‹è™½èƒ½äº§ç”Ÿé«˜ä¿çœŸç»“æœï¼Œä½†è®­ç»ƒä¸æ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶å¤§è§„æ¨¡åº”ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾åƒæ¡ä»¶çš„VAEæ¨¡å‹ï¼ˆReducio-VAEï¼‰ï¼Œèƒ½å°†è§†é¢‘æŠ•å½±åˆ°é«˜åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶åŸºäºå†…å®¹å›¾åƒè¿›è¡Œè§£ç ï¼Œå®ç°äº†æ½œåœ¨ç©ºé—´çš„64å€ç¼©å‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡è®­ç»ƒäº†é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ç”¨äºé«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†è§†é¢‘LDMçš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚Reducio-DiTæ¨¡å‹æ€»è®­ç»ƒæ—¶é—´ä»…ä¸º3.2K A100 GPUå°æ—¶ï¼Œå¯åœ¨å•ä¸ªA100 GPUä¸Š15.5ç§’å†…ç”Ÿæˆä¸€ä¸ª16å¸§çš„1024Ã—1024è§†é¢‘ç‰‡æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾åƒæ¡ä»¶çš„VAEæ¨¡å‹ï¼ˆReducio-VAEï¼‰ï¼Œèƒ½æœ‰æ•ˆå‹ç¼©è§†é¢‘æ•°æ®ã€‚</li>
<li>Reducio-VAEå®ç°äº†ä¸å¸¸è§„2D VAEç›¸æ¯”ï¼Œæ½œåœ¨ç©ºé—´64å€çš„ç¼©å‡ï¼ŒåŒæ—¶ä¿è¯äº†è§†é¢‘è´¨é‡ã€‚</li>
<li>åŸºäºReducio-VAEï¼Œç ”ç©¶å¼€å‘äº†é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ç”¨äºè§†é¢‘ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œå…ˆé€šè¿‡æ–‡æœ¬ç”Ÿæˆæ¡ä»¶å›¾åƒï¼Œå†é€šè¿‡Reducio-DiTç”Ÿæˆè§†é¢‘ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ˜¾è‘—æé«˜è§†é¢‘LDMçš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>Reducio-DiTæ¨¡å‹è®­ç»ƒæ—¶é—´çŸ­ï¼Œç”Ÿæˆè§†é¢‘é€Ÿåº¦å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-571c391c78cab17be60877cb88a92aaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4c95f59282bdf47ff89c5eedf1477e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1adf726c89f6b74113f93286d8e577fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67993c40c94fca2d14aa445f1d6c97b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8066dcf6a1c7b2c7b22214b2047adf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fcbb811558e582ebb7bfb3df0af18d2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SynthVLM-Towards-High-Quality-and-Efficient-Synthesis-of-Image-Caption-Datasets-for-Vision-Language-Models"><a href="#SynthVLM-Towards-High-Quality-and-Efficient-Synthesis-of-Image-Caption-Datasets-for-Vision-Language-Models" class="headerlink" title="SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption   Datasets for Vision-Language Models"></a>SynthVLM: Towards High-Quality and Efficient Synthesis of Image-Caption   Datasets for Vision-Language Models</h2><p><strong>Authors:Zheng Liu, Hao Liang, Bozhou Li, Wentao Xiong, Chong Chen, Conghui He, Wentao Zhang, Bin Cui</strong></p>
<p>Vision-Language Models (VLMs) have recently emerged, demonstrating remarkable vision-understanding capabilities. However, training these models requires large-scale datasets, which brings challenges related to efficiency, effectiveness, and quality of web data. In this paper, we introduce SynthVLM, a new data synthesis and curation method for generating image-caption pairs. Unlike traditional methods, where captions are generated from images, SynthVLM utilizes advanced diffusion models and high-quality captions to synthesize and select images from text captions, thereby creating precisely aligned image-text pairs. We further introduce SynthVLM-100K, a high-quality dataset consisting of 100K curated and synthesized image-caption pairs. In both model and human evaluations, SynthVLM-100K outperforms traditional real-world datasets. Leveraging this dataset, we develop a new family of multimodal large language models (MLLMs), SynthVLM-7B and SynthVLM-13B, which achieve state-of-the-art (SOTA) performance on various vision question-answering (VQA) tasks. Notably, our models outperform LLaVA across most metrics with only 18% pretrain data. Furthermore, SynthVLM-7B and SynthVLM-13B attain SOTA performance on the MMLU benchmark, demonstrating that the high-quality SynthVLM-100K dataset preserves language abilities. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘å´­éœ²å¤´è§’ï¼Œå±•ç°å‡ºå“è¶Šçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¿™å¸¦æ¥äº†ä¸ç½‘é¡µæ•°æ®çš„æ•ˆç‡ã€æœ‰æ•ˆæ€§å’Œè´¨é‡ç›¸å…³çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SynthVLMï¼Œä¸€ç§ç”¨äºç”Ÿæˆå›¾åƒ-å­—å¹•å¯¹çš„æ–°æ•°æ®åˆæˆå’Œç­›é€‰æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ä»å›¾åƒç”Ÿæˆå­—å¹•çš„æ–¹æ³•ä¸åŒï¼ŒSynthVLMåˆ©ç”¨å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å’Œé«˜è´¨é‡å­—å¹•ï¼Œä»æ–‡æœ¬å­—å¹•ä¸­åˆæˆå¹¶é€‰æ‹©å›¾åƒï¼Œä»è€Œåˆ›å»ºç²¾ç¡®å¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ä»‹ç»äº†SynthVLM-100Kï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡ä¸ªç»è¿‡ç­›é€‰å’Œåˆæˆçš„å›¾åƒ-å­—å¹•å¯¹ã€‚åœ¨æ¨¡å‹å’Œäººç±»è¯„ä¼°ä¸­ï¼ŒSynthVLM-100Kçš„è¡¨ç°éƒ½ä¼˜äºä¼ ç»Ÿçš„ç°å®ä¸–ç•Œæ•°æ®é›†ã€‚åˆ©ç”¨è¿™ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼ŒåŒ…æ‹¬SynthVLM-7Bå’ŒSynthVLM-13Bã€‚å®ƒä»¬åœ¨å„ç§è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºLLaVAï¼Œå¹¶ä¸”åªéœ€è¦18%çš„é¢„è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒSynthVLM-7Bå’ŒSynthVLM-13Båœ¨MMLUåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†é«˜è´¨é‡SynthVLM-100Kæ•°æ®é›†èƒ½å¤Ÿä¿ç•™è¯­è¨€èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20756v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆä¸ç­›é€‰æ–¹æ³•â€”â€”SynthVLMï¼Œç”¨äºç”Ÿæˆå›¾åƒ-æ–‡å­—æè¿°é…å¯¹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å’Œé«˜è´¨é‡çš„æ–‡å­—æè¿°æ¥åˆæˆå’Œç­›é€‰å›¾åƒï¼Œåˆ›å»ºç²¾å‡†å¯¹é½çš„å›¾åƒ-æ–‡å­—å¯¹ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†SynthVLM-100Kæ•°æ®é›†ï¼ŒåŒ…å«10ä¸‡ç»„é«˜è´¨é‡åˆæˆå›¾åƒ-æ–‡å­—æè¿°å¯¹ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”å‘äº†æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰â€”â€”SynthVLM-7Bå’ŒSynthVLM-13Bã€‚è¿™äº›æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”åœ¨ä»…ä½¿ç”¨18%çš„é¢„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°±å·²åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¶…è¶Šäº†LLaVAã€‚åŒæ—¶ï¼Œå®ƒä»¬åœ¨MMLUåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†å“è¶Šè¡¨ç°ï¼Œè¯æ˜äº†SynthVLM-100Kæ•°æ®é›†çš„é«˜è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynthVLMæ˜¯ä¸€ç§æ–°çš„æ•°æ®åˆæˆä¸ç­›é€‰æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå›¾åƒ-æ–‡å­—æè¿°é…å¯¹ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œé«˜è´¨é‡æ–‡å­—æè¿°è¿›è¡Œå›¾åƒåˆæˆå’Œç­›é€‰ã€‚</li>
<li>æ¨å‡ºäº†åŒ…å«10ä¸‡ç»„é«˜è´¨é‡åˆæˆå›¾åƒ-æ–‡å­—æè¿°å¯¹çš„æ•°æ®é›†SynthVLM-100Kã€‚</li>
<li>åŸºäºSynthVLM-100Kæ•°æ®é›†ï¼Œç ”å‘äº†æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰â€”â€”SynthVLM-7Bå’ŒSynthVLM-13Bã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>åœ¨ä»…ä½¿ç”¨å°‘é‡é¢„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ¨¡å‹å·²è¶…è¶ŠLLaVAåœ¨å¤šæ•°æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b623e02bb8b072379dc36d7916125286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6dce7ddbe61c1166b3e6618cd439c33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41b3fb4c4b495b987bbedceb762b5efa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b77e28fd620a30158a18c0fec5c9f028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f6e2355fbb6b4b94891bb5689bac9ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d4932c4d40f58fcb36fac4aa90f1cfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef951258f04c36af45882abd390ee095.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DreamStory-Open-Domain-Story-Visualization-by-LLM-Guided-Multi-Subject-Consistent-Diffusion"><a href="#DreamStory-Open-Domain-Story-Visualization-by-LLM-Guided-Multi-Subject-Consistent-Diffusion" class="headerlink" title="DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject   Consistent Diffusion"></a>DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject   Consistent Diffusion</h2><p><strong>Authors:Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang, Zeyu Liu, Wenhao Huang, Hongyang Chao, Jian Yin</strong></p>
<p>Story visualization aims to create visually compelling images or videos corresponding to textual narratives. Despite recent advances in diffusion models yielding promising results, existing methods still struggle to create a coherent sequence of subject-consistent frames based solely on a story. To this end, we propose DreamStory, an automatic open-domain story visualization framework by leveraging the LLMs and a novel multi-subject consistent diffusion model. DreamStory consists of (1) an LLM acting as a story director and (2) an innovative Multi-Subject consistent Diffusion model (MSD) for generating consistent multi-subject across the images. First, DreamStory employs the LLM to generate descriptive prompts for subjects and scenes aligned with the story, annotating each sceneâ€™s subjects for subsequent subject-consistent generation. Second, DreamStory utilizes these detailed subject descriptions to create portraits of the subjects, with these portraits and their corresponding textual information serving as multimodal anchors (guidance). Finally, the MSD uses these multimodal anchors to generate story scenes with consistent multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA and MMCA modules ensure appearance and semantic consistency with reference images and text, respectively. Both modules employ masking mechanisms to prevent subject blending. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, which can assess the overall performance of the story visualization framework, subject-identification accuracy, and the consistency of the generation model. Extensive experiments validate the effectiveness of DreamStory in both subjective and objective evaluations. Please visit our project homepage at <a target="_blank" rel="noopener" href="https://dream-xyz.github.io/dreamstory">https://dream-xyz.github.io/dreamstory</a>. </p>
<blockquote>
<p>æ•…äº‹å¯è§†åŒ–æ—¨åœ¨æ ¹æ®æ–‡æœ¬å™äº‹åˆ›å»ºå…·æœ‰è§†è§‰å¸å¼•åŠ›çš„å›¾åƒæˆ–è§†é¢‘ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹é¢†åŸŸæœ€è¿‘çš„è¿›å±•å¸¦æ¥äº†å……æ»¡å¸Œæœ›çš„ç»“æœï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥ä»…æ ¹æ®æ•…äº‹æ¥åˆ›å»ºè¿è´¯ä¸”ä¸»é¢˜ä¸€è‡´çš„ç”»é¢åºåˆ—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DreamStoryï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ–°å‹å¤šä¸»é¢˜ä¸€è‡´æ‰©æ•£æ¨¡å‹çš„è‡ªåŠ¨å¼€æ”¾åŸŸæ•…äº‹å¯è§†åŒ–æ¡†æ¶ã€‚DreamStoryåŒ…æ‹¬ï¼ˆ1ï¼‰ä½œä¸ºæ•…äº‹å¯¼æ¼”çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œï¼ˆ2ï¼‰ç”¨äºç”Ÿæˆå›¾åƒä¸­ä¸€è‡´å¤šä¸»é¢˜çš„åˆ›æ–°å¤šä¸»é¢˜ä¸€è‡´æ‰©æ•£æ¨¡å‹ï¼ˆMSDï¼‰ã€‚é¦–å…ˆï¼ŒDreamStoryä½¿ç”¨LLMç”Ÿæˆä¸æ•…äº‹å¯¹é½çš„ä¸»é¢˜å’Œåœºæ™¯çš„æè¿°æ€§æç¤ºï¼Œå¹¶ä¸ºæ¯ä¸ªåœºæ™¯çš„ä¸»é¢˜æ˜¯è¿›è¡Œæ ‡æ³¨ï¼Œä»¥ä¾¿äºåç»­çš„ä¸»é¢˜ä¸€è‡´ç”Ÿæˆã€‚å…¶æ¬¡ï¼ŒDreamStoryåˆ©ç”¨è¿™äº›è¯¦ç»†çš„ä¸»é¢˜æè¿°æ¥åˆ›å»ºä¸»é¢˜äººç‰©çš„è‚–åƒï¼Œè¿™äº›è‚–åƒåŠå…¶ç›¸åº”çš„æ–‡æœ¬ä¿¡æ¯ä½œä¸ºå¤šæ¨¡æ€é”šç‚¹ï¼ˆæŒ‡å¯¼ï¼‰ã€‚æœ€åï¼ŒMSDä½¿ç”¨è¿™äº›å¤šæ¨¡æ€é”šç‚¹æ¥ç”Ÿæˆå…·æœ‰ä¸€è‡´å¤šä¸»é¢˜çš„æ•…äº‹åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼ŒMSDåŒ…æ‹¬Masked Mutual Self-Attentionï¼ˆMMSAï¼‰å’ŒMasked Mutual Cross-Attentionï¼ˆMMCAï¼‰æ¨¡å—ã€‚MMSAå’ŒMMCAæ¨¡å—åˆ†åˆ«ç¡®ä¿ä¸å‚è€ƒå›¾åƒçš„å¤–è§‚å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸¤ä¸ªæ¨¡å—éƒ½ä½¿ç”¨å±è”½æœºåˆ¶æ¥é˜²æ­¢ä¸»é¢˜æ··åˆã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•å’Œä¿ƒè¿›æ•…äº‹å¯è§†åŒ–é¢†åŸŸçš„è¿›å±•ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•DS-500ï¼Œå®ƒå¯ä»¥è¯„ä¼°æ•…äº‹å¯è§†åŒ–æ¡†æ¶çš„æ•´ä½“æ€§èƒ½ã€ä¸»é¢˜è¯†åˆ«å‡†ç¡®æ€§å’Œç”Ÿæˆæ¨¡å‹çš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†DreamStoryåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://dream-xyz.github.io/dreamstory%E3%80%82">https://dream-xyz.github.io/dreamstoryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12899v3">PDF</a> Accepted by TPAMI</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ•…äº‹å¯è§†åŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ–°å‹å¤šä¸»é¢˜ä¸€è‡´æ‰©æ•£æ¨¡å‹çš„è‡ªåŠ¨å¼€æ”¾åŸŸæ•…äº‹å¯è§†åŒ–æ¡†æ¶DreamStoryã€‚DreamStoryé€šè¿‡LLMç”Ÿæˆä¸æ•…äº‹ç›¸ç¬¦çš„æè¿°æ€§æç¤ºï¼Œä¸ºåœºæ™¯å’Œä¸»é¢˜æä¾›æ³¨è§£ï¼Œå¹¶åˆ©ç”¨è¿™äº›æè¿°åˆ›å»ºäººç‰©è‚–åƒä½œä¸ºå¤šæ¨¡æ€é”šç‚¹ï¼ŒæŒ‡å¯¼åç»­ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¤šä¸»é¢˜ä¸€è‡´æ‰©æ•£æ¨¡å‹ï¼ˆMSDï¼‰ï¼ŒåŒ…æ‹¬Masked Mutual Self-Attention (MMSA)å’ŒMasked Mutual Cross-Attention (MMCA)æ¨¡å—ï¼Œç¡®ä¿å›¾åƒå’Œæ–‡æœ¬çš„ä¸€è‡´æ€§å’Œå‚ç…§æ€§ã€‚æœ€åå»ºç«‹äº†è¯„ä¼°æ•…äº‹å¯è§†åŒ–æ¡†æ¶æ€§èƒ½çš„åŸºå‡†DS-500ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamStoryæ˜¯ä¸€ä¸ªç»“åˆLLMå’Œæ–°å‹å¤šä¸»é¢˜ä¸€è‡´æ‰©æ•£æ¨¡å‹çš„è‡ªåŠ¨å¼€æ”¾åŸŸæ•…äº‹å¯è§†åŒ–æ¡†æ¶ã€‚</li>
<li>LLMç”¨äºç”Ÿæˆæè¿°æ€§æç¤ºï¼Œä¸ºæ•…äº‹åœºæ™¯å’Œä¸»é¢˜æä¾›æ³¨è§£ã€‚</li>
<li>åˆ©ç”¨è¯¦ç»†çš„ä¸»é¢˜æè¿°åˆ›å»ºäººç‰©è‚–åƒä½œä¸ºå¤šæ¨¡æ€é”šç‚¹ï¼ˆæŒ‡å¯¼ç”Ÿæˆï¼‰ã€‚</li>
<li>MSDæ¨¡å‹åŒ…æ‹¬MMSAå’ŒMMCAæ¨¡å—ï¼Œç¡®ä¿å›¾åƒå’Œæ–‡æœ¬çš„ä¸€è‡´æ€§å’Œå‚ç…§æ€§ã€‚</li>
<li>å»ºç«‹äº†DS-500åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ•…äº‹å¯è§†åŒ–æ¡†æ¶çš„æ€§èƒ½ã€ä¸»é¢˜è¯†åˆ«å‡†ç¡®æ€§å’Œç”Ÿæˆæ¨¡å‹çš„ä¸€è‡´æ€§ã€‚</li>
<li>DreamStoryåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-705014cd3394c2c5f70a82d4906fee5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a44805281088f09d77fadce5d5eaf81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bdf02fac29e33ad0cdf0b670f3cee7c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PointDreamer-Zero-shot-3D-Textured-Mesh-Reconstruction-from-Colored-Point-Cloud"><a href="#PointDreamer-Zero-shot-3D-Textured-Mesh-Reconstruction-from-Colored-Point-Cloud" class="headerlink" title="PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored   Point Cloud"></a>PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored   Point Cloud</h2><p><strong>Authors:Qiao Yu, Xianzhi Li, Yuan Tang, Xu Han, Jinfeng Xu, Long Hu, Min Chen</strong></p>
<p>Faithfully reconstructing textured meshes is crucial for many applications. Compared to text or image modalities, leveraging 3D colored point clouds as input (colored-PC-to-mesh) offers inherent advantages in comprehensively and precisely replicating the target objectâ€™s 360{\deg} characteristics. While most existing colored-PC-to-mesh methods suffer from blurry textures or require hard-to-acquire 3D training data, we propose PointDreamer, a novel framework that harnesses 2D diffusion prior for superior texture quality. Crucially, unlike prior 2D-diffusion-for-3D works driven by text or image inputs, PointDreamer successfully adapts 2D diffusion models to 3D point cloud data by a novel project-inpaint-unproject pipeline. Specifically, it first projects the point cloud into sparse 2D images and then performs diffusion-based inpainting. After that, diverging from most existing 3D reconstruction or generation approaches that predict texture in 3D&#x2F;UV space thus often yielding blurry texture, PointDreamer achieves high-quality texture by directly unprojecting the inpainted 2D images to the 3D mesh. Furthermore, we identify for the first time a typical kind of unprojection artifact appearing in occlusion borders, which is common in other multiview-image-to-3D pipelines but less-explored. To address this, we propose a novel solution named the Non-Border-First (NBF) unprojection strategy. Extensive qualitative and quantitative experiments on various synthetic and real-scanned datasets demonstrate that PointDreamer, though zero-shot, exhibits SoTA performance (30% improvement on LPIPS score from 0.118 to 0.068), and is robust to noisy, sparse, or even incomplete input data. Code at: <a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/PointDreamer">https://github.com/YuQiao0303/PointDreamer</a>. </p>
<blockquote>
<p>çº¹ç†ç½‘æ ¼çš„å¿ å®é‡å»ºå¯¹äºè®¸å¤šåº”ç”¨è‡³å…³é‡è¦ã€‚ä¸æ–‡æœ¬æˆ–å›¾åƒæ¨¡å¼ç›¸æ¯”ï¼Œåˆ©ç”¨ä¸‰ç»´å½©è‰²ç‚¹äº‘ä½œä¸ºè¾“å…¥ï¼ˆå½©è‰²ç‚¹äº‘åˆ°ç½‘æ ¼ï¼‰åœ¨å…¨é¢å’Œç²¾ç¡®åœ°å¤åˆ¶ç›®æ ‡å¯¹è±¡çš„360Â°ç‰¹å¾æ–¹é¢å…·æœ‰å›ºæœ‰çš„ä¼˜åŠ¿ã€‚å°½ç®¡å¤§å¤šæ•°ç°æœ‰çš„å½©è‰²ç‚¹äº‘åˆ°ç½‘æ ¼çš„æ–¹æ³•å­˜åœ¨çº¹ç†æ¨¡ç³Šçš„é—®é¢˜ï¼Œæˆ–è€…éœ€è¦éš¾ä»¥è·å–çš„ä¸‰ç»´è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†PointDreamerï¼Œä¸€ä¸ªåˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒæ¥è·å¾—ä¼˜è´¨çº¹ç†çš„æ–°æ¡†æ¶ã€‚å…³é”®çš„æ˜¯ï¼Œä¸åŒäºä»¥å¾€å—æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥é©±åŠ¨çš„äºŒç»´æ‰©æ•£åˆ°ä¸‰ç»´çš„å·¥ä½œï¼ŒPointDreameré€šè¿‡æ–°é¢–çš„é¡¹ç›®-å¡«å……-åé¡¹ç›®ç®¡é“æˆåŠŸåœ°å°†äºŒç»´æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ä¸‰ç»´ç‚¹äº‘æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé¦–å…ˆå°†ç‚¹äº‘æŠ•å½±åˆ°ç¨€ç–çš„äºŒç»´å›¾åƒä¸Šï¼Œç„¶åè¿›è¡ŒåŸºäºæ‰©æ•£çš„å¡«å……ã€‚ä¹‹åï¼Œä¸åŒäºå¤§å¤šæ•°ç°æœ‰çš„ä¸‰ç»´é‡å»ºæˆ–ç”Ÿæˆæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¸‰ç»´&#x2F;UVç©ºé—´ä¸­è¿›è¡Œçº¹ç†é¢„æµ‹ï¼Œå› æ­¤é€šå¸¸ä¼šäº§ç”Ÿæ¨¡ç³Šçš„çº¹ç†ï¼ŒPointDreameré€šè¿‡ç›´æ¥å°†å¡«å……çš„äºŒç»´å›¾åƒæŠ•å½±åˆ°ä¸‰ç»´ç½‘æ ¼ä¸Šï¼Œå®ç°äº†é«˜è´¨é‡çš„çº¹ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å‘ç°äº†ä¸€ç§åœ¨é®æŒ¡è¾¹ç•Œä¸­å‡ºç°çš„å…¸å‹æŠ•å½±ä¼ªå½±ï¼Œè¿™åœ¨å…¶ä»–å¤šè§†å›¾å›¾åƒåˆ°ä¸‰ç»´çš„ç®¡é“ä¸­å¾ˆå¸¸è§ï¼Œä½†ç ”ç©¶è¾ƒå°‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºéè¾¹ç•Œä¼˜å…ˆï¼ˆNBFï¼‰çš„æŠ•å½±ç­–ç•¥ã€‚åœ¨å„ç§åˆæˆå’ŒçœŸå®æ‰«ææ•°æ®é›†ä¸Šçš„å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒPointDreamerè™½ç„¶ä¸æ˜¯é€æ ·æœ¬æ‹æ‘„ï¼ˆzero-shotï¼‰ï¼Œä½†å±•ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆåœ¨LPIPSå¾—åˆ†ä¸Šæé«˜äº†30%ï¼Œä»0.118æé«˜åˆ°0.068ï¼‰ï¼Œå¹¶ä¸”å¯¹å™ªå£°ã€ç¨€ç–ç”šè‡³ä¸å®Œæ•´çš„æ•°æ®å…·æœ‰ç¨³å¥æ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/PointDreamer%E3%80%82">https://github.com/YuQiao0303/PointDreamerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15811v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPointDreamerçš„æ–°æ¡†æ¶ï¼Œç”¨äºä»å½©è‰²ç‚¹äº‘é‡å»ºçº¹ç†ç½‘æ ¼ã€‚è¯¥æ¡†æ¶åˆ©ç”¨äºŒç»´æ‰©æ•£å…ˆéªŒå®ç°é«˜è´¨é‡çº¹ç†ï¼Œé€šè¿‡æŠ•å½±ç‚¹äº‘åˆ°ç¨€ç–äºŒç»´å›¾åƒè¿›è¡Œæ‰©æ•£ä¿®å¤ï¼Œç„¶åç›´æ¥æŠ•å½±ä¿®å¤åçš„äºŒç»´å›¾åƒåˆ°ä¸‰ç»´ç½‘æ ¼å®ç°é«˜è´¨é‡çº¹ç†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è§£å†³äº†åœ¨é®æŒ¡è¾¹ç•Œä¸­å‡ºç°çš„æŠ•å½±ä¼ªå½±é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒPointDreamerå…·æœ‰é¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PointDreameræ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå¯ä»¥ä»å½©è‰²ç‚¹äº‘é‡å»ºé«˜è´¨é‡çº¹ç†ç½‘æ ¼ã€‚å®ƒåˆ©ç”¨äº†äºŒç»´æ‰©æ•£å…ˆéªŒæ¥å®ç°æ­¤ç›®çš„ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å°†ç‚¹äº‘æŠ•å½±åˆ°ç¨€ç–äºŒç»´å›¾åƒï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œä¿®å¤æ¥å¢å¼ºçº¹ç†è´¨é‡ã€‚ä¿®å¤åçš„å›¾åƒè¢«ç›´æ¥æŠ•å½±åˆ°ä¸‰ç»´ç½‘æ ¼ä¸Šã€‚è¿™ä¸åŒäºå¤§å¤šæ•°åœ¨ä¸‰ç»´ç©ºé—´æˆ–UVç©ºé—´é¢„æµ‹çº¹ç†çš„æ–¹æ³•ï¼Œä»è€Œé¿å…äº†æ¨¡ç³Šçº¹ç†çš„é—®é¢˜ã€‚</li>
<li>è§£å†³äº†ä¸€ç§æ–°çš„æŠ•å½±ä¼ªå½±é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é®æŒ¡è¾¹ç•Œå¤„çš„é—®é¢˜ã€‚è¯¥é—®é¢˜åœ¨å…¶ä»–å¤šè§†è§’å›¾åƒåˆ°ä¸‰ç»´ç½‘æ ¼çš„è½¬æ¢è¿‡ç¨‹ä¸­æ™®éå­˜åœ¨ï¼Œä½†é²œæœ‰ç ”ç©¶æ¶‰åŠã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†éè¾¹ç•Œä¼˜å…ˆï¼ˆNBFï¼‰æŠ•å½±ç­–ç•¥æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a3ad975aa7278de56cbdf06c71839de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-803ce6d4e16748d387890020f2bd54c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cf36866dfe16de154ca4c1c9d008c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d7324953a96a30606d5d1375cae1b12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86fce3ee8515a725e28d6b3f00330512.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4e34dec88aa8644cbf6c7e40a5bbe4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdc977156450c4fd61c3ad6dbe03063a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-1bca56415dbad4ad884e53a6c73268e7.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Uncertainty-aware Cross-training for Semi-supervised Medical Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30a868c4bb872291800eb6fa60ffb12f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  MonoPartNeRFHuman Reconstruction from Monocular Video via Part-Based   Neural Radiance Fields
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
