<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  BrowseMaster Towards Scalable Web Browsing via Tool-Augmented   Programmatic Agent Pair">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8c4ce7d6db511d4b844d219239c9ff2c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-14-æ›´æ–°"><a href="#2025-08-14-æ›´æ–°" class="headerlink" title="2025-08-14 æ›´æ–°"></a>2025-08-14 æ›´æ–°</h1><h2 id="BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair"><a href="#BrowseMaster-Towards-Scalable-Web-Browsing-via-Tool-Augmented-Programmatic-Agent-Pair" class="headerlink" title="BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented   Programmatic Agent Pair"></a>BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented   Programmatic Agent Pair</h2><p><strong>Authors:Xianghe Pang, Shuo Tang, Rui Ye, Yuwen Du, Yaxin Du, Siheng Chen</strong></p>
<p>Effective information seeking in the vast and ever-growing digital landscape requires balancing expansive search with strategic reasoning. Current large language model (LLM)-based agents struggle to achieve this balance due to limitations in search breadth and reasoning depth, where slow, serial querying restricts coverage of relevant sources and noisy raw inputs disrupt the continuity of multi-step reasoning. To address these challenges, we propose BrowseMaster, a scalable framework built around a programmatically augmented planner-executor agent pair. The planner formulates and adapts search strategies based on task constraints, while the executor conducts efficient, targeted retrieval to supply the planner with concise, relevant evidence. This division of labor preserves coherent, long-horizon reasoning while sustaining broad and systematic exploration, overcoming the trade-off that limits existing agents. Extensive experiments on challenging English and Chinese benchmarks show that BrowseMaster consistently outperforms open-source and proprietary baselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh, which demonstrates its strong capability in complex, reasoning-heavy information-seeking tasks at scale. </p>
<blockquote>
<p>åœ¨åºå¤§ä¸”ä¸æ–­å¢é•¿çš„æ•°å­—æ™¯è§‚ä¸­è¿›è¡Œæœ‰æ•ˆçš„ä¿¡æ¯æœç´¢ï¼Œéœ€è¦åœ¨å¹¿æ³›çš„æœç´¢ä¸ç­–ç•¥æ¨ç†ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç”±äºå½“å‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨æœç´¢èŒƒå›´å’Œæ¨ç†æ·±åº¦æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´ä»–ä»¬éš¾ä»¥å®ç°è¿™ç§å¹³è¡¡ï¼Œå…¶ä¸­ç¼“æ…¢çš„ä¸²è¡ŒæŸ¥è¯¢é™åˆ¶äº†ç›¸å…³æ¥æºçš„è¦†ç›–ï¼Œè€Œå˜ˆæ‚çš„åŸå§‹è¾“å…¥ç ´åäº†å¤šæ­¥éª¤æ¨ç†çš„è¿ç»­æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†BrowseMasterï¼Œè¿™æ˜¯ä¸€ä¸ªå›´ç»•ç¨‹åºå¢å¼ºå‹è§„åˆ’å™¨æ‰§è¡Œå™¨ä»£ç†å¯¹æ„å»ºçš„å¯æ‰©å±•æ¡†æ¶ã€‚è§„åˆ’å™¨æ ¹æ®ä»»åŠ¡çº¦æŸåˆ¶å®šå¹¶é€‚åº”æœç´¢ç­–ç•¥ï¼Œè€Œæ‰§è¡Œè€…è¿›è¡Œé«˜æ•ˆã€æœ‰é’ˆå¯¹æ€§çš„æ£€ç´¢ï¼Œä¸ºè§„åˆ’å™¨æä¾›ç®€æ´ã€ç›¸å…³çš„è¯æ®ã€‚è¿™ç§åˆ†å·¥ä¿æŒäº†è¿è´¯ã€é•¿è¿œçš„æ¨ç†ï¼ŒåŒæ—¶ç»´æŒäº†å¹¿æ³›å’Œæœ‰ç³»ç»Ÿçš„æ¢ç´¢ï¼Œå…‹æœäº†é™åˆ¶ç°æœ‰ä»£ç†äººçš„æƒè¡¡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è‹±è¯­å’Œä¸­æ–‡åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBrowseMasterå§‹ç»ˆä¼˜äºå¼€æºå’Œä¸“æœ‰åŸºå‡†æµ‹è¯•ï¼Œåœ¨BrowseComp-enä¸Šå¾—åˆ†30.0ï¼Œåœ¨BrowseComp-zhä¸Šå¾—åˆ†46.5ï¼Œè¿™è¯æ˜äº†å…¶åœ¨å¤æ‚ã€é‡æ¨ç†çš„å¤§è§„æ¨¡ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09129v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¿¡æ¯çˆ†ç‚¸çš„æ•°å­—æ—¶ä»£ï¼Œæœ‰æ•ˆçš„ä¿¡æ¯æœç´¢éœ€è¦å¹³è¡¡å¹¿æ³›çš„æœç´¢å’Œç­–ç•¥æ€§æ¨ç†ã€‚å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ç”±äºæœç´¢å¹¿åº¦æœ‰é™å’Œæ¨ç†æ·±åº¦ä¸è¶³è€Œéš¾ä»¥å®ç°è¿™ç§å¹³è¡¡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BrowseMasteræ¡†æ¶ï¼Œè¯¥æ¡†æ¶å›´ç»•å¯ç¼–ç¨‹çš„å¢å¼ºå‹è§„åˆ’å™¨æ‰§è¡Œå™¨ä»£ç†å¯¹æ„å»ºè€Œæˆã€‚è§„åˆ’å™¨æ ¹æ®ä»»åŠ¡çº¦æŸåˆ¶å®šå’Œè°ƒæ•´æœç´¢ç­–ç•¥ï¼Œè€Œæ‰§è¡Œå™¨è¿›è¡Œé«˜æ•ˆã€æœ‰é’ˆå¯¹æ€§çš„æ£€ç´¢ï¼Œä¸ºè§„åˆ’å™¨æä¾›ç®€æ´ã€ç›¸å…³çš„è¯æ®ã€‚è¿™ç§åˆ†å·¥åˆä½œæ—¢èƒ½ä¿æŒè¿è´¯çš„é•¿è¿œæ¨ç†åˆèƒ½ç»´æŒå¹¿æ³›çš„ç³»ç»Ÿæ€§æ¢ç´¢ï¼Œçªç ´äº†ç°æœ‰ä»£ç†çš„é™åˆ¶ã€‚ç»å¤§é‡è‹±è¯­å’Œä¸­æ–‡åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒBrowseMasterçš„æ€§èƒ½æŒç»­ä¼˜äºå¼€æºå’Œä¸“æœ‰åŸºçº¿ï¼Œåœ¨BrowseComp-enä¸Šçš„å¾—åˆ†è¾¾åˆ°30.0ï¼Œåœ¨BrowseComp-zhä¸Šçš„å¾—åˆ†è¾¾åˆ°46.5ï¼Œè¯æ˜å…¶åœ¨å¤§è§„æ¨¡å¤æ‚æ¨ç†å‹ä¿¡æ¯æœç´¢ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¿¡æ¯çˆ†ç‚¸æ—¶ä»£éœ€è¦æœ‰æ•ˆå¹³è¡¡å¹¿æ³›æœç´¢å’Œç­–ç•¥æ€§æ¨ç†ã€‚</li>
<li>å½“å‰LLMä»£ç†é¢ä¸´æœç´¢å¹¿åº¦ä¸æ¨ç†æ·±åº¦é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>BrowseMasteræ¡†æ¶é€šè¿‡è§„åˆ’å™¨ä¸æ‰§è¡Œå™¨çš„ç»“åˆæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>è§„åˆ’å™¨æ ¹æ®ä»»åŠ¡çº¦æŸåˆ¶å®šå’Œè°ƒæ•´æœç´¢ç­–ç•¥ã€‚</li>
<li>æ‰§è¡Œå™¨è¿›è¡Œé«˜æ•ˆã€æœ‰é’ˆå¯¹æ€§çš„æ£€ç´¢ï¼Œä¸ºè§„åˆ’å™¨æä¾›ç®€æ´è¯æ®ã€‚</li>
<li>è¿™ç§åˆ†å·¥åˆä½œå®ç°è¿è´¯çš„é•¿è¿œæ¨ç†ä¸å¹¿æ³›çš„ç³»ç»Ÿæ€§æ¢ç´¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90d5dd428b9cddae8711f44c26dae7af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-029a295f0f9b909a1632d8fbac54c64a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b04aa30a33b91b8744f50405180ea2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Complex-Logical-Instruction-Generation"><a href="#Complex-Logical-Instruction-Generation" class="headerlink" title="Complex Logical Instruction Generation"></a>Complex Logical Instruction Generation</h2><p><strong>Authors:Mian Zhang, Shujian Liu, Sixun Dong, Ming Yin, Yebowen Hu, Xun Wang, Steven Ma, Song Wang, Sathish Reddy Indurthi, Haoyun Deng, Zhiyu Zoey Chen, Kaiqiang Song</strong></p>
<p>Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: <a target="_blank" rel="noopener" href="https://github.com/mianzhang/LogicIF">https://github.com/mianzhang/LogicIF</a> </p>
<blockquote>
<p>æ¥ä¸‹æ¥çš„æŒ‡ä»¤å·²ç»å‚¬ç”Ÿäº†è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£ï¼Œå¹¶ä¸”æ˜¯æ”¯æ’‘æ¨ç†å’Œæ™ºèƒ½è¡Œä¸ºç­‰æ›´é«˜çº§æŠ€èƒ½çš„åŸºç¡€ã€‚éšç€ä»»åŠ¡å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­åµŒå…¥çš„é€»è¾‘ç»“æ„ä¹Ÿå˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚ç„¶è€Œï¼ŒLLMåœ¨é€»è¾‘ä¸°å¯Œçš„æŒ‡ä»¤ä¸Šçš„è¡¨ç°å¦‚ä½•ä»æœ‰å¾…æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†LogicIFGenå’ŒLogicIFEvalã€‚LogicIFGenæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¯ä»¥ä»ä»£ç å‡½æ•°ä¸­ç”Ÿæˆå¯éªŒè¯çš„æŒ‡ä»¤ï¼Œå®ƒèƒ½å¤Ÿè‡ªç„¶åœ°è¡¨è¾¾ä¸°å¯Œçš„é€»è¾‘ï¼Œå¦‚æ¡ä»¶ã€åµŒå¥—ã€é€’å½’å’Œå‡½æ•°è°ƒç”¨ã€‚æˆ‘ä»¬è¿˜ç²¾å¿ƒæ”¶é›†äº†ä¸€ç³»åˆ—å¤æ‚çš„ä»£ç å‡½æ•°ï¼Œå¹¶ä½¿ç”¨LogicIFGenæ„å»ºäº†LogicIFEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«426ä¸ªå¯éªŒè¯çš„é€»è¾‘ä¸°å¯ŒæŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMä»ç„¶éš¾ä»¥æ­£ç¡®éµå¾ªLogicIFEvalä¸­çš„æŒ‡ä»¤ã€‚å¤§å¤šæ•°LLMåªèƒ½éµå¾ªä¸åˆ°60%çš„æŒ‡ä»¤ï¼Œè¿™æš´éœ²äº†å…¶åœ¨æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ä¸Šçš„æ˜æ˜¾ç¼ºé™·ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•ï¼š<a target="_blank" rel="noopener" href="https://github.com/mianzhang/LogicIF">https://github.com/mianzhang/LogicIF</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09125v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>LLMæ—¶ä»£çš„æŒ‡ä»¤è·Ÿéšå‚¬ç”Ÿäº†ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å…·å¤‡æ¨ç†å’Œè‡ªä¸»è¡Œä¸ºç­‰é«˜çº§èƒ½åŠ›çš„åŸºç¡€æŠ€èƒ½ã€‚éšç€ä»»åŠ¡éš¾åº¦å¢åŠ ï¼Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­çš„é€»è¾‘ç»“æ„æ„ˆå‘å¤æ‚ï¼Œä½†LLMåœ¨è¿™äº›é€»è¾‘ä¸°å¯ŒæŒ‡ä»¤ä¸Šçš„è¡¨ç°å°šå¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºLogicIFGenå’ŒLogicIFEvalï¼Œå‰è€…æ˜¯ä¸€ä¸ªå¯è§„æ¨¡åŒ–ã€è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œèƒ½ä»ä»£ç åŠŸèƒ½ä¸­ç”Ÿæˆå¯éªŒè¯çš„æŒ‡ä»¤ï¼Œè‡ªç„¶è¡¨è¾¾æ¡ä»¶ã€åµŒå¥—ã€é€’å½’å’Œå‡½æ•°è°ƒç”¨ç­‰ä¸°å¯Œé€»è¾‘ï¼›åè€…åˆ™æ˜¯ä¸€ä¸ªåŒ…å«å¤æ‚ä»£ç åŠŸèƒ½çš„åŸºå‡†æµ‹è¯•é›†ï¼Œé€šè¿‡LogicIFGenç”Ÿæˆé€»è¾‘ä¸°å¯Œçš„å¯éªŒè¯æŒ‡ä»¤ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMåœ¨LogicIFEvalä¸­çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä»ç„¶ä¸è¶³ï¼Œåªèƒ½æ­£ç¡®è·Ÿéšä¸åˆ°60%çš„æŒ‡ä»¤ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç¼ºé™·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æŒ‡ä»¤è·Ÿéšæ–¹é¢å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>éšç€ä»»åŠ¡éš¾åº¦çš„å¢åŠ ï¼Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­çš„é€»è¾‘ç»“æ„æ„ˆå‘å¤æ‚ã€‚</li>
<li>å½“å‰LLMåœ¨é€»è¾‘ä¸°å¯ŒæŒ‡ä»¤ä¸Šçš„è¡¨ç°å°šå¾…æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†LogicIFGenæ¡†æ¶ï¼Œå¯ä»¥ä»ä»£ç åŠŸèƒ½ä¸­ç”Ÿæˆå¯éªŒè¯çš„æŒ‡ä»¤ï¼Œæ”¯æŒè¡¨è¾¾ä¸°å¯Œé€»è¾‘ã€‚</li>
<li>æ„å»ºäº†LogicIFEvalåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«å¤æ‚ä»£ç åŠŸèƒ½çš„é€»è¾‘ä¸°å¯ŒæŒ‡ä»¤ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå½“å‰æœ€å…ˆè¿›çš„LLMåœ¨LogicIFEvalä¸­çš„è¡¨ç°ä¸ä½³ï¼Œåªèƒ½æ­£ç¡®è·Ÿéšå°‘æ•°æŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e672e52ced37d92add42fb45516b6769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61ecc5c533c6199b811232087fbe1ca3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OdysseyBench-Evaluating-LLM-Agents-on-Long-Horizon-Complex-Office-Application-Workflows"><a href="#OdysseyBench-Evaluating-LLM-Agents-on-Long-Horizon-Complex-Office-Application-Workflows" class="headerlink" title="OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office   Application Workflows"></a>OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office   Application Workflows</h2><p><strong>Authors:Weixuan Wang, Dongge Han, Daniel Madrigal Diaz, Jin Xu, Victor RÃ¼hle, Saravan Rajmohan</strong></p>
<p>Autonomous agents powered by large language models (LLMs) are increasingly deployed in real-world applications requiring complex, long-horizon workflows. However, existing benchmarks predominantly focus on atomic tasks that are self-contained and independent, failing to capture the long-term contextual dependencies and multi-interaction coordination required in realistic scenarios. To address this gap, we introduce OdysseyBench, a comprehensive benchmark for evaluating LLM agents on long-horizon workflows across diverse office applications including Word, Excel, PDF, Email, and Calendar. Our benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks derived from real-world use cases, and OdysseyBench-Neo with 302 newly synthesized complex tasks. Each task requires agent to identify essential information from long-horizon interaction histories and perform multi-step reasoning across various applications. To enable scalable benchmark creation, we propose HomerAgents, a multi-agent framework that automates the generation of long-horizon workflow benchmarks through systematic environment exploration, task generation, and dialogue synthesis. Our extensive evaluation demonstrates that OdysseyBench effectively challenges state-of-the-art LLM agents, providing more accurate assessment of their capabilities in complex, real-world contexts compared to existing atomic task benchmarks. We believe that OdysseyBench will serve as a valuable resource for advancing the development and evaluation of LLM agents in real-world productivity scenarios. In addition, we release OdysseyBench and HomerAgents to foster research along this line. </p>
<blockquote>
<p>ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„è‡ªä¸»å¯¼èˆªä»£ç†è¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨éœ€è¦å¤æ‚ã€é•¿æœŸå·¥ä½œæµçš„ç°å®åº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨è‡ªæˆ‘å®Œå–„å’Œç‹¬ç«‹çš„åŸå­ä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰ç°å®åœºæ™¯ä¸­æ‰€éœ€çš„é•¿æœŸä¸Šä¸‹æ–‡ä¾èµ–æ€§å’Œå¤šäº¤äº’åè°ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OdysseyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°LLMä»£ç†åœ¨é•¿æœŸå·¥ä½œæµæ–¹é¢æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šæ ·åŒ–çš„åŠå…¬åº”ç”¨ï¼ŒåŒ…æ‹¬Wordã€Excelã€PDFã€ç”µå­é‚®ä»¶å’Œæ—¥å†ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥çš„éƒ¨åˆ†ï¼šOdysseyBench+ï¼ŒåŒ…å«300ä¸ªæ¥è‡ªçœŸå®ä¸–ç•Œç”¨ä¾‹çš„ä»»åŠ¡ï¼›ä»¥åŠOdysseyBench-Neoï¼ŒåŒ…å«302ä¸ªæ–°åˆæˆçš„å¤æ‚ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡éƒ½è¦æ±‚ä»£ç†ä»é•¿æœŸäº¤äº’å†å²ä¸­è¯†åˆ«å…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ç¨‹åºä¸­è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ã€‚ä¸ºäº†å®ç°å¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•åˆ›å»ºï¼Œæˆ‘ä»¬æå‡ºäº†HomerAgentsï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿçš„ç¯å¢ƒæ¢ç´¢ã€ä»»åŠ¡ç”Ÿæˆå’Œå¯¹è¯åˆæˆï¼Œè‡ªåŠ¨ç”Ÿæˆé•¿æœŸå·¥ä½œæµåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒOdysseyBenchæœ‰æ•ˆåœ°æŒ‘æˆ˜äº†æœ€å…ˆè¿›çš„LLMä»£ç†ï¼Œä¸ç°æœ‰çš„åŸå­ä»»åŠ¡åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæ›´èƒ½å‡†ç¡®åœ°è¯„ä¼°å®ƒä»¬åœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒOdysseyBenchå°†æˆä¸ºæ¨åŠ¨å’Œè¯„ä¼°LLMä»£ç†åœ¨ç°å®ç”Ÿäº§åœºæ™¯ä¸­çš„å‘å±•çš„å®è´µèµ„æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒOdysseyBenchå’ŒHomerAgentsä»¥ä¿ƒè¿›è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09124v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¼•å…¥OdysseyBenchï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°LLMä»£ç†é•¿æœŸå·¥ä½œæµç¨‹çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–Wordã€Excelã€PDFã€Emailå’ŒCalendarç­‰å¤šæ ·åŒ–åŠå…¬åº”ç”¨ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸¤ä¸ªäº’è¡¥éƒ¨åˆ†ï¼šOdysseyBench+å’ŒOdysseyBench-Neoï¼Œåˆ†åˆ«åŒ…å«æ¥è‡ªçœŸå®ä¸–ç•Œç”¨ä¾‹çš„300ä¸ªä»»åŠ¡å’Œå…¨æ–°åˆæˆçš„302ä¸ªå¤æ‚ä»»åŠ¡ã€‚æ¯ä¸ªä»»åŠ¡è¦æ±‚ä»£ç†è¯†åˆ«é•¿æœŸäº¤äº’å†å²ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ç¨‹åºä¸­è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ã€‚æå‡ºHomerAgentså¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿç¯å¢ƒæ¢ç´¢ã€ä»»åŠ¡ç”Ÿæˆå’Œå¯¹è¯åˆæˆï¼Œå®ç°é•¿æœŸå·¥ä½œæµç¨‹åŸºå‡†æµ‹è¯•çš„è‡ªåŠ¨ç”Ÿæˆã€‚OdysseyBenchç›¸æ¯”ç°æœ‰çš„åŸå­ä»»åŠ¡åŸºå‡†æµ‹è¯•ï¼Œæ›´æœ‰æ•ˆåœ°æŒ‘æˆ˜äº†æœ€å…ˆè¿›çš„LLMä»£ç†ï¼Œä¸ºå¤æ‚ç°å®è¯­å¢ƒä¸­è¯„ä¼°å…¶èƒ½åŠ›æä¾›äº†æ›´å‡†ç¡®çš„è¯„ä¼°ã€‚ç›¸ä¿¡OdysseyBenchå°†æˆä¸ºæ¨åŠ¨LLMä»£ç†åœ¨çœŸå®ä¸–ç•Œç”Ÿäº§åŠ›åœºæ™¯ä¸­åº”ç”¨å’Œå‘å±•çš„å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼•å…¥OdysseyBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMä»£ç†åœ¨æ¶‰åŠé•¿æœŸå·¥ä½œæµç¨‹çš„å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„è¡¨ç°ã€‚</li>
<li>OdysseyBenchåŒ…å«ä¸¤ä¸ªäº’è¡¥éƒ¨åˆ†ï¼šOdysseyBench+å’ŒOdysseyBench-Neoï¼Œæ¶µç›–å¤šç§åŠå…¬åº”ç”¨åœºæ™¯çš„å®é™…ä»»åŠ¡å’Œåˆæˆå¤æ‚ä»»åŠ¡ã€‚</li>
<li>æ¯ä¸ªä»»åŠ¡éœ€è¦ä»£ç†è¯†åˆ«é•¿æœŸäº¤äº’å†å²ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒåº”ç”¨ç¨‹åºä¸­è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ã€‚</li>
<li>æå‡ºHomerAgentså¤šä»£ç†æ¡†æ¶ï¼Œå®ç°é•¿æœŸå·¥ä½œæµç¨‹åŸºå‡†æµ‹è¯•çš„è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…æ‹¬ç¯å¢ƒæ¢ç´¢ã€ä»»åŠ¡ç”Ÿæˆå’Œå¯¹è¯åˆæˆç­‰ç¯èŠ‚ã€‚</li>
<li>OdysseyBenchå¯¹ç°æœ‰LLMä»£ç†æå‡ºäº†æœ‰æ•ˆæŒ‘æˆ˜ï¼Œç›¸æ¯”åŸå­ä»»åŠ¡åŸºå‡†æµ‹è¯•èƒ½æ›´å‡†ç¡®åœ°è¯„ä¼°ä»£ç†åœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚</li>
<li>OdysseyBenchè¢«è®¤ä¸ºæ˜¯æ¨åŠ¨LLMä»£ç†åœ¨çœŸå®ä¸–ç•Œç”Ÿäº§åŠ›åœºæ™¯ä¸­åº”ç”¨å’Œå‘å±•çš„å®è´µèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09124">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60fddf407c880022cb6691b01cc1e36f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1e77a28bf42556c04f23ee0a7d2c91d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0077e226d9772633a79fdf5ef40a6b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8586dfd501274a2b3bb42a0c4f80a8c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scaling-Up-Active-Testing-to-Large-Language-Models"><a href="#Scaling-Up-Active-Testing-to-Large-Language-Models" class="headerlink" title="Scaling Up Active Testing to Large Language Models"></a>Scaling Up Active Testing to Large Language Models</h2><p><strong>Authors:Gabrielle Berrada, Jannik Kossen, Muhammed Razzak, Freddie Bickford Smith, Yarin Gal, Tom Rainforth</strong></p>
<p>Active testing enables label-efficient evaluation of models through careful data acquisition. However, its significant computational costs have previously undermined its use for large models. We show how it can be successfully scaled up to the evaluation of large language models (LLMs). In particular we show that the surrogate model used to guide data acquisition can be constructed cheaply using in-context learning, does not require updating within an active-testing loop, and can be smaller than the target model. We even find we can make good data-acquisition decisions without computing predictions with the target model and further introduce a single-run error estimator to asses how well active testing is working on the fly. We find that our approach is able to more effectively evaluate LLM performance with less data than current standard practices. </p>
<blockquote>
<p>ä¸»åŠ¨æµ‹è¯•èƒ½å¤Ÿé€šè¿‡ç²¾å¿ƒæ”¶é›†æ•°æ®ï¼Œå®ç°æ¨¡å‹çš„é«˜æ•ˆè¯„ä¼°ã€‚ç„¶è€Œï¼Œå…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ä¹‹å‰æ›¾å¯¹å¤§å‹æ¨¡å‹çš„è¯„ä¼°é€ æˆä¸åˆ©å½±å“ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†å…¶æˆåŠŸæ‰©å±•åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°ä¸­ã€‚å°¤å…¶è¦æŒ‡å‡ºçš„æ˜¯ï¼Œç”¨äºæŒ‡å¯¼æ•°æ®æ”¶é›†çš„æ›¿ä»£æ¨¡å‹å¯ä»¥ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å»‰ä»·æ„å»ºï¼Œä¸éœ€è¦åœ¨ä¸»åŠ¨æµ‹è¯•å¾ªç¯ä¸­è¿›è¡Œæ›´æ–°ï¼Œå¹¶ä¸”å¯ä»¥æ¯”ç›®æ ‡æ¨¡å‹æ›´å°ã€‚æˆ‘ä»¬ç”šè‡³å‘ç°ï¼Œæ— éœ€è®¡ç®—ç›®æ ‡æ¨¡å‹çš„é¢„æµ‹å€¼ä¹Ÿèƒ½åšå‡ºè‰¯å¥½çš„æ•°æ®é‡‡é›†å†³ç­–ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å•æ¬¡è¿è¡Œè¯¯å·®ä¼°è®¡å™¨ï¼Œä»¥å®æ—¶è¯„ä¼°ä¸»åŠ¨æµ‹è¯•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ¯”å½“å‰æ ‡å‡†å®è·µæ›´å°‘çš„æ•°æ®ä¸Šæ›´æœ‰æ•ˆåœ°è¯„ä¼°LLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09093v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç§¯ææµ‹è¯•ï¼ˆActive Testingï¼‰ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡ç²¾å¿ƒé€‰æ‹©æ•°æ®å®ç°æ ‡ç­¾çš„é«˜æ•ˆåˆ©ç”¨ã€‚è™½ç„¶ç§¯ææµ‹è¯•çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä½†æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å»‰ä»·åœ°æ„å»ºä»£ç†æ¨¡å‹å¼•å¯¼æ•°æ®é‡‡é›†æ¥å®ç°å…¶è§„æ¨¡åŒ–æ‰©å±•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€åœ¨ä¸»åŠ¨æµ‹è¯•å¾ªç¯ä¸­æ›´æ–°ä»£ç†æ¨¡å‹ï¼Œè€Œä¸”ä»£ç†æ¨¡å‹å¯ä»¥å°äºç›®æ ‡æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”šè‡³èƒ½å¤Ÿåœ¨ä¸è®¡ç®—ç›®æ ‡æ¨¡å‹é¢„æµ‹çš„æƒ…å†µä¸‹åšå‡ºè‰¯å¥½çš„æ•°æ®é‡‡é›†å†³ç­–ï¼Œå¹¶å¼•å…¥å•æ¬¡è¿è¡Œè¯¯å·®ä¼°è®¡å™¨æ¥å®æ—¶è¯„ä¼°ç§¯ææµ‹è¯•çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ä»¥è¾ƒå°‘çš„æ•°æ®æ›´æœ‰æ•ˆåœ°è¯„ä¼°LLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§¯ææµ‹è¯•å¯ç”¨äºæœ‰æ•ˆè¯„ä¼°LLMæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒé€‰æ‹©æ•°æ®å®ç°æ ‡ç­¾çš„é«˜æ•ˆåˆ©ç”¨ã€‚</li>
<li>åˆ©ç”¨å»‰ä»·æ„å»ºçš„ä»£ç†æ¨¡å‹è¿›è¡Œæ•°æ®é‡‡é›†æ‰©å±•ç§¯ææµ‹è¯•ã€‚</li>
<li>æ— éœ€åœ¨ä¸»åŠ¨æµ‹è¯•å¾ªç¯ä¸­æ›´æ–°ä»£ç†æ¨¡å‹ã€‚</li>
<li>ä»£ç†æ¨¡å‹å¯ä»¥å°äºç›®æ ‡æ¨¡å‹ã€‚</li>
<li>ä¸è®¡ç®—ç›®æ ‡æ¨¡å‹é¢„æµ‹ä¹Ÿèƒ½åšå‡ºè‰¯å¥½çš„æ•°æ®é‡‡é›†å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d3a3a353ea54b7193544ea7a889d214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd108ca42ee3ae26c0e5127f6ca5313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65e6ba08bb52a6f8548246021a9330d4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Utilizing-Multilingual-Encoders-to-Improve-Large-Language-Models-for-Low-Resource-Languages"><a href="#Utilizing-Multilingual-Encoders-to-Improve-Large-Language-Models-for-Low-Resource-Languages" class="headerlink" title="Utilizing Multilingual Encoders to Improve Large Language Models for   Low-Resource Languages"></a>Utilizing Multilingual Encoders to Improve Large Language Models for   Low-Resource Languages</h2><p><strong>Authors:Imalsha Puranegedara, Themira Chathumina, Nisal Ranathunga, Nisansa de Silva, Surangika Ranathunga, Mokanarangan Thayaparan</strong></p>
<p>Large Language Models (LLMs) excel in English, but their performance degrades significantly on low-resource languages (LRLs) due to English-centric training. While methods like LangBridge align LLMs with multilingual encoders such as the Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically use only the final encoder layer. We propose a novel architecture that fuses all intermediate layers, enriching the linguistic information passed to the LLM. Our approach features two strategies: (1) a Global Softmax weighting for overall layer importance, and (2) a Transformer Softmax model that learns token-specific weights. The fused representations are mapped into the LLMâ€™s embedding space, enabling it to process multilingual inputs. The model is trained only on English data, without using any parallel or multilingual data. Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews, our Transformer Softmax model significantly outperforms the LangBridge baseline. We observe strong performance gains in LRLs, improving Sinhala classification accuracy from 71.66% to 75.86% and achieving clear improvements across Indic languages such as Tamil, Bengali, and Malayalam. These specific gains contribute to an overall boost in average XNLI accuracy from 70.36% to 71.50%. This approach offers a scalable, data-efficient path toward more capable and equitable multilingual LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‹±è¯­ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ–¹å¼ï¼Œå®ƒä»¬åœ¨ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰ä¸Šçš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚è™½ç„¶åƒLangBridgeè¿™æ ·çš„æ–¹æ³•å¯ä»¥å°†LLMä¸å¤šè¯­è¨€ç¼–ç å™¨ï¼ˆå¦‚å¤§è§„æ¨¡å¤šè¯­è¨€æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢è½¬æ¢å™¨mT5ï¼‰å¯¹é½ï¼Œä½†å®ƒä»¬é€šå¸¸åªä½¿ç”¨æœ€ç»ˆçš„ç¼–ç å™¨å±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§èåˆæ‰€æœ‰ä¸­é—´å±‚çš„æ–°å‹æ¶æ„ï¼Œä¸°å¯Œä¼ é€’ç»™LLMçš„è¯­è¨€ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰å…¨å±€SoftmaxåŠ æƒç”¨äºæ•´ä½“å±‚é‡è¦æ€§ï¼Œï¼ˆ2ï¼‰Transformer Softmaxæ¨¡å‹å­¦ä¹ ç‰¹å®šäºä»¤ç‰Œï¼ˆtokenï¼‰çš„æƒé‡ã€‚èåˆçš„è¡¨ç¤ºè¢«æ˜ å°„åˆ°LLMçš„åµŒå…¥ç©ºé—´ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤šè¯­è¨€è¾“å…¥ã€‚è¯¥æ¨¡å‹ä»…åœ¨è‹±è¯­æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•å¹³è¡Œæˆ–å¤šè¯­è¨€æ•°æ®ã€‚åœ¨XNLIã€IndicXNLIã€åƒ§ä¼½ç½—è¯­æ–°é—»åˆ†ç±»å’Œäºšé©¬é€Šè¯„è®ºä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„Transformer Softmaxæ¨¡å‹æ˜¾è‘—ä¼˜äºLangBridgeåŸºçº¿ã€‚æˆ‘ä»¬åœ¨ä½èµ„æºè¯­è¨€ä¸Šè§‚å¯Ÿåˆ°å¼ºå¤§çš„æ€§èƒ½æå‡ï¼Œåƒ§ä¼½ç½—è¯­åˆ†ç±»å‡†ç¡®ç‡ä»71.66%æé«˜åˆ°75.86%ï¼Œä»¥åŠåœ¨å°åº¦è¯­è¨€å¦‚æ³°ç±³å°”è¯­ã€å­ŸåŠ æ‹‰è¯­å’Œé©¬æ‹‰äºšæ‹‰å§†è¯­ç­‰ä¸Šå®ç°æ˜æ˜¾çš„æ”¹è¿›ã€‚è¿™äº›ç‰¹å®šæ”¹è¿›æœ‰åŠ©äºæé«˜å¹³å‡XNLIå‡†ç¡®ç‡ä»70.36%åˆ°71.50%ã€‚è¿™ç§æ–¹æ³•ä¸ºæ„å»ºæ›´å¼ºå¤§ã€æ›´å…¬å¹³çš„å¤šè¯­è¨€LLMæä¾›äº†ä¸€æ¡å¯æ‰©å±•ä¸”æ•°æ®é«˜æ•ˆçš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè¯­ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªèåˆå¤šå±‚ä¿¡æ¯çš„æ¨¡å‹æ¶æ„æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¶æ„ä½¿ç”¨å…¨å±€Softmaxæƒé‡å’ŒTransformer Softmaxæ¨¡å‹è¿›è¡Œåˆ†å±‚å’Œä»¤ç‰Œçº§èåˆï¼Œä»¥å¢å¼ºä¼ é€’ç»™LLMçš„è¯­è¨€ä¿¡æ¯ã€‚æ¨¡å‹ä»…åœ¨è‹±è¯­æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä½¿ç”¨å¹³è¡Œæˆ–å¤šè¯­ç§æ•°æ®ã€‚åœ¨XNLIã€IndicXNLIã€åƒ§ä¼½ç½—è¯­æ–°é—»åˆ†ç±»å’Œäºšé©¬é€Šè¯„è®ºç­‰ä»»åŠ¡ä¸Šï¼Œè¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºLangBridgeåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰æ–¹é¢è¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½æå‡ã€‚è¿™æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æ•°æ®é«˜æ•ˆçš„è·¯å¾„ï¼Œæœç€æ›´å¼ºå¤§å’Œæ›´å‡è¡¡çš„å¤šè¯­ç§LLMå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰æ—¶æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦ç”±äºå®ƒä»¬ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ–¹å¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚LangBridgeè™½ç„¶å°è¯•é€šè¿‡å¤šè¯­ç§ç¼–ç å™¨ä¸LLMå¯¹æ¥ï¼Œä½†ä»…ä½¿ç”¨æœ€ç»ˆç¼–ç å™¨å±‚çš„ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹æ¶æ„èåˆäº†æ‰€æœ‰ä¸­é—´å±‚ä¿¡æ¯ï¼Œé€šè¿‡å…¨å±€Softmaxæƒé‡å’ŒTransformer Softmaxæ¨¡å‹è¿›è¡Œåˆ†å±‚å’Œä»¤ç‰Œçº§èåˆã€‚</li>
<li>èåˆåçš„è¡¨ç¤ºæ˜ å°„åˆ°LLMçš„åµŒå…¥ç©ºé—´ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤šè¯­ç§è¾“å…¥ã€‚</li>
<li>æ¨¡å‹ä»…åœ¨è‹±è¯­æ•°æ®ä¸Šè®­ç»ƒï¼Œæ— éœ€å¹³è¡Œæˆ–å¤šè¯­ç§æ•°æ®ã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬XNLIã€IndicXNLIã€åƒ§ä¼½ç½—è¯­æ–°é—»åˆ†ç±»å’Œäºšé©¬é€Šè¯„è®ºç­‰ï¼Œæ–°æ¨¡å‹æ˜¾è‘—ä¼˜äºLangBridgeåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨LRLsæ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6543699a3a87fb9895fc7fdbbb7d3ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99ac4c167fd32ba03a07113b48b65e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb4ceb5e8942b2ff27cf0fa373c3872.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f9ef3d945bce3b55d7727ea833f23c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-674d60d30f96dd074e4c79a4a710e9ff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring"><a href="#Dynamic-Uncertainty-aware-Multimodal-Fusion-for-Outdoor-Health-Monitoring" class="headerlink" title="Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health   Monitoring"></a>Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health   Monitoring</h2><p><strong>Authors:Zihan Fang, Zheng Lin, Senkang Hu, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang</strong></p>
<p>Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness. </p>
<blockquote>
<p>æˆ·å¤–å¥åº·ç›‘æµ‹å¯¹äºåŠæ—©å‘ç°å¼‚å¸¸å¥åº·çŠ¶å†µä»¥ä¿éšœäººç±»å¥åº·å’Œå®‰å…¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æˆ·å¤–ç›‘æµ‹ä¾èµ–äºé™æ€å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œéœ€è¦ä»é›¶å¼€å§‹çš„å¤§é‡æ•°æ®è®­ç»ƒï¼Œå¹¶ä¸”æ— æ³•æ•æ‰åˆ°å¾®å¦™çš„å¥åº·çŠ¶å†µå˜åŒ–ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œå®ƒåªåˆ©ç”¨å°å‹æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„ä¿¡æ¯ä¸°å¯Œæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°å¼ºå¤§çš„å¥åº·çŠ¶æ€ç›‘æµ‹ã€‚ç„¶è€Œï¼ŒåŸºäºMLLMçš„æˆ·å¤–å¥åº·ç›‘æµ‹ä¹Ÿé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šä¸€ã€ä¼ æ„Ÿå™¨æ•°æ®åŒ…å«æ¥è‡ªä¼ æ„Ÿå™¨æ•°æ®è·å–çš„è¾“å…¥å™ªå£°å’Œç”±åŠ¨æ€å®¤å¤–ç¯å¢ƒå¼•èµ·çš„ç”Ÿç†ä¿¡å·çªå˜è€Œäº§ç”Ÿçš„æ³¢åŠ¨å™ªå£°ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæ€§èƒ½ï¼›äºŒã€åŸºäºå½“å‰å˜æ¢å™¨çš„MLLMséš¾ä»¥å®ç°ç¨³å¥çš„å¤šæ¨¡æ€èåˆï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹èåˆå™ªå£°æ¨¡æ€çš„è®¾è®¡ï¼›ä¸‰ã€ä¸åŒå™ªå£°æ°´å¹³çš„æ¨¡æ€é˜»ç¢äº†ä»æ³¢åŠ¨åˆ†å¸ƒä¸­å‡†ç¡®æ¢å¤ç¼ºå¤±æ•°æ®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŠ¨æ€å’Œå™ªå£°ç¯å¢ƒä¸‹æˆ·å¤–å¥åº·ç›‘æµ‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œåä¸ºDUAL-Healthã€‚é¦–å…ˆï¼Œä¸ºäº†è¯„ä¼°å™ªå£°çš„å½±å“ï¼Œæˆ‘ä»¬åˆ©ç”¨å½“å‰å’Œæ—¶é—´ç‰¹å¾å‡†ç¡®é‡åŒ–ç”±è¾“å…¥å’Œæ³¢åŠ¨å™ªå£°å¼•èµ·çš„æ¨¡æ€ä¸ç¡®å®šæ€§ã€‚å…¶æ¬¡ï¼Œä¸ºäº†èµ‹èƒ½ä½è´¨é‡æ¨¡æ€çš„æœ‰æ•ˆå¤šæ¨¡æ€èåˆï¼Œæˆ‘ä»¬æ ¹æ®é‡åŒ–æ ¡å‡†çš„ä¸ç¡®å®šæ€§ä¸ºæ¯ä¸ªæ¨¡æ€å®šåˆ¶èåˆæƒé‡ã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†æé«˜ä»æ³¢åŠ¨å™ªå£°æ¨¡æ€ä¸­æ¢å¤æ•°æ®çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†æ¨¡æ€åˆ†å¸ƒå¯¹é½åˆ°å…¬å…±è¯­ä¹‰ç©ºé—´ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DUAL-Healthåœ¨æ£€æµ‹ç²¾åº¦å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09085v1">PDF</a> 14 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ·å¤–å¥åº·ç›‘æµ‹çš„é‡è¦æ€§ä»¥åŠä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æˆ·å¤–å¥åº·ç›‘æµ‹æ–¹æ³•ã€‚æ–‡ç« æŒ‡å‡ºäº†MLLMsé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ä¼ æ„Ÿå™¨æ•°æ®çš„è¾“å…¥å™ªå£°ã€åŠ¨æ€ç¯å¢ƒä¸‹çš„æ³¢åŠ¨å™ªå£°ã€æ¨¡æ€èåˆçš„ä¸ç¨³å®šæ€§ä»¥åŠä¸åŒå™ªå£°æ°´å¹³æ¨¡æ€çš„æ•°æ®ç¼ºå¤±é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDUAL-Healthçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œé€šè¿‡é‡åŒ–ä¸ç¡®å®šæ€§ã€å®šåˆ¶èåˆæƒé‡ä»¥åŠå¯¹é½æ¨¡æ€åˆ†å¸ƒï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆ·å¤–å¥åº·ç›‘æµ‹å¯¹äºåŠæ—¶å‘ç°äººä½“å¥åº·çŠ¶æ€å¼‚å¸¸å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ä¼ ç»Ÿæˆ·å¤–ç›‘æµ‹æ–¹æ³•ä¾èµ–äºé™æ€å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå­˜åœ¨éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒã€æ— æ³•æ•æ‰å¾®å¦™å¥åº·çŠ¶æ€å˜åŒ–çš„é—®é¢˜ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ©ç”¨å°æ•°æ®é›†å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸ºå¥åº·çŠ¶æ€ç›‘æµ‹æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚</li>
<li>MLLMåœ¨æˆ·å¤–å¥åº·ç›‘æµ‹ä¸­é¢ä¸´ä¼ æ„Ÿå™¨æ•°æ®è¾“å…¥å™ªå£°ã€åŠ¨æ€ç¯å¢ƒä¸‹çš„æ³¢åŠ¨å™ªå£°ç­‰æŒ‘æˆ˜ã€‚</li>
<li>DUAL-Healthæ¡†æ¶é€šè¿‡é‡åŒ–ä¸ç¡®å®šæ€§ã€å®šåˆ¶èåˆæƒé‡å’Œå¯¹é½æ¨¡æ€åˆ†å¸ƒï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦å’Œç¨³å¥æ€§ã€‚</li>
<li>å®šé‡ä¸ç¡®å®šæ€§èƒ½æœ‰æ•ˆè¯„ä¼°å™ªå£°å¯¹ç›‘æµ‹ç»“æœçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b8f30d90f0934111e1ca53ae505fffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-405d0891cb7ebcce9ee0ccbd069933b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b386b421b9a195ec29dcfdae3b60c657.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6934f0668b120a0c69a47d1e48f405a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Scaling-Learned-Image-Compression-Models-up-to-1-Billion"><a href="#Scaling-Learned-Image-Compression-Models-up-to-1-Billion" class="headerlink" title="Scaling Learned Image Compression Models up to 1 Billion"></a>Scaling Learned Image Compression Models up to 1 Billion</h2><p><strong>Authors:Yuqi Li, Haotian Zhang, Li Li, Dong Liu, Feng Wu</strong></p>
<p>Recent advances in large language models (LLMs) highlight a strong connection between intelligence and compression. Learned image compression, a fundamental task in modern data compression, has made significant progress in recent years. However, current models remain limited in scale, restricting their representation capacity, and how scaling model size influences compression performance remains unexplored. In this work, we present a pioneering study on scaling up learned image compression models and revealing the performance trends through scaling laws. Using the recent state-of-the-art HPCM model as baseline, we scale model parameters from 68.5 millions to 1 billion and fit power-law relations between test loss and key scaling variables, including model size and optimal training compute. The results reveal a scaling trend, enabling extrapolation to larger scale models. Experimental results demonstrate that the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion performance. We hope this work inspires future exploration of large-scale compression models and deeper investigations into the connection between compression and intelligence. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•çªæ˜¾äº†æ™ºèƒ½ä¸å‹ç¼©ä¹‹é—´çš„å¼ºçƒˆè”ç³»ã€‚ç°ä»£æ•°æ®å‹ç¼©ä¸­çš„åŸºæœ¬ä»»åŠ¡â€”â€”å­¦ä¹ å›¾åƒå‹ç¼©è¿‘å¹´æ¥å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰æ¨¡å‹çš„è§„æ¨¡ä»ç„¶æœ‰é™ï¼Œé™åˆ¶äº†å…¶è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ä¸”æ‰©å¤§æ¨¡å‹è§„æ¨¡å¦‚ä½•å½±å“å‹ç¼©æ€§èƒ½å°šæœªå¾—åˆ°æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹æ‰©å¤§å­¦ä¹ å›¾åƒå‹ç¼©æ¨¡å‹çš„è§„æ¨¡è¿›è¡Œäº†ä¸€é¡¹å¼€åˆ›æ€§ç ”ç©¶ï¼Œå¹¶é€šè¿‡è§„æ¨¡å®šå¾‹æ­ç¤ºäº†æ€§èƒ½è¶‹åŠ¿ã€‚æˆ‘ä»¬ä»¥æœ€æ–°çš„æœ€å…ˆè¿›çš„HPCMæ¨¡å‹ä¸ºåŸºå‡†ï¼Œå°†æ¨¡å‹å‚æ•°ä»6850ä¸‡æ‰©å±•åˆ°1äº¿ï¼Œå¹¶åœ¨æµ‹è¯•æŸå¤±å’Œå…³é”®ç¼©æ”¾å˜é‡ï¼ˆåŒ…æ‹¬æ¨¡å‹å¤§å°å’Œæœ€ä½³è®­ç»ƒè®¡ç®—ï¼‰ä¹‹é—´æ‹Ÿåˆå¹‚å¾‹å…³ç³»ã€‚ç»“æœæ­ç¤ºäº†ç¼©æ”¾è¶‹åŠ¿ï¼Œå¯å®ç°å‘æ›´å¤§è§„æ¨¡æ¨¡å‹çš„æ¨ç®—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰©å±•çš„HPCM-1Bæ¨¡å‹è¾¾åˆ°äº†æœ€æ–°çš„ç‡å¤±çœŸæ€§èƒ½æ ‡å‡†ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¿€å‘å¯¹æœªæ¥å¤§è§„æ¨¡å‹ç¼©æ¨¡å‹çš„æ¢ç´¢ï¼Œä»¥åŠå¯¹å‹ç¼©ä¸æ™ºèƒ½ä¹‹é—´è”ç³»çš„æ·±å…¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09075v1">PDF</a> 11 pages, technical report</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•çªæ˜¾äº†æ™ºèƒ½ä¸å‹ç¼©ä¹‹é—´çš„ç´§å¯†è”ç³»ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†æ‰©å¤§å›¾åƒå‹ç¼©æ¨¡å‹è§„æ¨¡çš„è¶‹åŠ¿ï¼Œå¹¶ä»¥æœ€è¿‘çš„å…ˆè¿›HPCMæ¨¡å‹ä¸ºåŸºçº¿ï¼Œå°†å…¶å‚æ•°ä»68.5ç™¾ä¸‡æ‰©å±•åˆ°åäº¿è§„æ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œå‹ç¼©æ€§èƒ½å¾—ä»¥æå‡ï¼Œè¿™ä¸ºæœªæ¥çš„å¤§è§„æ¨¡å‹ç¼©æ¨¡å‹æ¢ç´¢æä¾›äº†å¯ç¤ºã€‚æœ¬ç ”ç©¶æ­ç¤ºçš„ç¼©æ”¾è¶‹åŠ¿æœ‰æœ›ä¿ƒè¿›å‹ç¼©ä¸æ™ºèƒ½ä¹‹é—´è”ç³»çš„æ·±å…¥ç ”ç©¶ã€‚éšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œå…¶åœ¨æµ‹è¯•æŸå¤±ä¸Šçš„è¡¨ç°å±•ç¤ºå‡ºæå¤§çš„æ½œåŠ›ã€‚æœŸæœ›æ­¤ç ”ç©¶èƒ½ä¸ºå°†æ¥çš„å‹ç¼©æ¨¡å‹ç ”ç©¶å’Œæ¨¡å‹è§„æ¨¡å¯¹å‹ç¼©æ€§èƒ½çš„å½±å“å¸¦æ¥æ›´æ·±çš„ç†è§£å’Œå¯å‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•å¯¹å¤§è§„æ¨¡æ¨¡å‹çš„æ¢ç´¢ï¼Œæœ‰æœ›æ¨åŠ¨å›¾åƒå‹ç¼©æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ‰©å¤§å›¾åƒå‹ç¼©æ¨¡å‹çš„è§„æ¨¡æ¥æå‡æ€§èƒ½ã€‚</li>
<li>ä»¥HPCMæ¨¡å‹ä¸ºåŸºçº¿ï¼Œæ¨¡å‹å‚æ•°ä»æ•°åç™¾ä¸‡æ‰©å±•åˆ°åäº¿è§„æ¨¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œæµ‹è¯•æŸå¤±è¡¨ç°å‡ºç‰¹å®šçš„è¶‹åŠ¿ã€‚</li>
<li>é€šè¿‡æ­ç¤ºç¼©æ”¾è¶‹åŠ¿ï¼Œç ”ç©¶ä¸ºæœªæ¥çš„å¤§è§„æ¨¡å‹ç¼©æ¨¡å‹æ¢ç´¢æä¾›äº†å¯ç¤ºã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ¨¡å‹è§„æ¨¡åœ¨å½±å“å‹ç¼©æ€§èƒ½æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶æœŸæœ›æœªæ¥æœ‰æ›´æ·±å…¥çš„ç ”ç©¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f08350b5f743512e0ac852cd3800db97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8139fe857bfb612d3d6b3abf2e2c2b79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bc7478f0513a7db895c89891922d7aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c4ce7d6db511d4b844d219239c9ff2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-081b40171c4b6180c67e21cd0245807e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-922e4ce8018a4e6836c926812de69744.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="READER-Retrieval-Assisted-Drafter-for-Efficient-LLM-Inference"><a href="#READER-Retrieval-Assisted-Drafter-for-Efficient-LLM-Inference" class="headerlink" title="READER: Retrieval-Assisted Drafter for Efficient LLM Inference"></a>READER: Retrieval-Assisted Drafter for Efficient LLM Inference</h2><p><strong>Authors:Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi</strong></p>
<p>Large Language Models (LLMs) generate tokens autoregressively, with each token depending on the preceding context. This sequential nature makes the inference process inherently difficult to accelerate, posing a significant challenge for efficient deployment. In recent years, various methods have been proposed to address this issue, with the most effective approaches often involving the training of additional draft models. In this paper, we introduce READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel lossless speculative decoding method that enhances model-based approaches by leveraging self-repetitions in the text. Our algorithm expands the speculative decoding tree using tokens obtained through statistical search. This work focuses on large batch sizes (&gt;&#x3D; 8), an underexplored yet important area for industrial applications. We also analyze the key-value (KV) cache size during speculative decoding and propose an optimization to improve performance for large batches. As a result, READER outperforms existing speculative decoding methods. Notably, READER requires no additional training and can reuse pre-trained speculator models, increasing the speedup by over 40%. Our method demonstrates particularly strong performance on search-based tasks, such as retrieval-augmented generation, where we achieve more than 10x speedup. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡‡ç”¨è‡ªå›å½’æ–¹å¼ç”Ÿæˆä»¤ç‰Œï¼Œæ¯ä¸ªä»¤ç‰Œéƒ½ä¾èµ–äºå‰é¢çš„ä¸Šä¸‹æ–‡ã€‚è¿™ç§åºåˆ—æ€§è´¨ä½¿å¾—æ¨ç†è¿‡ç¨‹æœ¬èº«éš¾ä»¥åŠ é€Ÿï¼Œç»™æœ‰æ•ˆéƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥ï¼Œå·²ç»æå‡ºäº†å„ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ€æœ‰æ•ˆçš„æ–¹æ³•é€šå¸¸æ¶‰åŠè®­ç»ƒé¢å¤–çš„è‰ç¨¿æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†READERï¼ˆç”¨äºé«˜æ•ˆLLMæ¨ç†çš„æ£€ç´¢è¾…åŠ©èµ·è‰è€…ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— æŸæ¨æµ‹è§£ç æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ–‡æœ¬ä¸­çš„è‡ªæˆ‘é‡å¤æ¥å¢å¼ºåŸºäºæ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç®—æ³•ä½¿ç”¨é€šè¿‡ç»Ÿè®¡æœç´¢è·å¾—çš„ä»¤ç‰Œæ¥æ‰©å±•æ¨æµ‹è§£ç æ ‘ã€‚æˆ‘ä»¬çš„å·¥ä½œä¾§é‡äºå¤§æ‰¹é‡ï¼ˆ&gt;&#x3D; 8ï¼‰ï¼Œè¿™æ˜¯å·¥ä¸šåº”ç”¨æ–¹é¢é²œæœ‰ç ”ç©¶ä½†éå¸¸é‡è¦çš„é¢†åŸŸã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ¨æµ‹è§£ç è¿‡ç¨‹ä¸­çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–æªæ–½ä»¥æé«˜å¤§æ‰¹é‡æ€§èƒ½ã€‚å› æ­¤ï¼ŒREADERçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æ¨æµ‹è§£ç æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒREADERæ— éœ€é¢å¤–çš„è®­ç»ƒï¼Œå¹¶èƒ½é‡ç”¨é¢„è®­ç»ƒçš„æ¨æµ‹æ¨¡å‹ï¼Œæé«˜äº†è¶…è¿‡40%çš„åŠ é€Ÿæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºæœç´¢çš„ä»»åŠ¡ä¸Šè¡¨ç°ç‰¹åˆ«å‡ºè‰²ï¼Œå¦‚åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†è¶…è¿‡10å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09072v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆtokençš„æ–¹å¼æ˜¯è‡ªåŠ¨å›å½’çš„ï¼Œæ¯ä¸ªtokenéƒ½ä¾èµ–äºå‰é¢çš„ä¸Šä¸‹æ–‡ã€‚è¿™ç§åºåˆ—æ€§è´¨ä½¿å¾—æ¨ç†è¿‡ç¨‹éš¾ä»¥åŠ é€Ÿï¼Œå¯¹é«˜æ•ˆéƒ¨ç½²æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ— æŸæ¨æµ‹è§£ç æ–¹æ³•READERï¼Œå®ƒå€ŸåŠ©æ–‡æœ¬ä¸­çš„è‡ªæˆ‘é‡å¤ï¼Œé€šè¿‡ç»Ÿè®¡æœç´¢æ‰©å±•æ¨æµ‹è§£ç æ ‘ï¼Œå¢å¼ºäº†åŸºäºæ¨¡å‹çš„æ–¹æ³•ã€‚READERé‡ç‚¹å…³æ³¨å¤§æ‰¹é‡æ•°æ®ï¼ˆ&gt;&#x3D; 8ï¼‰ï¼Œè¿™æ˜¯å·¥ä¸šåº”ç”¨ä¸­çš„ä¸€ä¸ªè¢«å¿½è§†ä½†é‡è¦çš„é¢†åŸŸã€‚é€šè¿‡å¯¹æ¨æµ‹è§£ç è¿‡ç¨‹ä¸­çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°çš„åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¼˜åŒ–æ–¹æ¡ˆä»¥æé«˜å¤§æ‰¹é‡æ•°æ®çš„æ€§èƒ½ã€‚READERè¡¨ç°ä¼˜äºç°æœ‰æ¨æµ‹è§£ç æ–¹æ³•ï¼Œå°¤å…¶åœ¨ä¸éœ€é¢å¤–è®­ç»ƒä¸”å¯é‡ç”¨é¢„è®­ç»ƒæ¨æµ‹æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæé€Ÿè¶…è¿‡40%ã€‚åœ¨åŸºäºæœç´¢çš„ä»»åŠ¡ä¸Šï¼Œå¦‚å¢å¼ºæ£€ç´¢ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†è¶…è¿‡10å€çš„é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æ¨ç†è¿‡ç¨‹ç”±äºåºåˆ—ç”Ÿæˆç‰¹æ€§è€Œéš¾ä»¥åŠ é€Ÿã€‚</li>
<li>READERæ˜¯ä¸€ç§æ–°å‹æ— æŸæ¨æµ‹è§£ç æ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬ä¸­çš„è‡ªæˆ‘é‡å¤æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>READERé€šè¿‡ç»Ÿè®¡æœç´¢æ‰©å±•æ¨æµ‹è§£ç æ ‘ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>READERå¯é‡ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œæé«˜é€Ÿåº¦ã€‚</li>
<li>READERåœ¨å¤§æ‰¹é‡æ•°æ®å¤„ç†æ—¶è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«é€‚ç”¨äºå·¥ä¸šåº”ç”¨ã€‚</li>
<li>é€šè¿‡åˆ†æé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å¤§å°ï¼Œä¼˜åŒ–äº†READERçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5d8119a849ff3d638df7467531b43f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e61a85c325686b22dd855cca71dbcdbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67139e4c5d96e09bf7d4e98ba62b7687.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58f054b8bfc644b99426b5e9ddb5e2bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80541de6c6274c7fc30da6a66b0e766f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-005c9f67af2261167cd6d4daf7a4f747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06f0308dfaf44cb833a8d43d6f75015e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dffb1d986d56619a85d58e575abd2ea0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c8125c9eafb3ca761ca493f625610e1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices"><a href="#P-D-Device-Disaggregated-Large-Language-Model-between-Cloud-and-Devices" class="headerlink" title="P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices"></a>P&#x2F;D-Device: Disaggregated Large Language Model between Cloud and Devices</h2><p><strong>Authors:Yibo Jin, Yixu Xu, Yue Chen, Chengbin Wang, Tao Wang, Jiaqi Huang, Rongfei Zhang, Yiming Dong, Yuting Yan, Ke Cheng, Yingjie Zhu, Shulan Wang, Qianqian Tang, Shuaishuai Meng, Guanxin Cheng, Ze Wang, Shuyan Miao, Ketao Wang, Wen Liu, Yifan Yang, Tong Zhang, Anran Wang, Chengzhou Lu, Tiantian Dong, Yongsheng Zhang, Zhe Wang, Hefei Guo, Hongjie Liu, Wei Lu, Zhengyong Zhang</strong></p>
<p>Serving disaggregated large language models has been widely adopted in industrial practice for enhanced performance. However, too many tokens generated in decoding phase, i.e., occupying the resources for a long time, essentially hamper the cloud from achieving a higher throughput. Meanwhile, due to limited on-device resources, the time to first token (TTFT), i.e., the latency of prefill phase, increases dramatically with the growth on prompt length. In order to concur with such a bottleneck on resources, i.e., long occupation in cloud and limited on-device computing capacity, we propose to separate large language model between cloud and devices. That is, the cloud helps a portion of the content for each device, only in its prefill phase. Specifically, after receiving the first token from the cloud, decoupling with its own prefill, the device responds to the user immediately for a lower TTFT. Then, the following tokens from cloud are presented via a speed controller for smoothed TPOT (the time per output token), until the device catches up with the progress. On-device prefill is then amortized using received tokens while the resource usage in cloud is controlled. Moreover, during cloud prefill, the prompt can be refined, using those intermediate data already generated, to further speed up on-device inference. We implement such a scheme P&#x2F;D-Device, and confirm its superiority over other alternatives. We further propose an algorithm to decide the best settings. Real-trace experiments show that TTFT decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud throughput increases by up to 15x. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹æ‹†åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†è¿›è¡ŒæœåŠ¡åœ¨å·¥ä¸šå®è·µä¸­å·²è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä»¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè§£ç é˜¶æ®µç”Ÿæˆçš„ä»¤ç‰Œè¿‡å¤šï¼Œå³é•¿æ—¶é—´å ç”¨èµ„æºï¼Œæœ¬è´¨ä¸Šé˜»ç¢äº†äº‘å®ç°æ›´é«˜çš„ååé‡ã€‚åŒæ—¶ï¼Œç”±äºè®¾å¤‡èµ„æºæœ‰é™ï¼Œé¦–ä»¤ç‰Œç”Ÿæˆæ—¶é—´ï¼ˆTTFTï¼‰éšç€æç¤ºé•¿åº¦çš„å¢åŠ è€Œæ€¥å‰§å¢åŠ ã€‚ä¸ºäº†åº”å¯¹äº‘å’Œè®¾å¤‡èµ„æºç“¶é¢ˆçš„é—®é¢˜ï¼Œå³é•¿æ—¶é—´å ç”¨å’Œæœ‰é™çš„è®¾å¤‡è®¡ç®—èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºåœ¨äº‘å’Œè®¾å¤‡ä¹‹é—´åˆ†ç¦»å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œäº‘ä»…åœ¨è®¾å¤‡çš„é¢„å¡«å……é˜¶æ®µå¸®åŠ©å¤„ç†éƒ¨åˆ†å†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¥æ”¶åˆ°æ¥è‡ªäº‘çš„é¦–ä¸ªä»¤ç‰Œåï¼Œè®¾å¤‡è„±ç¦»å…¶è‡ªå·±çš„é¢„å¡«å……é˜¶æ®µï¼Œç«‹å³å“åº”ç”¨æˆ·è¯·æ±‚ä»¥é™ä½TTFTã€‚ç„¶åï¼Œæ¥è‡ªäº‘çš„åç»­ä»¤ç‰Œé€šè¿‡é€Ÿåº¦æ§åˆ¶å™¨å¹³æ»‘åœ°å‘ˆç°ï¼Œä»¥é™ä½æ¯è¾“å‡ºä»¤ç‰Œçš„æ—¶é—´ï¼ˆTPOTï¼‰ï¼Œç›´åˆ°è®¾å¤‡è·Ÿä¸Šè¿›åº¦ã€‚åœ¨æ”¶åˆ°ä»¤ç‰Œçš„åŒæ—¶ä½¿ç”¨è®¾å¤‡çš„é¢„å¡«å……è¿›è¡Œæ‘Šé”€ï¼ŒåŒæ—¶æ§åˆ¶äº‘ä¸­çš„èµ„æºä½¿ç”¨ã€‚æ­¤å¤–ï¼Œåœ¨äº‘çš„é¢„å¡«å……æœŸé—´ï¼Œå¯ä»¥ä½¿ç”¨å·²ç»ç”Ÿæˆçš„ä¸­é—´æ•°æ®æ¥ä¼˜åŒ–æç¤ºï¼Œä»¥è¿›ä¸€æ­¥åŠ å¿«è®¾å¤‡ä¸Šçš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å®ç°äº†è¿™ç§P&#x2F;D-Deviceæ–¹æ¡ˆï¼Œå¹¶ç¡®è®¤äº†å…¶ä¼˜äºå…¶ä»–æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®—æ³•æ¥å†³å®šæœ€ä½³è®¾ç½®ã€‚çœŸå®è½¨è¿¹å®éªŒè¡¨æ˜ï¼ŒTTFTè‡³å°‘å‡å°‘äº†60%ï¼Œæœ€å¤§TPOTçº¦ä¸ºå‡ åæ¯«ç§’ï¼Œäº‘çš„ååé‡æé«˜äº†é«˜è¾¾15å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09035v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäº‘è®¡ç®—çš„å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡åœ¨å®é™…åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›é‡‡ç”¨ï¼Œä½†åœ¨è§£ç é˜¶æ®µç”Ÿæˆè¿‡å¤šä»¤ç‰Œï¼Œå¯¼è‡´èµ„æºå ç”¨æ—¶é—´è¿‡é•¿ï¼Œé™åˆ¶äº†äº‘æœåŠ¡çš„ååé‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨äº‘ç«¯å’Œè®¾å¤‡ç«¯åˆ†ç¦»å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆã€‚äº‘ç«¯è´Ÿè´£ä¸ºæ¯ä¸ªè®¾å¤‡æä¾›éƒ¨åˆ†å†…å®¹é¢„å¡«å……æœåŠ¡ï¼Œé™ä½è®¾å¤‡ç«¯å“åº”æ—¶é—´ã€‚é€šè¿‡é€Ÿåº¦æ§åˆ¶å™¨å¹³è¡¡äº‘å’Œè®¾å¤‡çš„å¤„ç†è¿›åº¦ï¼Œå‡å°‘è®¾å¤‡ç«¯å’Œäº‘ç«¯çš„èµ„æºæ¶ˆè€—ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æœåŠ¡åœ¨å®é™…åº”ç”¨ä¸­å¹¿æ³›é‡‡ç”¨ï¼Œä½†å­˜åœ¨è§£ç é˜¶æ®µç”Ÿæˆè¿‡å¤šä»¤ç‰Œçš„é—®é¢˜ã€‚</li>
<li>è¿‡å¤šä»¤ç‰Œç”Ÿæˆå¯¼è‡´èµ„æºå ç”¨æ—¶é—´é•¿ï¼Œé™åˆ¶äº†äº‘æœåŠ¡çš„ååé‡ã€‚</li>
<li>æå‡ºåœ¨äº‘ç«¯å’Œè®¾å¤‡ç«¯åˆ†ç¦»å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼Œäº‘ç«¯è´Ÿè´£é¢„å¡«å……æœåŠ¡ä»¥é™ä½è®¾å¤‡å“åº”æ—¶é—´ã€‚</li>
<li>é€šè¿‡é€Ÿåº¦æ§åˆ¶å™¨å¹³è¡¡äº‘å’Œè®¾å¤‡çš„å¤„ç†è¿›åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜è®¾å¤‡å“åº”æ—¶é—´ã€é™ä½æœ€å¤§ä»¤ç‰Œå¤„ç†æ—¶é—´å’Œæé«˜äº‘æœåŠ¡ååé‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-537ccaf4aaeef92282a3b41ce5e41ad6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b716c12ce6d85b33b315769c38618dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d67a4cc409aa92d9adb7415acfa0a65f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa9451f991a2418fd436556b4529231b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4d92990c6053176dd84c77c73dce04b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency"><a href="#E3-Rewrite-Learning-to-Rewrite-SQL-for-Executability-Equivalence-and-Efficiency" class="headerlink" title="E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and   Efficiency"></a>E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and   Efficiency</h2><p><strong>Authors:Dongjie Xu, Yue Cui, Weijie Shi, Qingzhi Ma, Hanghui Guo, Jiaming Li, Yao Zhao, Ruiyuan Zhang, Shimin Di, Jia Zhu, Kai Zheng, Jiajie Xu</strong></p>
<p>SQL query rewriting aims to reformulate a query into a more efficient form while preserving equivalence. Most existing methods rely on predefined rewrite rules. However, such rule-based approaches face fundamental limitations: (1) fixed rule sets generalize poorly to novel query patterns and struggle with complex queries; (2) a wide range of effective rewriting strategies cannot be fully captured by declarative rules. To overcome these issues, we propose using large language models (LLMs) to generate rewrites. LLMs can capture complex strategies, such as evaluation reordering and CTE rewriting. Despite this potential, directly applying LLMs often results in suboptimal or non-equivalent rewrites due to a lack of execution awareness and semantic grounding. To address these challenges, We present E3-Rewrite, an LLM-based SQL rewriting framework that produces executable, equivalent, and efficient queries. It integrates two core components: a context construction module and a reinforcement learning framework. First, the context module leverages execution plans and retrieved demonstrations to build bottleneck-aware prompts that guide inference-time rewriting. Second, we design a reward function targeting executability, equivalence, and efficiency, evaluated via syntax checks, equivalence verification, and cost estimation. Third, to ensure stable multi-objective learning, we adopt a staged curriculum that first emphasizes executability and equivalence, then gradually incorporates efficiency. Extensive experiments show that E3-Rewrite achieves up to a 25.6% reduction in query execution time compared to state-of-the-art methods across multiple SQL benchmarks. Moreover, it delivers up to 24.4% more successful rewrites, expanding coverage to complex queries that previous systems failed to handle. </p>
<blockquote>
<p>SQLæŸ¥è¯¢é‡å†™æ—¨åœ¨å°†æŸ¥è¯¢é‡æ–°è¡¨è¿°ä¸ºæ›´æœ‰æ•ˆç‡çš„å½¢å¼ï¼ŒåŒæ—¶ä¿æŒç­‰æ•ˆæ€§ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äºé¢„å®šä¹‰çš„é‡å†™è§„åˆ™ã€‚ç„¶è€Œï¼ŒåŸºäºè§„åˆ™çš„æ–¹æ³•é¢ä¸´æ ¹æœ¬æ€§çš„å±€é™ï¼šï¼ˆ1ï¼‰å›ºå®šè§„åˆ™é›†å¯¹æ–°å‹æŸ¥è¯¢æ¨¡å¼æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚æŸ¥è¯¢ï¼›ï¼ˆ2ï¼‰è®¸å¤šæœ‰æ•ˆçš„é‡å†™ç­–ç•¥æ— æ³•è¢«å£°æ˜å¼è§„åˆ™å®Œå…¨æ•è·ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆé‡å†™ã€‚LLMå¯ä»¥æ•æ‰å¤æ‚çš„ç­–ç•¥ï¼Œå¦‚è¯„ä¼°é‡æ’åºå’ŒCTEé‡å†™ã€‚å°½ç®¡æœ‰è¿™ä¸€æ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨LLMé€šå¸¸ä¼šå¯¼è‡´ç”±äºç¼ºä¹å¯¹æ‰§è¡Œå’Œè¯­ä¹‰åŸºç¡€çŸ¥è¯†çš„äº†è§£è€Œäº§ç”Ÿéæœ€ä¼˜æˆ–éç­‰æ•ˆçš„é‡å†™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†E3-Rewriteï¼Œä¸€ä¸ªåŸºäºLLMçš„SQLé‡å†™æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå¯æ‰§è¡Œã€ç­‰æ•ˆä¸”é«˜æ•ˆçš„æŸ¥è¯¢ã€‚å®ƒæ•´åˆäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸€ä¸ªä¸Šä¸‹æ–‡æ„å»ºæ¨¡å—å’Œä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸Šä¸‹æ–‡æ¨¡å—åˆ©ç”¨æ‰§è¡Œè®¡åˆ’å’Œæ£€ç´¢åˆ°çš„ç¤ºä¾‹æ¥æ„å»ºç“¶é¢ˆæ„ŸçŸ¥æç¤ºï¼Œå¼•å¯¼æ¨ç†æ—¶é—´é‡å†™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œä»¥æ‰§è¡Œæ€§ã€ç­‰æ•ˆæ€§å’Œæ•ˆç‡ä¸ºç›®æ ‡ï¼Œé€šè¿‡è¯­æ³•æ£€æŸ¥ã€ç­‰æ•ˆæ€§éªŒè¯å’Œæˆæœ¬è¯„ä¼°è¿›è¡Œè¯„ä»·ã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†ç¡®ä¿ç¨³å®šçš„å¤šç›®æ ‡å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ†é˜¶æ®µè¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œé¦–å…ˆå¼ºè°ƒæ‰§è¡Œæ€§å’Œç­‰æ•ˆæ€§ï¼Œç„¶åé€æ¸èå…¥æ•ˆç‡è€ƒé‡ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒE3-Rewriteåœ¨å¤šä¸ªSQLåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å¤šè¾¾25.6%çš„æŸ¥è¯¢æ‰§è¡Œæ—¶é—´å‡å°‘ã€‚æ­¤å¤–ï¼Œå®ƒæä¾›äº†æœ€å¤šè¾¾24.4%çš„æ›´æˆåŠŸé‡å†™ï¼Œæ‰©å±•äº†å¯¹ä»¥å‰ç³»ç»Ÿæ— æ³•å¤„ç†çš„å¤æ‚æŸ¥è¯¢çš„è¦†ç›–èŒƒå›´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09023v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>SQLæŸ¥è¯¢é‡å†™æ—¨åœ¨å°†æŸ¥è¯¢è½¬åŒ–ä¸ºæ›´é«˜æ•ˆçš„è¡¨è¾¾å½¢å¼ä¸”ä¿æŒåŸæ„ä¸å˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¤šä¾èµ–äºé¢„è®¾çš„æ”¹å†™è§„åˆ™ï¼Œä½†å­˜åœ¨è¯¸å¤šå±€é™ï¼šå¯¹æ–°æŸ¥è¯¢æ¨¡å¼æ³›åŒ–èƒ½åŠ›å¼±ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚æŸ¥è¯¢ï¼›è®¸å¤šæœ‰æ•ˆçš„æ”¹å†™ç­–ç•¥æ— æ³•è¢«å£°æ˜å¼è§„åˆ™å®Œå…¨æ•æ‰ã€‚ä¸ºå…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ”¹å†™ã€‚LLMèƒ½å¤Ÿæ•æ‰å¤æ‚çš„ç­–ç•¥ï¼Œå¦‚è¯„ä¼°é‡æ’åºå’ŒCTEé‡å†™ç­‰ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨LLMå¾€å¾€å¯¼è‡´æ¬¡ä¼˜æˆ–éç­‰æ•ˆçš„æ”¹å†™ç»“æœï¼Œç¼ºä¹æ‰§è¡Œæ„è¯†å’Œè¯­ä¹‰åŸºç¡€ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºLLMçš„SQLé‡å†™æ¡†æ¶E3-Rewriteï¼Œèƒ½å¤Ÿç”Ÿæˆå¯æ‰§è¡Œçš„ã€ç­‰æ•ˆçš„ã€é«˜æ•ˆçš„æŸ¥è¯¢ã€‚å®ƒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸Šä¸‹æ–‡æ„å»ºæ¨¡å—å’Œå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸Šä¸‹æ–‡æ¨¡å—åˆ©ç”¨æ‰§è¡Œè®¡åˆ’å’Œæ£€ç´¢åˆ°çš„ç¤ºä¾‹æ¥æ„å»ºç“¶é¢ˆæ„ŸçŸ¥æç¤ºï¼Œä»¥å¼•å¯¼æ¨ç†æ—¶çš„é‡å†™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œä»¥æ‰§è¡Œæ€§ã€ç­‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ä¸ºç›®æ ‡ï¼Œé€šè¿‡è¯­æ³•æ£€æŸ¥ã€ç­‰æ•ˆéªŒè¯å’Œæˆæœ¬ä¼°ç®—è¿›è¡Œè¯„ä»·ã€‚å†æ¬¡ï¼Œä¸ºç¡®ä¿ç¨³å®šçš„å¤šç›®æ ‡å­¦ä¹ ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•ï¼Œé¦–å…ˆå¼ºè°ƒæ‰§è¡Œæ€§å’Œç­‰æ•ˆæ€§ï¼Œç„¶åé€æ¸èå…¥é«˜æ•ˆæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒE3-Rewriteåœ¨å¤šä¸ªSQLåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å¤šè¾¾25.6%çš„æŸ¥è¯¢æ‰§è¡Œæ—¶é—´å‡å°‘ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½è¿›è¡Œæ›´å¤šè¾¾24.4%çš„æˆåŠŸæ”¹å†™ï¼Œè¦†ç›–ä»¥å‰ç³»ç»Ÿæ— æ³•å¤„ç†çš„å¤æ‚æŸ¥è¯¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SQLæŸ¥è¯¢é‡å†™æ—¨åœ¨æé«˜æŸ¥è¯¢æ•ˆç‡å¹¶ä¿æŒè¯­ä¹‰ä¸å˜ã€‚</li>
<li>ä¼ ç»Ÿè§„åˆ™æ–¹æ³•é¢ä¸´å¯¹æ–°æŸ¥è¯¢æ¨¡å¼å’Œå¤æ‚æŸ¥è¯¢çš„å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨SQLæŸ¥è¯¢é‡å†™ä¸­å…·æœ‰æ•æ‰å¤æ‚ç­–ç•¥çš„èƒ½åŠ›ã€‚</li>
<li>ç›´æ¥åº”ç”¨LLMå¯¼è‡´æ¬¡ä¼˜æˆ–éç­‰æ•ˆæ”¹å†™ç»“æœçš„é—®é¢˜ã€‚</li>
<li>E3-Rewriteæ¡†æ¶ç»“åˆLLMå’Œå¼ºåŒ–å­¦ä¹ ï¼Œç”Ÿæˆå¯æ‰§è¡Œã€ç­‰æ•ˆå’Œé«˜æ•ˆçš„SQLæŸ¥è¯¢ã€‚</li>
<li>E3-Rewriteé€šè¿‡ä¸Šä¸‹æ–‡æ¨¡å—å’Œå¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œèƒ½æœ‰æ•ˆè§£å†³æ‰§è¡Œæ€§ã€ç­‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6a8af38f7f0cbded893515a30922c3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ebd710c355cb6e63d5b284ca5e1dadc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d25d6129cfcb11ab0f749243f8cb3f2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73a0fc3d923d117f296336ce4c5640ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-795ea0060824e515a825f4d5627e86cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46097adbad0560da9fdf63e1bc4f7802.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs"><a href="#Activation-Steering-for-Bias-Mitigation-An-Interpretable-Approach-to-Safer-LLMs" class="headerlink" title="Activation Steering for Bias Mitigation: An Interpretable Approach to   Safer LLMs"></a>Activation Steering for Bias Mitigation: An Interpretable Approach to   Safer LLMs</h2><p><strong>Authors:Shivam Dubey</strong></p>
<p>As large language models (LLMs) become more integrated into societal systems, the risk of them perpetuating and amplifying harmful biases becomes a critical safety concern. Traditional methods for mitigating bias often rely on data filtering or post-hoc output moderation, which treat the model as an opaque black box. In this work, we introduce a complete, end-to-end system that uses techniques from mechanistic interpretability to both identify and actively mitigate bias directly within a modelâ€™s internal workings. Our method involves two primary stages. First, we train linear â€œprobesâ€ on the internal activations of a model to detect the latent representations of various biases (e.g., gender, race, age). Our experiments on \texttt{gpt2-large} demonstrate that these probes can identify biased content with near-perfect accuracy, revealing that bias representations become most salient in the modelâ€™s later layers. Second, we leverage these findings to compute â€œsteering vectorsâ€ by contrasting the modelâ€™s activation patterns for biased and neutral statements. By adding these vectors during inference, we can actively steer the modelâ€™s generative process away from producing harmful, stereotypical, or biased content in real-time. We demonstrate the efficacy of this activation steering technique, showing that it successfully alters biased completions toward more neutral alternatives. We present our work as a robust and reproducible system that offers a more direct and interpretable approach to building safer and more accountable LLMs. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¤¾ä¼šç³»ç»Ÿä¸­å¾—åˆ°æ›´æ·±å…¥çš„é›†æˆï¼Œå®ƒä»¬å¯èƒ½ä¼ æ’­å’Œæ”¾å¤§æœ‰å®³åè§çš„é£é™©æˆä¸ºäº†ä¸€ä¸ªå…³é”®çš„å®‰å…¨é—®é¢˜ã€‚ä¼ ç»Ÿçš„å‡è½»åè§çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºæ•°æ®è¿‡æ»¤æˆ–äº‹åè¾“å‡ºè°ƒè§£ï¼Œè¿™äº›æ–¹æ³•å°†æ¨¡å‹è§†ä¸ºä¸€ä¸ªä¸é€æ˜çš„é»‘ç®±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå®Œæ•´ã€ç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨æ¥è‡ªæœºæ¢°è§£é‡Šæ€§çš„æŠ€æœ¯æ¥è¯†åˆ«å’Œä¸»åŠ¨å‡è½»æ¨¡å‹å†…éƒ¨å·¥ä½œä¸­å­˜åœ¨çš„åè§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»ä¸Šè®­ç»ƒçº¿æ€§â€œæ¢é’ˆâ€ï¼Œä»¥æ£€æµ‹å„ç§åè§ï¼ˆå¦‚æ€§åˆ«ã€ç§æ—ã€å¹´é¾„ç­‰ï¼‰çš„æ½œåœ¨è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨<code>gpt2-large</code>ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ¢é’ˆå¯ä»¥è¿‘ä¹å®Œç¾åœ°è¯†åˆ«å‡ºæœ‰åè§çš„å†…å®¹ï¼Œæ­ç¤ºå‡ºåè§è¡¨ç¤ºåœ¨æ¨¡å‹çš„è¾ƒæ·±å±‚ä¸­å˜å¾—æœ€ä¸ºçªå‡ºã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›å‘ç°ï¼Œé€šè¿‡å¯¹æ¯”æ¨¡å‹å¯¹å¸¦æœ‰åè§å’Œä¸­æ€§é™ˆè¿°çš„æ¿€æ´»æ¨¡å¼æ¥è®¡ç®—â€œè½¬å‘å‘é‡â€ã€‚é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ·»åŠ è¿™äº›å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥å®æ—¶åœ°ä¸»åŠ¨å¼•å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œé¿å…äº§ç”Ÿæœ‰å®³çš„ã€åˆ»æ¿çš„æˆ–æœ‰åè§çš„å†…å®¹ã€‚æˆ‘ä»¬è¯æ˜äº†æ¿€æ´»è½¬å‘æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å®ƒæˆåŠŸåœ°æ”¹å˜äº†æœ‰åè§çš„å®Œæˆå†…å®¹ï¼Œä½¿å…¶æ›´åŠ ä¸­ç«‹ã€‚æˆ‘ä»¬æå‡ºçš„æ˜¯ä¸€ä¸ªç¨³å¥ä¸”å¯å¤åˆ¶çš„ç³»ç»Ÿï¼Œå®ƒæä¾›äº†ä¸€ç§æ›´ç›´æ¥å’Œå¯è§£é‡Šçš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºæ›´å®‰å…¨ã€æ›´è´Ÿè´£ä»»çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09019v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¤¾ä¼šç³»ç»Ÿä¸­çš„é›†æˆæ„ˆå‘æ™®åŠï¼Œå› æ­¤æ½œåœ¨çš„å¯¹ç¤¾ä¼šäº§ç”Ÿåè§å’ŒåŠ å‰§æ”¾å¤§æœ‰å®³åè§çš„æ‹…å¿§ä¸æ—¥ä¿±å¢ã€‚ä»¥å¾€è§£å†³åè§é—®é¢˜çš„ç­–ç•¥ä¸»è¦ä¾èµ–äºæ•°æ®è¿‡æ»¤æˆ–äº‹åè¾“å‡ºè°ƒèŠ‚æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•æœªå……åˆ†å…³æ³¨æ¨¡å‹çš„å†…éƒ¨è¿ä½œè¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹åƒé»‘ç®±ä¸€æ ·éš¾ä»¥ç†è§£å’Œæ§åˆ¶ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œé‡‡ç”¨æœºæ¢°è§£é‡Šæ€§æŠ€æœ¯ç›´æ¥è¯†åˆ«å¹¶ç¼“è§£æ¨¡å‹å†…éƒ¨å­˜åœ¨çš„åè§ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹æ¨¡å‹å†…éƒ¨æ¿€æ´»çŠ¶æ€è¿›è¡Œè®­ç»ƒæ¥æ¢æµ‹å„ç§åè§çš„æ½œåœ¨è¡¨å¾ã€‚å®éªŒæ˜¾ç¤ºè¿™äº›æ¢æµ‹å™¨å¯ä»¥è¿‘ä¹å®Œç¾åœ°è¯†åˆ«å‡ºæœ‰åè§çš„è¨€è®ºï¼Œå¹¶å‘ç°åè§ä¿¡æ¯åœ¨æ¨¡å‹çš„æ·±å±‚ç»“æ„ä¸­æœ€ä¸ºçªå‡ºã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¯¹æ¯”æœ‰åè§å’Œæ— åè§è¯­å¥çš„æ¿€æ´»æ¨¡å¼ï¼Œè®¡ç®—å‡ºæ‰€è°“çš„â€œå¼•å¯¼å‘é‡â€ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥è¿™äº›å‘é‡ï¼Œå¯ä»¥å®æ—¶å¼•å¯¼æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹é¿å…äº§ç”Ÿæœ‰å®³çš„åˆ»æ¿å°è±¡æˆ–åè§å†…å®¹ã€‚ç ”ç©¶è¯æ˜äº†è¿™ç§æ¿€æ´»å¼•å¯¼æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå³é€šè¿‡è¯¥æ–¹æ³•å¯ä»¥å°†åå‘æ€§çš„ç»“è®ºå¼•å‘æ›´ä¸ºä¸­ç«‹çš„æ–¹å‘ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªç¨³å¥å’Œå¯å¤åˆ¶çš„ç³»ç»Ÿæ¡†æ¶ï¼Œä¸ºæ„å»ºæ›´å®‰å…¨ã€æ›´è´Ÿè´£ä»»çš„å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ›´ç›´æ¥å’Œå¯è§£é‡Šçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¤¾ä¼šç³»ç»Ÿä¸­çš„åº”ç”¨å¼•å‘äº†å¯¹åè§é£é™©çš„å…³æ³¨ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¤„ç†åè§é—®é¢˜å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†è§£é‡Šæ¨¡å‹çš„å†…éƒ¨è¿ä½œè¿‡ç¨‹ã€‚</li>
<li>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç³»ç»Ÿæ¥ç›´æ¥è¯†åˆ«å’Œç¼“è§£æ¨¡å‹å†…éƒ¨çš„åè§ã€‚</li>
<li>é€šè¿‡è®­ç»ƒçº¿æ€§æ¢æµ‹å™¨æ¥æ¢æµ‹æ¨¡å‹çš„å†…éƒ¨æ¿€æ´»çŠ¶æ€ï¼Œå¯è¯†åˆ«åè§è¨€è®ºçš„æ½œåœ¨è¡¨å¾ã€‚å®éªŒè¯æ˜è¿™äº›æ¢æµ‹å™¨å¯ä»¥è¿‘ä¹å®Œç¾åœ°è¯†åˆ«åè§å†…å®¹ã€‚</li>
<li>æ­ç¤ºåè§ä¿¡æ¯åœ¨æ¨¡å‹çš„æ·±å±‚ç»“æ„ä¸­æœ€ä¸ºçªå‡ºã€‚</li>
<li>é€šè¿‡å¯¹æ¯”æœ‰åè§å’Œæ— åè§è¯­å¥çš„æ¿€æ´»æ¨¡å¼è®¡ç®—å¼•å¯¼å‘é‡ï¼Œå®æ—¶å¼•å¯¼æ¨¡å‹é¿å…ç”Ÿæˆæœ‰å®³çš„åˆ»æ¿å°è±¡æˆ–åè§å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00ddf9e1970514775cbc980975b67865.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a707658fad386634d818bb1b81f7d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fce2d583d4bfc09ffed03f0b307b858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4217925e719eabf9bf778a6f30a1abfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec92e72d4641d88dbc2b3a4c4a6e0282.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection"><a href="#QueryCraft-Transformer-Guided-Query-Initialization-for-Enhanced-Human-Object-Interaction-Detection" class="headerlink" title="QueryCraft: Transformer-Guided Query Initialization for Enhanced   Human-Object Interaction Detection"></a>QueryCraft: Transformer-Guided Query Initialization for Enhanced   Human-Object Interaction Detection</h2><p><strong>Authors:Yuxiao Wang, Wolin Liang, Yu Lei, Weiying Xue, Nan Zhuang, Qi Liu</strong></p>
<p>Human-Object Interaction (HOI) detection aims to localize human-object pairs and recognize their interactions in images. Although DETR-based methods have recently emerged as the mainstream framework for HOI detection, they still suffer from a key limitation: Randomly initialized queries lack explicit semantics, leading to suboptimal detection performance. To address this challenge, we propose QueryCraft, a novel plug-and-play HOI detection framework that incorporates semantic priors and guided feature learning through transformer-based query initialization. Central to our approach is \textbf{ACTOR} (\textbf{A}ction-aware \textbf{C}ross-modal \textbf{T}ransf\textbf{OR}mer), a cross-modal Transformer encoder that jointly attends to visual regions and textual prompts to extract action-relevant features. Rather than merely aligning modalities, ACTOR leverages language-guided attention to infer interaction semantics and produce semantically meaningful query representations. To further enhance object-level query quality, we introduce a \textbf{P}erceptual \textbf{D}istilled \textbf{Q}uery \textbf{D}ecoder (\textbf{PDQD}), which distills object category awareness from a pre-trained detector to serve as object query initiation. This dual-branch query initialization enables the model to generate more interpretable and effective queries for HOI detection. Extensive experiments on HICO-Det and V-COCO benchmarks demonstrate that our method achieves state-of-the-art performance and strong generalization. Code will be released upon publication. </p>
<blockquote>
<p>äººç±»ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹æ—¨åœ¨å®šä½å›¾åƒä¸­çš„äºº-ç‰©ä½“å¯¹å¹¶è¯†åˆ«å…¶äº¤äº’ã€‚å°½ç®¡åŸºäºDETRçš„æ–¹æ³•å·²ç»é€æ¸æˆä¸ºHOIæ£€æµ‹çš„ä¸»æµæ¡†æ¶ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼šéšæœºåˆå§‹åŒ–çš„æŸ¥è¯¢ç¼ºä¹æ˜ç¡®çš„è¯­ä¹‰ï¼Œå¯¼è‡´æ£€æµ‹æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†QueryCraftï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å³æ’å³ç”¨çš„äººæœºäº¤äº’æ£€æµ‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºå˜å‹å™¨çš„æŸ¥è¯¢åˆå§‹åŒ–ï¼Œç»“åˆäº†è¯­ä¹‰å…ˆéªŒçŸ¥è¯†å’Œå¼•å¯¼ç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŠ¨ä½œæ„ŸçŸ¥è·¨æ¨¡æ€è½¬æ¢å™¨ï¼ˆACTORï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è·¨æ¨¡æ€å˜å‹å™¨ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå…±åŒå…³æ³¨è§†è§‰åŒºåŸŸå’Œæ–‡æœ¬æç¤ºï¼Œä»¥æå–ä¸åŠ¨ä½œç›¸å…³çš„ç‰¹å¾ã€‚ACTORä¸ä»…å¯¹é½ä¸åŒæ¨¡æ€ï¼Œè¿˜åˆ©ç”¨è¯­è¨€å¼•å¯¼æ³¨æ„åŠ›æ¥æ¨æ–­äº¤äº’è¯­ä¹‰å¹¶äº§ç”Ÿè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æŸ¥è¯¢è¡¨ç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜äº†å¯¹è±¡çº§åˆ«çš„æŸ¥è¯¢è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ„ŸçŸ¥è’¸é¦æŸ¥è¯¢è§£ç å™¨ï¼ˆPDQDï¼‰ï¼Œå®ƒä»é¢„è®­ç»ƒçš„æ£€æµ‹å™¨ä¸­æç‚¼å‡ºç‰©ä½“ç±»åˆ«æ„è¯†ï¼Œä½œä¸ºç‰©ä½“æŸ¥è¯¢çš„åˆå§‹åŒ–ã€‚è¿™ç§åŒåˆ†æ”¯æŸ¥è¯¢åˆå§‹åŒ–ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´å…·è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§çš„æŸ¥è¯¢ï¼Œç”¨äºHOIæ£€æµ‹ã€‚åœ¨HICO-Detå’ŒV-COCOåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å°†åœ¨å‡ºç‰ˆæ—¶å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08590v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰å…ˆéªŒå’Œå¼•å¯¼ç‰¹å¾å­¦ä¹ çš„HOIæ£€æµ‹æ¡†æ¶QueryCraftï¼Œé€šè¿‡åˆå§‹åŒ–çš„æŸ¥è¯¢æ¥è¯†åˆ«å›¾åƒä¸­çš„äººä¸ç‰©ä½“çš„äº¤äº’ã€‚æ ¸å¿ƒæ–¹æ³•æ˜¯åˆ©ç”¨è·¨æ¨¡æ€Transformerç¼–ç å™¨ACTORï¼Œå®ƒå…³æ³¨è§†è§‰åŒºåŸŸå’Œæ–‡æœ¬æç¤ºï¼Œæå–åŠ¨ä½œç›¸å…³ç‰¹å¾ï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼æ³¨æ„åŠ›æ¨æ–­äº¤äº’è¯­ä¹‰å¹¶äº§ç”Ÿè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æŸ¥è¯¢è¡¨ç¤ºã€‚ä¸ºæé«˜å¯¹è±¡çº§åˆ«æŸ¥è¯¢è´¨é‡ï¼Œå¼•å…¥äº†æ„ŸçŸ¥è’¸é¦æŸ¥è¯¢è§£ç å™¨PDQDï¼Œä»é¢„è®­ç»ƒæ£€æµ‹å™¨ä¸­è’¸é¦å‡ºç‰©ä½“ç±»åˆ«æ„è¯†ç”¨äºå¯¹è±¡æŸ¥è¯¢å¯åŠ¨ã€‚åŒåˆ†æ”¯æŸ¥è¯¢åˆå§‹åŒ–ä½¿æ¨¡å‹ç”Ÿæˆæ›´å¯è§£é‡Šå’Œæœ‰æ•ˆçš„æŸ¥è¯¢è¿›è¡ŒHOIæ£€æµ‹ã€‚åœ¨HICO-Detå’ŒV-COCOåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>QueryCraftæ˜¯ä¸€ç§æ–°å‹çš„HOIæ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³DETRæ–¹æ³•ä¸­éšæœºåˆå§‹åŒ–æŸ¥è¯¢è¯­ä¹‰ä¸æ˜ç¡®çš„é—®é¢˜ã€‚</li>
<li>ACTORæ˜¯è·¨æ¨¡æ€Transformerç¼–ç å™¨ï¼Œå…³æ³¨è§†è§‰åŒºåŸŸå’Œæ–‡æœ¬æç¤ºä»¥æå–åŠ¨ä½œç›¸å…³ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¯­è¨€å¼•å¯¼æ³¨æ„åŠ›æ¨æ–­äº¤äº’è¯­ä¹‰ã€‚</li>
<li>PDQDè§£ç å™¨ç”¨äºæé«˜å¯¹è±¡çº§åˆ«æŸ¥è¯¢è´¨é‡ï¼Œé€šè¿‡ä»é¢„è®­ç»ƒæ£€æµ‹å™¨ä¸­è’¸é¦ç‰©ä½“ç±»åˆ«æ„è¯†æ¥åˆå§‹åŒ–å¯¹è±¡æŸ¥è¯¢ã€‚</li>
<li>åŒåˆ†æ”¯æŸ¥è¯¢åˆå§‹åŒ–ç”Ÿæˆæ›´å¯è§£é‡Šå’Œæœ‰æ•ˆçš„æŸ¥è¯¢è¿›è¡ŒHOIæ£€æµ‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc16ac7a16bc274e054f1113fcb7f876.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-079cc073a4afe9478335d808a9cd9849.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffaa47c5f13ffd2ea0f43477b2e8d4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67ac39bb6e6bf2a2e7dc614c498ec96c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b63ab9ee45f114b72295e4d74fd49761.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Context-Engineering-for-Multi-Agent-LLM-Code-Assistants-Using-Elicit-NotebookLM-ChatGPT-and-Claude-Code"><a href="#Context-Engineering-for-Multi-Agent-LLM-Code-Assistants-Using-Elicit-NotebookLM-ChatGPT-and-Claude-Code" class="headerlink" title="Context Engineering for Multi-Agent LLM Code Assistants Using Elicit,   NotebookLM, ChatGPT, and Claude Code"></a>Context Engineering for Multi-Agent LLM Code Assistants Using Elicit,   NotebookLM, ChatGPT, and Claude Code</h2><p><strong>Authors:Muhammad Haseeb</strong></p>
<p>Large Language Models (LLMs) have shown promise in automating code generation and software engineering tasks, yet they often struggle with complex, multi-file projects due to context limitations and knowledge gaps. We propose a novel context engineering workflow that combines multiple AI components: an Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered semantic literature retrieval for injecting domain knowledge, NotebookLM-based document synthesis for contextual understanding, and a Claude Code multi-agent system for code generation and validation. Our integrated approach leverages intent clarification, retrieval-augmented generation, and specialized sub-agents orchestrated via Claudeâ€™s agent framework. We demonstrate that this method significantly improves the accuracy and reliability of code assistants in real-world repositories, yielding higher single-shot success rates and better adherence to project context than baseline single-agent approaches. Qualitative results on a large Next.js codebase show the multi-agent system effectively plans, edits, and tests complex features with minimal human intervention. We compare our system with recent frameworks like CodePlan, MASAI, and HyperAgent, highlighting how targeted context injection and agent role decomposition lead to state-of-the-art performance. Finally, we discuss the implications for deploying LLM-based coding assistants in production, along with lessons learned on context management and future research directions. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºä¸Šä¸‹æ–‡é™åˆ¶å’ŒçŸ¥è¯†å·®è·ï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„å¤šæ–‡ä»¶é¡¹ç›®æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¤šä¸ªAIç»„ä»¶çš„æ–°å‹ä¸Šä¸‹æ–‡å·¥ç¨‹å·¥ä½œæµç¨‹ï¼šä½¿ç”¨GPT-5æ„å›¾ç¿»è¯‘å™¨æ¥æ˜ç¡®ç”¨æˆ·éœ€æ±‚ï¼Œåˆ©ç”¨Elicitå¢å¼ºçš„è¯­ä¹‰æ–‡çŒ®æ£€ç´¢æ¥æ³¨å…¥é¢†åŸŸçŸ¥è¯†ï¼Œä½¿ç”¨NotebookLMåŸºäºæ–‡æ¡£çš„åˆæˆè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œä»¥åŠä½¿ç”¨Claude Codeå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œä»£ç ç”Ÿæˆå’ŒéªŒè¯ã€‚æˆ‘ä»¬çš„ç»¼åˆæ–¹æ³•åˆ©ç”¨æ„å›¾æ¾„æ¸…ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé€šè¿‡Claudeçš„ä»£ç†æ¡†æ¶åè°ƒçš„ä¸“é—¨å­ä»£ç†ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†çœŸå®ä¸–ç•Œå­˜å‚¨åº“ä¸­ä»£ç åŠ©æ‰‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¸åŸºçº¿å•ä»£ç†æ–¹æ³•ç›¸æ¯”ï¼Œå•æ¬¡æˆåŠŸç‡å’Œé¡¹ç›®ä¸Šä¸‹æ–‡çš„éµå¾ªåº¦æ›´é«˜ã€‚åœ¨å¤§å‹Next.jsä»£ç åº“ä¸Šçš„å®šæ€§ç»“æœè¯æ˜äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°è§„åˆ’ã€ç¼–è¾‘å’Œæµ‹è¯•å¤æ‚åŠŸèƒ½ï¼Œå‡ ä¹æ— éœ€äººå·¥å¹²é¢„ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç³»ç»Ÿä¸æœ€è¿‘çš„CodePlanã€MASAIå’ŒHyperAgentç­‰æ¡†æ¶è¿›è¡Œäº†æ¯”è¾ƒï¼Œé‡ç‚¹ä»‹ç»äº†å¦‚ä½•é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„ä¸Šä¸‹æ–‡æ³¨å…¥å’Œæ™ºèƒ½ä½“è§’è‰²åˆ†è§£æ¥å®ç°æœ€æ–°æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²åŸºäºLLMçš„ç¼–ç åŠ©æ‰‹çš„å«ä¹‰ï¼Œä»¥åŠåœ¨ä¸Šä¸‹æ–‡ç®¡ç†æ–¹é¢çš„ç»éªŒæ•™è®­å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08322v1">PDF</a> 15 pages, 5 figures, research paper on multi-agent LLM systems for   code generation</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚å¤šæ–‡ä»¶é¡¹ç›®æ—¶é¢ä¸´è¯­å¢ƒé™åˆ¶å’ŒçŸ¥è¯†é¸¿æ²Ÿçš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¤šç§AIç»„ä»¶çš„æ–°å‹è¯­å¢ƒå·¥ç¨‹æµç¨‹ï¼ŒåŒ…æ‹¬æ„å›¾ç¿»è¯‘å™¨ï¼ˆGPT-5ï¼‰ã€Elicité©±åŠ¨çš„è¯­ä¹‰æ–‡çŒ®æ£€ç´¢ã€NotebookLMåŸºç¡€çš„æ–‡æ¡£åˆæˆä»¥åŠClaude Codeå¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¿›è¡Œä»£ç ç”Ÿæˆå’ŒéªŒè¯ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å›¾æ¾„æ¸…ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé€šè¿‡Claudeçš„æ™ºèƒ½ä½“æ¡†æ¶åè°ƒçš„ä¸“ä¸šå­æ™ºèƒ½ä½“ï¼Œæ˜¾è‘—æé«˜äº†ä»£ç åŠ©æ‰‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œå®ç°äº†æ›´é«˜çš„å•æ¬¡æˆåŠŸç‡ï¼Œæ›´å¥½åœ°é€‚åº”äº†é¡¹ç›®è¯­å¢ƒã€‚åœ¨Next.jsä»£ç åº“çš„å¤§è§„æ¨¡å®šæ€§ç»“æœä¸­ï¼Œæ˜¾ç¤ºäº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°è§„åˆ’ã€ç¼–è¾‘å’Œæµ‹è¯•å¤æ‚åŠŸèƒ½ï¼Œå‡ ä¹æ— éœ€äººå·¥å¹²é¢„ã€‚æœ€åï¼Œæœ¬æ–‡è®¨è®ºäº†å°†LLMåŸºäºçš„ç¼–ç åŠ©æ‰‹ç”¨äºç”Ÿäº§çš„å½±å“ï¼Œä»¥åŠå…³äºè¯­å¢ƒç®¡ç†å’Œæœªæ¥ç ”ç©¶æ–¹å‘çš„ç»éªŒæ•™è®­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚å¤šæ–‡ä»¶é¡¹ç›®æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è¯­å¢ƒå·¥ç¨‹æµç¨‹ï¼Œç»“åˆå¤šç§AIç»„ä»¶ä»¥æé«˜ä»£ç åŠ©æ‰‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>é€šè¿‡æ„å›¾æ¾„æ¸…ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œä¸“ç”¨å­æ™ºèƒ½ä½“åè°ƒï¼Œå®ç°äº†æ›´é«˜çš„å•æ¬¡æˆåŠŸç‡å’Œæ›´å¥½çš„é¡¹ç›®è¯­å¢ƒé€‚åº”æ€§ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨Next.jsä»£ç åº“çš„å¤§è§„æ¨¡æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§„åˆ’ã€ç¼–è¾‘å’Œæµ‹è¯•å¤æ‚åŠŸèƒ½ã€‚</li>
<li>ä¸CodePlanã€MASAIå’ŒHyperAgentç­‰æ¡†æ¶ç›¸æ¯”ï¼Œæœ¬ç³»ç»Ÿçš„é’ˆå¯¹æ€§è¯­å¢ƒæ³¨å…¥å’Œæ™ºèƒ½ä½“è§’è‰²åˆ†è§£è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</li>
<li>è®¨è®ºäº†å°†LLMç”¨äºç”Ÿäº§çš„å½±å“ï¼Œå¼ºè°ƒäº†éƒ¨ç½²æ—¶çš„æ½œåœ¨æŒ‘æˆ˜å’Œæœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cea457d446f6ad81cdd52d2074f4a17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f72e5aa86eda5999675137ed35bfbfc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-204b64e738d8ee86f198f107509b0bd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50ea7ab5a658311a78f1e37229ac86cc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge"><a href="#Grounding-Multilingual-Multimodal-LLMs-With-Cultural-Knowledge" class="headerlink" title="Grounding Multilingual Multimodal LLMs With Cultural Knowledge"></a>Grounding Multilingual Multimodal LLMs With Cultural Knowledge</h2><p><strong>Authors:Jean de Dieu Nyandwi, Yueqi Song, Simran Khanuja, Graham Neubig</strong></p>
<p>Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. CulturalPangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of 5.0 without degrading results on mainstream vision-language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¸¸å¸¸è¯¯è§£é•¿å°¾æ–‡åŒ–å®ä½“å¹¶åœ¨ä½èµ„æºè¯­è¨€ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥å°†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ ¹æ¤äºæ–‡åŒ–çŸ¥è¯†ä¸­ã€‚æˆ‘ä»¬åˆ©ç”¨æ¥è‡ªç»´åŸºç™¾ç§‘çš„å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ï¼Œæ”¶é›†ä»£è¡¨æ–‡åŒ–é‡è¦å®ä½“çš„å›¾åƒï¼Œå¹¶ç”Ÿæˆåˆæˆå¤šè¯­è¨€è§†è§‰é—®ç­”æ•°æ®ã€‚æ‰€å¾—æ•°æ®é›†CulturalGroundåŒ…å«2200ä¸‡é«˜è´¨é‡ã€æ–‡åŒ–ä¸°å¯Œçš„é—®ç­”å¯¹ï¼Œæ¶µç›–42ä¸ªå›½å®¶å’Œ39ç§è¯­è¨€ã€‚æˆ‘ä»¬åœ¨CulturalGroundä¸Šè®­ç»ƒå¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹CulturalPangeaï¼ŒåŒæ—¶äº¤ç»‡æ ‡å‡†çš„å¤šè¯­è¨€æŒ‡ä»¤å¾®è°ƒæ•°æ®ä»¥ä¿ç•™å…¶ä¸€èˆ¬èƒ½åŠ›ã€‚CulturalPangeaåœ¨å„ç§ä»¥æ–‡åŒ–ä¸ºé‡ç‚¹çš„å¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºå…ˆå‰çš„æ¨¡å‹ï¼Œå¹³å‡æ€§èƒ½æé«˜äº†5.0%ï¼ŒåŒæ—¶åœ¨ä¸»æµè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„ç»“æœå¹¶æœªé™ä½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬é’ˆå¯¹æ€§ã€ä»¥æ–‡åŒ–ä¸ºåŸºç¡€çš„æ–¹æ³•å¯ä»¥å¤§å¹…åº¦ç¼©å°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–å·®è·ï¼Œå¹¶ä¸ºå®ç°å…¨çƒåŒ…å®¹æ€§å¤šæ¨¡æ€ç³»ç»Ÿæä¾›å®é™…è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07414v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºç¯å¢ƒä¸‹è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é•¿å°¾æ–‡åŒ–å®ä½“æ–¹é¢å¸¸å¸¸è¯¯è§£ï¼Œå¹¶ä¸”åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œç›´æ¥ä»¥æ–‡åŒ–ä¸ºåŸºç¡€å¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡åˆ©ç”¨ç»´åŸºç™¾ç§‘çš„å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±æ”¶é›†ä»£è¡¨æ–‡åŒ–é‡è¦å®ä½“çš„å›¾åƒï¼Œå¹¶ç”Ÿæˆåˆæˆå¤šè¯­è¨€è§†è§‰é—®ç­”æ•°æ®ã€‚æ‰€æ„å»ºçš„CulturalGroundæ•°æ®é›†åŒ…å«è·¨è¶Š42ä¸ªå›½å®¶å’Œ39ç§è¯­è¨€çš„22äº¿é«˜è´¨é‡ã€æ–‡åŒ–ä¸°å¯Œçš„é—®ç­”å¯¹ã€‚è®­ç»ƒå‡ºçš„å¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹CulturalPangeaåœ¨å¤šç§é¢å‘æ–‡åŒ–çš„å¤šè¯­è¨€å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œå¹³å‡ä¼˜äºç°æœ‰æ¨¡å‹5.0%ï¼ŒåŒæ—¶åœ¨ä¸»æµè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šæœªå‡ºç°æ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ‰é’ˆå¯¹æ€§çš„æ–‡åŒ–åŸºç¡€æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—ç¼©å°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–å·®è·ï¼Œå¹¶ä¸ºå…¨çƒåŒ…å®¹æ€§å¤šæ¨¡æ€ç³»ç»Ÿæä¾›äº†å®é™…çš„å‘å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºç¯å¢ƒä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†é•¿å°¾æ–‡åŒ–å®ä½“å’Œä½èµ„æºè¯­è¨€æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç»´åŸºç™¾ç§‘çŸ¥è¯†å›¾è°±å’Œå›¾åƒæ•°æ®æ¥ä¸°å¯Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–‡åŒ–çŸ¥è¯†ã€‚</li>
<li>æ„å»ºäº†CulturalGroundæ•°æ®é›†ï¼ŒåŒ…å«è·¨è¶Šå¤šä¸ªå›½å®¶å’Œè¯­è¨€çš„ä¸°å¯Œæ–‡åŒ–è§†è§‰é—®ç­”å¯¹ã€‚</li>
<li>è®­ç»ƒå‡ºçš„å¼€æºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹CulturalPangeaåœ¨å¤šç§æ–‡åŒ–ç›¸å…³çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>CulturalPangeaæ¨¡å‹åœ¨ä¿æŒä¸»æµè§†è§‰è¯­è¨€ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†åœ¨æ–‡åŒ–é¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºï¼Œæœ‰é’ˆå¯¹æ€§çš„æ–‡åŒ–åŸºç¡€æ–¹æ³•æœ‰åŠ©äºç¼©å°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡åŒ–å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2145087567e8317b2d32f651e032b5f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2f526f54f9eff944fd028d31717a3c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef8b3cfc62d3e27b46cc426138d74d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da0c55689a98f2eb73691fb612b3135d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4ef3ab6c7fb782bb89e0c32f8937528.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language"><a href="#Technical-Report-Full-Stack-Fine-Tuning-for-the-Q-Programming-Language" class="headerlink" title="Technical Report: Full-Stack Fine-Tuning for the Q Programming Language"></a>Technical Report: Full-Stack Fine-Tuning for the Q Programming Language</h2><p><strong>Authors:Brendan R. Hogan, Will Brown, Adel Boyarsky, Anderson Schneider, Yuriy Nevmyvaka</strong></p>
<p>Even though large language models are becoming increasingly capable, it is still unreasonable to expect them to excel at tasks that are under-represented on the Internet. Leveraging LLMs for specialized applications, particularly in niche programming languages and private domains, remains challenging and largely unsolved. In this work, we address this gap by presenting a comprehensive, open-source approach for adapting LLMs to the Q programming language, a popular tool in quantitative finance that is much less present on the Internet compared to Python, C, Java, and other &#96;&#96;mainstreamâ€ languages and is therefore not a strong suit of general-purpose AI models. We introduce a new Leetcode style evaluation dataset for Q, benchmark major frontier models on the dataset, then do pretraining, supervised fine tuning, and reinforcement learning to train a suite of reasoning and non-reasoning models based on the Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our best model achieves a pass@1 accuracy of 59 percent on our Q benchmark, surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent. Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task. In addition to releasing models, code, and data, we provide a detailed blueprint for dataset construction, model pretraining, supervised fine-tuning, and reinforcement learning. Our methodology is broadly applicable, and we discuss how these techniques can be extended to other tasks, including those where evaluation may rely on soft or subjective signals. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œä½†æœŸæœ›å®ƒä»¬åœ¨äº’è”ç½‘ä¸Šè¡¨ç°ä¸è¶³çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ä»ç„¶ä¸åˆç†ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºä¸“ä¸šåº”ç”¨ç¨‹åºï¼Œç‰¹åˆ«æ˜¯åœ¨å°ä¼—ç¼–ç¨‹è¯­è¨€å’Œç§æœ‰é¢†åŸŸï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ä¸”å°šæœªå®Œå…¨è§£å†³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§é€‚åº”Qç¼–ç¨‹è¯­è¨€çš„å…¨é¢å¼€æºæ–¹æ³•æ¥è§£å†³è¿™ä¸€å·®è·ã€‚Qæ˜¯é‡åŒ–é‡‘èä¸­æµè¡Œçš„å·¥å…·ï¼Œä¸Pythonã€Cã€Javaç­‰â€œä¸»æµâ€è¯­è¨€ç›¸æ¯”ï¼Œå…¶åœ¨äº’è”ç½‘ä¸Šçš„å­˜åœ¨åº¦è¾ƒä½ï¼Œå› æ­¤ä¸æ˜¯é€šç”¨AIæ¨¡å‹çš„å¼ºé¡¹ã€‚æˆ‘ä»¬ä¸ºQè¯­è¨€å¼•å…¥äº†ä¸€ä¸ªæ–°çš„Leetcodeé£æ ¼è¯„ä¼°æ•°æ®é›†ï¼Œåœ¨è¯¥æ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸»è¦çš„å‰æ²¿æ¨¡å‹ï¼Œç„¶åè¿›è¡Œé¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ï¼ŒåŸºäºQwen-2.5ç³»åˆ—è®­ç»ƒäº†ä¸€å¥—æ¨ç†å’Œéæ¨ç†æ¨¡å‹ï¼Œæ¶µç›–äº”ç§å‚æ•°å¤§å°ï¼ˆ1.5Bã€3Bã€7Bã€14Bã€32Bï¼‰ã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹åœ¨æˆ‘ä»¬çš„QåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†59%çš„pass@1å‡†ç¡®ç‡ï¼Œæ¯”æœ€ä½³æ€§èƒ½çš„å‰æ²¿æ¨¡å‹Claude Opus-4é«˜å‡º29.5%ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹ï¼Œå³ä½¿æ˜¯æˆ‘ä»¬æœ€å°çš„1.5Bæ¨¡å‹ï¼Œåœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºGPT-4.1ã€‚é™¤äº†å‘å¸ƒæ¨¡å‹ã€ä»£ç å’Œæ•°æ®å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…³äºæ•°æ®é›†æ„å»ºã€æ¨¡å‹é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ çš„è¯¦ç»†è“å›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ™®éé€‚ç”¨çš„ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡ï¼ŒåŒ…æ‹¬é‚£äº›å¯èƒ½ä¾èµ–äºè½¯æˆ–ä¸»è§‚ä¿¡å·çš„è¯„ä¼°ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06813v2">PDF</a> 40 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨ä»£è¡¨æ€§ä¸è¶³çš„ç½‘ç»œä»»åŠ¡ä¸Šä»æœ‰å±€é™æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨å°ä¼—ç¼–ç¨‹è¯­è¨€å’Œç§æœ‰é¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œè¿ç”¨LLMä»å­˜åœ¨æŒ‘æˆ˜ä¸”å°šæœªå®Œå…¨è§£å†³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€ç¼ºå£ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹Qç¼–ç¨‹è¯­è¨€çš„å¼€æºé€‚åº”æ–¹æ³•ã€‚Qæ˜¯å®šé‡é‡‘èé¢†åŸŸçš„æµè¡Œå·¥å…·ï¼Œç›¸è¾ƒäºPythonã€Cã€Javaç­‰ä¸»æµè¯­è¨€ï¼Œå…¶åœ¨äº’è”ç½‘ä¸Šçš„å­˜åœ¨è¾ƒå°‘ï¼Œä¹Ÿæ˜¯é€šç”¨AIæ¨¡å‹çš„è–„å¼±ç¯èŠ‚ã€‚ç ”ç©¶æ„å»ºäº†é’ˆå¯¹Qè¯­è¨€çš„Leetcodeé£æ ¼è¯„ä¼°æ•°æ®é›†ï¼Œå¯¹å‰æ²¿æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶é‡‡ç”¨é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•ï¼ŒåŸºäºQwen-2.5ç³»åˆ—è®­ç»ƒäº†ä¸€å¥—æ¨ç†å’Œéæ¨ç†æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡åŒ…æ‹¬äº”ç§ï¼ˆ1.5Bã€3Bã€7Bã€14Bã€32Bï¼‰ã€‚æœ€ä½³æ¨¡å‹åœ¨QåŸºå‡†æµ‹è¯•ä¸Šçš„pass@1å‡†ç¡®ç‡è¾¾åˆ°äº†59%ï¼Œæ¯”æœ€ä½³å‰æ²¿æ¨¡å‹Claude Opus-4é«˜å‡º29.5%ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ€å°çš„1.5Bæ¨¡å‹ï¼Œåœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºGPT-4.1ã€‚é™¤äº†å‘å¸ƒæ¨¡å‹ã€ä»£ç å’Œæ•°æ®å¤–ï¼Œè¿˜æä¾›äº†è¯¦ç»†çš„æ•°æ®é›†æ„å»ºã€æ¨¡å‹é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„è“å›¾ã€‚æ­¤æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡ï¼ŒåŒ…æ‹¬é‚£äº›å¯èƒ½ä¾èµ–äºè½¯æ€§æŒ‡æ ‡æˆ–ä¸»è§‚ä¿¡å·çš„è¯„ä¼°ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£è¡¨æ€§ä¸è¶³çš„ç½‘ç»œä»»åŠ¡ä¸Šè¡¨ç°ä»æœ‰å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å°ä¼—ç¼–ç¨‹è¯­è¨€å’Œç§æœ‰é¢†åŸŸçš„åº”ç”¨ä¸­ã€‚</li>
<li>é’ˆå¯¹Qç¼–ç¨‹è¯­è¨€ï¼ˆåœ¨å®šé‡é‡‘èé¢†åŸŸæµè¡Œä½†äº’è”ç½‘ä¸Šè¾ƒå°‘æ¶‰åŠï¼‰çš„ç ”ç©¶å¡«è¡¥äº†LLMåœ¨è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†é’ˆå¯¹Qè¯­è¨€çš„Leetcodeé£æ ¼è¯„ä¼°æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹æ€§èƒ½è¯„ä¼°æä¾›äº†åŸºå‡†ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œè®­ç»ƒäº†åŸºäºQwen-2.5ç³»åˆ—çš„æ¨ç†å’Œéæ¨ç†æ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡å¤šæ ·ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨QåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡ç°æœ‰æœ€ä½³å‰æ²¿æ¨¡å‹ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹åœ¨ä»»åŠ¡è¡¨ç°ä¸Šå‡ä¼˜äºGPT-4.1ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92b614bf599590fa3389377d8548d29f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f51bd7dc885f5fb6c792d0e3d899a3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Argus-Inspection-Do-Multimodal-Large-Language-Models-Possess-the-Eye-of-Panoptes"><a href="#Argus-Inspection-Do-Multimodal-Large-Language-Models-Possess-the-Eye-of-Panoptes" class="headerlink" title="Argus Inspection: Do Multimodal Large Language Models Possess the Eye of   Panoptes?"></a>Argus Inspection: Do Multimodal Large Language Models Possess the Eye of   Panoptes?</h2><p><strong>Authors:Yang Yao, Lingyu Li, Jiaxin Song, Chiyu Chen, Zhenqi He, Yixu Wang, Xin Wang, Tianle Gu, Jie Li, Yan Teng, Yingchun Wang</strong></p>
<p>As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMsâ€™ responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œå…¶è®¤çŸ¥å’Œæ¨ç†èƒ½åŠ›å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœ¨ç²¾ç»†è§†è§‰æ„ŸçŸ¥å’Œå¸¸è¯†æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†Argus Inspectionï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰ä¸¤ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¼ºè°ƒè¯¦ç»†çš„è§†è§‰è¯†åˆ«ï¼ŒåŒæ—¶ç»“åˆç°å®ä¸–ç•Œå¸¸è¯†ç†è§£æ¥è¯„ä¼°å› æœæ¨ç†èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Panoptesä¹‹çœ¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†äºŒå…ƒå‚æ•°Sigmoidåº¦é‡ä¸æŒ‡ç¤ºå‡½æ•°ç›¸ç»“åˆï¼Œèƒ½å¤Ÿåœ¨åŸºäºæ„è§æ¨ç†ä»»åŠ¡ä¸­å¯¹MLLMsçš„å“åº”è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ã€‚å¯¹26ä¸ªä¸»æµMLLMsè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œç²¾ç»†è§†è§‰æ¨ç†çš„æœ€é«˜æ€§èƒ½ä»…è¾¾åˆ°0.46ï¼Œå‡¸æ˜¾äº†å·¨å¤§çš„æå‡æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºMLLMsçš„æŒç»­æ”¹è¿›æä¾›äº†å®è´µçš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14805v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œå…¶è®¤çŸ¥å’Œæ¨ç†èƒ½åŠ›å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œåœ¨ç²¾ç»†è§†è§‰æ„ŸçŸ¥å’Œå¸¸è¯†æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†Argus Inspectionå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¼ºè°ƒè¯¦ç»†çš„è§†è§‰è¯†åˆ«ï¼ŒåŒæ—¶ç»“åˆç°å®ä¸–ç•Œå¸¸è¯†ç†è§£æ¥è¯„ä¼°å› æœæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†Panoptesæ¡†æ¶çš„Eyeï¼Œè¯¥æ¡†æ¶ç»“åˆäº†äºŒå…ƒå‚æ•°SigmoidæŒ‡æ ‡å’ŒæŒ‡ç¤ºå‡½æ•°ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°MLLMsåœ¨åŸºäºæ„è§æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å¯¹ä¸»æµMLLMçš„å®éªŒè¡¨æ˜ï¼Œç²¾ç»†è§†è§‰æ¨ç†çš„æœ€é«˜æ€§èƒ½ä»…ä¸º0.46ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ”¹è¿›æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸ºMLLMçš„æŒç»­æ”¹è¿›æä¾›äº†å®è´µçš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è®¤çŸ¥å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>Argus Inspectionå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å¼ºè°ƒè§†è§‰ç²¾ç»†æ„ŸçŸ¥å’Œå¸¸è¯†æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚</li>
<li>Argus InspectionåŒ…å«ä¸¤ä¸ªéš¾åº¦çº§åˆ«ï¼Œæ—¨åœ¨å…¨é¢è¯„ä»·MLLMsçš„æ€§èƒ½ã€‚</li>
<li>Eye of Panoptesæ¡†æ¶èƒ½å¤Ÿæ›´å…¨é¢åœ°è¯„ä¼°MLLMsåœ¨åŸºäºæ„è§æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>ç°æœ‰MLLMåœ¨è§†è§‰ç²¾ç»†æ¨ç†æ–¹é¢çš„æ€§èƒ½æœ‰å¾…æé«˜ï¼Œå­˜åœ¨å·¨å¤§çš„æ”¹è¿›ç©ºé—´ã€‚</li>
<li>SigmoidæŒ‡æ ‡ä¸æŒ‡ç¤ºå‡½æ•°çš„ç»“åˆä¸ºè¯„ä¼°MLLMsæä¾›äº†æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-467adc90990784f39804b2c86009f296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a9048442380bf51385e63526eb83b0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc59160978cd76811fedabf1219988d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81db4cf795663740af348b7fafbef9d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce7ba5e5e40451edff35f094994571e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94663ba586996a72d61b1a093913d8e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57aa05ad98f7eb5e7efb3eba748de3d9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Dopamine-Audiobook-A-Training-free-MLLM-Agent-for-Emotional-and-Immersive-Audiobook-Generation"><a href="#Dopamine-Audiobook-A-Training-free-MLLM-Agent-for-Emotional-and-Immersive-Audiobook-Generation" class="headerlink" title="Dopamine Audiobook: A Training-free MLLM Agent for Emotional and   Immersive Audiobook Generation"></a>Dopamine Audiobook: A Training-free MLLM Agent for Emotional and   Immersive Audiobook Generation</h2><p><strong>Authors:Yan Rong, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Audiobook generation aims to create rich, immersive listening experiences from multimodal inputs, but current approaches face three critical challenges: (1) the lack of synergistic generation of diverse audio types (e.g., speech, sound effects, and music) with precise temporal and semantic alignment; (2) the difficulty in conveying expressive, fine-grained emotions, which often results in machine-like vocal outputs; and (3) the absence of automated evaluation frameworks that align with human preferences for complex and diverse audio. To address these issues, we propose Dopamine Audiobook, a novel unified training-free multi-agent system, where a multimodal large language model (MLLM) serves two specialized roles (i.e., speech designer and audio designer) for emotional, human-like, and immersive audiobook generation and evaluation. Specifically, we firstly propose a flow-based, context-aware framework for diverse audio generation with word-level semantic and temporal alignment. To enhance expressiveness, we then design word-level paralinguistic augmentation, utterance-level prosody retrieval, and adaptive TTS model selection. Finally, for evaluation, we introduce a novel MLLM-based evaluation framework incorporating self-critique, perspective-taking, and psychological MagicEmo prompts to ensure human-aligned and self-aligned assessments. Experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance on multiple metrics. Importantly, our evaluation framework shows better alignment with human preferences and transferability across audio tasks. </p>
<blockquote>
<p>æœ‰å£°ä¹¦ç”Ÿæˆæ—¨åœ¨ä»å¤šæ¨¡å¼è¾“å…¥åˆ›å»ºä¸°å¯Œã€æ²‰æµ¸å¼çš„å¬è§‰ä½“éªŒï¼Œä½†å½“å‰æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç¼ºä¹ååŒç”Ÿæˆå…·æœ‰ç²¾ç¡®æ—¶é—´å’Œè¯­ä¹‰å¯¹é½çš„å¤šç§éŸ³é¢‘ç±»å‹ï¼ˆå¦‚è¯­éŸ³ã€éŸ³æ•ˆå’ŒéŸ³ä¹ï¼‰ï¼›ï¼ˆ2ï¼‰è¡¨è¾¾ç»†è…»æƒ…æ„Ÿçš„å›°éš¾ï¼Œè¿™å¾€å¾€å¯¼è‡´æœºå™¨èˆ¬çš„è¯­éŸ³è¾“å‡ºï¼›ï¼ˆ3ï¼‰ç¼ºä¹ä¸å¤æ‚å’Œå¤šæ ·éŸ³é¢‘çš„äººç±»åå¥½ç›¸ç¬¦çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå·´èƒºæœ‰å£°ä¹¦ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„ç»Ÿä¸€ã€æ— éœ€è®­ç»ƒçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå…¶ä¸­å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰®æ¼”ä¸¤ä¸ªä¸“ä¸šè§’è‰²ï¼ˆå³è¯­éŸ³è®¾è®¡å¸ˆå’ŒéŸ³é¢‘è®¾è®¡å¸ˆï¼‰ï¼Œç”¨äºæƒ…æ„ŸåŒ–ã€äººæ€§åŒ–ã€æ²‰æµ¸å¼æœ‰å£°ä¹¦çš„ç”Ÿæˆå’Œè¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªåŸºäºæµçš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç”¨äºå…·æœ‰å•è¯çº§è¯­ä¹‰å’Œæ—¶é—´å¯¹é½çš„å¤šæ ·éŸ³é¢‘ç”Ÿæˆã€‚ä¸ºäº†æé«˜è¡¨è¾¾åŠ›ï¼Œç„¶åæˆ‘ä»¬è®¾è®¡äº†å•è¯çº§å‰¯è¯­è¨€å¢å¼ºã€å¥å­çº§è¯­è°ƒæ£€ç´¢å’Œè‡ªé€‚åº”TTSæ¨¡å‹é€‰æ‹©ã€‚æœ€åï¼Œåœ¨è¯„ä¼°æ–¹é¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºMLLMçš„è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆè‡ªæˆ‘æ‰¹è¯„ã€æ¢ä½æ€è€ƒå’Œå¿ƒç†MagicEmoæç¤ºï¼Œç¡®ä¿äººç±»å’Œè‡ªæˆ‘çš„è¯„ä¼°å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜¾ç¤ºå‡ºæ›´å¥½çš„ä¸äººç±»åå¥½å¯¹é½çš„èƒ½åŠ›ï¼Œå¹¶åœ¨éŸ³é¢‘ä»»åŠ¡ä¹‹é—´å…·æœ‰å¯è½¬ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11002v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘ä¹¦ç”Ÿæˆé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç±»å‹ååŒç”Ÿæˆä¸è¶³ã€è¡¨è¾¾ç²¾ç»†æƒ…ç»ªå›°éš¾å’Œç¼ºä¹ä¸äººç±»åå¥½å¯¹é½çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºå¤šå·´èƒºéŸ³é¢‘ä¹¦çš„ç»Ÿä¸€æ— è®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‹…ä»»è¯­éŸ³è®¾è®¡å¸ˆå’ŒéŸ³é¢‘è®¾è®¡å¸ˆä¸¤ä¸ªè§’è‰²ï¼Œå®ç°æƒ…æ„Ÿä¸°å¯Œã€äººæ€§åŒ–ã€æ²‰æµ¸å¼éŸ³é¢‘ä¹¦ç”Ÿæˆå’Œè¯„ä¼°ã€‚æå‡ºåŸºäºæµçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶è¿›è¡Œå¤šæ ·åŒ–éŸ³é¢‘ç”Ÿæˆï¼Œå®ç°è¯çº§è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ã€‚è®¾è®¡è¯çº§è¯­è¨€ä¿®è¾å¢å¼ºã€å¥å­çº§è¯­è°ƒæ£€ç´¢å’Œè‡ªé€‚åº”è¯­éŸ³åˆæˆæ¨¡å‹é€‰æ‹©ä»¥å¢å¼ºè¡¨ç°åŠ›ã€‚å¼•å…¥åŸºäºMLLMçš„è¯„ä¼°æ¡†æ¶è¿›è¡Œè‡ªè¯„ã€æ¢ä½æ€è€ƒå’Œå¿ƒç†MagicEmoæç¤ºï¼Œç¡®ä¿ä¸äººç±»åå¥½å’Œè‡ªæˆ‘å¯¹é½çš„è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œè¯„ä¼°æ¡†æ¶æ›´å¥½åœ°ä¸äººç±»åå¥½å¯¹é½å¹¶é€‚ç”¨äºå„ç§éŸ³é¢‘ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘ä¹¦ç”Ÿæˆé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šéŸ³é¢‘ç±»å‹ååŒç”Ÿæˆä¸è¶³ã€è¡¨è¾¾ç²¾ç»†æƒ…ç»ªå›°éš¾å’Œç¼ºä¹ä¸äººç±»åå¥½å¯¹é½çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>æå‡ºä¸€ç§ç»Ÿä¸€çš„æ— è®­ç»ƒå¤šæ™ºèƒ½ä½“ç³»ç»Ÿâ€”â€”å¤šå·´èƒºéŸ³é¢‘ä¹¦ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ‹…ä»»è¯­éŸ³è®¾è®¡å¸ˆå’ŒéŸ³é¢‘è®¾è®¡å¸ˆè§’è‰²ï¼Œå®ç°æƒ…æ„Ÿä¸°å¯Œã€äººæ€§åŒ–ã€æ²‰æµ¸å¼éŸ³é¢‘ä¹¦çš„ç”Ÿæˆã€‚</li>
<li>æå‡ºåŸºäºæµçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¡†æ¶è¿›è¡Œå¤šæ ·åŒ–éŸ³é¢‘ç”Ÿæˆï¼Œç¡®ä¿è¯çº§è¯­ä¹‰å’Œæ—¶é—´å¯¹é½ã€‚</li>
<li>é€šè¿‡è®¾è®¡è¯çº§è¯­è¨€ä¿®è¾å¢å¼ºã€å¥å­çº§è¯­è°ƒæ£€ç´¢å’Œè‡ªé€‚åº”è¯­éŸ³åˆæˆæ¨¡å‹é€‰æ‹©ï¼Œå¢å¼ºéŸ³é¢‘è¡¨è¾¾åŠ›ã€‚</li>
<li>å¼•å…¥åŸºäºMLLMçš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬è‡ªè¯„ã€æ¢ä½æ€è€ƒå’Œå¿ƒç†æç¤ºï¼Œç¡®ä¿è¯„ä¼°ä¸äººç±»åå¥½å’Œè‡ªæˆ‘å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5c19a23766aa15689eeb1d5756f72460.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4541c5fe70fa796b63778501741ebfbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4d5da7fa20addf0b3381941c6a3d9af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790438d134778cb2881119c8dc276b7f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Adaptive-Computation-Pruning-for-the-Forgetting-Transformer"><a href="#Adaptive-Computation-Pruning-for-the-Forgetting-Transformer" class="headerlink" title="Adaptive Computation Pruning for the Forgetting Transformer"></a>Adaptive Computation Pruning for the Forgetting Transformer</h2><p><strong>Authors:Zhixuan Lin, Johan Obando-Ceron, Xu Owen He, Aaron Courville</strong></p>
<p>The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. In particular, our method performs provably safe pruning via a dynamically set pruning threshold that guarantees the pruned attention weights are negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs and memory accesses in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 50% to 70% reduction in attention runtime (or a 2-3$\times$ speedup) and a roughly 10% to 40% increase in end-to-end training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer">https://github.com/zhixuan-lin/forgetting-transformer</a>. </p>
<blockquote>
<p>æœ€è¿‘æå‡ºçš„é—å¿˜è½¬æ¢å™¨ï¼ˆFoXï¼‰å°†é—å¿˜é—¨çº³å…¥softmaxæ³¨æ„åŠ›ä¸­ï¼Œä¸åŸºäºRoPEçš„æ ‡å‡†è½¬æ¢å™¨ç›¸æ¯”ï¼Œå…¶æ€§èƒ½ä¸€ç›´è¡¨ç°æ›´å¥½æˆ–ç›¸å½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFoXä¸­çš„è®¸å¤šæ³¨æ„åŠ›å¤´å€¾å‘äºå¿«é€Ÿé—å¿˜ï¼Œå¯¼è‡´å…¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä¸»è¦ä¾èµ–äºå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬ä¸ºFoXæå‡ºäº†è‡ªé€‚åº”è®¡ç®—ä¿®å‰ªï¼ˆACPï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŠ¨æ€åˆ é™¤æ¶‰åŠç”±é—å¿˜é—¨å¼ºçƒˆè¡°å‡çš„è¾“å…¥-è¾“å‡ºä¾èµ–å…³ç³»çš„è®¡ç®—ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŠ¨æ€è®¾ç½®çš„ä¿®å‰ªé˜ˆå€¼æ‰§è¡Œå¯è¯æ˜çš„å®‰å…¨ä¿®å‰ªï¼Œè¯¥é˜ˆå€¼ä¿è¯ä¿®å‰ªçš„æ³¨æ„åŠ›æƒé‡å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚æˆ‘ä»¬å°†ACPåº”ç”¨äºä½¿ç”¨FoXçš„è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒï¼Œå¹¶æ˜¾ç¤ºå®ƒå§‹ç»ˆå‡å°‘äº†softmaxæ³¨æ„åŠ›ä¸­çš„FLOPså’Œå†…å­˜è®¿é—®é‡çº¦70%ï¼Œè¿™é€‚ç”¨äºä¸åŒçš„æ¨¡å‹å¤§å°å’Œä¸Šä¸‹æ–‡é•¿åº¦ã€‚è¿™å¯¼è‡´æ³¨æ„åŠ›è¿è¡Œæ—¶é—´å¤§è‡´å‡å°‘äº†50%åˆ°70%ï¼ˆæˆ–æé«˜äº†2-3å€é€Ÿåº¦ï¼‰ï¼Œå¹¶ä¸”ç«¯åˆ°ç«¯è®­ç»ƒååé‡æé«˜äº†å¤§çº¦10%åˆ°40%ã€‚æ­¤å¤–ï¼Œè¾ƒé•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¼šå¸¦æ¥æ›´å¤§çš„è®¡ç®—èŠ‚çœã€‚æ‰€æœ‰è¿™äº›é€Ÿåº¦æå‡éƒ½æ˜¯åœ¨æ²¡æœ‰ä»»ä½•æ€§èƒ½ä¸‹é™çš„æƒ…å†µä¸‹å®ç°çš„ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer%E3%80%82">https://github.com/zhixuan-lin/forgetting-transformerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06949v2">PDF</a> Published as a conference paper at COLM 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæå‡ºçš„é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰ç»“åˆäº†é—å¿˜é—¨å’Œsoftmaxæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¡¨ç°å‡ºä¼˜äºæˆ–ç›¸å½“äºåŸºäºRoPEçš„Transformerçš„æ€§èƒ½ã€‚é’ˆå¯¹FoXä¸­è®¸å¤šæ³¨æ„åŠ›å¤´å¿«é€Ÿé—å¿˜çš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ã€‚ACPé€šè¿‡åŠ¨æ€è®¾ç½®çš„å‰ªæé˜ˆå€¼ï¼Œåœ¨ä¿è¯è¢«å‰ªæçš„æ³¨æ„åŠ›æƒé‡å¯ä»¥å¿½ç•¥ä¸è®¡çš„å‰æä¸‹è¿›è¡Œå®‰å…¨å‰ªæã€‚åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­åº”ç”¨ACPï¼Œå¯ä½¿softmaxæ³¨æ„åŠ›çš„FLOPså’Œå†…å­˜è®¿é—®é‡å‡å°‘çº¦70%ï¼Œæ³¨æ„åŠ›è¿è¡Œæ—¶é—´å‡å°‘çº¦50%è‡³70%ï¼ˆæˆ–åŠ é€Ÿ2-3å€ï¼‰ï¼Œæ•´ä½“è®­ç»ƒæ•ˆç‡æé«˜çº¦10%è‡³40%ã€‚åœ¨è¾ƒé•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ï¼Œè®¡ç®—èŠ‚çœæ›´ä¸ºæ˜¾è‘—ï¼Œä¸”è¿™äº›é€Ÿåº¦æå‡å¹¶æœªå¸¦æ¥æ€§èƒ½æŸå¤±ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰ç»“åˆäº†é—å¿˜é—¨å’Œsoftmaxæ³¨æ„åŠ›æœºåˆ¶ï¼Œå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>FoXä¸­çš„è®¸å¤šæ³¨æ„åŠ›å¤´å­˜åœ¨å¿«é€Ÿé—å¿˜ç°è±¡ï¼Œä¸»è¦ä¾èµ–å±€éƒ¨ä¸Šä¸‹æ–‡ã€‚</li>
<li>é’ˆå¯¹FoXæå‡ºäº†è‡ªé€‚åº”è®¡ç®—å‰ªæï¼ˆACPï¼‰æ–¹æ³•ï¼Œå¯åŠ¨æ€å‰ªæè¢«é—å¿˜é—¨å¼ºçƒˆè¡°å‡çš„è¾“å…¥è¾“å‡ºä¾èµ–è®¡ç®—ã€‚</li>
<li>ACPé€šè¿‡å®‰å…¨å‰ªææœºåˆ¶ï¼Œåœ¨ä¿è¯ä¸å½±å“æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹ï¼Œå‡å°‘äº†è®¡ç®—é‡å’Œå†…å­˜è®¿é—®ã€‚</li>
<li>ACPåœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­åº”ç”¨å¹¿æ³›ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ³¨æ„åŠ›è¿ç®—çš„FLOPså’Œå†…å­˜éœ€æ±‚ã€‚</li>
<li>ACPèƒ½æ˜¾è‘—æå‡æ³¨æ„åŠ›è¿è¡Œé€Ÿåº¦å’Œæ•´ä½“è®­ç»ƒæ•ˆç‡ï¼ŒåŠ é€Ÿæ¯”è¾¾åˆ°2-3å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-022fa25d6611cf296e26288a5cff5d84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12d00d1c6dc660123ea221f50cc7acbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccbe048a551a7c493e6d31bac074fd10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a55e21e98ce845c98491ffa46ffe168.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Triad-Empowering-LMM-based-Anomaly-Detection-with-Vision-Expert-guided-Visual-Tokenizer-and-Manufacturing-Process"><a href="#Triad-Empowering-LMM-based-Anomaly-Detection-with-Vision-Expert-guided-Visual-Tokenizer-and-Manufacturing-Process" class="headerlink" title="Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided   Visual Tokenizer and Manufacturing Process"></a>Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided   Visual Tokenizer and Manufacturing Process</h2><p><strong>Authors:Yuanze Li, Shihao Yuan, Haolin Wang, Qizhang Li, Ming Liu, Chen Xu, Guangming Shi, Wangmeng Zuo</strong></p>
<p>Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential anomalous areas identified by existing IAD models to the LMMs. On the other hand, existing methods mainly focus on identifying defects by learning defect patterns or comparing with normal samples, yet they fall short of understanding the causes of these defects. Considering that the generation of defects is closely related to the manufacturing process, we propose a manufacturing-driven IAD paradigm. An instruction-tuning dataset for IAD (InstructIAD) and a data organization approach for Chain-of-Thought with manufacturing (CoT-M) are designed to leverage the manufacturing process for IAD. Based on the above two modifications, we present Triad, a novel LMM-based method incorporating an expert-guided region-of-interest tokenizer and manufacturing process for industrial anomaly detection. Extensive experiments show that our Triad not only demonstrates competitive performance against current LMMs but also achieves further improved accuracy when equipped with manufacturing processes. Source code, training data, and pre-trained models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/tzjtatata/Triad">https://github.com/tzjtatata/Triad</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„æ–¹æ³•è¯•å›¾å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰ï¼Œä½†å®ƒä»¬åœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›è¿œè¿œè½åäºé€šç”¨ç›®çš„æ¨¡å‹ã€‚æˆ‘ä»¬æ€»ç»“äº†é€ æˆè¿™ç§å·®è·çš„ä¸»è¦åŸå› æœ‰ä¸¤ç‚¹ã€‚ä¸€æ–¹é¢ï¼Œé€šç”¨LMMç¼ºä¹å¯¹è§†è§‰æ¨¡æ€ç¼ºé™·çš„è®¤çŸ¥ï¼Œå› æ­¤æ— æ³•å……åˆ†å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ï¼Œå‘LMMæä¾›ç°æœ‰IADæ¨¡å‹è¯†åˆ«çš„æ½œåœ¨å¼‚å¸¸åŒºåŸŸã€‚å¦ä¸€æ–¹é¢ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾§é‡äºé€šè¿‡å­¦ä¹ ç¼ºé™·æ¨¡å¼æˆ–ä¸æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¯”è¾ƒæ¥è¯†åˆ«ç¼ºé™·ï¼Œä½†å®ƒä»¬ç¼ºä¹å¯¹ç¼ºé™·åŸå› çš„ç†è§£ã€‚è€ƒè™‘åˆ°ç¼ºé™·çš„äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥åˆ¶é€ ä¸ºæ ¸å¿ƒçš„IADèŒƒå¼ã€‚è®¾è®¡äº†ç”¨äºIADçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ˆInstructIADï¼‰å’Œç”¨äºä¸åˆ¶é€ ç›¸å…³çš„æ€ç»´é“¾æ•°æ®ç»„ç»‡æ–¹æ³•ï¼ˆCoT-Mï¼‰ï¼Œä»¥åˆ©ç”¨åˆ¶é€ è¿‡ç¨‹è¿›è¡ŒIADã€‚åŸºäºä¸Šè¿°ä¸¤é¡¹æ”¹è¿›ï¼Œæˆ‘ä»¬æå‡ºäº†Triadï¼Œè¿™æ˜¯ä¸€ç§åŸºäºLMMçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†ä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸæ ‡è®°å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Triadä¸ä»…åœ¨ä¸å½“å‰LMMçš„ç«äº‰ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨é…å¤‡åˆ¶é€ è¿‡ç¨‹åè¿˜å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æºä»£ç ã€è®­ç»ƒæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tzjtatata/Triad%E4%B8%8A%E5%BC%80%E6%94%BE%E6%9C%89%E3%80%82">https://github.com/tzjtatata/Triadä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13184v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸè™½å°è¯•å°†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¼•å…¥å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰ï¼Œä½†åœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä¸»è¦åŸå› ä¸ºä¸€èˆ¬ç”¨é€”çš„LMMså¯¹è§†è§‰ç¼ºé™·çš„è®¤çŸ¥ä¸è¶³ï¼Œæ— æ³•å……åˆ†å…³æ³¨ç¼ºé™·åŒºåŸŸã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ï¼Œå°†ç°æœ‰IADæ¨¡å‹è¯†åˆ«çš„æ½œåœ¨å¼‚å¸¸åŒºåŸŸæä¾›ç»™LMMsã€‚åŒæ—¶ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å­¦ä¹ ç¼ºé™·æ¨¡å¼æˆ–æ­£å¸¸æ ·æœ¬å¯¹æ¯”æ¥è¯†åˆ«ç¼ºé™·ï¼Œç¼ºä¹å¯¹ç¼ºé™·åŸå› çš„ç†è§£ã€‚è€ƒè™‘ç¼ºé™·çš„äº§ç”Ÿä¸åˆ¶é€ è¿‡ç¨‹å¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºåˆ¶é€ é©±åŠ¨å‹IADèŒƒå¼ã€‚è®¾è®¡äº†é¢å‘IADçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†InstructIADå’Œç»“åˆåˆ¶é€ çš„Chain-of-Thoughtæ•°æ®ç»„ç»‡æ–¹æ³•CoT-Mã€‚åŸºäºä¸Šè¿°ä¸¤é¡¹æ”¹è¿›ï¼Œæˆ‘ä»¬æå‡ºäº†Triadæ–°å‹LMMæ–¹æ³•ï¼Œç»“åˆä¸“å®¶å¼•å¯¼çš„å…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒTriadä¸ä»…è¡¨ç°å‡ºå¯¹ç°æœ‰çš„ç«äº‰åŠ›æ°´å¹³é«˜ä¸”å…·æœ‰ä¼˜å¼‚çš„å‡†ç¡®åº¦æå‡æ•ˆæœã€‚å¼€æºä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨æŒ‡å®šç½‘å€è·å–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LMMsåœ¨IADé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›æœ‰å¾…æé«˜ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹å¯¹è§†è§‰ç¼ºé™·çš„è®¤çŸ¥ã€‚</li>
<li>æå‡ºä¿®æ”¹LLaVAæ¨¡å‹çš„AnyResç»“æ„ä»¥æ•´åˆæ½œåœ¨å¼‚å¸¸åŒºåŸŸä¿¡æ¯ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨ç¼ºé™·è¯†åˆ«è€Œéç†è§£å…¶æˆå› ã€‚</li>
<li>è€ƒè™‘åˆ¶é€ è¿‡ç¨‹ä¸ç¼ºé™·ç”Ÿæˆçš„å…³ç³»ï¼Œæå‡ºåˆ¶é€ é©±åŠ¨å‹IADèŒƒå¼ã€‚</li>
<li>è®¾è®¡äº†InstructIADæ•°æ®é›†å’ŒCoT-Mæ•°æ®ç»„ç»‡æ–¹æ³•ç”¨äºæ”¹è¿›å·¥ä¸šå¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ã€‚</li>
<li>æå‡ºçš„Triadæ–¹æ³•ç»“åˆå…´è¶£åŒºåŸŸåˆ†è¯å™¨å’Œåˆ¶é€ è¿‡ç¨‹è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-297cadd4d9c42e28954509f2f920037b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bffdef29d6db095dcbae1ae3b91374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd343fb6c9f761ccc26c213544de57e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb6aa9ef17e2cdc844eeb10d10267cd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-295f4a881c763d6661a8e8d0b28dde27.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="X2I-Seamless-Integration-of-Multimodal-Understanding-into-Diffusion-Transformer-via-Attention-Distillation"><a href="#X2I-Seamless-Integration-of-Multimodal-Understanding-into-Diffusion-Transformer-via-Attention-Distillation" class="headerlink" title="X2I: Seamless Integration of Multimodal Understanding into Diffusion   Transformer via Attention Distillation"></a>X2I: Seamless Integration of Multimodal Understanding into Diffusion   Transformer via Attention Distillation</h2><p><strong>Authors:Jian Ma, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu, Zhenyu Yang</strong></p>
<p>Text-to-image (T2I) models are well known for their ability to produce highly realistic images, while multimodal large language models (MLLMs) are renowned for their proficiency in understanding and integrating multiple modalities. However, currently there is no straightforward and efficient framework to transfer the multimodal comprehension abilities of MLLMs to T2I models to enable them to understand multimodal inputs. In this paper, we propose the X2I framework, which endows Diffusion Transformer (DiT) models with the capability to comprehend various modalities, including multilingual text, screenshot documents, images, videos, and audio. X2I is trained using merely 100K English corpus with 160 GPU hours. Building on the DiT teacher model, we adopt an innovative distillation method to extract the inference capabilities of the teacher model and design a lightweight AlignNet structure to serve as an intermediate bridge. Compared to the teacher model, X2I shows a decrease in performance degradation of less than 1% while gaining various multimodal understanding abilities, including multilingual to image, image to image, image-text to image, video to image, audio to image, and utilizing creative fusion to enhance imagery. Furthermore, it is applicable for LoRA training in the context of image-text to image generation, filling a void in the industry in this area. We further design a simple LightControl to enhance the fidelity of instructional image editing. Finally, extensive experiments demonstrate the effectiveness, efficiency, multifunctionality, and transferability of our X2I. The open-source code and checkpoints for X2I can be found at the following link: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I">https://github.com/OPPO-Mente-Lab/X2I</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä»¥å…¶ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒçš„èƒ½åŠ›è€Œé—»åï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ™ä»¥å…¶åœ¨ç†è§£å’Œèåˆå¤šç§æ¨¡æ€æ–¹é¢çš„ä¸“é•¿è€Œè‘—ç§°ã€‚ç„¶è€Œï¼Œç›®å‰æ²¡æœ‰ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶èƒ½å°†MLLMçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›è½¬ç§»åˆ°T2Iæ¨¡å‹ä¸Šï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å’Œå¤„ç†å¤šæ¨¡æ€è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†X2Iæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èµ‹äºˆäº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹ç†è§£å„ç§æ¨¡æ€çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚X2Iä»…ä½¿ç”¨10ä¸‡ä¸ªè‹±æ–‡è¯­æ–™åº“å’Œ16ä¸‡ä¸ªGPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚åŸºäºDiTæ•™å¸ˆæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„è’¸é¦æ–¹æ³•ï¼Œæå–æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„AlignNetç»“æ„ä½œä¸ºä¸­é—´æ¡¥æ¢ã€‚ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒX2Içš„æ€§èƒ½ä¸‹é™å¹…åº¦ä¸åˆ°1%ï¼ŒåŒæ—¶è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾æ–‡åˆ°å›¾åƒã€è§†é¢‘åˆ°å›¾åƒã€éŸ³é¢‘åˆ°å›¾åƒçš„èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨åˆ›æ„èåˆå¢å¼ºå›¾åƒæ•ˆæœã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å›¾åƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„LoRAè®­ç»ƒæ–¹é¢è¡¨ç°å‡ºé€‚ç”¨æ€§ï¼Œå¡«è¡¥äº†è¡Œä¸šä¸­çš„ä¸€é¡¹ç©ºç™½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªç®€å•çš„LightControlï¼Œä»¥æé«˜æŒ‡ä»¤æ€§å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ã€‚æœ€åï¼Œå¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„X2Iæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€é«˜æ•ˆæ€§ã€å¤šåŠŸèƒ½æ€§å’Œå¯è¿ç§»æ€§ã€‚å…³äºX2Içš„å¼€æºä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I%E3%80%82">https://github.com/OPPO-Mente-Lab/X2Iã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06134v3">PDF</a> Accepted to ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºX2Içš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›èµ‹äºˆDiffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå¤šæ¨¡æ€è¾“å…¥ã€‚è¯¥æ¡†æ¶è®©DiTæ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚é€šè¿‡ä½¿ç”¨ä»…100Kçš„è‹±æ–‡è¯­æ–™åº“å’Œ160 GPUå°æ—¶çš„è®­ç»ƒï¼ŒX2Iå¾—ä»¥å®ç°ã€‚åŸºäºDiTæ•™å¸ˆæ¨¡å‹ï¼Œé‡‡ç”¨åˆ›æ–°è’¸é¦æ³•æå–å…¶æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡è½»é‡çº§çš„AlignNetç»“æ„ä½œä¸ºä¸­é—´æ¡¥æ¢ã€‚ç›¸è¾ƒäºæ•™å¸ˆæ¨¡å‹ï¼ŒX2Iæ€§èƒ½ä¸‹é™ä¸åˆ°1%ï¼ŒåŒæ—¶è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå®ƒé€‚ç”¨äºå›¾åƒæ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„LoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šç©ºç™½ã€‚ä¸ºæå‡æ•™å­¦å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ï¼Œè®¾è®¡äº†ä¸€ç§ç®€å•çš„LightControlã€‚å¤§é‡å®éªŒè¯æ˜äº†X2Içš„æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€å¤šåŠŸèƒ½æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>X2Iæ¡†æ¶æˆåŠŸå°†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›èµ‹äºˆDiffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹ã€‚</li>
<li>X2Ièƒ½å¤„ç†åŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘åœ¨å†…çš„å¤šç§æ¨¡æ€ã€‚</li>
<li>ä½¿ç”¨100Kçš„è‹±æ–‡è¯­æ–™åº“å’Œ160 GPUå°æ—¶çš„è®­ç»ƒå®ç°äº†X2Iã€‚</li>
<li>åˆ›æ–°è’¸é¦æ³•ç”¨äºæå–DiTæ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>X2Iæ€§èƒ½ä¸æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ä¸‹é™ä¸åˆ°1%ï¼ŒåŒæ—¶è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚</li>
<li>X2Ié€‚ç”¨äºå›¾åƒæ–‡æœ¬ç”Ÿæˆé¢†åŸŸçš„LoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šç©ºç™½ã€‚</li>
<li>ç®€å•çš„LightControlè®¾è®¡æå‡äº†æ•™å­¦å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dbb2c79329d8085abea7a1f30d6da9f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc88f560beeaa92946f220c4288b622a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e11e37d5745d44f3d2e6ea2a43306ff.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-02061d9c1f8f18efc09a0bf6d75be970.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  BrowseMaster Towards Scalable Web Browsing via Tool-Augmented   Programmatic Agent Pair
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5631d7ccebd390af9da0550d594e5497.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Prospect Theory Fails for LLMs Revealing Instability of Decision-Making   under Epistemic Uncertainty
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
