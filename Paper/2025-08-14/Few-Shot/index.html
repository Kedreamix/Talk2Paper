<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-14  Rational Inverse Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d068c5d39a98d6f86781e5908ab9af09.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    21 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-14-更新"><a href="#2025-08-14-更新" class="headerlink" title="2025-08-14 更新"></a>2025-08-14 更新</h1><h2 id="Rational-Inverse-Reasoning"><a href="#Rational-Inverse-Reasoning" class="headerlink" title="Rational Inverse Reasoning"></a>Rational Inverse Reasoning</h2><p><strong>Authors:Ben Zandonati, Tomás Lozano-Pérez, Leslie Pack Kaelbling</strong></p>
<p>Humans can observe a single, imperfect demonstration and immediately generalize to very different problem settings. Robots, in contrast, often require hundreds of examples and still struggle to generalize beyond the training conditions. We argue that this limitation arises from the inability to recover the latent explanations that underpin intelligent behavior, and that these explanations can take the form of structured programs consisting of high-level goals, sub-task decomposition, and execution constraints. In this work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring these latent programs through a hierarchical generative model of behavior. RIR frames few-shot imitation as Bayesian program induction: a vision-language model iteratively proposes structured symbolic task hypotheses, while a planner-in-the-loop inference scheme scores each by the likelihood of the observed demonstration under that hypothesis. This loop yields a posterior over concise, executable programs. We evaluate RIR on a suite of continuous manipulation tasks designed to test one-shot and few-shot generalization across variations in object pose, count, geometry, and layout. With as little as one demonstration, RIR infers the intended task structure and generalizes to novel settings, outperforming state-of-the-art vision-language model baselines. </p>
<blockquote>
<p>人类可以观察一个不完美的示范，并立即将其推广到截然不同的场景和问题设置中。相比之下，机器人通常需要数百个示例，仍然难以在训练条件之外进行推广。我们认为这种限制源于无法恢复支撑智能行为的潜在解释，这些解释可以采取结构化程序的形式，包括高级目标、子任务分解和执行约束。在这项工作中，我们引入了理性逆向推理（RIR），这是一个通过行为层次生成模型推断这些潜在程序的框架。RIR将少样本模仿作为贝叶斯程序归纳问题来解决：视觉语言模型会不断提出结构化符号任务假设，同时循环内规划推断方案会根据每个假设下的观测示范可能性对其进行评分。这一循环产生了一系列简洁、可执行程序的后验概率。我们在一套连续操作任务上评估了RIR，这些任务旨在测试物体姿态、数量、几何形状和布局变化中的一次性和少样本泛化能力。仅凭一次示范，RIR就能推断出预期的任务结构并推广到新的场景，超越了最先进的视觉语言模型基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了人类与机器人在观察与泛化能力上的差异。人类通过观察单一的、不完美的演示就能立即推广到不同的任务设置，而机器人通常需要大量的样本，仍然难以超越训练条件的泛化。文章提出，这种差异源于机器人无法获取潜在解释的能力，这些解释可以采取结构化程序的形式，包括高级目标、子任务分解和执行约束。本文介绍了一种名为理性逆向推理（RIR）的框架，该框架通过行为层次生成模型来推断这些潜在程序，并将少数镜头模仿视为贝叶斯程序归纳。RIR在连续操作任务上进行了评估，结果显示，在一镜头和少数镜头任务中，RIR能够推断出任务结构并推广到新的环境，优于现有的视觉语言模型基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类能从单一的演示中快速泛化到新任务，而机器人需要大量样本仍难以泛化。</li>
<li>这种差异源于机器人无法获取潜在解释的能力，这些解释是结构化程序的形式。</li>
<li>Rational Inverse Reasoning（RIR）框架用于推断潜在程序。</li>
<li>RIR采用贝叶斯程序归纳法来处理少数镜头模仿。</li>
<li>RIR通过层次生成模型来模拟行为。</li>
<li>RIR在一系列连续操作任务上的评估表现优越，能够从少数演示中泛化到新环境。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08983">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3330f903966fad5a8fe98bef595386af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6770922c1e1f45506095381db6ff9383.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f06616f9a5843e31a95b156acb1a8ae2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cda2c30ba04b593f68f2490a09c3394.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompt-Based-Approach-for-Czech-Sentiment-Analysis"><a href="#Prompt-Based-Approach-for-Czech-Sentiment-Analysis" class="headerlink" title="Prompt-Based Approach for Czech Sentiment Analysis"></a>Prompt-Based Approach for Czech Sentiment Analysis</h2><p><strong>Authors:Jakub Šmíd, Pavel Přibáň</strong></p>
<p>This paper introduces the first prompt-based methods for aspect-based sentiment analysis and sentiment classification in Czech. We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our prompt-based approach over traditional fine-tuning. In addition, we conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples compared to traditional fine-tuning. We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario. </p>
<blockquote>
<p>本文介绍了捷克语中基于提示的方面情感分析和情感分类的首批方法。我们采用序列到序列模型同时解决基于方面的任务，并证明我们的基于提示的方法优于传统的微调方法。此外，我们还进行了情感分类的零样本和少样本学习实验，并证明与传统的微调方法相比，提示方法在有限的训练样本下可以产生更好的结果。我们还证明，对目标领域数据进行预训练可以在零样本情况下产生重大改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08651v1">PDF</a> Published in Proceedings of the 14th International Conference on   Recent Advances in Natural Language Processing (RANLP 2023). Official   version: <a target="_blank" rel="noopener" href="https://aclanthology.org/2023.ranlp-1.118/">https://aclanthology.org/2023.ranlp-1.118/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于提示的方法，用于捷克语方面的情感分析和情感分类。采用序列到序列模型同时解决方面任务，展示基于提示的方法优于传统的微调技术。此外，进行了零样本和少样本学习实验，显示提示方法在有限训练样本情况下与传统微调相比具有显著优势。同时证明目标域数据的预训练可以在零样本场景下带来显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种基于提示的方法，用于解决捷克语中的面向方面情感分析和情感分类问题。</li>
<li>使用序列到序列模型实现同时处理面向方面的任务。</li>
<li>基于提示的方法展示了对传统微调技术的优越性。</li>
<li>通过少样本学习实验证明提示方法在有限训练样本下的表现优于传统微调。</li>
<li>预训练在目标域数据上能显著提高零样本场景下的表现。</li>
<li>这种方法对处理其他语言方面可能有借鉴意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08651">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8720b9742baba5f2b2fab3e0f92a2ecf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c9189e8dddd0254a2c5265bf4a22a6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfc9b3aa359d1e1693196d81c8b2de4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2b8393ba7232da86e9eb0954b72cd0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-409eed7fc40f2756c91ed09022f1513f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a83b5ef990128f259e534d954d09504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38d1c211ef1dbcd02ff34e5d21943836.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLaMA-Based-Models-for-Aspect-Based-Sentiment-Analysis"><a href="#LLaMA-Based-Models-for-Aspect-Based-Sentiment-Analysis" class="headerlink" title="LLaMA-Based Models for Aspect-Based Sentiment Analysis"></a>LLaMA-Based Models for Aspect-Based Sentiment Analysis</h2><p><strong>Authors:Jakub Šmíd, Pavel Přibáň, Pavel Král</strong></p>
<p>While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca~2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models. </p>
<blockquote>
<p>虽然大型语言模型（LLM）在各种任务中显示出潜力，但它们在进行基于复合方面的情感分析（ABSA）任务时的性能仍然落后于微调模型。然而，针对ABSA进行微调的LLM的潜力尚未被探索。本文旨在研究针对ABSA进行微调的开源LLM的能力，重点关注基于LLaMA的模型。我们在四个任务和八个英语数据集上评估了性能，发现经过调参的Orca ~ 2模型在所有任务中都超越了最先进的技术结果。然而，与完全微调过的模型相比，所有模型在零样本和少样本场景中表现困难。此外，我们还进行了误差分析，以确定微调模型所面临的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08649v1">PDF</a> Published in Proceedings of the 14th Workshop on Computational   Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis (WASSA 2024).   Official version: <a target="_blank" rel="noopener" href="https://aclanthology.org/2024.wassa-1.6/">https://aclanthology.org/2024.wassa-1.6/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在复合方面基础的情感分析（ABSA）任务上的表现尚待提升，但其潜力尚未被完全发掘。本文研究了针对ABSA任务进行微调后的开源LLMs的能力，特别是基于LLaMA的模型。通过四个任务和八个英语数据集的评估，发现经过调参的Orca~2模型在所有任务中都超越了现有技术。但在零样本和少样本场景下，所有模型的表现都不如完全调参的模型。此外，还进行了误差分析，以确定微调模型面临的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs在ABSA任务上的表现有待提高。</li>
<li>通过对LLMs进行微调，可以提高其在ABSA任务上的性能。</li>
<li>基于LLaMA的模型在ABSA任务中表现优异。</li>
<li>Orca~2模型在多个ABSA任务中超越现有技术。</li>
<li>在零样本和少样本场景下，微调模型的表现有所下降。</li>
<li>误差分析显示微调模型面临一些挑战。</li>
<li>对ABSA任务的深入研究有助于进一步提高LLMs的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08649">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-956bab2c559dc947e8784bd351c96127.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b7ddde047b0d992652dfeba0a5b644e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aebf8d5ddebe3557063260cf02e4df2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34de5bc83208c5e341d9ae43cf41ee8d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Steerable-Pluralism-Pluralistic-Alignment-via-Few-Shot-Comparative-Regression"><a href="#Steerable-Pluralism-Pluralistic-Alignment-via-Few-Shot-Comparative-Regression" class="headerlink" title="Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative   Regression"></a>Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative   Regression</h2><p><strong>Authors:Jadie Adams, Brian Hu, Emily Veenhuis, David Joy, Bharadwaj Ravichandran, Aaron Bray, Anthony Hoogs, Arslan Basharat</strong></p>
<p>Large language models (LLMs) are currently aligned using techniques such as reinforcement learning from human feedback (RLHF). However, these methods use scalar rewards that can only reflect user preferences on average. Pluralistic alignment instead seeks to capture diverse user preferences across a set of attributes, moving beyond just helpfulness and harmlessness. Toward this end, we propose a steerable pluralistic model based on few-shot comparative regression that can adapt to individual user preferences. Our approach leverages in-context learning and reasoning, grounded in a set of fine-grained attributes, to compare response options and make aligned choices. To evaluate our algorithm, we also propose two new steerable pluralistic benchmarks by adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets, demonstrating the applicability of our approach to value-aligned decision-making and reward modeling, respectively. Our few-shot comparative regression approach is interpretable and compatible with different attributes and LLMs, while outperforming multiple baseline and state-of-the-art methods. Our work provides new insights and research directions in pluralistic alignment, enabling a more fair and representative use of LLMs and advancing the state-of-the-art in ethical AI. </p>
<blockquote>
<p>大型语言模型（LLM）目前使用强化学习从人类反馈（RLHF）等技术进行对齐。然而，这些方法只能反映用户的平均偏好，使用标量奖励。而多元对齐则试图捕捉跨一系列属性的不同用户偏好，超越仅仅的有用性和无害性。为此，我们提出了一种基于少量比较回归的可转向多元模型，可以适应个别用户偏好。我们的方法利用上下文学习和推理，基于一组精细的颗粒属性，来比较响应选项并做出对齐选择。为了评估我们的算法，我们还通过适应道德完整语料库（MIC）和HelpSteer2数据集，提出了两个新的可转向多元基准测试，分别展示了我们的方法在价值对齐决策和奖励建模方面的适用性。我们的少量比较回归方法是可解释的，与不同的属性和LLM兼容，同时优于多个基准和最新方法。我们的工作提供了关于多元对齐的新见解和研究方向，使LLM的使用更加公平和代表性，并推动了伦理人工智能的最新进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08509v1">PDF</a> AIES ‘25: Proceedings of the 2025 AAAI&#x2F;ACM Conference on AI, Ethics,   and Society</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）的对齐问题，介绍了使用强化学习从人类反馈（RLHF）等技术的方法。然而，这些方法只能反映平均用户偏好，无法实现多元化的对齐。为此，提出了基于少样本比较回归的可控多元化模型，可以适应个人用户偏好。该方法利用语境学习和推理，在一组精细属性基础上比较响应选项并做出对齐选择。通过适应道德完整性语料库（MIC）和帮助Steer2数据集，提出了两个新的可控多元化基准测试，展示了该方法在价值对齐决策和奖励建模方面的适用性。少样本比较回归方法具有解释性，可与不同属性和LLM兼容，并优于多个基准和最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）目前使用强化学习从人类反馈（RLHF）等技术进行对齐，但只能反映平均用户偏好。</li>
<li>提倡使用Pluralistic Alignment来捕捉用户在不同属性上的多样化偏好。</li>
<li>提出了一种基于少样本比较回归的可控多元化模型，该模型能适应个人用户偏好。</li>
<li>方法利用语境学习和推理，在一组精细属性基础上比较响应选项并做出决策。</li>
<li>通过适应道德完整性语料库和帮助Steer2数据集，提出了两个新的基准测试来评估算法。</li>
<li>该少样本比较回归方法具有解释性，与不同属性和LLM兼容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08509">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a03675840e18429f6fc60ead5a346540.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d068c5d39a98d6f86781e5908ab9af09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833e49aeaf0daf63fcc3936cf8206d6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d6aeeb4358735dcf743915ae968c588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410cd24ba7d501d7cadf7c6503202753.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CObL-Toward-Zero-Shot-Ordinal-Layering-without-User-Prompting"><a href="#CObL-Toward-Zero-Shot-Ordinal-Layering-without-User-Prompting" class="headerlink" title="CObL: Toward Zero-Shot Ordinal Layering without User Prompting"></a>CObL: Toward Zero-Shot Ordinal Layering without User Prompting</h2><p><strong>Authors:Aneel Damaraju, Dean Hazineh, Todd Zickler</strong></p>
<p>Vision benefits from grouping pixels into objects and understanding their spatial relationships, both laterally and in depth. We capture this with a scene representation comprising an occlusion-ordered stack of “object layers,” each containing an isolated and amodally-completed object. To infer this representation from an image, we introduce a diffusion-based architecture named Concurrent Object Layers (CObL). CObL generates a stack of object layers in parallel, using Stable Diffusion as a prior for natural objects and inference-time guidance to ensure the inferred layers composite back to the input image. We train CObL using a few thousand synthetically-generated images of multi-object tabletop scenes, and we find that it zero-shot generalizes to photographs of real-world tabletops with varying numbers of novel objects. In contrast to recent models for amodal object completion, CObL reconstructs multiple occluded objects without user prompting and without knowing the number of objects beforehand. Unlike previous models for unsupervised object-centric representation learning, CObL is not limited to the world it was trained in. </p>
<blockquote>
<p>视觉受益于将像素分组为对象并理解它们之间的空间关系，包括横向和深度方向上的关系。我们通过场景表示，使用一个遮挡有序堆叠的“对象层”来捕捉这一点，每层包含一个孤立且模态完成的对象。要从图像推断这种表示，我们引入了一种基于扩散的架构，名为并发对象层（CObL）。CObL并行生成对象层堆栈，以Stable Diffusion作为自然物体的先验，并在推理时间提供指导，以确保推断的层可以合成回到输入图像。我们使用几千张合成生成的多对象桌面场景图像来训练CObL，并发现它对真实世界桌面照片进行零样本推广，这些照片中含有数量不等的新颖物体。与最近的模态对象完成模型相比，CObL能够重建多个被遮挡物体，而无需用户提示，并且不知道事先存在的物体数量。与以前的无监督对象中心表示学习模型不同，CObL不受其训练世界的限制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08498v1">PDF</a> ICCV 2025: Project page with demo, datasets, and code:   <a target="_blank" rel="noopener" href="https://vision.seas.harvard.edu/cobl/">https://vision.seas.harvard.edu/cobl/</a></p>
<p><strong>Summary</strong><br>     本文提出一种名为Concurrent Object Layers（CObL）的架构，用于从图像中推断场景表示。该架构通过并行生成包含孤立且无模态完成对象的“对象层”堆栈来捕捉像素分组为对象及其空间关系。使用合成图像训练CObL，并发现其对真实世界桌面照片具有零样本泛化能力，可重建多个被遮挡对象，无需用户提示和预先知道对象数量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CObL架构用于通过对象层堆栈表示场景，理解像素分组为对象及其空间关系。</li>
<li>CObL使用稳定扩散作为自然对象的先验，并在推理时间提供指导以确保推断的层能够组合回输入图像。</li>
<li>CObL通过合成图像进行训练，具有零样本泛化能力，适用于真实世界桌面照片。</li>
<li>CObL能够重建多个被遮挡的对象，无需用户提示。</li>
<li>CObL不需要预先知道对象的数量。</li>
<li>CObL在对象层堆栈中表示场景的方式有助于理解对象的深度关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08498">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6b386bd3d541b24eb07277c19811866a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1278d6bcd1508290d887ca59cf6d6913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d47195fed08f0e807203194015865cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e20e1ff62cbaa03b4560e25216d10f2f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multiple-Stochastic-Prompt-Tuning-for-Few-shot-Adaptation-under-Extreme-Domain-Shift"><a href="#Multiple-Stochastic-Prompt-Tuning-for-Few-shot-Adaptation-under-Extreme-Domain-Shift" class="headerlink" title="Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme   Domain Shift"></a>Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme   Domain Shift</h2><p><strong>Authors:Debarshi Brahma, Soma Biswas</strong></p>
<p>Foundation Vision-Language Models (VLMs) like CLIP exhibit strong generalization capabilities due to large-scale pretraining on diverse image-text pairs. However, their performance often degrades when applied to target datasets with significant distribution shifts in both visual appearance and class semantics. Recent few-shot learning approaches adapt CLIP to downstream tasks using limited labeled data via adapter or prompt tuning, but are not specifically designed to handle such extreme domain shifts. Conversely, some works addressing cross-domain few-shot learning consider such domain-shifted scenarios but operate in an episodic setting with only a few classes per episode, limiting their applicability to real-world deployment, where all classes must be handled simultaneously. To address this gap, we propose a novel framework, MIST (Multiple Stochastic Prompt Tuning), for efficiently adapting CLIP to datasets with extreme distribution shifts using only a few labeled examples, in scenarios involving all classes at once. Specifically, we introduce multiple learnable prompts per class to effectively capture diverse modes in visual representations arising from distribution shifts. To further enhance generalization, these prompts are modeled as learnable Gaussian distributions, enabling efficient exploration of the prompt parameter space and reducing overfitting caused by limited supervision. Extensive experiments and comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed framework. </p>
<blockquote>
<p>视觉语言模型（如CLIP）由于在各种图像文本对上进行了大规模预训练，因此表现出强大的泛化能力。然而，当应用于目标数据集时，如果数据集的视觉外观和类别语义出现显著分布偏移，其性能往往会下降。最近的 few-shot 学习方法通过使用适配器或提示调整将CLIP适应于下游任务，仅使用有限的有标签数据，但并没有专门设计来处理这种极端的域偏移。相反，一些解决跨域 few-shot 学习的作品考虑了这种域偏移场景，但它们只在每一集只有少数类别的情境下操作，限制了它们在现实世界部署中的应用，因为必须同时处理所有类别。为了弥补这一差距，我们提出了一种新的框架 MIST（多重随机提示调整），该框架可以有效地使用仅包含少量有标签示例的数据集来适应具有极端分布偏移的场景，并且同时涉及所有类别。具体来说，我们引入了每个类别的多个可学习提示，以有效地捕获由分布偏移引起的视觉表示中的不同模式。为了进一步增强泛化能力，我们将这些提示建模为可学习的高斯分布，能够高效探索提示参数空间，并减少因有限监督而导致的过拟合。大量实验与最新方法的比较表明，该框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03926v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型预训练的Vision-Language模型（如CLIP）在多样化图像文本对上表现出强大的泛化能力，但在面对视觉外观和类别语义显著变化的目标数据集时，其性能往往会下降。针对这一问题，研究提出了MIST框架，通过引入多个可学习的类别提示和模拟为高斯分布，来有效适应数据集分布的变化并增强泛化能力。该框架利用有限的标签样本快速适应极端分布变化的场景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Language模型（如CLIP）在大型预训练后具有强大的泛化能力，但在面对视觉外观和类别语义显著变化的目标数据集时性能会下降。</li>
<li>近来的少样本学习方法通过适配器或提示调整将CLIP应用于下游任务，但并未特别设计以处理极端的领域偏移。</li>
<li>MIST框架通过引入多个可学习的类别提示来适应数据集分布的变化，并有效地处理极端领域偏移问题。</li>
<li>MIST框架中的提示被建模为可学习的高斯分布，有助于高效探索提示参数空间并减少因有限监督而导致的过拟合。</li>
<li>MIST框架能在面对所有类别的场景中仅使用少量标签样本进行高效适应。</li>
<li>实验和与最新方法的比较证明了MIST框架的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f1bd49e72d953c181b1f34beb2891432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5f8119e8029415de92ad65faf2ac192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68500b8fb8959b3ee40599e77c730e25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bf719442a5b967b61caa7b1838958cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fa0a82c1a91b5816165f3e09150a3f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a32995cec81ca7989fc4bfe969882801.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-255a2dba8e7d10e05250b777e372066d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-14  Gotta Hear Them All Towards Sound Source Aware Audio Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cea728d3a479c58f3f0f0c06b9537c84.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-08-14  Style transfer between Microscopy and Magnetic Resonance Imaging via   Generative Adversarial Network in small sample size settings
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
