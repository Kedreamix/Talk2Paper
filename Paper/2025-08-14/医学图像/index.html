<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Uncertainty-aware Cross-training for Semi-supervised Medical Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-1bca56415dbad4ad884e53a6c73268e7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-14-æ›´æ–°"><a href="#2025-08-14-æ›´æ–°" class="headerlink" title="2025-08-14 æ›´æ–°"></a>2025-08-14 æ›´æ–°</h1><h2 id="Uncertainty-aware-Cross-training-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Uncertainty-aware-Cross-training-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Uncertainty-aware Cross-training for Semi-supervised Medical Image   Segmentation"></a>Uncertainty-aware Cross-training for Semi-supervised Medical Image   Segmentation</h2><p><strong>Authors:Kaiwen Huang, Tao Zhou, Huazhu Fu, Yizhe Zhang, Yi Zhou, Xiao-Jun Wu</strong></p>
<p>Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code will be released at <a target="_blank" rel="noopener" href="https://github.com/taozh2017/UCSeg">https://github.com/taozh2017/UCSeg</a>. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å› å…¶å‡å°‘ä¾èµ–ä¸“å®¶æ ‡æ³¨çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä¸€äº›åŸºäºå‡å€¼æ•™å¸ˆï¼ˆMTï¼‰çš„åŠç›‘ç£æ–¹æ³•åˆ©ç”¨ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¥æœ‰æ•ˆæŒ–æ˜æœªæ ‡æ³¨æ•°æ®ä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¸¥é‡ä¾èµ–äºå­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶å¿½ç•¥äº†æ¨¡å‹å†…è®¤çŸ¥åå·®çš„æ½œåœ¨å½±å“ã€‚æ­¤å¤–ï¼Œä¸€äº›æ–¹æ³•ä½¿ç”¨ä»ä¸åŒè¾“å…¥ä¸­æ´¾ç”Ÿå‡ºçš„ä¼ªæ ‡ç­¾è¿›è¡ŒååŒè®­ç»ƒï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»å—æ‰°è¾“å…¥ç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥äº¤å‰è®­ç»ƒæ¡†æ¶ï¼ˆUC-Segï¼‰ã€‚æˆ‘ä»¬çš„UC-Segæ¡†æ¶ç»“åˆäº†ä¸¤ä¸ªç‹¬ç«‹çš„å­ç½‘ï¼Œä»¥æœ‰æ•ˆåœ°æ¢ç´¢å’Œåˆ©ç”¨å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œå‡è½»æ¨¡å‹å†…çš„è®¤çŸ¥åå·®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨å­ç½‘ä¸€è‡´æ€§ä¿ç•™ï¼ˆCCPï¼‰ç­–ç•¥ï¼Œä»¥æé«˜ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶ç¡®ä¿ä¸¤ä¸ªå­ç½‘ä¹‹é—´çš„ç‰¹å¾ä¸€è‡´æ€§ã€‚è¯¥ç­–ç•¥ä½¿æ¯ä¸ªå­ç½‘èƒ½å¤Ÿçº æ­£è‡ªå·±çš„åå·®ï¼Œå¹¶ä»æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ å…±äº«è¯­ä¹‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç”Ÿæˆï¼ˆUPGï¼‰ç»„ä»¶ï¼Œå®ƒåˆ©ç”¨ä¸¤ä¸ªå­ç½‘çš„åˆ†å‰²ç»“æœå’Œç›¸åº”çš„ä¸ç¡®å®šæ€§å›¾æ¥ç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠä¸åŒæ¨¡æ€å›¾åƒï¼ˆå¦‚MRIã€CTã€è¶…å£°ã€ç»“è‚ é•œæ£€æŸ¥ç­‰ï¼‰çš„å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šå¹¿æ³›è¯„ä¼°äº†æ‰€æå‡ºçš„UC-Segã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–æœ€å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´é«˜çš„åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/taozh2017/UCSeg">https://github.com/taozh2017/UCSeg</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09014v1">PDF</a> 14 pages, 10 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ¡†æ¶â€”â€”ä¸ç¡®å®šæ€§æ„ŸçŸ¥äº¤å‰è®­ç»ƒæ¡†æ¶ï¼ˆUC-Segï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªç‹¬ç«‹å­ç½‘ç»œè¿›è¡Œæ¢ç´¢ä¸äº¤äº’ï¼Œé™ä½æ¨¡å‹å†…è®¤çŸ¥åè§çš„å½±å“ã€‚æ–‡ä¸­æå‡ºäº¤å‰å­ç½‘ä¸€è‡´æ€§ä¿æŠ¤ç­–ç•¥ï¼ˆCCPï¼‰ï¼Œæé«˜ç‰¹å¾è¡¨å¾èƒ½åŠ›å¹¶ç¡®ä¿è·¨å­ç½‘çš„ç‰¹å¾ä¸€è‡´æ€§ï¼Œä½¿æ¯ä¸ªå­ç½‘èƒ½çº æ­£è‡ªèº«åè§å¹¶ä»æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ å…±äº«è¯­ä¹‰ã€‚åŒæ—¶ï¼Œåˆ©ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¼ªæ ‡ç­¾ç”Ÿæˆï¼ˆUPGï¼‰ç»„ä»¶ï¼Œç»“åˆä¸¤ä¸ªå­ç½‘çš„åˆ†å‰²ç»“æœå’Œç›¸åº”çš„ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œç”Ÿæˆé«˜ä¿¡å¿ƒä¼ªæ ‡ç­¾ã€‚åœ¨æ¶‰åŠMRIã€CTã€è¶…å£°ã€ç»“è‚ é•œç­‰ä¸åŒæ¨¡æ€å›¾åƒçš„åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šï¼ŒUC-Segè¡¨ç°å‡ºå“è¶Šçš„åˆ†å‰²ç²¾åº¦å’Œæ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œèƒ½å¤Ÿå‡å°‘ä¸“å®¶æ ‡æ³¨çš„ä¾èµ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–å­¦ç”Ÿæ¨¡å‹ï¼Œå¿½è§†æ¨¡å‹å†…è®¤çŸ¥åè§çš„å½±å“ã€‚</li>
<li>æœ¬æ–‡æå‡ºUC-Segæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªç‹¬ç«‹å­ç½‘ï¼Œæå‡ç‰¹å¾è¡¨å¾èƒ½åŠ›å¹¶é™ä½æ¨¡å‹åè§ã€‚</li>
<li>å¼•å…¥CCPç­–ç•¥ç¡®ä¿è·¨å­ç½‘çš„ç‰¹å¾ä¸€è‡´æ€§ï¼Œæå‡åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>UPGç»„ä»¶ç»“åˆä¸¤ä¸ªå­ç½‘çš„åˆ†å‰²ç»“æœå’Œä¸ç¡®å®šæ€§åœ°å›¾ï¼Œç”Ÿæˆé«˜ä¿¡å¿ƒä¼ªæ ‡ç­¾ã€‚</li>
<li>UC-Segåœ¨ä¸åŒæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29ea91d76b9bc93d5d5c1199eddd5a0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98e238016ab0bbf8b5ae7c8395dbaaef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8a8df02a0fa04bbe541a7dc590dc82c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-398e0b5c65aa2a6e655aba3c7d7af591.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-doping-evolution-of-the-charge-density-wave-and-charge-density-fluctuations-in-La-2-x-Sr-x-CuO-4"><a href="#The-doping-evolution-of-the-charge-density-wave-and-charge-density-fluctuations-in-La-2-x-Sr-x-CuO-4" class="headerlink" title="The doping evolution of the charge density wave and charge density   fluctuations in La$_{2-x}$Sr$_x$CuO$_4$"></a>The doping evolution of the charge density wave and charge density   fluctuations in La$_{2-x}$Sr$_x$CuO$_4$</h2><p><strong>Authors:Charles C. Tam, Mengze Zhu, Maud C. BarthÃ©lemy, Lauren J. Cane, Oliver J. Lipscombe, Stefano Agrestini, Jaewon Choi, Mirian Garcia-Fernandez, Ke-Jin Zhou, Stephen M. Hayden</strong></p>
<p>Cuprate superconductors show various collective charge correlations that are intimately connected with their electronic properties. In particular, charge order in the form of an incommensurate charge density wave (CDW) order with an in-plane wavevector $\delta_{\text{CDW}} \approx $ 0.23â€“0.35~r.l.u. appears to be universally present. In addition to CDW, dynamic charge density fluctuations (CDF) are also present with wavevectors comparable to $\delta_{\text{CDW}}$. CDFs are present up to $\sim300;$K and have relatively short correlation lengths of $\xi \sim 20$;\AA. Here we use Cu-$L_3$ and O-$K$ resonant inelastic X-ray scattering (RIXS) to study the doping dependence of CDW and CDFs in La$<em>{2-x}$Sr$<em>x$CuO$<em>4$. We fit our data with (quasi)elastic peaks resulting from the CDW and up to four inelastic modes associated with oxygen phonons that can be strongly coupled to the CDFs. Our analysis allows us to separate the charge correlations into three components: the CDW with wavevector $\delta</em>{4a-\text{CDW}} \approx 0.24$ and two CDF components with $\delta</em>{4a-\text{CDF}} \approx 0.24$ and $\delta</em>{3a-\text{CDF}} \approx 0.30$. We find that for $T \approx T_c$ the CDW coexists with the CDFs for dopings near $x&#x3D;p \sim 1&#x2F;8$. The $4a$-CDW disappears beyond $x&#x3D;0.16$ and the $4a$-CDF beyond $x&#x3D;0.19$, leaving only a weak $3a$-CDF at the highest doping studied, $x&#x3D;0.22$. Our data suggest that low-energy charge fluctuations exist up to doping $x&#x3D;0.19&#x3D;p^{\star}$, where the pseudogap disappears, however, we find no evidence that they are associated with a quantum critical point. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œç ”ç©¶äº†é“œé…¸ç›è¶…å¯¼ä½“ä¸­çš„é›†ä½“ç”µè·å…³è”ä¸ç”µå­ç‰¹æ€§ä¹‹é—´çš„è”ç³»ã€‚ç‰¹åˆ«åœ°ï¼Œä»¥éå…¬åº¦ç”µè·å¯†åº¦æ³¢ï¼ˆCDWï¼‰çš„å½¢å¼å‡ºç°çš„ç”µè·åºä¼¼ä¹æ™®éå­˜åœ¨ï¼Œå…¶å¹³é¢æ³¢çŸ¢é‡çº¦ä¸º$\delta_{\text{CDW}} \approx 0.23â€“0.35$ç›¸å¯¹æ™¶æ ¼å•ä½ã€‚é™¤äº†CDWä¹‹å¤–ï¼Œè¿˜å­˜åœ¨åŠ¨æ€ç”µè·å¯†åº¦æ³¢åŠ¨ï¼ˆCDFï¼‰ï¼Œå…¶æ³¢çŸ¢é‡ä¸$\delta_{\text{CDW}}$ç›¸å½“ã€‚CDFå­˜åœ¨äºé«˜è¾¾$\sim300$Kçš„æ¸©åº¦ä¸‹ï¼Œå¹¶ä¸”å…·æœ‰ç›¸å¯¹è¾ƒçŸ­çš„ç›¸å¹²é•¿åº¦$\xi \sim 20$;\AAã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨Cu-$L_3$å’ŒO-$K$å…±æŒ¯éå¼¹æ€§Xå°„çº¿æ•£å°„ï¼ˆRIXSï¼‰æ¥ç ”ç©¶La$<em>{2-x}$Sr$<em>x$CuO$<em>4$ä¸­CDWå’ŒCDFçš„æºæ‚ä¾èµ–æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®æ‹Ÿåˆäº†ç”±CDWäº§ç”Ÿçš„ï¼ˆå‡†ï¼‰å¼¹æ€§å³°ä»¥åŠä¸æ°§å£°å­ç›¸å…³çš„å››ä¸ªéå¼¹æ€§æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯ä»¥ä¸CDFå¼ºçƒˆè€¦åˆã€‚æˆ‘ä»¬çš„åˆ†æä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†ç”µè·å…³è”åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼šå…·æœ‰æ³¢çŸ¢$\delta</em>{4a-\text{CDW}} \approx 0.24$çš„CDWä»¥åŠä¸¤ä¸ªå…·æœ‰$\delta</em>{4a-\text{CDF}} \approx 0.24$å’Œ$\delta</em>{3a-\text{CDF}} \approx 0.30$çš„CDFæˆåˆ†ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æ¥è¿‘ä¸´ç•Œæ¸©åº¦$T_c$æ—¶ï¼Œå¯¹äºæºæ‚æ¥è¿‘$x&#x3D;p \sim 1&#x2F;8$çš„æƒ…å†µï¼ŒCDWä¸CDFå…±å­˜ã€‚å½“æºæ‚è¶…è¿‡$x&#x3D;0.16$æ—¶ï¼Œ$4a$-CDWæ¶ˆå¤±ï¼Œå½“æºæ‚è¶…è¿‡$x&#x3D;0.19$æ—¶ï¼Œ$4a$-CDFæ¶ˆå¤±ï¼Œåªç•™ä¸‹æœ€é«˜æºæ‚æ°´å¹³ä¸‹çš„å¾®å¼±$3a$-CDFã€‚æˆ‘ä»¬çš„æ•°æ®è¡¨æ˜ï¼Œä½èƒ½ç”µè·æ³¢åŠ¨å­˜åœ¨äºæºæ‚åˆ°$x&#x3D;0.19&#x3D;p^{\star}$çš„æƒ…å†µä¸­ï¼Œåœ¨é‚£é‡Œä¼ªé—´éš™æ¶ˆå¤±ï¼Œç„¶è€Œæˆ‘ä»¬æ²¡æœ‰å‘ç°å®ƒä»¬ä¸é‡å­ä¸´ç•Œç‚¹ç›¸å…³çš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08885v1">PDF</a> </p>
<p><strong>Summary</strong><br>    è¯¥ç ”ç©¶åˆ©ç”¨Cu-L3å’ŒO-Kå…±æŒ¯éå¼¹æ€§Xå°„çº¿æ•£å°„ï¼ˆRIXSï¼‰ç ”ç©¶La2-xSrxCuO4ä¸­ç”µè·å¯†åº¦æ³¢ï¼ˆCDWï¼‰å’Œç”µè·å¯†åº¦æ¶¨è½ï¼ˆCDFï¼‰çš„æºæ‚ä¾èµ–æ€§ã€‚åˆ†æå‘ç°CDWå’ŒCDFåœ¨æ¥è¿‘Tcæ¸©åº¦æ—¶å…±å­˜ï¼Œä¸”æºæ‚é‡æ¥è¿‘x&#x3D;p~1&#x2F;8æ—¶å°¤ä¸ºæ˜æ˜¾ã€‚CDFç»„åˆ†éšæºæ‚é‡çš„å˜åŒ–è€Œæ¶ˆå¤±ï¼Œä½†æ²¡æœ‰è¯æ®è¡¨æ˜å®ƒä»¬ä¸é‡å­ä¸´ç•Œç‚¹æœ‰å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cuprate superconductorså±•ç°å‡ºé›†ä½“ç”µè·å…³è”ï¼Œä¸ç”µå­å±æ€§ç´§å¯†ç›¸å…³ã€‚</li>
<li>å­˜åœ¨ä¸€ç§ç§°ä¸ºç”µè·å¯†åº¦æ³¢ï¼ˆCDWï¼‰çš„ç°è±¡ï¼Œä¼¼ä¹æ™®éå­˜åœ¨ã€‚</li>
<li>é™¤äº†CDWï¼Œè¿˜å­˜åœ¨åŠ¨æ€ç”µè·å¯†åº¦æ¶¨è½ï¼ˆCDFï¼‰ï¼Œå…¶æ³¢çŸ¢ä¸CDWç›¸å½“ã€‚</li>
<li>ä½¿ç”¨Cu-L3å’ŒO-Kå…±æŒ¯éå¼¹æ€§Xå°„çº¿æ•£å°„ï¼ˆRIXSï¼‰ç ”ç©¶La2-xSrxCuO4ä¸­çš„CDWå’ŒCDFçš„æºæ‚ä¾èµ–æ€§ã€‚</li>
<li>CDWå’ŒCDFåœ¨æ¥è¿‘ä¸´ç•Œæ¸©åº¦Tcæ—¶å…±å­˜ï¼Œç‰¹å®šæºæ‚æµ“åº¦ä¸‹å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>CDFç»„åˆ†éšæºæ‚æµ“åº¦çš„å˜åŒ–è€Œæ¶ˆå¤±ã€‚</li>
<li>æ²¡æœ‰è¯æ®è¡¨æ˜CDFä¸é‡å­ä¸´ç•Œç‚¹æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cde74e59de7830bc62211825f84505fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42aae058d74170e89cbbb40ae675b8ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05da78eeccc77ecda88eaa1b0b372155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0814c0042ef5f956d5fd5ec7b075213f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1337131ca563724554e53dd06936079.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed01339aaea8d12c1b3f8de312018082.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Patient-Adaptive-Focused-Transmit-Beamforming-using-Cognitive-Ultrasound"><a href="#Patient-Adaptive-Focused-Transmit-Beamforming-using-Cognitive-Ultrasound" class="headerlink" title="Patient-Adaptive Focused Transmit Beamforming using Cognitive Ultrasound"></a>Patient-Adaptive Focused Transmit Beamforming using Cognitive Ultrasound</h2><p><strong>Authors:Wessel L. van Nierop, OisÃ­n Nolan, Tristan S. W. Stevens, Ruud J. G. van Sloun</strong></p>
<p>Focused transmit beamforming is the most commonly used acquisition scheme for echocardiograms, but suffers from relatively low frame rates, and in 3D, even lower volume rates. Fast imaging based on unfocused transmits has disadvantages such as motion decorrelation and limited harmonic imaging capabilities. This work introduces a patient-adaptive focused transmit scheme that has the ability to drastically reduce the number of transmits needed to produce a high-quality ultrasound image. The method relies on posterior sampling with a temporal diffusion model to perceive and reconstruct the anatomy based on partial observations, while subsequently taking an action to acquire the most informative transmits. This active perception modality outperforms random and equispaced subsampling on the 2D EchoNet-Dynamic dataset and a 3D Philips dataset, where we actively select focused elevation planes. Furthermore, we show it achieves better performance in terms of generalized contrast-to-noise ratio when compared to the same number of diverging waves transmits on three in-house echocardiograms. Additionally, we can estimate ejection fraction using only 2% of the total transmits and show that the method is robust to outlier patients. Finally, our method can be run in real-time on GPU accelerators from 2023. The code is publicly available at <a target="_blank" rel="noopener" href="https://tue-bmd.github.io/ulsa/">https://tue-bmd.github.io/ulsa/</a> </p>
<blockquote>
<p>èšç„¦å‘å°„æ³¢æŸå½¢æˆæ˜¯æœ€å¸¸ç”¨çš„è¶…å£°å¿ƒåŠ¨å›¾é‡‡é›†æ–¹æ¡ˆï¼Œä½†å…¶å¸§ç‡ç›¸å¯¹è¾ƒä½ï¼Œåœ¨3Dä¸­ä½“ç§¯ç‡æ›´ä½ã€‚åŸºäºéèšç„¦å‘å°„çš„å¿«é€Ÿæˆåƒå­˜åœ¨è¿åŠ¨å»ç›¸å…³å’Œè°æ³¢æˆåƒèƒ½åŠ›æœ‰é™ç­‰ç¼ºç‚¹ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§ç—…äººè‡ªé€‚åº”çš„èšç„¦å‘å°„æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¤§å¤§å‡å°‘äº§ç”Ÿé«˜è´¨é‡è¶…å£°å›¾åƒæ‰€éœ€è¦çš„å‘å°„æ¬¡æ•°ã€‚è¯¥æ–¹æ³•ä¾èµ–äºåé‡‡æ ·å’Œæ—¶åºæ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®éƒ¨åˆ†è§‚å¯Ÿæ¥æ„ŸçŸ¥å’Œé‡å»ºç»“æ„ï¼Œç„¶åé‡‡å–è¡ŒåŠ¨è·å–æœ€å…·ä¿¡æ¯é‡çš„å‘å°„ã€‚è¿™ç§ä¸»åŠ¨æ„ŸçŸ¥æ¨¡å¼åœ¨2D EchoNet-Dynamicæ•°æ®é›†å’Œ3Dé£åˆ©æµ¦æ•°æ®é›†ä¸Šä¼˜äºéšæœºå’Œç­‰è·å­é‡‡æ ·ï¼Œæˆ‘ä»¬ä¸»åŠ¨é€‰æ‹©èšç„¦çš„å‡é™å¹³é¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºå…¶åœ¨ä¸ä¸‰ç§å†…éƒ¨è¶…å£°å¿ƒåŠ¨å›¾çš„ç›¸åŒå‘æ•£æ³¢å‘å°„æ•°é‡ç›¸æ¯”æ—¶ï¼Œåœ¨å¹¿ä¹‰å¯¹æ¯”åº¦å™ªå£°æ¯”æ–¹é¢è¡¨ç°æ›´å¥½ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨æ€»å‘å°„æ¬¡æ•°çš„2%æ¥ä¼°è®¡å°„è¡€åˆ†æ•°ï¼Œå¹¶è¯æ˜è¯¥æ–¹æ³•å¯¹äºå¼‚å¸¸æ‚£è€…å…·æœ‰ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨2023å¹´çš„GPUåŠ é€Ÿå™¨ä¸Šå®æ—¶è¿è¡Œã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://tue-bmd.github.io/ulsa/">https://tue-bmd.github.io/ulsa/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªé€‚åº”ä¼ è¾“çš„è¶…å£°æˆåƒæŠ€æœ¯èƒ½æœ‰æ•ˆå‡å°‘é«˜è´¨é‡è¶…å£°å›¾åƒæ‰€éœ€çš„ä¼ è¾“æ¬¡æ•°ã€‚è¯¥æ–¹æ³•ç»“åˆåé‡‡æ ·å’Œæ—¶åºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½é‡å»ºå™¨å®˜ç»“æ„ï¼ŒåŒæ—¶é€‰æ‹©æ€§è·å–ä¿¡æ¯æœ€ä¸°å¯Œçš„ä¼ è¾“æ•°æ®ã€‚è¯¥æ–¹æ³•åœ¨äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜è¶Šï¼Œæ€§èƒ½ä¼˜äºéšæœºå’Œå‡åŒ€é—´éš”é‡‡æ ·æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¯¥æŠ€æœ¯è¿˜å…·å¤‡å®æ—¶è¿è¡Œèƒ½åŠ›ï¼Œå¯åœ¨GPUåŠ é€Ÿå™¨ä¸Šå®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¶…å£°æˆåƒä¸­å¸¸ç”¨çš„èšç„¦ä¼ è¾“æŸå½¢æˆæŠ€æœ¯å­˜åœ¨å¸§ç‡è¾ƒä½çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‰ç»´æƒ…å†µä¸‹ä½“ç§¯ç‡æ›´ä½ã€‚</li>
<li>åŸºäºéèšç„¦ä¼ è¾“çš„å¿«é€Ÿæˆåƒå­˜åœ¨è¿åŠ¨å»ç›¸å…³å’Œè°æ³¢æˆåƒèƒ½åŠ›å—é™çš„ç¼ºç‚¹ã€‚</li>
<li>æ–°æŠ€æœ¯ç»“åˆåé‡‡æ ·å’Œæ—¶åºæ‰©æ•£æ¨¡å‹ï¼Œå®ç°æ‚£è€…è‡ªé€‚åº”çš„èšç„¦ä¼ è¾“æ–¹æ¡ˆï¼Œèƒ½æ˜¾è‘—æé«˜è¶…å£°å›¾åƒè´¨é‡å¹¶å‡å°‘å¿…è¦çš„ä¼ è¾“æ¬¡æ•°ã€‚</li>
<li>è¯¥æŠ€æœ¯åœ¨äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºéšæœºå’Œå‡åŒ€é—´éš”å­é‡‡æ ·æŠ€æœ¯ã€‚</li>
<li>æŠ€æœ¯å¯å®æ—¶è¿è¡Œï¼Œèƒ½åœ¨GPUåŠ é€Ÿå™¨ä¸Šå®ç°é«˜æ€§èƒ½ã€‚ä¼°è®¡å–·å°„åˆ†æ•°ä»…éœ€ä½¿ç”¨æ€»ä¼ è¾“æ•°æ®çš„2%ã€‚å…·å¤‡ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ‚£è€…æƒ…å†µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f08d3e524f6970c80b4d52f6b447a70c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1611e8a3d0ef078cf5243c3e2728ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2fd0d1e21f7201bcd7cc82cc9044ab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-709afebca3f564696cd1b40b1a324814.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Variable-Importance-with-Statistical-Control-for-Medical-Data-Based-Prediction"><a href="#Hierarchical-Variable-Importance-with-Statistical-Control-for-Medical-Data-Based-Prediction" class="headerlink" title="Hierarchical Variable Importance with Statistical Control for Medical   Data-Based Prediction"></a>Hierarchical Variable Importance with Statistical Control for Medical   Data-Based Prediction</h2><p><strong>Authors:Joseph Paillard, Antoine Collas, Denis A. Engemann, Bertrand Thirion</strong></p>
<p>Recent advances in machine learning have greatly expanded the repertoire of predictive methods for medical imaging. However, the interpretability of complex models remains a challenge, which limits their utility in medical applications. Recently, model-agnostic methods have been proposed to measure conditional variable importance and accommodate complex non-linear models. However, they often lack power when dealing with highly correlated data, a common problem in medical imaging. We introduce Hierarchical-CPI, a model-agnostic variable importance measure that frames the inference problem as the discovery of groups of variables that are jointly predictive of the outcome. By exploring subgroups along a hierarchical tree, it remains computationally tractable, yet also enjoys explicit family-wise error rate control. Moreover, we address the issue of vanishing conditional importance under high correlation with a tree-based importance allocation mechanism. We benchmarked Hierarchical-CPI against state-of-the-art variable importance methods. Its effectiveness is demonstrated in two neuroimaging datasets: classifying dementia diagnoses from MRI data (ADNI dataset) and analyzing the Berger effect on EEG data (TDBRAIN dataset), identifying biologically plausible variables. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•æå¤§åœ°æ‰©å±•äº†åŒ»å­¦æˆåƒé¢„æµ‹æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¤æ‚æ¨¡å‹çš„è§£é‡Šæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨åŒ»å­¦åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚æœ€è¿‘ï¼Œä¸€äº›æ¨¡å‹æ— å…³çš„æ–¹æ³•è¢«æå‡ºæ¥æµ‹é‡æ¡ä»¶å˜é‡é‡è¦æ€§å¹¶é€‚åº”å¤æ‚çš„éçº¿æ€§æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†é«˜åº¦ç›¸å…³çš„æ•°æ®æ—¶ï¼Œå®ƒä»¬å¾€å¾€ç¼ºä¹æ•ˆåŠ›ï¼Œè¿™åœ¨åŒ»å­¦æˆåƒä¸­æ˜¯ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Hierarchical-CPIï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„å˜é‡é‡è¦æ€§åº¦é‡æ–¹æ³•ï¼Œå®ƒå°†æ¨ç†é—®é¢˜è¡¨è¿°ä¸ºå‘ç°ä¸€ç»„å…±åŒé¢„æµ‹ç»“æœçš„å˜é‡ã€‚é€šè¿‡æ²¿ç€å±‚æ¬¡æ ‘æ¢ç´¢å­ç»„ï¼Œå®ƒå¯ä»¥åœ¨è®¡ç®—ä¸Šä¿æŒå¯è¡Œæ€§ï¼ŒåŒæ—¶ä¹Ÿäº«å—æ˜ç¡®çš„å®¶æ—è¯¯å·®ç‡æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§£å†³äº†é«˜ç›¸å…³æ€§ä¸‹æ¡ä»¶é‡è¦æ€§æ¶ˆå¤±çš„é—®é¢˜ï¼Œé‡‡ç”¨åŸºäºæ ‘çš„çš„é‡è¦æ€§åˆ†é…æœºåˆ¶ã€‚æˆ‘ä»¬å°†Hierarchical-CPIä¸æœ€å…ˆè¿›çš„å˜é‡é‡è¦æ€§æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚å…¶åœ¨ä¸¤ä¸ªç¥ç»æˆåƒæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ï¼šä»MRIæ•°æ®ä¸­åˆ†ç±»ç—´å‘†è¯Šæ–­ï¼ˆADNIæ•°æ®é›†ï¼‰å’Œåˆ†æEEGæ•°æ®çš„ä¼¯æ ¼æ•ˆåº”ï¼ˆTDBRAINæ•°æ®é›†ï¼‰ï¼Œè¯†åˆ«å‡ºç”Ÿç‰©ä¸Šåˆç†çš„å˜é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08724v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒé¢„æµ‹æ–¹æ³•ä¸Šçš„æœ€æ–°è¿›å±•å¤§å¤§æ‰©å±•äº†é¢„æµ‹æ–¹æ³•çš„èŒƒå›´ã€‚ç„¶è€Œï¼Œå¤æ‚æ¨¡å‹çš„è§£é‡Šæ€§ä»æ˜¯æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ†å±‚æ¡ä»¶é¢„æµ‹é‡è¦æ€§ï¼ˆHierarchical-CPIï¼‰æ¨¡å‹ï¼Œé’ˆå¯¹å¤æ‚éçº¿æ€§æ¨¡å‹è¿›è¡Œæ¡ä»¶å˜é‡é‡è¦æ€§åº¦é‡ï¼Œå°¤å…¶é€‚ç”¨äºå¤„ç†åŒ»å­¦æˆåƒä¸­å¸¸è§çš„é«˜ç›¸å…³æ€§æ•°æ®é—®é¢˜ã€‚é€šè¿‡åˆ†å±‚æ ‘ç»“æ„æ¢ç´¢å˜é‡å­ç»„ï¼Œè¯¥æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ä¸”èƒ½æ§åˆ¶å®¶æ—è¯¯å·®ç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ ‘çŠ¶é‡è¦æ€§åˆ†é…æœºåˆ¶è§£å†³é«˜ç›¸å…³æ€§ä¸‹çš„æ¡ä»¶é‡è¦æ€§æ¶ˆå¤±é—®é¢˜ã€‚åœ¨ç¥ç»æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†ç±»ç—´å‘†è¯Šæ–­å’ŒEEGæ•°æ®ä¼¯æ ¼æ•ˆåº”åˆ†æä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶èƒ½è¯†åˆ«å‡ºå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„å˜é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒé¢„æµ‹æ–¹æ³•ä¸Šå–å¾—æœ€æ–°è¿›å±•ï¼Œä½†å¤æ‚æ¨¡å‹çš„è§£é‡Šæ€§ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†é«˜ç›¸å…³æ€§æ•°æ®æ—¶çš„æ•ˆèƒ½å—é™ã€‚</li>
<li>å¼•å…¥åˆ†å±‚æ¡ä»¶é¢„æµ‹é‡è¦æ€§ï¼ˆHierarchical-CPIï¼‰æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤æ‚éçº¿æ€§æ¨¡å‹å’Œé«˜ç›¸å…³æ€§æ•°æ®ã€‚</li>
<li>Hierarchical-CPIé€šè¿‡å°†æ¨ç†é—®é¢˜å®šä½ä¸ºå‘ç°è”åˆé¢„æµ‹ç»“æœçš„å˜é‡ç»„æ¥è¿›è¡Œå·¥ä½œã€‚</li>
<li>Hierarchical-CPIé€šè¿‡åˆ†å±‚æ ‘ç»“æ„è¿›è¡Œå˜é‡å­ç»„çš„æ¢ç´¢ï¼Œä¿æŒè®¡ç®—æ•ˆç‡å¹¶æ§åˆ¶å®¶æ—è¯¯å·®ç‡ã€‚</li>
<li>Hierarchical-CPIé€šè¿‡æ ‘çŠ¶é‡è¦æ€§åˆ†é…æœºåˆ¶è§£å†³æ¡ä»¶é‡è¦æ€§åœ¨é«˜ç›¸å…³æ€§ç¯å¢ƒä¸‹çš„æ¶ˆå¤±é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec84b199cc91447ee9cb89e71d13090d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-556e6bf99d6a49cd67fa9f2e8ce59e0d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences"><a href="#PADReg-Physics-Aware-Deformable-Registration-Guided-by-Contact-Force-for-Ultrasound-Sequences" class="headerlink" title="PADReg: Physics-Aware Deformable Registration Guided by Contact Force   for Ultrasound Sequences"></a>PADReg: Physics-Aware Deformable Registration Guided by Contact Force   for Ultrasound Sequences</h2><p><strong>Authors:Yimeng Geng, Mingyang Zhao, Fan Xu, Guanglin Cao, Gaofeng Meng, Hongbin Liu</strong></p>
<p>Ultrasound deformable registration estimates spatial transformations between pairs of deformed ultrasound images, which is crucial for capturing biomechanical properties and enhancing diagnostic accuracy in diseases such as thyroid nodules and breast cancer. However, ultrasound deformable registration remains highly challenging, especially under large deformation. The inherently low contrast, heavy noise and ambiguous tissue boundaries in ultrasound images severely hinder reliable feature extraction and correspondence matching. Existing methods often suffer from poor anatomical alignment and lack physical interpretability. To address the problem, we propose PADReg, a physics-aware deformable registration framework guided by contact force. PADReg leverages synchronized contact force measured by robotic ultrasound systems as a physical prior to constrain the registration. Specifically, instead of directly predicting deformation fields, we first construct a pixel-wise stiffness map utilizing the multi-modal information from contact force and ultrasound images. The stiffness map is then combined with force data to estimate a dense deformation field, through a lightweight physics-aware module inspired by Hookeâ€™s law. This design enables PADReg to achieve physically plausible registration with better anatomical alignment than previous methods relying solely on image similarity. Experiments on in-vivo datasets demonstrate that it attains a HD95 of 12.90, which is 21.34% better than state-of-the-art methods. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/evelynskip/PADReg">https://github.com/evelynskip/PADReg</a>. </p>
<blockquote>
<p>è¶…å£°å½¢å˜é…å‡†ï¼ˆUltrasound deformable registrationï¼‰æ˜¯ä¼°è®¡ä¸¤ä¸ªå½¢å˜è¶…å£°å›¾åƒä¹‹é—´çš„ç©ºé—´å˜æ¢çš„è¿‡ç¨‹ï¼Œè¿™å¯¹æ•æ‰ç”²çŠ¶è…ºç»“èŠ‚å’Œä¹³è…ºç™Œç­‰ç–¾ç—…çš„ç”Ÿç‰©åŠ›å­¦ç‰¹æ€§ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¶…å£°å½¢å˜é…å‡†ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å˜å½¢æƒ…å†µä¸‹ã€‚è¶…å£°å›¾åƒä¸­å›ºæœ‰çš„å¯¹æ¯”åº¦ä½ã€å™ªå£°ä¸¥é‡å’Œç»„ç»‡è¾¹ç•Œæ¨¡ç³Šç­‰é—®é¢˜ä¸¥é‡é˜»ç¢äº†å¯é çš„ç‰¹å¾æå–å’Œå¯¹åº”å…³ç³»åŒ¹é…ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å­˜åœ¨è§£å‰–ç»“æ„å¯¹é½ä¸è‰¯å’Œç¼ºä¹ç‰©ç†å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PADRegï¼Œè¿™æ˜¯ä¸€ä¸ªå—æ¥è§¦åŠ›å¼•å¯¼çš„ç‰©ç†æ„ŸçŸ¥å½¢å˜é…å‡†æ¡†æ¶ã€‚PADRegåˆ©ç”¨æœºå™¨äººè¶…å£°ç³»ç»ŸåŒæ­¥æµ‹é‡çš„æ¥è§¦åŠ›ä½œä¸ºç‰©ç†å…ˆéªŒæ¥çº¦æŸé…å‡†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸æ˜¯ç›´æ¥é¢„æµ‹å˜å½¢åœºï¼Œè€Œæ˜¯é¦–å…ˆåˆ©ç”¨æ¥è§¦åŠ›å’Œè¶…å£°å›¾åƒçš„å¤šæ¨¡æ€ä¿¡æ¯æ„å»ºåƒç´ çº§çš„åˆšåº¦å›¾ã€‚ç„¶åï¼Œå°†åˆšåº¦å›¾ä¸åŠ›æ•°æ®ç›¸ç»“åˆï¼Œé€šè¿‡å—Hookeå®šå¾‹å¯å‘çš„è½»é‡åŒ–ç‰©ç†æ„ŸçŸ¥æ¨¡å—æ¥ä¼°è®¡å¯†é›†çš„å˜å½¢åœºã€‚è¿™ç§è®¾è®¡ä½¿PADRegèƒ½å¤Ÿå®ç°ç‰©ç†ä¸Šåˆç†çš„é…å‡†ï¼Œä¸ä»…ä¾èµ–å›¾åƒç›¸ä¼¼æ€§çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¾¾åˆ°æ›´å¥½çš„è§£å‰–ç»“æ„å¯¹é½æ•ˆæœã€‚åœ¨çœŸå®äººä½“æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶å®ç°äº†HD95ä¸º12.90ï¼Œæ¯”æœ€æ–°æ–¹æ³•æé«˜äº†21.34%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/evelynskip/PADReg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/evelynskip/PADRegæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08685v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¶…å£°å½¢å˜é…å‡†æŠ€æœ¯åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”²çŠ¶è…ºç»“èŠ‚å’Œä¹³è…ºç™Œç­‰ç–¾ç—…çš„è¯Šæ–­ä¸­ã€‚é’ˆå¯¹è¶…å£°å½¢å˜é…å‡†åœ¨å¤§å½¢å˜ä¸‹é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ¥è§¦åŠ›æµ‹é‡çš„ç‰©ç†æ„ŸçŸ¥å½¢å˜é…å‡†æ¡†æ¶PADRegã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºåƒç´ çº§çš„åˆšåº¦å›¾æ¥ä¼°è®¡å¯†é›†å½¢å˜åœºï¼Œå®ç°äº†å…·æœ‰ç‰©ç†åˆç†æ€§çš„é…å‡†ï¼Œå¹¶è·å¾—äº†æ›´å¥½çš„è§£å‰–å­¦å¯¹é½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å½¢å˜é…å‡†å¯¹äºæ•æ‰ç”Ÿç‰©åŠ›å­¦ç‰¹æ€§ã€æé«˜ç”²çŠ¶è…ºç»“èŠ‚å’Œä¹³è…ºç™Œç­‰ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è¶…å£°å½¢å˜é…å‡†æ–¹æ³•åœ¨å¤§å½¢å˜ä¸‹é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºè¶…å£°å›¾åƒä¸­çš„ä½å¯¹æ¯”åº¦ã€å¼ºå™ªå£°å’Œæ¨¡ç³Šç»„ç»‡è¾¹ç•Œã€‚</li>
<li>PADRegæ¡†æ¶åˆ©ç”¨æ¥è§¦åŠ›ä½œä¸ºç‰©ç†å…ˆéªŒæ¥çº¦æŸé…å‡†ï¼Œé€šè¿‡åŒæ­¥æ¥è§¦åŠ›æµ‹é‡å¢å¼ºäº†é…å‡†çš„å‡†ç¡®æ€§ã€‚</li>
<li>PADRegé€šè¿‡æ„å»ºåƒç´ çº§çš„åˆšåº¦å›¾å¹¶ç»“åˆåŠ›æ•°æ®æ¥ä¼°è®¡å¯†é›†å½¢å˜åœºï¼Œå®ç°ç‰©ç†åˆç†çš„é…å‡†ã€‚</li>
<li>ä¸ä»…ä¾èµ–å›¾åƒç›¸ä¼¼æ€§çš„æ–¹æ³•ç›¸æ¯”ï¼ŒPADRegåœ¨è§£å‰–å­¦å¯¹é½æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚</li>
<li>åœ¨å®é™…æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPADRegè¾¾åˆ°äº†è¾ƒé«˜çš„é…å‡†ç²¾åº¦ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-125bfbe07a5d322060eff0a679c3ae3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c58162edff0a8b229f894fd46bc7cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b7cace663feb64ee83ad38ef926424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aee2f14dcd0a57ee28c7a5e46fc6099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84bb92481a4615adafa978fec74aa68d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def53e65741cd2521f173eb0a0101864.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion"><a href="#MMIF-AMIN-Adaptive-Loss-Driven-Multi-Scale-Invertible-Dense-Network-for-Multimodal-Medical-Image-Fusion" class="headerlink" title="MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for   Multimodal Medical Image Fusion"></a>MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for   Multimodal Medical Image Fusion</h2><p><strong>Authors:Tao Luo, Weihua Xu</strong></p>
<p>Multimodal medical image fusion (MMIF) aims to integrate images from different modalities to produce a comprehensive image that enhances medical diagnosis by accurately depicting organ structures, tissue textures, and metabolic information. Capturing both the unique and complementary information across multiple modalities simultaneously is a key research challenge in MMIF. To address this challenge, this paper proposes a novel image fusion method, MMIF-AMIN, which features a new architecture that can effectively extract these unique and complementary features. Specifically, an Invertible Dense Network (IDN) is employed for lossless feature extraction from individual modalities. To extract complementary information between modalities, a Multi-scale Complementary Feature Extraction Module (MCFEM) is designed, which incorporates a hybrid attention mechanism, convolutional layers of varying sizes, and Transformers. An adaptive loss function is introduced to guide model learning, addressing the limitations of traditional manually-designed loss functions and enhancing the depth of data mining. Extensive experiments demonstrate that MMIF-AMIN outperforms nine state-of-the-art MMIF methods, delivering superior results in both quantitative and qualitative analyses. Ablation experiments confirm the effectiveness of each component of the proposed method. Additionally, extending MMIF-AMIN to other image fusion tasks also achieves promising performance. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰æ—¨åœ¨å°†ä¸åŒæ¨¡æ€çš„å›¾åƒè¿›è¡Œèåˆï¼Œç”Ÿæˆä¸€å¹…å…¨é¢çš„å›¾åƒï¼Œè¯¥å›¾åƒèƒ½å‡†ç¡®åœ°æç»˜å‡ºå™¨å®˜ç»“æ„ã€ç»„ç»‡çº¹ç†å’Œä»£è°¢ä¿¡æ¯ï¼Œä»è€Œæé«˜åŒ»å­¦è¯Šæ–­æ°´å¹³ã€‚åœ¨MMIFä¸­ï¼ŒåŒæ—¶æ•è·å¤šç§æ¨¡æ€çš„ç‹¬æœ‰å’Œäº’è¡¥ä¿¡æ¯æ˜¯ä¸€é¡¹å…³é”®çš„ç ”ç©¶æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒèåˆæ–¹æ³•MMIF-AMINï¼Œè¯¥æ–¹æ³•å…·æœ‰èƒ½å¤Ÿæœ‰æ•ˆæå–è¿™äº›ç‹¬ç‰¹å’Œäº’è¡¥ç‰¹å¾çš„æ–°æ¶æ„ã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡ç”¨å¯é€†å¯†é›†ç½‘ç»œï¼ˆIDNï¼‰ä»å„ä¸ªæ¨¡æ€è¿›è¡Œæ— æŸç‰¹å¾æå–ã€‚ä¸ºäº†æå–æ¨¡æ€ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ç§å¤šå°ºåº¦äº’è¡¥ç‰¹å¾æå–æ¨¡å—ï¼ˆMCFEMï¼‰ï¼Œè¯¥æ¨¡å—ç»“åˆäº†æ··åˆæ³¨æ„åŠ›æœºåˆ¶ã€ä¸åŒå¤§å°çš„å·ç§¯å±‚å’ŒTransformerã€‚å¼•å…¥è‡ªé€‚åº”æŸå¤±å‡½æ•°æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ï¼Œè§£å†³ä¼ ç»Ÿæ‰‹åŠ¨è®¾è®¡çš„æŸå¤±å‡½æ•°çš„å±€é™æ€§ï¼Œæé«˜æ•°æ®æŒ–æ˜çš„æ·±åº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMMIF-AMINåœ¨å®šé‡å’Œå®šæ€§åˆ†ææ–¹é¢éƒ½ä¼˜äºä¹ç§æœ€å…ˆè¿›çš„MMIFæ–¹æ³•ï¼Œå–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚æ¶ˆèå®éªŒè¯å®äº†è¯¥æ–¹æ³•å„ç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå°†MMIF-AMINæ‰©å±•åˆ°å…¶ä»–å›¾åƒèåˆä»»åŠ¡ä¹Ÿå–å¾—äº†æœ‰å‰æ™¯çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08679v1">PDF</a> 10 pages, 6 figures,conference</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆï¼ˆMMIFï¼‰æ—¨åœ¨èåˆä¸åŒæ¨¡æ€çš„å›¾åƒï¼Œç”Ÿæˆå…¨é¢å›¾åƒï¼Œå‡†ç¡®æç»˜å™¨å®˜ç»“æ„ã€ç»„ç»‡çº¹ç†å’Œä»£è°¢ä¿¡æ¯ï¼Œæé«˜åŒ»å­¦è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„MMIFæ–¹æ³•MMIF-AMINï¼Œé‡‡ç”¨æ–°å‹æ¶æ„æœ‰æ•ˆæå–ç‹¬ç‰¹å’Œäº’è¡¥ç‰¹å¾ã€‚ä½¿ç”¨Invertible Dense Networkï¼ˆIDNï¼‰è¿›è¡Œæ— æŸç‰¹å¾æå–ï¼Œè®¾è®¡Multi-scale Complementary Feature Extraction Moduleï¼ˆMCFEMï¼‰æå–æ¨¡æ€é—´çš„äº’è¡¥ä¿¡æ¯ã€‚å¼•å…¥è‡ªé€‚åº”æŸå¤±å‡½æ•°æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ï¼Œè§£å†³ä¼ ç»Ÿæ‰‹åŠ¨è®¾è®¡æŸå¤±å‡½æ•°çš„å±€é™æ€§ï¼Œå¢å¼ºæ•°æ®æŒ–æ˜æ·±åº¦ã€‚å®éªŒè¯æ˜MMIF-AMINä¼˜äºä¹ç§æœ€æ–°MMIFæ–¹æ³•ï¼Œåœ¨å®šé‡å’Œå®šæ€§åˆ†æä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆæ—¨åœ¨æé«˜åŒ»å­¦è¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡èåˆä¸åŒæ¨¡æ€çš„å›¾åƒç”Ÿæˆå…¨é¢å›¾åƒã€‚</li>
<li>MMIF-AMINæ˜¯ä¸€ç§æ–°é¢–çš„åŒ»å­¦å›¾åƒèåˆæ–¹æ³•ï¼Œé‡‡ç”¨æ–°å‹æ¶æ„æå–ç‹¬ç‰¹å’Œäº’è¡¥ç‰¹å¾ã€‚</li>
<li>Invertible Dense Networkï¼ˆIDNï¼‰ç”¨äºæ— æŸç‰¹å¾æå–ã€‚</li>
<li>Multi-scale Complementary Feature Extraction Moduleï¼ˆMCFEMï¼‰è®¾è®¡ç”¨äºæå–æ¨¡æ€é—´çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”æŸå¤±å‡½æ•°ï¼Œè§£å†³ä¼ ç»ŸæŸå¤±å‡½æ•°çš„å±€é™æ€§ï¼Œå¢å¼ºæ•°æ®æŒ–æ˜æ·±åº¦ã€‚</li>
<li>MMIF-AMINåœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–ä¹ç§æœ€æ–°MMIFæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4df0602bec5667f5b1fdf6d94486d33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6fb6a1cd82acc828c704682393ec49c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f571118cc02ad8c1b1aa0c20d7f88d20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35fec250d883d5e8a28a6f6b6a7a5ef1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e21f990a73d6388b9554706ce7edd0f6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="Unified and Semantically Grounded Domain Adaptation for Medical Image   Segmentation"></a>Unified and Semantically Grounded Domain Adaptation for Medical Image   Segmentation</h2><p><strong>Authors:Xin Wang, Yin Guo, Jiamin Xia, Kaiyu Zhang, Niranjan Balu, Mahmud Mossa-Basha, Linda Shapiro, Chun Yuan</strong></p>
<p>Most prior unsupervised domain adaptation approaches for medical image segmentation are narrowly tailored to either the source-accessible setting, where adaptation is guided by source-target alignment, or the source-free setting, which typically resorts to implicit supervision mechanisms such as pseudo-labeling and model distillation. This substantial divergence in methodological designs between the two settings reveals an inherent flaw: the lack of an explicit, structured construction of anatomical knowledge that naturally generalizes across domains and settings. To bridge this longstanding divide, we introduce a unified, semantically grounded framework that supports both source-accessible and source-free adaptation. Fundamentally distinct from all prior works, our frameworkâ€™s adaptability emerges naturally as a direct consequence of the model architecture, without the need for any handcrafted adaptation strategies. Specifically, our model learns a domain-agnostic probabilistic manifold as a global space of anatomical regularities, mirroring how humans establish visual understanding. Thus, the structural content in each image can be interpreted as a canonical anatomy retrieved from the manifold and a spatial transformation capturing individual-specific geometry. This disentangled, interpretable formulation enables semantically meaningful prediction with intrinsic adaptability. Extensive experiments on challenging cardiac and abdominal datasets show that our framework achieves state-of-the-art results in both settings, with source-free performance closely approaching its source-accessible counterpart, a level of consistency rarely observed in prior works. Beyond quantitative improvement, we demonstrate strong interpretability of the proposed framework via manifold traversal for smooth shape manipulation. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œå¤§å¤šæ•°å…ˆå‰çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ä¸»è¦å±€é™äºæºå¯è®¿é—®è®¾ç½®ï¼Œå…¶ä¸­è‡ªé€‚åº”ç”±æºç›®æ ‡å¯¹é½å¼•å¯¼ï¼Œæˆ–è€…æ— æºçš„è®¾å®šï¼Œè¿™é€šå¸¸ä¾èµ–äºéšå¼ç›‘ç£æœºåˆ¶ï¼Œå¦‚ä¼ªæ ‡ç­¾å’Œæ¨¡å‹è’¸é¦ã€‚è¿™ä¸¤ç§è®¾ç½®ä¹‹é—´åœ¨æ–¹æ³•è®ºè®¾è®¡ä¸Šçš„å·¨å¤§å·®å¼‚æ­ç¤ºäº†ä¸€ä¸ªå›ºæœ‰çš„ç¼ºé™·ï¼šç¼ºä¹ä¸€ä¸ªæ˜ç¡®çš„ç»“æ„åŒ–æ„å»ºè§£å‰–çŸ¥è¯†ï¼Œè¿™ç§çŸ¥è¯†èƒ½åœ¨ä¸åŒé¢†åŸŸå’Œè®¾ç½®ä¸­è‡ªç„¶æ¨å¹¿ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€é•¿æœŸå­˜åœ¨çš„åˆ†æ­§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„ã€è¯­ä¹‰åŒ–çš„æ¡†æ¶ï¼Œæ”¯æŒæºå¯è®¿é—®å’Œæ— æºçš„é€‚åº”ã€‚ä¸æ‰€æœ‰å…ˆå‰çš„å·¥ä½œæ ¹æœ¬ä¸åŒï¼Œæˆ‘ä»¬æ¡†æ¶çš„é€‚åº”æ€§æ˜¯æ¨¡å‹æ¶æ„çš„ç›´æ¥ç»“æœï¼Œæ— éœ€ä»»ä½•æ‰‹å·¥åˆ¶å®šçš„é€‚åº”ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ ä¸€ä¸ªåŸŸæ— å…³çš„æ¦‚ç‡æµå½¢ä½œä¸ºè§£å‰–è§„å¾‹çš„å…¨å±€ç©ºé—´ï¼Œåæ˜ äººç±»å»ºç«‹è§†è§‰ç†è§£çš„æ–¹å¼ã€‚å› æ­¤ï¼Œæ¯ä¸ªå›¾åƒçš„ç»“æ„å†…å®¹å¯ä»¥è¢«è§£é‡Šä¸ºä»æµå½¢ä¸­æ£€ç´¢çš„è§„èŒƒè§£å‰–ç»“æ„å’Œæ•æ‰ä¸ªä½“ç‰¹å®šå‡ ä½•çš„ç©ºé—´å˜æ¢ã€‚è¿™ç§è§£è€¦ã€å¯è§£é‡Šçš„å…¬å¼åŒ–å®ç°äº†å…·æœ‰å†…åœ¨é€‚åº”æ€§çš„è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„é¢„æµ‹ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¿ƒè„å’Œè…¹éƒ¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨è¿™ä¸¤ç§è®¾ç½®ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œæ— æºçš„æ€§èƒ½æ¥è¿‘å…¶æºå¯è®¿é—®çš„å¯¹åº”ç‰©ï¼Œè¿™åœ¨ä»¥å‰çš„å·¥ä½œä¸­å¾ˆå°‘è§‚å¯Ÿåˆ°çš„ä¸€è‡´æ€§ã€‚é™¤äº†å®šé‡æ”¹è¿›ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æµå½¢éå†è¿›è¡Œå¹³æ»‘å½¢çŠ¶æ“çºµæ¥å±•ç¤ºæ‰€æå‡ºæ¡†æ¶çš„å¼ºå¤§å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08660v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸé€‚åº”é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºæºå¯è®¿é—®è®¾ç½®å’Œæºè‡ªç”±è®¾ç½®ä¸¤ç§ï¼Œç¼ºä¹æ˜ç¡®çš„ç»“æ„åŒ–æ„å»ºè§£å‰–çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯­ä¹‰æ¡†æ¶ï¼Œæ”¯æŒæºå¯è®¿é—®å’Œæºè‡ªç”±ä¸¤ç§é€‚åº”æ–¹å¼ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ¶æ„è‡ªç„¶åœ°äº§ç”Ÿäº†ä¸€ç§é€‚åº”èƒ½åŠ›ï¼Œæ— éœ€æ‰‹å·¥åˆ¶ä½œé€‚åº”ç­–ç•¥ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªé€šç”¨çš„è§£å‰–æ¦‚ç‡æµå½¢ç©ºé—´æ¥ä½“ç°äººç±»ç†è§£è§†è§‰çš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»æµå½¢ä¸­æ£€ç´¢å‡ºç»“æ„å†…å®¹ä½œä¸ºæ ‡å‡†è§£å‰–ç»“æ„ï¼Œå¹¶é€šè¿‡ç©ºé—´è½¬æ¢æ•æ‰ä¸ªä½“ç‰¹å®šçš„å‡ ä½•å½¢çŠ¶ã€‚è¿™ç§è§£é‡Šæ€§çš„å…¬å¼ä½¿å¾—é¢„æµ‹å…·æœ‰å†…åœ¨é€‚åº”æ€§å’Œè¯­ä¹‰æ„ä¹‰ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¿ƒè„å’Œè…¹éƒ¨æ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå¹¶ä¸”åœ¨æºè‡ªç”±è®¾ç½®ä¸­å®ç°äº†æ¥è¿‘æºå¯è®¿é—®è®¾ç½®çš„è¡¨ç°ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æµå½¢éå†å±•ç¤ºäº†è¯¥æ¡†æ¶çš„å¼ºè§£é‡Šæ€§ï¼Œä»¥å®ç°å¹³æ»‘çš„å½¢çŠ¶æ“æ§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œåˆ†ä¸ºæºå¯è®¿é—®å’Œæºè‡ªç”±ä¸¤ç§è®¾ç½®ã€‚ç¼ºä¹é€šç”¨çš„ç»“æ„åŒ–è§£å‰–çŸ¥è¯†æ˜¯å…¶ä¸­çš„å…³é”®é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯­ä¹‰æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªé€šç”¨çš„è§£å‰–æ¦‚ç‡æµå½¢ç©ºé—´ï¼Œåæ˜ äººç±»ç†è§£è§†è§‰çš„æ–¹å¼ã€‚ç»“æ„å†…å®¹è¢«è§£é‡Šä¸ºä»æµå½¢ä¸­æ£€ç´¢çš„æ ‡å‡†è§£å‰–ç»“æ„ï¼ŒåŒæ—¶é€šè¿‡ç©ºé—´è½¬æ¢æ•æ‰ä¸ªä½“ç‰¹å®šçš„å‡ ä½•å½¢çŠ¶ã€‚è¿™ç§ç»“æ„ä½¿å¾—é¢„æµ‹å…·æœ‰å†…åœ¨é€‚åº”æ€§å’Œè¯­ä¹‰æ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc3f4d46fbf3cfe20f1474caba11fdb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edd89e5d40ebaaf99cb4f5193e3967b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fecfb17b1288a04f3a0cb7f064ef5b8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-409561e94074b6647192f4029b265121.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation"><a href="#Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation" class="headerlink" title="Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse   Teaching and Label Propagation"></a>Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse   Teaching and Label Propagation</h2><p><strong>Authors:Wei Li, Pengcheng Zhou, Linye Ma, Wenyi Zhao, Huihua Yang</strong></p>
<p>Both limited annotation and domain shift are significant challenges frequently encountered in medical image segmentation, leading to derivative scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). Conventional methods are generally tailored to specific tasks in isolation, the error accumulation hinders the effective utilization of unlabeled data and limits further improvements, resulting in suboptimal performance when these issues occur. In this paper, we aim to develop a generic framework that masters all three tasks. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data and increasing the diversity of the model. To tackle this issue, we employ a Diverse Teaching and Label Propagation Network (DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation. Our DTLP-Net involves a single student model and two diverse teacher models, which can generate reliable pseudo-labels for the student model. The first teacher model decouple the training process with labeled and unlabeled data, The second teacher is momentum-updated periodically, thus generating reliable yet divers pseudo-labels. To fully utilize the information within the data, we adopt inter-sample and intra-sample data augmentation to learn the global and local knowledge. In addition, to further capture the voxel-level correlations, we propose label propagation to enhance the model robust. We evaluate our proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks. The results showcase notable improvements compared to state-of-the-art methods across all five settings, indicating the potential of our framework to tackle more challenging SSL scenarios. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œæœ‰é™çš„æ ‡æ³¨å’Œé¢†åŸŸåç§»æ˜¯ç»å¸¸é‡åˆ°çš„é‡å¤§æŒ‘æˆ˜ï¼Œè¿™å¯¼è‡´äº†è¯¸å¦‚åŠç›‘ç£åŒ»å­¦ï¼ˆSSMISï¼‰ã€åŠç›‘ç£åŒ»å­¦é¢†åŸŸæ³›åŒ–ï¼ˆSemi-MDGï¼‰å’Œæ— ç›‘ç£åŒ»å­¦é¢†åŸŸé€‚åº”ï¼ˆUMDAï¼‰ç­‰è¡ç”Ÿåœºæ™¯ã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„ä»»åŠ¡è¿›è¡Œå®šåˆ¶ï¼Œè¯¯å·®ç§¯ç´¯é˜»ç¢äº†æœªæ ‡æ³¨æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œé™åˆ¶äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚å½“è¿™äº›é—®é¢˜å‘ç”Ÿæ—¶ï¼Œæ€§èƒ½å¾€å¾€è¾¾ä¸åˆ°æœ€ä¼˜ã€‚æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤ŸæŒæ¡æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ï¼ˆSSMISã€UMDAå’ŒSemi-MDGï¼‰çš„é€šç”¨æ¡†æ¶ã€‚æˆ‘ä»¬å‘ç°è§£å†³é—®é¢˜çš„å…³é”®åœ¨äºå¦‚ä½•åœ¨å­˜åœ¨é¢†åŸŸåç§»å’Œæœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ï¼Œå¹¶å¢åŠ æ¨¡å‹çš„å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ ·æ•™å­¦æ ‡ç­¾ä¼ æ’­ç½‘ç»œï¼ˆDTLP-Netï¼‰æ¥æå‡é€šç”¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚æˆ‘ä»¬çš„DTLP-NetåŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªå¤šæ ·åŒ–çš„æ•™å¸ˆæ¨¡å‹ï¼Œå¯ä»¥ä¸ºå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ã€‚ç¬¬ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹å°†æ ‡æ³¨å’Œæœªæ ‡æ³¨æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹è§£è€¦ï¼›ç¬¬äºŒä¸ªæ•™å¸ˆæ¨¡å‹å®šæœŸæ›´æ–°åŠ¨é‡ï¼Œä»è€Œç”Ÿæˆå¯é ä¸”å¤šæ ·çš„ä¼ªæ ‡ç­¾ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ ·æœ¬é—´å’Œæ ·æœ¬å†…çš„æ•°æ®å¢å¼ºæ¥å­¦ä¹ å…¨å±€å’Œå±€éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥æ•æ‰ä½“ç´ çº§çš„å…³è”ï¼Œæˆ‘ä»¬æå‡ºäº†æ ‡ç­¾ä¼ æ’­ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨SSMISã€UMDAå’ŒSemi-MDGä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰äº”ä¸ªè®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶åœ¨è§£å†³æ›´å…·æŒ‘æˆ˜æ€§çš„SSLåœºæ™¯æ—¶å…·æœ‰æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08549v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¸¸è§çš„æœ‰é™æ ‡æ³¨å’Œé¢†åŸŸåç§»ä¸¤å¤§æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåº”å¯¹åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ã€åŠç›‘ç£åŒ»å­¦é¢†åŸŸæ³›åŒ–ï¼ˆSemi-MDGï¼‰å’Œæ— ç›‘ç£åŒ»å­¦é¢†åŸŸè‡ªé€‚åº”ï¼ˆUMDAï¼‰ç­‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶çš„å…³é”®åœ¨äºå¦‚ä½•åœ¨å­˜åœ¨é¢†åŸŸåç§»ä¸”æœ‰æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸ºæ— æ ‡æ³¨æ•°æ®ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ï¼Œå¹¶å¢åŠ æ¨¡å‹çš„å¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ ·æ•™å­¦æ ‡ç­¾ä¼ æ’­ç½‘ç»œï¼ˆDTLP-Netï¼‰æ¥æå‡é€šç”¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚è¯¥ç½‘ç»œåŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªå¤šæ ·åŒ–çš„æ•™å¸ˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä¸ºå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ã€‚é€šè¿‡æ ·æœ¬é—´å’Œæ ·æœ¬å†…çš„æ•°æ®å¢å¼ºï¼Œæˆ‘ä»¬å……åˆ†åˆ©ç”¨äº†æ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œå­¦ä¹ å…¨å±€å’Œå±€éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹å¯¹åƒç´ çº§å…³ç³»çš„æ•æ‰èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ ‡ç­¾ä¼ æ’­æŠ€æœ¯ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰äº”ä¸ªè®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¡¨æ˜å…¶è§£å†³æ›´å¤æ‚çš„SSLåœºæ™¯çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æœ‰é™æ ‡æ³¨å’Œé¢†åŸŸåç§»çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´åŠç›‘ç£ã€æ— ç›‘ç£ç­‰è¡ç”Ÿåœºæ™¯çš„å‡ºç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œè¯¯å·®ç´¯ç§¯é™åˆ¶äº†æ— æ ‡æ³¨æ•°æ®çš„åˆ©ç”¨å’Œæ¨¡å‹çš„è¿›ä¸€æ­¥æ”¹è¿›ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œèƒ½å¤Ÿåº”å¯¹å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬SSMISã€UMDAå’ŒSemi-MDGã€‚</li>
<li>æ¡†æ¶çš„å…³é”®åœ¨äºç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾å’Œå¢åŠ æ¨¡å‹å¤šæ ·æ€§ï¼Œé‡‡ç”¨DTLP-Netç½‘ç»œå®ç°ã€‚</li>
<li>é€šè¿‡æ ·æœ¬é—´å’Œæ ·æœ¬å†…çš„æ•°æ®å¢å¼ºä»¥åŠæ ‡ç­¾ä¼ æ’­æŠ€æœ¯ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚</li>
<li>åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e927148f957c034d275ea3462e6dd90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad11a959a124259da54444519632189.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d280b9c256d2440426f315a773ad9df6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SharpXR-Structure-Aware-Denoising-for-Pediatric-Chest-X-Rays"><a href="#SharpXR-Structure-Aware-Denoising-for-Pediatric-Chest-X-Rays" class="headerlink" title="SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays"></a>SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays</h2><p><strong>Authors:Ilerioluwakiiye Abolade, Emmanuel Idoko, Solomon Odelola, Promise Omoigui, Adetola Adebanwo, Aondana Iorumbur, Udunna Anazodo, Alessandro Crimi, Raymond Confidence</strong></p>
<p>Pediatric chest X-ray imaging is essential for early diagnosis, particularly in low-resource settings where advanced imaging modalities are often inaccessible. Low-dose protocols reduce radiation exposure in children but introduce substantial noise that can obscure critical anatomical details. Conventional denoising methods often degrade fine details, compromising diagnostic accuracy. In this paper, we present SharpXR, a structure-aware dual-decoder U-Net designed to denoise low-dose pediatric X-rays while preserving diagnostically relevant features. SharpXR combines a Laplacian-guided edge-preserving decoder with a learnable fusion module that adaptively balances noise suppression and structural detail retention. To address the scarcity of paired training data, we simulate realistic Poisson-Gaussian noise on the Pediatric Pneumonia Chest X-ray dataset. SharpXR outperforms state-of-the-art baselines across all evaluation metrics while maintaining computational efficiency suitable for resource-constrained settings. SharpXR-denoised images improved downstream pneumonia classification accuracy from 88.8% to 92.5%, underscoring its diagnostic value in low-resource pediatric care. </p>
<blockquote>
<p>å„¿ç«¥èƒ¸éƒ¨Xå°„çº¿æˆåƒå¯¹äºæ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹å…ˆè¿›æˆåƒæ¨¡å¼çš„èµ„æºåŒ®ä¹ç¯å¢ƒä¸­ã€‚ä½å‰‚é‡åè®®å‡å°‘äº†å„¿ç«¥æ¥å—çš„è¾å°„æš´éœ²ï¼Œä½†å¼•å…¥äº†å¤§é‡å™ªå£°ï¼Œå¯èƒ½ä¼šæ©ç›–å…³é”®è§£å‰–ç»†èŠ‚ã€‚ä¼ ç»Ÿçš„é™å™ªæ–¹æ³•ç»å¸¸ä¼šç ´åç»†èŠ‚ï¼Œä»è€Œå½±å“è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SharpXRï¼Œè¿™æ˜¯ä¸€ç§ç»“æ„æ„ŸçŸ¥çš„åŒè§£ç å™¨U-Netï¼Œæ—¨åœ¨é™ä½ä½å‰‚é‡å„¿ç«¥Xå°„çº¿çš„å™ªå£°ï¼ŒåŒæ—¶ä¿ç•™ä¸è¯Šæ–­ç›¸å…³çš„ç‰¹å¾ã€‚SharpXRç»“åˆäº†æ‹‰æ™®æ‹‰æ–¯å¼•å¯¼çš„è¾¹ç¼˜ä¿ç•™è§£ç å™¨å’Œå¯å­¦ä¹ çš„èåˆæ¨¡å—ï¼Œè‡ªé€‚åº”åœ°å¹³è¡¡å™ªå£°æŠ‘åˆ¶å’Œç»“æ„ç»†èŠ‚ä¿ç•™ã€‚ä¸ºäº†è§£å†³é…å¯¹è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å°å„¿è‚ºç‚èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šæ¨¡æ‹Ÿäº†ç°å®çš„Poisson-Gaussianå™ªå£°ã€‚SharpXRåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°çš„åŸºçº¿æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒçš„è®¡ç®—æ•ˆç‡ã€‚ç»è¿‡SharpXRå»å™ªçš„å›¾åƒæé«˜äº†ä¸‹æ¸¸è‚ºç‚åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œä»88.8%æé«˜åˆ°92.5%ï¼Œè¿™çªå‡ºäº†å…¶åœ¨èµ„æºæœ‰é™çš„å„¿ç§‘æŠ¤ç†ä¸­çš„è¯Šæ–­ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08518v1">PDF</a> Accepted at MICCAI 2025 MIRASOL Workshop, 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SharpXRæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä½å‰‚é‡å„¿ç«¥èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„å»å™ªæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯ç»“åˆäº†ç»“æ„æ„ŸçŸ¥çš„åŒè§£ç å™¨U-Netç½‘ç»œï¼Œæ—¨åœ¨ä¿ç•™è¯Šæ–­ç›¸å…³ç‰¹å¾çš„åŒæ—¶å»é™¤å™ªå£°ã€‚é€šè¿‡æ¨¡æ‹Ÿå°å„¿è‚ºç‚èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„Poisson-Gaussianå™ªå£°æ¥è§£å†³é…å¯¹è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚SharpXRåœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å…·æœ‰è®¡ç®—æ•ˆç‡ã€‚ä½¿ç”¨SharpXRå»å™ªåçš„å›¾åƒå¯æé«˜è‚ºç‚åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SharpXRæ˜¯ä¸€ç§ç”¨äºä½å‰‚é‡å„¿ç«¥èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„å»å™ªæŠ€æœ¯ã€‚</li>
<li>SharpXRç»“åˆäº†ç»“æ„æ„ŸçŸ¥çš„åŒè§£ç å™¨U-Netç½‘ç»œï¼Œæ—¨åœ¨åœ¨å»å™ªçš„åŒæ—¶ä¿ç•™è¯Šæ–­ç›¸å…³ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨äº†Laplacianå¼•å¯¼çš„è¾¹ç¼˜ä¿ç•™è§£ç å™¨ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸPoisson-Gaussianå™ªå£°æ¥è§£å†³é…å¯¹è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>SharpXRåœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>SharpXRåœ¨èµ„æºå—é™ç¯å¢ƒä¸­å…·æœ‰è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d379d1944339ff1a942bf36bf0ab3a67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50695d86e33329f92bdb8fe8bc659d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-060cffa88ee8879894fad12edf426ef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6650abf36f32fcf305862d91db714093.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Stochastic-Reconstruction-of-the-Speed-of-Sound-in-Breast-Ultrasound-Computed-Tomography-with-Phase-Encoding-in-the-Frequency-Domain"><a href="#Stochastic-Reconstruction-of-the-Speed-of-Sound-in-Breast-Ultrasound-Computed-Tomography-with-Phase-Encoding-in-the-Frequency-Domain" class="headerlink" title="Stochastic Reconstruction of the Speed of Sound in Breast Ultrasound   Computed Tomography with Phase Encoding in the Frequency Domain"></a>Stochastic Reconstruction of the Speed of Sound in Breast Ultrasound   Computed Tomography with Phase Encoding in the Frequency Domain</h2><p><strong>Authors:Luca A. Forte</strong></p>
<p>The framework of ultrasound computed tomography (USCT) has recently re-emerged as a powerful, safe and operator-independent way to image the breast. State of the art image reconstruction methods are performed with iterative techniques based on deterministic optimization algorithms in the frequency domain in the 300 kHz - 1 MHz bandwidth. Alternative algorithms with deterministic and stochastic optimization have been considered in the time-domain. In this paper, we present the equivalent stochastic inversion in the frequency domain (phase encoding), with a focus on reconstructing the speed of sound. We test the inversion algorithm on synthetic data in 2D and 3D, by explicitly differentiating between inverse crime and non-inverse crime scenarios, and compare against the deterministic inversion. We then show the results of the stochastic inversion in the frequency domain on experimental data. By leveraging on the concepts of multiple super-shots and stochastic ensembles, we provide robust evidence that image quality of a stochastic reconstruction of the speed of sound with phase encoding in the frequency domain is comparable, and essentially equivalent, to the one of a deterministic reconstruction, with the further benefit of drastically reducing reconstruction times by more than half. </p>
<blockquote>
<p>è¶…å£°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆUSCTï¼‰æ¡†æ¶æœ€è¿‘é‡æ–°å‡ºç°ä¸ºä¸€ç§å¼ºå¤§ã€å®‰å…¨ä¸”æ“ä½œè€…ç‹¬ç«‹çš„ä¹³è…ºæˆåƒæ–¹æ³•ã€‚æœ€æ–°çš„å›¾åƒé‡å»ºæ–¹æ³•é‡‡ç”¨åŸºäº300 kHzè‡³1 MHzå¸¦å®½é¢‘ç‡åŸŸçš„ç¡®å®šæ€§ä¼˜åŒ–ç®—æ³•çš„è¿­ä»£æŠ€æœ¯ã€‚æ—¶é—´åŸŸä¸­è€ƒè™‘äº†å…·æœ‰ç¡®å®šæ€§å’Œéšæœºæ€§ä¼˜åŒ–çš„æ›¿ä»£ç®—æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é¢‘ç‡åŸŸçš„ç­‰æ•ˆéšæœºåæ¼”ï¼ˆç›¸ä½ç¼–ç ï¼‰ï¼Œé‡ç‚¹ç ”ç©¶å£°é€Ÿçš„é‡å»ºã€‚æˆ‘ä»¬å¯¹äºŒç»´å’Œä¸‰ç»´çš„åˆæˆæ•°æ®è¿›è¡Œåæ¼”ç®—æ³•æµ‹è¯•ï¼Œé€šè¿‡åŒºåˆ†é€†å‘çŠ¯ç½ªå’Œéé€†å‘çŠ¯ç½ªåœºæ™¯ï¼Œå¹¶ä¸ç¡®å®šæ€§åæ¼”è¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å®éªŒæ•°æ®ä¸Šåº”ç”¨é¢‘ç‡åŸŸéšæœºåæ¼”çš„ç»“æœã€‚é€šè¿‡åˆ©ç”¨å¤šé‡è¶…çº§å°„å‡»å’Œéšæœºé›†åˆçš„æ¦‚å¿µï¼Œæˆ‘ä»¬æä¾›äº†å¼ºæœ‰åŠ›çš„è¯æ®è¡¨æ˜ï¼Œåœ¨é¢‘ç‡åŸŸä¸­ä½¿ç”¨ç›¸ä½ç¼–ç è¿›è¡Œå£°é€Ÿçš„éšæœºé‡å»ºçš„å›¾åƒè´¨é‡ä¸ç¡®å®šæ€§é‡å»ºçš„å›¾åƒè´¨é‡ç›¸å½“ï¼Œä¸”æœ¬è´¨ä¸Šç­‰æ•ˆï¼ŒåŒæ—¶è¿›ä¸€æ­¥çš„å¥½å¤„æ˜¯é‡å»ºæ—¶é—´å‡å°‘äº†è¶…è¿‡ä¸€åŠã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08434v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¶…å£°è®¡ç®—æœºå±‚ææˆåƒï¼ˆUSCTï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§ã€å®‰å…¨ã€æ“ä½œè€…ç‹¬ç«‹çš„ä¹³è…ºæˆåƒæ–¹æ³•é‡æ–°å‡ºç°ã€‚æœ€æ–°å›¾åƒé‡å»ºæ–¹æ³•é‡‡ç”¨åŸºäºç¡®å®šæ€§ä¼˜åŒ–ç®—æ³•çš„é¢‘åŸŸè¿­ä»£æŠ€æœ¯ï¼Œåœ¨300 kHzè‡³1 MHzå¸¦å®½èŒƒå›´å†…è¿›è¡Œã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é¢‘åŸŸä¸­çš„ç­‰æ•ˆéšæœºåæ¼”ï¼ˆç›¸ä½ç¼–ç ï¼‰ï¼Œé‡ç‚¹ç ”ç©¶å£°é€Ÿçš„é‡å»ºã€‚é€šè¿‡å¯¹åˆæˆæ•°æ®çš„åæ¼”ç®—æ³•è¿›è¡Œæµ‹è¯•ï¼Œå¹¶ä¸ç¡®å®šæ€§åæ¼”è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å±•ç¤ºäº†é¢‘åŸŸéšæœºåæ¼”çš„æˆæœã€‚é€šè¿‡è¿ç”¨å¤šé‡è¶…çŸ­è„‰å†²å’Œéšæœºé›†åˆçš„æ¦‚å¿µï¼Œæˆ‘ä»¬è¯æ˜åœ¨é¢‘åŸŸä¸­è¿›è¡Œç›¸ä½ç¼–ç çš„éšæœºé‡å»ºçš„å›¾åƒè´¨é‡ä¸ç¡®å®šæ€§é‡å»ºç›¸å½“ï¼Œå¹¶å¤§å¹…åº¦å‡å°‘é‡å»ºæ—¶é—´è¶…è¿‡ä¸€åŠã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°è®¡ç®—æœºå±‚ææˆåƒï¼ˆUSCTï¼‰ä½œä¸ºä¹³è…ºæˆåƒçš„å…ˆè¿›æ–¹æ³•é‡æ–°å‡ºç°ã€‚</li>
<li>å½“å‰å›¾åƒé‡å»ºæ–¹æ³•é‡‡ç”¨é¢‘åŸŸçš„ç¡®å®šæ€§ä¼˜åŒ–ç®—æ³•è¿­ä»£æŠ€æœ¯ã€‚</li>
<li>æå‡ºäº†é¢‘åŸŸä¸­çš„ç­‰æ•ˆéšæœºåæ¼”ï¼ˆç›¸ä½ç¼–ç ï¼‰æ–¹æ³•ï¼Œä¸“æ³¨äºå£°é€Ÿçš„é‡å»ºã€‚</li>
<li>å¯¹åˆæˆæ•°æ®è¿›è¡Œåæ¼”ç®—æ³•æµ‹è¯•ï¼ŒåŒºåˆ†äº†é€†çŠ¯ç½ªå’Œéé€†çŠ¯ç½ªåœºæ™¯ã€‚</li>
<li>éšæœºåæ¼”åœ¨é¢‘åŸŸçš„å®éªŒæ•°æ®ä¸Šå±•ç°å‡ºä¸ç¡®å®šæ€§åæ¼”ç›¸å½“çš„ç»“æœã€‚</li>
<li>é€šè¿‡å¤šé‡è¶…çŸ­è„‰å†²å’Œéšæœºé›†åˆï¼Œè¯æ˜éšæœºé‡å»ºçš„å›¾åƒè´¨é‡ä¸ç¡®å®šæ€§é‡å»ºç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11223d8ccef8aef216b5321bacadd959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bca56415dbad4ad884e53a6c73268e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-040d293c85ba9427083f0584807191c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec29b154735e331b9943a4f59fd292e4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Information-Bottleneck-based-Causal-Attention-for-Multi-label-Medical-Image-Recognition"><a href="#Information-Bottleneck-based-Causal-Attention-for-Multi-label-Medical-Image-Recognition" class="headerlink" title="Information Bottleneck-based Causal Attention for Multi-label Medical   Image Recognition"></a>Information Bottleneck-based Causal Attention for Multi-label Medical   Image Recognition</h2><p><strong>Authors:Xiaoxiao Cui, Yiran Li, Kai He, Shanzhi Jiang, Mengli Xue, Wentao Li, Junhong Leng, Zhi Liu, Lizhen Cui, Shuo Li</strong></p>
<p>Multi-label classification (MLC) of medical images aims to identify multiple diseases and holds significant clinical potential. A critical step is to learn class-specific features for accurate diagnosis and improved interpretability effectively. However, current works focus primarily on causal attention to learn class-specific features, yet they struggle to interpret the true cause due to the inadvertent attention to class-irrelevant features. To address this challenge, we propose a new structural causal model (SCM) that treats class-specific attention as a mixture of causal, spurious, and noisy factors, and a novel Information Bottleneck-based Causal Attention (IBCA) that is capable of learning the discriminative class-specific attention for MLC of medical images. Specifically, we propose learning Gaussian mixture multi-label spatial attention to filter out class-irrelevant information and capture each class-specific attention pattern. Then a contrastive enhancement-based causal intervention is proposed to gradually mitigate the spurious attention and reduce noise information by aligning multi-head attention with the Gaussian mixture multi-label spatial. Quantitative and ablation results on Endo and MuReD show that IBCA outperforms all methods. Compared to the second-best results for each metric, IBCA achieves improvements of 6.35% in CR, 7.72% in OR, and 5.02% in mAP for MuReD, 1.47% in CR, and 1.65% in CF1, and 1.42% in mAP for Endo. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒçš„å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆMLCï¼‰æ—¨åœ¨è¯†åˆ«å¤šç§ç–¾ç—…ï¼Œå…·æœ‰é‡è¦çš„ä¸´åºŠæ½œåŠ›ã€‚å…³é”®æ­¥éª¤æ˜¯å­¦ä¹ ç‰¹å®šç±»åˆ«çš„ç‰¹å¾ï¼Œä»¥å®ç°å‡†ç¡®çš„è¯Šæ–­å’Œæœ‰æ•ˆçš„å¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡å› æœæ³¨æ„åŠ›å­¦ä¹ ç‰¹å®šç±»åˆ«çš„ç‰¹å¾ä¸Šï¼Œä½†ç”±äºæ— æ„ä¸­å…³æ³¨ä¸ç±»åˆ«æ— å…³çš„ç‰¹å¾ï¼Œä»–ä»¬éš¾ä»¥è§£é‡ŠçœŸæ­£çš„ç—…å› ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ï¼Œå°†ç‰¹å®šç±»åˆ«çš„æ³¨æ„åŠ›è§†ä¸ºå› æœã€å¶ç„¶å’Œå™ªå£°å› ç´ çš„æ··åˆä½“ï¼Œä»¥åŠä¸€ç§åŸºäºä¿¡æ¯ç“¶é¢ˆçš„å› æœæ³¨æ„åŠ›ï¼ˆIBCAï¼‰ï¼Œèƒ½å¤Ÿå­¦ä¹ åŒ»å­¦å›¾åƒMLCä¸­å…·æœ‰åŒºåˆ†èƒ½åŠ›çš„ç‰¹å®šç±»åˆ«æ³¨æ„åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ é«˜æ–¯æ··åˆå¤šæ ‡ç­¾ç©ºé—´æ³¨æ„åŠ›æ¥è¿‡æ»¤æ‰ä¸ç±»åˆ«æ— å…³çš„ä¿¡æ¯ï¼Œå¹¶æ•æ‰æ¯ä¸ªç‰¹å®šç±»åˆ«çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚ç„¶åæå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å¢å¼ºçš„å› æœå¹²é¢„æªæ–½ï¼Œé€šè¿‡å¤šå¤´æ³¨æ„åŠ›ä¸é«˜æ–¯æ··åˆå¤šæ ‡ç­¾ç©ºé—´å¯¹é½ï¼Œé€æ­¥å‡è½»å¶ç„¶æ³¨æ„åŠ›å’Œå‡å°‘å™ªå£°ä¿¡æ¯ã€‚åœ¨Endoå’ŒMuReDä¸Šçš„å®šé‡å’Œæ¶ˆèç»“æœè¡¨æ˜ï¼ŒIBCAä¼˜äºæ‰€æœ‰æ–¹æ³•ã€‚ä¸æ¯ä¸ªæŒ‡æ ‡çš„ç¬¬äºŒåç»“æœç›¸æ¯”ï¼ŒIBCAåœ¨MuReDä¸Šçš„CRæŒ‡æ ‡ä¸Šæé«˜äº†6.35%ï¼ŒORæŒ‡æ ‡ä¸Šæé«˜äº†7.72%ï¼ŒmAPæŒ‡æ ‡ä¸Šæé«˜äº†5.02%ï¼Œåœ¨Endoä¸Šçš„CRæŒ‡æ ‡ä¸Šæé«˜äº†1.47%ï¼ŒCF1æŒ‡æ ‡ä¸Šæé«˜äº†1.65%ï¼ŒmAPæŒ‡æ ‡ä¸Šæé«˜äº†1.42%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08069v1">PDF</a> Early accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰å’ŒåŸºäºä¿¡æ¯ç“¶é¢ˆçš„å› æœæ³¨æ„åŠ›ï¼ˆIBCAï¼‰ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒå¤šæ ‡ç­¾åˆ†ç±»ï¼ˆMLCï¼‰ä¸­ç±»ç‰¹å®šç‰¹å¾å­¦ä¹ çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥é«˜æ–¯æ··åˆå¤šæ ‡ç­¾ç©ºé—´æ³¨æ„åŠ›æ¥è¿‡æ»¤æ‰ä¸ç±»æ— å…³çš„ä¿¡æ¯ï¼Œå¹¶æ•æ‰æ¯ä¸ªç±»çš„ç‰¹å®šæ³¨æ„åŠ›æ¨¡å¼ã€‚æå‡ºåŸºäºå¯¹æ¯”å¢å¼ºçš„å› æœå¹²é¢„ï¼Œé€æ­¥å‡è½»è™šå‡æ³¨æ„åŠ›å’Œå‡å°‘å™ªå£°ä¿¡æ¯ã€‚åœ¨Endoå’ŒMuReDæ•°æ®é›†ä¸Šçš„å®šé‡å’Œæ¶ˆèç»“æœè¡¨æ˜ï¼ŒIBCAä¼˜äºæ‰€æœ‰å…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆMLCï¼‰åœ¨åŒ»å­¦å›¾åƒä¸­ç”¨äºè¯†åˆ«å¤šç§ç–¾ç—…ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¸´åºŠæ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡å› æœæ³¨æ„åŠ›æ¥å­¦ä¹ ç±»ç‰¹å®šç‰¹å¾ï¼Œä½†å¾€å¾€å› æ— æ„ä¸­å…³æ³¨ä¸ç±»æ— å…³çš„ç‰¹å¾è€Œæ— æ³•è§£é‡ŠçœŸæ­£çš„åŸå› ã€‚</li>
<li>å¼•å…¥æ–°çš„ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ï¼Œå°†ç±»ç‰¹å®šæ³¨æ„åŠ›è§†ä¸ºå› æœã€å¶ç„¶å’Œå™ªå£°å› ç´ çš„æ··åˆã€‚</li>
<li>æå‡ºåŸºäºä¿¡æ¯ç“¶é¢ˆçš„å› æœæ³¨æ„åŠ›ï¼ˆIBCAï¼‰ï¼Œèƒ½å¤Ÿå­¦ä¹ ç”¨äºåŒ»å­¦å›¾åƒMLCçš„åˆ¤åˆ«ç±»ç‰¹å®šæ³¨æ„åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥é«˜æ–¯æ··åˆå¤šæ ‡ç­¾ç©ºé—´æ³¨æ„åŠ›ï¼Œè¿‡æ»¤æ‰ä¸ç±»æ— å…³çš„ä¿¡æ¯ï¼Œå¹¶æ•æ‰æ¯ä¸ªç±»çš„ç‰¹å®šæ³¨æ„åŠ›æ¨¡å¼ã€‚</li>
<li>æå‡ºåŸºäºå¯¹æ¯”å¢å¼ºçš„å› æœå¹²é¢„ï¼Œä»¥é€æ­¥å‡è½»è™šå‡æ³¨æ„åŠ›å’Œå‡å°‘å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b54a7d5f8236c396aad30905594e1779.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-930715415a4d0bbca7120947904ae5e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcecdb8e5c6f64c0bef2a98a12757b07.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PrIINeR-Towards-Prior-Informed-Implicit-Neural-Representations-for-Accelerated-MRI"><a href="#PrIINeR-Towards-Prior-Informed-Implicit-Neural-Representations-for-Accelerated-MRI" class="headerlink" title="PrIINeR: Towards Prior-Informed Implicit Neural Representations for   Accelerated MRI"></a>PrIINeR: Towards Prior-Informed Implicit Neural Representations for   Accelerated MRI</h2><p><strong>Authors:Ziad Al-Haj Hemidi, Eytan Kats, Mattias P. Heinrich</strong></p>
<p>Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often degrades image quality. While Implicit Neural Representations (INRs) show promise for MRI reconstruction, they struggle at high acceleration factors due to weak prior constraints, leading to structural loss and aliasing artefacts. To address this, we propose PrIINeR, an INR-based MRI reconstruction method that integrates prior knowledge from pre-trained deep learning models into the INR framework. By combining population-level knowledge with instance-based optimization and enforcing dual data consistency, PrIINeR aligns both with the acquired k-space data and the prior-informed reconstruction. Evaluated on the NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based approaches but also improves upon several learning-based state-of-the-art methods, significantly improving structural preservation and fidelity while effectively removing aliasing artefacts.PrIINeR bridges deep learning and INR-based techniques, offering a more reliable solution for high-quality, accelerated MRI reconstruction. The code is publicly available on <a target="_blank" rel="noopener" href="https://github.com/multimodallearning/PrIINeR">https://github.com/multimodallearning/PrIINeR</a>. </p>
<blockquote>
<p>åŠ é€Ÿç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è™½ç„¶èƒ½å‡å°‘æ‰«ææ—¶é—´ï¼Œä½†å¾€å¾€ä¼šé™ä½å›¾åƒè´¨é‡ã€‚è™½ç„¶éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åœ¨MRIé‡å»ºä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹å…ˆéªŒçº¦æŸï¼Œå®ƒä»¬åœ¨é«˜åŠ é€Ÿå› å­ä¸‹ä¼šå‡ºç°ç»“æ„æŸå¤±å’Œæ··å ä¼ªå½±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºINRçš„MRIé‡å»ºæ–¹æ³•PrIINeRï¼Œå®ƒå°†é¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†é›†æˆåˆ°INRæ¡†æ¶ä¸­ã€‚é€šè¿‡ç»“åˆç¾¤ä½“çŸ¥è¯†å’ŒåŸºäºå®ä¾‹çš„ä¼˜åŒ–ï¼Œå¹¶å¼ºåˆ¶æ‰§è¡ŒåŒé‡æ•°æ®ä¸€è‡´æ€§ï¼ŒPrIINeRæ—¢ç¬¦åˆè·å¾—çš„kç©ºé—´æ•°æ®ï¼Œä¹Ÿä¸å…ˆéªŒä¿¡æ¯é©±åŠ¨çš„é‡å»ºç›¸å»åˆã€‚åœ¨NYU fastMRIæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºæœ€æ–°çš„INRæ–¹æ³•ï¼Œä¹Ÿè¶…è¶Šäº†å¤šç§åŸºäºå­¦ä¹ çš„å‰æ²¿æ–¹æ³•ï¼Œåœ¨ä¿æŒç»“æ„å’Œä¿çœŸåº¦æ–¹é¢æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°æ¶ˆé™¤äº†æ··å ä¼ªå½±ã€‚PrIINeRç»“åˆäº†æ·±åº¦å­¦ä¹ å’ŒåŸºäºINRçš„æŠ€æœ¯ï¼Œä¸ºé«˜è´¨é‡ã€åŠ é€Ÿçš„MRIé‡å»ºæä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/multimodallearning/PrIINeR%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/multimodallearning/PrIINeRä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08058v1">PDF</a> Submitted to the British Machine Vision Conference (BMVC) 2025   (Before peer review version)</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨åŠ é€Ÿæ‰«ææ—¶å›¾åƒè´¨é‡ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†PrIINeRæ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰å’Œé¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å¼•å…¥å…ˆéªŒçŸ¥è¯†è§£å†³äº†INRåœ¨é«˜åŠ é€Ÿå› å­ä¸‹ç»“æ„æŸå¤±å’Œæ··å ä¼ªå½±çš„é—®é¢˜ã€‚åœ¨NYU fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPrIINeRä¸ä»…ä¼˜äºç°æœ‰çš„INRæ–¹æ³•ï¼Œä¹Ÿè¶…è¶Šäº†å…¶ä»–å­¦ä¹ åŸºå‡†æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†ç»“æ„ä¿ç•™æ€§å’Œä¿çœŸåº¦ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†æ··å ä¼ªå½±ã€‚PrIINeRä¸ºé«˜è´¨é‡ã€åŠ é€Ÿçš„MRIé‡å»ºæä¾›äº†æ›´å¯é çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PrIINeRæ–¹æ³•è§£å†³äº†MRIåŠ é€Ÿæ‰«æä¸­å›¾åƒè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>PrIINeRç»“åˆäº†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰å’Œé¢„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¼•å…¥å…ˆéªŒçŸ¥è¯†ï¼ŒPrIINeRè§£å†³äº†é«˜åŠ é€Ÿå› å­ä¸‹çš„ç»“æ„æŸå¤±å’Œæ··å ä¼ªå½±é—®é¢˜ã€‚</li>
<li>PrIINeRåœ¨NYU fastMRIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚</li>
<li>PrIINeRæé«˜äº†ç»“æ„ä¿ç•™æ€§å’Œå›¾åƒä¿çœŸåº¦ã€‚</li>
<li>PrIINeRæœ‰æ•ˆæ¶ˆé™¤äº†æ··å ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8401dc9cc277e1b8d2ceb6637283a3f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1f84eba0b366b82c25463208a14b61a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9c0da39bb38c143f19d9e9013622c0e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer"><a href="#MIND-A-Noise-Adaptive-Denoising-Framework-for-Medical-Images-Integrating-Multi-Scale-Transformer" class="headerlink" title="MIND: A Noise-Adaptive Denoising Framework for Medical Images   Integrating Multi-Scale Transformer"></a>MIND: A Noise-Adaptive Denoising Framework for Medical Images   Integrating Multi-Scale Transformer</h2><p><strong>Authors:Tao Tang, Chengxu Yang</strong></p>
<p>The core role of medical images in disease diagnosis makes their quality directly affect the accuracy of clinical judgment. However, due to factors such as low-dose scanning, equipment limitations and imaging artifacts, medical images are often accompanied by non-uniform noise interference, which seriously affects structure recognition and lesion detection. This paper proposes a medical image adaptive denoising model (MI-ND) that integrates multi-scale convolutional and Transformer architecture, introduces a noise level estimator (NLE) and a noise adaptive attention module (NAAB), and realizes channel-spatial attention regulation and cross-modal feature fusion driven by noise perception. Systematic testing is carried out on multimodal public datasets. Experiments show that this method significantly outperforms the comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS, and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing strong prac-tical value and promotional potential. The model has outstanding benefits in structural recovery, diagnostic sensitivity, and cross-modal robustness, and provides an effective solution for medical image enhancement and AI-assisted diagnosis and treatment. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåœ¨ç–¾ç—…è¯Šæ–­ä¸­çš„æ ¸å¿ƒä½œç”¨ä½¿å…¶è´¨é‡ç›´æ¥å½±å“ä¸´åºŠåˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºä½å‰‚é‡æ‰«æã€è®¾å¤‡é™åˆ¶å’Œæˆåƒä¼ªå½±ç­‰å› ç´ ï¼ŒåŒ»ç–—å›¾åƒå¾€å¾€ä¼´éšç€éå‡åŒ€å™ªå£°å¹²æ‰°ï¼Œè¿™ä¸¥é‡å½±å“äº†ç»“æ„è¯†åˆ«å’Œç—…ç¶æ£€æµ‹ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¶æ„çš„åŒ»ç–—å›¾åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼ˆMI-NDï¼‰ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å™ªå£°æ°´å¹³ä¼°è®¡å™¨ï¼ˆNLEï¼‰å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼ˆNAABï¼‰ï¼Œå®ç°äº†åŸºäºå™ªå£°æ„ŸçŸ¥çš„é€šé“-ç©ºé—´æ³¨æ„åŠ›è°ƒèŠ‚å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆã€‚åœ¨å¤šæ¨¡æ€å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚PSNRã€SSIMå’ŒLPIPSï¼‰ä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡çš„F1åˆ†æ•°å’ŒROC-AUCä¸Šæœ‰æ‰€æé«˜ï¼Œè¡¨ç°å‡ºå¾ˆå¼ºçš„å®ç”¨ä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚è¯¥æ¨¡å‹åœ¨ç»“æ„æ¢å¤ã€è¯Šæ–­æ•æ„Ÿåº¦å’Œè·¨æ¨¡æ€ç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„ä¼˜åŠ¿ï¼Œä¸ºåŒ»ç–—å›¾åƒå¢å¼ºå’ŒAIè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07817v1">PDF</a> 6 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦å›¾åƒåœ¨ç–¾ç—…è¯Šæ–­ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼ŒåŠå…¶è´¨é‡å¯¹ä¸´åºŠåˆ¤æ–­å‡†ç¡®æ€§çš„ç›´æ¥å½±å“ã€‚é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­å› ä½å‰‚é‡æ‰«æã€è®¾å¤‡é™åˆ¶å’Œæˆåƒä¼ªå½±ç­‰å› ç´ å¯¼è‡´çš„éå‡åŒ€å™ªå£°å¹²æ‰°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¶æ„çš„åŒ»å­¦å›¾åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼ˆMI-NDï¼‰ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†å™ªå£°æ°´å¹³ä¼°è®¡å™¨ï¼ˆNLEï¼‰å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼ˆNAABï¼‰ï¼Œå®ç°äº†åŸºäºå™ªå£°æ„ŸçŸ¥çš„é€šé“-ç©ºé—´æ³¨æ„åŠ›è°ƒæ§å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆã€‚åœ¨å¤šæ¨¡æ€å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç³»ç»Ÿæµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ï¼ˆå¦‚PSNRã€SSIMå’ŒLPIPSï¼‰ä¸Šæ˜¾è‘—ä¼˜äºå¯¹æ¯”æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸è¯Šæ–­ä»»åŠ¡ä¸­æé«˜äº†F1åˆ†æ•°å’ŒROC-AUCå€¼ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„å®è·µä»·å€¼å’Œæ¨å¹¿æ½œåŠ›ã€‚è¯¥æ¨¡å‹åœ¨ç»“æ„æ¢å¤ã€è¯Šæ–­æ•æ„Ÿåº¦å’Œè·¨æ¨¡æ€ç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºåŒ»å­¦å›¾åƒå¢å¼ºå’ŒAIè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè´¨é‡å¯¹ç–¾ç—…è¯Šæ–­çš„å‡†ç¡®æ€§å…·æœ‰ç›´æ¥å½±å“ã€‚</li>
<li>åŒ»å­¦å›¾åƒå¸¸å—åˆ°éå‡åŒ€å™ªå£°å¹²æ‰°ï¼Œå½±å“ç»“æ„è¯†åˆ«å’Œç—…ç¶æ£€æµ‹ã€‚</li>
<li>æå‡ºçš„åŒ»å­¦å›¾åƒè‡ªé€‚åº”å»å™ªæ¨¡å‹ï¼ˆMI-NDï¼‰ç»“åˆäº†å¤šå°ºåº¦å·ç§¯å’ŒTransformeræ¶æ„ã€‚</li>
<li>MI-NDæ¨¡å‹å¼•å…¥äº†å™ªå£°æ°´å¹³ä¼°è®¡å™¨ï¼ˆNLEï¼‰å’Œå™ªå£°è‡ªé€‚åº”æ³¨æ„åŠ›æ¨¡å—ï¼ˆNAABï¼‰ã€‚</li>
<li>MI-NDæ¨¡å‹å®ç°äº†åŸºäºå™ªå£°æ„ŸçŸ¥çš„é€šé“-ç©ºé—´æ³¨æ„åŠ›è°ƒæ§å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆã€‚</li>
<li>ç³»ç»Ÿæµ‹è¯•è¡¨æ˜ï¼ŒMI-NDæ¨¡å‹åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4aaf0d1e2f2487103fc78fa40cc9a1fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b51c288f5f37af774d3918ad8e57883b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-826d806ce10970a75f40457d055569bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa237f8e46a2766e82de0d29b1886f12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c6dd83ed907099fea700f522e7c071f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58ca1f5fe677868cc7d2f55b6a7968df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835fecec1bb2574665c10003350e6d78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa701c04b2c5d0a14d86db4f80002748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b1c3cd900e00f9d5b8dd05a58b28f46.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SynMatch-Rethinking-Consistency-in-Medical-Image-Segmentation-with-Sparse-Annotations"><a href="#SynMatch-Rethinking-Consistency-in-Medical-Image-Segmentation-with-Sparse-Annotations" class="headerlink" title="SynMatch: Rethinking Consistency in Medical Image Segmentation with   Sparse Annotations"></a>SynMatch: Rethinking Consistency in Medical Image Segmentation with   Sparse Annotations</h2><p><strong>Authors:Zhiqiang Shen, Peng Cao, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane</strong></p>
<p>Label scarcity remains a major challenge in deep learning-based medical image segmentation. Recent studies use strong-weak pseudo supervision to leverage unlabeled data. However, performance is often hindered by inconsistencies between pseudo labels and their corresponding unlabeled images. In this work, we propose \textbf{SynMatch}, a novel framework that sidesteps the need for improving pseudo labels by synthesizing images to match them instead. Specifically, SynMatch synthesizes images using texture and shape features extracted from the same segmentation model that generates the corresponding pseudo labels for unlabeled images. This design enables the generation of highly consistent synthesized-image-pseudo-label pairs without requiring any training parameters for image synthesis. We extensively evaluate SynMatch across diverse medical image segmentation tasks under semi-supervised learning (SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL) settings with increasingly limited annotations. The results demonstrate that SynMatch achieves superior performance, especially in the most challenging BSL setting. For example, it outperforms the recent strong-weak pseudo supervision-based method by 29.71% and 10.05% on the polyp segmentation task with 5% and 10% scribble annotations, respectively. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/Senyh/SynMatch">https://github.com/Senyh/SynMatch</a>. </p>
<blockquote>
<p>åœ¨æ·±åº¦å­¦ä¹ ä¸ºåŸºç¡€çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œæ ‡ç­¾ç¨€ç¼ºä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨å¼ºå¼±ä¼ªç›‘ç£æ¥åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®ã€‚ç„¶è€Œï¼Œæ€§èƒ½å¾€å¾€å—åˆ°ä¼ªæ ‡ç­¾ä¸å…¶ç›¸åº”çš„æœªæ ‡è®°å›¾åƒä¹‹é—´ä¸ä¸€è‡´æ€§çš„é˜»ç¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹æ¡†æ¶<strong>SynMatch</strong>ï¼Œå®ƒé€šè¿‡åˆæˆå›¾åƒæ¥åŒ¹é…ä¼ªæ ‡ç­¾ï¼Œä»è€Œé¿å…äº†æ”¹å–„ä¼ªæ ‡ç­¾çš„éœ€è¦ã€‚å…·ä½“æ¥è¯´ï¼ŒSynMatchä½¿ç”¨ä»åŒä¸€åˆ†å‰²æ¨¡å‹ä¸­æå–çš„çº¹ç†å’Œå½¢çŠ¶ç‰¹å¾æ¥åˆæˆå›¾åƒï¼Œè¯¥æ¨¡å‹ä¸ºæœªæ ‡è®°å›¾åƒç”Ÿæˆç›¸åº”çš„ä¼ªæ ‡ç­¾ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿç”Ÿæˆé«˜åº¦ä¸€è‡´çš„åˆæˆå›¾åƒ-ä¼ªæ ‡ç­¾å¯¹ï¼Œè€Œæ— éœ€ä¸ºå›¾åƒåˆæˆæä¾›ä»»ä½•è®­ç»ƒå‚æ•°ã€‚æˆ‘ä»¬åœ¨åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ã€å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSLï¼‰å’Œå‡ ä¹æ— ç›‘ç£å­¦ä¹ ï¼ˆBSLï¼‰è®¾ç½®ä¸‹ï¼Œå¯¹å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„SynMatchè¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®æ—¥ç›Šæœ‰é™ã€‚ç»“æœè¡¨æ˜ï¼ŒSynMatchå°¤å…¶åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„BSLè®¾ç½®ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¯è‚‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œå®ƒåœ¨5%å’Œ10%çš„æ¶‚é¸¦æ ‡æ³¨ä¸‹ï¼Œæ¯”æœ€è¿‘çš„å¼ºå¼±ä¼ªç›‘ç£æ–¹æ³•åˆ†åˆ«é«˜å‡º29.71%å’Œ10.05%ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Senyh/SynMatch%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Senyh/SynMatchå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07298v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ ‡ç­¾ç¨€ç¼ºä»æ˜¯æ·±åº¦å­¦ä¹ é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚æœ€æ–°ç ”ç©¶å°è¯•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ï¼Œé‡‡ç”¨å¼ºå¼±ä¼ªç›‘ç£æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¼ªæ ‡ç­¾ä¸å¯¹åº”æ— æ ‡ç­¾å›¾åƒé—´çš„ä¸ä¸€è‡´æ€§å¸¸å¸¸åˆ¶çº¦æ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹æ¡†æ¶SynMatchï¼Œé€šè¿‡åˆæˆå›¾åƒæ¥åŒ¹é…ä¼ªæ ‡ç­¾ï¼Œä»è€Œé¿å…æ”¹å–„ä¼ªæ ‡ç­¾çš„éœ€æ±‚ã€‚SynMatchåˆ©ç”¨ä»åŒä¸€åˆ†å‰²æ¨¡å‹ä¸­æå–çš„çº¹ç†å’Œå½¢çŠ¶ç‰¹å¾ï¼Œåˆæˆä¸ä¼ªæ ‡ç­¾ç›¸åŒ¹é…çš„å›¾åƒã€‚è¿™ç§è®¾è®¡å¯åœ¨æ— éœ€å›¾åƒåˆæˆè®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆé«˜åº¦ä¸€è‡´çš„åˆæˆå›¾åƒ-ä¼ªæ ‡ç­¾å¯¹ã€‚åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸åŒä»»åŠ¡ä¸­ï¼ŒSynMatchåœ¨åŠç›‘ç£å­¦ä¹ ã€å¼±ç›‘ç£å­¦ä¹ å’Œå‡ ä¹æ— ç›‘ç£å­¦ä¹ çš„ç¯å¢ƒä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„å‡ ä¹æ— ç›‘ç£å­¦ä¹ ç¯å¢ƒä¸‹è¡¨ç°å°¤ä¸ºçªå‡ºã€‚ä¾‹å¦‚ï¼Œåœ¨5%å’Œ10%æ¶‚é¸¦æ³¨é‡Šçš„å¤šå‘æ€§æ¯è‚‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒSynMatchçš„è¡¨ç°åœ¨æœ€æ–°çš„å¼ºå¼±ä¼ªç›‘ç£æ–¹æ³•çš„åŸºç¡€ä¸Šåˆ†åˆ«æé«˜äº†29.71%å’Œæé«˜äº†ä»…æå‡ç¬¬äºŒï¼Œè¿™æ¬¡è¶…è¿‡æœŸå¾…ç›®æ ‡ä»…é™äºåœ¨æ ¡æ ¡ç½‘çš„è¯­ä¹‰å†…ç½‘ç¯å¢ƒä¸‹ä½¿ç”¨ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Senyh/SynMatch%E4%B8%8A%E3%80%82">https://github.com/Senyh/SynMatchä¸Šã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡ç­¾ç¨€ç¼ºä»æ˜¯æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æœ€æ–°ç ”ç©¶å°è¯•ä½¿ç”¨å¼ºå¼±ä¼ªç›‘ç£æ–¹æ³•æ¥åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ã€‚</li>
<li>ä¼ªæ ‡ç­¾ä¸å¯¹åº”æ— æ ‡ç­¾å›¾åƒé—´çš„ä¸ä¸€è‡´æ€§é™åˆ¶äº†æ€§èƒ½æå‡ã€‚</li>
<li>æå‡ºæ–°å‹æ¡†æ¶SynMatchï¼Œé€šè¿‡åˆæˆå›¾åƒåŒ¹é…ä¼ªæ ‡ç­¾ï¼Œé¿å…æ”¹å–„ä¼ªæ ‡ç­¾çš„éœ€æ±‚ã€‚</li>
<li>SynMatchåˆ©ç”¨åŒä¸€åˆ†å‰²æ¨¡å‹ä¸­çš„çº¹ç†å’Œå½¢çŠ¶ç‰¹å¾è¿›è¡Œå›¾åƒåˆæˆã€‚</li>
<li>SynMatchåœ¨ä¸åŒåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä¹æ— ç›‘ç£å­¦ä¹ ç¯å¢ƒä¸‹è¡¨ç°çªå‡ºã€‚ä¾‹å¦‚åœ¨å¤šæ¯è‚‰åˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20340fa63a152c16c70226dbb73290d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5c33da184f3b280dbae42bc18717c23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7df99fa144f26aab1fdb079f5792695a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4fa79838adee433e9dd419fd1ec4172.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce72cedf61dadc6298767166a6f3139f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e990724e35edabab925dfb4c19994c95.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ASM-UNet-Adaptive-Scan-Mamba-Integrating-Group-Commonalities-and-Individual-Variations-for-Fine-Grained-Segmentation"><a href="#ASM-UNet-Adaptive-Scan-Mamba-Integrating-Group-Commonalities-and-Individual-Variations-for-Fine-Grained-Segmentation" class="headerlink" title="ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and   Individual Variations for Fine-Grained Segmentation"></a>ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and   Individual Variations for Fine-Grained Segmentation</h2><p><strong>Authors:Bo Wang, Mengyuan Xu, Yue Yan, Yuqun Yang, Kechen Shu, Wei Ping, Xu Tang, Wei Jiang, Zheng You</strong></p>
<p>Precise lesion resection depends on accurately identifying fine-grained anatomical structures. While many coarse-grained segmentation (CGS) methods have been successful in large-scale segmentation (e.g., organs), they fall short in clinical scenarios requiring fine-grained segmentation (FGS), which remains challenging due to frequent individual variations in small-scale anatomical structures. Although recent Mamba-based models have advanced medical image segmentation, they often rely on fixed manually-defined scanning orders, which limit their adaptability to individual variations in FGS. To address this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It introduces adaptive scan scores to dynamically guide the scanning order, generated by combining group-level commonalities and individual-level variations. Experiments on two public datasets (ACDC and Synapse) and a newly proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/YqunYang/ASM-UNet">https://github.com/YqunYang/ASM-UNet</a>. </p>
<blockquote>
<p>ç²¾ç¡®çš„ç—…å˜åˆ‡é™¤ä¾èµ–äºå¯¹ç²¾ç»†è§£å‰–ç»“æ„çš„å‡†ç¡®è¯†åˆ«ã€‚è™½ç„¶è®¸å¤šç²—ç²’åº¦åˆ†å‰²ï¼ˆCGSï¼‰æ–¹æ³•åœ¨å¤§è§„æ¨¡åˆ†å‰²ï¼ˆå¦‚å™¨å®˜ï¼‰ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨ä¸´åºŠåœºæ™¯ä¸­éœ€è¦è¿›è¡Œç²¾ç»†ç²’åº¦åˆ†å‰²ï¼ˆFGSï¼‰æ—¶ï¼Œå®ƒä»¬å°±æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚ç”±äºå°å°ºåº¦è§£å‰–ç»“æ„çš„ä¸ªä½“å˜å¼‚é¢‘ç¹ï¼Œç²¾ç»†ç²’åº¦åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„åŸºäºMambaçš„æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå›ºå®šçš„æ‰‹åŠ¨å®šä¹‰çš„æ‰«æé¡ºåºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹ç²¾ç»†ç²’åº¦åˆ†å‰²ä¸­ä¸ªä½“å·®å¼‚çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ASM-UNetï¼Œè¿™æ˜¯ä¸€ç§åŸºäºMambaçš„æ–°å‹ç²¾ç»†ç²’åº¦åˆ†å‰²æ¶æ„ã€‚å®ƒå¼•å…¥äº†è‡ªé€‚åº”æ‰«æå¾—åˆ†æ¥åŠ¨æ€å¼•å¯¼æ‰«æé¡ºåºï¼Œè¯¥å¾—åˆ†æ˜¯é€šè¿‡ç»“åˆç¾¤ç»„çº§åˆ«çš„å…±æ€§ä»¥åŠä¸ªä½“çº§åˆ«çš„å˜åŒ–ç”Ÿæˆçš„ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆACDCå’ŒSynapseï¼‰ä»¥åŠæ–°æå‡ºçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„èƒ†é“ç²¾ç»†ç²’åº¦åˆ†å‰²æ•°æ®é›†BTMSä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒASM-UNetåœ¨ç²—ç²’åº¦åˆ†å‰²å’Œç²¾ç»†ç²’åº¦åˆ†å‰²ä»»åŠ¡ä¸­éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YqunYang/ASM-UNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YqunYang/ASM-UNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07237v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMambaçš„æ–°å‹æ¶æ„ASM-UNetï¼Œç”¨äºç²¾ç»†ç²’åº¦åˆ†å‰²ï¼ˆFGSï¼‰ã€‚è¯¥æ¶æ„å¼•å…¥è‡ªé€‚åº”æ‰«æå¾—åˆ†æ¥åŠ¨æ€æŒ‡å¯¼æ‰«æé¡ºåºï¼Œè¯¥å¾—åˆ†ç»“åˆäº†ç¾¤ä½“çº§åˆ«å…±æ€§åŠä¸ªä½“çº§åˆ«å·®å¼‚ã€‚åœ¨ACDCã€Synapseä¸¤ä¸ªå…¬å¼€æ•°æ®é›†åŠæ–°æå‡ºçš„å…·æœ‰æŒ‘æˆ˜æ€§çš„èƒ†é“ç²¾ç»†åˆ†å‰²æ•°æ®é›†BTMSä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒASM-UNetåœ¨ç²—ç»†ç²’åº¦åˆ†å‰²ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>ç²¾ç»†ç²’åº¦åˆ†å‰²ï¼ˆFGSï¼‰åœ¨ä¸´åºŠåœºæ™¯ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°å°å°ºåº¦è§£å‰–ç»“æ„çš„ç²¾ç¡®è¯†åˆ«ã€‚</li>
<li>ä¼ ç»Ÿç²—ç²’åº¦åˆ†å‰²ï¼ˆCGSï¼‰æ–¹æ³•åœ¨å¤§è§„æ¨¡åˆ†å‰²ä¸­æˆåŠŸï¼Œä½†åœ¨FGSä¸­å› ä¸ªä½“å˜å¼‚è€Œå—é™ã€‚</li>
<li>Mambaæ¨¡å‹è™½åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰è¿›å±•ï¼Œä½†å›ºå®šæ‰«æé¡ºåºé™åˆ¶äº†å…¶é€‚åº”ä¸ªä½“å˜å¼‚çš„èƒ½åŠ›ã€‚</li>
<li>ASM-UNetæ˜¯ä¸€ç§æ–°å‹Mambaæ¶æ„ï¼Œå¼•å…¥è‡ªé€‚åº”æ‰«æå¾—åˆ†æ¥åŠ¨æ€è°ƒæ•´æ‰«æé¡ºåºã€‚</li>
<li>è‡ªé€‚åº”æ‰«æå¾—åˆ†ç»“åˆäº†ç¾¤ä½“çº§åˆ«å…±æ€§åŠä¸ªä½“çº§åˆ«å·®å¼‚ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†ASM-UNetåœ¨ç²—ç»†ç²’åº¦åˆ†å‰²ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7607dd9ead3ebff6c16166232341c719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-120de27681ed0db0947cfe7019c9f4e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e009ee8db36773dcc29cea6226272e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18047f7bb66017c2e9105be636facacf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce6617e133a8c07ce90ba3fa70a63a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b124a35318bdef6366f2c48ebd16df.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Early-Detection-of-Pancreatic-Cancer-Using-Multimodal-Learning-on-Electronic-Health-Record"><a href="#Early-Detection-of-Pancreatic-Cancer-Using-Multimodal-Learning-on-Electronic-Health-Record" class="headerlink" title="Early Detection of Pancreatic Cancer Using Multimodal Learning on   Electronic Health Record"></a>Early Detection of Pancreatic Cancer Using Multimodal Learning on   Electronic Health Record</h2><p><strong>Authors:Mosbah Aouad, Anirudh Choudhary, Awais Farooq, Steven Nevers, Lusine Demirkhanyan, Bhrandon Harris, Suguna Pappu, Christopher Gondi, Ravishankar Iyer</strong></p>
<p>Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MosbahAouad/EarlyPDAC-MML">https://github.com/MosbahAouad/EarlyPDAC-MML</a>. </p>
<blockquote>
<p>èƒ°è…ºç™Œå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰æ˜¯æœ€è‡´å‘½çš„ç™Œç—‡ä¹‹ä¸€ï¼Œç”±äºç¼ºå°‘ç‰¹å®šç—‡çŠ¶å’Œå¯é ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œæ—©æœŸæ£€æµ‹ä»æ˜¯ä¸´åºŠä¸Šçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†çºµå‘è¯Šæ–­ä»£ç å†å²å’Œç”µå­å¥åº·è®°å½•ä¸­å¸¸è§„æ”¶é›†çš„å®éªŒå®¤æµ‹é‡å€¼ï¼Œå¯åœ¨ä¸´åºŠè¯Šæ–­å‰ä¸€å¹´æ£€æµ‹åˆ°PDACã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç¥ç»æ§åˆ¶å¾®åˆ†æ–¹ç¨‹æ¥æ¨¡æ‹Ÿä¸è§„åˆ™çš„å®éªŒå®¤æ—¶é—´åºåˆ—æ•°æ®ã€é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œå¾ªç¯ç½‘ç»œæ¥å­¦ä¹ è¯Šæ–­ä»£ç è½¨è¿¹è¡¨ç¤ºï¼Œä»¥åŠäº¤å‰æ³¨æ„æœºåˆ¶æ¥æ•æ‰ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„äº¤äº’ã€‚æˆ‘ä»¬åœ¨åŒ…å«è¿‘4700åæ‚£è€…çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¼€å‘å’Œè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒAUCæœ‰6.5%è‡³15.5%çš„æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜ç¡®å®šäº†ä¸èƒ°è…ºç™Œé£é™©å¢åŠ ç›¸å…³çš„è¯Šæ–­ä»£ç å’Œå®éªŒå®¤æ£€æµ‹æ¿ï¼ŒåŒ…æ‹¬å·²çŸ¥å’Œæ–°ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/MosbahAouad/EarlyPDAC-MML">https://github.com/MosbahAouad/EarlyPDAC-MML</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06627v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆçºµå‘è¯Šæ–­ä»£ç å†å²å’Œç”µå­å¥åº·è®°å½•ä¸­å¸¸è§„æ”¶é›†çš„å®éªŒå®¤æµ‹é‡æ•°æ®ï¼Œå¯åœ¨ä¸´åºŠç¡®è¯Šå‰ä¸€å¹´é¢„æµ‹èƒ°è…ºç™Œã€‚è¯¥æ–¹æ³•ä½¿ç”¨ç¥ç»ç½‘ç»œæ§åˆ¶å¾®åˆ†æ–¹ç¨‹å¯¹ä¸è§„åˆ™å®éªŒå®¤æ—¶é—´åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œå¾ªç¯ç½‘ç»œå­¦ä¹ è¯Šæ–­ä»£ç è½¨è¿¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ•æ‰ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„äº¤äº’ã€‚åœ¨æ¥è¿‘4700åæ‚£è€…çš„çœŸå®æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•è¾ƒæœ€æ–°æŠ€æœ¯å–å¾—äº†AUCå€¼æé«˜6.5%è‡³15.5%çš„æ˜¾è‘—æˆæœã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ç¡®å®šäº†ä¸èƒ°è…ºç™Œé£é™©å‡é«˜çš„è¯Šæ–­ä»£ç å’Œå®éªŒå®¤æ£€æµ‹æ¿ï¼ŒåŒ…æ‹¬å·²çŸ¥å’Œæ–°ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼æ–¹æ³•ç”¨äºèƒ°è…ºç™Œæ—©æœŸæ£€æµ‹ã€‚</li>
<li>ç»“åˆçºµå‘è¯Šæ–­ä»£ç å†å²å’Œå®éªŒå®¤æµ‹é‡æ•°æ®ï¼Œå¯åœ¨ä¸´åºŠç¡®è¯Šå‰ä¸€å¹´è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ä½¿ç”¨ç¥ç»ç½‘ç»œæ§åˆ¶å¾®åˆ†æ–¹ç¨‹å¯¹ä¸è§„åˆ™å®éªŒå®¤æ—¶é—´åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œå¾ªç¯ç½‘ç»œå­¦ä¹ è¯Šæ–­ä»£ç è½¨è¿¹è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ•æ‰è¯Šæ–­ä»£ç å’Œå®éªŒå®¤æ•°æ®ä¹‹é—´çš„äº¤äº’ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e96ddaaf916f6eba61716c39f15a538.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e2cb4103950ebd603505b89311b34e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Hybrid-Approaches-for-Black-Hole-Spin-Estimation-From-Classical-Spectroscopy-to-Physics-Informed-Machine-Learning"><a href="#Hybrid-Approaches-for-Black-Hole-Spin-Estimation-From-Classical-Spectroscopy-to-Physics-Informed-Machine-Learning" class="headerlink" title="Hybrid Approaches for Black Hole Spin Estimation: From Classical   Spectroscopy to Physics-Informed Machine Learning"></a>Hybrid Approaches for Black Hole Spin Estimation: From Classical   Spectroscopy to Physics-Informed Machine Learning</h2><p><strong>Authors:Stella Menziltsidou</strong></p>
<p>The measurement of black hole spin is considered one of the key problems in relativistic astrophysics. Existing methods, such as continuum fitting, X-ray reflection spectroscopy and quasi-periodic oscillation analysis, have systematic limitations in accuracy, interpretability and scalability. In this work, a hybrid approach is proposed in which theoretical models based on the Teukolsky formalism are integrated with Physics-Informed Neural Networks (PINNs). A PINN model is developed to solve the linearized spin problem in the scalar case, with physical constraints directly embedded into the training process. Annotated data are not required; instead, the model is trained using the differential operator and boundary conditions as supervision. It is demonstrated that the PINN converges reliably, with residual loss values below 1e-7 and a root mean squared error (RMSE) of the order of 1e-6 (final approx 5.4 x 1e-8). Benchmarking results indicate that the proposed method outperforms both classical and data-driven machine learning approaches in terms of AUC and sensitivity, while also exhibiting superior interpretability, generalizability and adherence to physical principles, with moderate computational cost. Potential extensions include integration with general relativistic magnetohydrodynamics (GRMHD) solvers and application to real observational data. These findings support the viability of physics-based machine learning as a robust framework for accurate and interpretable black hole spin estimation. </p>
<blockquote>
<p>é»‘æ´è‡ªè½¬æµ‹é‡è¢«è®¤ä¸ºæ˜¯ç›¸å¯¹è®ºå¤©ä½“ç‰©ç†å­¦ä¸­çš„å…³é”®é—®é¢˜ä¹‹ä¸€ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚è¿ç»­è°±æ‹Ÿåˆã€Xå°„çº¿åå°„å…‰è°±å­¦å’Œå‡†å‘¨æœŸæŒ¯è¡åˆ†æï¼Œåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ç³»ç»Ÿå±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºTeukolskyå½¢å¼çš„ç†è®ºæ¨¡å‹ä¸ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰ã€‚å¼€å‘äº†ä¸€ä¸ªPINNæ¨¡å‹æ¥è§£å†³æ ‡é‡æƒ…å†µä¸‹çš„çº¿æ€§åŒ–è‡ªè½¬é—®é¢˜ï¼Œç‰©ç†çº¦æŸç›´æ¥åµŒå…¥åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚ä¸éœ€è¦æ³¨é‡Šæ•°æ®ï¼›ç›¸åï¼Œè¯¥æ¨¡å‹ä½¿ç”¨å¾®åˆ†ç®—å­å’Œè¾¹ç•Œæ¡ä»¶ä½œä¸ºç›‘ç£è¿›è¡Œè®­ç»ƒã€‚ç»“æœè¡¨æ˜ï¼ŒPINNæ”¶æ•›å¯é ï¼Œæ®‹å·®æŸå¤±å€¼ä½äº1e-7ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰çº¦ä¸º1e-6ï¼ˆæœ€ç»ˆè¿‘ä¼¼ä¸º5.4 x 1e-8ï¼‰ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AUCå’Œæ•æ„Ÿæ€§æ–¹é¢ä¼˜äºç»å…¸çš„å’Œæ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ŒåŒæ—¶è¡¨ç°å‡ºæ›´å¥½çš„å¯è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œå¯¹ç‰©ç†åŸåˆ™çš„éµå¾ªï¼Œè®¡ç®—æˆæœ¬é€‚ä¸­ã€‚æ½œåœ¨æ‰©å±•åŒ…æ‹¬ä¸å¹¿ä¹‰ç›¸å¯¹è®ºç£æµä½“åŠ¨åŠ›å­¦ï¼ˆGRMHDï¼‰æ±‚è§£å™¨é›†æˆä»¥åŠåº”ç”¨äºçœŸå®è§‚æµ‹æ•°æ®ã€‚è¿™äº›å‘ç°æ”¯æŒäº†åŸºäºç‰©ç†çš„æœºå™¨å­¦ä¹ ä½œä¸ºä¸€ä¸ªç¨³å¥æ¡†æ¶ï¼Œç”¨äºå‡†ç¡®å’Œå¯è§£é‡Šçš„é»‘æ´è‡ªè½¬ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06506v2">PDF</a> 8 pages, 4 figures, 4 tables, 4 equations</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†åŸºäºTeukolskyå½¢å¼å’Œç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰çš„æ··åˆæ–¹æ³•æ¥æµ‹é‡é»‘æ´è‡ªè½¬ã€‚å¼€å‘äº†ä¸€ä¸ªPINNæ¨¡å‹è§£å†³æ ‡é‡æƒ…å†µä¸‹çš„çº¿æ€§è‡ªè½¬é—®é¢˜ï¼Œå°†ç‰©ç†çº¦æŸç›´æ¥åµŒå…¥è®­ç»ƒè¿‡ç¨‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿå’ŒåŸºäºæ•°æ®é©±åŠ¨çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é»‘æ´è‡ªè½¬æµ‹é‡æ˜¯ç›¸å¯¹è®ºå¤©ä½“ç‰©ç†å­¦ä¸­çš„å…³é”®é—®é¢˜ä¹‹ä¸€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚è¿ç»­è°±æ‹Ÿåˆã€Xå°„çº¿åå°„å…‰è°±å­¦å’Œå‡†å‘¨æœŸæŒ¯è¡åˆ†æåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢å­˜åœ¨ç³»ç»Ÿå±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºTeukolskyå½¢å¼å’Œç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰çš„æ··åˆæ–¹æ³•ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªPINNæ¨¡å‹æ¥è§£å†³æ ‡é‡æƒ…å†µä¸‹çš„çº¿æ€§è‡ªè½¬é—®é¢˜ï¼Œå°†ç‰©ç†çº¦æŸç›´æ¥åµŒå…¥è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸éœ€è¦æ³¨é‡Šæ•°æ®ï¼Œè€Œæ˜¯ä½¿ç”¨å¾®åˆ†ç®—å­å’Œè¾¹ç•Œæ¡ä»¶ä½œä¸ºç›‘ç£è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¼˜äºä¼ ç»Ÿå’ŒåŸºäºæ•°æ®çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œä¾‹å¦‚ä¸å¹¿ä¹‰ç›¸å¯¹è®ºç£æµä½“åŠ¨åŠ›å­¦ï¼ˆGRMHDï¼‰æ±‚è§£å™¨é›†æˆï¼Œå¹¶åº”ç”¨äºå®é™…è§‚æµ‹æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6feba6c9c2c5df65438f45b44e14b427.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-138d03a49d2b72ee50dd5bbe65346cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccda0eaffb18446763ba25d7b49842bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d8317deacfe7448a9dc0bf60ef7fffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45c9428f1425f4c1511a4dd5b84952f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4ad339e4a098a01b7043a1cf95fb353.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DS-2-Net-Detail-Semantic-Deep-Supervision-Network-for-Medical-Image-Segmentation"><a href="#DS-2-Net-Detail-Semantic-Deep-Supervision-Network-for-Medical-Image-Segmentation" class="headerlink" title="DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image   Segmentation"></a>DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image   Segmentation</h2><p><strong>Authors:Zhaohong Huang, Yuxin Zhang, Taojian Zhou, Guorong Cai, Rongrong Ji</strong></p>
<p>Deep Supervision Networks exhibit significant efficacy for the medical imaging community. Nevertheless, existing work merely supervises either the coarse-grained semantic features or fine-grained detailed features in isolation, which compromises the fact that these two types of features hold vital relationships in medical image analysis. We advocate the powers of complementary feature supervision for medical image segmentation, by proposing a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates both low-level detailed and high-level semantic feature supervision through Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM respectively harness low-level and high-level feature maps to create detail and semantic masks for enhancing feature supervision. This is a novel shift from single-view deep supervision to multi-view deep supervision. DS$^2$Net is also equipped with a novel uncertainty-based supervision loss that adaptively assigns the supervision strength of features within distinct scales based on their uncertainty, thus circumventing the sub-optimal heuristic design that typifies previous works. Through extensive experiments on six benchmarks captured under either colonoscopy, ultrasound and microscope, we demonstrate that DS$^2$Net consistently outperforms state-of-the-art methods for medical image analysis. </p>
<blockquote>
<p>æ·±åº¦ç›‘ç£ç½‘ç»œå¯¹åŒ»å­¦å½±åƒç•Œå…·æœ‰æ˜¾è‘—æ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œä»…ä»…å•ç‹¬ç›‘ç£ç²—ç²’åº¦çš„è¯­ä¹‰ç‰¹å¾æˆ–ç»†ç²’åº¦çš„è¯¦ç»†ç‰¹å¾ï¼Œå¿½ç•¥äº†è¿™ä¸¤ç§ç‰¹å¾åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é‡è¦å…³ç³»çš„äº‹å®ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡ç»†èŠ‚è¯­ä¹‰æ·±åº¦ç›‘ç£ç½‘ç»œï¼ˆDS$^2$Netï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²çš„äº’è¡¥ç‰¹å¾ç›‘ç£ã€‚DS$^2$Neté€šè¿‡ç»†èŠ‚å¢å¼ºæ¨¡å—ï¼ˆDEMï¼‰å’Œè¯­ä¹‰å¢å¼ºæ¨¡å—ï¼ˆSEMï¼‰å¯¼èˆªä½çº§åˆ«çš„è¯¦ç»†ç‰¹å¾å’Œé«˜çº§åˆ«çš„è¯­ä¹‰ç‰¹å¾ç›‘ç£ã€‚DEMå’ŒSEMåˆ†åˆ«åˆ©ç”¨ä½çº§åˆ«å’Œé«˜çº§åˆ«çš„ç‰¹å¾å›¾åˆ›å»ºç»†èŠ‚å’Œè¯­ä¹‰æ©è†œï¼Œä»¥å¢å¼ºç‰¹å¾ç›‘ç£ã€‚è¿™æ˜¯ä¸€ç§ä»å•è§†å›¾æ·±åº¦ç›‘ç£åˆ°å¤šè§†å›¾æ·±åº¦ç›‘ç£çš„æ–°è½¬å˜ã€‚DS$^2$Netè¿˜é…å¤‡äº†ä¸€ç§æ–°å‹åŸºäºä¸ç¡®å®šæ€§çš„ç›‘ç£æŸå¤±ï¼Œè¯¥æŸå¤±å¯ä»¥æ ¹æ®ç‰¹å¾çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”åœ°åˆ†é…ä¸åŒå°ºåº¦å†…ç‰¹å¾çš„ç›‘ç£å¼ºåº¦ï¼Œä»è€Œé¿å…äº†ä»¥å‰å·¥ä½œä¸­å…¸å‹çš„æ¬¡ä¼˜å¯å‘å¼è®¾è®¡ã€‚é€šè¿‡åœ¨ç»“è‚ é•œã€è¶…å£°å’Œæ˜¾å¾®é•œä¸‹é‡‡é›†çš„å…­ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†DS$^2$Netåœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04131v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦ç›‘ç£ç½‘ç»œåœ¨åŒ»å­¦æˆåƒé¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—çš„æ•ˆæœã€‚ç°æœ‰å·¥ä½œé€šå¸¸åªç›‘ç£ç²—ç²’åº¦è¯­ä¹‰ç‰¹å¾æˆ–ç»†ç²’åº¦è¯¦ç»†ç‰¹å¾ï¼Œå¿½ç•¥äº†ä¸¤è€…ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»†èŠ‚è¯­ä¹‰æ·±åº¦ç›‘ç£ç½‘ç»œï¼ˆDS^2Netï¼‰ï¼Œé€šè¿‡ç»†èŠ‚å¢å¼ºæ¨¡å—ï¼ˆDEMï¼‰å’Œè¯­ä¹‰å¢å¼ºæ¨¡å—ï¼ˆSEMï¼‰è¿›è¡Œäº’è¡¥ç‰¹å¾ç›‘ç£ã€‚DS^2Netè¿˜é…å¤‡äº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§ç›‘ç£æŸå¤±ï¼Œæ ¹æ®ç‰¹å¾çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”åœ°åˆ†é…ä¸åŒå°ºåº¦çš„ç‰¹å¾ç›‘ç£å¼ºåº¦ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDS^2Netåœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç›‘ç£ç½‘ç»œåœ¨åŒ»å­¦æˆåƒé¢†åŸŸå…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦ç›‘ç£ç²—ç²’åº¦è¯­ä¹‰ç‰¹å¾æˆ–ç»†ç²’åº¦è¯¦ç»†ç‰¹å¾ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>ç»†èŠ‚è¯­ä¹‰æ·±åº¦ç›‘ç£ç½‘ç»œï¼ˆDS^2Netï¼‰ç»“åˆäº†ç»†èŠ‚å¢å¼ºæ¨¡å—ï¼ˆDEMï¼‰å’Œè¯­ä¹‰å¢å¼ºæ¨¡å—ï¼ˆSEMï¼‰è¿›è¡Œäº’è¡¥ç‰¹å¾ç›‘ç£ã€‚</li>
<li>DS^2Netä»å•è§†å›¾æ·±åº¦ç›‘ç£è½¬å‘å¤šè§†å›¾æ·±åº¦ç›‘ç£ã€‚</li>
<li>DS^2Neté…å¤‡äº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„ç›‘ç£æŸå¤±ï¼Œæ ¹æ®ç‰¹å¾çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”åˆ†é…ç›‘ç£å¼ºåº¦ã€‚</li>
<li>DS^2Netåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå§‹ç»ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†ææ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6071cb6daf9301d080994d3ab4328718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36455034a7cadfddbc87e837053a2210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-111bb76d0d970fccc1a81879799f9268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbccf6e1819059ed2d7eaceb4aba3bf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c008540d8e5195615f8fdd26c5d65594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48ea483f9e8b117e5de121c814510cbe.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-New-One-Shot-Federated-Learning-Framework-for-Medical-Imaging-Classification-with-Feature-Guided-Rectified-Flow-and-Knowledge-Distillation"><a href="#A-New-One-Shot-Federated-Learning-Framework-for-Medical-Imaging-Classification-with-Feature-Guided-Rectified-Flow-and-Knowledge-Distillation" class="headerlink" title="A New One-Shot Federated Learning Framework for Medical Imaging   Classification with Feature-Guided Rectified Flow and Knowledge Distillation"></a>A New One-Shot Federated Learning Framework for Medical Imaging   Classification with Feature-Guided Rectified Flow and Knowledge Distillation</h2><p><strong>Authors:Yufei Ma, Hanwen Zhang, Qiya Yang, Guibo Luo, Yuesheng Zhu</strong></p>
<p>In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted increasing attention due to its low communication overhead, requiring only a single round of transmission. However, existing generative model-based OSFL methods suffer from low training efficiency and potential privacy leakage in the healthcare domain. Additionally, achieving convergence within a single round of model aggregation is challenging under non-Independent and Identically Distributed (non-IID) data. To address these challenges, in this paper a modified OSFL framework is proposed, in which a new Feature-Guided Rectified Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation method are developed. FG-RF on the client side accelerates generative modeling in medical imaging scenarios while preserving privacy by synthesizing feature-level images rather than pixel-level images. To handle non-IID distributions, DLKD enables the global student model to simultaneously mimic the output logits and align the intermediate-layer features of client-side teacher models during aggregation. Experimental results on three non-IID medical imaging datasets show that our new framework and method outperform multi-round federated learning approaches, achieving up to 21.73% improvement, and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our experiments demonstrate that feature-level synthetic images significantly reduce privacy leakage risks compared to pixel-level synthetic images. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LMIAPC/one-shot-fl-medical">https://github.com/LMIAPC/one-shot-fl-medical</a>. </p>
<blockquote>
<p>åœ¨å¤šä¸­å¿ƒåœºæ™¯ä¸­ï¼Œç”±äºåªéœ€ä¸€è½®ä¼ è¾“ï¼Œä¸€æ¬¡è”é‚¦å­¦ä¹ ï¼ˆOSFLï¼‰å› å…¶ä½é€šä¿¡å¼€é”€è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºç”Ÿæˆæ¨¡å‹çš„OSFLæ–¹æ³•å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸæ½œåœ¨éšç§æ³„éœ²çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œåœ¨éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®ä¸‹ï¼Œåœ¨å•æ¬¡æ¨¡å‹èšåˆä¸­å®ç°æ”¶æ•›æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„OSFLæ¡†æ¶ï¼Œå…¶ä¸­å¼€å‘äº†ä¸€ç§æ–°çš„ç‰¹å¾å¼•å¯¼æ ¡æ­£æµæ¨¡å‹ï¼ˆFG-RFï¼‰å’ŒåŒå±‚çŸ¥è¯†è’¸é¦ï¼ˆDLKDï¼‰èšåˆæ–¹æ³•ã€‚å®¢æˆ·ç«¯çš„FG-RFåŠ é€ŸåŒ»å­¦æˆåƒåœºæ™¯ä¸­çš„ç”Ÿæˆå»ºæ¨¡ï¼Œé€šè¿‡åˆæˆç‰¹å¾çº§å›¾åƒè€Œä¸æ˜¯åƒç´ çº§å›¾åƒæ¥ä¿ç•™éšç§ã€‚ä¸ºäº†å¤„ç†éIIDåˆ†å¸ƒï¼ŒDLKDä½¿å…¨å±€å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨èšåˆè¿‡ç¨‹ä¸­åŒæ—¶æ¨¡ä»¿è¾“å‡ºé€»è¾‘å’Œä¸å®¢æˆ·ç«¯æ•™å¸ˆæ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾å¯¹é½ã€‚åœ¨ä¸‰ä¸ªéIIDåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–°æ¡†æ¶å’Œæ–¹æ³•ä¼˜äºå¤šè½®è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾21.73%çš„æ”¹è¿›ï¼Œå¹¶ä¸”æ¯”åŸºçº¿FedISCAå¹³å‡é«˜å‡º21.75%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸åƒç´ çº§åˆæˆå›¾åƒç›¸æ¯”ï¼Œç‰¹å¾çº§åˆæˆå›¾åƒæ˜¾è‘—é™ä½äº†éšç§æ³„éœ²é£é™©ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LMIAPC/one-shot-fl-medical%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LMIAPC/one-shot-fl-medicalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19045v2">PDF</a> Accepted at ECAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ä¸€æ¬¡æ€§è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œé’ˆå¯¹åŒ»ç–—å½±åƒåœºæ™¯ï¼Œé€šè¿‡ç‰¹å¾å¼•å¯¼ä¿®æ­£æµæ¨¡å‹å’ŒåŒå±‚çŸ¥è¯†è’¸é¦èšåˆæ–¹æ³•ï¼Œè§£å†³ç°æœ‰ç”Ÿæˆæ¨¡å‹å­˜åœ¨çš„è®­ç»ƒæ•ˆç‡ä½å’Œéšç§æ³„éœ²é—®é¢˜ã€‚ç‰¹å¾å¼•å¯¼ä¿®æ­£æµæ¨¡å‹åœ¨å®¢æˆ·ç«¯åŠ é€Ÿç”Ÿæˆæ¨¡å‹ï¼Œåˆæˆç‰¹å¾çº§å›¾åƒä»¥ä¿ç•™éšç§ã€‚åŒå±‚çŸ¥è¯†è’¸é¦è§£å†³äº†éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®çš„èšåˆé—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–°æ–¹æ³•åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒçš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå¤šè½®è”é‚¦å­¦ä¹ ï¼Œå¹¶é™ä½äº†éšç§æ³„éœ²é£é™©ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„å•æ¬¡è”é‚¦å­¦ä¹ æ¡†æ¶æ¥è§£å†³å¤šä¸­å¿ƒåœºæ™¯ä¸‹çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŒ»ç–—å½±åƒçš„å¤„ç†ã€‚</li>
<li>æå‡ºäº†ç‰¹å¾å¼•å¯¼ä¿®æ­£æµæ¨¡å‹ï¼ˆFG-RFï¼‰ï¼Œåœ¨å®¢æˆ·ç«¯åŠ é€Ÿç”Ÿæˆæ¨¡å‹ï¼Œå¹¶é€šè¿‡åˆæˆç‰¹å¾çº§å›¾åƒæ¥ä¿ç•™éšç§ã€‚</li>
<li>å¼•å…¥åŒå±‚çŸ¥è¯†è’¸é¦ï¼ˆDLKDï¼‰æ–¹æ³•ä»¥è§£å†³éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®çš„èšåˆæŒ‘æˆ˜ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ–°çš„æ¡†æ¶å’Œæ–¹æ³•åœ¨ä¸‰ä¸ªéç‹¬ç«‹åŒåˆ†å¸ƒçš„åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤šè½®è”é‚¦å­¦ä¹ ï¼Œå¹³å‡æå‡è¾¾21.75%ã€‚</li>
<li>ç‰¹å¾çº§åˆæˆå›¾åƒç›¸è¾ƒäºåƒç´ çº§åˆæˆå›¾åƒï¼Œæ˜¾è‘—é™ä½äº†éšç§æ³„éœ²é£é™©ã€‚</li>
<li>è®ºæ–‡æä¾›äº†å®éªŒä»£ç ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹ä¸€æ¬¡æ€§è”é‚¦å­¦ä¹ åœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„åº”ç”¨å…·æœ‰æ½œåœ¨çš„æ¨åŠ¨ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8a5225a4b53ba7e85fff710e024accf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de4118f4169489d5ce47586302563715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee25423ca2994f1b8bd4bcd20a4a314.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54e0816bad6a5211d1204aaa8e742eb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ac20713719ab573b2d7c8e1f49668f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61f2875ae73ddd7eb1fa8695e4cdc67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cafebc997e580538ad0a6b15b197e009.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Are-Vision-Foundation-Models-Ready-for-Out-of-the-Box-Medical-Image-Registration"><a href="#Are-Vision-Foundation-Models-Ready-for-Out-of-the-Box-Medical-Image-Registration" class="headerlink" title="Are Vision Foundation Models Ready for Out-of-the-Box Medical Image   Registration?"></a>Are Vision Foundation Models Ready for Out-of-the-Box Medical Image   Registration?</h2><p><strong>Authors:Hanxue Gu, Yaqian Chen, Nicholas Konz, Qihang Li, Maciej A. Mazurowski</strong></p>
<p>Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable anatomy. Breast MRI registration is particularly difficult due to significant anatomical variation between patients, deformation caused by patient positioning, and the presence of thin and complex internal structure of fibroglandular tissue, where accurate alignment is crucial. Whether foundation model-based registration algorithms can address this level of complexity remains an open question. In this study, we provide a comprehensive evaluation of foundation model-based registration algorithms for breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM, MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that capture variations in different years and dates, sequences, modalities, and patient disease status (lesion versus no lesion). Our results show that foundation model-based algorithms such as SAM outperform traditional registration baselines for overall breast alignment, especially under large domain shifts, but struggle with capturing fine details of fibroglandular tissue. Interestingly, additional pre-training or fine-tuning on medical or breast-specific images in MedSAM and SSLSAM, does not improve registration performance and may even decrease it in some cases. Further work is needed to understand how domain-specific training influences registration and to explore targeted strategies that improve both global alignment and fine structure accuracy. We also publicly release our code at \href{<a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/Foundation-based-reg%7D%7BGithub%7D">https://github.com/mazurowski-lab/Foundation-based-reg}{Github}</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹å›¾åƒæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ•è·ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºï¼Œæœ€è¿‘åœ¨é›¶æ ·æœ¬å›¾åƒæ³¨å†Œä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å¤§å¤šæ˜¯åœ¨åˆšæ€§æˆ–è¾ƒç®€å•çš„ç»“æ„èƒŒæ™¯ä¸‹æµ‹è¯•çš„ï¼Œå¦‚å¤§è„‘æˆ–è…¹éƒ¨å™¨å®˜ï¼Œå°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ã€å¯å˜å½¢çš„ç»“æ„ã€‚ä¹³æˆ¿MRIæ³¨å†Œç‰¹åˆ«å›°éš¾ï¼Œå› ä¸ºæ‚£è€…ä¹‹é—´å­˜åœ¨é‡å¤§çš„è§£å‰–å˜å¼‚ã€ç”±æ‚£è€…å®šä½å¼•èµ·çš„å˜å½¢ï¼Œä»¥åŠçº¤ç»´è…ºä½“ç»„ç»‡çš„è–„è€Œå¤æ‚çš„å†…éƒ¨ç»“æ„ï¼Œå‡†ç¡®å¯¹é½æ˜¯å…³é”®çš„ã€‚åŸºäºæ¨¡å‹çš„æ³¨å†Œç®—æ³•æ˜¯å¦èƒ½åº”å¯¹è¿™ç§å¤æ‚ç¨‹åº¦ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºæ¨¡å‹çš„ä¹³æˆ¿MRIæ³¨å†Œç®—æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†äº”ç§é¢„è®­ç»ƒçš„ç¼–ç å™¨ï¼ŒåŒ…æ‹¬DINO-v2ã€SAMã€MedSAMã€SSLSAMå’ŒMedCLIPï¼Œæ¶µç›–äº†å››ä¸ªå…³é”®çš„ä¹³æˆ¿æ³¨å†Œä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ•æ‰äº†ä¸åŒå¹´ä»½å’Œæ—¥æœŸçš„å˜åŒ–ã€åºåˆ—ã€æ¨¡æ€å’Œæ‚£è€…çš„ç–¾ç—…çŠ¶æ€ï¼ˆç—…å˜ä¸éç—…å˜ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¨¡å‹çš„ç®—æ³•ï¼Œå¦‚SAMï¼Œåœ¨æ€»ä½“ä¹³æˆ¿å¯¹é½æ–¹é¢ä¼˜äºä¼ ç»Ÿæ³¨å†ŒåŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§åŸŸåç§»çš„æƒ…å†µä¸‹ï¼Œä½†åœ¨æ•æ‰çº¤ç»´è…ºä½“ç»„ç»‡çš„ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨MedSAMå’ŒSSLSAMä¸­å¯¹åŒ»ç–—æˆ–ä¹³æˆ¿ç‰¹å®šå›¾åƒè¿›è¡Œé¢å¤–çš„é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œå¹¶ä¸ä¼šæ”¹å–„æ³¨å†Œæ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ä¼šé™ä½æ€§èƒ½ã€‚éœ€è¦è¿›ä¸€æ­¥çš„å·¥ä½œæ¥ç†è§£åŸŸç‰¹å®šè®­ç»ƒå¯¹æ³¨å†Œçš„å½±å“ï¼Œå¹¶æ¢ç´¢æ—¨åœ¨æé«˜å…¨å±€å¯¹é½å’Œç²¾ç»†ç»“æ„å‡†ç¡®æ€§çš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨Githubä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11569v2">PDF</a> 3 figures, 9 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºåŸºç¡€æ¨¡å‹çš„å›¾åƒé…å‡†ç®—æ³•åœ¨ä¹³è…ºMRIå›¾åƒé…å‡†ä¸­çš„åº”ç”¨ã€‚å®éªŒè¯„ä¼°äº†äº”ç§é¢„è®­ç»ƒç¼–ç å™¨åœ¨å››é¡¹ä¹³è…ºé…å‡†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‘ç°åŸºäºåŸºç¡€æ¨¡å‹çš„ç®—æ³•å¦‚SAMåœ¨æ•´ä½“ä¹³è…ºå¯¹é½æ–¹é¢è¡¨ç°ä¼˜äºä¼ ç»Ÿé…å‡†åŸºçº¿ï¼Œä½†åœ¨æ•æ‰ä¹³è…ºç»„ç»‡çš„ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œå¯¹äºç‰¹å®šåŒ»å­¦æˆ–ä¹³è…ºå›¾åƒçš„é¢„è®­ç»ƒæˆ–å¾®è°ƒå¹¶ä¸ä¸€å®šèƒ½æé«˜é…å‡†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒé…å‡†ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œå°¤å…¶æ˜¯ç”¨äºå¤„ç†å¤æ‚ã€å¯å˜å½¢ç»“æ„çš„å›¾åƒã€‚</li>
<li>ä¹³è…ºMRIé…å‡†é¢ä¸´æ‚£è€…é—´è§£å‰–ç»“æ„å·®å¼‚ã€æ‚£è€…å®šä½å¼•èµ·çš„å˜å½¢ä»¥åŠçº¤ç»´è…ºä½“ç»„ç»‡å†…éƒ¨å¤æ‚ç»“æ„ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†äº”ç§é¢„è®­ç»ƒç¼–ç å™¨åœ¨ä¹³è…ºMRIé…å‡†ä¸­çš„æ€§èƒ½ï¼Œæ¶‰åŠå››é¡¹å…³é”®ä»»åŠ¡ã€‚</li>
<li>åŸºäºåŸºç¡€æ¨¡å‹çš„ç®—æ³•å¦‚SAMåœ¨æ•´ä½“ä¹³è…ºå¯¹é½æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä½†åœ¨ç²¾ç»†ç»“æ„å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å®šåŒ»å­¦æˆ–ä¹³è…ºå›¾åƒçš„é¢„è®­ç»ƒæˆ–å¾®è°ƒä¸ä¸€å®šä¼šæé«˜é…å‡†æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½é™ä½æ€§èƒ½ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥äº†è§£é¢†åŸŸç‰¹å®šè®­ç»ƒå¯¹é…å‡†çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0f698bc11291de50006c8a62fd2ee02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91b7fa1349e6a1f15cfd3d77835a5f11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7004f333db841beb0d5302f1de7006f7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a66073a4b3bffcb248261761304cf33a.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Scalable Controllable Accented TTS
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-803ce6d4e16748d387890020f2bd54c5.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Training-Free Text-Guided Color Editing with Multi-Modal Diffusion   Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
