<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-08-14  Selection of Layers from Self-supervised Learning Models for Predicting   Mean-Opinion-Score of Speech">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f5c476151d0e5f6a6f049e72b9a92265.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-14-更新"><a href="#2025-08-14-更新" class="headerlink" title="2025-08-14 更新"></a>2025-08-14 更新</h1><h2 id="Selection-of-Layers-from-Self-supervised-Learning-Models-for-Predicting-Mean-Opinion-Score-of-Speech"><a href="#Selection-of-Layers-from-Self-supervised-Learning-Models-for-Predicting-Mean-Opinion-Score-of-Speech" class="headerlink" title="Selection of Layers from Self-supervised Learning Models for Predicting   Mean-Opinion-Score of Speech"></a>Selection of Layers from Self-supervised Learning Models for Predicting   Mean-Opinion-Score of Speech</h2><p><strong>Authors:Xinyu Liang, Fredrik Cumlin, Victor Ungureanu, Chandan K. A. Reddy, Christian Schuldt, Saikat Chatterjee</strong></p>
<p>Self-supervised learning (SSL) models like Wav2Vec2, HuBERT, and WavLM have been widely used in speech processing. These transformer-based models consist of multiple layers, each capturing different levels of representation. While prior studies explored their layer-wise representations for efficiency and performance, speech quality assessment (SQA) models predominantly rely on last-layer features, leaving intermediate layers underexamined. In this work, we systematically evaluate different layers of multiple SSL models for predicting mean-opinion-score (MOS). Features from each layer are fed into a lightweight regression network to assess effectiveness. Our experiments consistently show early-layers features outperform or match those from the last layer, leading to significant improvements over conventional approaches and state-of-the-art MOS prediction models. These findings highlight the advantages of early-layer selection, offering enhanced performance and reduced system complexity. </p>
<blockquote>
<p>自监督学习（SSL）模型，如Wav2Vec2、HuBERT和WavLM，在语音处理中得到了广泛应用。这些基于变压器的模型由多层组成，每一层都捕获不同级别的表示。虽然先前的研究探讨了它们的分层表示以提高效率和性能，但语音质量评估（SQA）模型主要依赖于最后一层的特征，而对中间层的研究不足。在这项工作中，我们系统地评估了多个SSL模型的不同层，以预测平均意见得分（MOS）。每一层的特征输入到一个轻量级的回归网络中进行有效性评估。我们的实验一直显示，早期层的特征在性能上优于或等同于最后一层的特征，这相对于传统方法和最先进的MOS预测模型有明显的改进。这些发现强调了早期层选择的优点，提供了增强的性能和降低了系统复杂性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08962v1">PDF</a> Accepted at IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了自监督学习（SSL）模型在语音处理中的应用，如Wav2Vec2、HuBERT和WavLM等。这些基于transformer的模型的多层结构在不同的层次上捕获表征。尽管之前的研究已经探讨了其分层表示以提高效率和性能，但语音质量评估（SQA）模型主要依赖于最后一层的特征，而忽略了对中间层的探索。本研究系统地评估了多个SSL模型的不同层次在预测平均意见得分（MOS）方面的能力。实验结果表明，早期层的特征在预测MOS时表现出优异性能，甚至超过最后一层的特征，显著优于传统方法和最先进的MOS预测模型。这些发现强调了选择早期层次的优势，能够在提高性能的同时降低系统复杂性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习（SSL）模型广泛应用于语音处理领域。</li>
<li>SSL模型的多层结构能够捕获不同的层次表征。</li>
<li>语音质量评估（SQA）模型主要依赖最后一层的特征，但中间层尚未得到充分研究。</li>
<li>研究系统地评估了多个SSL模型的不同层次在预测平均意见得分（MOS）方面的能力。</li>
<li>实验结果表明早期层的特征在预测MOS时表现出优异性能。</li>
<li>早期层的特征显著优于传统方法和最先进的MOS预测模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08962">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-91af6f6a99ea5ec6092f2b9ccef72175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f96704ed68b5b76e3654b29902a07a8e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a23b19cbcd9ca7c7cc1a1a761d0bc594.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08253cf1ece84cc825d5336deb43885d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd7a9a529da0acf608e178ebf265ea91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad23f6cc88ad544383dbb5c3dbe6e8d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3181fbdf076aa0a9e10c91aa814544a2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Munsit-at-NADI-2025-Shared-Task-2-Pushing-the-Boundaries-of-Multidialectal-Arabic-ASR-with-Weakly-Supervised-Pretraining-and-Continual-Supervised-Fine-tuning"><a href="#Munsit-at-NADI-2025-Shared-Task-2-Pushing-the-Boundaries-of-Multidialectal-Arabic-ASR-with-Weakly-Supervised-Pretraining-and-Continual-Supervised-Fine-tuning" class="headerlink" title="Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of   Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual   Supervised Fine-tuning"></a>Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of   Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual   Supervised Fine-tuning</h2><p><strong>Authors:Mahmoud Salhab, Shameed Sait, Mohammad Abusheikh, Hasan Abusheikh</strong></p>
<p>Automatic speech recognition (ASR) plays a vital role in enabling natural human-machine interaction across applications such as virtual assistants, industrial automation, customer support, and real-time transcription. However, developing accurate ASR systems for low-resource languages like Arabic remains a significant challenge due to limited labeled data and the linguistic complexity introduced by diverse dialects. In this work, we present a scalable training pipeline that combines weakly supervised learning with supervised fine-tuning to develop a robust Arabic ASR model. In the first stage, we pretrain the model on 15,000 hours of weakly labeled speech covering both Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the subsequent stage, we perform continual supervised fine-tuning using a mixture of filtered weakly labeled data and a small, high-quality annotated dataset. Our approach achieves state-of-the-art results, ranking first in the multi-dialectal Arabic ASR challenge. These findings highlight the effectiveness of weak supervision paired with fine-tuning in overcoming data scarcity and delivering high-quality ASR for low-resource, dialect-rich languages. </p>
<blockquote>
<p>自动语音识别（ASR）在虚拟助手、工业自动化、客户支持和实时转录等应用中发挥着至关重要的作用，实现了自然的人机交互。然而，由于标注数据有限和多种方言带来的语言复杂性，为阿拉伯语等低资源语言开发准确的ASR系统仍然是一个重大挑战。在这项工作中，我们提出了一种可扩展的训练流程，该流程结合了弱监督学习与监督微调来开发稳健的阿拉伯语ASR模型。在第一阶段，我们在15000小时的弱标签语音上对模型进行训练，这些语音涵盖了现代标准阿拉伯语（MSA）和各种方言阿拉伯语（DA）的变体。在随后的阶段，我们使用过滤后的弱标签数据和一小部分高质量注释数据集进行持续监督微调。我们的方法取得了最新结果，在多方言阿拉伯语ASR挑战中排名第一。这些发现突出了弱监督与微调相结合在克服数据稀缺问题、为资源贫乏、方言丰富的语言提供高质量ASR方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08912v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对阿拉伯语自动语音识别（ASR）的挑战，提出了一种结合弱监督学习与监督微调的可扩展训练管道。首先，在涵盖现代标准阿拉伯语（MSA）和各种方言阿拉伯语（DA）变体的15，000小时弱标签语音上预训练模型。然后，使用过滤后的弱标签数据和一小部分高质量注释数据集进行持续的监督微调。该方法在多方言阿拉伯语ASR挑战中排名第一，实现了最佳结果，突显了弱监督与微调配对在克服数据稀缺和为低资源、方言丰富的语言提供高质量ASR方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）在虚拟助理、工业自动化、客户支持和实时转录等应用中扮演着至关重要的角色。</li>
<li>对于像阿拉伯语这样的低资源语言，开发准确的ASR系统是一个重大挑战，因为存在有限的有标签数据和由不同方言引起的语言复杂性。</li>
<li>本文提出了一种结合弱监督学习与监督细调的可持续训练管道，用于开发稳健的阿拉伯语ASR模型。</li>
<li>在预训练阶段，模型在涵盖现代标准阿拉伯语和多种方言阿拉伯语的弱标签语音上进行训练。</li>
<li>在微调阶段，使用过滤后的弱标签数据和高质量注释数据集进行持续的监督微调。</li>
<li>该方法在多方言阿拉伯语ASR挑战中取得了最佳结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08912">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b9284af1168ff16ab8b2093d057f1b8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c14f22d72a4c73ffbef0844be62ebc78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b71987cd5d6544526ac3b42703e9f22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf92c8220bbb154a8d9c89d2bb1b4ccb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transient-Noise-Removal-via-Diffusion-based-Speech-Inpainting"><a href="#Transient-Noise-Removal-via-Diffusion-based-Speech-Inpainting" class="headerlink" title="Transient Noise Removal via Diffusion-based Speech Inpainting"></a>Transient Noise Removal via Diffusion-based Speech Inpainting</h2><p><strong>Authors:Mordehay Moradi, Sharon Gannot</strong></p>
<p>In this paper, we present PGDI, a diffusion-based speech inpainting framework for restoring missing or severely corrupted speech segments. Unlike previous methods that struggle with speaker variability or long gap lengths, PGDI can accurately reconstruct gaps of up to one second in length while preserving speaker identity, prosody, and environmental factors such as reverberation. Central to this approach is classifier guidance, specifically phoneme-level guidance, which substantially improves reconstruction fidelity. PGDI operates in a speaker-independent manner and maintains robustness even when long segments are completely masked by strong transient noise, making it well-suited for real-world applications, such as fireworks, door slams, hammer strikes, and construction noise. Through extensive experiments across diverse speakers and gap lengths, we demonstrate PGDI’s superior inpainting performance and its ability to handle challenging acoustic conditions. We consider both scenarios, with and without access to the transcript during inference, showing that while the availability of text further enhances performance, the model remains effective even in its absence. For audio samples, visit: <a target="_blank" rel="noopener" href="https://mordehaym.github.io/PGDI/">https://mordehaym.github.io/PGDI/</a> </p>
<blockquote>
<p>本文介绍了PGDI，这是一种基于扩散的语音补全框架，用于恢复缺失或严重损坏的语音片段。与以往处理扬声器变化或长间隔困难的方法不同，PGDI可以准确重建长达一秒的间隔，同时保留说话人的身份、语调和环境因素，如混响。该方法的核心是分类器指导，特别是音素级指导，这大大提高了重建的保真度。PGDI以独立于说话人的方式运行，即使在长片段被强烈的瞬态噪声完全掩盖时也能保持稳健，因此非常适合现实世界的应用，如烟花、关门声、敲击声和建筑噪音等环境。我们通过对不同说话人和间隔长度的广泛实验，证明了PGDI的优越补全性能以及处理具有挑战性的声学条件的能力。我们考虑了两种情景，一种是在推理过程中有文本可访问，另一种是不可访问的。实验表明，虽然文本的可用性会进一步提高性能，但在没有文本的情况下，该模型仍然有效。若想试听样本，请访问：<a target="_blank" rel="noopener" href="https://mordehaym.github.io/PGDI/%E3%80%82">https://mordehaym.github.io/PGDI/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08890v1">PDF</a> 23 pages, 3 figures, signal processing paper on speech inpainting</p>
<p><strong>Summary</strong></p>
<p>本文介绍了PGDI，一个基于扩散的语音补全框架，用于恢复缺失或严重损坏的语音片段。该框架通过分类器引导，特别是音素级引导，实现了长达一秒的间隙准确重建，同时保留说话人身份、语调和环境因素如混响。PGDI对说话人具有独立性，即使在长时间段被强瞬态噪声完全掩盖时也能保持稳健性，非常适合现实应用，如烟花、关门声、敲击声和建筑噪音等场景。实验表明，PGDI具有出色的补全性能，并能应对具有挑战性的声学条件。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PGDI是一个基于扩散的语音补全框架，能够恢复缺失或损坏的语音片段。</li>
<li>该框架可以实现长达一秒的间隙准确重建，同时保留说话人身份、语调和环境特性。</li>
<li>通过分类器引导，特别是音素级引导，重建保真度得到了显著提高。</li>
<li>PGDI具有说话人独立性，可以在各种现实应用中使用，如处理烟花、关门声等场景中的语音。</li>
<li>实验证明PGDI在多种说话人和不同间隙长度下表现出优异的补全性能。</li>
<li>即使有文本可用时，该模型依然表现出良好的性能提升，但在没有文本的情况下也能保持有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-02eb8a1ab6d86d3beb3ed51d63d578ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a667289f70c5327b4a15acea458167.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Out-of-the-Box-into-the-Clinic-Evaluating-State-of-the-Art-ASR-for-Clinical-Applications-for-Older-Adults"><a href="#Out-of-the-Box-into-the-Clinic-Evaluating-State-of-the-Art-ASR-for-Clinical-Applications-for-Older-Adults" class="headerlink" title="Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for   Clinical Applications for Older Adults"></a>Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for   Clinical Applications for Older Adults</h2><p><strong>Authors:Bram van Dijk, Tiberon Kuiper, Sirin Aoulad si Ahmed, Armel Levebvre, Jake Johnson, Jan Duin, Simon Mooijaart, Marco Spruit</strong></p>
<p>Voice-controlled interfaces can support older adults in clinical contexts, with chatbots being a prime example, but reliable Automatic Speech Recognition (ASR) for underrepresented groups remains a bottleneck. This study evaluates state-of-the-art ASR models on language use of older Dutch adults, who interacted with the Welzijn.AI chatbot designed for geriatric contexts. We benchmark generic multilingual ASR models, and models fine-tuned for Dutch spoken by older adults, while also considering processing speed. Our results show that generic multilingual models outperform fine-tuned models, which suggests recent ASR models can generalise well out of the box to realistic datasets. Furthermore, our results suggest that truncating existing architectures is helpful in balancing the accuracy-speed trade-off, though we also identify some cases with high WER due to hallucinations. </p>
<blockquote>
<p>语音控制界面在临床环境中可以支持老年人群，聊天机器人就是一个很好的例子，但对于代表性不足的群体的可靠自动语音识别（ASR）仍然是一个瓶颈。本研究评估了最前沿的ASR模型在老年荷兰成人语言使用上的表现，这些受试者通过与针对老年情境设计的Welzijn.AI聊天机器人进行互动。我们评估了通用多语言ASR模型和针对老年荷兰人进行精细调整的模型，同时考虑了处理速度。我们的结果表明，通用多语言模型的表现优于精细调整的模型，这表明最近的ASR模型可以很好地推广应用到现实数据集。此外，我们的结果表明，缩短现有架构有助于平衡准确性与速度之间的权衡，尽管我们也发现了一些由于幻听导致的高词错误率（WER）的情况。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08684v1">PDF</a> </p>
<p><strong>Summary</strong><br>     语音控制界面能支持临床环境下的老年人群，聊天机器人是其中典型案例，但可靠的针对未代表群体的自动语音识别（ASR）仍是瓶颈。本研究评估了针对老年荷兰人与Welzijn.AI聊天机器人互动的语音识别模型的表现。我们对比了通用的多语种语音识别模型和针对荷兰老年人群微调过的模型，同时考虑了处理速度。结果表明，通用多语种模型优于精细调校的模型，这显示最新语音识别模型在现实数据集上具有良好的通用性。此外，我们的结果还表明，缩短现有架构有助于平衡准确性与速度之间的权衡，但也存在因误读而导致的某些高词错率情况。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音控制界面在临床环境中对老年人群的支持作用显著，聊天机器人是其中的重要应用。</li>
<li>自动语音识别（ASR）技术在未代表群体中的可靠性仍是瓶颈。</li>
<li>评估了针对老年荷兰人与Welzijn.AI聊天机器人互动的语音识别模型表现。</li>
<li>通用多语种语音识别模型在针对老年荷兰人群的互动中表现较好。</li>
<li>处理速度是语音识别技术在实际应用中的重要考量因素。</li>
<li>缩短现有语音识别模型的架构有助于平衡准确性与处理速度之间的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8de50a15a2acffdf2cea7d7a5dded44b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3f0e765b20c579a13401ddabca0288d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1d06b16604a4e8e1d08b682bd2aeb9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84db052b09175418de6266f0c5da88ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f45296273015c4d7e3344798f8a355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c476151d0e5f6a6f049e72b9a92265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db568e191a3e68dce6409a3068c883f0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Audio-Visual-Speech-Enhancement-Architectural-Design-and-Deployment-Strategies"><a href="#Audio-Visual-Speech-Enhancement-Architectural-Design-and-Deployment-Strategies" class="headerlink" title="Audio-Visual Speech Enhancement: Architectural Design and Deployment   Strategies"></a>Audio-Visual Speech Enhancement: Architectural Design and Deployment   Strategies</h2><p><strong>Authors:Anis Hamadouche, Haifeng Luo, Mathini Sellathurai, Tharm Ratnarajah</strong></p>
<p>This paper introduces a new AI-based Audio-Visual Speech Enhancement (AVSE) system and presents a comparative performance analysis of different deployment architectures. The proposed AVSE system employs convolutional neural networks (CNNs) for spectral feature extraction and long short-term memory (LSTM) networks for temporal modeling, enabling robust speech enhancement through multimodal fusion of audio and visual cues. Multiple deployment scenarios are investigated, including cloud-based, edge-assisted, and standalone device implementations. Their performance is evaluated in terms of speech quality improvement, latency, and computational overhead. Real-world experiments are conducted across various network conditions, including Ethernet, Wi-Fi, 4G, and 5G, to analyze the trade-offs between processing delay, communication latency, and perceptual speech quality. The results show that while cloud deployment achieves the highest enhancement quality, edge-assisted architectures offer the best balance between latency and intelligibility, meeting real-time requirements under 5G and Wi-Fi 6 conditions. These findings provide practical guidelines for selecting and optimizing AVSE deployment architectures in diverse applications, including assistive hearing devices, telepresence, and industrial communications. </p>
<blockquote>
<p>本文介绍了一个基于人工智能的视听语音增强（AVSE）系统，并对不同的部署架构进行了性能对比分析。所提出的AVSE系统采用卷积神经网络（CNN）进行频谱特征提取和长短时记忆（LSTM）网络进行时间建模，通过音频和视觉线索的多模式融合实现稳健的语音增强。研究了多种部署场景，包括基于云、边缘辅助和独立设备实现。它们的性能评估指标包括语音质量改进、延迟和计算开销。实验在各种网络条件下进行，包括以太网、Wi-Fi、4G和5G，分析了处理延迟、通信延迟和感知语音质量之间的权衡。结果表明，虽然云部署达到了最高的增强质量，但边缘辅助架构在延迟和清晰度之间达到了最佳平衡，满足了5G和Wi-Fi 6条件下的实时要求。这些发现为在辅助听力设备、远程出席和工业通信等多样化应用中选择和优化AVSE部署架构提供了实用指南。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08468v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>一种新的基于人工智能的视听语音增强（AVSE）系统被引入，并对比分析了不同的部署架构性能。该系统利用卷积神经网络（CNN）进行频谱特征提取和长短时记忆（LSTM）网络进行时间建模，通过音频和视觉线索的多模态融合实现稳健的语音增强。研究涵盖了云、边缘计算和独立设备等多种部署场景，并从语音质量提升、延迟和计算开销等方面进行评估。现实实验表明，尽管云部署在增强质量方面表现最佳，但边缘辅助架构在延迟和清晰度之间达到最佳平衡，满足5G和Wi-Fi 6条件下的实时要求。这些发现对于在多种应用中选择和优化AVSE部署架构具有实际指导意义，如助听设备、远程出席和工业通信等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新AI视听语音增强系统通过融合音频和视觉线索实现稳健语音增强。</li>
<li>系统采用卷积神经网络进行频谱特征提取和长短时记忆网络进行时间建模。</li>
<li>对比了不同部署架构性能，包括云、边缘计算和独立设备部署。</li>
<li>实验表明云部署在增强质量方面表现最佳，边缘辅助架构在延迟和清晰度之间达到最佳平衡。</li>
<li>5G和Wi-Fi 6条件下的实时性能需求可通过边缘辅助架构满足。</li>
<li>该系统具有广泛的应用前景，如助听设备、远程出席和工业通信等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e61164a36f196b2eb0e7cfe7b10ce102.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afedbdce8a1d4305cf9e8abd57039ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ef9af335eee71b1785242e713a936a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6254d4266553438d1cfa45cd73625715.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9586ef03492794aa8e0f730d8d41bd07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c35e18767d51e25f783a9bfb349f2abb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="XEmoRAG-Cross-Lingual-Emotion-Transfer-with-Controllable-Intensity-Using-Retrieval-Augmented-Generation"><a href="#XEmoRAG-Cross-Lingual-Emotion-Transfer-with-Controllable-Intensity-Using-Retrieval-Augmented-Generation" class="headerlink" title="XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation"></a>XEmoRAG: Cross-Lingual Emotion Transfer with Controllable Intensity   Using Retrieval-Augmented Generation</h2><p><strong>Authors:Tianlun Zuo, Jingbin Hu, Yuke Li, Xinfa Zhu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie</strong></p>
<p>Zero-shot emotion transfer in cross-lingual speech synthesis refers to generating speech in a target language, where the emotion is expressed based on reference speech from a different source language. However, this task remains challenging due to the scarcity of parallel multilingual emotional corpora, the presence of foreign accent artifacts, and the difficulty of separating emotion from language-specific prosodic features. In this paper, we propose XEmoRAG, a novel framework to enable zero-shot emotion transfer from Chinese to Thai using a large language model (LLM)-based model, without relying on parallel emotional data. XEmoRAG extracts language-agnostic emotional embeddings from Chinese speech and retrieves emotionally matched Thai utterances from a curated emotional database, enabling controllable emotion transfer without explicit emotion labels. Additionally, a flow-matching alignment module minimizes pitch and duration mismatches, ensuring natural prosody. It also blends Chinese timbre into the Thai synthesis, enhancing rhythmic accuracy and emotional expression, while preserving speaker characteristics and emotional consistency. Experimental results show that XEmoRAG synthesizes expressive and natural Thai speech using only Chinese reference audio, without requiring explicit emotion labels. These results highlight XEmoRAG’s capability to achieve flexible and low-resource emotional transfer across languages. Our demo is available at <a target="_blank" rel="noopener" href="https://tlzuo-lesley.github.io/Demo-page/">https://tlzuo-lesley.github.io/Demo-page/</a> . </p>
<blockquote>
<p>跨语言语音合成中的零样本情感迁移是指生成目标语言中的语音，其中情感表达是基于来自不同源语言的参考语音。然而，由于缺乏并行多语言情感语料库、存在外来口音痕迹和从特定语言的韵律特征中分离情感的难度，此任务仍然具有挑战性。在本文中，我们提出了XEmoRAG，这是一个无需依赖并行情感数据，能够实现从中文到泰语零样本情感迁移的新型框架，它基于大型语言模型（LLM）。XEmoRAG从中文语音中提取与语言无关的情感嵌入，并从精选的情感数据库中检索情感匹配的泰语短语，从而实现可控的情感迁移，无需明确的情感标签。此外，流匹配对齐模块最小化音高和持续时间不匹配，确保自然的韵律。它将中文音色融入泰语合成中，提高了节奏准确性和情感表达，同时保留了说话人的特点和情感一致性。实验结果表明，XEmoRAG仅使用中文参考音频就能合成出表达力强、自然的泰语语音，且无需明确的情感标签。这些结果突出了XEmoRAG在不同语言之间实现灵活和低资源情感迁移的能力。我们的演示网站为：<a target="_blank" rel="noopener" href="https://tlzuo-lesley.github.io/Demo-page/">https://tlzuo-lesley.github.io/Demo-page/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07302v2">PDF</a> Accepted by ASRU 2025</p>
<p><strong>Summary</strong><br>中文零基础上跨语言语音合成中的情感转移是一项挑战。本研究提出了XEmoRAG框架，利用大型语言模型实现中文到泰语的零基础上情感转移，无需依赖平行情感数据。XEmoRAG从中文语音中提取语言无关的情感嵌入，并从精选的情感数据库中检索情感匹配的泰语片段。此外，XEmoRAG的流匹配对齐模块减少了音调与时长的不匹配，确保自然韵律。实验结果显示XEmoRAG能使用仅中文参考音频合成出表达自然的泰语语音，无需明确的情感标签。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零基础上跨语言语音合成中的情感转移是困难的，主要是由于缺乏平行多语言情感语料库和语言特有的韵律特征带来的挑战。</li>
<li>本研究提出了XEmoRAG框架，实现了中文到泰语的零基础上情感转移，无需依赖平行情感数据。</li>
<li>XEmoRAG能从中文语音中提取语言无关的情感嵌入，并从情感数据库中检索匹配的泰语片段。</li>
<li>XEmoRAG采用流匹配对齐模块，减少音调与时长的不匹配，确保自然韵律。</li>
<li>XEmoRAG能够融合中文音色到泰语合成中，提高节奏准确性和情感表达。</li>
<li>实验结果显示XEmoRAG能够使用仅中文参考音频合成出表达自然的泰语语音，这一成果突出了XEmoRAG在低资源情境下实现灵活情感转移的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07302">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-603ebca286b6ad6e433a0b6bbfcd672d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0daff41d1070cfcf1d5a5b90f4586a40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a0cdefcd1982fc48151026be6bed18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98c446b879264ffffdd0598c0b4ee0ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3ec16fb34760d159088fc5013024250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4213671b3f8811c1be015ccc2c022048.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dopamine-Audiobook-A-Training-free-MLLM-Agent-for-Emotional-and-Immersive-Audiobook-Generation"><a href="#Dopamine-Audiobook-A-Training-free-MLLM-Agent-for-Emotional-and-Immersive-Audiobook-Generation" class="headerlink" title="Dopamine Audiobook: A Training-free MLLM Agent for Emotional and   Immersive Audiobook Generation"></a>Dopamine Audiobook: A Training-free MLLM Agent for Emotional and   Immersive Audiobook Generation</h2><p><strong>Authors:Yan Rong, Shan Yang, Chenxing Li, Dong Yu, Li Liu</strong></p>
<p>Audiobook generation aims to create rich, immersive listening experiences from multimodal inputs, but current approaches face three critical challenges: (1) the lack of synergistic generation of diverse audio types (e.g., speech, sound effects, and music) with precise temporal and semantic alignment; (2) the difficulty in conveying expressive, fine-grained emotions, which often results in machine-like vocal outputs; and (3) the absence of automated evaluation frameworks that align with human preferences for complex and diverse audio. To address these issues, we propose Dopamine Audiobook, a novel unified training-free multi-agent system, where a multimodal large language model (MLLM) serves two specialized roles (i.e., speech designer and audio designer) for emotional, human-like, and immersive audiobook generation and evaluation. Specifically, we firstly propose a flow-based, context-aware framework for diverse audio generation with word-level semantic and temporal alignment. To enhance expressiveness, we then design word-level paralinguistic augmentation, utterance-level prosody retrieval, and adaptive TTS model selection. Finally, for evaluation, we introduce a novel MLLM-based evaluation framework incorporating self-critique, perspective-taking, and psychological MagicEmo prompts to ensure human-aligned and self-aligned assessments. Experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance on multiple metrics. Importantly, our evaluation framework shows better alignment with human preferences and transferability across audio tasks. </p>
<blockquote>
<p>有声书生成旨在从多模式输入创建丰富、沉浸式的听觉体验，但当前方法面临三个关键挑战：（1）缺乏多样音频类型的协同生成（例如，语音、音效和音乐），缺乏精确的时间性和语义对齐；（2）在传达细致、微妙的情感方面存在困难，这往往导致机器般的语音输出；（3）缺乏与复杂和多样音频的人类偏好相一致的自动评估框架。为了解决这些问题，我们提出了多巴胺有声书，这是一种新型的统一、无需训练的多智能体系统，其中多模式大型语言模型（MLLM）扮演两个专业角色（即语音设计师和音频设计师），用于情感化、人性化、沉浸式的有声书生成和评估。具体来说，我们首先提出一个基于流的、上下文感知的框架，用于多样音频生成，实现单词级别的语义和时间对齐。然后，为了提高表达力，我们设计了单词级别的副语言增强、话语级别的语调检索和自适应的TTS模型选择。最后，在评估方面，我们引入了一个新型MLLM评估框架，包含自我批评、换位思考和心理MagicEmo提示，以确保人类和自我的一致评估。实验结果证明，我们的方法在多个指标上达到了最新技术水平。重要的是，我们的评估框架显示出更好的与人类偏好一致性以及在音频任务之间的可迁移性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11002v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了音频书生成的目标和当前面临的挑战，包括缺乏协同生成多种音频类型、难以表达精细情感以及缺乏与人类偏好相符的自动评估框架。为应对这些问题，提出了多巴胺音频书生成系统，该系统是一个无需训练的多代理系统，通过多模态大型语言模型实现情感化、人性化的沉浸式音频书生成与评估。具体方法包括基于流的上下文感知框架进行多样化的音频生成、增强表达性的语言模型设计以及基于自适应文本转语音模型的构建。同时，引入了一种新的评估框架，通过自我批评、视角转换和心理魔法提示等手段确保与人类偏好相符的自我评估。实验结果表明，该方法在多个指标上达到最新技术水平，评估框架更好地符合人类偏好，并且在不同的音频任务中具有较好的迁移性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>音频书生成旨在从多模态输入创建丰富、沉浸式的听觉体验，但面临缺乏协同生成多种音频类型、表达精细情感的难度以及缺乏与人类偏好相符的自动评估框架等三大挑战。</li>
<li>提出了一种基于流的上下文感知框架，用于多样化的音频生成，实现了精确的语义和时间对齐。</li>
<li>通过语言模型设计增强表达性，包括单词级别的副语言增强、句子级别的韵律检索和自适应文本转语音模型选择。</li>
<li>引入了一种新的评估框架，结合自我批评、视角转换和心理提示等方法，确保评估与人类偏好相符。</li>
<li>实验结果显示，该方法在多个指标上达到最新技术水平，并且评估框架具有良好的迁移性。</li>
<li>该系统通过多模态大型语言模型实现情感化、人性化的沉浸式音频书生成，提高了用户体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11002">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5c19a23766aa15689eeb1d5756f72460.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4541c5fe70fa796b63778501741ebfbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4d5da7fa20addf0b3381941c6a3d9af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790438d134778cb2881119c8dc276b7f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-05e1ff4efe1a0a455ee8c6e32d1a5fa2.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-14  Identity-Preserving Aging and De-Aging of Faces in the StyleGAN Latent   Space
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e8b7cace663feb64ee83ad38ef926424.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-08-14  PADReg Physics-Aware Deformable Registration Guided by Contact Force   for Ultrasound Sequences
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
