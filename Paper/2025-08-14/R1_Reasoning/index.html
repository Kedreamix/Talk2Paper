<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Prospect Theory Fails for LLMs Revealing Instability of Decision-Making   under Epistemic Uncertainty">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5631d7ccebd390af9da0550d594e5497.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-14-æ›´æ–°"><a href="#2025-08-14-æ›´æ–°" class="headerlink" title="2025-08-14 æ›´æ–°"></a>2025-08-14 æ›´æ–°</h1><h2 id="Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty"><a href="#Prospect-Theory-Fails-for-LLMs-Revealing-Instability-of-Decision-Making-under-Epistemic-Uncertainty" class="headerlink" title="Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making   under Epistemic Uncertainty"></a>Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making   under Epistemic Uncertainty</h2><p><strong>Authors:Rui Wang, Qihan Lin, Jiayu Liu, Qing Zong, Tianshi Zheng, Weiqi Wang, Yangqiu Song</strong></p>
<p>Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMsâ€™ decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMsâ€™ decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in <a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/MarPT">https://github.com/HKUST-KnowComp/MarPT</a>. </p>
<blockquote>
<p>å‰æ™¯ç†è®ºï¼ˆPTï¼‰æ¨¡æ‹Ÿäº†äººç±»åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹çš„å†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œè€Œè®¤çŸ¥æ ‡è®°ï¼ˆä¾‹å¦‚â€œä¹Ÿè®¸â€ï¼‰åˆ™ç”¨äºè¡¨è¾¾è¯­è¨€ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œå‰æ™¯ç†è®ºæ˜¯å¦é€‚ç”¨äºå½“å‰çš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠè¡¨è¾¾äººç±»ä¸ç¡®å®šæ€§çš„è®¤çŸ¥æ ‡è®°æ˜¯å¦ä¼šå½±å“è¿™äº›æ¨¡å‹çš„å†³ç­–åˆ¶å®šè¡Œä¸ºï¼Œè¿™äº›é—®é¢˜ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæ²¡æœ‰å¾—åˆ°æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™äº›ç ”ç©¶ç©ºç™½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºç»æµé—®å·çš„ä¸‰é˜¶æ®µå®éªŒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´é€šç”¨å’Œç²¾ç¡®çš„è¯„ä»·æ¡†æ¶ï¼Œä»¥æ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‰æ™¯ç†è®ºä¸‹çš„å†³ç­–åˆ¶å®šè¡Œä¸ºï¼Œå¹¶é€šè¿‡ä¸å¸¸è§è¯­å¢ƒä¸­å¸¸ç”¨çš„è®¤çŸ¥æ ‡è®°ç›¸å…³çš„ç»éªŒæ¦‚ç‡å€¼æ¥å¼•å…¥ä¸ç¡®å®šæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®å¯¹åº”çš„æ¦‚ç‡å€¼å°†è®¤çŸ¥æ ‡è®°çº³å…¥è¯„ä»·æ¡†æ¶ï¼Œä»¥æ£€æŸ¥å®ƒä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å†³ç­–åˆ¶å®šè¡Œä¸ºçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç”¨å‰æ™¯ç†è®ºæ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„å†³ç­–åˆ¶å®šå¹¶ä¸æ€»æ˜¯å¯é ï¼Œç‰¹åˆ«æ˜¯å½“ä¸ç¡®å®šæ€§ä»¥ä¸åŒçš„è¯­è¨€å½¢å¼è¡¨è¾¾æ—¶ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/MarPT%E3%80%82">https://github.com/HKUST-KnowComp/MarPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08992v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†äººç±»å†³ç­–æ¨¡å‹çš„å‰æ™¯ç†è®ºåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯èƒ½æ€§ï¼Œå¹¶ç ”ç©¶äº†è¡¨è¾¾äººç±»ä¸ç¡®å®šæ€§çš„è®¤è¯†è®ºæ ‡è®°å¯¹è¯­è¨€æ¨¡å‹å†³ç­–è¡Œä¸ºçš„å½±å“ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…è®¾è®¡äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé˜¶æ®µçš„å®éªŒï¼Œåˆ©ç”¨ç»æµé—®å·ä¸ºåŸºç¡€æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åœ¨æ¯”è¾ƒæƒ…å¢ƒä¸­å¼•å…¥äº†è®¤è¯†è®ºæ ‡è®°ç›¸å…³çš„æ¦‚ç‡å€¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»¥æ¦‚ç‡ç†è®ºæ¥å»ºæ¨¡è¯­è¨€æ¨¡å‹çš„å†³ç­–å¹¶ä¸å®Œå…¨å¯é ï¼Œç‰¹åˆ«æ˜¯å½“ä¸ç¡®å®šæ€§ä»¥ä¸åŒè¯­è¨€å½¢å¼è¡¨è¾¾æ—¶ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/HKUST-KnowComp/MarPT%E3%80%82">https://github.com/HKUST-KnowComp/MarPTã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†å°†äººç±»å†³ç­–æ¨¡å‹çš„å‰æ™¯ç†è®ºåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¯èƒ½æ€§ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†è®¤è¯†è®ºæ ‡è®°å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„å†³ç­–è¡Œä¸ºã€‚</li>
<li>ç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé˜¶æ®µçš„å®éªŒï¼Œä»¥ç»æµé—®å·ä¸ºåŸºç¡€è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>å®éªŒå¼•å…¥äº†è®¤è¯†è®ºæ ‡è®°ç›¸å…³çš„æ¦‚ç‡å€¼ä»¥æ¨¡æ‹Ÿä¸ç¡®å®šæ€§ã€‚</li>
<li>å®éªŒå‘ç°ï¼Œä»¥æ¦‚ç‡ç†è®ºæ¥å»ºæ¨¡è¯­è¨€æ¨¡å‹çš„å†³ç­–å¹¶ä¸å®Œå…¨å¯é ã€‚</li>
<li>å½“ä¸ç¡®å®šæ€§ä»¥ä¸åŒè¯­è¨€å½¢å¼è¡¨è¾¾æ—¶ï¼Œè¿™ç§ä¸ç¡®å®šæ€§å¯¹è¯­è¨€æ¨¡å‹å†³ç­–çš„å½±å“æ›´å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5631d7ccebd390af9da0550d594e5497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eeb0aa5da1da185fe154b36d936b4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c131c2c1af1c6564d1dbe7eeb234b127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2942e8de3cd3a182188ae1e94349bb6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4308e506adedcdf2d7ccb6d32dba588e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571c816155ab66a031790652ee1b893d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rational-Inverse-Reasoning"><a href="#Rational-Inverse-Reasoning" class="headerlink" title="Rational Inverse Reasoning"></a>Rational Inverse Reasoning</h2><p><strong>Authors:Ben Zandonati, TomÃ¡s Lozano-PÃ©rez, Leslie Pack Kaelbling</strong></p>
<p>Humans can observe a single, imperfect demonstration and immediately generalize to very different problem settings. Robots, in contrast, often require hundreds of examples and still struggle to generalize beyond the training conditions. We argue that this limitation arises from the inability to recover the latent explanations that underpin intelligent behavior, and that these explanations can take the form of structured programs consisting of high-level goals, sub-task decomposition, and execution constraints. In this work, we introduce Rational Inverse Reasoning (RIR), a framework for inferring these latent programs through a hierarchical generative model of behavior. RIR frames few-shot imitation as Bayesian program induction: a vision-language model iteratively proposes structured symbolic task hypotheses, while a planner-in-the-loop inference scheme scores each by the likelihood of the observed demonstration under that hypothesis. This loop yields a posterior over concise, executable programs. We evaluate RIR on a suite of continuous manipulation tasks designed to test one-shot and few-shot generalization across variations in object pose, count, geometry, and layout. With as little as one demonstration, RIR infers the intended task structure and generalizes to novel settings, outperforming state-of-the-art vision-language model baselines. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿè§‚å¯Ÿå•ä¸€ã€ä¸å®Œç¾çš„ç¤ºèŒƒï¼Œå¹¶ç«‹å³å°†å…¶æ¨å¹¿åˆ°éå¸¸ä¸åŒçš„åœºæ™¯è®¾ç½®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœºå™¨äººé€šå¸¸éœ€è¦æ•°ç™¾ä¸ªç¤ºä¾‹ï¼Œä»ç„¶éš¾ä»¥åœ¨è®­ç»ƒæ¡ä»¶ä¹‹å¤–è¿›è¡Œæ¨å¹¿ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ä¸€å±€é™æ€§æºäºæ— æ³•æ¢å¤æ”¯æ’‘æ™ºèƒ½è¡Œä¸ºçš„æ½œåœ¨è§£é‡Šï¼Œè¿™äº›è§£é‡Šå¯ä»¥é‡‡å–ç»“æ„åŒ–ç¨‹åºçš„å½¢å¼ï¼ŒåŒ…æ‹¬é«˜çº§ç›®æ ‡ã€å­ä»»åŠ¡åˆ†è§£å’Œæ‰§è¡Œçº¦æŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç†æ€§é€†å‘æ¨ç†ï¼ˆRIRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è¡Œä¸ºå±‚æ¬¡ç”Ÿæˆæ¨¡å‹æ¥æ¨æ–­è¿™äº›æ½œåœ¨ç¨‹åºçš„æ¡†æ¶ã€‚RIRå°†å°‘æ•°æ¼”ç¤ºè§†ä¸ºè´å¶æ–¯ç¨‹åºå½’çº³ï¼šè§†è§‰è¯­è¨€æ¨¡å‹æå‡ºç»“æ„åŒ–ç¬¦å·ä»»åŠ¡å‡è®¾ï¼Œè€Œå¾ªç¯å†…çš„æ¨ç†æ–¹æ¡ˆåˆ™æ ¹æ®æ¯ä¸ªå‡è®¾ä¸‹è§‚å¯Ÿåˆ°çš„æ¼”ç¤ºçš„å¯èƒ½æ€§å¯¹å…¶è¿›è¡Œè¯„åˆ†ã€‚è¿™ä¸ªå¾ªç¯äº§ç”Ÿäº†ä¸€ç³»åˆ—ç®€æ´ã€å¯æ‰§è¡Œç¨‹åºçš„åæœŸæ¦‚ç‡ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—è¿ç»­æ“ä½œä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡æ—¨åœ¨æµ‹è¯•ç‰©ä½“å§¿æ€ã€æ•°é‡ã€å‡ ä½•å½¢çŠ¶å’Œå¸ƒå±€å˜åŒ–ä¸­çš„ä¸€æ¬¡æ€§å’Œå°‘æ•°æ¬¡æ³›åŒ–èƒ½åŠ›ã€‚ä»…å‡­ä¸€æ¬¡æ¼”ç¤ºï¼ŒRIRå°±èƒ½æ¨æ–­å‡ºé¢„æœŸçš„ä»»åŠ¡ç»“æ„ï¼Œå¹¶æ¨å¹¿åˆ°æ–°çš„åœºæ™¯ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººç±»ä¸æœºå™¨äººåœ¨è§‚å¯Ÿä¸æ³›åŒ–èƒ½åŠ›ä¸Šçš„å·®å¼‚ã€‚äººç±»å¯ä»¥ä»å•ä¸€çš„ã€ä¸å®Œç¾çš„ç¤ºèŒƒä¸­ç«‹å³æ¨å¹¿åˆ°ä¸åŒçš„ä»»åŠ¡è®¾ç½®ï¼Œè€Œæœºå™¨äººé€šå¸¸éœ€è¦å¤§é‡çš„ä¾‹å­ï¼Œä»ç„¶éš¾ä»¥æ¨å¹¿åˆ°è®­ç»ƒæ¡ä»¶ä¹‹å¤–ã€‚æ–‡ç« æå‡ºï¼Œè¿™ç§å·®å¼‚æºäºæœºå™¨äººæ— æ³•è·å–æ”¯æ’‘æ™ºèƒ½è¡Œä¸ºçš„æ½œåœ¨è§£é‡Šï¼Œè¿™äº›è§£é‡Šå¯ä»¥é‡‡å–ç»“æ„åŒ–ç¨‹åºçš„å½¢å¼ï¼ŒåŒ…æ‹¬é«˜çº§ç›®æ ‡ã€å­ä»»åŠ¡åˆ†è§£å’Œæ‰§è¡Œçº¦æŸã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºç†æ€§é€†å‘æ¨ç†ï¼ˆRIRï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è¡Œä¸ºå±‚æ¬¡ç”Ÿæˆæ¨¡å‹æ¥æ¨æ–­è¿™äº›æ½œåœ¨ç¨‹åºã€‚RIRå°†å°‘æ•°é•œå¤´æ¨¡ä»¿è§†ä¸ºè´å¶æ–¯ç¨‹åºå½’çº³ï¼šè§†è§‰è¯­è¨€æ¨¡å‹æå‡ºç»“æ„åŒ–ç¬¦å·ä»»åŠ¡å‡è®¾ï¼Œè€Œå¾ªç¯å†…çš„æ¨ç†æ–¹æ¡ˆæ ¹æ®è¯¥å‡è®¾ä¸‹è§‚å¯Ÿåˆ°çš„æ¼”ç¤ºçš„å¯èƒ½æ€§è¿›è¡Œè¯„åˆ†ã€‚è¿™ä¸ªå¾ªç¯äº§ç”Ÿäº†ä¸€ç³»åˆ—ç®€æ´ã€å¯æ‰§è¡Œç¨‹åºçš„åç»­æ¦‚ç‡ã€‚åœ¨é’ˆå¯¹ç‰©ä½“å§¿æ€ã€æ•°é‡ã€å‡ ä½•å½¢çŠ¶å’Œå¸ƒå±€å˜åŒ–è€Œè®¾è®¡çš„è¿ç»­æ“ä½œä»»åŠ¡å¥—ä»¶ä¸Šè¯„ä¼°RIRï¼Œä»…é€šè¿‡ä¸€æ¬¡æ¼”ç¤ºï¼ŒRIRå°±èƒ½æ¨æ–­å‡ºä»»åŠ¡ç»“æ„å¹¶æ¨å¹¿åˆ°æ–°çš„ç¯å¢ƒï¼Œä¼˜äºæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»èƒ½ä»å•ä¸€çš„ã€ä¸å®Œç¾çš„ç¤ºèŒƒä¸­å¿«é€Ÿæ³›åŒ–åˆ°æ–°ä»»åŠ¡ï¼Œè€Œæœºå™¨äººéœ€è¦æ›´å¤šçš„ä¾‹å­å¹¶éš¾ä»¥æ³›åŒ–ã€‚</li>
<li>æœºå™¨äººçš„é™åˆ¶åœ¨äºæ— æ³•æ¢å¤æ”¯æ’‘æ™ºèƒ½è¡Œä¸ºçš„æ½œåœ¨è§£é‡Šã€‚</li>
<li>Rational Inverse Reasoning (RIR)æ¡†æ¶ç”¨äºæ¨æ–­è¿™äº›æ½œåœ¨ç¨‹åºã€‚</li>
<li>RIRå°†å°‘æ•°é•œå¤´æ¨¡ä»¿è§†ä¸ºè´å¶æ–¯ç¨‹åºå½’çº³ã€‚</li>
<li>RIRåŒ…æ‹¬ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡ºç»“æ„åŒ–ç¬¦å·ä»»åŠ¡å‡è®¾ï¼Œå¹¶ç”±å¾ªç¯å†…çš„æ¨ç†æ–¹æ¡ˆè¯„åˆ†ã€‚</li>
<li>é€šè¿‡ä¸€æ¬¡æˆ–å°‘æ•°å‡ æ¬¡æ¼”ç¤ºï¼ŒRIRå°±èƒ½æ³›åŒ–åˆ°æ–°ç¯å¢ƒå¹¶ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3330f903966fad5a8fe98bef595386af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6770922c1e1f45506095381db6ff9383.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f06616f9a5843e31a95b156acb1a8ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cda2c30ba04b593f68f2490a09c3394.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning"><a href="#Train-Long-Think-Short-Curriculum-Learning-for-Efficient-Reasoning" class="headerlink" title="Train Long, Think Short: Curriculum Learning for Efficient Reasoning"></a>Train Long, Think Short: Curriculum Learning for Efficient Reasoning</h2><p><strong>Authors:Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem</strong></p>
<p>Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: <a target="_blank" rel="noopener" href="https://github.com/hammoudhasan/curriculum_grpo">https://github.com/hammoudhasan/curriculum_grpo</a>. </p>
<blockquote>
<p>è¿‘æœŸå…³äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„å·¥ä½œå·²ç»å¼•å…¥äº†æ˜ç¡®çš„é•¿åº¦æ§åˆ¶ï¼Œä½œä¸ºä¸€ç§åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ§åˆ¶è®¡ç®—æˆæœ¬çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºå›ºå®šé•¿åº¦çš„è®­ç»ƒé¢„ç®—ï¼Œæ²¡æœ‰åˆ©ç”¨å­¦ä¹ è¿‡ç¨‹ä¸­ä»æ¢ç´¢åˆ°å‹ç¼©çš„è‡ªç„¶è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨Group Relative Policy Optimization (GRPO)è¿›è¡Œé•¿åº¦æ§åˆ¶æ¨ç†çš„è¯¾ç¨‹ä½“ç³»å­¦ä¹ ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»å®½æ¾çš„ä»¤ç‰Œé¢„ç®—å¼€å§‹ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸æ”¶ç´§ï¼Œé¼“åŠ±æ¨¡å‹é¦–å…ˆå‘ç°æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆç­–ç•¥ï¼Œç„¶åå°†å®ƒä»¬è’¸é¦æˆæ›´ç®€æ´çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬ä¸ºGRPOå¢åŠ äº†ä¸€ä¸ªå¹³è¡¡ä¸‰ä¸ªä¿¡å·çš„å¥–åŠ±å‡½æ•°ï¼šä»»åŠ¡æ­£ç¡®æ€§ï¼ˆé€šè¿‡éªŒè¯å™¨åé¦ˆï¼‰ã€é•¿åº¦æ•ˆç‡å’Œæ ¼å¼éµå®ˆï¼ˆé€šè¿‡ç»“æ„æ ‡ç­¾ï¼‰ã€‚åœ¨GSM8Kã€MATH500ã€SVAMPã€College Mathå’ŒGSM+ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºè¯¾ç¨‹çš„è®­ç»ƒåœ¨ç›¸åŒçš„æœ€ç»ˆé¢„ç®—ä¸‹å§‹ç»ˆä¼˜äºå›ºå®šé¢„ç®—çš„åŸºçº¿ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„ä»¤ç‰Œæ•ˆç‡æå‡ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†å¥–åŠ±æƒé‡å’Œè¡°å‡è®¡åˆ’è®¾è®¡çš„å½±å“ï¼Œè¡¨æ˜æ¸è¿›çº¦æŸæ˜¯è®­ç»ƒé«˜æ•ˆæ¨ç†æ¨¡å‹çš„æœ‰åŠ›å½’çº³åç½®ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/hammoudhasan/curriculum_grpo%E3%80%82">https://github.com/hammoudhasan/curriculum_grpoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08940v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„é•¿åº¦æ§åˆ¶æ¨ç†ç­–ç•¥ï¼Œå³é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ¸è¿›çš„ä»¤ç‰Œé¢„ç®—é™åˆ¶è®­ç»ƒè¿‡ç¨‹ï¼Œé¼“åŠ±æ¨¡å‹å…ˆæ¢ç´¢æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆç­–ç•¥ï¼Œå†å°†å…¶æç‚¼æˆæ›´ç®€æ´çš„æ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å›ºå®šé¢„ç®—åŸºçº¿ç›¸æ¯”ï¼ŒåŸºäºè¯¾ç¨‹çš„è®­ç»ƒåœ¨ç›¸åŒæœ€ç»ˆé¢„ç®—ä¸‹è¡¨ç°æ›´ä¼˜ç§€ï¼Œå‡†ç¡®ç‡å’Œä»¤ç‰Œæ•ˆç‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œç”¨äºé•¿åº¦æ§åˆ¶çš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ¸è¿›çš„ä»¤ç‰Œé¢„ç®—é™åˆ¶è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹å…ˆåœ¨å®½æ¾çš„ä»¤ç‰Œé¢„ç®—ä¸‹è®­ç»ƒï¼Œå†é€æ¸æ”¶ç´§é¢„ç®—ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹æ¢ç´¢å¹¶æç‚¼æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥çš„ä¼˜è¶Šæ€§ï¼Œä¸å›ºå®šé¢„ç®—åŸºçº¿ç›¸æ¯”ï¼Œå‡†ç¡®ç‡å’Œä»¤ç‰Œæ•ˆç‡æ›´é«˜ã€‚</li>
<li>å¼•å…¥äº†å¥–åŠ±å‡½æ•°ï¼Œå¹³è¡¡äº†ä»»åŠ¡æ­£ç¡®æ€§ã€é•¿åº¦æ•ˆç‡å’Œæ ¼å¼éµå®ˆä¸‰ä¸ªä¿¡å·ã€‚</li>
<li>æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ¸è¿›çº¦æŸä½œä¸ºå¼ºå¤§çš„å½’çº³åç½®ï¼Œå¯¹è®­ç»ƒé«˜æ•ˆæ¨ç†æ¨¡å‹å…·æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7007b65395cb7977632ab584bf33c43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe4ba55f3f1597ef0ce3526751962c3c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Preview-WB-DH-Towards-Whole-Body-Digital-Human-Bench-for-the-Generation-of-Whole-body-Talking-Avatar-Videos"><a href="#Preview-WB-DH-Towards-Whole-Body-Digital-Human-Bench-for-the-Generation-of-Whole-body-Talking-Avatar-Videos" class="headerlink" title="Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation   of Whole-body Talking Avatar Videos"></a>Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation   of Whole-body Talking Avatar Videos</h2><p><strong>Authors:Chaoyi Wang, Yifan Yang, Jun Pei, Lijie Xia, Jianpo Liu, Xiaobing Yuan, Xinhan Di</strong></p>
<p>Creating realistic, fully animatable whole-body avatars from a single portrait is challenging due to limitations in capturing subtle expressions, body movements, and dynamic backgrounds. Current evaluation datasets and metrics fall short in addressing these complexities. To bridge this gap, we introduce the Whole-Body Benchmark Dataset (WB-DH), an open-source, multi-modal benchmark designed for evaluating whole-body animatable avatar generation. Key features include: (1) detailed multi-modal annotations for fine-grained guidance, (2) a versatile evaluation framework, and (3) public access to the dataset and tools at <a target="_blank" rel="noopener" href="https://github.com/deepreasonings/WholeBodyBenchmark">https://github.com/deepreasonings/WholeBodyBenchmark</a>. </p>
<blockquote>
<p>åˆ›å»ºä»å•å¹…è‚–åƒç”Ÿæˆé€¼çœŸã€å¯å…¨æ–¹ä½åŠ¨ç”»åŒ–çš„å…¨èº«è™šæ‹Ÿå½¢è±¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæ•æ‰å¾®å¦™çš„è¡¨æƒ…ã€èº«ä½“åŠ¨ä½œå’ŒåŠ¨æ€èƒŒæ™¯å­˜åœ¨å±€é™æ€§ã€‚å½“å‰çš„è¯„ä¼°æ•°æ®é›†å’ŒæŒ‡æ ‡åœ¨åº”å¯¹è¿™äº›å¤æ‚æ€§æ–¹é¢è¡¨ç°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨èº«åŸºå‡†æ•°æ®é›†ï¼ˆWB-DHï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ã€å¤šæ¨¡å¼åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å…¨èº«å¯åŠ¨ç”»è™šæ‹Ÿå½¢è±¡çš„ç”Ÿæˆã€‚å…¶ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ç”¨äºç²¾ç»†æŒ‡å¯¼çš„è¯¦ç»†å¤šæ¨¡å¼æ³¨é‡Šï¼Œï¼ˆ2ï¼‰é€šç”¨çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥åŠï¼ˆ3ï¼‰å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/deepreasonings/WholeBodyBenchmark%E8%AE%BF%E9%97%AE%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E5%B7%A5%E5%85%B7%E3%80%82">https://github.com/deepreasonings/WholeBodyBenchmarkè®¿é—®æ•°æ®é›†å’Œå·¥å…·ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08891v1">PDF</a> This paper has been accepted by ICCV 2025 Workshop MMFM4</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åˆ›å»ºçœŸå®ã€å¯åŠ¨åŒ–çš„å…¨èº«ä¸ªæ€§åŒ–è§’è‰²ï¼ˆavatarsï¼‰ä»å•å¹…è‚–åƒç…§ç‰‡æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•æ‰å¾®å¦™è¡¨æƒ…ã€èº«ä½“åŠ¨ä½œå’ŒåŠ¨æ€èƒŒæ™¯çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¼€æºã€å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•æ•°æ®é›†â€”â€”å…¨èº«åŸºå‡†æ•°æ®é›†ï¼ˆWB-DHï¼‰ï¼Œç”¨äºè¯„ä¼°å…¨èº«å¯åŠ¨ä¸ªæ€§åŒ–è§’è‰²çš„ç”Ÿæˆã€‚è¯¥æ•°æ®é›†åŒ…å«è¯¦ç»†çš„å¤šæ¨¡å¼æ³¨é‡Šã€çµæ´»çš„è¯„ä»·æ¡†æ¶ï¼Œå¹¶å¯åœ¨å…¬å¼€å¹³å°ä¸Šè®¿é—®ç›¸å…³æ•°æ®å’Œå·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ›å»ºçœŸå®ã€å¯åŠ¨åŒ–çš„å…¨èº«ä¸ªæ€§åŒ–è§’è‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>å½“å‰çš„è¯„ä»·æ•°æ®é›†å’ŒæŒ‡æ ‡åœ¨åº”å¯¹è¿™ç§å¤æ‚æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œå¼•å…¥äº†å…¨èº«åŸºå‡†æ•°æ®é›†ï¼ˆWB-DHï¼‰ã€‚</li>
<li>WB-DHæ˜¯ä¸€ä¸ªå¼€æºã€å¤šæ¨¡å¼åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å…¨èº«å¯åŠ¨ä¸ªæ€§åŒ–è§’è‰²çš„ç”Ÿæˆã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«è¯¦ç»†çš„å¤šæ¨¡å¼æ³¨é‡Šï¼Œä¸ºç²¾ç»†æŒ‡å¯¼æä¾›äº†ä¸°å¯Œçš„æ•°æ®ã€‚</li>
<li>WB-DHæä¾›äº†ä¸€ä¸ªçµæ´»çš„è¯„ä»·æ¡†æ¶ï¼Œä»¥é€‚åº”ä¸åŒçš„è¯„ä¼°éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84a9db8e83138a5f81de0552643c9b8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1874a6b96d9ba3c81df99d45e9e655e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a944f3330909817982fcf4d42015bf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-170c7d268e3cd03cc73d12c595b86a87.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Roots-of-International-Perceptions-Simulating-US-Attitude-Changes-Towards-China-with-LLM-Agents"><a href="#The-Roots-of-International-Perceptions-Simulating-US-Attitude-Changes-Towards-China-with-LLM-Agents" class="headerlink" title="The Roots of International Perceptions: Simulating US Attitude Changes   Towards China with LLM Agents"></a>The Roots of International Perceptions: Simulating US Attitude Changes   Towards China with LLM Agents</h2><p><strong>Authors:Nicholas Sukiennik, Yichuan Xu, Yuqing Kan, Jinghua Piao, Yuwei Yan, Chen Gao, Yong Li</strong></p>
<p>The rise of LLMs poses new possibilities in modeling opinion evolution, a long-standing task in simulation, by leveraging advanced reasoning abilities to recreate complex, large-scale human cognitive trends. While most prior works focus on opinion evolution surrounding specific isolated events or the views within a country, ours is the first to model the large-scale attitude evolution of a population representing an entire country towards another â€“ US citizensâ€™ perspectives towards China. To tackle the challenges of this broad scenario, we propose a framework that integrates media data collection, user profile creation, and cognitive architecture for opinion updates to successfully reproduce the real trend of US attitudes towards China over a 20-year period from 2005 to today. We also leverage LLMsâ€™ capabilities to introduce debiased media exposure, extracting neutral events from typically subjective news contents, to uncover the roots of polarized opinion formation, as well as a devils advocate agent to help explain the rare reversal from negative to positive attitudes towards China, corresponding with changes in the way Americans obtain information about the country. The simulation results, beyond validating our framework architecture, also reveal the impact of biased framing and selection bias in shaping attitudes. Overall, our work contributes to a new paradigm for LLM-based modeling of cognitive behaviors in a large-scale, long-term, cross-border social context, providing insights into the formation of international biases and offering valuable implications for media consumers to better understand the factors shaping their perspectives, and ultimately contributing to the larger social need for bias reduction and cross-cultural tolerance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ä¸ºæ¨¡æ‹Ÿæ„è§æ¼”å˜è¿™ä¸€ä»¿çœŸé¢†åŸŸçš„é•¿æœŸä»»åŠ¡å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°åˆ›é€ å¤æ‚çš„å¤§è§„æ¨¡äººç±»è®¤çŸ¥è¶‹åŠ¿ã€‚è™½ç„¶å¤§å¤šæ•°æ—©æœŸçš„ç ”ç©¶å·¥ä½œé›†ä¸­åœ¨å›´ç»•ç‰¹å®šå­¤ç«‹äº‹ä»¶æˆ–å›½å®¶å†…éƒ¨çš„è§‚ç‚¹è¿›è¡Œçš„æ„è§æ¼”å˜ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡å¯¹ä»£è¡¨æ•´ä¸ªå›½å®¶å¯¹å¦ä¸€ä¸ªå›½å®¶æ€åº¦çš„å¤§è§„æ¨¡æ¼”å˜è¿›è¡Œå»ºæ¨¡â€”â€”ç¾å›½å…¬æ°‘å¯¹ä¸­å›½çš„çœ‹æ³•ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å¹¿æ³›åœºæ™¯çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†åª’ä½“æ•°æ®é‡‡é›†ã€ç”¨æˆ·è§’è‰²åˆ›å»ºå’Œè®¤çŸ¥æ¶æ„æ¥è¿›è¡Œæ„è§æ›´æ–°ï¼Œä»¥æˆåŠŸå†ç°ä»2005å¹´åˆ°ä»Šå¤©çš„äºŒåå¹´æœŸé—´ç¾å›½å¯¹ä¸­å›½æ€åº¦çš„çœŸå®è¶‹åŠ¿ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨LLMsçš„èƒ½åŠ›å¼•å…¥æ— åè§çš„åª’ä½“æ›å…‰ï¼Œä»é€šå¸¸ä¸»è§‚çš„æ–°é—»å†…å®¹ä¸­æå–ä¸­æ€§äº‹ä»¶ï¼Œä»¥æ­ç¤ºæç«¯æ„è§å½¢æˆçš„æ ¹æºï¼Œä»¥åŠä¸€ä¸ªé­”é¬¼ä»£è¨€äººçš„ä»£ç†æœ‰åŠ©äºè§£é‡Šä»å¯¹ä¸­å›½è´Ÿé¢æ€åº¦è½¬å˜ä¸ºæ­£é¢æ€åº¦çš„ç½•è§é€†è½¬ç°è±¡ï¼Œè¿™ä¸ç¾å›½äººäº†è§£è¯¥å›½çš„æ–¹å¼å˜åŒ–ç›¸å¯¹åº”ã€‚é™¤äº†éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶æ¶æ„å¤–ï¼Œæ¨¡æ‹Ÿç»“æœè¿˜æ­ç¤ºäº†åè§æ¡†å®šå’Œé€‰æ‹©åè§åœ¨å¡‘é€ æ€åº¦æ–¹é¢çš„å½±å“ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œåœ¨å¤§å‹ã€é•¿æœŸã€è·¨å›½ç¤¾äº¤èƒŒæ™¯ä¸‹çš„LLMå»ºæ¨¡è®¤çŸ¥è¡Œä¸ºæ–¹é¢å¼€åˆ›äº†æ–°çš„èŒƒå¼ï¼Œä¸ºå›½é™…åè§çš„å½¢æˆæä¾›äº†è§è§£ï¼Œå¹¶ä¸ºåª’ä½“æ¶ˆè´¹è€…æä¾›äº†æœ‰ä»·å€¼çš„å¯ç¤ºï¼Œä»¥æ›´å¥½åœ°äº†è§£å¡‘é€ ä»–ä»¬è§‚ç‚¹çš„å› ç´ ï¼Œæœ€ç»ˆæ»¡è¶³æ›´å¤§çš„ç¤¾ä¼šéœ€æ±‚å‡å°‘åè§å’Œè·¨æ–‡åŒ–å®¹å¿åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08837v1">PDF</a> Submitted to AAAI Social Impact 2026</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å´›èµ·ä¸ºæ¨¡æ‹Ÿæ„è§æ¼”å˜æä¾›äº†æ–°çš„å¯èƒ½ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å»ºæ¨¡ä»£è¡¨ä¸€ä¸ªå›½å®¶æ°‘ä¼—å¯¹å¦ä¸€ä¸ªå›½å®¶çš„å¤§è§„æ¨¡æ€åº¦æ¼”å˜ï¼Œåˆ©ç”¨åª’ä½“æ•°æ®é‡‡é›†ã€ç”¨æˆ·è§’è‰²è®¾å®šä¸è®¤çŸ¥æ¶æ„æ›´æ–°ï¼Œå†ç°2005å¹´è‡³ä»Šç¾å›½æ°‘ä¼—å¯¹ä¸­å›½æ€åº¦çš„çœŸå®è¶‹åŠ¿ã€‚åŒæ—¶å¼•å…¥å»åè§åª’ä½“æ›å…‰ï¼Œå‘ç°èˆ†è®ºæåŒ–æ ¹æºï¼Œå¹¶åˆ©ç”¨å¯¹ç«‹é¢ä»£ç†äººè§£é‡Šæ€åº¦é€†è½¬ç°è±¡ã€‚æ¨¡æ‹Ÿç»“æœæ­ç¤ºåè§æ¡†æ¶ä¸é€‰æ‹©åè§å¯¹æ€åº¦å½¢æˆçš„å½±å“ã€‚æœ¬ç ”ç©¶ä¸ºå¤§å‹é•¿æœŸè·¨å¢ƒç¤¾ä¼šèƒŒæ™¯ä¸‹çš„è®¤çŸ¥è¡Œä¸ºå»ºæ¨¡æä¾›æ–°èŒƒä¾‹ï¼Œæœ‰åŠ©äºåª’ä½“æ¶ˆè´¹è€…ç†è§£å½¢æˆè§‚ç‚¹çš„å› ç´ ï¼Œä¸ºç¤¾ä¼šå‡å°‘åè§å’Œä¿ƒè¿›è·¨æ–‡åŒ–å®¹å¿åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsè¢«ç”¨äºæ¨¡æ‹Ÿé•¿æœŸå­˜åœ¨çš„æ„è§æ¼”å˜ä»»åŠ¡ï¼Œå±•ç¤ºå…¶é‡æ„å¤æ‚å¤§è§„æ¨¡äººç±»è®¤çŸ¥è¶‹åŠ¿çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å…³æ³¨ç¾å›½æ°‘ä¼—å¯¹ä¸­å›½æ€åº¦çš„é•¿æœŸæ¼”å˜ï¼Œé¦–æ¬¡è¿›è¡Œæ­¤ç±»å¤§è§„æ¨¡å»ºæ¨¡ã€‚</li>
<li>åˆ©ç”¨åª’ä½“æ•°æ®é‡‡é›†ã€ç”¨æˆ·è§’è‰²è®¾å®šå’Œè®¤çŸ¥æ¶æ„æ›´æ–°æ¥æˆåŠŸæ¨¡æ‹ŸçœŸå®è¶‹åŠ¿ã€‚</li>
<li>é€šè¿‡å»åè§åª’ä½“æ›å…‰å’Œæå–ä¸­æ€§äº‹ä»¶ï¼Œæ­ç¤ºèˆ†è®ºæåŒ–çš„æ ¹æºã€‚</li>
<li>å¯¹ç«‹é¢ä»£ç†äººç”¨äºè§£é‡Šæ€åº¦é€†è½¬ç°è±¡ã€‚</li>
<li>æ¨¡æ‹Ÿç»“æœå¼ºè°ƒåè§æ¡†æ¶å’Œé€‰æ‹©åè§åœ¨å¡‘é€ æ€åº¦æ–¹é¢çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ec26be29d91f39822e8714199824e14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de19b738470f8f69d21be43f2282641e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72ba663ea362447a6a8c7b47b5588008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5669579fb15a0eaabaeeb127304c6e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d2aba8a5dbdf7c5d64aa02cdcbd1130.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-725e933f2eb77f601ea9bdca6273938e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="Interpretable Reward Model via Sparse Autoencoder"></a>Interpretable Reward Model via Sparse Autoencoder</h2><p><strong>Authors:Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang</strong></p>
<p>Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (\textbf{SARM}), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm">https://github.com/schrieffer-z/sarm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰é€šè¿‡ä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä½¿LLMçš„è¡Œä¸ºä¸äººç±»ä»·å€¼è§‚ä¿æŒä¸€è‡´ï¼Œå› æ­¤RMçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹äºæœ‰æ•ˆçš„å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹äºå¥–åŠ±åˆ†é…èƒŒåçš„æ¨ç†æä¾›æœ‰é™çš„è§è§£ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚è™½ç„¶æœ€è¿‘çš„å¤šç»´RMæ—¨åœ¨æé«˜å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æä¾›ç‰¹å¾çº§åˆ«çš„å½’å±åº¦ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„æ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰é›†æˆåˆ°å¥–åŠ±æ¨¡å‹ä¸­ã€‚SARMå°†åŸºäºLLMçš„RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–å’Œå•è¯­ä¹‰ç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­æ ‡é‡å¤´èšåˆç‰¹å¾æ¿€æ´»ä»¥äº§ç”Ÿé€æ˜ä¸”æ¦‚å¿µä¸Šæ„ä¹‰æ˜ç¡®çš„å¥–åŠ±åˆ†æ•°ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSARMä¿ƒè¿›äº†å¥–åŠ±åˆ†é…çš„ç‰¹å¾çº§åˆ«ç›´æ¥å½’å±ï¼Œå…è®¸åŠ¨æ€è°ƒæ•´åå¥½å˜åŒ–ï¼Œä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç›¸æ¯”å®ç°äº†ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/schrieffer-z/sarmè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08746v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä»¥ä½¿LLMè¡Œä¸ºä¸äººçš„ä»·å€¼è§‚ä¿æŒä¸€è‡´ï¼Œå› æ­¤RMçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹äºæœ‰æ•ˆçš„å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹äºå¥–åŠ±åˆ†é…çš„æ¨ç†è¿‡ç¨‹æä¾›æœ‰é™çš„è§è§£ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚è™½ç„¶æœ€è¿‘çš„å¤šç»´RMæ—¨åœ¨æé«˜å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æä¾›ç‰¹å¾çº§åˆ«çš„å½’å±ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„æ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå°†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰é›†æˆåˆ°å¥–åŠ±æ¨¡å‹ä¸­ã€‚SARMå°†LLMåŸºäºRMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°ä¸€ä¸ªå¯è§£é‡Šã€ç¨€ç–å’Œå•è¯­ä¹‰çš„ç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­æ ‡é‡å¤´èšåˆç‰¹å¾æ¿€æ´»ä»¥äº§ç”Ÿé€æ˜ä¸”æ¦‚å¿µä¸Šæœ‰æ„ä¹‰çš„å¥–åŠ±åˆ†æ•°ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSARMä¿ƒè¿›äº†å¥–åŠ±åˆ†é…çš„ç‰¹å¾çº§åˆ«å½’å±ï¼Œå…è®¸åŠ¨æ€é€‚åº”åå¥½å˜åŒ–ï¼Œå¹¶å®ç°äº†ä¸å¸¸è§„å¥–åŠ±æ¨¡å‹ç›¸æ¯”æ›´ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç”¨äºå¯¹é½LLMè¡Œä¸ºä¸äººçš„ä»·å€¼è§‚ã€‚</li>
<li>ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹äºå¥–åŠ±åˆ†é…çš„æ¨ç†è¿‡ç¨‹æä¾›æœ‰é™çš„è§è§£ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚</li>
<li>æ–°å‹æ¶æ„SARMé€šè¿‡é›†æˆé¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åˆ°å¥–åŠ±æ¨¡å‹ä¸­ï¼Œæé«˜äº†RMçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>SARMå°†LLMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°ä¸€ä¸ªå¯è§£é‡Šã€ç¨€ç–å’Œå•è¯­ä¹‰çš„ç‰¹å¾ç©ºé—´ã€‚</li>
<li>SARMå…è®¸ç›´æ¥çš„ç‰¹å¾çº§åˆ«å½’å±çš„å¥–åŠ±åˆ†é…ï¼ŒåŠ¨æ€è°ƒæ•´åå¥½å˜åŒ–ï¼Œå¹¶å®ç°äº†ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚</li>
<li>SARMçš„ç»éªŒè¯„ä¼°è¯æ˜äº†å…¶ç›¸å¯¹äºä¼ ç»ŸRMçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2a0401118c47f42664c149a95254637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e38c8e6b12330c78ec551a169893282.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-926930e1ed1f7c34ee897abec757b01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-491c62cf4888b77b5071074321f6bb81.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision"><a href="#STELAR-VISION-Self-Topology-Aware-Efficient-Learning-for-Aligned-Reasoning-in-Vision" class="headerlink" title="STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned   Reasoning in Vision"></a>STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned   Reasoning in Vision</h2><p><strong>Authors:Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides</strong></p>
<p>Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks. We have released datasets, and code will be available. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å¤šåª’ä½“ä»»åŠ¡æ—¶å¾€å¾€é‡åˆ°å›°éš¾ï¼Œå¹¶å€¾å‘äºäº§ç”Ÿå†—é•¿çš„è¾“å‡ºã€‚ä¸€ä¸ªä¸»è¦çš„å±€é™æ€§åœ¨äºå®ƒä»¬ä¾èµ–äºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œå°½ç®¡è®¸å¤šä»»åŠ¡å¯ä»¥ä»æ ‘æˆ–å›¾ç­‰æ›¿ä»£æ‹“æ‰‘ç»“æ„ä¸­å—ç›Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†STELAR-Visionï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ‹“æ‰‘æ„ŸçŸ¥æ¨ç†çš„è®­ç»ƒæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯TopoAugï¼Œä¸€ä¸ªåˆæˆæ•°æ®ç®¡é“ï¼Œå®ƒä¸°å¯Œäº†å…·æœ‰å„ç§æ‹“æ‰‘ç»“æ„çš„è®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨æœ‰ç›‘ç£çš„å¾®è°ƒæ–¹æ³•å’Œå¼ºåŒ–å­¦ä¹ æ¥åè®­ç»ƒQwen2VLæ¨¡å‹ï¼ŒåŒæ—¶è€ƒè™‘åˆ°å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†èŠ‚ä¿­å­¦ä¹ ï¼ˆFrugal Learningï¼‰ï¼Œå®ƒå¯ä»¥åœ¨æŸå¤±æœ€å°ç²¾åº¦çš„æƒ…å†µä¸‹å‡å°‘è¾“å‡ºé•¿åº¦ã€‚åœ¨MATH-Vå’ŒVLM-S2Hä¸Šï¼ŒSTELAR-Visionåœ¨å…¶åŸºç¡€æ¨¡å‹ä¸Šæé«˜äº†9.7%çš„å‡†ç¡®ç‡ï¼Œå¹¶è¶…è¿‡äº†æ›´å¤§çš„Qwen2VL-72B-Instructæ¨¡å‹7.3%ã€‚åœ¨äº”ç»„åˆ†å¸ƒå¤–çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒçš„æ€§èƒ½è¶…è¿‡äº†Phi-4-Multimodal-Instructæ¨¡å‹é«˜è¾¾28.4%ï¼Œå¹¶è¶…è¿‡äº†LLaMA-3.2-11B-Vision-Instructæ¨¡å‹é«˜è¾¾13.2%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä»…é“¾å¼è®­ç»ƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šçš„æ€»ä½“å‡†ç¡®ç‡æé«˜äº†4.3%ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰å¤–éƒ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸€è‡´ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬å·²ç»å‘å¸ƒäº†æ•°æ®é›†ï¼Œä»£ç ä¹Ÿå°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08688v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ï¼Œå°½ç®¡æ¨ç†èƒ½åŠ›å·²æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”å®¹æ˜“äº§ç”Ÿå†—é•¿çš„è¾“å‡ºã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†STELAR-Visionè®­ç»ƒæ¡†æ¶ï¼Œä»¥æ‹“æ‰‘æ„ŸçŸ¥æ¨ç†ä¸ºæ ¸å¿ƒï¼Œå¼•å…¥TopoAugåˆæˆæ•°æ®ç®¡é“æ¥ä¸°å¯Œè®­ç»ƒä¸­çš„æ‹“æ‰‘ç»“æ„å¤šæ ·æ€§ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œå¯¹Qwen2VLæ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œå…¼é¡¾å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚åŒæ—¶ï¼Œæå‡ºèŠ‚ä¿­å­¦ä¹ ï¼ˆFrugal Learningï¼‰æ–¹æ³•ï¼Œä»¥æœ€å°åŒ–å‡†ç¡®æ€§æŸå¤±çš„æ–¹å¼ç¼©çŸ­è¾“å‡ºé•¿åº¦ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSTELAR-Visionè¾ƒåŸºç¡€æ¨¡å‹æé«˜äº†9.7%çš„å‡†ç¡®ç‡ï¼Œä¸”åœ¨äº”ä¸ªè·¨åˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°æœ‰é™ï¼Œç”Ÿæˆè¾“å‡ºè¿‡äºå†—é•¿ã€‚</li>
<li>STELAR-Visionè®­ç»ƒæ¡†æ¶å¼•å…¥æ‹“æ‰‘æ„ŸçŸ¥æ¨ç†ï¼Œä»¥åº”å¯¹æ­¤æŒ‘æˆ˜ã€‚</li>
<li>TopoAugåˆæˆæ•°æ®ç®¡é“ä¸°å¯Œè®­ç»ƒä¸­çš„æ‹“æ‰‘ç»“æ„å¤šæ ·æ€§ã€‚</li>
<li>ç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œå¯¹Qwen2VLæ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œæå‡å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚</li>
<li>æå‡ºèŠ‚ä¿­å­¦ä¹ æ–¹æ³•ï¼Œç¼©çŸ­è¾“å‡ºé•¿åº¦ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</li>
<li>STELAR-Visionåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æé«˜9.7%å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-764a915237702a180e8e9ac5ffbb5b92.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20bda661c4a78c9c7cac199cf0f9b33f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f35b9ca0981731265e444afad3667249.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64a72347ed3f361815bece8a0c27d8eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ea4e4ffd9447af9d1bf36c1e5796855.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45bf8f088716f959fcc3908df8a40f65.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Aryabhata-An-exam-focused-language-model-for-JEE-Math"><a href="#Aryabhata-An-exam-focused-language-model-for-JEE-Math" class="headerlink" title="Aryabhata: An exam-focused language model for JEE Math"></a>Aryabhata: An exam-focused language model for JEE Math</h2><p><strong>Authors:Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma</strong></p>
<p>We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-$n$ rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation along with novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback (<a target="_blank" rel="noopener" href="https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0">https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0</a>); PW is actively training future models to further improve learning outcomes for students. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Aryabhata 1.0ï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹å°åº¦é«˜è€ƒâ€”â€”è”åˆå…¥å­¦è€ƒè¯•ï¼ˆJEEï¼‰è¿›è¡Œä¼˜åŒ–çš„å°å‹7Bå‚æ•°æ•°å­¦æ¨ç†æ¨¡å‹ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•è¿…é€Ÿï¼Œä½†å½“å‰æ¨¡å‹åœ¨æ•™è‚²åº”ç”¨ä¸­ä»å¸¸å¸¸ä¸å¤ªé€‚ç”¨ã€‚Aryabhata 1.0çš„æ„å»ºèåˆäº†å¼ºå¤§çš„å¼€æ”¾æƒé‡æ¨ç†æ¨¡å‹ï¼Œç„¶åé€šè¿‡è¯¾ç¨‹å­¦ä¹ å¯¹ç»è¿‡éªŒè¯çš„é€æ­¥æ€è€ƒï¼ˆCoTï¼‰è½¨è¿¹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ€§èƒ½ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ï¼Œä½¿ç”¨A2Cç›®æ ‡ç»“åˆç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼Œä»¥åŠæ–°å‹æ¢ç´¢ç­–ç•¥ï¼Œå¦‚è‡ªé€‚åº”åˆ†ç»„è°ƒæ•´å’Œæ¸©åº¦ç¼©æ”¾ã€‚åœ¨å†…éƒ¨æ•°æ®é›†ï¼ˆJEE Main 2025ï¼‰å’Œå¤–éƒ¨æ•°æ®é›†ï¼ˆMATHï¼ŒGSM8Kï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAryabhataåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶æä¾›å¾ªåºæ¸è¿›çš„æ¨ç†æ­¥éª¤ï¼Œæœ‰åŠ©äºæ•™å­¦ã€‚æˆ‘ä»¬å‘å¸ƒAryabhataä½œä¸ºä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨ä»¥è€ƒè¯•ä¸ºä¸­å¿ƒçš„å°å‹å¼€æºè¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚è¿™æ˜¯æˆ‘ä»¬é¦–æ¬¡å…¬å¼€å‘å¸ƒæ¨¡å‹ï¼Œå¸Œæœ›å¾—åˆ°ç¤¾åŒºåé¦ˆï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0">https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0</a>ï¼›PWæ­£ç§¯æè®­ç»ƒæœªæ¥æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æ”¹å–„å­¦ç”Ÿçš„å­¦ä¹ æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08665v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>Aryabhata 1.0æ˜¯ä¸€æ¬¾é’ˆå¯¹å°åº¦è”è€ƒï¼ˆJEEï¼‰ä¼˜åŒ–çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œå‚æ•°ç²¾ç®€è‡³7Bã€‚å®ƒé€šè¿‡åˆå¹¶å¼ºå¤§çš„å¼€æ”¾æƒé‡æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œç»“åˆéªŒè¯çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹è¿›è¡Œè®­ç»ƒã€‚é‡‡ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åŠA2Cç›®æ ‡ã€å›¢ä½“ç›¸å¯¹ä¼˜åŠ¿è¯„ä¼°ç­‰æ–°æŠ€æœ¯ï¼Œæå‡æ€§èƒ½ã€‚Aryabhataåœ¨åˆ†å¸ƒå†…ï¼ˆJEE Main 2025ï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆMATHï¼ŒGSM8Kï¼‰çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä½œä¸ºè€ƒè¯•ä¸­å¿ƒç†æ¨¡å‹çš„å¼€ç«¯ï¼ŒAryabhataå·²å…¬å¼€å‘å¸ƒä»¥å¾æ±‚ç¤¾åŒºåé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<p>*Aryabhata 1.0æ˜¯é’ˆå¯¹å°åº¦è”è€ƒï¼ˆJEEï¼‰çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œä¸“ä¸ºå°åº¦å­¦æœ¯è€ƒè¯•è®¾è®¡ã€‚<br>*æ¨¡å‹é€šè¿‡åˆå¹¶å¼€æ”¾æƒé‡æ¨ç†æ¨¡å‹å¹¶ä¼˜åŒ–å‚æ•°è¾¾åˆ°7Bè§„æ¨¡ã€‚<br>*Aryabhata 1.0é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè¯¾ç¨‹å­¦ä¹ ï¼Œç»“åˆéªŒè¯çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚<br>*å¼ºåŒ–å­¦ä¹ æŠ€æœ¯RLVRä¸A2Cç›®æ ‡ã€å›¢ä½“ç›¸å¯¹ä¼˜åŠ¿è¯„ä¼°ç­‰æ–¹æ³•ç”¨äºè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚<br>*Aryabhataåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„æµ‹è¯•ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2336a102b2f2b183ecb657bb01f596ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92d8fbcc6f0655a6c4f6c0b63adbe2af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c04b16e74b504f1ac68318931ba86e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-656bf66b8fb335cabc2f735889d6b53a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36418397c22e384dd9e80ec778d5c8e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758e9b9cb81577b9e4930122aae8d6b3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MiGrATe-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time"><a href="#MiGrATe-Mixed-Policy-GRPO-for-Adaptation-at-Test-Time" class="headerlink" title="MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time"></a>MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time</h2><p><strong>Authors:Peter Phan, Dhruv Agarwal, Kavitha Srinivas, Horst Samulowitz, Pavan Kapanipathi, Andrew McCallum</strong></p>
<p>Large language models (LLMs) are increasingly being applied to black-box optimization tasks, from program synthesis to molecule design. Prior work typically leverages in-context learning to iteratively guide the model towards better solutions. Such methods, however, often struggle to balance exploration of new solution spaces with exploitation of high-reward ones. Recently, test-time training (TTT) with synthetic data has shown promise in improving solution quality. However, the need for hand-crafted training data tailored to each task limits feasibility and scalability across domains. To address this problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a search algorithm to adapt LLMs at inference without requiring external training data. MiGrATe operates via a mixed-policy group construction procedure that combines on-policy sampling with two off-policy data selection techniques: greedy sampling, which selects top-performing past completions, and neighborhood sampling (NS), which generates completions structurally similar to high-reward ones. Together, these components bias the policy gradient towards exploitation of promising regions in solution space, while preserving exploration through on-policy sampling. We evaluate MiGrATe on three challenging domains-word search, molecule optimization, and hypothesis+program induction on the Abstraction and Reasoning Corpus (ARC)-and find that it consistently outperforms both inference-only and TTT baselines, demonstrating the potential of online TTT as a solution for complex search tasks without external supervision. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«åº”ç”¨äºé»‘ç®±ä¼˜åŒ–ä»»åŠ¡ï¼Œä»ç¨‹åºåˆæˆåˆ°åˆ†å­è®¾è®¡ã€‚ä»¥å‰çš„å·¥ä½œé€šå¸¸åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ¥é€æ­¥å¼•å¯¼æ¨¡å‹èµ°å‘æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å¾ˆéš¾å¹³è¡¡æ¢ç´¢æ–°çš„è§£å†³æ–¹æ¡ˆç©ºé—´ä¸åˆ©ç”¨é«˜å›æŠ¥ç©ºé—´ä¹‹é—´çš„å…³ç³»ã€‚æœ€è¿‘ï¼Œä½¿ç”¨åˆæˆæ•°æ®çš„æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰åœ¨æ”¹è¿›è§£å†³æ–¹æ¡ˆè´¨é‡æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ã€‚ç„¶è€Œï¼Œéœ€è¦é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ‰‹å·¥åˆ¶ä½œè®­ç»ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„å¯è¡Œæ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MiGrATeæ–¹æ³•â€”â€”ä¸€ç§åœ¨çº¿TTTæ–¹æ³•ï¼Œå®ƒä½¿ç”¨GRPOä½œä¸ºæœç´¢ç®—æ³•ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‚åº”LLMï¼Œè€Œæ— éœ€å¤–éƒ¨è®­ç»ƒæ•°æ®ã€‚MiGrATeé€šè¿‡æ··åˆç­–ç•¥å°ç»„æ„å»ºç¨‹åºæ¥è¿è¡Œï¼Œè¯¥ç¨‹åºç»“åˆäº†åŸºäºç­–ç•¥çš„é‡‡æ ·ä¸ä¸¤ç§åŸºäºéç­–ç•¥çš„æ•°æ®é€‰æ‹©æŠ€æœ¯ï¼šè´ªå©ªé‡‡æ ·ï¼Œé€‰æ‹©è¡¨ç°æœ€ä½³çš„è¿‡å»å®Œæˆï¼›ä»¥åŠé‚»åŸŸé‡‡æ ·ï¼ˆNSï¼‰ï¼Œç”Ÿæˆä¸é«˜å›æŠ¥ç»“æ„ç›¸ä¼¼çš„å®Œæˆã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œä½¿ç­–ç•¥æ¢¯åº¦åå‘äºåˆ©ç”¨è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­å‰æ™¯å¹¿é˜”çš„åŒºåŸŸï¼ŒåŒæ—¶é€šè¿‡åŸºäºç­–ç•¥çš„é‡‡æ ·ä¿æŒæ¢ç´¢ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸâ€”â€”å•è¯æœç´¢ã€åˆ†å­ä¼˜åŒ–å’ŒæŠ½è±¡æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰ä¸Šçš„å‡è®¾+ç¨‹åºå½’çº³â€”â€”è¯„ä¼°äº†MiGrATeï¼Œå‘ç°å®ƒå§‹ç»ˆä¼˜äºä»…æ¨ç†å’ŒTTTåŸºçº¿ï¼Œè¯æ˜äº†åœ¨çº¿TTTä½œä¸ºè§£å†³å¤æ‚æœç´¢ä»»åŠ¡çš„æ–¹æ³•çš„æ½œåŠ›ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08641v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é»‘è‰²ä¼˜åŒ–ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¦‚ç¨‹åºåˆæˆåˆ°åˆ†å­è®¾è®¡ç­‰é¢†åŸŸã€‚å…ˆå‰çš„å·¥ä½œé€šå¸¸åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ¥å¼•å¯¼æ¨¡å‹å¯»æ‰¾æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¾€å¾€éš¾ä»¥å¹³è¡¡æ¢ç´¢æ–°è§£ç©ºé—´ä¸åˆ©ç”¨é«˜å›æŠ¥è§£ç©ºé—´çš„å…³ç³»ã€‚æœ€è¿‘ï¼Œä½¿ç”¨åˆæˆæ•°æ®çš„æµ‹è¯•æ—¶é—´è®­ç»ƒï¼ˆTTTï¼‰åœ¨æ”¹è¿›è§£å†³æ–¹æ¡ˆè´¨é‡æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ¯é¡¹ä»»åŠ¡æ‰‹å·¥åˆ¶ä½œè®­ç»ƒæ•°æ®çš„éœ€è¦é™åˆ¶äº†å…¶åœ¨ä¸åŒé¢†åŸŸçš„å¯è¡Œæ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MiGrATeæ–¹æ³•â€”â€”ä¸€ç§åœ¨çº¿TTTï¼Œä½¿ç”¨GRPOä½œä¸ºæœç´¢ç®—æ³•ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‚åº”LLMï¼Œæ— éœ€å¤–éƒ¨è®­ç»ƒæ•°æ®ã€‚MiGrATeé€šè¿‡æ··åˆç­–ç•¥ç¾¤ä½“æ„å»ºç¨‹åºï¼Œç»“åˆåœ¨ç­–ç•¥é‡‡æ ·ä¸ä¸¤ç§ç¦»ç­–ç•¥æ•°æ®é€‰æ‹©æŠ€æœ¯ï¼šè´ªå©ªé‡‡æ ·ï¼Œé€‰æ‹©è¡¨ç°æœ€ä½³çš„è¿‡å»å®Œæˆï¼›ä»¥åŠé‚»åŸŸé‡‡æ ·ï¼ˆNSï¼‰ï¼Œç”Ÿæˆä¸é«˜å›æŠ¥è§£ç»“æ„ç›¸ä¼¼çš„å®Œæˆã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œä½¿æ”¿ç­–æ¢¯åº¦åå‘äºåˆ©ç”¨è§£ç©ºé—´ä¸­æœ‰å¸Œæœ›çš„åŒºåŸŸï¼ŒåŒæ—¶é€šè¿‡ç­–ç•¥é‡‡æ ·ä¿æŒæ¢ç´¢ã€‚æˆ‘ä»¬åœ¨å­—æœç´¢ã€åˆ†å­ä¼˜åŒ–å’ŒæŠ½è±¡æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰ä¸Šçš„å‡è®¾+ç¨‹åºå½’çº³ç­‰ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸè¯„ä¼°äº†MiGrATeï¼Œå‘ç°å®ƒå§‹ç»ˆä¼˜äºä»…æ¨ç†å’ŒTTTåŸºçº¿ï¼Œè¡¨æ˜åœ¨çº¿TTTä½œä¸ºå¤æ‚æœç´¢ä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆå…·æœ‰æ½œåŠ›ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«å¹¿æ³›åº”ç”¨äºé»‘è‰²ä¼˜åŒ–ä»»åŠ¡ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ åœ¨å¼•å¯¼æ¨¡å‹å¯»æ‰¾è§£å†³æ–¹æ¡ˆæ–¹é¢å‘æŒ¥ä½œç”¨ï¼Œä½†å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æµ‹è¯•æ—¶é—´è®­ç»ƒï¼ˆTTTï¼‰ä½¿ç”¨åˆæˆæ•°æ®æé«˜äº†è§£å†³æ–¹æ¡ˆè´¨é‡ï¼Œä½†æ‰‹å·¥åˆ¶ä½œè®­ç»ƒæ•°æ®çš„éœ€è¦é™åˆ¶äº†å…¶è·¨é¢†åŸŸåº”ç”¨ã€‚</li>
<li>MiGrATeæ˜¯ä¸€ç§åœ¨çº¿TTTæ–¹æ³•ï¼Œä½¿ç”¨GRPOæœç´¢ç®—æ³•é€‚åº”LLMï¼Œæ— éœ€å¤–éƒ¨è®­ç»ƒæ•°æ®ã€‚</li>
<li>MiGrATeé€šè¿‡æ··åˆç­–ç•¥ç¾¤ä½“æ„å»ºç¨‹åºï¼Œç»“åˆåœ¨ç­–ç•¥é‡‡æ ·ä¸ç¦»ç­–ç•¥æ•°æ®é€‰æ‹©æŠ€æœ¯æ¥æé«˜æ”¿ç­–æ¢¯åº¦çš„æ•ˆç‡å’Œæ¢ç´¢èƒ½åŠ›ã€‚</li>
<li>MiGrATeåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§é¢†åŸŸä¸Šè¡¨ç°å‡ºè¶…è¶Šä»…æ¨ç†å’ŒTTTåŸºçº¿çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-765fd7affaed2e5575e16d37773b7397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd25fd7b9446a31b60ddafd4046a89d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="InternBootcamp-Technical-Report-Boosting-LLM-Reasoning-with-Verifiable-Task-Scaling"><a href="#InternBootcamp-Technical-Report-Boosting-LLM-Reasoning-with-Verifiable-Task-Scaling" class="headerlink" title="InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable   Task Scaling"></a>InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable   Task Scaling</h2><p><strong>Authors:Peiji Li, Jiasheng Ye, Yongkang Chen, Yichuan Ma, Zijie Yu, Kedi Chen, Ganqu Cui, Haozhan Li, Jiacheng Chen, Chengqi Lyu, Wenwei Zhang, Linyang Li, Qipeng Guo, Dahua Lin, Bowen Zhou, Kai Chen</strong></p>
<p>Large language models (LLMs) have revolutionized artificial intelligence by enabling complex reasoning capabilities. While recent advancements in reinforcement learning (RL) have primarily focused on domain-specific reasoning tasks (e.g., mathematics or code generation), real-world reasoning scenarios often require models to handle diverse and complex environments that narrow-domain benchmarks cannot fully capture. To address this gap, we present InternBootcamp, an open-source framework comprising 1000+ domain-diverse task environments specifically designed for LLM reasoning research. Our codebase offers two key functionalities: (1) automated generation of unlimited training&#x2F;testing cases with configurable difficulty levels, and (2) integrated verification modules for objective response evaluation. These features make InternBootcamp fundamental infrastructure for RL-based model optimization, synthetic data generation, and model evaluation. Although manually developing such a framework with enormous task coverage is extremely cumbersome, we accelerate the development procedure through an automated agent workflow supplemented by manual validation protocols, which enables the task scope to expand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an automatically generated benchmark for comprehensive performance assessment. Evaluation reveals that frontier models still underperform in many reasoning tasks, while training with InternBootcamp provides an effective way to significantly improve performance, leading to our 32B model that achieves state-of-the-art results on Bootcamp-EVAL and excels on other established benchmarks. In particular, we validate that consistent performance gains come from including more training tasks, namely \textbf{task scaling}, over two orders of magnitude, offering a promising route towards capable reasoning generalist. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å®ç°å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ€èµ·äº†é©å‘½ã€‚å°½ç®¡æœ€è¿‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›å±•ä¸»è¦å…³æ³¨ç‰¹å®šé¢†åŸŸçš„æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦æˆ–ä»£ç ç”Ÿæˆï¼‰ï¼Œä½†ç°å®ä¸–ç•Œçš„æ¨ç†åœºæ™¯é€šå¸¸éœ€è¦æ¨¡å‹å¤„ç†å¤šæ ·ä¸”å¤æ‚çš„ç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒæ˜¯ç‹­çª„é¢†åŸŸçš„åŸºå‡†æµ‹è¯•æ— æ³•å®Œå…¨æ•æ‰åˆ°çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†InternBootcampï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1000å¤šä¸ªè·¨é¢†åŸŸä»»åŠ¡ç¯å¢ƒçš„å¼€æºæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºLLMæ¨ç†ç ”ç©¶ã€‚æˆ‘ä»¬çš„ä»£ç åº“æä¾›äº†ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼šï¼ˆ1ï¼‰èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå…·æœ‰å¯é…ç½®éš¾åº¦çº§åˆ«çš„æ— é™è®­ç»ƒå’Œæµ‹è¯•æ¡ˆä¾‹ï¼›ï¼ˆ2ï¼‰é›†æˆäº†ç”¨äºå®¢è§‚å“åº”è¯„ä¼°çš„éªŒè¯æ¨¡å—ã€‚è¿™äº›åŠŸèƒ½ä½¿InternBootcampæˆä¸ºåŸºäºRLçš„æ¨¡å‹ä¼˜åŒ–ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œæ¨¡å‹è¯„ä¼°çš„åŸºæœ¬è®¾æ–½ã€‚è™½ç„¶æ‰‹åŠ¨å¼€å‘è¿™æ ·ä¸€ä¸ªå…·æœ‰å·¨å¤§ä»»åŠ¡è¦†ç›–èŒƒå›´çš„æ¡†æ¶éå¸¸ç¹çï¼Œä½†æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨åŒ–ä»£ç†å·¥ä½œæµç¨‹è¾…ä»¥æ‰‹åŠ¨éªŒè¯åè®®æ¥åŠ é€Ÿå¼€å‘ç¨‹åºï¼Œè¿™èƒ½å¤Ÿä½¿ä»»åŠ¡èŒƒå›´è¿…é€Ÿæ‰©å±•ã€‚é€šè¿‡è¿™äº›è®­ç»ƒè¥ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å»ºç«‹äº†Bootcamp-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…¨é¢æ€§èƒ½è¯„ä¼°çš„è‡ªåŠ¨ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹åœ¨è®¸å¤šæ¨ç†ä»»åŠ¡ä¸­ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œè€Œä½¿ç”¨InternBootcampè¿›è¡Œè®­ç»ƒæ˜¯æœ‰æ•ˆæé«˜æ€§èƒ½çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ï¼Œä½¿æˆ‘ä»¬çš„32Bæ¨¡å‹åœ¨Bootcamp-EVALä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨å…¶ä»–æ—¢å®šåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬ç‰¹åˆ«éªŒè¯äº†ï¼Œé€šè¿‡åŒ…å«æ›´å¤šçš„è®­ç»ƒä»»åŠ¡ï¼Œå³<strong>ä»»åŠ¡æ‰©å±•</strong>ï¼Œæ€§èƒ½æé«˜è¶…è¿‡ä¸¤ä¸ªæ•°é‡çº§ï¼Œè¿™ä¸ºæœ‰èƒ½åŠ›è¿›è¡Œæ¨ç†çš„é€šæ‰æä¾›äº†ä¸€æ¡æœ‰å‰é€”çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08636v1">PDF</a> InternBootcamp Tech Report</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»é€šè¿‡å®ç°å¤æ‚çš„æ¨ç†èƒ½åŠ›è€Œå½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šé¢†åŸŸçš„æ¨ç†ä»»åŠ¡ä¸Šï¼Œä½†ç°å®ä¸–ç•Œä¸­çš„æ¨ç†åœºæ™¯éœ€è¦æ¨¡å‹å¤„ç†å¤šæ ·ä¸”å¤æ‚çš„ç¯å¢ƒï¼Œè¿™æ˜¯ç‹­çª„é¢†åŸŸçš„åŸºå‡†æµ‹è¯•æ— æ³•å®Œå…¨æ•æ‰åˆ°çš„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†InternBootcampï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1000å¤šä¸ªä¸åŒé¢†åŸŸä»»åŠ¡ç¯å¢ƒçš„å¼€æºæ¡†æ¶ï¼Œä¸“ä¸ºLLMæ¨ç†ç ”ç©¶è®¾è®¡ã€‚è¯¥ä»£ç åº“æä¾›ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼šï¼ˆ1ï¼‰å¯é…ç½®éš¾åº¦çº§åˆ«çš„æ— é™è®­ç»ƒ&#x2F;æµ‹è¯•æ¡ˆä¾‹çš„è‡ªåŠ¨ç”Ÿæˆï¼›ï¼ˆ2ï¼‰ç”¨äºå®¢è§‚å“åº”è¯„ä¼°çš„é›†æˆéªŒè¯æ¨¡å—ã€‚è¿™äº›ç‰¹ç‚¹ä½¿InternBootcampæˆä¸ºåŸºäºRLçš„æ¨¡å‹ä¼˜åŒ–ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œæ¨¡å‹è¯„ä¼°çš„åŸºæœ¬è®¾æ–½ã€‚æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨åŒ–ä»£ç†å·¥ä½œæµç¨‹å’Œè¡¥å……æ‰‹åŠ¨éªŒè¯åè®®æ¥åŠ é€Ÿå¼€å‘è¿‡ç¨‹ï¼Œä½¿ä»»åŠ¡èŒƒå›´èƒ½å¤Ÿè¿…é€Ÿæ‰©å±•ã€‚é€šè¿‡å®ä¹ è®­ç»ƒè¥ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å»ºç«‹äº†è‡ªåŠ¨ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•Bootcamp-EVALï¼Œç”¨äºå…¨é¢æ€§èƒ½è¯„ä¼°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå‰æ²¿æ¨¡å‹åœ¨è®¸å¤šæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä»æ¬ ä½³ï¼Œè€Œé€šè¿‡InternBootcampè®­ç»ƒæä¾›äº†ä¸€ç§æœ‰æ•ˆæé«˜æ€§èƒ½çš„æ–¹æ³•ï¼Œä½¿æˆ‘ä»¬çš„32Bæ¨¡å‹åœ¨Bootcamp-EVALä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³å¹¶åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬éªŒè¯äº†é€šè¿‡åŒ…æ‹¬æ›´å¤šçš„è®­ç»ƒä»»åŠ¡ï¼Œå³ä»»åŠ¡æ‰©å±•è¶…è¿‡ä¸¤ä¸ªæ•°é‡çº§ï¼Œæ˜¯å®ç°æœ‰èƒ½åŠ›æ¨ç†ä¸“å®¶çš„æœ‰å‰é€”çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ä¸­çš„å¤æ‚æ¨ç†èƒ½åŠ›å‘å±•ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†é¢ä¸´å¤„ç†å¤šæ ·åŒ–å’Œå¤æ‚ç¯å¢ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>InternBootcampæ¡†æ¶åŒ…å«1000å¤šä¸ªä»»åŠ¡ç¯å¢ƒï¼Œç”¨äºLLMæ¨ç†ç ”ç©¶ï¼Œæ”¯æŒè‡ªåŠ¨æ¡ˆä¾‹ç”Ÿæˆå’Œå“åº”è¯„ä¼°ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨åŒ–ä»£ç†å·¥ä½œæµç¨‹å’Œæ‰‹åŠ¨éªŒè¯ç›¸ç»“åˆï¼ŒåŠ é€Ÿäº†å¼€å‘è¿‡ç¨‹å¹¶æ‰©å¤§äº†ä»»åŠ¡èŒƒå›´ã€‚</li>
<li>Bootcamp-EVALåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºå‰æ²¿æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ¬ ä½³ï¼Œè€ŒInternBootcampè®­ç»ƒèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57f550c44d60c35ff5eab12e1d3083e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef3f83f70a10889184f99cc9da071142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50fd4aa183230620ddfa5e819a8ffcc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b731fcea88a401dc6b3953579f705b3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d590820a29cbb4a0c830a9a9806d800.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-395502d424cacf33502decd738f5235d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00c7b30aaf0c9394d7d7659dee507344.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture"><a href="#AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture" class="headerlink" title="AgriGPT: a Large Language Model Ecosystem for Agriculture"></a>AgriGPT: a Large Language Model Ecosystem for Agriculture</h2><p><strong>Authors:Bo Yang, Yu Zhang, Lanfei Feng, Yunkui Chen, Jianyu Zhang, Xiao Xu, Nueraili Aierken, Yurui Li, Yuxuan Chen, Guijun Yang, Yong He, Runhe Huang, Shijian Li</strong></p>
<p>Despite the rapid progress of Large Language Models (LLMs), their application in agriculture remains limited due to the lack of domain-specific models, curated datasets, and robust evaluation frameworks. To address these challenges, we propose AgriGPT, a domain-specialized LLM ecosystem for agricultural usage. At its core, we design a multi-agent scalable data engine that systematically compiles credible data sources into Agri-342K, a high-quality, standardized question-answer (QA) dataset. Trained on this dataset, AgriGPT supports a broad range of agricultural stakeholders, from practitioners to policy-makers. To enhance factual grounding, we employ Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining dense retrieval, sparse retrieval, and multi-hop knowledge graph reasoning, thereby improving the LLMâ€™s reasoning reliability. For comprehensive evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks with varying types and complexities. Experiments demonstrate that AgriGPT significantly outperforms general-purpose LLMs on both domain adaptation and reasoning. Beyond the model itself, AgriGPT represents a modular and extensible LLM ecosystem for agriculture, comprising structured data construction, retrieval-enhanced generation, and domain-specific evaluation. This work provides a generalizable framework for developing scientific and industry-specialized LLMs. All models, datasets, and code will be released to empower agricultural communities, especially in underserved regions, and to promote open, impactful research. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•è¿…é€Ÿï¼Œä½†ç”±äºç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ã€ç²¾é€‰æ•°æ®é›†å’Œç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒä»¬åœ¨å†œä¸šé¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AgrGPTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå†œä¸šä½¿ç”¨çš„ä¸“ä¸šé¢†åŸŸLLMç”Ÿæ€ç³»ç»Ÿã€‚å…¶æ ¸å¿ƒæ˜¯æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“å¯æ‰©å±•æ•°æ®å¼•æ“ï¼Œè¯¥å¼•æ“ç³»ç»Ÿåœ°ç¼–è¯‘å¯ä¿¡æ•°æ®æºä»¥å½¢æˆAgr-342Ké«˜è´¨é‡æ ‡å‡†åŒ–é—®ç­”æ•°æ®é›†ã€‚ç»è¿‡æ­¤æ•°æ®é›†è®­ç»ƒçš„AgrGPTæ”¯æŒå¹¿æ³›çš„å†œä¸šåˆ©ç›Šç›¸å…³è€…ï¼Œä»å®è·µè€…åˆ°å†³ç­–è€…ã€‚ä¸ºäº†å¢å¼ºäº‹å®ä¾æ®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Tri-RAGï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å¯†é›†æ£€ç´¢ã€ç¨€ç–æ£€ç´¢å’Œå¤šè·³çŸ¥è¯†å›¾è°±æ¨ç†çš„ä¸‰é€šé“æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œä»è€Œæé«˜LLMçš„æ¨ç†å¯é æ€§ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AgrBench-13Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸åŒç±»å‹å’Œå¤æ‚æ€§çš„ä»»åŠ¡çš„åŸºå‡†å¥—ä»¶ã€‚å®éªŒè¡¨æ˜ï¼ŒAgrGPTåœ¨é¢†åŸŸé€‚åº”å’Œæ¨ç†æ–¹é¢å‡æ˜¾è‘—ä¼˜äºé€šç”¨LLMã€‚é™¤äº†æ¨¡å‹æœ¬èº«å¤–ï¼ŒAgrGPTæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å¯æ‰©å±•çš„å†œä¸šLLMç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç»“æ„åŒ–æ•°æ®æ„å»ºã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œç‰¹å®šé¢†åŸŸè¯„ä¼°ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªç”¨äºç§‘å­¦å’Œè¡Œä¸šä¸“ä¸šåŒ–LLMçš„é€šç”¨æ¡†æ¶ã€‚æ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç éƒ½å°†è¢«å‘å¸ƒä»¥å¸®åŠ©å†œä¸šç¤¾åŒºï¼Œç‰¹åˆ«æ˜¯æœåŠ¡ä¸è¶³çš„åœ°åŒºï¼Œå¹¶ä¿ƒè¿›å¼€æ”¾ä¸”æœ‰å½±å“åŠ›ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08632v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å†œä¸šé¢†åŸŸåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ä¸Šä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹é¢†åŸŸç‰¹å®šæ¨¡å‹ã€ç²¾é€‰æ•°æ®é›†å’Œç¨³å¥çš„è¯„ä¼°æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæå‡ºAgriGPTï¼Œä¸€ä¸ªé’ˆå¯¹å†œä¸šä½¿ç”¨çš„é¢†åŸŸä¸“ä¸šåŒ–LLMç”Ÿæ€ç³»ç»Ÿã€‚å…¶æ ¸å¿ƒè®¾è®¡äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“å¯æ‰©å±•æ•°æ®å¼•æ“ï¼Œç³»ç»Ÿåœ°ç¼–è¯‘å¯ä¿¡æ•°æ®æºå½¢æˆAgr-342Ké«˜è´¨é‡æ ‡å‡†åŒ–é—®ç­”æ•°æ®é›†ã€‚AgriGPTæ”¯æŒå¹¿æ³›çš„å†œä¸šåˆ©ç›Šç›¸å…³è€…ï¼ŒåŒ…æ‹¬å®è·µè€…åˆ°å†³ç­–è€…ã€‚é€šè¿‡é‡‡ç”¨Tri-RAGï¼ˆç»“åˆå¯†é›†æ£€ç´¢ã€ç¨€ç–æ£€ç´¢å’Œå¤šè·³çŸ¥è¯†å›¾è°±æ¨ç†çš„ä¸‰é€šé“æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼‰ï¼Œæé«˜LLMçš„æ¨ç†å¯é æ€§ã€‚åŒæ—¶å¼•å…¥AgriBench-13KåŸºå‡†å¥—ä»¶è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒAgriGPTåœ¨é¢†åŸŸé€‚åº”æ€§å’Œæ¨ç†æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºé€šç”¨LLMã€‚æ­¤å¤–ï¼ŒAgriGPTæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„å†œä¸šLLMç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç»“æ„åŒ–æ•°æ®æ„å»ºã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé¢†åŸŸç‰¹å®šè¯„ä¼°ã€‚æ­¤å·¥ä½œæä¾›äº†ä¸€ä¸ªå¯æ¨å¹¿çš„æ¡†æ¶ï¼Œç”¨äºå¼€å‘ç§‘å­¦å’Œè¡Œä¸šä¸“ä¸šåŒ–çš„LLMï¼Œå°†ä¸ºå†œä¸šç¤¾åŒºå°¤å…¶æ˜¯è¢«å¿½è§†çš„åœ°åŒºå¸¦æ¥èµ‹èƒ½ï¼Œå¹¶æ¨åŠ¨å¼€æ”¾ã€æœ‰å½±å“çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†œä¸šé¢†åŸŸåœ¨å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸Šé¢ä¸´ç‰¹å®šæ¨¡å‹å’Œæ•°æ®é›†ç¼ºä¹ä»¥åŠè¯„ä¼°æ¡†æ¶ä¸å¥å…¨çš„æŒ‘æˆ˜ã€‚</li>
<li>AgriGPTæ˜¯ä¸€ä¸ªé’ˆå¯¹å†œä¸šé¢†åŸŸçš„LLMç”Ÿæ€ç³»ç»Ÿï¼Œè§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>AgriGPTé€šè¿‡å¤šæ™ºèƒ½ä½“æ•°æ®å¼•æ“ç³»ç»Ÿåœ°ç¼–è¯‘æ•°æ®ï¼Œå½¢æˆAgr-342Ké«˜è´¨é‡æ ‡å‡†åŒ–é—®ç­”æ•°æ®é›†ã€‚</li>
<li>AgriGPTæ”¯æŒå¹¿æ³›çš„å†œä¸šåˆ©ç›Šç›¸å…³è€…ï¼Œå¹¶é‡‡ç”¨äº†Tri-RAGæŠ€æœ¯æé«˜æ¨ç†å¯é æ€§ã€‚</li>
<li>AgriGPTå¼•å…¥AgriBench-13KåŸºå‡†å¥—ä»¶è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºé€šç”¨LLMã€‚</li>
<li>AgriGPTæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•çš„ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…æ‹¬ç»“æ„åŒ–æ•°æ®æ„å»ºã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œé¢†åŸŸç‰¹å®šè¯„ä¼°ç­‰å¤šä¸ªç»„ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df6bbfdd0f54e743c0227d071e8cd1f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a50c15c3ad2f90c1b956994421cc1dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc16aa53ce5874b56749ebb285408ad5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e11dd0369d2b6e3a39a0f8223ec4a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35bbbdfe05135e7cc8033e1dcb066398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da3a6cc585f055acb5ea179d7ca072f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d657affb1098c46e45611da02b692f86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bf92b07c2729e4a8c02f32fb45e08f4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding"><a href="#DocThinker-Explainable-Multimodal-Large-Language-Models-with-Rule-based-Reinforcement-Learning-for-Document-Understanding" class="headerlink" title="DocThinker: Explainable Multimodal Large Language Models with Rule-based   Reinforcement Learning for Document Understanding"></a>DocThinker: Explainable Multimodal Large Language Models with Rule-based   Reinforcement Learning for Document Understanding</h2><p><strong>Authors:Wenwen Yu, Zhibo Yang, Yuliang Liu, Xiang Bai</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in document understanding. However, their reasoning processes remain largely black-box, making it difficult to ensure reliability and trustworthiness, especially in high-stakes domains such as legal, financial, and medical document analysis. Existing methods use fixed Chain-of-Thought (CoT) reasoning with supervised fine-tuning (SFT) but suffer from catastrophic forgetting, poor adaptability, and limited generalization across domain tasks. In this paper, we propose DocThinker, a rule-based Reinforcement Learning (RL) framework for dynamic inference-time reasoning. Instead of relying on static CoT templates, DocThinker autonomously refines reasoning strategies via policy learning, generating explainable intermediate results, including structured reasoning processes, rephrased questions, regions of interest (RoI) supporting the answer, and the final answer. By integrating multi-objective rule-based rewards and KL-constrained optimization, our method mitigates catastrophic forgetting and enhances both adaptability and transparency. Extensive experiments on multiple benchmarks demonstrate that DocThinker significantly improves generalization while producing more explainable and human-understandable reasoning steps. Our findings highlight RL as a powerful alternative for enhancing explainability and adaptability in MLLM-based document understanding. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/wenwenyu/DocThinker">https://github.com/wenwenyu/DocThinker</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æ¡£ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»ç„¶æ˜¯é»‘ç®±ï¼Œå¾ˆéš¾ä¿è¯å¯é æ€§å’Œå¯ä¿¡åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹ã€é‡‘èå’ŒåŒ»ç–—æ–‡æ¡£åˆ†æç­‰é«˜é£é™©é¢†åŸŸã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨å›ºå®šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†å­˜åœ¨ç¾éš¾æ€§é—å¿˜ã€é€‚åº”æ€§å·®å’Œè·¨åŸŸä»»åŠ¡æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DocThinkerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€æ¨ç†æ—¶é—´æ¨ç†ã€‚DocThinkerä¸ä¾èµ–äºé™æ€çš„CoTæ¨¡æ¿ï¼Œè€Œæ˜¯é€šè¿‡å­¦ä¹ ç­–ç•¥è‡ªä¸»ä¼˜åŒ–æ¨ç†ç­–ç•¥ï¼Œç”Ÿæˆå¯è§£é‡Šçš„ä¸­é—´ç»“æœï¼ŒåŒ…æ‹¬ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€é‡è¿°é—®é¢˜ã€æ”¯æŒç­”æ¡ˆçš„å…´è¶£åŒºåŸŸï¼ˆRoIï¼‰å’Œæœ€ç»ˆç­”æ¡ˆã€‚é€šè¿‡æ•´åˆå¤šç›®æ ‡åŸºäºè§„åˆ™çš„å¥–åŠ±å’ŒKLçº¦æŸä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶æé«˜äº†é€‚åº”æ€§å’Œé€æ˜åº¦ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDocThinkeråœ¨æ”¹å–„æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ï¼Œäº§ç”Ÿäº†æ›´å¯è§£é‡Šå’Œäººä»¬å¯ç†è§£æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯æé«˜åŸºäºMLLMçš„æ–‡æ¡£ç†è§£çš„è§£é‡Šæ€§å’Œé€‚åº”æ€§çš„æœ‰åŠ›æ›¿ä»£æ–¹æ³•ã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wenwenyu/DocThinker%E3%80%82">https://github.com/wenwenyu/DocThinkerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08589v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æ¡£ç†è§£æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹å¤§å¤šå±äºé»‘ç®±æ“ä½œï¼Œéš¾ä»¥ç¡®ä¿å…¶å¯é æ€§å’Œå¯ä¿¡åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹ã€é‡‘èå’ŒåŒ»ç–—æ–‡æ¡£åˆ†æç­‰é«˜é£é™©é¢†åŸŸã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨å›ºå®šçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†å­˜åœ¨ç¾éš¾æ€§é—å¿˜ã€é€‚åº”æ€§å·®å’Œè·¨åŸŸä»»åŠ¡æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºDocThinkerï¼Œä¸€ä¸ªåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€æ¨ç†ã€‚DocThinkeré€šè¿‡æ”¿ç­–å­¦ä¹ è‡ªä¸»ä¼˜åŒ–æ¨ç†ç­–ç•¥ï¼Œç”Ÿæˆå¯è§£é‡Šçš„ä¸­é—´ç»“æœï¼ŒåŒ…æ‹¬ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€é‡è¿°é—®é¢˜ã€æ”¯æŒç­”æ¡ˆçš„åŒºåŸŸï¼ˆRoIï¼‰å’Œæœ€ç»ˆç­”æ¡ˆã€‚é€šè¿‡æ•´åˆå¤šç›®æ ‡è§„åˆ™å¥–åŠ±å’ŒKLçº¦æŸä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œæé«˜äº†é€‚åº”æ€§å’Œé€æ˜åº¦ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDocThinkeræ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶äº§ç”Ÿäº†æ›´å¯è§£é‡Šå’Œäººä»¬æ›´å®¹æ˜“ç†è§£æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯å¢å¼ºåŸºäºMLLMçš„æ–‡æ¡£ç†è§£çš„è§£é‡Šæ€§å’Œé€‚åº”æ€§çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æ¡£ç†è§£ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ¨ç†è¿‡ç¨‹ä¸å¯è§£é‡Šçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨å›ºå®šçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå­˜åœ¨ç¾éš¾æ€§é—å¿˜ã€é€‚åº”æ€§å·®å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>DocThinkeræ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œå¯ä»¥è‡ªä¸»ä¼˜åŒ–æ¨ç†ç­–ç•¥ï¼Œå¹¶ç”Ÿæˆå¯è§£é‡Šçš„ä¸­é—´ç»“æœã€‚</li>
<li>DocThinkeré€šè¿‡æ•´åˆå¤šç›®æ ‡è§„åˆ™å¥–åŠ±å’ŒKLçº¦æŸä¼˜åŒ–ï¼Œæé«˜äº†é€‚åº”æ€§å’Œé€æ˜åº¦ï¼Œå¹¶å‡è½»äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒDocThinkeræ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶äº§ç”Ÿäº†æ›´å¯è§£é‡Šå’Œäººä»¬æ›´å®¹æ˜“ç†è§£çš„æ¨ç†æ­¥éª¤ã€‚</li>
<li>DocThinkerä»£ç å°†å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5e6f6486bb5d08e93abe63f780b9e91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3efe250cd69195c8a30ce2cbcb2275b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f8cb13d97b5dbdade0602f02014f02.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SynLLM-A-Comparative-Analysis-of-Large-Language-Models-for-Medical-Tabular-Synthetic-Data-Generation-via-Prompt-Engineering"><a href="#SynLLM-A-Comparative-Analysis-of-Large-Language-Models-for-Medical-Tabular-Synthetic-Data-Generation-via-Prompt-Engineering" class="headerlink" title="SynLLM: A Comparative Analysis of Large Language Models for Medical   Tabular Synthetic Data Generation via Prompt Engineering"></a>SynLLM: A Comparative Analysis of Large Language Models for Medical   Tabular Synthetic Data Generation via Prompt Engineering</h2><p><strong>Authors:Arshia Ilaty, Hossein Shirazi, Hajar Homayouni</strong></p>
<p>Access to real-world medical data is often restricted due to privacy regulations, posing a significant barrier to the advancement of healthcare research. Synthetic data offers a promising alternative; however, generating realistic, clinically valid, and privacy-conscious records remains a major challenge. Recent advancements in Large Language Models (LLMs) offer new opportunities for structured data generation; however, existing approaches frequently lack systematic prompting strategies and comprehensive, multi-dimensional evaluation frameworks.   In this paper, we present SynLLM, a modular framework for generating high-quality synthetic medical tabular data using 20 state-of-the-art open-source LLMs, including LLaMA, Mistral, and GPT variants, guided by structured prompts. We propose four distinct prompt types, ranging from example-driven to rule-based constraints, that encode schema, metadata, and domain knowledge to control generation without model fine-tuning. Our framework features a comprehensive evaluation pipeline that rigorously assesses generated data across statistical fidelity, clinical consistency, and privacy preservation.   We evaluate SynLLM across three public medical datasets, including Diabetes, Cirrhosis, and Stroke, using 20 open-source LLMs. Our results show that prompt engineering significantly impacts data quality and privacy risk, with rule-based prompts achieving the best privacy-quality balance. SynLLM establishes that, when guided by well-designed prompts and evaluated with robust, multi-metric criteria, LLMs can generate synthetic medical data that is both clinically plausible and privacy-aware, paving the way for safer and more effective data sharing in healthcare research. </p>
<blockquote>
<p>è®¿é—®çœŸå®ä¸–ç•ŒåŒ»ç–—æ•°æ®å¾€å¾€å› éšç§è§„å®šè€Œå—åˆ°é™åˆ¶ï¼Œè¿™ç»™åŒ»ç–—ç ”ç©¶çš„å‘å±•å¸¦æ¥äº†é‡å¤§éšœç¢ã€‚åˆæˆæ•°æ®æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼›ç„¶è€Œï¼Œç”Ÿæˆç°å®ã€ä¸´åºŠæœ‰æ•ˆä¸”æ³¨é‡éšç§çš„è®°å½•ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä¸ºç»“æ„åŒ–æ•°æ®ç”Ÿæˆæä¾›äº†æ–°çš„æœºä¼šï¼›ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ç¼ºä¹ç³»ç»Ÿçš„æç¤ºç­–ç•¥å’Œç»¼åˆçš„å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynLLMï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨20ä¸ªæœ€æ–°å¼€æºLLMsç”Ÿæˆé«˜è´¨é‡åˆæˆåŒ»ç–—è¡¨æ ¼æ•°æ®çš„æ¨¡å—åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬LLaMAã€Mistralå’ŒGPTå˜ä½“ï¼Œç”±ç»“æ„åŒ–æç¤ºå¼•å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†å››ç§ä¸åŒçš„æç¤ºç±»å‹ï¼Œä»ç¤ºä¾‹é©±åŠ¨åˆ°åŸºäºè§„åˆ™çš„çº¦æŸï¼Œé€šè¿‡ç¼–ç æ¨¡å¼ã€å…ƒæ•°æ®å’Œé¢†åŸŸçŸ¥è¯†æ¥æ§åˆ¶ç”Ÿæˆï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸€ä¸ªç»¼åˆè¯„ä¼°æµç¨‹ï¼Œä¸¥æ ¼è¯„ä¼°ç”Ÿæˆæ•°æ®åœ¨ç»Ÿè®¡çœŸå®æ€§ã€ä¸´åºŠä¸€è‡´æ€§å’Œéšç§ä¿æŠ¤æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…±åŒ»ç–—æ•°æ®é›†ï¼ˆåŒ…æ‹¬ç³–å°¿ç—…ã€è‚ç¡¬åŒ–å’Œä¸­é£ï¼‰ä¸Šè¯„ä¼°äº†SynLLMï¼Œä½¿ç”¨20ä¸ªå¼€æºLLMsã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæç¤ºå·¥ç¨‹å¯¹æ•°æ®è´¨é‡å’Œéšç§é£é™©æœ‰é‡å¤§å½±å“ï¼ŒåŸºäºè§„åˆ™çš„æç¤ºå®ç°äº†æœ€ä½³çš„éšç§ä¸è´¨é‡å¹³è¡¡ã€‚SynLLMè¯æ˜ï¼Œåœ¨ç²¾å¿ƒè®¾è®¡æç¤ºå’Œç”¨ç¨³å¥çš„å¤šæŒ‡æ ‡æ ‡å‡†è¿›è¡Œè¯„ä¼°çš„æŒ‡å¯¼ä¸‹ï¼ŒLLMså¯ä»¥ç”Ÿæˆæ—¢ä¸´åºŠå¯è¡Œåˆæ³¨é‡éšç§çš„åˆæˆåŒ»ç–—æ•°æ®ï¼Œä¸ºåŒ»ç–—ç ”ç©¶ä¸­æ›´å®‰å…¨ã€æ›´æœ‰æ•ˆçš„æ•°æ®å…±äº«é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08529v1">PDF</a> 10 Pages, 2 Supplementary Pages, 6 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºSynLLMçš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨20ç§æœ€å…ˆè¿›çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆé«˜è´¨é‡åˆæˆåŒ»ç–—è¡¨æ ¼æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“æ„åŒ–æç¤ºå¼•å¯¼æ•°æ®ç”Ÿæˆï¼Œå¹¶æå‡ºå››ç§ä¸åŒçš„æç¤ºç±»å‹ï¼Œä»¥æ§åˆ¶æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜åŒ…å«ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°ç®¡é“ï¼Œå¯¹ç”Ÿæˆæ•°æ®è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼ŒåŒ…æ‹¬ç»Ÿè®¡çœŸå®æ€§ã€ä¸´åºŠä¸€è‡´æ€§å’Œéšç§ä¿æŠ¤ã€‚åœ¨ä¸‰ä¸ªå…¬å…±åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæç¤ºå·¥ç¨‹å¯¹æ•°æ®è´¨é‡å’Œéšç§é£é™©æœ‰é‡å¤§å½±å“ï¼Œè§„åˆ™æç¤ºåœ¨å¹³è¡¡éšç§å’Œè´¨é‡æ–¹é¢è¡¨ç°æœ€ä½³ã€‚å› æ­¤ï¼ŒSynLLMæ¡†æ¶è¯æ˜äº†åœ¨è‰¯å¥½çš„æç¤ºå’Œå¼ºå¤§çš„å¤šæŒ‡æ ‡è¯„ä¼°æ ‡å‡†ä¸‹ï¼ŒLLMså¯ä»¥ç”Ÿæˆæ—¢ç¬¦åˆä¸´åºŠå®é™…åˆæ³¨é‡éšç§çš„åˆæˆåŒ»ç–—æ•°æ®ï¼Œä¸ºåŒ»ç–—ç ”ç©¶çš„æ•°æ®å…±äº«æä¾›äº†æ›´å®‰å…¨æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç§è§„å®šé™åˆ¶äº†çœŸå®åŒ»ç–—æ•°æ®çš„è®¿é—®ï¼Œé˜»ç¢äº†åŒ»ç–—ä¿å¥ç ”ç©¶çš„å‘å±•ã€‚</li>
<li>åˆæˆæ•°æ®ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆå­˜åœ¨ç”Ÿæˆé«˜è´¨é‡ã€ç¬¦åˆä¸´åºŠå®é™…å’Œæ³¨é‡éšç§çš„è®°å½•çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–æ•°æ®ç”Ÿæˆæ–¹é¢æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>SynLLMæ˜¯ä¸€ä¸ªåˆ©ç”¨LLMsç”ŸæˆåˆæˆåŒ»ç–—æ•°æ®çš„æ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æç¤ºå¼•å¯¼æ•°æ®ç”Ÿæˆã€‚</li>
<li>SynLLMæå‡ºäº†å››ç§ä¸åŒçš„æç¤ºç±»å‹ï¼Œä»¥å®ç°å¯¹ç”Ÿæˆæ•°æ®çš„æ§åˆ¶ã€‚</li>
<li>SynLLMæ¡†æ¶åŒ…å«å…¨é¢çš„è¯„ä¼°ç®¡é“ï¼Œè¯„ä¼°ç”Ÿæˆæ•°æ®çš„ç»Ÿè®¡çœŸå®æ€§ã€ä¸´åºŠä¸€è‡´æ€§å’Œéšç§ä¿æŠ¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d614512fe8d5df0e182d31083db7a9b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-550ef4ff2849a17027b1331b1a8d3d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc768cd2e68a3d4dd6d7b372c0f572ca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Steerable-Pluralism-Pluralistic-Alignment-via-Few-Shot-Comparative-Regression"><a href="#Steerable-Pluralism-Pluralistic-Alignment-via-Few-Shot-Comparative-Regression" class="headerlink" title="Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative   Regression"></a>Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative   Regression</h2><p><strong>Authors:Jadie Adams, Brian Hu, Emily Veenhuis, David Joy, Bharadwaj Ravichandran, Aaron Bray, Anthony Hoogs, Arslan Basharat</strong></p>
<p>Large language models (LLMs) are currently aligned using techniques such as reinforcement learning from human feedback (RLHF). However, these methods use scalar rewards that can only reflect user preferences on average. Pluralistic alignment instead seeks to capture diverse user preferences across a set of attributes, moving beyond just helpfulness and harmlessness. Toward this end, we propose a steerable pluralistic model based on few-shot comparative regression that can adapt to individual user preferences. Our approach leverages in-context learning and reasoning, grounded in a set of fine-grained attributes, to compare response options and make aligned choices. To evaluate our algorithm, we also propose two new steerable pluralistic benchmarks by adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets, demonstrating the applicability of our approach to value-aligned decision-making and reward modeling, respectively. Our few-shot comparative regression approach is interpretable and compatible with different attributes and LLMs, while outperforming multiple baseline and state-of-the-art methods. Our work provides new insights and research directions in pluralistic alignment, enabling a more fair and representative use of LLMs and advancing the state-of-the-art in ethical AI. </p>
<blockquote>
<p>ç›®å‰ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯è¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åªèƒ½åæ˜ ç”¨æˆ·çš„å¹³å‡åå¥½ï¼Œä½¿ç”¨æ ‡é‡å¥–åŠ±ã€‚è€Œå¤šå…ƒå¯¹é½åˆ™æ—¨åœ¨æ•è·ä¸€ç»„å±æ€§ä¸­å¤šæ ·åŒ–çš„ç”¨æˆ·åå¥½ï¼Œè¶…è¶Šä»…ä»…çš„æœ‰ç”¨æ€§å’Œæ— å®³æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå°‘æ ·æœ¬æ¯”è¾ƒå›å½’çš„å¯æ§å¤šå…ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€‚åº”ä¸ªåˆ«ç”¨æˆ·åå¥½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ¨ç†ï¼Œä»¥ä¸€ç»„ç²¾ç»†çš„å±æ€§ä¸ºåŸºç¡€ï¼Œæ¯”è¾ƒå“åº”é€‰é¡¹å¹¶åšå‡ºå¯¹é½é€‰æ‹©ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„ç®—æ³•ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡é€‚åº”é“å¾·å®Œæ•´æ€§è¯­æ–™åº“ï¼ˆMICï¼‰å’ŒHelpSteer2æ•°æ®é›†ï¼Œæå‡ºäº†ä¸¤ä¸ªæ–°çš„å¯æ§å¤šå…ƒåŸºå‡†æµ‹è¯•ï¼Œåˆ†åˆ«å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»·å€¼å¯¹é½å†³ç­–å’Œå¥–åŠ±å»ºæ¨¡æ–¹é¢çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„å°‘æ ·æœ¬æ¯”è¾ƒå›å½’æ–¹æ³•æ˜¯å¯è§£é‡Šçš„ï¼Œå¹¶ä¸ä¸åŒçš„å±æ€§å’ŒLLMå…¼å®¹ï¼ŒåŒæ—¶ä¼˜äºå¤šä¸ªåŸºå‡†å’Œæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†å…³äºå¤šå…ƒå¯¹é½çš„æ–°è§è§£å’Œç ”ç©¶æ–¹å‘ï¼Œä¸ºå®ç°æ›´å…¬å¹³å’Œä»£è¡¨æ€§çš„LLMä½¿ç”¨ä»¥åŠæ¨åŠ¨ä¼¦ç†äººå·¥æ™ºèƒ½çš„æœ€æ–°å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08509v1">PDF</a> AIES â€˜25: Proceedings of the 2025 AAAI&#x2F;ACM Conference on AI, Ethics,   and Society</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›®å‰é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯è¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨çš„æ ‡é‡å¥–åŠ±åªèƒ½åæ˜ ç”¨æˆ·çš„å¹³å‡åå¥½ã€‚æœ¬ç ”ç©¶æå‡ºäº†è¶…è¶Šå•ä¸€çš„å¯¹é½æ–¹å¼ï¼Œè¿½æ±‚å¯¹ç”¨æˆ·åå¥½çš„å¤šæ ·æ•è·ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå°‘é‡æ¯”è¾ƒå›å½’çš„å¯æ§å¤šå…ƒåŒ–æ¨¡å‹ï¼Œèƒ½å¤Ÿé€‚åº”ä¸ªåˆ«ç”¨æˆ·çš„åå¥½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ¨ç†ï¼ŒåŸºäºä¸€ç»„ç²¾ç»†å±æ€§æ¥æ¯”è¾ƒå“åº”é€‰é¡¹å¹¶åšå‡ºå¯¹é½é€‰æ‹©ã€‚é€šè¿‡æ”¹ç¼–é“å¾·è¯šä¿¡è¯­æ–™åº“ï¼ˆMICï¼‰å’Œå¸®åŠ©Steer2æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªæ–°çš„å¯æ§å¤šå…ƒåŒ–åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»·å€¼å¯¹é½å†³ç­–å’Œå¥–åŠ±å»ºæ¨¡æ–¹é¢çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„å°‘é‡æ¯”è¾ƒå›å½’æ–¹æ³•æ˜¯å¯è§£é‡Šçš„ï¼Œä¸ä¸åŒçš„å±æ€§å’ŒLLMå…¼å®¹ï¼ŒåŒæ—¶ä¼˜äºå¤šä¸ªåŸºå‡†å’Œæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†æ–°çš„è§è§£å’Œç ”ç©¶æ–¹å‘åœ¨å¤šå…ƒåŒ–å¯¹é½æ–¹é¢ï¼Œä½¿LLMçš„ä½¿ç”¨æ›´åŠ å…¬å¹³å’Œä»£è¡¨æ€§ï¼Œå¹¶æ¨åŠ¨ä¼¦ç†äººå·¥æ™ºèƒ½çš„æœ€æ–°å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›®å‰ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œå¯¹é½ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ ‡é‡å¥–åŠ±åæ˜ ç”¨æˆ·å¹³å‡åå¥½ï¼Œæœ¬ç ”ç©¶è¿½æ±‚å¯¹ç”¨æˆ·åå¥½çš„å¤šæ ·æ•è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå°‘é‡æ¯”è¾ƒå›å½’çš„å¯æ§å¤šå…ƒåŒ–æ¨¡å‹ï¼Œèƒ½å¤Ÿé€‚åº”ä¸ªåˆ«ç”¨æˆ·çš„åå¥½ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç²¾ç»†å±æ€§æ¯”è¾ƒå“åº”é€‰é¡¹ï¼Œè¿›è¡Œå¯¹é½é€‰æ‹©ã€‚</li>
<li>é€šè¿‡æ”¹ç¼–é“å¾·è¯šä¿¡è¯­æ–™åº“å’Œå¸®åŠ©Steer2æ•°æ®é›†ï¼Œæå‡ºäº†ä¸¤ä¸ªæ–°çš„å¯æ§å¤šå…ƒåŒ–åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å°‘é‡æ¯”è¾ƒå›å½’æ–¹æ³•å…·æœ‰å¯è§£é‡Šæ€§ï¼Œå…¼å®¹ä¸åŒå±æ€§å’ŒLLMï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a03675840e18429f6fc60ead5a346540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d068c5d39a98d6f86781e5908ab9af09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833e49aeaf0daf63fcc3936cf8206d6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d6aeeb4358735dcf743915ae968c588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410cd24ba7d501d7cadf7c6503202753.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Generating-Query-Relevant-Document-Summaries-via-Reinforcement-Learning"><a href="#Generating-Query-Relevant-Document-Summaries-via-Reinforcement-Learning" class="headerlink" title="Generating Query-Relevant Document Summaries via Reinforcement Learning"></a>Generating Query-Relevant Document Summaries via Reinforcement Learning</h2><p><strong>Authors:Nitin Yadav, Changsung Kang, Hongwei Shang, Ming Sun</strong></p>
<p>E-commerce search engines often rely solely on product titles as input for ranking models with latency constraints. However, this approach can result in suboptimal relevance predictions, as product titles often lack sufficient detail to capture query intent. While product descriptions provide richer information, their verbosity and length make them unsuitable for real-time ranking, particularly for computationally expensive architectures like cross-encoder ranking models. To address this challenge, we propose ReLSum, a novel reinforcement learning framework designed to generate concise, query-relevant summaries of product descriptions optimized for search relevance. ReLSum leverages relevance scores as rewards to align the objectives of summarization and ranking, effectively overcoming limitations of prior methods, such as misaligned learning targets. The framework employs a trainable large language model (LLM) to produce summaries, which are then used as input for a cross-encoder ranking model. Experimental results demonstrate significant improvements in offline metrics, including recall and NDCG, as well as online user engagement metrics. ReLSum provides a scalable and efficient solution for enhancing search relevance in large-scale e-commerce systems. </p>
<blockquote>
<p>ç”µå­å•†åŠ¡æœç´¢å¼•æ“é€šå¸¸ä»…ä¾èµ–äº§å“æ ‡é¢˜ä½œä¸ºå…·æœ‰å»¶è¿Ÿçº¦æŸçš„æ’åæ¨¡å‹çš„è¾“å…¥ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´ç›¸å…³æ€§é¢„æµ‹ä¸ä½³ï¼Œå› ä¸ºäº§å“æ ‡é¢˜é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„ç»†èŠ‚æ¥æ•æ‰æŸ¥è¯¢æ„å›¾ã€‚è™½ç„¶äº§å“æè¿°æä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä½†å…¶å†—é•¿å’Œç¯‡å¹…ä½¿å…¶ä¸é€‚åˆå®æ—¶æ’åï¼Œç‰¹åˆ«æ˜¯å¯¹äºè®¡ç®—æˆæœ¬è¾ƒé«˜çš„æ¶æ„ï¼Œå¦‚äº¤å‰ç¼–ç å™¨æ’åæ¨¡å‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReLSumï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆé’ˆå¯¹äº§å“æè¿°çš„ç®€æ´ã€ä¸æŸ¥è¯¢ç›¸å…³çš„æ‘˜è¦ï¼Œå¹¶ä¼˜åŒ–æœç´¢ç›¸å…³æ€§ã€‚ReLSumåˆ©ç”¨ç›¸å…³æ€§åˆ†æ•°ä½œä¸ºå¥–åŠ±æ¥å¯¹é½æ‘˜è¦å’Œæ’åçš„ç›®æ ‡ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†å…ˆå‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚å­¦ä¹ ç›®æ ‡é”™ä½ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸€ä¸ªå¯è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆæ‘˜è¦ï¼Œç„¶åå°†å…¶ä½œä¸ºäº¤å‰ç¼–ç å™¨æ’åæ¨¡å‹çš„è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç¦»çº¿æŒ‡æ ‡ï¼ˆåŒ…æ‹¬å¬å›ç‡å’ŒNDCGï¼‰ä»¥åŠåœ¨çº¿ç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚ReLSumä¸ºå¢å¼ºå¤§è§„æ¨¡ç”µå­å•†åŠ¡ç³»ç»Ÿä¸­çš„æœç´¢ç›¸å…³æ€§æä¾›äº†å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08404v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹ç”µå•†æœç´¢å¼•æ“åœ¨å®æ—¶æ’åä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶ReLSumã€‚è¯¥æ¡†æ¶æ—¨åœ¨ç”Ÿæˆç®€æ´ä¸”ä¸æŸ¥è¯¢ç›¸å…³çš„äº§å“æè¿°æ‘˜è¦ï¼Œä»¥æé«˜æœç´¢ç›¸å…³æ€§ã€‚ReLSumåˆ©ç”¨ç›¸å…³æ€§åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œå¯¹é½æ‘˜è¦å’Œæ’åçš„ç›®æ ‡ï¼Œæé«˜ç¦»çº¿æŒ‡æ ‡å’Œç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡ã€‚ReLSumä¸ºå¤§è§„æ¨¡ç”µå•†ç³»ç»Ÿæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æé«˜æœç´¢ç›¸å…³æ€§çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç”µå•†æœç´¢å¼•æ“ä¾èµ–äºäº§å“æ ‡é¢˜ä½œä¸ºå®æ—¶æ’åæ¨¡å‹çš„è¾“å…¥ã€‚</li>
<li>äº§å“æ ‡é¢˜ç¼ºä¹è¯¦ç»†ä¿¡æ¯ï¼Œå¯¼è‡´ç›¸å…³æ€§é¢„æµ‹ä¸ç†æƒ³ã€‚</li>
<li>äº§å“æè¿°æä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä½†å…¶å†—é•¿å’Œå¤æ‚æ€§ä¸é€‚åˆå®æ—¶æ’åã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ReLSumï¼Œæ—¨åœ¨ç”Ÿæˆé’ˆå¯¹æŸ¥è¯¢ç›¸å…³çš„äº§å“æè¿°æ‘˜è¦ã€‚</li>
<li>ReLSumåˆ©ç”¨ç›¸å…³æ€§åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œå¯¹é½æ‘˜è¦å’Œæ’åçš„ç›®æ ‡ã€‚</li>
<li>ReLSumå®éªŒç»“æœæ˜¾ç¤ºåœ¨ç¦»çº¿æŒ‡æ ‡å’Œç”¨æˆ·å‚ä¸åº¦æŒ‡æ ‡ä¸Šçš„æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-de112c6376c208492211057c2d26f4a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47230d8a1328a74fc8ee3abb046ff3d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52ce98176f62980617bc4374da6dc85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb060830d7d7b53acb19ef7d4c082a82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a3b5d2a57911e6612e19dd41f1104a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2b39d9278b2dc665353318ba66e9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4100439065101e8592f9048f9101a93a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation"><a href="#REX-RAG-Reasoning-Exploration-with-Policy-Correction-in-Retrieval-Augmented-Generation" class="headerlink" title="REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation"></a>REX-RAG: Reasoning Exploration with Policy Correction in   Retrieval-Augmented Generation</h2><p><strong>Authors:Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang</strong></p>
<p>Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as â€œdead endsâ€, committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ­£æˆä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œå°†RLä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥ä½¿LLMåŠ¨æ€åœ°èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä»è€Œå®ç°æ›´åŠ æ˜æ™ºå’Œç¨³å¥çš„å†³ç­–ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æ”¿ç­–é©±åŠ¨è½¨è¿¹é‡‡æ ·è¿‡ç¨‹ä¸­å‘ç°äº†ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šLLMç»å¸¸é™·å…¥ä¸äº§ç”Ÿæ¨ç†è·¯å¾„çš„â€œæ­»èƒ¡åŒâ€ï¼Œå¯¼è‡´è¿‡äºè‡ªä¿¡ä½†é”™è¯¯çš„ç»“è®ºã€‚è¿™ä¸¥é‡é˜»ç¢äº†æ¢ç´¢å¹¶ç ´åäº†æœ‰æ•ˆçš„æ”¿ç­–ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†REX-RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æ”¿ç­–ä¿®æ­£æ¨ç†æ¢ç´¢ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œåœ¨ä¿æŒä¸¥æ ¼æ”¿ç­–å­¦ä¹ çš„æƒ…å†µä¸‹æ¢ç´¢æ›¿ä»£æ¨ç†è·¯å¾„ï¼Œé€šè¿‡åŸåˆ™æ€§çš„åˆ†å¸ƒä¿®æ­£ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æ··åˆé‡‡æ ·ç­–ç•¥ï¼Œå®ƒå°†ä¸€ç§æ–°çš„æ¢é’ˆé‡‡æ ·æ–¹æ³•ä¸æ¢ç´¢æ€§æç¤ºç›¸ç»“åˆï¼Œä»¥é€ƒç¦»æ­»èƒ¡åŒï¼›ï¼ˆ2ï¼‰æ”¿ç­–ä¿®æ­£æœºåˆ¶ï¼Œå®ƒé‡‡ç”¨é‡è¦æ€§é‡‡æ ·æ¥çº æ­£æ··åˆé‡‡æ ·å¼•èµ·çš„åˆ†å¸ƒåç§»ï¼Œä»è€Œå‡è½»æ¢¯åº¦ä¼°è®¡åå·®ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒREX-RAGåœ¨Qwen2.5-3Bä¸Šå¹³å‡æ€§èƒ½æå‡5.1%ï¼Œåœ¨Qwen2.5-7Bä¸Šç›¸æ¯”å¼ºå¤§çš„åŸºå‡†æµ‹è¯•æå‡3.6%ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiliLab/REX-RAG">https://github.com/MiliLab/REX-RAG</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08149v2">PDF</a> 17 pages, 4 figures; updated references</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç­–ç•¥é©±åŠ¨è½¨è¿¹é‡‡æ ·è¿‡ç¨‹ä¸­å­˜åœ¨æŒ‘æˆ˜ï¼šLLMç»å¸¸é™·å…¥æ— æ•ˆæ¨ç†è·¯å¾„ï¼ˆå³â€œæ­»èƒ¡åŒâ€ï¼‰ï¼Œå¯¼è‡´è¿‡äºè‡ªä¿¡çš„é”™è¯¯ç»“è®ºã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†REX-RAGæ¡†æ¶ï¼Œé€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥å’Œç­–ç•¥æ ¡æ­£æœºåˆ¶ï¼Œåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­æ¢ç´¢æ¨ç†è·¯å¾„ï¼ŒåŒæ—¶ä¿æŒä¸¥æ ¼ç­–ç•¥å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREX-RAGåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šå–å¾—äº†å¹³å‡æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆç»“åˆï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ç­–ç•¥é©±åŠ¨è½¨è¿¹é‡‡æ ·å­˜åœ¨æŒ‘æˆ˜ï¼šLLMæ˜“é™·å…¥â€œæ­»èƒ¡åŒâ€ï¼Œå¯¼è‡´é”™è¯¯ç»“è®ºã€‚</li>
<li>REX-RAGæ¡†æ¶é€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥æ¢ç´¢æ¨ç†è·¯å¾„ã€‚</li>
<li>REX-RAGå¼•å…¥æ–°å‹ç­–ç•¥æ ¡æ­£æœºåˆ¶ï¼Œé€šè¿‡é‡è¦æ€§é‡‡æ ·çº æ­£åˆ†å¸ƒåç§»ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒREX-RAGåœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šå®ç°æ€§èƒ½æå‡ã€‚</li>
<li>REX-RAGå…¬å¼€å¯ç”¨ï¼Œä¸”å±•ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acdf04e79179dd94161e066fd205dc6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-925c386281592c204daba0d2804ce46d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e924d5161b832f79d77e06401ce8e82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d954327711861415d73d9babc1240fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda33d128ca0a01c8ffa82a0b9b7edec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362fcfbb60b1eb85011d6e0922cda71c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning"><a href="#Audio-Thinker-Guiding-Audio-Language-Model-When-and-How-to-Think-via-Reinforcement-Learning" class="headerlink" title="Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning"></a>Audio-Thinker: Guiding Audio Language Model When and How to Think via   Reinforcement Learning</h2><p><strong>Authors:Shu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang, Meng Yu, Dong Yu</strong></p>
<p>Recent advancements in large language models, multimodal large language models, and large audio language models (LALMs) have significantly improved their reasoning capabilities through reinforcement learning with rule-based rewards. However, the explicit reasoning process has yet to show significant benefits for audio question answering, and effectively leveraging deep reasoning remains an open challenge, with LALMs still falling short of human-level auditory-language reasoning. To address these limitations, we propose Audio-Thinker, a reinforcement learning framework designed to enhance the reasoning capabilities of LALMs, with a focus on improving adaptability, consistency, and effectiveness. Our approach introduces an adaptive think accuracy reward, enabling the model to adjust its reasoning strategies based on task complexity dynamically. Furthermore, we incorporate an external reward model to evaluate the overall consistency and quality of the reasoning process, complemented by think-based rewards that help the model distinguish between valid and flawed reasoning paths during training. Experimental results demonstrate that our Audio-Thinker model outperforms existing reasoning-oriented LALMs across various benchmark tasks, exhibiting superior reasoning and generalization capabilities. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿›æ­¥ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹åœ¨éŸ³é¢‘é—®ç­”ä¸­å°šæœªæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆåˆ©ç”¨æ·±åº¦æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼ŒLALMåœ¨éŸ³é¢‘è¯­è¨€æ¨ç†æ–¹é¢ä»æœªèƒ½è¾¾åˆ°äººç±»æ°´å¹³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Thinkerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºLALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹æé«˜é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„æ€è€ƒå‡†ç¡®æ€§å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŸºäºä»»åŠ¡çš„å¤æ‚æ€§åŠ¨æ€åœ°è°ƒæ•´å…¶æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªå¤–éƒ¨å¥–åŠ±æ¨¡å‹ï¼Œä»¥è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„æ•´ä½“ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œè¾…ä»¥åŸºäºæ€è€ƒçš„å¥–åŠ±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†æœ‰æ•ˆçš„å’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Audio-Thinkeræ¨¡å‹åœ¨å„ç§åŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„LALMï¼Œå±•ç°å‡ºå“è¶Šçš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08039v2">PDF</a> preprint</p>
<p><strong>Summary</strong>ï¼š<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ã€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„è¿‘æœŸè¿›å±•ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå…¶æ¨ç†èƒ½åŠ›å·²æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œåœ¨éŸ³é¢‘é—®ç­”æ–¹é¢ï¼Œæ˜ç¡®æ¨ç†è¿‡ç¨‹å°šæœªæ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¦‚ä½•åˆ©ç”¨æ·±åº¦æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒLALMåœ¨éŸ³é¢‘è¯­è¨€æ¨ç†æ–¹é¢ä»è¾¾ä¸åˆ°äººç±»æ°´å¹³ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Audio-Thinkerï¼Œä¸€ä¸ªæ—¨åœ¨æé«˜LALMæ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹æé«˜å…¶é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ€è€ƒç²¾åº¦å¥–åŠ±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¨ç†è¿‡ç¨‹çš„æ•´ä½“ä¸€è‡´æ€§å’Œè´¨é‡ï¼Œè¾…ä»¥åŸºäºæ€è€ƒçš„å¥–åŠ±ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒºåˆ†æ­£ç¡®çš„å’Œé”™è¯¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Audio-Thinkeræ¨¡å‹åœ¨å„é¡¹åŸºå‡†ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„ä»¥æ¨ç†ä¸ºå¯¼å‘çš„LALMï¼Œå±•ç°å‡ºæ›´å‡ºè‰²çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆåŒ…æ‹¬éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éŸ³é¢‘é—®ç­”ä¸­çš„æ·±åº¦æ¨ç†åˆ©ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒLALMçš„æ¨ç†èƒ½åŠ›å°šæœªè¾¾åˆ°äººç±»æ°´å¹³ã€‚</li>
<li>æå‡ºäº†Audio-Thinkeræ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LALMåœ¨é€‚åº”æ€§ã€ä¸€è‡´æ€§å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Audio-Thinkerå¼•å…¥äº†è‡ªé€‚åº”æ€è€ƒç²¾åº¦å¥–åŠ±ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥ã€‚</li>
<li>ç»“åˆå¤–éƒ¨å¥–åŠ±æ¨¡å‹å’ŒåŸºäºæ€è€ƒçš„å¥–åŠ±æ¥è¯„ä¼°å’Œä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¯æ˜Audio-Thinkeræ¨¡å‹åœ¨åŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b39658f1384d0c6a10e9f9c168cad4cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac49b30637545901965c2041fff1d0ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80681e9b6da53937bbadf3abefc1bafa.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DIVER-A-Multi-Stage-Approach-for-Reasoning-intensive-Information-Retrieval"><a href="#DIVER-A-Multi-Stage-Approach-for-Reasoning-intensive-Information-Retrieval" class="headerlink" title="DIVER: A Multi-Stage Approach for Reasoning-intensive Information   Retrieval"></a>DIVER: A Multi-Stage Approach for Reasoning-intensive Information   Retrieval</h2><p><strong>Authors:Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, Jiahai Wang</strong></p>
<p>Retrieval-augmented generation has achieved strong performance on knowledge-intensive tasks where query-document relevance can be identified through direct lexical or semantic matches. However, many real-world queries involve abstract reasoning, analogical thinking, or multi-step inference, which existing retrievers often struggle to capture. To address this challenge, we present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive information retrieval. DIVER consists of four components: document processing to improve input quality, LLM-driven query expansion via iterative document interaction, a reasoning-enhanced retriever fine-tuned on synthetic multi-domain data with hard negatives, and a pointwise reranker that combines LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark, DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original queries, consistently outperforming competitive reasoning-aware models. These results demonstrate the effectiveness of reasoning-aware retrieval strategies in complex real-world tasks. Our code and retrieval model will be released soon. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶ä¸­å¯ä»¥é€šè¿‡ç›´æ¥çš„è¯æ±‡æˆ–è¯­ä¹‰åŒ¹é…æ¥è¯†åˆ«æŸ¥è¯¢æ–‡æ¡£çš„ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œè®¸å¤šç°å®ä¸–ç•Œä¸­çš„æŸ¥è¯¢æ¶‰åŠæŠ½è±¡æ¨ç†ã€ç±»æ¯”æ€ç»´æˆ–å¤šæ­¥æ¨ç†ï¼Œç°æœ‰æ£€ç´¢å™¨å¾€å¾€éš¾ä»¥æ•è·ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>DIVER</strong>ï¼Œä¸€ä¸ªé’ˆå¯¹æ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢çš„æ£€ç´¢ç®¡é“ã€‚DIVERç”±å››ä¸ªç»„ä»¶æ„æˆï¼šæ”¹å–„è¾“å…¥è´¨é‡çš„æ–‡æ¡£å¤„ç†ï¼Œé€šè¿‡è¿­ä»£æ–‡æ¡£äº¤äº’é©±åŠ¨çš„LLMæŸ¥è¯¢æ‰©å±•ï¼Œåœ¨åˆæˆå¤šåŸŸæ•°æ®ä¸Šå¾®è°ƒä¸”å¸¦æœ‰ç¡¬é˜´æ€§çš„æ¨ç†å¢å¼ºæ£€ç´¢å™¨ï¼Œä»¥åŠç»“åˆLLMåˆ†é…çš„æœ‰ç”¨æ€§åˆ†æ•°å’Œæ£€ç´¢åˆ†æ•°çš„é€ç‚¹é‡æ–°æ’åå™¨ã€‚åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIVERåœ¨åŸå§‹æŸ¥è¯¢ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„nDCG@10åˆ†æ•°41.6å’Œ28.9ï¼ŒæŒç»­è¶…è¶Šç«äº‰æ€§çš„æ¨ç†æ„ŸçŸ¥æ¨¡å‹ã€‚è¿™äº›ç»“æœè¯æ˜äº†æ¨ç†æ„ŸçŸ¥æ£€ç´¢ç­–ç•¥åœ¨å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ£€ç´¢æ¨¡å‹å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07995v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éœ€è¦æ¨ç†èƒ½åŠ›çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡è®¾è®¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹DIVERã€‚DIVERåŒ…æ‹¬å››ä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬æ”¹è¿›è¾“å…¥è´¨é‡çš„æ–‡æ¡£å¤„ç†ã€é€šè¿‡è¿­ä»£æ–‡æ¡£äº¤äº’é©±åŠ¨çš„LLMæŸ¥è¯¢æ‰©å±•ã€åœ¨åˆæˆå¤šåŸŸæ•°æ®ä¸Šå¾®è°ƒå¹¶å¸¦æœ‰ç¡¬è´Ÿæ ·æœ¬çš„æ¨ç†å¢å¼ºæ£€ç´¢å™¨ï¼Œä»¥åŠç»“åˆLLMåˆ†é…çš„æœ‰ç”¨æ€§åˆ†æ•°å’Œæ£€ç´¢åˆ†æ•°çš„é€ç‚¹é‡æ–°æ’åå™¨ã€‚åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šï¼ŒDIVERå–å¾—äº†æœ€æ–°çš„nDCG@10å¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ä¸­æ¨ç†æ„ŸçŸ¥æ£€ç´¢ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å‹DIVERè¢«è®¾è®¡ç”¨äºå¤„ç†éœ€è¦æ¨ç†èƒ½åŠ›çš„ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ã€‚</li>
<li>DIVERåŒ…å«å››ä¸ªç»„ä»¶ï¼šæ–‡æ¡£å¤„ç†ã€LLMé©±åŠ¨çš„æŸ¥è¯¢æ‰©å±•ã€æ¨ç†å¢å¼ºæ£€ç´¢å™¨å’Œé€ç‚¹é‡æ–°æ’åå™¨ã€‚</li>
<li>æ–‡æ¡£å¤„ç†æ—¨åœ¨æé«˜è¾“å…¥è´¨é‡ã€‚</li>
<li>LLMé€šè¿‡è¿­ä»£æ–‡æ¡£äº¤äº’è¿›è¡ŒæŸ¥è¯¢æ‰©å±•ã€‚</li>
<li>æ¨ç†å¢å¼ºæ£€ç´¢å™¨åœ¨åˆæˆå¤šåŸŸæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶åŒ…å«ç¡¬è´Ÿæ ·æœ¬ã€‚</li>
<li>åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šï¼ŒDIVERå–å¾—äº†æ˜¾è‘—çš„nDCG@10å¾—åˆ†ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d1d0b313e4295227238d3aceb1f691d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a4c776bf9308df28d6721c4a4009d2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-412beb53088c7ed8037915006ee32cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55189ace169207efb82148918d1530c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a026615b274442b18b2e0d5a4cb1e26.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization"><a href="#Klear-Reasoner-Advancing-Reasoning-Capability-via-Gradient-Preserving-Clipping-Policy-Optimization" class="headerlink" title="Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving   Clipping Policy Optimization"></a>Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving   Clipping Policy Optimization</h2><p><strong>Authors:Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou</strong></p>
<p>We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the modelâ€™s exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5% on AIME 2024, 83.2% on AIME 2025, 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Klear-Reasonerï¼Œè¿™æ˜¯ä¸€æ¬¾å…·æœ‰å‡ºè‰²æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è§£å†³é—®é¢˜æ—¶å±•ç°å‡ºæ·±æ€ç†Ÿè™‘çš„è¿‡ç¨‹ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡å½“å‰ç¤¾åŒºå·²ç»æœ‰å¾ˆå¤šå…³äºæ¨ç†æ¨¡å‹çš„ä¼˜ç§€ä½œå“ï¼Œä½†ç”±äºåŸ¹è®­ç»†èŠ‚æŠ«éœ²ä¸å®Œæ•´ï¼Œå¯¼è‡´éš¾ä»¥å¤ç°é«˜æ€§èƒ½æ¨ç†æ¨¡å‹çš„é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚æœ¬æŠ¥å‘Šå¯¹æ¨ç†æ¨¡å‹è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ¶µç›–äº†ä»æ•°æ®å‡†å¤‡å’Œé•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒï¼ˆlong CoT SFTï¼‰åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ•´ä¸ªè®­ç»ƒåå·¥ä½œæµç¨‹ï¼Œå¹¶å¯¹æ¯ä¸ªå®éªŒç»„ä»¶è¿›è¡Œäº†è¯¦ç»†çš„æ¶ˆèç ”ç©¶ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œå¯¹äºSFTæ•°æ®ï¼Œæˆ‘ä»¬å‘ç°å°‘é‡é«˜è´¨é‡çš„æ•°æ®æºæ¯”å¤§é‡å¤šæ ·çš„æ•°æ®æºæ›´æœ‰æ•ˆï¼Œè€Œä¸”å›°éš¾æ ·æœ¬å¯ä»¥åœ¨ä¸è¿›è¡Œç²¾åº¦è¿‡æ»¤çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å½“å‰å¼ºåŒ–å­¦ä¹ ä¸­çš„è£å‰ªæœºåˆ¶çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šè£å‰ªä¼šæŠ‘åˆ¶å…³é”®æ¢ç´¢ä¿¡å·å¹¶å¿½ç•¥æ¬¡ä¼˜è½¨è¿¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢¯åº¦ä¿æŒè£å‰ªç­–ç•¥ä¼˜åŒ–ï¼ˆGPPOï¼‰ï¼Œå®ƒæ¸©å’Œåœ°åå‘ä¼ æ’­è¢«è£å‰ªä»¤ç‰Œçš„æ¢¯åº¦ã€‚GPPOä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œè€Œä¸”æé«˜äº†å…¶ä»è´Ÿé¢æ ·æœ¬ä¸­å­¦ä¹ çš„æ•ˆç‡ã€‚Klear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME 2024ä¸­å¾—åˆ†ä¸º90.5%ï¼Œåœ¨AIME 2025ä¸­å¾—åˆ†ä¸º83.2%ï¼Œåœ¨LiveCodeBench V5ä¸­å¾—åˆ†ä¸º66.0%ï¼Œåœ¨LiveCodeBench V6ä¸­å¾—åˆ†ä¸º58.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07629v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Klear-Reasoneræ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·å¤‡å‡ºè‰²çš„é•¿æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†è¯¥æ¨ç†æ¨¡å‹ï¼Œæ¶µç›–äº†ä»æ•°æ®å‡†å¤‡ã€é•¿æœŸæ€ç»´ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆlong CoT SFTï¼‰åˆ°å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ•´ä¸ªè®­ç»ƒå·¥ä½œæµç¨‹ï¼Œå¹¶è¯¦ç»†ç ”ç©¶äº†å„å®éªŒæˆåˆ†ã€‚æå‡ºGradient-Preservingè£å‰ªç­–ç•¥ä¼˜åŒ–ï¼ˆGPPOï¼‰è§£å†³å½“å‰è£å‰ªæœºåˆ¶çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œæé«˜æ¨¡å‹æ¢ç´¢èƒ½åŠ›å’Œè´Ÿæ ·æœ¬å­¦ä¹ æ•ˆç‡ã€‚Klear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢çš„æ¨ç†èƒ½åŠ›å‡ºä¼—ï¼Œåœ¨AIME 2024å’ŒAIME 2025ç­‰æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Klear-Reasoneræ˜¯ä¸€ä¸ªå…·æœ‰é•¿æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>è¯¥æ¨¡å‹å±•ç¤ºäº†è¯¦å°½çš„æ¨ç†è¿‡ç¨‹ï¼Œä»æ•°æ®å‡†å¤‡åˆ°å¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬é•¿æœŸæ€ç»´ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆlong CoT SFTï¼‰ã€‚</li>
<li>å®éªŒè¡¨æ˜å°‘é‡é«˜è´¨é‡æ•°æ®æºæ¯”å¤§é‡å¤šæ ·æ•°æ®æºæ›´æœ‰æ•ˆã€‚</li>
<li>æå‡ºGPPOè§£å†³å½“å‰è£å‰ªæœºåˆ¶çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œå¢å¼ºæ¨¡å‹æ¢ç´¢èƒ½åŠ›å’Œè´Ÿæ ·æœ¬å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>Klear-Reasoneråœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢çš„èƒ½åŠ›å‡ºä¼—ï¼Œå¦‚AIME 2024å’ŒAIME 2025æµ‹è¯•æˆç»©æ˜¾ç¤ºã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå¤„ç†å›°éš¾æ ·æœ¬ï¼Œæ— éœ€ç²¾ç¡®è¿‡æ»¤å³å¯è·å¾—æ›´å¥½çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f337ae7975df609331733a01b94dd295.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-14/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8c4ce7d6db511d4b844d219239c9ff2c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  BrowseMaster Towards Scalable Web Browsing via Tool-Augmented   Programmatic Agent Pair
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-14/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-170c7d268e3cd03cc73d12c595b86a87.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-14  Preview WB-DH Towards Whole Body Digital Human Bench for the Generation   of Whole-body Talking Avatar Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
