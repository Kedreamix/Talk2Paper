<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  Reconstruction vs. Generation Taming Optimization Dilemma in Latent   Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d8f85e42e9f14b456e9e4de302584947.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-06-æ›´æ–°"><a href="#2025-01-06-æ›´æ–°" class="headerlink" title="2025-01-06 æ›´æ–°"></a>2025-01-06 æ›´æ–°</h1><h2 id="Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models"><a href="#Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models" class="headerlink" title="Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models"></a>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models</h2><p><strong>Authors:Jingfeng Yao, Xinggang Wang</strong></p>
<p>Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochsâ€“representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a>. </p>
<blockquote>
<p>åŸºäºTransformeræ¶æ„çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†è¿™ç§ä¸¤é˜¶æ®µè®¾è®¡ä¸­çš„ä¸€ä¸ªä¼˜åŒ–å›°å¢ƒï¼šè™½ç„¶å¢åŠ è§†è§‰åˆ†è¯å™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦å¯ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œä½†è¦å®ç°ç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ï¼Œéœ€è¦å¤§å¹…æ‰©å±•æ‰©æ•£æ¨¡å‹çš„è§„æ¨¡å¹¶å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°ã€‚å› æ­¤ï¼Œç°æœ‰ç³»ç»Ÿé€šå¸¸é€‰æ‹©æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆï¼Œè¦ä¹ˆç”±äºåˆ†è¯å™¨ä¸­çš„ä¿¡æ¯ä¸¢å¤±è€Œäº§ç”Ÿè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆç”±äºè®¡ç®—æˆæœ¬é«˜æ˜‚è€Œæ— æ³•å®Œå…¨æ”¶æ•›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§å›°å¢ƒæºäºå­¦ä¹ æ— çº¦æŸçš„é«˜ç»´æ½œåœ¨ç©ºé—´çš„å›ºæœ‰å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®åœ¨è®­ç»ƒè§†è§‰åˆ†è¯å™¨æ—¶ï¼Œå°†å…¶æ½œåœ¨ç©ºé—´ä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬æå‡ºçš„VA-VAEï¼ˆä¸é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½çš„å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼‰æ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œï¼Œä½¿æ‰©æ•£Transformerï¼ˆDiTï¼‰åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­æ›´å¿«æ”¶æ•›ã€‚ä¸ºäº†å……åˆ†å‘æŒ¥VA-VAEçš„æ½œåŠ›ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¢å¼ºçš„DiTåŸºçº¿ï¼Œé‡‡ç”¨äº†æ”¹è¿›çš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡ï¼Œç§°ä¸ºLightningDiTã€‚é›†æˆç³»ç»Ÿåœ¨ImageNet 256x256ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼ŒFIDå¾—åˆ†ä¸º1.35ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„è®­ç»ƒæ•ˆç‡ï¼Œä»…åœ¨64ä¸ªå‘¨æœŸå†…å°±è¾¾åˆ°äº†2.11çš„FIDå¾—åˆ†ï¼Œä¸åŸå§‹DiTç›¸æ¯”ï¼Œå®ç°äº†è¶…è¿‡21å€çš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚æ¨¡å‹å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hustvl/LightningDiTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01423v1">PDF</a> Models and codes are available at:   <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºTransformeræ¶æ„çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ã€‚ç ”ç©¶æ­ç¤ºäº†ä¼˜åŒ–ä¸­çš„å›°å¢ƒï¼šè™½ç„¶å¢åŠ è§†è§‰æ ‡è®°å™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦å¯ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œä½†è¿™éœ€è¦æ›´å¤§çš„æ‰©æ•£æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒè¿­ä»£æ¥è¾¾åˆ°ç›¸åº”çš„ç”Ÿæˆæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½è®­ç»ƒè§†è§‰æ ‡è®°å™¨çš„æ–¹æ³•ï¼Œå¹¶æ¨å‡ºäº†VA-VAEï¼ˆä¸è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†å……åˆ†åˆ©ç”¨VA-VAEçš„æ½œåŠ›ï¼Œæ–‡ç« è¿˜å»ºç«‹äº†å¢å¼ºçš„DiTåŸºçº¿ï¼Œåä¸ºLightningDiTã€‚å®ƒåœ¨ImageNet 256x256ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶åœ¨ä»…64ä¸ªå‘¨æœŸå†…å°±è¾¾åˆ°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼Œä½†å­˜åœ¨ä¼˜åŒ–å›°å¢ƒã€‚</li>
<li>å¢åŠ è§†è§‰æ ‡è®°å™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦èƒ½æé«˜é‡å»ºè´¨é‡ï¼Œä½†è¦æ±‚æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤šè®­ç»ƒè¿­ä»£ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é¢ä¸´å­¦ä¹ æ— çº¦æŸé«˜ç»´æ½œåœ¨ç©ºé—´çš„å›°éš¾ã€‚</li>
<li>æå‡ºVA-VAEï¼ˆä¸è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>VA-VAEæ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œã€‚</li>
<li>å»ºç«‹äº†å¢å¼ºçš„DiTåŸºçº¿ï¼Œåä¸ºLightningDiTï¼Œä»¥å……åˆ†åˆ©ç”¨VA-VAEçš„æ½œåŠ›ã€‚</li>
<li>LightningDiTåœ¨ImageNet 256x256ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶åœ¨è®­ç»ƒæ•ˆç‡æ–¹é¢å®ç°æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebf56af0e63b24c7ebc99605c03aeec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb24cd12de278653264467c9004fa7dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09964e767c4f7fdd33d7339eb18bd6e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f26517370be80476674fbe171689e583.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-894ebe7be6e088053c7e92a4499d8296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a5491fc00b9cb05f455c3ee8b31acdd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Test-time-Controllable-Image-Generation-by-Explicit-Spatial-Constraint-Enforcement"><a href="#Test-time-Controllable-Image-Generation-by-Explicit-Spatial-Constraint-Enforcement" class="headerlink" title="Test-time Controllable Image Generation by Explicit Spatial Constraint   Enforcement"></a>Test-time Controllable Image Generation by Explicit Spatial Constraint   Enforcement</h2><p><strong>Authors:Z. Zhang, B. Liu, J. Bao, L. Chen, S. Zhu, J. Yu</strong></p>
<p>Recent text-to-image generation favors various forms of spatial conditions, e.g., masks, bounding boxes, and key points. However, the majority of the prior art requires form-specific annotations to fine-tune the original model, leading to poor test-time generalizability. Meanwhile, existing training-free methods work well only with simplified prompts and spatial conditions. In this work, we propose a novel yet generic test-time controllable generation method that aims at natural text prompts and complex conditions. Specifically, we decouple spatial conditions into semantic and geometric conditions and then enforce their consistency during the image-generation process individually. As for the former, we target bridging the gap between the semantic condition and text prompts, as well as the gap between such condition and the attention map from diffusion models. To achieve this, we propose to first complete the prompt w.r.t. semantic condition, and then remove the negative impact of distracting prompt words by measuring their statistics in attention maps as well as distances in word space w.r.t. this condition. To further cope with the complex geometric conditions, we introduce a geometric transform module, in which Region-of-Interests will be identified in attention maps and further used to translate category-wise latents w.r.t. geometric condition. More importantly, we propose a diffusion-based latents-refill method to explicitly remove the impact of latents at the RoI, reducing the artifacts on generated images. Experiments on Coco-stuff dataset showcase 30$%$ relative boost compared to SOTA training-free methods on layout consistency evaluation metrics. </p>
<blockquote>
<p>æœ€è¿‘æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯æ›´å€¾å‘äºå„ç§ç©ºé—´æ¡ä»¶å½¢å¼ï¼Œä¾‹å¦‚é®ç½©ã€è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰æŠ€æœ¯éœ€è¦ç‰¹å®šå½¢å¼çš„æ ‡æ³¨æ¥å¾®è°ƒåŸå§‹æ¨¡å‹ï¼Œå¯¼è‡´æµ‹è¯•æ—¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚åŒæ—¶ï¼Œç°æœ‰çš„æ— è®­ç»ƒæ–¹æ³•ä»…é€‚ç”¨äºç®€åŒ–æç¤ºå’Œç®€å•çš„ç©ºé—´æ¡ä»¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä¸”é€šç”¨çš„æµ‹è¯•æ—¶é—´å¯æ§ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ–‡æœ¬æç¤ºå’Œå¤æ‚æ¡ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç©ºé—´æ¡ä»¶åˆ†ä¸ºè¯­ä¹‰å’Œå‡ ä½•æ¡ä»¶ï¼Œç„¶ååœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­å•ç‹¬å¼ºåˆ¶æ‰§è¡Œå®ƒä»¬çš„ä¸€è‡´æ€§ã€‚å¯¹äºå‰è€…ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç¼©å°è¯­ä¹‰æ¡ä»¶å’Œæ–‡æœ¬æç¤ºä¹‹é—´çš„å·®è·ï¼Œä»¥åŠè¿™ç§æ¡ä»¶ä¸æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å›¾ä¹‹é—´çš„å·®è·ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºé¦–å…ˆæ ¹æ®è¯­ä¹‰æ¡ä»¶å®Œæˆæç¤ºï¼Œç„¶åé€šè¿‡æµ‹é‡æ³¨æ„åŠ›å›¾ä¸­çš„ç»Ÿè®¡æ•°æ®å’Œä¸æ­¤æ¡ä»¶ç›¸å…³çš„è¯ç©ºé—´è·ç¦»æ¥æ¶ˆé™¤å¹²æ‰°æç¤ºè¯çš„ä¸åˆ©å½±å“ã€‚ä¸ºäº†åº”å¯¹å¤æ‚çš„å‡ ä½•æ¡ä»¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•å˜æ¢æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åœ¨æ³¨æ„åŠ›å›¾ä¸­è¯†åˆ«æ„Ÿå…´è¶£åŒºåŸŸï¼Œå¹¶è¿›ä¸€æ­¥ç”¨äºæ ¹æ®å‡ ä½•æ¡ä»¶è½¬æ¢ç±»åˆ«æ½œåœ¨å˜é‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ½œåœ¨å˜é‡å¡«å……æ–¹æ³•ï¼Œä»¥æ˜ç¡®æ¶ˆé™¤æ„Ÿå…´è¶£åŒºåŸŸæ½œåœ¨å˜é‡çš„å½±å“ï¼Œå‡å°‘ç”Ÿæˆå›¾åƒä¸Šçš„ä¼ªå½±ã€‚åœ¨Coco-stuffæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ— è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¸ƒå±€ä¸€è‡´æ€§è¯„ä¼°æŒ‡æ ‡ä¸Šç›¸å¯¹æå‡äº†30%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01368v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç©ºé—´æ¡ä»¶æ§åˆ¶ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯å¯¹ç‰¹å®šå½¢å¼æ ‡æ³¨çš„ä¾èµ–ä»¥åŠæµ‹è¯•æ—¶é€šç”¨æ€§è¾ƒå·®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨æµ‹è¯•æ—¶é—´å¯æ§ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†ç©ºé—´æ¡ä»¶è§£è€¦ä¸ºè¯­ä¹‰å’Œå‡ ä½•æ¡ä»¶ï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­åˆ†åˆ«å®æ–½ä¸€è‡´æ€§çº¦æŸã€‚é€šè¿‡é’ˆå¯¹è¯­ä¹‰æ¡ä»¶å’Œå‡ ä½•æ¡ä»¶çš„å¤„ç†ï¼Œæé«˜äº†æ–‡æœ¬æç¤ºå’Œè‡ªç„¶æ¡ä»¶ä¸‹å›¾åƒç”Ÿæˆçš„è¡”æ¥æ€§ã€‚åœ¨Coco-stuffæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ— è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¸ƒå±€ä¸€è‡´æ€§è¯„ä¼°æŒ‡æ ‡ä¸Šç›¸å¯¹æå‡äº†30%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå¼€å§‹é‡è§†ç©ºé—´æ¡ä»¶æ§åˆ¶ï¼Œå¦‚æ©ç ã€è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹ç­‰ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°æŠ€æœ¯éœ€è¦å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œç‰¹å®šå½¢å¼çš„æ ‡æ³¨å¾®è°ƒï¼Œå¯¼è‡´æµ‹è¯•æ—¶çš„é€šç”¨æ€§è¾ƒå·®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨æµ‹è¯•æ—¶é—´å¯æ§ç”Ÿæˆæ–¹æ³•ï¼Œé€‚ç”¨äºè‡ªç„¶æ–‡æœ¬æç¤ºå’Œå¤æ‚æ¡ä»¶ã€‚</li>
<li>æ–¹æ³•å°†ç©ºé—´æ¡ä»¶è§£è€¦ä¸ºè¯­ä¹‰å’Œå‡ ä½•æ¡ä»¶ï¼Œå¹¶åˆ†åˆ«å®æ–½ä¸€è‡´æ€§çº¦æŸã€‚</li>
<li>é€šè¿‡å¤„ç†è¯­ä¹‰æ¡ä»¶æ¥ç¼©å°æ–‡æœ¬æç¤ºä¸æ³¨æ„åŠ›å›¾çš„å·®è·ã€‚</li>
<li>å¼•å…¥å‡ ä½•è½¬æ¢æ¨¡å—ä»¥å¤„ç†å¤æ‚çš„å‡ ä½•æ¡ä»¶ï¼Œé€šè¿‡è¯†åˆ«å…³æ³¨åŒºåŸŸï¼ˆRegion-of-Interestsï¼‰æ¥è°ƒæ•´ç±»åˆ«ç‰¹å®šçš„æ½œåœ¨å˜é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ½œåœ¨å˜é‡å¡«å……æ–¹æ³•ï¼Œä»¥æ˜ç¡®æ¶ˆé™¤å…³æ³¨åŒºåŸŸä¸­æ½œåœ¨å˜é‡çš„å½±å“ï¼Œå‡å°‘ç”Ÿæˆå›¾åƒçš„ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4d334436f68c524d080db7edf732c695.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c47332ef218e4f3988c8b0649b0ffc09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-544a44417916d7ec02c62715e1767753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00387cdb6f9f7e22d4e7cd382bd67ecf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f751dc4bf9edf573d4a9fb0735662e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Conditional-Consistency-Guided-Image-Translation-and-Enhancement"><a href="#Conditional-Consistency-Guided-Image-Translation-and-Enhancement" class="headerlink" title="Conditional Consistency Guided Image Translation and Enhancement"></a>Conditional Consistency Guided Image Translation and Enhancement</h2><p><strong>Authors:A. V. Subramanyam, Amil Bhagat, Milind Jain</strong></p>
<p>Consistency models have emerged as a promising alternative to diffusion models, offering high-quality generative capabilities through single-step sample generation. However, their application to multi-domain image translation tasks, such as cross-modal translation and low-light image enhancement remains largely unexplored. In this paper, we introduce Conditional Consistency Models (CCMs) for multi-domain image translation by incorporating additional conditional inputs. We implement these modifications by introducing task-specific conditional inputs that guide the denoising process, ensuring that the generated outputs retain structural and contextual information from the corresponding input domain. We evaluate CCMs on 10 different datasets demonstrating their effectiveness in producing high-quality translated images across multiple domains. Code is available at <a target="_blank" rel="noopener" href="https://github.com/amilbhagat/Conditional-Consistency-Models">https://github.com/amilbhagat/Conditional-Consistency-Models</a>. </p>
<blockquote>
<p>ä¸€è‡´æ€§æ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„å…·æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆå·²ç»å´­éœ²å¤´è§’ï¼Œå®ƒé€šè¿‡å•æ­¥æ ·æœ¬ç”Ÿæˆæä¾›é«˜è´¨é‡çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šåŸŸå›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¦‚è·¨æ¨¡æ€ç¿»è¯‘å’Œä½å…‰å›¾åƒå¢å¼ºï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥é¢å¤–çš„æ¡ä»¶è¾“å…¥ï¼Œä»‹ç»äº†ç”¨äºå¤šåŸŸå›¾åƒç¿»è¯‘çš„æ¡ä»¶ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCCMï¼‰ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä»»åŠ¡ç‰¹å®šçš„æ¡ä»¶è¾“å…¥æ¥å®ç°è¿™äº›ä¿®æ”¹ï¼Œå¼•å¯¼å»å™ªè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºä¿ç•™å¯¹åº”è¾“å…¥åŸŸçš„ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨10ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†CCMï¼Œè¯æ˜äº†å®ƒä»¬åœ¨å¤šä¸ªé¢†åŸŸç”Ÿæˆé«˜è´¨é‡ç¿»è¯‘å›¾åƒçš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/amilbhagat/Conditional-Consistency-Models%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/amilbhagat/Conditional-Consistency-Modelsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01223v1">PDF</a> 6 pages, 5 figures, 4 tables, ICME conference 2025</p>
<p><strong>Summary</strong></p>
<p>æ¡ä»¶ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCCMï¼‰æ˜¯ä¸€ç§æ–°å…´çš„æŠ€æœ¯ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æŠ€æœ¯é€šè¿‡å•æ­¥æ ·æœ¬ç”Ÿæˆå®ç°äº†é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶åœ¨å¤šé¢†åŸŸå›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚é€šè¿‡å¼•å…¥é¢å¤–çš„æ¡ä»¶è¾“å…¥ï¼ŒCCMèƒ½å¤Ÿç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºä¿ç•™è¾“å…¥é¢†åŸŸçš„ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æœ¬æ–‡åœ¨åä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†CCMçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å¤šé¢†åŸŸå›¾åƒç¿»è¯‘ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¡ä»¶ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCCMï¼‰æ˜¯ä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œå¯ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>CCMé€šè¿‡å•æ­¥æ ·æœ¬ç”Ÿæˆå®ç°é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>CCMåœ¨å¤šé¢†åŸŸå›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>CCMé€šè¿‡å¼•å…¥é¢å¤–çš„æ¡ä»¶è¾“å…¥ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºä¿ç•™è¾“å…¥é¢†åŸŸçš„ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>CCMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>CCMç”Ÿæˆçš„å›¾åƒè´¨é‡é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-229cedc28f25d71b6315cd0e250400ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6cf2bd7938e5bba0c1f29e3463ac4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adf17e39eec22ef5f6898b72e9c270b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb8dc9e781a61c7b6ddd74a232ded8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84e51776ffed69b84270fb397dd892a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantics-Guided-Diffusion-for-Deep-Joint-Source-Channel-Coding-in-Wireless-Image-Transmission"><a href="#Semantics-Guided-Diffusion-for-Deep-Joint-Source-Channel-Coding-in-Wireless-Image-Transmission" class="headerlink" title="Semantics-Guided Diffusion for Deep Joint Source-Channel Coding in   Wireless Image Transmission"></a>Semantics-Guided Diffusion for Deep Joint Source-Channel Coding in   Wireless Image Transmission</h2><p><strong>Authors:Maojun Zhang, Haotian Wu, Guangxu Zhu, Richeng Jin, Xiaoming Chen, Deniz GÃ¼ndÃ¼z</strong></p>
<p>Joint source-channel coding (JSCC) offers a promising avenue for enhancing transmission efficiency by jointly incorporating source and channel statistics into the system design. A key advancement in this area is the deep joint source and channel coding (DeepJSCC) technique that designs a direct mapping of input signals to channel symbols parameterized by a neural network, which can be trained for arbitrary channel models and semantic quality metrics. This paper advances the DeepJSCC framework toward a semantics-aligned, high-fidelity transmission approach, called semantics-guided diffusion DeepJSCC (SGD-JSCC). Existing schemes that integrate diffusion models (DMs) with JSCC face challenges in transforming random generation into accurate reconstruction and adapting to varying channel conditions. SGD-JSCC incorporates two key innovations: (1) utilizing some inherent information that contributes to the semantics of an image, such as text description or edge map, to guide the diffusion denoising process; and (2) enabling seamless adaptability to varying channel conditions with the help of a semantics-guided DM for channel denoising. The DM is guided by diverse semantic information and integrates seamlessly with DeepJSCC. In a slow fading channel, SGD-JSCC dynamically adapts to the instantaneous signal-to-noise ratio (SNR) directly estimated from the channel output, thereby eliminating the need for additional pilot transmissions for channel estimation. In a fast fading channel, we introduce a training-free denoising strategy, allowing SGD-JSCC to effectively adjust to fluctuations in channel gains. Numerical results demonstrate that, guided by semantic information and leveraging the powerful DM, our method outperforms existing DeepJSCC schemes, delivering satisfactory reconstruction performance even at extremely poor channel conditions. </p>
<blockquote>
<p>è”åˆæºä¿¡é“ç¼–ç ï¼ˆJSCCï¼‰é€šè¿‡å°†æºå’Œä¿¡é“ç»Ÿè®¡ä¿¡æ¯å…±åŒçº³å…¥ç³»ç»Ÿè®¾è®¡ï¼Œä¸ºæé«˜ä¼ è¾“æ•ˆç‡æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚è¯¥é¢†åŸŸçš„ä¸€ä¸ªå…³é”®è¿›å±•æ˜¯æ·±åº¦è”åˆæºå’Œä¿¡é“ç¼–ç ï¼ˆDeepJSCCï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è®¾è®¡äº†ä¸€ç§ç›´æ¥å°†è¾“å…¥ä¿¡å·æ˜ å°„åˆ°ç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ä¿¡é“ç¬¦å·çš„æ–¹æ³•ï¼Œå¯ä»¥é’ˆå¯¹ä»»æ„ä¿¡é“æ¨¡å‹å’Œè¯­ä¹‰è´¨é‡æŒ‡æ ‡è¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡æ¨è¿›äº†DeepJSCCæ¡†æ¶ï¼Œæœç€è¯­ä¹‰å¯¹é½ã€é«˜ä¿çœŸä¼ è¾“æ–¹æ³•å‘å±•ï¼Œç§°ä¸ºè¯­ä¹‰å¼•å¯¼æ‰©æ•£DeepJSCCï¼ˆSGD-JSCCï¼‰ã€‚ç°æœ‰å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä¸JSCCç›¸ç»“åˆçš„æ–¹æ¡ˆé¢ä¸´ç€å°†éšæœºç”Ÿæˆè½¬åŒ–ä¸ºç²¾ç¡®é‡å»ºä»¥åŠé€‚åº”ä¸åŒä¿¡é“æ¡ä»¶çš„æŒ‘æˆ˜ã€‚SGD-JSCCæœ‰ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰åˆ©ç”¨æ„æˆå›¾åƒè¯­ä¹‰çš„å›ºæœ‰ä¿¡æ¯ï¼Œå¦‚æ–‡æœ¬æè¿°æˆ–è¾¹ç¼˜å›¾ï¼Œæ¥æŒ‡å¯¼æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼›ï¼ˆ2ï¼‰å€ŸåŠ©è¯­ä¹‰å¼•å¯¼çš„DMè¿›è¡Œä¿¡é“å»å™ªï¼Œå®ç°å¯¹ä¸åŒä¿¡é“æ¡ä»¶çš„æ— ç¼é€‚åº”ã€‚DMåœ¨å¤šç§è¯­ä¹‰ä¿¡æ¯çš„å¼•å¯¼ä¸‹ï¼Œä¸DeepJSCCæ— ç¼é›†æˆã€‚åœ¨æ…¢è¡°å‡ä¿¡é“ä¸­ï¼ŒSGD-JSCCèƒ½å¤Ÿç›´æ¥ä»ä¿¡é“è¾“å‡ºä¼°è®¡ç¬æ—¶ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ï¼Œä»è€Œæ— éœ€é¢å¤–çš„å¯¼é¢‘ä¼ è¾“æ¥è¿›è¡Œä¿¡é“ä¼°è®¡ã€‚åœ¨å¿«è¡°å‡ä¿¡é“ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒçš„å»å™ªç­–ç•¥ï¼Œä½¿SGD-JSCCèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”ä¿¡é“å¢ç›Šçš„æ³¢åŠ¨ã€‚æ•°å€¼ç»“æœè¡¨æ˜ï¼Œåœ¨è¯­ä¹‰ä¿¡æ¯çš„æŒ‡å¯¼ä¸‹ï¼Œåˆ©ç”¨å¼ºå¤§çš„DMï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æå·®çš„ä¿¡é“æ¡ä»¶ä¸‹ä»èƒ½å®ç°å‡ºè‰²çš„é‡å»ºæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„DeepJSCCæ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01138v1">PDF</a> 13 pages, submitted to IEEE for possible publication</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†è¯­ä¹‰å¼•å¯¼æ‰©æ•£æ·±åº¦è”åˆæºä¿¡é“ç¼–ç ï¼ˆSGD-JSCCï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨å›¾åƒå†…åœ¨è¯­ä¹‰ä¿¡æ¯å¼•å¯¼æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼Œé€‚åº”ä¸åŒçš„ä¿¡é“æ¡ä»¶ï¼Œä»è€Œæé«˜è¯­ä¹‰å¯¹é½çš„é«˜ä¿çœŸä¼ è¾“æ•ˆç‡ã€‚å®ƒç»“åˆäº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å’ŒDeepJSCCæŠ€æœ¯çš„ä¼˜ç‚¹ï¼Œé€šè¿‡åœ¨è¯­ä¹‰å¼•å¯¼ä¸‹åŠ¨æ€é€‚åº”ä¸åŒçš„ä¿¡é“æ¡ä»¶æ¥å®ç°é«˜æ•ˆä¼ è¾“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JSCCæŠ€æœ¯ç»“åˆäº†æºå’Œä¿¡é“ç»Ÿè®¡ä¿¡æ¯ï¼Œæœ‰åŠ©äºæé«˜ä¼ è¾“æ•ˆç‡ã€‚</li>
<li>DeepJSCCæŠ€æœ¯æ˜¯è®¾è®¡è¾“å…¥ä¿¡å·åˆ°ä¿¡é“ç¬¦å·çš„ç›´æ¥æ˜ å°„çš„å…³é”®è¿›å±•ï¼Œå¯é€šè¿‡ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒã€‚</li>
<li>SGD-JSCCæŠ€æœ¯åˆ©ç”¨å›¾åƒå†…åœ¨è¯­ä¹‰ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬æè¿°æˆ–è¾¹ç¼˜æ˜ å°„ï¼‰æ¥æŒ‡å¯¼æ‰©æ•£å»å™ªè¿‡ç¨‹ã€‚</li>
<li>SGD-JSCCåˆ©ç”¨è¯­ä¹‰å¼•å¯¼DMæ— ç¼é€‚åº”ä¸åŒçš„ä¿¡é“æ¡ä»¶ã€‚</li>
<li>åœ¨æ…¢è¡°è½ä¿¡é“ä¸­ï¼ŒSGD-JSCCé€šè¿‡ç›´æ¥ä»ä¿¡é“è¾“å‡ºä¼°è®¡ç¬æ—¶ä¿¡å™ªæ¯”æ¥åŠ¨æ€é€‚åº”ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é¢å¤–è¯•ç‚¹ä¼ è¾“è¿›è¡Œä¿¡é“ä¼°è®¡çš„éœ€æ±‚ã€‚</li>
<li>åœ¨å¿«è¡°è½ä¿¡é“ä¸­ï¼ŒSGD-JSCCå¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒçš„å»å™ªç­–ç•¥ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”ä¿¡é“å¢ç›Šçš„æ³¢åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-461c82910e468b831da40239956263f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e79e256072c3ba1778c819bb3ad0d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3eb114a182ef78e9d0c2a7367056fafb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Novel-Diffusion-Model-for-Pairwise-Geoscience-Data-Generation-with-Unbalanced-Training-Dataset"><a href="#A-Novel-Diffusion-Model-for-Pairwise-Geoscience-Data-Generation-with-Unbalanced-Training-Dataset" class="headerlink" title="A Novel Diffusion Model for Pairwise Geoscience Data Generation with   Unbalanced Training Dataset"></a>A Novel Diffusion Model for Pairwise Geoscience Data Generation with   Unbalanced Training Dataset</h2><p><strong>Authors:Junhuan Yang, Yuzhou Zhang, Yi Sheng, Youzuo Lin, Lei Yang</strong></p>
<p>Recently, the advent of generative AI technologies has made transformational impacts on our daily lives, yet its application in scientific applications remains in its early stages. Data scarcity is a major, well-known barrier in data-driven scientific computing, so physics-guided generative AI holds significant promise. In scientific computing, most tasks study the conversion of multiple data modalities to describe physical phenomena, for example, spatial and waveform in seismic imaging, time and frequency in signal processing, and temporal and spectral in climate modeling; as such, multi-modal pairwise data generation is highly required instead of single-modal data generation, which is usually used in natural images (e.g., faces, scenery). Moreover, in real-world applications, the unbalance of available data in terms of modalities commonly exists; for example, the spatial data (i.e., velocity maps) in seismic imaging can be easily simulated, but real-world seismic waveform is largely lacking. While the most recent efforts enable the powerful diffusion model to generate multi-modal data, how to leverage the unbalanced available data is still unclear. In this work, we use seismic imaging in subsurface geophysics as a vehicle to present &#96;&#96;UB-Diffâ€™â€™, a novel diffusion model for multi-modal paired scientific data generation. One major innovation is a one-in-two-out encoder-decoder network structure, which can ensure pairwise data is obtained from a co-latent representation. Then, the co-latent representation will be used by the diffusion process for pairwise data generation. Experimental results on the OpenFWI dataset show that UB-Diff significantly outperforms existing techniques in terms of Fr&#39;{e}chet Inception Distance (FID) score and pairwise evaluation, indicating the generation of reliable and useful multi-modal pairwise data. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‡ºç°å¯¹æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»äº§ç”Ÿäº†å˜é©æ€§çš„å½±å“ï¼Œç„¶è€Œå…¶åœ¨ç§‘å­¦åº”ç”¨ä¸­çš„ä½¿ç”¨ä»å¤„äºæ—©æœŸé˜¶æ®µã€‚æ•°æ®ç¨€ç¼ºæ˜¯æ•°æ®é©±åŠ¨çš„ç§‘å­¦è®¡ç®—ä¸­ä¸€ä¸ªä¸»è¦ä¸”ä¼—æ‰€å‘¨çŸ¥çš„éšœç¢ï¼Œå› æ­¤ï¼Œç‰©ç†æŒ‡å¯¼çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚åœ¨ç§‘å­¦è®¡ç®—ä¸­ï¼Œå¤§å¤šæ•°ä»»åŠ¡ç ”ç©¶å¤šç§æ•°æ®æ¨¡æ€çš„è½¬æ¢ï¼Œä»¥æè¿°ç‰©ç†ç°è±¡ï¼Œä¾‹å¦‚åœ°éœ‡æˆåƒä¸­çš„ç©ºé—´å’Œæ³¢å½¢ã€ä¿¡å·å¤„ç†ä¸­çš„æ—¶é—´å’Œé¢‘ç‡ä»¥åŠæ°”å€™å»ºæ¨¡ä¸­çš„æ—¶é—´å’Œå…‰è°±ï¼›å› æ­¤ï¼Œå¤šæ¨¡æ€é…å¯¹æ•°æ®ç”Ÿæˆçš„éœ€æ±‚éå¸¸é«˜ï¼Œè€Œä¸æ˜¯é€šå¸¸åœ¨è‡ªç„¶å›¾åƒï¼ˆä¾‹å¦‚é¢éƒ¨ã€é£æ™¯ï¼‰ä¸­ä½¿ç”¨çš„å•æ¨¡æ€æ•°æ®ç”Ÿæˆã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œä¸åŒæ¨¡æ€çš„æ•°æ®å¯ç”¨é‡é€šå¸¸å­˜åœ¨ä¸å¹³è¡¡çš„æƒ…å†µï¼›ä¾‹å¦‚ï¼Œåœ°éœ‡æˆåƒä¸­çš„ç©ºé—´æ•°æ®ï¼ˆå³é€Ÿåº¦å›¾ï¼‰å¯ä»¥å¾ˆå®¹æ˜“åœ°æ¨¡æ‹Ÿï¼Œä½†çœŸå®ä¸–ç•Œçš„åœ°éœ‡æ³¢å½¢å´éå¸¸ç¼ºä¹ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›ä½¿å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ¨¡æ€æ•°æ®ï¼Œä½†å¦‚ä½•åˆ©ç”¨ä¸å¹³è¡¡çš„å¯ç”¨æ•°æ®ä»ç„¶ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»¥åœ°ä¸‹åœ°çƒç‰©ç†å­¦ä¸­çš„åœ°éœ‡æˆåƒä¸ºä¾‹ï¼Œä»‹ç»äº†ä¸€ç§åä¸ºUB-Diffçš„æ–°å‹å¤šæ¨¡æ€é…å¯¹ç§‘å­¦æ•°æ®ç”Ÿæˆæ‰©æ•£æ¨¡å‹ã€‚ä¸€ä¸ªä¸»è¦çš„åˆ›æ–°ç‚¹åœ¨äºé‡‡ç”¨ä¸€è¿›ä¸¤å‡ºçš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œç»“æ„ï¼Œå¯ä»¥ç¡®ä¿ä»ååŒæ½œåœ¨è¡¨ç¤ºä¸­è·å¾—é…å¯¹æ•°æ®ã€‚ç„¶åï¼Œæ‰©æ•£è¿‡ç¨‹å°†ä½¿ç”¨ååŒæ½œåœ¨è¡¨ç¤ºæ¥è¿›è¡Œé…å¯¹æ•°æ®ç”Ÿæˆã€‚åœ¨OpenFWIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUB-Diffåœ¨Frâ€™echet Inception Distanceï¼ˆFIDï¼‰å¾—åˆ†å’Œé…å¯¹è¯„ä¼°æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œè¯æ˜å…¶ç”Ÿæˆå¯é ä¸”æœ‰ç”¨çš„å¤šæ¨¡æ€é…å¯¹æ•°æ®çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00941v1">PDF</a> Accepted at AAAI 2025. This is the preprint version. Keywords:   Multi-modal generation, diffuison models, scientific data generation,   unbalanced modalities</p>
<p><strong>æ‘˜è¦</strong><br>    ç”Ÿæˆå¼AIæŠ€æœ¯åœ¨ç§‘å­¦åº”ç”¨ä¸­çš„æ½œåŠ›å·¨å¤§ï¼Œå°¤å…¶åœ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆæ–¹é¢ã€‚æœ¬ç ”ç©¶é’ˆå¯¹åœ°éœ‡æˆåƒç­‰ç§‘å­¦è®¡ç®—ä¸­çš„å¤šæ¨¡æ€æ•°æ®è½¬æ¢ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹UB-Diffï¼Œç”¨äºå¤šæ¨¡æ€é…å¯¹ç§‘å­¦æ•°æ®ç”Ÿæˆã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸€è¿›ä¸¤å‡ºçš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œç»“æ„ï¼Œç¡®ä¿ä»ååŒæ½œåœ¨è¡¨ç¤ºä¸­è·å¾—é…å¯¹æ•°æ®ï¼Œå¹¶é€šè¿‡æ‰©æ•£è¿‡ç¨‹è¿›è¡Œé…å¯¹æ•°æ®ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUB-Diffåœ¨OpenFWIæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç”Ÿæˆå¯é ä¸”æœ‰ç”¨çš„å¤šæ¨¡æ€é…å¯¹æ•°æ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIæŠ€æœ¯åœ¨ç§‘å­¦åº”ç”¨ä¸­çš„æ½œåŠ›å·¨å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>åœ¨ç§‘å­¦è®¡ç®—ä¸­ï¼Œå¤šæ¨¡æ€æ•°æ®è½¬æ¢ä»»åŠ¡æ™®éå­˜åœ¨ï¼Œä¾‹å¦‚åœ°éœ‡æˆåƒã€ä¿¡å·å¤„ç†å’Œæ°”å€™å»ºæ¨¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹UB-Diffï¼Œç”¨äºå¤šæ¨¡æ€é…å¯¹ç§‘å­¦æ•°æ®ç”Ÿæˆã€‚</li>
<li>UB-Diffé‡‡ç”¨ä¸€è¿›ä¸¤å‡ºçš„ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œç»“æ„ï¼Œç¡®ä¿ä»ååŒæ½œåœ¨è¡¨ç¤ºä¸­è·å¾—é…å¯¹æ•°æ®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUB-Diffåœ¨ç”Ÿæˆå¤šæ¨¡æ€é…å¯¹æ•°æ®æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨åœ°éœ‡æˆåƒç­‰å®é™…åº”ç”¨åœºæ™¯éªŒè¯äº†UB-Diffæ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹çš„å¼•å…¥å¯¹äºè§£å†³ç§‘å­¦è®¡ç®—ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜å…·æœ‰é‡å¤§æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c282b676068be60281e356b64aa4971a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdc75720e434d735fab68e3313c4332e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a153370fde6443201a99d50e249cab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cac708a908600a4655c8f8df35e84d48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2facc498179e863e1ab8bc3961703701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-965735f7bcbeb80a9921bf83c45e9068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8f85e42e9f14b456e9e4de302584947.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop"><a href="#RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop" class="headerlink" title="RORem: Training a Robust Object Remover with Human-in-the-Loop"></a>RORem: Training a Robust Object Remover with Human-in-the-Loop</h2><p><strong>Authors:Ruibin Li, Tao Yang, Song Guo, Lei Zhang</strong></p>
<p>Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18%. The dataset, source code and trained model are available at <a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„ç‰©ä½“å»é™¤æ–¹æ³•åœ¨å»é™¤ä¸å®Œæ•´ã€å†…å®¹åˆæˆä¸å‡†ç¡®ä»¥åŠåˆæˆåŒºåŸŸæ¨¡ç³Šç­‰æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æˆåŠŸç‡è¾ƒä½ã€‚è¿™äº›é—®é¢˜ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ä»¥åŠè¿™äº›æ–¹æ³•æ‰€é‡‡ç”¨çš„è‡ªç›‘ç£è®­ç»ƒèŒƒå¼å¯¼è‡´çš„ã€‚è‡ªç›‘ç£è®­ç»ƒèŒƒå¼è¿«ä½¿æ¨¡å‹å¯¹é®æŒ¡åŒºåŸŸè¿›è¡Œå¡«å……ï¼Œå¯¼è‡´åˆæˆé®æŒ¡ç‰©ä½“å’Œæ¢å¤èƒŒæ™¯ä¹‹é—´å­˜åœ¨æ¨¡ç³Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œå¹¶ç»“åˆäººå·¥å‚ä¸ï¼Œä»¥åˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥ç‰©ä½“å»é™¤å™¨ï¼ˆRORemï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆä»å…¬å¼€æ•°æ®æºæ”¶é›†6ä¸‡å¯¹è®­ç»ƒæ ·æœ¬ï¼Œè®­ç»ƒåˆå§‹ç‰©ä½“å»é™¤æ¨¡å‹ä»¥ç”Ÿæˆå»é™¤æ ·æœ¬ï¼Œç„¶ååˆ©ç”¨äººå·¥åé¦ˆé€‰æ‹©é«˜è´¨é‡ç‰©ä½“å»é™¤å¯¹ï¼Œå†ç”¨å…¶è®­ç»ƒåˆ¤åˆ«å™¨ï¼Œä»¥è‡ªåŠ¨åŒ–åç»­è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚ç»è¿‡å‡ è½®è¿­ä»£ï¼Œæˆ‘ä»¬æœ€ç»ˆè·å¾—äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡20ä¸‡å¯¹æ ·æœ¬çš„ç‰©ä½“å»é™¤æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°äº†æˆ‘ä»¬çš„RORemã€‚å®ƒåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„ç‰©ä½“å»é™¤æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼ŒRORemåœ¨æˆåŠŸç‡ä¸Šè¾ƒä»¥å‰çš„æ–¹æ³•æé«˜äº†18%ä»¥ä¸Šã€‚æ•°æ®é›†ã€æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00740v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆäººå·¥å‚ä¸ï¼Œåˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥çš„å¯¹è±¡ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚é€šè¿‡è¿­ä»£æ”¶é›†è®­ç»ƒæ ·æœ¬å¹¶åˆ©ç”¨äººç±»åé¦ˆé€‰æ‹©é«˜è´¨é‡çš„å¯¹è±¡ç§»é™¤é…å¯¹ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨è‡ªåŠ¨åŒ–åç»­è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚æœ€ç»ˆè·å¾—å¤§é‡å¯¹è±¡ç§»é™¤æ•°æ®é›†ï¼Œå¹¶ç”¨å…¶å¾®è°ƒé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¾—åˆ°æ€§èƒ½å“è¶Šçš„RORemï¼Œåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šçš„å¯¹è±¡ç§»é™¤æ€§èƒ½ï¼ŒæˆåŠŸç‡è¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜è¶…è¿‡18%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¯¹è±¡ç§»é™¤æ–¹æ³•å­˜åœ¨ä¸å®Œæ•´ç§»é™¤ã€é”™è¯¯å†…å®¹åˆæˆå’ŒåˆæˆåŒºåŸŸæ¨¡ç³Šç­‰é—®é¢˜ï¼Œå¯¼è‡´ä½æˆåŠŸç‡ã€‚</li>
<li>ä¸»è¦é—®é¢˜æºäºç¼ºä¹é«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®å’Œé‡‡ç”¨çš„è‡ªç›‘ç£è®­ç»ƒèŒƒå¼ã€‚</li>
<li>æå‡ºä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆäººå·¥å‚ä¸åˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œä»¥è®­ç»ƒç¨³å¥çš„å¯¹è±¡ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚</li>
<li>é€šè¿‡è¿­ä»£è¿‡ç¨‹æ”¶é›†è®­ç»ƒæ ·æœ¬å¹¶åˆ©ç”¨äººç±»åé¦ˆé€‰æ‹©é«˜è´¨é‡å¯¹è±¡ç§»é™¤é…å¯¹ï¼Œè®­ç»ƒåˆ¤åˆ«å™¨è‡ªåŠ¨åŒ–åç»­æ•°æ®ç”Ÿæˆã€‚</li>
<li>æœ€ç»ˆè·å¾—å¤§é‡å¯¹è±¡ç§»é™¤æ•°æ®é›†ï¼Œå¹¶ç”¨å…¶å¾®è°ƒé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>RORemåœ¨å¯¹è±¡ç§»é™¤æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒæˆåŠŸç‡é«˜ï¼Œä¸”å›¾åƒè´¨é‡è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d95f824dfa121c8f4ae9735bd980eaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60878675ad0033dc47bb8471462cf934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f722f4161e3bb58b6c1261c619403de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c48cd5acb064bff8293616686cf7ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e631ad6caf2fb30e89a432e95cce18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17980883c30b9aa63f07feb36883739d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca4c0ca7d933c0ed4bb655144410357e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SoundBrush-Sound-as-a-Brush-for-Visual-Scene-Editing"><a href="#SoundBrush-Sound-as-a-Brush-for-Visual-Scene-Editing" class="headerlink" title="SoundBrush: Sound as a Brush for Visual Scene Editing"></a>SoundBrush: Sound as a Brush for Visual Scene Editing</h2><p><strong>Authors:Kim Sung-Bin, Kim Jun-Seong, Junseok Ko, Yewon Kim, Tae-Hyun Oh</strong></p>
<p>We propose SoundBrush, a model that uses sound as a brush to edit and manipulate visual scenes. We extend the generative capabilities of the Latent Diffusion Model (LDM) to incorporate audio information for editing visual scenes. Inspired by existing image-editing works, we frame this task as a supervised learning problem and leverage various off-the-shelf models to construct a sound-paired visual scene dataset for training. This richly generated dataset enables SoundBrush to learn to map audio features into the textual space of the LDM, allowing for visual scene editing guided by diverse in-the-wild sound. Unlike existing methods, SoundBrush can accurately manipulate the overall scenery or even insert sounding objects to best match the audio inputs while preserving the original content. Furthermore, by integrating with novel view synthesis techniques, our framework can be extended to edit 3D scenes, facilitating sound-driven 3D scene manipulation. Demos are available at <a target="_blank" rel="noopener" href="https://soundbrush.github.io/">https://soundbrush.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SoundBrushæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å£°éŸ³ä½œä¸ºç”»ç¬”æ¥ç¼–è¾‘å’Œæ“ä½œè§†è§‰åœºæ™¯ã€‚æˆ‘ä»¬æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼ŒLDMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥èå…¥éŸ³é¢‘ä¿¡æ¯æ¥ç¼–è¾‘è§†è§‰åœºæ™¯ã€‚å—åˆ°ç°æœ‰å›¾åƒç¼–è¾‘ä½œå“çš„å¯å‘ï¼Œæˆ‘ä»¬å°†æ­¤ä»»åŠ¡å®šä½ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å„ç§ç°æˆçš„æ¨¡å‹æ„å»ºå£°éŸ³é…å¯¹è§†è§‰åœºæ™¯æ•°æ®é›†æ¥è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸ªä¸°å¯Œç”Ÿæˆçš„æ•°æ®é›†ä½¿SoundBrushå­¦ä¼šå°†éŸ³é¢‘ç‰¹å¾æ˜ å°„åˆ°LDMçš„æ–‡æœ¬ç©ºé—´ï¼Œä»è€Œå®ç°ç”±å„ç§é‡å¤–å£°éŸ³å¼•å¯¼çš„è§†è§‰åœºæ™¯ç¼–è¾‘ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSoundBrushèƒ½å¤Ÿå‡†ç¡®åœ°æ“ä½œæ•´ä½“é£æ™¯ï¼Œç”šè‡³æ’å…¥å£°éŸ³å¯¹è±¡ä»¥æœ€ä½³æ–¹å¼åŒ¹é…éŸ³é¢‘è¾“å…¥ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆæ–°é¢–è§†å›¾åˆæˆæŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ‰©å±•åˆ°ç¼–è¾‘3Dåœºæ™¯ï¼Œå®ç°å£°éŸ³é©±åŠ¨çš„3Dåœºæ™¯æ“ä½œã€‚æ¼”ç¤ºè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://soundbrush.github.io/">https://soundbrush.github.io/</a>æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00645v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†SoundBrushæ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å£°éŸ³ä½œä¸ºç”»ç¬”æ¥ç¼–è¾‘å’Œæ“ä½œè§†è§‰åœºæ™¯ã€‚è¯¥æ¨¡å‹æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼ŒLDMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥èå…¥éŸ³é¢‘ä¿¡æ¯æ¥ç¼–è¾‘è§†è§‰åœºæ™¯ã€‚æˆ‘ä»¬å—åˆ°ç°æœ‰å›¾åƒç¼–è¾‘å·¥ä½œçš„å¯å‘ï¼Œå°†æ­¤ä»»åŠ¡æ„å»ºä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å„ç§ç°æˆçš„æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªç”¨äºè®­ç»ƒçš„å£°éŸ³é…å¯¹è§†è§‰åœºæ™¯æ•°æ®é›†ã€‚è¿™ä¸ªä¸°å¯Œç”Ÿæˆçš„æ•°æ®é›†ä½¿SoundBrushèƒ½å¤Ÿå­¦ä¹ å°†éŸ³é¢‘ç‰¹å¾æ˜ å°„åˆ°LDMçš„æ–‡æœ¬ç©ºé—´ï¼Œä»è€Œå®ç°ç”±å„ç§è‡ªç„¶å£°éŸ³å¼•å¯¼çš„è§†è§‰åœºæ™¯ç¼–è¾‘ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒSoundBrushèƒ½å¤Ÿå‡†ç¡®åœ°æ“ä½œæ•´ä½“é£æ™¯ï¼Œç”šè‡³æ’å…¥ä¸å£°éŸ³æœ€åŒ¹é…çš„å¯¹è±¡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸æ–°é¢–è§†å›¾åˆæˆæŠ€æœ¯ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯æ‰©å±•åˆ°ç¼–è¾‘3Dåœºæ™¯ï¼Œå®ç°å£°éŸ³é©±åŠ¨çš„3Dåœºæ™¯æ“ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SoundBrushæ¨¡å‹åˆ©ç”¨å£°éŸ³ä½œä¸ºç”»ç¬”ç¼–è¾‘è§†è§‰åœºæ™¯ï¼Œæ‰©å±•äº†Latent Diffusion Modelï¼ˆLDMï¼‰çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ„å»ºå£°éŸ³é…å¯¹è§†è§‰åœºæ™¯æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°éŸ³é¢‘ä¿¡æ¯ä¸è§†è§‰åœºæ™¯çš„èåˆã€‚</li>
<li>SoundBrushèƒ½å¤Ÿå‡†ç¡®æ“ä½œæ•´ä½“é£æ™¯å’Œæ’å…¥ä¸éŸ³é¢‘åŒ¹é…çš„å¯¹è±¡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹ã€‚</li>
<li>SoundBrushæ¨¡å‹å…·æœ‰å…ˆè¿›çš„éŸ³é¢‘ç‰¹å¾æ˜ å°„åˆ°æ–‡æœ¬ç©ºé—´çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥ä¸æ–°é¢–è§†å›¾åˆæˆæŠ€æœ¯ç»“åˆï¼Œç¼–è¾‘3Dåœºæ™¯ï¼Œå®ç°å£°éŸ³é©±åŠ¨çš„3Dåœºæ™¯æ“ä½œã€‚</li>
<li>SoundBrushæ¨¡å‹åœ¨è§†è§‰åœºæ™¯ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºé«˜åº¦çš„åˆ›é€ æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53e79bd51d34e068b53134cf91b0eb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9de85a1d8ad2732e96a7e584e6b080b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44d0ca9f661dfd0f01220c61f161c1f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b549c3fc9530a08cf0cf08a86628c6d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Flash-Split-2D-Reflection-Removal-with-Flash-Cues-and-Latent-Diffusion-Separation"><a href="#Flash-Split-2D-Reflection-Removal-with-Flash-Cues-and-Latent-Diffusion-Separation" class="headerlink" title="Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion   Separation"></a>Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion   Separation</h2><p><strong>Authors:Tianfu Wang, Mingyang Xie, Haoming Cai, Sachin Shah, Christopher A. Metzler</strong></p>
<p>Transparent surfaces, such as glass, create complex reflections that obscure images and challenge downstream computer vision applications. We introduce Flash-Split, a robust framework for separating transmitted and reflected light using a single (potentially misaligned) pair of flash&#x2F;no-flash images. Our core idea is to perform latent-space reflection separation while leveraging the flash cues. Specifically, Flash-Split consists of two stages. Stage 1 separates apart the reflection latent and transmission latent via a dual-branch diffusion model conditioned on an encoded flash&#x2F;no-flash latent pair, effectively mitigating the flash&#x2F;no-flash misalignment issue. Stage 2 restores high-resolution, faithful details to the separated latents, via a cross-latent decoding process conditioned on the original images before separation. By validating Flash-Split on challenging real-world scenes, we demonstrate state-of-the-art reflection separation performance and significantly outperform the baseline methods. </p>
<blockquote>
<p>é€æ˜è¡¨é¢ï¼Œå¦‚ç»ç’ƒï¼Œä¼šäº§ç”Ÿå¤æ‚çš„åå°„ï¼Œä»è€Œæ©ç›–å›¾åƒå¹¶ç»™ä¸‹æ¸¸è®¡ç®—æœºè§†è§‰åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†Flash-Splitï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ä¸€å¯¹ï¼ˆå¯èƒ½æœªå¯¹é½çš„ï¼‰é—ªå…‰&#x2F;æ— é—ªå…‰å›¾åƒæ¥åˆ†ç¦»é€å°„å…‰å’Œåå°„å…‰çš„ç¨³å¥æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨åˆ©ç”¨é—ªå…‰çº¿ç´¢çš„åŒæ—¶ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œåå°„åˆ†ç¦»ã€‚å…·ä½“æ¥è¯´ï¼ŒFlash-Splitåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åŸºäºç¼–ç çš„é—ªå…‰&#x2F;æ— é—ªå…‰æ½œåœ¨å¯¹æ¡ä»¶çš„åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ï¼Œåˆ†ç¦»åå°„æ½œåœ¨å’Œä¼ è¾“æ½œåœ¨ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†é—ªå…‰&#x2F;æ— é—ªå…‰ä¸å¯¹é½çš„é—®é¢˜ã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡åŸºäºåˆ†ç¦»å‰çš„åŸå§‹å›¾åƒæ¡ä»¶çš„è·¨æ½œåœ¨è§£ç è¿‡ç¨‹ï¼Œå¯¹åˆ†ç¦»çš„æ½œåœ¨æ¢å¤é«˜åˆ†è¾¨ç‡ã€å¿ å®çš„ç»†èŠ‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨å®é™…æŒ‘æˆ˜åœºæ™¯ä¸­å¯¹Flash-Splitè¿›è¡ŒéªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨åå°„åˆ†ç¦»æ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00637v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Flash-Splitæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸€å¯¹é—ªå…‰&#x2F;æ— é—ªå…‰å›¾åƒåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œåå°„åˆ†ç¦»ï¼Œæœ‰æ•ˆåˆ†ç¦»ä¼ è¾“å…‰å’Œåå°„å…‰ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å¯¹é—ªå…‰&#x2F;æ— é—ªå…‰æ½œåœ¨å¯¹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œæœ‰æ•ˆç¼“è§£é—ªå…‰&#x2F;æ— é—ªå…‰å›¾åƒçš„å¯¹é½é—®é¢˜ï¼›ç¬¬äºŒé˜¶æ®µé€šè¿‡è·¨æ½œåœ¨è§£ç è¿‡ç¨‹æ¢å¤åˆ†ç¦»åçš„æ½œåœ¨çš„é«˜åˆ†è¾¨ç‡çœŸå®ç»†èŠ‚ã€‚Flash-Splitåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„åå°„åˆ†ç¦»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Flash-Splitæ¡†æ¶ç”¨äºåˆ†ç¦»é€æ˜è¡¨é¢ä¸Šçš„ä¼ è¾“å…‰å’Œåå°„å…‰ã€‚</li>
<li>åˆ©ç”¨é—ªå…‰&#x2F;æ— é—ªå…‰å›¾åƒå¯¹è¿›è¡Œæ½œåœ¨ç©ºé—´åå°„åˆ†ç¦»ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å¤„ç†é—ªå…‰&#x2F;æ— é—ªå…‰æ½œåœ¨å¯¹ï¼Œè§£å†³å¯¹é½é—®é¢˜ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡è·¨æ½œåœ¨è§£ç æ¢å¤é«˜åˆ†è¾¨ç‡çœŸå®ç»†èŠ‚ã€‚</li>
<li>Flash-Splitåœ¨çœŸå®åœºæ™¯ä¸­çš„åå°„åˆ†ç¦»æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c6b9bf7a104801308e07dab1140115dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93d34e7aaf3408efb5a3c30501be370a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99fae8b51f8b1f9c94d50162128fa82d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-987583c20f7f4a930ba95deb20484b02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e0ab394129dbae9b70d05a14661ce99.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unleashing-Text-to-Image-Diffusion-Prior-for-Zero-Shot-Image-Captioning"><a href="#Unleashing-Text-to-Image-Diffusion-Prior-for-Zero-Shot-Image-Captioning" class="headerlink" title="Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning"></a>Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning</h2><p><strong>Authors:Jianjie Luo, Jingwen Chen, Yehao Li, Yingwei Pan, Jianlin Feng, Hongyang Chao, Ting Yao</strong></p>
<p>Recently, zero-shot image captioning has gained increasing attention, where only text data is available for training. The remarkable progress in text-to-image diffusion model presents the potential to resolve this task by employing synthetic image-caption pairs generated by this pre-trained prior. Nonetheless, the defective details in the salient regions of the synthetic images introduce semantic misalignment between the synthetic image and text, leading to compromised results. To address this challenge, we propose a novel Patch-wise Cross-modal feature Mix-up (PCM) mechanism to adaptively mitigate the unfaithful contents in a fine-grained manner during training, which can be integrated into most of encoder-decoder frameworks, introducing our PCM-Net. Specifically, for each input image, salient visual concepts in the image are first detected considering the image-text similarity in CLIP space. Next, the patch-wise visual features of the input image are selectively fused with the textual features of the salient visual concepts, leading to a mixed-up feature map with less defective content. Finally, a visual-semantic encoder is exploited to refine the derived feature map, which is further incorporated into the sentence decoder for caption generation. Additionally, to facilitate the model training with synthetic data, a novel CLIP-weighted cross-entropy loss is devised to prioritize the high-quality image-text pairs over the low-quality counterparts. Extensive experiments on MSCOCO and Flickr30k datasets demonstrate the superiority of our PCM-Net compared with state-of-the-art VLMs-based approaches. It is noteworthy that our PCM-Net ranks first in both in-domain and cross-domain zero-shot image captioning. The synthetic dataset SynthImgCap and code are available at <a target="_blank" rel="noopener" href="https://jianjieluo.github.io/SynthImgCap">https://jianjieluo.github.io/SynthImgCap</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé›¶æ ·æœ¬å›¾åƒæè¿°ï¼ˆzero-shot image captioningï¼‰è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œè¿™é¡¹ä»»åŠ¡ä»…ä½¿ç”¨æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ˜¾è‘—è¿›å±•è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡åˆ©ç”¨ç”±æ­¤é¢„è®­ç»ƒå…ˆéªŒç”Ÿæˆçš„åˆæˆå›¾åƒ-æè¿°å¯¹æ¥è§£å†³æ­¤ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåˆæˆå›¾åƒçš„å…³é”®åŒºåŸŸçš„ç¼ºé™·ç»†èŠ‚ä¼šå¯¼è‡´åˆæˆå›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸åŒ¹é…ï¼Œä»è€Œäº§ç”Ÿå¦¥åç»“æœã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„Patch-wise Cross-modal feature Mix-upï¼ˆPCMï¼‰æœºåˆ¶ï¼Œä»¥åœ¨è®­ç»ƒæœŸé—´ä»¥ç²¾ç»†çš„æ–¹å¼è‡ªé€‚åº”åœ°å‡è½»ä¸çœŸå®çš„å†…å®¹ã€‚è¯¥æœºåˆ¶å¯ä»¥é›†æˆåˆ°å¤§å¤šæ•°ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ä¸­ï¼Œä»è€Œæ¨å‡ºæˆ‘ä»¬çš„PCM-Netã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä¼šè€ƒè™‘CLIPç©ºé—´ä¸­çš„å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼æ€§æ¥æ£€æµ‹è¾“å…¥å›¾åƒä¸­çš„å…³é”®è§†è§‰æ¦‚å¿µã€‚æ¥ä¸‹æ¥ï¼Œè¾“å…¥å›¾åƒçš„å—çº§è§†è§‰ç‰¹å¾ä¼šé€‰æ‹©æ€§åœ°ä¸å…³é”®è§†è§‰æ¦‚å¿µçš„æ–‡æœ¬ç‰¹å¾èåˆï¼Œä»è€Œäº§ç”Ÿå…·æœ‰è¾ƒå°‘ç¼ºé™·å†…å®¹çš„æ··åˆç‰¹å¾å›¾ã€‚æœ€åï¼Œåˆ©ç”¨è§†è§‰è¯­ä¹‰ç¼–ç å™¨æ¥ä¼˜åŒ–æ´¾ç”Ÿç‰¹å¾å›¾ï¼Œå¹¶è¿›ä¸€æ­¥å°†å…¶èå…¥å¥å­è§£ç å™¨ä»¥ç”Ÿæˆæè¿°ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒçš„ä¾¿åˆ©ï¼Œè®¾è®¡äº†ä¸€ç§æ–°é¢–çš„CLIPåŠ æƒäº¤å‰ç†µæŸå¤±ï¼Œä»¥ä¼˜å…ˆå¯¹å¾…é«˜è´¨é‡å›¾åƒ-æ–‡æœ¬å¯¹ä¼˜äºä½è´¨é‡å¯¹ã€‚åœ¨MSCOCOå’ŒFlickr30kæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PCM-Netä¼˜äºåŸºäºæœ€æ–°VLMsçš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„PCM-Netåœ¨åŸŸå†…å’Œè·¨åŸŸçš„é›¶æ ·æœ¬å›¾åƒæè¿°ä¸­éƒ½æ’åç¬¬ä¸€ã€‚åˆæˆæ•°æ®é›†SynthImgCapå’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://jianjieluo.github.io/SynthImgCap%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://jianjieluo.github.io/SynthImgCapä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00437v1">PDF</a> ECCV 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨é›¶æ ·æœ¬å›¾åƒæè¿°ä»»åŠ¡ï¼Œåˆ©ç”¨æ–‡æœ¬æ•°æ®è®­ç»ƒæ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°ã€‚é’ˆå¯¹åˆæˆå›¾åƒåœ¨æ˜¾è‘—åŒºåŸŸå­˜åœ¨çš„ç»†èŠ‚ç¼ºé™·å¯¼è‡´çš„è¯­ä¹‰å¯¹é½é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„Patch-wise Cross-modal feature Mix-upï¼ˆPCMï¼‰æœºåˆ¶ï¼Œä»¥ç²¾ç»†æ–¹å¼è‡ªé€‚åº”å‡è½»ä¸çœŸå®å†…å®¹ã€‚é€šè¿‡ç»“åˆå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œç”Ÿæˆæ··åˆç‰¹å¾å›¾å¹¶ä¼˜åŒ–è§†è§‰è¯­ä¹‰ç¼–ç å™¨ï¼Œç”¨äºç”Ÿæˆæè¿°ã€‚åŒæ—¶ï¼Œå¼•å…¥CLIPåŠ æƒäº¤å‰ç†µæŸå¤±ä»¥ä¼˜åŒ–åˆæˆæ•°æ®è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MSCOCOå’ŒFlickr30kæ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å›¾åƒæè¿°ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚ç›¸å…³æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…³æ³¨é›¶æ ·æœ¬å›¾åƒæè¿°ä»»åŠ¡ï¼Œå€ŸåŠ©æ–‡æœ¬æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æå‡ºæ–°é¢–çš„Patch-wise Cross-modal feature Mix-upï¼ˆPCMï¼‰æœºåˆ¶ï¼Œè‡ªé€‚åº”å‡è½»åˆæˆå›¾åƒä¸­çš„ä¸çœŸå®å†…å®¹ã€‚</li>
<li>ç»“åˆå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ç”Ÿæˆæ··åˆç‰¹å¾å›¾ï¼Œå¹¶ä¼˜åŒ–è§†è§‰è¯­ä¹‰ç¼–ç å™¨è¿›è¡Œæè¿°ç”Ÿæˆã€‚</li>
<li>å¼•å…¥CLIPåŠ æƒäº¤å‰ç†µæŸå¤±ï¼Œä»¥ä¼˜åŒ–åˆæˆæ•°æ®çš„æ¨¡å‹è®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨MSCOCOå’ŒFlickr30kæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PCM-Netåœ¨é›¶æ ·æœ¬å›¾åƒæè¿°ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ï¼Œæ— è®ºæ˜¯é’ˆå¯¹ç›®æ ‡åŸŸè¿˜æ˜¯è·¨åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-569d7002cabb5ab8c01ea0f433202da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2e72e871791e6b076fba321d55bb3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9650920b8ce48d32c04be8f38df0326c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="diffIRM-A-Diffusion-Augmented-Invariant-Risk-Minimization-Framework-for-Spatiotemporal-Prediction-over-Graphs"><a href="#diffIRM-A-Diffusion-Augmented-Invariant-Risk-Minimization-Framework-for-Spatiotemporal-Prediction-over-Graphs" class="headerlink" title="diffIRM: A Diffusion-Augmented Invariant Risk Minimization Framework for   Spatiotemporal Prediction over Graphs"></a>diffIRM: A Diffusion-Augmented Invariant Risk Minimization Framework for   Spatiotemporal Prediction over Graphs</h2><p><strong>Authors:Zhaobin Mo, Haotian Xiang, Xuan Di</strong></p>
<p>Spatiotemporal prediction over graphs (STPG) is challenging, because real-world data suffers from the Out-of-Distribution (OOD) generalization problem, where test data follow different distributions from training ones. To address this issue, Invariant Risk Minimization (IRM) has emerged as a promising approach for learning invariant representations across different environments. However, IRM and its variants are originally designed for Euclidean data like images, and may not generalize well to graph-structure data such as spatiotemporal graphs due to spatial correlations in graphs. To overcome the challenge posed by graph-structure data, the existing graph OOD methods adhere to the principles of invariance existence, or environment diversity. However, there is little research that combines both principles in the STPG problem. A combination of the two is crucial for efficiently distinguishing between invariant features and spurious ones. In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines these two principles for the STPG problem. Our diffIRM contains two processes: i) data augmentation and ii) invariant learning. In the data augmentation process, a causal mask generator identifies causal features and a graph-based diffusion model acts as an environment augmentor to generate augmented spatiotemporal graph data. In the invariant learning process, an invariance penalty is designed using the augmented data, and then serves as a regularizer for training the spatiotemporal prediction model. The real-world experiment uses three human mobility datasets, i.e. SafeGraph, PeMS04, and PeMS08. Our proposed diffIRM outperforms baselines. </p>
<blockquote>
<p>æ—¶ç©ºå›¾æ•°æ®é¢„æµ‹ï¼ˆSTPGï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºç°å®ä¸–ç•Œçš„æ•°æ®é¢ä¸´ç€åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ³›åŒ–é—®é¢˜ï¼Œå³æµ‹è¯•æ•°æ®ä¸è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒä¸åŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¸å˜é£é™©æœ€å°åŒ–ï¼ˆIRMï¼‰ä½œä¸ºä¸€ç§åœ¨ä¸åŒç¯å¢ƒä¸­å­¦ä¹ ä¸å˜è¡¨ç¤ºçš„æœ‰å‰é€”çš„æ–¹æ³•è€Œå‡ºç°ã€‚ç„¶è€Œï¼ŒIRMåŠå…¶å˜ä½“æœ€åˆæ˜¯ä¸ºæ¬§å‡ é‡Œå¾—æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰è®¾è®¡çš„ï¼Œå¯èƒ½ç”±äºå›¾çš„ç©ºé—´ç›¸å…³æ€§è€Œä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å›¾ç»“æ„æ•°æ®ï¼ˆå¦‚æ—¶ç©ºå›¾ï¼‰ã€‚ä¸ºäº†å…‹æœå›¾ç»“æ„æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç°æœ‰çš„å›¾OODæ–¹æ³•åšæŒä¸å˜æ€§å­˜åœ¨å’Œç¯å¢ƒå¤šæ ·æ€§çš„åŸåˆ™ã€‚ç„¶è€Œï¼Œåœ¨STPGé—®é¢˜ä¸­å¾ˆå°‘æœ‰ç ”ç©¶å°†ä¸¤è€…ç»“åˆèµ·æ¥ã€‚ä¸¤è€…çš„ç»“åˆå¯¹äºæœ‰æ•ˆåœ°åŒºåˆ†ä¸å˜ç‰¹å¾å’Œè™šå‡ç‰¹å¾è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å¡«è¡¥äº†è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆè¿™ä¸¤ä¸ªåŸåˆ™çš„æ—¶ç©ºå›¾æ•°æ®é¢„æµ‹æ‰©æ•£å¢å¼ºä¸å˜é£é™©æœ€å°åŒ–ï¼ˆdiffIRMï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„diffIRMåŒ…å«ä¸¤ä¸ªè¿‡ç¨‹ï¼šiï¼‰æ•°æ®å¢å¼ºå’Œiiï¼‰ã€‚åœ¨æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œå› æœæ©ç ç”Ÿæˆå™¨è¯†åˆ«å› æœç‰¹å¾ï¼ŒåŸºäºå›¾çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºç¯å¢ƒå¢å¼ºå™¨ç”Ÿæˆå¢å¼ºçš„æ—¶ç©ºå›¾æ•°æ®ã€‚åœ¨ä¸å˜å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¢å¼ºæ•°æ®è®¾è®¡ä¸å˜æ€§æƒ©ç½šï¼Œå¹¶å°†å…¶ä½œä¸ºæ—¶ç©ºé¢„æµ‹æ¨¡å‹çš„è®­ç»ƒæ­£åˆ™åŒ–å™¨ã€‚ä½¿ç”¨SafeGraphã€PeMS04å’ŒPeMS08ä¸‰ä¸ªäººç±»ç§»åŠ¨æ•°æ®é›†è¿›è¡Œçš„ç°å®ä¸–ç•Œå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„diffIRMä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00305v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ—¶ç©ºå›¾é¢„æµ‹é¢ä¸´æ¥è‡ªçœŸå®æ•°æ®çš„åˆ†å¸ƒå¤–æ³›åŒ–é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºèåˆå› æœç‰¹å¾è¯†åˆ«ä¸æ‰©æ•£æ¨¡å‹çš„æ‰©æ•£å¢å¼ºä¸å˜é£é™©æœ€å°åŒ–æ¡†æ¶ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œæ•°æ®ä¸å˜ç‰¹å¾çš„é‰´åˆ«ã€‚æ¡†æ¶åŒ…æ‹¬æ•°æ®å¢å¼ºä¸ä¸å˜å­¦ä¹ ä¸¤éƒ¨åˆ†ã€‚åœ¨ç°å®ä¸–ç•Œçš„å®éªŒæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œæ‰€ææ–¹æ³•è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶ç©ºå›¾é¢„æµ‹é¢ä¸´åˆ†å¸ƒå¤–æ³›åŒ–é—®é¢˜ï¼Œå³æµ‹è¯•æ•°æ®ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒçš„é—®é¢˜ã€‚</li>
<li>ä¸å˜é£é™©æœ€å°åŒ–ï¼ˆIRMï¼‰åŠå…¶å˜ä½“åœ¨åº”å¯¹å›¾ç»“æ„æ•°æ®æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ¨å¹¿åˆ°æ—¶ç©ºå›¾ç­‰å›¾å½¢ç»“æ„æ•°æ®ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡èåˆä¸å˜å­˜åœ¨åŸåˆ™å’Œç¯å¢ƒå¤šæ ·æ€§åŸåˆ™æ¥è§£å†³æ—¶ç©ºå›¾çš„STPGé—®é¢˜ã€‚è¿™æ˜¯ç°æœ‰ç ”ç©¶ä¸­å°šæœªå¾—åˆ°å……åˆ†å…³æ³¨çš„é¢†åŸŸã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºâ€œæ‰©æ•£å¢å¼ºä¸å˜é£é™©æœ€å°åŒ–â€ï¼ˆdiffIRMï¼‰çš„æ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºå’Œä¸å˜å­¦ä¹ ä¸¤ä¸ªè¿‡ç¨‹ã€‚</li>
<li>åœ¨æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å› æœæ©ç ç”Ÿæˆå™¨è¯†åˆ«å› æœç‰¹å¾ï¼Œå¹¶ä½¿ç”¨åŸºäºå›¾çš„æ‰©æ•£æ¨¡å‹ä½œä¸ºç¯å¢ƒå¢å¼ºå™¨æ¥ç”Ÿæˆå¢å¼ºçš„æ—¶ç©ºå›¾æ•°æ®ã€‚</li>
<li>åœ¨ä¸å˜å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¢å¼ºæ•°æ®è®¾è®¡ä¸å˜æ€§æƒ©ç½šï¼Œä½œä¸ºè®­ç»ƒæ—¶ç©ºé¢„æµ‹æ¨¡å‹çš„è§„åˆ™åŒ–å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b06ba59b968fe8938b4f58a4db96d839.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc9588296112fc286b53560519562489.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d875dd1ae81ed5f97737f7b767d89f76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72177119ea7412aabe99a206f397f1ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a9887969ed7fcc13dddf73fb7b4261e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9216ff3b6696c7df669a6c8783190fc4.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  nnY-Net Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0a3870e306181dbaebde1d1ae4393429.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  MalCL Leveraging GAN-Based Generative Replay to Combat Catastrophic   Forgetting in Malware Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19211.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
