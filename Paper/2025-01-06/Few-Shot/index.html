<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-06  Digital Guardians Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b6b582ce0516aae506d2f3b24ef77d16.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-06-更新"><a href="#2025-01-06-更新" class="headerlink" title="2025-01-06 更新"></a>2025-01-06 更新</h1><h2 id="Digital-Guardians-Can-GPT-4-Perspective-API-and-Moderation-API-reliably-detect-hate-speech-in-reader-comments-of-German-online-newspapers"><a href="#Digital-Guardians-Can-GPT-4-Perspective-API-and-Moderation-API-reliably-detect-hate-speech-in-reader-comments-of-German-online-newspapers" class="headerlink" title="Digital Guardians: Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?"></a>Digital Guardians: Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?</h2><p><strong>Authors:Manuel Weber, Moritz Huber, Maximilian Auch, Alexander Döschl, Max-Emanuel Keller, Peter Mandl</strong></p>
<p>In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsaw’s (Google) Perspective API, and OpenAI’s Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score. </p>
<blockquote>
<p>近年来，有毒内容和仇恨言论在互联网上变得日益普遍。网络报纸和论坛的管理人员现在需要根据法律要求进行仔细审查，必要时删除读者评论。这是一项劳动密集型的流程。一些大型语言模型的提供商已经提供自动仇恨言论检测或有毒内容识别的解决方案。这包括来自OpenAI的GPT-4o、谷歌Jigsaw的Perspective API以及OpenAI的Moderation API。这些解决方案是基于专门用于开发检测在线报纸读者评论中的仇恨言论的工具有HOCON34k数据集进行对比测试的。测试数据集包含一万五千九百二十二个经过注释的文本样本。GPT-4o采用了三种不同的提示方式，包括零样本、一样本和少样本方法。实验结果表明，GPT-4o在MCC和F2分数的综合指标上超过了Perspective API和Moderation API，并大约超出HOCON34k基线约五个百分点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01256v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了互联网上广泛存在的有毒内容和仇恨言论现象，网络报纸和论坛的版主因此需对读者评论进行仔细审查，甚至删除相关内容，这一过程劳动强度大。一些大型语言模型提供商如OpenAI、Google等已经提供了自动化仇恨言论检测或有毒内容识别的解决方案。基于专门用于开发仇恨言论检测工具的德国测试数据集HOCON34k，这些解决方案通过不同方法进行测试比较，包括Zero-Shot、One-Shot和Few-Shot方法。实验结果表明，GPT-4o的表现优于其他检测器，超过HOCON34k基准线约5个百分点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>互联网上存在大量的有毒内容和仇恨言论，对网络报纸和论坛的版主提出了新的挑战。</li>
<li>为了应对这一挑战，网络报纸和论坛的版主需仔细审查读者评论，甚至删除相关内容，这是一个劳动强度大的过程。</li>
<li>一些大型语言模型提供商已经开发出自动的仇恨言论检测或有毒内容识别解决方案。</li>
<li>GPT-4o在仇恨言论检测方面的表现优于其他检测器，如Google的Perspective API和OpenAI的Moderation API。</li>
<li>GPT-4o的表现是基于德国测试数据集HOCON34k进行验证的。</li>
<li>实验采用Zero-Shot、One-Shot和Few-Shot三种方法测试GPT-4o的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aaca630822b957a82e7dc78d74e30601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-086d0742d835ebf47e5651f255797590.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d25177ac36b022f34582717937ab9e96.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-06\./crop_Few-Shot/2501.01256v1/page_4_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8d62b209ee39806a6037c91bbe20e3e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automated-Self-Refinement-and-Self-Correction-for-LLM-based-Product-Attribute-Value-Extraction"><a href="#Automated-Self-Refinement-and-Self-Correction-for-LLM-based-Product-Attribute-Value-Extraction" class="headerlink" title="Automated Self-Refinement and Self-Correction for LLM-based Product   Attribute Value Extraction"></a>Automated Self-Refinement and Self-Correction for LLM-based Product   Attribute Value Extraction</h2><p><strong>Authors:Alexander Brinkmann, Christian Bizer</strong></p>
<p>Structured product data, in the form of attribute-value pairs, is essential for e-commerce platforms to support features such as faceted product search and attribute-based product comparison. However, vendors often provide unstructured product descriptions, making attribute value extraction necessary to ensure data consistency and usability. Large language models (LLMs) have demonstrated their potential for product attribute value extraction in few-shot scenarios. Recent research has shown that self-refinement techniques can improve the performance of LLMs on tasks such as code generation and text-to-SQL translation. For other tasks, the application of these techniques has resulted in increased costs due to processing additional tokens, without achieving any improvement in performance. This paper investigates applying two self-refinement techniques, error-based prompt rewriting and self-correction, to the product attribute value extraction task. The self-refinement techniques are evaluated across zero-shot, few-shot in-context learning, and fine-tuning scenarios using GPT-4o. The experiments show that both self-refinement techniques have only a marginal impact on the model’s performance across the different scenarios, while significantly increasing processing costs. For scenarios with training data, fine-tuning yields the highest performance, while the ramp-up costs of fine-tuning are balanced out as the amount of product descriptions increases. </p>
<blockquote>
<p>结构化产品数据以属性-值对的形式存在，对于电子商务平台来说，支持面向方面的产品搜索和基于属性的产品比较等功能至关重要。然而，供应商通常提供非结构化的产品描述，因此需要提取属性值以确保数据的一致性和可用性。大型语言模型（LLM）已在少数场景显示出其在产品属性值提取方面的潜力。最近的研究表明，自我完善技术可以提高LLM在代码生成和文本到SQL翻译等任务上的性能。然而，对于其他任务，这些技术的应用由于处理额外的令牌而增加了成本，而没有在性能上实现任何改进。本文研究了两种自我完善技术，基于错误的提示重写和自我修正，并将其应用于产品属性值提取任务。这些自我完善技术在零样本、少数情境内学习和微调场景下使用GPT-4o进行了评估。实验表明，这两种自我完善技术对模型在不同场景下的性能影响微乎其微，却大大提高了处理成本。对于存在训练数据的情况，微调会产生最高的性能，而随着产品描述数量的增加，微调的一次性成本得以平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01237v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于属性-值对的形式的结构化产品数据对电子商务平台至关重要，支持面向方面的产品搜索和基于属性的产品比较等功能。然而，由于供应商提供的产品描述通常是结构化的，因此需要进行属性值的提取以确保数据的一致性和可用性。大型语言模型在少量场景下的产品属性价值提取中表现出了潜力。本文主要探讨了两种自我优化技术——基于错误的提示重写和自我校正，用于产品属性价值提取任务。实验表明，这两种技术在不同场景下对模型性能的影响微乎其微，却显著增加了处理成本。在有训练数据的场景下，微调可带来最高性能，随着产品描述数量的增加，微调的一次性成本得以平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>结构化产品数据对于电子商务平台至关重要，支持多种功能如面向方面的产品搜索和基于属性的产品比较。</li>
<li>供应商提供的产品描述往往是未结构化的，需要进行属性值的提取。</li>
<li>大型语言模型在少量场景下的产品属性价值提取中展现出潜力。</li>
<li>自我优化技术如基于错误的提示重写和自我校正被应用于产品属性价值提取任务。</li>
<li>实验显示自我优化技术对模型性能影响微小，但增加了处理成本。</li>
<li>在有训练数据的场景下，微调能带来最高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca257fd6ccff008cd29a3d7256d6af3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb4a830a3fdd565da5e5a0e972a787f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80261884b7bcfd3392fb548a1804e25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07935791a01f786198363bd22d8e4f83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d18c4cdd794f04b0f197b82f03e163f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c406788329ed3ccacb4163d5129b30.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ValuesRAG-Enhancing-Cultural-Alignment-Through-Retrieval-Augmented-Contextual-Learning"><a href="#ValuesRAG-Enhancing-Cultural-Alignment-Through-Retrieval-Augmented-Contextual-Learning" class="headerlink" title="ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented   Contextual Learning"></a>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented   Contextual Learning</h2><p><strong>Authors:Wonduk Seo, Zonghao Yuan, Yi Bu</strong></p>
<p>Cultural values alignment in Large Language Models (LLMs) is a critical challenge due to their tendency to embed Western-centric biases from training data, leading to misrepresentations and fairness issues in cross-cultural contexts. Recent approaches, such as role-assignment and few-shot learning, often struggle with reliable cultural alignment as they heavily rely on pre-trained knowledge, lack scalability, and fail to capture nuanced cultural values effectively. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with in-context learning to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. Subsequently, we curated several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. ValuesRAG consistently outperforms baseline methods, both in the main experiment and in the ablation study where only the values summary was provided, highlighting ValuesRAG’s potential to foster culturally aligned AI systems and enhance the inclusivity of AI-driven applications. </p>
<blockquote>
<p>大型语言模型（LLM）中的文化价值观对齐是一项关键挑战，因为它们往往嵌入从训练数据中获得的西方中心偏见，导致跨文化背景下的误表征和公平性问题。最近的方法，如角色分配和少样本学习，往往难以可靠地进行文化对齐，因为它们严重依赖于预训练知识，缺乏可扩展性，并且未能有效地捕捉微妙的文化价值观。为了解决这些问题，我们提出了ValuesRAG，这是一个新颖有效的框架，应用检索增强生成（RAG）与上下文学习，以在文本生成过程中动态集成文化和人口统计知识。借助世界价值观调查（WVS）数据集，ValuesRAG首先为每个人生成价值观摘要。随后，我们精心挑选了几个具有代表性的区域数据集作为测试数据集，并根据人口统计特征检索相关的价值观摘要，然后进行重新排序步骤，选择前k个相关摘要。ValuesRAG在主要实验和仅提供价值观摘要的消融研究中均优于基准方法，凸显了ValuesRAG在促进文化对齐的AI系统和提高AI驱动应用包容性的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01031v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在文化价值观对齐方面面临挑战，因训练数据中的西方中心偏见导致跨文化背景下的误表征和公平问题。为解决这个问题，我们提出了ValuesRAG框架，通过检索增强生成（RAG）与上下文学习，动态整合文化和人口统计知识来进行文本生成。该框架利用世界价值观调查（WVS）数据集，首先为每个人生成价值观摘要，然后基于人口统计特征检索相关的价值观摘要并进行重新排序。ValuesRAG在主要实验和仅提供价值观摘要的消融研究中均优于基准方法，显示出其在促进文化对齐的AI系统和提高AI应用程序的包容性方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在跨文化环境中面临文化价值观对齐的挑战，因为训练数据中的西方中心偏见可能导致误表征和公平问题。</li>
<li>目前的方法（如角色分配和少样本学习）在可靠的文化对齐方面存在困难，因为它们严重依赖预训练知识，缺乏可扩展性，并且不能有效地捕捉微妙的文化价值观。</li>
<li>ValuesRAG框架通过结合检索增强生成（RAG）和上下文学习来解决这些问题，能够动态整合文化和人口统计知识来进行文本生成。</li>
<li>ValuesRAG利用世界价值观调查（WVS）数据集为每个个体生成价值观摘要。</li>
<li>ValuesRAG使用代表性区域数据集作为测试数据集，基于人口统计特征检索相关的价值观摘要。</li>
<li>ValuesRAG在主要实验和消融研究中均表现出优异性能，证明其在促进文化对齐的AI系统和提高AI应用程序的包容性方面具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a1c226eb480b3dd421ac7b86b38756e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8d1d70125bc0276ef2d036e518d62e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddf39b873c5ccab77835944e3c81fbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a90d50060385815a35df2bfdedd15f74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12dfab1b163c775011e96225773f8f2b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining"><a href="#2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining" class="headerlink" title="2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining"></a>2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining</h2><p><strong>Authors:Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing</strong></p>
<p>Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\footnote{Our code are available at \url{<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook%7D%7D">https://github.com/DAMO-NLP-SG/multimodal_textbook}}</a>. </p>
<blockquote>
<p>与图像文本配对数据相比，交织语料库使视觉语言模型（VLMs）能够更自然地像人类一样理解世界。然而，这些现有的数据集是从网页上爬取的，面临着知识密度低、图像文本关系松散、图像之间逻辑连贯性差等挑战。另一方面，互联网上有大量教学视频（如在线几何课程）被人类广泛用来学习基础学科，但这些宝贵资源在VLM训练中仍被忽视。在本文中，我们引入了一种高质量的多模式教科书语料库，其中包含用于VLM预训练的更丰富的基础知识。它收集了超过2.5年的教学视频，总计22,000节课时。我们首先使用大型语言模型提出的分类法来系统地收集教学视频。然后，我们从视频中逐步提取和精炼视觉（关键帧）、音频（ASR）和文本知识（OCR），并按时间顺序组织成图像文本交织语料库。与其他相比，我们这种以视频为中心教科书提供了更连贯的上下文、更丰富的知识和更好的图像文本对齐。实验证明了其出色的预训练性能，特别是在知识和推理密集型任务如ScienceQA和MathVista中。此外，在我们教科书上进行预训练的VLM表现出了出色的交织上下文意识，利用视觉和文本线索在少量上下文中进行任务解决。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/DAMO-NLP-SG/multimodal_textbook上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00958v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视觉语言模型（VLM）预训练的多模态教科书语料库。该语料库通过收集超过2.5年的教学视频构建而成，包含丰富的基础知识。通过系统收集教学视频，逐步提取和精炼视觉、音频和文本知识，形成基于时间顺序的图像文本交错语料库。与传统的数据集相比，以视频为中心的教学书籍提供了更连贯的上下文、更丰富的知识和更好的图像文本对齐。实验表明，该数据集在知识密集和推理密集型任务上的预训练性能出色，如ScienceQA和MathVista。此外，在此数据集上训练的VLM模型表现出色，具有出色的交错上下文意识，能够在少数情境下利用视觉和文本线索来解决问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态教科书语料库通过收集教学视频构建，包含丰富的知识。</li>
<li>系统地收集教学视频并提取视觉、音频和文本知识。</li>
<li>基于时间顺序构建图像文本交错语料库，实现更好的图像文本对齐。</li>
<li>与传统数据集相比，该语料库提供更连贯的上下文和丰富的知识。</li>
<li>在知识密集和推理密集型任务上表现出优异的预训练性能。</li>
<li>VLM模型在此数据集上表现出出色的交错上下文意识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3c71d2b7aa07934e40d959baa042e4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b18a17d387835489266645fa6a0aac18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b746baa935d9c774a53f28de48e6272a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4f45ac575c91a43a982075c7f7de11.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusion-Policies-for-Generative-Modeling-of-Spacecraft-Trajectories"><a href="#Diffusion-Policies-for-Generative-Modeling-of-Spacecraft-Trajectories" class="headerlink" title="Diffusion Policies for Generative Modeling of Spacecraft Trajectories"></a>Diffusion Policies for Generative Modeling of Spacecraft Trajectories</h2><p><strong>Authors:Julia Briden, Breanna Johnson, Richard Linares, Abhishek Cauligi</strong></p>
<p>Machine learning has demonstrated remarkable promise for solving the trajectory generation problem and in paving the way for online use of trajectory optimization for resource-constrained spacecraft. However, a key shortcoming in current machine learning-based methods for trajectory generation is that they require large datasets and even small changes to the original trajectory design requirements necessitate retraining new models to learn the parameter-to-solution mapping. In this work, we leverage compositional diffusion modeling to efficiently adapt out-of-distribution data and problem variations in a few-shot framework for 6 degree-of-freedom (DoF) powered descent trajectory generation. Unlike traditional deep learning methods that can only learn the underlying structure of one specific trajectory optimization problem, diffusion models are a powerful generative modeling framework that represents the solution as a probability density function (PDF) and this allows for the composition of PDFs encompassing a variety of trajectory design specifications and constraints. We demonstrate the capability of compositional diffusion models for inference-time 6 DoF minimum-fuel landing site selection and composable constraint representations. Using these samples as initial guesses for 6 DoF powered descent guidance enables dynamically feasible and computationally efficient trajectory generation. </p>
<blockquote>
<p>机器学习在解决轨迹生成问题方面显示出巨大的潜力，并为资源受限航天器的轨迹优化的在线使用奠定了基础。然而，当前基于机器学习的轨迹生成方法的一个关键缺陷是它们需要大量数据集，甚至对原始轨迹设计要求的微小变化都需要重新训练新模型来学习参数到解决方案的映射。在这项工作中，我们利用组合扩散建模，在少数镜头框架内有效地适应超出分配的数据和问题的变化，用于六自由度（DoF）动力下降轨迹生成。与传统只能学习特定轨迹优化问题底层结构的深度学习方法不同，扩散模型是一种强大的生成建模框架，它将解决方案表示为概率密度函数（PDF），这允许包含各种轨迹设计规范和约束的PDF的组合。我们展示了组合扩散模型在推理时间六自由度最低燃料着陆点选择和可组合约束表示方面的能力。使用这些样本作为六自由度动力下降指导的初始猜测，可实现动态可行和计算高效的轨迹生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00915v1">PDF</a> AIAA SCITECH 2025 Forum</p>
<p><strong>Summary</strong></p>
<p>基于机器学习的方法在解决轨迹生成问题上展现出巨大潜力，并为在线使用轨迹优化资源受限的航天器铺平了道路。然而，当前机器学习方法的关键短板在于需要大量数据集，且对原始轨迹设计要求的微小变动都需要重新训练模型来学习参数到解决方案的映射。本研究利用组合扩散建模，在少量样本的框架下有效地适应分布外的数据和问题变化，用于六自由度（DoF）动力下降轨迹生成。不同于只能学习单一轨迹优化问题结构的传统深度学习方法，扩散模型是一个强大的生成建模框架，将解决方案表示为概率密度函数（PDF），这允许包含各种轨迹设计规范和约束的PDF的组合。本研究展示了组合扩散模型在推理时间六自由度最低燃料着陆点选择和可组合约束表示方面的能力。将这些样本作为六自由度动力下降指导的初始猜测，可实现动态可行且计算高效的轨迹生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习在解决轨迹生成问题上具有显著潜力，特别是在在线使用轨迹优化资源受限的航天器中。</li>
<li>当前机器学习方法的一个关键短板是对数据的需求量大，且需要频繁重新训练模型以适应轨迹设计要求的变动。</li>
<li>研究采用了组合扩散建模来应对上述问题，能够在少量样本下有效地适应分布外的数据和问题变化。</li>
<li>扩散模型将解决方案表示为概率密度函数（PDF），不同于传统深度学习方法的单一轨迹优化学习。</li>
<li>扩散模型允许包含多种轨迹设计规范和约束的PDF组合，提高了模型的适应性和灵活性。</li>
<li>研究展示了组合扩散模型在六自由度最低燃料着陆点选择和可组合约束表示方面的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00915">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77671e12bace12163cd81e42b87043c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b9d72dca8e7ee5b724b1c47771f1eee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e119c5f6c12fd3f3909f77f9ea2f60ce.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Aligning-LLMs-with-Domain-Invariant-Reward-Models"><a href="#Aligning-LLMs-with-Domain-Invariant-Reward-Models" class="headerlink" title="Aligning LLMs with Domain Invariant Reward Models"></a>Aligning LLMs with Domain Invariant Reward Models</h2><p><strong>Authors:David Wu, Sanjiban Choudhury</strong></p>
<p>Aligning large language models (LLMs) to human preferences is challenging in domains where preference data is unavailable. We address the problem of learning reward models for such target domains by leveraging feedback collected from simpler source domains, where human preferences are easier to obtain. Our key insight is that, while domains may differ significantly, human preferences convey \emph{domain-agnostic} concepts that can be effectively captured by a reward model. We propose \method, a framework that trains domain-invariant reward models by optimizing a dual loss: a domain loss that minimizes the divergence between source and target distribution, and a source loss that optimizes preferences on the source domain. We show \method is a general approach that we evaluate and analyze across 4 distinct settings: (1) Cross-lingual transfer (accuracy: $0.621 \rightarrow 0.661$), (2) Clean-to-noisy (accuracy: $0.671 \rightarrow 0.703$), (3) Few-shot-to-full transfer (accuracy: $0.845 \rightarrow 0.920$), and (4) Simple-to-complex tasks transfer (correlation: $0.508 \rightarrow 0.556$). Our code, models and data are available at \url{<a target="_blank" rel="noopener" href="https://github.com/portal-cornell/dial%7D">https://github.com/portal-cornell/dial}</a>. </p>
<blockquote>
<p>在大规模语言模型（LLM）在无法获取偏好数据的领域与人类的偏好对齐是一项挑战。我们解决了针对此类目标领域学习奖励模型的问题，通过从更容易获取人类偏好的简单源领域收集反馈并利用其进行分析。我们的关键见解是，尽管域可能有所不同，但人类偏好传达了可以被奖励模型有效捕获的“域不可知”概念。我们提出了方法（Method），这是一个通过优化双重损失来训练域不变奖励模型的框架：一种域损失，用于最小化源域和目标域之间的分布差异；另一种源损失，用于优化源域上的偏好。我们展示了方法是一种通用方法，我们在四个不同的环境中对其进行了评估和分析了：（1）跨语言迁移（准确度：从0.621到0.661）；（2）从清洁到噪声迁移（准确度：从0.671到0.703）；（3）从少量到完整迁移（准确度：从0.845到0.920）；以及（4）从简单到复杂任务的迁移（相关性：从0.508到0.556）。我们的代码、模型和数据都可在<a target="_blank" rel="noopener" href="https://github.com/portal-cornell/dial%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/portal-cornell/dial上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种通过利用简单源域中容易获得的人类反馈来解决目标领域奖励模型学习问题的方法。该方法虽然关注跨域学习，但认为人类偏好传达的是领域通用的概念，通过训练领域不变的奖励模型，优化包括源域和目标域差异的领域损失以及优化源域偏好的源损失。在四种不同场景下的评估表明，该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种解决大型语言模型（LLMs）在缺乏人类偏好数据的领域中与人类偏好对齐的问题。</li>
<li>提出了一种利用简单源域中的反馈来解决目标领域奖励模型学习的方法。</li>
<li>该方法的关键在于认识到虽然领域可能有显著不同，但人类偏好传达的是领域通用的概念。</li>
<li>通过训练领域不变的奖励模型来优化领域损失和源损失，缩小源域和目标域之间的差异。</li>
<li>在跨语言转移、清洁到噪声、少拍转移到全转移以及简单任务到复杂任务转移等四个不同场景下进行了评估。</li>
<li>在所有评估场景中，该方法均显示出有效性，提高了准确性或相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4a9e1e84a83508fb0ef6c3e2dd69087.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfc1f2ba9fa173e5e32f852d06bf41b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecacdc1ff042f1948b8a2f78d5cb8dae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a509ba94f6510eeee145530ba149eb0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-192d060e06c2567a0fd63a717a8e2acb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="U-GIFT-Uncertainty-Guided-Firewall-for-Toxic-Speech-in-Few-Shot-Scenario"><a href="#U-GIFT-Uncertainty-Guided-Firewall-for-Toxic-Speech-in-Few-Shot-Scenario" class="headerlink" title="U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot   Scenario"></a>U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot   Scenario</h2><p><strong>Authors:Jiaxin Song, Xinyu Wang, Yihao Wang, Yifan Tang, Ru Zhang, Jianyi Liu, Gongshen Liu</strong></p>
<p>With the widespread use of social media, user-generated content has surged on online platforms. When such content includes hateful, abusive, offensive, or cyberbullying behavior, it is classified as toxic speech, posing a significant threat to the online ecosystem’s integrity and safety. While manual content moderation is still prevalent, the overwhelming volume of content and the psychological strain on human moderators underscore the need for automated toxic speech detection. Previously proposed detection methods often rely on large annotated datasets; however, acquiring such datasets is both costly and challenging in practice. To address this issue, we propose an uncertainty-guided firewall for toxic speech in few-shot scenarios, U-GIFT, that utilizes self-training to enhance detection performance even when labeled data is limited. Specifically, U-GIFT combines active learning with Bayesian Neural Networks (BNNs) to automatically identify high-quality samples from unlabeled data, prioritizing the selection of pseudo-labels with higher confidence for training based on uncertainty estimates derived from model predictions. Extensive experiments demonstrate that U-GIFT significantly outperforms competitive baselines in few-shot detection scenarios. In the 5-shot setting, it achieves a 14.92% performance improvement over the basic model. Importantly, U-GIFT is user-friendly and adaptable to various pre-trained language models (PLMs). It also exhibits robust performance in scenarios with sample imbalance and cross-domain settings, while showcasing strong generalization across various language applications. We believe that U-GIFT provides an efficient solution for few-shot toxic speech detection, offering substantial support for automated content moderation in cyberspace, thereby acting as a firewall to promote advancements in cybersecurity. </p>
<blockquote>
<p>随着社交媒体广泛使用，用户生成内容已在网上平台激增。当此类内容包含仇恨、滥用、攻击性或网络欺凌行为时，它就被归类为有毒言论，对在线生态系统的完整性和安全构成重大威胁。虽然手动内容管理仍然普遍存在，但内容数量过多以及人类管理者承受的心理压力凸显出需要自动化有毒言论检测。先前提出的检测方法往往依赖于大量注释数据集；然而，在实践中获取此类数据集既昂贵又具有挑战性。为解决此问题，我们针对少数场景的有毒言论提出了一种不确定性引导防火墙，名为U-GIFT。它利用自训练提高检测性能，即使在标记数据有限的情况下也能发挥作用。具体来说，U-GIFT结合主动学习与贝叶斯神经网络（BNNs），自动从未标记的数据中识别高质量样本，优先选择与模型预测得出不确定性估计置信度较高的伪标签进行训练。大量实验表明，在少数场景检测中，U-GIFT显著优于竞争基线。在5次射击设定中，它比基本模型高出14.92%的性能。重要的是，U-GIFT用户友好且适应各种预训练语言模型（PLM）。它在样本不平衡和跨域设置等场景中表现稳健，并且在各种语言应用中表现出强大的泛化能力。我们相信U-GIFT为少数有毒言论检测提供了有效的解决方案，为网络空间的自动化内容管理提供了有力支持，从而充当防火墙促进网络安全的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00907v1">PDF</a> 16 pages, 6 figures and 10 tables. Comments are welcome</p>
<p><strong>摘要</strong></p>
<p>社交媒体用户生成内容的激增带来了网络生态中的有毒言论问题。这些有毒言论包括仇恨、滥用、攻击或网络欺凌行为，对在线生态系统的完整性和安全性构成严重威胁。为了解决标注数据不足的问题，我们提出了基于不确定性的防火墙系统U-GIFT来解决数据受限场景下的有毒言论检测问题。该系统结合了主动学习、贝叶斯神经网络进行自训练，能够从无标签数据中自动识别高质量样本，并根据模型预测的不确定性估计来优先选取高置信度的伪标签进行训练。实验表明，在样本量较小的情况下，U-GIFT系统显著优于其他基线模型。相较于基本模型，其在5样本场景中实现了高达14.92%的性能提升。此外，U-GIFT具有良好的用户友好性和适应性，适用于多种预训练语言模型。系统在样本不均衡和跨域设置下展现了强大的性能，并在多种语言应用中表现出良好的泛化能力。我们相信U-GIFT能够有效支持网络安全领域的自动内容审核和网络安全防护的推动工作。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>用户生成内容的激增引发了网络中的有毒言论问题。</li>
<li>有毒言论对在线生态系统的完整性和安全性构成威胁。</li>
<li>提出了一种基于不确定性的防火墙系统U-GIFT解决该问题。在标注数据有限的情况下能够显著提升有毒言论检测性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f6b8cc808e21db9d8bf442fbfac9001d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6b582ce0516aae506d2f3b24ef77d16.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation"><a href="#FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation" class="headerlink" title="FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation"></a>FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation</h2><p><strong>Authors:Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt</strong></p>
<p>Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF’s core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals. </p>
<blockquote>
<p>在自然语言处理（NLP）中，反事实例子被广泛应用于提升模型价值的数据，并在可解释人工智能（XAI）中用于理解模型行为。尽管大型语言模型（LLM）在许多任务上表现出色，但自动生成反事实例子仍然是一个具有挑战性的任务。在本文中，我们首先介绍了ZeroCF，这是一种忠实的方法，利用特征归因方法得出的重要单词，在无样本设置下生成反事实例子。其次，我们提出了一个新的框架FitCF，它通过标签翻转验证进一步验证了上述反事实，然后将其作为演示用于少样本提示，优于两种最新技术水平的基线。通过消融研究，我们确定了FitCF每个核心组件在提高反事实质量（通过翻转率、困惑度和相似性度量进行评估）方面的重要性。此外，我们展示了LIME和集成梯度作为FitCF的骨干归因方法的有效性，并发现演示的数量对性能的影响最大。最后，我们发现特征归因分数的忠实性与生成的反事实质量之间存在强烈的关联。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00777v1">PDF</a> In submission</p>
<p><strong>Summary</strong></p>
<p>本论文介绍了在自然语言处理（NLP）和可解释人工智能（XAI）中广泛应用的反事实实例。针对大型语言模型（LLMs）在自动生成反事实实例方面的挑战，提出了ZeroCF方法，利用特征归属方法得出的重要词汇在零样本环境下生成反事实实例。进一步，提出了FitCF框架，通过标签翻转验证来验证上述反事实，并将其作为少样本提示插入，优于两种先进的基线方法。通过消融研究，确定了FitCF核心组件在提高反事实质量方面的作用，并通过翻转率、困惑度和相似性度量进行评估。同时，展示了LIME和集成梯度作为FitCF的骨干归属方法的有效性，并发现演示数量对性能的影响最大。最后，揭示了特征归属分数忠实性与生成反事实质量之间的强烈相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Counterfactual examples are valuable in NLP and XAI.<br>2.自动生成反事实实例对于大型语言模型（LLMs）来说是一项具有挑战性的任务。</li>
<li>ZeroCF方法利用特征归属方法得出的重要词汇在零样本环境下生成反事实实例。</li>
<li>FitCF框架通过标签翻转验证来验证反事实，并将其用于少样本提示。</li>
<li>FitCF框架优于两种先进的基线方法。</li>
<li>消融研究确定了FitCF核心组件对提高反事实质量的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca65d4511693a2436e8d401dcc19e61a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a27810c25050d378224316b8483e4e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a01cf4a6a482641125a7684a7732264.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Foreground-Covering-Prototype-Generation-and-Matching-for-SAM-Aided-Few-Shot-Segmentation"><a href="#Foreground-Covering-Prototype-Generation-and-Matching-for-SAM-Aided-Few-Shot-Segmentation" class="headerlink" title="Foreground-Covering Prototype Generation and Matching for SAM-Aided   Few-Shot Segmentation"></a>Foreground-Covering Prototype Generation and Matching for SAM-Aided   Few-Shot Segmentation</h2><p><strong>Authors:Suho Park, SuBeen Lee, Hyun Seok Seong, Jaejoon Yoo, Jae-Pil Heo</strong></p>
<p>We propose Foreground-Covering Prototype Generation and Matching to resolve Few-Shot Segmentation (FSS), which aims to segment target regions in unlabeled query images based on labeled support images. Unlike previous research, which typically estimates target regions in the query using support prototypes and query pixels, we utilize the relationship between support and query prototypes. To achieve this, we utilize two complementary features: SAM Image Encoder features for pixel aggregation and ResNet features for class consistency. Specifically, we construct support and query prototypes with SAM features and distinguish query prototypes of target regions based on ResNet features. For the query prototype construction, we begin by roughly guiding foreground regions within SAM features using the conventional pseudo-mask, then employ iterative cross-attention to aggregate foreground features into learnable tokens. Here, we discover that the cross-attention weights can effectively alternate the conventional pseudo-mask. Therefore, we use the attention-based pseudo-mask to guide ResNet features to focus on the foreground, then infuse the guided ResNet feature into the learnable tokens to generate class-consistent query prototypes. The generation of the support prototype is conducted symmetrically to that of the query one, with the pseudo-mask replaced by the ground-truth mask. Finally, we compare these query prototypes with support ones to generate prompts, which subsequently produce object masks through the SAM Mask Decoder. Our state-of-the-art performances on various datasets validate the effectiveness of the proposed method for FSS. Our official code is available at <a target="_blank" rel="noopener" href="https://github.com/SuhoPark0706/FCP">https://github.com/SuhoPark0706/FCP</a> </p>
<blockquote>
<p>我们提出前景覆盖原型生成与匹配来解决小样本分割（FSS）问题，其目标是在无标签的查询图像中基于有标签的支持图像对目标区域进行分割。与通常使用支持原型和查询像素来估计查询中的目标区域的先前研究不同，我们利用支持原型和查询原型之间的关系。为实现这一点，我们利用两个互补的特性：SAM图像编码器的像素聚合特征和ResNet的特性保持类一致性。具体来说，我们使用SAM特征构建支持和查询原型，并根据ResNet特征区分查询目标区域的原型。对于查询原型的构建，我们首先使用传统的伪掩码在SAM特征中大致指导前景区域，然后采用迭代交叉注意力将前景特征聚集到可学习的令牌中。在这里，我们发现交叉注意力权重可以有效地替代传统的伪掩码。因此，我们使用基于注意力的伪掩码来指导ResNet特征专注于前景，然后将引导的ResNet特征融合到可学习的令牌中，以生成类一致的查询原型。支持原型的生成方式与查询原型的生成方式对称，其中伪掩码被真实掩码所替代。最后，我们将这些查询原型与支持原型进行比较以生成提示，然后通过SAM Mask解码器产生对象掩码。我们在各种数据集上的最新性能表现验证了所提出方法在小样本分割中的有效性。我们的官方代码可在XXX中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00752v1">PDF</a> Association for the Advancement of Artificial Intelligence (AAAI)   2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种解决Few-Shot Segmentation（FSS）问题的方法，名为前景覆盖原型生成与匹配。该方法旨在基于标记的支持图像对未标记查询图像中的目标区域进行分割。通过利用支持图像和查询图像之间的原型关系，结合SAM图像编码器的像素聚合特征和ResNet模型的类别一致性特征，构建支持原型和查询原型。利用基于注意力的伪掩码指导ResNet特征聚焦于前景，生成类别一致的查询原型。该方法在多个数据集上的表现均达到领先水平，验证了其在FSS问题上的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入前景覆盖原型生成与匹配方法解决Few-Shot Segmentation（FSS）问题。</li>
<li>利用支持图像和查询图像之间的原型关系。</li>
<li>结合SAM图像编码器的像素聚合特征和ResNet模型的类别一致性特征。</li>
<li>通过基于注意力的伪掩码引导ResNet特征聚焦于前景。</li>
<li>生成类别一致的查询原型和支持原型。</li>
<li>在多个数据集上达到领先水平，验证了方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00752">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6d945c746c7b64d781c6c0caa96afc1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0739c0db8bfa28a7cd8bdbc400742f84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7e96b08916b8bd1329536b71cd92ad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34da2b4af9884b9d4881d80d79aae15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41e46ec5ba61fc4e7f34ccd2558ce4b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Reasoning-with-Multi-Path-Collaborative-Reactive-and-Reflection-agents"><a href="#Enhancing-LLM-Reasoning-with-Multi-Path-Collaborative-Reactive-and-Reflection-agents" class="headerlink" title="Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and   Reflection agents"></a>Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and   Reflection agents</h2><p><strong>Authors:Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, Huimin Ma</strong></p>
<p>Agents have demonstrated their potential in scientific reasoning tasks through large language models. However, they often face challenges such as insufficient accuracy and degeneration of thought when handling complex reasoning tasks, which impede their performance. To overcome these issues, we propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP) Framework, aimed at enhancing the reasoning capabilities of LLMs. Our approach improves scientific reasoning accuracy by employing a multi-path reasoning mechanism where each path consists of a reactive agent and a reflection agent that collaborate to prevent degeneration of thought inherent in single-agent reliance. Additionally, the RR-MP framework does not require additional training; it utilizes multiple dialogue instances for each reasoning path and a separate summarizer to consolidate insights from all paths. This design integrates diverse perspectives and strengthens reasoning across each path. We conducted zero-shot and few-shot evaluations on tasks involving moral scenarios, college-level physics, and mathematics. Experimental results demonstrate that our method outperforms baseline approaches, highlighting the effectiveness and advantages of the RR-MP framework in managing complex scientific reasoning tasks. </p>
<blockquote>
<p>人工智能在科研推理任务中展现出了巨大的潜力，这主要得益于大型语言模型的应用。然而，在处理复杂的推理任务时，它们常面临诸如准确度不足和思维退化等挑战，制约了其性能表现。为了克服这些问题，我们提出了带有多路推理的反应与反思代理（RR-MP）框架，旨在增强大型语言模型的推理能力。我们的方法采用多路推理机制来提升科学推理的准确度，每条路径包含一名反应代理和一名反思代理，他们协同工作，防止单一代理依赖导致的思维退化。此外，RR-MP框架无需额外训练，它利用每个推理路径的多个对话实例和一个单独的摘要器来整合所有路径的见解。这种设计融合了不同的观点，加强了每条路径的推理能力。我们对涉及道德情景、大学物理和数学的任务进行了零样本和少样本评估。实验结果表明，我们的方法优于基准方法，突显了RR-MP框架在处理复杂科学推理任务中的有效性和优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00430v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>反应与反思多路径推理框架（RR-MP）旨在增强大型语言模型（LLM）的推理能力，解决其在处理复杂推理任务时面临的准确性不足和思维退化问题。该框架通过采用多路径推理机制，每个路径包含反应和反思两个代理，协作防止单一代理依赖导致的思维退化。RR-MP框架无需额外训练，利用每个推理路径的多个对话实例和单独的总结者来整合不同观点，加强各路径的推理能力。在道德场景、大学物理和数学任务上的零样本和少样本评估表明，该方法优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>反应与反思多路径推理框架（RR-MP）旨在增强大型语言模型（LLM）在复杂科学推理任务中的性能。</li>
<li>RR-MP框架采用多路径推理机制，通过反应和反思代理的协作，解决思维退化问题。</li>
<li>该框架通过整合多个对话实例和不同观点，提高科学推理的准确性。</li>
<li>RR-MP框架设计独特，无需额外训练，通过单独的总结者模块来巩固各路径的见解。</li>
<li>实验结果表明，RR-MP框架在道德场景、大学物理和数学任务上的表现优于基线方法。</li>
<li>RR-MP框架能够管理复杂的科学推理任务，显示出其有效性和优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cabff3ef390637019e649bbb455e8c91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9feef85e267655219ff0aab8e9ad996d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27c975eca623b571c1b785f18c182a50.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAM-Aware-Graph-Prompt-Reasoning-Network-for-Cross-Domain-Few-Shot-Segmentation"><a href="#SAM-Aware-Graph-Prompt-Reasoning-Network-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot   Segmentation"></a>SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Shi-Feng Peng, Guolei Sun, Yong Li, Hongsong Wang, Guo-Sen Xie</strong></p>
<p>The primary challenge of cross-domain few-shot segmentation (CD-FSS) is the domain disparity between the training and inference phases, which can exist in either the input data or the target classes. Previous models struggle to learn feature representations that generalize to various unknown domains from limited training domain samples. In contrast, the large-scale visual model SAM, pre-trained on tens of millions of images from various domains and classes, possesses excellent generalizability. In this work, we propose a SAM-aware graph prompt reasoning network (GPRN) that fully leverages SAM to guide CD-FSS feature representation learning and improve prediction accuracy. Specifically, we propose a SAM-aware prompt initialization module (SPI) to transform the masks generated by SAM into visual prompts enriched with high-level semantic information. Since SAM tends to divide an object into many sub-regions, this may lead to visual prompts representing the same semantic object having inconsistent or fragmented features. We further propose a graph prompt reasoning (GPR) module that constructs a graph among visual prompts to reason about their interrelationships and enable each visual prompt to aggregate information from similar prompts, thus achieving global semantic consistency. Subsequently, each visual prompt embeds its semantic information into the corresponding mask region to assist in feature representation learning. To refine the segmentation mask during testing, we also design a non-parameter adaptive point selection module (APS) to select representative point prompts from query predictions and feed them back to SAM to refine inaccurate segmentation results. Experiments on four standard CD-FSS datasets demonstrate that our method establishes new state-of-the-art results. Code: <a target="_blank" rel="noopener" href="https://github.com/CVL-hub/GPRN">https://github.com/CVL-hub/GPRN</a>. </p>
<blockquote>
<p>跨域小样本分割（CD-FSS）的主要挑战在于训练和推理阶段之间的域差异，这种差异可能存在于输入数据或目标类别中。之前的模型很难从有限的训练域样本中学习能够推广到各种未知域的特征表示。相比之下，大型视觉模型SAM在来自各个域和类别的数千万图像上进行预训练，具有良好的泛化能力。在这项工作中，我们提出了一种基于SAM的图形提示推理网络（GPRN），该网络充分利用SAM来指导CD-FSS特征表示学习并提高预测精度。具体来说，我们提出了一个SAM感知提示初始化模块（SPI），将SAM生成的掩码转换为包含高级语义信息的视觉提示。由于SAM倾向于将一个对象分成多个子区域，这可能导致表示相同语义对象的视觉提示具有不一致或碎片化的特征。我们进一步提出了一个图形提示推理（GPR）模块，该模块在视觉提示之间构建图形，推理它们之间的关系，并允许每个视觉提示从类似提示中聚合信息，从而实现全局语义一致性。随后，每个视觉提示将其语义信息嵌入到相应的掩码区域中，以辅助特征表示学习。为了在测试期间细化分割掩码，我们还设计了一个非参数自适应点选择模块（APS），从查询预测中选择代表性的点提示并反馈到SAM，以优化不准确的分割结果。在四个标准的CD-FSS数据集上的实验表明，我们的方法达到了新的最新水平。代码地址：<a target="_blank" rel="noopener" href="https://github.com/CVL-hub/GPRN">https://github.com/CVL-hub/GPRN</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00303v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong><br>     跨域小样本分割（CD-FSS）的主要挑战在于训练和推理阶段存在的域差异问题，可能出现在输入数据或目标类别上。以前的方法难以从有限的训练域样本中学习特征表示来推广到多个未知域。本文提出了基于大规模视觉模型SAM的GPRN网络，利用SAM指导CD-FSS的特征表示学习，提高预测精度。通过SAM感知的提示初始化模块（SPI）转换SAM生成的掩膜为富含高级语义信息的视觉提示。为处理SAM倾向于将物体分为多个子区域造成的问题，提出图提示推理（GPR）模块，构建视觉提示之间的图进行关系推理，使每个视觉提示能聚合相似提示的信息，实现全局语义一致性。此外，设计了非参数自适应点选择模块（APS）来优化测试阶段的分割掩膜。在四个标准CD-FSS数据集上的实验表明，该方法取得了最新成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSS面临的主要挑战是训练和推理阶段的域差异问题。</li>
<li>之前的方法难以从有限的训练域样本中学习适应各种未知域的特征表示。</li>
<li>提出了基于大规模视觉模型SAM的GPRN网络来解决这个问题。</li>
<li>SAM感知的提示初始化模块（SPI）转换掩膜为富含高级语义信息的视觉提示。</li>
<li>图提示推理（GPR）模块用于处理视觉提示之间的不一致性和碎片化问题，实现全局语义一致性。</li>
<li>非参数自适应点选择模块（APS）用于优化测试阶段的分割掩膜。</li>
<li>在四个标准CD-FSS数据集上的实验取得了最新成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00303">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-70ad2ee418244e3e6c25cf12fca8b435.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8969bbdfe079fbb2ae5d597c4ca416c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0b392d1392bcd768f4f35a34e45f27f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Learning-in-Multiple-Spaces-Few-Shot-Network-Attack-Detection-with-Metric-Fused-Prototypical-Networks"><a href="#Learning-in-Multiple-Spaces-Few-Shot-Network-Attack-Detection-with-Metric-Fused-Prototypical-Networks" class="headerlink" title="Learning in Multiple Spaces: Few-Shot Network Attack Detection with   Metric-Fused Prototypical Networks"></a>Learning in Multiple Spaces: Few-Shot Network Attack Detection with   Metric-Fused Prototypical Networks</h2><p><strong>Authors:Fernando Martinez-Lopez, Lesther Santana, Mohamed Rahouti</strong></p>
<p>Network intrusion detection systems face significant challenges in identifying emerging attack patterns, especially when limited data samples are available. To address this, we propose a novel Multi-Space Prototypical Learning (MSPL) framework tailored for few-shot attack detection. The framework operates across multiple metric spaces-Euclidean, Cosine, Chebyshev, and Wasserstein distances-integrated through a constrained weighting scheme to enhance embedding robustness and improve pattern recognition. By leveraging Polyak-averaged prototype generation, the framework stabilizes the learning process and effectively adapts to rare and zero-day attacks. Additionally, an episodic training paradigm ensures balanced representation across diverse attack classes, enabling robust generalization. Experimental results on benchmark datasets demonstrate that MSPL outperforms traditional approaches in detecting low-profile and novel attack types, establishing it as a robust solution for zero-day attack detection. </p>
<blockquote>
<p>网络入侵检测系统在面对新兴攻击模式识别时面临重大挑战，特别是在有限的样本数据下。为解决这一问题，我们提出了一种针对小样本攻击检测的新型多空间原型学习（MSPL）框架。该框架在多个度量空间（欧几里得距离、余弦距离、切比雪夫距离和瓦瑟斯坦距离）中运行，通过约束加权方案进行整合，以提高嵌入的稳健性和模式识别能力。通过利用Polyak平均原型生成技术，该框架稳定了学习过程，并能有效地适应罕见和零日攻击。此外，采用情景训练模式确保不同攻击类别的均衡表示，从而实现稳健的泛化。在基准数据集上的实验结果表明，MSPL在检测低配置和新型攻击类型方面优于传统方法，已成为一种有效的零日攻击检测解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00050v1">PDF</a> The AAAI-25 Workshop on Artificial Intelligence for Cyber Security   (AICS)</p>
<p><strong>Summary</strong></p>
<p>文章提出了针对少量数据样本攻击检测的多空间原型学习（MSPL）框架。该框架利用多种度量空间技术提高嵌入稳健性和模式识别能力，并有效适应稀有和零日攻击。实验结果表明，MSPL在检测低轮廓和新型攻击类型方面优于传统方法，成为零日攻击检测的稳健解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>网络入侵检测系统面临识别新兴攻击模式的挑战，尤其是在有限数据样本的情况下。</li>
<li>提出了一种多空间原型学习（MSPL）框架，用于解决该问题。</li>
<li>MSPL框架在多种度量空间（如欧几里得距离、余弦距离、切比雪夫距离和瓦瑟斯坦距离）上进行操作，通过约束加权方案进行集成。</li>
<li>利用Polyak平均原型生成技术增强学习过程的稳定性，有效适应稀有和零日攻击。</li>
<li>采用周期训练模式确保各种攻击类的均衡表示，实现稳健的泛化。</li>
<li>实验结果表明，MSPL框架在检测低轮廓和新型攻击类型方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6079a54f9c1eb24a9757f50c8693f8c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8be21bd3cb66f318e02cda77494d91a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-918c059410e8702e6533fe45f38af1fb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fc6cf2bd7938e5bba0c1f29e3463ac4d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-01-06  Test-time Controllable Image Generation by Explicit Spatial Constraint   Enforcement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f1aa26cb833b8d8dc5bfb417adf754a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-06  A3 Android Agent Arena for Mobile GUI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11176.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
