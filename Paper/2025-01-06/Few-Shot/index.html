<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  Digital Guardians Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b6b582ce0516aae506d2f3b24ef77d16.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-06-æ›´æ–°"><a href="#2025-01-06-æ›´æ–°" class="headerlink" title="2025-01-06 æ›´æ–°"></a>2025-01-06 æ›´æ–°</h1><h2 id="Digital-Guardians-Can-GPT-4-Perspective-API-and-Moderation-API-reliably-detect-hate-speech-in-reader-comments-of-German-online-newspapers"><a href="#Digital-Guardians-Can-GPT-4-Perspective-API-and-Moderation-API-reliably-detect-hate-speech-in-reader-comments-of-German-online-newspapers" class="headerlink" title="Digital Guardians: Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?"></a>Digital Guardians: Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?</h2><p><strong>Authors:Manuel Weber, Moritz Huber, Maximilian Auch, Alexander DÃ¶schl, Max-Emanuel Keller, Peter Mandl</strong></p>
<p>In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsawâ€™s (Google) Perspective API, and OpenAIâ€™s Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæœ‰æ¯’å†…å®¹å’Œä»‡æ¨è¨€è®ºåœ¨äº’è”ç½‘ä¸Šå˜å¾—æ—¥ç›Šæ™®éã€‚ç½‘ç»œæŠ¥çº¸å’Œè®ºå›çš„ç®¡ç†äººå‘˜ç°åœ¨éœ€è¦æ ¹æ®æ³•å¾‹è¦æ±‚è¿›è¡Œä»”ç»†å®¡æŸ¥ï¼Œå¿…è¦æ—¶åˆ é™¤è¯»è€…è¯„è®ºã€‚è¿™æ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹çš„æµç¨‹ã€‚ä¸€äº›å¤§å‹è¯­è¨€æ¨¡å‹çš„æä¾›å•†å·²ç»æä¾›è‡ªåŠ¨ä»‡æ¨è¨€è®ºæ£€æµ‹æˆ–æœ‰æ¯’å†…å®¹è¯†åˆ«çš„è§£å†³æ–¹æ¡ˆã€‚è¿™åŒ…æ‹¬æ¥è‡ªOpenAIçš„GPT-4oã€è°·æ­ŒJigsawçš„Perspective APIä»¥åŠOpenAIçš„Moderation APIã€‚è¿™äº›è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºä¸“é—¨ç”¨äºå¼€å‘æ£€æµ‹åœ¨çº¿æŠ¥çº¸è¯»è€…è¯„è®ºä¸­çš„ä»‡æ¨è¨€è®ºçš„å·¥å…·æœ‰HOCON34kæ•°æ®é›†è¿›è¡Œå¯¹æ¯”æµ‹è¯•çš„ã€‚æµ‹è¯•æ•°æ®é›†åŒ…å«ä¸€ä¸‡äº”åƒä¹ç™¾äºŒåäºŒä¸ªç»è¿‡æ³¨é‡Šçš„æ–‡æœ¬æ ·æœ¬ã€‚GPT-4oé‡‡ç”¨äº†ä¸‰ç§ä¸åŒçš„æç¤ºæ–¹å¼ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨MCCå’ŒF2åˆ†æ•°çš„ç»¼åˆæŒ‡æ ‡ä¸Šè¶…è¿‡äº†Perspective APIå’ŒModeration APIï¼Œå¹¶å¤§çº¦è¶…å‡ºHOCON34kåŸºçº¿çº¦äº”ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01256v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äº’è”ç½‘ä¸Šå¹¿æ³›å­˜åœ¨çš„æœ‰æ¯’å†…å®¹å’Œä»‡æ¨è¨€è®ºç°è±¡ï¼Œç½‘ç»œæŠ¥çº¸å’Œè®ºå›çš„ç‰ˆä¸»å› æ­¤éœ€å¯¹è¯»è€…è¯„è®ºè¿›è¡Œä»”ç»†å®¡æŸ¥ï¼Œç”šè‡³åˆ é™¤ç›¸å…³å†…å®¹ï¼Œè¿™ä¸€è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦å¤§ã€‚ä¸€äº›å¤§å‹è¯­è¨€æ¨¡å‹æä¾›å•†å¦‚OpenAIã€Googleç­‰å·²ç»æä¾›äº†è‡ªåŠ¨åŒ–ä»‡æ¨è¨€è®ºæ£€æµ‹æˆ–æœ‰æ¯’å†…å®¹è¯†åˆ«çš„è§£å†³æ–¹æ¡ˆã€‚åŸºäºä¸“é—¨ç”¨äºå¼€å‘ä»‡æ¨è¨€è®ºæ£€æµ‹å·¥å…·çš„å¾·å›½æµ‹è¯•æ•°æ®é›†HOCON34kï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆé€šè¿‡ä¸åŒæ–¹æ³•è¿›è¡Œæµ‹è¯•æ¯”è¾ƒï¼ŒåŒ…æ‹¬Zero-Shotã€One-Shotå’ŒFew-Shotæ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oçš„è¡¨ç°ä¼˜äºå…¶ä»–æ£€æµ‹å™¨ï¼Œè¶…è¿‡HOCON34kåŸºå‡†çº¿çº¦5ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’è”ç½‘ä¸Šå­˜åœ¨å¤§é‡çš„æœ‰æ¯’å†…å®¹å’Œä»‡æ¨è¨€è®ºï¼Œå¯¹ç½‘ç»œæŠ¥çº¸å’Œè®ºå›çš„ç‰ˆä¸»æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç½‘ç»œæŠ¥çº¸å’Œè®ºå›çš„ç‰ˆä¸»éœ€ä»”ç»†å®¡æŸ¥è¯»è€…è¯„è®ºï¼Œç”šè‡³åˆ é™¤ç›¸å…³å†…å®¹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¼ºåº¦å¤§çš„è¿‡ç¨‹ã€‚</li>
<li>ä¸€äº›å¤§å‹è¯­è¨€æ¨¡å‹æä¾›å•†å·²ç»å¼€å‘å‡ºè‡ªåŠ¨çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æˆ–æœ‰æ¯’å†…å®¹è¯†åˆ«è§£å†³æ–¹æ¡ˆã€‚</li>
<li>GPT-4oåœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–æ£€æµ‹å™¨ï¼Œå¦‚Googleçš„Perspective APIå’ŒOpenAIçš„Moderation APIã€‚</li>
<li>GPT-4oçš„è¡¨ç°æ˜¯åŸºäºå¾·å›½æµ‹è¯•æ•°æ®é›†HOCON34kè¿›è¡ŒéªŒè¯çš„ã€‚</li>
<li>å®éªŒé‡‡ç”¨Zero-Shotã€One-Shotå’ŒFew-Shotä¸‰ç§æ–¹æ³•æµ‹è¯•GPT-4oçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aaca630822b957a82e7dc78d74e30601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-086d0742d835ebf47e5651f255797590.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d25177ac36b022f34582717937ab9e96.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-06\./crop_Few-Shot/2501.01256v1/page_4_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8d62b209ee39806a6037c91bbe20e3e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automated-Self-Refinement-and-Self-Correction-for-LLM-based-Product-Attribute-Value-Extraction"><a href="#Automated-Self-Refinement-and-Self-Correction-for-LLM-based-Product-Attribute-Value-Extraction" class="headerlink" title="Automated Self-Refinement and Self-Correction for LLM-based Product   Attribute Value Extraction"></a>Automated Self-Refinement and Self-Correction for LLM-based Product   Attribute Value Extraction</h2><p><strong>Authors:Alexander Brinkmann, Christian Bizer</strong></p>
<p>Structured product data, in the form of attribute-value pairs, is essential for e-commerce platforms to support features such as faceted product search and attribute-based product comparison. However, vendors often provide unstructured product descriptions, making attribute value extraction necessary to ensure data consistency and usability. Large language models (LLMs) have demonstrated their potential for product attribute value extraction in few-shot scenarios. Recent research has shown that self-refinement techniques can improve the performance of LLMs on tasks such as code generation and text-to-SQL translation. For other tasks, the application of these techniques has resulted in increased costs due to processing additional tokens, without achieving any improvement in performance. This paper investigates applying two self-refinement techniques, error-based prompt rewriting and self-correction, to the product attribute value extraction task. The self-refinement techniques are evaluated across zero-shot, few-shot in-context learning, and fine-tuning scenarios using GPT-4o. The experiments show that both self-refinement techniques have only a marginal impact on the modelâ€™s performance across the different scenarios, while significantly increasing processing costs. For scenarios with training data, fine-tuning yields the highest performance, while the ramp-up costs of fine-tuning are balanced out as the amount of product descriptions increases. </p>
<blockquote>
<p>ç»“æ„åŒ–äº§å“æ•°æ®ä»¥å±æ€§-å€¼å¯¹çš„å½¢å¼å­˜åœ¨ï¼Œå¯¹äºç”µå­å•†åŠ¡å¹³å°æ¥è¯´ï¼Œæ”¯æŒé¢å‘æ–¹é¢çš„äº§å“æœç´¢å’ŒåŸºäºå±æ€§çš„äº§å“æ¯”è¾ƒç­‰åŠŸèƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¾›åº”å•†é€šå¸¸æä¾›éç»“æ„åŒ–çš„äº§å“æè¿°ï¼Œå› æ­¤éœ€è¦æå–å±æ€§å€¼ä»¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å°‘æ•°åœºæ™¯æ˜¾ç¤ºå‡ºå…¶åœ¨äº§å“å±æ€§å€¼æå–æ–¹é¢çš„æ½œåŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè‡ªæˆ‘å®Œå–„æŠ€æœ¯å¯ä»¥æé«˜LLMåœ¨ä»£ç ç”Ÿæˆå’Œæ–‡æœ¬åˆ°SQLç¿»è¯‘ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œè¿™äº›æŠ€æœ¯çš„åº”ç”¨ç”±äºå¤„ç†é¢å¤–çš„ä»¤ç‰Œè€Œå¢åŠ äº†æˆæœ¬ï¼Œè€Œæ²¡æœ‰åœ¨æ€§èƒ½ä¸Šå®ç°ä»»ä½•æ”¹è¿›ã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯ï¼ŒåŸºäºé”™è¯¯çš„æç¤ºé‡å†™å’Œè‡ªæˆ‘ä¿®æ­£ï¼Œå¹¶å°†å…¶åº”ç”¨äºäº§å“å±æ€§å€¼æå–ä»»åŠ¡ã€‚è¿™äº›è‡ªæˆ‘å®Œå–„æŠ€æœ¯åœ¨é›¶æ ·æœ¬ã€å°‘æ•°æƒ…å¢ƒå†…å­¦ä¹ å’Œå¾®è°ƒåœºæ™¯ä¸‹ä½¿ç”¨GPT-4oè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯å¯¹æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½å½±å“å¾®ä¹å…¶å¾®ï¼Œå´å¤§å¤§æé«˜äº†å¤„ç†æˆæœ¬ã€‚å¯¹äºå­˜åœ¨è®­ç»ƒæ•°æ®çš„æƒ…å†µï¼Œå¾®è°ƒä¼šäº§ç”Ÿæœ€é«˜çš„æ€§èƒ½ï¼Œè€Œéšç€äº§å“æè¿°æ•°é‡çš„å¢åŠ ï¼Œå¾®è°ƒçš„ä¸€æ¬¡æ€§æˆæœ¬å¾—ä»¥å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01237v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå±æ€§-å€¼å¯¹çš„å½¢å¼çš„ç»“æ„åŒ–äº§å“æ•°æ®å¯¹ç”µå­å•†åŠ¡å¹³å°è‡³å…³é‡è¦ï¼Œæ”¯æŒé¢å‘æ–¹é¢çš„äº§å“æœç´¢å’ŒåŸºäºå±æ€§çš„äº§å“æ¯”è¾ƒç­‰åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºä¾›åº”å•†æä¾›çš„äº§å“æè¿°é€šå¸¸æ˜¯ç»“æ„åŒ–çš„ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå±æ€§å€¼çš„æå–ä»¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘é‡åœºæ™¯ä¸‹çš„äº§å“å±æ€§ä»·å€¼æå–ä¸­è¡¨ç°å‡ºäº†æ½œåŠ›ã€‚æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ä¸¤ç§è‡ªæˆ‘ä¼˜åŒ–æŠ€æœ¯â€”â€”åŸºäºé”™è¯¯çš„æç¤ºé‡å†™å’Œè‡ªæˆ‘æ ¡æ­£ï¼Œç”¨äºäº§å“å±æ€§ä»·å€¼æå–ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æŠ€æœ¯åœ¨ä¸åŒåœºæ™¯ä¸‹å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ï¼Œå´æ˜¾è‘—å¢åŠ äº†å¤„ç†æˆæœ¬ã€‚åœ¨æœ‰è®­ç»ƒæ•°æ®çš„åœºæ™¯ä¸‹ï¼Œå¾®è°ƒå¯å¸¦æ¥æœ€é«˜æ€§èƒ½ï¼Œéšç€äº§å“æè¿°æ•°é‡çš„å¢åŠ ï¼Œå¾®è°ƒçš„ä¸€æ¬¡æ€§æˆæœ¬å¾—ä»¥å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“æ„åŒ–äº§å“æ•°æ®å¯¹äºç”µå­å•†åŠ¡å¹³å°è‡³å…³é‡è¦ï¼Œæ”¯æŒå¤šç§åŠŸèƒ½å¦‚é¢å‘æ–¹é¢çš„äº§å“æœç´¢å’ŒåŸºäºå±æ€§çš„äº§å“æ¯”è¾ƒã€‚</li>
<li>ä¾›åº”å•†æä¾›çš„äº§å“æè¿°å¾€å¾€æ˜¯æœªç»“æ„åŒ–çš„ï¼Œéœ€è¦è¿›è¡Œå±æ€§å€¼çš„æå–ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘é‡åœºæ™¯ä¸‹çš„äº§å“å±æ€§ä»·å€¼æå–ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>è‡ªæˆ‘ä¼˜åŒ–æŠ€æœ¯å¦‚åŸºäºé”™è¯¯çš„æç¤ºé‡å†™å’Œè‡ªæˆ‘æ ¡æ­£è¢«åº”ç”¨äºäº§å“å±æ€§ä»·å€¼æå–ä»»åŠ¡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºè‡ªæˆ‘ä¼˜åŒ–æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½å½±å“å¾®å°ï¼Œä½†å¢åŠ äº†å¤„ç†æˆæœ¬ã€‚</li>
<li>åœ¨æœ‰è®­ç»ƒæ•°æ®çš„åœºæ™¯ä¸‹ï¼Œå¾®è°ƒèƒ½å¸¦æ¥æœ€é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca257fd6ccff008cd29a3d7256d6af3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb4a830a3fdd565da5e5a0e972a787f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80261884b7bcfd3392fb548a1804e25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07935791a01f786198363bd22d8e4f83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d18c4cdd794f04b0f197b82f03e163f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c406788329ed3ccacb4163d5129b30.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ValuesRAG-Enhancing-Cultural-Alignment-Through-Retrieval-Augmented-Contextual-Learning"><a href="#ValuesRAG-Enhancing-Cultural-Alignment-Through-Retrieval-Augmented-Contextual-Learning" class="headerlink" title="ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented   Contextual Learning"></a>ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented   Contextual Learning</h2><p><strong>Authors:Wonduk Seo, Zonghao Yuan, Yi Bu</strong></p>
<p>Cultural values alignment in Large Language Models (LLMs) is a critical challenge due to their tendency to embed Western-centric biases from training data, leading to misrepresentations and fairness issues in cross-cultural contexts. Recent approaches, such as role-assignment and few-shot learning, often struggle with reliable cultural alignment as they heavily rely on pre-trained knowledge, lack scalability, and fail to capture nuanced cultural values effectively. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with in-context learning to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. Subsequently, we curated several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. ValuesRAG consistently outperforms baseline methods, both in the main experiment and in the ablation study where only the values summary was provided, highlighting ValuesRAGâ€™s potential to foster culturally aligned AI systems and enhance the inclusivity of AI-driven applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–‡åŒ–ä»·å€¼è§‚å¯¹é½æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¾€å¾€åµŒå…¥ä»è®­ç»ƒæ•°æ®ä¸­è·å¾—çš„è¥¿æ–¹ä¸­å¿ƒåè§ï¼Œå¯¼è‡´è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯¯è¡¨å¾å’Œå…¬å¹³æ€§é—®é¢˜ã€‚æœ€è¿‘çš„æ–¹æ³•ï¼Œå¦‚è§’è‰²åˆ†é…å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œå¾€å¾€éš¾ä»¥å¯é åœ°è¿›è¡Œæ–‡åŒ–å¯¹é½ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–äºé¢„è®­ç»ƒçŸ¥è¯†ï¼Œç¼ºä¹å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”æœªèƒ½æœ‰æ•ˆåœ°æ•æ‰å¾®å¦™çš„æ–‡åŒ–ä»·å€¼è§‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ValuesRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåº”ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»¥åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€é›†æˆæ–‡åŒ–å’Œäººå£ç»Ÿè®¡çŸ¥è¯†ã€‚å€ŸåŠ©ä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰æ•°æ®é›†ï¼ŒValuesRAGé¦–å…ˆä¸ºæ¯ä¸ªäººç”Ÿæˆä»·å€¼è§‚æ‘˜è¦ã€‚éšåï¼Œæˆ‘ä»¬ç²¾å¿ƒæŒ‘é€‰äº†å‡ ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„åŒºåŸŸæ•°æ®é›†ä½œä¸ºæµ‹è¯•æ•°æ®é›†ï¼Œå¹¶æ ¹æ®äººå£ç»Ÿè®¡ç‰¹å¾æ£€ç´¢ç›¸å…³çš„ä»·å€¼è§‚æ‘˜è¦ï¼Œç„¶åè¿›è¡Œé‡æ–°æ’åºæ­¥éª¤ï¼Œé€‰æ‹©å‰kä¸ªç›¸å…³æ‘˜è¦ã€‚ValuesRAGåœ¨ä¸»è¦å®éªŒå’Œä»…æä¾›ä»·å€¼è§‚æ‘˜è¦çš„æ¶ˆèç ”ç©¶ä¸­å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå‡¸æ˜¾äº†ValuesRAGåœ¨ä¿ƒè¿›æ–‡åŒ–å¯¹é½çš„AIç³»ç»Ÿå’Œæé«˜AIé©±åŠ¨åº”ç”¨åŒ…å®¹æ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01031v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡åŒ–ä»·å€¼è§‚å¯¹é½æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå› è®­ç»ƒæ•°æ®ä¸­çš„è¥¿æ–¹ä¸­å¿ƒåè§å¯¼è‡´è·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯¯è¡¨å¾å’Œå…¬å¹³é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ValuesRAGæ¡†æ¶ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒåŠ¨æ€æ•´åˆæ–‡åŒ–å’Œäººå£ç»Ÿè®¡çŸ¥è¯†æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰æ•°æ®é›†ï¼Œé¦–å…ˆä¸ºæ¯ä¸ªäººç”Ÿæˆä»·å€¼è§‚æ‘˜è¦ï¼Œç„¶ååŸºäºäººå£ç»Ÿè®¡ç‰¹å¾æ£€ç´¢ç›¸å…³çš„ä»·å€¼è§‚æ‘˜è¦å¹¶è¿›è¡Œé‡æ–°æ’åºã€‚ValuesRAGåœ¨ä¸»è¦å®éªŒå’Œä»…æä¾›ä»·å€¼è§‚æ‘˜è¦çš„æ¶ˆèç ”ç©¶ä¸­å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¿ƒè¿›æ–‡åŒ–å¯¹é½çš„AIç³»ç»Ÿå’Œæé«˜AIåº”ç”¨ç¨‹åºçš„åŒ…å®¹æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è·¨æ–‡åŒ–ç¯å¢ƒä¸­é¢ä¸´æ–‡åŒ–ä»·å€¼è§‚å¯¹é½çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸­çš„è¥¿æ–¹ä¸­å¿ƒåè§å¯èƒ½å¯¼è‡´è¯¯è¡¨å¾å’Œå…¬å¹³é—®é¢˜ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•ï¼ˆå¦‚è§’è‰²åˆ†é…å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼‰åœ¨å¯é çš„æ–‡åŒ–å¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†ï¼Œç¼ºä¹å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”ä¸èƒ½æœ‰æ•ˆåœ°æ•æ‰å¾®å¦™çš„æ–‡åŒ–ä»·å€¼è§‚ã€‚</li>
<li>ValuesRAGæ¡†æ¶é€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œèƒ½å¤ŸåŠ¨æ€æ•´åˆæ–‡åŒ–å’Œäººå£ç»Ÿè®¡çŸ¥è¯†æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>ValuesRAGåˆ©ç”¨ä¸–ç•Œä»·å€¼è§‚è°ƒæŸ¥ï¼ˆWVSï¼‰æ•°æ®é›†ä¸ºæ¯ä¸ªä¸ªä½“ç”Ÿæˆä»·å€¼è§‚æ‘˜è¦ã€‚</li>
<li>ValuesRAGä½¿ç”¨ä»£è¡¨æ€§åŒºåŸŸæ•°æ®é›†ä½œä¸ºæµ‹è¯•æ•°æ®é›†ï¼ŒåŸºäºäººå£ç»Ÿè®¡ç‰¹å¾æ£€ç´¢ç›¸å…³çš„ä»·å€¼è§‚æ‘˜è¦ã€‚</li>
<li>ValuesRAGåœ¨ä¸»è¦å®éªŒå’Œæ¶ˆèç ”ç©¶ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œè¯æ˜å…¶åœ¨ä¿ƒè¿›æ–‡åŒ–å¯¹é½çš„AIç³»ç»Ÿå’Œæé«˜AIåº”ç”¨ç¨‹åºçš„åŒ…å®¹æ€§æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1c226eb480b3dd421ac7b86b38756e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8d1d70125bc0276ef2d036e518d62e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddf39b873c5ccab77835944e3c81fbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a90d50060385815a35df2bfdedd15f74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12dfab1b163c775011e96225773f8f2b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining"><a href="#2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining" class="headerlink" title="2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining"></a>2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining</h2><p><strong>Authors:Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing</strong></p>
<p>Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\footnote{Our code are available at \url{<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook%7D%7D">https://github.com/DAMO-NLP-SG/multimodal_textbook}}</a>. </p>
<blockquote>
<p>ä¸å›¾åƒæ–‡æœ¬é…å¯¹æ•°æ®ç›¸æ¯”ï¼Œäº¤ç»‡è¯­æ–™åº“ä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿæ›´è‡ªç„¶åœ°åƒäººç±»ä¸€æ ·ç†è§£ä¸–ç•Œã€‚ç„¶è€Œï¼Œè¿™äº›ç°æœ‰çš„æ•°æ®é›†æ˜¯ä»ç½‘é¡µä¸Šçˆ¬å–çš„ï¼Œé¢ä¸´ç€çŸ¥è¯†å¯†åº¦ä½ã€å›¾åƒæ–‡æœ¬å…³ç³»æ¾æ•£ã€å›¾åƒä¹‹é—´é€»è¾‘è¿è´¯æ€§å·®ç­‰æŒ‘æˆ˜ã€‚å¦ä¸€æ–¹é¢ï¼Œäº’è”ç½‘ä¸Šæœ‰å¤§é‡æ•™å­¦è§†é¢‘ï¼ˆå¦‚åœ¨çº¿å‡ ä½•è¯¾ç¨‹ï¼‰è¢«äººç±»å¹¿æ³›ç”¨æ¥å­¦ä¹ åŸºç¡€å­¦ç§‘ï¼Œä½†è¿™äº›å®è´µèµ„æºåœ¨VLMè®­ç»ƒä¸­ä»è¢«å¿½è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é«˜è´¨é‡çš„å¤šæ¨¡å¼æ•™ç§‘ä¹¦è¯­æ–™åº“ï¼Œå…¶ä¸­åŒ…å«ç”¨äºVLMé¢„è®­ç»ƒçš„æ›´ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†ã€‚å®ƒæ”¶é›†äº†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘ï¼Œæ€»è®¡22,000èŠ‚è¯¾æ—¶ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡ºçš„åˆ†ç±»æ³•æ¥ç³»ç»Ÿåœ°æ”¶é›†æ•™å­¦è§†é¢‘ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»è§†é¢‘ä¸­é€æ­¥æå–å’Œç²¾ç‚¼è§†è§‰ï¼ˆå…³é”®å¸§ï¼‰ã€éŸ³é¢‘ï¼ˆASRï¼‰å’Œæ–‡æœ¬çŸ¥è¯†ï¼ˆOCRï¼‰ï¼Œå¹¶æŒ‰æ—¶é—´é¡ºåºç»„ç»‡æˆå›¾åƒæ–‡æœ¬äº¤ç»‡è¯­æ–™åº“ã€‚ä¸å…¶ä»–ç›¸æ¯”ï¼Œæˆ‘ä»¬è¿™ç§ä»¥è§†é¢‘ä¸ºä¸­å¿ƒæ•™ç§‘ä¹¦æä¾›äº†æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡ã€æ›´ä¸°å¯Œçš„çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚å®éªŒè¯æ˜äº†å…¶å‡ºè‰²çš„é¢„è®­ç»ƒæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ¥è¯†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡å¦‚ScienceQAå’ŒMathVistaä¸­ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬æ•™ç§‘ä¹¦ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„VLMè¡¨ç°å‡ºäº†å‡ºè‰²çš„äº¤ç»‡ä¸Šä¸‹æ–‡æ„è¯†ï¼Œåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢åœ¨å°‘é‡ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œä»»åŠ¡è§£å†³ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/DAMO-NLP-SG/multimodal_textbookä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00958v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ•™ç§‘ä¹¦è¯­æ–™åº“ã€‚è¯¥è¯­æ–™åº“é€šè¿‡æ”¶é›†è¶…è¿‡2.5å¹´çš„æ•™å­¦è§†é¢‘æ„å»ºè€Œæˆï¼ŒåŒ…å«ä¸°å¯Œçš„åŸºç¡€çŸ¥è¯†ã€‚é€šè¿‡ç³»ç»Ÿæ”¶é›†æ•™å­¦è§†é¢‘ï¼Œé€æ­¥æå–å’Œç²¾ç‚¼è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çŸ¥è¯†ï¼Œå½¢æˆåŸºäºæ—¶é—´é¡ºåºçš„å›¾åƒæ–‡æœ¬äº¤é”™è¯­æ–™åº“ã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®é›†ç›¸æ¯”ï¼Œä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„æ•™å­¦ä¹¦ç±æä¾›äº†æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡ã€æ›´ä¸°å¯Œçš„çŸ¥è¯†å’Œæ›´å¥½çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†åœ¨çŸ¥è¯†å¯†é›†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒæ€§èƒ½å‡ºè‰²ï¼Œå¦‚ScienceQAå’ŒMathVistaã€‚æ­¤å¤–ï¼Œåœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„VLMæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å‡ºè‰²çš„äº¤é”™ä¸Šä¸‹æ–‡æ„è¯†ï¼Œèƒ½å¤Ÿåœ¨å°‘æ•°æƒ…å¢ƒä¸‹åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æ¥è§£å†³é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ•™ç§‘ä¹¦è¯­æ–™åº“é€šè¿‡æ”¶é›†æ•™å­¦è§†é¢‘æ„å»ºï¼ŒåŒ…å«ä¸°å¯Œçš„çŸ¥è¯†ã€‚</li>
<li>ç³»ç»Ÿåœ°æ”¶é›†æ•™å­¦è§†é¢‘å¹¶æå–è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çŸ¥è¯†ã€‚</li>
<li>åŸºäºæ—¶é—´é¡ºåºæ„å»ºå›¾åƒæ–‡æœ¬äº¤é”™è¯­æ–™åº“ï¼Œå®ç°æ›´å¥½çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ•°æ®é›†ç›¸æ¯”ï¼Œè¯¥è¯­æ–™åº“æä¾›æ›´è¿è´¯çš„ä¸Šä¸‹æ–‡å’Œä¸°å¯Œçš„çŸ¥è¯†ã€‚</li>
<li>åœ¨çŸ¥è¯†å¯†é›†å’Œæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„é¢„è®­ç»ƒæ€§èƒ½ã€‚</li>
<li>VLMæ¨¡å‹åœ¨æ­¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„äº¤é”™ä¸Šä¸‹æ–‡æ„è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3c71d2b7aa07934e40d959baa042e4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b18a17d387835489266645fa6a0aac18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b746baa935d9c774a53f28de48e6272a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a4f45ac575c91a43a982075c7f7de11.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusion-Policies-for-Generative-Modeling-of-Spacecraft-Trajectories"><a href="#Diffusion-Policies-for-Generative-Modeling-of-Spacecraft-Trajectories" class="headerlink" title="Diffusion Policies for Generative Modeling of Spacecraft Trajectories"></a>Diffusion Policies for Generative Modeling of Spacecraft Trajectories</h2><p><strong>Authors:Julia Briden, Breanna Johnson, Richard Linares, Abhishek Cauligi</strong></p>
<p>Machine learning has demonstrated remarkable promise for solving the trajectory generation problem and in paving the way for online use of trajectory optimization for resource-constrained spacecraft. However, a key shortcoming in current machine learning-based methods for trajectory generation is that they require large datasets and even small changes to the original trajectory design requirements necessitate retraining new models to learn the parameter-to-solution mapping. In this work, we leverage compositional diffusion modeling to efficiently adapt out-of-distribution data and problem variations in a few-shot framework for 6 degree-of-freedom (DoF) powered descent trajectory generation. Unlike traditional deep learning methods that can only learn the underlying structure of one specific trajectory optimization problem, diffusion models are a powerful generative modeling framework that represents the solution as a probability density function (PDF) and this allows for the composition of PDFs encompassing a variety of trajectory design specifications and constraints. We demonstrate the capability of compositional diffusion models for inference-time 6 DoF minimum-fuel landing site selection and composable constraint representations. Using these samples as initial guesses for 6 DoF powered descent guidance enables dynamically feasible and computationally efficient trajectory generation. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åœ¨è§£å†³è½¨è¿¹ç”Ÿæˆé—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶ä¸ºèµ„æºå—é™èˆªå¤©å™¨çš„è½¨è¿¹ä¼˜åŒ–çš„åœ¨çº¿ä½¿ç”¨å¥ å®šäº†åŸºç¡€ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºæœºå™¨å­¦ä¹ çš„è½¨è¿¹ç”Ÿæˆæ–¹æ³•çš„ä¸€ä¸ªå…³é”®ç¼ºé™·æ˜¯å®ƒä»¬éœ€è¦å¤§é‡æ•°æ®é›†ï¼Œç”šè‡³å¯¹åŸå§‹è½¨è¿¹è®¾è®¡è¦æ±‚çš„å¾®å°å˜åŒ–éƒ½éœ€è¦é‡æ–°è®­ç»ƒæ–°æ¨¡å‹æ¥å­¦ä¹ å‚æ•°åˆ°è§£å†³æ–¹æ¡ˆçš„æ˜ å°„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ç»„åˆæ‰©æ•£å»ºæ¨¡ï¼Œåœ¨å°‘æ•°é•œå¤´æ¡†æ¶å†…æœ‰æ•ˆåœ°é€‚åº”è¶…å‡ºåˆ†é…çš„æ•°æ®å’Œé—®é¢˜çš„å˜åŒ–ï¼Œç”¨äºå…­è‡ªç”±åº¦ï¼ˆDoFï¼‰åŠ¨åŠ›ä¸‹é™è½¨è¿¹ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿåªèƒ½å­¦ä¹ ç‰¹å®šè½¨è¿¹ä¼˜åŒ–é—®é¢˜åº•å±‚ç»“æ„çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œå®ƒå°†è§£å†³æ–¹æ¡ˆè¡¨ç¤ºä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œè¿™å…è®¸åŒ…å«å„ç§è½¨è¿¹è®¾è®¡è§„èŒƒå’Œçº¦æŸçš„PDFçš„ç»„åˆã€‚æˆ‘ä»¬å±•ç¤ºäº†ç»„åˆæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶é—´å…­è‡ªç”±åº¦æœ€ä½ç‡ƒæ–™ç€é™†ç‚¹é€‰æ‹©å’Œå¯ç»„åˆçº¦æŸè¡¨ç¤ºæ–¹é¢çš„èƒ½åŠ›ã€‚ä½¿ç”¨è¿™äº›æ ·æœ¬ä½œä¸ºå…­è‡ªç”±åº¦åŠ¨åŠ›ä¸‹é™æŒ‡å¯¼çš„åˆå§‹çŒœæµ‹ï¼Œå¯å®ç°åŠ¨æ€å¯è¡Œå’Œè®¡ç®—é«˜æ•ˆçš„è½¨è¿¹ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00915v1">PDF</a> AIAA SCITECH 2025 Forum</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•åœ¨è§£å†³è½¨è¿¹ç”Ÿæˆé—®é¢˜ä¸Šå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºåœ¨çº¿ä½¿ç”¨è½¨è¿¹ä¼˜åŒ–èµ„æºå—é™çš„èˆªå¤©å™¨é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œå½“å‰æœºå™¨å­¦ä¹ æ–¹æ³•çš„å…³é”®çŸ­æ¿åœ¨äºéœ€è¦å¤§é‡æ•°æ®é›†ï¼Œä¸”å¯¹åŸå§‹è½¨è¿¹è®¾è®¡è¦æ±‚çš„å¾®å°å˜åŠ¨éƒ½éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹æ¥å­¦ä¹ å‚æ•°åˆ°è§£å†³æ–¹æ¡ˆçš„æ˜ å°„ã€‚æœ¬ç ”ç©¶åˆ©ç”¨ç»„åˆæ‰©æ•£å»ºæ¨¡ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æ¡†æ¶ä¸‹æœ‰æ•ˆåœ°é€‚åº”åˆ†å¸ƒå¤–çš„æ•°æ®å’Œé—®é¢˜å˜åŒ–ï¼Œç”¨äºå…­è‡ªç”±åº¦ï¼ˆDoFï¼‰åŠ¨åŠ›ä¸‹é™è½¨è¿¹ç”Ÿæˆã€‚ä¸åŒäºåªèƒ½å­¦ä¹ å•ä¸€è½¨è¿¹ä¼˜åŒ–é—®é¢˜ç»“æ„çš„ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œå°†è§£å†³æ–¹æ¡ˆè¡¨ç¤ºä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œè¿™å…è®¸åŒ…å«å„ç§è½¨è¿¹è®¾è®¡è§„èŒƒå’Œçº¦æŸçš„PDFçš„ç»„åˆã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ç»„åˆæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶é—´å…­è‡ªç”±åº¦æœ€ä½ç‡ƒæ–™ç€é™†ç‚¹é€‰æ‹©å’Œå¯ç»„åˆçº¦æŸè¡¨ç¤ºæ–¹é¢çš„èƒ½åŠ›ã€‚å°†è¿™äº›æ ·æœ¬ä½œä¸ºå…­è‡ªç”±åº¦åŠ¨åŠ›ä¸‹é™æŒ‡å¯¼çš„åˆå§‹çŒœæµ‹ï¼Œå¯å®ç°åŠ¨æ€å¯è¡Œä¸”è®¡ç®—é«˜æ•ˆçš„è½¨è¿¹ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨è§£å†³è½¨è¿¹ç”Ÿæˆé—®é¢˜ä¸Šå…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åœ¨çº¿ä½¿ç”¨è½¨è¿¹ä¼˜åŒ–èµ„æºå—é™çš„èˆªå¤©å™¨ä¸­ã€‚</li>
<li>å½“å‰æœºå™¨å­¦ä¹ æ–¹æ³•çš„ä¸€ä¸ªå…³é”®çŸ­æ¿æ˜¯å¯¹æ•°æ®çš„éœ€æ±‚é‡å¤§ï¼Œä¸”éœ€è¦é¢‘ç¹é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥é€‚åº”è½¨è¿¹è®¾è®¡è¦æ±‚çš„å˜åŠ¨ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†ç»„åˆæ‰©æ•£å»ºæ¨¡æ¥åº”å¯¹ä¸Šè¿°é—®é¢˜ï¼Œèƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬ä¸‹æœ‰æ•ˆåœ°é€‚åº”åˆ†å¸ƒå¤–çš„æ•°æ®å’Œé—®é¢˜å˜åŒ–ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å°†è§£å†³æ–¹æ¡ˆè¡¨ç¤ºä¸ºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œä¸åŒäºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•çš„å•ä¸€è½¨è¿¹ä¼˜åŒ–å­¦ä¹ ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…è®¸åŒ…å«å¤šç§è½¨è¿¹è®¾è®¡è§„èŒƒå’Œçº¦æŸçš„PDFç»„åˆï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†ç»„åˆæ‰©æ•£æ¨¡å‹åœ¨å…­è‡ªç”±åº¦æœ€ä½ç‡ƒæ–™ç€é™†ç‚¹é€‰æ‹©å’Œå¯ç»„åˆçº¦æŸè¡¨ç¤ºæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77671e12bace12163cd81e42b87043c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b9d72dca8e7ee5b724b1c47771f1eee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e119c5f6c12fd3f3909f77f9ea2f60ce.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Aligning-LLMs-with-Domain-Invariant-Reward-Models"><a href="#Aligning-LLMs-with-Domain-Invariant-Reward-Models" class="headerlink" title="Aligning LLMs with Domain Invariant Reward Models"></a>Aligning LLMs with Domain Invariant Reward Models</h2><p><strong>Authors:David Wu, Sanjiban Choudhury</strong></p>
<p>Aligning large language models (LLMs) to human preferences is challenging in domains where preference data is unavailable. We address the problem of learning reward models for such target domains by leveraging feedback collected from simpler source domains, where human preferences are easier to obtain. Our key insight is that, while domains may differ significantly, human preferences convey \emph{domain-agnostic} concepts that can be effectively captured by a reward model. We propose \method, a framework that trains domain-invariant reward models by optimizing a dual loss: a domain loss that minimizes the divergence between source and target distribution, and a source loss that optimizes preferences on the source domain. We show \method is a general approach that we evaluate and analyze across 4 distinct settings: (1) Cross-lingual transfer (accuracy: $0.621 \rightarrow 0.661$), (2) Clean-to-noisy (accuracy: $0.671 \rightarrow 0.703$), (3) Few-shot-to-full transfer (accuracy: $0.845 \rightarrow 0.920$), and (4) Simple-to-complex tasks transfer (correlation: $0.508 \rightarrow 0.556$). Our code, models and data are available at \url{<a target="_blank" rel="noopener" href="https://github.com/portal-cornell/dial%7D">https://github.com/portal-cornell/dial}</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ— æ³•è·å–åå¥½æ•°æ®çš„é¢†åŸŸä¸äººç±»çš„åå¥½å¯¹é½æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æˆ‘ä»¬è§£å†³äº†é’ˆå¯¹æ­¤ç±»ç›®æ ‡é¢†åŸŸå­¦ä¹ å¥–åŠ±æ¨¡å‹çš„é—®é¢˜ï¼Œé€šè¿‡ä»æ›´å®¹æ˜“è·å–äººç±»åå¥½çš„ç®€å•æºé¢†åŸŸæ”¶é›†åé¦ˆå¹¶åˆ©ç”¨å…¶è¿›è¡Œåˆ†æã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡åŸŸå¯èƒ½æœ‰æ‰€ä¸åŒï¼Œä½†äººç±»åå¥½ä¼ è¾¾äº†å¯ä»¥è¢«å¥–åŠ±æ¨¡å‹æœ‰æ•ˆæ•è·çš„â€œåŸŸä¸å¯çŸ¥â€æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†æ–¹æ³•ï¼ˆMethodï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä¼˜åŒ–åŒé‡æŸå¤±æ¥è®­ç»ƒåŸŸä¸å˜å¥–åŠ±æ¨¡å‹çš„æ¡†æ¶ï¼šä¸€ç§åŸŸæŸå¤±ï¼Œç”¨äºæœ€å°åŒ–æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ï¼›å¦ä¸€ç§æºæŸå¤±ï¼Œç”¨äºä¼˜åŒ–æºåŸŸä¸Šçš„åå¥½ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ–¹æ³•æ˜¯ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„ç¯å¢ƒä¸­å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°å’Œåˆ†æäº†ï¼šï¼ˆ1ï¼‰è·¨è¯­è¨€è¿ç§»ï¼ˆå‡†ç¡®åº¦ï¼šä»0.621åˆ°0.661ï¼‰ï¼›ï¼ˆ2ï¼‰ä»æ¸…æ´åˆ°å™ªå£°è¿ç§»ï¼ˆå‡†ç¡®åº¦ï¼šä»0.671åˆ°0.703ï¼‰ï¼›ï¼ˆ3ï¼‰ä»å°‘é‡åˆ°å®Œæ•´è¿ç§»ï¼ˆå‡†ç¡®åº¦ï¼šä»0.845åˆ°0.920ï¼‰ï¼›ä»¥åŠï¼ˆ4ï¼‰ä»ç®€å•åˆ°å¤æ‚ä»»åŠ¡çš„è¿ç§»ï¼ˆç›¸å…³æ€§ï¼šä»0.508åˆ°0.556ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/portal-cornell/dial%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/portal-cornell/dialä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é€šè¿‡åˆ©ç”¨ç®€å•æºåŸŸä¸­å®¹æ˜“è·å¾—çš„äººç±»åé¦ˆæ¥è§£å†³ç›®æ ‡é¢†åŸŸå¥–åŠ±æ¨¡å‹å­¦ä¹ é—®é¢˜çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•è™½ç„¶å…³æ³¨è·¨åŸŸå­¦ä¹ ï¼Œä½†è®¤ä¸ºäººç±»åå¥½ä¼ è¾¾çš„æ˜¯é¢†åŸŸé€šç”¨çš„æ¦‚å¿µï¼Œé€šè¿‡è®­ç»ƒé¢†åŸŸä¸å˜çš„å¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–åŒ…æ‹¬æºåŸŸå’Œç›®æ ‡åŸŸå·®å¼‚çš„é¢†åŸŸæŸå¤±ä»¥åŠä¼˜åŒ–æºåŸŸåå¥½çš„æºæŸå¤±ã€‚åœ¨å››ç§ä¸åŒåœºæ™¯ä¸‹çš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼ºä¹äººç±»åå¥½æ•°æ®çš„é¢†åŸŸä¸­ä¸äººç±»åå¥½å¯¹é½çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨ç®€å•æºåŸŸä¸­çš„åé¦ˆæ¥è§£å†³ç›®æ ‡é¢†åŸŸå¥–åŠ±æ¨¡å‹å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•çš„å…³é”®åœ¨äºè®¤è¯†åˆ°è™½ç„¶é¢†åŸŸå¯èƒ½æœ‰æ˜¾è‘—ä¸åŒï¼Œä½†äººç±»åå¥½ä¼ è¾¾çš„æ˜¯é¢†åŸŸé€šç”¨çš„æ¦‚å¿µã€‚</li>
<li>é€šè¿‡è®­ç»ƒé¢†åŸŸä¸å˜çš„å¥–åŠ±æ¨¡å‹æ¥ä¼˜åŒ–é¢†åŸŸæŸå¤±å’ŒæºæŸå¤±ï¼Œç¼©å°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>åœ¨è·¨è¯­è¨€è½¬ç§»ã€æ¸…æ´åˆ°å™ªå£°ã€å°‘æ‹è½¬ç§»åˆ°å…¨è½¬ç§»ä»¥åŠç®€å•ä»»åŠ¡åˆ°å¤æ‚ä»»åŠ¡è½¬ç§»ç­‰å››ä¸ªä¸åŒåœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>åœ¨æ‰€æœ‰è¯„ä¼°åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•å‡æ˜¾ç¤ºå‡ºæœ‰æ•ˆæ€§ï¼Œæé«˜äº†å‡†ç¡®æ€§æˆ–ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4a9e1e84a83508fb0ef6c3e2dd69087.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfc1f2ba9fa173e5e32f852d06bf41b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecacdc1ff042f1948b8a2f78d5cb8dae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a509ba94f6510eeee145530ba149eb0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-192d060e06c2567a0fd63a717a8e2acb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="U-GIFT-Uncertainty-Guided-Firewall-for-Toxic-Speech-in-Few-Shot-Scenario"><a href="#U-GIFT-Uncertainty-Guided-Firewall-for-Toxic-Speech-in-Few-Shot-Scenario" class="headerlink" title="U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot   Scenario"></a>U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot   Scenario</h2><p><strong>Authors:Jiaxin Song, Xinyu Wang, Yihao Wang, Yifan Tang, Ru Zhang, Jianyi Liu, Gongshen Liu</strong></p>
<p>With the widespread use of social media, user-generated content has surged on online platforms. When such content includes hateful, abusive, offensive, or cyberbullying behavior, it is classified as toxic speech, posing a significant threat to the online ecosystemâ€™s integrity and safety. While manual content moderation is still prevalent, the overwhelming volume of content and the psychological strain on human moderators underscore the need for automated toxic speech detection. Previously proposed detection methods often rely on large annotated datasets; however, acquiring such datasets is both costly and challenging in practice. To address this issue, we propose an uncertainty-guided firewall for toxic speech in few-shot scenarios, U-GIFT, that utilizes self-training to enhance detection performance even when labeled data is limited. Specifically, U-GIFT combines active learning with Bayesian Neural Networks (BNNs) to automatically identify high-quality samples from unlabeled data, prioritizing the selection of pseudo-labels with higher confidence for training based on uncertainty estimates derived from model predictions. Extensive experiments demonstrate that U-GIFT significantly outperforms competitive baselines in few-shot detection scenarios. In the 5-shot setting, it achieves a 14.92% performance improvement over the basic model. Importantly, U-GIFT is user-friendly and adaptable to various pre-trained language models (PLMs). It also exhibits robust performance in scenarios with sample imbalance and cross-domain settings, while showcasing strong generalization across various language applications. We believe that U-GIFT provides an efficient solution for few-shot toxic speech detection, offering substantial support for automated content moderation in cyberspace, thereby acting as a firewall to promote advancements in cybersecurity. </p>
<blockquote>
<p>éšç€ç¤¾äº¤åª’ä½“å¹¿æ³›ä½¿ç”¨ï¼Œç”¨æˆ·ç”Ÿæˆå†…å®¹å·²åœ¨ç½‘ä¸Šå¹³å°æ¿€å¢ã€‚å½“æ­¤ç±»å†…å®¹åŒ…å«ä»‡æ¨ã€æ»¥ç”¨ã€æ”»å‡»æ€§æˆ–ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºæ—¶ï¼Œå®ƒå°±è¢«å½’ç±»ä¸ºæœ‰æ¯’è¨€è®ºï¼Œå¯¹åœ¨çº¿ç”Ÿæ€ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œå®‰å…¨æ„æˆé‡å¤§å¨èƒã€‚è™½ç„¶æ‰‹åŠ¨å†…å®¹ç®¡ç†ä»ç„¶æ™®éå­˜åœ¨ï¼Œä½†å†…å®¹æ•°é‡è¿‡å¤šä»¥åŠäººç±»ç®¡ç†è€…æ‰¿å—çš„å¿ƒç†å‹åŠ›å‡¸æ˜¾å‡ºéœ€è¦è‡ªåŠ¨åŒ–æœ‰æ¯’è¨€è®ºæ£€æµ‹ã€‚å…ˆå‰æå‡ºçš„æ£€æµ‹æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡æ³¨é‡Šæ•°æ®é›†ï¼›ç„¶è€Œï¼Œåœ¨å®è·µä¸­è·å–æ­¤ç±»æ•°æ®é›†æ—¢æ˜‚è´µåˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹å°‘æ•°åœºæ™¯çš„æœ‰æ¯’è¨€è®ºæå‡ºäº†ä¸€ç§ä¸ç¡®å®šæ€§å¼•å¯¼é˜²ç«å¢™ï¼Œåä¸ºU-GIFTã€‚å®ƒåˆ©ç”¨è‡ªè®­ç»ƒæé«˜æ£€æµ‹æ€§èƒ½ï¼Œå³ä½¿åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å‘æŒ¥ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼ŒU-GIFTç»“åˆä¸»åŠ¨å­¦ä¹ ä¸è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNsï¼‰ï¼Œè‡ªåŠ¨ä»æœªæ ‡è®°çš„æ•°æ®ä¸­è¯†åˆ«é«˜è´¨é‡æ ·æœ¬ï¼Œä¼˜å…ˆé€‰æ‹©ä¸æ¨¡å‹é¢„æµ‹å¾—å‡ºä¸ç¡®å®šæ€§ä¼°è®¡ç½®ä¿¡åº¦è¾ƒé«˜çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘æ•°åœºæ™¯æ£€æµ‹ä¸­ï¼ŒU-GIFTæ˜¾è‘—ä¼˜äºç«äº‰åŸºçº¿ã€‚åœ¨5æ¬¡å°„å‡»è®¾å®šä¸­ï¼Œå®ƒæ¯”åŸºæœ¬æ¨¡å‹é«˜å‡º14.92%çš„æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼ŒU-GIFTç”¨æˆ·å‹å¥½ä¸”é€‚åº”å„ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ã€‚å®ƒåœ¨æ ·æœ¬ä¸å¹³è¡¡å’Œè·¨åŸŸè®¾ç½®ç­‰åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ï¼Œå¹¶ä¸”åœ¨å„ç§è¯­è¨€åº”ç”¨ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡U-GIFTä¸ºå°‘æ•°æœ‰æ¯’è¨€è®ºæ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç½‘ç»œç©ºé—´çš„è‡ªåŠ¨åŒ–å†…å®¹ç®¡ç†æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œä»è€Œå……å½“é˜²ç«å¢™ä¿ƒè¿›ç½‘ç»œå®‰å…¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00907v1">PDF</a> 16 pages, 6 figures and 10 tables. Comments are welcome</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¤¾äº¤åª’ä½“ç”¨æˆ·ç”Ÿæˆå†…å®¹çš„æ¿€å¢å¸¦æ¥äº†ç½‘ç»œç”Ÿæ€ä¸­çš„æœ‰æ¯’è¨€è®ºé—®é¢˜ã€‚è¿™äº›æœ‰æ¯’è¨€è®ºåŒ…æ‹¬ä»‡æ¨ã€æ»¥ç”¨ã€æ”»å‡»æˆ–ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºï¼Œå¯¹åœ¨çº¿ç”Ÿæ€ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œå®‰å…¨æ€§æ„æˆä¸¥é‡å¨èƒã€‚ä¸ºäº†è§£å†³æ ‡æ³¨æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸ç¡®å®šæ€§çš„é˜²ç«å¢™ç³»ç»ŸU-GIFTæ¥è§£å†³æ•°æ®å—é™åœºæ™¯ä¸‹çš„æœ‰æ¯’è¨€è®ºæ£€æµ‹é—®é¢˜ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†ä¸»åŠ¨å­¦ä¹ ã€è´å¶æ–¯ç¥ç»ç½‘ç»œè¿›è¡Œè‡ªè®­ç»ƒï¼Œèƒ½å¤Ÿä»æ— æ ‡ç­¾æ•°æ®ä¸­è‡ªåŠ¨è¯†åˆ«é«˜è´¨é‡æ ·æœ¬ï¼Œå¹¶æ ¹æ®æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¥ä¼˜å…ˆé€‰å–é«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ ·æœ¬é‡è¾ƒå°çš„æƒ…å†µä¸‹ï¼ŒU-GIFTç³»ç»Ÿæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚ç›¸è¾ƒäºåŸºæœ¬æ¨¡å‹ï¼Œå…¶åœ¨5æ ·æœ¬åœºæ™¯ä¸­å®ç°äº†é«˜è¾¾14.92%çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒU-GIFTå…·æœ‰è‰¯å¥½çš„ç”¨æˆ·å‹å¥½æ€§å’Œé€‚åº”æ€§ï¼Œé€‚ç”¨äºå¤šç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ç³»ç»Ÿåœ¨æ ·æœ¬ä¸å‡è¡¡å’Œè·¨åŸŸè®¾ç½®ä¸‹å±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šç§è¯­è¨€åº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡U-GIFTèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒç½‘ç»œå®‰å…¨é¢†åŸŸçš„è‡ªåŠ¨å†…å®¹å®¡æ ¸å’Œç½‘ç»œå®‰å…¨é˜²æŠ¤çš„æ¨åŠ¨å·¥ä½œã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ç”¨æˆ·ç”Ÿæˆå†…å®¹çš„æ¿€å¢å¼•å‘äº†ç½‘ç»œä¸­çš„æœ‰æ¯’è¨€è®ºé—®é¢˜ã€‚</li>
<li>æœ‰æ¯’è¨€è®ºå¯¹åœ¨çº¿ç”Ÿæ€ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œå®‰å…¨æ€§æ„æˆå¨èƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„é˜²ç«å¢™ç³»ç»ŸU-GIFTè§£å†³è¯¥é—®é¢˜ã€‚åœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹èƒ½å¤Ÿæ˜¾è‘—æå‡æœ‰æ¯’è¨€è®ºæ£€æµ‹æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6b8cc808e21db9d8bf442fbfac9001d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6b582ce0516aae506d2f3b24ef77d16.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation"><a href="#FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation" class="headerlink" title="FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation"></a>FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation</h2><p><strong>Authors:Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian MÃ¶ller, Vera Schmitt</strong></p>
<p>Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCFâ€™s core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals. </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œåäº‹å®ä¾‹å­è¢«å¹¿æ³›åº”ç”¨äºæå‡æ¨¡å‹ä»·å€¼çš„æ•°æ®ï¼Œå¹¶åœ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­ç”¨äºç†è§£æ¨¡å‹è¡Œä¸ºã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†è‡ªåŠ¨ç”Ÿæˆåäº‹å®ä¾‹å­ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†ZeroCFï¼Œè¿™æ˜¯ä¸€ç§å¿ å®çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å¾å½’å› æ–¹æ³•å¾—å‡ºçš„é‡è¦å•è¯ï¼Œåœ¨æ— æ ·æœ¬è®¾ç½®ä¸‹ç”Ÿæˆåäº‹å®ä¾‹å­ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶FitCFï¼Œå®ƒé€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯è¿›ä¸€æ­¥éªŒè¯äº†ä¸Šè¿°åäº‹å®ï¼Œç„¶åå°†å…¶ä½œä¸ºæ¼”ç¤ºç”¨äºå°‘æ ·æœ¬æç¤ºï¼Œä¼˜äºä¸¤ç§æœ€æ–°æŠ€æœ¯æ°´å¹³çš„åŸºçº¿ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®å®šäº†FitCFæ¯ä¸ªæ ¸å¿ƒç»„ä»¶åœ¨æé«˜åäº‹å®è´¨é‡ï¼ˆé€šè¿‡ç¿»è½¬ç‡ã€å›°æƒ‘åº¦å’Œç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œè¯„ä¼°ï¼‰æ–¹é¢çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LIMEå’Œé›†æˆæ¢¯åº¦ä½œä¸ºFitCFçš„éª¨å¹²å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°æ¼”ç¤ºçš„æ•°é‡å¯¹æ€§èƒ½çš„å½±å“æœ€å¤§ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ç‰¹å¾å½’å› åˆ†æ•°çš„å¿ å®æ€§ä¸ç”Ÿæˆçš„åäº‹å®è´¨é‡ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„å…³è”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00777v1">PDF</a> In submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­å¹¿æ³›åº”ç”¨çš„åäº‹å®å®ä¾‹ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆåäº‹å®å®ä¾‹æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ZeroCFæ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å¾å½’å±æ–¹æ³•å¾—å‡ºçš„é‡è¦è¯æ±‡åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹ç”Ÿæˆåäº‹å®å®ä¾‹ã€‚è¿›ä¸€æ­¥ï¼Œæå‡ºäº†FitCFæ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯æ¥éªŒè¯ä¸Šè¿°åäº‹å®ï¼Œå¹¶å°†å…¶ä½œä¸ºå°‘æ ·æœ¬æç¤ºæ’å…¥ï¼Œä¼˜äºä¸¤ç§å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œç¡®å®šäº†FitCFæ ¸å¿ƒç»„ä»¶åœ¨æé«˜åäº‹å®è´¨é‡æ–¹é¢çš„ä½œç”¨ï¼Œå¹¶é€šè¿‡ç¿»è½¬ç‡ã€å›°æƒ‘åº¦å’Œç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œè¯„ä¼°ã€‚åŒæ—¶ï¼Œå±•ç¤ºäº†LIMEå’Œé›†æˆæ¢¯åº¦ä½œä¸ºFitCFçš„éª¨å¹²å½’å±æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°æ¼”ç¤ºæ•°é‡å¯¹æ€§èƒ½çš„å½±å“æœ€å¤§ã€‚æœ€åï¼Œæ­ç¤ºäº†ç‰¹å¾å½’å±åˆ†æ•°å¿ å®æ€§ä¸ç”Ÿæˆåäº‹å®è´¨é‡ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Counterfactual examples are valuable in NLP and XAI.<br>2.è‡ªåŠ¨ç”Ÿæˆåäº‹å®å®ä¾‹å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¯´æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>ZeroCFæ–¹æ³•åˆ©ç”¨ç‰¹å¾å½’å±æ–¹æ³•å¾—å‡ºçš„é‡è¦è¯æ±‡åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹ç”Ÿæˆåäº‹å®å®ä¾‹ã€‚</li>
<li>FitCFæ¡†æ¶é€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯æ¥éªŒè¯åäº‹å®ï¼Œå¹¶å°†å…¶ç”¨äºå°‘æ ·æœ¬æç¤ºã€‚</li>
<li>FitCFæ¡†æ¶ä¼˜äºä¸¤ç§å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶ç¡®å®šäº†FitCFæ ¸å¿ƒç»„ä»¶å¯¹æé«˜åäº‹å®è´¨é‡çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca65d4511693a2436e8d401dcc19e61a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a27810c25050d378224316b8483e4e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a01cf4a6a482641125a7684a7732264.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Foreground-Covering-Prototype-Generation-and-Matching-for-SAM-Aided-Few-Shot-Segmentation"><a href="#Foreground-Covering-Prototype-Generation-and-Matching-for-SAM-Aided-Few-Shot-Segmentation" class="headerlink" title="Foreground-Covering Prototype Generation and Matching for SAM-Aided   Few-Shot Segmentation"></a>Foreground-Covering Prototype Generation and Matching for SAM-Aided   Few-Shot Segmentation</h2><p><strong>Authors:Suho Park, SuBeen Lee, Hyun Seok Seong, Jaejoon Yoo, Jae-Pil Heo</strong></p>
<p>We propose Foreground-Covering Prototype Generation and Matching to resolve Few-Shot Segmentation (FSS), which aims to segment target regions in unlabeled query images based on labeled support images. Unlike previous research, which typically estimates target regions in the query using support prototypes and query pixels, we utilize the relationship between support and query prototypes. To achieve this, we utilize two complementary features: SAM Image Encoder features for pixel aggregation and ResNet features for class consistency. Specifically, we construct support and query prototypes with SAM features and distinguish query prototypes of target regions based on ResNet features. For the query prototype construction, we begin by roughly guiding foreground regions within SAM features using the conventional pseudo-mask, then employ iterative cross-attention to aggregate foreground features into learnable tokens. Here, we discover that the cross-attention weights can effectively alternate the conventional pseudo-mask. Therefore, we use the attention-based pseudo-mask to guide ResNet features to focus on the foreground, then infuse the guided ResNet feature into the learnable tokens to generate class-consistent query prototypes. The generation of the support prototype is conducted symmetrically to that of the query one, with the pseudo-mask replaced by the ground-truth mask. Finally, we compare these query prototypes with support ones to generate prompts, which subsequently produce object masks through the SAM Mask Decoder. Our state-of-the-art performances on various datasets validate the effectiveness of the proposed method for FSS. Our official code is available at <a target="_blank" rel="noopener" href="https://github.com/SuhoPark0706/FCP">https://github.com/SuhoPark0706/FCP</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºå‰æ™¯è¦†ç›–åŸå‹ç”Ÿæˆä¸åŒ¹é…æ¥è§£å†³å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰é—®é¢˜ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨æ— æ ‡ç­¾çš„æŸ¥è¯¢å›¾åƒä¸­åŸºäºæœ‰æ ‡ç­¾çš„æ”¯æŒå›¾åƒå¯¹ç›®æ ‡åŒºåŸŸè¿›è¡Œåˆ†å‰²ã€‚ä¸é€šå¸¸ä½¿ç”¨æ”¯æŒåŸå‹å’ŒæŸ¥è¯¢åƒç´ æ¥ä¼°è®¡æŸ¥è¯¢ä¸­çš„ç›®æ ‡åŒºåŸŸçš„å…ˆå‰ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬åˆ©ç”¨æ”¯æŒåŸå‹å’ŒæŸ¥è¯¢åŸå‹ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªäº’è¡¥çš„ç‰¹æ€§ï¼šSAMå›¾åƒç¼–ç å™¨çš„åƒç´ èšåˆç‰¹å¾å’ŒResNetçš„ç‰¹æ€§ä¿æŒç±»ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨SAMç‰¹å¾æ„å»ºæ”¯æŒå’ŒæŸ¥è¯¢åŸå‹ï¼Œå¹¶æ ¹æ®ResNetç‰¹å¾åŒºåˆ†æŸ¥è¯¢ç›®æ ‡åŒºåŸŸçš„åŸå‹ã€‚å¯¹äºæŸ¥è¯¢åŸå‹çš„æ„å»ºï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¼ ç»Ÿçš„ä¼ªæ©ç åœ¨SAMç‰¹å¾ä¸­å¤§è‡´æŒ‡å¯¼å‰æ™¯åŒºåŸŸï¼Œç„¶åé‡‡ç”¨è¿­ä»£äº¤å‰æ³¨æ„åŠ›å°†å‰æ™¯ç‰¹å¾èšé›†åˆ°å¯å­¦ä¹ çš„ä»¤ç‰Œä¸­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘ç°äº¤å‰æ³¨æ„åŠ›æƒé‡å¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£ä¼ ç»Ÿçš„ä¼ªæ©ç ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ä¼ªæ©ç æ¥æŒ‡å¯¼ResNetç‰¹å¾ä¸“æ³¨äºå‰æ™¯ï¼Œç„¶åå°†å¼•å¯¼çš„ResNetç‰¹å¾èåˆåˆ°å¯å­¦ä¹ çš„ä»¤ç‰Œä¸­ï¼Œä»¥ç”Ÿæˆç±»ä¸€è‡´çš„æŸ¥è¯¢åŸå‹ã€‚æ”¯æŒåŸå‹çš„ç”Ÿæˆæ–¹å¼ä¸æŸ¥è¯¢åŸå‹çš„ç”Ÿæˆæ–¹å¼å¯¹ç§°ï¼Œå…¶ä¸­ä¼ªæ©ç è¢«çœŸå®æ©ç æ‰€æ›¿ä»£ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿™äº›æŸ¥è¯¢åŸå‹ä¸æ”¯æŒåŸå‹è¿›è¡Œæ¯”è¾ƒä»¥ç”Ÿæˆæç¤ºï¼Œç„¶åé€šè¿‡SAM Maskè§£ç å™¨äº§ç”Ÿå¯¹è±¡æ©ç ã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†ä¸Šçš„æœ€æ–°æ€§èƒ½è¡¨ç°éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•åœ¨å°æ ·æœ¬åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®˜æ–¹ä»£ç å¯åœ¨XXXä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00752v1">PDF</a> Association for the Advancement of Artificial Intelligence (AAAI)   2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è§£å†³Few-Shot Segmentationï¼ˆFSSï¼‰é—®é¢˜çš„æ–¹æ³•ï¼Œåä¸ºå‰æ™¯è¦†ç›–åŸå‹ç”Ÿæˆä¸åŒ¹é…ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åŸºäºæ ‡è®°çš„æ”¯æŒå›¾åƒå¯¹æœªæ ‡è®°æŸ¥è¯¢å›¾åƒä¸­çš„ç›®æ ‡åŒºåŸŸè¿›è¡Œåˆ†å‰²ã€‚é€šè¿‡åˆ©ç”¨æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„åŸå‹å…³ç³»ï¼Œç»“åˆSAMå›¾åƒç¼–ç å™¨çš„åƒç´ èšåˆç‰¹å¾å’ŒResNetæ¨¡å‹çš„ç±»åˆ«ä¸€è‡´æ€§ç‰¹å¾ï¼Œæ„å»ºæ”¯æŒåŸå‹å’ŒæŸ¥è¯¢åŸå‹ã€‚åˆ©ç”¨åŸºäºæ³¨æ„åŠ›çš„ä¼ªæ©ç æŒ‡å¯¼ResNetç‰¹å¾èšç„¦äºå‰æ™¯ï¼Œç”Ÿæˆç±»åˆ«ä¸€è‡´çš„æŸ¥è¯¢åŸå‹ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒéªŒè¯äº†å…¶åœ¨FSSé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å‰æ™¯è¦†ç›–åŸå‹ç”Ÿæˆä¸åŒ¹é…æ–¹æ³•è§£å†³Few-Shot Segmentationï¼ˆFSSï¼‰é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„åŸå‹å…³ç³»ã€‚</li>
<li>ç»“åˆSAMå›¾åƒç¼–ç å™¨çš„åƒç´ èšåˆç‰¹å¾å’ŒResNetæ¨¡å‹çš„ç±»åˆ«ä¸€è‡´æ€§ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„ä¼ªæ©ç å¼•å¯¼ResNetç‰¹å¾èšç„¦äºå‰æ™¯ã€‚</li>
<li>ç”Ÿæˆç±»åˆ«ä¸€è‡´çš„æŸ¥è¯¢åŸå‹å’Œæ”¯æŒåŸå‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d945c746c7b64d781c6c0caa96afc1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0739c0db8bfa28a7cd8bdbc400742f84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7e96b08916b8bd1329536b71cd92ad8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34da2b4af9884b9d4881d80d79aae15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41e46ec5ba61fc4e7f34ccd2558ce4b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Reasoning-with-Multi-Path-Collaborative-Reactive-and-Reflection-agents"><a href="#Enhancing-LLM-Reasoning-with-Multi-Path-Collaborative-Reactive-and-Reflection-agents" class="headerlink" title="Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and   Reflection agents"></a>Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and   Reflection agents</h2><p><strong>Authors:Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, Huimin Ma</strong></p>
<p>Agents have demonstrated their potential in scientific reasoning tasks through large language models. However, they often face challenges such as insufficient accuracy and degeneration of thought when handling complex reasoning tasks, which impede their performance. To overcome these issues, we propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP) Framework, aimed at enhancing the reasoning capabilities of LLMs. Our approach improves scientific reasoning accuracy by employing a multi-path reasoning mechanism where each path consists of a reactive agent and a reflection agent that collaborate to prevent degeneration of thought inherent in single-agent reliance. Additionally, the RR-MP framework does not require additional training; it utilizes multiple dialogue instances for each reasoning path and a separate summarizer to consolidate insights from all paths. This design integrates diverse perspectives and strengthens reasoning across each path. We conducted zero-shot and few-shot evaluations on tasks involving moral scenarios, college-level physics, and mathematics. Experimental results demonstrate that our method outperforms baseline approaches, highlighting the effectiveness and advantages of the RR-MP framework in managing complex scientific reasoning tasks. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åœ¨ç§‘ç ”æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œè¿™ä¸»è¦å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬å¸¸é¢ä¸´è¯¸å¦‚å‡†ç¡®åº¦ä¸è¶³å’Œæ€ç»´é€€åŒ–ç­‰æŒ‘æˆ˜ï¼Œåˆ¶çº¦äº†å…¶æ€§èƒ½è¡¨ç°ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰å¤šè·¯æ¨ç†çš„ååº”ä¸åæ€ä»£ç†ï¼ˆRR-MPï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤šè·¯æ¨ç†æœºåˆ¶æ¥æå‡ç§‘å­¦æ¨ç†çš„å‡†ç¡®åº¦ï¼Œæ¯æ¡è·¯å¾„åŒ…å«ä¸€åååº”ä»£ç†å’Œä¸€ååæ€ä»£ç†ï¼Œä»–ä»¬ååŒå·¥ä½œï¼Œé˜²æ­¢å•ä¸€ä»£ç†ä¾èµ–å¯¼è‡´çš„æ€ç»´é€€åŒ–ã€‚æ­¤å¤–ï¼ŒRR-MPæ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œå®ƒåˆ©ç”¨æ¯ä¸ªæ¨ç†è·¯å¾„çš„å¤šä¸ªå¯¹è¯å®ä¾‹å’Œä¸€ä¸ªå•ç‹¬çš„æ‘˜è¦å™¨æ¥æ•´åˆæ‰€æœ‰è·¯å¾„çš„è§è§£ã€‚è¿™ç§è®¾è®¡èåˆäº†ä¸åŒçš„è§‚ç‚¹ï¼ŒåŠ å¼ºäº†æ¯æ¡è·¯å¾„çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹æ¶‰åŠé“å¾·æƒ…æ™¯ã€å¤§å­¦ç‰©ç†å’Œæ•°å­¦çš„ä»»åŠ¡è¿›è¡Œäº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œçªæ˜¾äº†RR-MPæ¡†æ¶åœ¨å¤„ç†å¤æ‚ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00430v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ååº”ä¸åæ€å¤šè·¯å¾„æ¨ç†æ¡†æ¶ï¼ˆRR-MPï¼‰æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè§£å†³å…¶åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´çš„å‡†ç¡®æ€§ä¸è¶³å’Œæ€ç»´é€€åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡é‡‡ç”¨å¤šè·¯å¾„æ¨ç†æœºåˆ¶ï¼Œæ¯ä¸ªè·¯å¾„åŒ…å«ååº”å’Œåæ€ä¸¤ä¸ªä»£ç†ï¼Œåä½œé˜²æ­¢å•ä¸€ä»£ç†ä¾èµ–å¯¼è‡´çš„æ€ç»´é€€åŒ–ã€‚RR-MPæ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œåˆ©ç”¨æ¯ä¸ªæ¨ç†è·¯å¾„çš„å¤šä¸ªå¯¹è¯å®ä¾‹å’Œå•ç‹¬çš„æ€»ç»“è€…æ¥æ•´åˆä¸åŒè§‚ç‚¹ï¼ŒåŠ å¼ºå„è·¯å¾„çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨é“å¾·åœºæ™¯ã€å¤§å­¦ç‰©ç†å’Œæ•°å­¦ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååº”ä¸åæ€å¤šè·¯å¾„æ¨ç†æ¡†æ¶ï¼ˆRR-MPï¼‰æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ç§‘å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>RR-MPæ¡†æ¶é‡‡ç”¨å¤šè·¯å¾„æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡ååº”å’Œåæ€ä»£ç†çš„åä½œï¼Œè§£å†³æ€ç»´é€€åŒ–é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ•´åˆå¤šä¸ªå¯¹è¯å®ä¾‹å’Œä¸åŒè§‚ç‚¹ï¼Œæé«˜ç§‘å­¦æ¨ç†çš„å‡†ç¡®æ€§ã€‚</li>
<li>RR-MPæ¡†æ¶è®¾è®¡ç‹¬ç‰¹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œé€šè¿‡å•ç‹¬çš„æ€»ç»“è€…æ¨¡å—æ¥å·©å›ºå„è·¯å¾„çš„è§è§£ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRR-MPæ¡†æ¶åœ¨é“å¾·åœºæ™¯ã€å¤§å­¦ç‰©ç†å’Œæ•°å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>RR-MPæ¡†æ¶èƒ½å¤Ÿç®¡ç†å¤æ‚çš„ç§‘å­¦æ¨ç†ä»»åŠ¡ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cabff3ef390637019e649bbb455e8c91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9feef85e267655219ff0aab8e9ad996d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27c975eca623b571c1b785f18c182a50.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAM-Aware-Graph-Prompt-Reasoning-Network-for-Cross-Domain-Few-Shot-Segmentation"><a href="#SAM-Aware-Graph-Prompt-Reasoning-Network-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot   Segmentation"></a>SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Shi-Feng Peng, Guolei Sun, Yong Li, Hongsong Wang, Guo-Sen Xie</strong></p>
<p>The primary challenge of cross-domain few-shot segmentation (CD-FSS) is the domain disparity between the training and inference phases, which can exist in either the input data or the target classes. Previous models struggle to learn feature representations that generalize to various unknown domains from limited training domain samples. In contrast, the large-scale visual model SAM, pre-trained on tens of millions of images from various domains and classes, possesses excellent generalizability. In this work, we propose a SAM-aware graph prompt reasoning network (GPRN) that fully leverages SAM to guide CD-FSS feature representation learning and improve prediction accuracy. Specifically, we propose a SAM-aware prompt initialization module (SPI) to transform the masks generated by SAM into visual prompts enriched with high-level semantic information. Since SAM tends to divide an object into many sub-regions, this may lead to visual prompts representing the same semantic object having inconsistent or fragmented features. We further propose a graph prompt reasoning (GPR) module that constructs a graph among visual prompts to reason about their interrelationships and enable each visual prompt to aggregate information from similar prompts, thus achieving global semantic consistency. Subsequently, each visual prompt embeds its semantic information into the corresponding mask region to assist in feature representation learning. To refine the segmentation mask during testing, we also design a non-parameter adaptive point selection module (APS) to select representative point prompts from query predictions and feed them back to SAM to refine inaccurate segmentation results. Experiments on four standard CD-FSS datasets demonstrate that our method establishes new state-of-the-art results. Code: <a target="_blank" rel="noopener" href="https://github.com/CVL-hub/GPRN">https://github.com/CVL-hub/GPRN</a>. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºè®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„åŸŸå·®å¼‚ï¼Œè¿™ç§å·®å¼‚å¯èƒ½å­˜åœ¨äºè¾“å…¥æ•°æ®æˆ–ç›®æ ‡ç±»åˆ«ä¸­ã€‚ä¹‹å‰çš„æ¨¡å‹å¾ˆéš¾ä»æœ‰é™çš„è®­ç»ƒåŸŸæ ·æœ¬ä¸­å­¦ä¹ èƒ½å¤Ÿæ¨å¹¿åˆ°å„ç§æœªçŸ¥åŸŸçš„ç‰¹å¾è¡¨ç¤ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è§†è§‰æ¨¡å‹SAMåœ¨æ¥è‡ªå„ä¸ªåŸŸå’Œç±»åˆ«çš„æ•°åƒä¸‡å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºSAMçš„å›¾å½¢æç¤ºæ¨ç†ç½‘ç»œï¼ˆGPRNï¼‰ï¼Œè¯¥ç½‘ç»œå……åˆ†åˆ©ç”¨SAMæ¥æŒ‡å¯¼CD-FSSç‰¹å¾è¡¨ç¤ºå­¦ä¹ å¹¶æé«˜é¢„æµ‹ç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªSAMæ„ŸçŸ¥æç¤ºåˆå§‹åŒ–æ¨¡å—ï¼ˆSPIï¼‰ï¼Œå°†SAMç”Ÿæˆçš„æ©ç è½¬æ¢ä¸ºåŒ…å«é«˜çº§è¯­ä¹‰ä¿¡æ¯çš„è§†è§‰æç¤ºã€‚ç”±äºSAMå€¾å‘äºå°†ä¸€ä¸ªå¯¹è±¡åˆ†æˆå¤šä¸ªå­åŒºåŸŸï¼Œè¿™å¯èƒ½å¯¼è‡´è¡¨ç¤ºç›¸åŒè¯­ä¹‰å¯¹è±¡çš„è§†è§‰æç¤ºå…·æœ‰ä¸ä¸€è‡´æˆ–ç¢ç‰‡åŒ–çš„ç‰¹å¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªå›¾å½¢æç¤ºæ¨ç†ï¼ˆGPRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è§†è§‰æç¤ºä¹‹é—´æ„å»ºå›¾å½¢ï¼Œæ¨ç†å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å…è®¸æ¯ä¸ªè§†è§‰æç¤ºä»ç±»ä¼¼æç¤ºä¸­èšåˆä¿¡æ¯ï¼Œä»è€Œå®ç°å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚éšåï¼Œæ¯ä¸ªè§†è§‰æç¤ºå°†å…¶è¯­ä¹‰ä¿¡æ¯åµŒå…¥åˆ°ç›¸åº”çš„æ©ç åŒºåŸŸä¸­ï¼Œä»¥è¾…åŠ©ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚ä¸ºäº†åœ¨æµ‹è¯•æœŸé—´ç»†åŒ–åˆ†å‰²æ©ç ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªéå‚æ•°è‡ªé€‚åº”ç‚¹é€‰æ‹©æ¨¡å—ï¼ˆAPSï¼‰ï¼Œä»æŸ¥è¯¢é¢„æµ‹ä¸­é€‰æ‹©ä»£è¡¨æ€§çš„ç‚¹æç¤ºå¹¶åé¦ˆåˆ°SAMï¼Œä»¥ä¼˜åŒ–ä¸å‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨å››ä¸ªæ ‡å‡†çš„CD-FSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ–°çš„æœ€æ–°æ°´å¹³ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/CVL-hub/GPRN">https://github.com/CVL-hub/GPRN</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00303v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong><br>     è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºè®­ç»ƒå’Œæ¨ç†é˜¶æ®µå­˜åœ¨çš„åŸŸå·®å¼‚é—®é¢˜ï¼Œå¯èƒ½å‡ºç°åœ¨è¾“å…¥æ•°æ®æˆ–ç›®æ ‡ç±»åˆ«ä¸Šã€‚ä»¥å‰çš„æ–¹æ³•éš¾ä»¥ä»æœ‰é™çš„è®­ç»ƒåŸŸæ ·æœ¬ä¸­å­¦ä¹ ç‰¹å¾è¡¨ç¤ºæ¥æ¨å¹¿åˆ°å¤šä¸ªæœªçŸ¥åŸŸã€‚æœ¬æ–‡æå‡ºäº†åŸºäºå¤§è§„æ¨¡è§†è§‰æ¨¡å‹SAMçš„GPRNç½‘ç»œï¼Œåˆ©ç”¨SAMæŒ‡å¯¼CD-FSSçš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ï¼Œæé«˜é¢„æµ‹ç²¾åº¦ã€‚é€šè¿‡SAMæ„ŸçŸ¥çš„æç¤ºåˆå§‹åŒ–æ¨¡å—ï¼ˆSPIï¼‰è½¬æ¢SAMç”Ÿæˆçš„æ©è†œä¸ºå¯Œå«é«˜çº§è¯­ä¹‰ä¿¡æ¯çš„è§†è§‰æç¤ºã€‚ä¸ºå¤„ç†SAMå€¾å‘äºå°†ç‰©ä½“åˆ†ä¸ºå¤šä¸ªå­åŒºåŸŸé€ æˆçš„é—®é¢˜ï¼Œæå‡ºå›¾æç¤ºæ¨ç†ï¼ˆGPRï¼‰æ¨¡å—ï¼Œæ„å»ºè§†è§‰æç¤ºä¹‹é—´çš„å›¾è¿›è¡Œå…³ç³»æ¨ç†ï¼Œä½¿æ¯ä¸ªè§†è§‰æç¤ºèƒ½èšåˆç›¸ä¼¼æç¤ºçš„ä¿¡æ¯ï¼Œå®ç°å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†éå‚æ•°è‡ªé€‚åº”ç‚¹é€‰æ‹©æ¨¡å—ï¼ˆAPSï¼‰æ¥ä¼˜åŒ–æµ‹è¯•é˜¶æ®µçš„åˆ†å‰²æ©è†œã€‚åœ¨å››ä¸ªæ ‡å‡†CD-FSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSSé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯è®­ç»ƒå’Œæ¨ç†é˜¶æ®µçš„åŸŸå·®å¼‚é—®é¢˜ã€‚</li>
<li>ä¹‹å‰çš„æ–¹æ³•éš¾ä»¥ä»æœ‰é™çš„è®­ç»ƒåŸŸæ ·æœ¬ä¸­å­¦ä¹ é€‚åº”å„ç§æœªçŸ¥åŸŸçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†åŸºäºå¤§è§„æ¨¡è§†è§‰æ¨¡å‹SAMçš„GPRNç½‘ç»œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>SAMæ„ŸçŸ¥çš„æç¤ºåˆå§‹åŒ–æ¨¡å—ï¼ˆSPIï¼‰è½¬æ¢æ©è†œä¸ºå¯Œå«é«˜çº§è¯­ä¹‰ä¿¡æ¯çš„è§†è§‰æç¤ºã€‚</li>
<li>å›¾æç¤ºæ¨ç†ï¼ˆGPRï¼‰æ¨¡å—ç”¨äºå¤„ç†è§†è§‰æç¤ºä¹‹é—´çš„ä¸ä¸€è‡´æ€§å’Œç¢ç‰‡åŒ–é—®é¢˜ï¼Œå®ç°å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>éå‚æ•°è‡ªé€‚åº”ç‚¹é€‰æ‹©æ¨¡å—ï¼ˆAPSï¼‰ç”¨äºä¼˜åŒ–æµ‹è¯•é˜¶æ®µçš„åˆ†å‰²æ©è†œã€‚</li>
<li>åœ¨å››ä¸ªæ ‡å‡†CD-FSSæ•°æ®é›†ä¸Šçš„å®éªŒå–å¾—äº†æœ€æ–°æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70ad2ee418244e3e6c25cf12fca8b435.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8969bbdfe079fbb2ae5d597c4ca416c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0b392d1392bcd768f4f35a34e45f27f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Learning-in-Multiple-Spaces-Few-Shot-Network-Attack-Detection-with-Metric-Fused-Prototypical-Networks"><a href="#Learning-in-Multiple-Spaces-Few-Shot-Network-Attack-Detection-with-Metric-Fused-Prototypical-Networks" class="headerlink" title="Learning in Multiple Spaces: Few-Shot Network Attack Detection with   Metric-Fused Prototypical Networks"></a>Learning in Multiple Spaces: Few-Shot Network Attack Detection with   Metric-Fused Prototypical Networks</h2><p><strong>Authors:Fernando Martinez-Lopez, Lesther Santana, Mohamed Rahouti</strong></p>
<p>Network intrusion detection systems face significant challenges in identifying emerging attack patterns, especially when limited data samples are available. To address this, we propose a novel Multi-Space Prototypical Learning (MSPL) framework tailored for few-shot attack detection. The framework operates across multiple metric spaces-Euclidean, Cosine, Chebyshev, and Wasserstein distances-integrated through a constrained weighting scheme to enhance embedding robustness and improve pattern recognition. By leveraging Polyak-averaged prototype generation, the framework stabilizes the learning process and effectively adapts to rare and zero-day attacks. Additionally, an episodic training paradigm ensures balanced representation across diverse attack classes, enabling robust generalization. Experimental results on benchmark datasets demonstrate that MSPL outperforms traditional approaches in detecting low-profile and novel attack types, establishing it as a robust solution for zero-day attack detection. </p>
<blockquote>
<p>ç½‘ç»œå…¥ä¾µæ£€æµ‹ç³»ç»Ÿåœ¨é¢å¯¹æ–°å…´æ”»å‡»æ¨¡å¼è¯†åˆ«æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™çš„æ ·æœ¬æ•°æ®ä¸‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å°æ ·æœ¬æ”»å‡»æ£€æµ‹çš„æ–°å‹å¤šç©ºé—´åŸå‹å­¦ä¹ ï¼ˆMSPLï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåº¦é‡ç©ºé—´ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ã€ä½™å¼¦è·ç¦»ã€åˆ‡æ¯”é›ªå¤«è·ç¦»å’Œç“¦ç‘Ÿæ–¯å¦è·ç¦»ï¼‰ä¸­è¿è¡Œï¼Œé€šè¿‡çº¦æŸåŠ æƒæ–¹æ¡ˆè¿›è¡Œæ•´åˆï¼Œä»¥æé«˜åµŒå…¥çš„ç¨³å¥æ€§å’Œæ¨¡å¼è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨Polyakå¹³å‡åŸå‹ç”ŸæˆæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶ç¨³å®šäº†å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”ç½•è§å’Œé›¶æ—¥æ”»å‡»ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨æƒ…æ™¯è®­ç»ƒæ¨¡å¼ç¡®ä¿ä¸åŒæ”»å‡»ç±»åˆ«çš„å‡è¡¡è¡¨ç¤ºï¼Œä»è€Œå®ç°ç¨³å¥çš„æ³›åŒ–ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMSPLåœ¨æ£€æµ‹ä½é…ç½®å’Œæ–°å‹æ”»å‡»ç±»å‹æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå·²æˆä¸ºä¸€ç§æœ‰æ•ˆçš„é›¶æ—¥æ”»å‡»æ£€æµ‹è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00050v1">PDF</a> The AAAI-25 Workshop on Artificial Intelligence for Cyber Security   (AICS)</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æå‡ºäº†é’ˆå¯¹å°‘é‡æ•°æ®æ ·æœ¬æ”»å‡»æ£€æµ‹çš„å¤šç©ºé—´åŸå‹å­¦ä¹ ï¼ˆMSPLï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šç§åº¦é‡ç©ºé—´æŠ€æœ¯æé«˜åµŒå…¥ç¨³å¥æ€§å’Œæ¨¡å¼è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶æœ‰æ•ˆé€‚åº”ç¨€æœ‰å’Œé›¶æ—¥æ”»å‡»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMSPLåœ¨æ£€æµ‹ä½è½®å»“å’Œæ–°å‹æ”»å‡»ç±»å‹æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæˆä¸ºé›¶æ—¥æ”»å‡»æ£€æµ‹çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘ç»œå…¥ä¾µæ£€æµ‹ç³»ç»Ÿé¢ä¸´è¯†åˆ«æ–°å…´æ”»å‡»æ¨¡å¼çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰é™æ•°æ®æ ·æœ¬çš„æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šç©ºé—´åŸå‹å­¦ä¹ ï¼ˆMSPLï¼‰æ¡†æ¶ï¼Œç”¨äºè§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>MSPLæ¡†æ¶åœ¨å¤šç§åº¦é‡ç©ºé—´ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»ã€ä½™å¼¦è·ç¦»ã€åˆ‡æ¯”é›ªå¤«è·ç¦»å’Œç“¦ç‘Ÿæ–¯å¦è·ç¦»ï¼‰ä¸Šè¿›è¡Œæ“ä½œï¼Œé€šè¿‡çº¦æŸåŠ æƒæ–¹æ¡ˆè¿›è¡Œé›†æˆã€‚</li>
<li>åˆ©ç”¨Polyakå¹³å‡åŸå‹ç”ŸæˆæŠ€æœ¯å¢å¼ºå­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œæœ‰æ•ˆé€‚åº”ç¨€æœ‰å’Œé›¶æ—¥æ”»å‡»ã€‚</li>
<li>é‡‡ç”¨å‘¨æœŸè®­ç»ƒæ¨¡å¼ç¡®ä¿å„ç§æ”»å‡»ç±»çš„å‡è¡¡è¡¨ç¤ºï¼Œå®ç°ç¨³å¥çš„æ³›åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMSPLæ¡†æ¶åœ¨æ£€æµ‹ä½è½®å»“å’Œæ–°å‹æ”»å‡»ç±»å‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6079a54f9c1eb24a9757f50c8693f8c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8be21bd3cb66f318e02cda77494d91a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-918c059410e8702e6533fe45f38af1fb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fc6cf2bd7938e5bba0c1f29e3463ac4d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  Test-time Controllable Image Generation by Explicit Spatial Constraint   Enforcement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f1aa26cb833b8d8dc5bfb417adf754a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  A3 Android Agent Arena for Mobile GUI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11176.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
