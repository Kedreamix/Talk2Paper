<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  Unifying Specialized Visual Encoders for Video Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c6709bd6ded869bc0c876103b2853df2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-06-æ›´æ–°"><a href="#2025-01-06-æ›´æ–°" class="headerlink" title="2025-01-06 æ›´æ–°"></a>2025-01-06 æ›´æ–°</h1><h2 id="Unifying-Specialized-Visual-Encoders-for-Video-Language-Models"><a href="#Unifying-Specialized-Visual-Encoders-for-Video-Language-Models" class="headerlink" title="Unifying Specialized Visual Encoders for Video Language Models"></a>Unifying Specialized Visual Encoders for Video Language Models</h2><p><strong>Authors:Jihoon Chung, Tyler Zhu, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Honglu Zhou, Olga Russakovsky</strong></p>
<p>The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€è¿‘å‡ºç°ï¼Œé€šè¿‡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰å°†å¤æ‚çš„æ¨ç†èƒ½åŠ›å¼•å…¥äº†è§†é¢‘é¢†åŸŸã€‚ç„¶è€Œï¼ŒVideoLLMç›®å‰ä¾èµ–äºå•ä¸€è§†è§‰ç¼–ç å™¨è¿›è¡Œæ‰€æœ‰è§†è§‰å¤„ç†ï¼Œè¿™é™åˆ¶äº†å¯ä»¥ä¼ è¾¾ç»™LLMçš„è§†è§‰ä¿¡æ¯é‡å’Œç±»å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒMERVï¼ˆè§†é¢‘çš„å¤šå…ƒç¼–ç å™¨è¡¨ç¤ºï¼‰ï¼Œç›¸ååœ°åˆ©ç”¨å¤šä¸ªå†»ç»“çš„è§†è§‰ç¼–ç å™¨æ¥åˆ›å»ºè§†é¢‘çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œä¸ºVideoLLMæä¾›ä¸€å¥—å…¨é¢çš„ä¸“ä¸šè§†è§‰çŸ¥è¯†ã€‚é€šè¿‡æ—¶ç©ºå¯¹é½æ¯ä¸ªç¼–ç å™¨çš„ç‰¹å¾ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè§£å†³æ›´å¹¿æ³›çš„å¼€æ”¾å¼å’Œé€‰æ‹©æ€§è§†é¢‘ç†è§£é—®é¢˜ï¼Œå¹¶è¶…è¶Šå…ˆå‰çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åœ¨æ ‡å‡†è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMERVçš„å‡†ç¡®åº¦æ¯”Video-LLaVAé«˜å‡º3.7%ï¼ŒåŒæ—¶æ‹¥æœ‰æ›´é«˜çš„Video-ChatGPTåˆ†æ•°ã€‚æˆ‘ä»¬è¿˜æé«˜äº†é›¶æ ·æœ¬æ„ŸçŸ¥æµ‹è¯•å‡†ç¡®æ€§çš„å‰ä»»æœ€ä½³SeViLAçš„å‡†ç¡®åº¦ï¼Œæé«˜äº†2.2%ã€‚MERVå¼•å…¥äº†æå°‘çš„é¢å¤–å‚æ•°ï¼Œå¹¶ä¸”ç›¸å¯¹äºå•ç¼–ç å™¨æ–¹æ³•è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶å¹¶è¡Œå¤„ç†è§†è§‰ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†å®šæ€§è¯æ®è¡¨æ˜MERVæˆåŠŸåœ°ä»æ¯ä¸ªç¼–ç å™¨ä¸­æ•è·äº†é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæä¾›äº†åˆ©ç”¨å¤šä¸ªè§†è§‰ç¼–ç å™¨è¿›è¡Œå…¨é¢è§†é¢‘ç†è§£çš„å……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01426v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://tylerzhu.com/merv/">https://tylerzhu.com/merv/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰ä¸ºè§†é¢‘é¢†åŸŸå¸¦æ¥äº†å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰VideoLLMsä»…ä¾èµ–å•ä¸€è§†è§‰ç¼–ç å™¨è¿›è¡Œæ‰€æœ‰è§†è§‰å¤„ç†ï¼Œé™åˆ¶äº†èƒ½å¤Ÿä¼ è¾¾ç»™LLMçš„è§†è§‰ä¿¡æ¯é‡å’Œç±»å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•MERVï¼ˆMulti-Encoder Representation of Videosï¼‰é‡‡ç”¨å¤šä¸ªå†»ç»“çš„è§†è§‰ç¼–ç å™¨æ¥åˆ›å»ºè§†é¢‘çš„ç»Ÿä¸€è¡¨ç¤ºï¼Œä¸ºVideoLLMæä¾›ä¸“é—¨çš„è§†è§‰çŸ¥è¯†é›†ã€‚é€šè¿‡æ—¶ç©ºå¯¹é½æ¯ä¸ªç¼–ç å™¨çš„ç‰¹å¾ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè§£å†³æ›´å¹¿æ³›çš„å¼€æ”¾é—®é¢˜å’Œå¤šé€‰æ‹©é¢˜å‹çš„è§†é¢‘ç†è§£é—®é¢˜ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„ä½œå“ã€‚MERVåœ¨æ ‡å‡†è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æ¯”Video-LLaVAé«˜å‡º3.7%ï¼ŒåŒæ—¶æ‹¥æœ‰æ›´é«˜çš„Video-ChatGPTåˆ†æ•°ã€‚æˆ‘ä»¬è¿˜æé«˜äº†é›¶æ ·æœ¬æ„ŸçŸ¥æµ‹è¯•å‡†ç¡®æ€§çš„å…ˆå‰æœ€ä½³æˆç»©SeViLAï¼Œæé«˜äº†2.2%ã€‚MERVåœ¨å¢åŠ å°‘é‡é¢å¤–å‚æ•°çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”å•ç¼–ç å™¨æ–¹æ³•æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶è¡Œè¿›è¡Œè§†è§‰å¤„ç†ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›å®šæ€§è¯æ®è¡¨æ˜MERVæˆåŠŸæ•è·äº†æ¯ä¸ªç¼–ç å™¨çš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨å¤šä¸ªè§†è§‰ç¼–ç å™¨è¿›è¡Œè§†é¢‘ç†è§£çš„æ½œåŠ›å·¨å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoLLMsç›®å‰ä¾èµ–äºå•ä¸€è§†è§‰ç¼–ç å™¨ï¼Œé™åˆ¶äº†è§†é¢‘ç†è§£çš„æ·±åº¦å’Œå¹¿åº¦ã€‚</li>
<li>MERVæ–¹æ³•é‡‡ç”¨å¤šä¸ªå†»ç»“çš„è§†è§‰ç¼–ç å™¨æ¥å¤„ç†è§†é¢‘ï¼Œä¸ºVideoLLMæä¾›æ›´å…¨é¢çš„è§†è§‰çŸ¥è¯†ã€‚</li>
<li>MERVé€šè¿‡æ—¶ç©ºå¯¹é½ç‰¹å¾ï¼Œæé«˜äº†å¯¹å¼€æ”¾é—®é¢˜å’Œå¤šé€‰æ‹©é¢˜å‹çš„è§†é¢‘ç†è§£é—®é¢˜çš„å¤„ç†èƒ½åŠ›ã€‚</li>
<li>MERVåœ¨æ ‡å‡†è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>MERVåœ¨é›¶æ ·æœ¬æ„ŸçŸ¥æµ‹è¯•å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>MERVå…·æœ‰è¾ƒå°‘çš„é¢å¤–å‚æ•°ï¼Œè®­ç»ƒé€Ÿåº¦å¿«ï¼Œå¯å¹¶è¡Œå¤„ç†è§†è§‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45a8f6f15783afcdcedb5fbe80a351cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23b1755514313ee9d9327abed3d8bb2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c17ec5e9b78a453be397fcf82958e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a33604a1b9e2e10edeb7f6a443d6245c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OmniChat-Enhancing-Spoken-Dialogue-Systems-with-Scalable-Synthetic-Data-for-Diverse-Scenarios"><a href="#OmniChat-Enhancing-Spoken-Dialogue-Systems-with-Scalable-Synthetic-Data-for-Diverse-Scenarios" class="headerlink" title="OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data   for Diverse Scenarios"></a>OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data   for Diverse Scenarios</h2><p><strong>Authors:Xize Cheng, Dongjie Fu, Xiaoda Yang, Minghui Fang, Ruofan Hu, Jingyu Lu, Bai Jionghao, Zehan Wang, Shengpeng Ji, Rongjie Huang, Linjun Li, Yu Chen, Tao Jin, Zhou Zhao</strong></p>
<p>With the rapid development of large language models, researchers have created increasingly advanced spoken dialogue systems that can naturally converse with humans. However, these systems still struggle to handle the full complexity of real-world conversations, including audio events, musical contexts, and emotional expressions, mainly because current dialogue datasets are constrained in both scale and scenario diversity. In this paper, we propose leveraging synthetic data to enhance the dialogue models across diverse scenarios. We introduce ShareChatX, the first comprehensive, large-scale dataset for spoken dialogue that spans diverse scenarios. Based on this dataset, we introduce OmniChat, a multi-turn dialogue system with a heterogeneous feature fusion module, designed to optimize feature selection in different dialogue contexts. In addition, we explored critical aspects of training dialogue systems using synthetic data. Through comprehensive experimentation, we determined the ideal balance between synthetic and real data, achieving state-of-the-art results on the real-world dialogue dataset DailyTalk. We also highlight the crucial importance of synthetic data in tackling diverse, complex dialogue scenarios, especially those involving audio and music. For more details, please visit our demo page at \url{<a target="_blank" rel="noopener" href="https://sharechatx.github.io/%7D">https://sharechatx.github.io/}</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶è€…å·²ç»å¼€å‘å‡ºäº†è¶Šæ¥è¶Šå…ˆè¿›çš„å¯¹è¯ç³»ç»Ÿï¼Œå¯ä»¥è‡ªç„¶åœ°ä¸äººç±»è¿›è¡Œå¯¹è¯ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨å¤„ç†çœŸå®ä¸–ç•Œçš„å¯¹è¯çš„å¤æ‚æ€§æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éŸ³é¢‘äº‹ä»¶ã€éŸ³ä¹èƒŒæ™¯å’Œæƒ…ç»ªè¡¨è¾¾ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå½“å‰çš„å¯¹è¯æ•°æ®é›†åœ¨è§„æ¨¡å’Œåœºæ™¯å¤šæ ·æ€§æ–¹é¢å­˜åœ¨é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨åˆæˆæ•°æ®æ¥å¢å¼ºå¯¹è¯æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†ShareChatXï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢ã€å¤§è§„æ¨¡çš„å¯¹è¯æ•°æ®é›†ï¼Œæ¶µç›–å„ç§åœºæ™¯ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OmniChatï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè½®å¯¹è¯ç³»ç»Ÿï¼Œé…å¤‡æœ‰å¼‚è´¨ç‰¹å¾èåˆæ¨¡å—ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸åŒå¯¹è¯è¯­å¢ƒä¸­çš„ç‰¹å¾é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒå¯¹è¯ç³»ç»Ÿçš„å…³é”®æ–¹é¢ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬ç¡®å®šäº†åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´çš„ç†æƒ³å¹³è¡¡ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œçš„å¯¹è¯æ•°æ®é›†DailyTalkä¸Šå–å¾—äº†æœ€æ–°çš„ç»“æœã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†åˆæˆæ•°æ®åœ¨å¤„ç†å¤šæ ·ã€å¤æ‚çš„å¯¹è¯åœºæ™¯ä¸­çš„å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠéŸ³é¢‘å’ŒéŸ³ä¹çš„åœºæ™¯ã€‚å¦‚éœ€æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„æ¼”ç¤ºé¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://sharechatx.github.io/]%E3%80%82">https://sharechatx.github.io/]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01384v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¹¶æŒ‡å‡ºå½“å‰å¯¹è¯ç³»ç»Ÿå¤„ç†çœŸå®ä¸–ç•Œå¯¹è¯çš„å¤æ‚æ€§æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†åˆ©ç”¨åˆæˆæ•°æ®å¢å¼ºå¯¹è¯æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶ä»‹ç»äº†ShareChatXæ•°æ®é›†å’ŒOmniChatå¤šè½®å¯¹è¯ç³»ç»Ÿã€‚å®éªŒè¡¨æ˜ï¼Œåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®çš„å¹³è¡¡å¯ä»¥å–å¾—æœ€ä½³æ•ˆæœï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œå¯¹è¯æ•°æ®é›†DailyTalkä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚åˆæˆæ•°æ®åœ¨åº”å¯¹æ¶‰åŠéŸ³é¢‘å’ŒéŸ³ä¹çš„å¤æ‚å¯¹è¯åœºæ™¯æ–¹é¢å°¤å…¶é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¯¹è¯ç³»ç»Ÿé¢ä¸´å¤„ç†çœŸå®ä¸–ç•Œå¯¹è¯å¤æ‚æ€§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯éŸ³é¢‘äº‹ä»¶ã€éŸ³ä¹èƒŒæ™¯å’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹é¢ã€‚</li>
<li>ShareChatXæ•°æ®é›†æ˜¯é¦–ä¸ªå…¨é¢ã€å¤§è§„æ¨¡çš„å¯¹è¯æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§åœºæ™¯ã€‚</li>
<li>OmniChatæ˜¯ä¸€ä¸ªå¤šè½®å¯¹è¯ç³»ç»Ÿï¼Œå…·æœ‰å¼‚è´¨ç‰¹å¾èåˆæ¨¡å—ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸åŒå¯¹è¯è¯­å¢ƒä¸­çš„ç‰¹å¾é€‰æ‹©ã€‚</li>
<li>åˆæˆæ•°æ®åœ¨è®­ç»ƒå¯¹è¯ç³»ç»Ÿä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®çš„å¹³è¡¡å¯¹äºå–å¾—æœ€ä½³å®éªŒæ•ˆæœè‡³å…³é‡è¦ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œå¯¹è¯æ•°æ®é›†DailyTalkä¸Šå–å¾—äº†æœ€æ–°æ°´å¹³çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1be0b048dda14ba42265ec8869c51d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b40dd3f7b7d11d54d9e1bbb140bff31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3ba33d3407c9df288142fb727743339.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09ce7a918e8a9d432498996f1b8a5c86.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Aligning-Large-Language-Models-for-Faithful-Integrity-Against-Opposing-Argument"><a href="#Aligning-Large-Language-Models-for-Faithful-Integrity-Against-Opposing-Argument" class="headerlink" title="Aligning Large Language Models for Faithful Integrity Against Opposing   Argument"></a>Aligning Large Language Models for Faithful Integrity Against Opposing   Argument</h2><p><strong>Authors:Yong Zhao, Yang Deng, See-Kiong Ng, Tat-Seng Chua</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks. However, they can be easily misled by unfaithful arguments during conversations, even when their original statements are correct. To this end, we investigate the problem of maintaining faithful integrity in LLMs. This involves ensuring that LLMs adhere to their faithful statements in the face of opposing arguments and are able to correct their incorrect statements when presented with faithful arguments. In this work, we propose a novel framework, named Alignment for Faithful Integrity with Confidence Estimation (AFICE), which aims to align the LLM responses with faithful integrity. Specifically, AFICE first designs a Bilateral Confidence Estimation (BCE) approach for estimating the uncertainty of each response generated by the LLM given a specific context, which simultaneously estimate the modelâ€™s confidence to the question based on the internal states during decoding as well as to the answer based on cumulative probability ratios. With the BCE, we construct a conversational preference dataset composed of context, original statement, and argument, which is adopted for aligning the LLM for faithful integrity using Direct Preference Optimization (DPO). Extensive experimental results on a wide range of benchmarks demonstrate significant improvements in the LLMâ€™s ability to maintain faithful responses when encountering opposing arguments, ensuring both the practical utility and trustworthiness of LLMs in complex interactive settings. Code and data will be released via <a target="_blank" rel="noopener" href="https://github.com/zhaoy777/AFICE.git">https://github.com/zhaoy777/AFICE.git</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¯¹è¯ä¸­å¾ˆå®¹æ˜“å—åˆ°ä¸çœŸå®è®ºæ®çš„è¯¯å¯¼ï¼Œå³ä½¿å®ƒä»¬çš„åŸå§‹é™ˆè¿°æ˜¯æ­£ç¡®çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨LLMä¸­ä¿æŒå¿ å®æ€§çš„é—®é¢˜ã€‚è¿™æ¶‰åŠåˆ°ç¡®ä¿LLMåœ¨é¢å¯¹ç›¸åçš„è®ºç‚¹æ—¶åšæŒå…¶å¿ å®çš„é™ˆè¿°ï¼Œå¹¶åœ¨é¢å¯¹çœŸå®çš„è®ºæ®æ—¶èƒ½å¤Ÿçº æ­£å…¶ä¸æ­£ç¡®çš„é™ˆè¿°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œåŸºäºä¿¡å¿ƒä¼°è®¡çš„å¿ å®æ€§å¯¹é½â€ï¼ˆAFICEï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMçš„å›åº”ä¸å¿ å®æ€§ä¿æŒä¸€è‡´ã€‚å…·ä½“æ¥è¯´ï¼ŒAFICEé¦–å…ˆè®¾è®¡äº†ä¸€ç§åŒå‘ä¿¡å¿ƒä¼°è®¡ï¼ˆBCEï¼‰æ–¹æ³•ï¼Œç”¨äºä¼°è®¡LLMåœ¨ç»™å®šçš„ç‰¹å®šä¸Šä¸‹æ–‡ä¸­æ‰€ç”Ÿæˆçš„æ¯ä¸ªå›åº”çš„ä¸ç¡®å®šæ€§ï¼Œè¯¥æ–¹æ³•åŒæ—¶åŸºäºè§£ç è¿‡ç¨‹ä¸­çš„å†…éƒ¨çŠ¶æ€ä¼°è®¡æ¨¡å‹å¯¹é—®é¢˜çš„ä¿¡å¿ƒä»¥åŠå¯¹ç­”æ¡ˆçš„ä¿¡å¿ƒæ˜¯åŸºäºç´¯ç§¯æ¦‚ç‡æ¯”çš„ã€‚é€šè¿‡BCEï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸Šä¸‹æ–‡ã€åŸå§‹é™ˆè¿°å’Œè®ºæ®çš„å¯¹è¯åå¥½æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥å¯¹é½LLMçš„å¿ å®æ€§ã€‚åœ¨å¹¿æ³›çš„æ ‡å‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é¢ä¸´ç›¸åçš„è®ºç‚¹æ—¶ï¼ŒLLMåœ¨ä¿æŒå¿ å®å›åº”çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç¡®ä¿äº†LLMåœ¨å¤æ‚çš„äº¤äº’å¼ç¯å¢ƒä¸­çš„å®ç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚ä»£ç å’Œæ•°æ®å°†é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/zhaoy777/AFICE.git%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/zhaoy777/AFICE.gitå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01336v1">PDF</a> 17 pages, 5 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¯¹è¯ä¸­å®¹æ˜“å—ä¸çœŸå®è®ºæ®çš„å½±å“è€Œåç¦»åŸæœ‰æ­£ç¡®è§‚ç‚¹ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åä¸ºAFICEçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMçš„å¿ å®åº¦å¹¶å¢å¼ºå…¶åœ¨é¢å¯¹å¯¹ç«‹è§‚ç‚¹æ—¶çš„ä¿¡å¿ƒè¯„ä¼°ã€‚AFICEé€šè¿‡è®¾è®¡åŒå‘ä¿¡å¿ƒä¼°è®¡ï¼ˆBCEï¼‰æ–¹æ³•ï¼Œè¯„ä¼°LLMåœ¨ç‰¹å®šè¯­å¢ƒä¸‹çš„å›åº”ä¸ç¡®å®šæ€§ï¼Œå¹¶åŸºäºè§£ç è¿‡ç¨‹ä¸­çš„å†…éƒ¨çŠ¶æ€å’Œç´¯ç§¯æ¦‚ç‡æ¯”ç‡æ¥è¯„ä¼°æ¨¡å‹å¯¹ç­”æ¡ˆçš„ä¿¡å¿ƒã€‚ä½¿ç”¨BCEæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¯­å¢ƒã€åŸå§‹é™ˆè¿°å’Œè®ºæ®çš„å¯¹è¯åå¥½æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥å¯¹é½LLMçš„å¿ å®åº¦ã€‚åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLLMåœ¨é¢ä¸´å¯¹ç«‹è§‚ç‚¹æ—¶ï¼Œä¿æŒå¿ å®å›åº”çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè¿™ç¡®ä¿äº†å…¶åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­çš„å®ç”¨æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†æ˜“å—ä¸çœŸå®è®ºæ®å½±å“ã€‚</li>
<li>AFICEæ¡†æ¶æ—¨åœ¨æé«˜LLMçš„å¿ å®åº¦å¹¶å¢å¼ºå…¶ä¿¡å¿ƒè¯„ä¼°ã€‚</li>
<li>AFICEé€šè¿‡BCEæ–¹æ³•è¯„ä¼°LLMå›åº”çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>åŒå‘ä¿¡å¿ƒä¼°è®¡è€ƒè™‘äº†æ¨¡å‹å¯¹é—®é¢˜å’Œç­”æ¡ˆçš„ä¿¡å¿ƒã€‚</li>
<li>ä½¿ç”¨BCEæ„å»ºäº†ä¸€ä¸ªå¯¹è¯åå¥½æ•°æ®é›†ï¼Œç”¨äºæé«˜LLMçš„å¿ å®åº¦ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç”¨äºå¯¹é½LLMçš„å¿ å®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e86c94b5fb90e83e1b253b8c02713c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61951e0600171c67922d6ab0a92d8dad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c9963d4d03ad1726cb6658af7421cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668ff00d8f5a5f23fdbea816394b8147.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7aa051dba9418d1fb729781ec3077909.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CySecBench-Generative-AI-based-CyberSecurity-focused-Prompt-Dataset-for-Benchmarking-Large-Language-Models"><a href="#CySecBench-Generative-AI-based-CyberSecurity-focused-Prompt-Dataset-for-Benchmarking-Large-Language-Models" class="headerlink" title="CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for   Benchmarking Large Language Models"></a>CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for   Benchmarking Large Language Models</h2><p><strong>Authors:Johan WahrÃ©us, Ahmed Mohamed Hussain, Panos Papadimitratos</strong></p>
<p>Numerous studies have investigated methods for jailbreaking Large Language Models (LLMs) to generate harmful content. Typically, these methods are evaluated using datasets of malicious prompts designed to bypass security policies established by LLM providers. However, the generally broad scope and open-ended nature of existing datasets can complicate the assessment of jailbreaking effectiveness, particularly in specific domains, notably cybersecurity. To address this issue, we present and publicly release CySecBench, a comprehensive dataset containing 12662 prompts specifically designed to evaluate jailbreaking techniques in the cybersecurity domain. The dataset is organized into 10 distinct attack-type categories, featuring close-ended prompts to enable a more consistent and accurate assessment of jailbreaking attempts. Furthermore, we detail our methodology for dataset generation and filtration, which can be adapted to create similar datasets in other domains. To demonstrate the utility of CySecBench, we propose and evaluate a jailbreaking approach based on prompt obfuscation. Our experimental results show that this method successfully elicits harmful content from commercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT and 88% with Gemini; in contrast, Claude demonstrated greater resilience with a jailbreaking SR of 17%. Compared to existing benchmark approaches, our method shows superior performance, highlighting the value of domain-specific evaluation datasets for assessing LLM security measures. Moreover, when evaluated using prompts from a widely used dataset (i.e., AdvBench), it achieved an SR of 78.5%, higher than the state-of-the-art methods. </p>
<blockquote>
<p>å·²æœ‰å¤§é‡ç ”ç©¶æ¢è®¨äº†çªç ´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæœ‰å®³å†…å®¹çš„æ–¹æ³•ã€‚é€šå¸¸ï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨æ¶æ„æç¤ºæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›æ•°æ®é›†æ—¨åœ¨ç»•è¿‡LLMæä¾›å•†å»ºç«‹çš„å®‰å…¨ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†çš„å¹¿æ³›èŒƒå›´å’Œå¼€æ”¾æ€§ç‰¹ç‚¹å¯èƒ½ä¼šä½¿è¯„ä¼°çªç ´æ•ˆæœå˜å¾—å¤æ‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸï¼Œå¦‚ç½‘ç»œå®‰å…¨é¢†åŸŸå°¤ä¸ºå¦‚æ­¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºå¹¶å…¬å¼€å‘å¸ƒCySecBenchæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸“é—¨ç”¨äºè¯„ä¼°ç½‘ç»œå®‰å…¨é¢†åŸŸçªç ´æŠ€æœ¯çš„12662ä¸ªæç¤ºã€‚æ•°æ®é›†åˆ†ä¸º10ä¸ªç‹¬ç‰¹çš„æ”»å‡»ç±»å‹ç±»åˆ«ï¼Œé‡‡ç”¨å°é—­å¼æç¤ºï¼Œä»¥ä¾¿æ›´ä¸€è‡´å’Œå‡†ç¡®åœ°è¯„ä¼°çªç ´å°è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†æ•°æ®é›†ç”Ÿæˆå’Œè¿‡æ»¤çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€‚åº”äºåœ¨å…¶ä»–é¢†åŸŸåˆ›å»ºç±»ä¼¼æ•°æ®é›†ã€‚ä¸ºäº†å±•ç¤ºCySecBenchçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæç¤ºæ¨¡ç³ŠåŒ–çš„çªç ´æ–¹æ³•ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°å’Œæ¼”ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°ä»å•†ä¸šé»‘ç®±LLMä¸­å¼•å‘äº†æœ‰å®³å†…å®¹ï¼Œä½¿ç”¨ChatGPTçš„æˆåŠŸç‡ä¸º65%ï¼Œä½¿ç”¨Geminiçš„æˆåŠŸç‡ä¸º88%ï¼Œè€ŒClaudeè¡¨ç°å‡ºæ›´å¼ºçš„æŠ—çªç ´æ€§ï¼ŒæˆåŠŸç‡ä¸º17%ã€‚ä¸ç°æœ‰çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°æ•°æ®é›†åœ¨è¯„ä¼°LLMå®‰å…¨æªæ–½æ–¹é¢çš„ä»·å€¼ã€‚æ­¤å¤–ï¼Œåœ¨ä½¿ç”¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆå³AdvBenchï¼‰ä¸­çš„æç¤ºè¿›è¡Œè¯„ä¼°æ—¶ï¼Œå…¶æˆåŠŸç‡è¾¾åˆ°äº†78.5%ï¼Œé«˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01335v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç ´è§£æ–¹æ³•çš„ç ”ç©¶ï¼Œç ”ç©¶äººå‘˜åˆ›å»ºå¹¶å…¬å¼€å‘å¸ƒäº†CySecBenchæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸“é—¨ç”¨äºè¯„ä¼°ç½‘ç»œå®‰å…¨é¢†åŸŸLLMç ´è§£æŠ€æœ¯çš„12662ä¸ªæç¤ºã€‚æ•°æ®é›†åˆ†ä¸º10ä¸ªä¸åŒçš„æ”»å‡»ç±»å‹ç±»åˆ«ï¼Œé‡‡ç”¨å°é—­å¼æç¤ºï¼Œä»¥ä¾¿æ›´ä¸€è‡´ã€å‡†ç¡®åœ°è¯„ä¼°ç ´è§£å°è¯•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¯¦ç»†ä»‹ç»äº†æ•°æ®é›†ç”Ÿæˆå’Œè¿‡æ»¤çš„æ–¹æ³•è®ºï¼Œè¯¥æ–¹æ³•å¯ä»¥é€‚åº”å…¶ä»–é¢†åŸŸçš„ç±»ä¼¼æ•°æ®é›†åˆ›å»ºã€‚ä¸ºè¯æ˜CySecBenchçš„å®ç”¨æ€§ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åŸºäºæç¤ºæ··æ·†çš„ç ´è§£æ–¹æ³•ï¼Œå¹¶è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸè¯±å¯¼å•†ä¸šé»‘ç®±LLMç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œåœ¨ChatGPTä¸Šçš„æˆåŠŸç‡ä¸º65%ï¼Œåœ¨Geminiä¸Šçš„æˆåŠŸç‡é«˜è¾¾88%ï¼Œè€ŒClaudeæ˜¾ç¤ºå‡ºæ›´å¼ºçš„æŠ—ç ´è§£æ€§ï¼ŒæˆåŠŸç‡ä»…ä¸º17%ã€‚ä¸ç°æœ‰çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¼ºè°ƒé¢†åŸŸç‰¹å®šè¯„ä¼°æ•°æ®é›†åœ¨è¯„ä¼°LLMå®‰å…¨æªæ–½æ–¹é¢çš„ä»·å€¼ã€‚ä½¿ç”¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆå¦‚AdvBenchï¼‰ä¸­çš„æç¤ºè¿›è¡Œè¯„ä¼°æ—¶ï¼Œå…¶æˆåŠŸç‡è¾¾åˆ°78.5%ï¼Œé«˜äºç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸“é—¨ç”¨äºè¯„ä¼°ç½‘ç»œå®‰å…¨é¢†åŸŸLLMç ´è§£æŠ€æœ¯çš„CySecBenchæ•°æ®é›†ï¼ŒåŒ…å«12662ä¸ªç‰¹å®šæç¤ºã€‚</li>
<li>æ•°æ®é›†åˆ†ä¸º10ä¸ªæ”»å‡»ç±»å‹ç±»åˆ«ï¼Œé‡‡ç”¨å°é—­å¼æç¤ºï¼Œä»¥æé«˜è¯„ä¼°çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¦ç»†ä»‹ç»æ•°æ®é›†ç”Ÿæˆå’Œè¿‡æ»¤çš„æ–¹æ³•è®ºï¼Œå¯é€‚åº”å…¶ä»–é¢†åŸŸçš„ç±»ä¼¼æ•°æ®é›†åˆ›å»ºã€‚</li>
<li>åŸºäºæç¤ºæ··æ·†çš„ç ´è§£æ–¹æ³•è¢«æå‡ºå¹¶è¯„ä¼°ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å•†ä¸šé»‘ç®±LLMä¸ŠæˆåŠŸè¯±å¯¼ç”Ÿæˆæœ‰å®³å†…å®¹ï¼Œä¸åŒæ¨¡å‹çš„æˆåŠŸç‡æœ‰æ‰€ä¸åŒã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è¯„ä¼°LLMå®‰å…¨æªæ–½æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„ä»·å€¼ã€‚</li>
<li>ä½¿ç”¨å…¶ä»–æ•°æ®é›†ï¼ˆå¦‚AdvBenchï¼‰è¿›è¡Œè¯„ä¼°æ—¶ï¼Œè¯¥æ–¹æ³•ä»è¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba83fbc2e6e39fce53c82a34a2b79e09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5095a178ebd3a9b8a11695e4d6b2167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7cc4d3593fa7fed602eaeb9a38b749.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b30e9743beb304eb43814e645434da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2504939c72f1a09c1a2674d06d6946f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d12abe606012f79b9091ec74aac63e95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-523aa0b00abc75fbbea374bb72e9e33a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Prompt-Alchemist-Automated-LLM-Tailored-Prompt-Optimization-for-Test-Case-Generation"><a href="#The-Prompt-Alchemist-Automated-LLM-Tailored-Prompt-Optimization-for-Test-Case-Generation" class="headerlink" title="The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for   Test Case Generation"></a>The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for   Test Case Generation</h2><p><strong>Authors:Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xiaoqian Jiao, Chun Yong Chong, Shan Gao, Michael Lyu</strong></p>
<p>Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMsâ€™ performance in the task. </p>
<blockquote>
<p>æµ‹è¯•ç”¨ä¾‹å¯¹äºéªŒè¯è½¯ä»¶åº”ç”¨çš„å¯é æ€§å’Œè´¨é‡è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿä¸ºç»™å®šçš„æºä»£ç ç”Ÿæˆæœ‰ç”¨çš„æµ‹è¯•ç”¨ä¾‹ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œä¸»è¦ä¾èµ–äºäººå·¥ç¼–å†™çš„æ™®é€šæç¤ºï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´ç»“æœä¸å°½å¦‚äººæ„ï¼Œå› ä¸ºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¼šå—åˆ°æç¤ºçš„å¾ˆå¤§å½±å“ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¯¹æ‰€æœ‰è¯­è¨€æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æç¤ºï¼Œå¿½ç•¥äº†ä¸åŒè¯­è¨€æ¨¡å‹å¯èƒ½æœ€é€‚åˆä¸åŒæç¤ºçš„äº‹å®ã€‚è€ƒè™‘åˆ°å¯èƒ½å­˜åœ¨çš„å„ç§æç¤ºå½¢å¼ï¼Œä¸ºæ¯ä¸ªè¯­è¨€æ¨¡å‹è‡ªåŠ¨å‘ç°æœ€ä½³æç¤ºæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰è‡ªåŠ¨æç¤ºä¼˜åŒ–çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬å¾ˆéš¾ä¸ºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆä»»åŠ¡ç”Ÿæˆæœ‰æ•ˆçš„æç¤ºã€‚é¦–å…ˆï¼Œè¿™äº›æ–¹æ³•é€šè¿‡ç®€å•åœ°ç»„åˆå’Œå˜å¼‚ç°æœ‰æç¤ºæ¥è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œè€Œæ²¡æœ‰é€‚å½“çš„æŒ‡å¯¼ï¼Œå¯¼è‡´ç”Ÿæˆçš„æç¤ºç¼ºä¹å¤šæ ·æ€§ï¼Œå¹¶åœ¨ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ä¸­é‡å¤ç›¸åŒçš„é”™è¯¯ã€‚å…¶æ¬¡ï¼Œè¿™äº›æç¤ºé€šå¸¸ç¼ºä¹é¢†åŸŸä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œé™åˆ¶äº†è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01329v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆæºä»£ç æµ‹è¯•æ¡ˆä¾‹æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦ä¾èµ–äººå·¥ç¼–å†™çš„æç¤ºï¼Œè¿™å¾€å¾€å¯¼è‡´ç»“æœä¸å°½å¦‚äººæ„ã€‚ä¸åŒLLMå¯èƒ½éœ€è¦ä¸åŒçš„æç¤ºï¼Œè‡ªåŠ¨å‘ç°æ¯ä¸ªLLMçš„æœ€ä½³æç¤ºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å½“å‰è‡ªåŠ¨ä¼˜åŒ–æç¤ºçš„æ–¹æ³•ç¼ºä¹å¤šæ ·æ€§ã€é‡å¤åº¦é«˜ä¸”ç¼ºä¹é¢†åŸŸä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œé™åˆ¶äº†LLMåœ¨æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ¡ˆä¾‹åœ¨éªŒè¯è½¯ä»¶åº”ç”¨çš„å¯é æ€§å’Œè´¨é‡æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>LLMæœ‰èƒ½åŠ›ç”Ÿæˆæœ‰ç”¨çš„æµ‹è¯•æ¡ˆä¾‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥ç¼–å†™çš„æç¤ºï¼Œå¯¼è‡´ç»“æœå¯èƒ½ä¸ä½³ã€‚</li>
<li>ä¸åŒLLMå¯èƒ½éœ€è¦ä¸åŒçš„æç¤ºã€‚</li>
<li>è‡ªåŠ¨å‘ç°æ¯ä¸ªLLMçš„æœ€ä½³æç¤ºæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è‡ªåŠ¨ä¼˜åŒ–æç¤ºçš„æ–¹æ³•ç¼ºä¹å¤šæ ·æ€§ï¼Œä¸”å®¹æ˜“é‡å¤é”™è¯¯ã€‚</li>
<li>å½“å‰æ–¹æ³•ç¼ºä¹é¢†åŸŸä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œé™åˆ¶äº†LLMåœ¨æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-924f0708c1ded040ff6e21fc6da97b3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c9b067d69a36f1ebe16765362747739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f45daeac905014e243e80390e209a514.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a026123a4e4f38a88b9fcb2160657ac3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-825e4bd9079b021d379dce14f4337dc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48584cad38fe6176811ef17e4c225802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a3c60b6dabfa5c1f39f6a4805551960.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Think-More-Hallucinate-Less-Mitigating-Hallucinations-via-Dual-Process-of-Fast-and-Slow-Thinking"><a href="#Think-More-Hallucinate-Less-Mitigating-Hallucinations-via-Dual-Process-of-Fast-and-Slow-Thinking" class="headerlink" title="Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process   of Fast and Slow Thinking"></a>Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process   of Fast and Slow Thinking</h2><p><strong>Authors:Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g. MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations of LLMs during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway for fully exploiting the internal knowledge of LLMs. To balance efficiency and quality, we introduce a hierarchical thinking system switch mechanism inspired by the dual process theory in cognitive science, which dynamically alternates between fast and slow thinking modes at both the instance and step levels, adapting to the complexity of questions and reasoning states. We conduct extensive experiments on both English and Chinese datasets and the results show that our approach significantly outperforms baseline approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´è™šæ„é—®é¢˜ã€‚å…¸å‹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•é‡‡ç”¨è‡ªå›å½’ç”Ÿæˆæ–¹å¼ï¼Œæ— éœ€åˆ»æ„æ¨ç†ï¼Œè¿™å¾€å¾€å¯¼è‡´ç”Ÿæˆä¸å¯ä¿¡çš„ã€äº‹å®é”™è¯¯çš„å›åº”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HaluSearchï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆæ ‘æœç´¢ç®—æ³•ï¼ˆå¦‚MCTSï¼‰çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ˜ç¡®çš„ç¼“æ…¢æ€è€ƒç”Ÿæˆè¿‡ç¨‹ï¼Œå‡è½»LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è™šæ„ç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼ŒHaluSearchå°†æ–‡æœ¬ç”Ÿæˆæ„å»ºä¸ºä¸€ä¸ªé€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œä½¿ç”¨è‡ªæˆ‘è¯„ä¼°å¥–åŠ±æ¨¡å‹å¯¹æ¯ä¸€æ­¥ç”Ÿæˆè¿›è¡Œè¯„åˆ†ï¼Œå¹¶å¼•å¯¼æ ‘æœç´¢èµ°å‘æœ€å¯é çš„ç”Ÿæˆè·¯å¾„ï¼Œä»¥å……åˆ†åˆ©ç”¨LLMçš„å†…éƒ¨çŸ¥è¯†ã€‚ä¸ºäº†å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè®¤çŸ¥ç§‘å­¦ä¸­çš„åŒè¿‡ç¨‹ç†è®ºçš„åˆ†å±‚æ€ç»´ç³»ç»Ÿåˆ‡æ¢æœºåˆ¶ã€‚è¯¥æœºåˆ¶åœ¨å®ä¾‹å’Œæ­¥éª¤å±‚é¢åŠ¨æ€åˆ‡æ¢å¿«é€Ÿå’Œæ…¢é€Ÿæ€è€ƒæ¨¡å¼ï¼Œä»¥é€‚åº”é—®é¢˜çš„å¤æ‚æ€§å’Œæ¨ç†çŠ¶æ€ã€‚æˆ‘ä»¬åœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01306v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMå­˜åœ¨hallucinationé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ ‘æœç´¢ç®—æ³•ï¼ˆå¦‚MCTSï¼‰çš„æ–°æ¡†æ¶HaluSearchï¼Œå®ç°æ˜¾å¼æ…¢æ€è€ƒç”Ÿæˆè¿‡ç¨‹ï¼Œä»¥å‡è½»æ¨ç†è¿‡ç¨‹ä¸­çš„hallucinationé—®é¢˜ã€‚HaluSearchå°†æ–‡æœ¬ç”Ÿæˆè§†ä¸ºé€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œåˆ©ç”¨è‡ªè¯„ä¼°å¥–åŠ±æ¨¡å‹ä¸ºæ¯ä¸€æ­¥æ‰“åˆ†ï¼Œå¼•å¯¼æ ‘æœç´¢èµ°å‘æœ€å¯é çš„ç”Ÿæˆè·¯å¾„ï¼Œå®ç°LLMå†…éƒ¨çŸ¥è¯†çš„å……åˆ†åˆ©ç”¨ã€‚ä¸ºæé«˜æ•ˆç‡å’Œç”Ÿæˆè´¨é‡ï¼Œå¼•å…¥åˆ†å±‚æ€ç»´ç³»ç»Ÿåˆ‡æ¢æœºåˆ¶ï¼Œæ ¹æ®é—®é¢˜å’Œæ¨ç†çŠ¶æ€çš„å¤æ‚æ€§åœ¨å¿«æ…¢æ€ç»´æ¨¡å¼é—´åŠ¨æ€åˆ‡æ¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´hallucinationé—®é¢˜ï¼Œéœ€è¦æ–°çš„æ¡†æ¶æ¥è§£å†³ã€‚</li>
<li>HaluSearchæ¡†æ¶ç»“åˆæ ‘æœç´¢ç®—æ³•ï¼Œå®ç°æ˜¾å¼æ…¢æ€è€ƒç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>HaluSearchå°†æ–‡æœ¬ç”Ÿæˆè§†ä¸ºé€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œåˆ©ç”¨è‡ªè¯„ä¼°å¥–åŠ±æ¨¡å‹å¼•å¯¼æ ‘æœç´¢ã€‚</li>
<li>åˆ†å±‚æ€ç»´ç³»ç»Ÿåˆ‡æ¢æœºåˆ¶æ ¹æ®é—®é¢˜å’Œæ¨ç†çŠ¶æ€çš„å¤æ‚æ€§åŠ¨æ€åˆ‡æ¢æ€ç»´æ¨¡å¼ã€‚</li>
<li>HaluSearchåœ¨å¹³è¡¡æ•ˆç‡å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒHaluSearchæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9f6ea53dd3060db674f4fab736c5a3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2fce5e566066b70484873a940b21818.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Citations-and-Trust-in-LLM-Generated-Responses"><a href="#Citations-and-Trust-in-LLM-Generated-Responses" class="headerlink" title="Citations and Trust in LLM Generated Responses"></a>Citations and Trust in LLM Generated Responses</h2><p><strong>Authors:Yifan Ding, Matthew Facciani, Amrit Poudel, Ellen Joyce, Salvador Aguinaga, Balaji Veeramani, Sanmitra Bhattacharya, Tim Weninger</strong></p>
<p>Question answering systems are rapidly advancing, but their opaque nature may impact user trust. We explored trust through an anti-monitoring framework, where trust is predicted to be correlated with presence of citations and inversely related to checking citations. We tested this hypothesis with a live question-answering experiment that presented text responses generated using a commercial Chatbot along with varying citations (zero, one, or five), both relevant and random, and recorded if participants checked the citations and their self-reported trust in the generated responses. We found a significant increase in trust when citations were present, a result that held true even when the citations were random; we also found a significant decrease in trust when participants checked the citations. These results highlight the importance of citations in enhancing trust in AI-generated content. </p>
<blockquote>
<p>é—®ç­”ç³»ç»Ÿæ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œä½†å…¶ä¸é€æ˜çš„ç‰¹æ€§å¯èƒ½å½±å“ç”¨æˆ·ä¿¡ä»»ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªåç›‘æ§æ¡†æ¶æ¥æ¢ç´¢ä¿¡ä»»ï¼Œåœ¨è¯¥æ¡†æ¶ä¸­ï¼Œä¿¡ä»»é¢„è®¡ä¸å¼•ç”¨å­˜åœ¨æ­£ç›¸å…³ï¼Œä¸å¼•ç”¨æ ¸æŸ¥æˆè´Ÿç›¸å…³ã€‚æˆ‘ä»¬é€šè¿‡å®æ—¶é—®ç­”å®éªŒæµ‹è¯•äº†è¿™ä¸€å‡è®¾ï¼Œå®éªŒä¸­å‘ˆç°äº†ä½¿ç”¨å•†ä¸šèŠå¤©æœºå™¨äººç”Ÿæˆçš„æ–‡æœ¬ç­”æ¡ˆï¼Œå¹¶é™„æœ‰ä¸åŒæ•°é‡çš„å¼•ç”¨ï¼ˆé›¶ä¸ªã€ä¸€ä¸ªæˆ–äº”ä¸ªï¼‰ï¼ŒåŒ…æ‹¬ç›¸å…³å¼•ç”¨å’Œéšæœºå¼•ç”¨ã€‚æˆ‘ä»¬è®°å½•äº†å‚ä¸è€…æ˜¯å¦æ ¸æŸ¥å¼•ç”¨ï¼Œä»¥åŠä»–ä»¬å¯¹ç”Ÿæˆç­”æ¡ˆçš„è‡ªæˆ‘æŠ¥å‘Šä¿¡ä»»ç¨‹åº¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å­˜åœ¨å¼•ç”¨æ—¶ï¼Œä¿¡ä»»åº¦æ˜¾è‘—å¢åŠ ï¼Œå³ä½¿åœ¨å¼•ç”¨æ˜¯éšæœºçš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼›æˆ‘ä»¬è¿˜å‘ç°ï¼Œå½“å‚ä¸è€…æ£€æŸ¥å¼•ç”¨æ—¶ï¼Œä¿¡ä»»åº¦æ˜¾è‘—é™ä½ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¼•ç”¨åœ¨æé«˜äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ä¿¡ä»»åº¦æ–¹é¢çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01303v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong>ï¼šé—®ç­”ç³»ç»Ÿçš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†ç”¨æˆ·ä¿¡ä»»çš„é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡åç›‘æ§æ¡†æ¶æ¢ç´¢ä¿¡ä»»ï¼Œé¢„æµ‹ä¿¡ä»»ä¸å¼•è¯çš„å­˜åœ¨æ­£ç›¸å…³ï¼Œä¸å¼•è¯æŸ¥è¯è¡Œä¸ºæˆè´Ÿç›¸å…³ã€‚é€šè¿‡å®æ—¶é—®ç­”å®éªŒéªŒè¯æ­¤å‡è®¾ï¼Œå‘ˆç°ç”±å•†ä¸šèŠå¤©æœºå™¨äººç”Ÿæˆçš„æ–‡æœ¬ç­”æ¡ˆï¼Œé™„å¸¦ä¸åŒæ•°é‡çš„å¼•è¯ï¼ˆé›¶ã€ä¸€æˆ–äº”ä¸ªï¼‰ï¼Œæ—¢æœ‰ç›¸å…³æ€§ä¹Ÿæœ‰éšæœºæ€§ã€‚å®éªŒå‘ç°å¼•è¯å¢åŠ æ˜¾è‘—æé«˜ä¿¡ä»»åº¦ï¼Œå³ä½¿å¼•è¯éšæœºä¹Ÿä¸å½±å“æ­¤ç»“æœï¼›å½“å‚ä¸è€…æŸ¥è¯å¼•è¯æ—¶ï¼Œä¿¡ä»»åº¦æ˜¾è‘—é™ä½ã€‚è¿™çªæ˜¾äº†å¼•è¯åœ¨å¢å¼ºå¯¹AIç”Ÿæˆå†…å®¹çš„ä¿¡ä»»ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é—®ç­”ç³»ç»Ÿå‘å±•å¿«é€Ÿï¼Œä½†ç”¨æˆ·ä¿¡ä»»æˆä¸ºå…³é”®é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åç›‘æ§æ¡†æ¶æ¢ç´¢ä¿¡ä»»ï¼Œå‘ç°ä¿¡ä»»ä¸å¼•è¯æ•°é‡æ­£ç›¸å…³ã€‚</li>
<li>å®æ—¶é—®ç­”å®éªŒå‘ç°ï¼Œå½“ç­”æ¡ˆé™„æœ‰å¼•è¯æ—¶ï¼Œç”¨æˆ·å¯¹AIç”Ÿæˆç­”æ¡ˆçš„ä¿¡ä»»åº¦æ˜¾è‘—å¢åŠ ã€‚</li>
<li>å³ä½¿å¼•è¯å†…å®¹éšæœºï¼Œå¯¹ä¿¡ä»»åº¦çš„æå‡ä¾ç„¶æ˜¾è‘—ã€‚</li>
<li>å½“ç”¨æˆ·æŸ¥è¯å¼•è¯æ—¶ï¼Œä»–ä»¬å¯¹AIç­”æ¡ˆçš„ä¿¡ä»»åº¦ä¼šæ˜¾è‘—é™ä½ã€‚</li>
<li>å¼•è¯çš„æ•°é‡å’Œè´¨é‡å¯¹å¢å¼ºAIç”Ÿæˆå†…å®¹çš„ä¿¡ä»»åº¦è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7bd35dfd2048d85b8e6016cd1624cb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec00339b11353b80568bd4155f4807d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db28e488202f72e0a9b8c6b8b5452730.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce05b5895a809833b035bfc7b5d14010.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de19039686a770eb1c9d599a14ded5a9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CodeElo-Benchmarking-Competition-level-Code-Generation-of-LLMs-with-Human-comparable-Elo-Ratings"><a href="#CodeElo-Benchmarking-Competition-level-Code-Generation-of-LLMs-with-Human-comparable-Elo-Ratings" class="headerlink" title="CodeElo: Benchmarking Competition-level Code Generation of LLMs with   Human-comparable Elo Ratings"></a>CodeElo: Benchmarking Competition-level Code Generation of LLMs with   Human-comparable Elo Ratings</h2><p><strong>Authors:Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, Junyang Lin</strong></p>
<p>With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CodeElo, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CodeElo benchmark is mainly based on the official CodeForces platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CodeElo, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 20 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies. </p>
<blockquote>
<p>éšç€ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›çš„å¢å¼ºï¼Œä»¥åŠå¦‚OpenAI o1å’Œo3ç­‰æ¨ç†æ¨¡å‹çš„çªç ´ï¼Œéœ€è¦å¼€å‘æ›´å…·æŒ‘æˆ˜æ€§å’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥æœ‰æ•ˆåœ°æµ‹è¯•å®ƒä»¬çš„é«˜çº§ç«èµ›çº§ç¼–ç èƒ½åŠ›ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚LiveCodeBenchå’ŒUSACOï¼Œç”±äºç¼ºå°‘ç§æœ‰æµ‹è¯•ç”¨ä¾‹ã€ä¸æ”¯æŒç‰¹è®¾è¯„å§”ä»¥åŠæ‰§è¡Œç¯å¢ƒä¸åŒ¹é…ç­‰åŸå› è€Œæ˜¾å¾—ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CodeEloï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç«èµ›çº§ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œé¦–æ¬¡æœ‰æ•ˆåœ°è§£å†³äº†æ‰€æœ‰è¿™äº›æŒ‘æˆ˜ã€‚CodeEloåŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºå®˜æ–¹çš„CodeForceså¹³å°ï¼Œå¹¶å°½å¯èƒ½ä¸å¹³å°ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬æ•´ç†äº†CodeForcesä¸Šè¿‘å…­ä¸ªæœˆçš„ç«èµ›é¢˜ç›®ï¼ŒåŒ…æ‹¬ç«èµ›åˆ†ç»„ã€é—®é¢˜éš¾åº¦è¯„çº§å’Œé—®é¢˜ç®—æ³•æ ‡ç­¾ç­‰è¯¦ç»†ä¿¡æ¯ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„è¯„åˆ¤æ–¹æ³•ï¼Œé—®é¢˜ç›´æ¥æäº¤åˆ°å¹³å°ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªå¯é çš„Eloè¯„åˆ†è®¡ç®—ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä¸å¹³å°ä¸€è‡´ï¼Œå¯ä¸äººç±»å‚èµ›è€…è¿›è¡Œæ¯”è¾ƒï¼Œä½†å…·æœ‰è¾ƒä½çš„å˜åŒ–æ€§ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„CodeEloä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬é¦–æ¬¡æä¾›äº†30ä¸ªç°æœ‰æµè¡Œå¼€æºå’Œ3ä¸ªä¸“æœ‰LLMçš„Eloè¯„åˆ†ã€‚ç»“æœè¡¨æ˜ï¼Œo1-miniå’ŒQwQ-3 é¢„ç¤ºç‰ˆæœ¬è¡¨ç°çªå‡ºï¼Œåˆ†åˆ«è·å¾—äº†Eloè¯„åˆ†è¾¾åˆ°åˆ†é«˜çš„æ•ˆæœè¾¾æé«˜è¯„ä»·å’Œä¸é”™çš„è¡¨ç°ä¸€èˆ¬ç›¸è¾ƒäºè€Œè¨€è¯„çº§å±…äºä¸­æ¸¸è¶…è¿‡å…¶ä»–æ¨¡å‹ã€‚è€Œå…¶ä»–æ¨¡å‹åœ¨è§£å†³æœ€ç®€å•çš„é—®é¢˜æ—¶éƒ½é¢ä¸´å›°éš¾ï¼Œåœ¨æ‰€æœ‰å‚èµ›äººç±»ä¸­ä½åˆ—æœ€åç™¾åˆ†ä¹‹äºŒåä»¥å†…ã€‚è¿˜è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒåˆ†æä»¥æ·±å…¥äº†è§£ä¸åŒç®—æ³•çš„æ€§èƒ½ä»¥åŠä½¿ç”¨C++å’ŒPythonä¹‹é—´çš„æ¯”è¾ƒå¯¹æ¯”æƒ…å†µå°†æä¾›æœªæ¥ç ”ç©¶çš„å»ºè®®æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01257v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¥ç›Šå¢å¼ºçš„ä»£ç æ¨ç†èƒ½åŠ›ï¼Œä»¥åŠå¦‚OpenAI o1å’Œo3ç­‰æ¨ç†æ¨¡å‹çš„çªç ´ï¼ŒäºŸéœ€å¼€å‘æ›´å…·æŒ‘æˆ˜æ€§å’Œç»¼åˆæ€§çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥æœ‰æ•ˆæµ‹è¯•å®ƒä»¬çš„é«˜çº§ç«èµ›çº§ç¼–ç¨‹èƒ½åŠ›ã€‚CodeEloåŸºå‡†æµ‹è¯•å¡«è¡¥äº†ç°æœ‰åŸºå‡†æµ‹è¯•ï¼ˆå¦‚LiveCodeBenchå’ŒUSACOï¼‰çš„ç©ºç™½ï¼Œé¦–æ¬¡æœ‰æ•ˆåœ°è§£å†³äº†è¿™äº›æŒ‘æˆ˜ã€‚CodeEloåŸºå‡†æµ‹è¯•ä¸»è¦åŸºäºå®˜æ–¹çš„CodeForceså¹³å°ï¼Œå¹¶å°½å¯èƒ½ä¸ä¹‹å¯¹é½ã€‚å®ƒé€šè¿‡æ”¶é›†è¿‘å…­ä¸ªæœˆçš„CodeForcesç«èµ›é—®é¢˜ï¼Œå¼•å…¥ç‹¬ç‰¹çš„è¯„åˆ¤æ–¹æ³•å’Œå¯é çš„Eloè¯„åˆ†è®¡ç®—ç³»ç»Ÿï¼Œä¸º30ä¸ªç°æœ‰æµè¡Œçš„å¼€æºå’Œ3ä¸ªä¸“æœ‰LLMæä¾›äº†ç¬¬ä¸€æ¬¡çš„Eloè¯„åˆ†ã€‚ç»“æœåˆæ­¥æ˜¾ç¤ºï¼Œo1-miniå’ŒQwQ-32B-Previewè¡¨ç°çªå‡ºï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™åœ¨æœ€ç®€å•çš„é—®é¢˜ä¸ŠæŒ£æ‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œéœ€è¦æ›´æŒ‘æˆ˜æ€§å’Œç»¼åˆæ€§çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å®ƒä»¬çš„ç«èµ›çº§ç¼–ç¨‹èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¦‚LiveCodeBenchå’ŒUSACOå­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ç§æœ‰æµ‹è¯•ç”¨ä¾‹ã€ç‰¹æ®Šè£åˆ¤æ”¯æŒå’Œæ‰§è¡Œç¯å¢ƒä¸åŒ¹é…ç­‰é—®é¢˜ã€‚</li>
<li>CodeEloåŸºå‡†æµ‹è¯•åŸºäºCodeForceså¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„ä¸è¶³ï¼Œæä¾›æ›´å…¨é¢ã€å¯é çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>CodeEloå¼•å…¥ç‹¬ç‰¹çš„è¯„åˆ¤æ–¹æ³•å’Œå¯é çš„Eloè¯„åˆ†è®¡ç®—ç³»ç»Ÿï¼Œä¸å¹³å°å¯¹é½å¹¶å…·æœ‰ä¸äººç±»å‚ä¸è€…å¯æ¯”æ€§ä½†å…·æœ‰è¾ƒä½æ–¹å·®ã€‚</li>
<li>CodeEloåŸºå‡†æµ‹è¯•æ”¶é›†äº†è¿‘å…­ä¸ªæœˆçš„CodeForcesç«èµ›é—®é¢˜ï¼Œä¸ºå¤šç§LLMæä¾›äº†ç¬¬ä¸€æ¬¡çš„Eloè¯„åˆ†ã€‚</li>
<li>åœ¨CodeEloæµ‹è¯•ä¸­ï¼Œo1-miniå’ŒQwQ-32B-Previewè¡¨ç°ä¼˜å¼‚ï¼Œè€Œå…¶ä»–æ¨¡å‹åœ¨ç®€å•é—®é¢˜ä¸Šè¡¨ç°æŒ£æ‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25dde1a937dabc6fecd32e533388a499.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed802ea29f28f5dbcaa915f9f54309cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e7e2006c4fdc7e006236469b9dca91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a79b6ccf23f2fdd455856b5e4e53f49.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Digital-Guardians-Can-GPT-4-Perspective-API-and-Moderation-API-reliably-detect-hate-speech-in-reader-comments-of-German-online-newspapers"><a href="#Digital-Guardians-Can-GPT-4-Perspective-API-and-Moderation-API-reliably-detect-hate-speech-in-reader-comments-of-German-online-newspapers" class="headerlink" title="Digital Guardians: Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?"></a>Digital Guardians: Can GPT-4, Perspective API, and Moderation API   reliably detect hate speech in reader comments of German online newspapers?</h2><p><strong>Authors:Manuel Weber, Moritz Huber, Maximilian Auch, Alexander DÃ¶schl, Max-Emanuel Keller, Peter Mandl</strong></p>
<p>In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsawâ€™s (Google) Perspective API, and OpenAIâ€™s Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæœ‰æ¯’å†…å®¹å’Œä»‡æ¨è¨€è®ºå·²æˆä¸ºäº’è”ç½‘ä¸Šçš„æ™®éç°è±¡ã€‚ç½‘ç»œæŠ¥çº¸å’Œè®ºå›çš„ç®¡ç†äººå‘˜ç°åœ¨éƒ¨åˆ†ç”±äºæ³•å¾‹æ¡ä¾‹çš„è¦æ±‚ï¼Œéœ€è¦ä»”ç»†å®¡æŸ¥å¹¶å¿…è¦æ—¶åˆ é™¤è¯»è€…è¯„è®ºã€‚è¿™æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹çš„æµç¨‹ã€‚ä¸€äº›å¤§å‹è¯­è¨€æ¨¡å‹æä¾›å•†å·²ç»æä¾›è‡ªåŠ¨åŒ–ä»‡æ¨è¨€è®ºæ£€æµ‹æˆ–æœ‰æ¯’å†…å®¹è¯†åˆ«çš„è§£å†³æ–¹æ¡ˆã€‚è¿™åŒ…æ‹¬OpenAIçš„GPT-4oã€Google Jigsawçš„Perspective APIå’ŒOpenAIçš„Moderation APIã€‚è¿™äº›è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºä¸“é—¨ä¸ºå¼€å‘åœ¨çº¿æŠ¥çº¸è¯»è€…è¯„è®ºä¸­æ£€æµ‹ä»‡æ¨è¨€è®ºçš„å·¥å…·è€Œåˆ›å»ºçš„å¾·å›½æµ‹è¯•æ•°æ®é›†HOCON34kè¿›è¡Œæ¯”è¾ƒçš„ã€‚æµ‹è¯•æ•°æ®é›†åŒ…å«1592ä¸ªæ³¨é‡Šæ–‡æœ¬æ ·æœ¬ã€‚å¯¹äºGPT-4oï¼Œä½¿ç”¨äº†ä¸‰ç§ä¸åŒçš„æç¤ºæ–¹æ³•ï¼Œå³é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹å’Œå‡ ä¸ªæ ·ä¾‹çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨ MCC å’Œ F2 åˆ†æ•°ç»¼åˆæŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äº Perspective API å’Œ Moderation APIï¼Œå¹¶ä¸”å¤§çº¦é«˜å‡º HOCON34k åŸºçº¿ 5 ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01256v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äº’è”ç½‘ä¸Šçš„æœ‰æ¯’å†…å®¹å’Œä»‡æ¨è¨€è®ºæ—¥ç›Šæ™®éï¼Œä¿ƒä½¿åœ¨çº¿æŠ¥çº¸å’Œè®ºå›çš„ç‰ˆä¸»éœ€ä¸¥æ ¼å®¡æŸ¥è¯»è€…è¯„è®ºï¼Œç”šè‡³å¿…è¦æ—¶è¿›è¡Œåˆ é™¤ï¼Œè¿™ä¸€è¿‡ç¨‹è€—è´¹å¤§é‡äººåŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æä¾›å•†å¦‚OpenAIçš„GPT-4oã€Googleçš„Jigsaw Perspective APIä»¥åŠOpenAIçš„Moderation APIç­‰å·²æä¾›è‡ªåŠ¨æ£€æµ‹ä»‡æ¨è¨€è®ºæˆ–æœ‰æ¯’å†…å®¹è§£å†³æ–¹æ¡ˆã€‚åŸºäºä¸“é—¨ä¸ºæ£€æµ‹åœ¨çº¿æŠ¥çº¸è¯»è€…è¯„è®ºä¸­çš„ä»‡æ¨è¨€è®ºè€Œå¼€å‘çš„å·¥å…·æ‰€é€‰æ‹©çš„å¾·å›½æµ‹è¯•æ•°æ®é›†HOCON34kï¼ŒGPT-4oé‡‡ç”¨é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹å’Œå°‘é‡æ ·æœ¬ä¸‰ç§æç¤ºæ–¹æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºGPT-4oåœ¨ç»¼åˆæŒ‡æ ‡MCCå’ŒF2åˆ†æ•°ä¸Šçš„è¡¨ç°ä¼˜äºPerspective APIå’ŒModeration APIï¼Œå¹¶é«˜å‡ºHOCON34kåŸºçº¿çº¦5ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº’è”ç½‘ä¸Šå­˜åœ¨å¹¿æ³›çš„æœ‰æ¯’å†…å®¹å’Œä»‡æ¨è¨€è®ºç°è±¡ï¼Œä¿ƒä½¿åœ¨çº¿æŠ¥çº¸å’Œè®ºå›ç‰ˆä¸»éœ€è¦ä¸¥æ ¼å®¡æŸ¥è¯»è€…è¯„è®ºã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æä¾›å•†å¦‚OpenAIå’ŒGoogleå·²ç»æä¾›äº†è‡ªåŠ¨æ£€æµ‹ä»‡æ¨è¨€è®ºæˆ–æœ‰æ¯’å†…å®¹çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åŸºäºå¾·å›½HOCON34kæµ‹è¯•æ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºGPT-4oåœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>GPT-4oé‡‡ç”¨ä¸‰ç§ä¸åŒçš„æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€ä¸€ä¸ªæ ·ä¾‹å’Œå°‘é‡æ ·æœ¬ï¼Œæ˜¾ç¤ºå‡ºå…¶çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>GPT-4oçš„ç»¼åˆæ€§èƒ½ä¼˜äºå…¶ä»–è§£å†³æ–¹æ¡ˆï¼Œå¦‚Perspective APIå’ŒModeration APIã€‚</li>
<li>GPT-4oçš„æ£€æµ‹å‡†ç¡®ç‡é«˜äºHOCON34kåŸºçº¿çº¦5ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aaca630822b957a82e7dc78d74e30601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-086d0742d835ebf47e5651f255797590.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d25177ac36b022f34582717937ab9e96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc63f769341973064072744775ee0dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d62b209ee39806a6037c91bbe20e3e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SeFAR-Semi-supervised-Fine-grained-Action-Recognition-with-Temporal-Perturbation-and-Learning-Stabilization"><a href="#SeFAR-Semi-supervised-Fine-grained-Action-Recognition-with-Temporal-Perturbation-and-Learning-Stabilization" class="headerlink" title="SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal   Perturbation and Learning Stabilization"></a>SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal   Perturbation and Learning Stabilization</h2><p><strong>Authors:Yongle Huang, Haodong Chen, Zhenbang Xu, Zihan Jia, Haozhou Sun, Dian Shao</strong></p>
<p>Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., â€œsalto backward tucked with 1 turnâ€). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher modelâ€™s predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics. </p>
<blockquote>
<p>äººç±»è¡Œä¸ºç†è§£å¯¹äºå¤šæ¨¡æ€ç³»ç»Ÿçš„è¿›æ­¥è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•ï¼Œå—åˆ°å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨åŠ¨ï¼Œæ—¨åœ¨æ¶µç›–å¹¿æ³›çš„ç±»åˆ«ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†æ›´ç‰¹å®šèƒ½åŠ›çš„éœ€æ±‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†æ›´å…·æŒ‘æˆ˜æ€§çš„ç²¾ç»†åŠ¨ä½œè¯†åˆ«ï¼ˆFARï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡å…³æ³¨æ›´çŸ­æ—¶é—´å†…çš„è¯¦ç»†è¯­ä¹‰æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œå‘åç¿»æ»šä¸€å‘¨å¹¶ä¿¯èº«â€ï¼‰ã€‚è€ƒè™‘åˆ°ç²¾ç»†æ ‡ç­¾æ ‡æ³¨çš„é«˜æˆæœ¬å’Œå¾®è°ƒLLMæ‰€éœ€çš„å¤§é‡æ•°æ®ï¼Œæˆ‘ä»¬æè®®é‡‡ç”¨åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶SeFARç»“åˆäº†å¤šç§åˆ›æ–°è®¾è®¡æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æ•è·è¶³å¤Ÿçš„è§†è§‰ç»†èŠ‚ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒå±‚æ¬¡æ—¶é—´å…ƒç´ ä½œä¸ºæ›´æœ‰æ•ˆçš„è¡¨ç¤ºï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè®¾è®¡äº†æ–°çš„å¼ºåŠ›å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡é€‚åº¦çš„æ—¶é—´æ‰°åŠ¨æ¥å‚ä¸æ•™å¸ˆ-å­¦ç”Ÿå­¦ä¹ èŒƒå¼ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†æ•™å¸ˆæ¨¡å‹å¯¹FARé¢„æµ‹çš„é«˜ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è°ƒèŠ‚æ¥ç¨³å®šå­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSeFARåœ¨ä¸¤ä¸ªç²¾ç»†åŠ¨ä½œè¯†åˆ«æ•°æ®é›†FineGymå’ŒFineDivingä¸Šï¼Œè·¨è¶Šå„ç§æ•°æ®èŒƒå›´éƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å¦å¤–ä¸¤ä¸ªç»å…¸çš„ç²—ç²’åº¦æ•°æ®é›†UCF101å’ŒHMDB51ä¸Šï¼Œå®ƒä¹Ÿä¼˜äºå…¶ä»–åŠç›‘ç£æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„SeFARæ‰€æå–çš„ç‰¹å¾å¯ä»¥æå¤§åœ°ä¿ƒè¿›å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¯¹ç²¾ç»†ç²’åº¦å’Œç‰¹å®šé¢†åŸŸçš„è¯­ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01245v1">PDF</a> AAAI 2025; Code: <a target="_blank" rel="noopener" href="https://github.com/KyleHuang9/SeFAR">https://github.com/KyleHuang9/SeFAR</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åœ¨å¤šæ¨¡æ€ç³»ç»Ÿå‘å±•ä¸­ï¼Œç²¾ç»†åŠ¨ä½œè¯†åˆ«ï¼ˆFARï¼‰çš„é‡è¦æ€§ã€‚é’ˆå¯¹ç²¾ç»†åŠ¨ä½œè¯†åˆ«æ‰€éœ€çš„å…·ä½“èƒ½åŠ›ï¼Œæå‡ºé‡‡ç”¨åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ–¹æ³•æ¥è§£å†³æ ‡æ³¨æˆæœ¬é«˜å’Œéœ€è¦å¤§é‡æ•°æ®å¾®è°ƒçš„é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºSeFARçš„æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºåŒå±‚æ¬¡æ—¶é—´å…ƒç´ ä½œä¸ºæ›´æœ‰æ•ˆçš„è¡¨ç¤ºï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„å¢å¼ºç­–ç•¥ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”è°ƒèŠ‚æ¥å¤„ç†æ•™å¸ˆæ¨¡å‹é¢„æµ‹çš„é«˜ä¸ç¡®å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒSeFARåœ¨ç²¾ç»†åŠ¨ä½œè¯†åˆ«æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨ç»å…¸ç²—ç²’åº¦æ•°æ®é›†ä¸Šä¹Ÿè¶…è¶Šäº†å…¶ä»–åŠç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSeFARæå–çš„ç‰¹å¾å¤§å¤§æé«˜äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¯¹ç²¾ç»†ç²’åº¦å’Œç‰¹å®šé¢†åŸŸçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººæœºäº¤äº’ç³»ç»Ÿéœ€è¦ç²¾ç»†åŠ¨ä½œè¯†åˆ«ï¼ˆFARï¼‰æ¥æå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€ç³»ç»Ÿä¸­ã€‚</li>
<li>æ ‡æ³¨ç²¾ç»†åŠ¨ä½œæ ‡ç­¾çš„æˆæœ¬é«˜æ˜‚ä¸”éœ€è¦å¤§é‡æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„SeFARæ¡†æ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>SeFARé€šè¿‡æ„å»ºåŒå±‚æ¬¡æ—¶é—´å…ƒç´ æ›´æœ‰æ•ˆåœ°æ•æ‰è§†è§‰ç»†èŠ‚ï¼Œå¹¶è®¾è®¡äº†åˆ›æ–°çš„å¢å¼ºç­–ç•¥ã€‚</li>
<li>SeFARé‡‡ç”¨è‡ªé€‚åº”è°ƒèŠ‚æ¥å¤„ç†æ•™å¸ˆæ¨¡å‹é¢„æµ‹çš„é«˜ä¸ç¡®å®šæ€§ï¼Œç¡®ä¿å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚</li>
<li>SeFARåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¶…è¶Šäº†å…¶ä»–åŠç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6522f6586dfd976f0d20e472d3173418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cb9d4ff01038b15f3dc3a17f223bdb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ffcbfaab27324aeea1ce3059e3c929d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cd50d06889371377d121114a2b5287e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-984dbab771f5909c8fdd386689d05070.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bb51146a53b3da499e03f7d53746d73.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Face-Human-Bench-A-Comprehensive-Benchmark-of-Face-and-Human-Understanding-for-Multi-modal-Assistants"><a href="#Face-Human-Bench-A-Comprehensive-Benchmark-of-Face-and-Human-Understanding-for-Multi-modal-Assistants" class="headerlink" title="Face-Human-Bench: A Comprehensive Benchmark of Face and Human   Understanding for Multi-modal Assistants"></a>Face-Human-Bench: A Comprehensive Benchmark of Face and Human   Understanding for Multi-modal Assistants</h2><p><strong>Authors:Lixiong Qin, Shilong Ou, Miaoxuan Zhang, Jiangning Wei, Yuhang Zhang, Xiaoshuai Song, Yuchen Liu, Mei Wang, Weiran Xu</strong></p>
<p>Faces and humans are crucial elements in social interaction and are widely included in everyday photos and videos. Therefore, a deep understanding of faces and humans will enable multi-modal assistants to achieve improved response quality and broadened application scope. Currently, the multi-modal assistant community lacks a comprehensive and scientific evaluation of face and human understanding abilities. In this paper, we first propose a hierarchical ability taxonomy that includes three levels of abilities. Then, based on this taxonomy, we collect images and annotations from publicly available datasets in the face and human community and build a semi-automatic data pipeline to produce problems for the new benchmark. Finally, the obtained Face-Human-Bench comprises a development set with 900 problems and a test set with 1800 problems, supporting both English and Chinese. We conduct evaluations over 25 mainstream multi-modal large language models (MLLMs) with our Face-Human-Bench, focusing on the correlation between abilities, the impact of the relative position of targets on performance, and the impact of Chain of Thought (CoT) prompting on performance. Moreover, inspired by multi-modal agents, we also explore which abilities of MLLMs need to be supplemented by specialist models. </p>
<blockquote>
<p>é¢å­”å’Œäººç±»æ˜¯ç¤¾ä¼šäº¤äº’ä¸­çš„é‡è¦å…ƒç´ ï¼Œå¹¿æ³›å­˜åœ¨äºæ—¥å¸¸ç…§ç‰‡å’Œè§†é¢‘ä¸­ã€‚å› æ­¤ï¼Œå¯¹é¢å­”å’Œäººç±»çš„æ·±åˆ»ç†è§£èƒ½ä½¿å¤šæ¨¡æ€åŠ©ç†æé«˜å“åº”è´¨é‡å’Œæ‰©å¤§åº”ç”¨èŒƒå›´ã€‚ç›®å‰ï¼Œå¤šæ¨¡æ€åŠ©ç†ç¤¾åŒºç¼ºä¹å¯¹é¢å­”å’Œäººç±»ç†è§£èƒ½åŠ›çš„å…¨é¢ç§‘å­¦è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ä¸ªåˆ†å±‚çš„èƒ½åŠ›åˆ†ç±»ä½“ç³»ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå±‚æ¬¡çš„èƒ½åŠ›ã€‚ç„¶åï¼ŒåŸºäºè¿™ä¸€åˆ†ç±»ä½“ç³»ï¼Œæˆ‘ä»¬ä»é¢å­”å’Œäººç±»ç¤¾åŒºå¯å…¬å¼€è®¿é—®çš„æ•°æ®é›†ä¸­æ”¶é›†å›¾åƒå’Œæ³¨é‡Šï¼Œå¹¶å»ºç«‹åŠè‡ªåŠ¨æ•°æ®ç®¡é“æ¥ç”Ÿæˆæ–°é—®é¢˜ï¼Œä»¥æ„å»ºæ–°çš„åŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œæ‰€è·å¾—çš„Face-Human-BenchåŒ…æ‹¬ä¸€ä¸ªåŒ…å«900ä¸ªé—®é¢˜çš„å¼€å‘é›†å’Œä¸€ä¸ªåŒ…å«1800ä¸ªé—®é¢˜çš„æµ‹è¯•é›†ï¼Œæ”¯æŒè‹±è¯­å’Œä¸­æ–‡ã€‚æˆ‘ä»¬ä½¿ç”¨Face-Human-Benchå¯¹25ä¸ªä¸»æµçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œäº†è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨èƒ½åŠ›ä¹‹é—´çš„ç›¸å…³æ€§ã€ç›®æ ‡ç›¸å¯¹ä½ç½®å¯¹æ€§èƒ½çš„å½±å“ã€ä»¥åŠæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¯¹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œå—åˆ°å¤šæ¨¡æ€ä»£ç†çš„å¯å‘ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†MLLMséœ€è¦å“ªäº›èƒ½åŠ›éœ€è¦ç”±ä¸“ä¸šæ¨¡å‹æ¥è¡¥å……ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01243v1">PDF</a> 50 pages, 14 figures, 41 tables. Submitted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé¢å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„Face-Human-BenchåŸºå‡†æµ‹è¯•é›†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹é¢éƒ¨ä¸äººç±»ç†è§£èƒ½åŠ›çš„å…¨é¢ç§‘å­¦è¯„ä¼°ã€‚è¯¥æµ‹è¯•é›†åŒ…å«ä¸‰ä¸ªçº§åˆ«çš„èƒ½åŠ›åˆ†ç±»ï¼Œé‡‡ç”¨æ¥è‡ªé¢éƒ¨ä¸äººç±»é¢†åŸŸçš„å…¬å¼€å¯ç”¨æ•°æ®é›†çš„å›¾ç‰‡å’Œæ³¨é‡Šï¼Œæ„å»ºäº†ä¸€ä¸ªåŠè‡ªåŠ¨çš„æ•°æ®ç®¡é“æ¥ç”Ÿæˆæµ‹è¯•é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒæ¨¡å‹èƒ½åŠ›ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç›®æ ‡ç›¸å¯¹ä½ç½®å¯¹æ€§èƒ½çš„å½±å“ä»¥åŠé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯¹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†MLLMséœ€è¦å“ªäº›èƒ½åŠ›éœ€è¦ç”±ä¸“ä¸šæ¨¡å‹è¿›è¡Œè¡¥å……ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨å’Œäººä½“ç†è§£åœ¨å¤šæ¨¡æ€äº¤äº’ä¸­éå¸¸é‡è¦ï¼Œèƒ½å¤Ÿæå‡æ™ºèƒ½åŠ©ç†çš„ååº”è´¨é‡å’Œæ‰©å¤§åº”ç”¨èŒƒå›´ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€åŠ©ç†ç¤¾åŒºç¼ºä¹å¯¹é¢éƒ¨å’Œäººç±»ç†è§£èƒ½åŠ›çš„å…¨é¢ç§‘å­¦è¯„ä¼°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå±‚æ¬¡åŒ–çš„èƒ½åŠ›åˆ†ç±»ä½“ç³»ï¼ŒåŒ…å«ä¸‰ä¸ªçº§åˆ«çš„èƒ½åŠ›ã€‚</li>
<li>åŸºäºè¯¥åˆ†ç±»ä½“ç³»ï¼Œæ”¶é›†å’Œæ„å»ºäº†Face-Human-BenchåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«å¼€å‘é›†å’Œæµ‹è¯•é›†ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†25ä¸ªä¸»æµå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„èƒ½åŠ›ï¼Œæ¢è®¨äº†èƒ½åŠ›é—´çš„ç›¸å…³æ€§ä»¥åŠç›®æ ‡ç›¸å¯¹ä½ç½®å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼ŒMLLMsåœ¨æŸäº›èƒ½åŠ›ä¸Šéœ€è¦ç”±ä¸“ä¸šæ¨¡å‹è¿›è¡Œè¡¥å……ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3325b90117fd6b80000c2a47f3322de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8489b1a3737cf9cd28fe00cef00666ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f32659448322bc964f4caff37b3e1ac4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ccd96b1b57ae9396e80ce38425f2e17.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Automated-Self-Refinement-and-Self-Correction-for-LLM-based-Product-Attribute-Value-Extraction"><a href="#Automated-Self-Refinement-and-Self-Correction-for-LLM-based-Product-Attribute-Value-Extraction" class="headerlink" title="Automated Self-Refinement and Self-Correction for LLM-based Product   Attribute Value Extraction"></a>Automated Self-Refinement and Self-Correction for LLM-based Product   Attribute Value Extraction</h2><p><strong>Authors:Alexander Brinkmann, Christian Bizer</strong></p>
<p>Structured product data, in the form of attribute-value pairs, is essential for e-commerce platforms to support features such as faceted product search and attribute-based product comparison. However, vendors often provide unstructured product descriptions, making attribute value extraction necessary to ensure data consistency and usability. Large language models (LLMs) have demonstrated their potential for product attribute value extraction in few-shot scenarios. Recent research has shown that self-refinement techniques can improve the performance of LLMs on tasks such as code generation and text-to-SQL translation. For other tasks, the application of these techniques has resulted in increased costs due to processing additional tokens, without achieving any improvement in performance. This paper investigates applying two self-refinement techniques, error-based prompt rewriting and self-correction, to the product attribute value extraction task. The self-refinement techniques are evaluated across zero-shot, few-shot in-context learning, and fine-tuning scenarios using GPT-4o. The experiments show that both self-refinement techniques have only a marginal impact on the modelâ€™s performance across the different scenarios, while significantly increasing processing costs. For scenarios with training data, fine-tuning yields the highest performance, while the ramp-up costs of fine-tuning are balanced out as the amount of product descriptions increases. </p>
<blockquote>
<p>ç»“æ„åŒ–äº§å“æ•°æ®ä»¥å±æ€§-å€¼å¯¹çš„å½¢å¼å­˜åœ¨ï¼Œå¯¹äºç”µå­å•†åŠ¡å¹³å°æ”¯æŒé¢å‘æ–¹é¢çš„äº§å“æœç´¢å’ŒåŸºäºå±æ€§çš„äº§å“æ¯”è¾ƒç­‰åŠŸèƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¾›åº”å•†é€šå¸¸æä¾›éç»“æ„åŒ–çš„äº§å“æè¿°ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå±æ€§å€¼æå–ä»¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°‘é‡åœºæ™¯æ¼”ç¤ºäº†å…¶åœ¨äº§å“å±æ€§å€¼æå–æ–¹é¢çš„æ½œåŠ›ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œè‡ªæˆ‘å®Œå–„æŠ€æœ¯å¯ä»¥æé«˜ä»£ç ç”Ÿæˆå’Œæ–‡æœ¬åˆ°SQLç¿»è¯‘ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚å¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œè¿™äº›æŠ€æœ¯çš„åº”ç”¨ç”±äºå¤„ç†é¢å¤–çš„ä»¤ç‰Œè€Œå¢åŠ äº†æˆæœ¬ï¼Œè€Œæ²¡æœ‰å®ç°æ€§èƒ½ä¸Šçš„ä»»ä½•æ”¹è¿›ã€‚æœ¬æ–‡ç ”ç©¶äº†å°†ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯â€”â€”åŸºäºé”™è¯¯çš„æç¤ºé‡å†™å’Œè‡ªæˆ‘æ ¡æ­£åº”ç”¨äºäº§å“å±æ€§å€¼æå–ä»»åŠ¡ã€‚è¿™äº›è‡ªæˆ‘å®Œå–„æŠ€æœ¯åœ¨é›¶æ ·æœ¬ã€å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¾®è°ƒåœºæ™¯ä¸­ï¼Œä½¿ç”¨GPT-4oè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯å¯¹æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½å½±å“å¾®ä¹å…¶å¾®ï¼Œä½†æ˜¾è‘—å¢åŠ äº†å¤„ç†æˆæœ¬ã€‚å¯¹äºæœ‰è®­ç»ƒæ•°æ®çš„åœºæ™¯ï¼Œå¾®è°ƒä¼šäº§ç”Ÿæœ€é«˜çš„æ€§èƒ½ï¼Œè€Œéšç€äº§å“æè¿°æ•°é‡çš„å¢åŠ ï¼Œå¾®è°ƒçš„ä¸€æ¬¡æ€§æˆæœ¬å¾—åˆ°äº†å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01237v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç»“æ„åŒ–äº§å“æ•°æ®ä»¥å±æ€§-å€¼å¯¹çš„å½¢å¼å¯¹äºç”µå­å•†åŠ¡å¹³å°è‡³å…³é‡è¦ï¼Œæ”¯æŒé¢å‘å±æ€§æœç´¢å’Œå±æ€§æ¯”è¾ƒç­‰åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºä¾›åº”å•†æä¾›çš„äº§å“æè¿°å¤šä¸ºéç»“æ„åŒ–å½¢å¼ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå±æ€§å€¼æå–ä»¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å°‘æ•°åœºæ™¯ä¸­å±•ç°å‡ºå…¶åœ¨äº§å“å±æ€§å€¼æå–æ–¹é¢çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯â€”â€”åŸºäºé”™è¯¯çš„æç¤ºé‡å†™å’Œè‡ªæˆ‘ä¿®æ­£â€”â€”åœ¨äº§å“å±æ€§å€¼æå–ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚åœ¨é›¶æ ·æœ¬ã€å°‘æ•°åœºæ™¯ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¾®è°ƒåœºæ™¯ä¸‹ä½¿ç”¨GPT-4oå¯¹è¿™ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯è¿›è¡Œè¯„ä¼°ï¼Œå®éªŒè¡¨æ˜è¿™äº›æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æœ‰é™ä¸”æé«˜äº†å¤„ç†æˆæœ¬ã€‚å­˜åœ¨è®­ç»ƒæ•°æ®æ—¶ï¼Œå¾®è°ƒå¯è·å¾—æœ€ä½³æ€§èƒ½ã€‚éšç€äº§å“æè¿°æ•°é‡çš„å¢åŠ ï¼Œå¾®è°ƒçš„åˆå§‹æˆæœ¬å¾—åˆ°å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç»“æ„åŒ–äº§å“æ•°æ®å¯¹äºç”µå­å•†åŠ¡å¹³å°è‡³å…³é‡è¦ï¼Œæ”¯æŒå¦‚é¢å‘å±æ€§æœç´¢å’Œå±æ€§æ¯”è¾ƒç­‰åŠŸèƒ½ã€‚</li>
<li>ä¾›åº”å•†æä¾›çš„äº§å“æè¿°å¤šä¸ºéç»“æ„åŒ–å½¢å¼ï¼Œéœ€è¦è¿›è¡Œå±æ€§å€¼æå–ä»¥ç¡®ä¿æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯ç”¨æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äº§å“å±æ€§å€¼æå–æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ä¸­æ¢è®¨äº†ä¸¤ç§è‡ªæˆ‘å®Œå–„æŠ€æœ¯ï¼šåŸºäºé”™è¯¯çš„æç¤ºé‡å†™å’Œè‡ªæˆ‘ä¿®æ­£åº”ç”¨äºäº§å“å±æ€§å€¼æå–ä»»åŠ¡ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬ã€å°‘æ•°åœºæ™¯å’Œå¾®è°ƒåœºæ™¯ä¸‹è¯„ä¼°è¿™ä¸¤ç§æŠ€æœ¯æ—¶ï¼Œå‘ç°å®ƒä»¬å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æœ‰é™ä¸”æé«˜äº†å¤„ç†æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca257fd6ccff008cd29a3d7256d6af3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fb4a830a3fdd565da5e5a0e972a787f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80261884b7bcfd3392fb548a1804e25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07935791a01f786198363bd22d8e4f83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d18c4cdd794f04b0f197b82f03e163f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c406788329ed3ccacb4163d5129b30.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HetGCoT-Rec-Heterogeneous-Graph-Enhanced-Chain-of-Thought-LLM-Reasoning-for-Journal-Recommendation"><a href="#HetGCoT-Rec-Heterogeneous-Graph-Enhanced-Chain-of-Thought-LLM-Reasoning-for-Journal-Recommendation" class="headerlink" title="HetGCoT-Rec: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning   for Journal Recommendation"></a>HetGCoT-Rec: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning   for Journal Recommendation</h2><p><strong>Authors:Runsong Jia, Mengjia Wu, Ying Ding, Jie Lu, Yi Zhang</strong></p>
<p>Academic journal recommendation requires effectively combining structural understanding of scholarly networks with interpretable recommendations. While graph neural networks (GNNs) and large language models (LLMs) excel in their respective domains, current approaches often fail to achieve true integration at the reasoning level. We propose HetGCoT-Rec, a framework that deeply integrates heterogeneous graph transformer with LLMs through chain-of-thought reasoning. Our framework features two key technical innovations: (1) a structure-aware mechanism that transforms heterogeneous graph neural network learned subgraph information into natural language contexts, utilizing predefined metapaths to capture academic relationships, and (2) a multi-step reasoning strategy that systematically embeds graph-derived contexts into the LLMâ€™s stage-wise reasoning process. Experiments on a dataset collected from OpenAlex demonstrate that our approach significantly outperforms baseline methods, achieving 96.48% Hit rate and 92.21% H@1 accuracy. Furthermore, we validate the frameworkâ€™s adaptability across different LLM architectures, showing consistent improvements in both recommendation accuracy and explanation quality. Our work demonstrates an effective approach for combining graph-structured reasoning with language models for interpretable academic venue recommendations. </p>
<blockquote>
<p>å­¦æœ¯æœŸåˆŠæ¨èéœ€è¦ç»“åˆå¯¹å­¦æœ¯ç½‘ç»œçš„æ·±åˆ»ç†è§£å’Œå¯è§£é‡Šçš„æ¨èã€‚è™½ç„¶å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„è‡ªçš„é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†å½“å‰çš„æ–¹æ³•å¾€å¾€æ— æ³•çœŸæ­£å®ç°åœ¨æ¨ç†å±‚é¢çš„æ•´åˆã€‚æˆ‘ä»¬æå‡ºHetGCoT-Recæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ€ç»´é“¾æ¨ç†ï¼Œå°†å¼‚æ„å›¾è½¬æ¢å™¨ä¸LLMsæ·±åº¦é›†æˆã€‚æˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šï¼ˆ1ï¼‰ä¸€ç§ç»“æ„æ„ŸçŸ¥æœºåˆ¶ï¼Œå®ƒå°†å¼‚æ„å›¾ç¥ç»ç½‘ç»œå­¦ä¹ çš„å­å›¾ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€ä¸Šä¸‹æ–‡ï¼Œå¹¶åˆ©ç”¨é¢„å®šä¹‰çš„å…ƒè·¯å¾„æ¥æ•æ‰å­¦æœ¯å…³ç³»ï¼›ï¼ˆ2ï¼‰ä¸€ç§å¤šæ­¥éª¤æ¨ç†ç­–ç•¥ï¼Œå®ƒç³»ç»Ÿåœ°åµŒå…¥å›¾æ´¾ç”Ÿçš„ä¸Šä¸‹æ–‡åˆ°LLMçš„é˜¶æ®µæ¨ç†è¿‡ç¨‹ä¸­ã€‚åœ¨OpenAlexæ”¶é›†çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†96.48%çš„å‘½ä¸­ç‡å’Œ92.21%çš„H@1å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†æ¡†æ¶åœ¨ä¸åŒLLMæ¶æ„ä¸­çš„é€‚åº”æ€§ï¼Œåœ¨æ¨èå‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡æ–¹é¢å‡æ˜¾ç¤ºå‡ºæŒç»­çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†å°†å›¾ç»“æ„æ¨ç†ä¸è¯­è¨€æ¨¡å‹ç›¸ç»“åˆè¿›è¡Œå¯è§£é‡Šçš„å­¦æœ¯æœŸåˆŠæ¨èçš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01203v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§ç»“åˆå¼‚è´¨å›¾ç¥ç»ç½‘ç»œä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å­¦æœ¯æ¨èæ¡†æ¶HetGCoT-Recã€‚è¯¥æ¡†æ¶é€šè¿‡é“¾å¼æ€ç»´æ¨ç†å®ç°æ·±åº¦æ•´åˆï¼Œå…·æœ‰ç»“æ„æ„ŸçŸ¥æœºåˆ¶å’Œå¤šæ­¥æ¨ç†ç­–ç•¥ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨OpenAlexæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå®ç°é«˜å‘½ä¸­ç‡å’Œé«˜å‡†ç¡®åº¦ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶é€‚åº”ä¸åŒçš„LLMæ¶æ„ï¼Œåœ¨æé«˜æ¨èå‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡æ–¹é¢è¡¨ç°ä¸€è‡´ã€‚ç ”ç©¶å±•ç¤ºäº†ç»“åˆå›¾ç»“æ„æ¨ç†å’Œè¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºå¯è§£é‡Šçš„å­¦æœ¯åœºæ‰€æ¨èæä¾›äº†æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HetGCoT-Recæ¡†æ¶ç»“åˆäº†å¼‚è´¨å›¾ç¥ç»ç½‘ç»œå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°å­¦æœ¯æ¨èçš„ç³»ç»Ÿæ€§æ•´åˆã€‚</li>
<li>ç»“æ„æ„ŸçŸ¥æœºåˆ¶èƒ½å¤Ÿå°†å¼‚è´¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ çš„å­å›¾ä¿¡æ¯è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ä¸Šä¸‹æ–‡ã€‚</li>
<li>é€šè¿‡é¢„å®šä¹‰çš„å…ƒè·¯å¾„æ•è·å­¦æœ¯å…³ç³»ã€‚</li>
<li>å¤šæ­¥æ¨ç†ç­–ç•¥å°†å›¾è¡ç”Ÿä¸Šä¸‹æ–‡ç³»ç»Ÿåœ°åµŒå…¥LLMçš„é˜¶æ®µæ€§æ¨ç†è¿‡ç¨‹ä¸­ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒHetGCoT-Recåœ¨OpenAlexæ•°æ®é›†ä¸Šçš„æ¨èæ•ˆæœä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰é«˜å‘½ä¸­ç‡å’Œå‡†ç¡®åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚åº”ä¸åŒçš„LLMæ¶æ„ï¼Œå¹¶åœ¨æ¨èå‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡æ–¹é¢è¡¨ç°ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0ec7f7c736f3e35d0357fee3b2931a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4baac5c4a12ddedc02b3e1ce1253b025.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fddf0093b86ea3a3c1efc2c27ae2dbbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e24b8f2809260e26b70319decb000ed.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Interactive-Deepfake-Analysis"><a href="#Towards-Interactive-Deepfake-Analysis" class="headerlink" title="Towards Interactive Deepfake Analysis"></a>Towards Interactive Deepfake Analysis</h2><p><strong>Authors:Lixiong Qin, Ning Jiang, Yang Zhang, Yuhan Qiu, Dingheng Zeng, Jiani Hu, Weihong Deng</strong></p>
<p>Existing deepfake analysis methods are primarily based on discriminative models, which significantly limit their application scenarios. This paper aims to explore interactive deepfake analysis by performing instruction tuning on multi-modal large language models (MLLMs). This will face challenges such as the lack of datasets and benchmarks, and low training efficiency. To address these issues, we introduce (1) a GPT-assisted data construction process resulting in an instruction-following dataset called DFA-Instruct, (2) a benchmark named DFA-Bench, designed to comprehensively evaluate the capabilities of MLLMs in deepfake detection, deepfake classification, and artifact description, and (3) construct an interactive deepfake analysis system called DFA-GPT, as a strong baseline for the community, with the Low-Rank Adaptation (LoRA) module. The dataset and code will be made available at <a target="_blank" rel="noopener" href="https://github.com/lxq1000/DFA-Instruct">https://github.com/lxq1000/DFA-Instruct</a> to facilitate further research. </p>
<blockquote>
<p>ç°æœ‰çš„æ·±åº¦ä¼ªé€ åˆ†ææ–¹æ³•ä¸»è¦åŸºäºåˆ¤åˆ«æ¨¡å‹ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æŒ‡ä»¤å¾®è°ƒï¼Œæ¢ç´¢äº¤äº’å¼æ·±åº¦ä¼ªé€ åˆ†æã€‚è¿™å°†é¢ä¸´æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ç¼ºä¹ä»¥åŠè®­ç»ƒæ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ï¼ˆ1ï¼‰ä¸€ç§GPTè¾…åŠ©çš„æ•°æ®æ„å»ºè¿‡ç¨‹ï¼Œäº§ç”Ÿäº†åä¸ºDFA-Instructçš„æŒ‡ä»¤éµå¾ªæ•°æ®é›†ï¼Œï¼ˆ2ï¼‰ä¸€ä¸ªåä¸ºDFA-Benchçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ã€æ·±åº¦ä¼ªé€ åˆ†ç±»å’Œä¼ªé€ ç‰©æè¿°æ–¹é¢çš„èƒ½åŠ›ï¼Œï¼ˆ3ï¼‰æ„å»ºä¸€ä¸ªåä¸ºDFA-GPTçš„äº¤äº’å¼æ·±åº¦ä¼ªé€ åˆ†æç³»ç»Ÿï¼Œä½œä¸ºç¤¾åŒºçš„å¼ºå¤§åŸºçº¿ï¼Œå¹¶é…å¤‡äº†ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¨¡å—ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lxq1000/DFA-Instruct%E4%B8%8A%E6%8F%90%E4%BE%9B%EF%BC%8C%E4%BB%A5%E4%BE%BF%E8%BF%9B%E8%A1%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/lxq1000/DFA-Instructä¸Šæä¾›ï¼Œä»¥ä¾¿è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01164v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå½“å‰æ·±åº¦ä¼ªé€ åˆ†ææ–¹æ³•çš„å±€é™æ€§ï¼Œæœ¬æ–‡æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æŒ‡ä»¤å¾®è°ƒï¼Œæ¢ç´¢äº¤äº’å¼æ·±åº¦ä¼ªé€ åˆ†æã€‚ä¸ºè§£å†³ç¼ºä¹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€è®­ç»ƒæ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†GPTè¾…åŠ©æ•°æ®æ„å»ºè¿‡ç¨‹ï¼Œåˆ›å»ºäº†æŒ‡ä»¤éµå¾ªæ•°æ®é›†DFA-Instructï¼Œè®¾è®¡äº†å…¨é¢è¯„ä¼°MLLMsåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ã€åˆ†ç±»å’Œä¼ªé€ ç‰©æè¿°èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•DFA-Benchï¼Œå¹¶æ„å»ºäº†äº¤äº’å¼æ·±åº¦ä¼ªé€ åˆ†æç³»ç»ŸDFA-GPTï¼Œä½œä¸ºç¤¾åŒºçš„å¼ºå¤§åŸºçº¿ï¼Œé…å¤‡Low-Rank Adaptationï¼ˆLoRAï¼‰æ¨¡å—ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lxq1000/DFA-Instruct">https://github.com/lxq1000/DFA-Instruct</a>ä¸Šæä¾›ï¼Œä»¥æ–¹ä¾¿è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ·±åº¦ä¼ªé€ åˆ†ææ–¹æ³•ä¸»è¦åŸºäºåˆ¤åˆ«æ¨¡å‹ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒï¼Œæ¢ç´¢äº¤äº’å¼æ·±åº¦ä¼ªé€ åˆ†æã€‚</li>
<li>é¢ä¸´ç¼ºä¹æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€è®­ç»ƒæ•ˆç‡ä½çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†GPTè¾…åŠ©æ•°æ®æ„å»ºè¿‡ç¨‹ï¼Œåˆ›å»ºäº†æŒ‡ä»¤éµå¾ªæ•°æ®é›†DFA-Instructã€‚</li>
<li>è®¾è®¡äº†å…¨é¢è¯„ä¼°MLLMsåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ã€åˆ†ç±»å’Œä¼ªé€ ç‰©æè¿°èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•DFA-Benchã€‚</li>
<li>æ„å»ºäº†äº¤äº’å¼æ·±åº¦ä¼ªé€ åˆ†æç³»ç»ŸDFA-GPTï¼Œä½œä¸ºç¤¾åŒºç ”ç©¶çš„åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9570ef0304639545d6be8e988eb82daf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4abdf9b6ffb80d296f426a408708608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84cbc57eae2e3ef70670a5dae3a1fafa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-245bf3e46fd7807ba7e82d96aa72d0c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6709bd6ded869bc0c876103b2853df2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="2-OLMo-2-Furious"><a href="#2-OLMo-2-Furious" class="headerlink" title="2 OLMo 2 Furious"></a>2 OLMo 2 Furious</h2><p><strong>Authors:Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi</strong></p>
<p>We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from T&quot;ulu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly â€“ models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†OLMo 2ï¼Œè¿™æ˜¯æˆ‘ä»¬å®Œå…¨å¼€æ”¾å¼è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä»£äº§å“ã€‚OLMo 2åŒ…æ‹¬å…·æœ‰æ”¹è¿›æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆçš„å¯†é›†è‡ªå›å½’æ¨¡å‹ã€é¢„è®­ç»ƒæ•°æ®æ··åˆä»¥åŠæŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ”¹è¿›æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆå®ç°äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§å’Œæ¯æ ‡è®°ç¬¦çš„æ•ˆç‡æå‡ã€‚æˆ‘ä»¬æ›´æ–°çš„é¢„è®­ç»ƒæ•°æ®æ··åˆå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸“é—¨æ•°æ®æ··åˆç§°ä¸ºDolmino Mix 1124ï¼Œé€šè¿‡åæœŸè¯¾ç¨‹è®­ç»ƒï¼ˆå³åœ¨é¢„è®­ç»ƒçš„é€€ç«é˜¶æ®µå¼•å…¥ä¸“é—¨æ•°æ®ï¼‰å¼•å…¥æ—¶ï¼Œå®ƒèƒ½åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜æ¨¡å‹èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆäº†Tulu 3çš„æœ€ä½³å®è·µæ¥å¼€å‘OLMo 2-Instructï¼Œä¸“æ³¨äºè®¸å¯æ•°æ®ï¼Œå¹¶æ‰©å±•æˆ‘ä»¬æœ€ç»ˆé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ã€‚æˆ‘ä»¬çš„OLMo 2åŸºç¡€æ¨¡å‹ä½äºæ€§èƒ½è®¡ç®—å‰æ²¿ï¼Œé€šå¸¸ä¸Llama 3.1å’ŒQwen 2.5ç­‰å…¬å¼€æƒé‡æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„æµ®ç‚¹è¿ç®—ï¼Œå¹¶å…·æœ‰å®Œå…¨é€æ˜çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œæ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å®Œå…¨å¼€æ”¾å¼OLMo 2-Instructæ¨¡å‹ä¸ç›¸åŒè§„æ¨¡çš„å…¬å¼€æƒé‡æ¨¡å‹ç«äº‰æˆ–è¡¨ç°æ›´ä½³ï¼ŒåŒ…æ‹¬Qwen 2.5ã€Llama 3.1å’ŒGemma 2ã€‚æˆ‘ä»¬å…¬å¼€æ‰€æœ‰OLMo 2åˆ¶å“â€”â€”è§„æ¨¡ä¸º7Bå’Œ13Bçš„æ¨¡å‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’ŒåæœŸè®­ç»ƒã€å®Œæ•´çš„è®­ç»ƒæ•°æ®ã€è®­ç»ƒä»£ç å’Œæ–¹æ¡ˆã€è®­ç»ƒæ—¥å¿—ä»¥åŠæ•°åƒä¸ªä¸­é—´æ£€æŸ¥ç‚¹ã€‚æœ€ç»ˆæŒ‡ä»¤æ¨¡å‹å¯åœ¨Ai2æ¸¸ä¹åœºä½œä¸ºå…è´¹ç ”ç©¶æ¼”ç¤ºä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00656v1">PDF</a> Model demo available at playground.allenai.org</p>
<p><strong>æ‘˜è¦</strong></p>
<p>OLMo 2ä½œä¸ºå…¨æ–°ä¸€ä»£å¼€æºè¯­è¨€æ¨¡å‹å‘å¸ƒï¼Œå…·æœ‰æ”¹è¿›åçš„æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆã€é¢„è®­ç»ƒæ•°æ®æ··åˆä»¥åŠæŒ‡ä»¤è°ƒæ•´é…æ–¹ã€‚æ–°æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆæé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œæ¯æ ‡è®°çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚æ›´æ–°çš„é¢„è®­ç»ƒæ•°æ®æ··åˆå¼•å…¥äº†åä¸ºDolmino Mix 1124çš„ä¸“é—¨æ•°æ®æ··åˆï¼Œé€šè¿‡æ™šæœŸè¯¾ç¨‹è®­ç»ƒï¼ˆå³åœ¨é¢„è®­ç»ƒçš„é€€ç«é˜¶æ®µå¼•å…¥ç‰¹å®šæ•°æ®ï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹èƒ½åŠ›ã€‚æœ€åï¼Œç»“åˆå›¾å¢3çš„æœ€ä½³å®è·µï¼Œå¼€å‘OLMo 2-Instructï¼Œä¸“æ³¨äºè®¸å¯æ•°æ®ï¼Œå¹¶åˆ©ç”¨å¯éªŒè¯å¥–åŠ±æ‰©å±•æœ€ç»ˆé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ ã€‚OLMo 2åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æ€§èƒ½ä¸Šå¤„äºå‰æ²¿ï¼Œå¾€å¾€èƒ½åŒ¹é…æˆ–è¶…è¶Šå¦‚Llama 3.1å’ŒQwen 2.5ç­‰ä»…å¼€æºæƒé‡æ¨¡å‹ï¼ŒåŒæ—¶å‡å°‘æµ®ç‚¹è¿ç®—ä½¿ç”¨é‡å¹¶å…¬å¼€é€æ˜åŒ–è®­ç»ƒæ•°æ®ã€ä»£ç å’Œé…æ–¹ã€‚å®Œå…¨å¼€æºçš„OLMo 2-Instructæ¨¡å‹åœ¨è§„æ¨¡ç›¸å½“çš„æƒ…å†µä¸‹ä¸Qwen 2.5ã€Llama 3.1å’ŒGemma 2ç­‰æ¨¡å‹ç«äº‰ï¼Œå¹¶è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬å…¬å¼€æ‰€æœ‰OLMo 2å·¥ä»¶â€”â€”åŒ…æ‹¬è§„æ¨¡ä¸º7Bå’Œ13Bçš„é¢„è®­ç»ƒå’Œåç»­è®­ç»ƒæ¨¡å‹ã€å®Œæ•´çš„è®­ç»ƒæ•°æ®ã€è®­ç»ƒä»£ç å’Œé…æ–¹ã€è®­ç»ƒæ—¥å¿—ä»¥åŠæ•°åƒä¸ªä¸­é—´æ£€æŸ¥ç‚¹ã€‚æœ€ç»ˆæŒ‡ä»¤æ¨¡å‹å¯åœ¨Ai2 Playgroundä¸Šä½œä¸ºå…è´¹ç ”ç©¶æ¼”ç¤ºä½¿ç”¨ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>OLMo 2æ˜¯æ–°ä¸€ä»£å¼€æºè¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰æ”¹è¿›çš„æ¶æ„å’Œè®­ç»ƒæ–¹æ¡ˆã€‚</li>
<li>å¼•å…¥æ–°çš„é¢„è®­ç»ƒæ•°æ®æ··åˆæ–¹æ³•Dolmino Mix 1124ï¼Œé€šè¿‡æ™šæœŸè¯¾ç¨‹è®­ç»ƒæé«˜æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>ç»“åˆå›¾å¢3çš„æœ€ä½³å®è·µï¼Œå¼€å‘OLMo 2-Instructæ¨¡å‹ï¼Œæ³¨é‡è®¸å¯æ•°æ®çš„ä½¿ç”¨å’Œå¼ºåŒ–å­¦ä¹ çš„å¯éªŒè¯å¥–åŠ±ã€‚</li>
<li>OLMo 2åŸºç¡€æ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œä¸å…¶ä»–å¼€æºæ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>å®Œå…¨å¼€æºçš„OLMo 2-Instructæ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œå¯åœ¨è§„æ¨¡ç›¸å½“çš„æƒ…å†µä¸‹ä¸å…¶ä»–ä¼˜ç§€æ¨¡å‹ç›¸åª²ç¾ã€‚</li>
<li>æ‰€æœ‰OLMo 2ç›¸å…³å·¥ä»¶å‡å…¬å¼€å‘å¸ƒï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æ•°æ®ã€ä»£ç å’Œæ—¥å¿—ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ced40acd685b78d49feacc9c6836bb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eecd1b8965668375805584fc960edf38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfa0d0b8d24471c154832ccf65a3059d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3596cc659d937eb50356bad4a4a3e28.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Online-Video-Understanding-A-Comprehensive-Benchmark-and-Memory-Augmented-Method"><a href="#Online-Video-Understanding-A-Comprehensive-Benchmark-and-Memory-Augmented-Method" class="headerlink" title="Online Video Understanding: A Comprehensive Benchmark and   Memory-Augmented Method"></a>Online Video Understanding: A Comprehensive Benchmark and   Memory-Augmented Method</h2><p><strong>Authors:Zhenpeng Huang, Xinhao Li, Jiaqi Li, Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, Limin Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown significant progress in offline video understanding. However, applying these models to real-world scenarios, such as autonomous driving and human-computer interaction, presents unique challenges due to the need for real-time processing of continuous online video streams. To this end, this paper presents systematic efforts from three perspectives: evaluation benchmark, model architecture, and training strategy. First, we introduce OVBench, a comprehensive question-answering benchmark specifically designed to evaluate modelsâ€™ ability to perceive, memorize, and reason within online video contexts. It features six core task types across three temporal contexts-past, present, and future-forming 16 subtasks from diverse datasets. Second, we propose a new Pyramid Memory Bank (PMB) that effectively retains key spatiotemporal information in video streams. Third, we proposed an offline-to-online learning paradigm, designing an interleaved dialogue format for online video data and constructing an instruction-tuning dataset tailored for online video training. This framework led to the development of VideoChat-Online, a robust and efficient model for online video understanding. Despite the lower computational cost and higher efficiency, VideoChat-Online outperforms existing state-of-the-art offline and online models across popular offline video benchmarks and OVBench, demonstrating the effectiveness of our model architecture and training strategy. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰ç°å®åœºæ™¯ï¼Œç”±äºéœ€è¦å¯¹è¿ç»­çš„åœ¨çº¿è§†é¢‘æµè¿›è¡Œå®æ—¶å¤„ç†ï¼Œé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»è¯„ä¼°åŸºå‡†ã€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ä¸‰ä¸ªæ–¹é¢è¿›è¡Œäº†ç³»ç»Ÿçš„åŠªåŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†OVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨åœ¨çº¿è§†é¢‘ä¸Šä¸‹æ–‡ä¸­çš„æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›çš„ç»¼åˆé—®ç­”åŸºå‡†ã€‚å®ƒæ¶µç›–äº†è¿‡å»ã€ç°åœ¨å’Œæœªæ¥çš„ä¸‰ä¸ªæ—¶é—´ä¸Šä¸‹æ–‡ä¸­çš„å…­ç§æ ¸å¿ƒä»»åŠ¡ç±»å‹ï¼Œå½¢æˆäº†16ä¸ªå­ä»»åŠ¡ï¼Œæ¥æºäºå„ç§æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é‡‘å­—å¡”è®°å¿†åº“ï¼ˆPMBï¼‰ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°ä¿ç•™è§†é¢‘æµä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä»ç¦»çº¿åˆ°åœ¨çº¿çš„å­¦ä¹ èŒƒå¼ï¼Œè®¾è®¡äº†é€‚åˆåœ¨çº¿è§†é¢‘æ•°æ®çš„äº¤äº’å¼å¯¹è¯æ ¼å¼ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹åœ¨çº¿è§†é¢‘è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚è¿™ä¸€æ¡†æ¶å‚¬ç”Ÿäº†VideoChat-Onlineçš„è¯ç”Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåœ¨çº¿è§†é¢‘ç†è§£çš„ç¨³å¥è€Œé«˜æ•ˆçš„æ¨¡å‹ã€‚å°½ç®¡è®¡ç®—æˆæœ¬è¾ƒä½ï¼Œæ•ˆç‡è¾ƒé«˜ï¼ŒVideoChat-Onlineåœ¨æµè¡Œçš„ç¦»çº¿è§†é¢‘åŸºå‡†æµ‹è¯•å’ŒOVBenchä¸Šçš„è¡¨ç°éƒ½ä¼˜äºç°æœ‰çš„å…ˆè¿›ç¦»çº¿å’Œåœ¨çº¿æ¨¡å‹ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00584v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢çš„æ˜¾è‘—è¿›æ­¥ï¼Œæœ¬æ–‡å›´ç»•åœ¨çº¿è§†é¢‘æµå®æ—¶å¤„ç†çš„éœ€æ±‚ï¼Œä»è¯„ä¼°åŸºå‡†ã€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ä¸‰ä¸ªæ–¹é¢è¿›è¡Œäº†ç³»ç»ŸåŠªåŠ›ã€‚ä»‹ç»äº†ä¸“é—¨ä¸ºåœ¨çº¿è§†é¢‘è¯­å¢ƒè®¾è®¡çš„é—®ç­”è¯„ä¼°åŸºå‡†OVBenchï¼Œæå‡ºäº†æœ‰æ•ˆä¿ç•™è§†é¢‘æµä¸­å…³é”®æ—¶ç©ºä¿¡æ¯çš„Pyramid Memory Bankï¼ˆPMBï¼‰ï¼Œä»¥åŠé€‚ç”¨äºåœ¨çº¿è§†é¢‘æ•°æ®çš„ç¦»çº¿åˆ°åœ¨çº¿å­¦ä¹ èŒƒå¼ã€‚è¿™äº›æˆæœæœ€ç»ˆä¿ƒæˆäº†VideoChat-Onlineæ¨¡å‹çš„è¯ç”Ÿï¼Œè¯¥æ¨¡å‹åœ¨ç¦»çº¿è§†é¢‘åŸºå‡†æµ‹è¯•å’ŒOVBenchä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸”è®¡ç®—æˆæœ¬ä½ã€æ•ˆç‡é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†è¿ç»­åœ¨çº¿è§†é¢‘æµæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ–°çš„é—®ç­”è¯„ä¼°åŸºå‡†OVBenchï¼Œæ¶µç›–åœ¨çº¿è§†é¢‘è¯­å¢ƒä¸­çš„æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>æå‡ºPyramid Memory Bankï¼ˆPMBï¼‰ï¼Œæœ‰æ•ˆä¿ç•™è§†é¢‘æµä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨ç¦»çº¿åˆ°åœ¨çº¿çš„å­¦ä¹ èŒƒå¼ï¼Œé€‚åº”åœ¨çº¿è§†é¢‘æ•°æ®ï¼Œæ„å»ºé’ˆå¯¹æ€§çš„è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>VideoChat-Onlineæ¨¡å‹å…·å¤‡ç¨³å¥æ€§å’Œé«˜æ•ˆæ€§ï¼Œé€‚ç”¨äºåœ¨çº¿è§†é¢‘ç†è§£ã€‚</li>
<li>VideoChat-Onlineåœ¨ç¦»çº¿è§†é¢‘åŸºå‡†æµ‹è¯•å’ŒOVBenchä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºç°æœ‰ä¸»æµæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d7f5cd06c024a29fb8283746dfa26fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1b29c27c78a436a35cc57b029445a95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d697ec4492a3e53a8455410a9a81a27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bc8326e30d4c1b6e5a2b3924400283c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1868ba8ecb7cd154f1c2b11cd04c978a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VisTabNet-Adapting-Vision-Transformers-for-Tabular-Data"><a href="#VisTabNet-Adapting-Vision-Transformers-for-Tabular-Data" class="headerlink" title="VisTabNet: Adapting Vision Transformers for Tabular Data"></a>VisTabNet: Adapting Vision Transformers for Tabular Data</h2><p><strong>Authors:Witold WydmaÅ„ski, Ulvi Movsum-zada, Jacek Tabor, Marek Åšmieja</strong></p>
<p>Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet â€“ a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNetâ€™s superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è§‚å¯Ÿåˆ°åœ¨è¡¨æ ¼æ•°æ®çš„æƒ…å†µä¸‹æœ‰ç›¸åº”çš„æ”¹è¿›ï¼Œè€Œè¡¨æ ¼æ•°æ®ä»ç„¶æ˜¯ç”Ÿç‰©ã€å·¥ä¸šå’Œè´¢åŠ¡åº”ç”¨ä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°åœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šå®šä¹‰çš„ä¸‹æ¸¸ä»»åŠ¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisTabNetâ€”â€”ä¸€ç§è·¨æ¨¡æ€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå…è®¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡çš„Vision Transformerï¼ˆViTï¼‰æ¥å¤„ç†è¡¨æ ¼æ•°æ®ã€‚é€šè¿‡å°†è¡¨æ ¼è¾“å…¥æŠ•å½±åˆ°ViTå¯æ¥å—çš„è¡¥ä¸åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†é¢„è®­ç»ƒçš„Transformerç¼–ç å™¨åº”ç”¨äºè¡¨æ ¼è¾“å…¥ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†ä¸ºå¤„ç†è¡¨æ ¼æ•°æ®è€Œè®¾è®¡åˆé€‚æ¶æ„çš„æ¦‚å¿µæˆæœ¬ï¼ŒåŒæ—¶é™ä½äº†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æˆæœ¬ã€‚åœ¨å¤šä¸ªå°å‹è¡¨æ ¼æ•°æ®é›†ï¼ˆå°‘äº1kä¸ªæ ·æœ¬ï¼‰ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†VisTabNetçš„ä¼˜è¶Šæ€§ï¼Œå®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„é›†æˆæ–¹æ³•å’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„è¿ç§»å­¦ä¹ å®è·µï¼Œè¡¨æ˜é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹å¯ä»¥è½¬ç§»åˆ°è§£å†³è¡¨æ ¼é—®é¢˜ï¼Œæ‰©å±•äº†è¿ç§»å­¦ä¹ çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00057v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±å­¸æ¨¡å‹åœ¨è‡ªç„¶èªè¨€è™•ç†å’Œè¨ˆç®—æ©Ÿè¦–è¦ºæ–¹é¢å–å¾—å·¨å¤§æˆåŠŸï¼Œä½†åœ¨è™•ç†è¡¨æ ¼æ•¸æ“šæ™‚ä¸¦æœªå‡ºç¾é¡ä¼¼é€²å±•ï¼Œè€Œè¡¨æ ¼æ•¸æ“šä»åœ¨ç”Ÿç‰©ã€å·¥ä¸šå’Œé‡‘èæ‡‰ç”¨ä¸­æ˜¯æœ€å¸¸ä½¿ç”¨çš„æ•¸æ“šé¡å‹ã€‚æœ¬æ–‡æå‡ºVisTabNetâ€”â€”ä¸€ç¨®è·¨æ¨¡æ…‹è½‰ç§»å­¸ç¿’æ–¹æ³•ï¼Œå¯å°‡é è¨“ç·´çš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹é©æ‡‰æ–¼è™•ç†è¡¨æ ¼æ•¸æ“šã€‚é€šéå°‡è¡¨æ ¼è¼¸å…¥æŠ•å½±è‡³é©æ‡‰ViTçš„patch embeddingsï¼Œå¯ç›´æ¥å°‡é è¨“ç·´çš„Transformer Encoderæ‡‰ç”¨æ–¼è¡¨æ ¼æ•¸æ“šã€‚æ­¤åšæ³•å…é™¤äº†ç‚ºè™•ç†è¡¨æ ¼æ•¸æ“šè€Œè¨­è¨ˆé©æ‡‰æ€§çµæ§‹çš„æ¦‚å¿µæˆæœ¬ï¼Œä¸¦é™ä½äº†å¾é›¶é–‹å§‹è¨“ç·´æ¨¡å‹çš„è¨ˆç®—æˆæœ¬ã€‚åœ¨æ•¸å€‹å°å‹è¡¨æ ¼æ•¸æ“šé›†ï¼ˆæ¨£æœ¬æ•¸å°‘æ–¼1kï¼‰ä¸Šçš„å¯¦é©—çµæœé¡¯ç¤ºVisTabNetçš„å„ªè¶Šæ€§ï¼Œè¶…è¶Šäº†å‚³çµ±é›†æˆæ–¹æ³•å’Œæœ€æ–°çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ã€‚æ­¤æ–¹æ³•çªç ´äº†å‚³çµ±è½‰ç§»å­¸ç¿’çš„ç¯„å¼ï¼Œè­‰æ˜é è¨“ç·´çš„åœ–åƒæ¨¡å‹å¯è½‰ç§»è‡³è§£æ±ºè¡¨æ ¼å•é¡Œï¼Œæ“´å±•äº†è½‰ç§»å­¸ç¿’çš„ç•Œé™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¸ç¿’æ¨¡å‹åœ¨è‡ªç„¶èªè¨€è™•ç†å’Œè¨ˆç®—æ©Ÿè¦–è¦ºæ–¹é¢è¡¨ç¾å‡ºè‰²ï¼Œä½†åœ¨è™•ç†è¡¨æ ¼æ•¸æ“šæ™‚æ”¹å–„æœ‰é™ã€‚</li>
<li>è¡¨æ ¼æ•¸æ“šåœ¨ç”Ÿç‰©ã€å·¥ä¸šå’Œé‡‘èæ‡‰ç”¨ä¸­å»£ç‚ºä½¿ç”¨ã€‚</li>
<li>VisTabNetæå‡ºä¸€ç¨®è·¨æ¨¡æ…‹è½‰ç§»å­¸ç¿’æ–¹æ³•ï¼Œå°‡Vision Transformerï¼ˆViTï¼‰æ‡‰ç”¨æ–¼è™•ç†è¡¨æ ¼æ•¸æ“šã€‚</li>
<li>VisTabNeté€šéå°‡è¡¨æ ¼è¼¸å…¥è½‰æ›ç‚ºpatch embeddingsï¼Œé©æ‡‰é è¨“ç·´çš„Transformer Encoderã€‚</li>
<li>æ­¤æ–¹æ³•ç°¡åŒ–äº†ç‚ºè™•ç†è¡¨æ ¼æ•¸æ“šè€Œè¨­è¨ˆç‰¹å®šçµæ§‹çš„éœ€æ±‚ï¼Œä¸¦é™ä½äº†æ¨¡å‹è¨“ç·´æˆæœ¬ã€‚</li>
<li>åœ¨å°å‹è¡¨æ ¼æ•¸æ“šé›†ä¸Šçš„å¯¦é©—é¡¯ç¤ºVisTabNetå„ªæ–¼å‚³çµ±é›†æˆæ–¹æ³•å’Œæ·±åº¦å­¸ç¿’æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-24853b348e03c2a9c96763fbd6009bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc50826b315dba46ad09f6156ce03c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8564a37b131aa26cb615c31330f9143e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f1aa26cb833b8d8dc5bfb417adf754a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  A3 Android Agent Arena for Mobile GUI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-7ed0c5e2f414879892079cedd6e44bb8.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-05  RLAIF-V Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
