<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  nnY-Net Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9216ff3b6696c7df669a6c8783190fc4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-06-æ›´æ–°"><a href="#2025-01-06-æ›´æ–°" class="headerlink" title="2025-01-06 æ›´æ–°"></a>2025-01-06 æ›´æ–°</h1><h2 id="nnY-Net-Swin-NeXt-with-Cross-Attention-for-3D-Medical-Images-Segmentation"><a href="#nnY-Net-Swin-NeXt-with-Cross-Attention-for-3D-Medical-Images-Segmentation" class="headerlink" title="nnY-Net: Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation"></a>nnY-Net: Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation</h2><p><strong>Authors:Haixu Liu, Zerui Tao, Wenzhen Dong, Qiuzhuang Sun</strong></p>
<p>This paper provides a novel 3D medical image segmentation model structure called nnY-Net. This name comes from the fact that our model adds a cross-attention module at the bottom of the U-net structure to form a Y structure. We integrate the advantages of the two latest SOTA models, MedNeXt and SwinUNETR, and use Swin Transformer as the encoder and ConvNeXt as the decoder to innovatively design the Swin-NeXt structure. Our model uses the lowest-level feature map of the encoder as Key and Value and uses patient features such as pathology and treatment information as Query to calculate the attention weights in a Cross Attention module. Moreover, we simplify some pre- and post-processing as well as data enhancement methods in 3D image segmentation based on the dynUnet and nnU-net frameworks. We integrate our proposed Swin-NeXt with Cross-Attention framework into this framework. Last, we construct a DiceFocalCELoss to improve the training efficiency for the uneven data convergence of voxel classification. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„3DåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ç»“æ„ï¼Œåä¸ºnnY-Netã€‚è¿™ä¸ªåå­—çš„ç”±æ¥æ˜¯å› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹åœ¨U-netç»“æ„çš„åº•éƒ¨æ·»åŠ äº†ä¸€ä¸ªäº¤å‰æ³¨æ„æ¨¡å—ï¼Œå½¢æˆYå½¢ç»“æ„ã€‚æˆ‘ä»¬æ•´åˆäº†æœ€æ–°ä¸¤ä¸ªé¡¶å°–æ¨¡å‹MedNeXtå’ŒSwinUNETRçš„ä¼˜ç‚¹ï¼Œåˆ›æ–°åœ°è®¾è®¡äº†Swin-NeXtç»“æ„ï¼Œä½¿ç”¨Swin Transformerä½œä¸ºç¼–ç å™¨ï¼ŒConvNeXtä½œä¸ºè§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç¼–ç å™¨çš„æœ€ä½çº§ç‰¹å¾å›¾ä½œä¸ºé”®å’Œå€¼ï¼Œä»¥æ‚£è€…ç‰¹å¾ï¼ˆå¦‚ç—…ç†å’Œæ²»ç–—ä¿¡æ¯ï¼‰ä½œä¸ºæŸ¥è¯¢ï¼Œåœ¨äº¤å‰æ³¨æ„æ¨¡å—ä¸­è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç®€åŒ–äº†åŸºäºdynUnetå’ŒnnU-netæ¡†æ¶çš„3Då›¾åƒåˆ†å‰²çš„é¢„å¤„ç†å’Œåå¤„ç†ä»¥åŠæ•°æ®å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬æå‡ºçš„å¸¦æœ‰äº¤å‰æ³¨æ„æ¡†æ¶çš„Swin-NeXté›†æˆåˆ°è¿™ä¸€æ¡†æ¶ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬æ„å»ºäº†DiceFocalCELossï¼Œä»¥æé«˜voxelåˆ†ç±»æ•°æ®ä¸å‡åŒ€æ—¶çš„è®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01406v1">PDF</a> MICCAI</p>
<p><strong>Summary</strong></p>
<p>åŸºäºU-netç»“æ„çš„æ–°å‹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹nnY-Netè¢«æå‡ºã€‚è¯¥æ¨¡å‹ç»“åˆäº†MedNeXtå’ŒSwinUNETRä¸¤ç§æœ€æ–°é¡¶å°–æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œé‡‡ç”¨Swin Transformerä½œä¸ºç¼–ç å™¨ï¼ŒConvNeXtä½œä¸ºè§£ç å™¨ï¼Œåˆ›æ–°è®¾è®¡äº†Swin-NeXtç»“æ„ã€‚æ¨¡å‹ä½¿ç”¨ç¼–ç å™¨æœ€ä½çº§åˆ«çš„ç‰¹å¾æ˜ å°„ä½œä¸ºé”®å’Œå€¼ï¼Œå¹¶åˆ©ç”¨æ‚£è€…ç‰¹å¾ï¼ˆå¦‚ç—…ç†å’Œæ²»ç–—ä¿¡æ¯ï¼‰ä½œä¸ºæŸ¥è¯¢æ¥è®¡ç®—äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­çš„æ³¨æ„åŠ›æƒé‡ã€‚æ­¤å¤–ï¼ŒåŸºäºdynUnetå’ŒnnU-netæ¡†æ¶ç®€åŒ–äº†éƒ¨åˆ†é¢„å¤„ç†å’Œåå¤„ç†ä»¥åŠæ•°æ®å¢å¼ºæ–¹æ³•ã€‚æœ€åæ„å»ºäº†DiceFocalCELossä»¥æé«˜æ•°æ®ä¸å‡è¡¡æ”¶æ•›æ—¶çš„è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnY-Netæ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼ŒåŸºäºU-netç»“æ„ï¼Œç»“åˆäº†MedNeXtå’ŒSwinUNETRçš„ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹åˆ›æ–°æ€§åœ°ç»“åˆäº†Swin Transformerç¼–ç å™¨å’ŒConvNeXtè§£ç å™¨ï¼Œæ„æˆSwin-NeXtç»“æ„ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨æ‚£è€…ç‰¹å¾ï¼ˆå¦‚ç—…ç†å’Œæ²»ç–—ä¿¡æ¯ï¼‰è®¡ç®—æ³¨æ„åŠ›æƒé‡ã€‚</li>
<li>è¯¥æ¨¡å‹ç®€åŒ–äº†é¢„å¤„ç†å’Œåå¤„ç†æµç¨‹ä»¥åŠæ•°æ®å¢å¼ºæ–¹æ³•ï¼ŒåŸºäºdynUnetå’ŒnnU-netæ¡†æ¶ã€‚</li>
<li>nnY-Netæ¡†æ¶ä¸­é›†æˆäº†Swin-NeXtä¸äº¤å‰æ³¨æ„åŠ›æ¡†æ¶ã€‚</li>
<li>æ¨¡å‹æ„å»ºäº†DiceFocalCELossï¼Œæ—¨åœ¨æé«˜æ•°æ®ä¸å‡è¡¡æ”¶æ•›æ—¶çš„è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cad70830084c6f01ba4c437c28fcab55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b53b193fe59428c708cd27185f8424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05340efecda79319b2b16eb40ac73c91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e961fc0ec9f3afa0888d01a904d260d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e809ede714ad84f20d613635db0a7709.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a31eff7d86229a1acb993402c8e5ee2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a79a0d9c7fa82d7ece5a772ce996feab.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ProjectedEx-Enhancing-Generation-in-Explainable-AI-for-Prostate-Cancer"><a href="#ProjectedEx-Enhancing-Generation-in-Explainable-AI-for-Prostate-Cancer" class="headerlink" title="ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer"></a>ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer</h2><p><strong>Authors:Xuyin Qi, Zeyu Zhang, Aaron Berliano Handoko, Huazhan Zheng, Mingxi Chen, Ta Duc Huy, Vu Minh Hieu Phan, Lei Zhang, Linqi Cheng, Shiyu Jiang, Zhiwei Zhang, Zhibin Liao, Yang Zhao, Minh-Son To</strong></p>
<p>Prostate cancer, a growing global health concern, necessitates precise diagnostic tools, with Magnetic Resonance Imaging (MRI) offering high-resolution soft tissue imaging that significantly enhances diagnostic accuracy. Recent advancements in explainable AI and representation learning have significantly improved prostate cancer diagnosis by enabling automated and precise lesion classification. However, existing explainable AI methods, particularly those based on frameworks like generative adversarial networks (GANs), are predominantly developed for natural image generation, and their application to medical imaging often leads to suboptimal performance due to the unique characteristics and complexity of medical image. To address these challenges, our paper introduces three key contributions. First, we propose ProjectedEx, a generative framework that provides interpretable, multi-attribute explanations, effectively linking medical image features to classifier decisions. Second, we enhance the encoder module by incorporating feature pyramids, which enables multiscale feedback to refine the latent space and improves the quality of generated explanations. Additionally, we conduct comprehensive experiments on both the generator and classifier, demonstrating the clinical relevance and effectiveness of ProjectedEx in enhancing interpretability and supporting the adoption of AI in medical settings. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/Richardqiyi/ProjectedEx">https://github.com/Richardqiyi/ProjectedEx</a> </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œæ˜¯ä¸€ä¸ªæ—¥ç›Šä¸¥é‡çš„å…¨çƒå¥åº·é—®é¢˜ï¼Œéœ€è¦ç²¾ç¡®çš„è¯Šæ–­å·¥å…·ã€‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æä¾›äº†é«˜åˆ†è¾¨ç‡çš„è½¯ç»„ç»‡æˆåƒï¼Œæ˜¾è‘—æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚æœ€è¿‘ï¼Œåœ¨å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½å’Œè¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„è¿›å±•å·²ç»é€šè¿‡å®ç°è‡ªåŠ¨åŒ–å’Œç²¾ç¡®çš„ç—…å˜åˆ†ç±»æ˜¾è‘—æ”¹å–„äº†å‰åˆ—è…ºç™Œçš„è¯Šæ–­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç­‰æ¡†æ¶çš„æ–¹æ³•ï¼Œä¸»è¦æ˜¯ä¸ºè‡ªç„¶å›¾åƒç”Ÿæˆè€Œå¼€å‘çš„ï¼Œç”±äºå…¶ç‹¬ç‰¹çš„ç‰¹ç‚¹å’ŒåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ï¼Œå®ƒä»¬åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨å¾€å¾€å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„è®ºæ–‡æå‡ºäº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ProjectedExï¼Œä¸€ä¸ªç”Ÿæˆæ¡†æ¶ï¼Œæä¾›å¯è§£é‡Šçš„å¤šå±æ€§è§£é‡Šï¼Œæœ‰æ•ˆåœ°å°†åŒ»å­¦å›¾åƒç‰¹å¾ä¸åˆ†ç±»å™¨å†³ç­–è”ç³»èµ·æ¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡èå…¥ç‰¹å¾é‡‘å­—å¡”å¢å¼ºäº†ç¼–ç å™¨æ¨¡å—ï¼Œè¿™å¯ç”¨äº†å¤šå°ºåº¦åé¦ˆæ¥ä¼˜åŒ–æ½œåœ¨ç©ºé—´å¹¶æé«˜äº†ç”Ÿæˆè§£é‡Šçš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ç”Ÿæˆå™¨å’Œåˆ†ç±»å™¨éƒ½è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè¯æ˜äº†ProjectedExåœ¨ä¸´åºŠç›¸å…³æ€§å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„ä½œç”¨ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å¹¶æ”¯æŒåœ¨åŒ»ç–—ç¯å¢ƒä¸­é‡‡ç”¨äººå·¥æ™ºèƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Richardqiyi/ProjectedEx%E5%85%AC%E5%B8%83%E3%80%82">https://github.com/Richardqiyi/ProjectedExå…¬å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01392v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æå‡ºäº†ä¸€ç§é’ˆå¯¹å‰åˆ—è…ºç™Œè¯Šæ–­çš„æ–°å‹è§£é‡Šæ€§äººå·¥æ™ºèƒ½æ¡†æ¶â€”â€”ProjectedExã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç£å…±æŒ¯æˆåƒæŠ€æœ¯å’Œè§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼Œèƒ½æœ‰æ•ˆé“¾æ¥åŒ»å­¦å›¾åƒç‰¹å¾ä¸åˆ†ç±»å™¨å†³ç­–ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥é¡¹ç›®é€šè¿‡èå…¥ç‰¹å¾é‡‘å­—å¡”å¢å¼ºç¼–ç å™¨æ¨¡å—ï¼Œæ”¹å–„äº†æ½œåœ¨ç©ºé—´çš„è´¨é‡ï¼Œæé«˜äº†ç”Ÿæˆçš„è§£é‡Šè´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒProjectedExåœ¨ä¸´åºŠç›¸å…³æ€§å’Œæé«˜äººå·¥æ™ºèƒ½è§£é‡Šæ€§æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰åˆ—è…ºç™Œæˆä¸ºå…¨çƒå¥åº·éš¾é¢˜ï¼Œéœ€è¦ç²¾ç¡®çš„è¯Šæ–­å·¥å…·ã€‚</li>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æŠ€æœ¯ä¸ºå‰åˆ—è…ºç™Œè¯Šæ–­æä¾›äº†é«˜åˆ†è¾¨ç‡è½¯ç»„ç»‡æˆåƒï¼Œæé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>äººå·¥æ™ºèƒ½å’Œè¡¨ç¤ºå­¦ä¹ çš„å‘å±•æ”¹å–„äº†å‰åˆ—è…ºç™Œçš„è‡ªåŠ¨å’Œç²¾ç¡®ç—…ç¶åˆ†ç±»ã€‚</li>
<li>ç°æœ‰çš„äººå·¥æ™ºèƒ½è§£é‡Šæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–¹æ³•ï¼Œåœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨å› åŒ»å­¦å›¾åƒçš„ç‹¬ç‰¹ç‰¹å¾å’Œå¤æ‚æ€§è€Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>ProjectedExæ˜¯ä¸€ä¸ªæ–°çš„è§£é‡Šæ€§äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œæœ‰æ•ˆé“¾æ¥åŒ»å­¦å›¾åƒç‰¹å¾å’Œåˆ†ç±»å™¨å†³ç­–ã€‚</li>
<li>é€šè¿‡èå…¥ç‰¹å¾é‡‘å­—å¡”å¢å¼ºç¼–ç å™¨æ¨¡å—ï¼ŒProjectedExæ”¹å–„äº†æ½œåœ¨ç©ºé—´çš„è´¨é‡ï¼Œæé«˜äº†ç”Ÿæˆçš„è§£é‡Šè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1858110555e0087fdc962b6bfa74404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb119cd92e1a31df669336472e554cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef4c3c7590662467a8c707cec07cf414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c203503ff0dbe71c81dc3e15610778c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e43db92f4d70b52fe56e663d19e37284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfbab2b7132b7f188418f81a6bb8d1eb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generalized-Task-Driven-Medical-Image-Quality-Enhancement-with-Gradient-Promotion"><a href="#Generalized-Task-Driven-Medical-Image-Quality-Enhancement-with-Gradient-Promotion" class="headerlink" title="Generalized Task-Driven Medical Image Quality Enhancement with Gradient   Promotion"></a>Generalized Task-Driven Medical Image Quality Enhancement with Gradient   Promotion</h2><p><strong>Authors:Dong Zhang, Kwang-Ting Cheng</strong></p>
<p>Thanks to the recent achievements in task-driven image quality enhancement (IQE) models like ESTR, the image enhancement model and the visual recognition model can mutually enhance each otherâ€™s quantitation while producing high-quality processed images that are perceivable by our human vision systems. However, existing task-driven IQE models tend to overlook an underlying fact â€“ different levels of vision tasks have varying and sometimes conflicting requirements of image features. To address this problem, this paper proposes a generalized gradient promotion (GradProm) training strategy for task-driven IQE of medical images. Specifically, we partition a task-driven IQE system into two sub-models, i.e., a mainstream model for image enhancement and an auxiliary model for visual recognition. During training, GradProm updates only parameters of the image enhancement model using gradients of the visual recognition model and the image enhancement model, but only when gradients of these two sub-models are aligned in the same direction, which is measured by their cosine similarity. In case gradients of these two sub-models are not in the same direction, GradProm only uses the gradient of the image enhancement model to update its parameters. Theoretically, we have proved that the optimization direction of the image enhancement model will not be biased by the auxiliary visual recognition model under the implementation of GradProm. Empirically, extensive experimental results on four public yet challenging medical image datasets demonstrated the superior performance of GradProm over existing state-of-the-art methods. </p>
<blockquote>
<p>å¾—ç›Šäºæœ€è¿‘ä»»åŠ¡é©±åŠ¨å›¾åƒè´¨é‡å¢å¼ºï¼ˆIQEï¼‰æ¨¡å‹ï¼ˆå¦‚ESTRï¼‰çš„æˆå°±ï¼Œå›¾åƒå¢å¼ºæ¨¡å‹å’Œè§†è§‰è¯†åˆ«æ¨¡å‹å¯ä»¥ç›¸äº’ä¿ƒè¿›å½¼æ­¤çš„é‡åŒ–ï¼ŒåŒæ—¶ç”Ÿæˆé«˜è´¨é‡çš„å¤„ç†å›¾åƒï¼Œè¿™äº›å›¾åƒå¯ä»¥è¢«æˆ‘ä»¬çš„è§†è§‰ç³»ç»Ÿæ‰€æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»»åŠ¡é©±åŠ¨IQEæ¨¡å‹å¾€å¾€å¿½è§†äº†ä¸€ä¸ªåŸºæœ¬äº‹å®â€”â€”ä¸åŒå±‚æ¬¡çš„è§†è§‰ä»»åŠ¡å¯¹å›¾åƒç‰¹å¾çš„è¦æ±‚å„ä¸ç›¸åŒï¼Œæœ‰æ—¶ç”šè‡³å­˜åœ¨å†²çªã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒä»»åŠ¡é©±åŠ¨çš„IQEçš„å¹¿ä¹‰æ¢¯åº¦æå‡ï¼ˆGradPromï¼‰è®­ç»ƒç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡é©±åŠ¨çš„IQEç³»ç»Ÿåˆ’åˆ†ä¸ºä¸¤ä¸ªå­æ¨¡å‹ï¼Œå³ç”¨äºå›¾åƒå¢å¼ºçš„ä¸»æµæ¨¡å‹å’Œç”¨äºè§†è§‰è¯†åˆ«çš„è¾…åŠ©æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGradPromä»…ä½¿ç”¨è§†è§‰è¯†åˆ«æ¨¡å‹å’Œå›¾åƒå¢å¼ºæ¨¡å‹çš„æ¢¯åº¦æ¥æ›´æ–°å›¾åƒå¢å¼ºæ¨¡å‹çš„å‚æ•°ï¼Œä½†ä»…åœ¨ä¸¤ä¸ªå­æ¨¡å‹çš„æ¢¯åº¦æ–¹å‘ç›¸åŒæ—¶æ‰è¿›è¡Œæ›´æ–°ï¼Œè¿™é€šè¿‡å®ƒä»¬çš„ä½™å¼¦ç›¸ä¼¼æ€§æ¥è¡¡é‡ã€‚å¦‚æœè¿™ä¸¤ä¸ªå­æ¨¡å‹çš„æ¢¯åº¦æ–¹å‘ä¸ä¸€è‡´ï¼ŒGradPromåªä½¿ç”¨å›¾åƒå¢å¼ºæ¨¡å‹çš„æ¢¯åº¦æ¥æ›´æ–°å…¶å‚æ•°ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬å·²ç»è¯æ˜ï¼Œåœ¨GradPromçš„å®ç°ä¸‹ï¼Œå›¾åƒå¢å¼ºæ¨¡å‹çš„ä¼˜åŒ–æ–¹å‘ä¸ä¼šå—åˆ°è¾…åŠ©è§†è§‰è¯†åˆ«æ¨¡å‹çš„å½±å“ã€‚åœ¨å››ä¸ªå…¬å¼€ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒGradPromçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01114v1">PDF</a> This paper has been accepted by IEEE Transactions on Pattern Analysis   and Machine Intelligence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒä»»åŠ¡é©±åŠ¨å›¾åƒè´¨é‡å¢å¼ºï¼ˆIQEï¼‰çš„å¹¿ä¹‰æ¢¯åº¦æå‡ï¼ˆGradPromï¼‰è®­ç»ƒç­–ç•¥ã€‚è¯¥ç­–ç•¥å°†IQEç³»ç»Ÿåˆ†ä¸ºä¸¤ä¸ªå­æ¨¡å‹ï¼šä¸»æµæ¨¡å‹ç”¨äºå›¾åƒå¢å¼ºï¼Œè¾…åŠ©æ¨¡å‹ç”¨äºè§†è§‰è¯†åˆ«ã€‚è®­ç»ƒæ—¶ï¼ŒGradPromä»…ä½¿ç”¨è§†è§‰è¯†åˆ«æ¨¡å‹å’Œå›¾åƒå¢å¼ºæ¨¡å‹çš„æ¢¯åº¦æ›´æ–°å›¾åƒå¢å¼ºæ¨¡å‹çš„å‚æ•°ï¼Œä¸”ä»…åœ¨ä¸¤è€…æ¢¯åº¦æ–¹å‘ç›¸åŒæ—¶è¿›è¡Œã€‚å½“æ¢¯åº¦æ–¹å‘ä¸ä¸€è‡´æ—¶ï¼Œä»…ä½¿ç”¨å›¾åƒå¢å¼ºæ¨¡å‹çš„æ¢¯åº¦æ›´æ–°å‚æ•°ã€‚ç†è®ºè¯æ˜ï¼ŒGradPromå®æ–½ä¸‹ï¼Œå›¾åƒå¢å¼ºæ¨¡å‹çš„ä¼˜åŒ–æ–¹å‘ä¸ä¼šå—åˆ°è¾…åŠ©è§†è§‰è¯†åˆ«æ¨¡å‹çš„å½±å“ã€‚åœ¨å››ä¸ªå…¬å…±ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGradPromä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»åŠ¡é©±åŠ¨å›¾åƒè´¨é‡å¢å¼ºï¼ˆIQEï¼‰æ¨¡å‹å¦‚ESTRèƒ½å¤Ÿäº’ç›¸æå‡å›¾åƒå¢å¼ºæ¨¡å‹å’Œè§†è§‰è¯†åˆ«æ¨¡å‹çš„é‡åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰IQEæ¨¡å‹å¿½ç•¥äº†ä¸åŒè§†è§‰ä»»åŠ¡å¯¹å›¾åƒç‰¹å¾çš„ä¸åŒç”šè‡³å†²çªçš„éœ€æ±‚ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„å¹¿ä¹‰æ¢¯åº¦æå‡ï¼ˆGradPromï¼‰è®­ç»ƒç­–ç•¥é’ˆå¯¹åŒ»å­¦å›¾åƒçš„ä»»åŠ¡é©±åŠ¨IQEã€‚</li>
<li>GradPromå°†IQEç³»ç»Ÿåˆ†ä¸ºä¸¤ä¸ªå­æ¨¡å‹ï¼šç”¨äºå›¾åƒå¢å¼ºçš„ä¸»æµæ¨¡å‹å’Œç”¨äºè§†è§‰è¯†åˆ«çš„è¾…åŠ©æ¨¡å‹ã€‚</li>
<li>GradPromåœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨ä¸¤ä¸ªå­æ¨¡å‹æ¢¯åº¦ä¸€è‡´æ—¶çš„æ¢¯åº¦æ›´æ–°å›¾åƒå¢å¼ºæ¨¡å‹çš„å‚æ•°ã€‚</li>
<li>ç†è®ºè¯æ˜ï¼ŒGradPromä¿è¯å›¾åƒå¢å¼ºæ¨¡å‹çš„ä¼˜åŒ–æ–¹å‘ä¸ä¼šå—åˆ°è¾…åŠ©è§†è§‰è¯†åˆ«æ¨¡å‹çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-41df2c2a7408f7e1c35d05cc1d105308.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e66697d11626d6e6c1dc5da2e7778d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ae5fdbf8db6c80fb79f7f3114c3ab0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-704768b0df9e3d5752f6357e24a17199.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scale-wise-Bidirectional-Alignment-Network-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Scale-wise-Bidirectional-Alignment-Network-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Scale-wise Bidirectional Alignment Network for Referring Remote Sensing   Image Segmentation"></a>Scale-wise Bidirectional Alignment Network for Referring Remote Sensing   Image Segmentation</h2><p><strong>Authors:Kun Li, George Vosselman, Michael Ying Yang</strong></p>
<p>The goal of referring remote sensing image segmentation (RRSIS) is to extract specific pixel-level regions within an aerial image via a natural language expression. Recent advancements, particularly Transformer-based fusion designs, have demonstrated remarkable progress in this domain. However, existing methods primarily focus on refining visual features using language-aware guidance during the cross-modal fusion stage, neglecting the complementary vision-to-language flow. This limitation often leads to irrelevant or suboptimal representations. In addition, the diverse spatial scales of ground objects in aerial images pose significant challenges to the visual perception capabilities of existing models when conditioned on textual inputs. In this paper, we propose an innovative framework called Scale-wise Bidirectional Alignment Network (SBANet) to address these challenges for RRSIS. Specifically, we design a Bidirectional Alignment Module (BAM) with learnable query tokens to selectively and effectively represent visual and linguistic features, emphasizing regions associated with key tokens. BAM is further enhanced with a dynamic feature selection block, designed to provide both macro- and micro-level visual features, preserving global context and local details to facilitate more effective cross-modal interaction. Furthermore, SBANet incorporates a text-conditioned channel and spatial aggregator to bridge the gap between the encoder and decoder, enhancing cross-scale information exchange in complex aerial scenarios. Extensive experiments demonstrate that our proposed method achieves superior performance in comparison to previous state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both quantitatively and qualitatively. The code will be released after publication. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼è¿›è¡Œè§£æï¼Œæå–èˆªç©ºå›¾åƒä¸­çš„ç‰¹å®šåƒç´ çº§åŒºåŸŸã€‚æœ€è¿‘çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„èåˆè®¾è®¡ï¼Œå·²ç»åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨åœ¨è·¨æ¨¡æ€èåˆé˜¶æ®µä½¿ç”¨è¯­è¨€æ„ŸçŸ¥æŒ‡å¯¼æ¥ä¼˜åŒ–è§†è§‰ç‰¹å¾ï¼Œå¿½è§†äº†ä»è§†è§‰åˆ°è¯­è¨€çš„äº’è¡¥æµã€‚è¿™ä¸€å±€é™æ€§é€šå¸¸ä¼šå¯¼è‡´è¡¨ç¤ºä¸ç›¸å…³æˆ–æ¬¡ä¼˜ã€‚æ­¤å¤–ï¼Œèˆªç©ºå›¾åƒä¸­åœ°é¢å¯¹è±¡çš„å„ç§ç©ºé—´å°ºåº¦å¯¹æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›æ„æˆäº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ¥å—æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå°ºåº¦åŒå‘å¯¹é½ç½‘ç»œï¼ˆSBANetï¼‰çš„åˆ›æ–°æ¡†æ¶æ¥è§£å†³RRSISçš„è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒå‘å¯¹é½æ¨¡å—ï¼ˆBAMï¼‰ï¼Œå…¶ä¸­åŒ…å«å¯å­¦ä¹ çš„æŸ¥è¯¢ä»¤ç‰Œï¼Œä»¥é€‰æ‹©æ€§å’Œæœ‰æ•ˆåœ°è¡¨ç¤ºè§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¼ºè°ƒä¸å…³é”®ä»¤ç‰Œç›¸å…³çš„åŒºåŸŸã€‚BAMé€šè¿‡åŠ¨æ€ç‰¹å¾é€‰æ‹©å—è¿›ä¸€æ­¥å¢å¼ºï¼Œæ—¨åœ¨æä¾›å®è§‚å’Œå¾®è§‚çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼ŒSBANetç»“åˆäº†æ–‡æœ¬æ¡ä»¶é€šé“å’Œç©ºé—´èšåˆå™¨ï¼Œä»¥ç¼©å°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å·®è·ï¼Œå¢å¼ºå¤æ‚èˆªç©ºåœºæ™¯ä¸­è·¨å°ºåº¦ä¿¡æ¯äº¤æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸RRSIS-Då’ŒRefSegRSæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨å‘è¡¨åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00851v1">PDF</a> Under review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼æå–èˆªç©ºå›¾åƒä¸­çš„ç‰¹å®šåƒç´ çº§åŒºåŸŸã€‚æœ€è¿‘çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformerçš„èåˆè®¾è®¡ï¼Œå·²ç»åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨è¯­è¨€æ„ŸçŸ¥æŒ‡å¯¼æ¥ä¼˜åŒ–è§†è§‰ç‰¹å¾ï¼Œå¿½è§†äº†ä»è§†è§‰åˆ°è¯­è¨€çš„äº’è¡¥æµåŠ¨ã€‚è¿™ç§å±€é™æ€§é€šå¸¸ä¼šå¯¼è‡´è¡¨ç¤ºä¸ç›¸å…³æˆ–æ¬¡ä¼˜ã€‚æ­¤å¤–ï¼Œèˆªç©ºå›¾åƒä¸­åœ°é¢å¯¹è±¡çš„ç©ºé—´å°ºåº¦å¤šæ ·æ€§ç»™ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬è¾“å…¥æ¡ä»¶ä¸‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºScale-wise Bidirectional Alignment Network (SBANet)ï¼Œä»¥è§£å†³RRSISçš„è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¾è®¡äº†å¸¦æœ‰å¯å­¦ä¹ æŸ¥è¯¢æ ‡è®°çš„åŒå‘å¯¹é½æ¨¡å—ï¼ˆBAMï¼‰ï¼Œä»¥é€‰æ‹©å’Œæœ‰æ•ˆåœ°è¡¨ç¤ºè§†è§‰å’Œè¯­è¨€ç‰¹å¾ï¼Œå¼ºè°ƒä¸å…³é”®æ ‡è®°ç›¸å…³çš„åŒºåŸŸã€‚BAMè¿›ä¸€æ­¥é€šè¿‡åŠ¨æ€ç‰¹å¾é€‰æ‹©å—å¢å¼ºï¼Œæ—¨åœ¨æä¾›å®è§‚å’Œå¾®è§‚çº§åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ï¼Œä»¥ä¿ƒè¿›æ›´æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’ã€‚æ­¤å¤–ï¼ŒSBANetç»“åˆäº†æ–‡æœ¬æ¡ä»¶é€šé“å’Œç©ºé—´èšåˆå™¨ï¼Œä»¥ç¼©å°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„å·®è·ï¼Œå¢å¼ºå¤æ‚èˆªç©ºåœºæ™¯ä¸­çš„è·¨å°ºåº¦ä¿¡æ¯äº¤æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸RRSIS-Då’ŒRefSegRSæ•°æ®é›†ä¸Šçš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰çš„ç›®æ ‡æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼æå–èˆªç©ºå›¾åƒä¸­çš„ç‰¹å®šåƒç´ çº§åŒºåŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºä½¿ç”¨è¯­è¨€æ„ŸçŸ¥æŒ‡å¯¼åœ¨è·¨æ¨¡æ€èåˆé˜¶æ®µä¼˜åŒ–è§†è§‰ç‰¹å¾ï¼Œä½†å¿½è§†äº†ä»è§†è§‰åˆ°è¯­è¨€çš„äº’è¡¥æµåŠ¨ã€‚</li>
<li>èˆªç©ºå›¾åƒä¸­åœ°é¢å¯¹è±¡çš„ç©ºé—´å°ºåº¦å¤šæ ·æ€§ç»™ç°æœ‰æ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¡†æ¶SBANetï¼ŒåŒ…æ‹¬åŒå‘å¯¹é½æ¨¡å—ï¼ˆBAMï¼‰å’ŒåŠ¨æ€ç‰¹å¾é€‰æ‹©å—ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†è§†è§‰å’Œè¯­è¨€ç‰¹å¾ã€‚</li>
<li>SBANeté€šè¿‡æ–‡æœ¬æ¡ä»¶é€šé“å’Œç©ºé—´èšåˆå™¨å¢å¼ºäº†è·¨å°ºåº¦ä¿¡æ¯äº¤æ¢ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSBANetåœ¨RRSIS-Då’ŒRefSegRSæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00851">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de9e1bb6199a8ef973959aeea86279cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b349292257179e51f0549c4c4d6c8e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Role-of-Chalcogen-atoms-in-In-Situ-Exfoliation-for-Large-Area-2D-Semiconducting-Transition-Metal-Dichalcogenides"><a href="#Role-of-Chalcogen-atoms-in-In-Situ-Exfoliation-for-Large-Area-2D-Semiconducting-Transition-Metal-Dichalcogenides" class="headerlink" title="Role of Chalcogen atoms in In Situ Exfoliation for Large-Area 2D   Semiconducting Transition Metal Dichalcogenides"></a>Role of Chalcogen atoms in In Situ Exfoliation for Large-Area 2D   Semiconducting Transition Metal Dichalcogenides</h2><p><strong>Authors:Zhiying Dan, Ronak Sarmasti Emami, Giovanna Feraco, Melina Vavali, Dominic Gerlach, Petra Rudolf, Antonija GrubiÅ¡iÄ‡-ÄŒabo</strong></p>
<p>Two-dimensional (2D) transition metal dichalcogenides have emerged as a promising platform for next-generation optoelectronic and spintronic devices. Mechanical exfoliation using adhesive tape remains the dominant method for preparing 2D materials of highest quality, including transition metal dichalcogenides, but always results in small-sized flakes. This limitation poses a significant challenge for investigations and applications where large scale flakes are needed. To overcome these constraints, we explored the preparation of 2D WS2 and WSe2 using a recently developed kinetic in situ single-layer synthesis method (KISS). In particular, we focused on the influence of different substrates, Au and Ag, and chalcogen atoms, S and Se, on the yield and quality of the 2D films. The crystallinity and spatial morphology of the 2D films were characterized using optical microscopy and atomic force microscopy, providing a comprehensive assessment of exfoliation quality. Low-energy electron diffraction verified that there is no preferential orientation between the 2D film and the substrate, while optical microscopy revealed that WSe2 consistently outperformed WS2 in producing large monolayers, regardless of the substrate used. Finally, X-ray diffraction and X-ray photoelectron spectroscopy demonstrate that no covalent bonds are formed between the 2D material and the underlying substrate. These results identify KISS method as a non-destructive approach for a more scalable approach of high-quality 2D transition metal dichalcogenides. </p>
<blockquote>
<p>äºŒç»´ï¼ˆ2Dï¼‰è¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©å·²æˆä¸ºä¸‹ä¸€ä»£å…‰ç”µå­å’Œè‡ªæ—‹ç”µå­å™¨ä»¶çš„æœ‰å‰é€”çš„å¹³å°ã€‚ä½¿ç”¨èƒ¶å¸¦å‰¥ç¦»æ³•ä»æ˜¯åˆ¶å¤‡åŒ…æ‹¬è¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©åœ¨å†…çš„é«˜è´¨é‡äºŒç»´ææ–™çš„ä¸»è¦æ–¹æ³•ï¼Œä½†æ€»æ˜¯äº§ç”Ÿå°ºå¯¸è¾ƒå°çš„è–„ç‰‡ã€‚è¿™ä¸€å±€é™æ€§å¯¹äºéœ€è¦å¤§è§„æ¨¡è–„ç‰‡çš„ç ”ç©¶å’Œåº”ç”¨æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨æœ€è¿‘å¼€å‘çš„åŠ¨åŠ›å­¦åŸä½å•å±‚åˆæˆæ–¹æ³•ï¼ˆKISSï¼‰åˆ¶å¤‡äºŒç»´WS2å’ŒWSe2ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å…³æ³¨ä¸åŒçš„åŸºåº•ï¼ˆAuå’ŒAgï¼‰ä»¥åŠç¡«æ—åŸå­ï¼ˆSå’ŒSeï¼‰å¯¹äºŒç»´è–„è†œçš„äº§é‡å’Œè´¨é‡çš„å½±å“ã€‚äºŒç»´è–„è†œçš„ç»“æ™¶åº¦å’Œç©ºé—´å½¢æ€é€šè¿‡å…‰å­¦æ˜¾å¾®é•œå’ŒåŸå­åŠ›æ˜¾å¾®é•œè¡¨å¾ï¼Œä»è€Œå…¨é¢è¯„ä¼°å‰¥ç¦»è´¨é‡ã€‚ä½èƒ½ç”µå­è¡å°„è¯æ˜äºŒç»´è–„è†œä¸åŸºåº•ä¹‹é—´ä¸å­˜åœ¨æ‹©ä¼˜å–å‘ï¼Œè€Œå…‰å­¦æ˜¾å¾®é•œæ˜¾ç¤ºï¼Œæ— è®ºä½¿ç”¨ä½•ç§åŸºåº•ï¼ŒWSe2åœ¨ç”Ÿæˆå¤§å‹å•å±‚æ–¹é¢å§‹ç»ˆä¼˜äºWS2ã€‚æœ€åï¼ŒXå°„çº¿è¡å°„å’ŒXå°„çº¿å…‰ç”µå­èƒ½è°±è¯æ˜äºŒç»´ææ–™ä¸åº•å±‚åŸºåº•ä¹‹é—´æ²¡æœ‰å½¢æˆå…±ä»·é”®ã€‚è¿™äº›ç»“æœå°†KISSæ–¹æ³•ç¡®å®šä¸ºä¸€ç§éç ´åæ€§çš„æ–¹æ³•ï¼Œå¯ä»¥æ›´å¤§è§„æ¨¡åœ°åº”ç”¨é«˜è´¨é‡äºŒç»´è¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00815v1">PDF</a> Article (13 pages, 5 figures) and supporting information (5 pages, 6   figures)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä½¿ç”¨æ–°å‹çš„KISSæ–¹æ³•åˆæˆäºŒç»´WS2å’ŒWSe2ææ–™ï¼Œç ”ç©¶äº†ä¸åŒåŸºåº•ï¼ˆAuå’ŒAgï¼‰å’Œç¡«æ—å…ƒç´ ï¼ˆSå’ŒSeï¼‰å¯¹äºŒç»´è–„è†œçš„äº§ç‡å’Œå“è´¨çš„å½±å“ã€‚è¯¥æ–¹æ³•è¢«è¯å®å¯ä»¥æœ‰æ•ˆåˆæˆé«˜è´¨é‡å¤§å°ºå¯¸äºŒç»´è–„è†œææ–™ï¼Œå¹¶æœªå¯¹åŸºåº•å½¢æˆä»»ä½•å…±ä»·é”®ï¼Œæ˜¯ä¸€ç§å¯æ‰©å±•çš„éç ´åæ€§æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡æ¸¡é‡‘å±äºŒå¤åŒ–ç‰©åœ¨å…‰ç”µå­å­¦å’Œè‡ªæ—‹ç”µå­å­¦å™¨ä»¶é¢†åŸŸå…·æœ‰æ½œåŠ›ã€‚</li>
<li>æœºæ¢°å‰¥ç¦»æ³•ä»æ˜¯åˆ¶å¤‡é«˜è´¨é‡äºŒç»´ææ–™çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†éš¾ä»¥è·å¾—å¤§å°ºå¯¸è–„è†œã€‚</li>
<li>KISSæ–¹æ³•å¯ç”¨äºåˆæˆäºŒç»´WS2å’ŒWSe2ææ–™ã€‚</li>
<li>åŸºåº•ç±»å‹å’Œç¡«æ—å…ƒç´ å¯¹äºŒç»´è–„è†œçš„äº§ç‡å’Œè´¨é‡æœ‰å½±å“ã€‚</li>
<li>KISSæ–¹æ³•åˆæˆçš„äºŒç»´è–„è†œå…·æœ‰ä¼˜è‰¯çš„å•æ™¶ç»“æ„å’Œç©ºé—´å½¢æ€ã€‚</li>
<li>éæ™¶åŒ–éªŒè¯æ˜¾ç¤ºåŸºåº•ä¸äºŒç»´è–„è†œé—´æ²¡æœ‰ä¼˜å…ˆå–å‘æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-092196aa8c5d5e856edd171d789849e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c704a21ae19b6686f29a3d9942d701a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a480a3dd57c6464fe7d30fb406d11b58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c1cc28fb69ea17e8f5dcd16fe996e61.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HCMA-UNet-A-Hybrid-CNN-Mamba-UNet-with-Inter-Slice-Self-Attention-for-Efficient-Breast-Cancer-Segmentation"><a href="#HCMA-UNet-A-Hybrid-CNN-Mamba-UNet-with-Inter-Slice-Self-Attention-for-Efficient-Breast-Cancer-Segmentation" class="headerlink" title="HCMA-UNet: A Hybrid CNN-Mamba UNet with Inter-Slice Self-Attention for   Efficient Breast Cancer Segmentation"></a>HCMA-UNet: A Hybrid CNN-Mamba UNet with Inter-Slice Self-Attention for   Efficient Breast Cancer Segmentation</h2><p><strong>Authors:Haoxuan Li, Wei song, Peiwu Qin, Xi Yuan, Zhenglin Chen</strong></p>
<p>Breast cancer lesion segmentation in DCE-MRI remains challenging due to heterogeneous tumor morphology and indistinct boundaries. To address these challenges, this study proposes a novel hybrid segmentation network, HCMA-UNet, for lesion segmentation of breast cancer. Our network consists of a lightweight CNN backbone and a Multi-view Inter-Slice Self-Attention Mamba (MISM) module. The MISM module integrates Visual State Space Block (VSSB) and Inter-Slice Self-Attention (ISSA) mechanism, effectively reducing parameters through Asymmetric Split Channel (ASC) strategy to achieve efficient tri-directional feature extraction. Our lightweight model achieves superior performance with 2.87M parameters and 126.44 GFLOPs. A Feature-guided Region-aware loss function (FRLoss) is proposed to enhance segmentation accuracy. Extensive experiments on one private and two public DCE-MRI breast cancer datasets demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. FRLoss also exhibits good cross-architecture generalization capabilities. The source code and dataset is available on this link. </p>
<blockquote>
<p>ä¹³è…ºç™Œåœ¨åŠ¨æ€å¯¹æ¯”å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰ä¸­çš„ç—…ç¶åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºè‚¿ç˜¤å½¢æ€å­¦å¼‚è´¨æ€§ä»¥åŠè¾¹ç•Œä¸æ¸…æ™°æ‰€å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆåˆ†å‰²ç½‘ç»œHCMA-UNetï¼Œç”¨äºä¹³è…ºç™Œç—…ç¶çš„åˆ†å‰²ã€‚æˆ‘ä»¬çš„ç½‘ç»œç”±è½»é‡çº§çš„CNNéª¨å¹²ç½‘å’Œå¤šè§†å›¾åˆ‡ç‰‡é—´è‡ªæ³¨æ„åŠ›Mambaï¼ˆMISMï¼‰æ¨¡å—ç»„æˆã€‚MISMæ¨¡å—ç»“åˆäº†è§†è§‰çŠ¶æ€ç©ºé—´å—ï¼ˆVSSBï¼‰å’Œåˆ‡ç‰‡é—´è‡ªæ³¨æ„åŠ›ï¼ˆISSAï¼‰æœºåˆ¶ï¼Œé€šè¿‡ä¸å¯¹ç§°åˆ†è£‚é€šé“ï¼ˆASCï¼‰ç­–ç•¥æœ‰æ•ˆåœ°å‡å°‘å‚æ•°ï¼Œå®ç°é«˜æ•ˆçš„ä¸‰å‘ç‰¹å¾æå–ã€‚æˆ‘ä»¬çš„è½»é‡çº§æ¨¡å‹å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…å«287ä¸‡å‚æ•°å’Œæ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°ä¸º126.44æ¬¡ã€‚ä¸ºäº†æé«˜åˆ†å‰²ç²¾åº¦ï¼Œæå‡ºäº†åŸºäºç‰¹å¾çš„åŒºåŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼ˆFRLossï¼‰ã€‚åœ¨ä¸€ä¸ªç§æœ‰å’Œä¸¤ä¸ªå…¬å¼€çš„åŠ¨æ€å¯¹æ¯”å¢å¼ºç£å…±æŒ¯æˆåƒä¹³è…ºç™Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°æœ€æ–°æ€§èƒ½çš„åŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒFRLossè¿˜å±•ç°å‡ºè‰¯å¥½çš„è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å’Œæ•°æ®é›†å¯åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00751v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆåˆ†å‰²ç½‘ç»œHCMA-UNetï¼Œç”¨äºä¹³è…ºç™Œç—…å˜çš„åˆ†å‰²ã€‚è¯¥ç½‘ç»œåŒ…å«è½»é‡çº§CNNéª¨å¹²å’ŒMulti-view Inter-Slice Self-Attention Mambaï¼ˆMISMï¼‰æ¨¡å—ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸‰å‘ç‰¹å¾æå–ã€‚é€šè¿‡é‡‡ç”¨ç‰¹å¾å¼•å¯¼çš„Regionæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼ˆFRLossï¼‰ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™ŒDCE-MRIå›¾åƒåˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦ç”±äºè‚¿ç˜¤å½¢æ€å¼‚è´¨æ€§å’Œè¾¹ç•Œæ¨¡ç³Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆåˆ†å‰²ç½‘ç»œHCMA-UNetï¼ŒåŒ…å«è½»é‡çº§CNNéª¨å¹²å’ŒMISMæ¨¡å—ã€‚</li>
<li>MISMæ¨¡å—ç»“åˆäº†VSSBå’ŒISSAæœºåˆ¶ï¼Œé€šè¿‡ASCç­–ç•¥æœ‰æ•ˆå‡å°‘å‚æ•°ï¼Œå®ç°ä¸‰å‘ç‰¹å¾æå–ã€‚</li>
<li>HCMA-UNetæ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œå‚æ•°ä¸º2.87Mï¼Œè®¡ç®—é‡ä¸º126.44 GFLOPsã€‚</li>
<li>å¼•å…¥FRLosså‡½æ•°æé«˜åˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>FRLosså‡½æ•°å…·æœ‰è‰¯å¥½çš„è·¨æ¶æ„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-084e301a87c6e24dffd403ab6a6dff3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd751d360b8308907f3456ebfc25f396.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c723be3e799c9c5d976f738c4b9f3c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd4d097bc68d769e9772f6d01967d69e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5934bd1982449c251184c2fdb630919f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecbd81ff230e91b720cc89a139427ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fb5afc2f821b301684dab330bccb2e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44433dcf49a57a1440b775b349924390.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deeply-Learned-Robust-Matrix-Completion-for-Large-scale-Low-rank-Data-Recovery"><a href="#Deeply-Learned-Robust-Matrix-Completion-for-Large-scale-Low-rank-Data-Recovery" class="headerlink" title="Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data   Recovery"></a>Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data   Recovery</h2><p><strong>Authors:HanQin Cai, Chandra Kundu, Jialin Liu, Wotao Yin</strong></p>
<p>Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fix-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery. </p>
<blockquote>
<p>é²æ£’çŸ©é˜µè¡¥å…¨ï¼ˆRMCï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºæœºå™¨å­¦ä¹ é¢†åŸŸçš„å·¥å…·ï¼Œå®ƒèƒ½åŒæ—¶è§£å†³ä½ç§©æ•°æ®åˆ†æä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæ•°æ®ç¼ºå¤±å’Œæç«¯å¼‚å¸¸å€¼ã€‚æœ¬æ–‡é’ˆå¯¹å¤§è§„æ¨¡RMCé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯ä¼¸ç¼©ã€å¯å­¦ä¹ çš„éå‡¸æ–¹æ³•ï¼Œç§°ä¸ºå­¦ä¹ é²æ£’çŸ©é˜µè¡¥å…¨ï¼ˆLRMCï¼‰ã€‚LRMCå…·æœ‰è¾ƒä½çš„è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶èƒ½å®ç°çº¿æ€§æ”¶æ•›ã€‚å—å®šç†çš„å¯å‘ï¼ŒLRMCçš„è‡ªç”±å‚æ•°å¯ä»¥é€šè¿‡æ·±åº¦å±•å¼€è¿›è¡Œæœ‰æ•ˆåœ°å­¦ä¹ ï¼Œä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§çµæ´»çš„åé¦ˆé€’å½’æ··åˆç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œå®ƒå°†æ·±åº¦å±•å¼€ä»å›ºå®šè¿­ä»£æ‰©å±•åˆ°æ— é™è¿­ä»£ã€‚é€šè¿‡å¤§é‡çš„å®éªŒéªŒè¯äº†LRMCåœ¨åˆæˆæ•°æ®é›†å’Œå®é™…åº”ç”¨ä¸­çš„å“è¶Šè¡¨ç°ï¼ŒåŒ…æ‹¬è§†é¢‘èƒŒæ™¯å»é™¤ã€è¶…å£°æˆåƒã€é¢éƒ¨å»ºæ¨¡ä»¥åŠå«æ˜Ÿå›¾åƒçš„äº‘å»é™¤ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00677v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2110.05649</p>
<p><strong>Summary</strong><br>    æ–°å‹å¯å­¦ä¹ éå‡¸æ–¹æ³•ï¼ˆLRMCï¼‰åº”ç”¨äºå¤§è§„æ¨¡é²æ£’çŸ©é˜µè¡¥å…¨ï¼ˆRMCï¼‰ï¼Œè¯¥æ–¹æ³•å…·å¤‡ä½è®¡ç®—å¤æ‚åº¦åŠçº¿æ€§æ”¶æ•›æ€§ï¼Œå¹¶èƒ½é€šè¿‡æ·±åº¦å±•å¼€æœ‰æ•ˆåœ°å­¦ä¹ è‡ªç”±å‚æ•°ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªçµæ´»çš„æ··åˆç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œå°†æ·±åº¦å±•å¼€ä»å›ºå®šè¿­ä»£æ¬¡æ•°æ‰©å±•åˆ°æ— é™è¿­ä»£æ¬¡æ•°ã€‚LRMCåœ¨åˆæˆæ•°æ®é›†å’Œå®é™…åº”ç”¨ä¸­çš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ï¼ŒåŒ…æ‹¬è§†é¢‘èƒŒæ™¯å»é™¤ã€è¶…å£°æˆåƒã€é¢éƒ¨å»ºæ¨¡å’Œå«æ˜Ÿå›¾åƒå»äº‘ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMCæ˜¯ä¸€ç§æ–°å‹çš„å¯å­¦ä¹ éå‡¸æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§è§„æ¨¡é²æ£’çŸ©é˜µè¡¥å…¨ï¼ˆRMCï¼‰é—®é¢˜ã€‚</li>
<li>LRMCå…·å¤‡ä½è®¡ç®—å¤æ‚åº¦å’Œçº¿æ€§æ”¶æ•›æ€§ã€‚</li>
<li>é€šè¿‡æ·±åº¦å±•å¼€ï¼ŒLRMCçš„è‡ªç”±å‚æ•°å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªçµæ´»çš„æ··åˆç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œå°†æ·±åº¦å±•å¼€ä»å›ºå®šè¿­ä»£æ‰©å±•åˆ°æ— é™è¿­ä»£ã€‚</li>
<li>LRMCåœ¨åˆæˆæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°æŠ€æœ¯ã€‚</li>
<li>LRMCåœ¨è§†é¢‘èƒŒæ™¯å»é™¤ã€è¶…å£°æˆåƒã€é¢éƒ¨å»ºæ¨¡ç­‰å®é™…åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f20ad059582fb62f303c903a6e75896b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00eea286e65f66fc6866401f5d57c293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0678c1412eeec80874e5be0d281bdd40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c68eb04538549d2235debdd2ea7fa1fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-707121b2f423acb3e0635ceb95bac1db.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leaf-diseases-detection-using-deep-learning-methods"><a href="#Leaf-diseases-detection-using-deep-learning-methods" class="headerlink" title="Leaf diseases detection using deep learning methods"></a>Leaf diseases detection using deep learning methods</h2><p><strong>Authors:El Houcine El Fatimi</strong></p>
<p>This study, our main topic is to devlop a new deep-learning approachs for plant leaf disease identification and detection using leaf image datasets. We also discussed the challenges facing current methods of leaf disease detection and how deep learning may be used to overcome these challenges and enhance the accuracy of disease detection. Therefore, we have proposed a novel method for the detection of various leaf diseases in crops, along with the identification and description of an efficient network architecture that encompasses hyperparameters and optimization methods. The effectiveness of different architectures was compared and evaluated to see the best architecture configuration and to create an effective model that can quickly detect leaf disease. In addition to the work done on pre-trained models, we proposed a new model based on CNN, which provides an efficient method for identifying and detecting plant leaf disease. Furthermore, we evaluated the efficacy of our model and compared the results to those of some pre-trained state-of-the-art architectures. </p>
<blockquote>
<p>æœ¬ç ”ç©¶çš„ä¸»è¦è¯¾é¢˜æ˜¯å¼€å‘ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ¥è¯†åˆ«å†œä½œç‰©å¶ç‰‡ç–¾ç—…å¹¶æ£€æµ‹å¶ç‰‡å›¾åƒæ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†å½“å‰å¶ç‰‡ç–¾ç—…æ£€æµ‹æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥å…‹æœè¿™äº›æŒ‘æˆ˜å¹¶å¢å¼ºç–¾ç—…æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ£€æµ‹å†œä½œç‰©å„ç§å¶ç‰‡ç–¾ç—…çš„æ–°æ–¹æ³•ï¼Œå¹¶ç¡®å®šå’Œæè¿°äº†ä¸€ä¸ªæœ‰æ•ˆçš„ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„æ¶µç›–äº†è¶…å‚æ•°å’Œä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬æ¯”è¾ƒå¹¶è¯„ä¼°äº†ä¸åŒæ¶æ„çš„æœ‰æ•ˆæ€§ï¼Œä»¥æ‰¾å‡ºæœ€ä½³çš„æ¶æ„é…ç½®å¹¶åˆ›å»ºä¸€ä¸ªå¯ä»¥å¿«é€Ÿæ£€æµ‹å¶ç‰‡ç–¾ç—…çš„æœ‰æ•ˆæ¨¡å‹ã€‚é™¤äº†å¯¹é¢„è®­ç»ƒæ¨¡å‹æ‰€åšçš„å·¥ä½œå¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºCNNçš„æ–°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æä¾›äº†ä¸€ç§æœ‰æ•ˆè¯†åˆ«ä¸æ£€æµ‹æ¤ç‰©å¶ç‰‡ç–¾ç—…çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†æˆ‘ä»¬æ¨¡å‹çš„æ•ˆåŠ›ï¼Œå¹¶å°†ç»“æœä¸æŸäº›å…ˆè¿›çš„é¢„è®­ç»ƒæ¶æ„è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00669v1">PDF</a> 252 pages , 42 images</p>
<p><strong>Summary</strong><br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œç½‘ç»œæ¶æ„ï¼Œç”¨äºå†œä½œç‰©å¶ç‰‡ç–¾ç—…çš„è¯†åˆ«å’Œæ£€æµ‹ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¶æ„çš„æ•ˆèƒ½ï¼Œæœ€ç»ˆç¡®å®šäº†æœ€ä½³é…ç½®ï¼Œæé«˜äº†å¶ç‰‡ç–¾ç—…æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œæœ¬ç ”ç©¶ä¹ŸåŸºäºCNNæå‡ºäº†ä¸€ä¸ªæ–°æ¨¡å‹ï¼Œå¹¶ä¸ä¸€äº›é¢„è®­ç»ƒçš„æœ€å…ˆè¿›æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒå’Œè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å…³æ³¨æ¤ç‰©å¶ç‰‡ç–¾ç—…çš„è¯†åˆ«å’Œæ£€æµ‹ï¼Œæå‡ºæ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œç½‘ç»œæ¶æ„ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†å½“å‰å¶ç‰‡ç–¾ç—…æ£€æµ‹æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ å¦‚ä½•å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”ä¸åŒæ¶æ„çš„æ•ˆèƒ½ï¼Œç¡®å®šäº†æœ€ä½³ç½‘ç»œé…ç½®ï¼Œæé«˜äº†å¶ç‰‡ç–¾ç—…æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºCNNçš„æ–°æ¨¡å‹ï¼Œç”¨äºå¶ç‰‡ç–¾ç—…çš„è¯†åˆ«å’Œæ£€æµ‹ã€‚</li>
<li>æ–°æ¨¡å‹å±•ç¤ºäº†é«˜æ•ˆçš„å¶ç‰‡ç–¾ç—…è¯†åˆ«ä¸æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å¯¹æå‡ºçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸ä¸€äº›é¢„è®­ç»ƒçš„æœ€å…ˆè¿›æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5cc5d76c2ec01101252294bc326d8f34.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Lightweight-G-YOLOv11-Advancing-Efficient-Fracture-Detection-in-Pediatric-Wrist-X-rays"><a href="#Lightweight-G-YOLOv11-Advancing-Efficient-Fracture-Detection-in-Pediatric-Wrist-X-rays" class="headerlink" title="Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in   Pediatric Wrist X-rays"></a>Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in   Pediatric Wrist X-rays</h2><p><strong>Authors:Abdesselam Ferdi</strong></p>
<p>Computer-aided diagnosis (CAD) systems have greatly improved the interpretation of medical images by radiologists and surgeons. However, current CAD systems for fracture detection in X-ray images primarily rely on large, resource-intensive detectors, which limits their practicality in clinical settings. To address this limitation, we propose a novel lightweight CAD system based on the YOLO detector for fracture detection. This system, named ghost convolution-based YOLOv11 (G-YOLOv11), builds on the latest version of the YOLO detector family and incorporates the ghost convolution operation for feature extraction. The ghost convolution operation generates the same number of feature maps as traditional convolution but requires fewer linear operations, thereby reducing the detectorâ€™s computational resource requirements. We evaluated the performance of the proposed G-YOLOv11 detector on the GRAZPEDWRI-DX dataset, achieving an <a href="mailto:&#109;&#65;&#x50;&#64;&#x30;&#46;&#x35;">&#109;&#65;&#x50;&#64;&#x30;&#46;&#x35;</a> of 0.535 with an inference time of 2.4 ms on an NVIDIA A10 GPU. Compared to the standard YOLOv11l, G-YOLOv11l achieved reductions of 13.6% in <a href="mailto:&#109;&#x41;&#80;&#64;&#x30;&#x2e;&#53;">&#109;&#x41;&#80;&#64;&#x30;&#x2e;&#53;</a> and 68.7% in size. These results establish a new state-of-the-art benchmark in terms of efficiency, outperforming existing detectors. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/AbdesselamFerdi/G-YOLOv11">https://github.com/AbdesselamFerdi/G-YOLOv11</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿæå¤§åœ°æé«˜äº†æ”¾å°„ç§‘åŒ»å¸ˆå’Œå¤–ç§‘åŒ»å¸ˆå¯¹åŒ»å­¦å›¾åƒçš„è§£é‡Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ç”¨äºXå°„çº¿å›¾åƒéª¨æŠ˜æ£€æµ‹çš„CADç³»ç»Ÿä¸»è¦ä¾èµ–äºå¤§å‹ã€èµ„æºå¯†é›†å‹çš„æ£€æµ‹å™¨ï¼Œè¿™åœ¨ä¸´åºŠä¸Šé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºYOLOæ£€æµ‹å™¨çš„è½»é‡çº§CADç³»ç»Ÿï¼Œç”¨äºéª¨æŠ˜æ£€æµ‹ã€‚è¯¥ç³»ç»Ÿåä¸ºåŸºäºGhostå·ç§¯çš„YOLOv11ï¼ˆG-YOLOv11ï¼‰ï¼Œå®ƒå»ºç«‹åœ¨YOLOæ£€æµ‹å™¨å®¶æ—æœ€æ–°ç‰ˆæœ¬çš„åŸºç¡€ä¸Šï¼Œå¹¶èå…¥äº†Ghostå·ç§¯æ“ä½œè¿›è¡Œç‰¹å¾æå–ã€‚Ghostå·ç§¯æ“ä½œç”Ÿæˆçš„ç‰¹æ€§å›¾æ•°é‡ä¸ä¼ ç»Ÿå·ç§¯ç›¸åŒï¼Œä½†æ‰€éœ€çš„çº¿æ€§æ“ä½œæ›´å°‘ï¼Œä»è€Œé™ä½äº†æ£€æµ‹å™¨çš„è®¡ç®—èµ„æºéœ€æ±‚ã€‚æˆ‘ä»¬åœ¨GRAZPEDWRI-DXæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„G-YOLOv11æ£€æµ‹å™¨çš„æ€§èƒ½ï¼Œåœ¨NVIDIA A10 GPUä¸Šå®ç°0.5çš„<a href="mailto:&#x6d;&#65;&#80;&#64;&#x30;&#46;&#x35;">&#x6d;&#65;&#80;&#64;&#x30;&#46;&#x35;</a>ä¸º0.535ï¼Œæ¨ç†æ—¶é—´ä¸º2.4æ¯«ç§’ã€‚ä¸æ ‡å‡†YOLOv11ç›¸æ¯”ï¼ŒG-YOLOv11åœ¨<a href="mailto:&#109;&#65;&#x50;&#x40;&#x30;&#46;&#x35;">&#109;&#65;&#x50;&#x40;&#x30;&#46;&#x35;</a>ä¸Šå®ç°äº†13.6%çš„é™ä½ï¼Œä½“ç§¯å‡å°‘äº†68.7%ã€‚è¿™äº›ç»“æœè¾¾åˆ°äº†æœ€æ–°çš„æ•ˆç‡åŸºå‡†ç‚¹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ£€æµ‹å™¨ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AbdesselamFerdi/G-YOLOv1%E5%AE%9E%E7%8E%B%E7%8E%B0%E8%AF%AD%E7%BB%BC%E%E8%BF%87%E9%A9%AC%E8%B7%AF%E8%BF%99%E4%B8%AA%E8%BF%87%E7%A8%8B%E7%9A%84%E9%9B%86%E6%88%90%E4%BB%A3%E7%A0%81%E4%B8%AD%E4%B9%9F%E6%9C%89%E7%9D%80%E6%98%BE%E8%91%97%E7%9A%84%E7%BB%86%E8%8A%82%E6%94%AF%E6%8C%81%EF%BC%9A%E6%88%91%E4%BB%AC%E6%98%AF%E8%BF%90%E7%94%A8%E9%A2%84%E8%AE%BE%E6%96%87%E4%BB%B6%E9%80%89%E6%8B%A9%E7%9B%B8%E5%85%B3%E7%89%B9%E6%80%A7%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8A%9F%E8%83%BD%E9%80%89%E9%A1%B9%E4%BB%A5%E4%BE%BF%E5%85%B6%E8%BE%93%E5%85%A5%E5%8F%AF%E8%87%AA%E4%B8%BB%E6%A3%80%E6%B5%8B%E7%89%B9%E5%AE%9A%E7%9A%84%E8%AE%AD%E7%BB%83%E5%AD%90%E9%9B%86%E4%BB%A5%E4%BE%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%88%86%E6%9E%90%EF%BC%89%E3%80%82%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E6%88%96%E5%BC%80%E5%8F%91%E8%80%85%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E4%B8%8A%E8%BF%B0%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%E5%AE%8C%E6%95%B4%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6%E5%B9%B6%E4%BA%86%E8%A7%A3%E6%9B%B4%E5%A4%9A%E7%9A%84%E7%A0%94%E7%A9%B6%E6%88%90%E6%9E%9C%E5%92%8C%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82%E3%80%82%E8%AF%B7%E6%B3%A8%E6%84%8F%E5%9C%A8%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8%E6%97%B6%E6%82%A8%E9%9C%80%E8%A6%81%E5%AE%89%E8%A3%85%E7%9B%B8%E5%BA%94%E7%9A%84%E4%BE%9D%E8%B5%96%E5%BA%93%E5%B9%B6%E9%81%B5%E5%BE%AA%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E3%80%82">https://github.com/AbdesselamFerdi/G-YOLOv1å®ç°è¿™ä¸€ç›®æ ‡çš„è¿‡ç¨‹ä¸­æˆ‘ä»¬çš„GitHubé¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä»“åº“ï¼ŒåŒ…å«äº†æ‰€æœ‰ç›¸å…³çš„ä»£ç å’Œæ¨¡å‹æ–‡ä»¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00647v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºYOLOæ£€æµ‹å™¨çš„è½»é‡åŒ–è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿç”¨äºXå…‰å½±åƒéª¨æŠ˜æ£€æµ‹ï¼Œå‘½åä¸ºG-YOLOv11ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æœ€æ–°YOLOæ¢æµ‹å™¨å®¶æ—ç‰ˆæœ¬ï¼Œå¹¶ç»“åˆghostå·ç§¯æ“ä½œè¿›è¡Œç‰¹å¾æå–ï¼Œä»¥è¾ƒå°‘çš„è®¡ç®—èµ„æºç”Ÿæˆç›¸åŒæ•°é‡çš„ç‰¹å¾å›¾ã€‚åœ¨GRAZPEDWRI-DXæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒG-YOLOv11æ¢æµ‹å™¨åœ¨NVIDIA A10 GPUä¸Šçš„æ¨ç†æ—¶é—´ä¸º2.4æ¯«ç§’ï¼Œ<a href="mailto:&#109;&#x41;&#x50;&#x40;&#48;&#46;&#x35;">&#109;&#x41;&#x50;&#x40;&#48;&#46;&#x35;</a>è¾¾åˆ°0.535ï¼Œç›¸è¾ƒäºæ ‡å‡†YOLOv11lï¼Œåœ¨<a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#x2e;&#x35;">&#109;&#65;&#80;&#64;&#x30;&#x2e;&#x35;</a>ä¸Šé™ä½äº†13.6%ï¼Œä½“ç§¯å‡å°äº†68.7%ï¼Œå±•ç°å‡ºå“è¶Šçš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADç³»ç»Ÿåœ¨åŒ»å­¦å›¾åƒè§£è¯»ä¸­çš„åº”ç”¨ï¼šè¯´æ˜è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ï¼ˆCADï¼‰ç³»ç»Ÿå¯¹æé«˜æ”¾å°„ç§‘åŒ»å¸ˆå’Œå¤–ç§‘åŒ»ç”Ÿå¯¹åŒ»å­¦å›¾åƒçš„è§£è¯»èƒ½åŠ›å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å½“å‰Xå…‰å½±åƒéª¨æŠ˜æ£€æµ‹CADç³»ç»Ÿçš„å±€é™æ€§ï¼šæŒ‡å‡ºå½“å‰ç”¨äºXå…‰å½±åƒéª¨æŠ˜æ£€æµ‹çš„CADç³»ç»Ÿä¸»è¦ä¾èµ–äºå¤§å‹ã€èµ„æºå¯†é›†å‹çš„æ¢æµ‹å™¨ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å®ç”¨æ€§ã€‚</li>
<li>G-YOLOv11ç³»ç»Ÿçš„æå‡ºï¼šä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºYOLOæ£€æµ‹å™¨çš„è½»é‡åŒ–CADç³»ç»Ÿï¼Œç”¨äºXå…‰å½±åƒéª¨æŠ˜æ£€æµ‹ã€‚</li>
<li>G-YOLOv11ç³»ç»Ÿçš„ç‰¹ç‚¹ï¼šå¼ºè°ƒG-YOLOv11ç³»ç»Ÿé‡‡ç”¨ghostå·ç§¯æ“ä½œè¿›è¡Œç‰¹å¾æå–ï¼Œèƒ½å¤Ÿå‡å°‘è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>G-YOLOv11ç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°ï¼šåœ¨GRAZPEDWRI-DXæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒG-YOLOv11ç³»ç»Ÿå…·æœ‰é«˜æ•ˆæ€§èƒ½ï¼Œç›¸è¾ƒäºæ ‡å‡†YOLOv11læœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>G-YOLOv11ç³»ç»Ÿçš„å¯ç”¨èµ„æºï¼šæä¾›G-YOLOv11ç³»ç»Ÿçš„ä»£ç å’Œæ¨¡å‹å¯ä¾›ä¸‹è½½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a166e5cb2fb8bed2465968e7f0a0346.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b98ab79ecb86036554c2a881bc15921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73eabb942debdd1d08c2b3899fc94815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b18e7d12a138d8533e141c8c443bd198.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00c7a8cebca5d6e9bd6b8e2252251e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba273ca2a24bf893451d32a6e7a5ce16.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Context-Length-and-Efficient-Transformers-for-Biomedical-Image-Analysis"><a href="#A-Study-on-Context-Length-and-Efficient-Transformers-for-Biomedical-Image-Analysis" class="headerlink" title="A Study on Context Length and Efficient Transformers for Biomedical   Image Analysis"></a>A Study on Context Length and Efficient Transformers for Biomedical   Image Analysis</h2><p><strong>Authors:Sarah M. Hooper, Hui Xue</strong></p>
<p>Biomedical imaging modalities often produce high-resolution, multi-dimensional images that pose computational challenges for deep neural networks. These computational challenges are compounded when training transformers due to the self-attention operator, which scales quadratically with context length. Recent developments in long-context models have potential to alleviate these difficulties and enable more efficient application of transformers to large biomedical images, although a systematic evaluation on this topic is lacking. In this study, we investigate the impact of context length on biomedical image analysis and we evaluate the performance of recently proposed long-context models. We first curate a suite of biomedical imaging datasets, including 2D and 3D data for segmentation, denoising, and classification tasks. We then analyze the impact of context length on network performance using the Vision Transformer and Swin Transformer by varying patch size and attention window size. Our findings reveal a strong relationship between context length and performance, particularly for pixel-level prediction tasks. Finally, we show that recent long-context models demonstrate significant improvements in efficiency while maintaining comparable performance, though we highlight where gaps remain. This work underscores the potential and challenges of using long-context models in biomedical imaging. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦æˆåƒæ¨¡å¼é€šå¸¸äº§ç”Ÿé«˜åˆ†è¾¨ç‡ã€å¤šç»´å›¾åƒï¼Œä¸ºæ·±åº¦ç¥ç»ç½‘ç»œå¸¦æ¥è®¡ç®—æŒ‘æˆ˜ã€‚ç”±äºè‡ªæ³¨æ„åŠ›ç®—å­çš„å½±å“ï¼Œè¿™äº›è®¡ç®—æŒ‘æˆ˜åœ¨è®­ç»ƒå˜å‹å™¨æ—¶æ›´åŠ å¤æ‚ï¼Œè‡ªæ³¨æ„åŠ›ç®—å­çš„è®¡ç®—é‡ä¸ä¸Šä¸‹æ–‡é•¿åº¦æˆäºŒæ¬¡æ–¹å…³ç³»ã€‚è™½ç„¶é’ˆå¯¹æ­¤ä¸»é¢˜çš„ç³»ç»Ÿæ€§è¯„ä¼°ä»æœ‰æ‰€æ¬ ç¼ºï¼Œä½†æœ€è¿‘çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„å‘å±•å…·æœ‰ç¼“è§£è¿™äº›å›°éš¾å¹¶ä¿ƒè¿›å˜å‹å™¨åœ¨å¤§å‹ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸Šæ›´æœ‰æ•ˆç‡åº”ç”¨çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†æœ€è¿‘æå‡ºçš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬é¦–å…ˆæ•´ç†äº†ä¸€å¥—ç”Ÿç‰©åŒ»å­¦æˆåƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç”¨äºåˆ†å‰²ã€å»å™ªå’Œåˆ†ç±»ä»»åŠ¡çš„äºŒç»´å’Œä¸‰ç»´æ•°æ®ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡æ”¹å˜è¡¥ä¸å¤§å°å’Œæ³¨æ„åŠ›çª—å£å¤§å°ï¼Œä½¿ç”¨è§†è§‰å˜å‹å™¨å’ŒSwinå˜å‹å™¨åˆ†æä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´å­˜åœ¨å¯†åˆ‡å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§é¢„æµ‹ä»»åŠ¡ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€è¿‘çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œå°½ç®¡æˆ‘ä»¬ä¹ŸæŒ‡å‡ºäº†ä»å­˜åœ¨å·®è·çš„åœ°æ–¹ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åœ¨é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›å’ŒæŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00619v1">PDF</a> Published at ML4H 2024</p>
<p><strong>æ‘˜è¦</strong><br>     ç”Ÿç‰©åŒ»å­¦æˆåƒæ¨¡æ€äº§ç”Ÿçš„é«˜åˆ†è¾¨ç‡ã€å¤šç»´å›¾åƒå¯¹æ·±åº¦ç¥ç»ç½‘ç»œå¸¦æ¥äº†è®¡ç®—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒå˜å‹å™¨æ—¶ï¼Œè‡ªæ³¨æ„åŠ›ç®—å­ä¼šä½¿è®¡ç®—é‡éšä¸Šä¸‹æ–‡é•¿åº¦äºŒæ¬¡æ–¹å¢é•¿ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶æå‡ºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹æœ‰æ½œåŠ›ç¼“è§£è¿™äº›å›°éš¾ï¼Œä½¿å˜å‹å™¨åœ¨å¤§å‹ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸Šçš„åº”ç”¨æ›´åŠ é«˜æ•ˆï¼Œä½†å¯¹æ­¤ä¸»é¢˜çš„ç³»ç»Ÿæ€§è¯„ä¼°ä»ç¼ºä¹ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨ä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„å½±å“ï¼Œå¹¶è¯„ä¼°æœ€è¿‘æå‡ºçš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€å¥—ç”Ÿç‰©åŒ»å­¦æˆåƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç”¨äºåˆ†å‰²ã€å»å™ªå’Œåˆ†ç±»ä»»åŠ¡çš„äºŒç»´å’Œä¸‰ç»´æ•°æ®ã€‚ç„¶åï¼Œé€šè¿‡æ”¹å˜è¡¥ä¸å¤§å°å’Œæ³¨æ„åŠ›çª—å£å¤§å°ï¼Œåˆ†æä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç½‘ç»œæ€§èƒ½çš„å½±å“ï¼Œä½¿ç”¨è§†è§‰å˜å‹å™¨å’ŒSwinå˜å‹å™¨è¿›è¡Œè¯•éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ€§èƒ½ä¹‹é—´å­˜åœ¨å¯†åˆ‡å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§é¢„æµ‹ä»»åŠ¡ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€æ–°çš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œä½†ä¹ŸæŒ‡å‡ºäº†ä»å­˜åœ¨çš„ä¸€äº›ä¸è¶³ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åœ¨é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›å’ŒæŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡ã€å¤šç»´ç”Ÿç‰©åŒ»å­¦å›¾åƒå¯¹æ·±åº¦ç¥ç»ç½‘ç»œçš„è®¡ç®—å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›ç®—å­çš„è®¡ç®—å¤æ‚åº¦éšä¸Šä¸‹æ–‡é•¿åº¦çš„å¢é•¿è€Œå¢åŠ ï¼Œæˆä¸ºè®­ç»ƒå˜å‹å™¨æ—¶çš„ç“¶é¢ˆã€‚</li>
<li>ç¼ºä¹å…³äºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡è¯•éªŒéªŒè¯äº†ä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç½‘ç»œæ€§èƒ½çš„é‡è¦å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§é¢„æµ‹ä»»åŠ¡ä¸­ã€‚</li>
<li>è¿‘æœŸæå‡ºçš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹åœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>å°½ç®¡æœ‰è¿™äº›æ”¹è¿›ï¼Œä½†åœ¨åº”ç”¨é•¿ä¸Šä¸‹æ–‡æ¨¡å‹è¿›è¡Œç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææ—¶ä»å­˜åœ¨ä¸€äº›ä¸è¶³å’Œæ½œåœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9216ff3b6696c7df669a6c8783190fc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0db9e96dd8bb048493140bc2db6db85d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02e3df988585fe6ce294e5a1ac4230e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab313b29362b5149b589d62e8693c992.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="H-Net-A-Multitask-Architecture-for-Simultaneous-3D-Force-Estimation-and-Stereo-Semantic-Segmentation-in-Intracardiac-Catheters"><a href="#H-Net-A-Multitask-Architecture-for-Simultaneous-3D-Force-Estimation-and-Stereo-Semantic-Segmentation-in-Intracardiac-Catheters" class="headerlink" title="H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and   Stereo Semantic Segmentation in Intracardiac Catheters"></a>H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and   Stereo Semantic Segmentation in Intracardiac Catheters</h2><p><strong>Authors:Pedram Fekri, Mehrdad Zadeh, Javad Dargahi</strong></p>
<p>The success rate of catheterization procedures is closely linked to the sensory data provided to the surgeon. Vision-based deep learning models can deliver both tactile and visual information in a sensor-free manner, while also being cost-effective to produce. Given the complexity of these models for devices with limited computational resources, research has focused on force estimation and catheter segmentation separately. However, there is a lack of a comprehensive architecture capable of simultaneously segmenting the catheter from two different angles and estimating the applied forces in 3D. To bridge this gap, this work proposes a novel, lightweight, multi-input, multi-output encoder-decoder-based architecture. It is designed to segment the catheter from two points of view and concurrently measure the applied forces in the x, y, and z directions. This network processes two simultaneous X-Ray images, intended to be fed by a biplane fluoroscopy system, showing a catheterâ€™s deflection from different angles. It uses two parallel sub-networks with shared parameters to output two segmentation maps corresponding to the inputs. Additionally, it leverages stereo vision to estimate the applied forces at the catheterâ€™s tip in 3D. The architecture features two input channels, two classification heads for segmentation, and a regression head for force estimation through a single end-to-end architecture. The output of all heads was assessed and compared with the literature, demonstrating state-of-the-art performance in both segmentation and force estimation. To the best of the authorsâ€™ knowledge, this is the first time such a model has been proposed </p>
<blockquote>
<p>å¯¼ç®¡æ’å…¥æ‰‹æœ¯çš„æˆåŠŸç‡ä¸ä¸ºå¤–ç§‘åŒ»ç”Ÿæä¾›çš„æ„Ÿå®˜æ•°æ®å¯†åˆ‡ç›¸å…³ã€‚åŸºäºè§†è§‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿä»¥æ— ä¼ æ„Ÿå™¨çš„æ–¹å¼æä¾›è§¦è§‰å’Œè§†è§‰ä¿¡æ¯ï¼ŒåŒæ—¶åˆ¶é€ æˆæœ¬æ•ˆç›Šé«˜ã€‚è€ƒè™‘åˆ°è¿™äº›æ¨¡å‹å¯¹äºæœ‰é™è®¡ç®—èµ„æºçš„è®¾å¤‡çš„å¤æ‚æ€§ï¼Œç ”ç©¶ä¸»è¦é›†ä¸­äºå•ç‹¬çš„åŠ›ä¼°è®¡å’Œå¯¼ç®¡åˆ†æ®µã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸€ç§èƒ½å¤ŸåŒæ—¶ä»ä¸¤ä¸ªä¸åŒè§’åº¦åˆ†å‰²å¯¼ç®¡å¹¶åœ¨3Dä¸­ä¼°è®¡æ–½åŠ åŠ›çš„ç»¼åˆæ¶æ„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§æ–°å‹è½»é‡çº§çš„å¤šè¾“å…¥å¤šè¾“å‡ºç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚å®ƒæ—¨åœ¨ä»ä¸¤ä¸ªè§‚ç‚¹åˆ†å‰²å¯¼ç®¡ï¼Œå¹¶åŒæ—¶æµ‹é‡xã€yå’Œzæ–¹å‘ä¸Šçš„æ–½åŠ åŠ›ã€‚è¯¥ç½‘ç»œå¤„ç†ä¸¤ä¸ªåŒæ—¶çš„Xå…‰å›¾åƒï¼Œæ—¨åœ¨ç”±åŒå¹³é¢è§å…‰é•œæ£€æŸ¥ç³»ç»Ÿæä¾›ï¼Œæ˜¾ç¤ºä»ä¸åŒè§’åº¦çœ‹åˆ°çš„å¯¼ç®¡çš„åè½¬ã€‚å®ƒä½¿ç”¨ä¸¤ä¸ªå…·æœ‰å…±äº«å‚æ•°çš„å¹¶è¡Œå­ç½‘ç»œæ¥è¾“å‡ºä¸è¾“å…¥ç›¸å¯¹åº”çš„ä¸¤ä¸ªåˆ†å‰²å›¾ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç«‹ä½“è§†è§‰æ¥ä¼°è®¡å¯¼ç®¡å°–ç«¯åœ¨3Dä¸­æ–½åŠ çš„åŠ›ã€‚è¯¥æ¶æ„å…·æœ‰ä¸¤ä¸ªè¾“å…¥é€šé“ã€ä¸¤ä¸ªç”¨äºåˆ†å‰²çš„åˆ†ç±»å¤´å’Œä¸€ä¸ªç”¨äºåŠ›ä¼°è®¡çš„å›å½’å¤´ï¼Œé€šè¿‡å•ä¸ªç«¯åˆ°ç«¯æ¶æ„å®ç°ã€‚æ‰€æœ‰å¤´éƒ¨çš„è¾“å‡ºå‡ç»è¿‡è¯„ä¼°å¹¶ä¸æ–‡çŒ®è¿›è¡Œæ¯”è¾ƒï¼Œåœ¨åˆ†å‰²å’ŒåŠ›ä¼°è®¡æ–¹é¢éƒ½æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚æ®ä½œè€…æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æå‡ºæ­¤ç±»æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è½»é‡çº§å¤šè¾“å…¥å¤šè¾“å‡ºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¯¥æ¶æ„æ—¨åœ¨ä»ä¸¤ä¸ªè§†è§’å¯¹å¯¼ç®¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶åŒæ—¶æµ‹é‡åœ¨XYZæ–¹å‘ä¸Šçš„åº”ç”¨åŠ›ã€‚å®ƒå¤„ç†æ¥è‡ªåŒå¹³é¢è§å…‰é•œæ£€æŸ¥ç³»ç»Ÿçš„å®æ—¶Xå…‰å›¾åƒï¼Œä½¿ç”¨ä¸¤ä¸ªå¹¶è¡Œå­ç½‘ç»œè¾“å‡ºä¸è¾“å…¥ç›¸å¯¹åº”çš„ä¸¤ä¸ªåˆ†å‰²å›¾ï¼Œå¹¶åˆ©ç”¨ç«‹ä½“è§†è§‰ä¼°è®¡å¯¼ç®¡çš„3DåŠ›ã€‚è¯¥æ¶æ„åœ¨åˆ†å‰²å’ŒåŠ›ä¼°è®¡æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½ä»¥æ— ä¼ æ„Ÿå™¨çš„æ–¹å¼ä¸ºå¤–ç§‘åŒ»ç”Ÿæä¾›è§¦è§‰å’Œè§†è§‰ä¿¡æ¯ï¼Œä¸”ç”Ÿäº§æˆæœ¬è¾ƒä¸ºä½å»‰ã€‚</li>
<li>ç›®å‰å¯¹äºè®¡ç®—èµ„æºæœ‰é™çš„è®¾å¤‡ï¼Œç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŠ›ä¼°è®¡å’Œå¯¼ç®¡åˆ†å‰²ä¸¤ä¸ªæ–¹é¢ã€‚</li>
<li>ç¼ºä¹ä¸€ç§èƒ½å¤ŸåŒæ—¶ä»ä¸¤ä¸ªä¸åŒè§’åº¦åˆ†å‰²å¯¼ç®¡å¹¶ä¼°è®¡3Dåº”ç”¨åŠ›çš„ç»¼åˆæ¶æ„ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§å¤šè¾“å…¥å¤šè¾“å‡ºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ä¸¤ä¸ªè§†è§’çš„Xå…‰å›¾åƒï¼Œå¹¶è¿›è¡Œå¯¼ç®¡åˆ†å‰²å’ŒåŠ›ä¼°è®¡ã€‚</li>
<li>è¯¥æ¶æ„ä½¿ç”¨ä¸¤ä¸ªå¹¶è¡Œå­ç½‘ç»œå¤„ç†è¾“å…¥å›¾åƒï¼Œç”Ÿæˆä¸¤ä¸ªåˆ†å‰²å›¾ï¼Œå¹¶æœ‰ä¸€ä¸ªå›å½’å¤´ç”¨äºåŠ›ä¼°è®¡ã€‚</li>
<li>è¯¥æ¶æ„åˆ©ç”¨ç«‹ä½“è§†è§‰æŠ€æœ¯ä¼°è®¡å¯¼ç®¡çš„3DåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e713b56697c96789f5a2b801d946f4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccfdacc1193a5246b238e0e8fb05bc8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0326ea51d935a6e65541df7c02a028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4baacebe5014c9d4c5b20ce1c7b291fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fb6703e41beb77c4e369e9a205b6712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51467eb167e70ad487b90cb2ed4392ca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CancerKG-ORG-A-Web-scale-Interactive-Verifiable-Knowledge-Graph-LLM-Hybrid-for-Assisting-with-Optimal-Cancer-Treatment-and-Care"><a href="#CancerKG-ORG-A-Web-scale-Interactive-Verifiable-Knowledge-Graph-LLM-Hybrid-for-Assisting-with-Optimal-Cancer-Treatment-and-Care" class="headerlink" title="CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM   Hybrid for Assisting with Optimal Cancer Treatment and Care"></a>CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM   Hybrid for Assisting with Optimal Cancer Treatment and Care</h2><p><strong>Authors:Michael Gubanov, Anna Pyayt, Aleksandra Karolak</strong></p>
<p>Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user. </p>
<blockquote>
<p>è¿™é‡Œæˆ‘ä»¬æè¿°äº†ä¸€ä¸ªé¦–æ‰¹åŸºäºWebçš„å¤§å‹æ··åˆçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹ä¸€ï¼Œå…¶ä¸­èå…¥äº†æœ€æ–°çš„ç»è¿‡åŒè¡Œè¯„å®¡çš„å…³äºç»“è‚ ç™Œçš„åŒ»å­¦çŸ¥è¯†ã€‚å®ƒç›®å‰æ­£åœ¨è«è²ç‰¹ç™Œç—‡ä¸­å¿ƒè¿›è¡Œè¯„ä¼°ï¼Œä»¥è¾…åŠ©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠä¿¡æ¯æ£€ç´¢ä»»åŠ¡ã€‚è«è²ç‰¹ç™Œç—‡ä¸­å¿ƒæ˜¯ç¾å›½ä¹ƒè‡³å…¨çƒé¡¶å°–çš„ç™Œç—‡ä¸­å¿ƒä¹‹ä¸€ã€‚æˆ‘ä»¬çš„æ··åˆæ¨¡å‹éå¸¸å‡ºè‰²ï¼Œå› ä¸ºå®ƒæ¯”å•ç‹¬ä½¿ç”¨LLMã€KGæˆ–æœç´¢å¼•æ“æ›´èƒ½æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼ŒLLMä¼šå‡ºç°å¹»è§‰å’Œç¾éš¾æ€§é—å¿˜ï¼Œå¹¶ä¸”æ˜¯åœ¨è¿‡æ—¶çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚æœ€æ–°æœ€å‰æ²¿çš„KGsï¼Œå¦‚PrimeKGã€cBioPortalã€ChEMBLå’ŒNCBIç­‰ï¼Œéœ€è¦äººå·¥ç»´æŠ¤ï¼Œå› æ­¤å¾ˆå®¹æ˜“è¿‡æ—¶ã€‚CancerKGæ˜¯æ— ç›‘ç£çš„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ‘„å–å¹¶æ•´ç†æœ€æ–°çš„åŒ»å­¦å‘ç°ã€‚ä¸ºäº†ç¼“è§£LLMçš„ä¸è¶³ï¼Œç»è¿‡éªŒè¯çš„KGä½œä¸ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç•Œé™ã€‚CancerKGå±•ç°äº†äº”ç§ä¸åŒçš„é«˜çº§ç”¨æˆ·ç•Œé¢ï¼Œæ¯ä¸ªç•Œé¢éƒ½é’ˆå¯¹ä¸åŒçš„æ•°æ®æ¨¡å¼è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ›´å¥½åœ°ä¸ºç”¨æˆ·æœåŠ¡å¹¶å¸¦æ¥ä¾¿åˆ©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00223v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æè¿°äº†ä¸€ç§ç»“åˆæœ€æ–°åŒè¡Œè¯„å®¡åŒ»å­¦çŸ¥è¯†ã€é’ˆå¯¹ç»“è‚ ç™Œçš„Webè§„æ¨¡æ··åˆçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¯¥æ¨¡å‹åœ¨Moffittç™Œç—‡ä¸­å¿ƒè¿›è¡ŒåŒ»å­¦ç ”ç©¶å’Œä¸´åºŠä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„è¯„ä¼°ï¼Œè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚ä¸ä¼ ç»Ÿçš„LLMã€KGæˆ–æœç´¢å¼•æ“ç›¸æ¯”ï¼Œå…¶ä¼˜åŠ¿æ˜æ˜¾ã€‚CancerKGå…·æœ‰è‡ªåŠ¨æ‘„å–å’Œç»„ç»‡æœ€æ–°åŒ»å­¦å‘ç°çš„èƒ½åŠ›ï¼Œå¹¶èƒ½ç¼“è§£LLMçš„ç¼ºç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æè¿°äº†ä¸€ç§é’ˆå¯¹ç»“è‚ ç™Œçš„Webè§„æ¨¡æ··åˆçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨Moffittç™Œç—‡ä¸­å¿ƒçš„åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CancerKGç»“åˆäº†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŠ¿ï¼Œå¯ä»¥æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·éœ€æ±‚ã€‚</li>
<li>LLMå­˜åœ¨çš„ç¼ºé™·å¦‚å¹»è§‰å’Œç¾éš¾æ€§é—å¿˜åœ¨è¯¥æ¨¡å‹ä¸­å¾—åˆ°äº†ç¼“è§£ã€‚</li>
<li>CancerKGå…·æœ‰è‡ªåŠ¨æ‘„å–å’Œç»„ç»‡æœ€æ–°åŒ»å­¦å‘ç°çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰5ç§é«˜çº§ç”¨æˆ·ç•Œé¢ï¼Œæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨å¹¶é€‚åº”ä¸åŒçš„æ•°æ®æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52c88ef204ce37873f1bfb2c10e96221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec213c03a44a5a6cef6bed644695f39f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71d8dd9eacc457bc9f1b75cd61e1412d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d21b1206dc864cad2ee2f3d57e6c0d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90283e91348f852c0be390e5a21f22a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-803253925897b30a25c48359f21790fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Implementing-Trust-in-Non-Small-Cell-Lung-Cancer-Diagnosis-with-a-Conformalized-Uncertainty-Aware-AI-Framework-in-Whole-Slide-Images"><a href="#Implementing-Trust-in-Non-Small-Cell-Lung-Cancer-Diagnosis-with-a-Conformalized-Uncertainty-Aware-AI-Framework-in-Whole-Slide-Images" class="headerlink" title="Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a   Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images"></a>Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a   Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images</h2><p><strong>Authors:Xiaoge Zhang, Tao Wang, Chao Yan, Fedaa Najdawi, Kai Zhou, Yuan Ma, Yiu-ming Cheung, Bradley A. Malin</strong></p>
<p>Ensuring trustworthiness is fundamental to the development of artificial intelligence (AI) that is considered societally responsible, particularly in cancer diagnostics, where a misdiagnosis can have dire consequences. Current digital pathology AI models lack systematic solutions to address trustworthiness concerns arising from model limitations and data discrepancies between model deployment and development environments. To address this issue, we developed TRUECAM, a framework designed to ensure both data and model trustworthiness in non-small cell lung cancer subtyping with whole-slide images. TRUECAM integrates 1) a spectral-normalized neural Gaussian process for identifying out-of-scope inputs and 2) an ambiguity-guided elimination of tiles to filter out highly ambiguous regions, addressing data trustworthiness, as well as 3) conformal prediction to ensure controlled error rates. We systematically evaluated the framework across multiple large-scale cancer datasets, leveraging both task-specific and foundation models, illustrate that an AI model wrapped with TRUECAM significantly outperforms models that lack such guidance, in terms of classification accuracy, robustness, interpretability, and data efficiency, while also achieving improvements in fairness. These findings highlight TRUECAM as a versatile wrapper framework for digital pathology AI models with diverse architectural designs, promoting their responsible and effective applications in real-world settings. </p>
<blockquote>
<p>ç¡®ä¿å¯ä¿¡åº¦æ˜¯å‘å±•ç¤¾ä¼šè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„åŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨ç™Œç—‡è¯Šæ–­ä¸­ï¼Œè¯¯è¯Šå¯èƒ½ä¼šå¸¦æ¥ä¸¥é‡çš„åæœã€‚å½“å‰çš„æ•°å­—ç—…ç†AIæ¨¡å‹ç¼ºä¹ç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œæ— æ³•è§£å†³æ¨¡å‹å±€é™æ€§å’Œæ¨¡å‹éƒ¨ç½²ä¸å¼€å‘ç¯å¢ƒä¹‹é—´æ•°æ®å·®å¼‚æ‰€äº§ç”Ÿçš„å¯ä¿¡åº¦é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†TRUECAMæ¡†æ¶ï¼Œæ—¨åœ¨ç¡®ä¿åœ¨éå°ç»†èƒè‚ºç™Œäºšå‹åˆ†ç±»çš„å…¨å¹»ç¯ç‰‡å›¾åƒä¸­æ•°æ®å’Œæ¨¡å‹çš„å¯ä¿¡åº¦ã€‚TRUECAMé›†æˆäº†1ï¼‰å…‰è°±å½’ä¸€åŒ–ç¥ç»é«˜æ–¯è¿‡ç¨‹ï¼Œç”¨äºè¯†åˆ«è¶…å‡ºèŒƒå›´è¾“å…¥ï¼›2ï¼‰æ­§ä¹‰å¼•å¯¼ç“¦ç‰‡æ¶ˆé™¤ï¼Œä»¥è¿‡æ»¤æ‰é«˜åº¦æ¨¡ç³ŠåŒºåŸŸï¼Œè§£å†³æ•°æ®å¯ä¿¡åº¦é—®é¢˜ï¼›ä»¥åŠ3ï¼‰ç¬¦åˆé¢„æµ‹è¦æ±‚ï¼Œä»¥ç¡®ä¿æ§åˆ¶è¯¯å·®ç‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤§è§„æ¨¡ç™Œç—‡æ•°æ®é›†ä¸Šç³»ç»Ÿåœ°è¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œåˆ©ç”¨ç‰¹å®šä»»åŠ¡å’ŒåŸºç¡€æ¨¡å‹ï¼Œè¡¨æ˜ç”¨TRUECAMåŒ…è£…çš„AIæ¨¡å‹åœ¨åˆ†ç±»ç²¾åº¦ã€ç¨³å¥æ€§ã€å¯è§£é‡Šæ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç¼ºä¹æ­¤ç±»æŒ‡å¯¼çš„æ¨¡å‹ï¼ŒåŒæ—¶åœ¨å…¬å¹³æ€§æ–¹é¢ä¹Ÿå®ç°äº†æ”¹è¿›ã€‚è¿™äº›å‘ç°çªå‡ºäº†TRUECAMä½œä¸ºä¸€ä¸ªé€šç”¨åŒ…è£…æ¡†æ¶ï¼Œé€‚ç”¨äºå…·æœ‰ä¸åŒæ¶æ„è®¾è®¡çš„æ•°å­—ç—…ç†AIæ¨¡å‹ï¼Œä¿ƒè¿›å…¶åœ¨å®é™…ç¯å¢ƒä¸­çš„è´Ÿè´£ä»»å’Œæœ‰æ•ˆåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äººå·¥æ™ºèƒ½åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„ä¿¡ä»»åº¦é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éå°ç»†èƒè‚ºç™Œäºšå‹è¯Šæ–­ä¸­ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†TRUECAMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¤šé¡¹æŠ€æœ¯ç¡®ä¿æ•°æ®å’Œæ¨¡å‹çš„ä¿¡ä»»åº¦ï¼ŒåŒ…æ‹¬è¯†åˆ«è¶…å‡ºèŒƒå›´è¾“å…¥çš„ç¥ç»é«˜æ–¯è¿‡ç¨‹ã€è¿‡æ»¤é«˜åº¦æ¨¡ç³ŠåŒºåŸŸçš„æ¨¡ç³Šå¼•å¯¼ç“·ç –æ¶ˆé™¤æ³•ä»¥åŠç¡®ä¿æ§åˆ¶é”™è¯¯ç‡çš„é¢„æµ‹ä¸€è‡´æ€§ã€‚ç»è¿‡å¤§è§„æ¨¡ç™Œç—‡æ•°æ®é›†çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°TRUECAMåŒ…è£…çš„AIæ¨¡å‹åœ¨åˆ†ç±»ç²¾åº¦ã€ç¨³å¥æ€§ã€å¯è§£é‡Šæ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç¼ºä¹æ­¤ç±»æŒ‡å¯¼çš„æ¨¡å‹ï¼Œå¹¶å®ç°äº†å…¬å¹³æ€§çš„æ”¹è¿›ã€‚è¿™è¡¨æ˜TRUECAMæ¡†æ¶é€‚ç”¨äºå„ç§è®¾è®¡çš„æ•°å­—ç—…ç†å­¦AIæ¨¡å‹ï¼Œä¿ƒè¿›äº†å…¶åœ¨ç°å®ç¯å¢ƒä¸­çš„è´Ÿè´£ä»»å’Œæœ‰æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„ä¿¡ä»»åº¦è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éå°ç»†èƒè‚ºç™Œäºšå‹è¯Šæ–­ä¸­ã€‚</li>
<li>å½“å‰æ•°å­—ç—…ç†å­¦AIæ¨¡å‹ç¼ºä¹è§£å†³ä¿¡ä»»åº¦é—®é¢˜çš„ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>TRUECAMæ¡†æ¶è¢«å¼€å‘å‡ºæ¥è§£å†³æ•°æ®å’Œæ¨¡å‹çš„ä¿¡ä»»åº¦é—®é¢˜ã€‚</li>
<li>TRUECAMé€šè¿‡å¤šé¡¹æŠ€æœ¯ç¡®ä¿ä¿¡ä»»åº¦ï¼ŒåŒ…æ‹¬ç¥ç»é«˜æ–¯è¿‡ç¨‹ã€æ¨¡ç³Šå¼•å¯¼ç“·ç –æ¶ˆé™¤æ³•å’Œé¢„æµ‹ä¸€è‡´æ€§ã€‚</li>
<li>TRUECAMåŒ…è£…çš„AIæ¨¡å‹åœ¨åˆ†ç±»ç²¾åº¦ã€ç¨³å¥æ€§ã€å¯è§£é‡Šæ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>TRUECAMå®ç°äº†å…¬å¹³æ€§çš„æ”¹è¿›ï¼Œé€‚ç”¨äºå„ç§è®¾è®¡çš„æ•°å­—ç—…ç†å­¦AIæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fc47ea8d33551c77062cdc1b02d2d11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59be06150b91d67a011e1b27563a9b22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64bbed08f3be1d56166db07a246ee3db.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-31fe4a3ff39073ac066258f84924473e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  RingFormer A Neural Vocoder with Ring Attention and   Convolution-Augmented Transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d8f85e42e9f14b456e9e4de302584947.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  Reconstruction vs. Generation Taming Optimization Dilemma in Latent   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18179.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
