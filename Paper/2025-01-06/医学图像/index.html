<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-06  nnY-Net Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9216ff3b6696c7df669a6c8783190fc4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    51 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-06-更新"><a href="#2025-01-06-更新" class="headerlink" title="2025-01-06 更新"></a>2025-01-06 更新</h1><h2 id="nnY-Net-Swin-NeXt-with-Cross-Attention-for-3D-Medical-Images-Segmentation"><a href="#nnY-Net-Swin-NeXt-with-Cross-Attention-for-3D-Medical-Images-Segmentation" class="headerlink" title="nnY-Net: Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation"></a>nnY-Net: Swin-NeXt with Cross-Attention for 3D Medical Images   Segmentation</h2><p><strong>Authors:Haixu Liu, Zerui Tao, Wenzhen Dong, Qiuzhuang Sun</strong></p>
<p>This paper provides a novel 3D medical image segmentation model structure called nnY-Net. This name comes from the fact that our model adds a cross-attention module at the bottom of the U-net structure to form a Y structure. We integrate the advantages of the two latest SOTA models, MedNeXt and SwinUNETR, and use Swin Transformer as the encoder and ConvNeXt as the decoder to innovatively design the Swin-NeXt structure. Our model uses the lowest-level feature map of the encoder as Key and Value and uses patient features such as pathology and treatment information as Query to calculate the attention weights in a Cross Attention module. Moreover, we simplify some pre- and post-processing as well as data enhancement methods in 3D image segmentation based on the dynUnet and nnU-net frameworks. We integrate our proposed Swin-NeXt with Cross-Attention framework into this framework. Last, we construct a DiceFocalCELoss to improve the training efficiency for the uneven data convergence of voxel classification. </p>
<blockquote>
<p>本文提出了一种新型的3D医学图像分割模型结构，名为nnY-Net。这个名字的由来是因为我们的模型在U-net结构的底部添加了一个交叉注意模块，形成Y形结构。我们整合了最新两个顶尖模型MedNeXt和SwinUNETR的优点，创新地设计了Swin-NeXt结构，使用Swin Transformer作为编码器，ConvNeXt作为解码器。我们的模型以编码器的最低级特征图作为键和值，以患者特征（如病理和治疗信息）作为查询，在交叉注意模块中计算注意力权重。此外，我们简化了基于dynUnet和nnU-net框架的3D图像分割的预处理和后处理以及数据增强方法。我们将我们提出的带有交叉注意框架的Swin-NeXt集成到这一框架中。最后，我们构建了DiceFocalCELoss，以提高voxel分类数据不均匀时的训练效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01406v1">PDF</a> MICCAI</p>
<p><strong>Summary</strong></p>
<p>基于U-net结构的新型三维医学图像分割模型nnY-Net被提出。该模型结合了MedNeXt和SwinUNETR两种最新顶尖模型的优势，采用Swin Transformer作为编码器，ConvNeXt作为解码器，创新设计了Swin-NeXt结构。模型使用编码器最低级别的特征映射作为键和值，并利用患者特征（如病理和治疗信息）作为查询来计算交叉注意力模块中的注意力权重。此外，基于dynUnet和nnU-net框架简化了部分预处理和后处理以及数据增强方法。最后构建了DiceFocalCELoss以提高数据不均衡收敛时的训练效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>nnY-Net是一种新型的三维医学图像分割模型，基于U-net结构，结合了MedNeXt和SwinUNETR的优势。</li>
<li>模型创新性地结合了Swin Transformer编码器和ConvNeXt解码器，构成Swin-NeXt结构。</li>
<li>模型使用交叉注意力模块，利用患者特征（如病理和治疗信息）计算注意力权重。</li>
<li>该模型简化了预处理和后处理流程以及数据增强方法，基于dynUnet和nnU-net框架。</li>
<li>nnY-Net框架中集成了Swin-NeXt与交叉注意力框架。</li>
<li>模型构建了DiceFocalCELoss，旨在提高数据不均衡收敛时的训练效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01406">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cad70830084c6f01ba4c437c28fcab55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b53b193fe59428c708cd27185f8424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05340efecda79319b2b16eb40ac73c91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e961fc0ec9f3afa0888d01a904d260d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e809ede714ad84f20d613635db0a7709.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a31eff7d86229a1acb993402c8e5ee2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a79a0d9c7fa82d7ece5a772ce996feab.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ProjectedEx-Enhancing-Generation-in-Explainable-AI-for-Prostate-Cancer"><a href="#ProjectedEx-Enhancing-Generation-in-Explainable-AI-for-Prostate-Cancer" class="headerlink" title="ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer"></a>ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer</h2><p><strong>Authors:Xuyin Qi, Zeyu Zhang, Aaron Berliano Handoko, Huazhan Zheng, Mingxi Chen, Ta Duc Huy, Vu Minh Hieu Phan, Lei Zhang, Linqi Cheng, Shiyu Jiang, Zhiwei Zhang, Zhibin Liao, Yang Zhao, Minh-Son To</strong></p>
<p>Prostate cancer, a growing global health concern, necessitates precise diagnostic tools, with Magnetic Resonance Imaging (MRI) offering high-resolution soft tissue imaging that significantly enhances diagnostic accuracy. Recent advancements in explainable AI and representation learning have significantly improved prostate cancer diagnosis by enabling automated and precise lesion classification. However, existing explainable AI methods, particularly those based on frameworks like generative adversarial networks (GANs), are predominantly developed for natural image generation, and their application to medical imaging often leads to suboptimal performance due to the unique characteristics and complexity of medical image. To address these challenges, our paper introduces three key contributions. First, we propose ProjectedEx, a generative framework that provides interpretable, multi-attribute explanations, effectively linking medical image features to classifier decisions. Second, we enhance the encoder module by incorporating feature pyramids, which enables multiscale feedback to refine the latent space and improves the quality of generated explanations. Additionally, we conduct comprehensive experiments on both the generator and classifier, demonstrating the clinical relevance and effectiveness of ProjectedEx in enhancing interpretability and supporting the adoption of AI in medical settings. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/Richardqiyi/ProjectedEx">https://github.com/Richardqiyi/ProjectedEx</a> </p>
<blockquote>
<p>前列腺癌是一个日益严重的全球健康问题，需要精确的诊断工具。磁共振成像（MRI）提供了高分辨率的软组织成像，显著提高了诊断的准确性。最近，在可解释的人工智能和表示学习方面的进展已经通过实现自动化和精确的病变分类显著改善了前列腺癌的诊断。然而，现有的可解释人工智能方法，特别是基于生成对抗网络（GANs）等框架的方法，主要是为自然图像生成而开发的，由于其独特的特点和医学图像的复杂性，它们在医学成像中的应用往往导致性能不佳。为了应对这些挑战，我们的论文提出了三个主要贡献。首先，我们提出了ProjectedEx，一个生成框架，提供可解释的多属性解释，有效地将医学图像特征与分类器决策联系起来。其次，我们通过融入特征金字塔增强了编码器模块，这启用了多尺度反馈来优化潜在空间并提高了生成解释的质量。此外，我们对生成器和分类器都进行了全面的实验，证明了ProjectedEx在临床相关性和有效性方面的作用，提高了可解释性并支持在医疗环境中采用人工智能。代码将在<a target="_blank" rel="noopener" href="https://github.com/Richardqiyi/ProjectedEx%E5%85%AC%E5%B8%83%E3%80%82">https://github.com/Richardqiyi/ProjectedEx公布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01392v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本摘要提出了一种针对前列腺癌诊断的新型解释性人工智能框架——ProjectedEx。该框架结合了磁共振成像技术和解释性人工智能，能有效链接医学图像特征与分类器决策，提高诊断准确性。此外，该项目通过融入特征金字塔增强编码器模块，改善了潜在空间的质量，提高了生成的解释质量。实验证明，ProjectedEx在临床相关性和提高人工智能解释性方面效果显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>前列腺癌成为全球健康难题，需要精确的诊断工具。</li>
<li>磁共振成像（MRI）技术为前列腺癌诊断提供了高分辨率软组织成像，提高了诊断准确性。</li>
<li>人工智能和表示学习的发展改善了前列腺癌的自动和精确病灶分类。</li>
<li>现有的人工智能解释方法，特别是基于生成对抗网络（GANs）的方法，在医学成像中的应用因医学图像的独特特征和复杂性而表现不佳。</li>
<li>ProjectedEx是一个新的解释性人工智能框架，有效链接医学图像特征和分类器决策。</li>
<li>通过融入特征金字塔增强编码器模块，ProjectedEx改善了潜在空间的质量，提高了生成的解释质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01392">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1858110555e0087fdc962b6bfa74404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb119cd92e1a31df669336472e554cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef4c3c7590662467a8c707cec07cf414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c203503ff0dbe71c81dc3e15610778c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e43db92f4d70b52fe56e663d19e37284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfbab2b7132b7f188418f81a6bb8d1eb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generalized-Task-Driven-Medical-Image-Quality-Enhancement-with-Gradient-Promotion"><a href="#Generalized-Task-Driven-Medical-Image-Quality-Enhancement-with-Gradient-Promotion" class="headerlink" title="Generalized Task-Driven Medical Image Quality Enhancement with Gradient   Promotion"></a>Generalized Task-Driven Medical Image Quality Enhancement with Gradient   Promotion</h2><p><strong>Authors:Dong Zhang, Kwang-Ting Cheng</strong></p>
<p>Thanks to the recent achievements in task-driven image quality enhancement (IQE) models like ESTR, the image enhancement model and the visual recognition model can mutually enhance each other’s quantitation while producing high-quality processed images that are perceivable by our human vision systems. However, existing task-driven IQE models tend to overlook an underlying fact – different levels of vision tasks have varying and sometimes conflicting requirements of image features. To address this problem, this paper proposes a generalized gradient promotion (GradProm) training strategy for task-driven IQE of medical images. Specifically, we partition a task-driven IQE system into two sub-models, i.e., a mainstream model for image enhancement and an auxiliary model for visual recognition. During training, GradProm updates only parameters of the image enhancement model using gradients of the visual recognition model and the image enhancement model, but only when gradients of these two sub-models are aligned in the same direction, which is measured by their cosine similarity. In case gradients of these two sub-models are not in the same direction, GradProm only uses the gradient of the image enhancement model to update its parameters. Theoretically, we have proved that the optimization direction of the image enhancement model will not be biased by the auxiliary visual recognition model under the implementation of GradProm. Empirically, extensive experimental results on four public yet challenging medical image datasets demonstrated the superior performance of GradProm over existing state-of-the-art methods. </p>
<blockquote>
<p>得益于最近任务驱动图像质量增强（IQE）模型（如ESTR）的成就，图像增强模型和视觉识别模型可以相互促进彼此的量化，同时生成高质量的处理图像，这些图像可以被我们的视觉系统所感知。然而，现有的任务驱动IQE模型往往忽视了一个基本事实——不同层次的视觉任务对图像特征的要求各不相同，有时甚至存在冲突。为了解决这一问题，本文提出了一种用于医学图像任务驱动的IQE的广义梯度提升（GradProm）训练策略。具体来说，我们将任务驱动的IQE系统划分为两个子模型，即用于图像增强的主流模型和用于视觉识别的辅助模型。在训练过程中，GradProm仅使用视觉识别模型和图像增强模型的梯度来更新图像增强模型的参数，但仅在两个子模型的梯度方向相同时才进行更新，这通过它们的余弦相似性来衡量。如果这两个子模型的梯度方向不一致，GradProm只使用图像增强模型的梯度来更新其参数。理论上，我们已经证明，在GradProm的实现下，图像增强模型的优化方向不会受到辅助视觉识别模型的影响。在四个公开且具有挑战性的医学图像数据集上的大量实验结果表明，GradProm的性能优于现有的最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01114v1">PDF</a> This paper has been accepted by IEEE Transactions on Pattern Analysis   and Machine Intelligence</p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对医学图像任务驱动图像质量增强（IQE）的广义梯度提升（GradProm）训练策略。该策略将IQE系统分为两个子模型：主流模型用于图像增强，辅助模型用于视觉识别。训练时，GradProm仅使用视觉识别模型和图像增强模型的梯度更新图像增强模型的参数，且仅在两者梯度方向相同时进行。当梯度方向不一致时，仅使用图像增强模型的梯度更新参数。理论证明，GradProm实施下，图像增强模型的优化方向不会受到辅助视觉识别模型的影响。在四个公共且具有挑战性的医学图像数据集上的实验结果表明，GradProm优于现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>任务驱动图像质量增强（IQE）模型如ESTR能够互相提升图像增强模型和视觉识别模型的量化能力。</li>
<li>现有IQE模型忽略了不同视觉任务对图像特征的不同甚至冲突的需求。</li>
<li>本文提出的广义梯度提升（GradProm）训练策略针对医学图像的任务驱动IQE。</li>
<li>GradProm将IQE系统分为两个子模型：用于图像增强的主流模型和用于视觉识别的辅助模型。</li>
<li>GradProm在训练时仅使用两个子模型梯度一致时的梯度更新图像增强模型的参数。</li>
<li>理论证明，GradProm保证图像增强模型的优化方向不会受到辅助视觉识别模型的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-41df2c2a7408f7e1c35d05cc1d105308.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e66697d11626d6e6c1dc5da2e7778d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ae5fdbf8db6c80fb79f7f3114c3ab0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-704768b0df9e3d5752f6357e24a17199.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scale-wise-Bidirectional-Alignment-Network-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Scale-wise-Bidirectional-Alignment-Network-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Scale-wise Bidirectional Alignment Network for Referring Remote Sensing   Image Segmentation"></a>Scale-wise Bidirectional Alignment Network for Referring Remote Sensing   Image Segmentation</h2><p><strong>Authors:Kun Li, George Vosselman, Michael Ying Yang</strong></p>
<p>The goal of referring remote sensing image segmentation (RRSIS) is to extract specific pixel-level regions within an aerial image via a natural language expression. Recent advancements, particularly Transformer-based fusion designs, have demonstrated remarkable progress in this domain. However, existing methods primarily focus on refining visual features using language-aware guidance during the cross-modal fusion stage, neglecting the complementary vision-to-language flow. This limitation often leads to irrelevant or suboptimal representations. In addition, the diverse spatial scales of ground objects in aerial images pose significant challenges to the visual perception capabilities of existing models when conditioned on textual inputs. In this paper, we propose an innovative framework called Scale-wise Bidirectional Alignment Network (SBANet) to address these challenges for RRSIS. Specifically, we design a Bidirectional Alignment Module (BAM) with learnable query tokens to selectively and effectively represent visual and linguistic features, emphasizing regions associated with key tokens. BAM is further enhanced with a dynamic feature selection block, designed to provide both macro- and micro-level visual features, preserving global context and local details to facilitate more effective cross-modal interaction. Furthermore, SBANet incorporates a text-conditioned channel and spatial aggregator to bridge the gap between the encoder and decoder, enhancing cross-scale information exchange in complex aerial scenarios. Extensive experiments demonstrate that our proposed method achieves superior performance in comparison to previous state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both quantitatively and qualitatively. The code will be released after publication. </p>
<blockquote>
<p>遥感图像分割（RRSIS）的目标是通过对自然语言表达式进行解析，提取航空图像中的特定像素级区域。最近的进展，特别是基于Transformer的融合设计，已经在这个领域取得了显著的进步。然而，现有的方法主要关注在跨模态融合阶段使用语言感知指导来优化视觉特征，忽视了从视觉到语言的互补流。这一局限性通常会导致表示不相关或次优。此外，航空图像中地面对象的各种空间尺度对模型的视觉感知能力构成了挑战，尤其是在接受文本输入的情况下。在本文中，我们提出了一种名为尺度双向对齐网络（SBANet）的创新框架来解决RRSIS的这些挑战。具体来说，我们设计了一个双向对齐模块（BAM），其中包含可学习的查询令牌，以选择性和有效地表示视觉和语言特征，强调与关键令牌相关的区域。BAM通过动态特征选择块进一步增强，旨在提供宏观和微观级别的视觉特征，保留全局上下文和局部细节，以促进更有效的跨模态交互。此外，SBANet结合了文本条件通道和空间聚合器，以缩小编码器和解码器之间的差距，增强复杂航空场景中跨尺度信息交换。大量实验表明，与RRSIS-D和RefSegRS数据集上的最新方法相比，我们的方法在定量和定性方面都实现了卓越的性能。代码将在发表后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00851v1">PDF</a> Under review</p>
<p><strong>摘要</strong></p>
<p>遥感图像分割（RRSIS）的目标是通过自然语言表达式提取航空图像中的特定像素级区域。最近的进步，特别是基于Transformer的融合设计，已经在这个领域取得了显著的进步。然而，现有方法主要集中在利用语言感知指导来优化视觉特征，忽视了从视觉到语言的互补流动。这种局限性通常会导致表示不相关或次优。此外，航空图像中地面对象的空间尺度多样性给现有模型在文本输入条件下的视觉感知能力带来了挑战。本文提出了一种创新的框架，称为Scale-wise Bidirectional Alignment Network (SBANet)，以解决RRSIS的这些挑战。我们设计了带有可学习查询标记的双向对齐模块（BAM），以选择和有效地表示视觉和语言特征，强调与关键标记相关的区域。BAM进一步通过动态特征选择块增强，旨在提供宏观和微观级别的视觉特征，保留全局上下文和局部细节，以促进更有效的跨模态交互。此外，SBANet结合了文本条件通道和空间聚合器，以缩小编码器和解码器之间的差距，增强复杂航空场景中的跨尺度信息交换。大量实验表明，与RRSIS-D和RefSegRS数据集上的最新方法相比，我们所提出的方法在定量和定性方面都实现了卓越的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>遥感图像分割（RRSIS）的目标是通过自然语言表达式提取航空图像中的特定像素级区域。</li>
<li>现有方法主要关注于使用语言感知指导在跨模态融合阶段优化视觉特征，但忽视了从视觉到语言的互补流动。</li>
<li>航空图像中地面对象的空间尺度多样性给现有模型带来挑战。</li>
<li>本文提出了一种创新的框架SBANet，包括双向对齐模块（BAM）和动态特征选择块，以更有效地处理视觉和语言特征。</li>
<li>SBANet通过文本条件通道和空间聚合器增强了跨尺度信息交换。</li>
<li>实验表明，SBANet在RRSIS-D和RefSegRS数据集上的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00851">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-de9e1bb6199a8ef973959aeea86279cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b349292257179e51f0549c4c4d6c8e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Role-of-Chalcogen-atoms-in-In-Situ-Exfoliation-for-Large-Area-2D-Semiconducting-Transition-Metal-Dichalcogenides"><a href="#Role-of-Chalcogen-atoms-in-In-Situ-Exfoliation-for-Large-Area-2D-Semiconducting-Transition-Metal-Dichalcogenides" class="headerlink" title="Role of Chalcogen atoms in In Situ Exfoliation for Large-Area 2D   Semiconducting Transition Metal Dichalcogenides"></a>Role of Chalcogen atoms in In Situ Exfoliation for Large-Area 2D   Semiconducting Transition Metal Dichalcogenides</h2><p><strong>Authors:Zhiying Dan, Ronak Sarmasti Emami, Giovanna Feraco, Melina Vavali, Dominic Gerlach, Petra Rudolf, Antonija Grubišić-Čabo</strong></p>
<p>Two-dimensional (2D) transition metal dichalcogenides have emerged as a promising platform for next-generation optoelectronic and spintronic devices. Mechanical exfoliation using adhesive tape remains the dominant method for preparing 2D materials of highest quality, including transition metal dichalcogenides, but always results in small-sized flakes. This limitation poses a significant challenge for investigations and applications where large scale flakes are needed. To overcome these constraints, we explored the preparation of 2D WS2 and WSe2 using a recently developed kinetic in situ single-layer synthesis method (KISS). In particular, we focused on the influence of different substrates, Au and Ag, and chalcogen atoms, S and Se, on the yield and quality of the 2D films. The crystallinity and spatial morphology of the 2D films were characterized using optical microscopy and atomic force microscopy, providing a comprehensive assessment of exfoliation quality. Low-energy electron diffraction verified that there is no preferential orientation between the 2D film and the substrate, while optical microscopy revealed that WSe2 consistently outperformed WS2 in producing large monolayers, regardless of the substrate used. Finally, X-ray diffraction and X-ray photoelectron spectroscopy demonstrate that no covalent bonds are formed between the 2D material and the underlying substrate. These results identify KISS method as a non-destructive approach for a more scalable approach of high-quality 2D transition metal dichalcogenides. </p>
<blockquote>
<p>二维（2D）过渡金属二卤化物已成为下一代光电子和自旋电子器件的有前途的平台。使用胶带剥离法仍是制备包括过渡金属二卤化物在内的高质量二维材料的主要方法，但总是产生尺寸较小的薄片。这一局限性对于需要大规模薄片的研究和应用构成重大挑战。为了克服这些限制，我们探索了使用最近开发的动力学原位单层合成方法（KISS）制备二维WS2和WSe2。特别是，我们关注不同的基底（Au和Ag）以及硫族原子（S和Se）对二维薄膜的产量和质量的影响。二维薄膜的结晶度和空间形态通过光学显微镜和原子力显微镜表征，从而全面评估剥离质量。低能电子衍射证明二维薄膜与基底之间不存在择优取向，而光学显微镜显示，无论使用何种基底，WSe2在生成大型单层方面始终优于WS2。最后，X射线衍射和X射线光电子能谱证明二维材料与底层基底之间没有形成共价键。这些结果将KISS方法确定为一种非破坏性的方法，可以更大规模地应用高质量二维过渡金属二卤化物。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00815v1">PDF</a> Article (13 pages, 5 figures) and supporting information (5 pages, 6   figures)</p>
<p><strong>Summary</strong></p>
<p>本文探索了使用新型的KISS方法合成二维WS2和WSe2材料，研究了不同基底（Au和Ag）和硫族元素（S和Se）对二维薄膜的产率和品质的影响。该方法被证实可以有效合成高质量大尺寸二维薄膜材料，并未对基底形成任何共价键，是一种可扩展的非破坏性方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>过渡金属二卤化物在光电子学和自旋电子学器件领域具有潜力。</li>
<li>机械剥离法仍是制备高质量二维材料的常用方法，但难以获得大尺寸薄膜。</li>
<li>KISS方法可用于合成二维WS2和WSe2材料。</li>
<li>基底类型和硫族元素对二维薄膜的产率和质量有影响。</li>
<li>KISS方法合成的二维薄膜具有优良的单晶结构和空间形态。</li>
<li>非晶化验证显示基底与二维薄膜间没有优先取向性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-092196aa8c5d5e856edd171d789849e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c704a21ae19b6686f29a3d9942d701a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a480a3dd57c6464fe7d30fb406d11b58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c1cc28fb69ea17e8f5dcd16fe996e61.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HCMA-UNet-A-Hybrid-CNN-Mamba-UNet-with-Inter-Slice-Self-Attention-for-Efficient-Breast-Cancer-Segmentation"><a href="#HCMA-UNet-A-Hybrid-CNN-Mamba-UNet-with-Inter-Slice-Self-Attention-for-Efficient-Breast-Cancer-Segmentation" class="headerlink" title="HCMA-UNet: A Hybrid CNN-Mamba UNet with Inter-Slice Self-Attention for   Efficient Breast Cancer Segmentation"></a>HCMA-UNet: A Hybrid CNN-Mamba UNet with Inter-Slice Self-Attention for   Efficient Breast Cancer Segmentation</h2><p><strong>Authors:Haoxuan Li, Wei song, Peiwu Qin, Xi Yuan, Zhenglin Chen</strong></p>
<p>Breast cancer lesion segmentation in DCE-MRI remains challenging due to heterogeneous tumor morphology and indistinct boundaries. To address these challenges, this study proposes a novel hybrid segmentation network, HCMA-UNet, for lesion segmentation of breast cancer. Our network consists of a lightweight CNN backbone and a Multi-view Inter-Slice Self-Attention Mamba (MISM) module. The MISM module integrates Visual State Space Block (VSSB) and Inter-Slice Self-Attention (ISSA) mechanism, effectively reducing parameters through Asymmetric Split Channel (ASC) strategy to achieve efficient tri-directional feature extraction. Our lightweight model achieves superior performance with 2.87M parameters and 126.44 GFLOPs. A Feature-guided Region-aware loss function (FRLoss) is proposed to enhance segmentation accuracy. Extensive experiments on one private and two public DCE-MRI breast cancer datasets demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. FRLoss also exhibits good cross-architecture generalization capabilities. The source code and dataset is available on this link. </p>
<blockquote>
<p>乳腺癌在动态对比增强磁共振成像（DCE-MRI）中的病灶分割仍然是一个挑战，这是由于肿瘤形态学异质性以及边界不清晰所导致的。为了解决这些挑战，本研究提出了一种新型的混合分割网络HCMA-UNet，用于乳腺癌病灶的分割。我们的网络由轻量级的CNN骨干网和多视图切片间自注意力Mamba（MISM）模块组成。MISM模块结合了视觉状态空间块（VSSB）和切片间自注意力（ISSA）机制，通过不对称分裂通道（ASC）策略有效地减少参数，实现高效的三向特征提取。我们的轻量级模型具有出色的性能，包含287万参数和每秒浮点运算次数为126.44次。为了提高分割精度，提出了基于特征的区域感知损失函数（FRLoss）。在一个私有和两个公开的动态对比增强磁共振成像乳腺癌数据集上进行的广泛实验表明，我们的方法在实现最新性能的同时保持了计算效率。此外，FRLoss还展现出良好的跨架构泛化能力。源代码和数据集可在链接中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00751v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的混合分割网络HCMA-UNet，用于乳腺癌病变的分割。该网络包含轻量级CNN骨干和Multi-view Inter-Slice Self-Attention Mamba（MISM）模块，实现了高效的三向特征提取。通过采用特征引导的Region感知损失函数（FRLoss），提高了分割精度。在多个数据集上的实验表明，该方法达到了最先进的性能，同时保持了计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>乳腺癌DCE-MRI图像分割具有挑战性，主要由于肿瘤形态异质性和边界模糊。</li>
<li>提出了一种新型的混合分割网络HCMA-UNet，包含轻量级CNN骨干和MISM模块。</li>
<li>MISM模块结合了VSSB和ISSA机制，通过ASC策略有效减少参数，实现三向特征提取。</li>
<li>HCMA-UNet模型性能优越，参数为2.87M，计算量为126.44 GFLOPs。</li>
<li>引入FRLoss函数提高分割精度，并在多个数据集上实现最佳性能。</li>
<li>FRLoss函数具有良好的跨架构泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-084e301a87c6e24dffd403ab6a6dff3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd751d360b8308907f3456ebfc25f396.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c723be3e799c9c5d976f738c4b9f3c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd4d097bc68d769e9772f6d01967d69e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5934bd1982449c251184c2fdb630919f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecbd81ff230e91b720cc89a139427ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fb5afc2f821b301684dab330bccb2e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44433dcf49a57a1440b775b349924390.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deeply-Learned-Robust-Matrix-Completion-for-Large-scale-Low-rank-Data-Recovery"><a href="#Deeply-Learned-Robust-Matrix-Completion-for-Large-scale-Low-rank-Data-Recovery" class="headerlink" title="Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data   Recovery"></a>Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data   Recovery</h2><p><strong>Authors:HanQin Cai, Chandra Kundu, Jialin Liu, Wotao Yin</strong></p>
<p>Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fix-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery. </p>
<blockquote>
<p>鲁棒矩阵补全（RMC）是一种广泛应用于机器学习领域的工具，它能同时解决低秩数据分析中的两个关键问题：数据缺失和极端异常值。本文针对大规模RMC问题，提出了一种新的可伸缩、可学习的非凸方法，称为学习鲁棒矩阵补全（LRMC）。LRMC具有较低的计算复杂度，并能实现线性收敛。受定理的启发，LRMC的自由参数可以通过深度展开进行有效地学习，以达到最佳性能。此外，本文提出了一种灵活的反馈递归混合神经网络框架，它将深度展开从固定迭代扩展到无限迭代。通过大量的实验验证了LRMC在合成数据集和实际应用中的卓越表现，包括视频背景去除、超声成像、面部建模以及卫星图像的云去除等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00677v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2110.05649</p>
<p><strong>Summary</strong><br>    新型可学习非凸方法（LRMC）应用于大规模鲁棒矩阵补全（RMC），该方法具备低计算复杂度及线性收敛性，并能通过深度展开有效地学习自由参数以获得最佳性能。此外，本文提出了一个灵活的混合神经网络框架，将深度展开从固定迭代次数扩展到无限迭代次数。LRMC在合成数据集和实际应用中的表现均优于其他最新技术，包括视频背景去除、超声成像、面部建模和卫星图像去云等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMC是一种新型的可学习非凸方法，用于解决大规模鲁棒矩阵补全（RMC）问题。</li>
<li>LRMC具备低计算复杂度和线性收敛性。</li>
<li>通过深度展开，LRMC的自由参数可以有效地学习，以实现最佳性能。</li>
<li>提出了一个灵活的混合神经网络框架，将深度展开从固定迭代扩展到无限迭代。</li>
<li>LRMC在合成数据集上的表现优于其他最新技术。</li>
<li>LRMC在视频背景去除、超声成像、面部建模等实际应用中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f20ad059582fb62f303c903a6e75896b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00eea286e65f66fc6866401f5d57c293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0678c1412eeec80874e5be0d281bdd40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c68eb04538549d2235debdd2ea7fa1fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-707121b2f423acb3e0635ceb95bac1db.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leaf-diseases-detection-using-deep-learning-methods"><a href="#Leaf-diseases-detection-using-deep-learning-methods" class="headerlink" title="Leaf diseases detection using deep learning methods"></a>Leaf diseases detection using deep learning methods</h2><p><strong>Authors:El Houcine El Fatimi</strong></p>
<p>This study, our main topic is to devlop a new deep-learning approachs for plant leaf disease identification and detection using leaf image datasets. We also discussed the challenges facing current methods of leaf disease detection and how deep learning may be used to overcome these challenges and enhance the accuracy of disease detection. Therefore, we have proposed a novel method for the detection of various leaf diseases in crops, along with the identification and description of an efficient network architecture that encompasses hyperparameters and optimization methods. The effectiveness of different architectures was compared and evaluated to see the best architecture configuration and to create an effective model that can quickly detect leaf disease. In addition to the work done on pre-trained models, we proposed a new model based on CNN, which provides an efficient method for identifying and detecting plant leaf disease. Furthermore, we evaluated the efficacy of our model and compared the results to those of some pre-trained state-of-the-art architectures. </p>
<blockquote>
<p>本研究的主要课题是开发一种新的深度学习方法来识别农作物叶片疾病并检测叶片图像数据集。我们还讨论了当前叶片疾病检测方法面临的挑战，以及如何利用深度学习来克服这些挑战并增强疾病检测的准确性。因此，我们提出了一种检测农作物各种叶片疾病的新方法，并确定和描述了一个有效的网络架构，该架构涵盖了超参数和优化方法。我们比较并评估了不同架构的有效性，以找出最佳的架构配置并创建一个可以快速检测叶片疾病的有效模型。除了对预训练模型所做的工作外，我们还提出了一种基于CNN的新模型，该模型提供了一种有效识别与检测植物叶片疾病的方法。此外，我们还评估了我们模型的效力，并将结果与某些先进的预训练架构进行了比较。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00669v1">PDF</a> 252 pages , 42 images</p>
<p><strong>Summary</strong><br>本研究提出了一种新的深度学习方法和网络架构，用于农作物叶片疾病的识别和检测。通过对比不同架构的效能，最终确定了最佳配置，提高了叶片疾病检测的准确性。同时，本研究也基于CNN提出了一个新模型，并与一些预训练的最先进架构进行了比较和评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究关注植物叶片疾病的识别和检测，提出新的深度学习方法和网络架构。</li>
<li>研究探讨了当前叶片疾病检测方法的挑战，并探讨了深度学习如何克服这些挑战。</li>
<li>通过对比不同架构的效能，确定了最佳网络配置，提高了叶片疾病检测的准确性。</li>
<li>研究提出了一种基于CNN的新模型，用于叶片疾病的识别和检测。</li>
<li>新模型展示了高效的叶片疾病识别与检测能力。</li>
<li>研究对提出的模型进行了评估，并与一些预训练的最先进架构进行了比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5cc5d76c2ec01101252294bc326d8f34.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Lightweight-G-YOLOv11-Advancing-Efficient-Fracture-Detection-in-Pediatric-Wrist-X-rays"><a href="#Lightweight-G-YOLOv11-Advancing-Efficient-Fracture-Detection-in-Pediatric-Wrist-X-rays" class="headerlink" title="Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in   Pediatric Wrist X-rays"></a>Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in   Pediatric Wrist X-rays</h2><p><strong>Authors:Abdesselam Ferdi</strong></p>
<p>Computer-aided diagnosis (CAD) systems have greatly improved the interpretation of medical images by radiologists and surgeons. However, current CAD systems for fracture detection in X-ray images primarily rely on large, resource-intensive detectors, which limits their practicality in clinical settings. To address this limitation, we propose a novel lightweight CAD system based on the YOLO detector for fracture detection. This system, named ghost convolution-based YOLOv11 (G-YOLOv11), builds on the latest version of the YOLO detector family and incorporates the ghost convolution operation for feature extraction. The ghost convolution operation generates the same number of feature maps as traditional convolution but requires fewer linear operations, thereby reducing the detector’s computational resource requirements. We evaluated the performance of the proposed G-YOLOv11 detector on the GRAZPEDWRI-DX dataset, achieving an <a href="mailto:&#109;&#65;&#x50;&#64;&#x30;&#46;&#x35;">&#109;&#65;&#x50;&#64;&#x30;&#46;&#x35;</a> of 0.535 with an inference time of 2.4 ms on an NVIDIA A10 GPU. Compared to the standard YOLOv11l, G-YOLOv11l achieved reductions of 13.6% in <a href="mailto:&#109;&#x41;&#80;&#64;&#x30;&#x2e;&#53;">&#109;&#x41;&#80;&#64;&#x30;&#x2e;&#53;</a> and 68.7% in size. These results establish a new state-of-the-art benchmark in terms of efficiency, outperforming existing detectors. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/AbdesselamFerdi/G-YOLOv11">https://github.com/AbdesselamFerdi/G-YOLOv11</a>. </p>
<blockquote>
<p>计算机辅助诊断（CAD）系统极大地提高了放射科医师和外科医师对医学图像的解释能力。然而，当前用于X射线图像骨折检测的CAD系统主要依赖于大型、资源密集型的检测器，这在临床上限制了它们的实用性。为了解决这一限制，我们提出了一种基于YOLO检测器的轻量级CAD系统，用于骨折检测。该系统名为基于Ghost卷积的YOLOv11（G-YOLOv11），它建立在YOLO检测器家族最新版本的基础上，并融入了Ghost卷积操作进行特征提取。Ghost卷积操作生成的特性图数量与传统卷积相同，但所需的线性操作更少，从而降低了检测器的计算资源需求。我们在GRAZPEDWRI-DX数据集上评估了所提出的G-YOLOv11检测器的性能，在NVIDIA A10 GPU上实现0.5的<a href="mailto:&#x6d;&#65;&#80;&#64;&#x30;&#46;&#x35;">&#x6d;&#65;&#80;&#64;&#x30;&#46;&#x35;</a>为0.535，推理时间为2.4毫秒。与标准YOLOv11相比，G-YOLOv11在<a href="mailto:&#109;&#65;&#x50;&#x40;&#x30;&#46;&#x35;">&#109;&#65;&#x50;&#x40;&#x30;&#46;&#x35;</a>上实现了13.6%的降低，体积减少了68.7%。这些结果达到了最新的效率基准点，超越了现有的检测器。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/AbdesselamFerdi/G-YOLOv1%E5%AE%9E%E7%8E%B%E7%8E%B0%E8%AF%AD%E7%BB%BC%E%E8%BF%87%E9%A9%AC%E8%B7%AF%E8%BF%99%E4%B8%AA%E8%BF%87%E7%A8%8B%E7%9A%84%E9%9B%86%E6%88%90%E4%BB%A3%E7%A0%81%E4%B8%AD%E4%B9%9F%E6%9C%89%E7%9D%80%E6%98%BE%E8%91%97%E7%9A%84%E7%BB%86%E8%8A%82%E6%94%AF%E6%8C%81%EF%BC%9A%E6%88%91%E4%BB%AC%E6%98%AF%E8%BF%90%E7%94%A8%E9%A2%84%E8%AE%BE%E6%96%87%E4%BB%B6%E9%80%89%E6%8B%A9%E7%9B%B8%E5%85%B3%E7%89%B9%E6%80%A7%E5%9B%BE%E5%83%8F%E7%9A%84%E5%8A%9F%E8%83%BD%E9%80%89%E9%A1%B9%E4%BB%A5%E4%BE%BF%E5%85%B6%E8%BE%93%E5%85%A5%E5%8F%AF%E8%87%AA%E4%B8%BB%E6%A3%80%E6%B5%8B%E7%89%B9%E5%AE%9A%E7%9A%84%E8%AE%AD%E7%BB%83%E5%AD%90%E9%9B%86%E4%BB%A5%E4%BE%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E5%88%86%E6%9E%90%EF%BC%89%E3%80%82%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E6%88%96%E5%BC%80%E5%8F%91%E8%80%85%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E4%B8%8A%E8%BF%B0%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%E5%AE%8C%E6%95%B4%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6%E5%B9%B6%E4%BA%86%E8%A7%A3%E6%9B%B4%E5%A4%9A%E7%9A%84%E7%A0%94%E7%A9%B6%E6%88%90%E6%9E%9C%E5%92%8C%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82%E3%80%82%E8%AF%B7%E6%B3%A8%E6%84%8F%E5%9C%A8%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8%E6%97%B6%E6%82%A8%E9%9C%80%E8%A6%81%E5%AE%89%E8%A3%85%E7%9B%B8%E5%BA%94%E7%9A%84%E4%BE%9D%E8%B5%96%E5%BA%93%E5%B9%B6%E9%81%B5%E5%BE%AA%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8D%8F%E8%AE%AE%E3%80%82">https://github.com/AbdesselamFerdi/G-YOLOv1实现这一目标的过程中我们的GitHub项目是一个完整的仓库，包含了所有相关的代码和模型文件。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00647v1">PDF</a> </p>
<p><strong>Summary</strong><br>     基于YOLO检测器的轻量化计算机辅助诊断（CAD）系统用于X光影像骨折检测，命名为G-YOLOv11。该系统采用最新YOLO探测器家族版本，并结合ghost卷积操作进行特征提取，以较少的计算资源生成相同数量的特征图。在GRAZPEDWRI-DX数据集上评估，G-YOLOv11探测器在NVIDIA A10 GPU上的推理时间为2.4毫秒，<a href="mailto:&#109;&#x41;&#x50;&#x40;&#48;&#46;&#x35;">&#109;&#x41;&#x50;&#x40;&#48;&#46;&#x35;</a>达到0.535，相较于标准YOLOv11l，在<a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#x2e;&#x35;">&#109;&#65;&#80;&#64;&#x30;&#x2e;&#x35;</a>上降低了13.6%，体积减小了68.7%，展现出卓越的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD系统在医学图像解读中的应用：说明计算机辅助诊断（CAD）系统对提高放射科医师和外科医生对医学图像的解读能力具有重要作用。</li>
<li>当前X光影像骨折检测CAD系统的局限性：指出当前用于X光影像骨折检测的CAD系统主要依赖于大型、资源密集型的探测器，限制了其在临床环境中的实用性。</li>
<li>G-YOLOv11系统的提出：介绍了一种新型的基于YOLO检测器的轻量化CAD系统，用于X光影像骨折检测。</li>
<li>G-YOLOv11系统的特点：强调G-YOLOv11系统采用ghost卷积操作进行特征提取，能够减少计算资源需求。</li>
<li>G-YOLOv11系统的性能评估：在GRAZPEDWRI-DX数据集上进行的评估表明，G-YOLOv11系统具有高效性能，相较于标准YOLOv11l有显著改善。</li>
<li>G-YOLOv11系统的可用资源：提供G-YOLOv11系统的代码和模型可供下载。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a166e5cb2fb8bed2465968e7f0a0346.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b98ab79ecb86036554c2a881bc15921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73eabb942debdd1d08c2b3899fc94815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b18e7d12a138d8533e141c8c443bd198.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00c7a8cebca5d6e9bd6b8e2252251e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba273ca2a24bf893451d32a6e7a5ce16.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Context-Length-and-Efficient-Transformers-for-Biomedical-Image-Analysis"><a href="#A-Study-on-Context-Length-and-Efficient-Transformers-for-Biomedical-Image-Analysis" class="headerlink" title="A Study on Context Length and Efficient Transformers for Biomedical   Image Analysis"></a>A Study on Context Length and Efficient Transformers for Biomedical   Image Analysis</h2><p><strong>Authors:Sarah M. Hooper, Hui Xue</strong></p>
<p>Biomedical imaging modalities often produce high-resolution, multi-dimensional images that pose computational challenges for deep neural networks. These computational challenges are compounded when training transformers due to the self-attention operator, which scales quadratically with context length. Recent developments in long-context models have potential to alleviate these difficulties and enable more efficient application of transformers to large biomedical images, although a systematic evaluation on this topic is lacking. In this study, we investigate the impact of context length on biomedical image analysis and we evaluate the performance of recently proposed long-context models. We first curate a suite of biomedical imaging datasets, including 2D and 3D data for segmentation, denoising, and classification tasks. We then analyze the impact of context length on network performance using the Vision Transformer and Swin Transformer by varying patch size and attention window size. Our findings reveal a strong relationship between context length and performance, particularly for pixel-level prediction tasks. Finally, we show that recent long-context models demonstrate significant improvements in efficiency while maintaining comparable performance, though we highlight where gaps remain. This work underscores the potential and challenges of using long-context models in biomedical imaging. </p>
<blockquote>
<p>生物医学成像模式通常产生高分辨率、多维图像，为深度神经网络带来计算挑战。由于自注意力算子的影响，这些计算挑战在训练变压器时更加复杂，自注意力算子的计算量与上下文长度成二次方关系。虽然针对此主题的系统性评估仍有所欠缺，但最近的长上下文模型的发展具有缓解这些困难并促进变压器在大型生物医学图像上更有效率应用的潜力。在这项研究中，我们研究了上下文长度对生物医学图像分析的影响，并评估了最近提出的长上下文模型的性能。我们首先整理了一套生物医学成像数据集，包括用于分割、去噪和分类任务的二维和三维数据。然后我们通过改变补丁大小和注意力窗口大小，使用视觉变压器和Swin变压器分析上下文长度对网络性能的影响。我们的研究发现上下文长度与性能之间存在密切关系，特别是在像素级预测任务中。最后，我们证明了最近的长上下文模型在保持性能的同时显示出显著的效率提升，尽管我们也指出了仍存在差距的地方。这项工作突出了在长上下文模型在生物医学成像中的潜力和挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00619v1">PDF</a> Published at ML4H 2024</p>
<p><strong>摘要</strong><br>     生物医学成像模态产生的高分辨率、多维图像对深度神经网络带来了计算挑战，尤其是在训练变压器时，自注意力算子会使计算量随上下文长度二次方增长。虽然已有研究提出长上下文模型有潜力缓解这些困难，使变压器在大型生物医学图像上的应用更加高效，但对此主题的系统性评估仍缺乏。本研究旨在探讨上下文长度对生物医学图像分析的影响，并评估最近提出的长上下文模型的性能。首先，我们整理了一套生物医学成像数据集，包括用于分割、去噪和分类任务的二维和三维数据。然后，通过改变补丁大小和注意力窗口大小，分析上下文长度对网络性能的影响，使用视觉变压器和Swin变压器进行试验。我们的研究发现上下文长度与性能之间存在密切关系，特别是在像素级预测任务中。最后，我们证明了最新的长上下文模型在保持性能的同时，显示出显著的效率提升，但也指出了仍存在的一些不足。这项工作突出了在长上下文模型在生物医学成像中的潜力和挑战。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>高分辨率、多维生物医学图像对深度神经网络的计算带来了挑战，特别是在处理长上下文信息时。</li>
<li>自注意力算子的计算复杂度随上下文长度的增长而增加，成为训练变压器时的瓶颈。</li>
<li>缺乏关于长上下文模型在生物医学图像分析中的系统性评估。</li>
<li>本研究通过试验验证了上下文长度对网络性能的重要影响，特别是在像素级预测任务中。</li>
<li>近期提出的长上下文模型在提高效率的同时保持了良好的性能。</li>
<li>尽管有这些改进，但在应用长上下文模型进行生物医学图像分析时仍存在一些不足和潜在挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9216ff3b6696c7df669a6c8783190fc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0db9e96dd8bb048493140bc2db6db85d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02e3df988585fe6ce294e5a1ac4230e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab313b29362b5149b589d62e8693c992.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="H-Net-A-Multitask-Architecture-for-Simultaneous-3D-Force-Estimation-and-Stereo-Semantic-Segmentation-in-Intracardiac-Catheters"><a href="#H-Net-A-Multitask-Architecture-for-Simultaneous-3D-Force-Estimation-and-Stereo-Semantic-Segmentation-in-Intracardiac-Catheters" class="headerlink" title="H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and   Stereo Semantic Segmentation in Intracardiac Catheters"></a>H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and   Stereo Semantic Segmentation in Intracardiac Catheters</h2><p><strong>Authors:Pedram Fekri, Mehrdad Zadeh, Javad Dargahi</strong></p>
<p>The success rate of catheterization procedures is closely linked to the sensory data provided to the surgeon. Vision-based deep learning models can deliver both tactile and visual information in a sensor-free manner, while also being cost-effective to produce. Given the complexity of these models for devices with limited computational resources, research has focused on force estimation and catheter segmentation separately. However, there is a lack of a comprehensive architecture capable of simultaneously segmenting the catheter from two different angles and estimating the applied forces in 3D. To bridge this gap, this work proposes a novel, lightweight, multi-input, multi-output encoder-decoder-based architecture. It is designed to segment the catheter from two points of view and concurrently measure the applied forces in the x, y, and z directions. This network processes two simultaneous X-Ray images, intended to be fed by a biplane fluoroscopy system, showing a catheter’s deflection from different angles. It uses two parallel sub-networks with shared parameters to output two segmentation maps corresponding to the inputs. Additionally, it leverages stereo vision to estimate the applied forces at the catheter’s tip in 3D. The architecture features two input channels, two classification heads for segmentation, and a regression head for force estimation through a single end-to-end architecture. The output of all heads was assessed and compared with the literature, demonstrating state-of-the-art performance in both segmentation and force estimation. To the best of the authors’ knowledge, this is the first time such a model has been proposed </p>
<blockquote>
<p>导管插入手术的成功率与为外科医生提供的感官数据密切相关。基于视觉的深度学习模型能够以无传感器的方式提供触觉和视觉信息，同时制造成本效益高。考虑到这些模型对于有限计算资源的设备的复杂性，研究主要集中于单独的力估计和导管分段。然而，缺乏一种能够同时从两个不同角度分割导管并在3D中估计施加力的综合架构。为了弥补这一空白，这项工作提出了一种新型轻量级的多输入多输出编码器-解码器架构。它旨在从两个观点分割导管，并同时测量x、y和z方向上的施加力。该网络处理两个同时的X光图像，旨在由双平面荧光镜检查系统提供，显示从不同角度看到的导管的偏转。它使用两个具有共享参数的并行子网络来输出与输入相对应的两个分割图。此外，它利用立体视觉来估计导管尖端在3D中施加的力。该架构具有两个输入通道、两个用于分割的分类头和一个用于力估计的回归头，通过单个端到端架构实现。所有头部的输出均经过评估并与文献进行比较，在分割和力估计方面都显示出最先进的技术性能。据作者所知，这是首次提出此类模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00514v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种新型的轻量级多输入多输出编码器-解码器架构，该架构旨在从两个视角对导管进行分割，并同时测量在XYZ方向上的应用力。它处理来自双平面荧光镜检查系统的实时X光图像，使用两个并行子网络输出与输入相对应的两个分割图，并利用立体视觉估计导管的3D力。该架构在分割和力估计方面均表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉深度学习模型能以无传感器的方式为外科医生提供触觉和视觉信息，且生产成本较为低廉。</li>
<li>目前对于计算资源有限的设备，研究主要集中在力估计和导管分割两个方面。</li>
<li>缺乏一种能够同时从两个不同角度分割导管并估计3D应用力的综合架构。</li>
<li>本文提出了一种新型的轻量级多输入多输出编码器-解码器架构，能够同时处理两个视角的X光图像，并进行导管分割和力估计。</li>
<li>该架构使用两个并行子网络处理输入图像，生成两个分割图，并有一个回归头用于力估计。</li>
<li>该架构利用立体视觉技术估计导管的3D力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5e713b56697c96789f5a2b801d946f4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccfdacc1193a5246b238e0e8fb05bc8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0326ea51d935a6e65541df7c02a028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4baacebe5014c9d4c5b20ce1c7b291fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fb6703e41beb77c4e369e9a205b6712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51467eb167e70ad487b90cb2ed4392ca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CancerKG-ORG-A-Web-scale-Interactive-Verifiable-Knowledge-Graph-LLM-Hybrid-for-Assisting-with-Optimal-Cancer-Treatment-and-Care"><a href="#CancerKG-ORG-A-Web-scale-Interactive-Verifiable-Knowledge-Graph-LLM-Hybrid-for-Assisting-with-Optimal-Cancer-Treatment-and-Care" class="headerlink" title="CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM   Hybrid for Assisting with Optimal Cancer Treatment and Care"></a>CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM   Hybrid for Assisting with Optimal Cancer Treatment and Care</h2><p><strong>Authors:Michael Gubanov, Anna Pyayt, Aleksandra Karolak</strong></p>
<p>Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user. </p>
<blockquote>
<p>这里我们描述了一个首批基于Web的大型混合知识图谱（KG）大语言模型（LLM）之一，其中融入了最新的经过同行评审的关于结肠癌的医学知识。它目前正在莫菲特癌症中心进行评估，以辅助医学研究和临床信息检索任务。莫菲特癌症中心是美国乃至全球顶尖的癌症中心之一。我们的混合模型非常出色，因为它比单独使用LLM、KG或搜索引擎更能满足用户需求。众所周知，LLM会出现幻觉和灾难性遗忘，并且是在过时的语料库上进行训练的。最新最前沿的KGs，如PrimeKG、cBioPortal、ChEMBL和NCBI等，需要人工维护，因此很容易过时。CancerKG是无监督的，能够自动摄取并整理最新的医学发现。为了缓解LLM的不足，经过验证的KG作为检索增强生成（RAG）的界限。CancerKG展现了五种不同的高级用户界面，每个界面都针对不同的数据模式进行了优化，更好地为用户服务并带来便利。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00223v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文描述了一种结合最新同行评审医学知识、针对结肠癌的Web规模混合知识图谱（KG）大型语言模型（LLM）。该模型在Moffitt癌症中心进行医学研究和临床信息检索任务的评估，表现优异，能更好地满足用户需求。与传统的LLM、KG或搜索引擎相比，其优势明显。CancerKG具有自动摄取和组织最新医学发现的能力，并能缓解LLM的缺点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>描述了一种针对结肠癌的Web规模混合知识图谱（KG）大型语言模型（LLM）。</li>
<li>该模型在Moffitt癌症中心的医学研究和临床信息检索任务中表现优异。</li>
<li>CancerKG结合了知识图谱（KG）和语言模型（LLM）的优势，可以更好地满足用户需求。</li>
<li>LLM存在的缺陷如幻觉和灾难性遗忘在该模型中得到了缓解。</li>
<li>CancerKG具有自动摄取和组织最新医学发现的能力。</li>
<li>该模型具有5种高级用户界面，方便用户使用并适应不同的数据模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-52c88ef204ce37873f1bfb2c10e96221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec213c03a44a5a6cef6bed644695f39f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71d8dd9eacc457bc9f1b75cd61e1412d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d21b1206dc864cad2ee2f3d57e6c0d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90283e91348f852c0be390e5a21f22a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-803253925897b30a25c48359f21790fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Implementing-Trust-in-Non-Small-Cell-Lung-Cancer-Diagnosis-with-a-Conformalized-Uncertainty-Aware-AI-Framework-in-Whole-Slide-Images"><a href="#Implementing-Trust-in-Non-Small-Cell-Lung-Cancer-Diagnosis-with-a-Conformalized-Uncertainty-Aware-AI-Framework-in-Whole-Slide-Images" class="headerlink" title="Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a   Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images"></a>Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a   Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images</h2><p><strong>Authors:Xiaoge Zhang, Tao Wang, Chao Yan, Fedaa Najdawi, Kai Zhou, Yuan Ma, Yiu-ming Cheung, Bradley A. Malin</strong></p>
<p>Ensuring trustworthiness is fundamental to the development of artificial intelligence (AI) that is considered societally responsible, particularly in cancer diagnostics, where a misdiagnosis can have dire consequences. Current digital pathology AI models lack systematic solutions to address trustworthiness concerns arising from model limitations and data discrepancies between model deployment and development environments. To address this issue, we developed TRUECAM, a framework designed to ensure both data and model trustworthiness in non-small cell lung cancer subtyping with whole-slide images. TRUECAM integrates 1) a spectral-normalized neural Gaussian process for identifying out-of-scope inputs and 2) an ambiguity-guided elimination of tiles to filter out highly ambiguous regions, addressing data trustworthiness, as well as 3) conformal prediction to ensure controlled error rates. We systematically evaluated the framework across multiple large-scale cancer datasets, leveraging both task-specific and foundation models, illustrate that an AI model wrapped with TRUECAM significantly outperforms models that lack such guidance, in terms of classification accuracy, robustness, interpretability, and data efficiency, while also achieving improvements in fairness. These findings highlight TRUECAM as a versatile wrapper framework for digital pathology AI models with diverse architectural designs, promoting their responsible and effective applications in real-world settings. </p>
<blockquote>
<p>确保可信度是发展社会负责任的人工智能（AI）的基础，特别是在癌症诊断中，误诊可能会带来严重的后果。当前的数字病理AI模型缺乏系统解决方案，无法解决模型局限性和模型部署与开发环境之间数据差异所产生的可信度问题。为了解决这一问题，我们开发了TRUECAM框架，旨在确保在非小细胞肺癌亚型分类的全幻灯片图像中数据和模型的可信度。TRUECAM集成了1）光谱归一化神经高斯过程，用于识别超出范围输入；2）歧义引导瓦片消除，以过滤掉高度模糊区域，解决数据可信度问题；以及3）符合预测要求，以确保控制误差率。我们在多个大规模癌症数据集上系统地评估了该框架，利用特定任务和基础模型，表明用TRUECAM包装的AI模型在分类精度、稳健性、可解释性和数据效率方面显著优于缺乏此类指导的模型，同时在公平性方面也实现了改进。这些发现突出了TRUECAM作为一个通用包装框架，适用于具有不同架构设计的数字病理AI模型，促进其在实际环境中的负责任和有效应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00053v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注人工智能在癌症诊断中的信任度问题，特别是在非小细胞肺癌亚型诊断中。为此，研究团队提出了TRUECAM框架，该框架通过多项技术确保数据和模型的信任度，包括识别超出范围输入的神经高斯过程、过滤高度模糊区域的模糊引导瓷砖消除法以及确保控制错误率的预测一致性。经过大规模癌症数据集的系统评估，发现TRUECAM包装的AI模型在分类精度、稳健性、可解释性和数据效率方面显著优于缺乏此类指导的模型，并实现了公平性的改进。这表明TRUECAM框架适用于各种设计的数字病理学AI模型，促进了其在现实环境中的负责任和有效应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在癌症诊断中的信任度至关重要，尤其是在非小细胞肺癌亚型诊断中。</li>
<li>当前数字病理学AI模型缺乏解决信任度问题的系统性解决方案。</li>
<li>TRUECAM框架被开发出来解决数据和模型的信任度问题。</li>
<li>TRUECAM通过多项技术确保信任度，包括神经高斯过程、模糊引导瓷砖消除法和预测一致性。</li>
<li>TRUECAM包装的AI模型在分类精度、稳健性、可解释性和数据效率方面表现优越。</li>
<li>TRUECAM实现了公平性的改进，适用于各种设计的数字病理学AI模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9fc47ea8d33551c77062cdc1b02d2d11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59be06150b91d67a011e1b27563a9b22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64bbed08f3be1d56166db07a246ee3db.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-31fe4a3ff39073ac066258f84924473e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-01-06  RingFormer A Neural Vocoder with Ring Attention and   Convolution-Augmented Transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d8f85e42e9f14b456e9e4de302584947.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-06  Reconstruction vs. Generation Taming Optimization Dilemma in Latent   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18179.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
