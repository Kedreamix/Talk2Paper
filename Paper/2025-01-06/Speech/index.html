<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  learning discriminative features from spectrograms using center loss for   speech emotion recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a30b4a6b7ce08090eba12d03788f7958.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-06-æ›´æ–°"><a href="#2025-01-06-æ›´æ–°" class="headerlink" title="2025-01-06 æ›´æ–°"></a>2025-01-06 æ›´æ–°</h1><h2 id="learning-discriminative-features-from-spectrograms-using-center-loss-for-speech-emotion-recognition"><a href="#learning-discriminative-features-from-spectrograms-using-center-loss-for-speech-emotion-recognition" class="headerlink" title="learning discriminative features from spectrograms using center loss for   speech emotion recognition"></a>learning discriminative features from spectrograms using center loss for   speech emotion recognition</h2><p><strong>Authors:Dongyang Dai, Zhiyong Wu, Runnan Li, Xixin Wu, Jia Jia, Helen Meng</strong></p>
<p>Identifying the emotional state from speech is essential for the natural interaction of the machine with the speaker. However, extracting effective features for emotion recognition is difficult, as emotions are ambiguous. We propose a novel approach to learn discriminative features from variable length spectrograms for emotion recognition by cooperating softmax cross-entropy loss and center loss together. The softmax cross-entropy loss enables features from different emotion categories separable, and center loss efficiently pulls the features belonging to the same emotion category to their center. By combining the two losses together, the discriminative power will be highly enhanced, which leads to network learning more effective features for emotion recognition. As demonstrated by the experimental results, after introducing center loss, both the unweighted accuracy and weighted accuracy are improved by over 3% on Mel-spectrogram input, and more than 4% on Short Time Fourier Transform spectrogram input. </p>
<blockquote>
<p>ä»è¯­éŸ³ä¸­è¯†åˆ«æƒ…æ„ŸçŠ¶æ€å¯¹äºæœºå™¨ä¸è¯´è¯äººä¹‹é—´çš„è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæƒ…æ„Ÿå…·æœ‰æ¨¡ç³Šæ€§ï¼Œæå–æœ‰æ•ˆçš„æƒ…æ„Ÿè¯†åˆ«ç‰¹å¾æ˜¯å›°éš¾çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡åˆä½œsoftmaxäº¤å‰ç†µæŸå¤±å’Œä¸­å¿ƒæŸå¤±æ¥ä»å¯å˜é•¿åº¦è°±å›¾ä¸­å­¦ä¹ åˆ¤åˆ«ç‰¹å¾çš„æ–°æ–¹æ³•ï¼Œä»¥è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ã€‚softmaxäº¤å‰ç†µæŸå¤±ä½¿ä¸åŒæƒ…æ„Ÿç±»åˆ«çš„ç‰¹å¾å¯åˆ†ç¦»ï¼Œè€Œä¸­å¿ƒæŸå¤±æœ‰æ•ˆåœ°å°†å±äºåŒä¸€æƒ…æ„Ÿç±»åˆ«çš„ç‰¹å¾æ‹‰å‘å®ƒä»¬çš„ä¸­å¿ƒã€‚é€šè¿‡å°†è¿™ä¸¤ç§æŸå¤±ç»“åˆåœ¨ä¸€èµ·ï¼Œåˆ¤åˆ«åŠ›å°†å¤§å¤§å¢å¼ºï¼Œè¿™å°†å¯¼è‡´ç½‘ç»œå­¦ä¹ æ›´æœ‰æ•ˆçš„æƒ…æ„Ÿè¯†åˆ«ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¼•å…¥ä¸­å¿ƒæŸå¤±åï¼ŒMelå…‰è°±å›¾è¾“å…¥çš„æœªåŠ æƒå‡†ç¡®ç‡å’ŒåŠ æƒå‡†ç¡®ç‡å‡æé«˜äº†3%ä»¥ä¸Šï¼ŒçŸ­æ—¶å‚…é‡Œå¶å˜æ¢å…‰è°±å›¾è¾“å…¥çš„å‡†ç¡®ç‡æé«˜äº†4%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01103v1">PDF</a> Accepted at ICASSP 2019</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆsoftmaxäº¤å‰ç†µæŸå¤±å’Œä¸­å¿ƒæŸå¤±çš„æ–¹æ³•ï¼Œä»å¯å˜é•¿åº¦è°±å›¾ä¸­å­¦ä¹ æƒ…æ„Ÿè¯†åˆ«åˆ¤åˆ«ç‰¹å¾ã€‚è¯¥æ–¹æ³•èƒ½æé«˜ä¸åŒæƒ…æ„Ÿç±»åˆ«ç‰¹å¾çš„åˆ†ç¦»åº¦ï¼Œå¹¶å°†åŒä¸€æƒ…æ„Ÿç±»åˆ«çš„ç‰¹å¾æ‹‰è¿‘å…¶ä¸­å¿ƒï¼Œä»è€Œæé«˜åˆ¤åˆ«èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¼•å…¥ä¸­å¿ƒæŸå¤±åï¼ŒMelè°±å›¾è¾“å…¥çš„æ— æƒé‡å‡†ç¡®ç‡å’ŒåŠ æƒå‡†ç¡®ç‡å‡æé«˜äº†3%ä»¥ä¸Šï¼ŒçŸ­æ—¶å‚…é‡Œå¶å˜æ¢è°±å›¾è¾“å…¥æé«˜äº†è¶…è¿‡4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿè¯†åˆ«å¯¹äºæœºå™¨ä¸äººçš„è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>æå–æœ‰æ•ˆçš„æƒ…æ„Ÿç‰¹å¾ç”¨äºè¯†åˆ«æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºæƒ…æ„Ÿå…·æœ‰æ¨¡ç³Šæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆsoftmaxäº¤å‰ç†µæŸå¤±å’Œä¸­å¿ƒæŸå¤±çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜æƒ…æ„Ÿè¯†åˆ«çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>softmaxäº¤å‰ç†µæŸå¤±ä½¿ä¸åŒæƒ…æ„Ÿç±»åˆ«çš„ç‰¹å¾å¯åˆ†ç¦»ã€‚</li>
<li>ä¸­å¿ƒæŸå¤±å°†åŒä¸€æƒ…æ„Ÿç±»åˆ«çš„ç‰¹å¾æ‹‰è¿‘å…¶ä¸­å¿ƒã€‚</li>
<li>ç»“åˆä¸¤ç§æŸå¤±å‡½æ•°ï¼Œå¢å¼ºäº†æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c69d2bbc8df527f9ce979a05ed8e9af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f0eb69800c61bd352a940c70e1270aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ea8ad8dd557b552efc477aef5b55d7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f293d30f4ec7f447649c1e74cdcc137.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Advancing-Singlish-Understanding-Bridging-the-Gap-with-Datasets-and-Multimodal-Models"><a href="#Advancing-Singlish-Understanding-Bridging-the-Gap-with-Datasets-and-Multimodal-Models" class="headerlink" title="Advancing Singlish Understanding: Bridging the Gap with Datasets and   Multimodal Models"></a>Advancing Singlish Understanding: Bridging the Gap with Datasets and   Multimodal Models</h2><p><strong>Authors:Bin Wang, Xunlong Zou, Shuo Sun, Wenyu Zhang, Yingxu He, Zhuohan Liu, Chengwei Wei, Nancy F. Chen, AiTi Aw</strong></p>
<p>Singlish, a Creole language rooted in English, is a key focus in linguistic research within multilingual and multicultural contexts. However, its spoken form remains underexplored, limiting insights into its linguistic structure and applications. To address this gap, we standardize and annotate the largest spoken Singlish corpus, introducing the Multitask National Speech Corpus (MNSC). These datasets support diverse tasks, including Automatic Speech Recognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue Summarization (SDS), and Paralinguistic Question Answering (PQA). We release standardized splits and a human-verified test set to facilitate further research. Additionally, we propose SingAudioLLM, a multi-task multimodal model leveraging multimodal large language models to handle these tasks concurrently. Experiments reveal our models adaptability to Singlish context, achieving state-of-the-art performance and outperforming prior models by 10-30% in comparison with other AudioLLMs and cascaded solutions. </p>
<blockquote>
<p>æ–°åŠ å¡å¼è‹±è¯­æ˜¯ä¸€ç§æ ¹æ¤äºè‹±è¯­çš„å…‹é‡Œå¥¥å°”è¯­è¨€ï¼Œåœ¨å¤šè¯­è¨€å’Œå¤šå…ƒæ–‡åŒ–èƒŒæ™¯ä¸‹ï¼Œå®ƒæ˜¯è¯­è¨€ç ”ç©¶çš„ä¸€ä¸ªé‡è¦ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œå…¶å£è¯­å½¢å¼ä»è¢«ä½ä¼°ï¼Œå¯¹å…¶è¯­è¨€ç»“æ„å’Œåº”ç”¨çš„äº†è§£æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹æœ€å¤§çš„æ–°åŠ å¡å¼è‹±è¯­å£è¯­è¯­æ–™åº“è¿›è¡Œäº†æ ‡å‡†åŒ–å’Œæ³¨é‡Šï¼Œæ¨å‡ºäº†å¤šä»»åŠ¡å›½å®¶è¯­éŸ³è¯­æ–™åº“ï¼ˆMNSCï¼‰ã€‚è¿™äº›æ•°æ®é›†æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­é—®ç­”ï¼ˆSQAï¼‰ã€å£è¯­å¯¹è¯æ‘˜è¦ï¼ˆSDSï¼‰å’Œå‰¯è¯­è¨€é—®ç­”ï¼ˆPQAï¼‰ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ ‡å‡†åŒ–åˆ†å‰²å’Œäººå·¥éªŒè¯çš„æµ‹è¯•é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†SingAudioLLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡å¤šæ¨¡å¼æ¨¡å‹ï¼Œåˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹åŒæ—¶å¤„ç†è¿™äº›ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°åŠ å¡å¼è‹±è¯­çš„è¯­å¢ƒï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸å…¶ä»–éŸ³é¢‘LLMå’Œçº§è”è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†10%-30%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01034v1">PDF</a> Open-Source: <a target="_blank" rel="noopener" href="https://github.com/AudioLLMs/Singlish">https://github.com/AudioLLMs/Singlish</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–°åŠ å¡è‹±è¯­ï¼ˆSinglishï¼‰å£è¯­å½¢å¼çš„ç ”ç©¶ä¸è¶³é—®é¢˜ï¼Œåˆ›å»ºäº†å¤šä»»åŠ¡çš„å…¨å›½è¯­éŸ³è¯­æ–™åº“ï¼ˆMNSCï¼‰ï¼Œæ”¯æŒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­é—®ç­”ï¼ˆSQAï¼‰ã€å£è¯­å¯¹è¯æ‘˜è¦ï¼ˆSDSï¼‰å’Œå‰¯è¯­è¨€é—®ç­”ï¼ˆPQAï¼‰ç­‰ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæå‡ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹SingAudioLLMï¼Œèƒ½åŒæ—¶å¤„ç†è¿™äº›ä»»åŠ¡ï¼Œå®éªŒè¯æ˜è¯¥æ¨¡å‹é€‚åº”æ–°åŠ å¡è¯­å¢ƒï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–AudioLLMså’Œçº§è”è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Singlishä½œä¸ºè‹±è¯­çš„ä¸€ç§å…‹é‡Œå¥¥å°”è¯­ï¼Œåœ¨å¤šè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹æ˜¯è¯­è¨€ç ”ç©¶çš„é‡è¦ç„¦ç‚¹ã€‚</li>
<li>ç°æœ‰çš„ç ”ç©¶å¯¹Singlishå£è¯­å½¢å¼æ¢ç´¢ä¸è¶³ï¼Œé™åˆ¶äº†å¯¹å…¶è¯­è¨€ç»“æ„å’Œåº”ç”¨çš„ç†è§£ã€‚</li>
<li>åˆ›å»ºäº†å¤šä»»åŠ¡çš„å…¨å›½è¯­éŸ³è¯­æ–™åº“ï¼ˆMNSCï¼‰ï¼Œä»¥æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­é—®ç­”ï¼ˆSQAï¼‰ç­‰ã€‚</li>
<li>å‘å¸ƒäº†æ ‡å‡†åŒ–çš„æ•°æ®é›†åˆ†å‰²å’Œäººå·¥éªŒè¯çš„æµ‹è¯•é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹SingAudioLLMï¼Œèƒ½åŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSingAudioLLMæ¨¡å‹é€‚åº”Singlishè¯­å¢ƒï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e07239126647fb880b9e20bf221a9dfb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92c789abf85fe61508ace3617b0bc6f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-935a46b619eceeffc0754cecb4f85fad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8176a7dc7f3aea032943449371fab791.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="U-GIFT-Uncertainty-Guided-Firewall-for-Toxic-Speech-in-Few-Shot-Scenario"><a href="#U-GIFT-Uncertainty-Guided-Firewall-for-Toxic-Speech-in-Few-Shot-Scenario" class="headerlink" title="U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot   Scenario"></a>U-GIFT: Uncertainty-Guided Firewall for Toxic Speech in Few-Shot   Scenario</h2><p><strong>Authors:Jiaxin Song, Xinyu Wang, Yihao Wang, Yifan Tang, Ru Zhang, Jianyi Liu, Gongshen Liu</strong></p>
<p>With the widespread use of social media, user-generated content has surged on online platforms. When such content includes hateful, abusive, offensive, or cyberbullying behavior, it is classified as toxic speech, posing a significant threat to the online ecosystemâ€™s integrity and safety. While manual content moderation is still prevalent, the overwhelming volume of content and the psychological strain on human moderators underscore the need for automated toxic speech detection. Previously proposed detection methods often rely on large annotated datasets; however, acquiring such datasets is both costly and challenging in practice. To address this issue, we propose an uncertainty-guided firewall for toxic speech in few-shot scenarios, U-GIFT, that utilizes self-training to enhance detection performance even when labeled data is limited. Specifically, U-GIFT combines active learning with Bayesian Neural Networks (BNNs) to automatically identify high-quality samples from unlabeled data, prioritizing the selection of pseudo-labels with higher confidence for training based on uncertainty estimates derived from model predictions. Extensive experiments demonstrate that U-GIFT significantly outperforms competitive baselines in few-shot detection scenarios. In the 5-shot setting, it achieves a 14.92% performance improvement over the basic model. Importantly, U-GIFT is user-friendly and adaptable to various pre-trained language models (PLMs). It also exhibits robust performance in scenarios with sample imbalance and cross-domain settings, while showcasing strong generalization across various language applications. We believe that U-GIFT provides an efficient solution for few-shot toxic speech detection, offering substantial support for automated content moderation in cyberspace, thereby acting as a firewall to promote advancements in cybersecurity. </p>
<blockquote>
<p>éšç€ç¤¾äº¤åª’ä½“å¹¿æ³›ä½¿ç”¨ï¼Œç”¨æˆ·ç”Ÿæˆå†…å®¹å·²åœ¨ç½‘ä¸Šå¹³å°æ¿€å¢ã€‚å½“æ­¤ç±»å†…å®¹åŒ…å«ä»‡æ¨ã€ä¾®è¾±ã€å†’çŠ¯æˆ–ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºæ—¶ï¼Œå®ƒå°±è¢«å½’ç±»ä¸ºæœ‰æ¯’è¨€è®ºï¼Œå¯¹åœ¨çº¿ç”Ÿæ€ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œå®‰å…¨æ„æˆé‡å¤§å¨èƒã€‚è™½ç„¶æ‰‹åŠ¨å†…å®¹ç®¡ç†ä»ç„¶æ™®éï¼Œä½†å†…å®¹çš„å¤§é‡æ¶Œç°ä»¥åŠäººç±»ç®¡ç†è€…æ‰¿å—çš„å¿ƒç†å‹åŠ›å‡¸æ˜¾äº†è‡ªåŠ¨åŒ–æœ‰æ¯’è¨€è®ºæ£€æµ‹çš„éœ€æ±‚ã€‚ä¹‹å‰æå‡ºçš„æ£€æµ‹æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼›ç„¶è€Œï¼Œåœ¨å®è·µä¸­è·å–è¿™æ ·çš„æ•°æ®é›†æ—¢æ˜‚è´µåˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬é’ˆå¯¹å°æ ·æœ¬åœºæ™¯ä¸‹çš„æœ‰æ¯’è¨€è®ºï¼Œæå‡ºäº†ä¸€ä¸ªä¸ç¡®å®šæ€§å¼•å¯¼é˜²ç«å¢™U-GIFTã€‚å®ƒåˆ©ç”¨è‡ªæˆ‘è®­ç»ƒæ¥æé«˜æ£€æµ‹æ€§èƒ½ï¼Œå³ä½¿åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å‘æŒ¥ä½œç”¨ã€‚å…·ä½“æ¥è¯´ï¼ŒU-GIFTç»“åˆä¸»åŠ¨å­¦ä¹ ä¸è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNsï¼‰ï¼Œä»æœªæ ‡è®°æ•°æ®ä¸­è‡ªåŠ¨è¯†åˆ«é«˜è´¨é‡æ ·æœ¬ï¼Œå¹¶æ ¹æ®æ¨¡å‹é¢„æµ‹äº§ç”Ÿçš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä¼˜å…ˆé€‰æ‹©å’ŒåŸ¹è®­å…·æœ‰æ›´é«˜è‡ªä¿¡çš„ä¼ªæ ‡ç­¾æ ·æœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘é‡æ£€æµ‹åœºæ™¯ä¸­ï¼ŒU-GIFTæ˜¾è‘—ä¼˜äºç«äº‰åŸºçº¿ã€‚åœ¨5æ¬¡å°„å‡»çš„åœºæ™¯ä¸­ï¼Œå…¶æ€§èƒ½æ¯”åŸºæœ¬æ¨¡å‹æé«˜äº†14.92ï¼…ã€‚é‡è¦çš„æ˜¯ï¼ŒU-GIFTå‹å¥½ä¸”æ˜“äºé€‚åº”å„ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ã€‚å®ƒåœ¨æ ·æœ¬ä¸å¹³è¡¡å’Œè·¨åŸŸåœºæ™¯ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§è¯­è¨€åº”ç”¨ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡U-GIFTä¸ºå°æ ·æœ‰æ¯’è¨€è®ºæ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç½‘ç»œç©ºé—´çš„è‡ªåŠ¨åŒ–å†…å®¹ç®¡ç†æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒï¼Œä»è€Œä½œä¸ºé˜²ç«å¢™ä¿ƒè¿›äº†ç½‘ç»œå®‰å…¨çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00907v1">PDF</a> 16 pages, 6 figures and 10 tables. Comments are welcome</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¤¾äº¤åª’ä½“å¹¿æ³›æ™®åŠå¯¼è‡´ç”¨æˆ·ç”Ÿæˆå†…å®¹å¤§é‡æ¶Œç°äºç½‘ç»œå¹³å°ã€‚å½“ä¸­åŒ…å«ä»‡æ¨ã€æ»¥ç”¨ã€å†’çŠ¯æˆ–ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºçš„è¨€è®ºè¢«è§†ä¸ºæœ‰æ¯’è¨€è®ºï¼Œå¯¹åœ¨çº¿ç”Ÿæ€ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œå®‰å…¨æ€§æ„æˆä¸¥é‡å¨èƒã€‚è™½ç„¶äººå·¥å†…å®¹ç®¡ç†ä»ç„¶æ™®éï¼Œä½†å†…å®¹çš„å·¨å¤§æ•°é‡å’Œäººç±»ç®¡ç†è€…é¢ä¸´çš„å‹åŠ›çªæ˜¾äº†è‡ªåŠ¨æ£€æµ‹æœ‰æ¯’è¨€è®ºçš„å¿…è¦æ€§ã€‚å…ˆå‰æå‡ºçš„æ£€æµ‹æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§å‹æ³¨é‡Šæ•°æ®é›†ï¼Œä½†åœ¨å®è·µä¸­è·å–è¿™äº›æ•°æ®é›†æ—¢æ˜‚è´µåˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä½èµ„æºåœºæ™¯çš„æœ‰æ¯’è¨€è®ºé˜²ç«å¢™U-GIFTï¼Œå®ƒåˆ©ç”¨è‡ªæˆ‘è®­ç»ƒå³ä½¿åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æé«˜æ£€æµ‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒU-GIFTç»“åˆäº†ä¸»åŠ¨å­¦ä¹ ä¸è´å¶æ–¯ç¥ç»ç½‘ç»œï¼ˆBNNsï¼‰ï¼Œè‡ªåŠ¨ä»æœªæ ‡è®°çš„æ•°æ®ä¸­è¯†åˆ«é«˜è´¨é‡æ ·æœ¬ï¼Œå¹¶æ ¹æ®æ¨¡å‹é¢„æµ‹å¾—å‡ºçš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä¼˜å…ˆç­›é€‰å…·æœ‰è¾ƒé«˜ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾ç”¨äºè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å°å‹æ£€æµ‹åœºæ™¯ä¸­ï¼ŒU-GIFTæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚åœ¨5æ¬¡å°„å‡»çš„åœºæ™¯ä¸‹ï¼Œå®ƒçš„æ€§èƒ½æ¯”åŸºæœ¬æ¨¡å‹æé«˜äº†14.92%ã€‚é‡è¦çš„æ˜¯ï¼ŒU-GIFTæ˜“äºç”¨æˆ·ä½¿ç”¨ä¸”é€‚åº”å¤šç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ã€‚åœ¨æ ·æœ¬ä¸å‡è¡¡å’Œè·¨åŸŸè®¾ç½®ä¸­ï¼Œå®ƒè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§è¯­è¨€åº”ç”¨ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡U-GIFTä¸ºä½èµ„æºæœ‰æ¯’è¨€è®ºæ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç½‘ç»œå®‰å…¨ä¸­çš„è‡ªåŠ¨å†…å®¹ç®¡ç†æä¾›äº†åšå®çš„æ”¯æŒã€‚ </p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ul>
<li>éšç€ç¤¾äº¤åª’ä½“çš„æ™®åŠï¼Œç”¨æˆ·ç”Ÿæˆå†…å®¹ä¸­åŒ…å«çš„æœ‰æ¯’è¨€è®ºå¯¹åœ¨çº¿ç”Ÿæ€ç³»ç»Ÿæ„æˆå¨èƒã€‚</li>
<li>ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†è·å–æˆæœ¬é«˜æ˜‚ä¸”æŒ‘æˆ˜é‡é‡ã€‚</li>
<li>U-GIFTç»“åˆäº†ä¸»åŠ¨å­¦ä¹ ä¸è´å¶æ–¯ç¥ç»ç½‘ç»œï¼Œæœ‰æ•ˆåº”å¯¹ä½èµ„æºåœºæ™¯ä¸‹çš„æœ‰æ¯’è¨€è®ºæ£€æµ‹ã€‚</li>
<li>U-GIFTåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æ€§èƒ½å“è¶Šï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>U-GIFTé€‚åº”å¤šç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°å‡ºç¨³å¥çš„è·¨åŸŸå’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6b8cc808e21db9d8bf442fbfac9001d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b582ce0516aae506d2f3b24ef77d16.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-Are-Read-Write-Policy-Makers-for-Simultaneous-Generation"><a href="#Large-Language-Models-Are-Read-Write-Policy-Makers-for-Simultaneous-Generation" class="headerlink" title="Large Language Models Are Read&#x2F;Write Policy-Makers for Simultaneous   Generation"></a>Large Language Models Are Read&#x2F;Write Policy-Makers for Simultaneous   Generation</h2><p><strong>Authors:Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Yang Feng</strong></p>
<p>Simultaneous generation models write generation results while reading streaming inputs, necessitating a policy-maker to determine the appropriate output timing. Existing simultaneous generation methods generally adopt the traditional encoder-decoder architecture and learn the generation and policy-making capabilities through complex dynamic programming techniques. Although LLMs excel at text generation, they face challenges in taking on the role of policy-makers through traditional training methods, limiting their exploration in simultaneous generation. To overcome these limitations, we propose a novel LLM-driven Simultaneous Generation (LSG) framework, which allows the off-the-shelf LLM to decide the generation timing and produce output concurrently. Specifically, LSG selects the generation policy that minimizes latency as the baseline policy. Referring to the baseline policy, LSG enables the LLM to devise an improved generation policy that better balances latency and generation quality, and writes generation results accordingly. Experiments on simultaneous translation and streaming automatic speech recognition tasks show that our method can achieve state-of-the-art performance utilizing the open-source LLMs and demonstrate practicality in real-world scenarios. </p>
<blockquote>
<p>åŒæ—¶ç”Ÿæˆæ¨¡å‹åœ¨é˜…è¯»æµå¼è¾“å…¥æ—¶ç”Ÿæˆç»“æœï¼Œè¿™éœ€è¦å†³ç­–è€…æ¥ç¡®å®šé€‚å½“çš„è¾“å‡ºæ—¶é—´ã€‚ç°æœ‰çš„åŒæ—¶ç”Ÿæˆæ–¹æ³•é€šå¸¸é‡‡ç”¨ä¼ ç»Ÿçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡å¤æ‚çš„åŠ¨æ€ç¼–ç¨‹æŠ€æœ¯æ¥å­¦ä¹ ç”Ÿæˆå’Œå†³ç­–èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šè¿‡ä¼ ç»Ÿè®­ç»ƒæ–¹æ³•æ‹…ä»»å†³ç­–è€…çš„è§’è‰²æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨åŒæ—¶ç”Ÿæˆæ–¹é¢çš„æ¢ç´¢ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åŒæ—¶ç”Ÿæˆï¼ˆLSGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç°æˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è‡ªè¡Œå†³å®šç”Ÿæˆæ—¶é—´å¹¶åŒæ—¶äº§ç”Ÿè¾“å‡ºã€‚å…·ä½“æ¥è¯´ï¼ŒLSGé€‰æ‹©ä»¥æœ€å°åŒ–å»¶è¿Ÿä¸ºåŸºå‡†çš„ç”Ÿæˆç­–ç•¥ä½œä¸ºåŸºå‡†ç­–ç•¥ã€‚å‚ç…§åŸºå‡†ç­–ç•¥ï¼ŒLSGä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆ¶å®šæ”¹è¿›çš„ç”Ÿæˆç­–ç•¥ï¼Œæ›´å¥½åœ°å¹³è¡¡å»¶è¿Ÿå’Œç”Ÿæˆè´¨é‡ï¼Œå¹¶æ®æ­¤ç¼–å†™ç”Ÿæˆç»“æœã€‚é’ˆå¯¹åŒå£°ä¼ è¯‘å’Œæµå¼è¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åˆ©ç”¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹å®ç°æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶åœ¨å®é™…åœºæ™¯ä¸­å±•ç¤ºå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00868v1">PDF</a> Accepted at AAAI 2025. 13 pages, 7 tables, 10 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæµå¼è¾“å…¥çš„å®æ—¶ç”Ÿæˆæ¨¡å‹éœ€è¦åœ¨é˜…è¯»è¿‡ç¨‹ä¸­å³æ—¶è¾“å‡ºç”Ÿæˆç»“æœï¼Œè¿™éœ€è¦å†³ç­–è€…ç¡®å®šé€‚å½“çš„è¾“å‡ºæ—¶é—´ã€‚ä¼ ç»Ÿçš„åŒæ—¶ç”Ÿæˆæ–¹æ³•é€šå¸¸é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€šè¿‡å¤æ‚çš„åŠ¨æ€è§„åˆ’æŠ€æœ¯å­¦ä¹ ç”Ÿæˆå’Œå†³ç­–èƒ½åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½æ“…é•¿æ–‡æœ¬ç”Ÿæˆï¼Œä½†éš¾ä»¥é€šè¿‡ä¼ ç»Ÿè®­ç»ƒæ–¹å¼æ‰¿æ‹…å†³ç­–è§’è‰²ï¼Œé™åˆ¶äº†å…¶åœ¨å®æ—¶ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹LLMé©±åŠ¨çš„åŒæ—¶ç”Ÿæˆï¼ˆLSGï¼‰æ¡†æ¶ï¼Œå…è®¸å³æ’å³ç”¨çš„LLMå†³å®šç”Ÿæˆæ—¶é—´å¹¶å®ç°å®æ—¶è¾“å‡ºã€‚LSGé€‰æ‹©æœ€å°åŒ–å»¶è¿Ÿçš„ç”Ÿæˆç­–ç•¥ä½œä¸ºåŸºå‡†ç­–ç•¥ï¼Œå¹¶æ®æ­¤ä½¿LLMåˆ¶å®šæ”¹è¿›ç”Ÿæˆç­–ç•¥ï¼Œæ›´å¥½åœ°å¹³è¡¡å»¶è¿Ÿå’Œç”Ÿæˆè´¨é‡ã€‚åœ¨åŒæ—¶ç¿»è¯‘å’Œæµå¼è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåˆ©ç”¨å¼€æºLLMçš„æ–¹æ³•èƒ½è¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½å¹¶åœ¨å®é™…åœºæ™¯ä¸­å±•ç°å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒæ—¶ç”Ÿæˆæ¨¡å‹éœ€è¦ç¡®å®šé€‚å½“çš„è¾“å‡ºæ—¶é—´ï¼Œä»¥å¤„ç†æµå¼è¾“å…¥å¹¶å³æ—¶ç”Ÿæˆç»“æœã€‚</li>
<li>ä¼ ç»Ÿçš„åŒæ—¶ç”Ÿæˆæ–¹æ³•ä¾èµ–äºç¼–ç å™¨-è§£ç å™¨æ¶æ„å’Œå¤æ‚çš„åŠ¨æ€è§„åˆ’æŠ€æœ¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰¿æ‹…å†³ç­–è§’è‰²æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„LLMé©±åŠ¨çš„åŒæ—¶ç”Ÿæˆï¼ˆLSGï¼‰æ¡†æ¶å…è®¸LLMå†³å®šç”Ÿæˆæ—¶é—´å¹¶å®ç°å®æ—¶è¾“å‡ºã€‚</li>
<li>LSGé€šè¿‡é€‰æ‹©æœ€å°åŒ–å»¶è¿Ÿçš„ç”Ÿæˆç­–ç•¥ä½œä¸ºåŸºå‡†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ”¹è¿›ç­–ç•¥ï¼Œä»¥å¹³è¡¡å»¶è¿Ÿå’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>LSGæ¡†æ¶åˆ©ç”¨å¼€æºLLMå®ç°ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60b3f23987e4ed2c3a716db7d585da59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-358c956e4420438ab05e21b5038f5033.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a7e9b6e37f64f137887d0c54b9559ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d15e9952753e395c672ee254048195e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c5caaff6304b0d6bd7d101353ae7ae8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Automatic-Text-Pronunciation-Correlation-Generation-and-Application-for-Contextual-Biasing"><a href="#Automatic-Text-Pronunciation-Correlation-Generation-and-Application-for-Contextual-Biasing" class="headerlink" title="Automatic Text Pronunciation Correlation Generation and Application for   Contextual Biasing"></a>Automatic Text Pronunciation Correlation Generation and Application for   Contextual Biasing</h2><p><strong>Authors:Gaofeng Cheng, Haitian Lu, Chengxu Yang, Xuyang Wang, Ta Li, Yonghong Yan</strong></p>
<p>Effectively distinguishing the pronunciation correlations between different written texts is a significant issue in linguistic acoustics. Traditionally, such pronunciation correlations are obtained through manually designed pronunciation lexicons. In this paper, we propose a data-driven method to automatically acquire these pronunciation correlations, called automatic text pronunciation correlation (ATPC). The supervision required for this method is consistent with the supervision needed for training end-to-end automatic speech recognition (E2E-ASR) systems, i.e., speech and corresponding text annotations. First, the iteratively-trained timestamp estimator (ITSE) algorithm is employed to align the speech with their corresponding annotated text symbols. Then, a speech encoder is used to convert the speech into speech embeddings. Finally, we compare the speech embeddings distances of different text symbols to obtain ATPC. Experimental results on Mandarin show that ATPC enhances E2E-ASR performance in contextual biasing and holds promise for dialects or languages lacking artificial pronunciation lexicons. </p>
<blockquote>
<p>åœ¨è¯­è¨€å£°å­¦é¢†åŸŸï¼Œæœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒä¹¦é¢æ–‡æœ¬ä¹‹é—´çš„å‘éŸ³å…³è”æ˜¯ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ç§å‘éŸ³å…³è”æ˜¯é€šè¿‡äººå·¥è®¾è®¡çš„å‘éŸ³è¯å…¸è·å¾—çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œè‡ªåŠ¨è·å–è¿™äº›å‘éŸ³å…³è”ï¼Œç§°ä¸ºè‡ªåŠ¨æ–‡æœ¬å‘éŸ³å…³è”ï¼ˆATPCï¼‰ã€‚è¯¥æ–¹æ³•æ‰€éœ€çš„ç›‘ç£ä¸è®­ç»ƒç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰ç³»ç»Ÿæ‰€éœ€çš„ç›‘ç£ä¸€è‡´ï¼Œå³è¯­éŸ³å’Œç›¸åº”çš„æ–‡æœ¬æ³¨é‡Šã€‚é¦–å…ˆï¼Œé‡‡ç”¨è¿­ä»£è®­ç»ƒçš„æ—¶é—´æˆ³ä¼°è®¡ï¼ˆITSEï¼‰ç®—æ³•å°†è¯­éŸ³ä¸ç›¸åº”çš„æ³¨é‡Šæ–‡æœ¬ç¬¦å·å¯¹é½ã€‚ç„¶åï¼Œä½¿ç”¨è¯­éŸ³ç¼–ç å™¨å°†è¯­éŸ³è½¬æ¢ä¸ºè¯­éŸ³åµŒå…¥ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒä¸åŒæ–‡æœ¬ç¬¦å·çš„è¯­éŸ³åµŒå…¥è·ç¦»ï¼Œä»¥è·å¾—ATPCã€‚åœ¨æ™®é€šè¯ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒATPCæé«˜äº†E2E-ASRåœ¨ä¸Šä¸‹æ–‡åå‘æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶æœ‰æœ›åº”ç”¨äºç¼ºä¹äººå·¥å‘éŸ³è¯å…¸çš„æ–¹è¨€æˆ–è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00804v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§æ•°æ®é©±åŠ¨æ–¹æ³•â€”â€”è‡ªåŠ¨æ–‡æœ¬å‘éŸ³å…³è”ï¼ˆATPCï¼‰ï¼Œä»¥è‡ªåŠ¨è·å–ä¸åŒæ–‡æœ¬é—´çš„å‘éŸ³å…³è”ï¼Œè§£å†³äº†ä¼ ç»Ÿæ‰‹åŠ¨è®¾è®¡å‘éŸ³è¯å…¸çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰ç³»ç»Ÿè®­ç»ƒç›¸åŒçš„ç›‘ç£æ–¹å¼ï¼Œå³è¯­éŸ³å’Œå¯¹åº”æ–‡æœ¬çš„æ³¨é‡Šï¼Œç”¨äºè·å–ä¸€è‡´çš„å‘éŸ³ã€‚é€šè¿‡ä½¿ç”¨è¿­ä»£è®­ç»ƒçš„timestampä¼°ç®—å™¨ç®—æ³•ï¼Œå®ç°è¯­éŸ³ä¸å¯¹åº”æ–‡æœ¬ç¬¦å·çš„å¯¹é½ã€‚éšåä½¿ç”¨è¯­éŸ³ç¼–ç å™¨å°†è¯­éŸ³è½¬æ¢ä¸ºè¯­éŸ³åµŒå…¥ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒæ–‡æœ¬ç¬¦å·çš„è¯­éŸ³åµŒå…¥è·ç¦»è·å¾—ATPCã€‚åœ¨æ™®é€šè¯ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒATPCèƒ½æé«˜E2E-ASRåœ¨è¯­å¢ƒåå‘æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶å¯¹ç¼ºä¹äººå·¥å‘éŸ³è¯å…¸çš„æ–¹è¨€æˆ–è¯­è¨€å…·æœ‰åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>è‡ªåŠ¨æ–‡æœ¬å‘éŸ³å…³è”ï¼ˆATPCï¼‰æ–¹æ³•æå‡ºï¼Œç”¨äºæ•°æ®é©±åŠ¨åœ°è·å–ä¸åŒæ–‡æœ¬çš„å‘éŸ³å…³è”ã€‚</li>
<li>é‡‡ç”¨ä¸ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆE2E-ASRï¼‰ç³»ç»Ÿç›¸åŒçš„ç›‘ç£æ–¹å¼æ¥è·å–å‘éŸ³å…³è”çš„ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨è¿­ä»£è®­ç»ƒçš„timestampä¼°ç®—å™¨ç®—æ³•å®ç°è¯­éŸ³ä¸å¯¹åº”æ–‡æœ¬ç¬¦å·çš„ç²¾å‡†å¯¹é½ã€‚</li>
<li>ä½¿ç”¨è¯­éŸ³ç¼–ç å™¨å°†è¯­éŸ³è½¬æ¢ä¸ºè¯­éŸ³åµŒå…¥ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒä¸åŒæ–‡æœ¬ç¬¦å·çš„è¯­éŸ³åµŒå…¥è·ç¦»æ¥è·å–å‘éŸ³å…³è”ã€‚</li>
<li>åœ¨æ™®é€šè¯çš„å®éªŒä¸­ï¼ŒATPCèƒ½æé«˜E2E-ASRç³»ç»Ÿçš„è¯­å¢ƒåå‘æ€§èƒ½ã€‚</li>
<li>ATPCæ–¹æ³•å¯¹ç¼ºä¹äººå·¥å‘éŸ³è¯å…¸çš„æ–¹è¨€æˆ–è¯­è¨€å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49e496dc939157d8b97715d604ca298b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efb5da304ef5bdf1c292631cb05bcb2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2304375e6ce8e1da3307ce93faabdd6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c4eeee7d58541fba5a0e9c2e1f92b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cae6891ce2df06c52042a181a901a49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12efdc759aa01c74ea27c736397bef91.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Decoding-the-Flow-CauseMotion-for-Emotional-Causality-Analysis-in-Long-form-Conversations"><a href="#Decoding-the-Flow-CauseMotion-for-Emotional-Causality-Analysis-in-Long-form-Conversations" class="headerlink" title="Decoding the Flow: CauseMotion for Emotional Causality Analysis in   Long-form Conversations"></a>Decoding the Flow: CauseMotion for Emotional Causality Analysis in   Long-form Conversations</h2><p><strong>Authors:Yuxuan Zhang, Yulong Li, Zichen Yu, Feilong Tang, Zhixiang Lu, Chong Li, Kang Dang, Jionglong Su</strong></p>
<p>Long-sequence causal reasoning seeks to uncover causal relationships within extended time series data but is hindered by complex dependencies and the challenges of validating causal links. To address the limitations of large-scale language models (e.g., GPT-4) in capturing intricate emotional causality within extended dialogues, we propose CauseMotion, a long-sequence emotional causal reasoning framework grounded in Retrieval-Augmented Generation (RAG) and multimodal fusion. Unlike conventional methods relying only on textual information, CauseMotion enriches semantic representations by incorporating audio-derived features-vocal emotion, emotional intensity, and speech rate-into textual modalities. By integrating RAG with a sliding window mechanism, it effectively retrieves and leverages contextually relevant dialogue segments, thus enabling the inference of complex emotional causal chains spanning multiple conversational turns. To evaluate its effectiveness, we constructed the first benchmark dataset dedicated to long-sequence emotional causal reasoning, featuring dialogues with over 70 turns. Experimental results demonstrate that the proposed RAG-based multimodal integrated approach, the efficacy of substantially enhances both the depth of emotional understanding and the causal inference capabilities of large-scale language models. A GLM-4 integrated with CauseMotion achieves an 8.7% improvement in causal accuracy over the original model and surpasses GPT-4o by 1.2%. Additionally, on the publicly available DiaASQ dataset, CauseMotion-GLM-4 achieves state-of-the-art results in accuracy, F1 score, and causal reasoning accuracy. </p>
<blockquote>
<p>é•¿åºåˆ—å› æœæ¨ç†æ—¨åœ¨æ­ç¤ºæ‰©å±•æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å› æœå…³ç³»ï¼Œä½†ç”±äºå¤æ‚çš„ä¾èµ–å…³ç³»å’ŒéªŒè¯å› æœè”ç³»æ‰€é¢ä¸´çš„æŒ‘æˆ˜è€Œå—åˆ°é˜»ç¢ã€‚ä¸ºäº†è§£å†³å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰åœ¨æ•æ‰æ‰©å±•å¯¹è¯ä¸­å¤æ‚æƒ…ç»ªå› æœå…³ç³»æ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CauseMotionï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€èåˆçš„é•¿åºåˆ—æƒ…ç»ªå› æœæ¨ç†æ¡†æ¶ã€‚ä¸åŒäºä»…ä¾èµ–æ–‡æœ¬ä¿¡æ¯çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒCauseMotioné€šè¿‡èå…¥éŸ³é¢‘è¡ç”Ÿç‰¹å¾ï¼ˆå¦‚è¯­éŸ³æƒ…ç»ªã€æƒ…ç»ªå¼ºåº¦å’Œè¯­é€Ÿï¼‰æ¥ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºï¼Œä¸°å¯Œæ–‡æœ¬æ¨¡å¼ã€‚é€šè¿‡ç»“åˆRAGå’Œæ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ£€ç´¢å’Œåˆ©ç”¨è¯­å¢ƒç›¸å…³çš„å¯¹è¯ç‰‡æ®µï¼Œä»è€Œæ¨æ–­è·¨è¶Šå¤šä¸ªå¯¹è¯å›åˆçš„å¤æ‚æƒ…ç»ªå› æœé“¾ã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†é¦–ä¸ªä¸“æ³¨äºé•¿åºåˆ—æƒ…ç»ªå› æœæ¨ç†çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡70å›åˆçš„å¯¹è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åŸºäºRAGçš„å¤šæ¨¡æ€é›†æˆæ–¹æ³•å¤§å¤§æé«˜äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¯¹æƒ…ç»ªçš„ç†è§£å’Œå› æœæ¨ç†èƒ½åŠ›ã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œé›†æˆäº†CauseMotionçš„GLM-4åœ¨å› æœå‡†ç¡®æ€§æ–¹é¢æé«˜äº†8.7%ï¼Œå¹¶è¶…è¶Šäº†GPT-4o 1.2%ã€‚æ­¤å¤–ï¼Œåœ¨å…¬å¼€çš„DiaASQæ•°æ®é›†ä¸Šï¼ŒCauseMotion-GLM-4åœ¨å‡†ç¡®æ€§ã€F1åˆ†æ•°å’Œå› æœæ¨ç†å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00778v1">PDF</a> 7pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é•¿åºåˆ—å› æœæ¨ç†ä¸­çš„æƒ…æ„Ÿå› æœå…³ç³»çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†CauseMotionæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€èåˆæŠ€æœ¯ï¼Œå°†éŸ³é¢‘è¡ç”Ÿç‰¹å¾ï¼ˆå¦‚è¯­éŸ³æƒ…æ„Ÿã€æƒ…æ„Ÿå¼ºåº¦å’Œè¯­é€Ÿï¼‰èå…¥æ–‡æœ¬æ¨¡æ€ï¼Œä»¥ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºã€‚é€šè¿‡RAGä¸æ»‘åŠ¨çª—å£æœºåˆ¶çš„é›†æˆï¼ŒCauseMotionèƒ½æœ‰æ•ˆæ£€ç´¢å’Œåˆ©ç”¨ç›¸å…³çš„å¯¹è¯ç‰‡æ®µï¼Œä»è€Œæ¨æ–­è·¨è¶Šå¤šä¸ªå¯¹è¯å›åˆçš„å¤æ‚æƒ…æ„Ÿå› æœå…³ç³»é“¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æƒ…æ„Ÿç†è§£å’Œå› æœæ¨ç†çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CauseMotionæ¡†æ¶æ—¨åœ¨è§£å†³é•¿åºåˆ—å› æœæ¨ç†ä¸­çš„æƒ…æ„Ÿå› æœå…³ç³»é—®é¢˜ï¼Œç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ¨¡æ€èåˆæŠ€æœ¯ã€‚</li>
<li>è¯¥æ¡†æ¶å°†éŸ³é¢‘è¡ç”Ÿç‰¹å¾ï¼ˆå¦‚è¯­éŸ³æƒ…æ„Ÿã€æƒ…æ„Ÿå¼ºåº¦å’Œè¯­é€Ÿï¼‰èå…¥æ–‡æœ¬æ¨¡æ€ï¼Œä»¥ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡RAGä¸æ»‘åŠ¨çª—å£æœºåˆ¶çš„é›†æˆï¼ŒCauseMotionèƒ½æœ‰æ•ˆå¤„ç†å¤æ‚çš„æƒ…æ„Ÿå› æœå…³ç³»é“¾ï¼Œå¹¶æ¨æ–­è·¨å¤šä¸ªå¯¹è¯å›åˆçš„å› æœå…³ç³»ã€‚</li>
<li>æ„å»ºäº†ä¸“é—¨ç”¨äºé•¿åºåˆ—æƒ…æ„Ÿå› æœæ¨ç†çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡70è½®å¯¹è¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCauseMotionæ¡†æ¶æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿç†è§£å’Œå› æœæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œé›†æˆäº†CauseMotionçš„GLM-4åœ¨å› æœå‡†ç¡®æ€§æ–¹é¢æé«˜äº†8.7%ï¼Œå¹¶åœ¨DiaASQæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€F1åˆ†æ•°å’Œå› æœæ¨ç†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b50b4555139231b83a36fef8a25b78c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fcb561dc25bf445eb052926fd19053e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa1f14b39717a7ae036f48c102a6d626.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cbfc44518327742b5fa7c28329353d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6409b7fbb0a1ac1c7684f2ea69d1537.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac9434203f905a1a1b5bc8e608e68e14.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Whisper-Turns-Stronger-Augmenting-Wav2Vec-2-0-for-Superior-ASR-in-Low-Resource-Languages"><a href="#Whisper-Turns-Stronger-Augmenting-Wav2Vec-2-0-for-Superior-ASR-in-Low-Resource-Languages" class="headerlink" title="Whisper Turns Stronger: Augmenting Wav2Vec 2.0 for Superior ASR in   Low-Resource Languages"></a>Whisper Turns Stronger: Augmenting Wav2Vec 2.0 for Superior ASR in   Low-Resource Languages</h2><p><strong>Authors:Or Haim Anidjar, Revital Marbel, Roi Yozevitch</strong></p>
<p>Approaching Speech-to-Text and Automatic Speech Recognition problems in low-resource languages is notoriously challenging due to the scarcity of validated datasets and the diversity of dialects. Arabic, Russian, and Portuguese exemplify these difficulties, being low-resource languages due to the many dialects of these languages across different continents worldwide. Moreover, the variety of accents and pronunciations of such languages complicate ASR modelsâ€™ success. With the increasing popularity of Deep Learning and Transformers, acoustic models like the renowned Wav2Vec2 have achieved superior performance in the Speech Recognition field compared to state-of-the-art approaches. However, despite Wav2Vec2â€™s improved efficiency over traditional methods, its performance significantly declines for under-represented languages, even though it requires significantly less labeled data. This paper introduces an end-to-end framework that enhances ASR systems fine-tuned on Wav2Vec2 through data augmentation techniques. To validate our frameworkâ€™s effectiveness, we conducted a detailed experimental evaluation using three datasets from Mozillaâ€™s Common Voice project in Arabic, Russian, and Portuguese. Additionally, the framework presented in this paper demonstrates robustness to different diacritics. Ultimately, our approach outperforms two previous baseline models, which are the pre-trained Wav2Vec2 and the well-known Whisper ASR model, resulting in an average relative improvement of 33.9% in Word Error Rate and a 53.2% relative improvement in Character Error Rate. </p>
<blockquote>
<p>é’ˆå¯¹ä½èµ„æºè¯­è¨€ä¸­çš„è¯­éŸ³åˆ°æ–‡æœ¬å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«é—®é¢˜ï¼Œç”±äºç¼ºä¹éªŒè¯è¿‡çš„æ•°æ®é›†å’Œæ–¹è¨€å¤šæ ·æ€§ï¼Œè§£å†³èµ·æ¥æä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œè‘¡è„ç‰™è¯­å°±æ˜¯è¿™äº›å›°éš¾çš„ä¾‹è¯ï¼Œè¿™äº›è¯­è¨€åœ¨ä¸–ç•Œå„åœ°çš„å¤§é™†ä¸Šéƒ½æœ‰è®¸å¤šæ–¹è¨€ï¼Œå› æ­¤æ˜¯ä½èµ„æºè¯­è¨€ã€‚æ­¤å¤–ï¼Œè¿™äº›è¯­è¨€çš„å‘éŸ³å’Œè¯­è°ƒå˜åŒ–ä½¿å¾—ASRæ¨¡å‹çš„æˆåŠŸå˜å¾—å¤æ‚ã€‚éšç€æ·±åº¦å­¦ä¹ å’ŒTransformerçš„æ—¥ç›Šæ™®åŠï¼Œå¦‚è‘—åçš„Wav2Vec2ç­‰å£°å­¦æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—äº†ç›¸æ¯”æœ€æ–°æŠ€æœ¯æ–¹æ³•çš„å“è¶Šè¡¨ç°ã€‚ç„¶è€Œï¼Œå°½ç®¡Wav2Vec2ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•åœ¨æ•ˆç‡ä¸Šæœ‰æ‰€æå‡ï¼Œä½†å¯¹äºä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ï¼Œå…¶è¡¨ç°å´å¤§å¹…ä¸‹é™ï¼Œå°½ç®¡å®ƒéœ€è¦çš„æ ‡æ³¨æ•°æ®è¦å°‘å¾—å¤šã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯æ”¹è¿›äº†åŸºäºWav2Vec2çš„ASRç³»ç»Ÿã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨Mozillaçš„Common Voiceé¡¹ç›®ä¸­çš„é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œè‘¡è„ç‰™è¯­çš„ä¸‰ä¸ªæ•°æ®é›†è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä»‹ç»çš„æ¡†æ¶æ˜¾ç¤ºå‡ºå¯¹ä¸åŒéŸ³æ ‡çš„ç¨³å¥æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†å…ˆå‰çš„ä¸¤ä¸ªåŸºå‡†æ¨¡å‹ï¼Œå³é¢„è®­ç»ƒçš„Wav2Vec2å’Œè‘—åçš„Whisper ASRæ¨¡å‹ï¼Œåœ¨å•è¯é”™è¯¯ç‡æ–¹é¢å®ç°äº†å¹³å‡ç›¸å¯¹æ”¹è¿›33.9%ï¼Œåœ¨å­—ç¬¦é”™è¯¯ç‡æ–¹é¢å®ç°äº†ç›¸å¯¹æ”¹è¿›53.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00425v1">PDF</a> 15 pagesm 3 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼ˆå¦‚é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œè‘¡è„ç‰™è¯­ï¼‰çš„è¯­éŸ³è¯†åˆ«å’Œæ–‡å­—è¯†åˆ«é—®é¢˜å…·æœ‰è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºç¼ºä¹éªŒè¯è¿‡çš„æ•°æ®é›†ä»¥åŠæ–¹è¨€å¤šæ ·æ€§ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ ï¼ˆå¦‚è‘—åçš„Wav2Vec2æ¨¡å‹ï¼‰åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹äºä»£è¡¨æ€§ä¸è¶³çš„è¯­ç§æ¥è¯´ï¼Œå…¶æ€§èƒ½ä¾ç„¶æœ‰æ‰€å—é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æå‡Wav2Vec2å¾®è°ƒåçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ€§èƒ½ã€‚é€šè¿‡Mozillaçš„Common Voiceé¡¹ç›®ä¸­çš„é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œè‘¡è„ç‰™è¯­çš„ä¸‰ä¸ªæ•°æ®é›†è¿›è¡Œè¯¦å°½çš„å®éªŒè¯„ä¼°ï¼ŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¯¹ä¸åŒè¯­ç§çš„ä¹¦å†™å˜ä½“å±•ç°å‡ºç¨³å¥æ€§ï¼Œæœ€ç»ˆè¶…è¶Šäº†Wav2Vec2é¢„è®­ç»ƒæ¨¡å‹å’Œè‘—åçš„Whisperè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œåœ¨å•è¯é”™è¯¯ç‡ä¸Šå¹³å‡ç›¸å¯¹æé«˜äº†33.9%ï¼Œå­—ç¬¦é”™è¯¯ç‡ä¸Šæé«˜äº†53.2%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½èµ„æºè¯­è¨€åœ¨è¯­éŸ³è½¬æ–‡å­—ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹éªŒè¯è¿‡çš„æ•°æ®é›†å’Œæ–¹è¨€å¤šæ ·æ€§ã€‚</li>
<li>Wav2Vec2æ¨¡å‹è™½åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨ä½èµ„æºè¯­ç§ä¸­è¡¨ç°ä»æœ‰é™åˆ¶ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥ä¸€ç§ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯æå‡ASRç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åœ¨Mozillaçš„Common Voiceé¡¹ç›®çš„é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­å’Œè‘¡è„ç‰™è¯­æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>æ¡†æ¶å¯¹ä¸åŒè¯­ç§çš„ä¹¦å†™å˜ä½“è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å•è¯é”™è¯¯ç‡å’Œå­—ç¬¦é”™è¯¯ç‡ä¸Šæ˜¾è‘—è¶…è¶Šäº†åŸºçº¿æ¨¡å‹Wav2Vec2å’ŒWhisperæ¨¡å‹ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a30b4a6b7ce08090eba12d03788f7958.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b1128dd2fc47c8400ed0d96406057bc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VoxVietnam-a-Large-Scale-Multi-Genre-Dataset-for-Vietnamese-Speaker-Recognition"><a href="#VoxVietnam-a-Large-Scale-Multi-Genre-Dataset-for-Vietnamese-Speaker-Recognition" class="headerlink" title="VoxVietnam: a Large-Scale Multi-Genre Dataset for Vietnamese Speaker   Recognition"></a>VoxVietnam: a Large-Scale Multi-Genre Dataset for Vietnamese Speaker   Recognition</h2><p><strong>Authors:Hoang Long Vu, Phuong Tuan Dat, Pham Thao Nhi, Nguyen Song Hao, Nguyen Thi Thu Trang</strong></p>
<p>Recent research in speaker recognition aims to address vulnerabilities due to variations between enrolment and test utterances, particularly in the multi-genre phenomenon where the utterances are in different speech genres. Previous resources for Vietnamese speaker recognition are either limited in size or do not focus on genre diversity, leaving studies in multi-genre effects unexplored. This paper introduces VoxVietnam, the first multi-genre dataset for Vietnamese speaker recognition with over 187,000 utterances from 1,406 speakers and an automated pipeline to construct a dataset on a large scale from public sources. Our experiments show the challenges posed by the multi-genre phenomenon to models trained on a single-genre dataset, and demonstrate a significant increase in performance upon incorporating the VoxVietnam into the training process. Our experiments are conducted to study the challenges of the multi-genre phenomenon in speaker recognition and the performance gain when the proposed dataset is used for multi-genre training. </p>
<blockquote>
<p>è¿‘æœŸå…³äºè¯­éŸ³è¯†åˆ«çš„ç ”ç©¶æ—¨åœ¨è§£å†³ç”±äºæ³¨å†Œå’Œæµ‹è¯•è¯­éŸ³ä¹‹é—´çš„å˜åŒ–é€ æˆçš„æ¼æ´ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒè¯­éŸ³ç±»å‹çš„å¤šç±»å‹ç°è±¡ä¸­ã€‚è¶Šå—è¯­éŸ³è¯†åˆ«çš„å…ˆå‰èµ„æºè§„æ¨¡æœ‰é™ï¼Œæˆ–è€…å¹¶ä¸ä¸“æ³¨äºç±»å‹å¤šæ ·æ€§ï¼Œè¿™ä½¿å¾—å¤šç±»å‹å½±å“çš„ç ”ç©¶å°šæœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†VoxVietnamæ•°æ®é›†ï¼Œå®ƒæ˜¯è¶Šå—è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„é¦–ä¸ªå¤šç±»å‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª1406åå‘è¨€äººçš„è¶…è¿‡18ä¸‡æ¡è¯­éŸ³è®°å½•ï¼Œå¹¶æœ‰ä¸€ä¸ªä»å…¬å…±æ¥æºå¤§è§„æ¨¡æ„å»ºæ•°æ®é›†çš„è‡ªåŠ¨åŒ–ç®¡é“ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºäº†åœ¨å•ç±»å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹é¢ä¸´çš„å¤šç±»å‹ç°è±¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå¹¶è¯æ˜äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥VoxVietnamåæ€§èƒ½çš„æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„å®éªŒæ—¨åœ¨ç ”ç©¶è¯­éŸ³è¯†åˆ«ä¸­å¤šç±»å‹ç°è±¡çš„æŒ‘æˆ˜ä»¥åŠä½¿ç”¨æ‰€æå‡ºæ•°æ®é›†è¿›è¡Œå¤šç±»å‹è®­ç»ƒæ—¶çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00328v1">PDF</a> Accepted to 2025 IEEE International Conference on Acoustics, Speech,   and Signal Processing (ICASSP 2025)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ­¤è®ºæ–‡é’ˆå¯¹è¶Šå—è¯­è¯­éŸ³è¯†åˆ«ä¸­çš„å¤šé£æ ¼ç°è±¡è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶ä»‹ç»äº†VoxVietnamæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸ºè¶Šå—è¯­è¯­éŸ³è¯†åˆ«æä¾›äº†é¦–ä¸ªå¤šé£æ ¼æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡187,000ä¸ªè¯´è¯äººçš„å‘éŸ³ç‰‡æ®µã€‚ç ”ç©¶å‘ç°å¤šé£æ ¼ç°è±¡ç»™å•ä¸€é£æ ¼è®­ç»ƒæ¨¡å‹å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠä½¿ç”¨VoxVietnamæ•°æ®é›†è¿›è¡Œå¤šé£æ ¼è®­ç»ƒå¸¦æ¥çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹è¶Šå—è¯­è¯­éŸ³è¯†åˆ«ä¸­çš„å¤šé£æ ¼ç°è±¡ã€‚</li>
<li>ä»‹ç»äº†VoxVietnamæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸ºè¶Šå—è¯­è¯­éŸ³è¯†åˆ«æä¾›äº†é¦–ä¸ªå¤šé£æ ¼æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡187,000ä¸ªè¯´è¯äººçš„å‘éŸ³ç‰‡æ®µï¼Œè§„æ¨¡è¾ƒå¤§ã€‚</li>
<li>å¤šé£æ ¼ç°è±¡å¯¹å•ä¸€é£æ ¼è®­ç»ƒæ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨VoxVietnamæ•°æ®é›†è¿›è¡Œå¤šé£æ ¼è®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡å®éªŒéªŒè¯äº†å¤šé£æ ¼ç°è±¡åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„æŒ‘æˆ˜ä»¥åŠä½¿ç”¨æ‰€æå‡ºæ•°æ®é›†çš„æ”¶ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d203a520774a1b628ba778bff686a23f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90613d89d6dab8d8940f34d385453d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d611b02713946745564e00777278a37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c13a2355c1ab397eacba51dcc708ee14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10dcd7e70a581712ad489220ba75b1e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d2b02fd99cfcb6ecc88227acfb855a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95852787b8475ff9875afa53a4a2e2ee.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Speech-Recognition-With-LLMs-Adapted-to-Disordered-Speech-Using-Reinforcement-Learning"><a href="#Speech-Recognition-With-LLMs-Adapted-to-Disordered-Speech-Using-Reinforcement-Learning" class="headerlink" title="Speech Recognition With LLMs Adapted to Disordered Speech Using   Reinforcement Learning"></a>Speech Recognition With LLMs Adapted to Disordered Speech Using   Reinforcement Learning</h2><p><strong>Authors:Chirag Nagpal, Subhashini Venugopalan, Jimmy Tobin, Marilyn Ladewig, Katherine Heller, Katrin Tomanek</strong></p>
<p>We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLMâ€™s vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§èƒ½å¤Ÿå¤„ç†è¯­éŸ³è¾“å…¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶å±•ç¤ºé€šè¿‡åŸºäºäººç±»åå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰è¿›ä¸€æ­¥è°ƒæ•´å®ƒï¼Œå¯ä»¥æ¯”ä¼ ç»Ÿå¾®è°ƒæ›´å¥½åœ°é€‚åº”ä¹±åºè¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”¨éŸ³é¢‘ä»¤ç‰Œæ›¿æ¢LLMè¯æ±‡è¡¨ä¸­çš„ä½é¢‘æ–‡æœ¬ä»¤ç‰Œï¼Œå¹¶é€šè¿‡åœ¨å¸¦æœ‰è½¬å½•çš„è¯­éŸ³ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«è¯­éŸ³ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºå¥æ³•å’Œè¯­ä¹‰å‡†ç¡®æ€§åº¦é‡çš„å¥–åŠ±æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥å°†LLMæ¨å¹¿ä»¥è¯†åˆ«ä¹±åºè¯­éŸ³ã€‚è™½ç„¶æ‰€å¾—LLMåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å¹¶æœªè¶…è¶Šç°æœ‰ç³»ç»Ÿï¼Œä½†æˆ‘ä»¬å‘ç°ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ è°ƒæ•´ï¼Œåœ¨é€‚åº”ä¸åŒç¯å¢ƒä¸‹çš„è¯­éŸ³æ—¶ï¼Œå…¶æ€§èƒ½æ˜æ˜¾ä¼˜äºå¯¹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒã€‚è¿™ä¸ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„æ›¿ä»£è°ƒæ•´ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00039v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¼ºåŒ–å­¦ä¹ å¯¹äººç±»åå¥½ï¼ˆRLHFï¼‰çš„è¿›ä¸€æ­¥è°ƒæ•´ï¼Œèƒ½æ›´å¥½åœ°é€‚åº”ç´Šä¹±è¯­éŸ³çš„å¤„ç†ã€‚æ–¹æ³•æ˜¯ç”¨éŸ³é¢‘ä»¤ç‰Œæ›¿æ¢LLMè¯æ±‡è¡¨ä¸­çš„ä½é¢‘æ–‡æœ¬ä»¤ç‰Œï¼Œä½¿æ¨¡å‹é€šè¿‡å¾®è°ƒè¯­éŸ³å’Œè½¬å½•æœ¬æ¥è¯†åˆ«è¯­éŸ³ã€‚ç„¶åï¼Œä½¿ç”¨åŸºäºå¥æ³•è¯­ä¹‰å‡†ç¡®æ€§çš„å¥–åŠ±æ¥å¼ºåŒ–å­¦ä¹ ï¼Œä½¿LLMè¿›ä¸€æ­¥æ³›åŒ–ï¼Œä»¥è¯†åˆ«ç´Šä¹±è¯­éŸ³ã€‚è™½ç„¶ç»“æœä¸å¦‚ç°æœ‰çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œä½†ä½¿ç”¨è‡ªå®šä¹‰å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è°ƒæ•´ç­–ç•¥ç›¸è¾ƒäºç›‘ç£å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ï¼Œå°¤å…¶åœ¨é€‚åº”ä¸åŒåœºæ™¯çš„è¯­éŸ³æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚è¿™ä¸ºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³è¯†åˆ«æä¾›äº†æœ‰å¸å¼•åŠ›çš„æ›¿ä»£ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¯ä»¥é€šè¿‡å¤„ç†è¯­éŸ³è¾“å…¥è¿›è¡Œè®­ç»ƒå’Œè°ƒæ•´ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºè°ƒæ•´LLMä»¥æ›´å¥½åœ°é€‚åº”ç´Šä¹±è¯­éŸ³ã€‚</li>
<li>ç”¨éŸ³é¢‘ä»¤ç‰Œæ›¿æ¢LLMä¸­çš„ä½é¢‘æ–‡æœ¬ä»¤ç‰Œï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«è¯­éŸ³ã€‚</li>
<li>åŸºäºå¥æ³•è¯­ä¹‰å‡†ç¡®æ€§çš„å¥–åŠ±ç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>è™½ç„¶LLMåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢çš„è¡¨ç°å°šæœªè¶…è¶Šç°æœ‰ç³»ç»Ÿï¼Œä½†å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç›¸è¾ƒäºä¼ ç»Ÿå¾®è°ƒæ–¹æ³•æ€§èƒ½æ›´ä¼˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç­–ç•¥åœ¨é€‚åº”ä¸åŒåœºæ™¯çš„è¯­éŸ³æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b78325a40d53d7e0e7a541d228accec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47dd4ed6be4308f3e6099952727e47a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cfb7453fdbdd75e8197730d3e70ad806.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e920e57704167bb0bbd7222617f956a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245092484dcb13600c7b7f3b52786460.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-85cd31d5e5fd720c39ef919387086d0e.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  ProjectedEx Enhancing Generation in Explainable AI for Prostate Cancer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fb1ca717d7f8bce4b330cb4b114ec67f.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  FGAseg Fine-Grained Pixel-Text Alignment for Open-Vocabulary Semantic   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
