<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-01-06  Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic   Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9e42f7e4b56a8c7527394390f980dd3a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-06-更新"><a href="#2025-01-06-更新" class="headerlink" title="2025-01-06 更新"></a>2025-01-06 更新</h1><h2 id="Leverage-Cross-Attention-for-End-to-End-Open-Vocabulary-Panoptic-Reconstruction"><a href="#Leverage-Cross-Attention-for-End-to-End-Open-Vocabulary-Panoptic-Reconstruction" class="headerlink" title="Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic   Reconstruction"></a>Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic   Reconstruction</h2><p><strong>Authors:Xuan Yu, Yuxuan Xie, Yili Liu, Haojian Lu, Rong Xiong, Yiyi Liao, Yue Wang</strong></p>
<p>Open-vocabulary panoptic reconstruction offers comprehensive scene understanding, enabling advances in embodied robotics and photorealistic simulation. In this paper, we propose PanopticRecon++, an end-to-end method that formulates panoptic reconstruction through a novel cross-attention perspective. This perspective models the relationship between 3D instances (as queries) and the scene’s 3D embedding field (as keys) through their attention map. Unlike existing methods that separate the optimization of queries and keys or overlook spatial proximity, PanopticRecon++ introduces learnable 3D Gaussians as instance queries. This formulation injects 3D spatial priors to preserve proximity while maintaining end-to-end optimizability. Moreover, this query formulation facilitates the alignment of 2D open-vocabulary instance IDs across frames by leveraging optimal linear assignment with instance masks rendered from the queries. Additionally, we ensure semantic-instance segmentation consistency by fusing query-based instance segmentation probabilities with semantic probabilities in a novel panoptic head supervised by a panoptic loss. During training, the number of instance query tokens dynamically adapts to match the number of objects. PanopticRecon++ shows competitive performance in terms of 3D and 2D segmentation and reconstruction performance on both simulation and real-world datasets, and demonstrates a user case as a robot simulator. Our project website is at: <a target="_blank" rel="noopener" href="https://yuxuan1206.github.io/panopticrecon_pp/">https://yuxuan1206.github.io/panopticrecon_pp/</a> </p>
<blockquote>
<p>开放词汇全景重建提供了全面的场景理解，促进了嵌入式机器人和真实感仿真技术的进展。在本文中，我们提出了PanopticRecon++，这是一种端到端的方法，通过新颖的跨注意力视角来进行全景重建。这个视角通过注意力图对3D实例（作为查询）和场景3D嵌入字段（作为键）之间的关系进行建模。与现有方法不同，这些方法要么将查询和键的优化分开，要么忽略了空间邻近关系，PanopticRecon++引入了可学习的3D高斯分布作为实例查询。这种表述注入了3D空间先验信息，以保留邻近关系并保持端到端的优化能力。此外，这种查询表述通过利用来自查询的实例掩膜进行最佳线性分配，促进了跨帧的2D开放词汇实例ID的对齐。通过融合基于查询的实例分割概率和语义概率在新型全景头中，由全景损失进行监管，我们还确保了语义实例分割的一致性。在训练过程中，实例查询令牌的数目会动态调整以适应对象的数量。PanopticRecon++在模拟和真实世界数据集上表现出具有竞争力的3D和2D分割和重建性能，并展示了作为机器人模拟器的用户案例。我们的项目网站是：[<a target="_blank" rel="noopener" href="https://yuxuan1206.github.io/panopticrecon_pp/]">https://yuxuan1206.github.io/panopticrecon_pp/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01119v1">PDF</a> 18 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为PanopticRecon++的端到端方法，用于通过新型跨注意力视角进行全景重建。该方法建立3D实例（作为查询）与场景3D嵌入字段（作为键）之间的关系，通过注意力图模型实现。此方法采用可学习的3D高斯作为实例查询，注入3D空间先验信息以保留邻近性并保持端到端优化能力。此外，通过利用基于查询的实例掩膜渲染，该方法实现了跨帧的2D开放词汇实例ID对齐。结合查询基础上的实例分割概率与语义概率，该方法保证了语义实例分割的一致性。PanopticRecon++在模拟和真实数据集上的3D和2D分割以及重建性能具有竞争力，并展示了在机器人模拟器中的应用案例。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PanopticRecon++是一种端到端的全景重建方法，通过新型跨注意力视角实现。</li>
<li>该方法建立3D实例与场景3D嵌入字段的关系，引入可学习的3D高斯作为实例查询。</li>
<li>PanopticRecon++能保留邻近性并保持端到端优化能力，注入3D空间先验信息。</li>
<li>通过利用基于查询的实例掩膜渲染，实现了跨帧的2D开放词汇实例ID对齐。</li>
<li>PanopticRecon++结合了查询基础上的实例分割概率与语义概率，确保语义实例分割的一致性。</li>
<li>该方法在模拟和真实数据集上的3D和2D分割以及重建性能具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01119">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4aa1e1355b4305a1530bbbffe6884344.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1a69954d0fa1bfbe7dc060e9be007a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cd26967025716fed18cfc04ce8450bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a77004d9592e74301b77e91c4fd34499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8a2e9103adc905b4e6553dae7fc95c5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Deformable-Gaussian-Splatting-for-Efficient-and-High-Fidelity-Reconstruction-of-Surgical-Scenes"><a href="#Deformable-Gaussian-Splatting-for-Efficient-and-High-Fidelity-Reconstruction-of-Surgical-Scenes" class="headerlink" title="Deformable Gaussian Splatting for Efficient and High-Fidelity   Reconstruction of Surgical Scenes"></a>Deformable Gaussian Splatting for Efficient and High-Fidelity   Reconstruction of Surgical Scenes</h2><p><strong>Authors:Jiwei Shan, Zeyu Cai, Cheng-Tai Hsieh, Shing Shin Cheng, Hesheng Wang</strong></p>
<p>Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation, which reduces rendering speed. To address these challenges, we introduce EH-SurGS, an efficient and high-fidelity reconstruction algorithm for deformable surgical scenes. We propose a deformation modeling approach that incorporates the life cycle of 3D Gaussians, effectively capturing both regular and irreversible deformations, thus enhancing reconstruction quality. Additionally, we present an adaptive motion hierarchy strategy that distinguishes between static and deformable regions within the surgical scene. This strategy reduces the number of 3D Gaussians passing through the deformation field, thereby improving rendering speed. Extensive experiments demonstrate that our method surpasses existing state-of-the-art approaches in both reconstruction quality and rendering speed. Ablation studies further validate the effectiveness and necessity of our proposed components. We will open-source our code upon acceptance of the paper. </p>
<blockquote>
<p>对于可变形手术场景的效率和高质量重建是一项至关重要但又充满挑战的任务。基于近年来对高斯二次积算法研究的最新进展，当前方法在重建质量和渲染速度方面都取得了重大进步。然而仍存在两大挑战：（1）处理不可逆的动态变化（如手术场景中常见的组织剪切）的难度；（2）缺乏手术场景变形的层次建模，这降低了渲染速度。为了应对这些挑战，我们引入了EH-SurGS，这是一个针对可变形手术场景的高效且高质量重建算法。我们提出了一种结合高斯生命周期的变形建模方法，可以有效地捕捉常规和不可逆的变形，从而提高重建质量。此外，我们还提出了一种自适应运动层次策略，可以区分手术场景中的静态和可变形区域。这种策略减少了通过变形场的三维高斯数量，从而提高了渲染速度。大量实验表明，我们的方法在重建质量和渲染速度方面都超过了现有的最新方法。消融研究进一步验证了所提出组件的有效性和必要性。论文接受后，我们将公开我们的源代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01101v1">PDF</a> 7 pages, 4 figures, submitted to ICRA 2025</p>
<p><strong>摘要</strong><br>高效且高保真地重建可变形手术场景是一项至关重要的任务，但具有挑战性。基于最近的三维高斯绘制技术的进展，当前的方法已在重建质量和渲染速度方面取得了显著改进。然而，仍存在两个主要挑战：一是难以处理手术场景中常见的不可逆动态变化，如组织剪切；二是缺乏手术场景变形的层次建模，这降低了渲染速度。为了解决这些挑战，我们引入了EH-SurGS，这是一种高效且高保真地重建可变形手术场景的方法。我们提出了一种变形建模方法，该方法结合了三维高斯的生命周期，可以有效地捕捉常规和不可逆变形，从而提高重建质量。此外，我们提出了一种自适应运动层次策略，可以区分手术场景中的静态和可变形区域。该策略减少了通过变形场的三维高斯数量，从而提高了渲染速度。大量实验表明，我们的方法在重建质量和渲染速度方面都超越了现有的最新方法。消融研究进一步验证了我们所提出组件的有效性和必要性。论文被接受后，我们将公开源代码。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>当前方法在重建手术场景时面临处理不可逆动态变化和缺乏层次建模的挑战。</li>
<li>引入EH-SurGS方法，有效结合三维高斯的生命周期进行变形建模，提高重建质量。</li>
<li>提出自适应运动层次策略，区分静态和可变形区域，提高渲染速度。</li>
<li>相比现有方法，EH-SurGS在重建质量和渲染速度方面都有显著优势。</li>
<li>消融研究验证了方法的有效性，论文被接受后将公开源代码。</li>
<li>EH-SurGS有助于更精确地模拟手术场景中的动态变化，为手术导航和模拟提供更强支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6376e2c14e00621ae0d55937f6ff065.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02505998ad0689e262c6c1b1c0df2034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a502c6f841487f2afc173408406df0f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a0bffce934954855261a9c1388405b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EasySplat-View-Adaptive-Learning-makes-3D-Gaussian-Splatting-Easy"><a href="#EasySplat-View-Adaptive-Learning-makes-3D-Gaussian-Splatting-Easy" class="headerlink" title="EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy"></a>EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy</h2><p><strong>Authors:Ao Gao, Luosong Guo, Tao Chen, Zhao Wang, Ying Tai, Jian Yang, Zhenyu Zhang</strong></p>
<p>3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis. </p>
<blockquote>
<p>3D Gaussian Splatting（3DGS）技术在3D场景表示方面取得了令人满意的成果。尽管其性能令人印象深刻，但它们仍然面临着由于运动结构（SfM）方法获取精确场景初始化的限制或密集化策略的效率低下所带来的挑战。在本文中，我们介绍了一个新型框架EasySplat，以实现高质量的3DGS建模。我们并不使用SfM进行场景初始化，而是采用了一种新方法，以释放大规模点云方法的潜力。具体来说，我们提出了一种基于视图相似性的高效分组策略，并使用稳健的点云先验知识来获得高质量的点云和用于3D场景初始化的相机姿态。在获得可靠的场景结构后，我们提出了一种新型的密集化方法，该方法根据相邻高斯椭圆体的平均形状自适应地拆分高斯原始数据，并利用KNN方案。通过这种方式，所提出的方法解决了初始化和优化方面的限制，实现了高效且准确的3DGS建模。大量实验表明，在处理新型视图合成方面，EasySplat的性能优于当前最先进的水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01003v1">PDF</a> 6 pages, 5figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于EasySplat框架的新型三维高斯映射（3DGS）建模方法。该方法通过基于视图相似性的分组策略和利用稳健的点图先验知识，实现了高质量的三维场景初始化。随后，提出了一种基于邻近高斯椭圆体平均形状的自适应分割密集化方法，克服了初始化和优化方面的局限性，实现了高效且准确的三维场景建模。实验结果证明，EasySplat在处理新型视图合成方面优于当前最先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>采用基于视图相似性的高效分组策略，结合点图先验实现高质量的三维场景初始化。</li>
<li>提出了一种新型自适应分割的密集化方法，根据邻近高斯椭圆体的平均形状进行自适应分割。</li>
<li>该方法克服了结构从运动（SfM）方法的局限性，提高了场景初始化的准确性。</li>
<li>EasySplat框架在解决三维场景建模的优化问题方面表现出了优越性。</li>
<li>通过广泛的实验验证，证明了EasySplat在处理新型视图合成方面的性能优于当前最先进技术。</li>
<li>该方法提高了三维场景建模的质量和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-57c53d6e960aeac9e07e38fe7ccebb6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3921f0c85449feb304d5a8851454653f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20d8b51c31fe0e35161b7831ad2fa802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d0ee591c1f1513bf899ab41708867d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e42f7e4b56a8c7527394390f980dd3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c646762c58b054b22668535602640d22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb41325b90c971f1364b8ff05551e83b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="STORM-Spatio-Temporal-Reconstruction-Model-for-Large-Scale-Outdoor-Scenes"><a href="#STORM-Spatio-Temporal-Reconstruction-Model-for-Large-Scale-Outdoor-Scenes" class="headerlink" title="STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor   Scenes"></a>STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor   Scenes</h2><p><strong>Authors:Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang, Marco Pavone</strong></p>
<p>We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations–parameterized by 3D Gaussians and their velocities–in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., “amodal”) reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding. </p>
<blockquote>
<p>我们提出了名为STORM的时空重建模型，该模型旨在从稀疏观测中重建动态室外场景。现有的动态重建方法通常依赖于针对每个场景的优化、跨越时空的密集观测和强烈的运动监督，这导致了优化时间长、对新颖视图或场景的概括能力有限，以及由动态伪标签噪声引起的质量下降。为了应对这些挑战，STORM利用数据驱动的Transformer架构，在单次前向传递中直接推断动态3D场景表示——通过3D高斯及其速度进行参数化。我们的关键设计是通过自监督场景流汇聚所有帧的3D高斯，并将其转换为目标时间步长，以实现从时间上任意的视点进行完整的（即“非模态”）重建。作为一个附加属性，STORM能够自动捕获动态实例，并仅使用重建损失生成高质量蒙版。在公共数据集上的广泛实验表明，STORM实现了精确的动态场景重建，超越了最先进的按场景优化方法（提高4.3至6.6 PSNR），并超越了现有的前馈方法（提高2.1至4.7 PSNR）在动态区域。STORM在200毫秒内重建大规模室外场景，支持实时渲染，并在场景流估计方面超越竞争对手，3D EPE提高0.422米，Acc5提高28.02%。除了重建之外，我们还展示了模型的其他四个应用，说明了自我监督学习在更广泛的动态场景理解中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00602v1">PDF</a> Project page at: <a target="_blank" rel="noopener" href="https://jiawei-yang.github.io/STORM/">https://jiawei-yang.github.io/STORM/</a></p>
<p><strong>Summary</strong><br>    本文提出一种名为STORM的时空重建模型，用于从稀疏观测中重建动态户外场景。该模型采用数据驱动的Transformer架构，直接推断动态的三维场景表示，通过一次前向传递即可完成。模型通过聚合所有帧的3D高斯分布并利用自监督场景流进行转换，实现任意时刻任意视角的完整重建。此外，STORM还能自动捕获动态实例并仅通过重建损失生成高质量掩膜。实验表明，STORM在公共数据集上实现了精确的动态场景重建，超越了基于场景优化的方法和现有的前馈方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STORM是一种用于重建动态户外场景的时空重建模型。</li>
<li>该模型采用数据驱动的Transformer架构，可快速推断动态的三维场景表示。</li>
<li>通过聚合所有帧的3D高斯分布并利用自监督场景流进行转换，实现任意时刻和视角的完整重建。</li>
<li>STORM能够自动捕获动态实例并生成高质量掩膜。</li>
<li>实验表明，STORM在动态场景重建上超越了现有的方法和竞争对手。</li>
<li>STORM具有实时渲染能力，处理速度较快。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ddce3d201138ede4e49e9b0c7fb51bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29a7f829a9062c72ab58a83d2c5ae560.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27075ee5741060d4831720d8b58a1743.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca135edd8fd37872854550d3b5481541.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PanoSLAM-Panoptic-3D-Scene-Reconstruction-via-Gaussian-SLAM"><a href="#PanoSLAM-Panoptic-3D-Scene-Reconstruction-via-Gaussian-SLAM" class="headerlink" title="PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM"></a>PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM</h2><p><strong>Authors:Runnan Chen, Zhaoqing Wang, Jiepeng Wang, Yuexin Ma, Mingming Gong, Wenping Wang, Tongliang Liu</strong></p>
<p>Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation within a unified framework. Our approach builds upon 3D Gaussian Splatting, modified with several critical components to enable efficient rendering of depth, color, semantic, and instance information from arbitrary viewpoints. To achieve panoptic 3D scene reconstruction from sequential RGB-D videos, we propose an online Spatial-Temporal Lifting (STL) module that transfers 2D panoptic predictions from vision models into 3D Gaussian representations. This STL module addresses the challenges of label noise and inconsistencies in 2D predictions by refining the pseudo labels across multi-view inputs, creating a coherent 3D representation that enhances segmentation accuracy. Our experiments show that PanoSLAM outperforms recent semantic SLAM methods in both mapping and tracking accuracy. For the first time, it achieves panoptic 3D reconstruction of open-world environments directly from the RGB-D video. (<a target="_blank" rel="noopener" href="https://github.com/runnanchen/PanoSLAM">https://github.com/runnanchen/PanoSLAM</a>) </p>
<blockquote>
<p>从连续的视频数据中理解三维场景中的几何、语义和实例信息，对于机器人技术和增强现实应用至关重要。然而，现有的同时定位与地图构建（SLAM）方法通常只关注几何或语义重建。在本文中，我们介绍了PanoSLAM，这是第一个在同一框架内集成几何重建、三维语义分割和三维实例分割的SLAM系统。我们的方法基于三维高斯贴图技术，通过几个关键组件的修改，可以从任意视角高效地呈现深度、颜色、语义和实例信息。为了实现从连续RGB-D视频进行的全景三维场景重建，我们提出了一种在线时空提升（STL）模块，该模块将视觉模型的二维全景预测转化为三维高斯表示。STL模块通过精炼多视角输入的伪标签，解决了标签噪声和二维预测中的不一致性挑战，创建了一个连贯的三维表示，提高了分割精度。我们的实验表明，PanoSLAM在映射和跟踪精度上均优于最新的语义SLAM方法。它首次实现了直接从RGB-D视频进行全景三维重建开放世界环境。（<a target="_blank" rel="noopener" href="https://github.com/runnanchen/PanoSLAM%EF%BC%89">https://github.com/runnanchen/PanoSLAM）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00352v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了PanoSLAM系统，该系统首次在一个统一框架中集成了几何重建、3D语义分割和3D实例分割。该系统基于3D高斯拼贴技术构建，通过在线时空提升模块实现从连续RGB-D视频进行全景3D场景重建。PanoSLAM解决了标签噪声和二维预测不一致的问题，提高了分割精度，并在映射和跟踪准确性方面优于最近的语义SLAM方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PanoSLAM系统是首个在一个框架内整合几何重建、3D语义分割和3D实例分割的SLAM系统。</li>
<li>它基于3D高斯拼贴技术构建，能够从任意视角高效渲染深度、颜色、语义和实例信息。</li>
<li>通过在线时空提升模块，PanoSLAM实现了从连续RGB-D视频进行全景3D场景重建。</li>
<li>该系统解决了标签噪声和二维预测不一致的问题，提高了分割精度。</li>
<li>PanoSLAM在映射和跟踪准确性方面优于最近的语义SLAM方法。</li>
<li>它首次实现了直接从RGB-D视频进行开放世界环境的全景3D重建。</li>
<li>PanoSLAM具有广泛的潜在应用，如机器人技术和增强现实。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa4047e63770fa7632449777fbda5226.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2c14af0de782e3063a6ae71f98c4bcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9033cc03c24cd388a144e93f78e0c99.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SG-Splatting-Accelerating-3D-Gaussian-Splatting-with-Spherical-Gaussians"><a href="#SG-Splatting-Accelerating-3D-Gaussian-Splatting-with-Spherical-Gaussians" class="headerlink" title="SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical   Gaussians"></a>SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical   Gaussians</h2><p><strong>Authors:Yiwen Wang, Siyuan Chen, Ran Yi</strong></p>
<p>3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications. </p>
<blockquote>
<p>3D高斯模糊技术作为一种先进的视图合成技术正逐渐崭露头角，以其视觉质量、速度和渲染效率之间的出色平衡而受到认可。然而，该技术使用三次球面谐波进行颜色表示，产生了巨大的存储需求和计算开销，导致内存占用较大且渲染速度较慢。我们引入了基于球面高斯颜色表示的SG模糊技术，这是一种提高视图合成中渲染速度和质量的新方法。我们的方法首先使用球面高斯而不是三次球面谐波来表示视图相关的颜色，这大大降低了用于颜色表示的参数数量，并大大加快了渲染过程。然后，我们开发了一种有效的策略来组织多个球面高斯，优化它们的排列以实现平衡和准确的场景表示。为了进一步提高渲染质量，我们提出了一种混合表示法，它将球面高斯与低阶球面谐波相结合，有效地捕捉了高低频颜色信息。SG模糊还具有即插即用功能，可轻松集成到现有系统中。这种方法提高了计算效率和总体视觉保真度，使其成为实时应用的实用解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了新兴的3D高斯展平技术及其在新型视图合成中的应用。该技术采用基于球面高斯的颜色表示法，提高了渲染速度和品质。通过优化球面高斯排列策略，实现了场景表示的平衡和准确性。结合低阶球面谐波，有效捕捉了高低频颜色信息，提高了渲染质量。SG-Splatting技术还具有易于集成现有系统的特点，计算效率高，视觉效果好，适合实时应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D高斯展平技术已成为最新视图合成的先进技术，其在视觉质量、速度和渲染效率之间取得了平衡。</li>
<li>原技术使用三度球面谐波进行颜色表示，存在存储需求大、计算量大、内存占用高和渲染速度慢的问题。</li>
<li>引入SG-Splatting技术，采用球面高斯进行颜色表示，大幅度减少了颜色表示所需的参数数量，加速了渲染过程。</li>
<li>优化了多个球面高斯的排列策略，实现了场景表示的平衡和准确性。</li>
<li>结合低阶球面谐波，提出了混合表示方法，有效捕捉高低频颜色信息，提高了渲染质量。</li>
<li>SG-Splatting技术具有易于集成现有系统的特点，具有插件和播放功能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-045b11e24954adf349e48051717ddfcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-828f7887e78002bff81de480a35a2880.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61a479591e126f34839e14a0fe994db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b8257d2b09c0d21477dbbf6a973bb64.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OVGaussian-Generalizable-3D-Gaussian-Segmentation-with-Open-Vocabularies"><a href="#OVGaussian-Generalizable-3D-Gaussian-Segmentation-with-Open-Vocabularies" class="headerlink" title="OVGaussian: Generalizable 3D Gaussian Segmentation with Open   Vocabularies"></a>OVGaussian: Generalizable 3D Gaussian Segmentation with Open   Vocabularies</h2><p><strong>Authors:Runnan Chen, Xiangyu Sun, Zhaoqing Wang, Youquan Liu, Jiepeng Wang, Lingdong Kong, Jiankang Deng, Mingming Gong, Liang Pan, Wenping Wang, Tongliang Liu</strong></p>
<p>Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \textbf{OVGaussian}, a generalizable \textbf{O}pen-\textbf{V}ocabulary 3D semantic segmentation framework based on the 3D \textbf{Gaussian} representation. We first construct a large-scale 3D scene dataset based on 3DGS, dubbed \textbf{SegGaussian}, which provides detailed semantic and instance annotations for both Gaussian points and multi-view images. To promote semantic generalization across scenes, we introduce Generalizable Semantic Rasterization (GSR), which leverages a 3D neural network to learn and predict the semantic property for each 3D Gaussian point, where the semantic property can be rendered as multi-view consistent 2D semantic maps. In the next, we propose a Cross-modal Consistency Learning (CCL) framework that utilizes open-vocabulary annotations of 2D images and 3D Gaussians within SegGaussian to train the 3D neural network capable of open-vocabulary semantic segmentation across Gaussian-based 3D scenes. Experimental results demonstrate that OVGaussian significantly outperforms baseline methods, exhibiting robust cross-scene, cross-domain, and novel-view generalization capabilities. Code and the SegGaussian dataset will be released. (<a target="_blank" rel="noopener" href="https://github.com/runnanchen/OVGaussian">https://github.com/runnanchen/OVGaussian</a>). </p>
<blockquote>
<p>使用基于三维高斯（3DGS）表示进行开放词汇场景理解已经引起了相当大的关注。然而，现有的方法大多基于场景对场景的方式将大型二维视觉模型的知识提升到三维高斯表示中，这限制了其在训练场景内开放词汇查询的能力，并缺乏泛化到新场景的能力。在这项工作中，我们提出了基于三维高斯表示的通用开放词汇三维语义分割框架，名为“OVGaussian”。我们首先构建了一个基于三维高斯表示的大规模三维场景数据集，称为“SegGaussian”，它为高斯点和多视图图像提供了详细的语义和实例注释。为了促进场景之间的语义泛化，我们引入了可泛化语义渲染（GSR），该渲染利用三维神经网络来学习和预测每个三维高斯点的语义属性，其中语义属性可以呈现为多视图一致的二维语义地图。接下来，我们提出了跨模态一致性学习（CCL）框架，该框架利用SegGaussian中的二维图像和三维高斯点的开放词汇注释来训练能够在基于高斯的三维场景上进行开放词汇语义分割的三维神经网络。实验结果表明，OVGaussian在基线方法上表现出显著的优势，展现出强大的跨场景、跨域和新视角的泛化能力。代码和SegGaussian数据集将公开发布。（<a target="_blank" rel="noopener" href="https://github.com/runnanchen/OVGaussian%EF%BC%89%E3%80%82">https://github.com/runnanchen/OVGaussian）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00326v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于三维高斯（3DGS）表示进行开放词汇场景理解已经引起了广泛关注。现有方法主要在场景基础上将二维视觉模型的知识迁移到三维场景上，限制了其在训练场景内的开放词汇查询能力，并且缺乏对新场景的泛化能力。本研究提出了基于三维高斯表示的通用开放词汇三维语义分割框架OVGaussian。首先构建了大规模三维场景数据集SegGaussian，为高斯点和多视角图像提供了详细的语义和实例注释。为了促进跨场景的语义泛化，我们引入了可泛化的语义栅格化（GSR），利用三维神经网络学习和预测每个三维高斯点的语义属性，并将其渲染为多视角一致性的二维语义图。随后，我们提出了跨模态一致性学习（CCL）框架，利用SegGaussian中的二维图像和三维高斯值的开放词汇注释来训练三维神经网络，实现对基于高斯的三维场景的开放词汇语义分割。实验结果表明，OVGaussian显著优于基准方法，具有强大的跨场景、跨域和新颖视角泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有方法大多将二维视觉模型的知识迁移到三维场景上，限制了开放词汇查询能力。</li>
<li>研究提出了基于三维高斯表示的通用开放词汇三维语义分割框架OVGaussian。</li>
<li>构建了大规模三维场景数据集SegGaussian，包含高斯点和多视角图像的详细语义和实例注释。</li>
<li>通过引入可泛化的语义栅格化（GSR），利用三维神经网络预测每个高斯点的语义属性。</li>
<li>提出了跨模态一致性学习（CCL）框架，用于训练能够执行开放词汇语义分割的三维神经网络。</li>
<li>OVGaussian显著优于基准方法，具有强大的跨场景、跨域和新颖视角泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00326">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c951ab512cb966bc359417a3a8d98d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47331af4a2f393736fa8e5a1797d836f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c77d3877b1e47c31dcb8c0944c911b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9eb6e9cc2141c9961d75cbe57c82b1c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-06/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0a3870e306181dbaebde1d1ae4393429.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-06  MalCL Leveraging GAN-Based Generative Replay to Combat Catastrophic   Forgetting in Malware Classification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5295795d6e9753e2cde0b50427285edb.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-01-06  L3D-Pose Lifting Pose for 3D Avatars from a Single Camera in the Wild
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
