<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  Cluster Spin Glass State in Ba$_3$Sb$_{1+x}$Co$_{2-x}$O$_{9-Î´}$   Cation Disorder and Mixed-Valence Co Dimers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ad4037ff005b2a5aee43cce0b85828eb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    91 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-26-æ›´æ–°"><a href="#2025-06-26-æ›´æ–°" class="headerlink" title="2025-06-26 æ›´æ–°"></a>2025-06-26 æ›´æ–°</h1><h2 id="Cluster-Spin-Glass-State-in-Ba-3-Sb-1-x-Co-2-x-O-9-Î´-Cation-Disorder-and-Mixed-Valence-Co-Dimers"><a href="#Cluster-Spin-Glass-State-in-Ba-3-Sb-1-x-Co-2-x-O-9-Î´-Cation-Disorder-and-Mixed-Valence-Co-Dimers" class="headerlink" title="Cluster Spin Glass State in Ba$3$Sb${1+x}$Co${2-x}$O${9-Î´}$:   Cation Disorder and Mixed-Valence Co Dimers"></a>Cluster Spin Glass State in Ba$<em>3$Sb$</em>{1+x}$Co$<em>{2-x}$O$</em>{9-Î´}$:   Cation Disorder and Mixed-Valence Co Dimers</h2><p><strong>Authors:Anzar Ali, Guratinder Kaur, Lukas Keller, Masahiko Isobe</strong></p>
<p>We investigate the structural, magnetic, and thermodynamic properties of \BSCO\ ($x$ &#x3D; 0.04, $\delta$ &#x3D; 0.54), a hexagonal perovskite featuring face-sharing CoO$_6$ octahedra that forms Co dimers. DC and AC magnetization measurements reveal a frequency-dependent spin-freezing transition consistent with glassy dynamics. AC susceptibility fits best to the Vogel-Fulcher model, indicating collective freezing of interacting spin clusters. Isothermal magnetization follows the Langevin function, suggesting finite-sized magnetic clusters rather than isolated paramagnetic moments. Non-equilibrium dynamics, evidenced by thermoremanent magnetization and memory effects, further support a spin-glass-like state. Heat capacity shows no sharp anomalies, and neutron powder diffraction confirms the absence of magnetic Bragg peaks down to 1.5<del>K, ruling out long-range magnetic order. Rietveld refinement reveals significant Co&#x2F;Sb intersite disorder ($\sim$</del>30\pct) and oxygen non-stoichiometry, introducing exchange randomness and frustration that drive the spin-glass-like behavior. Electrical resistivity exhibits Arrhenius-type temperature dependence with an activation energy of 0.173~eV, consistent with semiconducting behavior. Temperature-dependent X-ray diffraction shows no structural phase transitions, confirming that the spin-glass-like state is not lattice-driven. Our results establish \BSCO\ as a cluster spin-glass candidate, where Co dimers, disorder, and geometric frustration prevent long-range order, leading to slow spin dynamics. These findings highlight the role of cation disorder and oxygen vacancies in stabilizing unconventional magnetic states in cobalt-based hexagonal perovskites. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹BSCOï¼ˆx&#x3D;0.04ï¼ŒÎ´&#x3D;0.54ï¼‰çš„ç»“æ„ã€ç£æ€§å’Œçƒ­åŠ›å­¦æ€§è´¨è¿›è¡Œäº†ç ”ç©¶ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰é¢å…±äº«CoO6å…«é¢ä½“çš„å…­æ–¹é’™é’›çŸ¿ç»“æ„ï¼Œå½¢æˆCoäºŒèšä½“ã€‚ç›´æµå’Œäº¤æµç£åŒ–æµ‹é‡æ­ç¤ºäº†ä¸€ç§ä¸ç»ç’ƒæ€åŠ¨åŠ›å­¦ä¸€è‡´çš„é¢‘ç‡ä¾èµ–æ€§è‡ªæ—‹å†»ç»“è½¬å˜ã€‚äº¤æµç£åŒ–ç‡æœ€ç¬¦åˆVogel-Fulcheræ¨¡å‹ï¼Œè¡¨æ˜ç›¸äº’ä½œç”¨è‡ªæ—‹ç°‡çš„é›†ä½“å†»ç»“ã€‚ç­‰æ¸©ç£åŒ–éµå¾ªæœ—ä¹‹ä¸‡å‡½æ•°ï¼Œè¡¨æ˜å­˜åœ¨æœ‰é™å¤§å°çš„ç£ç°‡ï¼Œè€Œéå­¤ç«‹çš„é¡ºç£æ€§çŸ©ã€‚çƒ­å‰©ä½™ç£åŒ–å’Œè®°å¿†æ•ˆåº”æ‰€è¯æ˜çš„éå¹³è¡¡åŠ¨åŠ›å­¦è¿›ä¸€æ­¥æ”¯æŒè‡ªæ—‹ç»ç’ƒæ€ã€‚çƒ­å®¹æ²¡æœ‰æ˜¾ç¤ºå‡ºå°–é”çš„å¼‚å¸¸ç°è±¡ï¼Œä¸­å­ç²‰æœ«è¡å°„ç¡®è®¤åœ¨1.5Kä»¥ä¸‹æ²¡æœ‰ç£å¸ƒæ‹‰æ ¼å³°ï¼Œæ’é™¤äº†è¿œç¨‹ç£åºçš„å­˜åœ¨ã€‚Rietveldç²¾ä¿®æ­ç¤ºå‡ºæ˜¾è‘—çš„Co&#x2F;Sbä½ç‚¹é—´æ— åºï¼ˆçº¦30%ï¼‰å’Œæ°§çš„éåŒ–å­¦è®¡é‡æ¯”ï¼Œè¿™å¼•å…¥äº†äº¤æ¢éšæœºæ€§å’ŒæŒ«æŠ˜æ€§ï¼Œé©±åŠ¨äº†è‡ªæ—‹ç»ç’ƒæ€è¡Œä¸ºã€‚ç”µé˜»ç‡è¡¨ç°å‡ºArrheniusç±»å‹çš„æ¸©åº¦ä¾èµ–æ€§ï¼Œæ´»åŒ–èƒ½ä¸º0.173eVï¼Œä¸åŠå¯¼ä½“è¡Œä¸ºä¸€è‡´ã€‚æ¸©åº¦ä¾èµ–çš„Xå°„çº¿è¡å°„æœªæ˜¾ç¤ºç»“æ„ç›¸å˜ï¼Œè¯å®äº†è‡ªæ—‹ç»ç’ƒæ€ä¸æ˜¯æ™¶æ ¼é©±åŠ¨ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†BSCOä½œä¸ºå›¢ç°‡è‡ªæ—‹ç»ç’ƒå€™é€‰ç‰©çš„åœ°ä½ï¼Œå…¶ä¸­CoäºŒèšä½“ã€æ— åºå’Œå‡ ä½•å—æŒ«é˜²æ­¢äº†è¿œç¨‹æœ‰åºæ€§ï¼Œå¯¼è‡´ç¼“æ…¢çš„è‡ªæ—‹åŠ¨åŠ›å­¦ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é˜³ç¦»å­æ— åºå’Œæ°§ç©ºä½åœ¨ç¨³å®šé’´åŸºå…­æ–¹é’™é’›çŸ¿ä¸­çš„éä¼ ç»Ÿç£æ€ä¸­çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19716v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†BSCOï¼ˆ$x$ &#x3D; 0.04ï¼Œ$\delta$ &#x3D; 0.54ï¼‰çš„ç»“æ„ã€ç£æ€§å’Œçƒ­åŠ›å­¦æ€§è´¨ã€‚è¯¥ææ–™å…·æœ‰å…­è§’å½¢é’™é’›çŸ¿ç»“æ„ï¼Œè¡¨ç°ä¸ºå…±äº«é¢çš„CoO$_6$å…«é¢ä½“å½¢æˆçš„CoäºŒèšä½“ã€‚ç›´æµå’Œäº¤æµç£åŒ–æµ‹é‡æ­ç¤ºäº†ä¸€ç§ä¸ç»ç’ƒæ€åŠ¨åŠ›å­¦ä¸€è‡´çš„é¢‘ç‡ä¾èµ–æ€§è‡ªæ—‹å†»ç»“è½¬å˜ã€‚äº¤æµç£åŒ–ç‡ç¬¦åˆVogel-Fulcheræ¨¡å‹ï¼Œè¡¨æ˜ç›¸äº’ä½œç”¨è‡ªæ—‹ç°‡çš„é›†ä½“å†»ç»“ã€‚ç­‰æ¸©ç£åŒ–éµå¾ªæœ—ä¹‹ä¸‡å‡½æ•°ï¼Œè¡¨æ˜å­˜åœ¨æœ‰é™å¤§å°çš„ç£ç°‡ï¼Œè€Œéå­¤ç«‹çš„é¡ºç£çŸ©ã€‚çƒ­æ®‹ä½™ç£åŒ–å’Œè®°å¿†æ•ˆåº”ç­‰éå¹³è¡¡åŠ¨åŠ›å­¦è¿›ä¸€æ­¥æ”¯æŒè‡ªæ—‹ç»ç’ƒæ€ã€‚çƒ­å®¹æ— å°–é”å¼‚å¸¸ï¼Œä¸­å­ç²‰æœ«è¡å°„è¯å®ä½æ¸©ä¸‹æ— ç£å¸ƒæ‹‰æ ¼å³°ï¼Œæ’é™¤äº†è¿œç¨‹ç£åºã€‚Rietveldç²¾ç‚¼æ˜¾ç¤ºæ˜¾è‘—çš„Co&#x2F;Sbè·¨ä½æ— åºå’Œæ°§çš„éåŒ–å­¦è®¡é‡ï¼Œå¼•å…¥äº¤æ¢éšæœºæ€§å’ŒæŒ«æŠ˜ï¼Œé©±åŠ¨è‡ªæ—‹ç»ç’ƒæ€è¡Œä¸ºã€‚ç”µé˜»ç‡è¡¨ç°å‡ºArrheniuså‹æ¸©åº¦ä¾èµ–æ€§ï¼Œå…·æœ‰0.173eVçš„æ¿€æ´»èƒ½é‡ï¼Œç¬¦åˆåŠå¯¼ä½“è¡Œä¸ºã€‚æ¸©åº¦ä¾èµ–æ€§Xå°„çº¿è¡å°„æœªæ˜¾ç¤ºç»“æ„ç›¸å˜ï¼Œè¯å®è‡ªæ—‹ç»ç’ƒæ€ä¸æ˜¯æ™¶æ ¼é©±åŠ¨ã€‚æœ¬ç ”ç©¶ç»“æœç¡®è®¤BSCOæ˜¯ä¸€ç§é›†ç¾¤è‡ªæ—‹ç»ç’ƒå€™é€‰ææ–™ï¼Œå…¶ä¸­CoäºŒèšä½“ã€æ— åºå’Œå‡ ä½•å—æŒ«é˜²æ­¢äº†è¿œç¨‹æœ‰åºï¼Œå¯¼è‡´ç¼“æ…¢çš„è‡ªæ—‹åŠ¨åŠ›å­¦ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é˜³ç¦»å­æ— åºå’Œæ°§ç©ºä½åœ¨ç¨³å®šé’´åŸºå…­è§’é’™é’›çŸ¿ä¸­çš„ä¸å¯»å¸¸ç£æ€ä¸­çš„ä½œç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>BSCOè¡¨ç°å‡ºå…­è§’å½¢é’™é’›çŸ¿ç»“æ„ç‰¹å¾ï¼Œå…¶ä¸­åŒ…å«å½¢æˆCoäºŒèšä½“çš„å…±äº«é¢CoO$_6$å…«é¢ä½“ã€‚</li>
<li>ç£åŒ–æµ‹é‡æ­ç¤ºäº†ä¸€ç§é¢‘ç‡ä¾èµ–æ€§çš„è‡ªæ—‹å†»ç»“è½¬å˜ï¼Œè¡¨ç°å‡ºç»ç’ƒæ€åŠ¨åŠ›å­¦ç‰¹æ€§ã€‚</li>
<li>ACç£åŒ–ç‡ç¬¦åˆVogel-Fulcheræ¨¡å‹ï¼Œæš—ç¤ºé›†ä½“å†»ç»“çš„ç›¸äº’ä½œç”¨è‡ªæ—‹ç°‡ã€‚</li>
<li>ç­‰æ¸©ç£åŒ–éµå¾ªæœ—ä¹‹ä¸‡å‡½æ•°ï¼Œè¡¨æ˜å­˜åœ¨æœ‰é™å¤§å°çš„ç£ç°‡ã€‚</li>
<li>éå¹³è¡¡åŠ¨åŠ›å­¦è¯æ®è¿›ä¸€æ­¥æ”¯æŒè‡ªæ—‹ç»ç’ƒæ€çš„å­˜åœ¨ã€‚</li>
<li>ä¸­å­ç²‰æœ«è¡å°„å’Œçƒ­å®¹æµ‹é‡æ’é™¤äº†è¿œç¨‹ç£åºçš„å­˜åœ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-862e4243989184fa3f134c1b60ae5fb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0509de44baa619e1d869ee89cfc38f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb7dff3d059d88528fdd8761727d3393.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2178f87f5fc1560319136c0251339e29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-999ebe0a4281691dbea3e9f82c6a2686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee1c3beafc6a65a729942e939110e04b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UltraAD-Fine-Grained-Ultrasound-Anomaly-Classification-via-Few-Shot-CLIP-Adaptation"><a href="#UltraAD-Fine-Grained-Ultrasound-Anomaly-Classification-via-Few-Shot-CLIP-Adaptation" class="headerlink" title="UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot   CLIP Adaptation"></a>UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot   CLIP Adaptation</h2><p><strong>Authors:Yue Zhou, Yuan Bi, Wenjuan Tong, Wei Wang, Nassir Navab, Zhongliang Jiang</strong></p>
<p>Precise anomaly detection in medical images is critical for clinical decision-making. While recent unsupervised or semi-supervised anomaly detection methods trained on large-scale normal data show promising results, they lack fine-grained differentiation, such as benign vs. malignant tumors. Additionally, ultrasound (US) imaging is highly sensitive to devices and acquisition parameter variations, creating significant domain gaps in the resulting US images. To address these challenges, we propose UltraAD, a vision-language model (VLM)-based approach that leverages few-shot US examples for generalized anomaly localization and fine-grained classification. To enhance localization performance, the image-level token of query visual prototypes is first fused with learnable text embeddings. This image-informed prompt feature is then further integrated with patch-level tokens, refining local representations for improved accuracy. For fine-grained classification, a memory bank is constructed from few-shot image samples and corresponding text descriptions that capture anatomical and abnormality-specific features. During training, the stored text embeddings remain frozen, while image features are adapted to better align with medical data. UltraAD has been extensively evaluated on three breast US datasets, outperforming state-of-the-art methods in both lesion localization and fine-grained medical classification. The code will be released upon acceptance. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„ç²¾ç¡®å¼‚å¸¸æ£€æµ‹å¯¹äºä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„æ— ç›‘ç£æˆ–åŠç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨å¤§è§„æ¨¡æ­£å¸¸æ•°æ®è®­ç»ƒä¸Šæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬ç¼ºä¹ç²¾ç»†çš„å·®å¼‚åŒ–ï¼Œå¦‚è‰¯æ€§ä¸æ¶æ€§è‚¿ç˜¤ä¹‹é—´çš„åŒºåˆ†ã€‚æ­¤å¤–ï¼Œè¶…å£°ï¼ˆUSï¼‰æˆåƒå¯¹è®¾å¤‡å’Œé‡‡é›†å‚æ•°çš„å˜åŒ–é«˜åº¦æ•æ„Ÿï¼Œå¯¼è‡´è¶…å£°å›¾åƒå­˜åœ¨æ˜¾è‘—çš„åŸŸå·®è·ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UltraADï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å°‘é‡çš„è¶…å£°ç¤ºä¾‹è¿›è¡Œé€šç”¨å¼‚å¸¸å®šä½å’Œç²¾ç»†åˆ†ç±»ã€‚ä¸ºäº†æé«˜å®šä½æ€§èƒ½ï¼Œé¦–å…ˆèåˆæŸ¥è¯¢è§†è§‰åŸå‹çš„å›¾åƒçº§ä»¤ç‰Œå’Œå¯å­¦ä¹ çš„æ–‡æœ¬åµŒå…¥ã€‚ç„¶åï¼Œå°†å›¾åƒä¿¡æ¯æç¤ºç‰¹å¾ä¸è¡¥ä¸çº§ä»¤ç‰Œè¿›ä¸€æ­¥é›†æˆï¼Œç»†åŒ–å±€éƒ¨è¡¨ç¤ºä»¥æé«˜å‡†ç¡®æ€§ã€‚å¯¹äºç²¾ç»†åˆ†ç±»ï¼Œä»å°‘é‡å›¾åƒæ ·æœ¬å’Œç›¸åº”çš„æ–‡æœ¬æè¿°ä¸­æ„å»ºäº†ä¸€ä¸ªå†…å­˜é“¶è¡Œï¼Œä»¥æ•è·ç‰¹å®šçš„è§£å‰–å’Œå¼‚å¸¸ç‰¹å¾ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå­˜å‚¨çš„æ–‡æœ¬åµŒå…¥ä¿æŒä¸å˜ï¼Œè€Œå›¾åƒç‰¹å¾æ›´å¥½åœ°é€‚åº”åŒ»å­¦æ•°æ®ã€‚UltraADåœ¨ä¸‰ä¸ªä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œåœ¨ç—…ç¶å®šä½å’Œç²¾ç»†åŒ»å­¦åˆ†ç±»æ–¹é¢å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19694v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç²¾å‡†æ£€æµ‹å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œå¦‚ç¼ºä¹ç²¾ç»†ç²’åº¦åŒºåˆ†å’Œè¶…å£°å›¾åƒé¢†åŸŸå·®å¼‚çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºUltraADæ¨¡å‹ï¼Œé‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç»“åˆå°‘é‡è¶…å£°å›¾åƒæ ·æœ¬è¿›è¡Œå¼‚å¸¸å®šä½ä¸ç²¾ç»†åˆ†ç±»ã€‚é€šè¿‡èåˆå›¾åƒçº§ä»¤ç‰Œä¸å¯å­¦ä¹ æ–‡æœ¬åµŒå…¥ï¼Œå¢å¼ºå®šä½æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨è®°å¿†åº“å­˜å‚¨å°‘æ•°å›¾åƒæ ·æœ¬åŠå…¶æ–‡æœ¬æè¿°ï¼Œè¿›è¡Œç²¾ç»†åˆ†ç±»ã€‚åœ¨ä¸‰ä¸ªä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒUltraADåœ¨ç—…ç¶å®šä½å’Œç²¾ç»†åˆ†ç±»æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒç²¾å‡†æ£€æµ‹å¯¹ä¸´åºŠå†³ç­–çš„é‡è¦æ€§ã€‚</li>
<li>å½“å‰ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸Šçš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç»†åˆ†ç±»æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>UltraADæ¨¡å‹é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>UltraADé€šè¿‡èåˆå›¾åƒçº§ä»¤ç‰Œä¸æ–‡æœ¬åµŒå…¥å¢å¼ºå¼‚å¸¸å®šä½æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨è®°å¿†åº“è¿›è¡Œç²¾ç»†åˆ†ç±»ï¼Œé€šè¿‡å­˜å‚¨å°‘æ•°å›¾åƒæ ·æœ¬åŠå…¶æ–‡æœ¬æè¿°æ¥æ•æ‰ç‰¹å®šè§£å‰–ç»“æ„å’Œå¼‚å¸¸ç‰¹å¾ã€‚</li>
<li>UltraADåœ¨ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9228d5cad6de18bc5dfc422f865ea5d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a7d1ed8f49296903372cbc391abecf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-194586963cfb90b18014120ac78c2ba0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReCoGNet-Recurrent-Context-Guided-Network-for-3D-MRI-Prostate-Segmentation"><a href="#ReCoGNet-Recurrent-Context-Guided-Network-for-3D-MRI-Prostate-Segmentation" class="headerlink" title="ReCoGNet: Recurrent Context-Guided Network for 3D MRI Prostate   Segmentation"></a>ReCoGNet: Recurrent Context-Guided Network for 3D MRI Prostate   Segmentation</h2><p><strong>Authors:Ahmad Mustafa, Reza Rastegar, Ghassan AlRegib</strong></p>
<p>Prostate gland segmentation from T2-weighted MRI is a critical yet challenging task in clinical prostate cancer assessment. While deep learning-based methods have significantly advanced automated segmentation, most conventional approaches-particularly 2D convolutional neural networks (CNNs)-fail to leverage inter-slice anatomical continuity, limiting their accuracy and robustness. Fully 3D models offer improved spatial coherence but require large amounts of annotated data, which is often impractical in clinical settings. To address these limitations, we propose a hybrid architecture that models MRI sequences as spatiotemporal data. Our method uses a deep, pretrained DeepLabV3 backbone to extract high-level semantic features from each MRI slice and a recurrent convolutional head, built with ConvLSTM layers, to integrate information across slices while preserving spatial structure. This combination enables context-aware segmentation with improved consistency, particularly in data-limited and noisy imaging conditions. We evaluate our method on the PROMISE12 benchmark under both clean and contrast-degraded test settings. Compared to state-of-the-art 2D and 3D segmentation models, our approach demonstrates superior performance in terms of precision, recall, Intersection over Union (IoU), and Dice Similarity Coefficient (DSC), highlighting its potential for robust clinical deployment. </p>
<blockquote>
<p>å‰åˆ—è…ºT2åŠ æƒMRIçš„è…ºä½“åˆ†å‰²æ˜¯ä¸´åºŠå‰åˆ—è…ºç™Œè¯„ä¼°ä¸­ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»æå¤§åœ°æ¨åŠ¨äº†è‡ªåŠ¨åˆ†å‰²çš„è¿›å±•ï¼Œä½†å¤§å¤šæ•°ä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯äºŒç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œæ— æ³•åˆ©ç”¨åˆ‡ç‰‡é—´çš„è§£å‰–è¿ç»­æ€§ï¼Œä»è€Œé™åˆ¶äº†å…¶å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚å®Œå…¨ä¸‰ç»´æ¨¡å‹æä¾›äº†æ›´å¥½çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œä½†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™åœ¨ä¸´åºŠç¯å¢ƒä¸­å¾€å¾€ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„å°†MRIåºåˆ—å»ºæ¨¡ä¸ºæ—¶ç©ºæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ·±åº¦é¢„è®­ç»ƒDeepLabV[ç²¾ã€‘ç½‘ç»œä»æ¯ä¸ªMRIåˆ‡ç‰‡ä¸­æå–é«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰ConvLSTMå±‚çš„é€’å½’å·ç§¯å¤´æ¥æ•´åˆåˆ‡ç‰‡ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ç»“æ„ã€‚è¿™ç§ç»„åˆå¯å®ç°å…·æœ‰ä¸€è‡´æ€§çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™å’Œå›¾åƒå™ªå£°è¾ƒå¤§çš„æƒ…å†µä¸‹æ›´æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åœ¨å¹²å‡€å’Œå¯¹æ¯”åº¦é™ä½çš„æµ‹è¯•è®¾ç½®ä¸‹ä½¿ç”¨PROMISE12åŸºå‡†æ•°æ®é›†è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ä¸æœ€å…ˆè¿›çš„äºŒç»´å’Œä¸‰ç»´åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡ã€äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’ŒDiceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨ä¸´åºŠéƒ¨ç½²ä¸­çš„ç¨³å¥æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19687v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ç”¨äºå‰åˆ—è…ºMRIå›¾åƒçš„åˆ†å‰²ï¼Œè¯¥æ¶æ„å°†MRIåºåˆ—è§†ä¸ºæ—¶ç©ºæ•°æ®ã€‚å®ƒç»“åˆäº†DeepLabV3çš„æ·±åº¦å­¦ä¹ ç‰¹æ€§å’ŒConvLSTMå±‚çš„å·ç§¯å¤´ï¼Œèƒ½åœ¨æ•°æ®æœ‰é™å’Œå™ªå£°å¹²æ‰°çš„æƒ…å†µä¸‹å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ†å‰²ï¼Œæé«˜äº†åˆ†å‰²çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚åœ¨PROMISE12åŸºå‡†æµ‹è¯•ä¸‹ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„äºŒç»´å’Œä¸‰ç»´åˆ†å‰²æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰åˆ—è…ºMRIå›¾åƒåˆ†å‰²æ˜¯ä¸´åºŠå‰åˆ—è…ºç™Œè¯„ä¼°ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œä½†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²æ˜¾è‘—æ”¹å–„è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ³•ï¼Œä½†å¸¸è§„æ–¹æ³•ä»æœ‰é™åˆ¶ã€‚</li>
<li>å®Œå…¨ä¸‰ç»´æ¨¡å‹èƒ½æé«˜ç©ºé—´è¿è´¯æ€§ï¼Œä½†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä¸å®ç”¨äºä¸´åºŠç¯å¢ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„ï¼Œå°†MRIåºåˆ—è§†ä¸ºæ—¶ç©ºæ•°æ®ã€‚</li>
<li>è¯¥æ¶æ„ä½¿ç”¨DeepLabV3æå–é«˜å±‚æ¬¡çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ç»“åˆConvLSTMå±‚æ•´åˆåˆ‡ç‰‡ä¿¡æ¯ï¼Œä¿æŒç©ºé—´ç»“æ„ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½åœ¨æ•°æ®æœ‰é™å’Œå™ªå£°å¹²æ‰°çš„æƒ…å†µä¸‹å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a08e6d38f6a70706ffc9faf0e7d2bca4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2dad360ddb6fd2ae7d4afb977bfed07.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Recurrent-Visual-Feature-Extraction-and-Stereo-Attentions-for-CT-Report-Generation"><a href="#Recurrent-Visual-Feature-Extraction-and-Stereo-Attentions-for-CT-Report-Generation" class="headerlink" title="Recurrent Visual Feature Extraction and Stereo Attentions for CT Report   Generation"></a>Recurrent Visual Feature Extraction and Stereo Attentions for CT Report   Generation</h2><p><strong>Authors:Yuanhe Tian, Lei Mao, Yan Song</strong></p>
<p>Generating reports for computed tomography (CT) images is a challenging task, while similar to existing studies for medical image report generation, yet has its unique characteristics, such as spatial encoding of multiple images, alignment between image volume and texts, etc. Existing solutions typically use general 2D or 3D image processing techniques to extract features from a CT volume, where they firstly compress the volume and then divide the compressed CT slices into patches for visual encoding. These approaches do not explicitly account for the transformations among CT slices, nor do they effectively integrate multi-level image features, particularly those containing specific organ lesions, to instruct CT report generation (CTRG). In considering the strong correlation among consecutive slices in CT scans, in this paper, we propose a large language model (LLM) based CTRG method with recurrent visual feature extraction and stereo attentions for hierarchical feature modeling. Specifically, we use a vision Transformer to recurrently process each slice in a CT volume, and employ a set of attentions over the encoded slices from different perspectives to selectively obtain important visual information and align them with textual features, so as to better instruct an LLM for CTRG. Experiment results and further analysis on the benchmark M3D-Cap dataset show that our method outperforms strong baseline models and achieves state-of-the-art results, demonstrating its validity and effectiveness. </p>
<blockquote>
<p>ç”Ÿæˆè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒçš„æŠ¥å‘Šæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶å®ƒä¸ç°æœ‰çš„åŒ»å­¦å›¾åƒæŠ¥å‘Šç”Ÿæˆç ”ç©¶ç›¸ä¼¼ï¼Œä½†å®ƒå…·æœ‰ç‹¬ç‰¹çš„ç‰¹æ€§ï¼Œä¾‹å¦‚å¤šä¸ªå›¾åƒçš„ç©ºé—´ç¼–ç ã€å›¾åƒä½“ç§¯ä¸æ–‡æœ¬ä¹‹é—´çš„å¯¹é½ç­‰ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä½¿ç”¨é€šç”¨çš„äºŒç»´æˆ–ä¸‰ç»´å›¾åƒå¤„ç†æŠ€æœ¯ä»CTä½“ç§¯ä¸­æå–ç‰¹å¾ï¼Œå…¶ä¸­ä»–ä»¬é¦–å…ˆå‹ç¼©ä½“ç§¯ï¼Œç„¶åå°†å‹ç¼©çš„CTåˆ‡ç‰‡åˆ†æˆæ–‘å—è¿›è¡Œè§†è§‰ç¼–ç ã€‚è¿™äº›æ–¹æ³•å¹¶æ²¡æœ‰æ˜ç¡®è€ƒè™‘CTåˆ‡ç‰‡ä¹‹é—´çš„è½¬æ¢ï¼Œä¹Ÿæ²¡æœ‰æœ‰æ•ˆåœ°æ•´åˆå¤šçº§å›¾åƒç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ç‰¹å®šå™¨å®˜ç—…å˜çš„ç‰¹å¾ï¼Œä»¥æŒ‡å¯¼CTæŠ¥å‘Šç”Ÿæˆï¼ˆCTRGï¼‰ã€‚è€ƒè™‘åˆ°CTæ‰«æä¸­è¿ç»­åˆ‡ç‰‡ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„CTRGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¾ªç¯è§†è§‰ç‰¹å¾æå–å’Œç«‹ä½“æ³¨æ„åŠ›ï¼Œç”¨äºåˆ†å±‚ç‰¹å¾å»ºæ¨¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨è§†è§‰Transformerå¾ªç¯å¤„ç†CTä½“ç§¯ä¸­çš„æ¯ä¸ªåˆ‡ç‰‡ï¼Œå¹¶é€šè¿‡å¯¹ä¸åŒè§†è§’çš„ç¼–ç åˆ‡ç‰‡é›†æ³¨æ„åŠ›æ¥æœ‰é€‰æ‹©åœ°è·å–é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œä»¥æ›´å¥½åœ°æŒ‡å¯¼LLMè¿›è¡ŒCTRGã€‚åœ¨åŸºå‡†æ•°æ®é›†M3D-Capä¸Šçš„å®éªŒç»“æœå’Œè¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19665v1">PDF</a> 7 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡çš„ç‹¬ç‰¹æ€§ï¼Œæå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆé€’å½’è§†è§‰ç‰¹å¾æå–å’Œç«‹ä½“æ³¨æ„åŠ›è¿›è¡Œåˆ†å±‚ç‰¹å¾å»ºæ¨¡ã€‚é€šè¿‡ä½¿ç”¨è§†è§‰Transformeré€’å½’å¤„ç†CTä½“ç§¯ä¸­çš„æ¯ä¸ªåˆ‡ç‰‡ï¼Œå¹¶ä»ä¸åŒè§’åº¦å¯¹ç¼–ç åˆ‡ç‰‡è¿›è¡Œæ³¨æ„åŠ›é›†ä¸­ï¼Œé€‰æ‹©æ€§è·å–é‡è¦è§†è§‰ä¿¡æ¯å¹¶ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œä»¥æ›´å¥½åœ°æŒ‡å¯¼LLMè¿›è¡ŒCTæŠ¥å‘Šç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨M3D-Capæ•°æ®é›†ä¸Šä¼˜äºå¼ºåŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTå›¾åƒæŠ¥å‘Šç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è€ƒè™‘ç©ºé—´ç¼–ç å’Œå¤šå›¾åƒå¯¹é½ç­‰ç‹¬ç‰¹ç‰¹æ€§ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸é‡‡ç”¨ä¸€èˆ¬çš„2Dæˆ–3Då›¾åƒå¤„ç†æŠ€æœ¯ä»CTä½“ç§¯ä¸­æå–ç‰¹å¾ï¼Œä½†ç¼ºä¹å¯¹ä¸åŒåˆ‡ç‰‡é—´å˜æ¢çš„æ˜¾å¼è€ƒè™‘ä»¥åŠå¤šçº§åˆ«å›¾åƒç‰¹å¾çš„æ•´åˆã€‚</li>
<li>æ–‡ä¸­æå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„CTæŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆé€’å½’è§†è§‰ç‰¹å¾æå–å’Œç«‹ä½“æ³¨æ„åŠ›è¿›è¡Œåˆ†å±‚ç‰¹å¾å»ºæ¨¡ã€‚</li>
<li>ä½¿ç”¨è§†è§‰Transformeré€’å½’å¤„ç†CTä½“ç§¯ä¸­çš„æ¯ä¸ªåˆ‡ç‰‡ï¼Œä»¥è·å–æ›´å…¨é¢çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ä¸åŒè§’åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹ç¼–ç åˆ‡ç‰‡è¿›è¡Œæ³¨æ„åŠ›é›†ä¸­ï¼Œé€‰æ‹©æ€§è·å–é‡è¦è§†è§‰ä¿¡æ¯å¹¶ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨M3D-Capæ•°æ®é›†ä¸Šæ€§èƒ½ä¼˜è¶Šï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ee423f750f3eb6047ce95ff03588ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d687f01d109c7708e443ee73b36929.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5d146356967cb8c10e3b9c05f72c8f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-714702f18ffb5fcfde60ae09487701a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2d0218b5d4ae54b8d437e368df5e814.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SAM2-SGP-Enhancing-SAM2-for-Medical-Image-Segmentation-via-Support-Set-Guided-Prompting"><a href="#SAM2-SGP-Enhancing-SAM2-for-Medical-Image-Segmentation-via-Support-Set-Guided-Prompting" class="headerlink" title="SAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set   Guided Prompting"></a>SAM2-SGP: Enhancing SAM2 for Medical Image Segmentation via Support-Set   Guided Prompting</h2><p><strong>Authors:Yang Xing, Jiong Wu, Yuheng Bu, Kuang Gong</strong></p>
<p>Although new vision foundation models such as Segment Anything Model 2 (SAM2) have significantly enhanced zero-shot image segmentation capabilities, reliance on human-provided prompts poses significant challenges in adapting SAM2 to medical image segmentation tasks. Moreover, SAM2â€™s performance in medical image segmentation was limited by the domain shift issue, since it was originally trained on natural images and videos. To address these challenges, we proposed SAM2 with support-set guided prompting (SAM2-SGP), a framework that eliminated the need for manual prompts. The proposed model leveraged the memory mechanism of SAM2 to generate pseudo-masks using image-mask pairs from a support set via a Pseudo-mask Generation (PMG) module. We further introduced a novel Pseudo-mask Attention (PMA) module, which used these pseudo-masks to automatically generate bounding boxes and enhance localized feature extraction by guiding attention to relevant areas. Furthermore, a low-rank adaptation (LoRA) strategy was adopted to mitigate the domain shift issue. The proposed framework was evaluated on both 2D and 3D datasets across multiple medical imaging modalities, including fundus photography, X-ray, computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), and ultrasound. The results demonstrated a significant performance improvement over state-of-the-art models, such as nnUNet and SwinUNet, as well as foundation models, such as SAM2 and MedSAM2, underscoring the effectiveness of the proposed approach. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/astlian9/SAM_Support">https://github.com/astlian9/SAM_Support</a>. </p>
<blockquote>
<p>å°½ç®¡Segment Anything Model 2ï¼ˆSAM2ï¼‰ç­‰æ–°å‹è§†è§‰åŸºç¡€æ¨¡å‹åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²èƒ½åŠ›æ–¹é¢æœ‰äº†æ˜¾è‘—æå‡ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¾èµ–äººå·¥æç¤ºï¼Œè¿™å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç”±äºSAM2æœ€åˆæ˜¯åœ¨è‡ªç„¶å›¾åƒå’Œè§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢å­˜åœ¨é¢†åŸŸåç§»é—®é¢˜ï¼Œé™åˆ¶äº†å…¶æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æ”¯æŒé›†å¼•å¯¼æç¤ºçš„SAM2ï¼ˆSAM2-SGPï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¶ˆé™¤äº†å¯¹äººå·¥æç¤ºçš„éœ€æ±‚ã€‚æ‰€æå‡ºæ¨¡å‹åˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶ï¼Œé€šè¿‡ä¼ªæ©ç ç”Ÿæˆï¼ˆPMGï¼‰æ¨¡å—ï¼Œä½¿ç”¨æ”¯æŒé›†ä¸­çš„å›¾åƒ-æ©ç å¯¹ç”Ÿæˆä¼ªæ©ç ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¼ªæ©ç æ³¨æ„åŠ›ï¼ˆPMAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨è¿™äº›ä¼ªæ©ç è‡ªåŠ¨ç”Ÿæˆè¾¹ç•Œæ¡†ï¼Œå¹¶é€šè¿‡å¼•å¯¼æ³¨æ„åŠ›å…³æ³¨ç›¸å…³åŒºåŸŸæ¥å¢å¼ºå±€éƒ¨ç‰¹å¾æå–ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰ç­–ç•¥æ¥ç¼“è§£é¢†åŸŸåç§»é—®é¢˜ã€‚æ‰€æå‡ºçš„æ¡†æ¶åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€çš„äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬çœ¼åº•æ‘„å½±ã€Xå°„çº¿ã€è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å’Œè¶…å£°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ¨¡å‹ï¼ˆå¦‚nnUNetå’ŒSwinUNetï¼‰ä»¥åŠåŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAM2å’ŒMedSAM2ï¼‰ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ï¼Œè¿™çªæ˜¾äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/astlian9/SAM_Support%E5%BC%B9%E7%A4%BC%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/astlian9/SAM_Supportå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19658v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰çš„æ–°è§†è§‰åŸºç¡€æ¨¡å‹è™½ç„¶å¤§å¹…æå‡äº†é›¶æ ·æœ¬å›¾åƒåˆ†å‰²èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä»é¢ä¸´ä¾èµ–äººå·¥æç¤ºçš„æŒ‘æˆ˜ä»¥åŠé¢†åŸŸåç§»é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SAM2æ”¯æŒé›†å¼•å¯¼æç¤ºï¼ˆSAM2-SGPï¼‰æ¡†æ¶ï¼Œæ— éœ€æ‰‹åŠ¨æç¤ºå³å¯ç”Ÿæˆä¼ªæ©è†œï¼Œé€šè¿‡ä¼ªæ©è†œç”Ÿæˆï¼ˆPMGï¼‰æ¨¡å—åˆ©ç”¨æ”¯æŒé›†ä¸­çš„å›¾åƒ-æ©è†œå¯¹ï¼Œå¹¶é€šè¿‡ä¼ªæ©è†œæ³¨æ„åŠ›ï¼ˆPMAï¼‰æ¨¡å—è‡ªåŠ¨ç”Ÿæˆè¾¹ç•Œæ¡†ï¼Œå¢å¼ºå±€éƒ¨ç‰¹å¾æå–ã€‚åŒæ—¶é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­–ç•¥ç¼“è§£é¢†åŸŸåç§»é—®é¢˜ã€‚åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡æ€çš„äºŒç»´å’Œä¸‰ç»´æ•°æ®é›†ä¸Šè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶è¾ƒnnUNetã€SwinUNetç­‰å‰æ²¿æ¨¡å‹åŠSAM2ç­‰åŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAM2åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´äººå·¥æç¤ºå’Œé¢†åŸŸåç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>SAM2-SGPæ¡†æ¶é€šè¿‡æ”¯æŒé›†è‡ªåŠ¨ç”Ÿæˆä¼ªæ©è†œï¼Œæ¶ˆé™¤å¯¹äººå·¥æç¤ºçš„ä¾èµ–ã€‚</li>
<li>ä¼ªæ©è†œç”Ÿæˆï¼ˆPMGï¼‰æ¨¡å—åˆ©ç”¨å›¾åƒ-æ©è†œå¯¹äº§ç”Ÿä¼ªæ©è†œã€‚</li>
<li>ä¼ªæ©è†œæ³¨æ„åŠ›ï¼ˆPMAï¼‰æ¨¡å—è‡ªåŠ¨ç”Ÿæˆè¾¹ç•Œæ¡†ï¼Œå¢å¼ºå±€éƒ¨ç‰¹å¾æå–ã€‚</li>
<li>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­–ç•¥ç”¨äºç¼“è§£é¢†åŸŸåç§»é—®é¢˜ã€‚</li>
<li>åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæ˜¾ç¤ºSAM2-SGPæ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3cf34113f8b3bf6921f06fd40b783dbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f4e992771552933672d114163a1038a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c9f547b28bcd5faaf72589bd771640.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2d8eb8140e3e6b969adfa6441f032be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-886dd78184132d6859bf5c056779ff35.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HOIverse-A-Synthetic-Scene-Graph-Dataset-With-Human-Object-Interactions"><a href="#HOIverse-A-Synthetic-Scene-Graph-Dataset-With-Human-Object-Interactions" class="headerlink" title="HOIverse: A Synthetic Scene Graph Dataset With Human Object Interactions"></a>HOIverse: A Synthetic Scene Graph Dataset With Human Object Interactions</h2><p><strong>Authors:Mrunmai Vivek Phatak, Julian Lorenz, Nico HÃ¶rmann, JÃ¶rg HÃ¤hner, Rainer Lienhart</strong></p>
<p>When humans and robotic agents coexist in an environment, scene understanding becomes crucial for the agents to carry out various downstream tasks like navigation and planning. Hence, an agent must be capable of localizing and identifying actions performed by the human. Current research lacks reliable datasets for performing scene understanding within indoor environments where humans are also a part of the scene. Scene Graphs enable us to generate a structured representation of a scene or an image to perform visual scene understanding. To tackle this, we present HOIverse a synthetic dataset at the intersection of scene graph and human-object interaction, consisting of accurate and dense relationship ground truths between humans and surrounding objects along with corresponding RGB images, segmentation masks, depth images and human keypoints. We compute parametric relations between various pairs of objects and human-object pairs, resulting in an accurate and unambiguous relation definitions. In addition, we benchmark our dataset on state-of-the-art scene graph generation models to predict parametric relations and human-object interactions. Through this dataset, we aim to accelerate research in the field of scene understanding involving people. </p>
<blockquote>
<p>å½“äººç±»å’Œæœºå™¨äººä»£ç†å…±å­˜äºåŒä¸€ç¯å¢ƒä¸­æ—¶ï¼Œåœºæ™¯ç†è§£å¯¹äºä»£ç†æ‰§è¡Œå¯¼èˆªå’Œè§„åˆ’ç­‰å„ç§ä¸‹æ¸¸ä»»åŠ¡å˜å¾—è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œä»£ç†å¿…é¡»å…·å¤‡å®šä½å¹¶è¯†åˆ«äººç±»æ‰€æ‰§è¡ŒåŠ¨ä½œçš„èƒ½åŠ›ã€‚å½“å‰çš„ç ”ç©¶ç¼ºä¹å¯é çš„å®¤å†…ç¯å¢ƒæ•°æ®é›†æ¥è¿›è¡Œåœºæ™¯ç†è§£ï¼Œè€Œäººç±»ä¹Ÿæ˜¯åœºæ™¯çš„ä¸€éƒ¨åˆ†ã€‚åœºæ™¯å›¾ä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆåœºæ™¯æˆ–å›¾åƒçš„ç»“æ„åŒ–è¡¨ç¤ºæ¥è¿›è¡Œè§†è§‰åœºæ™¯ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HOIverseï¼Œè¿™æ˜¯ä¸€ä¸ªåœºæ™¯å›¾å’Œäººæœºäº¤äº’ä¹‹é—´çš„åˆæˆæ•°æ®é›†ï¼ŒåŒ…å«äº†äººç±»ä¸å‘¨å›´ç‰©ä½“ä¹‹é—´å‡†ç¡®ä¸”å¯†é›†çš„å…³ç³»çœŸå®æƒ…å†µï¼Œä»¥åŠç›¸åº”çš„RGBå›¾åƒã€åˆ†å‰²æ©è†œã€æ·±åº¦å›¾åƒå’Œäººç±»å…³é”®ç‚¹ã€‚æˆ‘ä»¬è®¡ç®—äº†å„ç‰©ä½“å¯¹ä»¥åŠäººæœºäº¤äº’å¯¹ä¹‹é—´çš„å‚æ•°å…³ç³»ï¼Œä»è€Œå¾—åˆ°å‡†ç¡®ä¸”æ˜ç¡®çš„å…³è”å®šä¹‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æœ€å…ˆè¿›çš„åœºæ™¯å›¾ç”Ÿæˆæ¨¡å‹ä¸Šå¯¹æˆ‘ä»¬çš„æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä»¥é¢„æµ‹å‚æ•°å…³ç³»å’Œäººæœºäº¤äº’ã€‚é€šè¿‡æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æ—¨åœ¨åŠ é€Ÿæ¶‰åŠäººç±»çš„åœºæ™¯ç†è§£é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19639v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å®¤å†…ç¯å¢ƒä¸‹äººç±»ä¸æœºå™¨äººååŒå·¥ä½œæ—¶ï¼Œåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚å½“å‰ç¼ºä¹å¯é æ•°æ®é›†æ”¯æŒå®¤å†…ç¯å¢ƒä¸‹äººç±»å‚ä¸çš„åœºæ™¯ç†è§£ã€‚HOIverseæ•°æ®é›†ç»“åˆåœºæ™¯å›¾å’Œäººç±»ç‰©ä½“äº¤äº’ï¼ŒåŒ…å«ç²¾å‡†å¯†é›†å…³ç³»ä¿¡æ¯ä»¥åŠå¯¹åº”RGBå›¾åƒç­‰å¤šåª’ä½“ä¿¡æ¯ã€‚æ•°æ®é›†é€šè¿‡è®¡ç®—ç‰©ä½“å’Œäººä¸ç‰©ä½“é—´çš„å‚æ•°å…³ç³»ï¼Œæä¾›æ¸…æ™°çš„å…³ç³»å®šä¹‰ï¼Œå¹¶æ”¯æŒåœºæ™¯å›¾çš„ç”Ÿæˆæ¨¡å‹é¢„æµ‹å’Œäººä¸ç‰©ä½“çš„äº¤äº’ã€‚æ­¤æ•°æ®é›†æ—¨åœ¨åŠ é€Ÿæ¶‰åŠäººç±»çš„åœºæ™¯ç†è§£ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¤å†…ç¯å¢ƒä¸‹äººç±»ä¸æœºå™¨äººååŒå·¥ä½œçš„åœºæ™¯ä¸­ï¼Œåœºæ™¯ç†è§£æ˜¯å®Œæˆä¸‹æ¸¸ä»»åŠ¡å¦‚å¯¼èˆªå’Œè§„åˆ’çš„å…³é”®ã€‚</li>
<li>å½“å‰ç¼ºä¹é’ˆå¯¹å®¤å†…ç¯å¢ƒä¸”åŒ…å«äººç±»åœ¨å†…çš„å¯é æ•°æ®é›†æ”¯æŒåœºæ™¯ç†è§£ç ”ç©¶ã€‚</li>
<li>HOIverseæ•°æ®é›†ç»“åˆäº†åœºæ™¯å›¾å’Œäººç±»ç‰©ä½“äº¤äº’ï¼Œæä¾›ç»“æ„åŒ–åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>HOIverseåŒ…å«ç²¾å‡†å¯†é›†çš„å…³ç³»ä¿¡æ¯ä»¥åŠå¤šåª’ä½“æ•°æ®å¦‚RGBå›¾åƒç­‰ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡è®¡ç®—å‚æ•°å…³ç³»æä¾›æ¸…æ™°çš„å…³ç³»å®šä¹‰ã€‚</li>
<li>æ•°æ®é›†æ”¯æŒåœºæ™¯å›¾çš„ç”Ÿæˆæ¨¡å‹é¢„æµ‹å’Œäººä¸ç‰©ä½“çš„äº¤äº’é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1698ce06ecce618c6d287296e5a0b679.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a268c169fd2a711cfc5fbff6a39c59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ae585d7a6be36d76ff640c7ff0587f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c741edf840c3eeab011e6ff11ed26a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dab72a26d32eb48210068114792918e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d2838d63b7ef080aa908b60364aa658.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95371561a9f26cc12c4aa8f9071ce894.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Vision-Transformer-Based-Time-Series-Image-Reconstruction-for-Cloud-Filling-Applications"><a href="#Vision-Transformer-Based-Time-Series-Image-Reconstruction-for-Cloud-Filling-Applications" class="headerlink" title="Vision Transformer-Based Time-Series Image Reconstruction for   Cloud-Filling Applications"></a>Vision Transformer-Based Time-Series Image Reconstruction for   Cloud-Filling Applications</h2><p><strong>Authors:Lujun Li, Yiqun Wang, Radu State</strong></p>
<p>Cloud cover in multispectral imagery (MSI) poses significant challenges for early season crop mapping, as it leads to missing or corrupted spectral information. Synthetic aperture radar (SAR) data, which is not affected by cloud interference, offers a complementary solution, but lack sufficient spectral detail for precise crop mapping. To address this, we propose a novel framework, Time-series MSI Image Reconstruction using Vision Transformer (ViT), to reconstruct MSI data in cloud-covered regions by leveraging the temporal coherence of MSI and the complementary information from SAR from the attention mechanism. Comprehensive experiments, using rigorous reconstruction evaluation metrics, demonstrate that Time-series ViT framework significantly outperforms baselines that use non-time-series MSI and SAR or time-series MSI without SAR, effectively enhancing MSI image reconstruction in cloud-covered regions. </p>
<blockquote>
<p>åœ¨å¤šå…‰è°±å½±åƒï¼ˆMSIï¼‰ä¸­ï¼Œäº‘è¦†ç›–å¯¹æ—©æœŸå­£èŠ‚ä½œç‰©æ˜ å°„å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´å…‰è°±ä¿¡æ¯ä¸¢å¤±æˆ–æŸåã€‚åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰æ•°æ®ä¸å—äº‘å±‚å¹²æ‰°çš„å½±å“ï¼Œæä¾›äº†ä¸€ä¸ªè¡¥å……è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨ç²¾ç¡®ä½œç‰©æ˜ å°„æ–¹é¢ç¼ºä¹è¶³å¤Ÿçš„å…‰è°±ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå³åˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œæ—¶é—´åºåˆ—MSIå›¾åƒé‡å»ºï¼Œé€šè¿‡åˆ©ç”¨MSIçš„æ—¶é—´è¿è´¯æ€§å’Œæ¥è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„SARçš„è¡¥å……ä¿¡æ¯ï¼Œé‡å»ºäº‘å±‚è¦†ç›–åŒºåŸŸä¸­çš„MSIæ•°æ®ã€‚ä½¿ç”¨ä¸¥æ ¼çš„é‡å»ºè¯„ä¼°æŒ‡æ ‡çš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œæ—¶é—´åºåˆ—ViTæ¡†æ¶æ˜¾è‘—ä¼˜äºä½¿ç”¨éæ—¶é—´åºåˆ—MSIå’ŒSARæˆ–ä»…ä½¿ç”¨æ—¶é—´åºåˆ—MSIçš„åŸºçº¿æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æé«˜äº†äº‘å±‚è¦†ç›–åŒºåŸŸä¸­MSIå›¾åƒçš„é‡å»ºæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19591v1">PDF</a> This paper has been accepted as a conference paper at the 2025 IEEE   International Geoscience and Remote Sensing Symposium (IGARSS)</p>
<p><strong>Summary</strong></p>
<p>å¤šå…‰è°±å½±åƒä¸­çš„äº‘å±‚è¦†ç›–å¯¹æ—©æœŸå†œä½œç‰©åˆ¶å›¾å¸¦æ¥æŒ‘æˆ˜ï¼Œç¼ºå¤±æˆ–æŸåçš„è°±ä¿¡æ¯å½±å“ç²¾å‡†åº¦ã€‚åˆæˆå­”å¾„é›·è¾¾æ•°æ®ä¸å—äº‘å±‚å¹²æ‰°ï¼Œæä¾›äº’è¡¥è§£å†³æ–¹æ¡ˆï¼Œä½†ç¼ºä¹è¶³å¤Ÿçš„è°±ç»†èŠ‚ç”¨äºç²¾ç¡®å†œä½œç‰©åˆ¶å›¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹æ¡†æ¶â€”â€”åŸºäºè§†è§‰è½¬æ¢å™¨çš„æ—¶åºå¤šå…‰è°±å½±åƒé‡å»ºï¼ˆTime-series MSI Image Reconstruction using Vision Transformerï¼Œç®€ç§°ViTï¼‰ï¼Œåˆ©ç”¨å¤šå…‰è°±å½±åƒçš„æ—¶åºè¿è´¯æ€§å’Œåˆæˆå­”å¾„é›·è¾¾æ•°æ®çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œé‡å»ºäº‘å±‚è¦†ç›–åŒºåŸŸçš„å¤šå…‰è°±å½±åƒæ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œæ—¶åºViTæ¡†æ¶æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨éæ—¶åºå¤šå…‰è°±å½±åƒå’ŒSARæˆ–ä»…ä½¿ç”¨æ—¶åºå¤šå…‰è°±å½±åƒçš„åŸºçº¿æ–¹æ³•ï¼Œæœ‰æ•ˆæé«˜äº‘å±‚è¦†ç›–åŒºåŸŸçš„å¤šå…‰è°±å½±åƒé‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‘å±‚è¦†ç›–åœ¨å¤šå…‰è°±å½±åƒä¸­ä¼šå¯¹æ—©æœŸå†œä½œç‰©åˆ¶å›¾å¸¦æ¥æŒ‘æˆ˜ï¼Œå¯¼è‡´è°±ä¿¡æ¯ç¼ºå¤±æˆ–æŸåã€‚</li>
<li>åˆæˆå­”å¾„é›·è¾¾æ•°æ®ä¸å—äº‘å±‚å¹²æ‰°ï¼Œä½†ç¼ºä¹è¶³å¤Ÿçš„è°±ç»†èŠ‚ï¼Œæ— æ³•ç²¾å‡†è¿›è¡Œå†œä½œç‰©åˆ¶å›¾ã€‚</li>
<li>æå‡ºäº†åŸºäºè§†è§‰è½¬æ¢å™¨çš„æ—¶åºå¤šå…‰è°±å½±åƒé‡å»ºæ¡†æ¶ï¼ˆTime-series MSI Image Reconstruction using Vision Transformerï¼Œç®€ç§°ViTï¼‰ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å¤šå…‰è°±å½±åƒçš„æ—¶åºè¿è´¯æ€§å’Œåˆæˆå­”å¾„é›·è¾¾æ•°æ®çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>æ—¶åºViTæ¡†æ¶åœ¨é‡å»ºäº‘å±‚è¦†ç›–åŒºåŸŸçš„å¤šå…‰è°±å½±åƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>ä¸¥è°¨çš„å®éªŒå’Œé‡å»ºè¯„ä¼°æŒ‡æ ‡è¯æ˜ï¼Œæ—¶åºViTæ¡†æ¶ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44cad3508c5627fa3446cf10e2002e9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4052aa217f51a192a503bf8f454e8b2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d17b126d8e7d05d25197c9f247446d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-498684391d2f715fe3f3e2ce47421099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b0cdc7f2a1053933ec1bdbeeeac8fb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learning-from-Anatomy-Supervised-Anatomical-Pretraining-SAP-for-Improved-Metastatic-Bone-Disease-Segmentation-in-Whole-Body-MRI"><a href="#Learning-from-Anatomy-Supervised-Anatomical-Pretraining-SAP-for-Improved-Metastatic-Bone-Disease-Segmentation-in-Whole-Body-MRI" class="headerlink" title="Learning from Anatomy: Supervised Anatomical Pretraining (SAP) for   Improved Metastatic Bone Disease Segmentation in Whole-Body MRI"></a>Learning from Anatomy: Supervised Anatomical Pretraining (SAP) for   Improved Metastatic Bone Disease Segmentation in Whole-Body MRI</h2><p><strong>Authors:Joris Wuts, Jakub Ceranka, Nicolas Michoux, FrÃ©dÃ©ric Lecouvet, Jef Vandemeulebroucke</strong></p>
<p>The segmentation of metastatic bone disease (MBD) in whole-body MRI (WB-MRI) is a challenging problem. Due to varying appearances and anatomical locations of lesions, ambiguous boundaries, and severe class imbalance, obtaining reliable segmentations requires large, well-annotated datasets capturing lesion variability. Generating such datasets requires substantial time and expertise, and is prone to error. While self-supervised learning (SSL) can leverage large unlabeled datasets, learned generic representations often fail to capture the nuanced features needed for accurate lesion detection.   In this work, we propose a Supervised Anatomical Pretraining (SAP) method that learns from a limited dataset of anatomical labels. First, an MRI-based skeletal segmentation model is developed and trained on WB-MRI scans from healthy individuals for high-quality skeletal delineation. Then, we compare its downstream efficacy in segmenting MBD on a cohort of 44 patients with metastatic prostate cancer, against both a baseline random initialization and a state-of-the-art SSL method.   SAP significantly outperforms both the baseline and SSL-pretrained models, achieving a normalized surface Dice of 0.76 and a Dice coefficient of 0.64. The method achieved a lesion detection F2 score of 0.44, improving on 0.24 (baseline) and 0.31 (SSL). When considering only clinically relevant lesions larger than 1~ml, SAP achieves a detection sensitivity of 100% in 28 out of 32 patients.   Learning bone morphology from anatomy yields an effective and domain-relevant inductive bias that can be leveraged for the downstream segmentation task of bone lesions. All code and models are made publicly available. </p>
<blockquote>
<p>å…¨èº«MRIï¼ˆWB-MRIï¼‰ä¸­è½¬ç§»æ€§éª¨ç—…ï¼ˆMBDï¼‰çš„åˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç”±äºç—…å˜çš„å¤–è§‚å’Œè§£å‰–ä½ç½®å„å¼‚ï¼Œè¾¹ç•Œæ¨¡ç³Šä»¥åŠç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡ï¼Œè¦è·å¾—å¯é çš„åˆ†å‰²ç»“æœï¼Œéœ€è¦æ•æ‰ç—…å˜å˜å¼‚çš„å¤§è€Œæ ‡æ³¨è‰¯å¥½çš„æ•°æ®é›†ã€‚ç”Ÿæˆè¿™æ ·çš„æ•°æ®é›†éœ€è¦å¤§é‡çš„æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚è™½ç„¶è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å¯ä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ•°æ®é›†ï¼Œä½†æ‰€å­¦çš„é€šç”¨è¡¨ç¤ºé€šå¸¸éš¾ä»¥æ•æ‰å‡†ç¡®çš„ç—…å˜æ£€æµ‹æ‰€éœ€çš„ç»†å¾®ç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºç›‘ç£è§£å‰–é¢„è®­ç»ƒï¼ˆSAPï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»æœ‰é™çš„è§£å‰–æ ‡ç­¾æ•°æ®é›†ä¸­å­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºMRIçš„éª¨éª¼åˆ†å‰²æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªå¥åº·ä¸ªä½“çš„WB-MRIæ‰«æè¿›è¡Œè®­ç»ƒï¼Œä»¥è¿›è¡Œé«˜è´¨é‡çš„éª¨éª¼æç»˜ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨ä¸€ç»„æ‚£æœ‰è½¬ç§»æ€§å‰åˆ—è…ºç™Œçš„44åæ‚£è€…ä¸­ï¼Œå°†å…¶ä¸‹æ¸¸å¯¹MBDåˆ†å‰²çš„æœ‰æ•ˆæ€§ä¸åŸºçº¿éšæœºåˆå§‹åŒ–å’Œæœ€å…ˆè¿›çš„SSLæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚SAPæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹å’ŒSSLé¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ç°äº†å½’ä¸€åŒ–è¡¨é¢Diceç³»æ•°ä¸º0.76å’ŒDiceç³»æ•°ä¸º0.64ã€‚è¯¥æ–¹æ³•åœ¨ç—…å˜æ£€æµ‹æ–¹é¢çš„F2åˆ†æ•°ä¸º0.44ï¼Œä¼˜äºåŸºçº¿ï¼ˆ0.24ï¼‰å’ŒSSLï¼ˆ0.31ï¼‰ã€‚åœ¨è€ƒè™‘ä½“ç§¯å¤§äº1æ¯«å‡çš„ä¸´åºŠç›¸å…³ç—…å˜æ—¶ï¼ŒSAPåœ¨32åæ‚£è€…ä¸­çš„28åè¾¾åˆ°äº†100%çš„æ£€æµ‹çµæ•åº¦ã€‚ä»è§£å‰–å­¦ä¸­å­¦ä¹ éª¨éª¼å½¢æ€ä¸ºä¸‹æ¸¸éª¨ç—…å˜åˆ†å‰²ä»»åŠ¡æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆä¸”ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†åè§ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹éƒ½å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19590v1">PDF</a> This preprint is currently under review at <em>Computers in Biology and   Medicine</em> (Elsevier). This version has not been peer-reviewed</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…¨èº«MRIï¼ˆWB-MRIï¼‰ä¸­è½¬ç§»æ€§éª¨ç—…ï¼ˆMBDï¼‰çš„åˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç”±äºç—…å˜çš„å¤–è§‚å’Œä½ç½®ã€è¾¹ç•Œæ¨¡ç³Šä»¥åŠä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ç­‰é—®é¢˜ï¼Œè·å¾—å¯é çš„åˆ†å‰²ç»“æœéœ€è¦å¤§è§„æ¨¡ã€æ ‡æ³¨è‰¯å¥½çš„æ•°æ®é›†æ¥æ•æ‰ç—…å˜çš„å˜å¼‚æ€§ã€‚ç”Ÿæˆæ­¤ç±»æ•°æ®é›†éœ€è¦å¤§é‡æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚è™½ç„¶è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å¯ä»¥åˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ•°æ®é›†ï¼Œä½†å­¦åˆ°çš„é€šç”¨è¡¨ç¤ºé€šå¸¸éš¾ä»¥æ•æ‰å‡†ç¡®çš„ç—…å˜æ£€æµ‹æ‰€éœ€çš„ç»†å¾®ç‰¹å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSupervised Anatomical Pretrainingï¼ˆSAPï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»æœ‰é™çš„è§£å‰–æ ‡ç­¾æ•°æ®é›†ä¸­å­¦ä¹ ã€‚é¦–å…ˆï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºMRIçš„éª¨éª¼åˆ†å‰²æ¨¡å‹ï¼Œåœ¨æ¥è‡ªå¥åº·ä¸ªä½“çš„WB-MRIæ‰«æä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥è¿›è¡Œé«˜è´¨é‡çš„éª¨éª¼æç»˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å…¶åœ¨æ‚£æœ‰è½¬ç§»æ€§å‰åˆ—è…ºç™Œçš„44ä¾‹æ‚£è€…é˜Ÿåˆ—ä¸­å¯¹MBDåˆ†å‰²çš„ä¸‹æ¸¸åŠŸæ•ˆä¸åŸºçº¿éšæœºåˆå§‹åŒ–å’Œæœ€å…ˆè¿›çš„SSLæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚SAPæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹å’ŒSSLé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¾¾åˆ°è¡¨é¢Diceå½’ä¸€åŒ–ç³»æ•°ä¸º0.76å’ŒDiceç³»æ•°ä¸º0.64ã€‚è¯¥æ–¹æ³•åœ¨ç—…å˜æ£€æµ‹F2åˆ†æ•°æ–¹é¢è¾¾åˆ°0.44ï¼Œä¼˜äºåŸºçº¿ï¼ˆ0.24ï¼‰å’ŒSSLï¼ˆ0.31ï¼‰ã€‚å½“ä»…è€ƒè™‘å¤§äº1æ¯«å‡çš„ä¸´åºŠç›¸å…³ç—…å˜æ—¶ï¼ŒSAPåœ¨28åæ‚£è€…ä¸­çš„æ£€æµ‹çµæ•åº¦è¾¾åˆ°ç™¾åˆ†ä¹‹ç™¾ã€‚ä»éª¨éª¼å½¢æ€ä¸­å­¦ä¹ è§£å‰–ç»“æ„ä¸ºä¸‹æ¸¸çš„éª¨éª¼ç—…å˜åˆ†å‰²ä»»åŠ¡æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„ã€é¢†åŸŸç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å‡å…¬å¼€å‘å¸ƒã€‚ </p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ol>
<li>MBDåœ¨WB-MRIä¸­çš„åˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œéœ€è¦å¤§è§„æ¨¡ã€æ ‡æ³¨è‰¯å¥½çš„æ•°æ®é›†æ¥å‡†ç¡®åˆ†å‰²ç—…å˜ã€‚</li>
<li>SAPæ–¹æ³•é€šè¿‡åˆ©ç”¨æœ‰é™çš„è§£å‰–æ ‡ç­¾æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥æé«˜MBDåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>SAPæ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿å’ŒSSLé¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨Diceç³»æ•°å’ŒF2åˆ†æ•°æ–¹é¢å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</li>
<li>SAPæ–¹æ³•å¯¹ä¸´åºŠç›¸å…³ç—…å˜çš„æ£€æµ‹å…·æœ‰è¾ƒé«˜çš„çµæ•åº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>SAPæ–¹æ³•å…¬å¼€æ‰€æœ‰ä»£ç å’Œæ¨¡å‹ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>å­¦ä¹ éª¨éª¼å½¢æ€å’Œåˆ©ç”¨è§£å‰–ç»“æ„ä¿¡æ¯æœ‰åŠ©äºæé«˜MBDåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-191a4eeafc373b31e1d583aa7911101f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04994e3bc24619ae1dca887055c40f85.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Euclid-Quick-Data-Release-Q1-â€“-Watching-ICM-selected-galaxy-clusters-with-Euclid-eyes-â€“-prospects-of-Euclid-data-in-the-context-of-large-SZ-and-X-ray-based-surveys"><a href="#Euclid-Quick-Data-Release-Q1-â€“-Watching-ICM-selected-galaxy-clusters-with-Euclid-eyes-â€“-prospects-of-Euclid-data-in-the-context-of-large-SZ-and-X-ray-based-surveys" class="headerlink" title="Euclid: Quick Data Release (Q1) â€“ Watching ICM-selected galaxy clusters   with Euclid eyes â€“ prospects of Euclid data in the context of large SZ and   X-ray based surveys"></a>Euclid: Quick Data Release (Q1) â€“ Watching ICM-selected galaxy clusters   with Euclid eyes â€“ prospects of Euclid data in the context of large SZ and   X-ray based surveys</h2><p><strong>Authors:M. Klein, K. George, J. J. Mohr, B. Altieri, L. Amendola, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, A. Balestra, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. CaÃ±as-Herrera, V. Capobianco, C. Carbone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, G. De Lucia, C. Dolding, H. Dole, F. Dubath, F. Ducret, X. Dupac, S. Dusini, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, I. M. Hook, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, E. KeihÃ¤nen, S. Kermiche, B. Kubik, M. KÃ¼mmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, D. Le Mignant, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. Massey, S. Maurogordato, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, L. Moscardini, R. Nakajima, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, C. Rosset, H. J. A. Rottgering, R. Saglia, Z. Sakr, D. Sapone, M. Schirmer, P. Schneider, T. Schrabback, A. Secroun, E. Sefusatti, G. Seidel, S. Serrano, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-CrespÃ­, A. N. Taylor, I. Tereno, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, F. M. Zerbi, E. Zucca, C. Burigana, V. Scottez, M. Sereno, M. Viel</strong></p>
<p>Galaxy clusters detected through their X-ray emission or Sunyaevâ€“Zeldovich effect (SZE), both produced by the intra-cluster medium (ICM), are key probes in cosmological and astrophysical studies. To maximise the scientific return of such surveys, complementary data are required for cluster confirmation and redshift estimation. This is typically provided by wide-field optical and infrared surveys, which are increasingly challenged by ongoing and future ICM-selected samples. In particular, at high redshifts ($z&gt;1$) probed by upcoming SZE-selected samples, current large surveys may be insufficient for reliable confirmation. Deep, high-resolution infrared surveys like Euclid will thus be essential for confirming most high-redshift clusters. We present an analysis of the first sizeable Euclid dataset (Q1), overlapping with several ICM-selected cluster samples. We apply an adaptation of the MCMF cluster confirmation tool to estimate key properties, including redshift and richness, and to predict Euclidâ€™s capabilities for high-redshift cluster confirmation. We find promising performance, particularly at high redshifts, while richness estimates at low redshifts ($z&lt;0.4$) are currently limited by Q1 data quality but should improve with future releases. Using MCMF runs on random lines of sight, we predict that Euclid will confirm clusters at $1&lt;z&lt;2$ as effectively as current optical surveys at $z&lt;0.6$, significantly enhancing high-redshift confirmation. SZE-selected samples will thus greatly benefit from Euclid overlap. Among five known high-$z$ SZE clusters in Q1, we identify the highest-redshift jellyfish galaxy candidate to date, EUCLJ035330.86$-$504347.6 in SPT-CLJ0353$-$5043 ($z&#x3D;1.32$), two massive star-forming galaxies near ACT-CLJ0350.0$-$4819 ($z&#x3D;1.46$), and strong lensing features in SPT-CLJ0353$-$5043 and SPT-CLJ0421$-$4845. </p>
<blockquote>
<p>é€šè¿‡Xå°„çº¿å‘å°„æˆ–Sunyaev-Zeldovichæ•ˆåº”ï¼ˆSZEï¼‰æ£€æµ‹åˆ°çš„æ˜Ÿç³»å›¢ï¼Œè¿™ä¸¤ç§æ•ˆåº”éƒ½æ˜¯ç”±æ˜Ÿç³»å›¢å†…ä»‹è´¨ï¼ˆICMï¼‰äº§ç”Ÿçš„ï¼Œæ˜¯å®‡å®™å­¦å’Œå¤©ä½“ç‰©ç†å­¦ç ”ç©¶ä¸­çš„å…³é”®æ¢é’ˆã€‚ä¸ºäº†æœ€å¤§åŒ–æ­¤ç±»è°ƒæŸ¥çš„ç§‘å­¦å›æŠ¥ï¼Œéœ€è¦è¡¥å……æ•°æ®è¿›è¡Œé›†ç¾¤ç¡®è®¤å’Œçº¢ç§»ä¼°è®¡ã€‚è¿™é€šå¸¸ç”±å®½è§†åœºå…‰å­¦å’Œçº¢å¤–è°ƒæŸ¥æä¾›ï¼Œä½†éšç€ICMé€‰æ‹©çš„æ ·æœ¬çš„å½“å‰å’Œæœªæ¥æŒ‘æˆ˜ï¼Œè¿™ä¸€ä»»åŠ¡è¶Šæ¥è¶Šå›°éš¾ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡å³å°†å‡ºç°çš„SZEé€‰å®šçš„é«˜çº¢ç§»ï¼ˆz&gt; 1ï¼‰æ ·æœ¬ä¸­ï¼Œå½“å‰çš„å¤§è§„æ¨¡è°ƒæŸ¥å¯èƒ½ä¸è¶³ä»¥è¿›è¡Œå¯é çš„ç¡®è®¤ã€‚å› æ­¤ï¼ŒåƒEuclidè¿™æ ·æ·±å…¥ã€é«˜åˆ†è¾¨ç‡çš„çº¢å¤–è°ƒæŸ¥å¯¹äºç¡®è®¤å¤§å¤šæ•°é«˜çº¢ç§»é›†ç¾¤å°†è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åˆ†æäº†é¦–æ‰¹è§„æ¨¡è¾ƒå¤§çš„Euclidæ•°æ®é›†ï¼ˆQ1ï¼‰ï¼Œå®ƒä¸å¤šä¸ªICMé€‰å®šçš„é›†ç¾¤æ ·æœ¬é‡å ã€‚æˆ‘ä»¬åº”ç”¨äº†MCMFé›†ç¾¤ç¡®è®¤å·¥å…·æ¥ä¼°è®¡å…³é”®å±æ€§ï¼ŒåŒ…æ‹¬çº¢ç§»å’Œä¸°å¯Œåº¦ï¼Œå¹¶é¢„æµ‹Euclidåœ¨é«˜çº¢ç§»é›†ç¾¤ç¡®è®¤æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°è¡¨ç°æœ‰å¸Œæœ›çš„è¿¹è±¡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº¢ç§»æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œè€Œä½çº¢ç§»ï¼ˆz&lt; 0.4ï¼‰çš„ä¸°å¯Œåº¦ä¼°è®¡ç›®å‰å—åˆ°Q1æ•°æ®è´¨é‡çš„é™åˆ¶ï¼Œä½†éšç€æœªæ¥çš„å‘å¸ƒå°†ä¼šæ”¹å–„ã€‚é€šè¿‡åœ¨éšæœºè§†çº¿æ–¹å‘ä¸Šè¿è¡ŒMCMFï¼Œæˆ‘ä»¬é¢„è®¡Euclidåœ¨ç¡®è®¤é«˜çº¢ç§»èŒƒå›´ï¼ˆå¦‚ç¡®è®¤çš„é›†ç¾¤å°†åƒå½“å‰å…‰å­¦è°ƒæŸ¥åœ¨z &lt; 0.6æ—¶ä¸€æ ·æœ‰æ•ˆï¼‰çš„æ•ˆèƒ½å¤§å¤§æå‡ï¼‰ï¼Œè¿™å°†å¯¹é€šè¿‡SZEé€‰æ‹©çš„æ ·æœ¬å¤§æœ‰è£¨ç›Šã€‚åœ¨Q1ä¸­çš„äº”ä¸ªå·²çŸ¥é«˜çº¢ç§»SZEé›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†è¿„ä»Šä¸ºæ­¢æœ€é«˜çº¢ç§»çš„æœå†»é±¼å€™é€‰è€…EUCLJ035330.86-504347.6ï¼ˆä½äºSPT-CLJ0353-5043ä¸­ï¼‰ï¼ˆz&#x3D; 1.32ï¼‰ï¼ŒACT-CLJ0350.0-4819é™„è¿‘æœ‰ä¸¤ä¸ªè´¨é‡å·¨å¤§çš„æ’æ˜Ÿå½¢æˆæ˜Ÿç³»ï¼ˆz&#x3D; 1.46ï¼‰ï¼Œä»¥åŠåœ¨SPT-CLJ0353-5043å’ŒSPT-CLJ0421-4845ä¸­å‡ºç°çš„å¼ºçƒˆé€é•œç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19566v1">PDF</a> 11 pages, 10 figures</p>
<p><strong>Summary</strong><br>    æ¬§å‡ é‡Œå¾—æ•°æ®é¦–æ¬¡å¤§è§„æ¨¡åˆ†ææ­ç¤ºå…¶åœ¨é«˜çº¢ç§»æ˜Ÿç³»å›¢ç¡®è®¤ä¸­çš„æ½œåŠ›ï¼Œä¸å¤ªé˜³äºš-æ³½å°”å¤šç»´å¥‡æ•ˆåº”é€‰å®šçš„æ˜Ÿç³»å›¢æ ·æœ¬é‡å åŒºåŸŸè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ˜Ÿç³»å›¢é€šè¿‡Xå°„çº¿å‘å°„å’ŒSunyaev-Zeldovichæ•ˆåº”æ£€æµ‹æ˜¯å®‡å®™å­¦å’Œå¤©ä½“ç‰©ç†å­¦ç ”ç©¶çš„å…³é”®æ¢é’ˆã€‚</li>
<li>ä¸ºæœ€å¤§åŒ–æ­¤ç±»è°ƒæŸ¥çš„ç§‘ç ”å›æŠ¥ï¼Œéœ€è¦å®½è§†åœºå…‰å­¦å’Œçº¢å¤–è°ƒæŸ¥æ•°æ®æ¥è¿›è¡Œé›†ç¾¤ç¡®è®¤å’Œçº¢ç§»ä¼°è®¡ã€‚</li>
<li>å¯¹äºå³å°†å‡ºç°çš„SZEé€‰å®šçš„æ ·æœ¬æ‰€æ¢æµ‹çš„é«˜çº¢ç§»ï¼ˆ$z&gt;1$ï¼‰ï¼Œå½“å‰çš„å¤§å‹è°ƒæŸ¥å¯èƒ½ä¸è¶³ä»¥è¿›è¡Œå¯é çš„ç¡®è®¤ã€‚</li>
<li>æ·±éƒ¨ã€é«˜åˆ†è¾¨ç‡çš„çº¢å¤–è°ƒæŸ¥å¦‚æ¬§å‡ é‡Œå¾—å¯¹äºç¡®è®¤é«˜çº¢ç§»é›†ç¾¤è‡³å…³é‡è¦ã€‚</li>
<li>å¯¹é¦–æ‰¹å¤§è§„æ¨¡æ¬§å‡ é‡Œå¾—æ•°æ®é›†ï¼ˆQ1ï¼‰çš„åˆ†æä¸å¤šä¸ªICMé€‰å®šé›†ç¾¤æ ·æœ¬é‡å ï¼Œåº”ç”¨MCMFé›†ç¾¤ç¡®è®¤å·¥å…·ä¼°è®¡å…³é”®å±æ€§ï¼ŒåŒ…æ‹¬çº¢ç§»å’Œä¸°å¯Œåº¦ã€‚</li>
<li>åœ¨é«˜çº¢ç§»ä¸‹çš„æ€§èƒ½è¡¨ç°æœ‰å‰æ™¯ï¼Œå°¤å…¶æ˜¯åœ¨çº¢ç§»1è‡³2çš„èŒƒå›´å†…ï¼Œæ¬§å‡ é‡Œå¾—çš„ç¡®è®¤èƒ½åŠ›ä¸å½“å‰å…‰å­¦è°ƒæŸ¥åœ¨çº¢ç§»å°äº0.6æ—¶çš„æ•ˆæœç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d20ff1bffcd6dd076907bc88723fb995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91d9751cc628883d37686d8a689f8845.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad4037ff005b2a5aee43cce0b85828eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f7975d0c676fe59fb17eb9535e8eb54.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-deep-X-ray-UV-look-into-the-reflaring-stage-of-the-accreting-millisecond-pulsar-SAX-J1808-4-3658"><a href="#A-deep-X-ray-UV-look-into-the-reflaring-stage-of-the-accreting-millisecond-pulsar-SAX-J1808-4-3658" class="headerlink" title="A deep X-ray&#x2F;UV look into the reflaring stage of the accreting   millisecond pulsar SAX J1808.4-3658"></a>A deep X-ray&#x2F;UV look into the reflaring stage of the accreting   millisecond pulsar SAX J1808.4-3658</h2><p><strong>Authors:Caterina Ballocco, Alessandro Papitto, Arianna Miraval Zanon, Giulia Illiano, Tiziana Di Salvo, Filippo Ambrosino, Luciano Burderi, Sergio Campana, Francesco Coti Zelati, Alessandro Di Marco, Christian Malacaria, Maura Pilia, Juri Poutanen, Tuomo Salmi, Andrea Sanna</strong></p>
<p>We present a detailed X-ray&#x2F;UV high-time resolution monitoring of the final reflaring phase of the accreting millisecond pulsar SAX<del>J1808.4$-$3658. During its 2022 outburst, we obtained simultaneous XMM-Newton and Hubble Space Telescope (HST) observations. We detected coherent X-ray pulsations down to a 0.5-10</del>keV luminosity of $L_{X(low),0.5-10} \simeq 6.21^{+0.20}<em>{-0.15} \times 10^{34} , d^2</em>{3.5},\mathrm{erg , s^{-1}}$, among the lowest ever observed in this source. The uninterrupted coverage provided by XMM-Newton enabled a detailed characterisation of the spectral and temporal evolution of the source X-ray emission as the flux varied by approximately one order of magnitude. At the lowest flux levels, we observed significant variations in pulse amplitude and phase correlated with the X-ray source flux. We found a sharp phase jump of $\sim 0.4$ cycles, accompanied by a doubling of the pulse amplitude and a softening of the X-ray emission. We interpret the changes in the X-ray pulse profiles as drifts of emission regions on the neutron star surface due to an increase of the inner disk radius occurring when the mass accretion rate decreases. The phase evolution was consistent with a magnetospheric radius scaling as $R_{m} \propto \dot{M}^{\Lambda}$, with $\Lambda &#x3D; -0.17(9)$, in broad agreement with theoretical predictions. Simultaneous HST observations confirmed the presence of significant UV pulsations. The measured pulsed luminosity $-$ $L_{pulsed}^{UV} &#x3D; (9 \pm 2) \times 10^{31} , \text{erg} , \text{s}^{-1}$ $-$ was approximately half that observed during the 2019 outburst, but the pulsed X-ray to UV luminosity ratio simultaneously measured remained consistent. Yet, such a UV luminosity exceeds the predictions of standard emission models, as further confirmed by the shape of the pulsed spectral energy distribution. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹æ¯«ç§’çº§è„‰å†²æ˜ŸSAX J1808.4-3658çš„æœ€ç»ˆå†é—ªé˜¶æ®µçš„Xå°„çº¿&#x2F;ç´«å¤–é«˜æ—¶é—´åˆ†è¾¨ç‡è¿›è¡Œäº†è¯¦ç»†ç›‘æµ‹ã€‚åœ¨å…¶2022å¹´çš„çˆ†å‘æœŸé—´ï¼Œæˆ‘ä»¬è·å¾—äº†åŒæ—¶ç”±XMM-ç‰›é¡¿å’Œå“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œï¼ˆHSTï¼‰è¿›è¡Œçš„è§‚æµ‹ã€‚æˆ‘ä»¬æ£€æµ‹åˆ°ç›¸å¹²Xå°„çº¿è„‰åŠ¨ï¼Œå…¶æœ€ä½èƒ½é‡ä¸ºLx(low)â€‹0.5âˆ’â€‹ ä¸‹çš„å‘å…‰åº¦ çº¦ä¸º 6.21Â±â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹Ã—â€‹â€‹ dâ€‹â€‹erg sâ€‹â€‹erg sâ€‹â€‹erg sâ€‹â€‹ï¼Œè¿™æ˜¯åœ¨æºä¸­è§‚å¯Ÿåˆ°çš„æœ€ä½å€¼ä¹‹ä¸€ã€‚XMM-ç‰›é¡¿æä¾›çš„è¿ç»­è¦†ç›–ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯¦ç»†è¡¨å¾æºXå°„çº¿å‘å°„çš„å…‰è°±å’Œæ—¶é—´æ¼”åŒ–ï¼Œåœ¨æµé‡å˜åŒ–çº¦ä¸€ä¸ªæ•°é‡çº§çš„æƒ…å†µä¸‹å°¤ä¸ºæ˜æ˜¾ã€‚åœ¨æœ€ä½æµé‡æ°´å¹³ä¸Šï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è„‰å†²å¹…åº¦å’Œç›¸ä½ä¸Xå°„çº¿æºæµé‡ç›¸å…³æ˜¾è‘—å˜åŒ–ã€‚æˆ‘ä»¬å‘ç°è„‰å†²ç›¸ä½å‘ç”Ÿäº†çº¦0.4ä¸ªå‘¨æœŸçš„è·³è·ƒï¼Œä¼´éšç€è„‰å†²å¹…åº¦åŠ å€ä»¥åŠXå°„çº¿å‘å°„çš„è½¯åŒ–ã€‚æˆ‘ä»¬å°†Xå°„çº¿è„‰å†²è½®å»“çš„å˜åŒ–è§£é‡Šä¸ºä¸­å­æ˜Ÿè¡¨é¢å‘å°„åŒºåŸŸæ¼‚ç§»çš„ç»“æœï¼Œè¿™æ˜¯ç”±äºå½“è´¨é‡å¸ç§¯ç‡ä¸‹é™æ—¶å†…éƒ¨ç›˜åŠå¾„çš„å¢åŠ æ‰€å¯¼è‡´çš„ã€‚ç›¸ä½æ¼”åŒ–ä¸ç£å±‚åŠå¾„éšè´¨é‡å¸ç§¯ç‡çš„å˜åŒ–è€Œå˜åŒ–çš„å°ºåº¦ä¸€è‡´ï¼Œå³RmâˆMdÌ‡mçš„ç‚¹åº¦ Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î»Î» Î»ï¼Œå…¶ä¸­Î›&#x3D;âˆ’â€‹â€‹ï¼ˆåæ˜ äº†å½“è´¨é‡å¸ç§¯ç‡é™ä½æ—¶ï¼Œå†…ç›˜åŠå¾„çš„å¢åŠ ï¼‰ã€‚è¿™ä¸ç†è®ºé¢„æµ‹å¤§è‡´ç›¸ç¬¦ã€‚åŒæ—¶è¿›è¡Œçš„HSTè§‚æµ‹è¯å®äº†æ˜¾è‘—çš„ç´«å¤–è„‰åŠ¨å­˜åœ¨ã€‚æµ‹å¾—çš„è„‰å†²å‘å…‰åº¦çº¦ä¸ºLpulsedUV&#x3D;ï¼ˆè§‚å¯Ÿåˆ°çš„å³°å€¼ï¼‰ã€‚ç´«å¤–å…‰åº¦çš„æ•°å€¼è¶…å‡ºäº†æ ‡å‡†å‘å°„æ¨¡å‹çš„é¢„æµ‹ï¼Œè¿™ä¸è„‰å†²è°±èƒ½é‡åˆ†å¸ƒçš„å½¢æ€ç›¸ç¬¦è¿›ä¸€æ­¥ç¡®è®¤äº†è¿™ä¸€ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19556v1">PDF</a> 12 pages, 13 figures, submitted to A&amp;A</p>
<p><strong>æ‘˜è¦</strong><br>     å¯¹SAX J1808.4-3658æ¯«ç§’è„‰å†²æ˜Ÿæœ€ç»ˆå†æ´»è·ƒé˜¶æ®µçš„Xå°„çº¿å’Œç´«å¤–å…‰é«˜æ—¶é—´åˆ†è¾¨ç‡ç›‘æµ‹è¿›è¡Œè¯¦ç»†å‘ˆç°ã€‚åœ¨2022å¹´çˆ†å‘æœŸé—´ï¼Œæˆ‘ä»¬è·å¾—äº†XMM-ç‰›é¡¿å’Œå“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œçš„åŒæ—¶è§‚æµ‹æ•°æ®ã€‚æˆ‘ä»¬åœ¨ä½è‡³Lx(low) 0.5-10 keVçš„äº®åº¦ä¸‹æ£€æµ‹åˆ°Xå°„çº¿è„‰å†²ï¼Œè¿™æ˜¯è¯¥æºä¸­è§‚å¯Ÿåˆ°çš„æœ€ä½äº®åº¦ä¹‹ä¸€ã€‚XMM-ç‰›é¡¿çš„æ— é—´æ–­è¦†ç›–ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè¯¦ç»†è¡¨å¾æºXå°„çº¿å‘å°„çš„è°±å’Œæ—¶é—´æ¼”åŒ–ï¼Œéšç€æµé‡å˜åŒ–çº¦ä¸€ä¸ªæ•°é‡çº§ã€‚åœ¨æœ€ä½æµé‡æ°´å¹³ä¸‹ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è„‰å†²æŒ¯å¹…å’Œç›¸ä½ä¸Xå°„çº¿æºæµé‡çš„æ˜¾è‘—ç›¸å…³æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°çº¦0.4ä¸ªå‘¨æœŸçš„çªç„¶ç›¸ä½è·³è·ƒï¼Œä¼´éšç€è„‰å†²æŒ¯å¹…åŠ å€å’ŒXå°„çº¿å‘å°„çš„è½¯åŒ–ã€‚æˆ‘ä»¬è®¤ä¸ºXå°„çº¿è„‰å†²å‰–é¢çš„å˜åŒ–æ˜¯ç”±äºä¸­å­æ˜Ÿè¡¨é¢å‘å°„åŒºåŸŸæ¼‚ç§»å¼•èµ·çš„ï¼Œè¿™æ˜¯ç”±äºè´¨é‡å¸ç§¯ç‡é™ä½æ—¶å†…ç›˜åŠå¾„çš„å¢åŠ æ‰€å¯¼è‡´çš„ã€‚ç›¸ä½æ¼”åŒ–ä¸ç£å±‚åŠå¾„ä¸å¸ç§¯ç‡æˆæ¯”ä¾‹R_m \propto \dot{M}^{\Lambda}ï¼Œå…¶ä¸­Î›&#x3D;-0.17(9)ï¼Œä¸ç†è®ºé¢„æµ‹å¤§è‡´ç›¸ç¬¦ã€‚åŒæ—¶çš„è½©å°¼è¯—å¤©æ–‡å°è§‚æµ‹è¯å®äº†æ˜¾è‘—çš„ç´«å¤–è„‰å†²çš„å­˜åœ¨ã€‚æµ‹é‡çš„è„‰å†²å…‰åº¦çº¦ä¸ºæ ‡å‡†å‘å°„æ¨¡å‹é¢„æµ‹å€¼çš„ä¸¤å€ã€‚åŒæ—¶æµ‹é‡çš„Xå°„çº¿å’Œç´«å¤–å…‰è„‰å†²äº®åº¦æ¯”å€¼ä¿æŒä¸€è‡´æ€§ï¼Œä½†ç´«å¤–å…‰åº¦è¶…è¿‡äº†æ ‡å‡†æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œè¿™ä¹Ÿå¾—åˆ°äº†è„‰å†²å…‰è°±èƒ½é‡åˆ†å¸ƒçš„è¯å®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹SAX J1808.4-3658åœ¨2022å¹´çˆ†å‘è¿›è¡Œäº†è¯¦ç»†çš„Xå°„çº¿å’Œç´«å¤–çº¿é«˜æ—¶é—´åˆ†è¾¨ç‡ç›‘æµ‹ã€‚</li>
<li>åœ¨ä½äº®åº¦ä¸‹æ£€æµ‹åˆ°Xå°„çº¿è„‰å†²ï¼Œå…¶å…‰è°±å’Œæ—¶é—´çš„æ¼”åŒ–å¾—åˆ°äº†ç²¾ç»†è¡¨å¾ã€‚</li>
<li>è§‚å¯Ÿåˆ°äº†è„‰å†²æŒ¯å¹…å’Œç›¸ä½ä¸Xå°„çº¿æºæµé‡çš„ç›¸å…³æ€§ã€‚</li>
<li>ä¸­å­æ˜Ÿè¡¨é¢å‘å°„åŒºåŸŸçš„æ¼‚ç§»è¢«è§£é‡Šä¸ºXå°„çº¿è„‰å†²å‰–é¢çš„å˜åŒ–åŸå› ã€‚</li>
<li>ç£å±‚åŠå¾„ä¸å¸ç§¯ç‡ä¹‹é—´çš„å…³ç³»å¾—åˆ°éªŒè¯ï¼Œä¸ç†è®ºé¢„æµ‹ç›¸ç¬¦ã€‚</li>
<li>åŒæ—¶çš„ç´«å¤–çº¿è§‚æµ‹è¯å®äº†æ˜¾è‘—çš„ç´«å¤–è„‰å†²çš„å­˜åœ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f03785806f745a6a19159d1d308a210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ee5600a6f70bb0d1603262bbfb528a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9032b6551d4d2120d09f4cd5ae7f7c63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebdb2c63f233372689784bd96fe9f0d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83b92d2e4a0b404dda32a473a67c92d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91620d7f38302c2b966a73545698a714.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-491d6b0ba5c9637bd3a595cc2398073a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39f53811502c30f93ceda26970ad4659.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="General-Methods-Make-Great-Domain-specific-Foundation-Models-A-Case-study-on-Fetal-Ultrasound"><a href="#General-Methods-Make-Great-Domain-specific-Foundation-Models-A-Case-study-on-Fetal-Ultrasound" class="headerlink" title="General Methods Make Great Domain-specific Foundation Models: A   Case-study on Fetal Ultrasound"></a>General Methods Make Great Domain-specific Foundation Models: A   Case-study on Fetal Ultrasound</h2><p><strong>Authors:Jakob Ambsdorf, AsbjÃ¸rn Munk, Sebastian Llambias, Anders Nymark Christensen, Kamil Mikolaj, Randall Balestriero, Martin Tolsgaard, Aasa Feragen, Mads Nielsen</strong></p>
<p>With access to large-scale, unlabeled medical datasets, researchers are confronted with two questions: Should they attempt to pretrain a custom foundation model on this medical data, or use transfer-learning from an existing generalist model? And, if a custom model is pretrained, are novel methods required? In this paper we explore these questions by conducting a case-study, in which we train a foundation model on a large regional fetal ultrasound dataset of 2M images. By selecting the well-established DINOv2 method for pretraining, we achieve state-of-the-art results on three fetal ultrasound datasets, covering data from different countries, classification, segmentation, and few-shot tasks. We compare against a series of models pretrained on natural images, ultrasound images, and supervised baselines. Our results demonstrate two key insights: (i) Pretraining on custom data is worth it, even if smaller models are trained on less data, as scaling in natural image pretraining does not translate to ultrasound performance. (ii) Well-tuned methods from computer vision are making it feasible to train custom foundation models for a given medical domain, requiring no hyperparameter tuning and little methodological adaptation. Given these findings, we argue that a bias towards methodological innovation should be avoided when developing domain specific foundation models under common computational resource constraints. </p>
<blockquote>
<p>åœ¨è·å¾—å¤§è§„æ¨¡ã€æœªæ ‡æ³¨çš„åŒ»ç–—æ•°æ®é›†æ—¶ï¼Œç ”ç©¶è€…å°†é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼šä»–ä»¬æ˜¯å¦åº”è¯¥å°è¯•åœ¨æ­¤åŒ»ç–—æ•°æ®ä¸Šé¢„è®­ç»ƒä¸€ä¸ªè‡ªå®šä¹‰åŸºç¡€æ¨¡å‹ï¼Œæˆ–è€…ä»ç°æœ‰çš„é€šç”¨æ¨¡å‹ä¸­è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Ÿå¦‚æœé¢„è®­ç»ƒäº†è‡ªå®šä¹‰æ¨¡å‹ï¼Œæ˜¯å¦éœ€è¦æ–°çš„æ–¹æ³•ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¿›è¡Œä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶æ¥æ¢è®¨è¿™äº›é—®é¢˜ã€‚åœ¨è¯¥ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨åŒ…å«2ç™¾ä¸‡å¼ å›¾åƒçš„åŒºåŸŸæ€§èƒå„¿è¶…å£°æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸€ä¸ªåŸºç¡€æ¨¡å‹ã€‚é€šè¿‡é€‰æ‹©æˆç†Ÿçš„DINOv2é¢„è®­ç»ƒæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªèƒå„¿è¶…å£°æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œæ¶µç›–æ¥è‡ªä¸åŒå›½å®¶çš„æ•°æ®ã€åˆ†ç±»ã€åˆ†å‰²å’Œå°‘é‡ä»»åŠ¡ã€‚æˆ‘ä»¬å°†é¢„è®­ç»ƒäºè‡ªç„¶å›¾åƒã€è¶…å£°å›¾åƒå’Œç»è¿‡ç›‘ç£çš„åŸºå‡†æ¨¡å‹çš„ä¸€ç³»åˆ—æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†ä¸¤ä¸ªå…³é”®è§‚ç‚¹ï¼šï¼ˆiï¼‰å³ä½¿åœ¨å°æ¨¡å‹ä¸Šä½¿ç”¨è¾ƒå°‘æ•°æ®è¿›è¡Œè‡ªå®šä¹‰æ•°æ®é¢„è®­ç»ƒä¹Ÿæ˜¯å€¼å¾—çš„ï¼Œå› ä¸ºè‡ªç„¶å›¾åƒé¢„è®­ç»ƒçš„æ‰©å±•å¹¶ä¸èƒ½è½¬åŒ–ä¸ºè¶…å£°æ€§èƒ½ã€‚ï¼ˆiiï¼‰ç»è¿‡è‰¯å¥½è°ƒæ•´çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ä½¿å¾—ä¸ºç»™å®šåŒ»ç–—é¢†åŸŸè®­ç»ƒè‡ªå®šä¹‰åŸºç¡€æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œæ— éœ€è¿›è¡Œè¶…å‚æ•°è°ƒæ•´å’Œæ–¹æ³•å¾®è°ƒã€‚æ ¹æ®è¿™äº›å‘ç°ï¼Œæˆ‘ä»¬è®¤ä¸ºåœ¨å¼€å‘ç‰¹å®šé¢†åŸŸçš„åŸºç¡€æ¨¡å‹æ—¶ï¼Œåº”é¿å…è¿‡åˆ†åå‘æ–¹æ³•åˆ›æ–°çš„å€¾å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¸è§çš„è®¡ç®—èµ„æºçº¦æŸä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19552v1">PDF</a> Submitted version of paper accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡æ— æ ‡ç­¾åŒ»å­¦æ•°æ®é›†çš„ä½¿ç”¨é—®é¢˜ï¼Œç ”ç©¶æ˜¯å¦åº”è¯¥ä½¿ç”¨è‡ªå®šä¹‰åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿˜æ˜¯ä½¿ç”¨ç°æœ‰é€šç”¨æ¨¡å‹çš„è¿ç§»å­¦ä¹ ã€‚é€šè¿‡å¯¹ä¸€ä¸ªåŒ…å«2ç™¾ä¸‡å¼ èƒå„¿è¶…å£°å›¾åƒçš„åŒºåŸŸæ•°æ®é›†è¿›è¡Œå®è¯ç ”ç©¶ï¼Œé‡‡ç”¨æˆç†Ÿçš„DINOv2é¢„è®­ç»ƒæ³•ï¼Œåœ¨ä¸‰ä¸ªèƒå„¿è¶…å£°æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚å¯¹æ¯”åœ¨è‡ªç„¶å›¾åƒã€è¶…å£°å›¾åƒä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ä»¥åŠç›‘ç£å­¦ä¹ åŸºçº¿æ¨¡å‹ï¼Œæœ¬æ–‡å‘ç°ï¼šä¸€ã€å³ä½¿åœ¨å°æ•°æ®é›†ä¸Šè®­ç»ƒå°å‹æ¨¡å‹ï¼Œå¯¹è‡ªå®šä¹‰æ•°æ®è¿›è¡Œé¢„è®­ç»ƒä¹Ÿæ˜¯å€¼å¾—çš„ï¼›äºŒã€åœ¨é€šç”¨è®¡ç®—èµ„æºé™åˆ¶ä¸‹ï¼Œå¯¹äºç‰¹å®šåŒ»å­¦é¢†åŸŸçš„è‡ªå®šä¹‰åŸºç¡€æ¨¡å‹å¼€å‘ï¼Œåº”é¿å…è¿‡äºè¿½æ±‚æ–¹æ³•åˆ›æ–°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜é¢ä¸´æ˜¯å¦ä½¿ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾åŒ»å­¦æ•°æ®é›†è¿›è¡Œè‡ªå®šä¹‰åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å®è¯ç ”ç©¶ï¼Œå‘ç°å¯¹è‡ªå®šä¹‰åŒ»å­¦æ•°æ®è¿›è¡Œé¢„è®­ç»ƒå€¼å¾—æŠ•å…¥ï¼Œå³ä½¿åœ¨æœ‰é™æ•°æ®é›†ä¸Šè®­ç»ƒå°å‹æ¨¡å‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>å•çº¯æ‰©å¤§è‡ªç„¶å›¾åƒé¢„è®­ç»ƒè§„æ¨¡å¹¶ä¸ç­‰åŒäºè¶…å£°å›¾åƒæ€§èƒ½çš„æå‡ã€‚</li>
<li>æˆç†Ÿçš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ä½¿å¾—é’ˆå¯¹ç‰¹å®šåŒ»å­¦é¢†åŸŸè®­ç»ƒè‡ªå®šä¹‰åŸºç¡€æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œæ— éœ€è¿›è¡Œè¶…å‚æ•°è°ƒæ•´å’Œæ–¹æ³•è½»å¾®è°ƒæ•´ã€‚</li>
<li>åœ¨å¼€å‘ç‰¹å®šé¢†åŸŸçš„åŒ»å­¦åŸºç¡€æ¨¡å‹æ—¶ï¼Œåº”é¿å…è¿‡äºè¿½æ±‚æ–¹æ³•åˆ›æ–°ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒç ”ç©¶ï¼Œä½¿ç”¨DINOv2é¢„è®­ç»ƒæ³•åœ¨èƒå„¿è¶…å£°å›¾åƒä¸Šå–å¾—äº†æœ€å…ˆè¿›æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e62c6ee38cc8295c946d79104144a2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f187459b97356a43ee42829244f71039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c97adca598dc8ee954f72c7289c06656.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15499e903613e58b2484c581e68a730.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Assessing-Risk-of-Stealing-Proprietary-Models-for-Medical-Imaging-Tasks"><a href="#Assessing-Risk-of-Stealing-Proprietary-Models-for-Medical-Imaging-Tasks" class="headerlink" title="Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks"></a>Assessing Risk of Stealing Proprietary Models for Medical Imaging Tasks</h2><p><strong>Authors:Ankita Raj, Harsh Swaika, Deepankar Varma, Chetan Arora</strong></p>
<p>The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the modelâ€™s functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim modelâ€™s training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using publicly available datasets. To further enhance MS capabilities with limited query budgets, we propose a two-step model stealing approach termed QueryWise. This method capitalizes on unlabeled data obtained from a proxy distribution to train the thief model without incurring additional queries. Evaluation on two medical imaging models for Gallbladder Cancer and COVID-19 classification substantiates the effectiveness of the proposed attack. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/rajankita/QueryWise">https://github.com/rajankita/QueryWise</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒåº”ç”¨ä¸­çš„æˆåŠŸï¼Œä¿ƒä½¿å¤šå®¶å…¬å¸åœ¨è¯Šæ–­å·¥ä½œä¸­éƒ¨ç½²ä¸“æœ‰æ¨¡å‹ï¼Œæä¾›æ”¶è´¹æœåŠ¡ã€‚å°½ç®¡ä¸ºäº†ä¿æŠ¤æœåŠ¡æä¾›è€…çš„çŸ¥è¯†äº§æƒï¼Œæ¨¡å‹æƒé‡æ˜¯éšè—çš„ï¼Œä½†è¿™äº›æ¨¡å‹ä»é¢ä¸´æ¨¡å‹çªƒå–ï¼ˆMSï¼‰æ”»å‡»çš„é£é™©ï¼Œå…¶ä¸­å¯¹æ‰‹å¯ä»¥é€šè¿‡ä½¿ç”¨ä»£ç†æ•°æ®é›†æŸ¥è¯¢æ¥å…‹éš†æ¨¡å‹çš„åŠŸèƒ½ï¼Œå¹¶åœ¨è·å¾—çš„é¢„æµ‹ä¸Šè®­ç»ƒçªƒå–æ¨¡å‹ã€‚è™½ç„¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼ŒåŒ»å­¦å½±åƒæ¨¡å‹å¯¹MSæ”»å‡»çš„æ•æ„Ÿæ€§ä»ç„¶æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿçš„æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶äº†åœ¨ç°å®ä¸–ç•Œæ¡ä»¶ä¸‹ï¼Œé»‘ç›’åŒ»å­¦å½±åƒæ¨¡å‹å¯¹æ¨¡å‹çªƒå–æ”»å‡»çš„è„†å¼±æ€§ï¼Œå…¶ä¸­å¯¹æ‰‹æ— æ³•è·å¾—å—å®³è€…æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨æœ‰é™çš„æŸ¥è¯¢é¢„ç®—ä¸‹æ“ä½œã€‚æˆ‘ä»¬è¯æ˜å¯¹æ‰‹å¯ä»¥ä½¿ç”¨å…¬å¼€æ•°æ®é›†æœ‰æ•ˆåœ°æ‰§è¡ŒMSæ”»å‡»ã€‚ä¸ºäº†è¿›ä¸€æ­¥åœ¨æœ‰é™çš„æŸ¥è¯¢é¢„ç®—ä¸‹å¢å¼ºMSèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤æ­¥æ¨¡å‹çªƒå–æ–¹æ³•ï¼Œç§°ä¸ºQueryWiseã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»ä»£ç†åˆ†å¸ƒè·å¾—çš„æ— æ ‡ç­¾æ•°æ®æ¥è®­ç»ƒçªƒå–æ¨¡å‹ï¼Œè€Œæ— éœ€äº§ç”Ÿé¢å¤–çš„æŸ¥è¯¢ã€‚å¯¹ä¸¤ä¸ªç”¨äºèƒ†å›Šç™Œå’ŒCOVID-19åˆ†ç±»çš„åŒ»å­¦å½±åƒæ¨¡å‹è¿›è¡Œçš„è¯„ä¼°è¯å®äº†æ‰€æå‡ºæ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rajankita/QueryWise%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rajankita/QueryWiseæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19464v1">PDF</a> Accepted to MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒåº”ç”¨ä¸­çš„æˆåŠŸä¿ƒä½¿å¤šå®¶å…¬å¸åœ¨è¯Šæ–­æµç¨‹ä¸­éƒ¨ç½²ä¸“æœ‰æ¨¡å‹ï¼Œæä¾›æœ‰å¿æœåŠ¡ã€‚å°½ç®¡æ¨¡å‹æƒé‡å—åˆ°ä¿æŠ¤ä»¥éšè—æœåŠ¡æä¾›å•†çš„çŸ¥è¯†äº§æƒï¼Œä½†è¿™äº›æ¨¡å‹ä»é¢ä¸´æ¨¡å‹çªƒå–ï¼ˆMSï¼‰æ”»å‡»çš„é£é™©ã€‚å¯¹æ‰‹å¯ä»¥é€šè¿‡ä½¿ç”¨ä»£ç†æ•°æ®é›†æŸ¥è¯¢æ¨¡å‹å¹¶åŸºäºè·å–çš„é¢„æµ‹è®­ç»ƒç›—ç”¨æ¨¡å‹æ¥å…‹éš†æ¨¡å‹åŠŸèƒ½ã€‚å°½ç®¡åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šå¯¹æ¨¡å‹çªƒå–æ”»å‡»è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ï¼Œä½†åŒ»å­¦å½±åƒæ¨¡å‹å¯¹å…¶çš„æ˜“æ„Ÿæ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨ç°å®æ¡ä»¶ä¸‹ï¼Œå¯¹æ‰‹æ— æ³•è®¿é—®å—å®³æ¨¡å‹è®­ç»ƒæ•°æ®ä¸”æ“ä½œå—é™äºæŸ¥è¯¢é¢„ç®—æ—¶ï¼Œé»‘ç®±åŒ»å­¦å½±åƒæ¨¡å‹å¯¹æ¨¡å‹çªƒå–æ”»å‡»ï¼ˆMSæ”»å‡»ï¼‰çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬è¯æ˜å¯¹æ‰‹å¯ä»¥ä½¿ç”¨å…¬å¼€æ•°æ®é›†æœ‰æ•ˆåœ°æ‰§è¡ŒMSæ”»å‡»ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æœ‰é™çš„æŸ¥è¯¢é¢„ç®—ä¸‹çš„æ¨¡å‹çªƒå–èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºQueryWiseçš„ä¸¤æ­¥æ¨¡å‹çªƒå–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä»ä»£ç†åˆ†å¸ƒè·å¾—çš„æ— æ ‡ç­¾æ•°æ®æ¥è®­ç»ƒç›—ç”¨æ¨¡å‹ï¼Œæ— éœ€é¢å¤–æŸ¥è¯¢ã€‚å¯¹ç”¨äºèƒ†å›Šç™Œå’ŒCOVID-19åˆ†ç±»çš„ä¸¤ä¸ªåŒ»å­¦å½±åƒæ¨¡å‹è¿›è¡Œçš„è¯„ä¼°è¯å®äº†æ‰€ææ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rajankita/QueryWise%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rajankita/QueryWiseæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒåº”ç”¨ä¸­çš„æˆåŠŸä¿ƒä½¿å…¬å¸éƒ¨ç½²ä¸“æœ‰æ¨¡å‹æä¾›æœåŠ¡ï¼Œå¸¦æ¥çŸ¥è¯†äº§æƒä¿æŠ¤çš„æŒ‘æˆ˜ã€‚</li>
<li>åŒ»å­¦å½±åƒæ¨¡å‹é¢ä¸´æ¨¡å‹çªƒå–æ”»å‡»çš„é£é™©ï¼Œå³ä½¿æ¨¡å‹æƒé‡ä¿å¯†ï¼Œå¯¹æ‰‹ä»èƒ½é€šè¿‡æŸ¥è¯¢æ¨¡å‹å’Œè®­ç»ƒç›—ç”¨æ¨¡å‹å…‹éš†å…¶åŠŸèƒ½ã€‚</li>
<li>å°½ç®¡å¯¹ä¸€èˆ¬è§†è§‰ä»»åŠ¡çš„æ¨¡å‹çªƒå–æ”»å‡»è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ï¼Œä½†åŒ»å­¦å½±åƒæ¨¡å‹çš„è„†å¼±æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>åœ¨å¯¹æ‰‹æ— æ³•è®¿é—®å—å®³æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸”æŸ¥è¯¢é¢„ç®—æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé»‘ç®±åŒ»å­¦å½±åƒæ¨¡å‹æ˜“å—MSæ”»å‡»ã€‚</li>
<li>ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¯ä»¥æœ‰æ•ˆåœ°æ‰§è¡Œæ¨¡å‹çªƒå–æ”»å‡»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºQueryWiseçš„ä¸¤æ­¥æ¨¡å‹çªƒå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®è®­ç»ƒç›—ç”¨æ¨¡å‹ï¼Œæ— éœ€é¢å¤–æŸ¥è¯¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6d7783f23727d9acb3b6c5620fd0771.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5157cf5877f73426e48a266abfee527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37517a8fe99488ac377b91ace01bb331.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Angio-Diff-Learning-a-Self-Supervised-Adversarial-Diffusion-Model-for-Angiographic-Geometry-Generation"><a href="#Angio-Diff-Learning-a-Self-Supervised-Adversarial-Diffusion-Model-for-Angiographic-Geometry-Generation" class="headerlink" title="Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for   Angiographic Geometry Generation"></a>Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for   Angiographic Geometry Generation</h2><p><strong>Authors:Zhifeng Wang, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu, Kunlun He</strong></p>
<p>Vascular diseases pose a significant threat to human health, with X-ray angiography established as the gold standard for diagnosis, allowing for detailed observation of blood vessels. However, angiographic X-rays expose personnel and patients to higher radiation levels than non-angiographic X-rays, which are unwanted. Thus, modality translation from non-angiographic to angiographic X-rays is desirable. Data-driven deep approaches are hindered by the lack of paired large-scale X-ray angiography datasets. While making high-quality vascular angiography synthesis crucial, it remains challenging. We find that current medical image synthesis primarily operates at pixel level and struggles to adapt to the complex geometric structure of blood vessels, resulting in unsatisfactory quality of blood vessel image synthesis, such as disconnections or unnatural curvatures. To overcome this issue, we propose a self-supervised method via diffusion models to transform non-angiographic X-rays into angiographic X-rays, mitigating data shortages for data-driven approaches. Our model comprises a diffusion model that learns the distribution of vascular data from diffusion latent, a generator for vessel synthesis, and a mask-based adversarial module. To enhance geometric accuracy, we propose a parametric vascular model to fit the shape and distribution of blood vessels. The proposed method contributes a pipeline and a synthetic dataset for X-ray angiography. We conducted extensive comparative and ablation experiments to evaluate the Angio-Diff. The results demonstrate that our method achieves state-of-the-art performance in synthetic angiography image quality and more accurately synthesizes the geometric structure of blood vessels. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zfw-cv/AngioDiff">https://github.com/zfw-cv/AngioDiff</a>. </p>
<blockquote>
<p>è¡€ç®¡ç–¾ç—…å¯¹äººç±»å¥åº·æ„æˆé‡å¤§å¨èƒï¼ŒXå°„çº¿è¡€ç®¡é€ å½±æœ¯å·²ç¡®ç«‹ä¸ºè¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œå¯ä»¥è¯¦ç»†è§‚å¯Ÿè¡€ç®¡ã€‚ç„¶è€Œï¼Œè¡€ç®¡é€ å½±Xå°„çº¿ä½¿äººå‘˜å’Œæ‚£è€…æš´éœ²äºæ¯”éè¡€ç®¡é€ å½±Xå°„çº¿æ›´é«˜çš„è¾å°„æ°´å¹³ï¼Œè¿™æ˜¯äººä»¬æ‰€ä¸æ„¿æ„çœ‹åˆ°çš„ã€‚å› æ­¤ï¼Œä»éè¡€ç®¡é€ å½±Xå°„çº¿åˆ°è¡€ç®¡é€ å½±Xå°„çº¿çš„æ¨¡æ€è½¬æ¢æ˜¯å¯è¡Œçš„ã€‚æ•°æ®é©±åŠ¨çš„æ·±åº¦æ–¹æ³•ç”±äºç¼ºä¹é…å¯¹çš„å¤§è§„æ¨¡Xå°„çº¿è¡€ç®¡é€ å½±æ•°æ®é›†è€Œå—åˆ°é˜»ç¢ã€‚å°½ç®¡è¿›è¡Œé«˜è´¨é‡è¡€ç®¡é€ å½±åˆæˆè‡³å…³é‡è¦ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„åŒ»å­¦å›¾åƒåˆæˆä¸»è¦åœ¨åƒç´ çº§åˆ«æ“ä½œï¼Œå¹¶éš¾ä»¥é€‚åº”è¡€ç®¡çš„å¤æ‚å‡ ä½•ç»“æ„ï¼Œå¯¼è‡´è¡€ç®¡å›¾åƒåˆæˆè´¨é‡ä¸ä½³ï¼Œä¾‹å¦‚æ–­è£‚æˆ–ä¸è‡ªç„¶çš„å¼¯æ›²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œéè¡€ç®¡é€ å½±Xå°„çº¿è½¬æ¢ä¸ºè¡€ç®¡é€ å½±Xå°„çº¿çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œä»¥å‡è½»æ•°æ®é©±åŠ¨æ–¹æ³•çš„æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªä»æ‰©æ•£æ½œåœ¨ä¸­å­¦ä¹ è¡€ç®¡æ•°æ®åˆ†å¸ƒçš„æ‰©æ•£æ¨¡å‹ã€ä¸€ä¸ªç”¨äºè¡€ç®¡åˆæˆçš„ç”Ÿæˆå™¨ä»¥åŠä¸€ä¸ªåŸºäºæ©ç çš„å¯¹æŠ—æ¨¡å—ã€‚ä¸ºäº†æé«˜å‡ ä½•ç²¾åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚æ•°åŒ–è¡€ç®¡æ¨¡å‹ï¼Œä»¥æ‹Ÿåˆè¡€ç®¡çš„å½¢çŠ¶å’Œåˆ†å¸ƒã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸ºXå°„çº¿è¡€ç®¡é€ å½±æä¾›äº†ä¸€ä¸ªç®¡é“å’Œåˆæˆæ•°æ®é›†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ¯”è¾ƒå’Œæ¶ˆèå®éªŒæ¥è¯„ä¼°Angio-Diffã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆè¡€ç®¡é€ å½±å›¾åƒè´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶æ›´å‡†ç¡®åœ°åˆæˆäº†è¡€ç®¡çš„å‡ ä½•ç»“æ„ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zfw-cv/AngioDiff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zfw-cv/AngioDiffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19455v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éè¡€ç®¡é€ å½±Xå°„çº¿è½¬ä¸ºè¡€ç®¡é€ å½±Xå°„çº¿çš„è‡ªç›‘ç£æ–¹æ³•è¢«æå‡ºï¼Œä»¥å…‹æœæ•°æ®çŸ­ç¼ºçš„é—®é¢˜ã€‚æ–°æ–¹æ³•é€šè¿‡æ‰©æ•£æ¨¡å‹å­¦ä¹ è¡€ç®¡æ•°æ®çš„åˆ†å¸ƒï¼Œå¹¶åˆ©ç”¨å‚æ•°åŒ–è¡€ç®¡æ¨¡å‹æå‡å‡ ä½•ç²¾åº¦ã€‚æ­¤æ¨¡å‹æé«˜äº†åˆæˆè¡€ç®¡é€ å½±å›¾åƒçš„è´¨é‡å¹¶æ›´å‡†ç¡®åœ°åˆæˆè¡€ç®¡å‡ ä½•ç»“æ„ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡€ç®¡ç–¾ç—…å¯¹äººç±»å¥åº·æ„æˆé‡å¤§å¨èƒï¼ŒXå°„çº¿è¡€ç®¡é€ å½±æ˜¯è¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œä½†å­˜åœ¨è¾å°„æš´éœ²é—®é¢˜ã€‚</li>
<li>æ•°æ®é©±åŠ¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¡€ç®¡é€ å½±å›¾åƒåˆæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç¼ºä¹å¤§è§„æ¨¡é…å¯¹Xå°„çº¿è¡€ç®¡é€ å½±æ•°æ®é›†ã€‚</li>
<li>å½“å‰åŒ»å­¦å›¾åƒåˆæˆä¸»è¦åœ¨åƒç´ å±‚é¢æ“ä½œï¼Œéš¾ä»¥é€‚åº”è¡€ç®¡å¤æ‚å‡ ä½•ç»“æ„ï¼Œå¯¼è‡´è¡€ç®¡å›¾åƒåˆæˆè´¨é‡ä¸ä½³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œè‡ªç›‘ç£çš„æ–¹æ³•ï¼Œå°†éè¡€ç®¡é€ å½±Xå°„çº¿è½¬æ¢ä¸ºè¡€ç®¡é€ å½±Xå°„çº¿ï¼Œä»¥ç¼“è§£æ•°æ®é©±åŠ¨æ–¹æ³•çš„æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚</li>
<li>æ¨¡å‹åŒ…æ‹¬ä»æ‰©æ•£æ½œåœ¨å­¦ä¹ ä¸­è¡€ç®¡æ•°æ®åˆ†å¸ƒçš„æ‰©æ•£æ¨¡å‹ã€è¡€ç®¡åˆæˆçš„ç”Ÿæˆå™¨ä»¥åŠåŸºäºæ©è†œçš„å¯¹æŠ—æ¨¡å—ã€‚</li>
<li>ä¸ºæé«˜å‡ ä½•ç²¾åº¦ï¼Œå¼•å…¥äº†å‚æ•°åŒ–è¡€ç®¡æ¨¡å‹æ¥æ‹Ÿåˆè¡€ç®¡çš„å½¢çŠ¶å’Œåˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-91c3f22c3f82f493835456755506a182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60f79b6a87d7c00107663b707464b9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-131d067765873e753309cc9324ef1deb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Birthplaces-of-X-ray-emission-lines-in-Cygnus-X-3"><a href="#Birthplaces-of-X-ray-emission-lines-in-Cygnus-X-3" class="headerlink" title="Birthplaces of X-ray emission lines in Cygnus X-3"></a>Birthplaces of X-ray emission lines in Cygnus X-3</h2><p><strong>Authors:Osmi Vilhu, Karri I. I. Koljonen</strong></p>
<p>We investigate the formation of X-ray emission lines in the wind of the Wolf-Rayet (WR) companion in Cyg X-3 by analyzing their orbital dynamics using Chandra High Energy Transmission Grating (HEG) observations during a hypersoft state. Our goal is to constrain the X-ray transparency of the recently discovered funnel-like structure surrounding the compact star, as revealed by X-ray polarimetry. All lines exhibit sinusoidal orbital modulation, with the velocity amplitude generally increasing and the orbital phase with the highest blueshift generally decreasing for ions with a higher ionisation potential. The {Fe}{xxvi}-line displays velocity extremes at phase 0.25 (blueshift) and 0.75 (redshift), indicating that the line-emitting region is close to the compact component (disc or corona) and thus reflects the orbital motion. The {Fe}{xxv}-line shows a complex behaviour that cannot be fully resolved with the Chandra&#x2F;HEG resolution. Other lines display velocity extremes scattered around phase 0.5 (blueshift) and 0.0 (redshift), with velocity amplitudes of 100â€“300 km&#x2F;s, suggesting their origin in the WR stellar wind between the two components along 3D \xi-surfaces. Parts of the emission lines of {Ar}{xviii} and {Ca}{xx} originated around the compact star (disc or corona). The recent polarisation funnel-modelling is consistent with the present results during the hypersoft state. </p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡å¯¹å¤©é¹…åº§X-3ä¸­çš„Wolf-Rayetï¼ˆWRï¼‰ä¼´ä¾£é£çš„Xå°„çº¿å‘å°„çº¿çš„è½¨é“åŠ¨åŠ›å­¦è¿›è¡Œåˆ†æï¼Œåˆ©ç”¨é’±å¾·æ‹‰é«˜èƒ½ä¼ è¾“å…‰æ …ï¼ˆHEGï¼‰åœ¨è¶…è½¯çŠ¶æ€ä¸‹çš„è§‚æµ‹ç»“æœï¼Œç ”ç©¶å…¶Xå°„çº¿å‘å°„çº¿çš„å½¢æˆã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯çº¦æŸæœ€è¿‘å‘ç°çš„å›´ç»•è‡´å¯†æ˜Ÿçš„æ¼æ–—çŠ¶ç»“æ„çš„Xå°„çº¿é€æ˜åº¦ï¼Œå¦‚Xå°„çº¿åæŒ¯æµ‹é‡æ‰€æ­ç¤ºçš„ã€‚æ‰€æœ‰å‘å°„çº¿éƒ½è¡¨ç°å‡ºæ­£å¼¦è½¨é“è°ƒåˆ¶ï¼Œé€Ÿåº¦æŒ¯å¹…ä¸€èˆ¬å¢åŠ ï¼Œå¯¹äºå…·æœ‰è¾ƒé«˜ç”µç¦»åŠ¿çš„ç¦»å­ï¼Œå…·æœ‰æœ€é«˜è“ç§»çš„è½¨é“ç›¸ä½ä¸€èˆ¬å‡å°ã€‚{Fe}{xxvi}çº¿åœ¨ç›¸ä½0.25ï¼ˆè“ç§»ï¼‰å’Œ0.75ï¼ˆçº¢ç§»ï¼‰æ—¶è¡¨ç°å‡ºé€Ÿåº¦æç«¯å€¼ï¼Œè¿™è¡¨æ˜å‘å°„çº¿åŒºåŸŸé è¿‘è‡´å¯†ç»„ä»¶ï¼ˆç£ç›˜æˆ–å†•ï¼‰ï¼Œå› æ­¤åæ˜ äº†è½¨é“è¿åŠ¨ã€‚{Fe}{xxv}çº¿è¡¨ç°å‡ºå¤æ‚çš„è¡Œä¸ºï¼Œæ— æ³•ç”¨é’±å¾·æ‹‰&#x2F;HEGåˆ†è¾¨ç‡å®Œå…¨è§£å†³ã€‚å…¶ä»–çº¿è·¯çš„é€Ÿåº¦æç«¯å€¼æ•£è½åœ¨ç›¸ä½0.5ï¼ˆè“ç§»ï¼‰å’Œ0.0ï¼ˆçº¢ç§»ï¼‰å‘¨å›´ï¼Œé€Ÿåº¦æŒ¯å¹…ä¸º100-300å…¬é‡Œ&#x2F;ç§’ï¼Œè¿™è¡¨æ˜å®ƒä»¬èµ·æºäºWRæ’æ˜Ÿé£åœ¨æ²¿ä¸‰ç»´Î¾è¡¨é¢çš„ä¸¤ä¸ªç»„ä»¶ä¹‹é—´ã€‚{Ar}{xviii}å’Œ{Ca}{xx}çš„å‘å°„çº¿éƒ¨åˆ†èµ·æºäºè‡´å¯†æ˜Ÿï¼ˆç£ç›˜æˆ–å†•ï¼‰ã€‚æœ€è¿‘çš„åæŒ¯æ¼æ–—æ¨¡å‹ä¸è¶…è½¯çŠ¶æ€ä¸‹çš„å½“å‰ç»“æœä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19380v1">PDF</a> accepted 12.6.2025 for A&amp;A, 9 pages 7 figures</p>
<p><strong>æ‘˜è¦</strong><br>     åŸºäºChandra High Energy Transmission Gratingï¼ˆHEGï¼‰è§‚æµ‹æ•°æ®ï¼Œç ”ç©¶Cygnus X-3ä¸­çš„Wolf-Rayetï¼ˆWRï¼‰ä¼´ä¾£æ˜Ÿé£çš„Xå°„çº¿å‘å°„çº¿å½¢æˆæœºåˆ¶ï¼Œå¹¶åˆ†æå…¶è½¨é“åŠ¨åŠ›å­¦ç‰¹å¾ã€‚ç›®çš„æ˜¯é€šè¿‡Xå°„çº¿åæŒ¯æµ‹é‡æ­ç¤ºçš„æ¼æ–—çŠ¶ç»“æ„çº¦æŸXå°„çº¿é€æ˜åº¦ã€‚æ‰€æœ‰å‘å°„çº¿å‡è¡¨ç°å‡ºæ­£å¼¦è½¨é“è°ƒåˆ¶ï¼Œå¯¹äºé«˜ç”µç¦»ç”µä½çš„ç¦»å­ï¼Œé€Ÿåº¦æŒ¯å¹…ä¸€èˆ¬å¢åŠ ï¼Œè€Œå…·æœ‰æœ€é«˜è“ç§»çš„è½¨é“ç›¸ä½ä¸€èˆ¬å‡å°ã€‚Fe xxviçº¿åœ¨ç›¸ä½0.25ï¼ˆè“ç§»ï¼‰å’Œ0.75ï¼ˆçº¢ç§»ï¼‰æ—¶æ˜¾ç¤ºå‡ºé€Ÿåº¦æå€¼ï¼Œè¡¨æ˜å‘å°„çº¿åŒºåŸŸé è¿‘è‡´å¯†æˆåˆ†ï¼ˆç£ç›˜æˆ–å†•ï¼‰ï¼Œåæ˜ äº†è½¨é“è¿åŠ¨ã€‚å…¶ä»–çº¿è·¯çš„é€Ÿåº¦æå€¼åˆ™åˆ†æ•£åœ¨ç›¸ä½0.5ï¼ˆè“ç§»ï¼‰å’Œ0.0ï¼ˆçº¢ç§»ï¼‰ï¼Œé€Ÿåº¦æŒ¯å¹…ä¸º100-300å…¬é‡Œ&#x2F;ç§’ï¼Œè¡¨æ˜å®ƒä»¬èµ·æºäºWRæ’æ˜Ÿé£åœ¨ä¸¤é¢—æ’æ˜Ÿä¹‹é—´çš„ä¸‰ç»´Î¾è¡¨é¢ã€‚éƒ¨åˆ†Ar xviiiå’ŒCa xxçš„å‘å°„çº¿èµ·æºäºè‡´å¯†æ˜Ÿï¼ˆç£ç›˜æˆ–å†•ï¼‰ã€‚æœ€è¿‘çš„åæŒ¯æ¼æ–—æ¨¡å‹ä¸å½“å‰å¤„äºè¶…è½¯çŠ¶æ€çš„è§‚æµ‹ç»“æœä¸€è‡´ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é€šè¿‡åˆ†æChandra HEGè§‚æµ‹æ•°æ®ï¼Œç ”ç©¶äº†Cygnus X-3ä¸­Wolf-Rayetä¼´ä¾£æ˜Ÿçš„Xå°„çº¿å‘å°„çº¿çš„è½¨é“åŠ¨åŠ›å­¦ç‰¹å¾ã€‚</li>
<li>æ‰€æœ‰å‘å°„çº¿éƒ½è¡¨ç°å‡ºè½¨é“è°ƒåˆ¶ï¼Œåæ˜ äº†WRæ’æ˜Ÿé£çš„åŠ¨æ€å˜åŒ–ã€‚</li>
<li>Fe xxviçº¿çš„é€Ÿåº¦æå€¼ä¸è½¨é“è¿åŠ¨ç´§å¯†ç›¸å…³ï¼Œè¡¨æ˜å…¶å‘å°„çº¿åŒºåŸŸé è¿‘è‡´å¯†æˆåˆ†ï¼ˆå¦‚ç£ç›˜æˆ–å†•ï¼‰ã€‚</li>
<li>å…¶ä»–çº¿è·¯çš„å‘å°„æºäºWRæ’æ˜Ÿé£åœ¨ä¸¤é¢—æ’æ˜Ÿä¹‹é—´çš„ä¸‰ç»´Î¾è¡¨é¢ï¼Œé€Ÿåº¦æŒ¯å¹…åœ¨100-300å…¬é‡Œ&#x2F;ç§’ä¹‹é—´ã€‚</li>
<li>éƒ¨åˆ†Ar xviiiå’ŒCa xxçš„å‘å°„çº¿èµ·æºäºè‡´å¯†æ˜ŸåŒºåŸŸã€‚</li>
<li>Xå°„çº¿åæŒ¯æµ‹é‡æ­ç¤ºçš„æ¼æ–—çŠ¶ç»“æ„ä¸è§‚æµ‹ç»“æœä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e24798e5e49e4e3660d277fc97691d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df5087ad8358a962eb528c52449c701e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015dba748b0bac7c21506ea34f25dbeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c40571c7f6ed00c3feaa7a2bcb551bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08a39463e0102272a01cec98a683f825.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Reconsidering-Explicit-Longitudinal-Mammography-Alignment-for-Enhanced-Breast-Cancer-Risk-Prediction"><a href="#Reconsidering-Explicit-Longitudinal-Mammography-Alignment-for-Enhanced-Breast-Cancer-Risk-Prediction" class="headerlink" title="Reconsidering Explicit Longitudinal Mammography Alignment for Enhanced   Breast Cancer Risk Prediction"></a>Reconsidering Explicit Longitudinal Mammography Alignment for Enhanced   Breast Cancer Risk Prediction</h2><p><strong>Authors:Solveig Thrun, Stine Hansen, Zijun Sun, Nele Blum, Suaiba A. Salahuddin, Kristoffer WickstrÃ¸m, Elisabeth Wetzer, Robert Jenssen, Maik Stille, Michael Kampffmeyer</strong></p>
<p>Regular mammography screening is essential for early breast cancer detection. Deep learning-based risk prediction methods have sparked interest to adjust screening intervals for high-risk groups. While early methods focused only on current mammograms, recent approaches leverage the temporal aspect of screenings to track breast tissue changes over time, requiring spatial alignment across different time points. Two main strategies for this have emerged: explicit feature alignment through deformable registration and implicit learned alignment using techniques like transformers, with the former providing more control. However, the optimal approach for explicit alignment in mammography remains underexplored. In this study, we provide insights into where explicit alignment should occur (input space vs. representation space) and if alignment and risk prediction should be jointly optimized. We demonstrate that jointly learning explicit alignment in representation space while optimizing risk estimation performance, as done in the current state-of-the-art approach, results in a trade-off between alignment quality and predictive performance and show that image-level alignment is superior to representation-level alignment, leading to better deformation field quality and enhanced risk prediction accuracy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/sot176/Longitudinal_Mammogram_Alignment.git">https://github.com/sot176/Longitudinal_Mammogram_Alignment.git</a>. </p>
<blockquote>
<p>å®šæœŸä¹³è…ºXå…‰æ‘„å½±ç­›æŸ¥å¯¹äºæ—©æœŸä¹³è…ºç™Œæ£€æµ‹è‡³å…³é‡è¦ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„é£é™©é¢„æµ‹æ–¹æ³•å¼•å‘äº†é’ˆå¯¹é«˜é£é™©ç¾¤ä½“è°ƒæ•´ç­›æŸ¥é—´éš”çš„å…´è¶£ã€‚æ—©æœŸçš„æ–¹æ³•ä»…å…³æ³¨å½“å‰çš„ä¹³è…ºXå…‰æ‘„å½±ï¼Œè€Œæœ€è¿‘çš„æ–¹æ³•åˆ™åˆ©ç”¨ç­›æŸ¥çš„æ—¶é—´æ–¹é¢æ¥è·Ÿè¸ªéšæ—¶é—´å˜åŒ–çš„ä¹³è…ºç»„ç»‡ï¼Œè¿™éœ€è¦è·¨ä¸åŒæ—¶é—´ç‚¹çš„ç©ºé—´å¯¹é½ã€‚ä¸ºæ­¤å‡ºç°äº†ä¸¤ç§ä¸»è¦ç­–ç•¥ï¼šé€šè¿‡å¯å˜å½¢æ³¨å†Œè¿›è¡Œæ˜¾å¼ç‰¹å¾å¯¹é½å’Œä½¿ç”¨å˜å‹å™¨æŠ€æœ¯ç­‰éšå¼å­¦ä¹ å¯¹é½ï¼Œå‰è€…æä¾›äº†æ›´å¤šçš„æ§åˆ¶ã€‚ç„¶è€Œï¼Œä¹³è…ºXå…‰æ‘„å½±ä¸­æ˜¾å¼å¯¹é½çš„æœ€ä½³æ–¹æ³•ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºæ˜¾å¼å¯¹é½åº”è¯¥å‘ç”Ÿçš„ä½ç½®ï¼ˆè¾“å…¥ç©ºé—´ä¸è¡¨ç¤ºç©ºé—´ï¼‰ä»¥åŠæ˜¯å¦åº”è¯¥è”åˆä¼˜åŒ–å¯¹é½å’Œé£é™©é¢„æµ‹çš„è§è§£ã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨è¡¨ç¤ºç©ºé—´ä¸­è”åˆå­¦ä¹ æ˜¾å¼å¯¹é½çš„åŒæ—¶ä¼˜åŒ–é£é™©ä¼°è®¡æ€§èƒ½ï¼ˆå¦‚å½“å‰æœ€å…ˆè¿›çš„åšæ³•æ‰€ç¤ºï¼‰ï¼Œè¿™ä¼šå¯¼è‡´å¯¹é½è´¨é‡å’Œé¢„æµ‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ˜¾ç¤ºå›¾åƒçº§å¯¹é½ä¼˜äºè¡¨ç¤ºçº§å¯¹é½ï¼Œä»è€Œæé«˜äº†å˜å½¢åœºçš„è´¨é‡å’Œé£é™©é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sot176/Longitudinal_Mammogram_Alignment.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/sot176/Longitudinal_Mammogram_Alignment.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19363v1">PDF</a> MICCAI 2025, early accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨ä¹³è…ºç™Œç­›æŸ¥ä¸­ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œé£é™©é¢„æµ‹çš„æ–¹æ³•ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç»“åˆæ˜ç¡®ç‰¹å¾å¯¹é½ä¸é£é™©é¢„æµ‹è”åˆä¼˜åŒ–ï¼Œåœ¨è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œæ˜¾å¼å¯¹é½å­¦ä¹ ï¼Œå¯ä»¥åœ¨å¯¹é½è´¨é‡å’Œé¢„æµ‹æ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åŒæ—¶ï¼Œå›¾åƒçº§åˆ«çš„å¯¹é½ä¼˜äºè¡¨ç¤ºçº§åˆ«çš„å¯¹é½ï¼Œèƒ½æä¾›æ›´å¥½çš„å˜å½¢åœºè´¨é‡å’Œæ›´é«˜çš„é£é™©é¢„æµ‹å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œç­›æŸ¥ä¸­ï¼Œæ·±åº¦å­¦ä¹ ç”¨äºé£é™©é¢„æµ‹å·²å¼•å‘å…³æ³¨ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹é«˜é£é™©ç¾¤ä½“çš„ç­›æŸ¥é—´éš”è°ƒæ•´ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å·²ç»ä»å•çº¯å…³æ³¨å½“å‰é’¼é¶å›¾åƒè½¬å‘åˆ©ç”¨æ—¶é—´ç»´åº¦è¿›è¡Œå±å¹•è·Ÿè¸ªï¼Œä»¥ç›‘æµ‹ä¹³è…ºç»„ç»‡éšæ—¶é—´çš„å˜åŒ–ã€‚</li>
<li>ä¸»è¦æœ‰ä¸¤ç§ç­–ç•¥è¿›è¡Œå±å¹•è·Ÿè¸ªï¼šé€šè¿‡å¯å˜å½¢æ³¨å†Œè¿›è¡Œæ˜¾å¼ç‰¹å¾å¯¹é½å’Œä½¿ç”¨å˜å‹å™¨æŠ€æœ¯ç­‰éšå¼å­¦ä¹ å¯¹é½ã€‚</li>
<li>æ˜¾å¼å¯¹é½çš„æœ€ä¼˜æ–¹æ³•å¯¹äºä¹³è…ºç™Œé’¼é¶å›¾åƒä»ç„¶éœ€è¦æ›´å¤šçš„ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶æå‡ºåœ¨è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œæ˜¾å¼å¯¹é½ï¼ŒåŒæ—¶ä¼˜åŒ–é£é™©é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯¹é½è´¨é‡å’Œé¢„æµ‹æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œå‘ç°å›¾åƒçº§åˆ«çš„å¯¹é½ä¼˜äºè¡¨ç¤ºçº§åˆ«çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-096f673bef1e210bdf6c693fa39240c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b62884d6509a49f8bb153b095e695c32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c64e4063f171c4391ed2af625dc2d7f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Deep-Learning-Based-Method-for-Fast-Registration-of-Cardiac-Magnetic-Resonance-Images"><a href="#A-Deep-Learning-Based-Method-for-Fast-Registration-of-Cardiac-Magnetic-Resonance-Images" class="headerlink" title="A Deep Learning Based Method for Fast Registration of Cardiac Magnetic   Resonance Images"></a>A Deep Learning Based Method for Fast Registration of Cardiac Magnetic   Resonance Images</h2><p><strong>Authors:Benjamin Graham</strong></p>
<p>Image registration is used in many medical image analysis applications, such as tracking the motion of tissue in cardiac images, where cardiac kinematics can be an indicator of tissue health. Registration is a challenging problem for deep learning algorithms because ground truth transformations are not feasible to create, and because there are potentially multiple transformations that can produce images that appear correlated with the goal. Unsupervised methods have been proposed to learn to predict effective transformations, but these methods take significantly longer to predict than established baseline methods. For a deep learning method to see adoption in wider research and clinical settings, it should be designed to run in a reasonable time on common, mid-level hardware. Fast methods have been proposed for the task of image registration but often use patch-based methods which can affect registration accuracy for a highly dynamic organ such as the heart.   In this thesis, a fast, volumetric registration model is proposed for the use of quantifying cardiac strain. The proposed Deep Learning Neural Network (DLNN) is designed to utilize an architecture that can compute convolutions incredibly efficiently, allowing the model to achieve registration fidelity similar to other state-of-the-art models while taking a fraction of the time to perform inference. The proposed fast and lightweight registration (FLIR) model is used to predict tissue motion which is then used to quantify the non-uniform strain experienced by the tissue. For acquisitions taken from the same patient at approximately the same time, it would be expected that strain values measured between the acquisitions would have very small differences. Using this metric, strain values computed using the FLIR method are shown to be very consistent. </p>
<blockquote>
<p>å›¾åƒé…å‡†åœ¨è¯¸å¤šåŒ»å­¦å›¾åƒåˆ†æåº”ç”¨ä¸­éƒ½å¾—åˆ°äº†ä½¿ç”¨ï¼Œä¾‹å¦‚åœ¨å¿ƒè„å›¾åƒä¸­è¿½è¸ªç»„ç»‡çš„è¿åŠ¨ï¼Œå…¶ä¸­ï¼Œå¿ƒè„è¿åŠ¨å­¦å¯ä»¥ä½œä¸ºç»„ç»‡å¥åº·çš„æŒ‡æ ‡ã€‚é…å‡†å¯¹äºæ·±åº¦å­¦ä¹ ç®—æ³•æ¥è¯´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºåˆ›å»ºçœŸå®çš„å˜æ¢åŸºå‡†æ•°æ®å¹¶ä¸å¯è¡Œï¼Œè€Œä¸”å¯èƒ½æœ‰å¤šé‡å˜æ¢ä¼šäº§ç”Ÿä¸ç›®æ ‡å›¾åƒç›¸å…³çš„ç»“æœã€‚å·²æœ‰ç ”ç©¶è€…æå‡ºäº†æ— ç›‘ç£æ–¹æ³•ç”¨ä»¥é¢„æµ‹æœ‰æ•ˆå˜æ¢ï¼Œä½†è¿™äº›æ–¹æ³•çš„é¢„æµ‹æ—¶é—´ç›¸æ¯”ä¼ ç»ŸåŸºçº¿æ–¹æ³•æ˜æ˜¾æ›´é•¿ã€‚ä¸ºäº†åœ¨æ›´å¹¿æ³›çš„ç ”ç©¶å’Œä¸´åºŠç¯å¢ƒä¸­é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œåº”è®¾è®¡èƒ½åœ¨å¸¸è§ä¸­çº§ç¡¬ä»¶ä¸Šåˆç†è¿è¡Œçš„æ–¹æ³•ã€‚è™½ç„¶å·²æœ‰é’ˆå¯¹å›¾åƒé…å‡†çš„å¿«é€Ÿæ–¹æ³•è¢«æå‡ºï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€é‡‡ç”¨åŸºäºè¡¥ä¸çš„æ–¹å¼ï¼Œå¯¹äºé«˜åº¦åŠ¨æ€çš„ç»„ç»‡ï¼ˆå¦‚å¿ƒè„ï¼‰çš„é…å‡†ç²¾åº¦äº§ç”Ÿå½±å“ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºé‡åŒ–å¿ƒè„åº”å˜çš„å¿«é€Ÿä½“ç§¯é…å‡†æ¨¡å‹ã€‚æ‰€æå‡ºæ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œï¼ˆDLNNï¼‰çš„è®¾è®¡æ—¨åœ¨åˆ©ç”¨ä¸€ç§å¯ä»¥éå¸¸é«˜æ•ˆåœ°è¿›è¡Œå·ç§¯çš„æ¶æ„ï¼Œä½¿è¯¥æ¨¡å‹åœ¨æ¨æ–­æ—¶é—´ä»…éœ€ä¸€å°éƒ¨åˆ†å°±èƒ½è¾¾åˆ°ä¸å…¶ä»–æœ€æ–°æ¨¡å‹ç±»ä¼¼çš„é…å‡†ä¿çœŸåº¦ã€‚æ‰€æå‡ºå¿«é€Ÿè½»é‡çº§æ³¨å†Œï¼ˆFLIRï¼‰æ¨¡å‹ç”¨äºé¢„æµ‹ç»„ç»‡è¿åŠ¨ï¼Œç„¶åç”¨äºé‡åŒ–ç»„ç»‡æ‰€ç»å†çš„éå‡åŒ€åº”å˜ã€‚å¯¹äºä»åŒä¸€æ‚£è€…èº«ä¸Šå¤§çº¦åœ¨åŒä¸€æ—¶é—´é‡‡é›†çš„æ•°æ®ï¼Œé¢„æœŸåœ¨é‡‡é›†æ•°æ®ä¹‹é—´æµ‹é‡çš„åº”å˜å€¼å·®å¼‚ä¼šå¾ˆå°ã€‚åˆ©ç”¨è¿™ä¸€æŒ‡æ ‡ï¼Œä½¿ç”¨FLIRæ–¹æ³•è®¡ç®—çš„åº”å˜å€¼è¡¨ç°å‡ºéå¸¸ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿã€è½»é‡çº§çš„åŒ»å­¦å›¾åƒé…å‡†æ¨¡å‹ï¼ˆFLIRæ¨¡å‹ï¼‰ï¼Œç”¨äºé‡åŒ–å¿ƒè„åº”å˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œé«˜æ•ˆè®¡ç®—å·ç§¯ï¼Œåœ¨ä¿è¯é…å‡†ç²¾åº¦çš„åŒæ—¶ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚FLIRæ¨¡å‹é¢„æµ‹çš„ç»„ç»‡çš„è¿åŠ¨è¢«ç”¨æ¥é‡åŒ–ç»„ç»‡ç»å†çš„éå‡åŒ€åº”å˜ã€‚å¯¹äºæ¥è‡ªåŒä¸€æ‚£è€…åœ¨å¤§çº¦åŒä¸€æ—¶é—´é‡‡é›†çš„é‡‡é›†ç‰©ï¼Œä½¿ç”¨FLIRæ–¹æ³•è®¡ç®—çš„åº”å˜å€¼è¡¨ç°å‡ºéå¸¸ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒé…å‡†åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­æœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚å¿ƒè„å›¾åƒç»„ç»‡è¿åŠ¨è·Ÿè¸ªï¼Œå¯åæ˜ ç»„ç»‡å¥åº·çŠ¶å†µã€‚</li>
<li>é…å‡†å¯¹äºæ·±åº¦å­¦ä¹ ç®—æ³•æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºéš¾ä»¥åˆ›å»ºçœŸå®çš„è½¬æ¢ï¼Œå¹¶ä¸”å¯èƒ½å­˜åœ¨å¤šç§äº§ç”Ÿä¸ç›®æ ‡å›¾åƒç›¸å…³çš„è½¬æ¢ã€‚</li>
<li>ç›®å‰å­˜åœ¨ä¸€äº›æ— ç›‘ç£æ–¹æ³•é¢„æµ‹æœ‰æ•ˆè½¬æ¢ï¼Œä½†é¢„æµ‹æ—¶é—´è¾ƒé•¿ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿã€è½»é‡çº§çš„é…å‡†æ¨¡å‹ï¼ˆFLIRï¼‰ï¼Œæ—¨åœ¨å®ç°åœ¨ä¸­ç­‰ç¡¬ä»¶ä¸Šçš„åˆç†è¿è¡Œæ—¶é—´ã€‚</li>
<li>FLIRæ¨¡å‹é‡‡ç”¨é«˜æ•ˆå·ç§¯è®¡ç®—ï¼Œå®ç°ä¸æœ€æ–°æ¨¡å‹ç›¸ä¼¼çš„é…å‡†ç²¾åº¦ï¼ŒåŒæ—¶æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
<li>FLIRæ¨¡å‹é¢„æµ‹ç»„ç»‡è¿åŠ¨ï¼Œç”¨äºé‡åŒ–ç»„ç»‡çš„éå‡åŒ€åº”å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf6fdae7850ca29e81facd9cf54e4768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab684c1bf50a5dcc29d72644d4793e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f0b91a2b0e2ec25d73708a7691eb757.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification"><a href="#LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification" class="headerlink" title="LKA: Large Kernel Adapter for Enhanced Medical Image Classification"></a>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</h2><p><strong>Authors:Ziquan Zhu, Si-Yuan Lu, Tianjin Huang, Lu Liu, Zhe Liu</strong></p>
<p>Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets. </p>
<blockquote>
<p>å°½ç®¡å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå®ƒä»¬åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœå´æœªèƒ½è¾¾åˆ°é¢„æœŸã€‚è¿™ä¸€å±€é™æ€§æºäºä¸¤ä¸ªå…³é”®å› ç´ ï¼šï¼ˆ1ï¼‰åŒ»å­¦å›¾åƒè¡¨ç°å‡ºå¹¿æ³›çš„è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡æ¥æ•æ‰å…³é”®ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ç°æœ‰çš„PEFTæ–¹æ³•æ²¡æœ‰æ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºçš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶æ‰©å¤§æ„Ÿå—é‡ã€‚æ‰€æå‡ºçš„LKAç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å‹å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚é€šè¿‡å¯¹å„ç§æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ›´å¤§çš„å†…æ ¸å°ºå¯¸å¯¹äºå¢å¼ºé¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºçš„å¤§å‹å†…æ ¸é€‚é…å™¨LKAåœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å‰ä¸€ç§ç²¾åº¦å‡†ç¡®ç‡æ–¹é¢ä¼˜äºå¸¸è§çš„PEFTæ–¹æ³•ä¸­çš„åä¸€æ¡å¸¸è§„æ ‡å‡†æµç¨‹ä¸”è¶…è¿‡äº†å½“å‰æœ€ä½³æ°´å¹³3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19118v1">PDF</a> 8 pages, 3 figures, MICCAI</p>
<p><strong>Summary</strong><br>     å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¤šé¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœä¸å°½å¦‚äººæ„ã€‚ä¸»è¦ç”±äºåŒ»å­¦å›¾åƒå­˜åœ¨è§£å‰–ç»“æ„å˜å¼‚å¤§ã€å¯¹æ¯”åº¦ä½çš„é—®é¢˜ï¼Œéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡æ¥æ•æ‰å…³é”®ç‰¹å¾ï¼Œè€Œç°æœ‰PEFTæ–¹æ³•å¹¶æœªæ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºé—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºLarge Kernel Adapterï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨æ‰©å¤§æ„Ÿå—é‡çš„åŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œæ›´å¤§çš„å·ç§¯æ ¸å°ºå¯¸å¯¹æå‡é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚LKAä¼˜äº11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œåœ¨äº”å¥—åŒ»å­¦æ•°æ®é›†ä¸Šæœ€é«˜ç²¾åº¦è¶…è¿‡ç°æœ‰æœ€ä½³æ°´å¹³3.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå­˜åœ¨è§£å‰–ç»“æ„å˜å¼‚å¤§ã€å¯¹æ¯”åº¦ä½çš„é—®é¢˜ï¼Œéœ€è¦æ›´å¤§çš„æ„Ÿå—é‡æ•æ‰å…³é”®ç‰¹å¾ã€‚</li>
<li>å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœæœ‰é™ã€‚</li>
<li>LKAï¼ˆLarge Kernel Adapterï¼‰æ—¨åœ¨æ‰©å¤§æ„Ÿå—é‡ï¼ŒåŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚</li>
<li>LKAåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å‹å·ç§¯æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ›´å¤§çš„å·ç§¯æ ¸å°ºå¯¸å¯¹æå‡é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚</li>
<li>LKAåœ¨å¤šä¸ªæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äº11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ã€‚</li>
<li>LKAåœ¨äº”å¥—åŒ»å­¦æ•°æ®é›†ä¸Šçš„æœ€é«˜ç²¾åº¦è¶…è¿‡ç°æœ‰æœ€ä½³æ°´å¹³3.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-73619839500ded2ef30b7caf04d24e18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbec7e26dc8fb1af5f39c9886ac4b020.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d3822294460fec056aef4a4ced486a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55936770fbc3ceffe9ede2093420c107.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DiffRIS-Enhancing-Referring-Remote-Sensing-Image-Segmentation-with-Pre-trained-Text-to-Image-Diffusion-Models"><a href="#DiffRIS-Enhancing-Referring-Remote-Sensing-Image-Segmentation-with-Pre-trained-Text-to-Image-Diffusion-Models" class="headerlink" title="DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with   Pre-trained Text-to-Image Diffusion Models"></a>DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with   Pre-trained Text-to-Image Diffusion Models</h2><p><strong>Authors:Zhe Dong, Yuzhe Sun, Tianzhu Liu, Yanfeng Gu</strong></p>
<p>Referring remote sensing image segmentation (RRSIS) enables the precise delineation of regions within remote sensing imagery through natural language descriptions, serving critical applications in disaster response, urban development, and environmental monitoring. Despite recent advances, current approaches face significant challenges in processing aerial imagery due to complex object characteristics including scale variations, diverse orientations, and semantic ambiguities inherent to the overhead perspective. To address these limitations, we propose DiffRIS, a novel framework that harnesses the semantic understanding capabilities of pre-trained text-to-image diffusion models for enhanced cross-modal alignment in RRSIS tasks. Our framework introduces two key innovations: a context perception adapter (CP-adapter) that dynamically refines linguistic features through global context modeling and object-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD) that iteratively aligns textual descriptions with visual regions for precise segmentation. The CP-adapter bridges the domain gap between general vision-language understanding and remote sensing applications, while PCMRD enables fine-grained semantic alignment through multi-scale feature interaction. Comprehensive experiments on three benchmark datasets-RRSIS-D, RefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms existing methods across all standard metrics, establishing a new state-of-the-art for RRSIS tasks. The significant performance improvements validate the effectiveness of leveraging pre-trained diffusion models for remote sensing applications through our proposed adaptive framework. </p>
<blockquote>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç²¾ç¡®åœ°ç•Œå®šé¥æ„Ÿå›¾åƒä¸­çš„åŒºåŸŸï¼Œåœ¨ç¾å®³å“åº”ã€åŸå¸‚å‘å±•å’Œç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚å°½ç®¡æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºå¯¹è±¡ç‰¹å¾çš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬å°ºåº¦å˜åŒ–ã€æ–¹å‘å¤šæ ·ä»¥åŠå¤´é¡¶è§†è§’æ‰€å›ºæœ‰çš„è¯­ä¹‰æ¨¡ç³Šæ€§ï¼Œå½“å‰æ–¹æ³•åœ¨å¤„ç†èˆªç©ºå›¾åƒæ—¶ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DiffRISè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå¢å¼ºRRSISä»»åŠ¡çš„è·¨æ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€ä¸ªæ˜¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‚é…å™¨ï¼ˆCP-adapterï¼‰ï¼Œå®ƒé€šè¿‡å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå¯¹è±¡æ„ŸçŸ¥æ¨ç†æ¥åŠ¨æ€ä¼˜åŒ–è¯­è¨€ç‰¹å¾ï¼›å¦ä¸€ä¸ªæ˜¯æ¸è¿›å¼è·¨æ¨¡æ€æ¨ç†è§£ç å™¨ï¼ˆPCMRDï¼‰ï¼Œå®ƒé€šè¿‡è¿­ä»£å°†æ–‡æœ¬æè¿°ä¸è§†è§‰åŒºåŸŸå¯¹é½ï¼Œä»¥å®ç°ç²¾ç¡®åˆ†å‰²ã€‚CP-adapterç¼©å°äº†é€šç”¨è§†è§‰è¯­è¨€ç†è§£ä¸é¥æ„Ÿåº”ç”¨ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè€ŒPCMRDåˆ™é€šè¿‡å¤šå°ºåº¦ç‰¹å¾äº¤äº’å®ç°äº†ç²¾ç»†çš„è¯­ä¹‰å¯¹é½ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†RRSIS-Dã€RefSegRSå’ŒRISBenchä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDiffRISåœ¨æ‰€æœ‰æ ‡å‡†æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºRRSISä»»åŠ¡å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›éªŒè¯äº†é€šè¿‡æˆ‘ä»¬æå‡ºçš„è‡ªé€‚åº”æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œé¥æ„Ÿåº”ç”¨çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶DiffRISï¼Œç”¨äºè§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å¯¹é½é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŠ¨æ€è°ƒæ•´è¯­è¨€ç‰¹å¾çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‚é…å™¨ï¼ˆCP-adapterï¼‰å’Œæ¸è¿›å¼è·¨æ¨¡æ€æ¨ç†è§£ç å™¨ï¼ˆPCMRDï¼‰ã€‚CP-adapterèƒ½å¤Ÿç¼©å°é€šç”¨è§†è§‰è¯­è¨€ç†è§£ä¸é¥æ„Ÿåº”ç”¨ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè€ŒPCMRDåˆ™é€šè¿‡å¤šå°ºåº¦ç‰¹å¾äº¤äº’å®ç°ç²¾ç»†è¯­ä¹‰å¯¹é½ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffRISåœ¨æ ‡å‡†åº¦é‡æŒ‡æ ‡ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºé¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffRISæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¢å¼ºé¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>å¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‚é…å™¨ï¼ˆCP-adapterï¼‰ä»¥åŠ¨æ€è°ƒæ•´è¯­è¨€ç‰¹å¾ï¼Œå¹¶é€šè¿‡å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå¯¹è±¡æ„ŸçŸ¥æ¨ç†å®ç°ç²¾ç¡®åˆ†å‰²ã€‚</li>
<li>æ¸è¿›å¼è·¨æ¨¡æ€æ¨ç†è§£ç å™¨ï¼ˆPCMRDï¼‰é€šè¿‡è¿­ä»£å°†æ–‡æœ¬æè¿°ä¸è§†è§‰åŒºåŸŸå¯¹é½ï¼Œä»¥å®ç°ç²¾å‡†åˆ†å‰²ã€‚</li>
<li>CP-adapterç¼©å°äº†é€šç”¨è§†è§‰è¯­è¨€ç†è§£ä¸é¥æ„Ÿåº”ç”¨ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚</li>
<li>PCMRDé€šè¿‡å¤šå°ºåº¦ç‰¹å¾äº¤äº’å®ç°ç²¾ç»†è¯­ä¹‰å¯¹é½ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffRISæ¡†æ¶åœ¨æ ‡å‡†åº¦é‡æŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4304fa1e0185ddafb5a45c54c71b696e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b46a0b7fb7c7b05240268e60cd7fea14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2a557d5dce46c382918c9049b05cb90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92d1796582168054c75b92faef57085f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9aa1e3036fa697b8b6a9cdece5008ac.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VMRA-MaR-An-Asymmetry-Aware-Temporal-Framework-for-Longitudinal-Breast-Cancer-Risk-Prediction"><a href="#VMRA-MaR-An-Asymmetry-Aware-Temporal-Framework-for-Longitudinal-Breast-Cancer-Risk-Prediction" class="headerlink" title="VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast   Cancer Risk Prediction"></a>VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast   Cancer Risk Prediction</h2><p><strong>Authors:Zijun Sun, Solveig Thrun, Michael Kampffmeyer</strong></p>
<p>Breast cancer remains a leading cause of mortality worldwide and is typically detected via screening programs where healthy people are invited in regular intervals. Automated risk prediction approaches have the potential to improve this process by facilitating dynamically screening of high-risk groups. While most models focus solely on the most recent screening, there is growing interest in exploiting temporal information to capture evolving trends in breast tissue, as inspired by clinical practice. Early methods typically relied on two time steps, and although recent efforts have extended this to multiple time steps using Transformer architectures, challenges remain in fully harnessing the rich temporal dynamics inherent in longitudinal imaging data. In this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. To further enhance our approach, we incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector (SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant bilateral differences. This integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Mortal-Suen/VMRA-MaR.git">https://github.com/Mortal-Suen/VMRA-MaR.git</a>. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒä¸»è¦çš„è‡´æ­»åŸå› ä¹‹ä¸€ï¼Œé€šå¸¸é€šè¿‡ç­›æŸ¥è®¡åˆ’ï¼ˆå®šæœŸé‚€è¯·å¥åº·äººç¾¤å‚ä¸ï¼‰æ¥æ£€æµ‹ã€‚è‡ªåŠ¨åŒ–é£é™©é¢„æµ‹æ–¹æ³•æœ‰æœ›é€šè¿‡åŠ¨æ€ç­›é€‰é«˜é£é™©ç¾¤ä½“æ¥æ”¹å–„è¿™ä¸€æµç¨‹ã€‚è™½ç„¶å¤§å¤šæ•°æ¨¡å‹åªå…³æ³¨æœ€è¿‘çš„ç­›æŸ¥ç»“æœï¼Œä½†ä¸´åºŠä¸Šè¶Šæ¥è¶Šæœ‰å…´è¶£åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ¥æ•æ‰ä¹³è…ºç»„ç»‡çš„æ¼”å˜è¶‹åŠ¿ã€‚æ—©æœŸçš„æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸¤ä¸ªæ—¶é—´æ­¥éª¤ï¼Œå°½ç®¡æœ€è¿‘çš„ç ”ç©¶åŠªåŠ›ä½¿ç”¨Transformeræ¶æ„å°†å…¶æ‰©å±•åˆ°å¤šä¸ªæ—¶é—´æ­¥éª¤ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå³æ— æ³•å……åˆ†åˆ©ç”¨çºµå‘æˆåƒæ•°æ®ä¸­çš„ä¸°å¯Œæ—¶é—´åŠ¨æ€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¸¦æœ‰çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„Vision Mamba RNNï¼ˆVMRNNï¼‰ä»¥åŠLSTMç±»è®°å¿†æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆæ•æ‰ä¹³è…ºç»„ç»‡æ¼”å˜çš„ç»†å¾®è¶‹åŠ¿ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªä¸å¯¹ç§°æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ç©ºé—´ä¸å¯¹ç§°æ£€æµ‹å™¨ï¼ˆSADï¼‰å’Œçºµå‘ä¸å¯¹ç§°è·Ÿè¸ªå™¨ï¼ˆLATï¼‰æ¥è¯†åˆ«ä¸´åºŠä¸Šç›¸å…³çš„åŒä¾§å·®å¼‚ã€‚è¿™ä¸€ç»¼åˆæ¡†æ¶åœ¨é¢„æµ‹ç™Œç—‡å‘ç”Ÿæ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›æ•ˆæœï¼Œç‰¹åˆ«æ˜¯å¯¹é«˜å¯†åº¦ä¹³è…ºç—…ä¾‹æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”åœ¨è¾ƒé•¿æ—¶é—´ç‚¹ï¼ˆç¬¬å››å¹´å’Œç¬¬äº”å¹´ï¼‰å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¿™çªæ˜¾äº†å…¶æ¨åŠ¨æ—©æœŸä¹³è…ºç™Œè¯†åˆ«å¹¶å¯ç”¨æ›´ä¸ªæ€§åŒ–çš„ç­›æŸ¥ç­–ç•¥çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mortal-Suen/VMRA-MaR.git">https://github.com/Mortal-Suen/VMRA-MaR.git</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17412v1">PDF</a> MICCAI 2025, Provisional Accept</p>
<p><strong>Summary</strong><br>ä¹³è…ºç™Œä»æ˜¯å…¨çƒä¸»è¦çš„è‡´æ­»åŸå› ï¼Œé€šè¿‡ç­›æŸ¥ç¨‹åºæ£€æµ‹ã€‚è‡ªåŠ¨åŒ–é£é™©é¢„æµ‹æ–¹æ³•å¯é€šè¿‡åŠ¨æ€ç­›é€‰é«˜é£é™©ç¾¤ä½“æ”¹è¿›æ£€æµ‹è¿‡ç¨‹ã€‚è™½ç„¶å¤šæ•°æ¨¡å‹é›†ä¸­äºæœ€æ–°ç­›æŸ¥ï¼Œä½†ä¸´åºŠå®è·µä¸­å­˜åœ¨åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ•æ‰ä¹³è…ºç»„ç»‡æ¼”å˜è¶‹åŠ¿çš„æ—¥ç›Šå¢é•¿çš„å…´è¶£ã€‚æ—©æœŸæ–¹æ³•é€šå¸¸ä¾èµ–äºä¸¤ä¸ªæ—¶é—´æ­¥éª¤ï¼Œå°½ç®¡è¿‘æœŸåŠªåŠ›å·²å°†å…¶æ‰©å±•åˆ°å¤šä¸ªæ—¶é—´æ­¥éª¤ï¼Œä½¿ç”¨Transformeræ¶æ„ï¼Œä½†ä»æœ‰æŒ‘æˆ˜åœ¨äºå®Œå…¨åˆ©ç”¨çºµå‘æˆåƒæ•°æ®çš„ä¸°å¯Œæ—¶é—´åŠ¨æ€ã€‚æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨å¸¦æœ‰çŠ¶æ€ç©ºé—´æ¨¡å‹çš„Vision Mamba RNNï¼ˆVMRNNï¼‰å’ŒLSTMç±»è®°å¿†æœºåˆ¶æ•æ‰ä¹³è…ºç»„ç»‡æ¼”å˜è¶‹åŠ¿çš„å¾®å¦™å˜åŒ–ã€‚ä¸ºè¿›ä¸€æ­¥æ”¹è¿›æ–¹æ³•ï¼Œç»“åˆä¸å¯¹ç§°æ¨¡å—ï¼Œåˆ©ç”¨ç©ºé—´ä¸å¯¹ç§°æ£€æµ‹å™¨ï¼ˆSADï¼‰å’Œçºµå‘ä¸å¯¹ç§°è·Ÿè¸ªå™¨ï¼ˆLATï¼‰è¯†åˆ«ä¸´åºŠç›¸å…³çš„åŒä¾§å·®å¼‚ã€‚è¯¥ç»¼åˆæ¡†æ¶åœ¨é¢„æµ‹ç™Œç—‡å‘ç”Ÿæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¯†åº¦è¾ƒé«˜çš„ä¹³è…ºç—…ä¾‹ï¼Œä¸”åœ¨é•¿æœŸæ—¶é—´ç‚¹ï¼ˆç¬¬å››å’Œäº”å¹´ï¼‰è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡¸æ˜¾å…¶æ¨åŠ¨æ—©æœŸä¹³è…ºç™Œè¯†åˆ«å’Œä¸ªæ€§åŒ–ç­›æŸ¥ç­–ç•¥çš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Mortal-Suen/VMRA-MaR.git%E3%80%82">https://github.com/Mortal-Suen/VMRA-MaR.gitã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œè‡ªåŠ¨åŒ–é£é™©é¢„æµ‹æ–¹æ³•å¯ä»¥æ”¹è¿›æ£€æµ‹è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨çºµå‘æ•°æ®çš„æ—¶é—´ä¿¡æ¯æ˜¯ä¸´åºŠå®è·µä¸­æ—¥ç›Šå…³æ³¨çš„é—®é¢˜ã€‚</li>
<li>Vision Mamba RNNï¼ˆVMRNNï¼‰ç»“åˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å’ŒLSTMç±»è®°å¿†æœºåˆ¶èƒ½æœ‰æ•ˆæ•æ‰ä¹³è…ºç»„ç»‡æ¼”å˜çš„å¾®å¦™è¶‹åŠ¿ã€‚</li>
<li>é€šè¿‡ç»“åˆä¸å¯¹ç§°æ¨¡å—å’Œç©ºé—´ä¸å¯¹ç§°æ£€æµ‹å™¨ï¼ˆSADï¼‰åŠçºµå‘ä¸å¯¹ç§°è·Ÿè¸ªå™¨ï¼ˆLATï¼‰ï¼Œå¯è¯†åˆ«åŒä¾§å·®å¼‚ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨é«˜å¯†åº¦ä¹³è…ºç—…ä¾‹çš„ç™Œç—‡é¢„æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é•¿æœŸæ—¶é—´ç‚¹å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68854a6c9a06ed4e7c167c9693ae16bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a345a301228643d22670f3518700b55.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TextBraTS-Text-Guided-Volumetric-Brain-Tumor-Segmentation-with-Innovative-Dataset-Development-and-Fusion-Module-Exploration"><a href="#TextBraTS-Text-Guided-Volumetric-Brain-Tumor-Segmentation-with-Innovative-Dataset-Development-and-Fusion-Module-Exploration" class="headerlink" title="TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with   Innovative Dataset Development and Fusion Module Exploration"></a>TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with   Innovative Dataset Development and Fusion Module Exploration</h2><p><strong>Authors:Xiaoyu Shi, Rahul Kumar Jain, Yinhao Li, Ruibo Hou, Jingliang Cheng, Jie Bai, Guohua Zhao, Lanfen Lin, Rui Xu, Yen-wei Chen</strong></p>
<p>Deep learning has demonstrated remarkable success in medical image segmentation and computer-aided diagnosis. In particular, numerous advanced methods have achieved state-of-the-art performance in brain tumor segmentation from MRI scans. While recent studies in other medical imaging domains have revealed that integrating textual reports with visual data can enhance segmentation accuracy, the field of brain tumor analysis lacks a comprehensive dataset that combines radiological images with corresponding textual annotations. This limitation has hindered the exploration of multimodal approaches that leverage both imaging and textual data.   To bridge this critical gap, we introduce the TextBraTS dataset, the first publicly available volume-level multimodal dataset that contains paired MRI volumes and rich textual annotations, derived from the widely adopted BraTS2020 benchmark. Building upon this novel dataset, we propose a novel baseline framework and sequential cross-attention method for text-guided volumetric medical image segmentation. Through extensive experiments with various text-image fusion strategies and templated text formulations, our approach demonstrates significant improvements in brain tumor segmentation accuracy, offering valuable insights into effective multimodal integration techniques.   Our dataset, implementation code, and pre-trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS">https://github.com/Jupitern52/TextBraTS</a>. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç‰¹åˆ«æ˜¯ï¼Œè®¸å¤šå…ˆè¿›çš„æ–¹æ³•åœ¨MRIæ‰«æçš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚è™½ç„¶å…¶ä»–åŒ»å­¦å½±åƒé¢†åŸŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ–‡æœ¬æŠ¥å‘Šä¸è§†è§‰æ•°æ®ç›¸ç»“åˆå¯ä»¥æé«˜åˆ†å‰²ç²¾åº¦ï¼Œä½†è„‘è‚¿ç˜¤åˆ†æé¢†åŸŸç¼ºä¹ä¸€ä¸ªç»“åˆäº†æ”¾å°„å›¾åƒå’Œç›¸åº”æ–‡æœ¬æ³¨é‡Šçš„ç»¼åˆæ•°æ®é›†ã€‚è¿™ä¸€å±€é™æ€§é˜»ç¢äº†åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬æ•°æ®çš„å¤šæ¨¡å¼æ–¹æ³•çš„æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TextBraTSæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ä½“ç§¯çº§å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹çš„MRIä½“ç§¯å’Œä¸°å¯Œçš„æ–‡æœ¬æ³¨é‡Šï¼Œè¿™äº›æ³¨é‡Šæ¥æºäºå¹¿æ³›é‡‡ç”¨çš„BraTS2020åŸºå‡†æµ‹è¯•ã€‚åŸºäºè¿™ä¸€æ–°å‹æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºçº¿æ¡†æ¶å’Œé¡ºåºäº¤å‰æ³¨æ„æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬å¼•å¯¼çš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡å¯¹å„ç§æ–‡æœ¬å›¾åƒèåˆç­–ç•¥å’Œæ¨¡æ¿æ–‡æœ¬æ ¼å¼çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ç²¾åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æé«˜ï¼Œä¸ºæœ‰æ•ˆçš„å¤šæ¨¡å¼é›†æˆæŠ€æœ¯æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€å®ç°ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jupitern52/TextBraTS%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Jupitern52/TextBraTSå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16784v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸæ·±åº¦å­¦ä¹ å–å¾—æ˜¾è‘—æˆå°±ï¼Œå°¤å…¶åœ¨è„‘è‚¿ç˜¤åˆ†å‰²æ–¹é¢ã€‚ä¸ºæé«˜åˆ†å‰²ç²¾åº¦ï¼Œæœ‰ç ”ç©¶èåˆè§†è§‰æ•°æ®ä¸æ–‡æœ¬æŠ¥å‘Šã€‚ç„¶è€Œï¼Œè„‘è‚¿ç˜¤åˆ†æé¢†åŸŸç¼ºä¹ç»¼åˆæ•°æ®é›†ï¼Œç»“åˆå½±åƒå­¦å›¾åƒä¸ç›¸åº”æ–‡æœ¬æ³¨é‡Šã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ¨å‡ºTextBraTSæ•°æ®é›†ï¼ŒåŒ…å«MRIä½“ç§¯æ•°æ®ä¸ä¸°å¯Œæ–‡æœ¬æ³¨é‡Šã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæå‡ºåŸºçº¿æ¡†æ¶ä¸é¡ºåºäº¤å‰æ³¨æ„æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬å¼•å¯¼çš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜è„‘è‚¿ç˜¤åˆ†å‰²ç²¾åº¦ï¼Œä¸ºæœ‰æ•ˆå¤šæ¨¡å¼èåˆæŠ€æœ¯æä¾›æœ‰ä»·å€¼è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²å’Œè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­å–å¾—æ˜¾è‘—æˆå°±ã€‚</li>
<li>è„‘è‚¿ç˜¤åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒé¢†åŸŸçš„çƒ­ç‚¹ç ”ç©¶è¯¾é¢˜ã€‚</li>
<li>èåˆè§†è§‰æ•°æ®å’Œæ–‡æœ¬æŠ¥å‘Šèƒ½æé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>å½“å‰ç¼ºä¹åŒ…å«å½±åƒå­¦å›¾åƒå’Œç›¸åº”æ–‡æœ¬æ³¨é‡Šçš„è„‘è‚¿ç˜¤åˆ†æç»¼åˆæ•°æ®é›†ã€‚</li>
<li>TextBraTSæ•°æ®é›†æ˜¯é¦–ä¸ªå…¬å¼€çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«MRIä½“ç§¯æ•°æ®å’Œæ–‡æœ¬æ³¨é‡Šã€‚</li>
<li>æå‡ºåŸºçº¿æ¡†æ¶å’Œé¡ºåºäº¤å‰æ³¨æ„æ–¹æ³•ï¼Œåˆ©ç”¨TextBraTSæ•°æ®é›†è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„åŒ»å­¦å›¾åƒä½“ç§¯åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd8b8584ba29cfc9b4943a764306a40b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5421326fcb118fa6bd8d28b3ca467cb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cae5c881a345cbcaaa933d435588a7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7712b6c06345e13a816d1edf847b4c03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19ff5c0f0a832197dc212f6468ec549d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d305b3dca5faa38fae39cd04a952d1a4.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  Tailored Conversations beyond LLMs A RL-Based Dialogue Manager
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cfa578afea188c79431b0ace2e72e5f.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  NAADA A Noise-Aware Attention Denoising Autoencoder for Dental   Panoramic Radiographs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
