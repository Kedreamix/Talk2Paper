<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  MAM Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fe150bde0113af265eac13e528652ce5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-26-æ›´æ–°"><a href="#2025-06-26-æ›´æ–°" class="headerlink" title="2025-06-26 æ›´æ–°"></a>2025-06-26 æ›´æ–°</h1><h2 id="MAM-Modular-Multi-Agent-Framework-for-Multi-Modal-Medical-Diagnosis-via-Role-Specialized-Collaboration"><a href="#MAM-Modular-Multi-Agent-Framework-for-Multi-Modal-Medical-Diagnosis-via-Role-Specialized-Collaboration" class="headerlink" title="MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration"></a>MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration</h2><p><strong>Authors:Yucheng Zhou, Lingran Song, Jianbing Shen</strong></p>
<p>Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/yczhou001/MAM">https://github.com/yczhou001/MAM</a>. </p>
<blockquote>
<p>åŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘çš„è¿›å±•å±•ç¤ºå‡ºäº†å®ƒä»¬å¼ºå¤§çš„æ¨ç†å’Œè¯Šæ–­èƒ½åŠ›ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†å½“å‰çš„ç»Ÿä¸€å¤šæ¨¡å¼åŒ»ç–—LLMåœ¨çŸ¥è¯†æ›´æ–°æˆæœ¬ã€å…¨é¢æ€§å’Œçµæ´»æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºå¤šæ¨¡å¼åŒ»ç–—è¯Šæ–­çš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMAMï¼‰ã€‚å—æˆ‘ä»¬å®è¯ç ”ç©¶çš„å¯å‘ï¼Œè¯¥å®è¯ç ”ç©¶çªå‡ºäº†è§’è‰²åˆ†é…å’Œè¯Šæ–­é‰´åˆ«åœ¨LLMä¸­çš„ä¼˜åŠ¿ï¼ŒMAMå°†åŒ»ç–—è¯Šæ–­è¿‡ç¨‹åˆ†è§£ä¸ºä¸“ä¸šè§’è‰²ï¼šæ™®é€šç§‘åŒ»ç”Ÿã€ä¸“å®¶å›¢é˜Ÿã€æ”¾å°„ç§‘åŒ»ç”Ÿã€åŒ»ç–—åŠ©ç†å’Œä¸»ä»»ï¼Œæ¯ä¸ªè§’è‰²å‡ç”±åŸºäºLLMçš„æ™ºèƒ½ä½“æ„æˆã€‚è¿™ç§æ¨¡å—åŒ–å’Œåä½œçš„æ¡†æ¶èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„çŸ¥è¯†æ›´æ–°ï¼Œå¹¶å……åˆ†åˆ©ç”¨ç°æœ‰çš„åŒ»ç–—LLMå’ŒçŸ¥è¯†åº“ã€‚åœ¨åŒ…å«æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡å¼çš„å…¬å¼€å¯è®¿é—®çš„å¤šæ¨¡å¼åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMAMçš„æ€§èƒ½å§‹ç»ˆè¶…è¿‡äº†ç‰¹å®šæ¨¡å¼çš„LLMã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒMAMå®ç°äº†ä»18%åˆ°365%çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/yczhou001/MAM%E3%80%82">https://github.com/yczhou001/MAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19835v1">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸåŒ»ç–—é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œè¯Šæ–­èƒ½åŠ›ï¼Œä½†ç»Ÿä¸€çš„å¤šæ¨¡æ€åŒ»ç–—LLMåœ¨çŸ¥è¯†æ›´æ–°æˆæœ¬ã€å…¨é¢æ€§å’Œçµæ´»æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆMAMï¼‰ï¼Œå°†åŒ»ç–—è¯Šæ–­è¿‡ç¨‹åˆ†è§£ä¸ºä¸åŒè§’è‰²ï¼ŒåŒ…æ‹¬å…¨ç§‘åŒ»ç”Ÿã€ä¸“å®¶å›¢é˜Ÿã€æ”¾å°„ç§‘åŒ»ç”Ÿç­‰ï¼Œæ¯ä¸ªè§’è‰²ç”±LLMæ™ºèƒ½ä½“æ‰®æ¼”ã€‚è¯¥æ¡†æ¶å¯æé«˜çŸ¥è¯†æ›´æ–°æ•ˆç‡å¹¶å€ŸåŠ©ç°æœ‰åŒ»ç–—LLMå’ŒçŸ¥è¯†åº“ã€‚å®éªŒè¯æ˜ï¼ŒMAMåœ¨å¤šç§å…¬å¼€å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç‰¹å®šæ¨¡æ€çš„LLMï¼Œæ€§èƒ½æå‡å¹…åº¦ä»18%è‡³365%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—LLMå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œè¯Šæ–­èƒ½åŠ›ã€‚</li>
<li>ç»Ÿä¸€å¤šæ¨¡æ€åŒ»ç–—LLMé¢ä¸´çŸ¥è¯†æ›´æ–°æˆæœ¬ã€å…¨é¢æ€§å’Œçµæ´»æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>MAMæ¡†æ¶é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ï¼Œå°†åŒ»ç–—è¯Šæ–­è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªè§’è‰²ï¼Œæé«˜çŸ¥è¯†æ›´æ–°çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MAMåˆ©ç”¨ç°æœ‰åŒ»ç–—LLMå’ŒçŸ¥è¯†åº“ï¼Œå®ç°æ›´å…¨é¢çš„è¯Šæ–­èƒ½åŠ›ã€‚</li>
<li>MAMæ¡†æ¶åœ¨å¤šç§å…¬å¼€å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½ä¼˜äºç‰¹å®šæ¨¡æ€çš„LLMã€‚</li>
<li>MAMçš„æ€§èƒ½æå‡å¹…åº¦æ˜¾è‘—ï¼Œä»18%è‡³365%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b9c6a0c23235ace69af8d6dd4dd5c3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dccbccaef9a18cf4f368eaf89e5a4e69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3fddf03b8db99bdf9503d907ea7f746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5701d717bfa9108fe91b6b0d5ce368f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="KnowRL-Exploring-Knowledgeable-Reinforcement-Learning-for-Factuality"><a href="#KnowRL-Exploring-Knowledgeable-Reinforcement-Learning-for-Factuality" class="headerlink" title="KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"></a>KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality</h2><p><strong>Authors:Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL">https://github.com/zjunlp/KnowRL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°¤å…¶æ˜¯æ…¢æ€è€ƒæ¨¡å‹ï¼Œå¸¸å¸¸è¡¨ç°å‡ºä¸¥é‡çš„å¹»è§‰ï¼Œç”±äºæ¨ç†è¿‡ç¨‹ä¸­æ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œä»è€Œè¾“å‡ºé”™è¯¯çš„å†…å®¹ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æå‡å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä»¥ç»“æœä¸ºå¯¼å‘çš„å¥–åŠ±æœºåˆ¶å¾€å¾€ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„çœŸå®ç›‘ç£ï¼Œä»è€ŒåŠ å‰§äº†å¹»è§‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³æ…¢æ€è€ƒæ¨¡å‹ä¸­çš„é«˜å¹»è§‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¢å¼ºå‹RLï¼Œå³KnowRLã€‚KnowRLé€šè¿‡å°†åŸºäºçŸ¥è¯†éªŒè¯çš„çœŸå®æ€§å¥–åŠ±èå…¥RLè®­ç»ƒè¿‡ç¨‹ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€è€ƒï¼Œå¸®åŠ©å®ƒä»¬è¯†åˆ«è‡ªèº«çš„çŸ¥è¯†è¾¹ç•Œã€‚KnowRLçš„ç‰¹è‰²åœ¨äºï¼Œå®ƒé’ˆå¯¹æ€§åœ°åœ¨å®é™…æ¨ç†æ­¥éª¤ä¸­å¥–åŠ±å¯¹äº‹å®çš„éµå¾ªï¼Œä»è€ŒåŸ¹å…»æ¨¡å‹æ›´å¯é çš„æ€è€ƒè¿‡ç¨‹ã€‚åœ¨ä¸‰ä¸ªå¹»è§‰è¯„ä¼°æ•°æ®é›†å’Œä¸¤ä¸ªæ¨ç†è¯„ä¼°æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLæœ‰æ•ˆå‡è½»äº†æ…¢æ€è€ƒæ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†å…¶åŸæœ‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL%E3%80%82">https://github.com/zjunlp/KnowRLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19807v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>åŸºäºçŸ¥è¯†éªŒè¯çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ä¸­çš„äº‹å®å¥–åŠ±ï¼Œå¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢é€Ÿæ€è€ƒï¼Œè¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œä»è€Œæœ‰æ•ˆç¼“è§£æ…¢é€Ÿæ€è€ƒæ¨¡å‹ä¸­çš„ä¸¥é‡å¹»è§†é—®é¢˜ã€‚è¿™ä¸€ç­–ç•¥çš„å®éªŒç»“æœè¯æ˜ï¼Œå…¶åœ¨æŠ‘åˆ¶å¹»è§†çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„å¼ºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”±äºæ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œå¸¸å¸¸å‡ºç°ä¸¥é‡å¹»è§†é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½ç„¶èƒ½æé«˜å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä»¥ç»“æœä¸ºå¯¼å‘çš„å¥–åŠ±æœºåˆ¶ç¼ºä¹è¿‡ç¨‹æ€§çš„äº‹å®ç›‘ç£ï¼Œä¼šåŠ å‰§å¹»è§†é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹»è§†é—®é¢˜ï¼Œæå‡ºç»“åˆçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•â€”â€”KnowRLã€‚</li>
<li>KnowRLé€šè¿‡å°†åŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢é€Ÿæ€è€ƒï¼Œä»è€Œå¸®åŠ©æ¨¡å‹è¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚</li>
<li>KnowRLæ–¹æ³•é€šè¿‡å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­éµå¾ªäº‹å®çš„è¡Œä¸ºï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ å¹¶å†…åŒ–åŸºäºäº‹å®çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLåœ¨æŠ‘åˆ¶å¹»è§†çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab8f299c1dd6b353f2ed6bee47c6134b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e4aea7049441064ef28fa4892d409e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-005676593173fc0a316d6d0a953cf685.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d5a070aa67b5197507e23af02899af0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1217ab4abeba55946bd602d4f51725b9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Why-Do-Open-Source-LLMs-Struggle-with-Data-Analysis-A-Systematic-Empirical-Study"><a href="#Why-Do-Open-Source-LLMs-Struggle-with-Data-Analysis-A-Systematic-Empirical-Study" class="headerlink" title="Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic   Empirical Study"></a>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic   Empirical Study</h2><p><strong>Authors:Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMsâ€™ analytical reasoning capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ•°æ®åˆ†æä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œå¼€æºæ¨¡å‹åœ¨é¢å¯¹è¿™äº›æ¨ç†å¯†é›†å‹çš„åœºæ™¯æ—¶é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æé«˜å¼€æºLLMæ•°æ®åˆ†æèƒ½åŠ›çš„ç­–ç•¥ã€‚é€šè¿‡ç­›é€‰å¤šç§å®é™…åœºæ™¯çš„ç§å­æ•°æ®é›†ï¼Œæˆ‘ä»¬ä»æ•°æ®ç†è§£ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’ä¸‰ä¸ªæ–¹é¢å¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰æˆ˜ç•¥è§„åˆ’è´¨é‡æ˜¯æ¨¡å‹æ€§èƒ½çš„ä¸»è¦å†³å®šå› ç´ ï¼›ï¼ˆ2ï¼‰äº¤äº’è®¾è®¡å’Œä»»åŠ¡å¤æ‚æ€§å¯¹æ¨ç†èƒ½åŠ›æœ‰é‡å¤§å½±å“ï¼›ï¼ˆ3ï¼‰æ•°æ®è´¨é‡åœ¨å®ç°æœ€ä½³æ€§èƒ½æ–¹é¢çš„å½±å“å¤§äºå¤šæ ·æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™äº›è§è§£å¼€å‘äº†ä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¼€æºLLMçš„åˆ†ææ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19794v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§æ•°æ®åˆ†æä¸­ï¼Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´å¤šç§èƒ½åŠ›æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é€šè¿‡ç‰¹å®šç­–ç•¥æ¥å¼ºåŒ–LLMsçš„æ•°æ®åˆ†ææ€§èƒ½ï¼Œé‡‡ç”¨ä¸€ä¸ªåŒ…æ‹¬å¤šæ ·æ€§å’ŒçœŸå®åœºæ™¯çš„ç§å­æ•°æ®é›†æ¥è¯„ä»·æ¨¡å‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°äº†ä¸‰ä¸ªå…³é”®ç‚¹ï¼šæˆ˜ç•¥è§„åˆ’çš„è´¨é‡å†³å®šæ¨¡å‹æ€§èƒ½ã€äº¤äº’è®¾è®¡å’Œä»»åŠ¡å¤æ‚æ€§å½±å“æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€æ•°æ®è´¨é‡å¯¹äºè¾¾åˆ°æœ€ä½³æ€§èƒ½æœ‰é‡è¦ä½œç”¨ã€‚åŸºäºæ­¤ï¼Œå¼€å‘äº†ä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å¼€æºLLMsçš„åˆ†ææ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆ˜ç•¥è§„åˆ’è´¨é‡æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½çš„ä¸»è¦å› ç´ ã€‚</li>
<li>äº¤äº’è®¾è®¡å’Œä»»åŠ¡å¤æ‚æ€§å¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>æ•°æ®è´¨é‡æ¯”å¤šæ ·æ€§å¯¹å®ç°æœ€ä½³æ€§èƒ½æœ‰æ›´é‡è¦çš„ä½œç”¨ã€‚</li>
<li>LLMsåœ¨æ•°æ®åˆæˆæ–¹æ³•çš„è¾…åŠ©ä¸‹ï¼Œå¯ä»¥æå‡å…¶åˆ†ææ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç‰¹å®šçš„ç§å­æ•°æ®é›†å¯ä»¥è¯„ä»·æ¨¡å‹åœ¨æ•°æ®ç†è§£ã€ä»£ç ç”Ÿæˆå’Œæˆ˜ç•¥è§„åˆ’ä¸‰ä¸ªæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶ä¸­å¼ºè°ƒçœŸå®åœºæ™¯å’Œæ•°æ®å¤šæ ·æ€§çš„é‡è¦æ€§ä»¥æå‡æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-271d21ca5f6a8ca3d4a8d576eac366d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828440384e7fbcdbe9f1fdb27971e3e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef67cc7deb0b65d9a9679db8478a9895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa759edae6ff5c88807530cf38df4ebe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be01f3179bbdec5504aa6b674a95cbf9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SAGE-Strategy-Adaptive-Generation-Engine-for-Query-Rewriting"><a href="#SAGE-Strategy-Adaptive-Generation-Engine-for-Query-Rewriting" class="headerlink" title="SAGE: Strategy-Adaptive Generation Engine for Query Rewriting"></a>SAGE: Strategy-Adaptive Generation Engine for Query Rewriting</h2><p><strong>Authors:Teng Wang, Hailei Gong, Changwang Zhang, Jun Wang</strong></p>
<p>Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems. </p>
<blockquote>
<p>æŸ¥è¯¢æ”¹å†™å¯¹äºæé«˜å¯†é›†æ£€ç´¢è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•éœ€è¦å¤§é‡ç›‘ç£æ•°æ®ï¼Œæˆ–è€…é¢ä¸´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šï¼Œä½¿ç”¨ä¸€å¥—ç®€æ´çš„ä¸“å®¶ç­–ç•¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚è¯­ä¹‰æ‰©å±•å’Œå®ä½“æ¶ˆæ­§ï¼Œå¯ä»¥å¤§å¤§æé«˜åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•é›†ä¸Šçš„æ£€ç´¢æ•ˆæœï¼ŒåŒ…æ‹¬HotpotQAã€FEVERã€NFCorpuså’ŒSciFactã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç­–ç•¥è‡ªé€‚åº”ç”Ÿæˆå¼•æ“ï¼ˆSAGEï¼‰ï¼Œè¯¥å¼•æ“åœ¨RLæ¡†æ¶ä¸­å®æ–½è¿™äº›ç­–ç•¥ã€‚SAGEå¼•å…¥ä¸¤ç§æ–°å‹å¥–åŠ±å¡‘é€ æœºåˆ¶â€”â€”æˆ˜ç•¥ä¿¡ç”¨å¡‘é€ ï¼ˆSCSï¼‰å’Œå¯¹æ¯”å¥–åŠ±å¡‘é€ ï¼ˆCRSï¼‰ï¼Œä»¥æä¾›æ›´å…·ä¿¡æ¯é‡çš„å­¦ä¹ ä¿¡å·ã€‚è¿™ç§ç­–ç•¥æŒ‡å¯¼çš„æ–¹æ³•ä¸ä»…å®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„NDCG@10ç»“æœï¼Œè¿˜æ­ç¤ºäº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ–°å…´è¡Œä¸ºï¼šä»£ç†å­¦ä¼šé€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„æ¢ç´¢ï¼Œå¹¶äº§ç”Ÿç®€æ´çš„æ”¹å†™ï¼Œé™ä½æ¨ç†æˆæœ¬ï¼Œè€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»¥ç­–ç•¥ä¸ºæŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¾®å¦™çš„å¥–åŠ±å¡‘é€ å¢å¼ºï¼Œä¸ºå¼€å‘ä¸‹ä¸€ä»£ç¨³å¥çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆä¸”æ›´å…·è§£é‡Šæ€§çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æŸ¥è¯¢é‡å†™å¯¹äºå¢å¼ºå¯†é›†æ£€ç´¢çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå½“å‰æ–¹æ³•éœ€è¦å¤§é‡ç›‘ç£æ•°æ®æˆ–é¢ä¸´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç°ï¼Œåˆ©ç”¨ç®€æ´çš„ä¸“å®¶ç­–ç•¥ï¼Œå¦‚è¯­ä¹‰æ‰©å±•å’Œå®ä½“æ¶ˆæ­§ï¼Œå¯ä»¥æ˜¾è‘—æé«˜åœ¨HotpotQAã€FEVERã€NFCorpuså’ŒSciFactç­‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸Šçš„æ£€ç´¢æ•ˆæœã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ç­–ç•¥è‡ªé€‚åº”ç”Ÿæˆå¼•æ“ï¼ˆSAGEï¼‰ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶å®ç°äº†è¿™äº›ç­–ç•¥çš„æ“ä½œåŒ–ã€‚SAGEå¼•å…¥ä¸¤ç§æ–°å‹å¥–åŠ±å¡‘é€ æœºåˆ¶â€”â€”æˆ˜ç•¥ä¿¡ç”¨å¡‘é€ ï¼ˆSCSï¼‰å’Œå¯¹æ¯”å¥–åŠ±å¡‘é€ ï¼ˆCRSï¼‰ï¼Œä»¥æä¾›æ›´ä¸°å¯Œçš„å­¦ä¹ ä¿¡å·ã€‚è¿™ç§ç­–ç•¥å¯¼å‘çš„æ–¹æ³•ä¸ä»…å®ç°äº†æ–°çš„NDCG@10ç»“æœï¼Œè¿˜æ­ç¤ºäº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ–°å…´è¡Œä¸ºï¼šä»£ç†èƒ½å¤Ÿé€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„æ¢ç´¢ï¼Œå¹¶äº§ç”Ÿç®€æ´çš„é‡å†™ï¼Œé™ä½äº†æ¨ç†æˆæœ¬ï¼Œä¸ç‰ºç‰²æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç­–ç•¥å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ç»“åˆå¾®å¦™çš„å¥–åŠ±å¡‘é€ ä¸ºä¸‹ä¸€ä»£ç¨³å¥çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆå’Œæ›´å…·è§£é‡Šæ€§çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢é‡å†™å¯¹äºå¢å¼ºå¯†é›†æ£€ç´¢è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æŸ¥è¯¢é‡å†™æ–¹æ³•å­˜åœ¨éœ€è¦å¤§é‡ç›‘ç£æ•°æ®æˆ–å¼ºåŒ–å­¦ä¹ æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨ä¸“å®¶ç­–ç•¥ï¼ˆå¦‚è¯­ä¹‰æ‰©å±•å’Œå®ä½“æ¶ˆæ­§ï¼‰å¯ä»¥æ˜¾è‘—æé«˜æ£€ç´¢æ•ˆæœã€‚</li>
<li>æå‡ºç­–ç•¥è‡ªé€‚åº”ç”Ÿæˆå¼•æ“ï¼ˆSAGEï¼‰ç»“åˆå¼ºåŒ–å­¦ä¹ å®ç°ç­–ç•¥æ“ä½œåŒ–ã€‚</li>
<li>SAGEå¼•å…¥æˆ˜ç•¥ä¿¡ç”¨å¡‘é€ ï¼ˆSCSï¼‰å’Œå¯¹æ¯”å¥–åŠ±å¡‘é€ ï¼ˆCRSï¼‰ä¸¤ç§æ–°å‹å¥–åŠ±å¡‘é€ æœºåˆ¶ã€‚</li>
<li>ç­–ç•¥å¯¼å‘çš„æ–¹æ³•ä¸ä»…æé«˜äº†æ£€ç´¢æ•ˆæœï¼Œè¿˜å®ç°äº†æ–°çš„NDCG@10ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c264ad322dc55dd46e0fed3decc8e27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3449cc35bc3e01640785f1a420747247.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45f281adfd2d5a680f04ee32aec7fc2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb111d9dff7abbb37ee3789f53e9411.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Arabic-Dialect-Classification-using-RNNs-Transformers-and-Large-Language-Models-A-Comparative-Analysis"><a href="#Arabic-Dialect-Classification-using-RNNs-Transformers-and-Large-Language-Models-A-Comparative-Analysis" class="headerlink" title="Arabic Dialect Classification using RNNs, Transformers, and Large   Language Models: A Comparative Analysis"></a>Arabic Dialect Classification using RNNs, Transformers, and Large   Language Models: A Comparative Analysis</h2><p><strong>Authors:Omar A. Essameldin, Ali O. Elbeih, Wael H. Gomaa, Wael F. Elsersy</strong></p>
<p>The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in usersâ€™ dialects, social media monitoring, and greater accessibility for Arabic communities. </p>
<blockquote>
<p>é˜¿æ‹‰ä¼¯è¯­æ˜¯ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„è¯­ç§ä¹‹ä¸€ï¼Œåœ¨22ä¸ªå›½å®¶æœ‰ä¼—å¤šä¸åŒçš„æ–¹è¨€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å¯¹é˜¿æ‹‰ä¼¯æ¨ç‰¹æ•°æ®é›†ä¸­çš„18ç§é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¿›è¡Œåˆ†ç±»çš„é—®é¢˜ã€‚åˆ›å»ºäº†å¹¶æµ‹è¯•äº†RNNæ¨¡å‹ã€Transformeræ¨¡å‹ä»¥åŠé€šè¿‡æç¤ºå·¥ç¨‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å…¶ä¸­ï¼ŒMARBERTv2çš„è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå‡†ç¡®ç‡ä¸º65%ï¼ŒF1åˆ†æ•°ä¸º64%ã€‚é€šè¿‡é‡‡ç”¨æœ€å‰æ²¿çš„é¢„å¤„ç†æŠ€æœ¯å’Œæœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œæœ¬æ–‡ç¡®å®šäº†é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«ä¸­æœ€å…³é”®çš„è¯­è¨€é—®é¢˜ã€‚ç»“æœè¯å®äº†ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººå“åº”ç”¨æˆ·æ–¹è¨€ã€ç¤¾äº¤åª’ä½“ç›‘æ§ä»¥åŠæé«˜é˜¿æ‹‰ä¼¯ç¤¾åŒºçš„è®¿é—®é‡ç­‰åº”ç”¨çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å…³æ³¨é˜¿æ‹‰ä¼¯è¯­çš„æ–¹è¨€åˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨äº†å¤šç§æ¨¡å‹åŒ…æ‹¬RNNã€Transformerä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡å¯¹QADIæ•°æ®é›†é˜¿æ‹‰ä¼¯æ¨æ–‡çš„å®éªŒï¼Œå‘ç°MARBERTv2æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º65%å’Œ64%ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯å’Œæœ€æ–°è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œè§£å†³äº†é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«ä¸­çš„é‡å¤§è¯­è¨€é—®é¢˜ï¼Œå¹¶è¯å®äº†å…¶åœ¨ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººã€ç¤¾äº¤åª’ä½“ç›‘æ§å’Œå¢å¼ºé˜¿æ‹‰ä¼¯ç¤¾åŒºå¯åŠæ€§æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„æ–¹è¨€åˆ†ç±»é—®é¢˜ï¼Œæ¶µç›–äº†22ä¸ªå›½å®¶çš„å¤šç§æ–¹è¨€ã€‚</li>
<li>ç ”ç©¶ä¸­ä½¿ç”¨äº†RNNã€Transformerä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­‰æ¨¡å‹è¿›è¡Œè¯•éªŒã€‚</li>
<li>MARBERTv2æ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º65%å’Œ64%ã€‚</li>
<li>å…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯å’Œæœ€æ–°è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹è¢«ç”¨äºè§£å†³é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«ä¸­çš„è¯­è¨€é—®é¢˜ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯å®äº†æ¨¡å‹åœ¨ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººã€ç¤¾äº¤åª’ä½“ç›‘æ§ç­‰åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶å¢å¼ºäº†é˜¿æ‹‰ä¼¯ç¤¾åŒºçš„å¯åŠæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5febbe38a97720928cfa2b5a694b40f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baf5b71f259aa88b98ca70fb81f805bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd1fbfbef681402fce3c5cd9210ef516.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b517abceca84845cdecafd94f769ee86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5711d5e7fa85e7d1a25d63fadeb14c58.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-Driven-Medical-Document-Analysis-Enhancing-Trustworthy-Pathology-and-Differential-Diagnosis"><a href="#LLM-Driven-Medical-Document-Analysis-Enhancing-Trustworthy-Pathology-and-Differential-Diagnosis" class="headerlink" title="LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology   and Differential Diagnosis"></a>LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology   and Differential Diagnosis</h2><p><strong>Authors:Lei Kang, Xuanshuo Fu, Oriol Ramos Terrades, Javier Vazquez-Corral, Ernest Valveny, Dimosthenis Karatzas</strong></p>
<p>Medical document analysis plays a crucial role in extracting essential clinical insights from unstructured healthcare records, supporting critical tasks such as differential diagnosis. Determining the most probable condition among overlapping symptoms requires precise evaluation and deep medical expertise. While recent advancements in large language models (LLMs) have significantly enhanced performance in medical document analysis, privacy concerns related to sensitive patient data limit the use of online LLMs services in clinical settings. To address these challenges, we propose a trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using low-rank adaptation, specifically optimized for differential diagnosis tasks. Our approach utilizes DDXPlus, the largest benchmark dataset for differential diagnosis, and demonstrates superior performance in pathology prediction and variable-length differential diagnosis compared to existing methods. The developed web-based platform allows users to submit their own unstructured medical documents and receive accurate, explainable diagnostic results. By incorporating advanced explainability techniques, the system ensures transparent and reliable predictions, fostering user trust and confidence. Extensive evaluations confirm that the proposed method surpasses current state-of-the-art models in predictive accuracy while offering practical utility in clinical settings. This work addresses the urgent need for reliable, explainable, and privacy-preserving artificial intelligence solutions, representing a significant advancement in intelligent medical document analysis for real-world healthcare applications. The code can be found at \href{<a target="_blank" rel="noopener" href="https://github.com/leitro/Differential-Diagnosis-LoRA%7D%7Bhttps://github.com/leitro/Differential-Diagnosis-LoRA%7D">https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}</a>. </p>
<blockquote>
<p>åŒ»ç–—æ–‡æ¡£åˆ†æåœ¨ä»éç»“æ„åŒ–åŒ»ç–—è®°å½•ä¸­æå–å…³é”®ä¸´åºŠæ´å¯ŸåŠ›æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ”¯æŒå¦‚é‰´åˆ«è¯Šæ–­ç­‰å…³é”®ä»»åŠ¡ã€‚åœ¨å…·æœ‰é‡å ç—‡çŠ¶çš„æƒ…å†µä¸‹ç¡®å®šæœ€å¯èƒ½çš„ç–¾ç—…éœ€è¦ç²¾ç¡®è¯„ä¼°å’Œæ·±åšçš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ã€‚è™½ç„¶æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å·²ç»æ˜¾è‘—æé«˜äº†åŒ»ç–—æ–‡æ¡£åˆ†æçš„æ€§èƒ½ï¼Œä½†å…³äºæ•æ„Ÿæ‚£è€…æ•°æ®çš„éšç§æ‹…å¿§é™åˆ¶äº†åœ¨çº¿LLMæœåŠ¡åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯é çš„åŒ»ç–—æ–‡æ¡£åˆ†æå¹³å°ï¼Œè¯¥å¹³å°ä½¿ç”¨ä½ç§©é€‚åº”æŠ€æœ¯å¯¹LLaMA-v3è¿›è¡Œå¾®è°ƒï¼Œå¹¶é’ˆå¯¹é‰´åˆ«è¯Šæ–­ä»»åŠ¡è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨DXPlusæ•°æ®é›†ï¼ˆç”¨äºé‰´åˆ«è¯Šæ–­çš„æœ€å¤§åŸºå‡†æ•°æ®é›†ï¼‰ï¼Œåœ¨ç—…ç†é¢„æµ‹å’Œå¯å˜é•¿åº¦é‰´åˆ«è¯Šæ–­æ–¹é¢å±•ç¤ºäº†ä¼˜äºç°æœ‰æ–¹æ³•æ€§èƒ½ã€‚å¼€å‘çš„åŸºäºwebçš„å¹³å°å…è®¸ç”¨æˆ·æäº¤ä»–ä»¬è‡ªå·±çš„éç»“æ„åŒ–åŒ»ç–—è®°å½•ï¼Œå¹¶è·å¾—å‡†ç¡®ã€å¯è§£é‡Šçš„è¯Šæ–­ç»“æœã€‚é€šè¿‡é‡‡ç”¨å…ˆè¿›çš„å¯è§£é‡Šæ€§æŠ€æœ¯ï¼Œè¯¥ç³»ç»Ÿç¡®ä¿äº†é€æ˜å’Œå¯é çš„é¢„æµ‹ï¼Œå¢å¼ºäº†ç”¨æˆ·çš„ä¿¡ä»»åº¦ã€‚å¹¿æ³›è¯„ä¼°è¯å®ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨é¢„æµ‹ç²¾åº¦ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒæ—¶åœ¨ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰å®ç”¨ä»·å€¼ã€‚è¿™é¡¹å·¥ä½œæ»¡è¶³äº†äººä»¬å¯¹å¯é ã€å¯è§£é‡Šå’Œä¿æŠ¤éšç§çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„è¿«åˆ‡éœ€æ±‚ï¼Œä»£è¡¨äº†æ™ºèƒ½åŒ»ç–—æ–‡æ¡£åˆ†æåœ¨çœŸå®ä¸–ç•ŒåŒ»ç–—åº”ç”¨ä¸­çš„é‡å¤§è¿›å±•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/leitro/Differential-Diagnosis-LoRA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/leitro/Differential-Diagnosis-LoRAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19702v1">PDF</a> Accepted at ICDAR 2025</p>
<p><strong>Summary</strong>ï¼šåŒ»ç–—æ–‡æ¡£åˆ†æåœ¨æå–å…³é”®ä¸´åºŠä¿¡æ¯ä»¥æ”¯æŒç–¾ç—…é‰´åˆ«è¯Šæ–­ç­‰ä»»åŠ¡ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§å¯é çš„åŒ»ç–—æ–‡æ¡£åˆ†æå¹³å°ï¼Œé‡‡ç”¨LLaMA-v3æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åˆ©ç”¨ä½ç§©é€‚åº”æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œä¸“é—¨ç”¨äºé‰´åˆ«è¯Šæ–­ä»»åŠ¡ã€‚å¹³å°ä½¿ç”¨DDXPlusæ•°æ®é›†ï¼Œå±•ç¤ºå‡ºè‰²çš„ç—…ç†é¢„æµ‹å’Œå¯å˜é•¿åº¦é‰´åˆ«è¯Šæ–­æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¹³å°æä¾›é€æ˜çš„é¢„æµ‹ç»“æœè§£é‡Šä»¥å¢å¼ºç”¨æˆ·ä¿¡ä»»ã€‚æ­¤ç ”ç©¶è§£å†³äº†ä¸´åºŠç¯å¢ƒä¸­å¯¹å¯é ã€å¯è§£é‡Šä¸”ä¿æŠ¤éšç§çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„è¿«åˆ‡éœ€æ±‚ï¼Œä¸ºåŒ»ç–—æ–‡æ¡£çš„æ™ºèƒ½åˆ†æå¸¦æ¥äº†æ˜¾è‘—è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»ç–—æ–‡æ¡£åˆ†æå¯¹æå–ä¸´åºŠæ´å¯Ÿè‡³å…³é‡è¦ï¼Œæ”¯æŒå¦‚é‰´åˆ«è¯Šæ–­ç­‰å…³é”®ä»»åŠ¡ã€‚</li>
<li>æå‡ºçš„åŒ»ç–—æ–‡æ¡£åˆ†æå¹³å°åŸºäºLLaMA-v3æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ä½ç§©é€‚åº”æŠ€æœ¯ä¼˜åŒ–ã€‚</li>
<li>å¹³å°ä½¿ç”¨DDXPlusæ•°æ®é›†ï¼Œåœ¨ç—…ç†é¢„æµ‹å’Œå¯å˜é•¿åº¦é‰´åˆ«è¯Šæ–­æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å¹³å°æä¾›é€æ˜çš„é¢„æµ‹ç»“æœè§£é‡Šä»¥å¢å¼ºç”¨æˆ·ä¿¡ä»»å’Œä¿¡å¿ƒã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢æ›´èƒœä¸€ç­¹ï¼Œå¹¶åœ¨ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰å®ç”¨æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶è§£å†³äº†å¯¹å¯é ã€å¯è§£é‡Šä¸”ä¿æŠ¤éšç§çš„AIè§£å†³æ–¹æ¡ˆçš„è¿«åˆ‡éœ€æ±‚ã€‚</li>
<li>å¹³å°ä»£ç å¯è®¿é—®[<a target="_blank" rel="noopener" href="https://github.com/leitro/Differential-Diagnosis-LoRA]%E3%80%82">https://github.com/leitro/Differential-Diagnosis-LoRA]ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0568ba7c7acf5bf339388a54ab38c65a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2db7bf86ffa4910a82f62eb367a5d403.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Outlier-Safe-Pre-Training-for-Robust-4-Bit-Quantization-of-Large-Language-Models"><a href="#Outlier-Safe-Pre-Training-for-Robust-4-Bit-Quantization-of-Large-Language-Models" class="headerlink" title="Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large   Language Models"></a>Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large   Language Models</h2><p><strong>Authors:Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang</strong></p>
<p>Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Outlier-Safe-Pre-Training">https://github.com/dmis-lab/Outlier-Safe-Pre-Training</a>. </p>
<blockquote>
<p>æç«¯æ¿€æ´»å€¼å¼‚å¸¸åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ä¼šä¸¥é‡é™ä½é‡åŒ–æ€§èƒ½ï¼Œé˜»ç¢å…¶åœ¨è®¾å¤‡ä¸Šçš„æœ‰æ•ˆéƒ¨ç½²ã€‚è™½ç„¶å·²çŸ¥é€šé“æ“ä½œå’Œè‡ªé€‚åº”æ¢¯åº¦ç¼©æ”¾æ˜¯åŸå› ï¼Œä½†å®é™…çš„ç¼“è§£æªæ–½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†å¼‚å¸¸å®‰å…¨é¢„è®­ç»ƒï¼ˆOSPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å®ç”¨çš„æŒ‡å¯¼æ–¹é’ˆï¼Œèƒ½å¤Ÿæå‰é˜²æ­¢å¼‚å¸¸å€¼å½¢æˆï¼Œè€Œä¸æ˜¯ä¾èµ–åç»­ç¼“è§£æªæ–½ã€‚OSPç»“åˆäº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰Muonä¼˜åŒ–å™¨ï¼Œèƒ½å¤Ÿæ¶ˆé™¤ç‰¹æƒåŸºåŒæ—¶ä¿æŒè®­ç»ƒæ•ˆç‡ï¼›ï¼ˆ2ï¼‰å•å°ºåº¦RMSNormï¼Œé˜²æ­¢é€šé“æ”¾å¤§ï¼›ï¼ˆ3ï¼‰å¯å­¦ä¹ åµŒå…¥æŠ•å½±ï¼Œé‡æ–°åˆ†é…æ¥è‡ªåµŒå…¥çŸ©é˜µçš„æ¿€æ´»å€¼å¹…åº¦ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸€ä¸ªæ‹¥æœ‰1.4ä¸‡äº¿å‚æ•°çš„æ¨¡å‹åœ¨1ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šæ¥éªŒè¯OSPï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨ç”Ÿäº§è§„æ¨¡ä¸Šè®­ç»ƒçš„æ²¡æœ‰æ­¤ç±»å¼‚å¸¸çš„LLMã€‚åœ¨æ¿€çƒˆçš„4ä½é‡åŒ–ä¸‹ï¼Œæˆ‘ä»¬çš„OSPæ¨¡å‹åœ¨10ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°å¹³å‡å¾—åˆ†35.7ï¼ˆä¸Adamè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ä¸º26.5ï¼‰ï¼Œä»…å¢åŠ äº†2%çš„è®­ç»ƒå¼€é”€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒOSPæ¨¡å‹çš„è¶…é¢å³°åº¦æ¥è¿‘äºé›¶ï¼ˆ0.04ï¼‰ï¼Œä¸æ ‡å‡†æ¨¡å‹çš„æç«¯å€¼ï¼ˆ1818.56ï¼‰ç›¸æ¯”ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜äº†LLMçš„é‡åŒ–è¡Œä¸ºã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œå¼‚å¸¸å€¼å¹¶éLLMæ‰€å›ºæœ‰ï¼Œè€Œæ˜¯è®­ç»ƒç­–ç•¥çš„ç»“æœï¼Œä¸ºæ›´é«˜æ•ˆçš„LLMéƒ¨ç½²é“ºå¹³äº†é“è·¯ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Outlier-Safe-Pre-Training%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dmis-lab/Outlier-Safe-Pre-Trainingä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19697v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å­˜åœ¨æç«¯æ¿€æ´»å¼‚å¸¸å€¼é—®é¢˜ï¼Œä¸¥é‡å½±å“æ¨¡å‹åœ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²æ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†Outlier-Safe Pre-Trainingï¼ˆOSPï¼‰æ–¹æ¡ˆï¼Œé€šè¿‡å¼•å…¥ä¸‰ç§å…³é”®åˆ›æ–°æŠ€æœ¯ï¼Œæœ‰æ•ˆé¢„é˜²å¼‚å¸¸å€¼äº§ç”Ÿï¼Œæé«˜æ¨¡å‹é‡åŒ–æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒOSPè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æç«¯é‡åŒ–æ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®­ç»ƒå¼€é”€è¾ƒå°ã€‚æœ¬æ–‡å·¥ä½œè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼å¹¶éå›ºæœ‰ï¼Œè€Œæ˜¯è®­ç»ƒç­–ç•¥çš„åæœï¼Œä¸ºæ›´é«˜æ•ˆéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡åŒ–è¿‡ç¨‹ä¸­å­˜åœ¨æç«¯æ¿€æ´»å¼‚å¸¸å€¼é—®é¢˜ï¼Œä¸¥é‡å½±å“æ€§èƒ½ã€‚</li>
<li>Outlier-Safe Pre-Trainingï¼ˆOSPï¼‰æ–¹æ¡ˆé€šè¿‡å¼•å…¥ä¸‰ç§å…³é”®åˆ›æ–°æŠ€æœ¯ï¼Œæœ‰æ•ˆé¢„é˜²å¼‚å¸¸å€¼äº§ç”Ÿã€‚</li>
<li>OSPæ–¹æ¡ˆåŒ…æ‹¬ï¼šMuonä¼˜åŒ–å™¨ã€Single-Scale RMSNormå’Œå¯å­¦ä¹ åµŒå…¥æŠ•å½±ã€‚</li>
<li>OSPè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æç«¯é‡åŒ–æ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å¾—åˆ†æå‡æ˜¾è‘—ã€‚</li>
<li>OSPæ¨¡å‹è®­ç»ƒå¼€é”€è¾ƒå°ï¼Œä»…æœ‰2%çš„é¢å¤–è´Ÿæ‹…ã€‚</li>
<li>OSPæ¨¡å‹çš„è¿‡åº¦å³°åº¦å€¼å¤§å¹…é™ä½ï¼Œæ›´æ¥è¿‘ç†æƒ³æ¨¡å‹è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab5228fbb7de9bad437cbf0bf6b423ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6af1804accd07389a1b9b2e9d3bd71d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-229c0f27612537a91269e77ea78c07c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d9f81b5c8e4c95d23c367d16f493997.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Adaptive-Request-Scheduling-for-CodeLLM-Serving-with-SLA-Guarantees"><a href="#Adaptive-Request-Scheduling-for-CodeLLM-Serving-with-SLA-Guarantees" class="headerlink" title="Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees"></a>Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees</h2><p><strong>Authors:Shi Chang, Boyuan Chen, Kishanthan Thangarajah, Hanan Lutfiyya, Ahmed E. Hassan</strong></p>
<p>Code Large Language Models (CodeLLMs) are increasingly integrated into modern software development workflows, yet efficiently serving them in resource-constrained, self-hosted environments remains a significant challenge. Existing LLM serving systems employs Continuous Batching for throughput improvement. However, they rely on static batch size configurations that cannot adapt to fluctuating request rates or heterogeneous workloads, leading to frequent SLA (Service Level Agreement) violations and unstable performance. In this study, We propose SABER, a dynamic batching strategy that predicts per-request SLA feasibility and adjusts decisions in real time. SABER improves goodput by up to 26% over the best static configurations and reduces latency variability by up to 45%, all without manual tuning or service restarts. Our results demonstrate that SLA-aware, adaptive scheduling is key to robust, high-performance CodeLLM serving. </p>
<blockquote>
<p>ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCodeLLMsï¼‰åœ¨ç°ä»£è½¯ä»¶å¼€å‘æµç¨‹ä¸­çš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œä½†åœ¨èµ„æºå—é™ã€è‡ªä¸»æ‰˜ç®¡çš„ç¯å¢ƒä¸­æœ‰æ•ˆåœ°æä¾›æœåŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„LLMæœåŠ¡ç³»ç»Ÿé‡‡ç”¨è¿ç»­æ‰¹å¤„ç†æ¥æé«˜ååé‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºé™æ€æ‰¹å¤„ç†å¤§å°é…ç½®ï¼Œæ— æ³•é€‚åº”æ³¢åŠ¨çš„è¯·æ±‚ç‡æˆ–å¼‚æ„å·¥ä½œé‡ï¼Œå¯¼è‡´æœåŠ¡ç­‰çº§åè®®ï¼ˆSLAï¼‰é¢‘ç¹è¿çº¦å’Œæ€§èƒ½ä¸ç¨³å®šã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºSABERï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼Œå¯ä»¥é¢„æµ‹æ¯ä¸ªè¯·æ±‚çš„SLAå¯è¡Œæ€§å¹¶å®æ—¶è°ƒæ•´å†³ç­–ã€‚SABERåœ¨æœ€ä½³é™æ€é…ç½®çš„åŸºç¡€ä¸Šå°†goodputæé«˜äº†é«˜è¾¾26%ï¼Œå¹¶å°†å»¶è¿Ÿå˜åŒ–å‡å°‘äº†é«˜è¾¾45%ï¼Œè€Œä¸”æ— éœ€æ‰‹åŠ¨è°ƒæ•´æˆ–æœåŠ¡é‡å¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œäº†è§£SLAçš„è‡ªé€‚åº”è°ƒåº¦æ˜¯ç¨³å¥ã€é«˜æ€§èƒ½CodeLLMæœåŠ¡çš„å…³é”®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19677v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å¼€å‘æµç¨‹ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†åœ¨èµ„æºå—é™çš„è‡ªå®šä¹‰æ‰˜ç®¡ç¯å¢ƒä¸­é«˜æ•ˆåœ°æä¾›æœåŠ¡ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿé‡‡ç”¨è¿ç»­æ‰¹å¤„ç†æ¥æé«˜ååé‡ï¼Œä½†ä¾èµ–äºæ— æ³•é€‚åº”è¯·æ±‚ç‡æ³¢åŠ¨æˆ–å¼‚æ„å·¥ä½œè´Ÿè½½çš„é™æ€æ‰¹å¤„ç†é…ç½®ï¼Œå¯¼è‡´æœåŠ¡ç­‰çº§åè®®ï¼ˆSLAï¼‰é¢‘ç¹è¿è§„å’Œæ€§èƒ½ä¸ç¨³å®šã€‚æœ¬ç ”ç©¶æå‡ºSABERåŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥ï¼Œå®ƒèƒ½é¢„æµ‹æ¯ä¸ªè¯·æ±‚çš„SLAå¯è¡Œæ€§å¹¶å®æ—¶è°ƒæ•´å†³ç­–ã€‚SABERåœ¨æœ€ä½³é™æ€é…ç½®çš„åŸºç¡€ä¸Šå°†goodputæé«˜äº†é«˜è¾¾26%ï¼Œå¹¶å°†å»¶è¿Ÿå˜åŒ–é™ä½äº†é«˜è¾¾45%ï¼Œä¸”æ— éœ€æ‰‹åŠ¨è°ƒæ•´æˆ–æœåŠ¡é‡å¯ã€‚ç»“æœè¯æ˜ï¼ŒåŸºäºSLAçš„è°ƒåº¦å¯¹äºå®ç°ç¨³å¥ã€é«˜æ€§èƒ½çš„CodeLLMæœåŠ¡è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Code Large Language Models (CodeLLMs) åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿä¸»è¦ä¾èµ–é™æ€æ‰¹å¤„ç†é…ç½®ï¼Œæ— æ³•é€‚åº”å˜åŒ–çš„è¯·æ±‚ç‡å’Œå¼‚æ„å·¥ä½œé‡ï¼Œå¯¼è‡´æ€§èƒ½ä¸ç¨³å®šå’ŒSLAè¿è§„ã€‚</li>
<li>SABERåŠ¨æ€æ‰¹å¤„ç†ç­–ç•¥è¢«æå‡ºä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡é¢„æµ‹æ¯ä¸ªè¯·æ±‚çš„SLAå¯è¡Œæ€§å¹¶å®æ—¶è°ƒæ•´å†³ç­–æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>SABERç­–ç•¥ç›¸è¾ƒäºæœ€ä½³é™æ€é…ç½®ï¼Œåœ¨goodputä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œæœ€å¤šå¯æé«˜26%ï¼ŒåŒæ—¶é™ä½å»¶è¿Ÿå˜åŒ–è¾¾45%ã€‚</li>
<li>è¯¥ç­–ç•¥æ— éœ€æ‰‹åŠ¨è°ƒæ•´æˆ–æœåŠ¡é‡å¯ï¼Œå…·æœ‰é«˜åº¦çš„å®ç”¨æ€§å’Œä¾¿æ·æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-940d00e06f56cd71500238d7c7d8964b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e8d1d8ba95bc1d58185c0e19250fcd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d4499c693a1d2d5d2beecbd1d45e6a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5c013ac1100039a38d52a49d36a527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ec51acba1f18fbfb2f0b2a1a041ccf.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Recurrent-Visual-Feature-Extraction-and-Stereo-Attentions-for-CT-Report-Generation"><a href="#Recurrent-Visual-Feature-Extraction-and-Stereo-Attentions-for-CT-Report-Generation" class="headerlink" title="Recurrent Visual Feature Extraction and Stereo Attentions for CT Report   Generation"></a>Recurrent Visual Feature Extraction and Stereo Attentions for CT Report   Generation</h2><p><strong>Authors:Yuanhe Tian, Lei Mao, Yan Song</strong></p>
<p>Generating reports for computed tomography (CT) images is a challenging task, while similar to existing studies for medical image report generation, yet has its unique characteristics, such as spatial encoding of multiple images, alignment between image volume and texts, etc. Existing solutions typically use general 2D or 3D image processing techniques to extract features from a CT volume, where they firstly compress the volume and then divide the compressed CT slices into patches for visual encoding. These approaches do not explicitly account for the transformations among CT slices, nor do they effectively integrate multi-level image features, particularly those containing specific organ lesions, to instruct CT report generation (CTRG). In considering the strong correlation among consecutive slices in CT scans, in this paper, we propose a large language model (LLM) based CTRG method with recurrent visual feature extraction and stereo attentions for hierarchical feature modeling. Specifically, we use a vision Transformer to recurrently process each slice in a CT volume, and employ a set of attentions over the encoded slices from different perspectives to selectively obtain important visual information and align them with textual features, so as to better instruct an LLM for CTRG. Experiment results and further analysis on the benchmark M3D-Cap dataset show that our method outperforms strong baseline models and achieves state-of-the-art results, demonstrating its validity and effectiveness. </p>
<blockquote>
<p>ç”Ÿæˆè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒçš„æŠ¥å‘Šæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶å®ƒä¸åŒ»ç–—å›¾åƒæŠ¥å‘Šç”Ÿæˆé¢†åŸŸä¸­çš„ç°æœ‰ç ”ç©¶ç›¸ä¼¼ï¼Œä½†å®ƒå…·æœ‰ç‹¬ç‰¹çš„ç‰¹æ€§ï¼Œä¾‹å¦‚å¤šä¸ªå›¾åƒçš„ç©ºé—´ç¼–ç ã€å›¾åƒä½“ç§¯ä¸æ–‡æœ¬ä¹‹é—´çš„å¯¹é½ç­‰ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸ä½¿ç”¨é€šç”¨çš„äºŒç»´æˆ–ä¸‰ç»´å›¾åƒå¤„ç†æŠ€æœ¯ä»CTä½“ç§¯ä¸­æå–ç‰¹å¾ï¼Œå®ƒä»¬é¦–å…ˆå‹ç¼©ä½“ç§¯ï¼Œç„¶åå°†å‹ç¼©çš„CTåˆ‡ç‰‡åˆ†æˆæ–‘å—è¿›è¡Œè§†è§‰ç¼–ç ã€‚è¿™äº›æ–¹æ³•æ²¡æœ‰æ˜ç¡®åœ°è€ƒè™‘åˆ°CTåˆ‡ç‰‡ä¹‹é—´çš„è½¬æ¢ï¼Œä¹Ÿæ²¡æœ‰æœ‰æ•ˆåœ°æ•´åˆå¤šçº§å›¾åƒç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ç‰¹å®šå™¨å®˜ç—…å˜çš„ç‰¹å¾ï¼Œä»¥æŒ‡å¯¼CTæŠ¥å‘Šç”Ÿæˆï¼ˆCTRGï¼‰ã€‚è€ƒè™‘åˆ°CTæ‰«æä¸­è¿ç»­åˆ‡ç‰‡ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„CTRGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¾ªç¯è§†è§‰ç‰¹å¾æå–å’Œç«‹ä½“æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºåˆ†å±‚ç‰¹å¾å»ºæ¨¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨è§†è§‰è½¬æ¢å™¨å¾ªç¯å¤„ç†CTä½“ç§¯ä¸­çš„æ¯ä¸ªåˆ‡ç‰‡ï¼Œå¹¶ä»ä¸åŒè§’åº¦å¯¹ç¼–ç åçš„åˆ‡ç‰‡é›†åº”ç”¨ä¸€ç»„æ³¨æ„åŠ›ï¼Œä»¥é€‰æ‹©æ€§è·å–é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œä»è€Œæ›´å¥½åœ°æŒ‡å¯¼LLMè¿›è¡ŒCTRGã€‚åœ¨åŸºå‡†æ•°æ®é›†M3D-Capä¸Šçš„å®éªŒç»“æœå’Œè¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19665v1">PDF</a> 7 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒCTå›¾åƒæŠ¥å‘Šç”Ÿæˆçš„æ–¹æ³•ã€‚é’ˆå¯¹CTå›¾åƒçš„ç‰¹æ€§ï¼Œè®ºæ–‡ä½¿ç”¨è§†è§‰Transformerå¯¹CTä½“ç§¯ä¸­çš„åˆ‡ç‰‡è¿›è¡Œé€’å½’å¤„ç†ï¼Œå¹¶é€šè¿‡ä¸åŒè§’åº¦çš„æ³¨æ„åŠ›æœºåˆ¶è·å–é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œä¸æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œä»¥æŒ‡å¯¼LLMç”ŸæˆæŠ¥å‘Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTå›¾åƒæŠ¥å‘Šç”Ÿæˆæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¤„ç†å›¾åƒçš„ç©ºé—´ç¼–ç å’Œä¸æ–‡æœ¬çš„å¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨2Dæˆ–3Då›¾åƒå¤„ç†æŠ€æœ¯ä»CTä½“ç§¯ä¸­æå–ç‰¹å¾ï¼Œä½†æœªèƒ½å……åˆ†è€ƒè™‘CTåˆ‡ç‰‡é—´çš„å˜æ¢ä»¥åŠå¤šçº§åˆ«å›¾åƒç‰¹å¾çš„æ•´åˆã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„CTå›¾åƒæŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆé€’å½’è§†è§‰ç‰¹å¾æå–å’Œç«‹ä½“æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œåˆ†å±‚ç‰¹å¾å»ºæ¨¡ã€‚</li>
<li>ä½¿ç”¨è§†è§‰Transformeré€’å½’å¤„ç†CTä½“ç§¯ä¸­çš„æ¯ä¸ªåˆ‡ç‰‡ã€‚</li>
<li>é€šè¿‡ä¸åŒè§’åº¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€‰æ‹©æ€§è·å–é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½ï¼Œä»¥æŒ‡å¯¼LLMç”ŸæˆæŠ¥å‘Šã€‚</li>
<li>è®ºæ–‡åœ¨M3D-Capæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œè¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ee423f750f3eb6047ce95ff03588ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d687f01d109c7708e443ee73b36929.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5d146356967cb8c10e3b9c05f72c8f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-714702f18ffb5fcfde60ae09487701a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2d0218b5d4ae54b8d437e368df5e814.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Tailored-Conversations-beyond-LLMs-A-RL-Based-Dialogue-Manager"><a href="#Tailored-Conversations-beyond-LLMs-A-RL-Based-Dialogue-Manager" class="headerlink" title="Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"></a>Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager</h2><p><strong>Authors:Lucie Galland, Catherine Pelachaud, Florian Pecune</strong></p>
<p>In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŸºäºRLçš„å¯¹è¯ç®¡ç†å™¨é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨äºå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¥æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼Œå¹¶å€ŸåŠ©å…ƒå­¦ä¹ æ¥æé«˜ä¸åŒç”¨æˆ·é…ç½®æ–‡ä»¶ä¹‹é—´çš„é€‚åº”æ€§ï¼Œä»è€Œæé«˜é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œåœ¨å¯¹è¯é˜¶æ®µä¹‹é—´å¹³ç¨³è¿‡æ¸¡ï¼Œå¹¶æ ¹æ®ä¸åŒçš„æ‚£è€…éœ€æ±‚ä¸ªæ€§åŒ–å“åº”ã€‚æˆ‘ä»¬å°†æ¡†æ¶åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œå¹¶è¯æ˜æ‰€æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå°†å¤§å‹è¯­è¨€æ¨¡å‹è®¾ç½®ä¸ºå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ç³»ç»Ÿçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19652v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥å·¥ä½œæå‡ºä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¯¹è¯ç®¡ç†å™¨ï¼Œå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¯¹è¯ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼Œå¹¶é€šè¿‡å…ƒå­¦ä¹ æé«˜å¯¹ä¸åŒç”¨æˆ·ç‰¹å¾çš„é€‚åº”æ€§ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„é€‚åº”æ€§å’Œæ•ˆç‡ã€‚ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œæµç•…åœ°è½¬æ¢å¯¹è¯é˜¶æ®µï¼Œå¹¶å¯¹ä¸åŒæ‚£è€…çš„éœ€æ±‚åšå‡ºä¸ªæ€§åŒ–å“åº”ã€‚æœ¬æ–‡åœ¨åŠ¨æœºè®¿è°ˆæ–¹é¢çš„åº”ç”¨ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œè¯æ˜æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºå…ˆè¿›LLMåŸºçº¿ï¼Œå±•ç¤ºäº†ä¸ºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¯¹è¯ç³»ç»Ÿè°ƒèŠ‚LLMçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„æ–°å‹æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹è¯ç®¡ç†å™¨ï¼Œé€‚ç”¨äºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¯¹è¯ã€‚</li>
<li>ä½¿ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µã€‚</li>
<li>åˆ©ç”¨å…ƒå­¦ä¹ æé«˜ç³»ç»Ÿçš„é€‚åº”æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç”¨æˆ·ç‰¹å¾å’Œå¯¹è¯ç¯å¢ƒã€‚</li>
<li>ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®ä¸­å­¦ä¹ ï¼Œæµç•…è½¬æ¢å¯¹è¯é˜¶æ®µã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä¸ªæ€§åŒ–å“åº”ä¸åŒæ‚£è€…çš„éœ€æ±‚ã€‚</li>
<li>åœ¨åŠ¨æœºè®¿è°ˆæ–¹é¢çš„åº”ç”¨å±•ç¤ºäº†å…¶ä¿ƒè¿›è¡Œä¸ºæ”¹å˜çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1f9405f26a1a67abd221f62dbb2da549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ddc4ed63cd160846acbe54e92b164c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14465b24ac8ac3cfe6e2093cc18adeb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HeurAgenix-Leveraging-LLMs-for-Solving-Complex-Combinatorial-Optimization-Challenges"><a href="#HeurAgenix-Leveraging-LLMs-for-Solving-Complex-Combinatorial-Optimization-Challenges" class="headerlink" title="HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial   Optimization Challenges"></a>HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial   Optimization Challenges</h2><p><strong>Authors:Xianliang Yang, Ling Zhang, Haolong Qian, Lei Song, Jiang Bian</strong></p>
<p>Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce \textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLMâ€™s perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/HeurAgenix">https://github.com/microsoft/HeurAgenix</a>. </p>
<blockquote>
<p>å¯å‘å¼ç®—æ³•åœ¨è§£å†³ç»„åˆä¼˜åŒ–ï¼ˆCOï¼‰é—®é¢˜ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œä¼ ç»Ÿè®¾è®¡ä¸¥é‡ä¾èµ–äºäººå·¥ç»éªŒï¼Œéš¾ä»¥åœ¨ä¸åŒå®ä¾‹ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>HeurAgenix</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä¸¤é˜¶æ®µè¶…å¯å‘å¼æ¡†æ¶ï¼Œé¦–å…ˆè¿›åŒ–å¯å‘å¼ï¼Œç„¶åè‡ªåŠ¨ä»ä¸­é€‰æ‹©ã€‚åœ¨å¯å‘å¼è¿›åŒ–é˜¶æ®µï¼ŒHeurAgenixåˆ©ç”¨LLMæ¯”è¾ƒç§å­å¯å‘å¼è§£å†³æ–¹æ¡ˆä¸é«˜è´¨é‡è§£å†³æ–¹æ¡ˆï¼Œå¹¶æå–å¯é‡å¤ä½¿ç”¨çš„è¿›åŒ–ç­–ç•¥ã€‚åœ¨é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ï¼Œå®ƒæ ¹æ®LLMçš„æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŠ¨æ€é€‰æ‹©æ¯ä¸ªé—®é¢˜çŠ¶æ€ä¸‹æœ€æœ‰å‰é€”çš„å¯å‘å¼ã€‚ä¸ºäº†çµæ´»æ€§ï¼Œè¿™ä¸ªé€‰æ‹©å™¨å¯ä»¥æ˜¯æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥æ˜¯å…·æœ‰è¾ƒä½æ¨ç†æˆæœ¬çš„ç²¾ç»†è°ƒæ•´è½»é‡çº§æ¨¡å‹ã€‚ä¸ºäº†ç¼“è§£ç”±äºç»„åˆä¼˜åŒ–å¤æ‚æ€§å¯¼è‡´çš„å¯é ç›‘ç£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒé‡å¥–åŠ±æœºåˆ¶å¾®è°ƒè½»é‡çº§å¯å‘å¼é€‰æ‹©å™¨ï¼Œè¯¥æœºåˆ¶è”åˆåˆ©ç”¨é€‰æ‹©åå¥½å’ŒçŠ¶æ€æ„ŸçŸ¥çš„ä¿¡å·ï¼Œå®ç°åœ¨å˜ˆæ‚æ³¨é‡Šä¸‹çš„ç¨³å¥é€‰æ‹©ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHeurAgenixä¸ä»…ä¼˜äºç°æœ‰çš„åŸºäºLLMçš„è¶…å¯å‘å¼ï¼Œè€Œä¸”ä¸ä¸“ç”¨æ±‚è§£å™¨ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/HeurAgenix">https://github.com/microsoft/HeurAgenix</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15196v2">PDF</a> 27 pages,9 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯å‘å¼ç®—æ³•åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„å…³é”®ä½œç”¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†HeurAgenixï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ä¸¤é˜¶æ®µè¶…å¯å‘å¼æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¿›åŒ–å¹¶é€‰æ‹©åˆé€‚çš„å¯å‘å¼ç®—æ³•ã€‚å®ƒé€šè¿‡LLMæ¯”è¾ƒç§å­å¯å‘å¼è§£å†³æ–¹æ¡ˆä¸é«˜è´¨é‡è§£å†³æ–¹æ¡ˆï¼Œæå–å¯é‡å¤ä½¿ç”¨çš„è¿›åŒ–ç­–ç•¥ã€‚åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œå®ƒæ ¹æ®LLMçš„æ„ŸçŸ¥èƒ½åŠ›åŠ¨æ€é€‰æ‹©æœ€æœ‰å‰é€”çš„å¯å‘å¼æ–¹æ³•ã€‚ä¸ºåº”å¯¹å¤æ‚æ€§å¸¦æ¥çš„å¯é ç›‘ç£ä¸è¶³é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨åŒé‡å¥–åŠ±æœºåˆ¶å¾®è°ƒè½»é‡çº§å¯å‘å¼é€‰æ‹©å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿä»é€‰æ‹©åå¥½å’ŒçŠ¶æ€æ„ŸçŸ¥ä¸­è”åˆè·å–ä¿¡å·ï¼Œå®ç°å™ªå£°æ ‡æ³¨ä¸‹çš„ç¨³å¥é€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼ŒHeurAgenixä¸ä»…ä¼˜äºç°æœ‰çš„LLMè¶…å¯å‘å¼ï¼Œè€Œä¸”ä¸ä¸“ç”¨æ±‚è§£å™¨ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HeurAgenixæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¶…å¯å‘å¼æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å®ƒé€šè¿‡è¿›åŒ–å¯å‘å¼ç®—æ³•å¹¶é€‰æ‹©æœ€ä½³æ–¹æ¡ˆæ¥å·¥ä½œï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯è‡ªåŠ¨å®Œæˆçš„ã€‚</li>
<li>HeurAgenixåˆ©ç”¨LLMæ¯”è¾ƒä¸åŒçš„å¯å‘å¼è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä»ä¸­æå–å¯é‡å¤ä½¿ç”¨çš„è¿›åŒ–ç­–ç•¥ã€‚</li>
<li>åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œå®ƒä¼šæ ¹æ®LLMçš„æ„ŸçŸ¥èƒ½åŠ›åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„å¯å‘å¼æ–¹æ³•ã€‚</li>
<li>ä¸ºåº”å¯¹ç¼ºä¹å¯é ç›‘ç£çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†å…·æœ‰åŒé‡å¥–åŠ±æœºåˆ¶çš„è½»é‡çº§å¯å‘å¼é€‰æ‹©å™¨ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ ‡å‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„LLMè¶…å¯å‘å¼ï¼Œå¹¶ä¸”ä¸ä¸“ç”¨æ±‚è§£å™¨ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>HeurAgenixçš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-083b82295a77b4e95df0b878186bf17d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c662ba8a118999fd8b6fd371df6232be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-527a2f23a4c722f85898c5d9de48ec08.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FLAT-LLM-Fine-grained-Low-rank-Activation-Space-Transformation-for-Large-Language-Model-Compression"><a href="#FLAT-LLM-Fine-grained-Low-rank-Activation-Space-Transformation-for-Large-Language-Model-Compression" class="headerlink" title="FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression"></a>FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression</h2><p><strong>Authors:Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang</strong></p>
<p>Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis (PCA), and employ an importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç„¶è€Œå®ƒä»¬çš„é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æ„æˆäº†æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„ä½ç§©åˆ†è§£æ–¹æ³•ä¸ºè§£å†³ç»“æ„å‹ç¼©é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è·¯å¾„ï¼Œä½†å®ƒä»¬ç»å¸¸é¢ä¸´ç²¾åº¦ä¸‹é™ã€æ ¡å‡†ç¨‹åºæ˜‚è´µä»¥åŠå¯¼è‡´é˜»ç¢ç°å®ä¸–ç•Œæ¨ç†é€Ÿåº¦æå‡çš„ä½æ•ˆæ¨¡å‹æ¶æ„çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FLAT-LLMï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿä¸”å‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•ï¼ŒåŸºäºæ¿€æ´»ç©ºé—´ä¸­çš„ç²¾ç»†ç²’åº¦ä½ç§©è½¬æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¤´ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è®¡ç®—çš„æˆªæ–­ç‰¹å¾å‘é‡æ¥è½¬æ¢æƒé‡ï¼Œä»¥é™ä½éšè—ç»´åº¦ï¼Œå¹¶åŸºäºé‡è¦æ€§çš„åº¦é‡æ¥åœ¨è§£ç å™¨ä¹‹é—´è‡ªé€‚åº”åˆ†é…ç­‰çº§ã€‚FLAT-LLMå®ç°äº†é«˜æ•ˆä¸”æœ‰æ•ˆçš„æƒé‡å‹ç¼©ï¼Œæ— éœ€æ¢å¤å¾®è°ƒï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆæ ¡å‡†ã€‚åœ¨4ä¸ªæ¨¡å‹å’Œ11ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFLAT-LLMåœ¨æ³›åŒ–å’Œä¸‹æ¸¸æ€§èƒ½ä¸Šè¶…è¿‡äº†ç»“æ„å‰ªæåŸºçº¿ï¼ŒåŒæ—¶åœ¨åŸºäºåˆ†è§£çš„æ–¹æ³•ä¸Šå®ç°äº†æ¨ç†é€Ÿåº¦çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23966v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMé¢ä¸´è®¡ç®—èµ„æºå’Œå†…å­˜çš„æŒ‘æˆ˜ï¼Œç°æœ‰ç»“æ„å‹ç¼©æ–¹æ³•å­˜åœ¨ç²¾åº¦æŸå¤±ã€æ ¡å‡†ç¹çå’Œæ•ˆç‡ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºåŸºäºæ¿€æ´»ç©ºé—´çš„ç²¾ç»†ç²’åº¦ä½ç§©è½¬æ¢çš„ç»“æ„å‹ç¼©æ–¹æ³•FLAT-LLMï¼Œé‡‡ç”¨æˆªæ–­ç‰¹å¾å€¼å’Œå¤´ä¸»æˆåˆ†åˆ†æå®ç°å¿«é€Ÿéƒ¨ç½²ä¸æ€§èƒ½ä¿è¯ï¼Œä¸éœ€ç²¾ç»†è®­ç»ƒä¼˜åŒ–è¿‡ç¨‹ï¼ŒåŒæ—¶æä¾›æ’åçµæ´»åˆ†é…çš„è§£è¯‘èƒ½åŠ›åº¦é‡æ¨¡å‹æ€§èƒ½ä¼˜åŠ¿ã€‚<br>åœ¨å¤šé¡¹æ¨¡å‹ä¸­å®è·µç»“æœå‡è¡¨æ˜ï¼ŒFLAT-LLMåœ¨ä¿è¯æ³›åŒ–èƒ½åŠ›å’Œä¸‹æ¸¸æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°æ¯”ç°æœ‰ç»“æ„è£å‰ªæ–¹æ³•æ›´é«˜çš„å‹ç¼©æ•ˆç‡ï¼Œå¹¶èƒ½æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æŒ‘æˆ˜ï¼Œéœ€è¦è§£å†³è®¡ç®—èµ„æºå’Œå†…å­˜éœ€æ±‚é«˜çš„éš¾é¢˜ã€‚</li>
<li>å½“å‰çš„ç»“æ„å‹ç¼©æ–¹æ³•å­˜åœ¨ç²¾åº¦æŸå¤±å’Œæ ¡å‡†è¿‡ç¨‹å¤æ‚çš„é—®é¢˜ã€‚</li>
<li>FLAT-LLMæå‡ºä¸€ç§åŸºäºæ¿€æ´»ç©ºé—´çš„ä½ç§©è½¬æ¢ç»“æ„å‹ç¼©æ–¹æ³•ï¼Œèƒ½å¿«é€Ÿè¿›è¡Œç»“æ„å‹ç¼©åŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¤´ä¸»æˆåˆ†åˆ†æè¿›è¡Œæƒé‡è½¬æ¢ï¼Œé€šè¿‡æˆªæ–­ç‰¹å¾å€¼å®ç°éšè—ç»´åº¦çš„é™ä½ã€‚</li>
<li>FLAT-LLMé‡‡ç”¨åŸºäºé‡è¦æ€§çš„åº¦é‡æ¥çµæ´»åˆ†é…ä¸åŒè§£ç å™¨çš„æ’åã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ¢å¤ç²¾ç»†è®­ç»ƒè¿‡ç¨‹ï¼Œèƒ½åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆæ ¡å‡†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe150bde0113af265eac13e528652ce5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a036fd34fc011d593588ac5124f062a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-224bf5f2b0bc2321f5c0e72be0d73a31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-814b389a08a67162d0d7c0fb9bc532dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d8b9ef47877f16d808c2b4989bc8450.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EditLord-Learning-Code-Transformation-Rules-for-Code-Editing"><a href="#EditLord-Learning-Code-Transformation-Rules-for-Code-Editing" class="headerlink" title="EditLord: Learning Code Transformation Rules for Code Editing"></a>EditLord: Learning Code Transformation Rules for Code Editing</h2><p><strong>Authors:Weichen Li, Albert Jan, Baishakhi Ray, Junfeng Yang, Chengzhi Mao, Kexin Pei</strong></p>
<p>Code editing is a foundational task in software development, where its effectiveness depends on whether it introduces desired code property changes without changing the original codeâ€™s intended functionality. Existing approaches often formulate code editing as an implicit end-to-end task, omitting the fact that code-editing procedures inherently consist of discrete and explicit steps. Thus, they suffer from suboptimal performance and lack of robustness and generalization. We introduce EditLord, a code editing framework that makes the code transformation steps explicit. Our key insight is to employ a language model (LM) as an inductive learner to extract code editing rules from the training code pairs as concise meta-rule sets. Such rule sets will be manifested for each training sample to augment them for finetuning or assist in prompting- and iterative-based code editing. EditLordoutperforms the state-of-the-art by an average of 22.7% in editing performance and 58.1% in robustness while achieving 20.2% higher functional correctness across critical software engineering and security applications, LM models, and editing modes. </p>
<blockquote>
<p>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„æƒ…å†µä¸‹å¼•å…¥äº†æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»£ç ç¼–è¾‘åˆ¶å®šä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘è¿‡ç¨‹æœ¬è´¨ä¸ŠåŒ…å«ç¦»æ•£å’Œæ˜ç¡®æ­¥éª¤è¿™ä¸€äº‹å®ã€‚å› æ­¤ï¼Œå®ƒä»¬é¢ä¸´æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EditLordï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜ç¡®çš„ä»£ç ç¼–è¾‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé›‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ è€…ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ç®€æ´çš„å…ƒè§„åˆ™é›†ä½œä¸ºä»£ç ç¼–è¾‘è§„åˆ™ã€‚è¿™æ ·çš„è§„åˆ™é›†å°†ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æä¾›å¢å¼ºï¼Œç”¨äºå¾®è°ƒæˆ–è¾…åŠ©åŸºäºæç¤ºå’Œè¿­ä»£çš„ä»£ç ç¼–è¾‘ã€‚EditLordåœ¨ç¼–è¾‘æ€§èƒ½ä¸Šå¹³å‡é«˜å‡ºæœ€æ–°æŠ€æœ¯22.7%ï¼Œåœ¨ç¨³å¥æ€§ä¸Šé«˜å‡º58.1%ï¼ŒåŒæ—¶åœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºã€LMæ¨¡å‹å’Œç¼–è¾‘æ¨¡å¼ä¸‹å®ç°äº†20.2%æ›´é«˜çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15284v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦èƒ½åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„å‰æä¸‹å¼•å…¥æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å°†ä»£ç ç¼–è¾‘åˆ¶å®šä¸ºéšå¼å’Œç«¯åˆ°ç«¯çš„ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘ç¨‹åºæœ¬èº«åŒ…å«ç¦»æ•£å’Œæ˜¾å¼æ­¥éª¤çš„äº‹å®ã€‚å› æ­¤ï¼Œå®ƒä»¬å­˜åœ¨æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EditLordä»£ç ç¼–è¾‘æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜¾å¼åŒ–ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ å™¨ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ä»£ç ç¼–è¾‘è§„åˆ™ï¼Œå½¢æˆç®€æ´çš„å…ƒè§„åˆ™é›†ã€‚è¿™äº›è§„åˆ™é›†å°†ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æä¾›æ”¯æŒï¼Œç”¨äºå¾®è°ƒæˆ–è¾…åŠ©æç¤ºå’Œè¿­ä»£å¼ä»£ç ç¼–è¾‘ã€‚EditLordåœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºä¸­ï¼Œå¹³å‡ç¼–è¾‘æ€§èƒ½æé«˜22.7%ï¼Œç¨³å¥æ€§æé«˜58.1%ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§æé«˜20.2%ï¼Œä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„å‰æä¸‹å¼•å…¥æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°†ä»£ç ç¼–è¾‘è§†ä¸ºéšå¼å’Œç«¯åˆ°ç«¯çš„ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘çš„ç¦»æ•£å’Œæ˜¾å¼æ­¥éª¤ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>EditLordæ¡†æ¶ä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜¾å¼åŒ–ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ å™¨ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ä»£ç ç¼–è¾‘è§„åˆ™ã€‚</li>
<li>è¿™äº›è§„åˆ™é›†å¯ä»¥ç”¨äºå¾®è°ƒæˆ–è¾…åŠ©æç¤ºå’Œè¿­ä»£å¼ä»£ç ç¼–è¾‘ã€‚</li>
<li>EditLordé€šè¿‡ä½¿ç¼–è¾‘æ­¥éª¤æ˜¾å¼åŒ–ï¼Œæé«˜äº†ä»£ç ç¼–è¾‘çš„æ€§èƒ½ã€ç¨³å¥æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>åœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºä¸­ï¼ŒEditLordçš„è¡¨ç°ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ï¼Œå¹³å‡ç¼–è¾‘æ€§èƒ½æé«˜22.7%ï¼Œç¨³å¥æ€§æé«˜58.1%ï¼ŒåŠŸèƒ½æ­£ç¡®æ€§æé«˜20.2%ã€‚</li>
<li>EditLordæ¡†æ¶æœ‰æœ›ä¸ºæœªæ¥çš„ä»£ç ç¼–è¾‘å·¥ä½œæä¾›æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc0d391f30d1f382a0578615aa027c02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15264410816cd52f80f8d74350f5ba33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a1d85c0e98bae79079f2f6d8d7a061.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65e5aa27aba09d9764491a66544b5ac2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66dcd7c6ad52512cebe76c6755fda936.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Lemmanaid-Neuro-Symbolic-Lemma-Conjecturing"><a href="#Lemmanaid-Neuro-Symbolic-Lemma-Conjecturing" class="headerlink" title="Lemmanaid: Neuro-Symbolic Lemma Conjecturing"></a>Lemmanaid: Neuro-Symbolic Lemma Conjecturing</h2><p><strong>Authors:Yousef Alhessi, SÃ³lrÃºn Halla EinarsdÃ³ttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone</strong></p>
<p>Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Lemmanaid outperforms both neural and symbolic methods on test sets from Isabelleâ€™s HOL library and from its Archive of Formal Proofs, discovering between 29-39.5% of the gold standard human written lemmas. This is 8-15% more lemmas than the neural-only method. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization. </p>
<blockquote>
<p>è‡ªåŠ¨æ¨æµ‹æœ‰ç”¨ã€æœ‰è¶£å’Œæ–°é¢–çš„å‘½é¢˜å°†æå¤§åœ°æé«˜è‡ªåŠ¨åŒ–æ¨ç†å·¥å…·çš„æ€§èƒ½ï¼Œé™ä½è¯æ˜è¾…åŠ©å·¥å…·ä¸­å½¢å¼åŒ–æ•°å­¦çš„é—¨æ§›ã€‚ç„¶è€Œï¼Œè¿™å¯¹äºç¥ç»å’Œç¬¦å·æ–¹æ³•æ¥è¯´æ˜¯ä¸€é¡¹éå¸¸æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¿ˆå‡ºäº†æ„å»ºå®ç”¨ç¥ç»ç¬¦å·å‘½é¢˜æ¨æµ‹å·¥å…·çš„ç¬¬ä¸€æ­¥ï¼Œå³ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç¬¦å·æ–¹æ³•çš„Lemmanaidï¼Œå¹¶åœ¨é’ˆå¯¹Isabelleè¯æ˜åŠ©æ‰‹çš„è¯æ˜åº“ä¸­å¯¹å®ƒè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªLLMæ¥ç”Ÿæˆæè¿°å‘½é¢˜å½¢çŠ¶çš„å‘½é¢˜æ¨¡æ¿ï¼Œå¹¶ä½¿ç”¨ç¬¦å·æ–¹æ³•æ¥å¡«å……ç»†èŠ‚ã€‚æˆ‘ä»¬å°†Lemmanaidä¸è®­ç»ƒç”Ÿæˆå®Œæ•´å‘½é¢˜é™ˆè¿°çš„LLMä»¥åŠä»¥å‰çš„å®Œå…¨ç¬¦å·æ¨æµ‹æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨Isabelleçš„HOLåº“åŠå…¶å½¢å¼åŒ–è¯æ˜å­˜æ¡£çš„æµ‹è¯•é›†ä¸Šï¼ŒLemmanaidçš„è¡¨ç°ä¼˜äºç¥ç»å’Œç¬¦å·æ–¹æ³•ï¼Œå‘ç°äº†é»„é‡‘æ ‡å‡†äººç±»ä¹¦å†™å‘½é¢˜çš„29-39.5%ã€‚è¿™æ¯”ä»…ä½¿ç”¨ç¥ç»æ–¹æ³•å¤šå‡º8-15%çš„å‘½é¢˜ã€‚é€šè¿‡åˆ©ç”¨ç¬¦å·æ–¹æ³•å’Œç¥ç»æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºå¹¿æ³›çš„è¾“å…¥åŸŸç”Ÿæˆæœ‰ç”¨çš„å‘½é¢˜ï¼Œä¿ƒè¿›è®¡ç®—æœºè¾…åŠ©ç†è®ºå¼€å‘å’Œå½¢å¼åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04942v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†è‡ªåŠ¨æ¨æµ‹æœ‰ç”¨ã€æœ‰è¶£å’Œæ–°é¢–å‘½é¢˜çš„å·¥å…·å¯¹è‡ªåŠ¨åŒ–æ¨ç†å·¥å…·å’Œå½¢å¼åŒ–æ•°å­¦è¯æ˜åŠ©æ‰‹çš„é‡è¦æ€§ã€‚æ–‡ç« æå‡ºäº†é¦–ä¸ªå®ç”¨çš„ç¥ç»ç¬¦å·å‘½é¢˜æ¨æµ‹å·¥å…·Lemmanaidï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç¬¦å·æ–¹æ³•ï¼Œå¹¶åœ¨Isabelleè¯æ˜åŠ©æ‰‹çš„è¯æ˜åº“ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¯¥å·¥å…·é€šè¿‡è®­ç»ƒLLMç”Ÿæˆæè¿°å‘½é¢˜å½¢çŠ¶çš„æ¨¡æ¿ï¼Œå¹¶ä½¿ç”¨ç¬¦å·æ–¹æ³•å¡«å……ç»†èŠ‚ã€‚æ¯”è¾ƒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºä»…ä½¿ç”¨ç¥ç»ç½‘ç»œæˆ–å®Œå…¨ç¬¦å·çš„æ¨æµ‹æ–¹æ³•ï¼ŒLemmanaidåœ¨Isabelleçš„HOLåº“å’Œå½¢å¼åŒ–è¯æ˜æ¡£æ¡ˆæµ‹è¯•é›†ä¸Šçš„è¡¨ç°æ›´ä¼˜ï¼Œèƒ½å¤Ÿå‘ç°äººç±»ç¼–å†™çš„é»„é‡‘æ ‡å‡†å‘½é¢˜çš„29-39.5%ã€‚é€šè¿‡ç»“åˆç¬¦å·å’Œç¥ç»ç½‘ç»œæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œè¯¥å·¥å…·èƒ½å¤Ÿä¸ºå¹¿æ³›çš„è¾“å…¥é¢†åŸŸç”Ÿæˆæœ‰ç”¨çš„å‘½é¢˜ï¼Œä¿ƒè¿›è®¡ç®—æœºè¾…åŠ©ç†è®ºå‘å±•å’Œå½¢å¼åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>è‡ªåŠ¨æ¨æµ‹æœ‰ç”¨ã€æœ‰è¶£å’Œæ–°é¢–çš„å‘½é¢˜å¯¹äºæ”¹è¿›è‡ªåŠ¨åŒ–æ¨ç†å·¥å…·å’Œå½¢å¼åŒ–æ•°å­¦è¯æ˜åŠ©æ‰‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>Lemmanaidæ˜¯é¦–ä¸ªç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç¬¦å·æ–¹æ³•çš„å®ç”¨ç¥ç»ç¬¦å·å‘½é¢˜æ¨æµ‹å·¥å…·ã€‚</li>
<li>Lemmanaidé€šè¿‡åœ¨Isabelleè¯æ˜åŠ©æ‰‹çš„è¯æ˜åº“ä¸Šè®­ç»ƒå’Œè¯„ä¼°æ¥è¡¨ç°å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>LLMè¢«è®­ç»ƒç”Ÿæˆæè¿°å‘½é¢˜å½¢çŠ¶çš„æ¨¡æ¿ï¼Œè€Œç¬¦å·æ–¹æ³•ç”¨äºå¡«å……ç»†èŠ‚ã€‚</li>
<li>ä¸ä»…ä½¿ç”¨ç¥ç»ç½‘ç»œæˆ–å®Œå…¨ç¬¦å·çš„æ¨æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒLemmanaidè¡¨ç°æ›´ä¼˜ã€‚</li>
<li>Lemmanaidèƒ½å¤Ÿå‘ç°é»„é‡‘æ ‡å‡†ä¸­äººç±»ç¼–å†™çš„å‘½é¢˜çš„ç™¾åˆ†æ¯”åœ¨29-39.5%ä¹‹é—´ï¼Œè¿™æ¯”ç¥ç»ç½‘ç»œæ–¹æ³•é«˜å‡º8-15%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ec97a72e54ab5e4870764f599b8069fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2213c03b21672409f0f40566fa0398e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce746f169d7eccd7a30c27689e4f3e46.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Foundational-individual-Mobility-Prediction-Model-based-on-Open-Source-Large-Language-Models"><a href="#A-Foundational-individual-Mobility-Prediction-Model-based-on-Open-Source-Large-Language-Models" class="headerlink" title="A Foundational individual Mobility Prediction Model based on Open-Source   Large Language Models"></a>A Foundational individual Mobility Prediction Model based on Open-Source   Large Language Models</h2><p><strong>Authors:Zhenlin Qin, Leizhen Wang, Francisco Camara Pereira, Zhenliang Ma</strong></p>
<p>Large Language Models (LLMs) are widely applied to domain-specific tasks due to their massive general knowledge and remarkable inference capacities. Current studies on LLMs have shown immense potential in applying LLMs to model individual mobility prediction problems. However, most LLM-based mobility prediction models only train on specific datasets or use single well-designed prompts, leading to difficulty in adapting to different cities and users with diverse contexts. To fill these gaps, this paper proposes a unified fine-tuning framework to train a foundational open source LLM-based mobility prediction model. We conducted extensive experiments on six real-world mobility datasets to validate the proposed model. The results showed that the proposed model achieved the best performance in prediction accuracy and transferability over state-of-the-art models based on deep learning and LLMs. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶ä¸°å¯Œçš„é€šç”¨çŸ¥è¯†å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œè¢«å¹¿æ³›åº”ç”¨äºç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ã€‚å½“å‰å…³äºLLMçš„ç ”ç©¶å·²ç»æ˜¾ç¤ºå‡ºå°†LLMåº”ç”¨äºä¸ªäººå‡ºè¡Œé¢„æµ‹é—®é¢˜çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºLLMçš„å‡ºè¡Œé¢„æµ‹æ¨¡å‹åªåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨å•ä¸€ç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥é€‚åº”ä¸åŒåŸå¸‚å’Œç”¨æˆ·çš„ä¸åŒä¸Šä¸‹æ–‡ç¯å¢ƒã€‚ä¸ºäº†å¡«è¡¥è¿™äº›ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¾®è°ƒæ¡†æ¶æ¥è®­ç»ƒä¸€ä¸ªåŸºäºå¼€æºLLMçš„åŸºç¡€å‡ºè¡Œé¢„æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œçš„å‡ºè¡Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥éªŒè¯æ‰€æå‡ºæ¨¡å‹çš„æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŸºäºæ·±åº¦å­¦ä¹ å’ŒLLMçš„æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦å’Œå¯è¿ç§»æ€§æ–¹é¢è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16553v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶åœ¨ä¸€èˆ¬çŸ¥è¯†æ–¹é¢çš„ä¼˜åŠ¿åŠå¼ºå¤§çš„æ¨ç†èƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ã€‚åœ¨ä¸ªä½“ç§»åŠ¨é¢„æµ‹é—®é¢˜ä¸Šï¼ŒLLMså±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°LLMç§»åŠ¨é¢„æµ‹æ¨¡å‹ä»…åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒæˆ–ä¾èµ–ç²¾å¿ƒè®¾è®¡çš„ä¸€æ¬¡æ€§æç¤ºï¼Œè¿™é™åˆ¶äº†æ¨¡å‹é€‚åº”ä¸åŒåŸå¸‚å’Œç”¨æˆ·èƒŒæ™¯çš„èƒ½åŠ›ã€‚ä¸ºå¼¥è¡¥è¿™äº›ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªç»Ÿä¸€çš„å¾®è°ƒæ¡†æ¶æ¥è®­ç»ƒåŸºäºå¼€æºLLMçš„ç§»åŠ¨é¢„æµ‹æ¨¡å‹ã€‚åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œç§»åŠ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹ç²¾åº¦å’Œè¿ç§»èƒ½åŠ›æ–¹é¢ä¼˜äºåŸºäºæ·±åº¦å­¦ä¹ å’ŒLLMçš„å…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¢«å¹¿æ³›åº”ç”¨äºç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œå¾—ç›Šäºå…¶å¤§è§„æ¨¡çš„ä¸€èˆ¬çŸ¥è¯†å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸ªä½“ç§»åŠ¨é¢„æµ‹é—®é¢˜ä¸Šï¼ŒLLMså…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰LLMç§»åŠ¨é¢„æµ‹æ¨¡å‹ä¸»è¦å±€é™äºåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè®­ç»ƒæˆ–ä¾èµ–ä¸€æ¬¡æ€§æç¤ºï¼Œç¼ºä¹é€‚åº”ä¸åŒåŸå¸‚å’Œç”¨æˆ·èƒŒæ™¯çš„èƒ½åŠ›ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„å¾®è°ƒæ¡†æ¶æ¥è®­ç»ƒåŸºäºå¼€æºLLMçš„ç§»åŠ¨é¢„æµ‹æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒæ–¹å¼å’Œåˆ©ç”¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦å’Œè¿ç§»èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œç§»åŠ¨æ•°æ®é›†ä¸Šçš„é¢„æµ‹æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-853b9bcd5210c0ad1784b4309d157831.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a1f4091f4a29bfc032860ad13e3829f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="â€œI-know-myself-better-but-not-really-greatlyâ€-How-Well-Can-LLMs-Detect-and-Explain-LLM-Generated-Texts"><a href="#â€œI-know-myself-better-but-not-really-greatlyâ€-How-Well-Can-LLMs-Detect-and-Explain-LLM-Generated-Texts" class="headerlink" title="â€œI know myself better, but not really greatlyâ€: How Well Can LLMs Detect   and Explain LLM-Generated Texts?"></a>â€œI know myself better, but not really greatlyâ€: How Well Can LLMs Detect   and Explain LLM-Generated Texts?</h2><p><strong>Authors:Jiazhou Ji, Jie Guo, Weidong Qiu, Zheng Huang, Yang Xu, Xinru Lu, Xiaoyu Jiang, Ruizhe Li, Shujun Li</strong></p>
<p>Distinguishing between human- and LLM-generated texts is crucial given the risks associated with misuse of LLMs. This paper investigates detection and explanation capabilities of current LLMs across two settings: binary (human vs. LLM-generated) and ternary classification (including an &#96;&#96;undecidedâ€™â€™ class). We evaluate 6 close- and open-source LLMs of varying sizes and find that self-detection (LLMs identifying their own outputs) consistently outperforms cross-detection (identifying outputs from other LLMs), though both remain suboptimal. Introducing a ternary classification framework improves both detection accuracy and explanation quality across all models. Through comprehensive quantitative and qualitative analyses using our human-annotated dataset, we identify key explanation failures, primarily reliance on inaccurate features, hallucinations, and flawed reasoning. Our findings underscore the limitations of current LLMs in self-detection and self-explanation, highlighting the need for further research to address overfitting and enhance generalizability. </p>
<blockquote>
<p>åŒºåˆ†äººç±»å’ŒLLMç”Ÿæˆæ–‡æœ¬è‡³å…³é‡è¦ï¼Œå› ä¸ºæ»¥ç”¨LLMå­˜åœ¨é£é™©ã€‚æœ¬æ–‡è°ƒæŸ¥äº†å½“å‰LLMçš„æ£€æµ‹å’Œè§£é‡Šèƒ½åŠ›ï¼Œæ¶‰åŠä¸¤ç§è®¾ç½®ï¼šäºŒå…ƒï¼ˆäººç±»ä¸LLMç”Ÿæˆï¼‰å’Œä¸‰å…ƒåˆ†ç±»ï¼ˆåŒ…æ‹¬â€œæœªå†³å®šâ€ç±»åˆ«ï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†6ç§å¤§å°å’Œæ¥æºå„å¼‚çš„LLMï¼Œå‘ç°è‡ªæˆ‘æ£€æµ‹ï¼ˆLLMè¯†åˆ«å…¶è‡ªèº«è¾“å‡ºï¼‰å§‹ç»ˆä¼˜äºäº¤å‰æ£€æµ‹ï¼ˆè¯†åˆ«å…¶ä»–LLMçš„è¾“å‡ºï¼‰ï¼Œå°½ç®¡ä¸¤è€…éƒ½ä¸å¤Ÿç†æƒ³ã€‚å¼•å…¥ä¸‰å…ƒåˆ†ç±»æ¡†æ¶æé«˜äº†æ‰€æœ‰æ¨¡å‹çš„æ£€æµ‹å‡†ç¡®æ€§å’Œè§£é‡Šè´¨é‡ã€‚é€šè¿‡ä½¿ç”¨æˆ‘ä»¬äººç±»æ³¨é‡Šçš„æ•°æ®é›†è¿›è¡Œçš„ç»¼åˆå®šé‡å’Œå®šæ€§åˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸»è¦çš„è§£é‡Šå¤±è´¥åŸå› ï¼Œä¸»è¦æ˜¯ä¾èµ–ä¸å‡†ç¡®çš„åŠŸèƒ½ã€å¹»æƒ³å’Œæœ‰ç¼ºé™·çš„æ¨ç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å½“å‰LLMåœ¨è‡ªæˆ‘æ£€æµ‹å’Œè‡ªæˆ‘è§£é‡Šæ–¹é¢çš„å±€é™æ€§ï¼Œå¼ºè°ƒäº†éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ä»¥è§£å†³è¿‡åº¦æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºé€šç”¨æ€§çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12743v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>å½“å‰è®ºæ–‡æ¢è®¨äº†LLMåœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„è‡ªæˆ‘æ£€æµ‹ä¸è§£é‡Šèƒ½åŠ›ã€‚ç ”ç©¶æ¶‰åŠäºŒå…ƒï¼ˆäººç±»ä¸LLMç”Ÿæˆæ–‡æœ¬ï¼‰ä¸ä¸‰å…ƒåˆ†ç±»ï¼ˆåŒ…æ‹¬â€œæœªå†³å®šâ€ç±»åˆ«ï¼‰çš„åœºæ™¯ã€‚è¯„ä¼°äº†ä¸åŒè§„æ¨¡å’Œå¼€æºçš„LLMæ¨¡å‹ï¼Œå‘ç°è‡ªæˆ‘æ£€æµ‹æ€§èƒ½é€šå¸¸ä¼˜äºè·¨æ£€æµ‹ï¼Œä½†äºŒè€…éƒ½å­˜åœ¨ä¸è¶³ã€‚å¼•å…¥ä¸‰å…ƒåˆ†ç±»æ¡†æ¶æœ‰åŠ©äºæé«˜æ‰€æœ‰æ¨¡å‹çš„æ£€æµ‹å‡†ç¡®ç‡å’Œè§£é‡Šè´¨é‡ã€‚é€šè¿‡ç»¼åˆå®šé‡å’Œå®šæ€§åˆ†æï¼Œç ”ç©¶æŒ‡å‡ºäº†å…³é”®çš„è§£é‡Šå¤±è´¥åŸå› ï¼Œå¦‚ä¾èµ–ä¸å‡†ç¡®ç‰¹å¾ã€è™šæ„å†…å®¹å’Œæ¨ç†ç¼ºé™·ã€‚å½“å‰LLMåœ¨è‡ªæˆ‘æ£€æµ‹å’Œè§£é‡Šæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ä»¥è§£å†³è¿‡åº¦æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ–‡æœ¬ç”Ÿæˆä¸­çš„è‡ªæˆ‘æ£€æµ‹å’Œè§£é‡Šèƒ½åŠ›æ˜¯å½“å‰ç ”ç©¶çš„å…³é”®é¢†åŸŸã€‚</li>
<li>äºŒå…ƒåˆ†ç±»ä¸ä¸‰å…ƒåˆ†ç±»æ˜¯è¯„ä¼°LLMæ£€æµ‹èƒ½åŠ›çš„ä¸¤ç§ä¸»è¦åœºæ™¯ã€‚</li>
<li>è‡ªæˆ‘æ£€æµ‹æ€§èƒ½é€šå¸¸ä¼˜äºè·¨æ£€æµ‹ï¼Œä½†äºŒè€…éƒ½æœ‰å±€é™æ€§ã€‚</li>
<li>å¼•å…¥ä¸‰å…ƒåˆ†ç±»æ¡†æ¶å¯ä»¥æé«˜æ£€æµ‹å‡†ç¡®ç‡å’Œè§£é‡Šè´¨é‡ã€‚</li>
<li>å½“å‰LLMåœ¨è‡ªæˆ‘æ£€æµ‹æ–¹é¢å­˜åœ¨è¿‡åº¦æ‹Ÿåˆå’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„å±€é™æ€§ã€‚</li>
<li>LLMåœ¨è§£é‡Šå¤±è´¥æ—¶ï¼Œå¸¸å¸¸ä¾èµ–äºä¸å‡†ç¡®ç‰¹å¾ã€è™šæ„å†…å®¹å’Œæœ‰ç¼ºé™·çš„æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-286305fec58962f3aa79c2c986838434.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-398d3c1e36740f9ae12989ab73657bea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-238026fe88b2a0869b73a0c2e19541fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74ec416a335bf9e43a08d963cbd785b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f09c98165c8a73bbe4cc014ca79a4e4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Potential-of-Encoder-free-Architectures-in-3D-LMMs"><a href="#Exploring-the-Potential-of-Encoder-free-Architectures-in-3D-LMMs" class="headerlink" title="Exploring the Potential of Encoder-free Architectures in 3D LMMs"></a>Exploring the Potential of Encoder-free Architectures in 3D LMMs</h2><p><strong>Authors:Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao</strong></p>
<p>Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at <a target="_blank" rel="noopener" href="https://github.com/Ivan-Tang-3D/ENEL">https://github.com/Ivan-Tang-3D/ENEL</a> </p>
<blockquote>
<p>æ— ç¼–ç å™¨æ¶æ„å·²åœ¨2Dè§†è§‰é¢†åŸŸè¿›è¡Œäº†åˆæ­¥æ¢ç´¢ï¼Œä½†å…¶åœ¨3Dç†è§£åœºæ™¯ä¸­çš„æœ‰æ•ˆåº”ç”¨ä»æ˜¯å¼€æ”¾æ€§é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¢è®¨äº†æ— ç¼–ç å™¨æ¶æ„åœ¨ç¼“è§£åŸºäºç¼–ç å™¨çš„3Då¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬æ— æ³•é€‚åº”ä¸åŒçš„ç‚¹äº‘åˆ†è¾¨ç‡ä»¥åŠç¼–ç å™¨ä¸­çš„ç‚¹ç‰¹å¾ä¸ç¬¦åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰éœ€æ±‚ã€‚æˆ‘ä»¬ç¡®å®šäº†ä½¿3D LMMå»é™¤ç¼–ç å™¨å¹¶å…è®¸LLMæ‰¿æ‹…3Dç¼–ç å™¨è§’è‰²çš„å…³é”®æ–¹é¢ï¼š1ï¼‰æˆ‘ä»¬åœ¨é¢„è®­ç»ƒé˜¶æ®µæå‡ºäº†LLMåµŒå…¥è¯­ä¹‰ç¼–ç ç­–ç•¥ï¼Œæ¢ç´¢äº†å„ç§ç‚¹äº‘è‡ªç›‘ç£æŸå¤±çš„å½±å“ã€‚å¹¶æå‡ºäº†æ··åˆè¯­ä¹‰æŸå¤±æ¥æå–é«˜çº§è¯­ä¹‰ã€‚2ï¼‰æˆ‘ä»¬åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µå¼•å…¥äº†åˆ†å±‚å‡ ä½•èšåˆç­–ç•¥ã€‚è¿™å°†å½’çº³åç½®èå…¥LLMå±‚ï¼Œä»¥å…³æ³¨ç‚¹äº‘çš„å±€éƒ¨ç»†èŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªæ— ç¼–ç å™¨3D LMMï¼ŒENELã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ShapeLLM-13Bç›¸æŠ—è¡¡ï¼Œåœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†55.10%ã€50.98%å’Œ43.10%çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ— ç¼–ç å™¨æ¶æ„åœ¨3Dç†è§£é¢†åŸŸæ›¿ä»£åŸºäºç¼–ç å™¨çš„æ¶æ„å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Ivan-Tang-3D/ENEL">https://github.com/Ivan-Tang-3D/ENEL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09620v3">PDF</a> During the review process, we discovered that a portion of the test   dataset used in our submission contained content that may have infringed upon   the commercial copyrights of others. Due to the conflict regarding these   commercial copyrights, we have unfortunately had to retract the submission</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¢è®¨äº†æ— ç¼–ç å™¨æ¶æ„åœ¨ç¼“è§£åŸºäºç¼–ç å™¨çš„ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶è§£å†³äº†é€‚åº”ä¸åŒç‚¹äº‘åˆ†è¾¨ç‡çš„é—®é¢˜ä»¥åŠç¼–ç å™¨ç‚¹ç‰¹å¾ä¸ç¬¦åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯­ä¹‰éœ€æ±‚çš„é—®é¢˜ã€‚ç ”ç©¶æå‡ºäº†åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨LLMåµŒå…¥è¯­ä¹‰ç¼–ç ç­–ç•¥ï¼Œå¹¶æ¢ç´¢äº†å„ç§ç‚¹äº‘è‡ªç›‘ç£æŸå¤±çš„å½±å“ï¼Œå¹¶æå‡ºäº†æ··åˆè¯­ä¹‰æŸå¤±ä»¥æå–é«˜çº§è¯­ä¹‰ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¼•å…¥äº†åˆ†å±‚å‡ ä½•èšåˆç­–ç•¥ï¼Œå°†å½’çº³åç½®èå…¥LLMå±‚ï¼Œä»¥å…³æ³¨ç‚¹äº‘çš„å±€éƒ¨ç»†èŠ‚ã€‚æœ€ç»ˆï¼Œç ”ç©¶æ¨å‡ºäº†é¦–ä¸ªæ— ç¼–ç å™¨3D LMMï¼ŒENELã€‚è¯¥æ¨¡å‹çš„7Bæ€§èƒ½ä¸å½“å‰çš„å…ˆè¿›æ¨¡å‹ShapeLLM-13Bç›¸å½“ï¼Œåœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†55.10%ã€50.98%å’Œ43.10%çš„å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ— ç¼–ç å™¨æ¶æ„åœ¨ä¸‰ç»´ç†è§£é¢†åŸŸæ›¿ä»£ç¼–ç å™¨æ¶æ„å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ— ç¼–ç å™¨æ¶æ„åœ¨ä¸‰ç»´è§†è§‰é¢†åŸŸçš„åº”ç”¨æ˜¯æ–°é¢–çš„ï¼Œå¯¹äºè§£å†³ç¼–ç å™¨åŸºäºçš„ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æŒ‘æˆ˜å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†LLMåµŒå…¥è¯­ä¹‰ç¼–ç ç­–ç•¥ï¼Œé€šè¿‡åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨è‡ªç›‘ç£æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>æ··åˆè¯­ä¹‰æŸå¤±è¢«ç”¨æ¥æå–é«˜çº§è¯­ä¹‰ï¼Œè¿™æ˜¯å®ç°é«˜æ•ˆä¸‰ç»´ç†è§£çš„å…³é”®ã€‚</li>
<li>åˆ†å±‚å‡ ä½•èšåˆç­–ç•¥åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µè¢«å¼•å…¥ï¼Œä»¥å…³æ³¨ç‚¹äº‘çš„å±€éƒ¨ç»†èŠ‚ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨å‡ºçš„æ— ç¼–ç å™¨3D LMMï¼ŒENELï¼Œåœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œè¯æ˜äº†è¯¥æ¶æ„çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ENELçš„æ€§èƒ½ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼Œå±•ç¤ºäº†æ— ç¼–ç å™¨æ¶æ„åœ¨ä¸‰ç»´ç†è§£é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2b27ff9324f7d878ecc3fdeb643dd91b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c01aa1281ce5577c7d9714bb9b54c07a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42a2ef0f117bd00f1bbfe04267ca733f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f77bd8e38aed258fde6e920fd9aefd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-533ef7105ecdabdbc0b93ba25df04bf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0962701a83416d82e29054661185dc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3985fb578795947e31dff68211e343bb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DecDEC-A-Systems-Approach-to-Advancing-Low-Bit-LLM-Quantization"><a href="#DecDEC-A-Systems-Approach-to-Advancing-Low-Bit-LLM-Quantization" class="headerlink" title="DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization"></a>DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization</h2><p><strong>Authors:Yeonhong Park, Jake Hyun, Hojoon Kim, Jae W. Lee</strong></p>
<p>Quantization of Large Language Models (LLMs) has recently gained popularity, particularly for on-device settings with limited hardware resources. While efficient, quantization inevitably degrades model quality, especially in aggressive low-bit settings such as 3-bit and 4-bit precision. In this paper, we propose DecDEC, an inference scheme that improves the quality of low-bit LLMs while preserving the key benefits of quantization: GPU memory savings and latency reduction. DecDEC stores the residual matrix â€“ the difference between full-precision and quantized weights â€“ in CPU, and dynamically fetches the residuals for only a small portion of the weights. This portion corresponds to the salient channels, marked by activation outliers, with the fetched residuals helping to correct quantization errors in these channels. Salient channels are identified dynamically at each decoding step by analyzing the input activations â€“ this enables adaptation to the dynamic nature of activation distribution, thus maximizing the effectiveness of error compensation. We demonstrate the effectiveness of DecDEC by augmenting state-of-the-art quantization methods. For example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model from 10.15 to 9.12 â€“ outperforming its 3.5-bit counterpart â€“ while adding less than 0.0003% to GPU memory usage and incurring only a 1.7% inference slowdown on NVIDIA RTX 4050 Mobile. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–æœ€è¿‘å˜å¾—æµè¡Œèµ·æ¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬ä»¶èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šã€‚è™½ç„¶æ•ˆç‡å¾ˆé«˜ï¼Œä½†é‡åŒ–ä¸å¯é¿å…åœ°ä¼šé™ä½æ¨¡å‹è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æç«¯çš„ä½æ¯”ç‰¹è®¾ç½®ï¼ˆå¦‚3ä½å’Œ4ä½ç²¾åº¦ï¼‰ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DecDECï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ–¹æ¡ˆï¼Œæ—¨åœ¨æé«˜ä½æ¯”ç‰¹LLMçš„è´¨é‡ï¼ŒåŒæ—¶ä¿ç•™é‡åŒ–çš„ä¸»è¦ä¼˜ç‚¹ï¼šèŠ‚çœGPUå†…å­˜å’Œå‡å°‘å»¶è¿Ÿã€‚DecDECåœ¨CPUä¸­å­˜å‚¨æ®‹å·®çŸ©é˜µï¼ˆå³å…¨ç²¾åº¦å’Œé‡åŒ–æƒé‡ä¹‹é—´çš„å·®å¼‚ï¼‰ï¼Œå¹¶åŠ¨æ€æå–æƒé‡çš„å°éƒ¨åˆ†æ®‹å·®ã€‚è¿™éƒ¨åˆ†å¯¹åº”äºæ˜¾è‘—é€šé“ï¼Œç”±æ¿€æ´»å¼‚å¸¸å€¼æ ‡è®°ï¼Œæå–çš„æ®‹å·®æœ‰åŠ©äºæ ¡æ­£è¿™äº›é€šé“çš„é‡åŒ–è¯¯å·®ã€‚åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ï¼Œé€šè¿‡åŠ¨æ€åˆ†æè¾“å…¥æ¿€æ´»æ¥è¯†åˆ«æ˜¾è‘—é€šé“â€”â€”è¿™æœ‰åŠ©äºé€‚åº”æ¿€æ´»åˆ†å¸ƒçš„åŠ¨æ€ç‰¹æ€§ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°æé«˜è¯¯å·®è¡¥å¿çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡å¢å¼ºæœ€æ–°çš„é‡åŒ–æ–¹æ³•æ¥è¯æ˜DecDECçš„æœ‰æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼ŒDecDECå°†3ä½Llama-3-8B-Instructæ¨¡å‹çš„å›°æƒ‘åº¦ä»10.15é™ä½åˆ°9.12â€”â€”è¶…è¿‡äº†å…¶3.5ä½å¯¹åº”çš„æ¨¡å‹æ€§èƒ½â€”â€”åŒæ—¶GPUå†…å­˜ä½¿ç”¨ç‡ä»…å¢åŠ ä¸åˆ°0.0003%ï¼Œå¹¶ä¸”åœ¨NVIDIA RTX 4050 Mobileä¸Šçš„æ¨ç†é€Ÿåº¦ä»…ä¸‹é™1.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20185v2">PDF</a> OSDI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡åŒ–è¿‘æœŸå—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬ä»¶èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šã€‚è™½ç„¶é‡åŒ–èƒ½æé«˜æ•ˆç‡ï¼Œä½†ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´æ¨¡å‹è´¨é‡ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨3ä½å’Œ4ä½ç²¾åº¦çš„æ¿€è¿›ä½ä½è®¾ç½®ä¸­ã€‚æœ¬æ–‡æå‡ºDecDECæ¨ç†æ–¹æ¡ˆï¼Œæ—¨åœ¨æé«˜ä½ä½LLMçš„è´¨é‡ï¼ŒåŒæ—¶ä¿æŒé‡åŒ–çš„ä¸»è¦ä¼˜åŠ¿ï¼šèŠ‚çœGPUå†…å­˜å’Œå‡å°‘å»¶è¿Ÿã€‚DecDECå°†æ®‹å·®çŸ©é˜µï¼ˆå…¨ç²¾åº¦å’Œé‡åŒ–æƒé‡ä¹‹é—´çš„å·®å¼‚ï¼‰å­˜å‚¨åœ¨CPUä¸­ï¼Œå¹¶ä»…åŠ¨æ€è·å–ä¸€å°éƒ¨åˆ†æƒé‡çš„æ®‹å·®ã€‚è¿™éƒ¨åˆ†å¯¹åº”äºç”±æ¿€æ´»å¼‚å¸¸å€¼æ ‡è®°çš„æ˜¾è‘—é€šé“ï¼Œè·å–çš„æ®‹å·®æœ‰åŠ©äºæ ¡æ­£è¿™äº›é€šé“ä¸­çš„é‡åŒ–è¯¯å·®ã€‚DecDECé€šè¿‡åŠ¨æ€åˆ†æè¾“å…¥æ¿€æ´»æ¥è¯†åˆ«æ˜¾è‘—é€šé“ï¼Œè¿™ä½¿å…¶èƒ½å¤Ÿé€‚åº”æ¿€æ´»åˆ†å¸ƒçš„åŠ¨æ€æ€§è´¨ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°æé«˜è¯¯å·®è¡¥å¿çš„æ•ˆæœã€‚é€šè¿‡å¢å¼ºæœ€å…ˆè¿›é‡åŒ–æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒDecDECå°†3ä½Llama-3-8B-Instructæ¨¡å‹çš„å›°æƒ‘åº¦ä»10.15é™ä½åˆ°9.12ï¼Œè¶…è¶Šäº†å…¶3.5ä½åŒè¡Œï¼ŒåŒæ—¶GPUå†…å­˜ä½¿ç”¨ç‡ä»…å¢åŠ ä¸åˆ°0.0003%ï¼Œå¹¶ä¸”åœ¨NVIDIA RTX 4050 Mobileä¸Šçš„æ¨ç†é€Ÿåº¦ä»…ä¸‹é™1.7%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMé‡åŒ–æ—¨åœ¨æé«˜æ•ˆç‡å’Œé€‚åº”ç¡¬ä»¶èµ„æºé™åˆ¶ï¼Œä½†ä¼šå¯¼è‡´æ¨¡å‹è´¨é‡ä¸‹é™ã€‚</li>
<li>DecDECæ˜¯ä¸€ç§æ¨ç†æ–¹æ¡ˆï¼Œæ—¨åœ¨æé«˜ä½ä½LLMçš„è´¨é‡ï¼ŒåŒæ—¶ä¿æŒé‡åŒ–çš„ä¼˜åŠ¿ã€‚</li>
<li>DecDECé€šè¿‡å­˜å‚¨å’ŒåŠ¨æ€è·å–æ®‹å·®çŸ©é˜µæ¥æé«˜é‡åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>æ˜¾è‘—é€šé“ç”±æ¿€æ´»å¼‚å¸¸å€¼æ ‡è®°ï¼Œè·å–çš„æ®‹å·®æœ‰åŠ©äºæ ¡æ­£è¿™äº›é€šé“ä¸­çš„é‡åŒ–è¯¯å·®ã€‚</li>
<li>DecDECèƒ½åŠ¨æ€é€‚åº”æ¿€æ´»åˆ†å¸ƒçš„å˜åŒ–ï¼Œæœ€å¤§é™åº¦åœ°æé«˜è¯¯å·®è¡¥å¿æ•ˆæœã€‚</li>
<li>DecDECå¢å¼ºäº†ç°æœ‰é‡åŒ–æ–¹æ³•çš„æ•ˆæœï¼Œé™ä½äº†æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-132688a00e0c207f75733700c1ae4eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a8f46d74ac06a76252cd99e774d3c29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a82d62bab1bb4d532748fec18170b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d917ea369dce0ed4a4f69f7bec72d7bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-730f854e86168b7df21602179f963bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45fda78e5376e5484d1cc64bcaaa3aa8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de7b26487526c9e22afc2524021c32eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0681639e9647a3f3c5d53d49bc52bce1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FinGPT-Enhancing-Sentiment-Based-Stock-Movement-Prediction-with-Dissemination-Aware-and-Context-Enriched-LLMs"><a href="#FinGPT-Enhancing-Sentiment-Based-Stock-Movement-Prediction-with-Dissemination-Aware-and-Context-Enriched-LLMs" class="headerlink" title="FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with   Dissemination-Aware and Context-Enriched LLMs"></a>FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with   Dissemination-Aware and Context-Enriched LLMs</h2><p><strong>Authors:Yixuan Liang, Yuncong Liu, Neng Wang, Hongyang Yang, Boyu Zhang, Christina Dan Wang</strong></p>
<p>Financial sentiment analysis is crucial for understanding the influence of news on stock prices. Recently, large language models (LLMs) have been widely adopted for this purpose due to their advanced text analysis capabilities. However, these models often only consider the news content itself, ignoring its dissemination, which hampers accurate prediction of short-term stock movements. Additionally, current methods often lack sufficient contextual data and explicit instructions in their prompts, limiting LLMsâ€™ ability to interpret news. In this paper, we propose a data-driven approach that enhances LLM-powered sentiment-based stock movement predictions by incorporating news dissemination breadth, contextual data, and explicit instructions. We cluster recent company-related news to assess its reach and influence, enriching prompts with more specific data and precise instructions. This data is used to construct an instruction tuning dataset to fine-tune an LLM for predicting short-term stock price movements. Our experimental results show that our approach improves prediction accuracy by 8% compared to existing methods. </p>
<blockquote>
<p>é‡‘èæƒ…æ„Ÿåˆ†æå¯¹äºç†è§£æ–°é—»å¯¹è‚¡ç¥¨ä»·æ ¼çš„å½±å“è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å…ˆè¿›çš„æ–‡æœ¬åˆ†æèƒ½åŠ›ï¼Œå› æ­¤è¢«å¹¿æ³›åº”ç”¨äºæ­¤ç›®çš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åªè€ƒè™‘æ–°é—»å†…å®¹æœ¬èº«ï¼Œè€Œå¿½ç•¥äº†å…¶ä¼ æ’­æƒ…å†µï¼Œè¿™é˜»ç¢äº†çŸ­æœŸè‚¡ç¥¨èµ°åŠ¿çš„å‡†ç¡®é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®çš„æŒ‡ä»¤æç¤ºï¼Œé™åˆ¶äº†LLMè§£é‡Šæ–°é—»çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ–°é—»ä¼ æ’­çš„å¹¿åº¦ã€ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®çš„æŒ‡ä»¤ï¼Œæé«˜äº†åŸºäºLLMçš„æƒ…æ„Ÿé©±åŠ¨è‚¡ç¥¨èµ°åŠ¿é¢„æµ‹ã€‚æˆ‘ä»¬å¯¹æœ€è¿‘çš„ä¸å…¬å¸ç›¸å…³çš„æ–°é—»è¿›è¡Œèšç±»ï¼Œä»¥è¯„ä¼°å…¶ä¼ æ’­èŒƒå›´å’Œå½±å“åŠ›ï¼Œå¹¶ä¸ºæç¤ºæ·»åŠ äº†æ›´å…·ä½“çš„æ•°æ®å’Œç²¾ç¡®æŒ‡ä»¤ã€‚è¿™äº›æ•°æ®ç”¨äºæ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œä»¥å¾®è°ƒLLMï¼Œä»¥é¢„æµ‹çŸ­æœŸè‚¡ç¥¨ä»·æ ¼èµ°åŠ¿ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é¢„æµ‹ç²¾åº¦æé«˜äº†8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10823v2">PDF</a> 1st Workshop on Preparing Good Data for Generative AI: Challenges and   Approaches@ AAAI 2025, ai4finance.org</p>
<p><strong>æ‘˜è¦</strong><br>    é‡‘èæƒ…æ„Ÿåˆ†æå¯¹äºç†è§£æ–°é—»å¯¹è‚¡ç¥¨ä»·æ ¼çš„å½±å“è‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶å…ˆè¿›çš„æ–‡æœ¬åˆ†æèƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºæ­¤ç›®çš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åªè€ƒè™‘æ–°é—»å†…å®¹æœ¬èº«ï¼Œå¿½ç•¥äº†æ–°é—»çš„ä¼ æ’­æƒ…å†µï¼Œè¿™é˜»ç¢äº†çŸ­æœŸè‚¡ç¥¨èµ°åŠ¿çš„å‡†ç¡®é¢„æµ‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ–°é—»çš„ä¼ æ’­å¹¿åº¦ã€ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®æŒ‡ä»¤ï¼Œæé«˜åŸºäºLLMçš„æƒ…æ„Ÿé©±åŠ¨è‚¡ç¥¨èµ°åŠ¿é¢„æµ‹èƒ½åŠ›ã€‚é€šè¿‡èšç±»ä¸å…¬å¸ç›¸å…³çš„æ–°é—»æ¥è¯„ä¼°å…¶ä¼ æ’­èŒƒå›´å’Œå½±å“åŠ›ï¼Œä½¿ç”¨æ›´å…·ä½“çš„æ•°æ®å’Œç²¾ç¡®æŒ‡ä»¤ä¸°å¯Œæç¤ºã€‚è¯¥æ•°æ®ç”¨äºæ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œå¾®è°ƒLLMä»¥é¢„æµ‹çŸ­æœŸè‚¡ç¥¨ä»·æ ¼èµ°åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æé«˜äº†8%çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>é‡‘èæƒ…æ„Ÿåˆ†æå¯¹äºç†è§£æ–°é—»å¯¹è‚¡ç¥¨ä»·æ ¼çš„å½±å“éå¸¸é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æƒ…æ„Ÿåˆ†ææ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰LLMæ¨¡å‹å¿½ç•¥äº†æ–°é—»çš„ä¼ æ’­æƒ…å†µï¼Œé™åˆ¶äº†çŸ­æœŸè‚¡ç¥¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç»“åˆæ–°é—»çš„ä¼ æ’­å¹¿åº¦ã€ä¸Šä¸‹æ–‡æ•°æ®å’Œæ˜ç¡®æŒ‡ä»¤ï¼Œæé«˜LLMåœ¨æƒ…æ„Ÿé©±åŠ¨è‚¡ç¥¨é¢„æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡èšç±»å…¬å¸ç›¸å…³æ–°é—»æ¥è¯„ä¼°å…¶å½±å“åŠ›å’Œä¼ æ’­èŒƒå›´ã€‚</li>
<li>ä½¿ç”¨æ›´ä¸°å¯Œã€æ›´å…·ä½“çš„æ•°æ®å’Œç²¾ç¡®æŒ‡ä»¤æ¥ä¸°å¯Œæç¤ºï¼Œæ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†çŸ­æœŸè‚¡ç¥¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-018f6897ae8c8044f6d050a41d10b9f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4e262c8cd56bc4ec2af71a0b35e28e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ff33bb2edc39a3d71ac7198957de718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2ede9e6bd19551df6f47fe39e567a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b33247fdf4fd6de0009f37fcabc11f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dccc3fa4dc3b6354945d9af3d8beabef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-341390bb3917c9b563716050c585e673.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0469eff4eb091deead82cb88e7e96c6e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Sensitive-Content-Classification-in-Social-Media-A-Holistic-Resource-and-Evaluation"><a href="#Sensitive-Content-Classification-in-Social-Media-A-Holistic-Resource-and-Evaluation" class="headerlink" title="Sensitive Content Classification in Social Media: A Holistic Resource   and Evaluation"></a>Sensitive Content Classification in Social Media: A Holistic Resource   and Evaluation</h2><p><strong>Authors:Dimosthenis Antypas, Indira Sen, Carla Perez-Almendros, Jose Camacho-Collados, Francesco Barbieri</strong></p>
<p>The detection of sensitive content in large datasets is crucial for ensuring that shared and analysed data is free from harmful material. However, current moderation tools, such as external APIs, suffer from limitations in customisation, accuracy across diverse sensitive categories, and privacy concerns. Additionally, existing datasets and open-source models focus predominantly on toxic language, leaving gaps in detecting other sensitive categories such as substance abuse or self-harm. In this paper, we put forward a unified dataset tailored for social media content moderation across six sensitive categories: conflictual language, profanity, sexually explicit material, drug-related content, self-harm, and spam. By collecting and annotating data with consistent retrieval strategies and guidelines, we address the shortcomings of previous focalised research. Our analysis demonstrates that fine-tuning large language models (LLMs) on this novel dataset yields significant improvements in detection performance compared to open off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which underperform by 10-15% overall. This limitation is even more pronounced on popular moderation APIs, which cannot be easily tailored to specific sensitive content categories, among others. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸­æ£€æµ‹æ•æ„Ÿå†…å®¹å¯¹äºç¡®ä¿å…±äº«å’Œåˆ†æçš„æ•°æ®ä¸å«æœ‰å®³ææ–™è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„å®¡æ ¸å·¥å…·ï¼ˆå¦‚å¤–éƒ¨APIï¼‰åœ¨å®šåˆ¶æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯¹ä¸åŒæ•æ„Ÿç±»åˆ«çš„å‡†ç¡®æ€§ä¹Ÿæœ‰é—®é¢˜ï¼Œè¿˜å¼•å‘éšç§æ‹…å¿§ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ•°æ®é›†å’Œå¼€æºæ¨¡å‹ä¸»è¦å…³æ³¨æœ‰æ¯’è¯­è¨€ï¼Œåœ¨æ£€æµ‹å…¶ä»–æ•æ„Ÿç±»åˆ«ï¼ˆå¦‚æ»¥ç”¨ç‰©è´¨æˆ–è‡ªæ®‹ï¼‰æ–¹é¢å­˜åœ¨ç©ºç™½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ï¼Œä¸“ä¸ºç¤¾äº¤åª’ä½“å†…å®¹å®¡æ ¸å…­ä¸ªæ•æ„Ÿç±»åˆ«è€Œè®¾è®¡ï¼šå†²çªè¯­è¨€ã€ç²—ä¿—è¯­è¨€ã€æ€§æ˜ç¡®ææ–™ã€è¯ç‰©ç›¸å…³å†…å®¹ã€è‡ªæ®‹å’Œåƒåœ¾å¹¿å‘Šã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ä¸€è‡´çš„æ•°æ®æ£€ç´¢ç­–ç•¥å’ŒæŒ‡å—æ¥æ”¶é›†å’Œæ³¨é‡Šæ•°æ®ï¼Œè§£å†³äº†ä»¥å¾€ç ”ç©¶ä¸­çš„ä¸è¶³ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸å¸‚é¢ä¸Šçš„æ¨¡å‹ï¼ˆå¦‚LLaMAï¼‰å’Œè¡¨ç°è¾ƒå·®çš„ä¸“æœ‰OpenAIæ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ­¤æ–°å‹æ•°æ®é›†å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯æ˜¾è‘—æé«˜æ£€æµ‹æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ä¸åŠä½¿ç”¨æœ¬æ•°æ®é›†è¡¨ç°çš„æ¨¡å‹çš„ååˆ†ä¹‹ä¸€åˆ°åäº”åˆ†ä¹‹ä¸€ã€‚è¿™ä¸€å±€é™åœ¨æµè¡Œçš„å®¡æ ¸APIä¸Šå°¤å…¶æ˜æ˜¾ï¼Œè¿™äº›APIä¸èƒ½è½»æ˜“é’ˆå¯¹ç‰¹å®šæ•æ„Ÿå†…å®¹è¿›è¡Œå®šåˆ¶å’Œè°ƒæ•´ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19832v3">PDF</a> Accepted at the 9th Workshop on Online Abuse and Harms (WOAH)</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹æ•°æ®é›†ä¸­æ£€æµ‹æ•æ„Ÿå†…å®¹å¯¹äºç¡®ä¿å…±äº«å’Œåˆ†æçš„æ•°æ®ä¸åŒ…å«æœ‰å®³ææ–™è‡³å…³é‡è¦ã€‚å½“å‰ä½¿ç”¨çš„è¯¸å¦‚å¤–éƒ¨APIä¹‹ç±»çš„å®¡æŸ¥å·¥å…·å­˜åœ¨å®šåˆ¶åŒ–é™åˆ¶ã€å¯¹ä¸åŒæ•æ„Ÿç±»åˆ«çš„å‡†ç¡®æ€§é—®é¢˜ä»¥åŠéšç§æ‹…å¿§ç­‰å±€é™æ€§ã€‚æ­¤å¤–ï¼Œç°æœ‰æ•°æ®é›†å’Œå¼€æºæ¨¡å‹ä¸»è¦é›†ä¸­åœ¨æœ‰æ¯’è¯­è¨€ä¸Šï¼Œåœ¨æ£€æµ‹å…¶ä»–æ•æ„Ÿç±»åˆ«ï¼ˆå¦‚æ»¥ç”¨ç‰©è´¨æˆ–è‡ªæˆ‘ä¼¤å®³ï¼‰æ–¹é¢å­˜åœ¨ç©ºç™½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç¤¾äº¤åª’ä½“å†…å®¹å®¡æŸ¥çš„ç»Ÿä¸€æ•°æ®é›†ï¼Œæ¶µç›–å…­å¤§æ•æ„Ÿç±»åˆ«ï¼šå†²çªè¯­è¨€ã€ç²—é²è¯­è¨€ã€æ€§æ˜ç¡®ææ–™ã€ä¸è¯ç‰©ç›¸å…³å†…å®¹ã€è‡ªæˆ‘ä¼¤å®³å’Œåƒåœ¾é‚®ä»¶ã€‚é€šè¿‡é‡‡ç”¨ä¸€è‡´çš„æ•°æ®æ£€ç´¢ç­–ç•¥å’Œæ ‡æ³¨æŒ‡å—ï¼Œæˆ‘ä»¬è§£å†³äº†ä»¥å‰èšç„¦ç ”ç©¶çš„ä¸è¶³ã€‚åˆ†æè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚LLaMAï¼‰å’Œè¡¨ç°ä¸ä½³çš„ä¸“æœ‰OpenAIæ¨¡å‹ç›¸æ¯”ï¼Œå¯¹æ­¤æ–°æ•°æ®é›†è¿›è¡Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ã€‚è¿™ä¸€å·®è·åœ¨æµè¡Œçš„å®¡æŸ¥APIä¸Šå°¤ä¸ºçªå‡ºï¼Œè¿™äº›APIéš¾ä»¥é’ˆå¯¹ç‰¹å®šæ•æ„Ÿç±»åˆ«è¿›è¡Œå®šåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æµ‹å¤§å‹æ•°æ®é›†ä¸­çš„æ•æ„Ÿå†…å®¹å¯¹äºé¿å…å…±äº«å’Œåˆ†ææœ‰å®³æ•°æ®è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ä½¿ç”¨çš„å®¡æŸ¥å·¥å…·å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å®šåˆ¶å›°éš¾ã€å‡†ç¡®æ€§ä¸è¶³å’Œéšç§æ‹…å¿§ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦å…³æ³¨æœ‰æ¯’è¯­è¨€æ£€æµ‹ï¼Œå¿½ç•¥äº†å…¶ä»–æ•æ„Ÿç±»åˆ«ï¼Œå¦‚æ»¥ç”¨ç‰©è´¨å’Œè‡ªæˆ‘ä¼¤å®³ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€æ•°æ®é›†ï¼Œæ¶µç›–å…­å¤§æ•æ„Ÿç±»åˆ«ï¼Œé€‚ç”¨äºç¤¾äº¤åª’ä½“å†…å®¹å®¡æŸ¥ã€‚</li>
<li>é€šè¿‡ä¸€è‡´çš„æ•°æ®æ£€ç´¢ç­–ç•¥å’Œæ ‡æ³¨æŒ‡å—ï¼Œè§£å†³äº†ä»¥å¾€ç ”ç©¶çš„ä¸è¶³ã€‚</li>
<li>ç›¸è¾ƒäºç°æœ‰å¼€æºæ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ï¼Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•æ„Ÿå†…å®¹æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3be7503a75bf7ec0825660c0d2e88a69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ddb52d02ad5e6fbc7a02d75ad41992f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3bf32f84b20b5f16eb9fd520dccd5ea.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-43911e7a52debe8d9745cab395780d94.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  MAM Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-86fc58e24e5810b6523af6504d1a57cd.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  Scaling Speculative Decoding with Lookahead Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
