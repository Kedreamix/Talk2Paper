<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  Scaling Speculative Decoding with Lookahead Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-86fc58e24e5810b6523af6504d1a57cd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-26-æ›´æ–°"><a href="#2025-06-26-æ›´æ–°" class="headerlink" title="2025-06-26 æ›´æ–°"></a>2025-06-26 æ›´æ–°</h1><h2 id="Scaling-Speculative-Decoding-with-Lookahead-Reasoning"><a href="#Scaling-Speculative-Decoding-with-Lookahead-Reasoning" class="headerlink" title="Scaling Speculative Decoding with Lookahead Reasoning"></a>Scaling Speculative Decoding with Lookahead Reasoning</h2><p><strong>Authors:Yichao Fu, Rui Ge, Zelei Shao, Zhijie Deng, Hao Zhang</strong></p>
<p>Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level speculative decoding (SD) helps, but its benefit is capped, because the chance that an entire $\gamma$-token guess is correct falls exponentially as $\gamma$ grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling â€“ making the speedup modest and hardware-agnostic. We raise this ceiling with Lookahead Reasoning, which exploits a second, step-level layer of parallelism. Our key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching. In Lookahead Reasoning, a lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and a verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hao-ai-lab/LookaheadReasoning">https://github.com/hao-ai-lab/LookaheadReasoning</a> </p>
<blockquote>
<p>æ¨ç†æ¨¡å‹é€šè¿‡ç”Ÿæˆé•¿é“¾æ€ç»´è¡¨ç°å‡ºè‰²ï¼Œä½†è§£ç äº§ç”Ÿçš„æˆåƒä¸Šä¸‡ä¸ªä»¤ç‰Œå´å¾ˆæ…¢ã€‚ä»¤ç‰Œçº§åˆ«çš„æŠ•æœºè§£ç ï¼ˆSDï¼‰æœ‰æ‰€å¸®åŠ©ï¼Œä½†å…¶å¥½å¤„æ˜¯æœ‰é™çš„ï¼Œå› ä¸ºæ•´ä¸ª$\gamma$-ä»¤ç‰Œçš„çŒœæµ‹æ­£ç¡®çš„æœºä¼šéšç€$\gamma$çš„å¢é•¿è€ŒæŒ‡æ•°çº§ä¸‹é™ã€‚è¿™æ„å‘³ç€ä¸ºæ›´é•¿çš„ä»¤ç‰Œè‰ç¨¿åˆ†é…æ›´å¤šçš„è®¡ç®—é‡ä¼šé¢ä¸´ç®—æ³•å¤©èŠ±æ¿â€”â€”ä½¿åŠ é€Ÿæ•ˆæœæ¸©å’Œä¸”ä¸ç¡¬ä»¶æ— å…³ã€‚æˆ‘ä»¬é€šè¿‡å‰ç»æ€§æ¨ç†æ¥æé«˜è¿™ä¸ªä¸Šé™ï¼Œå®ƒåˆ©ç”¨ç¬¬äºŒå±‚æ­¥éª¤çº§åˆ«çš„å¹¶è¡Œæ€§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œæ¨ç†æ¨¡å‹æ˜¯é€æ­¥ç”Ÿæˆçš„ï¼Œæ¯ä¸ªæ­¥éª¤åªéœ€è¦è¯­ä¹‰æ­£ç¡®ï¼Œè€Œä¸éœ€è¦ç²¾ç¡®çš„ä»¤ç‰ŒåŒ¹é…ã€‚åœ¨å‰ç»æ€§æ¨ç†ä¸­ï¼Œä¸€ä¸ªè½»é‡çº§çš„è‰ç¨¿æ¨¡å‹ä¼šæå‡ºå‡ ä¸ªæœªæ¥æ­¥éª¤ï¼›ç›®æ ‡æ¨¡å‹åœ¨ä¸€æ¬¡æ‰¹é‡ä¼ é€’ä¸­æ‰©å±•æ¯ä¸ªææ¡ˆï¼ŒéªŒè¯å™¨åˆ™ä¿æŒè¯­ä¹‰æ­£ç¡®çš„æ­¥éª¤ï¼ŒåŒæ—¶è®©ç›®æ ‡é‡æ–°ç”Ÿæˆä»»ä½•å¤±è´¥çš„æ­¥éª¤ã€‚ä»¤ç‰Œçº§åˆ«çš„SDä»ç„¶åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­è¿è¡Œï¼Œå› æ­¤è¿™ä¸¤å±‚å¹¶è¡Œæ€§æ˜¯ç›¸ä¹˜çš„ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå‰ç»æ€§æ¨ç†ä»ç†è®ºå’Œå®é™…ä¸Šæé«˜äº†SDçš„å³°å€¼åŠ é€Ÿæ•ˆæœã€‚åœ¨GSM8Kã€AIMEå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸­ï¼Œå‰ç»æ€§æ¨ç†å°†SDçš„åŠ é€Ÿæ¯”ä»1.4å€æé«˜åˆ°2.1å€ï¼ŒåŒæ—¶ä¿æŒç­”æ¡ˆè´¨é‡ï¼Œå¹¶ä¸”å…¶åŠ é€Ÿæ•ˆæœéšç€GPUååé‡çš„å¢åŠ è€Œæ›´å¥½åœ°æ‰©å±•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hao-ai-lab/LookaheadReasoning%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hao-ai-lab/LookaheadReasoningä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19830v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬è®¨è®ºäº†æ¨ç†æ¨¡å‹ç”Ÿæˆé•¿é“¾æ€ç»´æ—¶çš„æ•ˆç‡é—®é¢˜ã€‚è™½ç„¶å­˜åœ¨è¯¸å¦‚tokençº§åˆ«æ¨æµ‹è§£ç ç­‰æŠ€æœ¯ï¼Œä½†å…¶é€Ÿåº¦æå‡æœ‰é™ã€‚ä½œè€…æå‡ºäº†Lookahead Reasoningæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ­¥éª¤çº§åˆ«å¼•å…¥å¹¶è¡Œæ€§æ¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•å…è®¸æ¨¡å‹æå‡ºå¤šä¸ªæœªæ¥æ­¥éª¤çš„è‰æ¡ˆï¼Œå¹¶åœ¨ä¸€æ¬¡æ‰¹é‡ä¼ é€’ä¸­æ‰©å±•æ¯ä¸ªææ¡ˆã€‚éªŒè¯å™¨ä¿ç•™è¯­ä¹‰æ­£ç¡®çš„æ­¥éª¤ï¼Œå¹¶è®©ç›®æ ‡æ¨¡å‹é‡æ–°ç”Ÿæˆå¤±è´¥çš„æ­¥éª¤ã€‚Lookahead Reasoningæå‡äº†SDçš„é€Ÿåº¦æå‡æ•ˆæœï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†ç­”æ¡ˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨é•¿é“¾æ€ç»´ç”Ÿæˆè¿‡ç¨‹ä¸­é¢ä¸´æ•ˆç‡æŒ‘æˆ˜ã€‚</li>
<li>Tokençº§åˆ«æ¨æµ‹è§£ç ï¼ˆSDï¼‰è™½ç„¶æœ‰æ‰€å¸®åŠ©ï¼Œä½†å…¶æ•ˆç›Šæœ‰é™ã€‚</li>
<li>Lookahead Reasoningæ–¹æ³•é€šè¿‡å¼•å…¥æ­¥éª¤çº§åˆ«çš„å¹¶è¡Œæ€§æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>Lookahead Reasoningå…è®¸æ¨¡å‹æå‡ºå¤šä¸ªæœªæ¥æ­¥éª¤çš„è‰æ¡ˆï¼Œå¹¶ä¸€æ¬¡æ€§æ‰©å±•ã€‚</li>
<li>éªŒè¯å™¨åœ¨æ–¹æ³•ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä¿ç•™è¯­ä¹‰æ­£ç¡®çš„æ­¥éª¤ã€‚</li>
<li>Lookahead Reasoningæå‡äº†SDçš„é€Ÿåº¦æå‡æ•ˆæœï¼Œå¹¶åœ¨å¤šä¸ªæµ‹è¯•ä¸­ä¿æŒäº†ç­”æ¡ˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dcbc3d396d13828939a2c67f028cac29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74d0a890a32d6665ba1d2b2139aac569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dff0213ef67e4dfeef5fbf91b2b96cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff9dc484b50e72793ffc81bfb68c72ec.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="KnowRL-Exploring-Knowledgeable-Reinforcement-Learning-for-Factuality"><a href="#KnowRL-Exploring-Knowledgeable-Reinforcement-Learning-for-Factuality" class="headerlink" title="KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"></a>KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality</h2><p><strong>Authors:Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang</strong></p>
<p>Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL">https://github.com/zjunlp/KnowRL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°¤å…¶æ˜¯æ…¢æ€è€ƒæ¨¡å‹ï¼Œå¸¸å¸¸è¡¨ç°å‡ºä¸¥é‡çš„å¹»è§‰ï¼Œç”±äºæ¨ç†è¿‡ç¨‹ä¸­æ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œä»è€Œè¾“å‡ºé”™è¯¯å†…å®¹ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¢å¼ºå¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä»¥ç»“æœä¸ºå¯¼å‘çš„å¥–åŠ±æœºåˆ¶å¾€å¾€ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„çœŸå®ç›‘ç£ï¼Œä»è€ŒåŠ å‰§äº†å¹»è§‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³æ…¢æ€è€ƒæ¨¡å‹ä¸­çš„é«˜å¹»è§‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†å¢å¼ºRLï¼Œå³KnowRLã€‚KnowRLé€šè¿‡å°†åŸºäºçŸ¥è¯†éªŒè¯çš„çœŸå®æ€§å¥–åŠ±èå…¥RLè®­ç»ƒè¿‡ç¨‹ï¼ŒæŒ‡å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€è€ƒï¼Œå¸®åŠ©å®ƒä»¬è¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚KnowRLçš„ç‰¹è‰²åœ¨äºï¼Œå®ƒé’ˆå¯¹æ€§åœ°åœ¨å®é™…æ¨ç†æ­¥éª¤ä¸­å¥–åŠ±å¯¹äº‹å®çš„éµå¾ªï¼Œä»è€ŒåŸ¹è‚²ä¸€ä¸ªæ›´å¯é çš„æ€è€ƒè¿‡ç¨‹ã€‚åœ¨ä¸‰ä¸ªå¹»è§‰è¯„ä¼°æ•°æ®é›†å’Œä¸¤ä¸ªæ¨ç†è¯„ä¼°æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLæœ‰æ•ˆå‡è½»äº†æ…¢æ€è€ƒæ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒäº†å…¶åŸæœ‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL%E3%80%82">https://github.com/zjunlp/KnowRLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19807v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ—¶ï¼Œç”±äºæ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œå¸¸å¸¸å‡ºç°ä¸¥é‡çš„å¹»è§†é—®é¢˜ï¼Œå³è¾“å‡ºé”™è¯¯çš„å†…å®¹ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä»¥ç»“æœä¸ºå¯¼å‘çš„å¥–åŠ±æœºåˆ¶ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„çœŸå®ç›‘ç£ï¼Œä»è€ŒåŠ å‰§äº†å¹»è§†é—®é¢˜ã€‚ä¸ºè§£å†³æ…¢æ€è€ƒæ¨¡å‹ä¸­çš„é«˜å¹»è§†é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆçŸ¥è¯†éªŒè¯çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ³•â€”â€”KnowRLã€‚KnowRLé€šè¿‡å°†åŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±èå…¥RLè®­ç»ƒè¿‡ç¨‹ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€è€ƒï¼Œå¸®åŠ©æ¨¡å‹è®¤è¯†å…¶çŸ¥è¯†è¾¹ç•Œã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å’Œå†…åŒ–åŸºäºäº‹å®çš„æ¨ç†ç­–ç•¥ï¼Œå¹¶é€šè¿‡ç›´æ¥å¥–åŠ±ç¬¦åˆäº‹å®çš„æ¨ç†æ­¥éª¤æ¥ç¡®ä¿æ›´å¯é çš„æ€è€ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKnowRLèƒ½æœ‰æ•ˆå‡è½»æ…¢æ€è€ƒæ¨¡å‹çš„å¹»è§†é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒå…¶åŸæœ‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowRL%E3%80%82">https://github.com/zjunlp/KnowRLã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ—¶å› æ— æ³•å‡†ç¡®è¯†åˆ«çŸ¥è¯†è¾¹ç•Œï¼Œæ˜“å‡ºç°è¾“å‡ºé”™è¯¯å†…å®¹çš„é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è™½èƒ½æé«˜å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶å¥–åŠ±æœºåˆ¶ç¼ºä¹å¯¹æ€è€ƒè¿‡ç¨‹çš„çœŸå®ç›‘ç£ï¼Œå¯¼è‡´å¹»è§†é—®é¢˜åŠ å‰§ã€‚</li>
<li>KnowRLæ–¹æ³•é€šè¿‡ç»“åˆçŸ¥è¯†éªŒè¯å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡ŒåŸºäºäº‹å®çš„æ…¢æ€è€ƒã€‚</li>
<li>KnowRLå¸®åŠ©æ¨¡å‹è®¤è¯†å…¶çŸ¥è¯†è¾¹ç•Œï¼Œé€šè¿‡èå…¥åŸºäºçŸ¥è¯†éªŒè¯çš„äº‹å®å¥–åŠ±åˆ°RLè®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ã€‚</li>
<li>KnowRLä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å’Œå†…åŒ–åŸºäºäº‹å®çš„æ¨ç†ç­–ç•¥ã€‚</li>
<li>KnowRLé€šè¿‡ç›´æ¥å¥–åŠ±ç¬¦åˆäº‹å®çš„æ¨ç†æ­¥éª¤æ¥ç¡®ä¿æ›´å¯é çš„æ€è€ƒè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab8f299c1dd6b353f2ed6bee47c6134b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e4aea7049441064ef28fa4892d409e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-005676593173fc0a316d6d0a953cf685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5a070aa67b5197507e23af02899af0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1217ab4abeba55946bd602d4f51725b9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SAGE-Strategy-Adaptive-Generation-Engine-for-Query-Rewriting"><a href="#SAGE-Strategy-Adaptive-Generation-Engine-for-Query-Rewriting" class="headerlink" title="SAGE: Strategy-Adaptive Generation Engine for Query Rewriting"></a>SAGE: Strategy-Adaptive Generation Engine for Query Rewriting</h2><p><strong>Authors:Teng Wang, Hailei Gong, Changwang Zhang, Jun Wang</strong></p>
<p>Query rewriting is pivotal for enhancing dense retrieval, yet current methods demand large-scale supervised data or suffer from inefficient reinforcement learning (RL) exploration. In this work, we first establish that guiding Large Language Models (LLMs) with a concise set of expert-crafted strategies, such as semantic expansion and entity disambiguation, substantially improves retrieval effectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus, and SciFact. Building on this insight, we introduce the Strategy-Adaptive Generation Engine (SAGE), which operationalizes these strategies in an RL framework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit Shaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative learning signals. This strategy-guided approach not only achieves new state-of-the-art NDCG@10 results, but also uncovers a compelling emergent behavior: the agent learns to select optimal strategies, reduces unnecessary exploration, and generates concise rewrites, lowering inference cost without sacrificing performance. Our findings demonstrate that strategy-guided RL, enhanced with nuanced reward shaping, offers a scalable, efficient, and more interpretable paradigm for developing the next generation of robust information retrieval systems. </p>
<blockquote>
<p>æŸ¥è¯¢æ”¹å†™å¯¹äºæé«˜å¯†é›†æ£€ç´¢è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•éœ€è¦å¤§è§„æ¨¡çš„ç›‘ç£æ•°æ®ï¼Œæˆ–è€…é¢ä¸´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šï¼Œä½¿ç”¨ç®€æ´çš„ä¸“å®¶ç­–ç•¥ï¼ˆå¦‚è¯­ä¹‰æ‰©å±•å’Œå®ä½“æ¶ˆæ­§ï¼‰æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½æ˜¾è‘—æé«˜åŒ…æ‹¬HotpotQAã€FEVERã€NFCorpuså’ŒSciFactç­‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸Šçš„æ£€ç´¢æ•ˆæœã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ç­–ç•¥è‡ªé€‚åº”ç”Ÿæˆå¼•æ“ï¼ˆSAGEï¼‰ï¼Œè¯¥å¼•æ“åœ¨RLæ¡†æ¶ä¸­å®æ–½è¿™äº›ç­–ç•¥ã€‚SAGEå¼•å…¥ä¸¤ç§æ–°å‹å¥–åŠ±å¡‘é€ æœºåˆ¶â€”â€”æˆ˜ç•¥ä¿¡ç”¨å¡‘é€ ï¼ˆSCSï¼‰å’Œå¯¹æ¯”å¥–åŠ±å¡‘é€ ï¼ˆCRSï¼‰ï¼Œä»¥æä¾›æ›´å…·ä¿¡æ¯çš„å­¦ä¹ ä¿¡å·ã€‚è¿™ç§ç­–ç•¥æŒ‡å¯¼çš„æ–¹æ³•ä¸ä»…å®ç°äº†æ–°çš„NDCG@10ç»“æœçš„æœ€å…ˆè¿›æ°´å¹³ï¼Œè¿˜æ­ç¤ºäº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ–°å…´è¡Œä¸ºï¼šä»£ç†èƒ½å¤Ÿå­¦ä¼šé€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„æ¢ç´¢ï¼Œå¹¶äº§ç”Ÿç®€æ´çš„æ”¹å†™ï¼Œé™ä½æ¨ç†æˆæœ¬è€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç­–ç•¥æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ åŠ ä¸Šå¾®å¦™çš„å¥–åŠ±å¡‘é€ æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆã€æ›´å…·è§£é‡Šæ€§çš„èŒƒå¼ï¼Œä¸ºä¸‹ä¸€ä»£ç¨³å¥çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿå¼€å‘æä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¯†é›†æ£€ç´¢ä¸­æŸ¥è¯¢æ”¹å†™çš„é‡è¦æ€§ï¼Œé’ˆå¯¹å½“å‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ç­–ç•¥æŒ‡å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚é€šè¿‡è¯­ä¹‰æ‰©å±•å’Œå®ä½“æ¶ˆæ­§ç­‰ä¸“å®¶åˆ¶å®šçš„ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†ä¸€äº›æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸Šçš„æ£€ç´¢æ•ˆæœã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼•å…¥ç­–ç•¥è‡ªé€‚åº”ç”Ÿæˆå¼•æ“ï¼ˆSAGEï¼‰ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†ä¸¤ç§æ–°é¢–çš„å¥–åŠ±å¡‘é€ æœºåˆ¶ã€‚è¿™ç§ç­–ç•¥å¯¼å‘çš„æ–¹æ³•ä¸ä»…è¾¾åˆ°äº†æ–°çš„æœ€ä½³çŠ¶æ€NDCG@10ç»“æœï¼Œè¿˜å±•ç°äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ–°å…´è¡Œä¸ºï¼šæ™ºèƒ½ä½“èƒ½å¤Ÿé€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„æ¢ç´¢ï¼Œç”Ÿæˆç®€æ´çš„æ”¹å†™ï¼Œé™ä½æ¨ç†æˆæœ¬è€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚è¿™æ˜¾ç¤ºäº†ç­–ç•¥å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ç»“åˆå¾®å¦™çš„å¥–åŠ±å¡‘é€ ï¼Œä¸ºä¸‹ä¸€ä»£ç¨³å¥çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆå’Œæ›´å…·è§£é‡Šæ€§çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢æ”¹å†™åœ¨å¯†é›†æ£€ç´¢ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•éœ€è¦å¤§é‡ç›‘ç£æ•°æ®æˆ–é¢ä¸´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¢ç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¸“å®¶åˆ¶å®šçš„ç­–ç•¥ï¼ˆå¦‚è¯­ä¹‰æ‰©å±•å’Œå®ä½“æ¶ˆæ­§ï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ£€ç´¢æ•ˆæœã€‚</li>
<li>ç­–ç•¥è‡ªé€‚åº”ç”Ÿæˆå¼•æ“ï¼ˆSAGEï¼‰ç»“åˆäº†å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¹¶è¿ç”¨æˆ˜ç•¥æ€§å’Œå¯¹æ¯”å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œå¸¦æ¥æ›´é«˜æ•ˆå’Œæ›´æœ‰è§£é‡Šæ€§çš„ç»“æœã€‚</li>
<li>ç­–ç•¥å¯¼å‘çš„æ–¹æ³•ä¸ä»…åœ¨è¯„ä¼°æ ‡å‡†ä¸Šå–å¾—æ–°çªç ´ï¼Œè€Œä¸”åœ¨æ¨ç†æˆæœ¬ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ã€‚æ™ºèƒ½ä½“èƒ½å¤Ÿé€‰æ‹©æœ€ä½³ç­–ç•¥å¹¶ç”Ÿæˆç®€æ´çš„æ”¹å†™ã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç¤ºäº†å…¶å¹¿æ³›é€‚ç”¨æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°å‡ºè‰²çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c264ad322dc55dd46e0fed3decc8e27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3449cc35bc3e01640785f1a420747247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45f281adfd2d5a680f04ee32aec7fc2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb111d9dff7abbb37ee3789f53e9411.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment"><a href="#Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment" class="headerlink" title="Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment"></a>Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment</h2><p><strong>Authors:Yuhui Sun, Xiyao Wang, Zixi Li, Jinman Zhao</strong></p>
<p>While large-scale unsupervised language models (LMs) capture broad world knowledge and reasoning capabilities, steering their behavior toward desired objectives remains challenging due to the lack of explicit supervision. Existing alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on training a reward model and performing reinforcement learning to align with human preferences. However, RLHF is often computationally intensive, unstable, and sensitive to hyperparameters.   To address these limitations, Direct Preference Optimization (DPO) was introduced as a lightweight and stable alternative, enabling direct alignment of language models with pairwise preference data via classification loss. However, DPO and its extensions generally assume a single static preference distribution, limiting flexibility in multi-objective or dynamic alignment settings.   In this paper, we propose a novel framework: Multi-Preference Lambda-weighted Listwise DPO, which extends DPO to incorporate multiple human preference dimensions (e.g., helpfulness, harmlessness, informativeness) and enables dynamic interpolation through a controllable simplex-weighted formulation. Our method supports both listwise preference feedback and flexible alignment across varying user intents without re-training. Empirical and theoretical analysis demonstrates that our method is as effective as traditional DPO on static objectives while offering greater generality and adaptability for real-world deployment. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ— ç›‘ç£è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è™½ç„¶æ•æ‰åˆ°äº†å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹æ˜ç¡®çš„ç›‘ç£ï¼Œå°†å…¶è¡Œä¸ºå¯¼å‘æ—¢å®šçš„ç›®æ ‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„å¯¹é½æŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼Œä¾èµ–äºè®­ç»ƒå¥–åŠ±æ¨¡å‹å’Œè¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ç¬¦åˆäººç±»åå¥½ã€‚ç„¶è€Œï¼ŒRLHFé€šå¸¸è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šï¼Œå¯¹è¶…å‚æ•°æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œå¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºè½»é‡çº§å’Œç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡åˆ†ç±»æŸå¤±ç›´æ¥å¯¹é½è¯­è¨€æ¨¡å‹ä¸é…å¯¹åå¥½æ•°æ®ã€‚ç„¶è€Œï¼ŒDPOåŠå…¶æ‰©å±•é€šå¸¸å‡è®¾å•ä¸€çš„é™æ€åå¥½åˆ†å¸ƒï¼Œè¿™åœ¨å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½è®¾ç½®ä¸­é™åˆ¶äº†çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼šå¤šåå¥½LambdaåŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒæ‰©å±•äº†DPOä»¥èå…¥å¤šç§äººç±»åå¥½ç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œæœ‰å¸®åŠ©æ€§ã€æ— å®³æ€§ã€ä¿¡æ¯æ€§ï¼‰ï¼Œå¹¶é€šè¿‡å¯æ§çš„å•çº¯åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆå’Œçµæ´»çš„è·¨ä¸åŒç”¨æˆ·æ„å›¾å¯¹é½ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å®è¯å’Œç†è®ºåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šä¸ä¼ ç»ŸDPOåŒæ ·æœ‰æ•ˆï¼ŒåŒæ—¶æä¾›äº†æ›´å¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19780v1">PDF</a> 10 pages, 4 figures, appendix included. To appear in Proceedings of   AAAI 2026. Code:   <a target="_blank" rel="noopener" href="https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO">https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</a></p>
<p><strong>Summary</strong><br>å¤§å‹æ— ç›‘ç£è¯­è¨€æ¨¡å‹æ•æ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹æ˜ç¡®çš„ç›‘ç£ä½¿å…¶éš¾ä»¥ç¬¦åˆæœŸæœ›ç›®æ ‡ã€‚ç°æœ‰å¯¹é½æŠ€æœ¯å¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰éœ€è¦è®­ç»ƒå¥–åŠ±æ¨¡å‹å¹¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥ç¬¦åˆäººç±»åå¥½ï¼Œä½†è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šä¸”å¯¹è¶…å‚æ•°æ•æ„Ÿã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºè½»é‡çº§ç¨³å®šæ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡åˆ†ç±»æŸå¤±ç›´æ¥å¯¹é½è¯­è¨€æ¨¡å‹ä¸é…å¯¹åå¥½æ•°æ®ã€‚ç„¶è€Œï¼ŒDPOåŠå…¶æ‰©å±•å‡è®¾å•ä¸€é™æ€åå¥½åˆ†å¸ƒï¼Œé™åˆ¶äº†å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½è®¾ç½®çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºæ–°å‹æ¡†æ¶ï¼šå¤šåå¥½Î»åŠ æƒåˆ—è¡¨å¼DPOï¼Œå°†DPOæ‰©å±•åˆ°åŒ…å«å¤šç§äººç±»åå¥½ç»´åº¦ï¼ˆå¦‚æœ‰ç”¨æ€§ã€æ— å®³æ€§ã€ä¿¡æ¯é‡ç­‰ï¼‰ï¼Œå¹¶é€šè¿‡å¯æ§çš„ç®€å•åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ã€‚è¯¥æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯é€‚åº”ä¸åŒç”¨æˆ·æ„å›¾çš„å¯¹é½ã€‚ç†è®ºå’Œå®è¯åˆ†æè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™æ€ç›®æ ‡ä¸Šçš„æ•ˆæœä¸ä¼ ç»ŸDPOç›¸å½“ï¼ŒåŒæ—¶æä¾›æ›´å¤§çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ— ç›‘ç£è¯­è¨€æ¨¡å‹è™½ç„¶å…·å¤‡å¹¿æ³›çŸ¥è¯†æ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹ç›‘ç£ä½¿å…¶éš¾ä»¥ç¬¦åˆæœŸæœ›ç›®æ ‡ã€‚</li>
<li>ç°æœ‰å¯¹é½æŠ€æœ¯å¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å­˜åœ¨è®¡ç®—é‡å¤§ã€ä¸ç¨³å®šåŠå¯¹è¶…å‚æ•°æ•æ„Ÿçš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§è½»é‡çº§ç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆè¢«å¼•å…¥ï¼Œèƒ½å¤Ÿé€šè¿‡åˆ†ç±»æŸå¤±ç›´æ¥å¯¹é½è¯­è¨€æ¨¡å‹ä¸é…å¯¹åå¥½æ•°æ®ã€‚</li>
<li>DPOåŠå…¶æ‰©å±•åœ¨å‡è®¾å•ä¸€é™æ€åå¥½åˆ†å¸ƒä¸Šå­˜åœ¨å±€é™æ€§ï¼Œé™åˆ¶äº†å¤šç›®æ ‡æˆ–åŠ¨æ€å¯¹é½çš„çµæ´»æ€§ã€‚</li>
<li>æ–°å‹æ¡†æ¶Multi-Preference Lambda-weighted Listwise DPOæ‰©å±•äº†DPOï¼Œçº³å…¥å¤šç§äººç±»åå¥½ç»´åº¦ï¼Œå¹¶å…è®¸é€šè¿‡å¯æ§çš„ç®€å•åŠ æƒå…¬å¼å®ç°åŠ¨æ€æ’å€¼ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒåˆ—è¡¨å¼åå¥½åé¦ˆï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯é€‚åº”ä¸åŒç”¨æˆ·æ„å›¾çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad3881fa94966bde72253875f32fb0a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d1a6f2c7dff993092c2230a89716ab.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tailored-Conversations-beyond-LLMs-A-RL-Based-Dialogue-Manager"><a href="#Tailored-Conversations-beyond-LLMs-A-RL-Based-Dialogue-Manager" class="headerlink" title="Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"></a>Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager</h2><p><strong>Authors:Lucie Galland, Catherine Pelachaud, Florian Pecune</strong></p>
<p>In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŸºäºRLçš„å¯¹è¯ç®¡ç†å™¨ç›¸ç»“åˆï¼Œä»¥å®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ¥æ¨¡æ‹Ÿå¯¹è¯çš„ç»“æ„åŒ–é˜¶æ®µï¼Œå¹¶è¿ç”¨å…ƒå­¦ä¹ æ¥æé«˜å¯¹ä¸åŒç”¨æˆ·é…ç½®çš„é€‚åº”æ€§ï¼Œä»è€Œæé«˜äº†é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œåœ¨å¯¹è¯é˜¶æ®µä¹‹é—´å¹³ç¨³è¿‡æ¸¡ï¼Œå¹¶å¯¹å¤šæ ·åŒ–çš„æ‚£è€…éœ€æ±‚åšå‡ºä¸ªæ€§åŒ–å“åº”ã€‚æˆ‘ä»¬å°†è¯¥æ¡†æ¶åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œå¹¶è¯æ˜æ‰€æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºæœ€æ–°çš„LLMåŸºå‡†æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºå°†LLMç”¨äºåˆ›å»ºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¼å¯¹è¯ç³»ç»Ÿçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19652v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¯¹è¯ç®¡ç†å™¨ç›¸ç»“åˆï¼Œç”¨äºå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¯¹è¯ã€‚é€šè¿‡åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ å¯¹å¯¹è¯çš„ç»“æ„é˜¶æ®µè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åº”ç”¨å…ƒå­¦ä¹ æ¥æé«˜å¯¹ä¸åŒç”¨æˆ·é…ç½®çš„é€‚åº”æ€§ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†ç³»ç»Ÿçš„é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œåœ¨å¯¹è¯é˜¶æ®µä¹‹é—´æµç•…è¿‡æ¸¡ï¼Œå¹¶å¯¹ä¸åŒæ‚£è€…çš„éœ€æ±‚åšå‡ºä¸ªæ€§åŒ–å›åº”ã€‚æˆ‘ä»¬å°†è¯¥æ¡†æ¶åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ï¼Œå¹¶è¯æ˜æ‰€æå‡ºçš„å¯¹è¯ç®¡ç†å™¨åœ¨å¥–åŠ±æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå°†å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨äºåˆ›å»ºå…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¯¹è¯ç³»ç»Ÿçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–°å‹æ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ å¯¹è¯ç®¡ç†å™¨ï¼Œç”¨äºå®ç°å…·æœ‰ç‰¹å®šç›®æ ‡çš„å¼€æ”¾å¯¹è¯ã€‚</li>
<li>åˆ©ç”¨åˆ†å±‚å¼ºåŒ–å­¦ä¹ å¯¹å¯¹è¯çš„ç»“æ„é˜¶æ®µè¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>åº”ç”¨å…ƒå­¦ä¹ æé«˜å¯¹ä¸åŒç”¨æˆ·é…ç½®çš„é€‚åº”æ€§ã€‚</li>
<li>ç³»ç»Ÿèƒ½ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä¹‹é—´æµç•…è¿‡æ¸¡ã€‚</li>
<li>èƒ½å¤Ÿä¸ªæ€§åŒ–å›åº”ä¸åŒæ‚£è€…çš„éœ€æ±‚ã€‚</li>
<li>æ¡†æ¶è¢«åº”ç”¨äºåŠ¨æœºé¢è¯•ï¼Œæ—¨åœ¨ä¿ƒè¿›è¡Œä¸ºæ”¹å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f9405f26a1a67abd221f62dbb2da549.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0ddc4ed63cd160846acbe54e92b164c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14465b24ac8ac3cfe6e2093cc18adeb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis"><a href="#V2T-CoT-From-Vision-to-Text-Chain-of-Thought-for-Medical-Reasoning-and-Diagnosis" class="headerlink" title="V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and   Diagnosis"></a>V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and   Diagnosis</h2><p><strong>Authors:Yuan Wang, Jiaxiang Liu, Shujian Gao, Bin Feng, Zhihang Tang, Xiaotang Gai, Jian Wu, Zuozhu Liu</strong></p>
<p>Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€æŠ€æœ¯çš„è¿›å±•åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¾§é‡äºå…¨å±€å›¾åƒç‰¹å¾ï¼Œè€Œéå®šä½å¯¹è¯Šæ–­è‡³å…³é‡è¦çš„ç‰¹å®šç–¾ç—…åŒºåŸŸã€‚æ­¤å¤–ï¼Œå½“å‰ç ”ç©¶å¾€å¾€è¿‡åˆ†å¼ºè°ƒç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œè€Œå¿½è§†äº†æ¨ç†è¿‡ç¨‹çš„é‡è¦æ€§ï¼Œä½†è¿™ä¸¤è€…å¯¹äºä¸´åºŠå†³ç­–éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä»è§†è§‰åˆ°æ–‡æœ¬æ€ç»´é“¾ï¼ˆV2T-CoTï¼‰â€è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨å®šä½ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­çš„åå¥½åŒºåŸŸï¼Œå¹¶å°†è¿™äº›åŒºåŸŸçš„å®šä½ä¿¡æ¯çº³å…¥åŒºåŸŸçº§åƒç´ æ³¨æ„åŠ›ä½œä¸ºè§†è§‰æ¨ç†çš„çŸ¥è¯†ã€‚é€šè¿‡åœ¨æ„å»ºçš„R-Med 39Kæ•°æ®é›†ä¸Šå¯¹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒV2T-CoTæä¾›äº†æ˜ç¡®çš„åŒ»ç–—æ¨ç†è·¯å¾„ã€‚V2T-CoTå°†è§†è§‰å®šä½ä¸æ–‡æœ¬ç†ç”±ç”Ÿæˆç›¸ç»“åˆï¼Œå»ºç«‹ç²¾ç¡®ä¸”å¯è§£é‡Šçš„è¯Šæ–­ç»“æœã€‚åœ¨å››ä¸ªåŒ»ç–—VQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢éƒ½å–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19610v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€æŠ€æœ¯çš„è¿›å±•åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¤šå…³æ³¨å…¨å±€å›¾åƒç‰¹å¾ï¼Œå¿½è§†äº†å¯¹è¯Šæ–­è‡³å…³é‡è¦çš„ç‰¹å®šç–¾ç—…åŒºåŸŸå®šä½ã€‚æ­¤å¤–ï¼Œç°æœ‰ç ”ç©¶å¾€å¾€é‡è§†ç­”æ¡ˆå‡†ç¡®æ€§ï¼Œå¿½è§†äº†æ¨ç†è·¯å¾„çš„é‡è¦æ€§ï¼ŒäºŒè€…å¯¹ä¸´åºŠå†³ç­–å‡è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†From Vision to Text Chain-of-Thoughtï¼ˆV2T-CoTï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨å®šä½ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­çš„åå¥½åŒºåŸŸï¼Œå¹¶å°†è¿™äº›åŒºåŸŸçš„å®šä½ä¿¡æ¯èå…¥åƒç´ çº§æ³¨æ„åŠ›çŸ¥è¯†ä¸­ï¼Œå½¢æˆè§†è§‰æ¨ç†é“¾ã€‚é€šè¿‡å¾®è°ƒæ„å»ºçš„R-Med 39Kæ•°æ®é›†ä¸Šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒV2T-CoTæä¾›äº†æ˜ç¡®çš„åŒ»å­¦æ¨ç†è·¯å¾„ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è§†è§‰å®šä½ä¸æ–‡æœ¬ç†ç”±ç”Ÿæˆï¼Œä¸ºåŒ»ç–—è¯Šæ–­æä¾›äº†ç²¾ç¡®ä¸”å¯è§£é‡Šçš„ç»“æœã€‚åœ¨å››ä¸ªMed-VQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒV2T-CoTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰æ¨¡å‹å¿½ç•¥äº†å¯¹ç‰¹å®šç–¾ç—…åŒºåŸŸå®šä½çš„é‡è¦æ€§ã€‚</li>
<li>V2T-CoTæ–¹æ³•èƒ½è‡ªåŠ¨å®šä½ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸­çš„å…³é”®åŒºåŸŸï¼Œå¹¶èå…¥åƒç´ çº§æ³¨æ„åŠ›çŸ¥è¯†ã€‚</li>
<li>V2T-CoTé€šè¿‡å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ„å»ºçš„R-Med 39Kæ•°æ®é›†ä¸Šæä¾›æ˜ç¡®çš„åŒ»å­¦æ¨ç†è·¯å¾„ã€‚</li>
<li>V2T-CoTç»“åˆäº†è§†è§‰å®šä½ä¸æ–‡æœ¬ç†ç”±ç”Ÿæˆï¼Œæä¾›ç²¾ç¡®ä¸”å¯è§£é‡Šçš„è¯Šæ–­ç»“æœã€‚</li>
<li>V2T-CoTåœ¨å››ä¸ªMed-VQAåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åŒ»ç–—è§†è§‰é—®ç­”çš„å‡†ç¡®æ€§å’Œæ¨ç†è·¯å¾„çš„æ¸…æ™°åº¦ä¸Šå‡å®ç°äº†æ˜¾è‘—æå‡ã€‚</li>
<li>V2T-CoTä¸ºä¸´åºŠå†³ç­–æ”¯æŒæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·ï¼Œæœ‰åŠ©äºæå‡åŒ»ç–—è¯Šæ–­çš„ç²¾ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94f72853ddfbf84ff85d3c9c098467ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb6f0dfff90785fc16e0f604a7f580b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-995e3ab4218c0cdec77a7a434be632a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee3492c30902c22fa5aba64673b7ad0b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="T-Rex-Task-Adaptive-Spatial-Representation-Extraction-for-Robotic-Manipulation-with-Vision-Language-Models"><a href="#T-Rex-Task-Adaptive-Spatial-Representation-Extraction-for-Robotic-Manipulation-with-Vision-Language-Models" class="headerlink" title="T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic   Manipulation with Vision-Language Models"></a>T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic   Manipulation with Vision-Language Models</h2><p><strong>Authors:Yiteng Chen, Wenbo Li, Shiyi Wang, Huiping Zhuang, Qingyao Wu</strong></p>
<p>Building a general robotic manipulation system capable of performing a wide variety of tasks in real-world settings is a challenging task. Vision-Language Models (VLMs) have demonstrated remarkable potential in robotic manipulation tasks, primarily due to the extensive world knowledge they gain from large-scale datasets. In this process, Spatial Representations (such as points representing object positions or vectors representing object orientations) act as a bridge between VLMs and real-world scene, effectively grounding the reasoning abilities of VLMs and applying them to specific task scenarios. However, existing VLM-based robotic approaches often adopt a fixed spatial representation extraction scheme for various tasks, resulting in insufficient representational capability or excessive extraction time. In this work, we introduce T-Rex, a Task-Adaptive Framework for Spatial Representation Extraction, which dynamically selects the most appropriate spatial representation extraction scheme for each entity based on specific task requirements. Our key insight is that task complexity determines the types and granularity of spatial representations, and Stronger representational capabilities are typically associated with Higher overall system operation costs. Through comprehensive experiments in real-world robotic environments, we show that our approach delivers significant advantages in spatial understanding, efficiency, and stability without additional training. </p>
<blockquote>
<p>æ„å»ºèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æ‰§è¡Œå¤šç§ä»»åŠ¡çš„é€šç”¨æœºå™¨äººæ“ä½œç³»ç»Ÿæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ï¼Œè¿™ä¸»è¦å½’åŠŸäºå®ƒä»¬ä»å¤§è§„æ¨¡æ•°æ®ä¸­è·å–çš„å¤§é‡ä¸–ç•ŒçŸ¥è¯†ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç©ºé—´è¡¨ç¤ºï¼ˆå¦‚è¡¨ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹æˆ–è¡¨ç¤ºç‰©ä½“æ–¹å‘çš„å‘é‡ï¼‰å……å½“äº†VLMå’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¹‹é—´çš„æ¡¥æ¢ï¼Œæœ‰æ•ˆåœ°å°†VLMçš„æ¨ç†èƒ½åŠ›åº”ç”¨äºç‰¹å®šä»»åŠ¡åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºVLMçš„æœºå™¨äººæ–¹æ³•å¾€å¾€é‡‡ç”¨å›ºå®šçš„ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆæ¥åº”å¯¹å„ç§ä»»åŠ¡ï¼Œå¯¼è‡´è¡¨ç¤ºèƒ½åŠ›ä¸è¶³æˆ–æå–æ—¶é—´è¿‡é•¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†T-Rexï¼Œä¸€ä¸ªç”¨äºç©ºé—´è¡¨ç¤ºæå–çš„ä»»åŠ¡è‡ªé€‚åº”æ¡†æ¶ï¼Œå®ƒæ ¹æ®ç‰¹å®šä»»åŠ¡è¦æ±‚åŠ¨æ€é€‰æ‹©æ¯ä¸ªå®ä½“çš„æœ€åˆé€‚ç©ºé—´è¡¨ç¤ºæå–æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œä»»åŠ¡å¤æ‚æ€§å†³å®šäº†ç©ºé—´è¡¨ç¤ºçš„ç±»å‹å’Œç²’åº¦ï¼Œè€Œæ›´å¼ºçš„è¡¨ç¤ºèƒ½åŠ›é€šå¸¸ä¸æ›´é«˜çš„æ•´ä½“ç³»ç»Ÿæ“ä½œæˆæœ¬ç›¸å…³ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„æœºå™¨äººç¯å¢ƒä¸­è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç©ºé—´ç†è§£ã€æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19498v1">PDF</a> submitted to NeurIPS 2025</p>
<p><strong>Summary</strong><br>åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿæ‰§è¡Œå¤šç§ä»»åŠ¡çš„é€šç”¨æœºå™¨äººæ“çºµç³»ç»Ÿæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æœºå™¨äººæ“çºµä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œä¸»è¦ç”±äºå®ƒä»¬ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­è·å¾—å¤§é‡ä¸–ç•ŒçŸ¥è¯†ã€‚ç©ºé—´è¡¨å¾ï¼ˆå¦‚è¡¨ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹æˆ–è¡¨ç¤ºç‰©ä½“æ–¹å‘çš„å‘é‡ï¼‰ä½œä¸ºVLMså’ŒçœŸå®åœºæ™¯ä¹‹é—´çš„æ¡¥æ¢ï¼Œæœ‰æ•ˆåœ°å°†VLMsçš„æ¨ç†èƒ½åŠ›åº”ç”¨äºç‰¹å®šä»»åŠ¡åœºæ™¯ã€‚ç°æœ‰åŸºäºVLMçš„æœºå™¨äººæ–¹æ³•é€šå¸¸é‡‡ç”¨å›ºå®šçš„ç©ºé—´è¡¨å¾æå–æ–¹æ¡ˆï¼Œå¯¼è‡´è¡¨å¾èƒ½åŠ›ä¸è¶³æˆ–æå–æ—¶é—´è¿‡é•¿ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†T-Rexï¼Œä¸€ä¸ªä»»åŠ¡è‡ªé€‚åº”çš„ç©ºé—´è¡¨å¾æå–æ¡†æ¶ï¼Œæ ¹æ®ç‰¹å®šä»»åŠ¡è¦æ±‚åŠ¨æ€é€‰æ‹©æœ€é€‚å½“çš„ç©ºé—´è¡¨å¾æå–æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æœºå™¨äººç¯å¢ƒä¸­çš„ç©ºé—´ç†è§£ã€æ•ˆç‡å’Œç¨³å®šæ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººæ“çºµç³»ç»Ÿåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„é€šç”¨æ€§æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æœºå™¨äººæ“çºµä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>ç©ºé—´è¡¨å¾æ˜¯è¿æ¥VLMså’ŒçœŸå®åœºæ™¯çš„å…³é”®æ¡¥æ¢ã€‚</li>
<li>ç°æœ‰åŸºäºVLMçš„æœºå™¨äººæ–¹æ³•å­˜åœ¨è¡¨å¾èƒ½åŠ›ä¸è¶³å’Œæå–æ—¶é—´è¿‡é•¿çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥T-Rexï¼Œä¸€ä¸ªä»»åŠ¡è‡ªé€‚åº”çš„ç©ºé—´è¡¨å¾æå–æ¡†æ¶ã€‚</li>
<li>T-Rexèƒ½æ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€é€‰æ‹©ç©ºé—´è¡¨å¾æå–æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ff768184aa8215c731270652a14ea0e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8ce20358484efc56cd6f1d71d5079f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8103856219b21eaf4b4ca3fa744a8ffd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afcee36152b820f7129f5c30481c90a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5354b42ed391b1c320fc97f51a088f1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Commonsense-Generation-and-Evaluation-for-Dialogue-Systems-using-Large-Language-Models"><a href="#Commonsense-Generation-and-Evaluation-for-Dialogue-Systems-using-Large-Language-Models" class="headerlink" title="Commonsense Generation and Evaluation for Dialogue Systems using Large   Language Models"></a>Commonsense Generation and Evaluation for Dialogue Systems using Large   Language Models</h2><p><strong>Authors:Marcos Estecha-Garitagoitia, Chen Zhang, Mario RodrÃ­guez-Cantelar, Luis Fernando Dâ€™Haro</strong></p>
<p>This paper provides preliminary results on exploring the task of performing turn-level data augmentation for dialogue system based on different types of commonsense relationships, and the automatic evaluation of the generated synthetic turns. The proposed methodology takes advantage of the extended knowledge and zero-shot capabilities of pretrained Large Language Models (LLMs) to follow instructions, understand contextual information, and their commonsense reasoning capabilities. The approach draws inspiration from methodologies like Chain-of-Thought (CoT), applied more explicitly to the task of prompt-based generation for dialogue-based data augmentation conditioned on commonsense attributes, and the automatic evaluation of the generated dialogues.   To assess the effectiveness of the proposed approach, first we extracted 200 randomly selected partial dialogues, from 5 different well-known dialogue datasets, and generate alternative responses conditioned on different event commonsense attributes. This novel dataset allows us to measure the proficiency of LLMs in generating contextually relevant commonsense knowledge, particularly up to 12 different specific ATOMIC [10] database relations. Secondly, we propose an evaluation framework to automatically detect the quality of the generated dataset inspired by the ACCENT [26] metric, which offers a nuanced approach to assess event commonsense. However, our method does not follow ACCENTâ€™s complex eventrelation tuple extraction process. Instead, we propose an instruction-based prompt for each commonsense attribute and use state-of-the-art LLMs to automatically detect the original attributes used when creating each augmented turn in the previous step.   Preliminary results suggest that our approach effectively harnesses LLMs capabilities for commonsense reasoning and evaluation in dialogue systems. </p>
<blockquote>
<p>æœ¬æ–‡åˆæ­¥æ¢è®¨äº†åŸºäºä¸åŒç±»å‹å¸¸è¯†å…³ç³»çš„å¯¹è¯ç³»ç»Ÿæ‰§è¡Œå›åˆçº§åˆ«æ•°æ®å¢å¼ºçš„ä»»åŠ¡ï¼Œä»¥åŠç”Ÿæˆçš„åˆæˆå›åˆçš„è‡ªåŠ¨è¯„ä¼°ã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‰©å±•çŸ¥è¯†å’Œé›¶æ ·æœ¬èƒ½åŠ›æ¥æ‰§è¡ŒæŒ‡ä»¤ã€ç†è§£ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠä»–ä»¬çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å—åˆ°â€œæ€ç»´é“¾â€ï¼ˆCoTï¼‰ç­‰æ–¹æ³•çš„å¯å‘ï¼Œæ›´æ˜ç¡®åœ°åº”ç”¨äºåŸºäºæç¤ºçš„ç”Ÿæˆä»»åŠ¡ï¼Œç”¨äºåŸºäºå¸¸è¯†å±æ€§è¿›è¡Œå¯¹è¯æ•°æ®å¢å¼ºï¼Œå¹¶å¯¹ç”Ÿæˆçš„å¯¹è¯è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é¦–å…ˆä»äº”ä¸ªä¸åŒçš„çŸ¥åå¯¹è¯æ•°æ®é›†ä¸­éšæœºæŠ½å–äº†200ä¸ªéƒ¨åˆ†å¯¹è¯ï¼Œå¹¶æ ¹æ®ä¸åŒçš„äº‹ä»¶å¸¸è¯†å±æ€§ç”Ÿæˆäº†æ›¿ä»£å“åº”ã€‚è¿™ä¸ªæ–°æ•°æ®é›†å…è®¸æˆ‘ä»¬è¡¡é‡LLMåœ¨ç”Ÿæˆä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„å¸¸è¯†çŸ¥è¯†æ–¹é¢çš„ç†Ÿç»ƒç¨‹åº¦ï¼Œç‰¹åˆ«æ˜¯å¤šè¾¾12ç§ä¸åŒçš„ATOMIC [10]æ•°æ®åº“å…³ç³»ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä»¥è‡ªåŠ¨æ£€æµ‹ç”Ÿæˆæ•°æ®é›†çš„è´¨é‡ï¼Œè¯¥æ¡†æ¶å—åˆ°ACCENT [26]æŒ‡æ ‡çš„å¯å‘ï¼Œæä¾›äº†ä¸€ä¸ªå¾®å¦™çš„æ–¹æ³•æ¥è¯„ä¼°äº‹ä»¶å¸¸è¯†ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¹¶ä¸éµå¾ªACCENTçš„å¤æ‚äº‹ä»¶å…³ç³»å…ƒç»„æå–è¿‡ç¨‹ã€‚ç›¸åï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå¸¸è¯†å±æ€§æå‡ºäº†åŸºäºæŒ‡ä»¤çš„æç¤ºï¼Œå¹¶ä½¿ç”¨æœ€æ–°çš„LLMæ¥è‡ªåŠ¨æ£€æµ‹åœ¨åˆ›å»ºæ¯ä¸ªå¢å¼ºå›åˆæ—¶ä½¿ç”¨çš„åŸå§‹å±æ€§ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†LLMåœ¨å¯¹è¯ç³»ç»Ÿä¸­çš„å¸¸è¯†æ¨ç†å’Œè¯„ä¼°èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19483v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆæ­¥æ¢è®¨äº†åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¯¹è¯ç³»ç»Ÿå±‚é¢çš„æ•°æ®å¢å¼ºä»»åŠ¡ã€‚ç ”ç©¶é€šè¿‡åŸºäºä¸åŒå¸¸è¯†å…³ç³»ç”Ÿæˆåˆæˆå¯¹è¯ï¼Œå¹¶è‡ªåŠ¨è¯„ä¼°å…¶è´¨é‡ã€‚ç ”ç©¶çµæ„Ÿæ¥æºäºChain-of-Thoughtï¼ˆCoTï¼‰æ–¹æ³•ï¼Œå¹¶åº”ç”¨äºåŸºäºæç¤ºçš„ç”Ÿæˆå¼å¯¹è¯æ•°æ®å¢å¼ºä»»åŠ¡ä¸­ã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨LLMsçš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆè¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¯¹è¯ç³»ç»Ÿçš„æ•°æ®å¢å¼ºä»»åŠ¡ã€‚</li>
<li>åŸºäºä¸åŒå¸¸è¯†å…³ç³»ç”Ÿæˆåˆæˆå¯¹è¯ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨è¯„ä¼°æ¡†æ¶è¯„ä¼°ç”Ÿæˆå¯¹è¯çš„è´¨é‡ã€‚</li>
<li>å€Ÿé‰´Chain-of-Thoughtï¼ˆCoTï¼‰æ–¹æ³•ï¼Œåº”ç”¨äºåŸºäºæç¤ºçš„ç”Ÿæˆå¼å¯¹è¯æ•°æ®å¢å¼ºä»»åŠ¡ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªæ–°æ•°æ®é›†æ¥æµ‹è¯•LLMsåœ¨ç”Ÿæˆä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„å¸¸è¯†çŸ¥è¯†æ–¹é¢çš„ç†Ÿç»ƒç¨‹åº¦ã€‚</li>
<li>æå‡ºä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æç¤ºæ¥è‡ªåŠ¨æ£€æµ‹ç”Ÿæˆæ•°æ®é›†çš„åŸå§‹å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fbf917b59c8131cd7ac699e53577f39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab34624f1b74365055b294fa25729e1e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Surgery-R1-Advancing-Surgical-VQLA-with-Reasoning-Multimodal-Large-Language-Model-via-Reinforcement-Learning"><a href="#Surgery-R1-Advancing-Surgical-VQLA-with-Reasoning-Multimodal-Large-Language-Model-via-Reinforcement-Learning" class="headerlink" title="Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large   Language Model via Reinforcement Learning"></a>Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large   Language Model via Reinforcement Learning</h2><p><strong>Authors:Pengfei Hao, Shuaibo Li, Hongqiu Wang, Zhizhuo Kou, Junhang Zhang, Guang Yang, Lei Zhu</strong></p>
<p>In recent years, significant progress has been made in the field of surgical scene understanding, particularly in the task of Visual Question Localized-Answering in robotic surgery (Surgical-VQLA). However, existing Surgical-VQLA models lack deep reasoning capabilities and interpretability in surgical scenes, which limits their reliability and potential for development in clinical applications. To address this issue, inspired by the development of Reasoning Multimodal Large Language Models (MLLMs), we first build the Surgery-R1-54k dataset, including paired data for Visual-QA, Grounding-QA, and Chain-of-Thought (CoT). Then, we propose the first Reasoning MLLM for Surgical-VQLA (Surgery-R1). In our Surgery-R1, we design a two-stage fine-tuning mechanism to enable the basic MLLM with complex reasoning abilities by utilizing supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). Furthermore, for an efficient and high-quality rule-based reward system in our RFT, we design a Multimodal Coherence reward mechanism to mitigate positional illusions that may arise in surgical scenarios. Experiment results demonstrate that Surgery-R1 outperforms other existing state-of-the-art (SOTA) models in the Surgical-VQLA task and widely-used MLLMs, while also validating its reasoning capabilities and the effectiveness of our approach. The code and dataset will be organized in <a target="_blank" rel="noopener" href="https://github.com/FiFi-HAO467/Surgery-R1">https://github.com/FiFi-HAO467/Surgery-R1</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰‹æœ¯åœºæ™¯ç†è§£é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæ‰‹æœ¯çš„è§†è§‰é—®ç­”å®šä½å›ç­”ï¼ˆSurgical-VQLAï¼‰ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Surgical-VQLAæ¨¡å‹åœ¨æ‰‹æœ¯åœºæ™¯çš„æ·±åº¦æ¨ç†èƒ½åŠ›å’Œè§£é‡Šæ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠåº”ç”¨çš„å¯é æ€§å’Œå‘å±•æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å—åˆ°å¤šæ¨¡æ€æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•çš„å¯å‘ï¼Œé¦–å…ˆæ„å»ºäº†Surgery-R1-54kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è§†è§‰é—®ç­”ã€å®šä½é—®ç­”å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰çš„é…å¯¹æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªé’ˆå¯¹Surgical-VQLAçš„æ¨ç†MLLMæ¨¡å‹ï¼ˆSurgery-R1ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„Surgery-R1ä¸­ï¼Œé€šè¿‡è®¾è®¡ä¸¤é˜¶æ®µå¾®è°ƒæœºåˆ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä½¿åŸºç¡€MLLMå…·å¤‡å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†åœ¨æˆ‘ä»¬çš„RFTä¸­å»ºç«‹é«˜æ•ˆé«˜è´¨é‡çš„åŸºäºè§„åˆ™å¥–åŠ±ç³»ç»Ÿï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šæ¨¡æ€ä¸€è‡´æ€§å¥–åŠ±æœºåˆ¶æ¥ç¼“è§£æ‰‹æœ¯ä¸­å¯èƒ½å‡ºç°çš„å®šä½é”™è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurgery-R1åœ¨Surgical-VQLAä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹ä»¥åŠå¸¸ç”¨çš„MLLMsï¼ŒåŒæ—¶éªŒè¯äº†å…¶æ¨ç†èƒ½åŠ›å’Œæˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†æ•´ç†åœ¨<a target="_blank" rel="noopener" href="https://github.com/FiFi-HAO467/Surgery-R1%E3%80%82">https://github.com/FiFi-HAO467/Surgery-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19469v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œæ‰‹æœ¯åœºæ™¯ç†è§£é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæ‰‹æœ¯çš„è§†è§‰é—®ç­”å®šä½å›ç­”ï¼ˆSurgical-VQLAï¼‰ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Surgical-VQLAæ¨¡å‹åœ¨æ‰‹æœ¯åœºæ™¯ä¸­ç¼ºä¹æ·±åº¦æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å¯é æ€§å’Œå‘å±•æ½œåŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«è§†è§‰é—®ç­”ã€å®šä½é—®ç­”å’Œæ€ç»´é“¾é—®ç­”çš„é…å¯¹æ•°æ®é›†Surgery-R1-54kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†é¦–ä¸ªç”¨äºSurgical-VQLAçš„æ¨ç†MLLMæ¨¡å‹ï¼ˆSurgery-R1ï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è®¾è®¡ä¸¤é˜¶æ®µå¾®è°ƒæœºåˆ¶å’Œå¤šæ¨¡æ€ä¸€è‡´æ€§å¥–åŠ±æœºåˆ¶ï¼Œå®ç°äº†å¤æ‚æ¨ç†èƒ½åŠ›çš„é«˜æ•ˆè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSurgery-R1åœ¨Surgical-VQLAä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶éªŒè¯äº†å…¶æ¨ç†èƒ½åŠ›å’Œæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/FiFi-HAO467/Surgery-R1%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/FiFi-HAO467/Surgery-R1å…¬å¼€ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰Surgical-VQLAæ¨¡å‹ç¼ºä¹æ·±åº¦æ¨ç†èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>æ„å»ºSurgery-R1-54kæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§é—®ç­”ç±»å‹æ•°æ®ã€‚</li>
<li>æå‡ºé¦–ä¸ªç”¨äºSurgical-VQLAçš„æ¨ç†MLLMæ¨¡å‹ï¼ˆSurgery-R1ï¼‰ã€‚</li>
<li>è®¾è®¡ä¸¤é˜¶æ®µå¾®è°ƒæœºåˆ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä½¿åŸºç¡€MLLMå…·å¤‡å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡å¤šæ¨¡æ€ä¸€è‡´æ€§å¥–åŠ±æœºåˆ¶ä»¥ç¼“è§£æ‰‹æœ¯åœºæ™¯ä¸­çš„ä½ç½®é”™è§‰é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSurgery-R1åœ¨Surgical-VQLAä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-675410bdd8e90d857270e35e1bb610ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5832dd4ac3247cf8785bc95cc7b4319c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6faa024727ecf5036c9c87a0dec831a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d28e37cd96eaffdcef4a9433cd1847.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Capture-Human-Annotator-Disagreements"><a href="#Can-Large-Language-Models-Capture-Human-Annotator-Disagreements" class="headerlink" title="Can Large Language Models Capture Human Annotator Disagreements?"></a>Can Large Language Models Capture Human Annotator Disagreements?</h2><p><strong>Authors:Jingwei Ni, Yu Fan, VilÃ©m Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash</strong></p>
<p>Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted â€œground truthâ€ labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMsâ€™ ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at <a target="_blank" rel="noopener" href="https://github.com/EdisonNi-hku/Disagreement_Prediction">https://github.com/EdisonNi-hku/Disagreement_Prediction</a>. </p>
<blockquote>
<p>äººç±»æ ‡æ³¨å·®å¼‚ï¼ˆå³æ ‡æ³¨åˆ†æ­§ï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ˜¯å¸¸è§çš„ï¼Œå¹¶ä¸”é€šå¸¸åæ˜ äº†é‡è¦ä¿¡æ¯ï¼Œå¦‚ä»»åŠ¡çš„ä¸»è§‚æ€§å’Œæ ·æœ¬çš„æ¨¡ç³Šæ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè‡ªåŠ¨æ ‡æ³¨ï¼Œä»¥å‡å°‘äººåŠ›æŠ•å…¥ï¼Œä½†å…¶è¯„ä¼°é€šå¸¸ä¾§é‡äºé¢„æµ‹å¤šæ•°æŠ•ç¥¨çš„â€œçœŸå®â€æ ‡ç­¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ˜¯å¦ä¹Ÿèƒ½æ•æ‰äººç±»æ ‡æ³¨çš„å·®å¼‚æ€§ä¿¡æ¯ï¼Œä»ç„¶ä¸æ˜ç¡®ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å¹¿æ³›è¯„ä¼°LLMåœ¨æ²¡æœ‰é‡å¤äººå·¥æ ‡ç­¾çš„æƒ…å†µä¸‹é¢„æµ‹æ ‡æ³¨åˆ†æ­§çš„èƒ½åŠ›æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨å»ºæ¨¡åˆ†æ­§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¯èƒ½ä¼šè¢«åŸºäºå¤šæ•°æ ‡ç­¾çš„è¯„ä¼°æ‰€å¿½è§†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶RLVRé£æ ¼ï¼ˆä¸€ç§å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼‰çš„æ¨ç†é€šå¸¸ä¼šæå‡LLMæ€§èƒ½ï¼Œä½†åœ¨åˆ†æ­§é¢„æµ‹æ–¹é¢å´ä¼šé™ä½æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è¯„ä¼°å’Œæ”¹è‰¯LLMåœ¨åˆ†æ­§å»ºæ¨¡ä¸­çš„æ ‡æ³¨è€…çš„è¿«åˆ‡éœ€æ±‚ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/EdisonNi-hku/Disagreement_Prediction%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/EdisonNi-hku/Disagreement_Predictionè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19467v1">PDF</a> Preprint Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä¸­äººç±»æ ‡æ³¨å·®å¼‚ï¼ˆå³æ ‡æ³¨åˆ†æ­§ï¼‰çš„é—®é¢˜ï¼Œè¿™æ˜¯å¸¸è§çš„ç°è±¡å¹¶åæ˜ äº†ä»»åŠ¡ä¸»è§‚æ€§å’Œæ ·æœ¬æ¨¡ç³Šæ€§ç­‰é‡è¦ä¿¡æ¯ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨äºè‡ªåŠ¨æ ‡æ³¨ä»¥å‡å°‘äººåŠ›æŠ•å…¥ï¼Œä½†å…¶è¯„ä¼°é€šå¸¸ä¾§é‡äºé¢„æµ‹å¤šæ•°æŠ•ç¥¨çš„â€œçœŸå®â€æ ‡ç­¾ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½æ•æ‰äººç±»æ ‡æ³¨åˆ†æ­§çš„ä¿¡æ¯ã€‚æœ¬ç ”ç©¶é€šè¿‡å¹¿æ³›è¯„ä¼°LLMsé¢„æµ‹åˆ†æ­§çš„èƒ½åŠ›ï¼Œè§£å†³äº†è¿™ä¸€ç©ºç™½ï¼ŒåŒæ—¶ä¸ä¾èµ–é‡å¤çš„äººç±»æ ‡ç­¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨å»ºæ¨¡åˆ†æ­§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œè€Œè¿™å¯èƒ½è¢«åŸºäºå¤šæ•°æ ‡ç­¾çš„è¯„ä¼°æ‰€å¿½è§†ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é£æ ¼çš„æ¨ç†é€šå¸¸èƒ½æé«˜LLMæ€§èƒ½ï¼Œä½†åœ¨åˆ†æ­§é¢„æµ‹æ–¹é¢å´ä¼šèµ·åˆ°åä½œç”¨ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è¯„ä¼°å’Œæ”¹è¿›LLMåˆ†æ­§å»ºæ¨¡èƒ½åŠ›çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»æ ‡æ³¨åˆ†æ­§æ˜¯NLPä¸­çš„å¸¸è§ç°è±¡ï¼Œåæ˜ äº†ä»»åŠ¡ä¸»è§‚æ€§å’Œæ ·æœ¬æ¨¡ç³Šæ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨äºè‡ªåŠ¨æ ‡æ³¨ï¼Œä½†å…¶è¯„ä¼°ä¸»è¦å…³æ³¨é¢„æµ‹å¤šæ•°æŠ•ç¥¨çš„â€œçœŸå®â€æ ‡ç­¾ã€‚</li>
<li>LLMsåœ¨å»ºæ¨¡äººç±»æ ‡æ³¨åˆ†æ­§æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¯èƒ½è¢«åŸºäºå¤šæ•°æ ‡ç­¾çš„è¯„ä¼°æ‰€å¿½è§†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é£æ ¼çš„æ¨ç†èƒ½æé«˜LLMæ€§èƒ½ï¼Œä½†åœ¨åˆ†æ­§é¢„æµ‹æ–¹é¢å´ä¼šèµ·åˆ°åä½œç”¨ã€‚</li>
<li>éœ€è¦è¯„ä¼°å’Œæ”¹è¿›LLMåœ¨å»ºæ¨¡åˆ†æ­§æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« æä¾›äº†ç›¸å…³ä»£ç å’Œæ•°æ®ï¼Œå¯åœ¨æŒ‡å®šçš„GitHubä»“åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/EdisonNi-hku/Disagreement_Prediction%EF%BC%89%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EdisonNi-hku/Disagreement_Predictionï¼‰ä¸­æ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b58db0e7f6188a091540f68aaf1b286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40addaae8ba1e2149019d525d3a94b4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bea0c533093bddbbe0644947dcab5b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="KunLunBaizeRAG-Reinforcement-Learning-Driven-Inference-Performance-Leap-for-Large-Language-Models"><a href="#KunLunBaizeRAG-Reinforcement-Learning-Driven-Inference-Performance-Leap-for-Large-Language-Models" class="headerlink" title="KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap   for Large Language Models"></a>KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap   for Large Language Models</h2><p><strong>Authors:Cheng Li, Jiexiong Liu, Yixuan Chen, Qihang Zhou, KunLun Meta</strong></p>
<p>This paper introduces KunLunBaizeRAG, a reinforcement learning-driven reasoning framework designed to enhance the reasoning capabilities of large language models (LLMs) in complex multi-hop question-answering tasks. The framework addresses key limitations of traditional RAG, such as retrieval drift, information redundancy, and strategy rigidity. Key innovations include the RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative Enhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR) mechanism, and a progressive hybrid training strategy. Experimental results demonstrate significant improvements in exact match (EM) and LLM-judged score (LJ) across four benchmarks, highlighting the frameworkâ€™s robustness and effectiveness in complex reasoning scenarios. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æ˜†ä»‘ç™½å­RAAGï¼ˆKunLunBaizeRAGï¼‰è¿™ä¸€ç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨ï¼ˆReinforcement Learning-drivenï¼‰çš„æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»ŸRAAGçš„å…³é”®å±€é™æ€§ï¼Œå¦‚æ£€ç´¢æ¼‚ç§»ã€ä¿¡æ¯å†—ä½™å’Œç­–ç•¥åƒµåŒ–ç­‰ã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬RAAGé©±åŠ¨çš„æ¨ç†å¯¹é½ï¼ˆRDRAï¼‰æœºåˆ¶ã€æœç´¢æ€è€ƒè¿­ä»£å¢å¼ºï¼ˆSTIEï¼‰æœºåˆ¶ã€ç½‘ç»œå±€éƒ¨æ™ºèƒ½è·¯ç”±ï¼ˆNLRï¼‰æœºåˆ¶å’Œä¸€ç§æ¸è¿›å¼æ··åˆè®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰å’ŒLLMåˆ¤æ–­å¾—åˆ†ï¼ˆLJï¼‰å‡æ˜¾è‘—æé«˜ï¼Œå‡¸æ˜¾å…¶åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19466v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†KunLunBaizeRAGè¿™ä¸€å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»ŸRAGçš„å…³é”®å±€é™æ€§ï¼Œå¦‚æ£€ç´¢æ¼‚ç§»ã€ä¿¡æ¯å†—ä½™å’Œç­–ç•¥åƒµåŒ–ã€‚åˆ›æ–°ç‚¹åŒ…æ‹¬RAGé©±åŠ¨çš„æ¨ç†å¯¹é½æœºåˆ¶ï¼ˆRDRAï¼‰ã€æœç´¢æ€è€ƒè¿­ä»£å¢å¼ºï¼ˆSTIEï¼‰æœºåˆ¶ã€ç½‘ç»œæœ¬åœ°æ™ºèƒ½è·¯ç”±ï¼ˆNLRï¼‰æœºåˆ¶ä»¥åŠæ¸è¿›å¼æ··åˆè®­ç»ƒç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰å’ŒLLMè¯„åˆ†ï¼ˆLJï¼‰å‡æœ‰æ˜¾è‘—æé«˜ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>KunLunBaizeRAGæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„æ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶è§£å†³äº†ä¼ ç»ŸRAGåœ¨å¤æ‚å¤šè·³é—®ç­”ä»»åŠ¡ä¸­çš„å…³é”®å±€é™æ€§ã€‚</li>
<li>æ¡†æ¶åŒ…å«å¤šä¸ªåˆ›æ–°æœºåˆ¶ï¼Œå¦‚RAGé©±åŠ¨çš„æ¨ç†å¯¹é½æœºåˆ¶ï¼ˆRDRAï¼‰ã€æœç´¢æ€è€ƒè¿­ä»£å¢å¼ºï¼ˆSTIEï¼‰ä»¥åŠç½‘ç»œæœ¬åœ°æ™ºèƒ½è·¯ç”±ï¼ˆNLRï¼‰ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›å¼æ··åˆè®­ç»ƒç­–ç•¥ï¼Œæé«˜æ¡†æ¶çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒKunLunBaizeRAGåœ¨ç²¾ç¡®åŒ¹é…å’ŒLLMè¯„åˆ†æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹äºè§£å†³å¤æ‚æ¨ç†åœºæ™¯å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-165309aa0f46d520b4dcf3cc2aae0ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e6fe6e76edebdac48c7409c026111bb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Style-Transfer-A-Decade-Survey"><a href="#Style-Transfer-A-Decade-Survey" class="headerlink" title="Style Transfer: A Decade Survey"></a>Style Transfer: A Decade Survey</h2><p><strong>Authors:Tianshan Zhang, Hao Tang</strong></p>
<p>The revolutionary advancement of Artificial Intelligence Generated Content (AIGC) has fundamentally transformed the landscape of visual content creation and artistic expression. While remarkable progress has been made in image generation and style transfer, the underlying mechanisms and aesthetic implications of these technologies remain insufficiently understood. This paper presents a comprehensive survey of AIGC technologies in visual arts, tracing their evolution from early algorithmic frameworks to contemporary deep generative models. We identify three pivotal paradigms: Variational Autoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion Models, and examine their roles in bridging the gap between human creativity and machine synthesis. To support our analysis, we systematically review over 500 research papers published in the past decade, spanning both foundational developments and state-of-the-art innovations. Furthermore, we propose a multidimensional evaluation framework that incorporates Technical Innovation, Artistic Merit, Visual Quality, Computational Efficiency, and Creative Potential. Our findings reveal both the transformative capacities and current limitations of AIGC systems, emphasizing their profound impact on the future of creative practices. Through this extensive synthesis, we offer a unified perspective on the convergence of artificial intelligence and artistic expression, while outlining key challenges and promising directions for future research in this rapidly evolving field. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„é©å‘½æ€§è¿›æ­¥ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è§†è§‰å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯è¡¨è¾¾é¢†åŸŸçš„æ ¼å±€ã€‚è™½ç„¶åœ¨å›¾åƒç”Ÿæˆå’Œé£æ ¼è½¬æ¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æŠ€æœ¯çš„åŸºç¡€æœºåˆ¶å’Œç¾å­¦å½±å“å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚æœ¬æ–‡å¯¹AIGCæŠ€æœ¯åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸè¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œè¿½æº¯äº†ä»æ—©æœŸç®—æ³•æ¡†æ¶åˆ°å½“å‰æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„æ¼”å˜è¿‡ç¨‹ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸‰ç§å…³é”®èŒƒå¼ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨å¼¥åˆäººç±»åˆ›é€ åŠ›å’Œæœºå™¨åˆæˆä¹‹é—´çš„å·®è·æ–¹é¢çš„ä½œç”¨ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†è¿‡å»åå¹´é—´å‘è¡¨çš„500å¤šç¯‡ç ”ç©¶è®ºæ–‡ï¼Œè¿™äº›è®ºæ–‡æ¶µç›–äº†åŸºç¡€å‘å±•å’Œæœ€æ–°åˆ›æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç»´è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†æŠ€æœ¯åˆ›æ–°ã€è‰ºæœ¯ä»·å€¼ã€è§†è§‰è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œåˆ›æ„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†AIGCç³»ç»Ÿçš„å˜é©èƒ½åŠ›å’Œå½“å‰å±€é™æ€§ï¼Œå¼ºè°ƒäº†å®ƒä»¬å¯¹åˆ›æ„å®è·µæœªæ¥å‘å±•çš„æ·±è¿œå½±å“ã€‚é€šè¿‡è¿™ä¸€å…¨é¢çš„ç»¼è¿°ï¼Œæˆ‘ä»¬å¯¹äººå·¥æ™ºèƒ½å’Œè‰ºæœ¯è¡¨è¾¾çš„èåˆæä¾›äº†ç»Ÿä¸€çš„è§†è§’ï¼ŒåŒæ—¶æ¦‚è¿°äº†æœªæ¥åœ¨è¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸè¿›è¡Œç ”ç©¶çš„å…³é”®æŒ‘æˆ˜å’Œå……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19278v1">PDF</a> 32 pages</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„é©å‘½æ€§è¿›å±•ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è§†è§‰å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯è¡¨è¾¾é¢†åŸŸçš„æ ¼å±€ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†AIGCæŠ€æœ¯åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸçš„å‘å±•ï¼Œä»æ—©æœŸçš„ç®—æ³•æ¡†æ¶åˆ°å½“å‰çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡ç³»ç»Ÿå›é¡¾è¿‡å»åå¹´çš„500å¤šç¯‡ç ”ç©¶è®ºæ–‡ï¼Œæœ¬æ–‡åˆ†æäº†ä¸‰ç§å…³é”®èŒƒå¼ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œå®ƒä»¬åœ¨å¼¥åˆäººç±»åˆ›é€ åŠ›å’Œæœºå™¨åˆæˆä¹‹é—´çš„å·®è·æ–¹é¢å‘æŒ¥çš„ä½œç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯åˆ›æ–°ã€è‰ºæœ¯ä»·å€¼ã€è§†è§‰è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œåˆ›æ„æ½œåŠ›ã€‚ç ”ç©¶å‘ç°æ­ç¤ºäº†AIGCç³»ç»Ÿçš„å˜é©æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼Œå¼ºè°ƒäº†å…¶å¯¹æœªæ¥åˆ›ä½œå®è·µçš„æ·±è¿œå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIGCæŠ€æœ¯å·²ç»æ˜¾è‘—æ”¹å˜äº†è§†è§‰å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯è¡¨è¾¾é¢†åŸŸçš„æ ¼å±€ã€‚</li>
<li>æœ¬æ–‡ç»¼è¿°äº†AIGCæŠ€æœ¯åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸçš„å‘å±•ï¼Œä»æ—©æœŸçš„ç®—æ³•æ¡†æ¶åˆ°å½“å‰çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ä¸‰ç§å…³é”®èŒƒå¼ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹åœ¨æ¡¥æ¢äººç±»åˆ›é€ åŠ›å’Œæœºå™¨åˆæˆæ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿå›é¡¾è¿‡å»åå¹´çš„ç ”ç©¶è®ºæ–‡ï¼Œæœ¬æ–‡æä¾›äº†å¯¹AIGCæŠ€æœ¯çš„å…¨é¢ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯åˆ›æ–°ã€è‰ºæœ¯ä»·å€¼ã€è§†è§‰è´¨é‡ç­‰æ–¹é¢æ¥è¯„ä¼°AIGCæŠ€æœ¯ã€‚</li>
<li>AIGCç³»ç»Ÿå…·æœ‰æ˜¾è‘—çš„å˜é©æ½œåŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å½“å‰å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d157e7d8274a0f9b1701c6e9aa3067f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40875cf66121601d702d012682effad9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0b6ebaad90e2906e6fca0512d556b4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7733ca583ae75b96a7acb551370954d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b21f6d35d979f474fec6fb0f2a4a399d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-400ecf746a338e4685d879d6132eb1c0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MSR-Align-Policy-Grounded-Multimodal-Alignment-for-Safety-Aware-Reasoning-in-Vision-Language-Models"><a href="#MSR-Align-Policy-Grounded-Multimodal-Alignment-for-Safety-Aware-Reasoning-in-Vision-Language-Models" class="headerlink" title="MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware   Reasoning in Vision-Language Models"></a>MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware   Reasoning in Vision-Language Models</h2><p><strong>Authors:Yinan Xia, Yilei Jiang, Yingshui Tan, Xiaoyong Zhu, Xiangyu Yue, Bo Zheng</strong></p>
<p>Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning tasks through enhanced chain-of-thought capabilities. However, this advancement also introduces novel safety risks, as these models become increasingly vulnerable to harmful multimodal prompts that can trigger unethical or unsafe behaviors. Existing safety alignment approaches, primarily designed for unimodal language models, fall short in addressing the complex and nuanced threats posed by multimodal inputs. Moreover, current safety datasets lack the fine-grained, policy-grounded reasoning required to robustly align reasoning-capable VLMs. In this work, we introduce {MSR-Align}, a high-quality Multimodal Safety Reasoning dataset tailored to bridge this gap. MSR-Align supports fine-grained, deliberative reasoning over standardized safety policies across both vision and text modalities. Our data generation pipeline emphasizes multimodal diversity, policy-grounded reasoning, and rigorous quality filtering using strong multimodal judges. Extensive experiments demonstrate that fine-tuning VLMs on MSR-Align substantially improves robustness against both textual and vision-language jailbreak attacks, while preserving or enhancing general reasoning performance. MSR-Align provides a scalable and effective foundation for advancing the safety alignment of reasoning-capable VLMs. Our dataset is made publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Leigest/MSR-Align">https://huggingface.co/datasets/Leigest/MSR-Align</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å¢å¼ºçš„æ€ç»´é“¾èƒ½åŠ›åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿›å±•ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨é£é™©ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹è¶Šæ¥è¶Šå®¹æ˜“å—åˆ°æœ‰å®³çš„å¤šæ¨¡æ€æç¤ºçš„è§¦å‘ï¼Œå¯èƒ½å¯¼è‡´ä¸é“å¾·æˆ–ä¸å®‰å…¨çš„è¡Œä¸ºã€‚ç°æœ‰çš„å®‰å…¨å¯¹é½æ–¹æ³•ä¸»è¦é’ˆå¯¹å•æ¨¡æ€è¯­è¨€æ¨¡å‹è®¾è®¡ï¼Œåœ¨åº”å¯¹ç”±å¤šæ¨¡æ€è¾“å…¥å¸¦æ¥çš„å¤æ‚å’Œå¾®å¦™å¨èƒæ–¹é¢æ˜¾å¾—åŠ›ä¸ä»å¿ƒã€‚æ­¤å¤–ï¼Œå½“å‰çš„å®‰å…¨æ•°æ®é›†ç¼ºä¹ç²¾ç»†çš„ã€ä»¥æ”¿ç­–ä¸ºåŸºç¡€çš„ç†ç”±ï¼Œæ— æ³•ç¨³å¥åœ°å¯¹å…·å¤‡æ¨ç†èƒ½åŠ›çš„VLMè¿›è¡Œå¯¹é½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†MSR-Alignï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€å®‰å…¨æ¨ç†æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå¼¥åˆè¿™ä¸€å·®è·ã€‚MSR-Alignæ”¯æŒè·¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„æ ‡å‡†å®‰å…¨æ”¿ç­–çš„ç²¾ç»†å®¡æ…æ¨ç†ã€‚æˆ‘ä»¬çš„æ•°æ®ç”Ÿæˆç®¡é“å¼ºè°ƒå¤šæ¨¡æ€å¤šæ ·æ€§ã€ä»¥æ”¿ç­–ä¸ºåŸºç¡€çš„æ¨ç†ï¼Œä»¥åŠä½¿ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€åˆ¤å®˜è¿›è¡Œä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨MSR-Alignä¸Šå¾®è°ƒVLMså¯ä»¥æ˜¾è‘—æé«˜å¯¹æŠ—æ–‡æœ¬å’Œè§†è§‰è¯­è¨€è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜ä¸€èˆ¬æ¨ç†æ€§èƒ½ã€‚MSR-Alignä¸ºæ¨è¿›å…·å¤‡æ¨ç†èƒ½åŠ›çš„VLMçš„å®‰å…¨å¯¹é½æä¾›äº†å¯ä¼¸ç¼©å’Œæœ‰æ•ˆçš„åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Leigest/MSR-Align%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://huggingface.co/datasets/Leigest/MSR-Alignå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19257v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†Vision-Language Modelsï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°åŠå…¶å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†MSR-Alignæ•°æ®é›†ä»¥åº”å¯¹åŸºäºå®‰å…¨æ€§å’Œç²¾ç»†æ¨ç†çš„å¤šæ¨¡æ€å®‰å…¨æ¨ç†éœ€æ±‚ã€‚è¯¥æ•°æ®é›†æ”¯æŒè·¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„æ ‡å‡†å®‰å…¨æ”¿ç­–çš„ç²¾ç»†å®¡æ…æ¨ç†ï¼Œå¹¶å¼ºè°ƒå¤šæ¨¡æ€å¤šæ ·æ€§ã€åŸºäºæ”¿ç­–çš„æ¨ç†å’Œä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨MSR-Alignä¸Šå¾®è°ƒVLMså¯ä»¥æé«˜å¯¹æ–‡æœ¬å’Œè§†è§‰è¯­è¨€è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜ä¸€èˆ¬æ¨ç†æ€§èƒ½ã€‚è¯¥æ•°æ®é›†ä¸ºæ¨è¿›å…·å¤‡æ¨ç†èƒ½åŠ›çš„VLMçš„å®‰å…¨å¯¹é½æä¾›äº†å¯æ‰©å±•å’Œæœ‰æ•ˆçš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰çš„å®‰å…¨å¯¹é½æ–¹æ³•åœ¨é¢å¯¹å¤šæ¨¡æ€è¾“å…¥æ—¶çš„å¤æ‚å’Œå¾®å¦™å¨èƒæ—¶è¡¨ç°ä¸è¶³ã€‚</li>
<li>MSR-Alignæ•°æ®é›†æ—¨åœ¨å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæ”¯æŒè·¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„æ ‡å‡†å®‰å…¨æ”¿ç­–çš„ç²¾ç»†å®¡æ…æ¨ç†ã€‚</li>
<li>æ•°æ®ç”Ÿæˆæµç¨‹å¼ºè°ƒå¤šæ¨¡æ€å¤šæ ·æ€§ã€åŸºäºæ”¿ç­–çš„æ¨ç†å’Œä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œåœ¨MSR-Alignä¸Šå¾®è°ƒVLMså¯ä»¥æé«˜å¯¹æ–‡æœ¬å’Œè§†è§‰è¯­è¨€è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>MSR-Alignä¸ºæ¨è¿›VLMçš„å®‰å…¨å¯¹é½æä¾›äº†æœ‰æ•ˆçš„åŸºç¡€ã€‚</li>
<li>MSR-Alignæ•°æ®é›†å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-908a805cea42afb1d580ba2780d398ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f989ebd036512dff10acec349ee34b88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0807f5eab06413d33890b90ec9a1228e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ce7e8f3cfaab901fbbe239045ecaa9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MOSCARD-â€“-Causal-Reasoning-and-De-confounding-for-Multimodal-Opportunistic-Screening-of-Cardiovascular-Adverse-Events"><a href="#MOSCARD-â€“-Causal-Reasoning-and-De-confounding-for-Multimodal-Opportunistic-Screening-of-Cardiovascular-Adverse-Events" class="headerlink" title="MOSCARD â€“ Causal Reasoning and De-confounding for Multimodal   Opportunistic Screening of Cardiovascular Adverse Events"></a>MOSCARD â€“ Causal Reasoning and De-confounding for Multimodal   Opportunistic Screening of Cardiovascular Adverse Events</h2><p><strong>Authors:Jialu Pi, Juan Maria Farina, Rimita Lahiri, Jiwoong Jeong, Archana Gurudu, Hyung-Bok Park, Chieh-Ju Chao, Chadi Ayoub, Reza Arsanjani, Imon Banerjee</strong></p>
<p>Major Adverse Cardiovascular Events (MACE) remain the leading cause of mortality globally, as reported in the Global Disease Burden Study 2021. Opportunistic screening leverages data collected from routine health check-ups and multimodal data can play a key role to identify at-risk individuals. Chest X-rays (CXR) provide insights into chronic conditions contributing to major adverse cardiovascular events (MACE), while 12-lead electrocardiogram (ECG) directly assesses cardiac electrical activity and structural abnormalities. Integrating CXR and ECG could offer a more comprehensive risk assessment than conventional models, which rely on clinical scores, computed tomography (CT) measurements, or biomarkers, which may be limited by sampling bias and single modality constraints. We propose a novel predictive modeling framework - MOSCARD, multimodal causal reasoning with co-attention to align two distinct modalities and simultaneously mitigate bias and confounders in opportunistic risk estimation. Primary technical contributions are - (i) multimodal alignment of CXR with ECG guidance; (ii) integration of causal reasoning; (iii) dual back-propagation graph for de-confounding. Evaluated on internal, shift data from emergency department (ED) and external MIMIC datasets, our model outperformed single modality and state-of-the-art foundational models - AUC: 0.75, 0.83, 0.71 respectively. Proposed cost-effective opportunistic screening enables early intervention, improving patient outcomes and reducing disparities. </p>
<blockquote>
<p>ä¸»è¦å¿ƒè¡€ç®¡ä¸è‰¯äº‹ä»¶ï¼ˆMACEï¼‰ä»æ˜¯å…¨çƒä¸»è¦æ­»å› ï¼Œå¦‚ã€Šå…¨çƒç–¾ç—…è´Ÿæ‹…ç ”ç©¶ 2021ã€‹æ‰€æŠ¥é“ã€‚æœºä¼šæ€§ç­›æŸ¥å……åˆ†åˆ©ç”¨å¸¸è§„å¥åº·æ£€æŸ¥å’Œå¤šç§æ¨¡å¼æ”¶é›†çš„æ•°æ®ï¼Œå¯ä»¥åœ¨è¯†åˆ«é«˜å±ä¸ªä½“æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ã€‚èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æä¾›äº†å¯¼è‡´ä¸»è¦å¿ƒè¡€ç®¡ä¸è‰¯äº‹ä»¶ï¼ˆMACEï¼‰çš„æ…¢æ€§ç–¾ç—…çš„è§è§£ï¼Œè€Œ12å¯¼è”å¿ƒç”µå›¾ï¼ˆECGï¼‰åˆ™ç›´æ¥è¯„ä¼°å¿ƒè„ç”µæ´»åŠ¨å’Œç»“æ„å¼‚å¸¸ã€‚å°†CXRå’ŒECGç›¸ç»“åˆï¼Œå¯ä»¥æä¾›æ¯”ä¼ ç»Ÿæ¨¡å‹æ›´å…¨é¢çš„é£é™©è¯„ä¼°ã€‚ä¼ ç»Ÿæ¨¡å‹ä¾èµ–äºä¸´åºŠè¯„åˆ†ã€è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æµ‹é‡æˆ–ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œè¿™äº›å¯èƒ½ä¼šå—åˆ°é‡‡æ ·åè§å’Œå•ä¸€æ¨¡å¼çº¦æŸçš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é¢„æµ‹å»ºæ¨¡æ¡†æ¶â€”â€”MOSCARDï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å› æœæ¨ç†ï¼ŒåŒæ—¶ä½¿ç”¨è”åˆæ³¨æ„åŠ›å°†ä¸¤ä¸ªä¸åŒæ¨¡æ€å¯¹é½ï¼Œå¹¶åœ¨æœºä¼šæ€§é£é™©ä¼°è®¡ä¸­åŒæ—¶å‡è½»åè§å’Œæ··æ‚å› ç´ ã€‚ä¸»è¦æŠ€æœ¯è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆiï¼‰ä»¥å¿ƒç”µå›¾ä¸ºæŒ‡å¯¼çš„CXRå¤šæ¨¡æ€å¯¹é½ï¼›ï¼ˆiiï¼‰é›†æˆå› æœæ¨ç†ï¼›ï¼ˆiiiï¼‰åŒé‡åå‘ä¼ æ’­å›¾ä»¥æ¶ˆé™¤æ··æ·†ã€‚åœ¨æ¥è‡ªæ€¥è¯Šéƒ¨ï¼ˆEDï¼‰çš„å†…éƒ¨è½¬ç§»æ•°æ®ä»¥åŠå¤–éƒ¨MIMICæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å•æ¨¡æ€å’Œæœ€æ–°åŸºç¡€æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²â€”â€”AUCåˆ†åˆ«ä¸º0.75ã€0.83å’Œ0.71ã€‚æå‡ºçš„æˆæœ¬æ•ˆç›Šé«˜çš„æœºä¼šæ€§ç­›æŸ¥å¯å®ç°æ—©æœŸå¹²é¢„ï¼Œæ”¹å–„æ‚£è€…ç»“æœå¹¶å‡å°‘å·®å¼‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19174v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å…¨çƒç–¾ç—…è´Ÿæ‹…ç ”ç©¶2021çš„ç»“æœï¼Œé‡å¤§å¿ƒè¡€ç®¡ä¸è‰¯äº‹ä»¶ï¼ˆMACEï¼‰ä»æ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ã€‚æ–‡ç« æå‡ºåˆ©ç”¨æœºä¼šæ€§ç­›æŸ¥ï¼ˆOpportunistic screeningï¼‰å’Œå¤šæ¨¡æ€æ•°æ®æ¥è¯†åˆ«é«˜å±ä¸ªä½“ï¼Œé€šè¿‡èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰å’Œå¿ƒç”µå›¾ï¼ˆECGï¼‰çš„é›†æˆæä¾›æ›´å…¨é¢çš„é£é™©è¯„ä¼°ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é¢„æµ‹å»ºæ¨¡æ¡†æ¶MOSCARDï¼Œé€šè¿‡å¤šæ¨¡æ€å› æœæ¨ç†ä¸ååŒæ³¨æ„åŠ›æœºåˆ¶å¯¹é½ä¸¤ç§ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼ŒåŒæ—¶å‡è½»æœºä¼šæ€§é£é™©è¯„ä¼°ä¸­çš„åè§å’Œæ··æ‚å› ç´ ã€‚è¯¥æ¨¡å‹åœ¨å†…éƒ¨ã€æ€¥è¯Šç§‘è½¬ç§»æ•°æ®ä»¥åŠå¤–éƒ¨MIMICæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•æ¨¡æ€å’Œç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒAUCåˆ†åˆ«ä¸º0.75ã€0.83å’Œ0.71ã€‚æå‡ºçš„ç»æµå®æƒ çš„æœºä¼šæ€§ç­›æŸ¥å¯å®ç°æ—©æœŸå¹²é¢„ï¼Œæ”¹å–„æ‚£è€…é¢„åå¹¶å‡å°‘å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å¤§å¿ƒè¡€ç®¡ä¸è‰¯äº‹ä»¶ï¼ˆMACEï¼‰ä»æ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ã€‚</li>
<li>æœºä¼šæ€§ç­›æŸ¥å’Œå¤šæ¨¡æ€æ•°æ®å¯ç”¨äºè¯†åˆ«å¿ƒè¡€ç®¡ç–¾ç—…çš„é«˜å±ä¸ªä½“ã€‚</li>
<li>èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰å’Œå¿ƒç”µå›¾ï¼ˆECGï¼‰çš„é›†æˆå¯ä»¥æä¾›æ›´å…¨é¢çš„é£é™©è¯„ä¼°ã€‚</li>
<li>æ–°çš„é¢„æµ‹å»ºæ¨¡æ¡†æ¶MOSCARDé€šè¿‡å¤šæ¨¡æ€å› æœæ¨ç†ä¸ååŒæ³¨æ„åŠ›æœºåˆ¶å¯¹é½æ•°æ®ã€‚</li>
<li>MOSCARDæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>æœºä¼šæ€§ç­›æŸ¥æœ‰åŠ©äºæ—©æœŸå¹²é¢„ï¼Œæ”¹å–„æ‚£è€…é¢„åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-906f1e8a6cb88bd90f02bf51f612c8de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86fc58e24e5810b6523af6504d1a57cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb935cf73dfc966faa5a7b4300f60afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab2e21c2f083ee7f078eeb290b93c4a9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Thought-Anchors-Which-LLM-Reasoning-Steps-Matter"><a href="#Thought-Anchors-Which-LLM-Reasoning-Steps-Matter" class="headerlink" title="Thought Anchors: Which LLM Reasoning Steps Matter?"></a>Thought Anchors: Which LLM Reasoning Steps Matter?</h2><p><strong>Authors:Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy</strong></p>
<p>Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentenceâ€™s counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified <code>broadcasting&#39;&#39; sentences that receive disproportionate attention from all future sentences via </code>receiverâ€™â€™ attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentenceâ€™s tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (<a target="_blank" rel="noopener" href="http://www.thought-anchors.com/">www.thought-anchors.com</a>) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models. </p>
<blockquote>
<p>æ¨ç†å¤§è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é•¿å½¢å¼æ€ç»´é“¾æ¨ç†å¸¦æ¥äº†å¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ¯ä¸ªç”Ÿæˆçš„ä»¤ç‰Œéƒ½ä¾èµ–äºæ‰€æœ‰å…ˆå‰çš„ä»¤ç‰Œï¼Œä½¿å¾—è®¡ç®—æ›´éš¾åˆ†è§£ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨å¥å­å±‚é¢åˆ†ææ¨ç†ç—•è¿¹æ˜¯ç†è§£æ¨ç†è¿‡ç¨‹çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§äº’è¡¥çš„å½’å› æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä¸€ç§é»‘ç›’æ–¹æ³•ï¼Œé€šè¿‡æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆè¯¥å¥å­æˆ–å…·æœ‰ä¸åŒå«ä¹‰çš„å¥å­çš„æœ€ç»ˆç­”æ¡ˆï¼Œæ¥è¡¡é‡æ¯ä¸ªå¥å­çš„åäº‹å®é‡è¦æ€§ï¼›ï¼ˆ2ï¼‰ä¸€ç§ç™½ç›’æ–¹æ³•ï¼Œèšé›†æˆå¯¹å¥å­ä¹‹é—´çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¯†åˆ«å‡ºé€šè¿‡â€œæ¥æ”¶æ–¹â€æ³¨æ„åŠ›å¤´æ¥æ”¶åˆ°æ¥è‡ªæ‰€æœ‰æœªæ¥å¥å­çš„ä¸æˆæ¯”ä¾‹çš„æ³¨æ„åŠ›çš„â€œå¹¿æ’­â€å¥å­ï¼›ï¼ˆ3ï¼‰ä¸€ç§å› æœå½’å› æ–¹æ³•ï¼Œé€šè¿‡æŠ‘åˆ¶å¯¹ä¸€å¥è¯çš„æ³¨æ„åŠ›å¹¶è¡¡é‡å¯¹æ¯ä¸€æœªæ¥å¥å­ä»¤ç‰Œçš„å½±å“æ¥æµ‹é‡å¥å­ä¹‹é—´çš„é€»è¾‘è”ç³»ã€‚æ¯ç§æ–¹æ³•éƒ½è¯æ˜äº†æ€ç»´é”šç‚¹çš„å­˜åœ¨ï¼Œè¿™äº›æ€ç»´é”šç‚¹æ˜¯å…·æœ‰é‡å¤§é‡è¦æ€§çš„æ¨ç†æ­¥éª¤ï¼Œå®ƒä»¬ä¼šä¸æˆæ¯”ä¾‹åœ°å½±å“éšåçš„æ¨ç†è¿‡ç¨‹ã€‚è¿™äº›æ€ç»´é”šç‚¹é€šå¸¸æ˜¯è®¡åˆ’æˆ–å›æº¯å¥å­ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºå·¥å…·ï¼ˆ<a target="_blank" rel="noopener" href="http://www.thought-anchors.com),ç”¨äºå¯è§†åŒ–æˆ‘ä»¬çš„æ–¹æ³•è¾“å‡º,å¹¶å±•ç¤ºäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶,å±•ç¤ºå„æ–¹æ³•ä¹‹é—´çš„è¶‹åŒæ¨¡å¼,æ˜ å°„æ¨¡å‹å¦‚ä½•è¿›è¡Œå¤šæ­¥éª¤æ¨ç†.å„æ–¹æ³•ä¹‹é—´çš„ä¸€è‡´æ€§è¡¨æ˜äº†å¥å­å±‚é¢åˆ†æåœ¨æ·±å…¥ç†è§£æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›./">www.thought-anchors.comï¼‰ï¼Œç”¨äºå¯è§†åŒ–æˆ‘ä»¬çš„æ–¹æ³•è¾“å‡ºï¼Œå¹¶å±•ç¤ºäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºå„æ–¹æ³•ä¹‹é—´çš„è¶‹åŒæ¨¡å¼ï¼Œæ˜ å°„æ¨¡å‹å¦‚ä½•è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ã€‚å„æ–¹æ³•ä¹‹é—´çš„ä¸€è‡´æ€§è¡¨æ˜äº†å¥å­å±‚é¢åˆ†æåœ¨æ·±å…¥ç†è§£æ¨ç†æ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19143v1">PDF</a> Paul C. Bogdan and Uzay Macar contributed equally to this work, and   their listed order was determined by coinflip. Neel Nanda and Arthur Conmy   contributed equally to this work as senior authors, and their listed order   was determined by coinflip</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å…¶é•¿å½¢å¼çš„æ€ç»´é“¾æ¨ç†å¸¦æ¥äº†å¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†åœ¨å¥å­å±‚é¢åˆ†ææ¨ç†è½¨è¿¹çš„æ–¹æ³•ï¼Œå¹¶ä»‹ç»äº†ä¸‰ç§äº’è¡¥çš„å½’å› æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬ï¼šæµ‹é‡æ¯ä¸ªå¥å­çš„åäº‹å®é‡è¦æ€§çš„é»‘ç®±æ–¹æ³•ã€èšåˆå¥å­é—´æ³¨æ„åŠ›æ¨¡å¼çš„ç™½ç®±æ–¹æ³•ï¼Œä»¥åŠæµ‹é‡å¥å­é—´é€»è¾‘è”ç³»çš„å› æœå½’å› æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•éƒ½è¯æ˜äº†æ€ç»´é”šç‚¹çš„å­˜åœ¨ï¼Œå³å…·æœ‰é‡å¤§å½±å“çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶ä¼šå¯¹éšåçš„æ¨ç†è¿‡ç¨‹äº§ç”Ÿä¸æˆæ¯”ä¾‹çš„å½±å“ã€‚è¿™äº›æ€ç»´é”šé€šå¸¸æ˜¯è§„åˆ’æˆ–å›æº¯çš„å¥å­ã€‚æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå¼€æºå·¥å…·ï¼Œç”¨äºå¯è§†åŒ–è¿™äº›æ–¹æ³•çš„ç»“æœï¼Œå¹¶å±•ç¤ºäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†å„ç§æ–¹æ³•ä¹‹é—´çš„æ”¶æ•›æ¨¡å¼ï¼Œä»¥åŠå®ƒä»¬åœ¨æ¨¡å‹è¿›è¡Œå¤šæ­¥éª¤æ¨ç†æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¥å­å±‚é¢çš„åˆ†æå¯¹äºæ›´æ·±å…¥åœ°ç†è§£æ¨ç†æ¨¡å‹å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†é¢ä¸´å¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¥å­å±‚é¢åˆ†ææ¨ç†è½¨è¿¹æ˜¯ä¸€ç§ç†è§£æ¨ç†è¿‡ç¨‹çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ä¸‰ç§äº’è¡¥çš„å½’å› æ–¹æ³•ç”¨äºåˆ†ææ¨ç†è¿‡ç¨‹ï¼šé»‘ç®±æ–¹æ³•ã€ç™½ç®±æ–¹æ³•å’Œå› æœå½’å› æ–¹æ³•ã€‚</li>
<li>è¿™äº›æ–¹æ³•è¯æ˜äº†æ€ç»´é”šç‚¹çš„å­˜åœ¨ï¼Œå³å¯¹åç»­æ¨ç†è¿‡ç¨‹äº§ç”Ÿé‡å¤§å½±å“çš„æ¨ç†æ­¥éª¤ã€‚</li>
<li>æ€ç»´é”šç‚¹é€šå¸¸æ˜¯è§„åˆ’æˆ–å›æº¯çš„å¥å­ã€‚</li>
<li>å¼€æºå·¥å…·å¯ç”¨äºå¯è§†åŒ–åˆ†ææ–¹æ³•çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e72fe364fa256f10759055ea892c2158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdea3335500968ce818f6fba1d902529.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a61ead1e975ced32a761c1c5d0a204c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c154753648bca28752c395685217b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b85e1c2bf3e627443ff6e9f2a827be.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Baba-is-LLM-Reasoning-in-a-Game-with-Dynamic-Rules"><a href="#Baba-is-LLM-Reasoning-in-a-Game-with-Dynamic-Rules" class="headerlink" title="Baba is LLM: Reasoning in a Game with Dynamic Rules"></a>Baba is LLM: Reasoning in a Game with Dynamic Rules</h2><p><strong>Authors:Fien van Wetten, Aske Plaat, Max van Duijn</strong></p>
<p>Large language models (LLMs) are known to perform well on language tasks, but struggle with reasoning tasks. This paper explores the ability of LLMs to play the 2D puzzle game Baba is You, in which players manipulate rules by rearranging text blocks that define object properties. Given that this rule-manipulation relies on language abilities and reasoning, it is a compelling challenge for LLMs. Six LLMs are evaluated using different prompt types, including (1) simple, (2) rule-extended and (3) action-extended prompts. In addition, two models (Mistral, OLMo) are finetuned using textual and structural data from the game. Results show that while larger models (particularly GPT-4o) perform better in reasoning and puzzle solving, smaller unadapted models struggle to recognize game mechanics or apply rule changes. Finetuning improves the ability to analyze the game levels, but does not significantly improve solution formulation. We conclude that even for state-of-the-art and finetuned LLMs, reasoning about dynamic rule changes is difficult (specifically, understanding the use-mention distinction). The results provide insights into the applicability of LLMs to complex problem-solving tasks and highlight the suitability of games with dynamically changing rules for testing reasoning and reflection by LLMs. </p>
<blockquote>
<p>ä¼—æ‰€å‘¨çŸ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ã€‚æœ¬æ–‡æ¢è®¨äº†LLMsç©äºŒç»´æ‹¼å›¾æ¸¸æˆã€ŠBaba is Youã€‹çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªæ¸¸æˆä¸­ï¼Œç©å®¶é€šè¿‡é‡æ–°æ’åˆ—å®šä¹‰å¯¹è±¡å±æ€§çš„æ–‡æœ¬å—æ¥æ“çºµè§„åˆ™ã€‚é‰´äºè¿™ç§è§„åˆ™æ“çºµä¾èµ–äºè¯­è¨€èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºLLMsæ¥è¯´æ˜¯ä¸€ä¸ªå¸å¼•äººçš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä½¿ç”¨å…­ç§ä¸åŒçš„æç¤ºç±»å‹å¯¹LLMsè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰ç®€å•æç¤ºã€ï¼ˆ2ï¼‰è§„åˆ™æ‰©å±•æç¤ºå’Œï¼ˆ3ï¼‰åŠ¨ä½œæ‰©å±•æç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼ˆMistralã€OLMoï¼‰ä½¿ç”¨æ¸¸æˆå†…çš„æ–‡æœ¬å’Œç»“æ„æ•°æ®è¿›è¡Œå¾®è°ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œè¾ƒå¤§çš„æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯GPT-4oï¼‰åœ¨æ¨ç†å’Œæ‹¼å›¾è§£å†³æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œè¾ƒå°çš„æœªç»è°ƒæ•´æ¨¡å‹åœ¨è¯†åˆ«æ¸¸æˆæœºåˆ¶å’Œåº”ç”¨è§„åˆ™å˜åŒ–æ–¹é¢é‡åˆ°å›°éš¾ã€‚å¾®è°ƒæé«˜äº†åˆ†ææ¸¸æˆå…³å¡çš„èƒ½åŠ›ï¼Œä½†å¹¶ä¸æ˜¾è‘—æ”¹è¿›è§£å†³æ–¹æ¡ˆçš„åˆ¶å®šã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå³ä½¿æ˜¯æœ€æ–°ã€ç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯¹äºåŠ¨æ€è§„åˆ™å˜åŒ–çš„æ¨ç†ä¹Ÿæ˜¯å›°éš¾çš„ï¼ˆç‰¹åˆ«æ˜¯ç†è§£å’Œä½¿ç”¨æåŠçš„åŒºåˆ«ï¼‰ã€‚è¿™äº›ç»“æœæä¾›äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„åº”ç”¨æ€§çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†åŠ¨æ€è§„åˆ™å˜åŒ–çš„æ¸¸æˆåœ¨æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œåæ€èƒ½åŠ›æ–¹é¢çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19095v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¸¸æˆè§„åˆ™æ“æ§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†åŠ¨æ€è§„åˆ™å˜åŒ–çš„æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å…­ä¸ªLLMsåœ¨ä¸åŒæç¤ºç±»å‹ä¸‹çš„è¡¨ç°ï¼Œå¹¶å‘ç°æ›´å¤§çš„æ¨¡å‹åœ¨æ¨ç†å’Œè§£å†³é—®é¢˜æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œæœªç»é€‚åº”çš„å°æ¨¡å‹éš¾ä»¥è¯†åˆ«æ¸¸æˆæœºåˆ¶å’Œè¿ç”¨è§„åˆ™å˜åŒ–ã€‚å¾®è°ƒè™½æœ‰åŠ©äºæé«˜åˆ†ææ¸¸æˆå…³å¡çš„èƒ½åŠ›ï¼Œä½†å¯¹è§£å†³æ–¹æ¡ˆåˆ¶å®šçš„å½±å“å¹¶ä¸æ˜¾è‘—ã€‚ç ”ç©¶è®¤ä¸ºï¼Œå³ä½¿æ˜¯æœ€æ–°å’Œæœ€å…ˆè¿›çš„LLMsåœ¨åº”å¯¹åŠ¨æ€è§„åˆ™å˜åŒ–æ—¶ä»é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£ä½¿ç”¨å’ŒæåŠçš„åŒºåˆ«æ–¹é¢ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†å¯¹LLMsåœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„åº”ç”¨æ€§çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†åŠ¨æ€è§„åˆ™æ¸¸æˆåœ¨æµ‹è¯•LLMsæ¨ç†å’Œåæ€èƒ½åŠ›æ–¹é¢çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¸¸æˆè§„åˆ™æ“æ§ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>LLMsåœ¨ç†è§£å’Œè¿ç”¨åŠ¨æ€è§„åˆ™å˜åŒ–çš„æ¨ç†ä»»åŠ¡ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>å¤§å‹æ¨¡å‹åœ¨æ¨ç†å’Œè§£å†³é—®é¢˜æ–¹é¢è¡¨ç°ä¼˜äºå°å‹æœªé€‚åº”æ¨¡å‹ã€‚</li>
<li>å¾®è°ƒèƒ½å¤Ÿæé«˜LLMsåˆ†ææ¸¸æˆå…³å¡çš„èƒ½åŠ›ï¼Œä½†å¯¹è§£å†³æ–¹æ¡ˆåˆ¶å®šçš„æ”¹å–„ä¸æ˜æ˜¾ã€‚</li>
<li>LLMsåœ¨åº”å¯¹åŠ¨æ€è§„åˆ™å˜åŒ–æ—¶é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£ä½¿ç”¨å’ŒæåŠçš„åŒºåˆ«æ–¹é¢ã€‚</li>
<li>ç ”ç©¶æä¾›äº†LLMsåœ¨å¤æ‚é—®é¢˜è§£å†³ä»»åŠ¡ä¸­çš„åº”ç”¨æ€§çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e509b6ed6b46ed5d2cc6eb6effb85c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ce6b95e5a208fe35c10ec78ed1199e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c78ec7a6fecfcc221bb836c1c737c46.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents"><a href="#From-Web-Search-towards-Agentic-Deep-Research-Incentivizing-Search-with-Reasoning-Agents" class="headerlink" title="From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents"></a>From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents</h2><p><strong>Authors:Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu</strong></p>
<p>Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a>. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢æ˜¯ç°ä»£çŸ¥è¯†è·å–çš„æ ¸å¿ƒåŸºçŸ³ï¼Œæ¯å¤©æ”¯æŒæ•°åäº¿æ¬¡çš„è·¨åŸŸæŸ¥è¯¢ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“è¶Šæ¥è¶Šéš¾ä»¥æ»¡è¶³å¤æ‚çš„ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œæ‹¥æœ‰æ¨ç†å’Œæ™ºèƒ½èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å¼€åˆ›ä¸€ç§åä¸ºæ™ºèƒ½æ·±åº¦ç ”ç©¶çš„æ–°èŒƒå¼ã€‚è¿™äº›ç³»ç»Ÿé€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆåˆ°ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯ä¸­ï¼Œä»è€Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¿¡æ¯æœç´¢æŠ€æœ¯ã€‚æˆ‘ä»¬è¿½æº¯äº†ä»é™æ€ç½‘é¡µæœç´¢åˆ°åŸºäºäº¤äº’ã€æ™ºèƒ½ç³»ç»Ÿçš„æ¼”å˜ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥è®¡åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæµ‹è¯•æ—¶çš„è§„æ¨¡å®šå¾‹ï¼Œä»¥æ­£å¼ç¡®å®šè®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ã€‚åœ¨åŸºå‡†æµ‹è¯•ç»“æœçš„æ”¯æŒä¸‹ï¼Œä»¥åŠå¼€æºå®ç°çš„å…´èµ·ï¼Œæˆ‘ä»¬è¯æ˜äº†æ™ºèƒ½æ·±åº¦ç ”ç©¶ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”å·²æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚æ‰€æœ‰ç›¸å…³èµ„æºï¼ŒåŒ…æ‹¬å·¥ä¸šäº§å“ã€ç ”ç©¶è®ºæ–‡ã€åŸºå‡†æ•°æ®é›†å’Œå¼€æºå®ç°ï¼Œéƒ½æ”¶é›†åœ¨äº† <a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research">https://github.com/DavidZWZ/Awesome-Deep-Research</a> ç¤¾åŒºä¸­ä¸ºå…¬ä¼—å¼€æ”¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸï¼Œä¼ ç»ŸåŸºäºå…³é”®è¯çš„æœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ã€å¤šæ­¥éª¤çš„ä¿¡æ¯éœ€æ±‚æ—¶æ—¥ç›Šæ˜¾å¾—ä¸è¶³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œä¸ºä¿¡æ¯æ£€ç´¢é¢†åŸŸå¸¦æ¥äº†å…¨æ–°çš„å˜é©ã€‚é€šè¿‡ç´§å¯†é›†æˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼ŒLLMå½¢æˆäº†ä¸€ä¸ªåŠ¨æ€åé¦ˆå¾ªç¯æœºåˆ¶ï¼Œå¼€å¯äº†åä¸ºAgentic Deep Researchçš„æ–°èŒƒå¼ã€‚Agentic Deep Researchä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”æœ‰æœ›æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚ç›¸å…³èµ„æºå·²æ±‡æ€»äº<a target="_blank" rel="noopener" href="https://github.com/DavidZWZ/Awesome-Deep-Research%E3%80%82">https://github.com/DavidZWZ/Awesome-Deep-Researchã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢æ­£ç»å†å˜é©ï¼Œä¼ ç»Ÿæœç´¢å¼•æ“åœ¨å¤„ç†å¤æ‚ä¿¡æ¯éœ€æ±‚æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ¨ç†å’Œä»£ç†èƒ½åŠ›ï¼Œä¸ºä¿¡æ¯æ£€ç´¢é¢†åŸŸå¸¦æ¥å…¨æ–°èŒƒå¼ã€‚</li>
<li>Agentic Deep Researché€šè¿‡æ•´åˆè‡ªä¸»æ¨ç†ã€è¿­ä»£æ£€ç´¢å’Œä¿¡æ¯åˆæˆï¼Œå½¢æˆåŠ¨æ€åé¦ˆå¾ªç¯æœºåˆ¶ã€‚</li>
<li>Agentic Deep Researchæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æœ‰æœ›æˆä¸ºæœªæ¥ä¿¡æ¯æœç´¢çš„ä¸»å¯¼èŒƒå¼ã€‚</li>
<li>Agentic Deep Researchç³»ç»Ÿå…·å¤‡è®¡åˆ’ã€æ¢ç´¢å’Œå­¦ä¹ èƒ½åŠ›ï¼Œå®ç°ä»é™æ€ç½‘é¡µæœç´¢åˆ°äº¤äº’å¼ã€åŸºäºä»£ç†çš„ç³»ç»Ÿçš„è¿›åŒ–ã€‚</li>
<li>æµ‹è¯•æ—¶çš„å°ºåº¦å¾‹å½±å“è®¡ç®—æ·±åº¦å¯¹æ¨ç†å’Œæœç´¢çš„å½±å“ï¼Œè¿™åœ¨ç ”ç©¶ä¸­å¾—åˆ°äº†éªŒè¯å’Œæ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-070f0fb313cdc1362efe4b9cf6eee5b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76a584b347a5f8a48347111db296ccc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-155280869ad2bd126b54c3da5ebc7cd6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ConciseHint-Boosting-Efficient-Reasoning-via-Continuous-Concise-Hints-during-Generation"><a href="#ConciseHint-Boosting-Efficient-Reasoning-via-Continuous-Concise-Hints-during-Generation" class="headerlink" title="ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints   during Generation"></a>ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints   during Generation</h2><p><strong>Authors:Siao Tang, Xinyin Ma, Gongfan Fang, Xinchao Wang</strong></p>
<p>Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and OpenAI o1 series have achieved notable performance enhancements on complex reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT). However, an emerging issue is their inclination to produce excessively verbose reasoning processes, leading to the inefficiency problem. Existing literature on improving efficiency mainly adheres to the before-reasoning paradigms such as prompting and reasoning or fine-tuning and reasoning, but ignores the promising direction of directly encouraging the model to speak concisely by intervening during the generation of reasoning. In order to fill the blank, we propose a framework dubbed ConciseHint, which continuously encourages the reasoning model to speak concisely by injecting the textual hint (manually designed or trained on the concise data) during the token generation of the reasoning process. Besides, ConciseHint is adaptive to the complexity of the query by adaptively adjusting the hint intensity, which ensures it will not undermine model performance. Experiments on the state-of-the-art LRMs, including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can effectively produce concise reasoning processes while maintaining performance well. For instance, we achieve a reduction ratio of 65% for the reasoning length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¦‚DeepSeek-R1å’ŒOpenAI o1ç³»åˆ—ç­‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ‰©å±•äº†ç”Ÿæˆé•¿åº¦ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ–°å‡ºç°çš„é—®é¢˜æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿè¿‡äºå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´äº†æ•ˆç‡é—®é¢˜ã€‚ç°æœ‰å…³äºæé«˜æ•ˆç‡çš„æ–‡çŒ®ä¸»è¦éµå¾ªé¢„å…ˆæ¨ç†çš„æ¨¡å¼ï¼Œå¦‚æç¤ºå’Œæ¨ç†æˆ–å¾®è°ƒåæ¨ç†ï¼Œä½†å¿½è§†äº†é€šè¿‡å¹²é¢„æ¨ç†ç”Ÿæˆè¿‡ç¨‹æ¥ç›´æ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºConciseHintçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ³¨å…¥æ–‡æœ¬æç¤ºï¼ˆäººä¸ºè®¾è®¡æˆ–åœ¨ç®€æ´æ•°æ®ä¸Šè®­ç»ƒï¼‰æ¥æŒç»­é¼“åŠ±æ¨ç†æ¨¡å‹ç®€æ´è¡¨è¾¾ã€‚æ­¤å¤–ï¼ŒConciseHintèƒ½å¤Ÿé€‚åº”æŸ¥è¯¢çš„å¤æ‚æ€§ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æç¤ºå¼ºåº¦ï¼Œç¡®ä¿ä¸ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚åœ¨åŒ…æ‹¬DeepSeek-R1å’ŒQwen-3ç³»åˆ—åœ¨å†…çš„æœ€æ–°LRMsä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆåœ°äº§ç”Ÿç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œåœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨Qwen-3 4Bæ—¶ï¼Œæˆ‘ä»¬å®ç°äº†æ¨ç†é•¿åº¦å‡å°‘65%ï¼ŒåŒæ—¶å‡ ä¹æ²¡æœ‰æŸå¤±å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18810v2">PDF</a> Codes are available at <a target="_blank" rel="noopener" href="https://github.com/tsa18/ConciseHint">https://github.com/tsa18/ConciseHint</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1å’ŒOpenAI o1ç³»åˆ—ï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œé€šè¿‡é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æ‰©å±•äº†ç”Ÿæˆé•¿åº¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å€¾å‘ç”Ÿæˆè¿‡äºå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ•ˆç‡é—®é¢˜ã€‚ä¸ºæé«˜æ•ˆç‡ï¼Œæå‡ºä¸€ä¸ªåä¸ºConciseHintçš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹çš„æ ‡è®°ç”Ÿæˆé˜¶æ®µæ³¨å…¥ç®€æ´çš„æ–‡æœ¬æç¤ºæ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾ã€‚ConciseHintå¯æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§è‡ªé€‚åº”è°ƒæ•´æç¤ºå¼ºåº¦ï¼Œç¡®ä¿ä¸å½±å“æ¨¡å‹æ€§èƒ½ã€‚åœ¨åŒ…æ‹¬DeepSeek-R1å’ŒQwen-3ç³»åˆ—ç­‰å…ˆè¿›LRMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç”Ÿæˆç®€æ´çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI o1ç³»åˆ—åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œä¸»è¦é€šè¿‡æ‰©å±•ç”Ÿæˆé•¿åº¦å®ç°ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å­˜åœ¨è¿‡åº¦å†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ•ˆç‡é—®é¢˜ã€‚</li>
<li>ä¸ºæé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡ï¼Œæå‡ºConciseHintæ¡†æ¶ï¼Œé€šè¿‡æ³¨å…¥ç®€æ´æ–‡æœ¬æç¤ºæ¥é¼“åŠ±æ¨¡å‹ç®€æ´è¡¨è¾¾ã€‚</li>
<li>ConciseHintå¯è‡ªé€‚åº”æŸ¥è¯¢å¤æ‚æ€§ï¼Œè°ƒæ•´æç¤ºå¼ºåº¦ï¼Œç¡®ä¿ä¸å½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒConciseHintåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆç”Ÿæˆæ›´ç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åœ¨GSM8KåŸºå‡†æµ‹è¯•ä¸Šï¼Œä½¿ç”¨Qwen-3 4Bæ¨¡å‹æ—¶ï¼Œå®ç°äº†æ¨ç†é•¿åº¦å‡å°‘65%è€Œå‡ ä¹æ— å‡†ç¡®ç‡æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f08db57690e5b1a09e27722e0ee6604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dab27b579484f26a10f29913787ad90c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58ec51723ee427687a67bafe438fb028.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a6cfc346d16bad7821677f07d029345.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization"><a href="#ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization" class="headerlink" title="ReDit: Reward Dithering for Improved LLM Policy Optimization"></a>ReDit: Reward Dithering for Improved LLM Policy Optimization</h2><p><strong>Authors:Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu</strong></p>
<p>DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While itâ€™s a â€˜â€™perfectâ€™â€™ reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages. </p>
<blockquote>
<p>DeepSeek-R1å·²æˆåŠŸé€šè¿‡å…¶åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è™½ç„¶å®ƒæ˜¯ä¸€ä¸ªâ€œå®Œç¾â€çš„å¥–åŠ±ç³»ç»Ÿï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°éåˆ¶å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œä½†è¿™æ ·çš„å¥–åŠ±åŠŸèƒ½æ˜¯ç¦»æ•£çš„ã€‚æˆ‘ä»¬çš„å®éªŒè§‚å¯Ÿè¡¨æ˜ï¼Œç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ·»åŠ ç®€å•çš„éšæœºå™ªå£°æ¥æ‰°ä¹±ç¦»æ•£å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡è¿™ç§å—æ‰°çš„å¥–åŠ±ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¼šæŒç»­æä¾›æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä»è€Œå®ç°æ›´å¹³æ»‘çš„æ¢¯åº¦æ›´æ–°å¹¶åŠ é€Ÿæ”¶æ•›ã€‚æ³¨å…¥çš„å™ªå£°è¿˜ä¸ºå¹³å¦å¥–åŠ±åŒºåŸŸå¼•å…¥äº†éšæœºæ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥å¹¶é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å®éªŒå±•ç¤ºäº†ReDitçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚å¹³å‡è€Œè¨€ï¼ŒReDitåœ¨ä»…ä½¿ç”¨å¤§çº¦10%çš„è®­ç»ƒæ­¥éª¤çš„æƒ…å†µä¸‹å°±è¾¾åˆ°äº†ä¸å¸¸è§„GRPOç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ç»è¿‡ç±»ä¼¼æ—¶é—´çš„è®­ç»ƒåï¼Œå…¶æ€§èƒ½è¿˜æé«˜äº†4%ã€‚å¯è§†åŒ–ç»“æœè¯å®äº†ReDitæ˜¾è‘—ç¼“è§£äº†æ¢¯åº¦é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ç†è®ºåˆ†æä»¥è¿›ä¸€æ­¥éªŒè¯è¿™äº›ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18631v2">PDF</a> 10 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç¦»æ•£å¥–åŠ±å¯èƒ½å¯¼è‡´æ¢¯åº¦å¼‚å¸¸ã€ä¼˜åŒ–ä¸ç¨³å®šå’Œæ”¶æ•›ç¼“æ…¢ã€‚ä¸ºæ­¤ï¼Œæå‡ºReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ç®€å•éšæœºå™ªå£°ä½¿ç¦»æ•£å¥–åŠ±ä¿¡å·æŠ–åŠ¨ã€‚æ‰°åŠ¨å¥–åŠ±æä¾›æŒç»­çš„æ¢ç´¢æ€§æ¢¯åº¦ï¼Œä½¿æ¢¯åº¦æ›´æ–°æ›´å¹³æ»‘ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚æ³¨å…¥çš„å™ªå£°è¿˜å¼•å…¥äº†å¹³å¦å¥–åŠ±åŒºåŸŸçš„éšæœºæ€§ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥å¹¶é€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚å®éªŒè¯æ˜ReDitåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œå¹³å‡è€Œè¨€ï¼ŒReDitåœ¨ç±»ä¼¼è®­ç»ƒæ—¶é•¿ä¸‹å®ç°äº†ä¸GRPOç›¸å½“çš„æ€§èƒ½ï¼Œä¸”åœ¨ç›¸åŒè®­ç»ƒæ—¶é•¿ä¸‹æ€§èƒ½æå‡4%ã€‚å¯è§†åŒ–ç»“æœæ˜¾è‘—ç¼“è§£äº†æ¢¯åº¦é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1é€šè¿‡è§„åˆ™å¥–åŠ±ç³»ç»Ÿå¢å¼ºäº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨ç¦»æ•£å¥–åŠ±å¯¼è‡´çš„é—®é¢˜ã€‚</li>
<li>ReDitï¼ˆå¥–åŠ±æŠ–åŠ¨ï¼‰æ–¹æ³•é€šè¿‡æ·»åŠ éšæœºå™ªå£°è§£å†³ç¦»æ•£å¥–åŠ±å¯¼è‡´çš„æ¢¯åº¦é—®é¢˜ã€‚</li>
<li>æ‰°åŠ¨å¥–åŠ±æä¾›æŒç»­æ¢ç´¢æ€§æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›å¹¶æ”¹å–„ä¼˜åŒ–ç¨³å®šæ€§ã€‚</li>
<li>æ³¨å…¥çš„å™ªå£°é¼“åŠ±æ¨¡å‹æ¢ç´¢æ–°ç­–ç•¥ï¼Œé€ƒç¦»å±€éƒ¨æœ€ä¼˜ã€‚</li>
<li>å®éªŒè¯æ˜ReDitåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œä¸GRPOç›¸æ¯”ï¼Œè®­ç»ƒæ­¥éª¤å‡å°‘çº¦10%ï¼Œæ€§èƒ½æå‡4%ã€‚</li>
<li>å¯è§†åŒ–ç»“æœæ˜¾è‘—ç¼“è§£æ¢¯åº¦é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-712221e60e204f9f465a0d805e6b9a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e010b13fac71cb176d41a679b3f75af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13b91fda44396c38c245cdcb90f834dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b24e0c343428a0e18c38e2da86f95b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e5fa2168295813a8ea25075968eb7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-259e0feedb5b0c9439c1e5ca8fde4ce5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="JarvisArt-Liberating-Human-Artistic-Creativity-via-an-Intelligent-Photo-Retouching-Agent"><a href="#JarvisArt-Liberating-Human-Artistic-Creativity-via-an-Intelligent-Photo-Retouching-Agent" class="headerlink" title="JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo   Retouching Agent"></a>JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo   Retouching Agent</h2><p><strong>Authors:Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, Chenxin Li, Haoyu Chen, Zhongdao Wang, Xinghao Ding, Wenbo Li, Shuicheng Yan</strong></p>
<p>Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: <a target="_blank" rel="noopener" href="https://jarvisart.vercel.app/">https://jarvisart.vercel.app/</a>. </p>
<blockquote>
<p>ç…§ç‰‡ä¿®é¥°å·²æˆä¸ºå½“ä»£è§†è§‰å™äº‹ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿæ•æ‰ç¾å­¦å¹¶è¡¨è¾¾åˆ›é€ åŠ›ã€‚è™½ç„¶Adobe Lightroomç­‰ä¸“ä¸šå·¥å…·æä¾›äº†å¼ºå¤§çš„åŠŸèƒ½ï¼Œä½†å®ƒä»¬éœ€è¦ç›¸å½“çš„ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨æ“ä½œåŠªåŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç°æœ‰çš„åŸºäºAIçš„è§£å†³æ–¹æ¡ˆæä¾›äº†è‡ªåŠ¨åŒ–ï¼Œä½†é€šå¸¸è°ƒæ•´çµæ´»æ€§æœ‰é™å’Œé€šç”¨æ€§ä¸è¶³ï¼Œæ— æ³•æ»¡è¶³å¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–çš„ç¼–è¾‘éœ€æ±‚ã€‚ä¸ºäº†å¼¥å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†JarvisArtï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ä»£ç†ï¼Œå®ƒç†è§£ç”¨æˆ·æ„å›¾ï¼Œæ¨¡ä»¿ä¸“ä¸šè‰ºæœ¯å®¶çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶æ™ºèƒ½åœ°åè°ƒLightroomä¸­çš„è¶…è¿‡200ç§ä¿®é¥°å·¥å…·ã€‚JarvisArtç»å†äº†ä¸¤ä¸ªé˜¶æ®µæ€§çš„è®­ç»ƒè¿‡ç¨‹ï¼šæœ€åˆçš„Chain-of-Thoughtç›‘ç£å¾®è°ƒæ¥å»ºç«‹åŸºæœ¬çš„æ¨ç†å’Œå·¥å…·ä½¿ç”¨æŠ€èƒ½ï¼Œå…¶æ¬¡æ˜¯ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO-Rï¼‰ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶å†³ç­–èƒ½åŠ›å’Œå·¥å…·ç†Ÿç»ƒç¨‹åº¦ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Agent-to-Lightroomåè®®ï¼Œä»¥ä¿ƒè¿›ä¸Lightroomæ— ç¼é›†æˆã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†MMArt-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±çœŸå®ç”¨æˆ·ç¼–è¾‘æ„æˆçš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚JarvisArtå±•ç¤ºäº†ç”¨æˆ·å‹å¥½çš„äº¤äº’ã€å‡ºè‰²çš„é€šç”¨æ€§ä»¥åŠå…¨å±€å’Œå±€éƒ¨è°ƒæ•´çš„ç²¾ç»†æ§åˆ¶ï¼Œä¸ºæ™ºèƒ½ç…§ç‰‡ä¿®é¥°å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸GPT-4ç›¸æ¯”ï¼Œå®ƒåœ¨MMArt-Benchä¸Šçš„å†…å®¹ä¿çœŸåº¦å¹³å‡åƒç´ çº§æŒ‡æ ‡æé«˜äº†60%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“é«˜çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jarvisart.vercel.app/%E3%80%82">https://jarvisart.vercel.app/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17612v1">PDF</a> 40 pages, 26 figures</p>
<p><strong>Summary</strong>ï¼šå½“ä»£ç¤¾ä¼šï¼Œç…§ç‰‡ä¿®é¥°å·²æˆä¸ºè§†è§‰å™è¿°ä¸å¯æˆ–ç¼ºçš„ä¸€ç¯ï¼Œèƒ½å¤Ÿæ•æ‰ç¾å­¦å¹¶å±•ç°åˆ›é€ åŠ›ã€‚å°½ç®¡ä¸“ä¸šå·¥å…·å¦‚Adobe LightroomåŠŸèƒ½å¼ºå¤§ï¼Œä½†éœ€è¦ä¸°å¯Œçš„ä¸“ä¸šçŸ¥è¯†å’Œå¤§é‡æ‰‹åŠ¨æ“ä½œã€‚ç°æœ‰çš„AIè§£å†³æ–¹æ¡ˆè™½ç„¶æä¾›äº†è‡ªåŠ¨åŒ–åŠŸèƒ½ï¼Œä½†å¾€å¾€è°ƒæ•´æ€§æœ‰é™ï¼Œé€šç”¨æ€§å·®ï¼Œæ— æ³•æ»¡è¶³å¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–çš„ç¼–è¾‘éœ€æ±‚ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œç ”ç©¶è€…æ¨å‡ºäº†JarvisArtï¼Œè¿™æ˜¯ä¸€æ¬¾ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ™ºèƒ½ä»£ç†ï¼Œå¯ä»¥ç†è§£ç”¨æˆ·æ„å›¾ï¼Œæ¨¡ä»¿ä¸“ä¸šè‰ºæœ¯å®¶çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶åœ¨Lightroomä¸­æ™ºèƒ½åè°ƒè¶…è¿‡200ç§ä¿®é¥°å·¥å…·ã€‚ç»è¿‡ä¸¤é˜¶æ®µè®­ç»ƒâ€”â€”Chain-of-Thoughtçš„ç²¾ç»†è°ƒæ•´ä»¥åŠGroup Relative Policy Optimization for Retouching (GRPO-R)å¢å¼ºå†³ç­–èƒ½åŠ›ï¼Œä»¥åŠä¸“ç”¨çš„Agent-to-Lightroomåè®®å®ç°æ— ç¼é›†æˆã€‚é€šè¿‡MMArt-BenchåŸºå‡†æµ‹è¯•å‘ç°ï¼ŒJarvisArtåœ¨ç”¨æˆ·å‹å¥½äº’åŠ¨ã€å“è¶Šæ³›åŒ–èƒ½åŠ›ä»¥åŠå¯¹å…¨å±€å’Œå±€éƒ¨è°ƒæ•´çš„ç²¾ç»†æ§åˆ¶æ–¹é¢éƒ½è¡¨ç°ä¼˜ç§€ã€‚å°¤å…¶ç›¸å¯¹äºGPT-4ï¼Œå…¶åœ¨å†…å®¹å¿ å®åº¦çš„å¹³å‡åƒç´ çº§æŒ‡æ ‡ä¸Šæé«˜äº†60%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jarvisart.vercel.app/">https://jarvisart.vercel.app/</a>ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>JarvisArtæ˜¯ä¸€æ¬¾åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä»£ç†ï¼Œç”¨äºç…§ç‰‡ä¿®é¥°ã€‚</li>
<li>JarvisArtèƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾å¹¶æ¨¡ä»¿ä¸“ä¸šè‰ºæœ¯å®¶çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>JarvisArtå¯ä»¥åœ¨Lightroomä¸­æ™ºèƒ½åè°ƒå¤šç§ä¿®é¥°å·¥å…·ã€‚</li>
<li>JarvisArtç»å†äº†ä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬Chain-of-Thoughtçš„ç²¾ç»†è°ƒæ•´å’ŒGRPO-Rå¢å¼ºå†³ç­–èƒ½åŠ›ã€‚</li>
<li>JarvisArtä¸Lightroomæ— ç¼é›†æˆï¼Œé€šè¿‡Agent-to-Lightroomåè®®å®ç°äº¤äº’ã€‚</li>
<li>JarvisArtåœ¨MMArt-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç”¨æˆ·å‹å¥½äº’åŠ¨ã€å“è¶Šæ³›åŒ–èƒ½åŠ›ä»¥åŠå¯¹å…¨å±€å’Œå±€éƒ¨è°ƒæ•´çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d2d9e42e132f2d83a4593faa239e3179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687b8b2eec36054d552c8a02fe6912cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b95833260771fc152fb2544a2c1e6eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47bee3cc3e0408544bf25f05fbdf143c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fe150bde0113af265eac13e528652ce5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  MAM Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via   Role-Specialized Collaboration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-25/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9e82b950fb50ed12a69cd9fb05928ec5.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-25  PlanMoGPT Flow-Enhanced Progressive Planning for Text to Motion   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
