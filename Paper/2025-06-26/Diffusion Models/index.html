<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  GenHSI Controllable Generation of Human-Scene Interaction Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d157e7d8274a0f9b1701c6e9aa3067f4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-26-æ›´æ–°"><a href="#2025-06-26-æ›´æ–°" class="headerlink" title="2025-06-26 æ›´æ–°"></a>2025-06-26 æ›´æ–°</h1><h2 id="GenHSI-Controllable-Generation-of-Human-Scene-Interaction-Videos"><a href="#GenHSI-Controllable-Generation-of-Human-Scene-Interaction-Videos" class="headerlink" title="GenHSI: Controllable Generation of Human-Scene Interaction Videos"></a>GenHSI: Controllable Generation of Human-Scene Interaction Videos</h2><p><strong>Authors:Zekun Li, Rui Zhou, Rahul Sajnani, Xiaoyan Cong, Daniel Ritchie, Srinath Sridhar</strong></p>
<p>Large-scale pre-trained video diffusion models have exhibited remarkable capabilities in diverse video generation. However, existing solutions face several challenges in using these models to generate long movie-like videos with rich human-object interactions that include unrealistic human-scene interaction, lack of subject identity preservation, and require expensive training. We propose GenHSI, a training-free method for controllable generation of long human-scene interaction videos (HSI). Taking inspiration from movie animation, our key insight is to overcome the limitations of previous work by subdividing the long video generation task into three stages: (1) script writing, (2) pre-visualization, and (3) animation. Given an image of a scene, a user description, and multiple images of a person, we use these three stages to generate long-videos that preserve human-identity and provide rich human-scene interactions. Script writing converts complex human tasks into simple atomic tasks that are used in the pre-visualization stage to generate 3D keyframes (storyboards). These 3D keyframes are rendered and animated by off-the-shelf video diffusion models for consistent long video generation with rich contacts in a 3D-aware manner. A key advantage of our work is that we alleviate the need for scanned, accurate scenes and create 3D keyframes from single-view images. We are the first to generate a long video sequence with a consistent camera pose that contains arbitrary numbers of character actions without training. Experiments demonstrate that our method can generate long videos that effectively preserve scene content and character identity with plausible human-scene interaction from a single image scene. Visit our project homepage <a target="_blank" rel="noopener" href="https://kunkun0w0.github.io/project/GenHSI/">https://kunkun0w0.github.io/project/GenHSI/</a> for more information. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨å¤šç§è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨ä½¿ç”¨è¿™äº›æ¨¡å‹ç”ŸæˆåŒ…å«ä¸°å¯Œäººæœºäº’åŠ¨çš„é•¿ç”µå½±å¼è§†é¢‘æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸çœŸå®çš„äººæ™¯äº’åŠ¨ã€ç¼ºä¹ä¸»é¢˜èº«ä»½ä¿ç•™ä»¥åŠè®­ç»ƒæˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬æå‡ºäº†GenHSIï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯æ§åˆ¶é•¿äººæœºäº’åŠ¨è§†é¢‘ï¼ˆHSIï¼‰ç”Ÿæˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬ä»ç”µå½±åŠ¨ç”»ä¸­æ±²å–çµæ„Ÿï¼Œé€šè¿‡å°†é•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ç»†åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µæ¥å…‹æœä»¥å‰å·¥ä½œçš„å±€é™æ€§ï¼šï¼ˆ1ï¼‰å‰§æœ¬ç¼–å†™ã€ï¼ˆ2ï¼‰é¢„å¯è§†åŒ–ã€ï¼ˆ3ï¼‰åŠ¨ç”»åˆ¶ä½œã€‚ç»™å®šåœºæ™¯å›¾åƒã€ç”¨æˆ·æè¿°ä»¥åŠå¤šä¸ªäººç‰©å›¾åƒï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸‰ä¸ªé˜¶æ®µæ¥ç”Ÿæˆä¿ç•™äººç‰©èº«ä»½å¹¶æä¾›ä¸°å¯Œäººæœºäº’åŠ¨çš„é•¿è§†é¢‘ã€‚å‰§æœ¬ç¼–å†™å°†å¤æ‚çš„äººç‰©ä»»åŠ¡è½¬æ¢ä¸ºç®€å•çš„åŸå­ä»»åŠ¡ï¼Œç”¨äºåœ¨é¢„å¯è§†åŒ–é˜¶æ®µç”Ÿæˆ3Då…³é”®å¸§ï¼ˆæ•…äº‹æ¿ï¼‰ã€‚è¿™äº›3Då…³é”®å¸§ç”±ç°æˆçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¸²æŸ“å’ŒåŠ¨ç”»å¤„ç†ï¼Œä»¥åœ¨3Dæ„ŸçŸ¥æ–¹å¼ä¸‹ç”Ÿæˆè¿è´¯çš„é•¿è§†é¢‘ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„æ¥è§¦ã€‚æˆ‘ä»¬å·¥ä½œçš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯ï¼Œæˆ‘ä»¬å‡è½»äº†å¯¹æ‰«æç²¾ç¡®åœºæ™¯çš„éœ€æ±‚ï¼Œå¹¶èƒ½ä»å•è§†å›¾å›¾åƒåˆ›å»º3Då…³é”®å¸§ã€‚æˆ‘ä»¬æ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸€è‡´ç›¸æœºå§¿æ€çš„é•¿è§†é¢‘åºåˆ—çš„å›¢é˜Ÿï¼Œè¯¥åºåˆ—åŒ…å«ä»»æ„æ•°é‡çš„äººç‰©åŠ¨ä½œè€Œæ— éœ€è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä»å•ä¸ªå›¾åƒåœºæ™¯ä¸­ç”Ÿæˆæœ‰æ•ˆä¿ç•™åœºæ™¯å†…å®¹å’Œäººç‰©èº«ä»½ã€å…·æœ‰åˆç†äººæœºäº’åŠ¨çš„é•¿è§†é¢‘ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®ä¸»é¡µ<a target="_blank" rel="noopener" href="https://kunkun0w0.github.io/project/GenHSI/%E3%80%82">https://kunkun0w0.github.io/project/GenHSI/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19840v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤§å‹é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹GenHSIï¼Œç”¨äºå¯æ§åœ°ç”Ÿæˆé•¿è§†é¢‘çš„äººæœºäº¤äº’ï¼ˆHSIï¼‰ã€‚é€šè¿‡å°†é•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡åˆ†ä¸ºå‰§æœ¬åˆ›ä½œã€é¢„å¯è§†åŒ–å’ŒåŠ¨ç”»åˆ¶ä½œä¸‰ä¸ªé˜¶æ®µï¼ŒGenHSIèƒ½å¤Ÿä»åœºæ™¯å›¾åƒã€ç”¨æˆ·æè¿°å’Œäººç‰©å¤šå¼ å›¾åƒä¸­ç”Ÿæˆä¿æŒäººç‰©èº«ä»½ã€æä¾›ä¸°å¯Œäººæœºäº¤äº’çš„é•¿è§†é¢‘ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆä¿ç•™åœºæ™¯å†…å®¹å’Œäººç‰©èº«ä»½ã€å…·æœ‰åˆç†äººæœºäº¤äº’çš„é•¿è§†é¢‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GenHSIæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤§å‹é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé•¿è§†é¢‘çš„äººæœºäº¤äº’ã€‚</li>
<li>GenHSIé€šè¿‡å°†é•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡åˆ†ä¸ºå‰§æœ¬åˆ›ä½œã€é¢„å¯è§†åŒ–å’ŒåŠ¨ç”»åˆ¶ä½œä¸‰ä¸ªé˜¶æ®µï¼Œå…‹æœç°æœ‰è§£å†³æ–¹æ¡ˆçš„æŒ‘æˆ˜ã€‚</li>
<li>GenHSIèƒ½å¤Ÿä»åœºæ™¯å›¾åƒã€ç”¨æˆ·æè¿°å’Œäººç‰©å¤šå¼ å›¾åƒä¸­ç”Ÿæˆä¿æŒäººç‰©èº«ä»½ã€æä¾›ä¸°å¯Œäººæœºäº¤äº’çš„é•¿è§†é¢‘ã€‚</li>
<li>GenHSIé€šè¿‡åˆ©ç”¨ç°æˆçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥3Dæ„ŸçŸ¥çš„æ–¹å¼ç”Ÿæˆä¸€è‡´çš„é•¿è§†é¢‘ï¼Œå…·æœ‰ä¸°å¯Œçš„å†…å®¹æ¥è§¦ã€‚</li>
<li>GenHSIä¸éœ€è¦ç²¾ç¡®çš„æ‰«æåœºæ™¯ï¼Œèƒ½å¤Ÿä»å•è§†å›¾å›¾åƒåˆ›å»º3Då…³é”®å¸§ã€‚</li>
<li>GenHSIèƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸€è‡´æ‘„åƒæœºå§¿æ€çš„é•¿è§†é¢‘åºåˆ—ï¼ŒåŒ…å«ä»»æ„æ•°é‡çš„äººç‰©åŠ¨ä½œï¼Œæ— éœ€è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e847fda84f5deeef52b400bf3fcd1e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34f9f59a1c26418c3d7e4a69cf39c098.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51be25ac24c7f69a6265e908b6e5ee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d82414426ad357b3524aaacf14b0863d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Progressive-Generation-with-Decomposable-Flow-Matching"><a href="#Improving-Progressive-Generation-with-Decomposable-Flow-Matching" class="headerlink" title="Improving Progressive Generation with Decomposable Flow Matching"></a>Improving Progressive Generation with Decomposable Flow Matching</h2><p><strong>Authors:Moayed Haji-Ali, Willi Menapace, Ivan Skorokhodov, Arpit Sahni, Sergey Tulyakov, Vicente Ordonez, Aliaksandr Siarohin</strong></p>
<p>Generating high-dimensional visual modalities is a computationally intensive task. A common solution is progressive generation, where the outputs are synthesized in a coarse-to-fine spectral autoregressive manner. While diffusion models benefit from the coarse-to-fine nature of denoising, explicit multi-stage architectures are rarely adopted. These architectures have increased the complexity of the overall approach, introducing the need for a custom diffusion formulation, decomposition-dependent stage transitions, add-hoc samplers, or a model cascade. Our contribution, Decomposable Flow Matching (DFM), is a simple and effective framework for the progressive generation of visual media. DFM applies Flow Matching independently at each level of a user-defined multi-scale representation (such as Laplacian pyramid). As shown by our experiments, our approach improves visual quality for both images and videos, featuring superior results compared to prior multistage frameworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores over the base architecture and 26.4% over the best-performing baseline, under the same training compute. When applied to finetuning of large models, such as FLUX, DFM shows faster convergence speed to the training distribution. Crucially, all these advantages are achieved with a single model, architectural simplicity, and minimal modifications to existing training pipelines. </p>
<blockquote>
<p>ç”Ÿæˆé«˜ç»´è§†è§‰æ¨¡å¼æ˜¯ä¸€é¡¹è®¡ç®—å¯†é›†å‹çš„ä»»åŠ¡ã€‚å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯æ¸è¿›ç”Ÿæˆï¼Œå…¶ä¸­è¾“å‡ºä»¥ç”±ç²—åˆ°ç»†çš„é¢‘è°±è‡ªå›å½’æ–¹å¼åˆæˆã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å—ç›Šäºå»å™ªçš„ç”±ç²—åˆ°ç»†çš„ç‰¹æ€§ï¼Œä½†æ˜ç¡®çš„å¤šé˜¶æ®µæ¶æ„å¾ˆå°‘è¢«é‡‡ç”¨ã€‚è¿™äº›æ¶æ„å¢åŠ äº†æ•´ä½“æ–¹æ³•çš„å¤æ‚æ€§ï¼Œéœ€è¦è‡ªå®šä¹‰çš„æ‰©æ•£å…¬å¼ã€ä¾èµ–äºåˆ†è§£çš„é˜¶æ®µè½¬æ¢ã€ä¸“é—¨çš„é‡‡æ ·å™¨æˆ–æ¨¡å‹çº§è”ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ˜¯æå‡ºå¯åˆ†è§£æµåŒ¹é…ï¼ˆDFMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„è§†è§‰åª’ä½“æ¸è¿›ç”Ÿæˆæ¡†æ¶ã€‚DFMåœ¨ç”¨æˆ·å®šä¹‰çš„å¤šå°ºåº¦è¡¨ç¤ºï¼ˆå¦‚æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ï¼‰çš„æ¯ä¸€çº§ç‹¬ç«‹åº”ç”¨æµåŒ¹é…ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘æ–¹é¢æé«˜äº†è§†è§‰è´¨é‡ï¼Œä¸å…ˆå‰çš„å¤šé˜¶æ®µæ¡†æ¶ç›¸æ¯”ï¼Œç»“æœæ›´ä¸ºä¼˜è¶Šã€‚åœ¨Imagenet-1k 512pxä¸Šï¼ŒDFMåœ¨FDDå¾—åˆ†ä¸Šè¾ƒåŸºç¡€æ¶æ„æé«˜äº†35.2%ï¼Œåœ¨ç›¸åŒè®­ç»ƒè®¡ç®—ä¸‹è¾ƒæœ€ä½³åŸºçº¿æé«˜äº†26.4%ã€‚å½“åº”ç”¨äºå¤§å‹æ¨¡å‹çš„å¾®è°ƒï¼ˆå¦‚FLUXï¼‰æ—¶ï¼ŒDFMæ˜¾ç¤ºå‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ä»¥è¾¾åˆ°è®­ç»ƒåˆ†å¸ƒã€‚å…³é”®çš„æ˜¯ï¼Œæ‰€æœ‰è¿™äº›ä¼˜åŠ¿éƒ½æ˜¯ç”¨ä¸€ä¸ªå•ä¸€æ¨¡å‹å®ç°çš„ï¼Œå…·æœ‰æ¶æ„ç®€å•æ€§ï¼Œå¯¹ç°æœ‰è®­ç»ƒç®¡é“è¿›è¡Œäº†æœ€å°‘çš„ä¿®æ”¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19839v1">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://snap-research.github.io/dfm/">https://snap-research.github.io/dfm/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ç”Ÿæˆé«˜ç»´è§†è§‰æ¨¡æ€çš„è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œä¸€ç§åä¸ºå¯åˆ†è§£æµåŒ¹é…ï¼ˆDFMï¼‰çš„æ¸è¿›ç”Ÿæˆè§†è§‰åª’ä½“çš„ç®€å•æœ‰æ•ˆæ¡†æ¶ã€‚DFMåœ¨ç”¨æˆ·å®šä¹‰çš„å¤šå°ºåº¦è¡¨ç¤ºï¼ˆå¦‚æ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ï¼‰çš„æ¯ä¸ªçº§åˆ«ä¸Šç‹¬ç«‹åº”ç”¨æµåŒ¹é…ï¼Œæé«˜äº†å›¾åƒå’Œè§†é¢‘çš„è§†è§‰è´¨é‡ï¼Œå¹¶åœ¨Imagenet-1k 512pxä¸Šå®ç°äº†å¯¹åŸºç¡€æ¶æ„çš„35.2%çš„FDDåˆ†æ•°æ”¹è¿›ï¼Œä»¥åŠå¯¹æœ€ä½³åŸºå‡†çš„26.4%æ”¹è¿›ã€‚åŒæ—¶ï¼Œåº”ç”¨äºå¤§å‹æ¨¡å‹çš„å¾®è°ƒæ—¶ï¼ŒDFMå±•ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚å…³é”®æ˜¯ï¼Œæ‰€æœ‰è¿™äº›ä¼˜åŠ¿éƒ½é€šè¿‡ä¸€ä¸ªç®€å•çš„æ¨¡å‹ã€æœ€å°‘çš„æ¶æ„æ”¹åŠ¨å’Œç°æœ‰çš„è®­ç»ƒç®¡é“å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆé«˜ç»´è§†è§‰æ¨¡æ€æ˜¯è®¡ç®—å¯†é›†å‹çš„ä»»åŠ¡ï¼Œé€šå¸¸é‡‡ç”¨æ¸è¿›ç”Ÿæˆçš„æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å—ç›Šäºä»ç²—ç³™åˆ°ç²¾ç»†çš„é™å™ªç‰¹æ€§ï¼Œä½†å¤šé˜¶æ®µæ¶æ„çš„å¤æ‚æ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚</li>
<li>å¯åˆ†è§£æµåŒ¹é…ï¼ˆDFMï¼‰æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¸è¿›ç”Ÿæˆè§†è§‰åª’ä½“çš„æ¡†æ¶ã€‚</li>
<li>DFMåœ¨ç”¨æˆ·å®šä¹‰çš„å¤šå°ºåº¦è¡¨ç¤ºçš„æ¯ä¸ªçº§åˆ«ä¸Šç‹¬ç«‹åº”ç”¨æµåŒ¹é…ã€‚</li>
<li>DFMæé«˜äº†å›¾åƒå’Œè§†é¢‘çš„è§†è§‰è´¨é‡ï¼Œå¹¶åœ¨Imagenet-1k 512pxä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>DFMå¯åº”ç”¨äºå¤§å‹æ¨¡å‹çš„å¾®è°ƒï¼Œå¹¶å±•ç°æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>DFMçš„å®ç°å…·æœ‰ç®€å•çš„æ¨¡å‹ã€æœ€å°‘çš„æ¶æ„æ”¹åŠ¨å’Œç°æœ‰çš„è®­ç»ƒç®¡é“éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb739bd4987ec84f4dae6845725d2a7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c43bc603de52184f8563d47514ed94d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c50a4b890fdf43226be2811643bd1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15e06c874f6f69512c959d36d72ec114.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CoCo4D-Comprehensive-and-Complex-4D-Scene-Generation"><a href="#CoCo4D-Comprehensive-and-Complex-4D-Scene-Generation" class="headerlink" title="CoCo4D: Comprehensive and Complex 4D Scene Generation"></a>CoCo4D: Comprehensive and Complex 4D Scene Generation</h2><p><strong>Authors:Junwei Zhou, Xueting Li, Lu Qi, Ming-Hsuan Yang</strong></p>
<p>Existing 4D synthesis methods primarily focus on object-level generation or dynamic scene synthesis with limited novel views, restricting their ability to generate multi-view consistent and immersive dynamic 4D scenes. To address these constraints, we propose a framework (dubbed as CoCo4D) for generating detailed dynamic 4D scenes from text prompts, with the option to include images. Our method leverages the crucial observation that articulated motion typically characterizes foreground objects, whereas background alterations are less pronounced. Consequently, CoCo4D divides 4D scene synthesis into two responsibilities: modeling the dynamic foreground and creating the evolving background, both directed by a reference motion sequence. Given a text prompt and an optional reference image, CoCo4D first generates an initial motion sequence utilizing video diffusion models. This motion sequence then guides the synthesis of both the dynamic foreground object and the background using a novel progressive outpainting scheme. To ensure seamless integration of the moving foreground object within the dynamic background, CoCo4D optimizes a parametric trajectory for the foreground, resulting in realistic and coherent blending. Extensive experiments show that CoCo4D achieves comparable or superior performance in 4D scene generation compared to existing methods, demonstrating its effectiveness and efficiency. More results are presented on our website <a target="_blank" rel="noopener" href="https://colezwhy.github.io/coco4d/">https://colezwhy.github.io/coco4d/</a>. </p>
<blockquote>
<p>å½“å‰å­˜åœ¨çš„å››ç»´åˆæˆæ–¹æ³•ä¸»è¦èšç„¦äºç‰©ä½“çº§åˆ«çš„ç”Ÿæˆæˆ–åŠ¨æ€åœºæ™¯åˆæˆï¼Œå¹¶ä¸”å…·æœ‰æœ‰é™çš„è§†è§’èŒƒå›´ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç”Ÿæˆå¤šè§†è§’ä¸€è‡´ä¸”æ²‰æµ¸å¼åŠ¨æ€å››ç»´åœºæ™¯æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCoCo4Dçš„æ¡†æ¶ï¼Œç”¨äºä»æ–‡æœ¬æç¤ºç”Ÿæˆè¯¦ç»†çš„åŠ¨æ€å››ç»´åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥é€‰æ‹©åŒ…å«å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†å…³é”®è§‚å¯Ÿç»“æœï¼Œå³å…³èŠ‚è¿åŠ¨é€šå¸¸è¡¨å¾å‰æ™¯ç‰©ä½“ï¼Œè€ŒèƒŒæ™¯å˜åŒ–åˆ™ä¸å¤ªæ˜æ˜¾ã€‚å› æ­¤ï¼ŒCoCo4Då°†å››ç»´åœºæ™¯åˆæˆåˆ†ä¸ºä¸¤ä¸ªä»»åŠ¡ï¼šå¯¹åŠ¨æ€å‰æ™¯è¿›è¡Œå»ºæ¨¡å¹¶åˆ›å»ºä¸æ–­å˜åŒ–çš„èƒŒæ™¯ï¼Œä¸¤è€…å‡ç”±å‚è€ƒè¿åŠ¨åºåˆ—æ§åˆ¶ã€‚ç»™å®šæ–‡æœ¬æç¤ºå’Œå¯é€‰çš„å‚è€ƒå›¾åƒï¼ŒCoCo4Dé¦–å…ˆåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆå§‹è¿åŠ¨åºåˆ—ã€‚ç„¶åï¼Œæ­¤è¿åŠ¨åºåˆ—é€šè¿‡ä¸€ç§æ–°é¢–çš„è¿›æ­¥å¼å¤–æ¨æ–¹æ¡ˆæ¥æŒ‡å¯¼åŠ¨æ€å‰æ™¯ç‰©ä½“å’ŒèƒŒæ™¯çš„åˆæˆã€‚ä¸ºäº†ç¡®ä¿ç§»åŠ¨å‰æ™¯ç‰©ä½“åœ¨åŠ¨æ€èƒŒæ™¯ä¸­çš„æ— ç¼é›†æˆï¼ŒCoCo4Dä¼˜åŒ–äº†å‰æ™¯çš„å‚æ•°è½¨è¿¹ï¼Œä»è€Œå®ç°çœŸå®ä¸”è¿è´¯çš„èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å››ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢ï¼ŒCoCo4Dä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”è¾¾åˆ°äº†ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚æ›´å¤šç»“æœè¯·å‚è§æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://colezwhy.github.io/coco4d/%E3%80%82">https://colezwhy.github.io/coco4d/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19798v1">PDF</a> 16 pages,10 figures</p>
<p><strong>Summary</strong><br>åŠ¨æ€å››ç»´åº¦åœºæ™¯ç”Ÿæˆæ–°æ–¹æ³•ï¼Œä»æ–‡æœ¬æç¤ºå‡ºå‘å¹¶å¯é€‰æ‹©åŒ…å«å›¾åƒã€‚é€šè¿‡ç†è§£å‰æ™¯ä¸èƒŒæ™¯åŠ¨æ€å˜åŒ–çš„å·®å¼‚ï¼Œåˆ†ä¸ºæ¨¡æ‹ŸåŠ¨æ€å‰æ™¯å’Œåˆ›å»ºå˜åŒ–èƒŒæ™¯ä¸¤éƒ¨åˆ†ã€‚é‡‡ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆå§‹è¿åŠ¨åºåˆ—ï¼Œé€šè¿‡æ¸è¿›å¼æ‰©å±•æ–¹æ¡ˆåˆæˆåŠ¨æ€å‰æ™¯å’ŒèƒŒæ™¯ã€‚ä¼˜åŒ–å‰æ™¯å‚æ•°è½¨è¿¹ï¼Œå®ç°æ— ç¼é›†æˆã€‚åœ¨å››ç»´åº¦åœºæ™¯ç”Ÿæˆä¸Šè¡¨ç°å“è¶Šï¼Œè¯¦æƒ…è¯·è§ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å››ç»´åº¦ï¼ˆ4Dï¼‰åˆæˆæ–¹æ³•ä¸»è¦å…³æ³¨å¯¹è±¡çº§åˆ«çš„ç”Ÿæˆæˆ–åŠ¨æ€åœºæ™¯åˆæˆï¼Œå…·æœ‰å±€é™æ€§ã€‚</li>
<li>æå‡ºåä¸ºCoCo4Dçš„æ¡†æ¶ï¼Œå¯ä»æ–‡æœ¬æç¤ºç”Ÿæˆè¯¦ç»†åŠ¨æ€4Dåœºæ™¯ï¼Œå¹¶å¯é€‰æ‹©åŒ…å«å›¾åƒã€‚</li>
<li>CoCo4Dåˆ©ç”¨å‰æ™¯å’ŒèƒŒæ™¯åŠ¨æ€å˜åŒ–å·®å¼‚è¿›è¡Œå»ºæ¨¡ï¼Œåˆ†ä¸ºæ¨¡æ‹ŸåŠ¨æ€å‰æ™¯å’Œåˆ›å»ºèƒŒæ™¯ä¸¤éƒ¨åˆ†ã€‚</li>
<li>é€šè¿‡è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆå§‹è¿åŠ¨åºåˆ—ï¼Œå¼•å¯¼å‰æ™¯å’ŒèƒŒæ™¯çš„åˆæˆã€‚</li>
<li>é‡‡ç”¨æ¸è¿›å¼æ‰©å±•æ–¹æ¡ˆå®ç°åœºæ™¯åˆæˆã€‚</li>
<li>ä¼˜åŒ–å‰æ™¯å‚æ•°è½¨è¿¹ï¼Œç¡®ä¿æ— ç¼é›†æˆåŠ¨æ€å‰æ™¯äºèƒŒæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19798">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef5ba5936ae223840209ac99579c7f69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9433076a9070db898ba2da36a941b06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37e19556b85a32b83a8f57c416f7122c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Noise-Consistency-Training-A-Native-Approach-for-One-Step-Generator-in-Learning-Additional-Controls"><a href="#Noise-Consistency-Training-A-Native-Approach-for-One-Step-Generator-in-Learning-Additional-Controls" class="headerlink" title="Noise Consistency Training: A Native Approach for One-Step Generator in   Learning Additional Controls"></a>Noise Consistency Training: A Native Approach for One-Step Generator in   Learning Additional Controls</h2><p><strong>Authors:Yihong Luo, Shuchen Xue, Tianyang Hu, Jing Tang</strong></p>
<p>The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditionsâ€“such as structural constraints, semantic guidelines, or external inputsâ€“poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted modelâ€™s generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/NCT">https://github.com/Luo-Yihong/NCT</a> </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰é¢†åŸŸï¼Œè¿½æ±‚é«˜æ•ˆä¸”å¯æ§çš„é«˜è´¨é‡å†…å®¹ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸€æ­¥ç”Ÿæˆå™¨é€šè¿‡æ‰©æ•£è’¸é¦æŠ€æœ¯å®ç°äº†å‡ºè‰²çš„ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ï¼Œä½†å°†å…¶é€‚åº”æ–°çš„æ§åˆ¶æ¡ä»¶ï¼ˆå¦‚ç»“æ„çº¦æŸã€è¯­ä¹‰æŒ‡å—æˆ–å¤–éƒ¨è¾“å…¥ï¼‰å´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œæ˜‚è´µçš„ä¿®æ”¹å’Œéšåçš„æ‰©æ•£è’¸é¦ã€‚æœ¬æ–‡ä»‹ç»äº†å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼ˆNCTï¼‰è¿™ä¸€æ–°é¢–è€Œè½»é‡çº§çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥ç›´æ¥å°†æ–°çš„æ§åˆ¶ä¿¡å·é›†æˆåˆ°é¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨ä¸­ï¼Œæ— éœ€è®¿é—®åŸå§‹è®­ç»ƒå›¾åƒæˆ–é‡æ–°è®­ç»ƒåŸºç¡€æ‰©æ•£æ¨¡å‹ã€‚NCTé€šè¿‡å¼•å…¥é€‚é…å™¨æ¨¡å—å¹¶é‡‡ç”¨ç”Ÿæˆå™¨å™ªå£°ç©ºé—´ä¸­çš„å™ªå£°ä¸€è‡´æ€§æŸå¤±æ¥è¿è¡Œã€‚è¿™ç§æŸå¤±ä¼šè°ƒæ•´é€‚åº”æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºï¼Œä½¿å…¶åœ¨ä¸åŒç¨‹åº¦çš„æ¡ä»¶å™ªå£°ä¹‹é—´ä¿æŒä¸€è‡´ï¼Œä»è€Œéšå¼åœ°æŒ‡å¯¼å…¶é€‚åº”æ–°çš„æ§åˆ¶ã€‚ä»ç†è®ºä¸Šè®²ï¼Œè¿™ç§è®­ç»ƒç›®æ ‡å¯ä»¥ç†è§£ä¸ºæœ€å°åŒ–é€‚åº”ç”Ÿæˆå™¨ä¸ç”±æ–°æ¡ä»¶å¼•èµ·çš„æ¡ä»¶åˆ†å¸ƒä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ã€‚NCTå…·æœ‰æ¨¡å—åŒ–ã€æ•°æ®é«˜æ•ˆã€æ˜“äºéƒ¨ç½²ç­‰ç‰¹ç‚¹ï¼Œä»…ä¾èµ–äºé¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨å’Œæ§åˆ¶ä¿¡å·æ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNCTåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¯æ§ç”Ÿæˆï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰çš„å¤šæ­¥éª¤å’ŒåŸºäºè’¸é¦çš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/NCT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Luo-Yihong/NCTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19741v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”è½»é‡çº§çš„æ–¹æ³•â€”â€”å™ªå£°ä¸€è‡´æ€§è®­ç»ƒï¼ˆNCTï¼‰ï¼Œç”¨äºå°†æ–°çš„æ§åˆ¶ä¿¡å·ç›´æ¥é›†æˆåˆ°é¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨ä¸­ï¼Œå®ç°é«˜æ•ˆå¯æ§çš„é«˜è´¨é‡å†…å®¹ç”Ÿæˆã€‚è¯¥æ–¹æ³•æ— éœ€è®¿é—®åŸå§‹è®­ç»ƒå›¾åƒæˆ–é‡æ–°è®­ç»ƒåŸºç¡€æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥é€‚é…å™¨æ¨¡å—å’Œå™ªå£°ç©ºé—´ä¸­çš„å™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼Œä½¿é€‚åº”æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºåœ¨ä¸åŒæ¡ä»¶ä¸‹ä¿æŒä¸€è‡´ï¼Œä»è€Œå®ç°æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒNCTåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å®ç°å¯æ§ç”Ÿæˆï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰çš„å¤šæ­¥éª¤å’ŒåŸºäºè’¸é¦çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NCTæ˜¯ä¸€ç§ç”¨äºé€‚åº”é¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨ä»¥å“åº”æ–°æ§åˆ¶æ¡ä»¶çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¼•å…¥é€‚é…å™¨æ¨¡å—å’Œå™ªå£°ä¸€è‡´æ€§æŸå¤±ï¼ŒNCTèƒ½å¤Ÿåœ¨ä¸éœ€è¦åŸå§‹è®­ç»ƒå›¾åƒæˆ–é‡æ–°è®­ç»ƒåŸºç¡€æ‰©æ•£æ¨¡å‹çš„æƒ…å†µä¸‹é›†æˆæ–°çš„æ§åˆ¶ä¿¡å·ã€‚</li>
<li>NCTæ–¹æ³•å®ç°äº†åœ¨å™ªå£°ç©ºé—´ä¸­çš„ä¸€è‡´æ€§ï¼Œä½¿é€‚åº”æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºåœ¨ä¸åŒæ¡ä»¶ä¸‹ä¿æŒä¸€è‡´ã€‚</li>
<li>NCTé€šè¿‡æœ€å°åŒ–é€‚åº”ç”Ÿæˆå™¨ä¸ç”±æ–°æ¡ä»¶å¼•èµ·çš„æ¡ä»¶åˆ†å¸ƒä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ï¼Œå®ç°äº†ç†è®ºä¸Šçš„è®­ç»ƒç›®æ ‡ã€‚</li>
<li>NCTå…·æœ‰æ¨¡å—åŒ–ã€æ•°æ®é«˜æ•ˆå’Œæ˜“äºéƒ¨ç½²çš„ç‰¹ç‚¹ï¼Œä»…ä¾èµ–äºé¢„è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆå™¨å’Œæ§åˆ¶ä¿¡å·æ¨¡å‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒNCTåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­å®ç°äº†å¯æ§ç”Ÿæˆï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42720881530e712878136ee29f951273.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f689b79ee588e849766aa50f92780fa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SceneCrafter-Controllable-Multi-View-Driving-Scene-Editing"><a href="#SceneCrafter-Controllable-Multi-View-Driving-Scene-Editing" class="headerlink" title="SceneCrafter: Controllable Multi-View Driving Scene Editing"></a>SceneCrafter: Controllable Multi-View Driving Scene Editing</h2><p><strong>Authors:Zehao Zhu, Yuliang Zou, Chiyu Max Jiang, Bo Sun, Vincent Casser, Xiukun Huang, Jiahao Wang, Zhenpei Yang, Ruiqi Gao, Leonidas Guibas, Mingxing Tan, Dragomir Anguelov</strong></p>
<p>Simulation is crucial for developing and evaluating autonomous vehicle (AV) systems. Recent literature builds on a new generation of generative models to synthesize highly realistic images for full-stack simulation. However, purely synthetically generated scenes are not grounded in reality and have difficulty in inspiring confidence in the relevance of its outcomes. Editing models, on the other hand, leverage source scenes from real driving logs, and enable the simulation of different traffic layouts, behaviors, and operating conditions such as weather and time of day. While image editing is an established topic in computer vision, it presents fresh sets of challenges in driving simulation: (1) the need for cross-camera 3D consistency, (2) learning &#96;&#96;empty streetâ€ priors from driving data with foreground occlusions, and (3) obtaining paired image tuples of varied editing conditions while preserving consistent layout and geometry. To address these challenges, we propose SceneCrafter, a versatile editor for realistic 3D-consistent manipulation of driving scenes captured from multiple cameras. We build on recent advancements in multi-view diffusion models, using a fully controllable framework that scales seamlessly to multi-modality conditions like weather, time of day, agent boxes and high-definition maps. To generate paired data for supervising the editing model, we propose a novel framework on top of Prompt-to-Prompt to generate geometrically consistent synthetic paired data with global edits. We also introduce an alpha-blending framework to synthesize data with local edits, leveraging a model trained on empty street priors through novel masked training and multi-view repaint paradigm. SceneCrafter demonstrates powerful editing capabilities and achieves state-of-the-art realism, controllability, 3D consistency, and scene editing quality compared to existing baselines. </p>
<blockquote>
<p>æ¨¡æ‹Ÿå¯¹äºå¼€å‘å’Œè¯„ä¼°è‡ªåŠ¨é©¾é©¶ï¼ˆAVï¼‰ç³»ç»Ÿè‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶æ–‡çŒ®å»ºç«‹åœ¨ä¸€ä»£æ–°çš„ç”Ÿæˆæ¨¡å‹ä¸Šï¼Œåˆæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œç”¨äºå…¨æ ˆæ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œçº¯ç²¹åˆæˆç”Ÿæˆçš„åœºæ™¯å¹¶æ²¡æœ‰å®é™…åŸºç¡€ï¼Œä¸”éš¾ä»¥æ¿€å‘å¯¹å…¶ç»“æœç›¸å…³æ€§çš„ä¿¡å¿ƒã€‚å¦ä¸€æ–¹é¢ï¼Œç¼–è¾‘æ¨¡å‹åˆ™åˆ©ç”¨æ¥è‡ªçœŸå®é©¾é©¶æ—¥å¿—çš„æºåœºæ™¯ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿä¸åŒçš„äº¤é€šå¸ƒå±€ã€è¡Œä¸ºå’Œæ“ä½œæ¡ä»¶ï¼Œä¾‹å¦‚å¤©æ°”å’Œä¸€å¤©ä¸­çš„æ—¶é—´ã€‚è™½ç„¶å›¾åƒç¼–è¾‘æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªæ—¢å®šä¸»é¢˜ï¼Œä½†å®ƒç»™é©¾é©¶æ¨¡æ‹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼š(1)éœ€è¦è·¨ç›¸æœº3Dä¸€è‡´æ€§ï¼Œ(2)ä»å¸¦æœ‰å‰æ™¯é®æŒ¡çš„é©¾é©¶æ•°æ®ä¸­å­¦ä¹ â€œç©ºè¡—â€å…ˆéªŒï¼Œä»¥åŠ(3)åœ¨ä¿æŒä¸€è‡´å¸ƒå±€å’Œå‡ ä½•ç»“æ„çš„åŒæ—¶ï¼Œè·å–å…·æœ‰ä¸åŒç¼–è¾‘æ¡ä»¶çš„é…å¯¹å›¾åƒå…ƒç»„ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SceneCrafterï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„ç¼–è¾‘å™¨ï¼Œå¯ä»¥å¯¹ä»å¤šä¸ªç›¸æœºæ•è·çš„é©¾é©¶åœºæ™¯è¿›è¡Œé€¼çœŸçš„3Dä¸€è‡´æ“ä½œã€‚æˆ‘ä»¬å»ºç«‹åœ¨æœ€æ–°çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹è¿›å±•ä¹‹ä¸Šï¼Œä½¿ç”¨å®Œå…¨å¯æ§çš„æ¡†æ¶ï¼Œæ— ç¼æ‰©å±•åˆ°å¤šæ¨¡å¼æ¡ä»¶ï¼Œå¦‚å¤©æ°”ã€ä¸€å¤©ä¸­çš„æ—¶é—´ã€ä»£ç†æ¡†å’Œé«˜æ¸…æ™°åº¦åœ°å›¾ã€‚ä¸ºäº†ç”Ÿæˆç”¨äºç›‘ç£ç¼–è¾‘æ¨¡å‹çš„æ•°æ®å¯¹ï¼Œæˆ‘ä»¬åœ¨Prompt-to-Promptä¹‹ä¸Šæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œä»¥ç”Ÿæˆå…·æœ‰å…¨å±€ç¼–è¾‘çš„å‡ ä½•ä¸€è‡´åˆæˆæ•°æ®å¯¹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªalphaæ··åˆæ¡†æ¶ï¼Œä»¥åˆæˆå…·æœ‰å±€éƒ¨ç¼–è¾‘çš„æ•°æ®ï¼Œåˆ©ç”¨ä¸€ä¸ªé€šè¿‡æ–°é¢–æ©è†œè®­ç»ƒå’Œå¤šå…ƒè§†å›¾é‡ç»˜èŒƒå¼è®­ç»ƒçš„ç©ºè¡—å…ˆéªŒæ¨¡å‹ã€‚SceneCrafterå±•ç°å‡ºå¼ºå¤§çš„ç¼–è¾‘èƒ½åŠ›ï¼Œå¹¶åœ¨ç°å®æ„Ÿã€å¯æ§æ€§ã€3Dä¸€è‡´æ€§å’Œåœºæ™¯ç¼–è¾‘è´¨é‡æ–¹é¢è¾¾åˆ°äº†ç°æœ‰åŸºå‡†çš„æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19488v1">PDF</a> CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆé«˜åº¦é€¼çœŸçš„å›¾åƒä»¥è¿›è¡Œå…¨å †æ ˆæ¨¡æ‹Ÿæ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œè¿™å¯¹äºå¼€å‘å’Œè¯„ä¼°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œçº¯ç²¹çš„åˆæˆåœºæ™¯ç¼ºä¹ç°å®åŸºç¡€ï¼Œéš¾ä»¥è®©äººä¿¡æœå…¶ç»“æœçš„ç°å®æ€§ã€‚å› æ­¤ï¼Œç¼–è¾‘æ¨¡å‹åº”è¿è€Œç”Ÿï¼Œå®ƒåˆ©ç”¨çœŸå®é©¾é©¶æ—¥å¿—çš„åœºæ™¯ä½œä¸ºæ¥æºï¼Œå¹¶æ¨¡æ‹Ÿä¸åŒçš„äº¤é€šå¸ƒå±€ã€è¡Œä¸ºå’Œæ“ä½œæ¡ä»¶ï¼Œå¦‚å¤©æ°”å’Œæ—¶é—´ã€‚ç„¶è€Œï¼Œé©¾é©¶æ¨¡æ‹Ÿä¸­çš„å›¾åƒç¼–è¾‘è™½ç„¶å¸¦æ¥äº†è®¸å¤šæ–°æŒ‘æˆ˜ï¼Œä¾‹å¦‚éœ€è¦è·¨ç›¸æœºè¿›è¡Œä¸‰ç»´ä¸€è‡´æ€§å¤„ç†ã€å­¦ä¹ å­˜åœ¨å‰æ™¯é®æŒ¡çš„â€œç©ºè¡—â€å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åŠåœ¨ä¿æŒä¸€è‡´çš„å¸ƒå±€å’Œå‡ ä½•çš„åŒæ—¶è·å¾—é…å¯¹å›¾åƒçš„ä¸åŒç¼–è¾‘æ¡ä»¶ç­‰ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SceneCrafterï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»å¤šä¸ªç›¸æœºæ•è·çš„é©¾é©¶åœºæ™¯è¿›è¡Œç°å®ä¸‰ç»´ä¸€è‡´æ€§æ“ä½œçš„é€šç”¨ç¼–è¾‘å™¨ã€‚å€ŸåŠ©æœ€æ–°çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯å®Œå…¨æ§åˆ¶çš„æ¡†æ¶ï¼Œå¯æ— ç¼æ‰©å±•åˆ°å¤šæ¨¡å¼æ¡ä»¶ä¸‹ï¼Œå¦‚å¤©æ°”ã€æ—¶é—´ã€ä»£ç†æ¡†å’Œé«˜ç²¾åº¦åœ°å›¾ç­‰ã€‚ä¸ºäº†ç”Ÿæˆç”¨äºç›‘ç£ç¼–è¾‘æ¨¡å‹çš„æ•°æ®å¯¹ï¼Œæˆ‘ä»¬åœ¨Prompt-to-Promptçš„åŸºç¡€ä¸Šæå‡ºäº†ä¸€ä¸ªæ–°å‹æ¡†æ¶æ¥ç”Ÿæˆå…·æœ‰å…¨å±€ç¼–è¾‘çš„å‡ ä½•ä¸€è‡´æ€§åˆæˆæ•°æ®å¯¹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªalphaæ··åˆæ¡†æ¶æ¥åˆæˆå…·æœ‰å±€éƒ¨ç¼–è¾‘çš„æ•°æ®ï¼Œå¹¶åˆ©ç”¨æ–°å‹æ©è†œè®­ç»ƒå’Œè·¨è§†å›¾é‡ç»˜èŒƒå¼æ¥è®­ç»ƒä¸€ä¸ªç©ºè¡—å…ˆéªŒæ¨¡å‹ã€‚SceneCrafterå±•ç¤ºäº†å¼ºå¤§çš„ç¼–è¾‘èƒ½åŠ›ï¼Œå¹¶åœ¨ç°å®æ„Ÿã€å¯æ§æ€§ã€ä¸‰ç»´ä¸€è‡´æ€§å’Œåœºæ™¯ç¼–è¾‘è´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†æ¨¡æ‹Ÿä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿåˆæˆé«˜åº¦é€¼çœŸçš„å›¾åƒã€‚</li>
<li>çº¯ç²¹åˆæˆåœºæ™¯ç”±äºç¼ºä¹ç°å®åŸºç¡€ï¼Œåœ¨è¯„ä¼°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ—¶éš¾ä»¥å»ºç«‹ä¿¡å¿ƒã€‚</li>
<li>ç¼–è¾‘æ¨¡å‹åˆ©ç”¨çœŸå®é©¾é©¶æ—¥å¿—çš„åœºæ™¯ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿä¸åŒçš„äº¤é€šæ¡ä»¶ï¼Œå¦‚å¤©æ°”å’Œæ—¶é—´ã€‚</li>
<li>é©¾é©¶æ¨¡æ‹Ÿä¸­çš„å›¾åƒç¼–è¾‘é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è·¨ç›¸æœºçš„ä¸‰ç»´ä¸€è‡´æ€§ã€å­¦ä¹ ç©ºè¡—å…ˆéªŒçŸ¥è¯†ç­‰ã€‚</li>
<li>SceneCrafteræ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç¼–è¾‘å·¥å…·ï¼Œèƒ½å¤Ÿå®ç°ç°å®ä¸‰ç»´ä¸€è‡´çš„é©¾é©¶åœºæ™¯æ“ä½œã€‚</li>
<li>SceneCrafteråˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œå¯æ— ç¼é€‚åº”å¤šç§æ¨¡å¼æ¡ä»¶ï¼Œå¦‚å¤©æ°”ã€æ—¶é—´ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f86f0b9ebfe761cb66656a815c663ac5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b33061a96159865f6a00f2b9c5bab758.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d420cb5942305841a0369f99b6e8e43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112f60f6c10eaa255ce4081c1381ade8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Angio-Diff-Learning-a-Self-Supervised-Adversarial-Diffusion-Model-for-Angiographic-Geometry-Generation"><a href="#Angio-Diff-Learning-a-Self-Supervised-Adversarial-Diffusion-Model-for-Angiographic-Geometry-Generation" class="headerlink" title="Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for   Angiographic Geometry Generation"></a>Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for   Angiographic Geometry Generation</h2><p><strong>Authors:Zhifeng Wang, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu, Kunlun He</strong></p>
<p>Vascular diseases pose a significant threat to human health, with X-ray angiography established as the gold standard for diagnosis, allowing for detailed observation of blood vessels. However, angiographic X-rays expose personnel and patients to higher radiation levels than non-angiographic X-rays, which are unwanted. Thus, modality translation from non-angiographic to angiographic X-rays is desirable. Data-driven deep approaches are hindered by the lack of paired large-scale X-ray angiography datasets. While making high-quality vascular angiography synthesis crucial, it remains challenging. We find that current medical image synthesis primarily operates at pixel level and struggles to adapt to the complex geometric structure of blood vessels, resulting in unsatisfactory quality of blood vessel image synthesis, such as disconnections or unnatural curvatures. To overcome this issue, we propose a self-supervised method via diffusion models to transform non-angiographic X-rays into angiographic X-rays, mitigating data shortages for data-driven approaches. Our model comprises a diffusion model that learns the distribution of vascular data from diffusion latent, a generator for vessel synthesis, and a mask-based adversarial module. To enhance geometric accuracy, we propose a parametric vascular model to fit the shape and distribution of blood vessels. The proposed method contributes a pipeline and a synthetic dataset for X-ray angiography. We conducted extensive comparative and ablation experiments to evaluate the Angio-Diff. The results demonstrate that our method achieves state-of-the-art performance in synthetic angiography image quality and more accurately synthesizes the geometric structure of blood vessels. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zfw-cv/AngioDiff">https://github.com/zfw-cv/AngioDiff</a>. </p>
<blockquote>
<p>è¡€ç®¡ç–¾ç—…å¯¹äººç±»å¥åº·æ„æˆé‡å¤§å¨èƒï¼ŒXçº¿è¡€ç®¡é€ å½±æœ¯å·²ç¡®ç«‹ä¸ºè¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œèƒ½å¤Ÿè¯¦ç»†è§‚å¯Ÿè¡€ç®¡ã€‚ç„¶è€Œï¼Œè¡€ç®¡é€ å½±Xçº¿ä½¿äººå‘˜å’Œæ‚£è€…æš´éœ²åœ¨æ¯”éè¡€ç®¡é€ å½±Xçº¿æ›´é«˜çš„è¾å°„æ°´å¹³ä¸‹ï¼Œè¿™æ˜¯ä¸å¿…è¦çš„ã€‚å› æ­¤ï¼Œä»éè¡€ç®¡é€ å½±Xçº¿åˆ°è¡€ç®¡é€ å½±Xçº¿çš„æ¨¡æ€è½¬æ¢æ˜¯å¯è¡Œçš„ã€‚æ•°æ®é©±åŠ¨çš„æ·±åº¦æ–¹æ³•å› ç¼ºä¹é…å¥—çš„å¤§è§„æ¨¡Xçº¿è¡€ç®¡é€ å½±æ•°æ®é›†è€Œå—åˆ°é˜»ç¢ã€‚å°½ç®¡è¿›è¡Œé«˜è´¨é‡çš„è¡€ç®¡é€ å½±åˆæˆè‡³å…³é‡è¦ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„åŒ»å­¦å›¾åƒåˆæˆä¸»è¦åœç•™åœ¨åƒç´ å±‚é¢ï¼Œéš¾ä»¥é€‚åº”è¡€ç®¡çš„å¤æ‚å‡ ä½•ç»“æ„ï¼Œå¯¼è‡´è¡€ç®¡å›¾åƒåˆæˆçš„è´¨é‡ä¸ä½³ï¼Œå¦‚æ–­è£‚æˆ–ä¸è‡ªç„¶çš„å¼¯æ›²ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç™½ç›‘ç£æ–¹æ³•ï¼Œå°†éè¡€ç®¡é€ å½±Xçº¿è½¬åŒ–ä¸ºè¡€ç®¡é€ å½±Xçº¿ï¼Œç¼“è§£æ•°æ®é©±åŠ¨æ–¹æ³•çš„çŸ­ç¼ºæ•°æ®é—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…æ‹¬ä¸€ä¸ªä»æ‰©æ•£æ½œåœ¨å­¦ä¹ ä¸­è¡€ç®¡æ•°æ®åˆ†å¸ƒçš„æ‰©æ•£æ¨¡å‹ã€ä¸€ä¸ªè¡€ç®¡åˆæˆç”Ÿæˆå™¨å’Œä¸€ä¸ªåŸºäºé®ç½©çš„å¯¹æŠ—æ¨¡å—ã€‚ä¸ºäº†æé«˜å‡ ä½•ç²¾åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå‚æ•°åŒ–è¡€ç®¡æ¨¡å‹ï¼Œä»¥æ‹Ÿåˆè¡€ç®¡çš„å½¢çŠ¶å’Œåˆ†å¸ƒã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸ºXçº¿è¡€ç®¡é€ å½±è´¡çŒ®äº†ä¸€æ¡ç®¡é“å’Œåˆæˆæ•°æ®é›†ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ¯”è¾ƒå’Œæ¶ˆèå®éªŒæ¥è¯„ä¼°Angio-Diffã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆè¡€ç®¡é€ å½±å›¾åƒè´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œå¹¶æ›´å‡†ç¡®åœ°åˆæˆè¡€ç®¡çš„å‡ ä½•ç»“æ„ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zfw-cv/AngioDiff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zfw-cv/AngioDiffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19455v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éè¡€ç®¡é€ å½±Xå°„çº¿å› è¾å°„æ°´å¹³è¾ƒä½è€Œæ›´å—æ¬¢è¿ï¼Œä½†å…¶å¯¹è¡€ç®¡ç–¾ç—…çš„è¯Šæ–­æ•ˆæœä¸å¦‚è¡€ç®¡é€ å½±Xå°„çº¿ã€‚ç”±äºç¼ºä¹å¤§è§„æ¨¡é…å¯¹è¡€ç®¡é€ å½±Xå°„çº¿æ•°æ®é›†ï¼Œæ•°æ®é©±åŠ¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å—é™ã€‚å½“å‰åŒ»å­¦å›¾åƒåˆæˆä¸»è¦åœ¨åƒç´ å±‚é¢æ“ä½œï¼Œéš¾ä»¥é€‚åº”è¡€ç®¡å¤æ‚å‡ ä½•ç»“æ„ï¼Œå¯¼è‡´è¡€ç®¡å›¾åƒåˆæˆè´¨é‡ä¸ä½³ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œå°†éè¡€ç®¡é€ å½±Xå°„çº¿è½¬åŒ–ä¸ºè¡€ç®¡é€ å½±Xå°„çº¿ï¼Œè§£å†³æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æ‰©æ•£æ¨¡å‹ã€è¡€ç®¡åˆæˆç”Ÿæˆå™¨å’ŒåŸºäºæ©è†œçš„å¯¹æŠ—æ¨¡å—ï¼Œå¹¶æå‡ºå‚æ•°åŒ–è¡€ç®¡æ¨¡å‹æé«˜å‡ ä½•å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆè¡€ç®¡é€ å½±å›¾åƒè´¨é‡å’Œè¡€ç®¡å‡ ä½•ç»“æ„åˆæˆå‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡€ç®¡ç–¾ç—…è¯Šæ–­ä¸­ï¼ŒXå°„çº¿è¡€ç®¡é€ å½±ä»æ˜¯é‡‘æ ‡å‡†ï¼Œä½†å…¶è¾ƒé«˜çš„è¾å°„æš´éœ²æˆä¸ºç—›ç‚¹ã€‚</li>
<li>æ•°æ®é©±åŠ¨æ–¹æ³•å—é™äºç¼ºä¹é…å¯¹çš„å¤§è§„æ¨¡è¡€ç®¡é€ å½±Xå°„çº¿æ•°æ®é›†ã€‚</li>
<li>å½“å‰åŒ»å­¦å›¾åƒåˆæˆæ–¹æ³•åœ¨é€‚åº”è¡€ç®¡å¤æ‚å‡ ä½•ç»“æ„æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´åˆæˆè´¨é‡ä¸ä½³ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è‡ªç›‘ç£æ–¹æ³•ï¼Œå®ç°ä»éè¡€ç®¡é€ å½±Xå°„çº¿åˆ°è¡€ç®¡é€ å½±Xå°„çº¿çš„è½¬æ¢ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬æ‰©æ•£æ¨¡å‹ã€è¡€ç®¡åˆæˆç”Ÿæˆå™¨å’ŒåŸºäºæ©è†œçš„å¯¹æŠ—æ¨¡å—ï¼Œä»¥ç¡®ä¿å›¾åƒè´¨é‡ã€‚</li>
<li>å¼•å…¥å‚æ•°åŒ–è¡€ç®¡æ¨¡å‹ä»¥æé«˜å‡ ä½•å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨åˆæˆè¡€ç®¡é€ å½±å›¾åƒè´¨é‡å’Œå‡†ç¡®æ€§æ–¹é¢é¢†å…ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91c3f22c3f82f493835456755506a182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60f79b6a87d7c00107663b707464b9e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-131d067765873e753309cc9324ef1deb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Style-Transfer-A-Decade-Survey"><a href="#Style-Transfer-A-Decade-Survey" class="headerlink" title="Style Transfer: A Decade Survey"></a>Style Transfer: A Decade Survey</h2><p><strong>Authors:Tianshan Zhang, Hao Tang</strong></p>
<p>The revolutionary advancement of Artificial Intelligence Generated Content (AIGC) has fundamentally transformed the landscape of visual content creation and artistic expression. While remarkable progress has been made in image generation and style transfer, the underlying mechanisms and aesthetic implications of these technologies remain insufficiently understood. This paper presents a comprehensive survey of AIGC technologies in visual arts, tracing their evolution from early algorithmic frameworks to contemporary deep generative models. We identify three pivotal paradigms: Variational Autoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion Models, and examine their roles in bridging the gap between human creativity and machine synthesis. To support our analysis, we systematically review over 500 research papers published in the past decade, spanning both foundational developments and state-of-the-art innovations. Furthermore, we propose a multidimensional evaluation framework that incorporates Technical Innovation, Artistic Merit, Visual Quality, Computational Efficiency, and Creative Potential. Our findings reveal both the transformative capacities and current limitations of AIGC systems, emphasizing their profound impact on the future of creative practices. Through this extensive synthesis, we offer a unified perspective on the convergence of artificial intelligence and artistic expression, while outlining key challenges and promising directions for future research in this rapidly evolving field. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„é©å‘½æ€§è¿›æ­¥ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è§†è§‰å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯è¡¨è¾¾é¢†åŸŸçš„æ ¼å±€ã€‚è™½ç„¶åœ¨å›¾åƒç”Ÿæˆå’Œé£æ ¼è½¬æ¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿™äº›æŠ€æœ¯çš„åŸºç¡€æœºåˆ¶å’Œç¾å­¦å½±å“å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†AIGCæŠ€æœ¯åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸçš„åº”ç”¨ï¼Œè¿½æº¯äº†ä»æ—©æœŸç®—æ³•æ¡†æ¶åˆ°å½“å‰æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„æ¼”å˜è¿‡ç¨‹ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸‰ä¸ªå…³é”®èŒƒå¼ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨å¼¥åˆäººç±»åˆ›é€ åŠ›å’Œæœºå™¨åˆæˆä¹‹é—´çš„å·®è·æ–¹é¢çš„ä½œç”¨ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†è¿‡å»åå¹´å‘è¡¨çš„500å¤šç¯‡ç ”ç©¶è®ºæ–‡ï¼ŒåŒ…æ‹¬åŸºç¡€å‘å±•å’Œæœ€æ–°åˆ›æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç»´è¯„ä»·æ¡†æ¶ï¼Œçº³å…¥æŠ€æœ¯åˆ›æ–°ã€è‰ºæœ¯ä»·å€¼ã€è§†è§‰è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œåˆ›æ„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ç³»ç»Ÿçš„å˜é©èƒ½åŠ›å’Œå½“å‰å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†å®ƒå¯¹æœªæ¥åˆ›ä½œå®è·µçš„æ·±è¿œå½±å“ã€‚é€šè¿‡è¿™ä¸€ç»¼åˆç»¼è¿°ï¼Œæˆ‘ä»¬å¯¹äººå·¥æ™ºèƒ½å’Œè‰ºæœ¯è¡¨è¾¾çš„èåˆæä¾›äº†ç»Ÿä¸€çš„è§†è§’ï¼ŒåŒæ—¶æ¦‚è¿°äº†æœªæ¥åœ¨è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸç ”ç©¶çš„å…³é”®æŒ‘æˆ˜å’Œå‰æ™¯æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19278v1">PDF</a> 32 pages</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„è¿›å±•ï¼Œæ”¹å˜äº†è§†è§‰å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯è¡¨è¾¾çš„æ–¹å¼ã€‚æœ¬æ–‡ä»ç®—æ³•æ¡†æ¶åˆ°å½“å‰æ·±åº¦ç”Ÿæˆæ¨¡å‹å…¨é¢å›é¡¾äº†AIGCæŠ€æœ¯åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸçš„å‘å±•ï¼Œå¹¶æŒ‡å‡ºäº†ä¸‰ç§å…³é”®èŒƒå¼ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡å¯¹è¶…è¿‡500ç¯‡ç ”ç©¶è®ºæ–‡çš„ç³»ç»Ÿå›é¡¾å’Œåˆ†æï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†æŠ€æœ¯åˆ›æ–°ã€è‰ºæœ¯ä»·å€¼ã€è§†è§‰è´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œåˆ›æ„æ½œåŠ›ç­‰æ–¹é¢ã€‚ç ”ç©¶æ­ç¤ºäº†AIGCç³»ç»Ÿçš„å˜é©æ½œåŠ›å’Œå½“å‰å±€é™ï¼Œå¼ºè°ƒäº†å…¶å¯¹æœªæ¥è‰ºæœ¯åˆ›ä½œå®è·µçš„æ·±è¿œå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨è§†è§‰è‰ºæœ¯é¢†åŸŸå¸¦æ¥é©å‘½æ€§è¿›å±•ã€‚</li>
<li>AIGCæŠ€æœ¯ä»æ—©æœŸç®—æ³•æ¡†æ¶å‘å±•åˆ°å½“å‰æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>æ–‡ä¸­æŒ‡å‡ºäº†ä¸‰ç§å…³é”®èŒƒå¼ï¼šå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ç³»ç»Ÿå›é¡¾äº†è¶…è¿‡500ç¯‡ç ”ç©¶è®ºæ–‡ï¼ŒåŒ…æ‹¬åŸºç¡€å‘å±•åŠæœ€æ–°åˆ›æ–°ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¤šç»´åº¦çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æŠ€æœ¯åˆ›æ–°ã€è‰ºæœ¯ä»·å€¼ã€è§†è§‰è´¨é‡ç­‰æ–¹é¢ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†AIGCç³»ç»Ÿçš„å˜é©æ½œåŠ›å’Œå½“å‰å±€é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d157e7d8274a0f9b1701c6e9aa3067f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40875cf66121601d702d012682effad9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0b6ebaad90e2906e6fca0512d556b4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7733ca583ae75b96a7acb551370954d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b21f6d35d979f474fec6fb0f2a4a399d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-400ecf746a338e4685d879d6132eb1c0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Inverse-and-Edit-Effective-and-Fast-Image-Editing-by-Cycle-Consistency-Models"><a href="#Inverse-and-Edit-Effective-and-Fast-Image-Editing-by-Cycle-Consistency-Models" class="headerlink" title="Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency   Models"></a>Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency   Models</h2><p><strong>Authors:Ilia Beletskii, Andrey Kuznetsov, Aibek Alanov</strong></p>
<p>Recent advances in image editing with diffusion models have achieved impressive results, offering fine-grained control over the generation process. However, these methods are computationally intensive because of their iterative nature. While distilled diffusion models enable faster inference, their editing capabilities remain limited, primarily because of poor inversion quality. High-fidelity inversion and reconstruction are essential for precise image editing, as they preserve the structural and semantic integrity of the source image. In this work, we propose a novel framework that enhances image inversion using consistency models, enabling high-quality editing in just four steps. Our method introduces a cycle-consistency optimization strategy that significantly improves reconstruction accuracy and enables a controllable trade-off between editability and content preservation. We achieve state-of-the-art performance across various image editing tasks and datasets, demonstrating that our method matches or surpasses full-step diffusion models while being substantially more efficient. The code of our method is available on GitHub at <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/Inverse-and-Edit">https://github.com/ControlGenAI/Inverse-and-Edit</a>. </p>
<blockquote>
<p>åœ¨æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘æ–¹é¢çš„æœ€æ–°è¿›å±•å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œå¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œäº†ç²¾ç»†æ§åˆ¶ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ–¹æ³•çš„è¿­ä»£æ€§è´¨ï¼Œè®¡ç®—é‡å¾ˆå¤§ã€‚å°½ç®¡è’¸é¦æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå®ç°æ›´å¿«çš„æ¨ç†ï¼Œä½†å…¶ç¼–è¾‘åŠŸèƒ½ä»ç„¶æœ‰é™ï¼Œä¸»è¦æ˜¯å› ä¸ºåè½¬è´¨é‡è¾ƒå·®ã€‚é«˜ä¿çœŸåè½¬å’Œé‡å»ºå¯¹äºç²¾ç¡®å›¾åƒç¼–è¾‘è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬ä¿ç•™äº†æºå›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ä¸€è‡´æ€§æ¨¡å‹å¢å¼ºå›¾åƒåè½¬çš„æ–°å‹æ¡†æ¶ï¼Œåªéœ€å››ä¸ªæ­¥éª¤å³å¯å®ç°é«˜è´¨é‡ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¾ªç¯ä¸€è‡´æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºç²¾åº¦ï¼Œå¹¶åœ¨å¯ç¼–è¾‘æ€§å’Œå†…å®¹ä¿ç•™ä¹‹é—´å®ç°äº†å¯æ§çš„æƒè¡¡ã€‚æˆ‘ä»¬åœ¨å„ç§å›¾åƒç¼–è¾‘ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ¹é…æˆ–è¶…è¶Šå…¨æ­¥æ‰©æ•£æ¨¡å‹çš„åŒæ—¶ï¼Œæ•ˆç‡æ›´é«˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä»£ç å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/Inverse-and-Edit%E3%80%82">https://github.com/ControlGenAI/Inverse-and-Editã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19103v1">PDF</a> The code of our method is available on GitHub at   <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/Inverse-and-Edit">https://github.com/ControlGenAI/Inverse-and-Edit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„å›¾åƒç¼–è¾‘æŠ€æœ¯çš„æ–°è¿›å±•ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè™½ç„¶æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæä¾›ç²¾ç»†çš„ç”Ÿæˆè¿‡ç¨‹æ§åˆ¶ï¼Œä½†å®ƒä»¬è®¡ç®—é‡å¤§ä¸”è¿­ä»£æ€§è´¨å¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆConsistency Modelsï¼‰å¢å¼ºå›¾åƒåè½¬ï¼ˆinversionï¼‰çš„æ–°æ¡†æ¶ï¼Œå®ç°äº†é«˜è´¨é‡ç¼–è¾‘ä»…éœ€å››æ­¥å®Œæˆã€‚æ–°æ–¹æ³•å¼•å…¥äº†å¾ªç¯ä¸€è‡´æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜é‡å»ºç²¾åº¦ï¼Œå¯åœ¨ç¼–è¾‘å’Œå†…å®¹ä¿ç•™ä¹‹é—´å®ç°å¯æ§æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå›¾åƒç¼–è¾‘ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œç›¸æ¯”å…¨æ­¥æ‰©æ•£æ¨¡å‹æ›´é«˜æ•ˆï¼Œä»£ç å·²å¼€æºäºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è®¡ç®—é‡å¤§ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è’¸é¦æ‰©æ•£æ¨¡å‹è™½åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œä½†ç¼–è¾‘èƒ½åŠ›å—é™ï¼Œä¸»è¦å› ä¸ºåè½¬è´¨é‡ä¸ä½³ã€‚</li>
<li>é«˜ä¿çœŸåè½¬å’Œé‡å»ºå¯¹äºç²¾ç¡®å›¾åƒç¼–è¾‘è‡³å…³é‡è¦ï¼Œèƒ½ä¿ç•™æºå›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰å®Œæ•´æ€§ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¸€è‡´æ€§æ¨¡å‹å¢å¼ºå›¾åƒåè½¬ã€‚</li>
<li>æ–°æ–¹æ³•å¼•å…¥å¾ªç¯ä¸€è‡´æ€§ä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºç²¾åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ç¼–è¾‘å’Œå†…å®¹ä¿ç•™ä¹‹é—´çš„å¯æ§æƒè¡¡ï¼Œåœ¨å„ç§å›¾åƒç¼–è¾‘ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f15d3dabe99bc43e87496f63cb00faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e24b3f8cc0eb53f864098f6f18d9c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125c65292c2e8b6ab7faeb3be90f321c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58df3551862b635ebd78b27d73770724.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="When-Diffusion-Models-Memorize-Inductive-Biases-in-Probability-Flow-of-Minimum-Norm-Shallow-Neural-Nets"><a href="#When-Diffusion-Models-Memorize-Inductive-Biases-in-Probability-Flow-of-Minimum-Norm-Shallow-Neural-Nets" class="headerlink" title="When Diffusion Models Memorize: Inductive Biases in Probability Flow of   Minimum-Norm Shallow Neural Nets"></a>When Diffusion Models Memorize: Inductive Biases in Probability Flow of   Minimum-Norm Shallow Neural Nets</h2><p><strong>Authors:Chen Zeno, Hila Manor, Greg Ongie, Nir Weinberger, Tomer Michaeli, Daniel Soudry</strong></p>
<p>While diffusion models generate high-quality images via probability flow, the theoretical understanding of this process remains incomplete. A key question is when probability flow converges to training samples or more general points on the data manifold. We analyze this by studying the probability flow of shallow ReLU neural network denoisers trained with minimal $\ell^2$ norm. For intuition, we introduce a simpler score flow and show that for orthogonal datasets, both flows follow similar trajectories, converging to a training point or a sum of training points. However, early stopping by the diffusion time scheduler allows probability flow to reach more general manifold points. This reflects the tendency of diffusion models to both memorize training samples and generate novel points that combine aspects of multiple samples, motivating our study of such behavior in simplified settings. We extend these results to obtuse simplex data and, through simulations in the orthogonal case, confirm that probability flow converges to a training point, a sum of training points, or a manifold point. Moreover, memorization decreases when the number of training samples grows, as fewer samples accumulate near training points. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹é€šè¿‡æ¦‚ç‡æµç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å¯¹æ­¤è¿‡ç¨‹çš„ç†è®ºç†è§£ä»ç„¶ä¸å®Œæ•´ã€‚ä¸€ä¸ªå…³é”®çš„é—®é¢˜æ˜¯æ¦‚ç‡æµä½•æ—¶æ”¶æ•›äºè®­ç»ƒæ ·æœ¬æˆ–æ•°æ®æµå½¢ä¸Šçš„æ›´ä¸€èˆ¬ç‚¹ã€‚æˆ‘ä»¬é€šè¿‡ç ”ç©¶ç”¨æœ€å°lÂ²èŒƒæ•°è®­ç»ƒçš„æµ…å±‚ReLUç¥ç»ç½‘ç»œå»å™ªå™¨çš„æ¦‚ç‡æµæ¥åˆ†æè¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†ç›´è§‚ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ›´ç®€å•çš„åˆ†æ•°æµï¼Œå¹¶è¡¨æ˜å¯¹äºæ­£äº¤æ•°æ®é›†ï¼Œä¸¤ç§æµéµå¾ªç›¸ä¼¼çš„è½¨è¿¹ï¼Œæ”¶æ•›åˆ°è®­ç»ƒç‚¹æˆ–è®­ç»ƒç‚¹çš„å’Œã€‚ç„¶è€Œï¼Œæ‰©æ•£æ—¶é—´è°ƒåº¦å™¨çš„æå‰åœæ­¢å…è®¸æ¦‚ç‡æµè¾¾åˆ°æ›´ä¸€èˆ¬çš„æµå½¢ç‚¹ã€‚è¿™åæ˜ äº†æ‰©æ•£æ¨¡å‹æ—¢ä¼šè®°ä½è®­ç»ƒæ ·æœ¬ï¼Œåˆä¼šç”Ÿæˆç»“åˆå¤šä¸ªæ ·æœ¬æ–¹é¢çš„æ–°ç‚¹çš„è¶‹åŠ¿ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬åœ¨ç®€åŒ–ç¯å¢ƒä¸­ç ”ç©¶è¿™ç§è¡Œä¸ºã€‚æˆ‘ä»¬å°†è¿™äº›ç»“æœæ‰©å±•åˆ°æ™¦æ¶©çš„å•çº¯å½¢æ•°æ®ï¼Œå¹¶é€šè¿‡æ­£äº¤æƒ…å†µçš„æ¨¡æ‹Ÿè¯å®ï¼Œæ¦‚ç‡æµæ”¶æ•›äºè®­ç»ƒç‚¹ã€è®­ç»ƒç‚¹çš„å’Œæˆ–æµå½¢ç‚¹ã€‚æ­¤å¤–ï¼Œå½“è®­ç»ƒæ ·æœ¬çš„æ•°é‡å¢åŠ æ—¶ï¼Œè®°å¿†ä¼šå‡å°‘ï¼Œå› ä¸ºè¾ƒå°‘çš„æ ·æœ¬åœ¨è®­ç»ƒç‚¹é™„è¿‘ç§¯ç´¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19031v1">PDF</a> Accepted to the Forty-second International Conference on Machine   Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡æ¦‚ç‡æµç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å…¶ç†è®ºç†è§£å°šä¸å®Œå…¨ã€‚æœ¬æ–‡é€šè¿‡åˆ†ææµ…å±‚ReLUç¥ç»ç½‘ç»œå»å™ªå™¨åœ¨æœ€å°äºŒèŒƒæ•°ä¸‹çš„æ¦‚ç‡æµæ¥æ¢ç©¶æ¦‚ç‡æµä½•æ—¶æ”¶æ•›äºè®­ç»ƒæ ·æœ¬æˆ–æ•°æ®æµå½¢ä¸Šçš„æ›´ä¸€èˆ¬ç‚¹ã€‚å¼•å…¥æ›´ç®€å•çš„åˆ†æ•°æµï¼Œå¯¹äºæ­£äº¤æ•°æ®é›†ï¼Œä¸¤ç§æµéµå¾ªç›¸ä¼¼çš„è½¨è¿¹ï¼Œæ”¶æ•›åˆ°è®­ç»ƒç‚¹æˆ–è®­ç»ƒç‚¹çš„ç»„åˆã€‚ä½†æ‰©æ•£æ—¶é—´è°ƒåº¦å™¨çš„æå‰åœæ­¢å…è®¸æ¦‚ç‡æµè¾¾åˆ°æ›´ä¸€èˆ¬çš„æµå½¢ç‚¹ã€‚è¿™åæ˜ äº†æ‰©æ•£æ¨¡å‹æ—¢ä¼šè®°å¿†è®­ç»ƒæ ·æœ¬ï¼Œä¹Ÿä¼šç”Ÿæˆç»“åˆå¤šä¸ªæ ·æœ¬æ–¹é¢çš„æ–°ç‚¹ï¼Œæ¿€åŠ±æˆ‘ä»¬åœ¨ç®€åŒ–ç¯å¢ƒä¸­ç ”ç©¶è¿™ç§è¡Œä¸ºã€‚æœ¬æ–‡å°†è¿™äº›ç»“æœæ‰©å±•åˆ°å¤æ‚çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡æ­£äº¤æƒ…å†µçš„æ¨¡æ‹Ÿè¯å®æ¦‚ç‡æµæ”¶æ•›äºè®­ç»ƒç‚¹ã€è®­ç»ƒç‚¹çš„ç»„åˆæˆ–æµå½¢ç‚¹ã€‚æ­¤å¤–ï¼Œéšç€è®­ç»ƒæ ·æœ¬æ•°é‡çš„å¢é•¿ï¼Œè®°å¿†ä¼šé€æ¸å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡æ¦‚ç‡æµç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å…¶ç†è®ºç†è§£å°šæœªå®Œå…¨æˆç†Ÿã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„æ¦‚ç‡æµæ”¶æ•›é—®é¢˜è¢«ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ…å±‚ReLUç¥ç»ç½‘ç»œå»å™ªå™¨çš„ä¸Šä¸‹æ–‡ä¸­ã€‚</li>
<li>å¼•å…¥æ›´ç®€å•åˆ†æ•°æµçš„æ¦‚å¿µï¼Œå…¶ä¸æ¦‚ç‡æµçš„è½¨è¿¹åœ¨æ­£äº¤æ•°æ®é›†ä¸Šç›¸ä¼¼ã€‚</li>
<li>æ‰©æ•£æ—¶é—´è°ƒåº¦å™¨çš„æå‰åœæ­¢å…è®¸æ¦‚ç‡æµæ¢ç´¢æ•°æ®ç©ºé—´ä¸­çš„æ›´å¹¿æ³›åŒºåŸŸã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ—¢ä¼šè®°å¿†è®­ç»ƒæ ·æœ¬ï¼Œä¹Ÿèƒ½ç”Ÿæˆç»“åˆå¤šä¸ªæ ·æœ¬ç‰¹å¾çš„æ–°ç‚¹ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿçš„æ­£äº¤æƒ…å†µä¸‹ï¼Œæ¦‚ç‡æµè¢«è§‚å¯Ÿåˆ°æ”¶æ•›äºè®­ç»ƒç‚¹ã€è®­ç»ƒç‚¹çš„ç»„åˆæˆ–æµå½¢ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-468334da1aa57288962299846b35a899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b33cba685fec657f693ffc30498c5d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning"></a>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning</h2><p><strong>Authors:Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue</strong></p>
<p>Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our approach preserves background regions while enabling controllable edits propagation. This solution offers efficient and adaptable video editing without altering the model architecture. To better steer this process, we incorporate additional references, such as alternate viewpoints or representative scene states, which serve as visual anchors for how content should unfold. We address the control challenge using a mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model to the editing context. The model must learn from two distinct sources: the input video provides spatial structure and motion cues, while reference images offer appearance guidance. A spatial mask enables region-specific learning by dynamically modulating what the model attends to, ensuring that each area draws from the appropriate source. Experimental results show our method achieves superior video editing performance compared to state-of-the-art methods. Project Page: <a target="_blank" rel="noopener" href="https://cjeen.github.io/LoraEditPaper">https://cjeen.github.io/LoraEditPaper</a> </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘ç¼–è¾‘å·²ç»åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚è™½ç„¶åŸºäºé¦–å¸§å¼•å¯¼ç¼–è¾‘å¯ä»¥æ§åˆ¶é¦–å¸§ï¼Œä½†å¯¹äºåç»­å¸§çš„æ§åˆ¶å´ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©ç çš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€‚åº”é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œå®ç°çµæ´»çš„è§†é¢‘ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿ç•™èƒŒæ™¯åŒºåŸŸçš„åŒæ—¶ï¼Œå®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚è¿™ç§è§£å†³æ–¹æ¡ˆåœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†é«˜æ•ˆä¸”å¯é€‚åº”çš„è§†é¢‘ç¼–è¾‘ã€‚ä¸ºäº†æ›´å¥½åœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å¤–çš„å‚è€ƒï¼Œå¦‚ä¸åŒçš„è§†è§’æˆ–å…·æœ‰ä»£è¡¨æ€§çš„åœºæ™¯çŠ¶æ€ï¼Œå®ƒä»¬ä½œä¸ºå†…å®¹å±•å¼€çš„å¯è§†é”šç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨æ©è†œé©±åŠ¨çš„LoRAè°ƒä¼˜ç­–ç•¥æ¥è§£å†³æ§åˆ¶æŒ‘æˆ˜ï¼Œè¯¥ç­–ç•¥ä½¿é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚åº”ç¼–è¾‘ä¸Šä¸‹æ–‡ã€‚æ¨¡å‹å¿…é¡»ä»ä¸¤ä¸ªç‹¬ç‰¹çš„ä¿¡æ¯æºä¸­å­¦ä¹ ï¼šè¾“å…¥è§†é¢‘æä¾›ç©ºé—´ç»“æ„å’Œè¿åŠ¨çº¿ç´¢ï¼Œè€Œå‚è€ƒå›¾åƒæä¾›å¤–è§‚æŒ‡å¯¼ã€‚ç©ºé—´æ©è†œé€šè¿‡åŠ¨æ€è°ƒåˆ¶æ¨¡å‹å…³æ³¨çš„åŒºåŸŸæ¥å®ç°ç‰¹å®šåŒºåŸŸçš„å­¦ä¹ ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½èƒ½ä»é€‚å½“çš„æºä¸­è·å–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›çš„è§†é¢‘ç¼–è¾‘æ€§èƒ½æ°´å¹³ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cjeen.github.io/LoraEditPaper">https://cjeen.github.io/LoraEditPaper</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10082v3">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰çš„è§†é¢‘ç¼–è¾‘æŠ€æœ¯å·²ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘æ•ˆæœã€‚ä¸ºæå‡è§†é¢‘ç¼–è¾‘çš„çµæ´»æ€§å’Œå¯æ§æ€§ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ©è†œï¼ˆmask-basedï¼‰çš„LoRAï¼ˆä½ç§©é€‚åº”ï¼ŒLow-Rank Adaptationï¼‰è°ƒå‚æ–¹æ³•ï¼Œé€‚ç”¨äºé¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹çš„çµæ´»è§†é¢‘ç¼–è¾‘ã€‚é€šè¿‡å¼•å…¥é¢å¤–å‚è€ƒå›¾åƒä½œä¸ºè§†è§‰é”šç‚¹ï¼Œè§£å†³äº†æ§åˆ¶éš¾é¢˜ã€‚ä½¿ç”¨æ©è†œé©±åŠ¨çš„LoRAè°ƒå‚ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç¼–è¾‘ç¯å¢ƒä¸­é€‚åº”å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œè§†é¢‘ç¼–è¾‘æ€§èƒ½æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºè§†é¢‘ç¼–è¾‘å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç¼ºä¹ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºåŸºäºæ©è†œçš„LoRAè°ƒå‚æ–¹æ³•ï¼Œç”¨äºé€‚åº”é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œçµæ´»è§†é¢‘ç¼–è¾‘ã€‚</li>
<li>æ–¹æ³•èƒ½ä¿ç•™èƒŒæ™¯åŒºåŸŸï¼Œå®ç°å¯æ§çš„ç¼–è¾‘ä¼ æ’­ã€‚</li>
<li>æ–¹æ³•é«˜æ•ˆä¸”å¯é€‚åº”è§†é¢‘ç¼–è¾‘ï¼Œæ— éœ€æ”¹å˜æ¨¡å‹æ¶æ„ã€‚</li>
<li>å¼•å…¥é¢å¤–å‚è€ƒå›¾åƒä½œä¸ºè§†è§‰é”šç‚¹ï¼Œä»¥æ›´å¥½åœ°å¼•å¯¼ç¼–è¾‘è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cc13fd847c861d1247b3e715ed157ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53ab42514b300b74be0259d543e6d0e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ded00efc85abcdf453a2a70ff969270.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TeViR-Text-to-Video-Reward-with-Diffusion-Models-for-Efficient-Reinforcement-Learning"><a href="#TeViR-Text-to-Video-Reward-with-Diffusion-Models-for-Efficient-Reinforcement-Learning" class="headerlink" title="TeViR: Text-to-Video Reward with Diffusion Models for Efficient   Reinforcement Learning"></a>TeViR: Text-to-Video Reward with Diffusion Models for Efficient   Reinforcement Learning</h2><p><strong>Authors:Yuhui Chen, Haoran Li, Zhennan Jiang, Haowei Wen, Dongbin Zhao</strong></p>
<p>Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViRâ€™s ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation. </p>
<blockquote>
<p>å¼€å‘å¯æ‰©å±•ä¸”å¯é€šç”¨çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¥–åŠ±å·¥ç¨‹å¯¹äºåˆ›å»ºé€šç”¨æ™ºèƒ½ä½“è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæ“çºµç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸä¸­ã€‚è™½ç„¶æœ€è¿‘ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¥–åŠ±å·¥ç¨‹è¿›å±•æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†å…¶ç¨€ç–å¥–åŠ±çš„ç‰¹æ€§æå¤§åœ°é™åˆ¶äº†æ ·æœ¬æ•ˆç‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•TeViRï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ¯”è¾ƒé¢„æµ‹çš„å›¾åƒåºåˆ—ä¸å½“å‰è§‚å¯Ÿç»“æœæ¥ç”Ÿæˆå¯†é›†å¥–åŠ±ã€‚åœ¨è·¨è¶Š1eä¸ªå¤æ‚æœºå™¨äººä»»åŠ¡çš„å®éªŒä¸­ï¼ŒTeViRçš„è¡¨ç°ä¼˜äºåˆ©ç”¨ç¨€ç–å¥–åŠ±çš„ä¼ ç»Ÿæ–¹æ³•å’Œå…¶å®ƒæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€çœŸå®ç¯å¢ƒå¥–åŠ±çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚TeViRåœ¨å¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆæŒ‡å¯¼æ™ºèƒ½ä½“çš„èƒ½åŠ›çªæ˜¾äº†å…¶åœ¨æœºå™¨äººæ“çºµçš„å¼ºåŒ–å­¦ä¹ åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19769v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬åˆ°è§†é¢‘çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹TeViRï¼Œé€šè¿‡å¯¹æ¯”é¢„æµ‹å›¾åƒåºåˆ—ä¸å½“å‰è§‚æµ‹å€¼æ¥ç”Ÿæˆå¯†é›†å¥–åŠ±ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œåœ¨å¤æ‚çš„æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿä½¿ç”¨ç¨€ç–å¥–åŠ±çš„æ–¹æ³•ä»¥åŠå…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒTeViRåœ¨ä¸ä¾èµ–ç¯å¢ƒçœŸå®å¥–åŠ±çš„æƒ…å†µä¸‹å®ç°äº†æ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚è¿™æ˜¾ç¤ºå‡ºTeViRåœ¨å¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆæŒ‡å¯¼æ™ºèƒ½ä½“çš„æ½œåŠ›ï¼Œæœ‰æœ›æ¨åŠ¨æœºå™¨äººåœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeViRåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯†é›†å¥–åŠ±ã€‚</li>
<li>å¯¹æ¯”é¢„æµ‹å›¾åƒåºåˆ—ä¸å½“å‰è§‚æµ‹å€¼ä»¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚</li>
<li>åœ¨11ä¸ªå¤æ‚çš„æœºå™¨äººä»»åŠ¡ä¸­ï¼ŒTeViRè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>TeViRä¸ä¼ ç»Ÿæ–¹æ³•å’Œå…¶å®ƒå…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´é«˜çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>TeViRåœ¨ä¸ä¾èµ–ç¯å¢ƒçœŸå®å¥–åŠ±çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>TeViRèƒ½æœ‰æ•ˆæŒ‡å¯¼æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5bc80ee40fa5e2d18c57439b67ca5a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b0a1279713be6f89fe1730b0b6a73e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fafd6c1ec1074ccce026759b2f88d21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db9afc65dc801644665ef868913bfd6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d17c76b4aa85c00e2d038234c3f0097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c516c2723aa70e88c451d1fdb546c53.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Privacy-Attacks-on-Image-AutoRegressive-Models"><a href="#Privacy-Attacks-on-Image-AutoRegressive-Models" class="headerlink" title="Privacy Attacks on Image AutoRegressive Models"></a>Privacy Attacks on Image AutoRegressive Models</h2><p><strong>Authors:Antoni Kowalczuk, Jan DubiÅ„ski, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Image AutoRegressive generation has emerged as a new powerful paradigm with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns regarding their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to the ones of DMs as reference points. Concretely, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images (with a True Positive Rate at False Positive Rate &#x3D; 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our novel MIA to provide dataset inference (DI) for IARs, and show that it requires as few as 6 samples to detect dataset membership (compared to 200 for DI in DMs), confirming a higher information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. We release the code at <a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars">https://github.com/sprintml/privacy_attacks_against_iars</a> for reproducibility. </p>
<blockquote>
<p>å›¾åƒè‡ªå›å½’ç”Ÿæˆå·²ç»æˆä¸ºä¸€ç§æ–°å‹å¼ºå¤§çš„èŒƒå¼ï¼Œå›¾åƒè‡ªå›å½’æ¨¡å‹ï¼ˆIARï¼‰åœ¨å›¾åƒè´¨é‡æ–¹é¢ä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç›¸åŒ¹é…ï¼ˆFIDï¼š1.48ä¸DMçš„FIDä¸º1.58ï¼‰ï¼ŒåŒæ—¶ç”Ÿæˆé€Ÿåº¦æ›´å¿«ã€‚ç„¶è€Œï¼Œä¸IARç›¸å…³çš„éšç§é£é™©å°šæœªå¾—åˆ°æ¢ç´¢ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹å®ƒä»¬è´Ÿè´£éƒ¨ç½²çš„æ‹…å¿§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹IARè¿›è¡Œäº†å…¨é¢çš„éšç§åˆ†æï¼Œå¹¶å°†å…¶éšç§é£é™©ä¸DMä½œä¸ºå‚è€ƒç‚¹è¿›è¡Œæ¯”è¾ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œåœ¨æ£€æµ‹è®­ç»ƒå›¾åƒæ–¹é¢å–å¾—äº†éå¸¸é«˜çš„æˆåŠŸç‡ï¼ˆåœ¨å‡é˜³æ€§ç‡ä¸ºç™¾åˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ï¼ŒçœŸæ­£é˜³æ€§ç‡ä¸ºç™¾åˆ†ä¹‹å…«åå…­ç‚¹ä¸‰å…«ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒDMé¢ä¸´ç±»ä¼¼æ”»å‡»æ—¶çš„çœŸæ­£é˜³æ€§ç‡ä¸ºç™¾åˆ†ä¹‹å…­ç‚¹ä¸‰å…«ï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨æ–°å‹MIAä¸ºIARæä¾›æ•°æ®é›†æ¨ç†ï¼ˆDIï¼‰ï¼Œå¹¶è¡¨æ˜ä»…éœ€å…­ä¸ªæ ·æœ¬å³å¯æ£€æµ‹æ•°æ®é›†æˆå‘˜èº«ä»½ï¼ˆç›¸æ¯”ä¹‹ä¸‹ï¼ŒDMä¸­çš„DIéœ€è¦äºŒç™¾ä¸ªæ ·æœ¬ï¼‰ï¼Œè¯å®äº†IARå­˜åœ¨æ›´é«˜çš„ä¿¡æ¯æ³„éœ²ã€‚æœ€åï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»ä¸€ä¸ªIARä¸­æå–æ•°ç™¾ä¸ªè®­ç»ƒæ•°æ®ç‚¹ï¼ˆä¾‹å¦‚ä»VAR-d30ä¸­æå–äº†å…­ç™¾ä¹åå…«ä¸ªï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜å­˜åœ¨ä¸€ä¸ªåŸºæœ¬çš„éšç§æ•ˆç”¨æƒè¡¡ï¼šè™½ç„¶IARåœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸å®ç°ç±»ä¼¼æ€§èƒ½çš„DMç›¸æ¯”ï¼Œå®ƒä»¬åœ¨å®è¯ä¸Šæ›´å®¹æ˜“å—åˆ°éšç§æ”»å‡»ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç äº<a target="_blank" rel="noopener" href="https://github.com/sprintml/privacy_attacks_against_iars%E4%BB%A5%E4%BF%83%E8%BF%9B%E9%87%8D%E5%A4%8D%E5%AE%9E%E9%AA%8C%E3%80%82">https://github.com/sprintml/privacy_attacks_against_iarsä»¥ä¿ƒè¿›é‡å¤å®éªŒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02514v4">PDF</a> Accepted at ICML2025</p>
<p><strong>Summary</strong><br>     å›¾åƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼ˆIARsï¼‰ä½œä¸ºä¸€ç§æ–°å…´çš„å¼ºå¤§èŒƒå¼ï¼Œåœ¨å›¾åƒè´¨é‡ä¸Šä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç›¸åŒ¹é…ï¼ŒåŒæ—¶ç”Ÿæˆé€Ÿåº¦æ›´å¿«ã€‚ç„¶è€Œï¼ŒIARsçš„éšç§é£é™©å°šæœªè¢«æ¢ç´¢ï¼Œè¿™å¼•å‘äº†å¯¹å…¶è´Ÿè´£ä»»éƒ¨ç½²çš„æ‹…å¿§ã€‚æœ¬ç ”ç©¶å¯¹IARsè¿›è¡Œäº†å…¨é¢çš„éšç§åˆ†æï¼Œå¹¶ä¸DMsè¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ï¼Œåœ¨æ£€æµ‹è®­ç»ƒå›¾åƒæ–¹é¢å–å¾—äº†éå¸¸é«˜çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¯å®IARsçš„ä¿¡æ¯æ³„éœ²æ›´é«˜ï¼Œåªéœ€å°‘æ•°æ ·æœ¬å³å¯è¿›è¡Œæ•°æ®é›†æ¨æ–­ï¼ˆDIï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»IARä¸­æå–å¤§é‡è®­ç»ƒæ•°æ®ç‚¹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒIARsåœ¨éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å¼±ç‚¹ï¼Œå°½ç®¡å®ƒä»¬åœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè‡ªå›å½’ç”Ÿæˆæ¨¡å‹ï¼ˆIARsï¼‰ä¸æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒè´¨é‡ä¸Šè¡¨ç°ç›¸è¿‘ï¼Œä½†ç”Ÿæˆé€Ÿåº¦æ›´å¿«ã€‚</li>
<li>IARsçš„éšç§é£é™©å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œéœ€è¦è¿›è¡Œå…¨é¢çš„éšç§åˆ†æã€‚</li>
<li>æ–°å‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰åœ¨æ£€æµ‹IARsçš„è®­ç»ƒå›¾åƒæ–¹é¢å–å¾—äº†éå¸¸é«˜çš„æˆåŠŸç‡ã€‚</li>
<li>IARsçš„ä¿¡æ¯æ³„éœ²è¾ƒé«˜ï¼Œåªéœ€å°‘æ•°æ ·æœ¬å³å¯è¿›è¡Œæ•°æ®é›†æ¨æ–­ï¼ˆDIï¼‰ã€‚</li>
<li>IARsåœ¨éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å¼±ç‚¹ï¼Œç›¸æ¯”DMsæ›´å®¹æ˜“å—åˆ°éšç§æ”»å‡»ã€‚</li>
<li>ç ”ç©¶ç»“æœæ­ç¤ºäº†éšç§ä¿æŠ¤ä¸å®ç”¨æ€§ä¹‹é—´çš„æƒè¡¡ï¼šå°½ç®¡IARsåœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éšç§ä¿æŠ¤æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eca718a239b80885f373c6ec79c78b06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122fd4f01453a183d838e675be1b6120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d0cb6a712a77e853edb15ee286ed654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f33297bccb9fe69551b889d197775ed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ff31cad1db1971c97b5d74f7f04be75.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MIFNet-Learning-Modality-Invariant-Features-for-Generalizable-Multimodal-Image-Matching"><a href="#MIFNet-Learning-Modality-Invariant-Features-for-Generalizable-Multimodal-Image-Matching" class="headerlink" title="MIFNet: Learning Modality-Invariant Features for Generalizable   Multimodal Image Matching"></a>MIFNet: Learning Modality-Invariant Features for Generalizable   Multimodal Image Matching</h2><p><strong>Authors:Yepeng Liu, Zhichao Sun, Baosheng Yu, Yitian Zhao, Bo Du, Yongchao Xu, Jun Cheng</strong></p>
<p>Many keypoint detection and description methods have been proposed for image matching or registration. While these methods demonstrate promising performance for single-modality image matching, they often struggle with multimodal data because the descriptors trained on single-modality data tend to lack robustness against the non-linear variations present in multimodal data. Extending such methods to multimodal image matching often requires well-aligned multimodal data to learn modality-invariant descriptors. However, acquiring such data is often costly and impractical in many real-world scenarios. To address this challenge, we propose a modality-invariant feature learning network (MIFNet) to compute modality-invariant features for keypoint descriptions in multimodal image matching using only single-modality training data. Specifically, we propose a novel latent feature aggregation module and a cumulative hybrid aggregation module to enhance the base keypoint descriptors trained on single-modality data by leveraging pre-trained features from Stable Diffusion models. %, our approach generates robust and invariant features across diverse and unknown modalities. We validate our method with recent keypoint detection and description methods in three multimodal retinal image datasets (CF-FA, CF-OCT, EMA-OCTA) and two remote sensing datasets (Optical-SAR and Optical-NIR). Extensive experiments demonstrate that the proposed MIFNet is able to learn modality-invariant feature for multimodal image matching without accessing the targeted modality and has good zero-shot generalization ability. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/lyp-deeplearning/MIFNet">https://github.com/lyp-deeplearning/MIFNet</a>. </p>
<blockquote>
<p>é’ˆå¯¹å›¾åƒåŒ¹é…æˆ–æ³¨å†Œï¼Œå·²ç»æå‡ºäº†è®¸å¤šå…³é”®ç‚¹æ£€æµ‹ä¸æè¿°æ–¹æ³•ã€‚è™½ç„¶è¿™äº›æ–¹æ³•åœ¨å•æ¨¡æ€å›¾åƒåŒ¹é…ä¸Šè¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œå› ä¸ºé‚£äº›åœ¨å•æ¨¡æ€æ•°æ®ä¸Šè®­ç»ƒçš„æè¿°ç¬¦å¾€å¾€ç¼ºä¹å¯¹å¤šæ¨¡æ€æ•°æ®ä¸­å­˜åœ¨çš„éçº¿æ€§å˜å¼‚çš„ç¨³å¥æ€§ã€‚å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤šæ¨¡æ€å›¾åƒåŒ¹é…é€šå¸¸éœ€è¦å¯¹é½è‰¯å¥½çš„å¤šæ¨¡æ€æ•°æ®æ¥å­¦ä¹ æ¨¡æ€ä¸å˜æè¿°ç¬¦ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šçœŸå®åœºæ™¯ä¸­ï¼Œè·å–è¿™æ ·çš„æ•°æ®é€šå¸¸æˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ€ä¸å˜ç‰¹å¾å­¦ä¹ ç½‘ç»œï¼ˆMIFNetï¼‰ï¼Œè¯¥ç½‘ç»œä»…ä½¿ç”¨å•æ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œä¸ºå¤šæ¨¡æ€å›¾åƒåŒ¹é…ä¸­çš„å…³é”®ç‚¹æè¿°è®¡ç®—æ¨¡æ€ä¸å˜ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ½œåœ¨ç‰¹å¾èšåˆæ¨¡å—å’Œç´¯ç§¯æ··åˆèšåˆæ¨¡å—ï¼Œä»¥åˆ©ç”¨æ¥è‡ªStable Diffusionæ¨¡å‹çš„é¢„è®­ç»ƒç‰¹å¾ï¼Œå¢å¼ºåœ¨å•æ¨¡æ€æ•°æ®ä¸Šè®­ç»ƒçš„åŸºå‡†å…³é”®ç‚¹æè¿°ç¬¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ ·ä¸”æœªçŸ¥çš„æ¨¡å¼ä¸‹ç”Ÿæˆç¨³å¥ä¸”ä¸å˜çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€è§†ç½‘è†œå›¾åƒæ•°æ®é›†ï¼ˆCF-FAã€CF-OCTã€EMA-OCTAï¼‰å’Œä¸¤ä¸ªé¥æ„Ÿæ•°æ®é›†ï¼ˆOptical-SARå’ŒOptical-NIRï¼‰ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€æ–°çš„å…³é”®ç‚¹æ£€æµ‹å’Œæè¿°æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MIFNetèƒ½å¤Ÿåœ¨ä¸è®¿é—®ç›®æ ‡æ¨¡æ€çš„æƒ…å†µä¸‹å­¦ä¹ æ¨¡æ€ä¸å˜ç‰¹å¾ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lyp-deeplearning/MIFNet">https://github.com/lyp-deeplearning/MIFNet</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11299v3">PDF</a> Accept by IEEE TIP 2025</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå¤šæ¨¡æ€å›¾åƒåŒ¹é…æŒ‘æˆ˜ï¼Œæå‡ºäº†æ¨¡æ€æ— å…³ç‰¹å¾å­¦ä¹ ç½‘ç»œï¼ˆMIFNetï¼‰ï¼Œç”¨äºè®¡ç®—å…³é”®ç‚¹çš„æ¨¡æ€æ— å…³ç‰¹å¾æè¿°ã€‚é€šè¿‡ä½¿ç”¨ä»…å•æ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œå¢å¼ºåŸºäºå•æ¨¡æ€æ•°æ®è®­ç»ƒçš„åŸºå‡†å…³é”®ç‚¹æè¿°ç¬¦ï¼Œå¹¶å€ŸåŠ©é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼ŒMIFNetåœ¨å¤šç§æ¨¡æ€å›¾åƒåŒ¹é…ä¸­å…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€è®¿é—®ç›®æ ‡æ¨¡æ€å³å¯å­¦ä¹ æ¨¡æ€æ— å…³ç‰¹å¾ã€‚ä»£ç å°†å‘å¸ƒåœ¨ç›¸åº”é“¾æ¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>é’ˆå¯¹å¤šæ¨¡æ€å›¾åƒåŒ¹é…ï¼Œç°æœ‰çš„åŸºäºå•æ¨¡æ€çš„æ–¹æ³•é€šå¸¸ç¼ºä¹ç¨³å¥æ€§ä»¥åº”å¯¹éçº¿æ€§çš„å¤šæ¨¡æ€æ•°æ®å˜åŒ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡æ€æ— å…³ç‰¹å¾å­¦ä¹ ç½‘ç»œï¼ˆMIFNetï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å›¾åƒåŒ¹é…ä¸­çš„å…³é”®é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ½œåœ¨ç‰¹å¾èšåˆæ¨¡å—å’Œç´¯ç§¯æ··åˆèšåˆæ¨¡å—ï¼Œå¢å¼ºåŸºäºå•æ¨¡æ€æ•°æ®è®­ç»ƒçš„åŸºå‡†å…³é”®ç‚¹æè¿°ç¬¦ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç‰¹å¾è¿›ä¸€æ­¥å¼ºåŒ–äº†æè¿°å­ã€‚</li>
<li>MIFNetå…·æœ‰è‰¯å¥½çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨ä¸æ¥è§¦ç›®æ ‡æ¨¡æ€çš„æƒ…å†µä¸‹å­¦ä¹ æ¨¡æ€æ— å…³ç‰¹å¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7adb7fabbb3bf304efc945465dae4d1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b23b0417fbc9c7d57258fc22b14d9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9301b36cd8ac02cc2062bcc8adb298ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a83d029a462456056eed4b9a0746dcc.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TD-Paint-Faster-Diffusion-Inpainting-Through-Time-Aware-Pixel-Conditioning"><a href="#TD-Paint-Faster-Diffusion-Inpainting-Through-Time-Aware-Pixel-Conditioning" class="headerlink" title="TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel   Conditioning"></a>TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel   Conditioning</h2><p><strong>Authors:Tsiry Mayet, Pourya Shamsolmoali, Simon Bernard, Eric Granger, Romain HÃ©rault, Clement Chatelain</strong></p>
<p>Diffusion models have emerged as highly effective techniques for inpainting, however, they remain constrained by slow sampling rates. While recent advances have enhanced generation quality, they have also increased sampling time, thereby limiting scalability in real-world applications. We investigate the generative sampling process of diffusion-based inpainting models and observe that these models make minimal use of the input condition during the initial sampling steps. As a result, the sampling trajectory deviates from the data manifold, requiring complex synchronization mechanisms to realign the generation process. To address this, we propose Time-aware Diffusion Paint (TD-Paint), a novel approach that adapts the diffusion process by modeling variable noise levels at the pixel level. This technique allows the model to efficiently use known pixel values from the start, guiding the generation process toward the target manifold. By embedding this information early in the diffusion process, TD-Paint significantly accelerates sampling without compromising image quality. Unlike conventional diffusion-based inpainting models, which require a dedicated architecture or an expensive generation loop, TD-Paint achieves faster sampling times without architectural modifications. Experimental results across three datasets show that TD-Paint outperforms state-of-the-art diffusion models while maintaining lower complexity. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»åœ¨å›¾åƒä¿®å¤ä¸­å±•ç°å‡ºé«˜æ•ˆçš„æŠ€æœ¯ç‰¹æ€§ï¼Œç„¶è€Œå®ƒä»¬ä»ç„¶å—åˆ°é‡‡æ ·é€Ÿç‡è¾ƒæ…¢çš„é™åˆ¶ã€‚å°½ç®¡æœ€è¿‘çš„è¿›å±•æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œä½†ä¹Ÿå¢åŠ äº†é‡‡æ ·æ—¶é—´ï¼Œä»è€Œé™åˆ¶äº†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æ¨¡å‹çš„ç”Ÿæˆé‡‡æ ·è¿‡ç¨‹ï¼Œå¹¶è§‚å¯Ÿåˆ°è¿™äº›æ¨¡å‹åœ¨åˆå§‹é‡‡æ ·æ­¥éª¤ä¸­å¯¹è¾“å…¥æ¡ä»¶çš„åˆ©ç”¨å¾®ä¹å…¶å¾®ã€‚å› æ­¤ï¼Œé‡‡æ ·è½¨è¿¹åç¦»äº†æ•°æ®æµå½¢ï¼Œéœ€è¦å¤æ‚çš„åŒæ­¥æœºåˆ¶æ¥é‡æ–°å¯¹é½ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´æ„ŸçŸ¥æ‰©æ•£ç»˜å›¾ï¼ˆTD-Paintï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åƒç´ çº§å¯å˜å™ªå£°æ°´å¹³å¯¹æ‰©æ•£è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡çš„æ–°æ–¹æ³•ã€‚è¯¥æŠ€æœ¯å…è®¸æ¨¡å‹ä»èµ·å§‹é˜¶æ®µå°±æœ‰æ•ˆåœ°ä½¿ç”¨å·²çŸ¥çš„åƒç´ å€¼ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœç€ç›®æ ‡æµå½¢è¿›è¡Œã€‚é€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å°½æ—©åµŒå…¥è¿™äº›ä¿¡æ¯ï¼ŒTD-Paintå¯ä»¥åœ¨ä¸æŸå®³å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—åŠ é€Ÿé‡‡æ ·ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æ¨¡å‹ä¸åŒï¼ŒTD-Paintæ— éœ€ä¸“é—¨çš„æ¶æ„æˆ–æ˜‚è´µçš„ç”Ÿæˆå¾ªç¯ï¼Œå³å¯å®ç°æ›´å¿«çš„é‡‡æ ·é€Ÿåº¦ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTD-Paintåœ¨ä¿æŒè¾ƒä½å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09306v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤ä¸­è¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼Œä½†é‡‡æ ·é€Ÿç‡è¾ƒæ…¢é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯æ‰©å±•æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç©¶äº†åŸºäºæ‰©æ•£çš„ä¿®å¤æ¨¡å‹çš„ç”Ÿæˆé‡‡æ ·è¿‡ç¨‹ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨åˆå§‹é‡‡æ ·æ­¥éª¤ä¸­å¯¹è¾“å…¥æ¡ä»¶çš„åˆ©ç”¨ç”šå¾®ï¼Œå¯¼è‡´é‡‡æ ·è½¨è¿¹åç¦»æ•°æ®æµå½¢ã€‚ä¸ºæ­¤ï¼Œå›¢é˜Ÿæå‡ºäº†æ—¶é—´æ„ŸçŸ¥æ‰©æ•£æ¶‚æ–™ï¼ˆTD-Paintï¼‰æ–¹æ³•ï¼Œé€šè¿‡åƒç´ çº§å¯å˜å™ªå£°æ°´å¹³å¯¹æ‰©æ•£è¿‡ç¨‹è¿›è¡Œé€‚åº”ã€‚æ­¤æ–¹æ³•ä½¿æ¨¡å‹ä»ä¸€å¼€å§‹å°±èƒ½æœ‰æ•ˆåˆ©ç”¨å·²çŸ¥åƒç´ å€¼ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœå‘ç›®æ ‡æµå½¢ã€‚å°†ä¿¡æ¯åµŒå…¥æ‰©æ•£è¿‡ç¨‹çš„æ—©æœŸé˜¶æ®µï¼ŒTD-Paintå¯æ˜¾è‘—åŠ é€Ÿé‡‡æ ·è€Œä¸æŸå®³å›¾åƒè´¨é‡ã€‚ä¸éœ€è¦ä¸“é—¨æ¶æ„æˆ–æ˜‚è´µç”Ÿæˆå¾ªç¯çš„ä¼ ç»Ÿæ‰©æ•£ä¿®å¤æ¨¡å‹ä¸åŒï¼ŒTD-Paintåœ¨ä¸è¿›è¡Œæ¶æ„ä¿®æ”¹çš„æƒ…å†µä¸‹å®ç°äº†æ›´å¿«çš„é‡‡æ ·æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤ä¸­è¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼Œä½†é‡‡æ ·é€Ÿç‡è¾ƒæ…¢ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨åˆå§‹é‡‡æ ·é˜¶æ®µå¯¹è¾“å…¥æ¡ä»¶çš„åˆ©ç”¨ä¸è¶³ï¼Œå¯¼è‡´é‡‡æ ·è½¨è¿¹åç¦»æ•°æ®æµå½¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•TD-Paintï¼Œé€šè¿‡åƒç´ çº§çš„å¯å˜å™ªå£°æ°´å¹³é€‚åº”æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>TD-Paintå…è®¸æ¨¡å‹ä»å¼€å§‹å°±æœ‰æ•ˆåˆ©ç”¨å·²çŸ¥åƒç´ å€¼ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹å‘ç›®æ ‡æµå½¢ã€‚</li>
<li>TD-Paintå°†ä¿¡æ¯åµŒå…¥æ‰©æ•£è¿‡ç¨‹çš„æ—©æœŸé˜¶æ®µï¼Œèƒ½æ˜¾è‘—åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>TD-Paintåœ¨ä¸è¿›è¡Œæ¶æ„ä¿®æ”¹çš„æƒ…å†µä¸‹å®ç°äº†æ›´å¿«çš„é‡‡æ ·æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a5a3e284a05e99c1f6834a35340ec313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2c0cf3eab9eaaa336b482b9e7ebde91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52a64c87d2eaad9ba6d1aee47d40fee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d7992d862b725d546e4cef4d7794473.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ReconX-Reconstruct-Any-Scene-from-Sparse-Views-with-Video-Diffusion-Model"><a href="#ReconX-Reconstruct-Any-Scene-from-Sparse-Views-with-Video-Diffusion-Model" class="headerlink" title="ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion   Model"></a>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion   Model</h2><p><strong>Authors:Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, Yueqi Duan</strong></p>
<p>Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from insufficient captured views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. However, 3D view consistency struggles to be accurately preserved in directly generated video frames from pre-trained models. To address this, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are both detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of our ReconX over state-of-the-art methods in terms of quality and generalizability. </p>
<blockquote>
<p>éšç€ä¸‰ç»´åœºæ™¯é‡å»ºæŠ€æœ¯çš„è¿›æ­¥ï¼Œèƒ½å¤Ÿå°†æ¥è‡ªç°å®ä¸–ç•Œçš„äºŒç»´å›¾åƒè½¬åŒ–ä¸ºä¸‰ç»´æ¨¡å‹ï¼Œå¹¶ä»æ•°ç™¾å¼ è¾“å…¥ç…§ç‰‡ä¸­ç”Ÿæˆé€¼çœŸçš„ä¸‰ç»´ç»“æœã€‚è™½ç„¶åœ¨å¯†é›†è§†å›¾é‡å»ºåœºæ™¯ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ä»ä¸è¶³å¤Ÿçš„æ•è·è§†è§’æ¸²æŸ“è¯¦ç»†åœºæ™¯ä»ç„¶æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„ä¼˜åŒ–é—®é¢˜ï¼Œè¿™å¸¸å¸¸å¯¼è‡´åœ¨çœ‹ä¸è§çš„åŒºåŸŸå‡ºç°ä¼ªå½±å’Œå¤±çœŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºReconXçš„æ–°å‹ä¸‰ç»´åœºæ™¯é‡å»ºèŒƒå¼ï¼Œå®ƒå°†æ¨¡ç³Šçš„é‡å»ºæŒ‘æˆ˜é‡æ–°æ„å»ºä¸ºæ—¶é—´ç”Ÿæˆä»»åŠ¡ï¼Œå…¶å…³é”®è§è§£æ˜¯é‡Šæ”¾å¤§å‹é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒï¼Œç”¨äºç¨€ç–è§†å›¾é‡å»ºã€‚ç„¶è€Œï¼Œç›´æ¥ä»é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„è§†é¢‘å¸§ä¸­å‡†ç¡®ä¿æŒä¸‰ç»´è§†å›¾çš„ä¸€è‡´æ€§æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç»™å®šæœ‰é™çš„è¾“å…¥è§†è§’ï¼Œæ‰€æå‡ºReconXé¦–å…ˆæ„å»ºå…¨å±€ç‚¹äº‘å¹¶å°†å…¶ç¼–ç ä¸ºä¸Šä¸‹æ–‡ç©ºé—´ä½œä¸ºä¸‰ç»´ç»“æ„æ¡ä»¶ã€‚åœ¨æ¡ä»¶çš„å¼•å¯¼ä¸‹ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹éšååˆæˆæ—¢ä¿ç•™ç»†èŠ‚åˆå±•ç°é«˜åº¦ä¸‰ç»´ä¸€è‡´æ€§çš„è§†é¢‘å¸§ï¼Œç¡®ä¿ä»ä¸åŒè§’åº¦çš„åœºæ™¯è¿è´¯æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä¿¡å¿ƒæ„ŸçŸ¥çš„ä¸‰ç»´é«˜æ–¯æ‹¼è´´ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»ç”Ÿæˆçš„è§†é¢‘ä¸­æ¢å¤ä¸‰ç»´åœºæ™¯ã€‚åœ¨å¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ReconXåœ¨è´¨é‡å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16767v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://liuff19.github.io/ReconX">https://liuff19.github.io/ReconX</a></p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ–°å‹çš„ä¸‰ç»´åœºæ™¯é‡å»ºæ–¹æ³•ReconXï¼Œå°†ç¨€ç–è§†è§’ä¸‹çš„é‡å»ºæŒ‘æˆ˜è½¬åŒ–ä¸ºæ—¶é—´ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å¼ºç”Ÿæˆå…ˆéªŒã€‚é€šè¿‡æ„å»ºå…¨å±€ç‚¹äº‘å¹¶å°†å…¶ç¼–ç ä¸ºä¸‰ç»´ç»“æ„æ¡ä»¶ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆç»†èŠ‚ä¸°å¯Œä¸”ä¸‰ç»´ä¸€è‡´æ€§é«˜çš„è§†é¢‘å¸§ã€‚æœ€åï¼Œé€šè¿‡ç½®ä¿¡åº¦æ„ŸçŸ¥çš„3Dé«˜æ–¯æ‹¼è´´ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»ç”Ÿæˆçš„è§†é¢‘ä¸­æ¢å¤ä¸‰ç»´åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´åœºæ™¯é‡å»ºæ–¹æ³•ReconXï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒæ¥è§£å†³ç¨€ç–è§†è§’ä¸‹çš„é‡å»ºé—®é¢˜ã€‚</li>
<li>ReconXå°†é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºæ—¶é—´ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡æ„å»ºå…¨å±€ç‚¹äº‘å¹¶å°†å…¶ç¼–ç ä¸ºä¸‰ç»´ç»“æ„æ¡ä»¶ï¼Œä»¥æé«˜ç”Ÿæˆè§†é¢‘å¸§çš„è¯¦ç»†æ€§å’Œä¸‰ç»´ä¸€è‡´æ€§ã€‚</li>
<li>ä¸ºäº†ç¡®ä¿åœºæ™¯ä»ä¸åŒè§’åº¦çš„ä¸€è‡´æ€§ï¼Œè®ºæ–‡ä½¿ç”¨äº†æ¡ä»¶å¼•å¯¼çš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥åˆæˆè§†é¢‘å¸§ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„3Dé«˜æ–¯æ‹¼è´´ä¼˜åŒ–æ–¹æ¡ˆä»ç”Ÿæˆçš„è§†é¢‘ä¸­æ¢å¤ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒReconXåœ¨è´¨é‡å’Œæ³›åŒ–æ€§æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„é‡å»ºæ–¹æ³•ã€‚</li>
<li>ReconXèƒ½å¤„ç†ä»ä¸åŒè§†è§’æ‹æ‘„çš„ä¸è¶³ç…§ç‰‡ï¼Œå¹¶é€šè¿‡æ„å»ºå…¨å±€ç‚¹äº‘æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf77641e409cd62cc3cd64e005c1c5be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ab4d17415ddae8d6a44b1f709ce86c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3446048336245ce217c90c61105a9a85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5acf6435c1553fdebcecaa20b1dd498.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac51fc81edf20af2945cf8fd06482003.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="IgCONDA-PET-Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-â€“-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study"><a href="#IgCONDA-PET-Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-â€“-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study" class="headerlink" title="IgCONDA-PET: Weakly-Supervised PET Anomaly Detection using   Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling â€“   a Multi-Center, Multi-Cancer, and Multi-Tracer Study"></a>IgCONDA-PET: Weakly-Supervised PET Anomaly Detection using   Implicitly-Guided Attention-Conditional Counterfactual Diffusion Modeling â€“   a Multi-Center, Multi-Cancer, and Multi-Tracer Study</h2><p><strong>Authors:Shadab Ahamed, Arman Rahmim</strong></p>
<p>Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current unsupervised or weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks (GANs) trained only on healthy data. While these approaches reduce annotation dependency, GAN-based methods are notably more challenging to train than non-GAN alternatives (such as autoencoders) due to issues such as the simultaneous optimization of two competing networks, mode collapse, and training instability. In this paper, we present the weakly-supervised $\textbf{I}$mplicitly-$\textbf{g}$uided $\textbf{CO}$u$\textbf{N}$terfactual diffusion model for $\textbf{D}$etecting $\textbf{A}$nomalies in $\textbf{PET}$ images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases (multi-cancer, multi-tracer) containing both local and public datasets (spanning multiple centers). The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates â€œunhealthy-to-healthyâ€ domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUV$_\text{max}$ thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a>. </p>
<blockquote>
<p>å‡å°‘è®­ç»ƒPETç—…å˜æ£€æµ‹ä¸åˆ†å‰²ç½‘ç»œæ—¶å¯¹åƒç´ çº§åˆ«æ ‡æ³¨æ•°æ®çš„éœ€æ±‚æ˜¯éå¸¸ç†æƒ³çš„ï¼Œè€ƒè™‘åˆ°ä¸ä¸“å®¶æ ‡æ³¨ç›¸å…³çš„æ—¶é—´å’Œæˆæœ¬çº¦æŸï¼Œè¿™å¯èƒ½ä¼šå¸¦æ¥å˜é©ã€‚å½“å‰çš„æ— ç›‘ç£æˆ–å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–äºè‡ªç¼–ç å™¨æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„è®­ç»ƒï¼Œè¿™äº›ç½‘ç»œä»…åœ¨å¥åº·æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è™½ç„¶è¿™äº›æ–¹æ³•å‡å°‘äº†æ ‡æ³¨çš„ä¾èµ–æ€§ï¼Œä½†åŸºäºGANçš„æ–¹æ³•ä¸åŸºäºéGANçš„æ›¿ä»£æ–¹æ³•ï¼ˆå¦‚è‡ªç¼–ç å™¨ï¼‰ç›¸æ¯”ï¼Œè®­ç»ƒéš¾åº¦æ›´å¤§ï¼Œå› ä¸ºå­˜åœ¨è¯¸å¦‚ä¸¤ä¸ªç«äº‰ç½‘ç»œçš„åŒæ­¥ä¼˜åŒ–ã€æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºPETå›¾åƒæ£€æµ‹å¼‚å¸¸çš„å¼±ç›‘ç£éšå¼å¼•å¯¼å¯¹æ¯”æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ã€‚è¯¥è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æ¥è‡ªå…­ä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„PETæ‰«æè¿›è¡Œå¼€å‘å’ŒéªŒè¯çš„ï¼Œè¿™äº›é˜Ÿåˆ—å…±æœ‰2652ä¸ªç—…ä¾‹ï¼ˆå¤šç™Œç—‡ã€å¤šè¿½è¸ªå‰‚ï¼‰ï¼ŒåŒ…å«æœ¬åœ°å’Œå…¬å…±æ•°æ®é›†ï¼ˆè·¨è¶Šå¤šä¸ªä¸­å¿ƒï¼‰ã€‚è®­ç»ƒæ˜¯é€šè¿‡æ³¨æ„åŠ›æ¨¡å—å¯¹å›¾åƒç±»åˆ«æ ‡ç­¾ï¼ˆå¥åº·ä¸éå¥åº·ï¼‰è¿›è¡Œæ¡ä»¶åŒ–ï¼Œæˆ‘ä»¬é‡‡ç”¨éšå¼æ‰©æ•£æŒ‡å¯¼ã€‚æˆ‘ä»¬æ‰§è¡Œåäº‹å®ç”Ÿæˆï¼Œé€šè¿‡ç”Ÿæˆä¸å¥åº·è¾“å…¥å›¾åƒçš„åˆæˆå¥åº·ç‰ˆæœ¬ï¼Œä¿ƒè¿›â€œä¸å¥åº·åˆ°å¥åº·â€çš„é¢†åŸŸè½¬æ¢ï¼Œé€šè¿‡è®¡ç®—å·®å¼‚æ¥å®ç°å¼‚å¸¸æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸åŸºäºæ·±åº¦å­¦ä¹ çš„å…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„å¦‚SUVmaxé˜ˆå€¼æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†åœ¨ç½‘ç»œä¸­èå…¥æ³¨æ„åŠ›æ¨¡å—æ£€æµ‹å°å¼‚å¸¸çš„é‡è¦æ€§ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00239v3">PDF</a> 48 pages, 13 figures, 4 tables</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„éšå¼å¼•å¯¼æ‰©æ•£æ¨¡å‹IgCONDA-PETï¼Œç”¨äºPETå›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ³¨æ„åŠ›æ¨¡å—å’Œéšå¼æ‰©æ•£æŒ‡å¯¼ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå¥åº·å›¾åƒæ¥æ£€æµ‹å¼‚å¸¸ã€‚åœ¨å¤šä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„PETæ‰«ææ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IgCONDA-PETæ¨¡å‹æ˜¯ä¸€ç§åŸºäºå¼±ç›‘ç£çš„PETå›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå¥åº·å›¾åƒæ¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œè®­ç»ƒï¼Œå¯æ£€æµ‹å°åˆ°å¤§çš„å¼‚å¸¸ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„PETæ‰«ææ•°æ®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬æœ¬åœ°å’Œå…¬å¼€æ•°æ®é›†ã€‚</li>
<li>ä¸å…¶ä»–æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒIgCONDA-PETæ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹ä»£ç å·²å…¬å¼€å¯ç”¨ï¼Œæ–¹ä¾¿è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.00239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4dea5a4badfaf05a351a0942e1c915e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea5879b0416dd4775bc1dc39d0277270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1d0702ab0f1c92d333c9d274c50736.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Diff-Def-Diffusion-Generated-Deformation-Fields-for-Conditional-Atlases"><a href="#Diff-Def-Diffusion-Generated-Deformation-Fields-for-Conditional-Atlases" class="headerlink" title="Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases"></a>Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases</h2><p><strong>Authors:Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin J. Menten, Tamara T. Mueller, Daniel Rueckert</strong></p>
<p>Anatomical atlases are widely used for population studies and analysis. Conditional atlases target a specific sub-population defined via certain conditions, such as demographics or pathologies, and allow for the investigation of fine-grained anatomical differences like morphological changes associated with ageing or disease. Existing approaches use either registration-based methods that are often unable to handle large anatomical variations or generative adversarial models, which are challenging to train since they can suffer from training instabilities. Instead of generating atlases directly in as intensities, we propose using latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. Our approach ensures structural integrity, enhances interpretability and avoids hallucinations that may arise during direct image synthesis by generating this deformation field and regularising it using a neighbourhood of images. We compare our method to several state-of-the-art atlas generation methods using brain MR images from the UK Biobank. Our method generates highly realistic atlases with smooth transformations and high anatomical fidelity, outperforming existing baselines. We demonstrate the quality of these atlases through comprehensive evaluations, including quantitative metrics for anatomical accuracy, perceptual similarity, and qualitative analyses displaying the consistency and realism of the generated atlases. </p>
<blockquote>
<p>è§£å‰–å›¾è°±åœ¨äººç¾¤ç ”ç©¶å’Œåˆ†æä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æ¡ä»¶å›¾è°±é’ˆå¯¹é€šè¿‡ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚äººå£ç»Ÿè®¡å­¦æˆ–ç—…ç†å­¦ï¼‰å®šä¹‰çš„å…·ä½“äºšç¾¤ä½“ï¼Œå¹¶å…è®¸ç ”ç©¶ç²¾ç»†çš„è§£å‰–å·®å¼‚ï¼Œä¾‹å¦‚ä¸è¡°è€æˆ–ç–¾ç—…ç›¸å…³çš„å½¢æ€å˜åŒ–ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨åŸºäºæ³¨å†Œçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•å¤„ç†è¾ƒå¤§çš„è§£å‰–å˜å¼‚ï¼Œæˆ–ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—æ¨¡å‹ï¼Œç”±äºè®­ç»ƒä¸ç¨³å®šæ€§çš„æŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹çš„è®­ç»ƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¹¶ä¸ç›´æ¥ç”Ÿæˆå¼ºåº¦å›¾è°±ï¼Œè€Œæ˜¯å»ºè®®ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå˜å½¢åœºï¼Œè¯¥å˜å½¢åœºå°†ä¸€ä¸ªé€šç”¨äººç¾¤å›¾è°±è½¬å˜ä¸ºä»£è¡¨ç‰¹å®šäºšäººç¾¤çš„å›¾è°±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ç»“æ„çš„å®Œæ•´æ€§ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”é€šè¿‡åœ¨å›¾åƒé‚»åŸŸç”Ÿæˆå¹¶æ­£åˆ™åŒ–è¿™ä¸ªå˜å½¢åœºï¼Œé¿å…äº†åœ¨ç›´æ¥å›¾åƒåˆæˆè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¹»è§‰ã€‚æˆ‘ä»¬ä½¿ç”¨è‹±å›½ç”Ÿç‰©é“¶è¡Œçš„å¤§è„‘MRIå›¾åƒå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸å‡ ç§æœ€å…ˆè¿›çš„å›¾è°±ç”Ÿæˆæ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†é«˜åº¦é€¼çœŸçš„å›¾è°±ï¼Œå…·æœ‰å¹³æ»‘çš„å˜æ¢å’Œé«˜åº¦çš„è§£å‰–ä¿çœŸåº¦ï¼Œè¶…è¿‡äº†ç°æœ‰çš„åŸºçº¿ã€‚æˆ‘ä»¬é€šè¿‡ç»¼åˆè¯„ä¼°è¯æ˜äº†è¿™äº›å›¾è°±çš„è´¨é‡ï¼ŒåŒ…æ‹¬ç”¨äºè§£å‰–ç²¾åº¦çš„å®šé‡æŒ‡æ ‡ã€æ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼Œä»¥åŠæ˜¾ç¤ºç”Ÿæˆå›¾è°±çš„ä¸€è‡´æ€§å’ŒçœŸå®æ€§çš„å®šæ€§åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16776v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç‰¹å®šå­äººç¾¤è§£å‰–å›¾è°±çš„å˜å½¢åœºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å˜æ¢é€šç”¨äººç¾¤å›¾è°±æ¥åæ˜ ç‰¹å®šå­äººç¾¤ç‰¹å¾ï¼Œç¡®ä¿äº†ç»“æ„å®Œæ•´æ€§ã€æé«˜äº†å¯è§£é‡Šæ€§ï¼Œé¿å…äº†ç›´æ¥å›¾åƒåˆæˆä¸­å¯èƒ½å‡ºç°çš„å¹»è§‰ã€‚åœ¨UK Biobankçš„è„‘MRIå›¾åƒä¸Šï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„é«˜åº¦é€¼çœŸçš„å›¾è°±å…·æœ‰å¹³æ»‘å˜æ¢å’Œé«˜è§£å‰–å­¦ä¿çœŸåº¦ï¼Œä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¡ä»¶æ€§å›¾è°±é’ˆå¯¹ç‰¹å®šå­äººç¾¤ï¼Œé€šè¿‡ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚äººå£ç»Ÿè®¡å­¦æˆ–ç—…ç†å­¦ï¼‰è¿›è¡Œç ”ç©¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨éš¾ä»¥å¤„ç†å¤§è§£å‰–å­¦å˜å¼‚æˆ–è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå˜å½¢åœºï¼Œå°†é€šç”¨å›¾è°±è½¬æ¢ä¸ºç‰¹å®šå­äººç¾¤å›¾è°±ã€‚</li>
<li>é€šè¿‡å›¾åƒé‚»åŸŸè¿›è¡Œæ­£åˆ™åŒ–ï¼Œç¡®ä¿ç»“æ„å®Œæ•´æ€§ã€æé«˜å¯è§£é‡Šæ€§å¹¶é¿å…å¹»è§‰ã€‚</li>
<li>åœ¨UK Biobankçš„è„‘MRIå›¾åƒä¸Šè¿›è¡Œäº†æ–¹æ³•æ¯”è¾ƒï¼Œè¡¨ç°å‡ºé«˜åº¦é€¼çœŸã€å¹³æ»‘å˜æ¢å’Œé«˜è§£å‰–å­¦ä¿çœŸåº¦ã€‚</li>
<li>é€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆè§£å‰–å­¦å‡†ç¡®æ€§ã€æ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼‰å’Œå®šæ€§åˆ†æéªŒè¯äº†ç”Ÿæˆå›¾è°±çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fbef8d4215c944bcb4427aa74e5d13bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-408ed9206ccb695f45bd321c900cc6ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a61ea8762649a22436d415d6895230c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c74306708cdd16a8cc8034842e1465a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217b0f532f3520b2fbff502761696f58.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="DeltaSpace-A-Semantic-aligned-Feature-Space-for-Flexible-Text-guided-Image-Editing"><a href="#DeltaSpace-A-Semantic-aligned-Feature-Space-for-Flexible-Text-guided-Image-Editing" class="headerlink" title="DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided   Image Editing"></a>DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided   Image Editing</h2><p><strong>Authors:Yueming Lyu, Kang Zhao, Bo Peng, Huafeng Chen, Yue Jiang, Yingya Zhang, Jing Dong, Caifeng Shan</strong></p>
<p>Text-guided image editing faces significant challenges when considering training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models have been proposed to avoid data collection, but they are limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP textual feature differences during the inference phase. And this design endows DeltaEdit with two advantages: (1) text-free training; (2) generalization to various text prompts for zero-shot inference. Extensive experiments validate the effectiveness and versatility of DeltaEdit with different generative models, including both the GAN model and the diffusion model, in achieving flexible text-guided image editing. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Yueming6568/DeltaEdit">https://github.com/Yueming6568/DeltaEdit</a>. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘åœ¨è€ƒè™‘è®­ç»ƒå’Œæ¨ç†çµæ´»æ€§æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å¤§é‡æ–‡çŒ®æ”¶é›†äº†å¤§é‡çš„å¸¦æ³¨é‡Šçš„å›¾åƒ-æ–‡æœ¬å¯¹æ¥ä»å¤´è®­ç»ƒæ–‡æœ¬æ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œè¿™æ—¢æ˜‚è´µåˆæ— æ•ˆã€‚ä¹‹åï¼Œä¸€äº›åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹çš„æ–¹æ³•è¢«æå‡ºæ¥é¿å…æ•°æ®æ”¶é›†ï¼Œä½†å®ƒä»¬å—é™äºæ–‡æœ¬æç¤ºä¼˜åŒ–æˆ–æ¨ç†æ—¶é—´è¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶å’Œç¡®å®šäº†ä¸€ä¸ªç‰¹å®šç©ºé—´ï¼Œç§°ä¸ºCLIP DeltaSpaceï¼Œåœ¨è¯¥ç©ºé—´ä¸­ï¼Œä¸¤å¼ å›¾åƒä¹‹é—´çš„CLIPè§†è§‰ç‰¹å¾å·®å¼‚ä¸å…¶å¯¹åº”æ–‡æœ¬æè¿°çš„CLIPæ–‡æœ¬ç‰¹å¾å·®å¼‚åœ¨è¯­ä¹‰ä¸Šå¯¹é½ã€‚åŸºäºDeltaSpaceï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºDeltaEditï¼Œå®ƒåœ¨è®­ç»ƒé˜¶æ®µå°†CLIPè§†è§‰ç‰¹å¾å·®å¼‚æ˜ å°„åˆ°ç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´æ–¹å‘ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µä»CLIPæ–‡æœ¬ç‰¹å¾å·®å¼‚é¢„æµ‹æ½œåœ¨ç©ºé—´æ–¹å‘ã€‚è¿™ç§è®¾è®¡èµ‹äºˆäº†DeltaEditä¸¤ä¸ªä¼˜ç‚¹ï¼šï¼ˆ1ï¼‰æ— æ–‡æœ¬è®­ç»ƒï¼›ï¼ˆ2ï¼‰æ³›åŒ–åˆ°å„ç§æ–‡æœ¬æç¤ºè¿›è¡Œé›¶æ ·æœ¬æ¨ç†ã€‚å¤§é‡å®éªŒéªŒè¯äº†DeltaEditåœ¨ä¸åŒç”Ÿæˆæ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼ŒåŒ…æ‹¬GANæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œåœ¨å®ç°çµæ´»çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æ–¹é¢éƒ½æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yueming6568/DeltaEdit%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yueming6568/DeltaEditæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08785v2">PDF</a> 18 pages. arXiv admin note: text overlap with arXiv:2303.06285</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç¼–è¾‘åœ¨è€ƒè™‘è®­ç»ƒå’Œæ¨ç†çµæ´»æ€§æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–‡çŒ®å¤§å¤šæ”¶é›†å¤§é‡çš„å›¾æ–‡é…å¯¹è¿›è¡Œæ–‡æœ¬æ§åˆ¶çš„ç”Ÿæˆæ¨¡å‹è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚è™½æœ‰åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ¥é¿å…æ•°æ®æ”¶é›†çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬å—é™äºæ–‡æœ¬æç¤ºä¼˜åŒ–æˆ–æ¨ç†æ—¶é—´è¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æ¢ç´¢å¹¶ç¡®å®šäº†ä¸€ä¸ªç‰¹å®šç©ºé—´ï¼Œç§°ä¸ºCLIP DeltaSpaceï¼Œå…¶ä¸­CLIPå›¾åƒç‰¹å¾å·®å¼‚ä¸å…¶å¯¹åº”æ–‡æœ¬æè¿°çš„ç‰¹å¾å·®å¼‚è¯­ä¹‰å¯¹é½ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†åä¸ºDeltaEditçš„æ–°æ¡†æ¶ï¼Œåœ¨è®­ç»ƒé˜¶æ®µå°†CLIPè§†è§‰ç‰¹å¾å·®å¼‚æ˜ å°„åˆ°ç”Ÿæˆæ¨¡å‹çš„æ½œåœ¨ç©ºé—´æ–¹å‘ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µé¢„æµ‹CLIPæ–‡æœ¬ç‰¹å¾å·®å¼‚å¯¹åº”çš„æ½œåœ¨ç©ºé—´æ–¹å‘ã€‚æ­¤è®¾è®¡ä½¿DeltaEditå…·æœ‰ä¸¤å¤§ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰æ— éœ€æ–‡æœ¬è®­ç»ƒï¼›ï¼ˆ2ï¼‰èƒ½å¤Ÿé’ˆå¯¹å„ç§æ–‡æœ¬æç¤ºè¿›è¡Œé›¶æ ·æœ¬æ¨ç†ã€‚å®éªŒè¯æ˜ï¼ŒDeltaEditåœ¨ä¸åŒç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬GANæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼‰ä¸Šå‡èƒ½æœ‰æ•ˆå®ç°çµæ´»æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è®­ç»ƒä¸æ¨ç†çš„çµæ´»æ€§æ˜¯æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æ”¶é›†å¤§é‡æ ‡æ³¨çš„å›¾æ–‡é…å¯¹è¿›è¡Œè®­ç»ƒæˆæœ¬é«˜ä¸”æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>CLIP DeltaSpaceè¢«å‘ç°ï¼Œå…¶ä¸­CLIPå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„å·®å¼‚åœ¨æ­¤ç©ºé—´å†…è¯­ä¹‰å¯¹é½ã€‚</li>
<li>åŸºäºDeltaSpaceï¼Œæå‡ºäº†åä¸ºDeltaEditçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾å·®å¼‚ã€‚</li>
<li>DeltaEditå…·æœ‰ä¸¤å¤§ä¼˜åŠ¿ï¼šæ— éœ€æ–‡æœ¬è®­ç»ƒï¼Œå¹¶èƒ½é’ˆå¯¹å¤šç§æ–‡æœ¬æç¤ºè¿›è¡Œé›¶æ ·æœ¬æ¨ç†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.08785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6c773d9f1561ed6dc2d17e44b7f52ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c690ba9055fd0f0bfac8613ed9d8bf82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-658bc69619bb9281ba1d156173f0ff28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72f38b66478f980bf7bf7eea1f295742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9973105ecdc737193a160bea7f2b5fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9c1b6818ad0cdbcd78eed8f8d1287e0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-26/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cfa578afea188c79431b0ace2e72e5f.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  NAADA A Noise-Aware Attention Denoising Autoencoder for Dental   Panoramic Radiographs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-26/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-b4dbe171fcf30dc9ab90f5b9fb9ea69e.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-26  Self-Supervised Multimodal NeRF for Autonomous Driving
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
