<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-30  ConfLUNet Multiple sclerosis lesion instance segmentation in presence   of confluent lesions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2c3ced45756a21dd1f9c2e96f1a3d172.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-30-更新"><a href="#2025-05-30-更新" class="headerlink" title="2025-05-30 更新"></a>2025-05-30 更新</h1><h2 id="ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions"><a href="#ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions" class="headerlink" title="ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions"></a>ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions</h2><p><strong>Authors:Maxence Wynen, Pedro M. Gordaliza, Maxime Istasse, Anna Stölting, Pietro Maggi, Benoît Macq, Meritxell Bach Cuadra</strong></p>
<p>Accurate lesion-level segmentation on MRI is critical for multiple sclerosis (MS) diagnosis, prognosis, and disease monitoring. However, current evaluation practices largely rely on semantic segmentation post-processed with connected components (CC), which cannot separate confluent lesions (aggregates of confluent lesion units, CLUs) due to reliance on spatial connectivity. To address this misalignment with clinical needs, we introduce formal definitions of CLUs and associated CLU-aware detection metrics, and include them in an exhaustive instance segmentation evaluation framework. Within this framework, we systematically evaluate CC and post-processing-based Automated Confluent Splitting (ACLS), the only existing methods for lesion instance segmentation in MS. Our analysis reveals that CC consistently underestimates CLU counts, while ACLS tends to oversplit lesions, leading to overestimated lesion counts and reduced precision. To overcome these limitations, we propose ConfLUNet, the first end-to-end instance segmentation framework for MS lesions. ConfLUNet jointly optimizes lesion detection and delineation from a single FLAIR image. Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the held-out test set (n&#x3D;13) in instance segmentation (Panoptic Quality: 42.0% vs. 37.5%&#x2F;36.8%; p &#x3D; 0.017&#x2F;0.005) and lesion detection (F1: 67.3% vs. 61.6%&#x2F;59.9%; p &#x3D; 0.028&#x2F;0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU] (81.5%), improving recall over CC (+12.5%, p &#x3D; 0.015) and precision over ACLS (+31.2%, p &#x3D; 0.003). By combining rigorous definitions, new CLU-aware metrics, a reproducible evaluation framework, and the first dedicated end-to-end model, this work lays the foundation for lesion instance segmentation in MS. </p>
<blockquote>
<p>在核磁共振成像（MRI）上进行准确的病灶级别分割对于多发性硬化症（MS）的诊断、预后和疾病监测至关重要。然而，当前的评估方法主要依赖于通过连通组件（CC）进行后处理的语义分割，由于依赖空间连通性，它们无法分离融合病灶（融合病灶单位（CLU）的聚集体）。为了解决这一与临床需求的不符，我们引入了CLU的正式定义和相关CLU感知检测指标，并将它们纳入详尽的实例分割评估框架。在此框架内，我们系统地评估了基于连通组件（CC）和后处理自动融合分割（ACLS）的方法，这是多发性硬化症中病灶实例分割的现有唯一方法。我们的分析表明，CC始终低估CLU计数，而ACLS倾向于过分割病灶，导致病灶计数过高，精度降低。为了克服这些局限性，我们提出了ConfLUNet，这是多发性硬化症病灶实例分割的第一个端到端框架。ConfLUNet从单个FLAIR图像中联合优化病灶检测和轮廓描绘。在50名患者上进行训练后，ConfLUNet在实例分割（泛全景质量：42.0%对比37.5%&#x2F;36.8%，p&#x3D;0.017&#x2F;0.005）和病灶检测（F1：67.3%对比61.6%&#x2F;59.9%，p&#x3D;0.028&#x2F;0.013）方面显著优于CC和ACLS。对于CLU检测，ConfLUNet达到了最高的F1[CLU]（81.5%），在召回率上超过了CC（+12.5%，p&#x3D;0.015），并在精确度上超过了ACLS（+31.2%，p&#x3D;0.003）。通过结合严格定义、新的CLU感知指标、可重现的评估框架和第一个专用端到端模型，这项工作为多发性硬化症中的病灶实例分割奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22537v1">PDF</a> </p>
<p><strong>摘要</strong><br>准确地在MRI上进行病变级别的分割对于多发性硬化症（MS）的诊断、预后和疾病监测至关重要。然而，当前的评估实践主要依赖于通过连通组件（CC）进行语义分割，但由于依赖空间连通性，无法分离融合病变（合并病变单位CLUs）。为应对这种与临床需求的错位，我们对CLUs和相关CLU感知检测指标进行了正式定义，并将它们纳入详尽的实例分割评估框架中。在此框架中，我们对基于CC和后处理的自动化融合分割方法（ACLS）进行了系统评价，这是MS病变实例分割中唯一现有的方法。分析表明，CC始终低估了CLU计数，而ACLS倾向于过度分割病变，导致病变计数过高且精度降低。为了克服这些局限性，我们提出了ConfLUNet，这是首个端到端的MS病变实例分割框架。ConfLUNet从单个FLAIR图像中联合优化病变检测和轮廓绘制。在50名患者上进行训练后，ConfLUNet在保留的测试集（n&#x3D;13）上的实例分割和病变检测方面显著优于CC和ACLS（全景质量：42.0%比37.5%&#x2F;36.8%；p&#x3D;0.017&#x2F;0.005；F1：67.3%比61.6%&#x2F;59.9%；p&#x3D;0.028&#x2F;0.013）。对于CLU检测，ConfLUNet的F1[CLU]值最高（81.5%），在召回率上较CC提高了（+12.5%，p&#x3D;0.015），在精确度上较ACLS提高了（+31.2%，p&#x3D;0.003）。通过严格的定义、新的CLU感知指标、可复制的评估框架和首个端到端的专用模型，这项工作为MS的病变实例分割奠定了基础。</p>
<p><strong>要点归纳</strong></p>
<ol>
<li>当前MRI病变级别分割在多发性硬化症诊断、预后和监测中的重要性。</li>
<li>当前评估方法主要依赖连通组件进行语义分割，无法有效分离融合病变。</li>
<li>引入CLUs的定义和相关CLU感知检测指标的重要性。</li>
<li>系统评估了基于连通组件的自动化分割方法和现有的ACLS方法，发现其局限性。</li>
<li>提出ConfLUNet作为首个端到端的MS病变实例分割框架，联合优化病变检测和轮廓绘制。</li>
<li>ConfLUNet在实例分割、病变检测以及CLU检测方面显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22537">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ea288273904119d417d16aaca76f63eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf79eb7b33b69d1b411bcf0f30d2cd5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf0b319fb695934d809adb4bdef3c31b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Universal-Domain-Adaptation-for-Semantic-Segmentation"><a href="#Universal-Domain-Adaptation-for-Semantic-Segmentation" class="headerlink" title="Universal Domain Adaptation for Semantic Segmentation"></a>Universal Domain Adaptation for Semantic Segmentation</h2><p><strong>Authors:Seun-An Choe, Keon-Hee Park, Jinwoo Choi, Gyeong-Moon Park</strong></p>
<p>Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to transfer knowledge from labeled source data to unlabeled target data. However, traditional UDA-SS methods assume that category settings between source and target domains are known, which is unrealistic in real-world scenarios. This leads to performance degradation if private classes exist. To address this limitation, we propose Universal Domain Adaptation for Semantic Segmentation (UniDA-SS), achieving robust adaptation even without prior knowledge of category settings. We define the problem in the UniDA-SS scenario as low confidence scores of common classes in the target domain, which leads to confusion with private classes. To solve this problem, we propose UniMAP: UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework composed of two key components. First, Domain-Specific Prototype-based Distinction (DSPD) divides each class into two domain-specific prototypes, enabling finer separation of domain-specific features and enhancing the identification of common classes across domains. Second, Target-based Image Matching (TIM) selects a source image containing the most common-class pixels based on the target pseudo-label and pairs it in a batch to promote effective learning of common classes. We also introduce a new UniDA-SS benchmark and demonstrate through various experiments that UniMAP significantly outperforms baselines. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/KU-VGI/UniMAP%7D%7Bthis">https://github.com/KU-VGI/UniMAP}{this</a> https URL}. </p>
<blockquote>
<p>无监督领域自适应语义分割（UDA-SS）旨在将来自带标签源数据的知识转移到无标签目标数据。然而，传统的UDA-SS方法假设源域和目标域之间的类别设置是已知的，这在现实场景中是不现实的。如果存在私有类别，这会导致性能下降。为了解决这一局限性，我们提出了通用领域自适应语义分割（UniDA-SS），实现了稳健的适应，即使在没有类别设置的先验知识的情况下也是如此。我们将UniDA-SS场景中的问题定义为目标域中常见类别的置信度得分较低，这导致与私有类别的混淆。为了解决这一问题，我们提出了UniMAP：基于图像匹配的UniDA-SS以及基于原型的区别，这是一个新型框架，由两个关键组件组成。首先，基于域特定原型的区别（DSPD）将每个类别分为两个域特定原型，能够更精细地分离域特定特征，并增强跨域的常见类别的识别。其次，基于目标的图像匹配（TIM）选择包含最多常见类别像素的源图像，基于目标伪标签将其分批配对，以促进常见类别的有效学习。我们还引入了一个新的UniDA-SS基准测试，并通过各种实验证明UniMAP显著优于基线。代码可在<a target="_blank" rel="noopener" href="https://github.com/KU-VGI/UniMAP">this https URL</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22458v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为UniMAP的框架，用于解决无监督域自适应语义分割中的局限性问题。框架针对公共类别低置信度导致的私有类别混淆问题，采用域特定原型区分和目标基于图像的匹配技术。此外，还引入了一个新的UniDA-SS基准测试，并通过实验证明UniMAP在性能上显著优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UDA-SS的目标是将知识从标记的源数据转移到未标记的目标数据。然而，传统的UDA-SS方法假设源域和目标域之间的类别设置已知，这在现实世界中并不常见。因此存在私有类别时性能会下降。</li>
<li>UniMAP框架解决了该问题，通过图像匹配和基于原型的区分技术，实现了无需类别设置的先验知识的稳健适应。</li>
<li>UniMAP解决了目标域中公共类别低置信度导致的问题，避免与私有类别的混淆。这通过两个关键组件实现：域特定原型区分和目标基于图像的匹配。域特定原型区分将每个类分为两个域特定原型，实现跨域的域特定特征的精细分离，提高公共类别的识别能力。目标基于图像的匹配选择包含最多公共类别像素的源图像，基于目标伪标签进行配对，以促进有效学习公共类别。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9553e3f0b864a8c655a50cb7e307b36c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112e1d8c281d033a7772a495bc88c606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a613f97954360173b4d7516d9b4314f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Training-free-Open-Vocabulary-Semantic-Segmentation"><a href="#A-Survey-on-Training-free-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="A Survey on Training-free Open-Vocabulary Semantic Segmentation"></a>A Survey on Training-free Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Naomi Kombol, Ivan Martinović, Siniša Šegvić</strong></p>
<p>Semantic segmentation is one of the most fundamental tasks in image understanding with a long history of research, and subsequently a myriad of different approaches. Traditional methods strive to train models up from scratch, requiring vast amounts of computational resources and training data. In the advent of moving to open-vocabulary semantic segmentation, which asks models to classify beyond learned categories, large quantities of finely annotated data would be prohibitively expensive. Researchers have instead turned to training-free methods where they leverage existing models made for tasks where data is more easily acquired. Specifically, this survey will cover the history, nuance, idea development and the state-of-the-art in training-free open-vocabulary semantic segmentation that leverages existing multi-modal classification models. We will first give a preliminary on the task definition followed by an overview of popular model archetypes and then spotlight over 30 approaches split into broader research branches: purely CLIP-based, those leveraging auxiliary visual foundation models and ones relying on generative methods. Subsequently, we will discuss the limitations and potential problems of current research, as well as provide some underexplored ideas for future study. We believe this survey will serve as a good onboarding read to new researchers and spark increased interest in the area. </p>
<blockquote>
<p>语义分割是图像理解中最基本的任务之一，有着悠久的历史和众多的研究方法。传统方法致力于从头开始训练模型，需要大量的计算资源和训练数据。随着开放词汇语义分割的出现，该任务要求模型对学到的类别进行分类之外的内容，大量精细标注的数据将变得极为昂贵。因此，研究人员转而采用无训练方法，他们利用为更容易获取数据的任务构建的现有模型。具体来说，这篇综述将介绍无训练开放词汇语义分割的历史、细微差别、思想发展和最新研究情况，该分割利用现有的多模式分类模型。我们将首先给出任务定义的初步介绍，然后概述流行的模型原型，再重点介绍超过30种方法分为几个较大的研究分支：纯基于CLIP的方法、利用辅助视觉基础模型的方法和依赖生成方法的方法。随后，我们将讨论当前研究的局限性和潜在问题，并提供一些尚未探索的想法供未来研究。我们相信这篇综述将为新研究人员提供良好的入门读物，并激发对该领域的兴趣。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22209v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语义分割在图像理解领域的重要性及其研究历史。随着开放词汇语义分割的兴起，研究人员开始转向无需训练的方法，利用现有模型解决数据更易获取的任务。本文回顾了训练免费开放词汇语义分割的历史、细微差别、思想发展和最新进展，特别是利用多模式分类模型的方法。文章概述了任务定义、流行模型原型，并重点介绍了30多种方法的不同研究分支。同时，文章也探讨了当前研究的局限性和潜在问题，并为未来的研究提供了一些尚未探索的想法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割是图像理解中的基础任务，有着丰富的研究历史和各种不同方法。</li>
<li>开放词汇语义分割要求模型进行分类超出已学类别，需要大量精细标注的数据，成本高昂。</li>
<li>研究人员开始转向无需训练的方法，利用现有模型，这些方法针对数据更易获取的任务。</li>
<li>本文回顾了训练免费开放词汇语义分割的历史、发展和最新进展。</li>
<li>文章概述了任务定义、流行模型原型，并重点介绍了30多种方法的不同研究分支，包括纯CLIP方法、利用辅助视觉基础模型的方法和依赖生成方法。</li>
<li>当前研究的局限性在于存在潜在问题，本文也讨论了这些问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22209">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dc000e389ff1dc0b8b1ed28540501e48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1afc6a8da7323b1107dcd0b8cb254ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdcafac5b6077858b3d864af9b956d53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3af6181361d7d460663eb87cc9ef6c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3129a09b4816d6d02e60301cba3fd59.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CAST-Contrastive-Adaptation-and-Distillation-for-Semi-Supervised-Instance-Segmentation"><a href="#CAST-Contrastive-Adaptation-and-Distillation-for-Semi-Supervised-Instance-Segmentation" class="headerlink" title="CAST: Contrastive Adaptation and Distillation for Semi-Supervised   Instance Segmentation"></a>CAST: Contrastive Adaptation and Distillation for Semi-Supervised   Instance Segmentation</h2><p><strong>Authors:Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu</strong></p>
<p>Instance segmentation demands costly per-pixel annotations and large models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pretrained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, (2) distillation into a compact student via a unified multi-objective loss that couples standard supervision and pseudo-labels with our instance-aware pixel-wise contrastive term, and (3) fine-tuning on labeled data to remove residual pseudo-label bias. Central to CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to mine informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs. 15.2) and outperforms state-of-the-art semi-supervised approaches. </p>
<blockquote>
<p>实例分割需要昂贵的像素级标注和大模型。我们引入了CAST，这是一种半监督知识蒸馏（SSKD）框架，它利用有限的标记数据和大量的无标记数据，将预训练的视觉基础模型（VFM）压缩成紧凑的专家模型。CAST分为三个阶段：（1）通过对比像素校准进行自我训练，对VFM教师进行域适应；（2）通过统一的多目标损失进行蒸馏，该损失将标准监督和伪标签与我们的实例感知像素级对比项相结合，对紧凑的学生模型进行蒸馏；（3）在标记数据上进行微调，以消除残留的伪标签偏见。CAST的核心是<em>实例感知像素级对比损失</em>，它融合掩膜和类分数来挖掘信息阴性样本并强制执行清晰的实例间边界。通过在整个适应和蒸馏过程中保持这种对比信号，我们对齐教师和学生嵌入，并充分利用无标签图像。在Cityscapes和ADE20K上，我们较小的学生模型（~11倍）超越了其适应的VFM教师模型（+3.4 AP（33.9对30.5）和+1.5 AP（16.7对15.2）），并优于最新的半监督方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21904v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了基于半监督知识蒸馏（SSKD）的压缩框架CAST，该框架利用有限的标签数据和大量的无标签数据将预训练的视觉基础模型（VFM）压缩成紧凑的专家模型。CAST分为三个主要阶段：VFM教师的域自适应、通过统一的多目标损失蒸馏到紧凑的学生模型，以及使用标记数据进行微调以消除残留的伪标签偏见。核心在于实例感知像素级对比损失，该损失融合了掩膜和类别分数以挖掘信息负样本并明确实例间的边界。通过在整个适应和蒸馏过程中保持对比信号，我们实现了教师和学生嵌入的对齐，并充分利用了无标签图像。在Cityscapes和ADE20K数据集上，我们的学生模型超越了其适应的VFM教师模型，并超过了现有的最先进的半监督方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>CAST利用有限的标签数据和大量的无标签数据压缩预训练视觉基础模型。</li>
<li>CAST分为三个主要阶段：教师域自适应、通过多目标损失蒸馏学生模型和微调。</li>
<li>实例感知像素级对比损失是CAST的核心，融合了掩膜和类别分数。</li>
<li>对比信号在适应和蒸馏过程中都被保持。</li>
<li>学生模型大小和性能超越了其适应的VFM教师模型。</li>
<li>在Cityscapes和ADE20K数据集上，CAST表现优于现有的最先进的半监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21904">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0db3cb819b5be1bb89bb0c46b711f056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b63e42239366ca09998e9f4c0458884.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f70f00a53c481d6c196dbc3cf0e47d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Test-Time-Adaptation-of-Vision-Language-Models-for-Open-Vocabulary-Semantic-Segmentation"><a href="#Test-Time-Adaptation-of-Vision-Language-Models-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Test-Time Adaptation of Vision-Language Models for Open-Vocabulary   Semantic Segmentation"></a>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers</strong></p>
<p>Recently, test-time adaptation has attracted wide interest in the context of vision-language models for image classification. However, to the best of our knowledge, the problem is completely overlooked in dense prediction tasks such as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a novel TTA method tailored to adapting VLMs for segmentation during test time. Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates features from intermediate vision-encoder layers and is performed with different text-prompt templates at both the global CLS token and local pixel-wise levels. Our approach could be used as plug-and-play for any segmentation network, does not require additional training data or labels, and remains effective even with a single test sample. Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which integrates a rigorous evaluation protocol, seven segmentation datasets, and 15 common corruptions, with a total of 82 distinct test scenarios, establishing a standardized and comprehensive testbed for future TTA research in open-vocabulary segmentation. Our experiments on this suite demonstrate that our segmentation-tailored method consistently delivers significant gains over direct adoption of TTA classification baselines. </p>
<blockquote>
<p>最近，测试时间适应在图像分类的视觉语言模型背景下引起了广泛关注。然而，据我们所知，该问题在密集预测任务（如开放词汇语义分割）中完全被忽略了。作为回应，我们提出了一种新型的TTA方法，专为测试时适应VLMs进行分割而设计。与用于图像分类的TTA方法不同，我们的多层次多提示（MLMP）熵最小化结合了中间视觉编码器层的特征，并在全局CLS标记和局部像素级使用不同的文本提示模板进行。我们的方法可以作为任何分割网络的即插即用工具，无需额外的训练数据或标签，即使在单个测试样本上也能保持有效。此外，我们引入了一个全面的OVSS TTA基准套件，其中包括严格的评估协议、七个分割数据集和15种常见腐蚀，总共82种不同的测试场景，为未来的开放词汇分割TTA研究建立了标准化和全面的测试平台。在此套件上的实验表明，我们的针对分割的方法始终在直接采用TTA分类基线时表现出显著的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注测试时间自适应技术在视觉语言模型中的图像分类应用，但在密集预测任务如开放词汇语义分割（OVSS）中却被忽视。为此，提出了一种针对分割任务的测试时间自适应方法，通过多级别和多提示的熵最小化策略，整合视觉编码器中间层的特征，并在全局CLS标记和局部像素级使用不同的文本提示模板。该方法可作为任何分割网络的即插即用模块，无需额外的训练数据或标签，甚至在单个测试样本上也能保持有效。同时，引入了一个全面的OVSS TTA基准套件，为未来TTA在开放词汇分割领域的研究提供了标准化和全面的测试平台。实验表明，该方法在分割任务上相较于直接采用TTA分类基线有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时间自适应技术在视觉语言模型的图像分类中受到关注，但在密集预测任务如开放词汇语义分割（OVSS）中被忽视。</li>
<li>提出了一种针对分割任务的测试时间自适应方法，通过多级别和多提示的熵最小化策略整合特征。</li>
<li>该方法可作为任何分割网络的即插即用模块，无需额外的训练数据或标签。</li>
<li>引入了一个全面的OVSS TTA基准套件，为未来TTA研究提供了标准化测试平台。</li>
<li>实验表明，该方法在分割任务上表现优越，可以显著提高模型性能。</li>
<li>通过使用不同的文本提示模板，该方法在全局CLS标记和局部像素级都有良好表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-311bcde9f628fe1a7218fc71e0e3c759.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3fd30ab03280f77a9da90e2ec751830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c823f526a63d7958516de32023d809e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation"><a href="#SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation" class="headerlink" title="SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation"></a>SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</h2><p><strong>Authors:Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone</strong></p>
<p>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a>. </p>
<blockquote>
<p>少数样本分割旨在从少量标注的样本中对未见过的目标类别进行分割。这需要能够在图像中识别语义相关对象并准确生成分割掩码的机制。我们注意到，借助提示和传播的Segment Anything 2（SAM2）提供了强大的分割能力和内置的特征匹配过程。然而，我们表明其表示与针对对象跟踪优化的特定任务线索纠缠在一起，这损害了其在需要高级语义理解的任务中的使用。我们的关键见解是，尽管SAM2具有类无关的预训练，但它已经在特征中编码了丰富的语义结构。我们提出了SANSA（语义对齐的Segment Anything 2），一个使这种潜在结构明确的框架，并通过最少的特定任务修改将SAM2重新用于少数样本分割。SANSA在专门为评估泛化能力设计的少数样本分割基准测试上实现了最先进的性能，在流行的上下文设置中的全能方法表现出色，支持通过点、框或涂鸦进行各种提示的灵活交互，并且相较于先前的方法，速度更快、更紧凑。代码可在<a target="_blank" rel="noopener" href="https://github.com/Claudiacuttano/SANSA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ClaudiaCuttano/SANSA上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21795v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a></p>
<p><strong>Summary</strong></p>
<p>SAM2模型具有强大的分割能力和内置的特征匹配过程，但其在少数样本分割任务中的表现受到任务特定线索的干扰。我们提出SANSA框架，通过使SAM2的潜在结构显性化并对其进行最小特定的任务修改，使其成为适合少数样本分割任务的有效工具。SANSA具有出色的性能，支持各种提示灵活的交互方式，并且相较于先前的方法更加快速和紧凑。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2模型具有强大的分割能力和特征匹配过程。</li>
<li>SAM2模型在少数样本分割任务中的表现受到任务特定线索的干扰。</li>
<li>SANSA框架利用SAM2的潜在结构进行少数样本分割任务。</li>
<li>SANSA框架通过最小特定的任务修改使SAM2模型适合少数样本分割任务。</li>
<li>SANSA在少数样本分割任务上具有出色的性能，且优于先前的通用方法。</li>
<li>SANSA支持各种提示灵活的交互方式，便于用户进行使用和操作。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21795">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-776453b1b78a989e79af018be9145c4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf2b505a75d0ad66863efb87ec4357c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-022dba601551ed556c01f471a95365d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84853eff613e8afa690dddf511e22788.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SegRet-An-Efficient-Design-for-Semantic-Segmentation-with-Retentive-Network"><a href="#SegRet-An-Efficient-Design-for-Semantic-Segmentation-with-Retentive-Network" class="headerlink" title="SegRet: An Efficient Design for Semantic Segmentation with Retentive   Network"></a>SegRet: An Efficient Design for Semantic Segmentation with Retentive   Network</h2><p><strong>Authors:Zhiyuan Li, Yi Chang, Yuan Wu</strong></p>
<p>With the rapid evolution of autonomous driving technology and intelligent transportation systems, semantic segmentation has become increasingly critical. Precise interpretation and analysis of real-world environments are indispensable for these advanced applications. However, traditional semantic segmentation approaches frequently face challenges in balancing model performance with computational efficiency, especially regarding the volume of model parameters. To address these constraints, we propose SegRet, a novel model employing the Retentive Network (RetNet) architecture coupled with a lightweight residual decoder that integrates zero-initialization. SegRet offers three distinctive advantages: (1) Lightweight Residual Decoder: by embedding a zero-initialization layer within the residual network structure, the decoder remains computationally streamlined without sacrificing essential information propagation; (2) Robust Feature Extraction: adopting RetNet as its backbone enables SegRet to effectively capture hierarchical image features, thereby enriching the representation quality of extracted features; (3) Parameter Efficiency: SegRet attains state-of-the-art (SOTA) segmentation performance while markedly decreasing the number of parameters, ensuring high accuracy without imposing additional computational burdens. Comprehensive empirical evaluations on prominent benchmarks, such as ADE20K, Citycapes, and COCO-Stuff, highlight the effectiveness and superiority of our method. </p>
<blockquote>
<p>随着自动驾驶技术和智能交通系统的快速发展，语义分割的重要性日益凸显。精确解读和分析真实环境对这些高级应用来说是不可或缺的。然而，传统的语义分割方法常常在平衡模型性能和计算效率方面面临挑战，特别是关于模型参数体积的问题。为了解决这些限制，我们提出了SegRet，一个采用Retentive Network（RetNet）架构结合轻量级残差解码器的新型模型，该解码器集成了零初始化。SegRet具有三个显著优势：</p>
</blockquote>
<p>（1）轻量级残差解码器：通过在残差网络结构中嵌入零初始化层，解码器在计算上保持简洁，同时不牺牲重要信息的传播；</p>
<p>（2）稳健的特征提取：采用RetNet作为骨干网，使SegRet能够有效地捕获分层图像特征，从而丰富提取特征的表现质量；</p>
<p>（3）参数效率：SegRet在减少参数数量的同时达到了最先进的分割性能，确保高精度不会带来额外的计算负担。在ADE20K、Citycapes和COCO-Stuff等主流基准测试上的综合实证评估，突显了我们方法的有效性和优越性。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14014v2">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>随着自动驾驶技术与智能交通系统的飞速发展，语义分割技术变得尤为重要。精确解读和分析真实环境对这些高级应用不可或缺。传统语义分割方法常在模型性能与计算效率之间面临挑战，尤其体现在模型参数规模上。我们提出SegRet模型，采用Retentive Network（RetNet）架构结合轻量化残差解码器，集成零初始化技术。SegRet具备三大优势：一、轻量化残差解码器：在残差网络结构中嵌入零初始化层，保证解码器计算流程简洁，同时不牺牲关键信息传输；二、稳健特征提取：采用RetNet作为骨干网，使SegRet能够有效捕捉图像层次特征，从而提高特征表示质量；三、参数高效：SegRet在减少参数数量的同时达到最先进的分割性能，确保高精度且不会增加计算负担。在ADE20K、Citycapes和COCO-Stuff等主流基准测试上的综合评估，凸显了我们方法的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割在自动驾驶和智能交通系统中至关重要，需要精确解读和分析真实环境。</li>
<li>传统语义分割方法面临模型性能和计算效率的平衡挑战，尤其是模型参数规模。</li>
<li>SegRet模型采用RetNet架构和轻量化残差解码器，集成零初始化技术。</li>
<li>SegRet具有三大优势：轻量化、稳健特征提取和参数高效。</li>
<li>SegRet在保证高分割性能的同时，显著减少了参数数量，且不增加计算负担。</li>
<li>在主流基准测试上，SegRet表现出有效性和优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14014">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5e4de0369a070737f85c6d8bfe21669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6901509a51befecb4138f7ffd6d09d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3065b470f75b9358e94c1113aa9662c1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Complex-Wavelet-Mutual-Information-Loss-A-Multi-Scale-Loss-Function-for-Semantic-Segmentation"><a href="#Complex-Wavelet-Mutual-Information-Loss-A-Multi-Scale-Loss-Function-for-Semantic-Segmentation" class="headerlink" title="Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for   Semantic Segmentation"></a>Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for   Semantic Segmentation</h2><p><strong>Authors:Renhao Lu</strong></p>
<p>Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose the complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited to capturing high-dimensional directional features and offers greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lurenhaothu/CWMI">https://github.com/lurenhaothu/CWMI</a> </p>
<blockquote>
<p>最近深度神经网络的发展极大地提高了语义分割的性能。然而，类别不平衡和实例不平衡仍然持续存在挑战，较小的实例和薄边界通常被较大的结构所掩盖。为了解决分割对象的多尺度特性，各种模型已经融入了空间注意力机制和特征金字塔网络等机制。尽管有了这些进展，大多数损失函数仍然是基于像素的，而区域性和边界聚焦的损失函数往往带来较高的计算成本或仅限于小规模区域。为了解决这一局限性，我们提出了复杂小波互信息（CWMI）损失这一新型损失函数，它利用由复杂可转向金字塔分解得到的子带图像的互信息。复杂可转向金字塔能够捕获多个方向的特性，并在不同尺度上保留结构相似性。同时，互信息非常适合捕捉高维方向特性，并提供了更强的噪声鲁棒性。在多种分割数据集上的广泛实验表明，与最新方法相比，CWMI损失在像素级精度和拓扑指标方面取得了显著改进，同时引入了极低的计算开销。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/lurenhaothu/CWMI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lurenhaothu/CWMI找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00563v2">PDF</a> Accepted at ICML 2025. This version corresponds to the official   camera-ready submission</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对语义分割中的多尺度问题，提出了一种新的损失函数——复杂小波互信息（CWMI）损失。该函数利用由复杂可转向金字塔分解得到的子带图像的互信息，有效地提高了像素级精度和拓扑度量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割面临的挑战包括类别不平衡和实例不平衡，其中较小的实例和薄边界常被较大的结构所掩盖。</li>
<li>为了应对分割对象的多尺度特性，已存在的模型采用了空间注意力机制和特征金字塔网络等方法。</li>
<li>现有的损失函数主要是像素级的，而区域和边界聚焦的损失函数计算成本较高或仅限于小规模区域。</li>
<li>提出了一种新的损失函数——复杂小波互信息（CWMI）损失，它利用复杂可转向金字塔分解的互信息。</li>
<li>复杂可转向金字塔能够捕捉多个方向的特性并保持跨尺度的结构相似性。</li>
<li>互信息适用于捕捉高维方向特性，并提供了更好的噪声鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00563">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f6a3e7875c53d8265ae2ea74d6e5322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6df59c774af50decb248437acc23be55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed968d3b72687fb19af829e2601c917b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad5d01713314315aafdced9f0f4fa40.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Cross-Layer-Feature-Pyramid-Transformer-for-Small-Object-Detection-in-Aerial-Images"><a href="#Cross-Layer-Feature-Pyramid-Transformer-for-Small-Object-Detection-in-Aerial-Images" class="headerlink" title="Cross-Layer Feature Pyramid Transformer for Small Object Detection in   Aerial Images"></a>Cross-Layer Feature Pyramid Transformer for Small Object Detection in   Aerial Images</h2><p><strong>Authors:Zewen Du, Zhenjiang Hu, Guiyu Zhao, Ying Jin, Hongbin Ma</strong></p>
<p>Object detection in aerial images has always been a challenging task due to the generally small size of the objects. Most current detectors prioritize the development of new detection frameworks, often overlooking research on fundamental components such as feature pyramid networks. In this paper, we introduce the Cross-Layer Feature Pyramid Transformer (CFPT), a novel upsampler-free feature pyramid network designed specifically for small object detection in aerial images. CFPT incorporates two meticulously designed attention blocks with linear computational complexity: Cross-Layer Channel-Wise Attention (CCA) and Cross-Layer Spatial-Wise Attention (CSA). CCA achieves cross-layer interaction by dividing channel-wise token groups to perceive cross-layer global information along the spatial dimension, while CSA enables cross-layer interaction by dividing spatial-wise token groups to perceive cross-layer global information along the channel dimension. By integrating these modules, CFPT enables efficient cross-layer interaction in a single step, thereby avoiding the semantic gap and information loss associated with element-wise summation and layer-by-layer transmission. In addition, CFPT incorporates global contextual information, which improves detection performance for small objects. To further enhance location awareness during cross-layer interaction, we propose the Cross-Layer Consistent Relative Positional Encoding (CCPE) based on inter-layer mutual receptive fields. We evaluate the effectiveness of CFPT on three challenging object detection datasets in aerial images: VisDrone2019-DET, TinyPerson, and xView. Extensive experiments demonstrate that CFPT outperforms state-of-the-art feature pyramid networks while incurring lower computational costs. The code is available at <a target="_blank" rel="noopener" href="https://github.com/duzw9311/CFPT">https://github.com/duzw9311/CFPT</a>. </p>
<blockquote>
<p>在航空图像中进行目标检测一直是一项具有挑战性的任务，主要是因为目标通常尺寸较小。目前大多数探测器都侧重于开发新的检测框架，往往忽视了关于基础组件的研究，如特征金字塔网络。在本文中，我们介绍了跨层特征金字塔转换器（CFPT），这是一种专为航空图像中的小目标检测设计的无上采样器特征金字塔网络。CFPT融合了两个精心设计的具有线性计算复杂度的注意力块：跨层通道注意力（CCA）和跨层空间注意力（CSA）。CCA通过沿空间维度将通道令牌分组来实现跨层交互，以感知跨层全局信息，而CSA通过沿通道维度将空间令牌分组来实现跨层交互。通过集成这些模块，CFPT能够在单步中实现高效的跨层交互，从而避免了逐元素求和和逐层传输所带来的语义鸿沟和信息损失。此外，CFPT结合了全局上下文信息，提高了对小目标的检测性能。为了进一步提高跨层交互过程中的位置感知能力，我们基于层间相互感受野提出了跨层一致相对位置编码（CCPE）。我们在三个具有挑战性的航空图像目标检测数据集上评估了CFPT的有效性：VisDrone2019-DET、TinyPerson和xView。大量实验表明，CFPT在具有较低计算成本的同时，优于最新的特征金字塔网络。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/duzw9311/CFPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/duzw9311/CFPT获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19696v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文提出了一种新型的跨层特征金字塔转换器（CFPT），这是一种专为空中图像小目标检测设计的无上采样器特征金字塔网络。它包含两种设计精巧的注意力块：跨层通道注意力（CCA）和跨层空间注意力（CSA）。CFPT通过整合这些模块，实现了跨层交互，避免了语义鸿沟和信息损失。此外，CFPT还融入了全局上下文信息，提高了对小目标的检测性能。该研究在三个具有挑战性的空中图像目标检测数据集上评估了CFPT的有效性，并证明其性能优于当前主流的特征金字塔网络，同时计算成本更低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出了一种新型的跨层特征金字塔转换器（CFPT），针对空中图像的小目标检测。</li>
<li>CFPT包含两种注意力块：跨层通道注意力（CCA）和跨层空间注意力（CSA），可实现高效的跨层交互。</li>
<li>CFPT避免了语义鸿沟和信息损失的问题。</li>
<li>CFPT融入了全局上下文信息以提高对小目标的检测性能。</li>
<li>在三个具有挑战性的空中图像目标检测数据集上进行了实验验证，证明CFPT的性能优于其他主流方法。</li>
<li>CFPT的计算成本较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.19696">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0d64c200e936f6b5a66b59f1d959813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45ee09da84121ea441c4bdbe28e254d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3ced45756a21dd1f9c2e96f1a3d172.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d8bb05fce3134439d8a4c244a6be4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97cd0e753066239efea29f6e8289f8ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b79df1325bd39020ad41e1c176ebf6b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6e7815ecf4da49216c74c947f4a8ef96.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-05-30  Single Domain Generalization for Alzheimer's Detection from 3D MRIs with   Pseudo-Morphological Augmentations and Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9b73bf18f8ffd9cdbf8a8688234a1e2a.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-30  Beyond 1D Vision Transformers and Multichannel Signal Images for   PPG-to-ECG Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
