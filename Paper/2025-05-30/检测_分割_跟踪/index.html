<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  ConfLUNet Multiple sclerosis lesion instance segmentation in presence   of confluent lesions">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2c3ced45756a21dd1f9c2e96f1a3d172.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions"><a href="#ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions" class="headerlink" title="ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions"></a>ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions</h2><p><strong>Authors:Maxence Wynen, Pedro M. Gordaliza, Maxime Istasse, Anna StÃ¶lting, Pietro Maggi, BenoÃ®t Macq, Meritxell Bach Cuadra</strong></p>
<p>Accurate lesion-level segmentation on MRI is critical for multiple sclerosis (MS) diagnosis, prognosis, and disease monitoring. However, current evaluation practices largely rely on semantic segmentation post-processed with connected components (CC), which cannot separate confluent lesions (aggregates of confluent lesion units, CLUs) due to reliance on spatial connectivity. To address this misalignment with clinical needs, we introduce formal definitions of CLUs and associated CLU-aware detection metrics, and include them in an exhaustive instance segmentation evaluation framework. Within this framework, we systematically evaluate CC and post-processing-based Automated Confluent Splitting (ACLS), the only existing methods for lesion instance segmentation in MS. Our analysis reveals that CC consistently underestimates CLU counts, while ACLS tends to oversplit lesions, leading to overestimated lesion counts and reduced precision. To overcome these limitations, we propose ConfLUNet, the first end-to-end instance segmentation framework for MS lesions. ConfLUNet jointly optimizes lesion detection and delineation from a single FLAIR image. Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the held-out test set (n&#x3D;13) in instance segmentation (Panoptic Quality: 42.0% vs. 37.5%&#x2F;36.8%; p &#x3D; 0.017&#x2F;0.005) and lesion detection (F1: 67.3% vs. 61.6%&#x2F;59.9%; p &#x3D; 0.028&#x2F;0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU] (81.5%), improving recall over CC (+12.5%, p &#x3D; 0.015) and precision over ACLS (+31.2%, p &#x3D; 0.003). By combining rigorous definitions, new CLU-aware metrics, a reproducible evaluation framework, and the first dedicated end-to-end model, this work lays the foundation for lesion instance segmentation in MS. </p>
<blockquote>
<p>åœ¨æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸Šè¿›è¡Œå‡†ç¡®çš„ç—…ç¶çº§åˆ«åˆ†å‰²å¯¹äºå¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„è¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–äºé€šè¿‡è¿é€šç»„ä»¶ï¼ˆCCï¼‰è¿›è¡Œåå¤„ç†çš„è¯­ä¹‰åˆ†å‰²ï¼Œç”±äºä¾èµ–ç©ºé—´è¿é€šæ€§ï¼Œå®ƒä»¬æ— æ³•åˆ†ç¦»èåˆç—…ç¶ï¼ˆèåˆç—…ç¶å•ä½ï¼ˆCLUï¼‰çš„èšé›†ä½“ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸ä¸´åºŠéœ€æ±‚çš„ä¸ç¬¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†CLUçš„æ­£å¼å®šä¹‰å’Œç›¸å…³CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡ï¼Œå¹¶å°†å®ƒä»¬çº³å…¥è¯¦å°½çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†åŸºäºè¿é€šç»„ä»¶ï¼ˆCCï¼‰å’Œåå¤„ç†è‡ªåŠ¨èåˆåˆ†å‰²ï¼ˆACLSï¼‰çš„æ–¹æ³•ï¼Œè¿™æ˜¯å¤šå‘æ€§ç¡¬åŒ–ç—‡ä¸­ç—…ç¶å®ä¾‹åˆ†å‰²çš„ç°æœ‰å”¯ä¸€æ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒCCå§‹ç»ˆä½ä¼°CLUè®¡æ•°ï¼Œè€ŒACLSå€¾å‘äºè¿‡åˆ†å‰²ç—…ç¶ï¼Œå¯¼è‡´ç—…ç¶è®¡æ•°è¿‡é«˜ï¼Œç²¾åº¦é™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ConfLUNetï¼Œè¿™æ˜¯å¤šå‘æ€§ç¡¬åŒ–ç—‡ç—…ç¶å®ä¾‹åˆ†å‰²çš„ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ã€‚ConfLUNetä»å•ä¸ªFLAIRå›¾åƒä¸­è”åˆä¼˜åŒ–ç—…ç¶æ£€æµ‹å’Œè½®å»“æç»˜ã€‚åœ¨50åæ‚£è€…ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒConfLUNetåœ¨å®ä¾‹åˆ†å‰²ï¼ˆæ³›å…¨æ™¯è´¨é‡ï¼š42.0%å¯¹æ¯”37.5%&#x2F;36.8%ï¼Œp&#x3D;0.017&#x2F;0.005ï¼‰å’Œç—…ç¶æ£€æµ‹ï¼ˆF1ï¼š67.3%å¯¹æ¯”61.6%&#x2F;59.9%ï¼Œp&#x3D;0.028&#x2F;0.013ï¼‰æ–¹é¢æ˜¾è‘—ä¼˜äºCCå’ŒACLSã€‚å¯¹äºCLUæ£€æµ‹ï¼ŒConfLUNetè¾¾åˆ°äº†æœ€é«˜çš„F1[CLU]ï¼ˆ81.5%ï¼‰ï¼Œåœ¨å¬å›ç‡ä¸Šè¶…è¿‡äº†CCï¼ˆ+12.5%ï¼Œp&#x3D;0.015ï¼‰ï¼Œå¹¶åœ¨ç²¾ç¡®åº¦ä¸Šè¶…è¿‡äº†ACLSï¼ˆ+31.2%ï¼Œp&#x3D;0.003ï¼‰ã€‚é€šè¿‡ç»“åˆä¸¥æ ¼å®šä¹‰ã€æ–°çš„CLUæ„ŸçŸ¥æŒ‡æ ‡ã€å¯é‡ç°çš„è¯„ä¼°æ¡†æ¶å’Œç¬¬ä¸€ä¸ªä¸“ç”¨ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºå¤šå‘æ€§ç¡¬åŒ–ç—‡ä¸­çš„ç—…ç¶å®ä¾‹åˆ†å‰²å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22537v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å‡†ç¡®åœ°åœ¨MRIä¸Šè¿›è¡Œç—…å˜çº§åˆ«çš„åˆ†å‰²å¯¹äºå¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„è¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°å®è·µä¸»è¦ä¾èµ–äºé€šè¿‡è¿é€šç»„ä»¶ï¼ˆCCï¼‰è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œä½†ç”±äºä¾èµ–ç©ºé—´è¿é€šæ€§ï¼Œæ— æ³•åˆ†ç¦»èåˆç—…å˜ï¼ˆåˆå¹¶ç—…å˜å•ä½CLUsï¼‰ã€‚ä¸ºåº”å¯¹è¿™ç§ä¸ä¸´åºŠéœ€æ±‚çš„é”™ä½ï¼Œæˆ‘ä»¬å¯¹CLUså’Œç›¸å…³CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡è¿›è¡Œäº†æ­£å¼å®šä¹‰ï¼Œå¹¶å°†å®ƒä»¬çº³å…¥è¯¦å°½çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ä¸­ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºCCå’Œåå¤„ç†çš„è‡ªåŠ¨åŒ–èåˆåˆ†å‰²æ–¹æ³•ï¼ˆACLSï¼‰è¿›è¡Œäº†ç³»ç»Ÿè¯„ä»·ï¼Œè¿™æ˜¯MSç—…å˜å®ä¾‹åˆ†å‰²ä¸­å”¯ä¸€ç°æœ‰çš„æ–¹æ³•ã€‚åˆ†æè¡¨æ˜ï¼ŒCCå§‹ç»ˆä½ä¼°äº†CLUè®¡æ•°ï¼Œè€ŒACLSå€¾å‘äºè¿‡åº¦åˆ†å‰²ç—…å˜ï¼Œå¯¼è‡´ç—…å˜è®¡æ•°è¿‡é«˜ä¸”ç²¾åº¦é™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ConfLUNetï¼Œè¿™æ˜¯é¦–ä¸ªç«¯åˆ°ç«¯çš„MSç—…å˜å®ä¾‹åˆ†å‰²æ¡†æ¶ã€‚ConfLUNetä»å•ä¸ªFLAIRå›¾åƒä¸­è”åˆä¼˜åŒ–ç—…å˜æ£€æµ‹å’Œè½®å»“ç»˜åˆ¶ã€‚åœ¨50åæ‚£è€…ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒConfLUNetåœ¨ä¿ç•™çš„æµ‹è¯•é›†ï¼ˆn&#x3D;13ï¼‰ä¸Šçš„å®ä¾‹åˆ†å‰²å’Œç—…å˜æ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºCCå’ŒACLSï¼ˆå…¨æ™¯è´¨é‡ï¼š42.0%æ¯”37.5%&#x2F;36.8%ï¼›p&#x3D;0.017&#x2F;0.005ï¼›F1ï¼š67.3%æ¯”61.6%&#x2F;59.9%ï¼›p&#x3D;0.028&#x2F;0.013ï¼‰ã€‚å¯¹äºCLUæ£€æµ‹ï¼ŒConfLUNetçš„F1[CLU]å€¼æœ€é«˜ï¼ˆ81.5%ï¼‰ï¼Œåœ¨å¬å›ç‡ä¸Šè¾ƒCCæé«˜äº†ï¼ˆ+12.5%ï¼Œp&#x3D;0.015ï¼‰ï¼Œåœ¨ç²¾ç¡®åº¦ä¸Šè¾ƒACLSæé«˜äº†ï¼ˆ+31.2%ï¼Œp&#x3D;0.003ï¼‰ã€‚é€šè¿‡ä¸¥æ ¼çš„å®šä¹‰ã€æ–°çš„CLUæ„ŸçŸ¥æŒ‡æ ‡ã€å¯å¤åˆ¶çš„è¯„ä¼°æ¡†æ¶å’Œé¦–ä¸ªç«¯åˆ°ç«¯çš„ä¸“ç”¨æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºMSçš„ç—…å˜å®ä¾‹åˆ†å‰²å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å½“å‰MRIç—…å˜çº§åˆ«åˆ†å‰²åœ¨å¤šå‘æ€§ç¡¬åŒ–ç—‡è¯Šæ–­ã€é¢„åå’Œç›‘æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–è¿é€šç»„ä»¶è¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œæ— æ³•æœ‰æ•ˆåˆ†ç¦»èåˆç—…å˜ã€‚</li>
<li>å¼•å…¥CLUsçš„å®šä¹‰å’Œç›¸å…³CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡çš„é‡è¦æ€§ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†åŸºäºè¿é€šç»„ä»¶çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ³•å’Œç°æœ‰çš„ACLSæ–¹æ³•ï¼Œå‘ç°å…¶å±€é™æ€§ã€‚</li>
<li>æå‡ºConfLUNetä½œä¸ºé¦–ä¸ªç«¯åˆ°ç«¯çš„MSç—…å˜å®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œè”åˆä¼˜åŒ–ç—…å˜æ£€æµ‹å’Œè½®å»“ç»˜åˆ¶ã€‚</li>
<li>ConfLUNetåœ¨å®ä¾‹åˆ†å‰²ã€ç—…å˜æ£€æµ‹ä»¥åŠCLUæ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea288273904119d417d16aaca76f63eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf79eb7b33b69d1b411bcf0f30d2cd5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf0b319fb695934d809adb4bdef3c31b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Universal-Domain-Adaptation-for-Semantic-Segmentation"><a href="#Universal-Domain-Adaptation-for-Semantic-Segmentation" class="headerlink" title="Universal Domain Adaptation for Semantic Segmentation"></a>Universal Domain Adaptation for Semantic Segmentation</h2><p><strong>Authors:Seun-An Choe, Keon-Hee Park, Jinwoo Choi, Gyeong-Moon Park</strong></p>
<p>Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to transfer knowledge from labeled source data to unlabeled target data. However, traditional UDA-SS methods assume that category settings between source and target domains are known, which is unrealistic in real-world scenarios. This leads to performance degradation if private classes exist. To address this limitation, we propose Universal Domain Adaptation for Semantic Segmentation (UniDA-SS), achieving robust adaptation even without prior knowledge of category settings. We define the problem in the UniDA-SS scenario as low confidence scores of common classes in the target domain, which leads to confusion with private classes. To solve this problem, we propose UniMAP: UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework composed of two key components. First, Domain-Specific Prototype-based Distinction (DSPD) divides each class into two domain-specific prototypes, enabling finer separation of domain-specific features and enhancing the identification of common classes across domains. Second, Target-based Image Matching (TIM) selects a source image containing the most common-class pixels based on the target pseudo-label and pairs it in a batch to promote effective learning of common classes. We also introduce a new UniDA-SS benchmark and demonstrate through various experiments that UniMAP significantly outperforms baselines. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/KU-VGI/UniMAP%7D%7Bthis">https://github.com/KU-VGI/UniMAP}{this</a> https URL}. </p>
<blockquote>
<p>æ— ç›‘ç£é¢†åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆUDA-SSï¼‰æ—¨åœ¨å°†æ¥è‡ªå¸¦æ ‡ç­¾æºæ•°æ®çš„çŸ¥è¯†è½¬ç§»åˆ°æ— æ ‡ç­¾ç›®æ ‡æ•°æ®ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„UDA-SSæ–¹æ³•å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„ç±»åˆ«è®¾ç½®æ˜¯å·²çŸ¥çš„ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­æ˜¯ä¸ç°å®çš„ã€‚å¦‚æœå­˜åœ¨ç§æœ‰ç±»åˆ«ï¼Œè¿™ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨é¢†åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆUniDA-SSï¼‰ï¼Œå®ç°äº†ç¨³å¥çš„é€‚åº”ï¼Œå³ä½¿åœ¨æ²¡æœ‰ç±»åˆ«è®¾ç½®çš„å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬å°†UniDA-SSåœºæ™¯ä¸­çš„é—®é¢˜å®šä¹‰ä¸ºç›®æ ‡åŸŸä¸­å¸¸è§ç±»åˆ«çš„ç½®ä¿¡åº¦å¾—åˆ†è¾ƒä½ï¼Œè¿™å¯¼è‡´ä¸ç§æœ‰ç±»åˆ«çš„æ··æ·†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniMAPï¼šåŸºäºå›¾åƒåŒ¹é…çš„UniDA-SSä»¥åŠåŸºäºåŸå‹çš„åŒºåˆ«ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆã€‚é¦–å…ˆï¼ŒåŸºäºåŸŸç‰¹å®šåŸå‹çš„åŒºåˆ«ï¼ˆDSPDï¼‰å°†æ¯ä¸ªç±»åˆ«åˆ†ä¸ºä¸¤ä¸ªåŸŸç‰¹å®šåŸå‹ï¼Œèƒ½å¤Ÿæ›´ç²¾ç»†åœ°åˆ†ç¦»åŸŸç‰¹å®šç‰¹å¾ï¼Œå¹¶å¢å¼ºè·¨åŸŸçš„å¸¸è§ç±»åˆ«çš„è¯†åˆ«ã€‚å…¶æ¬¡ï¼ŒåŸºäºç›®æ ‡çš„å›¾åƒåŒ¹é…ï¼ˆTIMï¼‰é€‰æ‹©åŒ…å«æœ€å¤šå¸¸è§ç±»åˆ«åƒç´ çš„æºå›¾åƒï¼ŒåŸºäºç›®æ ‡ä¼ªæ ‡ç­¾å°†å…¶åˆ†æ‰¹é…å¯¹ï¼Œä»¥ä¿ƒè¿›å¸¸è§ç±»åˆ«çš„æœ‰æ•ˆå­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„UniDA-SSåŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡å„ç§å®éªŒè¯æ˜UniMAPæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KU-VGI/UniMAP">this https URL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22458v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºUniMAPçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³æ— ç›‘ç£åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ä¸­çš„å±€é™æ€§é—®é¢˜ã€‚æ¡†æ¶é’ˆå¯¹å…¬å…±ç±»åˆ«ä½ç½®ä¿¡åº¦å¯¼è‡´çš„ç§æœ‰ç±»åˆ«æ··æ·†é—®é¢˜ï¼Œé‡‡ç”¨åŸŸç‰¹å®šåŸå‹åŒºåˆ†å’Œç›®æ ‡åŸºäºå›¾åƒçš„åŒ¹é…æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„UniDA-SSåŸºå‡†æµ‹è¯•ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜UniMAPåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UDA-SSçš„ç›®æ ‡æ˜¯å°†çŸ¥è¯†ä»æ ‡è®°çš„æºæ•°æ®è½¬ç§»åˆ°æœªæ ‡è®°çš„ç›®æ ‡æ•°æ®ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„UDA-SSæ–¹æ³•å‡è®¾æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„ç±»åˆ«è®¾ç½®å·²çŸ¥ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­å¹¶ä¸å¸¸è§ã€‚å› æ­¤å­˜åœ¨ç§æœ‰ç±»åˆ«æ—¶æ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>UniMAPæ¡†æ¶è§£å†³äº†è¯¥é—®é¢˜ï¼Œé€šè¿‡å›¾åƒåŒ¹é…å’ŒåŸºäºåŸå‹çš„åŒºåˆ†æŠ€æœ¯ï¼Œå®ç°äº†æ— éœ€ç±»åˆ«è®¾ç½®çš„å…ˆéªŒçŸ¥è¯†çš„ç¨³å¥é€‚åº”ã€‚</li>
<li>UniMAPè§£å†³äº†ç›®æ ‡åŸŸä¸­å…¬å…±ç±»åˆ«ä½ç½®ä¿¡åº¦å¯¼è‡´çš„é—®é¢˜ï¼Œé¿å…ä¸ç§æœ‰ç±»åˆ«çš„æ··æ·†ã€‚è¿™é€šè¿‡ä¸¤ä¸ªå…³é”®ç»„ä»¶å®ç°ï¼šåŸŸç‰¹å®šåŸå‹åŒºåˆ†å’Œç›®æ ‡åŸºäºå›¾åƒçš„åŒ¹é…ã€‚åŸŸç‰¹å®šåŸå‹åŒºåˆ†å°†æ¯ä¸ªç±»åˆ†ä¸ºä¸¤ä¸ªåŸŸç‰¹å®šåŸå‹ï¼Œå®ç°è·¨åŸŸçš„åŸŸç‰¹å®šç‰¹å¾çš„ç²¾ç»†åˆ†ç¦»ï¼Œæé«˜å…¬å…±ç±»åˆ«çš„è¯†åˆ«èƒ½åŠ›ã€‚ç›®æ ‡åŸºäºå›¾åƒçš„åŒ¹é…é€‰æ‹©åŒ…å«æœ€å¤šå…¬å…±ç±»åˆ«åƒç´ çš„æºå›¾åƒï¼ŒåŸºäºç›®æ ‡ä¼ªæ ‡ç­¾è¿›è¡Œé…å¯¹ï¼Œä»¥ä¿ƒè¿›æœ‰æ•ˆå­¦ä¹ å…¬å…±ç±»åˆ«ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9553e3f0b864a8c655a50cb7e307b36c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112e1d8c281d033a7772a495bc88c606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a613f97954360173b4d7516d9b4314f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Training-free-Open-Vocabulary-Semantic-Segmentation"><a href="#A-Survey-on-Training-free-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="A Survey on Training-free Open-Vocabulary Semantic Segmentation"></a>A Survey on Training-free Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Naomi Kombol, Ivan MartinoviÄ‡, SiniÅ¡a Å egviÄ‡</strong></p>
<p>Semantic segmentation is one of the most fundamental tasks in image understanding with a long history of research, and subsequently a myriad of different approaches. Traditional methods strive to train models up from scratch, requiring vast amounts of computational resources and training data. In the advent of moving to open-vocabulary semantic segmentation, which asks models to classify beyond learned categories, large quantities of finely annotated data would be prohibitively expensive. Researchers have instead turned to training-free methods where they leverage existing models made for tasks where data is more easily acquired. Specifically, this survey will cover the history, nuance, idea development and the state-of-the-art in training-free open-vocabulary semantic segmentation that leverages existing multi-modal classification models. We will first give a preliminary on the task definition followed by an overview of popular model archetypes and then spotlight over 30 approaches split into broader research branches: purely CLIP-based, those leveraging auxiliary visual foundation models and ones relying on generative methods. Subsequently, we will discuss the limitations and potential problems of current research, as well as provide some underexplored ideas for future study. We believe this survey will serve as a good onboarding read to new researchers and spark increased interest in the area. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯å›¾åƒç†è§£ä¸­æœ€åŸºæœ¬çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œæœ‰ç€æ‚ ä¹…çš„å†å²å’Œä¼—å¤šçš„ç ”ç©¶æ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•è‡´åŠ›äºä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®ã€‚éšç€å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å‡ºç°ï¼Œè¯¥ä»»åŠ¡è¦æ±‚æ¨¡å‹å¯¹å­¦åˆ°çš„ç±»åˆ«è¿›è¡Œåˆ†ç±»ä¹‹å¤–çš„å†…å®¹ï¼Œå¤§é‡ç²¾ç»†æ ‡æ³¨çš„æ•°æ®å°†å˜å¾—æä¸ºæ˜‚è´µã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜è½¬è€Œé‡‡ç”¨æ— è®­ç»ƒæ–¹æ³•ï¼Œä»–ä»¬åˆ©ç”¨ä¸ºæ›´å®¹æ˜“è·å–æ•°æ®çš„ä»»åŠ¡æ„å»ºçš„ç°æœ‰æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™ç¯‡ç»¼è¿°å°†ä»‹ç»æ— è®­ç»ƒå¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å†å²ã€ç»†å¾®å·®åˆ«ã€æ€æƒ³å‘å±•å’Œæœ€æ–°ç ”ç©¶æƒ…å†µï¼Œè¯¥åˆ†å‰²åˆ©ç”¨ç°æœ‰çš„å¤šæ¨¡å¼åˆ†ç±»æ¨¡å‹ã€‚æˆ‘ä»¬å°†é¦–å…ˆç»™å‡ºä»»åŠ¡å®šä¹‰çš„åˆæ­¥ä»‹ç»ï¼Œç„¶åæ¦‚è¿°æµè¡Œçš„æ¨¡å‹åŸå‹ï¼Œå†é‡ç‚¹ä»‹ç»è¶…è¿‡30ç§æ–¹æ³•åˆ†ä¸ºå‡ ä¸ªè¾ƒå¤§çš„ç ”ç©¶åˆ†æ”¯ï¼šçº¯åŸºäºCLIPçš„æ–¹æ³•ã€åˆ©ç”¨è¾…åŠ©è§†è§‰åŸºç¡€æ¨¡å‹çš„æ–¹æ³•å’Œä¾èµ–ç”Ÿæˆæ–¹æ³•çš„æ–¹æ³•ã€‚éšåï¼Œæˆ‘ä»¬å°†è®¨è®ºå½“å‰ç ”ç©¶çš„å±€é™æ€§å’Œæ½œåœ¨é—®é¢˜ï¼Œå¹¶æä¾›ä¸€äº›å°šæœªæ¢ç´¢çš„æƒ³æ³•ä¾›æœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ç¯‡ç»¼è¿°å°†ä¸ºæ–°ç ”ç©¶äººå‘˜æä¾›è‰¯å¥½çš„å…¥é—¨è¯»ç‰©ï¼Œå¹¶æ¿€å‘å¯¹è¯¥é¢†åŸŸçš„å…´è¶£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22209v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­ä¹‰åˆ†å‰²åœ¨å›¾åƒç†è§£é¢†åŸŸçš„é‡è¦æ€§åŠå…¶ç ”ç©¶å†å²ã€‚éšç€å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å…´èµ·ï¼Œç ”ç©¶äººå‘˜å¼€å§‹è½¬å‘æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰æ¨¡å‹è§£å†³æ•°æ®æ›´æ˜“è·å–çš„ä»»åŠ¡ã€‚æœ¬æ–‡å›é¡¾äº†è®­ç»ƒå…è´¹å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å†å²ã€ç»†å¾®å·®åˆ«ã€æ€æƒ³å‘å±•å’Œæœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤šæ¨¡å¼åˆ†ç±»æ¨¡å‹çš„æ–¹æ³•ã€‚æ–‡ç« æ¦‚è¿°äº†ä»»åŠ¡å®šä¹‰ã€æµè¡Œæ¨¡å‹åŸå‹ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†30å¤šç§æ–¹æ³•çš„ä¸åŒç ”ç©¶åˆ†æ”¯ã€‚åŒæ—¶ï¼Œæ–‡ç« ä¹Ÿæ¢è®¨äº†å½“å‰ç ”ç©¶çš„å±€é™æ€§å’Œæ½œåœ¨é—®é¢˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€äº›å°šæœªæ¢ç´¢çš„æƒ³æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²æ˜¯å›¾åƒç†è§£ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œæœ‰ç€ä¸°å¯Œçš„ç ”ç©¶å†å²å’Œå„ç§ä¸åŒæ–¹æ³•ã€‚</li>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²è¦æ±‚æ¨¡å‹è¿›è¡Œåˆ†ç±»è¶…å‡ºå·²å­¦ç±»åˆ«ï¼Œéœ€è¦å¤§é‡ç²¾ç»†æ ‡æ³¨çš„æ•°æ®ï¼Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç ”ç©¶äººå‘˜å¼€å§‹è½¬å‘æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰æ¨¡å‹ï¼Œè¿™äº›æ–¹æ³•é’ˆå¯¹æ•°æ®æ›´æ˜“è·å–çš„ä»»åŠ¡ã€‚</li>
<li>æœ¬æ–‡å›é¡¾äº†è®­ç»ƒå…è´¹å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„å†å²ã€å‘å±•å’Œæœ€æ–°è¿›å±•ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†ä»»åŠ¡å®šä¹‰ã€æµè¡Œæ¨¡å‹åŸå‹ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†30å¤šç§æ–¹æ³•çš„ä¸åŒç ”ç©¶åˆ†æ”¯ï¼ŒåŒ…æ‹¬çº¯CLIPæ–¹æ³•ã€åˆ©ç”¨è¾…åŠ©è§†è§‰åŸºç¡€æ¨¡å‹çš„æ–¹æ³•å’Œä¾èµ–ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>å½“å‰ç ”ç©¶çš„å±€é™æ€§åœ¨äºå­˜åœ¨æ½œåœ¨é—®é¢˜ï¼Œæœ¬æ–‡ä¹Ÿè®¨è®ºäº†è¿™äº›é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc000e389ff1dc0b8b1ed28540501e48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1afc6a8da7323b1107dcd0b8cb254ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdcafac5b6077858b3d864af9b956d53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3af6181361d7d460663eb87cc9ef6c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3129a09b4816d6d02e60301cba3fd59.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CAST-Contrastive-Adaptation-and-Distillation-for-Semi-Supervised-Instance-Segmentation"><a href="#CAST-Contrastive-Adaptation-and-Distillation-for-Semi-Supervised-Instance-Segmentation" class="headerlink" title="CAST: Contrastive Adaptation and Distillation for Semi-Supervised   Instance Segmentation"></a>CAST: Contrastive Adaptation and Distillation for Semi-Supervised   Instance Segmentation</h2><p><strong>Authors:Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu</strong></p>
<p>Instance segmentation demands costly per-pixel annotations and large models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pretrained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, (2) distillation into a compact student via a unified multi-objective loss that couples standard supervision and pseudo-labels with our instance-aware pixel-wise contrastive term, and (3) fine-tuning on labeled data to remove residual pseudo-label bias. Central to CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to mine informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs. 15.2) and outperforms state-of-the-art semi-supervised approaches. </p>
<blockquote>
<p>å®ä¾‹åˆ†å‰²éœ€è¦æ˜‚è´µçš„åƒç´ çº§æ ‡æ³¨å’Œå¤§æ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†CASTï¼Œè¿™æ˜¯ä¸€ç§åŠç›‘ç£çŸ¥è¯†è’¸é¦ï¼ˆSSKDï¼‰æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å’Œå¤§é‡çš„æ— æ ‡è®°æ•°æ®ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å‹ç¼©æˆç´§å‡‘çš„ä¸“å®¶æ¨¡å‹ã€‚CASTåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰é€šè¿‡å¯¹æ¯”åƒç´ æ ¡å‡†è¿›è¡Œè‡ªæˆ‘è®­ç»ƒï¼Œå¯¹VFMæ•™å¸ˆè¿›è¡ŒåŸŸé€‚åº”ï¼›ï¼ˆ2ï¼‰é€šè¿‡ç»Ÿä¸€çš„å¤šç›®æ ‡æŸå¤±è¿›è¡Œè’¸é¦ï¼Œè¯¥æŸå¤±å°†æ ‡å‡†ç›‘ç£å’Œä¼ªæ ‡ç­¾ä¸æˆ‘ä»¬çš„å®ä¾‹æ„ŸçŸ¥åƒç´ çº§å¯¹æ¯”é¡¹ç›¸ç»“åˆï¼Œå¯¹ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œè’¸é¦ï¼›ï¼ˆ3ï¼‰åœ¨æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥æ¶ˆé™¤æ®‹ç•™çš„ä¼ªæ ‡ç­¾åè§ã€‚CASTçš„æ ¸å¿ƒæ˜¯<em>å®ä¾‹æ„ŸçŸ¥åƒç´ çº§å¯¹æ¯”æŸå¤±</em>ï¼Œå®ƒèåˆæ©è†œå’Œç±»åˆ†æ•°æ¥æŒ–æ˜ä¿¡æ¯é˜´æ€§æ ·æœ¬å¹¶å¼ºåˆ¶æ‰§è¡Œæ¸…æ™°çš„å®ä¾‹é—´è¾¹ç•Œã€‚é€šè¿‡åœ¨æ•´ä¸ªé€‚åº”å’Œè’¸é¦è¿‡ç¨‹ä¸­ä¿æŒè¿™ç§å¯¹æ¯”ä¿¡å·ï¼Œæˆ‘ä»¬å¯¹é½æ•™å¸ˆå’Œå­¦ç”ŸåµŒå…¥ï¼Œå¹¶å……åˆ†åˆ©ç”¨æ— æ ‡ç­¾å›¾åƒã€‚åœ¨Cityscapeså’ŒADE20Kä¸Šï¼Œæˆ‘ä»¬è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ï¼ˆ~11å€ï¼‰è¶…è¶Šäº†å…¶é€‚åº”çš„VFMæ•™å¸ˆæ¨¡å‹ï¼ˆ+3.4 APï¼ˆ33.9å¯¹30.5ï¼‰å’Œ+1.5 APï¼ˆ16.7å¯¹15.2ï¼‰ï¼‰ï¼Œå¹¶ä¼˜äºæœ€æ–°çš„åŠç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21904v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºåŠç›‘ç£çŸ¥è¯†è’¸é¦ï¼ˆSSKDï¼‰çš„å‹ç¼©æ¡†æ¶CASTï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æœ‰é™çš„æ ‡ç­¾æ•°æ®å’Œå¤§é‡çš„æ— æ ‡ç­¾æ•°æ®å°†é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰å‹ç¼©æˆç´§å‡‘çš„ä¸“å®¶æ¨¡å‹ã€‚CASTåˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šVFMæ•™å¸ˆçš„åŸŸè‡ªé€‚åº”ã€é€šè¿‡ç»Ÿä¸€çš„å¤šç›®æ ‡æŸå¤±è’¸é¦åˆ°ç´§å‡‘çš„å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥åŠä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒä»¥æ¶ˆé™¤æ®‹ç•™çš„ä¼ªæ ‡ç­¾åè§ã€‚æ ¸å¿ƒåœ¨äºå®ä¾‹æ„ŸçŸ¥åƒç´ çº§å¯¹æ¯”æŸå¤±ï¼Œè¯¥æŸå¤±èåˆäº†æ©è†œå’Œç±»åˆ«åˆ†æ•°ä»¥æŒ–æ˜ä¿¡æ¯è´Ÿæ ·æœ¬å¹¶æ˜ç¡®å®ä¾‹é—´çš„è¾¹ç•Œã€‚é€šè¿‡åœ¨æ•´ä¸ªé€‚åº”å’Œè’¸é¦è¿‡ç¨‹ä¸­ä¿æŒå¯¹æ¯”ä¿¡å·ï¼Œæˆ‘ä»¬å®ç°äº†æ•™å¸ˆå’Œå­¦ç”ŸåµŒå…¥çš„å¯¹é½ï¼Œå¹¶å……åˆ†åˆ©ç”¨äº†æ— æ ‡ç­¾å›¾åƒã€‚åœ¨Cityscapeså’ŒADE20Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„å­¦ç”Ÿæ¨¡å‹è¶…è¶Šäº†å…¶é€‚åº”çš„VFMæ•™å¸ˆæ¨¡å‹ï¼Œå¹¶è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CASTåˆ©ç”¨æœ‰é™çš„æ ‡ç­¾æ•°æ®å’Œå¤§é‡çš„æ— æ ‡ç­¾æ•°æ®å‹ç¼©é¢„è®­ç»ƒè§†è§‰åŸºç¡€æ¨¡å‹ã€‚</li>
<li>CASTåˆ†ä¸ºä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ•™å¸ˆåŸŸè‡ªé€‚åº”ã€é€šè¿‡å¤šç›®æ ‡æŸå¤±è’¸é¦å­¦ç”Ÿæ¨¡å‹å’Œå¾®è°ƒã€‚</li>
<li>å®ä¾‹æ„ŸçŸ¥åƒç´ çº§å¯¹æ¯”æŸå¤±æ˜¯CASTçš„æ ¸å¿ƒï¼Œèåˆäº†æ©è†œå’Œç±»åˆ«åˆ†æ•°ã€‚</li>
<li>å¯¹æ¯”ä¿¡å·åœ¨é€‚åº”å’Œè’¸é¦è¿‡ç¨‹ä¸­éƒ½è¢«ä¿æŒã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹å¤§å°å’Œæ€§èƒ½è¶…è¶Šäº†å…¶é€‚åº”çš„VFMæ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>åœ¨Cityscapeså’ŒADE20Kæ•°æ®é›†ä¸Šï¼ŒCASTè¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0db3cb819b5be1bb89bb0c46b711f056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b63e42239366ca09998e9f4c0458884.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f70f00a53c481d6c196dbc3cf0e47d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Test-Time-Adaptation-of-Vision-Language-Models-for-Open-Vocabulary-Semantic-Segmentation"><a href="#Test-Time-Adaptation-of-Vision-Language-Models-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Test-Time Adaptation of Vision-Language Models for Open-Vocabulary   Semantic Segmentation"></a>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers</strong></p>
<p>Recently, test-time adaptation has attracted wide interest in the context of vision-language models for image classification. However, to the best of our knowledge, the problem is completely overlooked in dense prediction tasks such as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a novel TTA method tailored to adapting VLMs for segmentation during test time. Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates features from intermediate vision-encoder layers and is performed with different text-prompt templates at both the global CLS token and local pixel-wise levels. Our approach could be used as plug-and-play for any segmentation network, does not require additional training data or labels, and remains effective even with a single test sample. Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which integrates a rigorous evaluation protocol, seven segmentation datasets, and 15 common corruptions, with a total of 82 distinct test scenarios, establishing a standardized and comprehensive testbed for future TTA research in open-vocabulary segmentation. Our experiments on this suite demonstrate that our segmentation-tailored method consistently delivers significant gains over direct adoption of TTA classification baselines. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæµ‹è¯•æ—¶é—´é€‚åº”åœ¨å›¾åƒåˆ†ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹èƒŒæ™¯ä¸‹å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¯¥é—®é¢˜åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼‰ä¸­å®Œå…¨è¢«å¿½ç•¥äº†ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„TTAæ–¹æ³•ï¼Œä¸“ä¸ºæµ‹è¯•æ—¶é€‚åº”VLMsè¿›è¡Œåˆ†å‰²è€Œè®¾è®¡ã€‚ä¸ç”¨äºå›¾åƒåˆ†ç±»çš„TTAæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„å¤šå±‚æ¬¡å¤šæç¤ºï¼ˆMLMPï¼‰ç†µæœ€å°åŒ–ç»“åˆäº†ä¸­é—´è§†è§‰ç¼–ç å™¨å±‚çš„ç‰¹å¾ï¼Œå¹¶åœ¨å…¨å±€CLSæ ‡è®°å’Œå±€éƒ¨åƒç´ çº§ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿è¿›è¡Œã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„å³æ’å³ç”¨å·¥å…·ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ï¼Œå³ä½¿åœ¨å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šä¹Ÿèƒ½ä¿æŒæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¥æ ¼çš„è¯„ä¼°åè®®ã€ä¸ƒä¸ªåˆ†å‰²æ•°æ®é›†å’Œ15ç§å¸¸è§è…èš€ï¼Œæ€»å…±82ç§ä¸åŒçš„æµ‹è¯•åœºæ™¯ï¼Œä¸ºæœªæ¥çš„å¼€æ”¾è¯æ±‡åˆ†å‰²TTAç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–å’Œå…¨é¢çš„æµ‹è¯•å¹³å°ã€‚åœ¨æ­¤å¥—ä»¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é’ˆå¯¹åˆ†å‰²çš„æ–¹æ³•å§‹ç»ˆåœ¨ç›´æ¥é‡‡ç”¨TTAåˆ†ç±»åŸºçº¿æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æµ‹è¯•æ—¶é—´è‡ªé€‚åº”æŠ€æœ¯åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å›¾åƒåˆ†ç±»åº”ç”¨ï¼Œä½†åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡å¦‚å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­å´è¢«å¿½è§†ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹åˆ†å‰²ä»»åŠ¡çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡å¤šçº§åˆ«å’Œå¤šæç¤ºçš„ç†µæœ€å°åŒ–ç­–ç•¥ï¼Œæ•´åˆè§†è§‰ç¼–ç å™¨ä¸­é—´å±‚çš„ç‰¹å¾ï¼Œå¹¶åœ¨å…¨å±€CLSæ ‡è®°å’Œå±€éƒ¨åƒç´ çº§ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿ã€‚è¯¥æ–¹æ³•å¯ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ï¼Œç”šè‡³åœ¨å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šä¹Ÿèƒ½ä¿æŒæœ‰æ•ˆã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼Œä¸ºæœªæ¥TTAåœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–å’Œå…¨é¢çš„æµ‹è¯•å¹³å°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šç›¸è¾ƒäºç›´æ¥é‡‡ç”¨TTAåˆ†ç±»åŸºçº¿æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´è‡ªé€‚åº”æŠ€æœ¯åœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å›¾åƒåˆ†ç±»ä¸­å—åˆ°å…³æ³¨ï¼Œä½†åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡å¦‚å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­è¢«å¿½è§†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹åˆ†å‰²ä»»åŠ¡çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡å¤šçº§åˆ«å’Œå¤šæç¤ºçš„ç†µæœ€å°åŒ–ç­–ç•¥æ•´åˆç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼Œä¸ºæœªæ¥TTAç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–æµ‹è¯•å¹³å°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿ï¼Œè¯¥æ–¹æ³•åœ¨å…¨å±€CLSæ ‡è®°å’Œå±€éƒ¨åƒç´ çº§éƒ½æœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-311bcde9f628fe1a7218fc71e0e3c759.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3fd30ab03280f77a9da90e2ec751830.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c823f526a63d7958516de32023d809e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation"><a href="#SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation" class="headerlink" title="SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation"></a>SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</h2><p><strong>Authors:Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone</strong></p>
<p>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a>. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬åˆ†å‰²æ—¨åœ¨ä»å°‘é‡æ ‡æ³¨çš„æ ·æœ¬ä¸­å¯¹æœªè§è¿‡çš„ç›®æ ‡ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚è¿™éœ€è¦èƒ½å¤Ÿåœ¨å›¾åƒä¸­è¯†åˆ«è¯­ä¹‰ç›¸å…³å¯¹è±¡å¹¶å‡†ç¡®ç”Ÿæˆåˆ†å‰²æ©ç çš„æœºåˆ¶ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå€ŸåŠ©æç¤ºå’Œä¼ æ’­çš„Segment Anything 2ï¼ˆSAM2ï¼‰æä¾›äº†å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œå†…ç½®çš„ç‰¹å¾åŒ¹é…è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜å…¶è¡¨ç¤ºä¸é’ˆå¯¹å¯¹è±¡è·Ÿè¸ªä¼˜åŒ–çš„ç‰¹å®šä»»åŠ¡çº¿ç´¢çº ç¼ åœ¨ä¸€èµ·ï¼Œè¿™æŸå®³äº†å…¶åœ¨éœ€è¦é«˜çº§è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­çš„ä½¿ç”¨ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡SAM2å…·æœ‰ç±»æ— å…³çš„é¢„è®­ç»ƒï¼Œä½†å®ƒå·²ç»åœ¨ç‰¹å¾ä¸­ç¼–ç äº†ä¸°å¯Œçš„è¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†SANSAï¼ˆè¯­ä¹‰å¯¹é½çš„Segment Anything 2ï¼‰ï¼Œä¸€ä¸ªä½¿è¿™ç§æ½œåœ¨ç»“æ„æ˜ç¡®çš„æ¡†æ¶ï¼Œå¹¶é€šè¿‡æœ€å°‘çš„ç‰¹å®šä»»åŠ¡ä¿®æ”¹å°†SAM2é‡æ–°ç”¨äºå°‘æ•°æ ·æœ¬åˆ†å‰²ã€‚SANSAåœ¨ä¸“é—¨ä¸ºè¯„ä¼°æ³›åŒ–èƒ½åŠ›è®¾è®¡çš„å°‘æ•°æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æµè¡Œçš„ä¸Šä¸‹æ–‡è®¾ç½®ä¸­çš„å…¨èƒ½æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒé€šè¿‡ç‚¹ã€æ¡†æˆ–æ¶‚é¸¦è¿›è¡Œå„ç§æç¤ºçš„çµæ´»äº¤äº’ï¼Œå¹¶ä¸”ç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•ï¼Œé€Ÿåº¦æ›´å¿«ã€æ›´ç´§å‡‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Claudiacuttano/SANSA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ClaudiaCuttano/SANSAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21795v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a></p>
<p><strong>Summary</strong></p>
<p>SAM2æ¨¡å‹å…·æœ‰å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œå†…ç½®çš„ç‰¹å¾åŒ¹é…è¿‡ç¨‹ï¼Œä½†å…¶åœ¨å°‘æ•°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°å—åˆ°ä»»åŠ¡ç‰¹å®šçº¿ç´¢çš„å¹²æ‰°ã€‚æˆ‘ä»¬æå‡ºSANSAæ¡†æ¶ï¼Œé€šè¿‡ä½¿SAM2çš„æ½œåœ¨ç»“æ„æ˜¾æ€§åŒ–å¹¶å¯¹å…¶è¿›è¡Œæœ€å°ç‰¹å®šçš„ä»»åŠ¡ä¿®æ”¹ï¼Œä½¿å…¶æˆä¸ºé€‚åˆå°‘æ•°æ ·æœ¬åˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆå·¥å…·ã€‚SANSAå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œæ”¯æŒå„ç§æç¤ºçµæ´»çš„äº¤äº’æ–¹å¼ï¼Œå¹¶ä¸”ç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•æ›´åŠ å¿«é€Ÿå’Œç´§å‡‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2æ¨¡å‹å…·æœ‰å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œç‰¹å¾åŒ¹é…è¿‡ç¨‹ã€‚</li>
<li>SAM2æ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°å—åˆ°ä»»åŠ¡ç‰¹å®šçº¿ç´¢çš„å¹²æ‰°ã€‚</li>
<li>SANSAæ¡†æ¶åˆ©ç”¨SAM2çš„æ½œåœ¨ç»“æ„è¿›è¡Œå°‘æ•°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>SANSAæ¡†æ¶é€šè¿‡æœ€å°ç‰¹å®šçš„ä»»åŠ¡ä¿®æ”¹ä½¿SAM2æ¨¡å‹é€‚åˆå°‘æ•°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>SANSAåœ¨å°‘æ•°æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œä¸”ä¼˜äºå…ˆå‰çš„é€šç”¨æ–¹æ³•ã€‚</li>
<li>SANSAæ”¯æŒå„ç§æç¤ºçµæ´»çš„äº¤äº’æ–¹å¼ï¼Œä¾¿äºç”¨æˆ·è¿›è¡Œä½¿ç”¨å’Œæ“ä½œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-776453b1b78a989e79af018be9145c4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf2b505a75d0ad66863efb87ec4357c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-022dba601551ed556c01f471a95365d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84853eff613e8afa690dddf511e22788.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SegRet-An-Efficient-Design-for-Semantic-Segmentation-with-Retentive-Network"><a href="#SegRet-An-Efficient-Design-for-Semantic-Segmentation-with-Retentive-Network" class="headerlink" title="SegRet: An Efficient Design for Semantic Segmentation with Retentive   Network"></a>SegRet: An Efficient Design for Semantic Segmentation with Retentive   Network</h2><p><strong>Authors:Zhiyuan Li, Yi Chang, Yuan Wu</strong></p>
<p>With the rapid evolution of autonomous driving technology and intelligent transportation systems, semantic segmentation has become increasingly critical. Precise interpretation and analysis of real-world environments are indispensable for these advanced applications. However, traditional semantic segmentation approaches frequently face challenges in balancing model performance with computational efficiency, especially regarding the volume of model parameters. To address these constraints, we propose SegRet, a novel model employing the Retentive Network (RetNet) architecture coupled with a lightweight residual decoder that integrates zero-initialization. SegRet offers three distinctive advantages: (1) Lightweight Residual Decoder: by embedding a zero-initialization layer within the residual network structure, the decoder remains computationally streamlined without sacrificing essential information propagation; (2) Robust Feature Extraction: adopting RetNet as its backbone enables SegRet to effectively capture hierarchical image features, thereby enriching the representation quality of extracted features; (3) Parameter Efficiency: SegRet attains state-of-the-art (SOTA) segmentation performance while markedly decreasing the number of parameters, ensuring high accuracy without imposing additional computational burdens. Comprehensive empirical evaluations on prominent benchmarks, such as ADE20K, Citycapes, and COCO-Stuff, highlight the effectiveness and superiority of our method. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿçš„å¿«é€Ÿå‘å±•ï¼Œè¯­ä¹‰åˆ†å‰²çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚ç²¾ç¡®è§£è¯»å’Œåˆ†æçœŸå®ç¯å¢ƒå¯¹è¿™äº›é«˜çº§åº”ç”¨æ¥è¯´æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•å¸¸å¸¸åœ¨å¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…³äºæ¨¡å‹å‚æ•°ä½“ç§¯çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†SegRetï¼Œä¸€ä¸ªé‡‡ç”¨Retentive Networkï¼ˆRetNetï¼‰æ¶æ„ç»“åˆè½»é‡çº§æ®‹å·®è§£ç å™¨çš„æ–°å‹æ¨¡å‹ï¼Œè¯¥è§£ç å™¨é›†æˆäº†é›¶åˆå§‹åŒ–ã€‚SegRetå…·æœ‰ä¸‰ä¸ªæ˜¾è‘—ä¼˜åŠ¿ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰è½»é‡çº§æ®‹å·®è§£ç å™¨ï¼šé€šè¿‡åœ¨æ®‹å·®ç½‘ç»œç»“æ„ä¸­åµŒå…¥é›¶åˆå§‹åŒ–å±‚ï¼Œè§£ç å™¨åœ¨è®¡ç®—ä¸Šä¿æŒç®€æ´ï¼ŒåŒæ—¶ä¸ç‰ºç‰²é‡è¦ä¿¡æ¯çš„ä¼ æ’­ï¼›</p>
<p>ï¼ˆ2ï¼‰ç¨³å¥çš„ç‰¹å¾æå–ï¼šé‡‡ç”¨RetNetä½œä¸ºéª¨å¹²ç½‘ï¼Œä½¿SegRetèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·åˆ†å±‚å›¾åƒç‰¹å¾ï¼Œä»è€Œä¸°å¯Œæå–ç‰¹å¾çš„è¡¨ç°è´¨é‡ï¼›</p>
<p>ï¼ˆ3ï¼‰å‚æ•°æ•ˆç‡ï¼šSegRetåœ¨å‡å°‘å‚æ•°æ•°é‡çš„åŒæ—¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œç¡®ä¿é«˜ç²¾åº¦ä¸ä¼šå¸¦æ¥é¢å¤–çš„è®¡ç®—è´Ÿæ‹…ã€‚åœ¨ADE20Kã€Citycapeså’ŒCOCO-Stuffç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®è¯è¯„ä¼°ï¼Œçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14014v2">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>éšç€è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„é£é€Ÿå‘å±•ï¼Œè¯­ä¹‰åˆ†å‰²æŠ€æœ¯å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç²¾ç¡®è§£è¯»å’Œåˆ†æçœŸå®ç¯å¢ƒå¯¹è¿™äº›é«˜çº§åº”ç”¨ä¸å¯æˆ–ç¼ºã€‚ä¼ ç»Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•å¸¸åœ¨æ¨¡å‹æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶ä½“ç°åœ¨æ¨¡å‹å‚æ•°è§„æ¨¡ä¸Šã€‚æˆ‘ä»¬æå‡ºSegRetæ¨¡å‹ï¼Œé‡‡ç”¨Retentive Networkï¼ˆRetNetï¼‰æ¶æ„ç»“åˆè½»é‡åŒ–æ®‹å·®è§£ç å™¨ï¼Œé›†æˆé›¶åˆå§‹åŒ–æŠ€æœ¯ã€‚SegRetå…·å¤‡ä¸‰å¤§ä¼˜åŠ¿ï¼šä¸€ã€è½»é‡åŒ–æ®‹å·®è§£ç å™¨ï¼šåœ¨æ®‹å·®ç½‘ç»œç»“æ„ä¸­åµŒå…¥é›¶åˆå§‹åŒ–å±‚ï¼Œä¿è¯è§£ç å™¨è®¡ç®—æµç¨‹ç®€æ´ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å…³é”®ä¿¡æ¯ä¼ è¾“ï¼›äºŒã€ç¨³å¥ç‰¹å¾æå–ï¼šé‡‡ç”¨RetNetä½œä¸ºéª¨å¹²ç½‘ï¼Œä½¿SegRetèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å›¾åƒå±‚æ¬¡ç‰¹å¾ï¼Œä»è€Œæé«˜ç‰¹å¾è¡¨ç¤ºè´¨é‡ï¼›ä¸‰ã€å‚æ•°é«˜æ•ˆï¼šSegRetåœ¨å‡å°‘å‚æ•°æ•°é‡çš„åŒæ—¶è¾¾åˆ°æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œç¡®ä¿é«˜ç²¾åº¦ä¸”ä¸ä¼šå¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚åœ¨ADE20Kã€Citycapeså’ŒCOCO-Stuffç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œéœ€è¦ç²¾ç¡®è§£è¯»å’Œåˆ†æçœŸå®ç¯å¢ƒã€‚</li>
<li>ä¼ ç»Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•é¢ä¸´æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¨¡å‹å‚æ•°è§„æ¨¡ã€‚</li>
<li>SegRetæ¨¡å‹é‡‡ç”¨RetNetæ¶æ„å’Œè½»é‡åŒ–æ®‹å·®è§£ç å™¨ï¼Œé›†æˆé›¶åˆå§‹åŒ–æŠ€æœ¯ã€‚</li>
<li>SegRetå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šè½»é‡åŒ–ã€ç¨³å¥ç‰¹å¾æå–å’Œå‚æ•°é«˜æ•ˆã€‚</li>
<li>SegRetåœ¨ä¿è¯é«˜åˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œä¸”ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>åœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSegRetè¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14014">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5e4de0369a070737f85c6d8bfe21669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6901509a51befecb4138f7ffd6d09d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3065b470f75b9358e94c1113aa9662c1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Complex-Wavelet-Mutual-Information-Loss-A-Multi-Scale-Loss-Function-for-Semantic-Segmentation"><a href="#Complex-Wavelet-Mutual-Information-Loss-A-Multi-Scale-Loss-Function-for-Semantic-Segmentation" class="headerlink" title="Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for   Semantic Segmentation"></a>Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for   Semantic Segmentation</h2><p><strong>Authors:Renhao Lu</strong></p>
<p>Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose the complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited to capturing high-dimensional directional features and offers greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lurenhaothu/CWMI">https://github.com/lurenhaothu/CWMI</a> </p>
<blockquote>
<p>æœ€è¿‘æ·±åº¦ç¥ç»ç½‘ç»œçš„å‘å±•æå¤§åœ°æé«˜äº†è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç±»åˆ«ä¸å¹³è¡¡å’Œå®ä¾‹ä¸å¹³è¡¡ä»ç„¶æŒç»­å­˜åœ¨æŒ‘æˆ˜ï¼Œè¾ƒå°çš„å®ä¾‹å’Œè–„è¾¹ç•Œé€šå¸¸è¢«è¾ƒå¤§çš„ç»“æ„æ‰€æ©ç›–ã€‚ä¸ºäº†è§£å†³åˆ†å‰²å¯¹è±¡çš„å¤šå°ºåº¦ç‰¹æ€§ï¼Œå„ç§æ¨¡å‹å·²ç»èå…¥äº†ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶å’Œç‰¹å¾é‡‘å­—å¡”ç½‘ç»œç­‰æœºåˆ¶ã€‚å°½ç®¡æœ‰äº†è¿™äº›è¿›å±•ï¼Œå¤§å¤šæ•°æŸå¤±å‡½æ•°ä»ç„¶æ˜¯åŸºäºåƒç´ çš„ï¼Œè€ŒåŒºåŸŸæ€§å’Œè¾¹ç•Œèšç„¦çš„æŸå¤±å‡½æ•°å¾€å¾€å¸¦æ¥è¾ƒé«˜çš„è®¡ç®—æˆæœ¬æˆ–ä»…é™äºå°è§„æ¨¡åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤æ‚å°æ³¢äº’ä¿¡æ¯ï¼ˆCWMIï¼‰æŸå¤±è¿™ä¸€æ–°å‹æŸå¤±å‡½æ•°ï¼Œå®ƒåˆ©ç”¨ç”±å¤æ‚å¯è½¬å‘é‡‘å­—å¡”åˆ†è§£å¾—åˆ°çš„å­å¸¦å›¾åƒçš„äº’ä¿¡æ¯ã€‚å¤æ‚å¯è½¬å‘é‡‘å­—å¡”èƒ½å¤Ÿæ•è·å¤šä¸ªæ–¹å‘çš„ç‰¹æ€§ï¼Œå¹¶åœ¨ä¸åŒå°ºåº¦ä¸Šä¿ç•™ç»“æ„ç›¸ä¼¼æ€§ã€‚åŒæ—¶ï¼Œäº’ä¿¡æ¯éå¸¸é€‚åˆæ•æ‰é«˜ç»´æ–¹å‘ç‰¹æ€§ï¼Œå¹¶æä¾›äº†æ›´å¼ºçš„å™ªå£°é²æ£’æ€§ã€‚åœ¨å¤šç§åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒCWMIæŸå¤±åœ¨åƒç´ çº§ç²¾åº¦å’Œæ‹“æ‰‘æŒ‡æ ‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶å¼•å…¥äº†æä½çš„è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lurenhaothu/CWMI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lurenhaothu/CWMIæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00563v2">PDF</a> Accepted at ICML 2025. This version corresponds to the official   camera-ready submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­ä¹‰åˆ†å‰²ä¸­çš„å¤šå°ºåº¦é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°â€”â€”å¤æ‚å°æ³¢äº’ä¿¡æ¯ï¼ˆCWMIï¼‰æŸå¤±ã€‚è¯¥å‡½æ•°åˆ©ç”¨ç”±å¤æ‚å¯è½¬å‘é‡‘å­—å¡”åˆ†è§£å¾—åˆ°çš„å­å¸¦å›¾åƒçš„äº’ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°æé«˜äº†åƒç´ çº§ç²¾åº¦å’Œæ‹“æ‰‘åº¦é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ç±»åˆ«ä¸å¹³è¡¡å’Œå®ä¾‹ä¸å¹³è¡¡ï¼Œå…¶ä¸­è¾ƒå°çš„å®ä¾‹å’Œè–„è¾¹ç•Œå¸¸è¢«è¾ƒå¤§çš„ç»“æ„æ‰€æ©ç›–ã€‚</li>
<li>ä¸ºäº†åº”å¯¹åˆ†å‰²å¯¹è±¡çš„å¤šå°ºåº¦ç‰¹æ€§ï¼Œå·²å­˜åœ¨çš„æ¨¡å‹é‡‡ç”¨äº†ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶å’Œç‰¹å¾é‡‘å­—å¡”ç½‘ç»œç­‰æ–¹æ³•ã€‚</li>
<li>ç°æœ‰çš„æŸå¤±å‡½æ•°ä¸»è¦æ˜¯åƒç´ çº§çš„ï¼Œè€ŒåŒºåŸŸå’Œè¾¹ç•Œèšç„¦çš„æŸå¤±å‡½æ•°è®¡ç®—æˆæœ¬è¾ƒé«˜æˆ–ä»…é™äºå°è§„æ¨¡åŒºåŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°â€”â€”å¤æ‚å°æ³¢äº’ä¿¡æ¯ï¼ˆCWMIï¼‰æŸå¤±ï¼Œå®ƒåˆ©ç”¨å¤æ‚å¯è½¬å‘é‡‘å­—å¡”åˆ†è§£çš„äº’ä¿¡æ¯ã€‚</li>
<li>å¤æ‚å¯è½¬å‘é‡‘å­—å¡”èƒ½å¤Ÿæ•æ‰å¤šä¸ªæ–¹å‘çš„ç‰¹æ€§å¹¶ä¿æŒè·¨å°ºåº¦çš„ç»“æ„ç›¸ä¼¼æ€§ã€‚</li>
<li>äº’ä¿¡æ¯é€‚ç”¨äºæ•æ‰é«˜ç»´æ–¹å‘ç‰¹æ€§ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„å™ªå£°é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f6a3e7875c53d8265ae2ea74d6e5322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6df59c774af50decb248437acc23be55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed968d3b72687fb19af829e2601c917b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad5d01713314315aafdced9f0f4fa40.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Cross-Layer-Feature-Pyramid-Transformer-for-Small-Object-Detection-in-Aerial-Images"><a href="#Cross-Layer-Feature-Pyramid-Transformer-for-Small-Object-Detection-in-Aerial-Images" class="headerlink" title="Cross-Layer Feature Pyramid Transformer for Small Object Detection in   Aerial Images"></a>Cross-Layer Feature Pyramid Transformer for Small Object Detection in   Aerial Images</h2><p><strong>Authors:Zewen Du, Zhenjiang Hu, Guiyu Zhao, Ying Jin, Hongbin Ma</strong></p>
<p>Object detection in aerial images has always been a challenging task due to the generally small size of the objects. Most current detectors prioritize the development of new detection frameworks, often overlooking research on fundamental components such as feature pyramid networks. In this paper, we introduce the Cross-Layer Feature Pyramid Transformer (CFPT), a novel upsampler-free feature pyramid network designed specifically for small object detection in aerial images. CFPT incorporates two meticulously designed attention blocks with linear computational complexity: Cross-Layer Channel-Wise Attention (CCA) and Cross-Layer Spatial-Wise Attention (CSA). CCA achieves cross-layer interaction by dividing channel-wise token groups to perceive cross-layer global information along the spatial dimension, while CSA enables cross-layer interaction by dividing spatial-wise token groups to perceive cross-layer global information along the channel dimension. By integrating these modules, CFPT enables efficient cross-layer interaction in a single step, thereby avoiding the semantic gap and information loss associated with element-wise summation and layer-by-layer transmission. In addition, CFPT incorporates global contextual information, which improves detection performance for small objects. To further enhance location awareness during cross-layer interaction, we propose the Cross-Layer Consistent Relative Positional Encoding (CCPE) based on inter-layer mutual receptive fields. We evaluate the effectiveness of CFPT on three challenging object detection datasets in aerial images: VisDrone2019-DET, TinyPerson, and xView. Extensive experiments demonstrate that CFPT outperforms state-of-the-art feature pyramid networks while incurring lower computational costs. The code is available at <a target="_blank" rel="noopener" href="https://github.com/duzw9311/CFPT">https://github.com/duzw9311/CFPT</a>. </p>
<blockquote>
<p>åœ¨èˆªç©ºå›¾åƒä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹ä¸€ç›´æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦æ˜¯å› ä¸ºç›®æ ‡é€šå¸¸å°ºå¯¸è¾ƒå°ã€‚ç›®å‰å¤§å¤šæ•°æ¢æµ‹å™¨éƒ½ä¾§é‡äºå¼€å‘æ–°çš„æ£€æµ‹æ¡†æ¶ï¼Œå¾€å¾€å¿½è§†äº†å…³äºåŸºç¡€ç»„ä»¶çš„ç ”ç©¶ï¼Œå¦‚ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è·¨å±‚ç‰¹å¾é‡‘å­—å¡”è½¬æ¢å™¨ï¼ˆCFPTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºèˆªç©ºå›¾åƒä¸­çš„å°ç›®æ ‡æ£€æµ‹è®¾è®¡çš„æ— ä¸Šé‡‡æ ·å™¨ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œã€‚CFPTèåˆäº†ä¸¤ä¸ªç²¾å¿ƒè®¾è®¡çš„å…·æœ‰çº¿æ€§è®¡ç®—å¤æ‚åº¦çš„æ³¨æ„åŠ›å—ï¼šè·¨å±‚é€šé“æ³¨æ„åŠ›ï¼ˆCCAï¼‰å’Œè·¨å±‚ç©ºé—´æ³¨æ„åŠ›ï¼ˆCSAï¼‰ã€‚CCAé€šè¿‡æ²¿ç©ºé—´ç»´åº¦å°†é€šé“ä»¤ç‰Œåˆ†ç»„æ¥å®ç°è·¨å±‚äº¤äº’ï¼Œä»¥æ„ŸçŸ¥è·¨å±‚å…¨å±€ä¿¡æ¯ï¼Œè€ŒCSAé€šè¿‡æ²¿é€šé“ç»´åº¦å°†ç©ºé—´ä»¤ç‰Œåˆ†ç»„æ¥å®ç°è·¨å±‚äº¤äº’ã€‚é€šè¿‡é›†æˆè¿™äº›æ¨¡å—ï¼ŒCFPTèƒ½å¤Ÿåœ¨å•æ­¥ä¸­å®ç°é«˜æ•ˆçš„è·¨å±‚äº¤äº’ï¼Œä»è€Œé¿å…äº†é€å…ƒç´ æ±‚å’Œå’Œé€å±‚ä¼ è¾“æ‰€å¸¦æ¥çš„è¯­ä¹‰é¸¿æ²Ÿå’Œä¿¡æ¯æŸå¤±ã€‚æ­¤å¤–ï¼ŒCFPTç»“åˆäº†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜äº†å¯¹å°ç›®æ ‡çš„æ£€æµ‹æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è·¨å±‚äº¤äº’è¿‡ç¨‹ä¸­çš„ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Œæˆ‘ä»¬åŸºäºå±‚é—´ç›¸äº’æ„Ÿå—é‡æå‡ºäº†è·¨å±‚ä¸€è‡´ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆCCPEï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„èˆªç©ºå›¾åƒç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†CFPTçš„æœ‰æ•ˆæ€§ï¼šVisDrone2019-DETã€TinyPersonå’ŒxViewã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCFPTåœ¨å…·æœ‰è¾ƒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¼˜äºæœ€æ–°çš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/duzw9311/CFPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/duzw9311/CFPTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.19696v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å±‚ç‰¹å¾é‡‘å­—å¡”è½¬æ¢å™¨ï¼ˆCFPTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºç©ºä¸­å›¾åƒå°ç›®æ ‡æ£€æµ‹è®¾è®¡çš„æ— ä¸Šé‡‡æ ·å™¨ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œã€‚å®ƒåŒ…å«ä¸¤ç§è®¾è®¡ç²¾å·§çš„æ³¨æ„åŠ›å—ï¼šè·¨å±‚é€šé“æ³¨æ„åŠ›ï¼ˆCCAï¼‰å’Œè·¨å±‚ç©ºé—´æ³¨æ„åŠ›ï¼ˆCSAï¼‰ã€‚CFPTé€šè¿‡æ•´åˆè¿™äº›æ¨¡å—ï¼Œå®ç°äº†è·¨å±‚äº¤äº’ï¼Œé¿å…äº†è¯­ä¹‰é¸¿æ²Ÿå’Œä¿¡æ¯æŸå¤±ã€‚æ­¤å¤–ï¼ŒCFPTè¿˜èå…¥äº†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜äº†å¯¹å°ç›®æ ‡çš„æ£€æµ‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºä¸­å›¾åƒç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè¯„ä¼°äº†CFPTçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜å…¶æ€§èƒ½ä¼˜äºå½“å‰ä¸»æµçš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å±‚ç‰¹å¾é‡‘å­—å¡”è½¬æ¢å™¨ï¼ˆCFPTï¼‰ï¼Œé’ˆå¯¹ç©ºä¸­å›¾åƒçš„å°ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>CFPTåŒ…å«ä¸¤ç§æ³¨æ„åŠ›å—ï¼šè·¨å±‚é€šé“æ³¨æ„åŠ›ï¼ˆCCAï¼‰å’Œè·¨å±‚ç©ºé—´æ³¨æ„åŠ›ï¼ˆCSAï¼‰ï¼Œå¯å®ç°é«˜æ•ˆçš„è·¨å±‚äº¤äº’ã€‚</li>
<li>CFPTé¿å…äº†è¯­ä¹‰é¸¿æ²Ÿå’Œä¿¡æ¯æŸå¤±çš„é—®é¢˜ã€‚</li>
<li>CFPTèå…¥äº†å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥æé«˜å¯¹å°ç›®æ ‡çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºä¸­å›¾åƒç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜CFPTçš„æ€§èƒ½ä¼˜äºå…¶ä»–ä¸»æµæ–¹æ³•ã€‚</li>
<li>CFPTçš„è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.19696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0d64c200e936f6b5a66b59f1d959813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45ee09da84121ea441c4bdbe28e254d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3ced45756a21dd1f9c2e96f1a3d172.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d8bb05fce3134439d8a4c244a6be4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97cd0e753066239efea29f6e8289f8ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b79df1325bd39020ad41e1c176ebf6b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6e7815ecf4da49216c74c947f4a8ef96.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Single Domain Generalization for Alzheimer's Detection from 3D MRIs with   Pseudo-Morphological Augmentations and Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9b73bf18f8ffd9cdbf8a8688234a1e2a.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Beyond 1D Vision Transformers and Multichannel Signal Images for   PPG-to-ECG Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
