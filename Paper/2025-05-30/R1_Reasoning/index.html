<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  The Climb Carves Wisdom Deeper Than the Summit On the Noisy Rewards in   Learning to Reason">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3680abbc5a69eaec1fad8570334733c8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="The-Climb-Carves-Wisdom-Deeper-Than-the-Summit-On-the-Noisy-Rewards-in-Learning-to-Reason"><a href="#The-Climb-Carves-Wisdom-Deeper-Than-the-Summit-On-the-Noisy-Rewards-in-Learning-to-Reason" class="headerlink" title="The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in   Learning to Reason"></a>The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in   Learning to Reason</h2><p><strong>Authors:Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan</strong></p>
<p>Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward functionâ€™s outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as &#96;&#96;first, I need toâ€™â€™-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLMâ€™s performance on open-ended tasks. These findings suggest the importance of improving modelsâ€™ foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at <a target="_blank" rel="noopener" href="https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason">https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason</a>. </p>
<blockquote>
<p>å…³äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¨ç†è®­ç»ƒçš„è¿‘æœŸç ”ç©¶é€šå¸¸ä¾§é‡äºå¯ä»¥å‡†ç¡®éªŒè¯å’Œå¥–åŠ±çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è§£å†³æ•°å­¦é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†å¥–åŠ±å™ªå£°çš„å½±å“ï¼Œè¿™æ˜¯ä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹LLMè¿›è¡Œåè®­ç»ƒæ‰€æ¶‰åŠçš„æ›´å®é™…çš„è€ƒè™‘å› ç´ ï¼Œè¿™äº›è€ƒè™‘å› ç´ é€‚ç”¨äºçœŸå®ä¸–ç•Œåœºæ™¯ã€‚æˆ‘ä»¬å‘ç°LLMå¯¹å¤§é‡çš„å¥–åŠ±å™ªå£°è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸­ï¼Œæ‰‹åŠ¨ç¿»è½¬å¥–åŠ±åŠŸèƒ½è¾“å‡ºçš„40%ï¼Œä»ç„¶å…è®¸Qwen-2.5-7Bæ¨¡å‹å®ç°å¿«é€Ÿæ”¶æ•›ï¼Œå…¶åœ¨æ•°å­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä»5%æé«˜åˆ°72%ï¼Œè€Œä½¿ç”¨æ— å™ªå£°å¥–åŠ±è®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡ä¸º75%ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»…é€šè¿‡å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­çš„å‡ºç°ï¼ˆå³æ¨ç†æ¨¡å¼å¥–åŠ±ï¼ŒRPRï¼‰ï¼Œå¦‚â€œé¦–å…ˆï¼Œæˆ‘éœ€è¦â€ï¼Œè€Œä¸éªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¯¥æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½è¾¾åˆ°å³°å€¼ï¼ˆQwen-2.5-7Bçš„å‡†ç¡®ç‡è¶…è¿‡70%ï¼‰ï¼Œå¯ä¸ç»è¿‡ä¸¥æ ¼æ­£ç¡®æ€§éªŒè¯å’Œå‡†ç¡®å¥–åŠ±è®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾ã€‚æˆ‘ä»¬è®¤è¯†åˆ°æ¨ç†è¿‡ç¨‹æ¯”æœ€ç»ˆç»“æœæ›´é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬å°†RPRä¸å˜ˆæ‚çš„å¥–åŠ±æ¨¡å‹ç›¸ç»“åˆã€‚RPRæœ‰åŠ©äºæ ¡å‡†å˜ˆæ‚çš„å¥–åŠ±æ¨¡å‹ï¼Œå‡å°‘æ½œåœ¨çš„å‡é˜´æ€§ï¼Œå¹¶å¢å¼ºLLMåœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜åœ¨é¢„è®­ç»ƒé˜¶æ®µæé«˜æ¨¡å‹çš„åŸºæœ¬èƒ½åŠ›çš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¸ºæ”¹è¿›åè®­ç»ƒæŠ€æœ¯æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å’Œè„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason">https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22653v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè®­ç»ƒåçš„å¥–åŠ±å™ªå£°å½±å“ã€‚ç ”ç©¶å‘ç°åœ¨æ•°å­¦ä»»åŠ¡ä¸­ï¼Œæ‰‹åŠ¨æ”¹å˜å¥–åŠ±å‡½æ•°çš„è¾“å‡ºä¸ä¼šå½±å“LLMæ¨¡å‹çš„å¿«é€Ÿæ”¶æ•›å’Œå…¶æ€§èƒ½çš„å¤§å¹…æå‡ã€‚åŒæ—¶ï¼Œé€šè¿‡ä»…å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­ï¼ˆå¦‚â€œé¦–å…ˆï¼Œæˆ‘éœ€è¦â€ï¼‰è€Œä¸éªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œæ¨¡å‹å–å¾—äº†ä¸ä¸¥æ ¼éªŒè¯å’Œå‡†ç¡®å¥–åŠ±è®­ç»ƒç›¸å½“çš„ä¸‹æ¸¸æ€§èƒ½ã€‚ç»“åˆå™ªå£°å¥–åŠ±æ¨¡å‹ä½¿ç”¨æ¨ç†æ¨¡å¼å¥–åŠ±ï¼ˆRPRï¼‰æœ‰åŠ©äºæ ¡å‡†å™ªå£°å¥–åŠ±æ¨¡å‹ï¼Œæé«˜LLMåœ¨å¼€æ”¾ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤ç ”ç©¶å¯¹äºæ”¹å–„æ¨¡å‹çš„é¢„è®­ç»ƒèƒ½åŠ›ï¼Œå¹¶æ¨è¿›åè®­ç»ƒæŠ€æœ¯æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒåçš„å¥–åŠ±å™ªå£°å½±å“ã€‚</li>
<li>LLMå¯¹å¥–åŠ±å™ªå£°å±•ç°å‡ºå¼ºé²æ£’æ€§ï¼Œå¦‚æ‰‹åŠ¨æ”¹å˜æ•°å­¦ä»»åŠ¡çš„å¥–åŠ±å‡½æ•°è¾“å‡ºä»å¯å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>ä»…é€šè¿‡å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­ï¼ˆå¦‚â€œé¦–å…ˆï¼Œæˆ‘éœ€è¦â€ï¼‰ï¼Œå³ä½¿ä¸éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§ï¼Œä¹Ÿèƒ½å–å¾—è‰¯å¥½ä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>æ¨ç†æ¨¡å¼å¥–åŠ±ï¼ˆRPRï¼‰ä¸å™ªå£°å¥–åŠ±æ¨¡å‹ç»“åˆä½¿ç”¨æœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å¼€æ”¾ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>RPRæœ‰åŠ©äºæ ¡å‡†å™ªå£°å¥–åŠ±æ¨¡å‹ï¼Œå‡å°‘æ½œåœ¨è¯¯åˆ¤å¹¶æé«˜LLMæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†é¢„è®­ç»ƒé˜¶æ®µå¯¹æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16bcf8b564b64385335468dfa1d50af5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c46b0d65b4e54c65bc92cb4df2af7af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0238020e3779feba698a7924322fc023.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa53a4a3018cc97c89f6cf6c99ed7c37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a69874cbf294342a5eb20545791ead6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Sherlock-Self-Correcting-Reasoning-in-Vision-Language-Models"><a href="#Sherlock-Self-Correcting-Reasoning-in-Vision-Language-Models" class="headerlink" title="Sherlock: Self-Correcting Reasoning in Vision-Language Models"></a>Sherlock: Self-Correcting Reasoning in Vision-Language Models</h2><p><strong>Authors:Yi Ding, Ruqi Zhang</strong></p>
<p>Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMsâ€™ self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic $\beta$ for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚çš„å¤šåª’ä½“ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šé«˜åº¦æ•æ„Ÿäºæ¨ç†é”™è¯¯ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®æˆ–ç²¾ç¡®éªŒè¯å™¨ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šé¢†åŸŸä¹‹å¤–éš¾ä»¥æ¨å¹¿ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è‡ªæˆ‘æ ¡æ­£ä½œä¸ºä¸€ç§å¢å¼ºæ¨ç†VLMsçš„ç­–ç•¥ã€‚æˆ‘ä»¬é¦–å…ˆæ·±å…¥åˆ†æäº†æ¨ç†VLMsçš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ï¼Œå¹¶è¯†åˆ«äº†å…³é”®å·®è·ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†Sherlockï¼Œä¸€ä¸ªè‡ªæˆ‘æ ¡æ­£å’Œè‡ªæˆ‘æå‡çš„è®­ç»ƒæ¡†æ¶ã€‚Sherlockå¼•å…¥äº†ä¸€ä¸ªè½¨è¿¹çº§çš„è‡ªæˆ‘æ ¡æ­£ç›®æ ‡ã€ä¸€ä¸ªåŸºäºè§†è§‰æ‰°åŠ¨çš„åå¥½æ•°æ®æ„å»ºæ–¹æ³•ä»¥åŠä¸€ä¸ªç”¨äºåå¥½è°ƒæ•´çš„åŠ¨æ€Î²å€¼ã€‚æ¨¡å‹ä»…ä½¿ç”¨2ä¸‡æ¡éšæœºé‡‡æ ·æ ‡æ³¨æ•°æ®è·å¾—è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›åï¼Œå³å¯ç»§ç»­è¿›è¡Œè‡ªæˆ‘æå‡è€Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚åŸºäºLlama3.2-Vision-11Bæ¨¡å‹ï¼ŒSherlockåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œç›´æ¥ç”Ÿæˆæ—¶çš„å¹³å‡å‡†ç¡®ç‡ä¸º64.1ï¼Œè‡ªæˆ‘æ ¡æ­£åæé«˜åˆ°65.4ã€‚å®ƒçš„æ€§èƒ½ä¼˜äºLLaVA-CoTï¼ˆ63.2ï¼‰ã€Mulberryï¼ˆ63.9ï¼‰å’ŒLlamaV-o1ï¼ˆ63.4ï¼‰ï¼Œè€Œä¸”ä½¿ç”¨çš„æ ‡æ³¨æ•°æ®ä¸åˆ°20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22651v1">PDF</a> 27 pages</p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­ï¼Œä¸»è¦è®¨è®ºäº†å…³äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢ä¸´çš„æŒ‘æˆ˜åŠå…¶æ”¹è¿›æ–¹æ³•ã€‚ä¸ºæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶è€…æ¢ç´¢äº†è‡ªæˆ‘æ ¡æ­£ç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†åä¸ºSherlockçš„è‡ªæˆ‘æ ¡æ­£å’Œè‡ªæˆ‘æå‡è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è½¨è¿¹çº§åˆ«çš„è‡ªæˆ‘æ ¡æ­£ç›®æ ‡ã€åŸºäºè§†è§‰æ‰°åŠ¨çš„åå¥½æ•°æ®æ„å»ºæ–¹æ³•å’ŒåŠ¨æ€Î²åå¥½è°ƒæ•´ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä»…æœ‰2ä¸‡éšæœºé‡‡æ ·æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è·å¾—è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ï¼Œå¹¶èƒ½åœ¨æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹æŒç»­è‡ªæˆ‘æå‡ã€‚åŸºäºLlama3.2-Vision-11Bæ¨¡å‹æ„å»ºçš„Sherlockåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½¿ç”¨è‡ªæˆ‘æ ¡æ­£åå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†65.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ä»é¢ä¸´æ¨ç†è¯¯å·®æ•æ„Ÿã€éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®å’Œå‡†ç¡®éªŒè¯å™¨ä»¥åŠéš¾ä»¥è·¨åŸŸæ³›åŒ–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è€…æ¢ç´¢äº†è‡ªæˆ‘æ ¡æ­£ç­–ç•¥ï¼Œä»¥æé«˜VLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†åä¸ºSherlockçš„è‡ªæˆ‘æ ¡æ­£å’Œè‡ªæˆ‘æå‡è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬è½¨è¿¹çº§åˆ«çš„è‡ªæˆ‘æ ¡æ­£ç›®æ ‡ã€åŸºäºè§†è§‰æ‰°åŠ¨çš„åå¥½æ•°æ®æ„å»ºæ–¹æ³•å’ŒåŠ¨æ€Î²åå¥½è°ƒæ•´ã€‚</li>
<li>Sherlockä»…éœ€2ä¸‡éšæœºé‡‡æ ·æ ‡æ³¨æ•°æ®å°±èƒ½è·å¾—è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ï¼Œå¹¶èƒ½åœ¨æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹æŒç»­è‡ªæˆ‘æå‡ã€‚</li>
<li>åŸºäºLlama3.2-Vision-11Bæ¨¡å‹æ„å»ºçš„Sherlockåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†65.4%ã€‚</li>
<li>Sherlockç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ˆLLaVA-CoTã€Mulberryã€LlamaV-o1ï¼‰ä½¿ç”¨äº†æ›´å°‘æ ‡æ³¨æ•°æ®ä¾¿å–å¾—äº†æ›´é«˜å‡†ç¡®ç‡ã€‚</li>
<li>Sherlockçš„è‡ªæˆ‘æ ¡æ­£å’Œè‡ªæˆ‘æå‡ç­–ç•¥å¯¹äºè§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d67654fa4ac1f0e5d5bb08c1ce2f067.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8857c0044ab428c9cdd00a9d274a13b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-266dc3707a1118cbdabfc15a716aca4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27a4f4f8d80cc7d8dc049c7ee052a6d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c0a112630e2fd8212fbdeb42b34aa24.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="WebDancer-Towards-Autonomous-Information-Seeking-Agency"><a href="#WebDancer-Towards-Autonomous-Information-Seeking-Agency" class="headerlink" title="WebDancer: Towards Autonomous Information Seeking Agency"></a>WebDancer: Towards Autonomous Information Seeking Agency</h2><p><strong>Authors:Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou</strong></p>
<p>Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in <a target="_blank" rel="noopener" href="https://github.com/Alibaba-NLP/WebAgent">https://github.com/Alibaba-NLP/WebAgent</a>. </p>
<blockquote>
<p>è§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œé—®é¢˜éœ€è¦è¿›è¡Œæ·±å…¥çš„ä¿¡æ¯æœç´¢å’Œå¤šæ­¥éª¤æ¨ç†ã€‚ä»¥æ·±åº¦ç ”ç©¶ä¸ºä¾‹çš„ä»£ç†ç³»ç»Ÿï¼ˆagentic systemsï¼‰çš„æœ€æ–°è¿›å±•çªæ˜¾äº†è‡ªä¸»å¤šæ­¥éª¤ç ”ç©¶çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æ•°æ®ä¸ºä¸­å¿ƒå’Œè®­ç»ƒé˜¶æ®µçš„è§’åº¦ï¼Œæå‡ºäº†æ„å»ºç«¯åˆ°ç«¯ä»£ç†ä¿¡æ¯æœç´¢ä»£ç†çš„è¿è´¯èŒƒå¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å››ä¸ªå…³é”®é˜¶æ®µï¼šï¼ˆ1ï¼‰æµè§ˆæ•°æ®æ„å»ºï¼Œï¼ˆ2ï¼‰è½¨è¿¹é‡‡æ ·ï¼Œï¼ˆ3ï¼‰æœ‰æ•ˆå†·å¯åŠ¨çš„ç›‘ç£å¾®è°ƒï¼Œä»¥åŠï¼ˆ4ï¼‰å¢å¼ºé€šç”¨çš„å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬ä»¥åŸºäºReActçš„Webä»£ç†WebDancerå®ä¾‹åŒ–æ­¤æ¡†æ¶ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•GAIAå’ŒWebWalkerQAä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒWebDancerè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼Œå¹¶çªå‡ºäº†æˆ‘ä»¬è®­ç»ƒèŒƒå¼çš„æœ‰æ•ˆæ€§ã€‚å¯¹ä»£ç†è®­ç»ƒçš„è¿›ä¸€æ­¥åˆ†ææä¾›äº†å®è´µçš„è§è§£å’Œå¯è¡Œçš„ç³»ç»Ÿæ€§é€”å¾„ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å…·èƒ½åŠ›çš„ä»£ç†æ¨¡å‹ã€‚ä»£ç å’Œæ¼”ç¤ºå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Alibaba-NLP/WebAgent%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Alibaba-NLP/WebAgentå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†ä¸ºè§£å†³ç°å®ä¸–ç•Œä¸­çš„å¤æ‚é—®é¢˜ï¼Œéœ€è¦æ·±å…¥çš„ä¿¡æ¯æœç´¢å’Œå¤šæ­¥éª¤æ¨ç†ã€‚é€šè¿‡ä»‹ç»ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯æ™ºèƒ½ä¿¡æ¯æœç´¢ä»£ç†çš„æ„å»ºæ–¹æ³•ï¼Œå±•ç¤ºäº†è‡ªä¸»å¤šæ­¥éª¤ç ”ç©¶çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å››ä¸ªé˜¶æ®µï¼šæµè§ˆæ•°æ®æ„å»ºã€è½¨è¿¹é‡‡æ ·ã€ç›‘ç£å¾®è°ƒå®ç°å†·å¯åŠ¨ä»¥åŠå¼ºåŒ–å­¦ä¹ æå‡æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³ç°å®ä¸–ç•Œå¤æ‚é—®é¢˜éœ€è¦æ·±å…¥ä¿¡æ¯æœç´¢å’Œå¤šæ­¥éª¤æ¨ç†ã€‚</li>
<li>æ–°å‹ç«¯åˆ°ç«¯æ™ºèƒ½ä¿¡æ¯æœç´¢ä»£ç†çš„æ„å»ºæ–¹æ³•åŒ…æ‹¬æµè§ˆæ•°æ®æ„å»ºã€è½¨è¿¹é‡‡æ ·ç­‰å››ä¸ªé˜¶æ®µã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç›‘ç£å¾®è°ƒå®ç°å†·å¯åŠ¨ï¼Œå¼ºåŒ–å­¦ä¹ æå‡ä»£ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¿¡æ¯æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°è¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥ä»£ç†æ¡†æ¶åœ¨WebAgentä¸Šçš„å®ä¾‹åº”ç”¨å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>é€šè¿‡å¯¹ä»£ç†è®­ç»ƒçš„åˆ†æï¼Œæä¾›äº†å¼€å‘æ›´å¼ºå¤§ä»£ç†æ¨¡å‹çš„å¯è¡Œè·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb74276aacfc589873c3883bbf65b88d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f50b2b3a4f8b0c26e699c47a6dd3595.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a54c78477eaa8b2c976237a50c67f199.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ClaimPKG-Enhancing-Claim-Verification-via-Pseudo-Subgraph-Generation-with-Lightweight-Specialized-LLM"><a href="#ClaimPKG-Enhancing-Claim-Verification-via-Pseudo-Subgraph-Generation-with-Lightweight-Specialized-LLM" class="headerlink" title="ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation   with Lightweight Specialized LLM"></a>ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation   with Lightweight Specialized LLM</h2><p><strong>Authors:Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui</strong></p>
<p>Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones. </p>
<blockquote>
<p>å°†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰æ•´åˆåˆ°å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ä¸­ï¼Œæ˜¯å£°æ˜éªŒè¯é¢†åŸŸçš„æ–°å…´ç ”ç©¶æŒ‘æˆ˜ã€‚è™½ç„¶çŸ¥è¯†å›¾è°±æä¾›äº†ç»“æ„åŒ–å’Œè¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼ï¼Œéå¸¸é€‚åˆè¿›è¡Œæ¨ç†ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„éªŒè¯æ–¹æ³•ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå°½ç®¡å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šæ­¥éª¤æ¨¡å—åŒ–ç®¡é“å’ŒåŸºäºçŸ¥è¯†å›¾è°±çš„æ¨ç†æ—¶ä»é¢ä¸´å›°éš¾ï¼Œé™¤éè¿›è¡Œé€‚åº”ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ClaimPKGç«¯åˆ°ç«¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼ŒClaimPKGçš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨ä¸€ä¸ªè½»é‡çº§çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥å°†è¾“å…¥å£°æ˜è¡¨ç¤ºä¸ºä¼ªå­å›¾ï¼Œå¼•å¯¼ä¸“ç”¨çš„å­å›¾æ£€ç´¢æ¨¡å—è¯†åˆ«ç›¸å…³çš„çŸ¥è¯†å›¾è°±å­å›¾ã€‚ç„¶åï¼Œè¿™äº›æ£€ç´¢åˆ°çš„å­å›¾è¢«é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†ï¼Œä»¥å¾—å‡ºæœ€ç»ˆè£å†³å’Œè§£é‡Šã€‚åœ¨FactKGæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒClaimPKGè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨è¯¥ç ”ç©¶é¢†åŸŸçš„å¤šä¸ªç±»åˆ«ä¸­æ¯”å¼ºå¤§çš„åŸºçº¿é«˜å‡º9%-12%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒClaimPKGåœ¨æ— ç»“æ„æ•°æ®é›†ï¼ˆå¦‚HoVerå’ŒFEVEROUSï¼‰ä¸Šè¡¨ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22552v1">PDF</a> Accepted by ACL 2025 findings</p>
<p><strong>Summary</strong>ï¼šæ•´åˆçŸ¥è¯†å›¾è°±ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¯å£°æ˜éªŒè¯é¢†åŸŸçš„æ–°å…´ç ”ç©¶æŒ‘æˆ˜ã€‚çŸ¥è¯†å›¾è°±æä¾›ç»“æ„åŒ–å’Œè¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼ï¼Œé€‚åˆè¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„éªŒè¯æ–¹æ³•ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºClaimPKGæ¡†æ¶ï¼Œå°†LLMæ¨ç†ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†æ— ç¼é›†æˆã€‚å®ƒé€šè¿‡è½»é‡çº§LLMè¡¨ç¤ºè¾“å…¥å£°æ˜ä¸ºä¼ªå­å›¾ï¼ŒæŒ‡å¯¼ç‰¹å®šçš„å­å›¾æ£€ç´¢æ¨¡å—è¯†åˆ«ç›¸å…³çš„KGå­å›¾ã€‚è¿™äº›æ£€ç´¢åˆ°çš„å­å›¾å†è¢«é€šç”¨LLMå¤„ç†ï¼Œäº§ç”Ÿæœ€ç»ˆè£å†³å’Œä¾æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒClaimPKGåœ¨FactKGæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¯”è¯¥é¢†åŸŸå¼ºå¤§çš„åŸºçº¿é«˜å‡º9%-12%çš„å‡†ç¡®ç‡ç‚¹ã€‚æ­¤å¤–ï¼ŒClaimPKGåœ¨HoVerå’ŒFEVEROUSç­‰éç»“æ„åŒ–æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å…·æœ‰æ½œåŠ›ã€‚</li>
<li>ç°æœ‰éªŒè¯æ–¹æ³•ä¸»è¦ä¾èµ–éç»“æ„åŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œé™åˆ¶äº†çŸ¥è¯†å›¾è°±çš„æœ‰æ•ˆåˆ©ç”¨ã€‚</li>
<li>ClaimPKGæ¡†æ¶é€šè¿‡é›†æˆLLMæ¨ç†å’ŒçŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ClaimPKGä½¿ç”¨è½»é‡çº§LLMå°†è¾“å…¥å£°æ˜è¡¨ç¤ºä¸ºä¼ªå­å›¾ï¼Œå¹¶å¼•å¯¼å­å›¾æ£€ç´¢ã€‚</li>
<li>æ£€ç´¢åˆ°çš„å­å›¾ç”±é€šç”¨LLMå¤„ç†ï¼Œä»¥äº§ç”Ÿæœ€ç»ˆè£å†³å’Œä¾æ®ã€‚</li>
<li>åœ¨FactKGæ•°æ®é›†ä¸Šï¼ŒClaimPKGè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb75a6d33f124191a082d239ceb1c258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c859465138917efe4971dab2fa0c923f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions"><a href="#ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions" class="headerlink" title="ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions"></a>ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions</h2><p><strong>Authors:Maxence Wynen, Pedro M. Gordaliza, Maxime Istasse, Anna StÃ¶lting, Pietro Maggi, BenoÃ®t Macq, Meritxell Bach Cuadra</strong></p>
<p>Accurate lesion-level segmentation on MRI is critical for multiple sclerosis (MS) diagnosis, prognosis, and disease monitoring. However, current evaluation practices largely rely on semantic segmentation post-processed with connected components (CC), which cannot separate confluent lesions (aggregates of confluent lesion units, CLUs) due to reliance on spatial connectivity. To address this misalignment with clinical needs, we introduce formal definitions of CLUs and associated CLU-aware detection metrics, and include them in an exhaustive instance segmentation evaluation framework. Within this framework, we systematically evaluate CC and post-processing-based Automated Confluent Splitting (ACLS), the only existing methods for lesion instance segmentation in MS. Our analysis reveals that CC consistently underestimates CLU counts, while ACLS tends to oversplit lesions, leading to overestimated lesion counts and reduced precision. To overcome these limitations, we propose ConfLUNet, the first end-to-end instance segmentation framework for MS lesions. ConfLUNet jointly optimizes lesion detection and delineation from a single FLAIR image. Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the held-out test set (n&#x3D;13) in instance segmentation (Panoptic Quality: 42.0% vs. 37.5%&#x2F;36.8%; p &#x3D; 0.017&#x2F;0.005) and lesion detection (F1: 67.3% vs. 61.6%&#x2F;59.9%; p &#x3D; 0.028&#x2F;0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU] (81.5%), improving recall over CC (+12.5%, p &#x3D; 0.015) and precision over ACLS (+31.2%, p &#x3D; 0.003). By combining rigorous definitions, new CLU-aware metrics, a reproducible evaluation framework, and the first dedicated end-to-end model, this work lays the foundation for lesion instance segmentation in MS. </p>
<blockquote>
<p>åœ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸Šè¿›è¡Œç²¾ç¡®çš„ç—…å˜çº§åˆ«åˆ†å‰²å¯¹äºå¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„è¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°å®è·µä¸»è¦ä¾èµ–äºé€šè¿‡è¿é€šç»„ä»¶ï¼ˆCCï¼‰è¿›è¡Œåå¤„ç†çš„è¯­ä¹‰åˆ†å‰²ï¼Œè¿™æ— æ³•åˆ†ç¦»èåˆæ€§ç—…å˜ï¼ˆèåˆæ€§ç—…å˜å•ä½èšé›†çš„CLUsï¼‰ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºç©ºé—´è¿é€šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä¸´åºŠéœ€æ±‚çš„é”™é…é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CLUsçš„æ­£å¼å®šä¹‰å’Œç›¸å…³çš„CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡ï¼Œå¹¶å°†å…¶çº³å…¥è¯¦å°½çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ä¸­ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†åŸºäºè¿é€šç»„ä»¶ï¼ˆCCï¼‰å’Œåå¤„ç†çš„è‡ªåŠ¨èåˆåˆ†å‰²ï¼ˆACLSï¼‰ï¼Œè¿™æ˜¯å¤šå‘æ€§ç¡¬åŒ–ç—‡ç—…å˜å®ä¾‹åˆ†å‰²ä¸­å”¯ä¸€å­˜åœ¨çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒCCå§‹ç»ˆä½ä¼°äº†CLUè®¡æ•°ï¼Œè€ŒACLSå€¾å‘äºè¿‡åº¦åˆ†å‰²ç—…å˜ï¼Œå¯¼è‡´ç—…å˜è®¡æ•°è¿‡é«˜å’Œç²¾åº¦é™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ConfLUNetï¼Œè¿™æ˜¯å¤šå‘æ€§ç¡¬åŒ–ç—‡ç—…å˜å®ä¾‹åˆ†å‰²çš„ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ã€‚ConfLUNetè”åˆä¼˜åŒ–ä»å•ä¸ªFLAIRå›¾åƒä¸­æ£€æµ‹ç—…å˜å’Œè½®å»“æç»˜ã€‚åœ¨50åæ‚£è€…ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒConfLUNetåœ¨ä¿ç•™çš„æµ‹è¯•é›†ï¼ˆn&#x3D;13ï¼‰ä¸Šçš„å®ä¾‹åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºCCå’ŒACLSï¼ˆå…¨æ™¯è´¨é‡ï¼š42.0% vs. 37.5%&#x2F;36.8%ï¼›p &#x3D; 0.017&#x2F;0.005ï¼‰ï¼Œåœ¨ç—…å˜æ£€æµ‹æ–¹é¢ä¹Ÿæ˜¯å¦‚æ­¤ï¼ˆF1ï¼š67.3% vs. 61.6%&#x2F;59.9%ï¼›p &#x3D; 0.028&#x2F;0.013ï¼‰ã€‚å¯¹äºCLUæ£€æµ‹ï¼ŒConfLUNetå®ç°äº†æœ€é«˜çš„F1 [CLU]ï¼ˆ81.5%ï¼‰ï¼Œåœ¨å¬å›ç‡ä¸Šè¾ƒCCæé«˜äº†ï¼ˆ+12.5%ï¼Œp &#x3D; 0.015ï¼‰ï¼Œåœ¨ç²¾åº¦ä¸Šè¾ƒACLSæé«˜äº†ï¼ˆ+31.2%ï¼Œp &#x3D; 0.003ï¼‰ã€‚é€šè¿‡ç»“åˆä¸¥æ ¼çš„å®šä¹‰ã€æ–°çš„CLUæ„ŸçŸ¥æŒ‡æ ‡ã€å¯å¤åˆ¶çš„è¯„ä¼°æ¡†æ¶å’Œç¬¬ä¸€ä¸ªä¸“ç”¨çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºå¤šå‘æ€§ç¡¬åŒ–ç—‡ä¸­çš„ç—…å˜å®ä¾‹åˆ†å‰²å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22537v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„MRIå›¾åƒè¿›è¡Œç²¾ç¡®çš„ç—…ç¶çº§åˆ«åˆ†å‰²å¯¹äºè¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä»·å®è·µä¸»è¦ä¾èµ–äºåŸºäºè¿é€šç»„ä»¶ï¼ˆCCï¼‰çš„è¯­ä¹‰åˆ†å‰²ï¼Œæ— æ³•åˆ†ç¦»èåˆç—…ç¶ï¼ˆCLUsï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥CLUsçš„æ­£å¼å®šä¹‰å’Œç›¸å…³CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡ï¼Œå¹¶å°†å…¶çº³å…¥è¯¦å°½çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ä¸­ã€‚åˆ†æè¡¨æ˜ï¼ŒCCä½ä¼°äº†CLUè®¡æ•°ï¼Œè€Œç°æœ‰çš„ACLSæ–¹æ³•å€¾å‘äºè¿‡åº¦åˆ†å‰²ç—…ç¶ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºConfLUNetï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹MSç—…ç¶çš„ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²æ¡†æ¶ã€‚ConfLUNetä»å•ä¸ªFLAIRå›¾åƒä¸­è”åˆä¼˜åŒ–ç—…ç¶æ£€æµ‹å’Œè½®å»“æç»˜ã€‚åœ¨50åæ‚£è€…ä¸Šçš„è®­ç»ƒç»“æœæ˜¾ç¤ºï¼ŒConfLUNetåœ¨å®ä¾‹åˆ†å‰²å’Œç—…ç¶æ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºCCå’ŒACLSã€‚æ­¤å¤–ï¼ŒConfLUNetå®ç°äº†æœ€é«˜çš„CLUæ£€æµ‹F1åˆ†æ•°ï¼Œåœ¨å¬å›ç‡ä¸Šè¶…è¿‡äº†CCï¼Œå¹¶åœ¨ç²¾ç¡®åº¦ä¸Šè¶…è¶Šäº†ACLSã€‚é€šè¿‡ä¸¥æ ¼çš„å®šä¹‰ã€æ–°çš„CLUæ„ŸçŸ¥æŒ‡æ ‡ã€å¯å¤åˆ¶çš„è¯„ä»·æ¡†æ¶å’Œé¦–ä¸ªç«¯åˆ°ç«¯æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºMSçš„ç—…ç¶å®ä¾‹åˆ†å‰²å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®çš„å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰MRIç—…ç¶çº§åˆ«åˆ†å‰²å¯¹è¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ä½¿ç”¨çš„åŸºäºè¿é€šç»„ä»¶ï¼ˆCCï¼‰çš„è¯­ä¹‰åˆ†å‰²æ— æ³•æœ‰æ•ˆåˆ†ç¦»èåˆç—…ç¶ï¼ˆCLUsï¼‰ã€‚</li>
<li>å¼•å…¥CLUsçš„æ­£å¼å®šä¹‰å’Œç›¸å…³CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡ï¼Œå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºï¼ŒCCä½ä¼°äº†CLUè®¡æ•°ï¼Œè€Œç°æœ‰çš„ACLSæ–¹æ³•å­˜åœ¨è¿‡åº¦åˆ†å‰²çš„é—®é¢˜ã€‚</li>
<li>æå‡ºConfLUNetï¼Œä¸€ä¸ªé’ˆå¯¹MSçš„ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œèƒ½è”åˆä¼˜åŒ–ç—…ç¶æ£€æµ‹å’Œè½®å»“æç»˜ã€‚</li>
<li>ConfLUNetåœ¨å®ä¾‹åˆ†å‰²å’Œç—…ç¶æ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºCCå’ŒACLSã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea288273904119d417d16aaca76f63eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf79eb7b33b69d1b411bcf0f30d2cd5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf0b319fb695934d809adb4bdef3c31b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EvolveSearch-An-Iterative-Self-Evolving-Search-Agent"><a href="#EvolveSearch-An-Iterative-Self-Evolving-Search-Agent" class="headerlink" title="EvolveSearch: An Iterative Self-Evolving Search Agent"></a>EvolveSearch: An Iterative Self-Evolving Search Agent</h2><p><strong>Authors:Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, Fei Huang</strong></p>
<p>The rapid advancement of large language models (LLMs) has transformed the landscape of agentic information seeking capabilities through the integration of tools such as search engines and web browsers. However, current mainstream approaches for enabling LLM web search proficiency face significant challenges: supervised fine-tuning struggles with data production in open-search domains, while RL converges quickly, limiting their data utilization efficiency. To address these issues, we propose EvolveSearch, a novel iterative self-evolution framework that combines SFT and RL to enhance agentic web search capabilities without any external human-annotated reasoning data. Extensive experiments on seven multi-hop question-answering (MHQA) benchmarks demonstrate that EvolveSearch consistently improves performance across iterations, ultimately achieving an average improvement of 4.7% over the current state-of-the-art across seven benchmarks, opening the door to self-evolution agentic capabilities in open web search domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•é€šè¿‡æœç´¢å¼•æ“å’Œæµè§ˆå™¨ç­‰å·¥å…·çš„é›†æˆï¼Œæ”¹å˜äº†æ™ºèƒ½ä¿¡æ¯æ£€ç´¢èƒ½åŠ›çš„ç ”ç©¶æ ¼å±€ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµæ–¹æ³•åœ¨å®ç°LLMç½‘ç»œæœç´¢èƒ½åŠ›æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼šç›‘ç£å¾®è°ƒåœ¨å¼€æ”¾æœç´¢é¢†åŸŸçš„æ•°æ®ç”Ÿäº§æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè€Œå¼ºåŒ–å­¦ä¹ è™½ç„¶æ”¶æ•›è¿…é€Ÿï¼Œä½†æ•°æ®åˆ©ç”¨æ•ˆç‡æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EvolveSearchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹è¿­ä»£è‡ªè¿›åŒ–æ¡†æ¶ï¼Œç»“åˆäº†SFTå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€ä»»ä½•å¤–éƒ¨äººå·¥æ³¨é‡Šæ¨ç†æ•°æ®å³å¯å¢å¼ºæ™ºèƒ½ç½‘ç»œæœç´¢èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEvolveSearchåœ¨è¿­ä»£è¿‡ç¨‹ä¸­è¡¨ç°ä¸€è‡´åœ°æ”¹è¿›æ€§èƒ½ï¼Œæœ€ç»ˆåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æ¯”å½“å‰æœ€ä½³æ°´å¹³æé«˜äº†4.7%ï¼Œä¸ºå¼€æ”¾ç½‘ç»œæœç´¢é¢†åŸŸçš„æ™ºèƒ½è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›çš„å‘å±•æ‰“å¼€äº†å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22501v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•é€šè¿‡é›†æˆæœç´¢å¼•æ“å’Œæµè§ˆå™¨ç­‰æŠ€æœ¯ï¼Œæ”¹å˜äº†ä¿¡æ¯æ£€ç´¢èƒ½åŠ›çš„æ ¼å±€ã€‚ç„¶è€Œï¼Œä¸»æµæ–¹æ³•åœ¨å®ç°LLMç½‘ç»œæœç´¢èƒ½åŠ›æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼šç›‘ç£å¾®è°ƒé¢ä¸´å¼€æ”¾æœç´¢é¢†åŸŸæ•°æ®ç”Ÿæˆçš„é—®é¢˜ï¼Œè€Œå¼ºåŒ–å­¦ä¹ æ”¶æ•›è¿…é€Ÿï¼Œé™åˆ¶äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºEvolveSearchæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜ç½‘ç»œæœç´¢èƒ½åŠ›ï¼Œæ— éœ€å¤–éƒ¨äººç±»æ³¨é‡Šçš„æ¨ç†æ•°æ®ã€‚åœ¨ä¸ƒä¸ªå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEvolveSearchåœ¨è¿­ä»£ä¸­æ€§èƒ½æŒç»­æé«˜ï¼Œæœ€ç»ˆè¾ƒç°æœ‰æŠ€æœ¯å¹³å‡æé«˜äº†4.7%ï¼Œä¸ºå¼€æ”¾ç½‘ç»œæœç´¢é¢†åŸŸçš„è‡ªä¸»è¿›åŒ–èƒ½åŠ›æ‰“å¼€äº†å¤§é—¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸæœ‰é‡è¦åº”ç”¨ï¼Œä½†é¢ä¸´ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„æŒ‘æˆ˜ã€‚</li>
<li>EvolveSearchæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ—¨åœ¨æé«˜LLMçš„ç½‘ç»œæœç´¢èƒ½åŠ›ã€‚</li>
<li>EvolveSearchæ— éœ€å¤–éƒ¨äººç±»æ³¨é‡Šçš„æ¨ç†æ•°æ®ï¼Œæé«˜äº†æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚</li>
<li>åœ¨ä¸ƒä¸ªå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šï¼ŒEvolveSearchè¾ƒç°æœ‰æŠ€æœ¯å¹³å‡æé«˜äº†4.7%çš„æ€§èƒ½ã€‚</li>
<li>EvolveSearchçš„è¿­ä»£æ€§è´¨ä½¿å…¶èƒ½å¤Ÿåœ¨ä¸æ–­è¿›åŒ–ä¸­ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æå‡ºä¸ºå¼€æ”¾ç½‘ç»œæœç´¢é¢†åŸŸçš„è‡ªä¸»è¿›åŒ–èƒ½åŠ›ç ”ç©¶å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfd9154c86cac4e5e5cfc7e08d4e607e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557d0c5829cdfab924638210d82ff347.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88614c79c35ddbfdb75eb084c3839332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87deee97464e91d6fcb3200ba6e65738.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb307dc4e1461f88213a6d3aa7fc133c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Post-Training-for-Multi-Modal-LLM-Reasoning-via-GRPO"><a href="#Unsupervised-Post-Training-for-Multi-Modal-LLM-Reasoning-via-GRPO" class="headerlink" title="Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO"></a>Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO</h2><p><strong>Authors:Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun</strong></p>
<p>Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal dataâ€“an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9 %$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/waltonfuture/MM-UPT">https://github.com/waltonfuture/MM-UPT</a>. </p>
<blockquote>
<p>åœ¨è®­ç»ƒåé˜¶æ®µæ”¹è¿›å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šå¸¸ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›ç›‘ç£æ–¹æ³•éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€ç»ˆæ— æ³•æŒç»­çš„èµ„æºã€‚è™½ç„¶æœ€è¿‘æœ‰å°è¯•æ¢ç´¢æ— ç›‘ç£çš„åè®­ç»ƒï¼Œä½†ä»–ä»¬çš„æ–¹æ³•å¤æ‚ä¸”éš¾ä»¥è¿­ä»£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡è°ƒæŸ¥äº†GRPOè¿™ä¸€ç¨³å®šä¸”å¯æ‰©å±•çš„åœ¨çº¿RLç®—æ³•çš„ä½¿ç”¨æƒ…å†µï¼Œä»¥å®ç°æ— éœ€ä»»ä½•å¤–éƒ¨ç›‘ç£çš„æŒç»­è‡ªæˆ‘æ”¹è¿›ã€‚æˆ‘ä»¬æå‡ºäº†MM-UPTï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ— ç›‘ç£åè®­ç»ƒæ¡†æ¶ã€‚MM-UPTå»ºç«‹åœ¨GRPOçš„åŸºç¡€ä¸Šï¼Œç”¨åŸºäºå¤šæ•°æŠ•ç¥¨çš„å¤šä¸ªé‡‡æ ·å“åº”çš„è‡ªæˆ‘å¥–åŠ±æœºåˆ¶å–ä»£äº†ä¼ ç»Ÿçš„å¥–åŠ±ä¿¡å·ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šï¼Œæ— éœ€çœŸå®æ ‡ç­¾ï¼ŒMM-UPTå°±èƒ½æ˜¾è‘—æé«˜Qwen2.5-VL-7Bçš„æ¨ç†èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼ŒMathVistaä¸Šçš„66.3%æå‡åˆ°72.9%ï¼ŒWe-Mathä¸Šçš„62.9%æå‡åˆ°68.7%ï¼‰ã€‚MM-UPTä¹Ÿä¼˜äºå…ˆå‰çš„æ— ç›‘ç£åŸºå‡†æµ‹è¯•ï¼Œç”šè‡³æ¥è¿‘æœ‰ç›‘ç£GRPOçš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†é€šè¿‡MLLMæœ¬èº«ç”Ÿæˆçš„åˆæˆé—®é¢˜ä¹Ÿå¯ä»¥æé«˜æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†ä¸€ç§æœ‰å‰æ™¯çš„å¯æ‰©å±•è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼ŒMM-UPTä¸ºåœ¨æ— å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹å¯¹MLLMè¿›è¡ŒæŒç»­è‡ªä¸»çš„æ”¹è¿›æä¾›äº†æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/waltonfuture/MM-UPT">https://github.com/waltonfuture/MM-UPT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22453v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>MLLMçš„åè®­ç»ƒé˜¶æ®µæ”¹è¿›é€šå¸¸ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰æˆ–å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›ç›‘ç£æ–¹æ³•éœ€è¦æ˜‚è´µä¸”æ‰‹åŠ¨æ³¨é‡Šçš„å¤šæ¨¡æ€æ•°æ®ï¼Œè¿™æ˜¯ä¸€ç§æœ€ç»ˆä¸å¯æŒç»­çš„èµ„æºã€‚æœ€è¿‘çš„ç ”ç©¶è™½ç„¶å¼€å§‹æ¢ç´¢æ— ç›‘ç£çš„åè®­ç»ƒï¼Œä½†å…¶æ–¹æ³•å¤æ‚ä¸”éš¾ä»¥è¿­ä»£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡è°ƒæŸ¥äº†GRPOè¿™ä¸€ç¨³å®šä¸”å¯æ‰©å±•çš„åœ¨çº¿RLç®—æ³•åœ¨æ— éœ€å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹å®ç°æŒç»­è‡ªæˆ‘æ”¹è¿›çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†MM-UPTï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„MLLMæ— ç›‘ç£åè®­ç»ƒæ¡†æ¶ã€‚MM-UPTåŸºäºGRPOæ„å»ºï¼Œç”¨åŸºäºå¤šæ•°æŠ•ç¥¨çš„è‡ªæˆ‘å¥–åŠ±æœºåˆ¶å–ä»£äº†ä¼ ç»Ÿçš„å¥–åŠ±ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒMM-UPTæ˜¾è‘—æé«˜äº†Qwen2.5-VL-7Bçš„æ¨ç†èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼ŒMathVistaä¸Šçš„66.3%æé«˜åˆ°72.9%ï¼ŒWe-Mathä¸Šçš„62.9%æé«˜åˆ°68.7%ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨äº†æ²¡æœ‰çœŸå®æ ‡ç­¾çš„æ ‡å‡†æ•°æ®é›†ã€‚MM-UPTè¿˜ä¼˜äºå…ˆå‰çš„æ— ç›‘ç£åŸºçº¿ï¼Œç”šè‡³æ¥è¿‘æœ‰ç›‘ç£GRPOçš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†é€šè¿‡MLLMæœ¬èº«ç”Ÿæˆçš„åˆæˆé—®é¢˜ä¹Ÿå¯ä»¥æé«˜æ€§èƒ½ï¼Œè¿™æ˜¾ç¤ºå‡ºäº†ä¸€ç§æœ‰å‰æ™¯çš„å¯æ‰©å±•è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼ŒMM-UPTä¸ºåœ¨æ²¡æœ‰å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹å®ç°MLLMçš„æŒç»­è‡ªä¸»å¢å¼ºæä¾›äº†æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/waltonfuture/MM-UPT%E3%80%82">https://github.com/waltonfuture/MM-UPTã€‚</a><br><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†MM-UPTä½œä¸ºä¸€ç§æ–°å‹çš„æ¡†æ¶åœ¨æ— ç›‘ç£çš„åè®­ç»ƒé˜¶æ®µç”¨äºæ”¹è¿›å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>MM-UPTé¦–æ¬¡æ¢ç´¢ä½¿ç”¨GRPOåœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œæ¨¡å‹è‡ªä¸»æ”¹è¿›çš„å¯èƒ½ã€‚</li>
<li>é€šè¿‡å¤šæ•°æŠ•ç¥¨çš„è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ï¼Œä½¿å¾—æ¨¡å‹èƒ½åœ¨æ— å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹å–„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†MM-UPTåœ¨æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦é¢†åŸŸçš„ä»»åŠ¡ä¸Šã€‚</li>
<li>MM-UPTçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•ï¼Œå¹¶æ¥è¿‘äº†å—ç›‘ç£å­¦ä¹ çš„ç»“æœã€‚</li>
<li>é›†æˆç”±MLLMè‡ªèº«ç”Ÿæˆçš„åˆæˆé—®é¢˜èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7e0e33891ac791d603ffdbc3a73ba25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ff6c193fe4803bd8fdabd9a38c6d4af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-881147f6a6bf300af57c6c18e950a712.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d20f0af021521ff54d5c65da0817623.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Scaling-Reasoning-without-Attention"><a href="#Scaling-Reasoning-without-Attention" class="headerlink" title="Scaling Reasoning without Attention"></a>Scaling Reasoning without Attention</h2><p><strong>Authors:Xueliang Zhao, Wei Wu, Lingpeng Kong</strong></p>
<p>Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6% on AIME 24, 0.6% on AIME 25, and 3.0% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šå› ä¾èµ–Transformerè€Œå¯¼è‡´çš„æ¶æ„æ•ˆç‡ä½ä¸‹ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹é«˜éš¾åº¦é¢†åŸŸçš„ç»“æ„åŒ–å¾®è°ƒã€‚æˆ‘ä»¬æ¨å‡ºäº†æ— æ³¨æ„åŠ›è¯­è¨€æ¨¡å‹â€”â€”ourmodelï¼Œå®ƒé€šè¿‡æ¶æ„å’Œæ•°æ®ä¸­å¿ƒçš„åˆ›æ–°æ¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŸºäºMamba-2çš„çŠ¶æ€ç©ºé—´åŒï¼ˆSSDï¼‰å±‚ï¼Œæ¶ˆé™¤äº†å¯¹è‡ªæ³¨æ„åŠ›å’Œé”®å€¼ç¼“å­˜çš„éœ€æ±‚ï¼Œå®ç°äº†å›ºå®šå†…å­˜ã€æ’å®šæ—¶é—´çš„æ¨ç†ã€‚ä¸ºäº†å¯¹å…¶è¿›è¡Œå¤æ‚æ¨ç†è®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºPromptCoTåˆæˆèŒƒå¼çš„ä¸¤é˜¶æ®µè¯¾ç¨‹å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡æŠ½è±¡æ¦‚å¿µé€‰æ‹©å’Œç†æ€§å¼•å¯¼ç”Ÿæˆï¼Œç”Ÿæˆæ•™è‚²ç»“æ„åŒ–é—®é¢˜ã€‚åœ¨åŸºå‡†è¯„ä¼°ä¸­ï¼Œourmodel-7Bè¡¨ç°ä¼˜äºåŒè§„æ¨¡çš„ä¼˜åŠ¿Transformerå’Œæ··åˆæ¨¡å‹ï¼Œç”šè‡³åœ¨AIME 24ä¸Šè¶…è¶Šäº†æ›´å¤§çš„Gemma3-27B 2.6%ã€AIME 25ä¸Šæé«˜äº†0.6%ï¼Œä»¥åŠåœ¨Livecodebenchä¸Šæé«˜äº†3.0%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºé«˜æ•ˆä¸”å¯æ‰©å±•çš„æ›¿ä»£æ³¨æ„åŠ›åŸºç¡€æ¶æ„è¿›è¡Œé«˜å®¹é‡æ¨ç†çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22425v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¾èµ–Transformerçš„æ¶æ„ä½æ•ˆæ€§å’Œç¼ºä¹é’ˆå¯¹é«˜éš¾åº¦åŸŸçš„ç»“æ„åŒ–å¾®è°ƒã€‚æœ¬æ–‡æå‡ºçš„ourmodelæ˜¯ä¸€ç§è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜çš„æ³¨æ„åŠ›è‡ªç”±çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ¶æ„å’Œæ•°æ®ä¸­å¿ƒåˆ›æ–°å®ç°ã€‚è¯¥æ¨¡å‹å»ºç«‹åœ¨Mamba-2çš„çŠ¶æ€ç©ºé—´åŒé‡ï¼ˆSSDï¼‰å±‚ä¸Šï¼Œæ¶ˆé™¤äº†å¯¹è‡ªæ³¨æ„åŠ›å’Œé”®å€¼ç¼“å­˜çš„éœ€æ±‚ï¼Œå®ç°äº†å›ºå®šå†…å­˜ã€æ’å®šæ—¶é—´çš„æ¨ç†ã€‚ä¸ºäº†å¯¹å…¶è¿›è¡Œå¤æ‚æ¨ç†è®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº\text{PromptCoT}åˆæˆèŒƒå¼çš„ä¸¤é˜¶æ®µè¯¾ç¨‹å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡æŠ½è±¡æ¦‚å¿µé€‰æ‹©å’Œç†æ€§å¼•å¯¼ç”Ÿæˆæ¥ç”Ÿæˆæ•™å­¦ç»“æ„åŒ–é—®é¢˜ã€‚åœ¨åŸºå‡†è¯„ä¼°ä¸­ï¼Œourmodel-7Bè¡¨ç°å‡ºä¼˜äºåŒç±»è§„æ¨¡çš„å¼ºå¤§Transformerå’Œæ··åˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œç”šè‡³åœ¨AIME 24ä¸Šæ¯”æ›´å¤§çš„Gemma3-27Bé«˜å‡º2.6%ï¼Œåœ¨AIME 25ä¸Šé«˜å‡º0.6%ï¼Œåœ¨Livecodebenchä¸Šé«˜å‡º3.0%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„æ›¿ä»£æ³¨æ„åŠ›åŸºç¡€æ¶æ„è¿›è¡Œé«˜å®¹é‡æ¨ç†çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ¶æ„ä½æ•ˆæ€§å’Œç¼ºä¹ç»“æ„åŒ–å¾®è°ƒã€‚</li>
<li>ourmodelæ˜¯ä¸€ç§æ³¨æ„åŠ›è‡ªç”±çš„è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œé€šè¿‡æ¶æ„å’Œæ•°æ®ä¸­å¿ƒåˆ›æ–°å®ç°ã€‚</li>
<li>ourmodelå»ºç«‹åœ¨Mamba-2çš„çŠ¶æ€ç©ºé—´åŒé‡ï¼ˆSSDï¼‰å±‚ä¸Šï¼Œä¸éœ€è¦è‡ªæ³¨æ„åŠ›å’Œé”®å€¼ç¼“å­˜ï¼Œå®ç°å›ºå®šå†…å­˜ã€æ’å®šæ—¶é—´æ¨ç†ã€‚</li>
<li>æå‡ºäº†åŸºäº\text{PromptCoT}åˆæˆèŒƒå¼çš„ä¸¤é˜¶æ®µè¯¾ç¨‹å¾®è°ƒç­–ç•¥ï¼Œç”¨äºå¤æ‚æ¨ç†è®­ç»ƒã€‚</li>
<li>ourmodelé€šè¿‡æŠ½è±¡æ¦‚å¿µé€‰æ‹©å’Œç†æ€§å¼•å¯¼ç”Ÿæˆç”Ÿæˆæ•™å­¦ç»“æ„åŒ–é—®é¢˜ã€‚</li>
<li>ourmodel-7Båœ¨åŸºå‡†è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºåŒç±»è§„æ¨¡çš„æ¨¡å‹ï¼Œç”šè‡³åœ¨æŸäº›è¯„ä¼°ä¸Šè¶…è¿‡äº†æ›´å¤§çš„Gemma3-27Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c75f6e5153fb914d00b2a94141c5d88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d31a15af10bb311d9fd865b9e8177a75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-059e56c57aabf828f26a0c9520096104.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mitigating-Overthinking-in-Large-Reasoning-Models-via-Manifold-Steering"><a href="#Mitigating-Overthinking-in-Large-Reasoning-Models-via-Manifold-Steering" class="headerlink" title="Mitigating Overthinking in Large Reasoning Models via Manifold Steering"></a>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</h2><p><strong>Authors:Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong</strong></p>
<p>Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the modelâ€™s activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Aries-iai/Manifold_Steering">https://github.com/Aries-iai/Manifold_Steering</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è§£å†³æ•°å­¦å’Œç¼–ç¨‹ç­‰å¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç»å¸¸è¡¨ç°å‡ºä¸€ç§è¢«ç§°ä¸ºâ€œè¿‡åº¦æ€è€ƒâ€çš„ç°è±¡ï¼Œå…¶ç‰¹å¾åœ¨äºè¿‡å¤šçš„éªŒè¯å¾ªç¯å’Œå†—ä½™çš„æ€ç´¢ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¤§å¤§å¢åŠ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»æœºæ¢°å¯è§£é‡Šæ€§çš„è§’åº¦æ¢è®¨æ½œåœ¨æœºåˆ¶ï¼Œæ—¨åœ¨å‡è½»è¿‡åº¦æ€è€ƒã€‚æˆ‘ä»¬é¦–å…ˆå±•ç¤ºï¼Œè¿‡åº¦æ€è€ƒçš„å€¾å‘å¯ä»¥é€šè¿‡æ¨¡å‹æ¿€æ´»ç©ºé—´ä¸­çš„ä¸€ä¸ªæ–¹å‘æœ‰æ•ˆåœ°æ•è·ï¼Œé€šè¿‡æ²¿æ­¤æ–¹å‘å¹²é¢„æ¿€æ´»å¯ä»¥ç¼“è§£è¯¥é—®é¢˜ã€‚ç„¶è€Œï¼Œéšç€å¹²é¢„å¼ºåº¦çš„å¢åŠ ï¼Œè¿™ç§æ•ˆæœå¾ˆå¿«è¾¾åˆ°é¥±å’Œï¼Œç”šè‡³æ¶åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†æ¿€æ´»ç©ºé—´ï¼Œå¹¶å‘ç°è¿‡åº¦æ€è€ƒç°è±¡å®é™…ä¸Šä¸ä½ç»´æµå½¢æœ‰å…³ï¼Œè¿™è¡¨æ˜æœ‰é™çš„æ•ˆæœæºäºé«˜ç»´å¼•å¯¼æ–¹å‘å¼•å…¥çš„å™ªå£°ã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†æµå½¢è½¬å‘ï¼ˆManifold Steeringï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒå°†å¼•å¯¼æ–¹å‘å·§å¦™åœ°æŠ•å½±åˆ°ä½ç»´æ¿€æ´»æµå½¢ä¸Šï¼Œå®ç°äº†å¹²æ‰°å™ªå£°çš„ç†è®ºè¿‘ä¼¼ã€‚åœ¨DeepSeek-R1è’¸é¦æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è¾“å‡ºä»¤ç‰Œå‡å°‘äº†é«˜è¾¾71%ï¼ŒåŒæ—¶åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šç»´æŒç”šè‡³æé«˜äº†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜è¡¨ç°å‡ºå¼ºå¤§çš„è·¨åŸŸå¯è¿ç§»æ€§ï¼Œåœ¨ä»£ç ç”Ÿæˆå’ŒçŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¸€è‡´çš„ä»¤ç‰Œå‡å°‘æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Aries-iai/Manifold_Steering">https://github.com/Aries-iai/Manifold_Steering</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22411v1">PDF</a> 17 pages, 7 figures</p>
<p><strong>Summary</strong><br>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è§£å†³æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å¸¸å¸¸å‡ºç°è¿‡åº¦æ€è€ƒç°è±¡ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¢å¤§ã€‚æœ¬æ–‡ä»æœºæ¢°è§£é‡Šæ€§çš„è§’åº¦æ¢è®¨ç¼“è§£è¿‡åº¦æ€è€ƒçš„æ–¹æ³•ã€‚é€šè¿‡å¹²é¢„æ¨¡å‹çš„æ¿€æ´»ç©ºé—´æ–¹å‘ï¼Œå¯ä»¥æœ‰æ•ˆæ•æ‰è¿‡åº¦æ€è€ƒå€¾å‘å¹¶ç¼“è§£è¯¥é—®é¢˜ï¼Œä½†éšç€å¹²é¢„å¼ºåº¦çš„å¢åŠ ï¼Œæ•ˆæœä¼šè¾¾åˆ°ç“¶é¢ˆç”šè‡³æ¶åŒ–ã€‚ç ”ç©¶å‘ç°è¿‡åº¦æ€è€ƒç°è±¡ä¸ä½ç»´æµå½¢æœ‰å…³ï¼Œå› æ­¤æå‡ºæµå½¢å¯¼èˆªæ–¹æ³•ï¼Œå°†å¯¼èˆªæ–¹å‘æŠ•å½±åˆ°ä½ç»´æ¿€æ´»æµå½¢ä¸Šï¼Œä»¥å‡å°‘å¹²æ‰°å™ªå£°çš„ç†è®ºè¿‘ä¼¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç”šè‡³æé«˜æ•°å­¦åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾71%çš„è¾“å‡ºæ ‡è®°ã€‚æ­¤æ–¹æ³•è¿˜è¡¨ç°å‡ºè·¨åŸŸçš„å¯è½¬ç§»æ€§ï¼Œåœ¨ä»£ç ç”Ÿæˆå’ŒçŸ¥è¯†é—®ç­”ä»»åŠ¡ä¸­å®ç°äº†æŒç»­å‡å°‘æ ‡è®°çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨è¿‡åº¦æ€è€ƒç°è±¡ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å¢å¤§ã€‚</li>
<li>è¿‡åº¦æ€è€ƒç°è±¡å¯ä»¥é€šè¿‡å¹²é¢„æ¨¡å‹çš„æ¿€æ´»ç©ºé—´æ–¹å‘æ¥æ•æ‰å’Œç¼“è§£ã€‚</li>
<li>è¿‡åº¦æ€è€ƒç°è±¡ä¸ä½ç»´æµå½¢æœ‰å…³ï¼Œè€Œç°æœ‰çš„å¹²é¢„æ–¹æ³•åœ¨é«˜ç»´å¯¼èˆªæ–¹å‘ä¸Šå¼•å…¥å™ªå£°ï¼Œå¯¼è‡´æ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºæµå½¢å¯¼èˆªæ–¹æ³•ï¼Œå°†å¯¼èˆªæ–¹å‘æŠ•å½±åˆ°ä½ç»´æ¿€æ´»æµå½¢ä¸Šï¼Œå‡å°‘å¹²æ‰°å™ªå£°ï¼Œæœ‰æ•ˆå‡å°‘è¾“å‡ºæ ‡è®°ï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>æµå½¢å¯¼èˆªæ–¹æ³•åœ¨ä¿æŒç”šè‡³æé«˜æ•°å­¦åŸºå‡†æµ‹è¯•å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾71%çš„è¾“å‡ºæ ‡è®°ã€‚</li>
<li>æµå½¢å¯¼èˆªæ–¹æ³•å…·æœ‰è·¨åŸŸçš„å¯è½¬ç§»æ€§ï¼Œé€‚ç”¨äºä»£ç ç”Ÿæˆå’ŒçŸ¥è¯†é—®ç­”ç­‰å¤šç§ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9eb8f61632a9854f7e06552ad8c5a9c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea23e5a760e47b67945c94e3a11de4e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a17e612981f7f913c089e8adb14bd84.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Self-Reflective-Reinforcement-Learning-for-Diffusion-based-Image-Reasoning-Generation"><a href="#Self-Reflective-Reinforcement-Learning-for-Diffusion-based-Image-Reasoning-Generation" class="headerlink" title="Self-Reflective Reinforcement Learning for Diffusion-based Image   Reasoning Generation"></a>Self-Reflective Reinforcement Learning for Diffusion-based Image   Reasoning Generation</h2><p><strong>Authors:Jiadong Pan, Zhiyuan Ma, Kaiyan Zhang, Ning Ding, Bowen Zhou</strong></p>
<p>Diffusion models have recently demonstrated exceptional performance in image generation task. However, existing image generation methods still significantly suffer from the dilemma of image reasoning, especially in logic-centered image generation tasks. Inspired by the success of Chain of Thought (CoT) and Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL algorithm for diffusion models to achieve reasoning generation of logical images by performing reflection and iteration across generation trajectories. The intermediate samples in the denoising process carry noise, making accurate reward evaluation difficult. To address this challenge, SRRL treats the entire denoising trajectory as a CoT step with multi-round reflective denoising process and introduces condition guided forward process, which allows for reflective iteration between CoT steps. Through SRRL-based iterative diffusion training, we introduce image reasoning through CoT into generation tasks adhering to physical laws and unconventional physical phenomena for the first time. Notably, experimental results of case study exhibit that the superior performance of our SRRL algorithm even compared with GPT-4o. The project page is <a target="_blank" rel="noopener" href="https://jadenpan0.github.io/srrl.github.io/">https://jadenpan0.github.io/srrl.github.io/</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒç”Ÿæˆæ–¹æ³•ä»ç„¶é¢ä¸´ç€å›¾åƒæ¨ç†çš„å›°å¢ƒï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥é€»è¾‘ä¸ºä¸­å¿ƒçš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ã€‚å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SRRLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„è‡ªåæ€RLç®—æ³•ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè½¨è¿¹ä¸Šè¿›è¡Œåæ€å’Œè¿­ä»£ï¼Œå®ç°é€»è¾‘å›¾åƒçš„æ¨ç†ç”Ÿæˆã€‚å»å™ªè¿‡ç¨‹ä¸­çš„ä¸­é—´æ ·æœ¬å¸¦æœ‰å™ªå£°ï¼Œä½¿å¾—å‡†ç¡®çš„å¥–åŠ±è¯„ä¼°å˜å¾—å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒSRRLå°†æ•´ä¸ªå»å™ªè½¨è¿¹è§†ä¸ºå¤šè½®åæ€å»å™ªè¿‡ç¨‹çš„æ€ç»´é“¾æ­¥éª¤ï¼Œå¹¶å¼•å…¥æ¡ä»¶å¼•å¯¼å‰å‘è¿‡ç¨‹ï¼Œå…è®¸åœ¨æ€ç»´é“¾æ­¥éª¤ä¹‹é—´è¿›è¡Œåæ€è¿­ä»£ã€‚é€šè¿‡åŸºäºSRRLçš„è¿­ä»£æ‰©æ•£è®­ç»ƒï¼Œæˆ‘ä»¬é¦–æ¬¡å°†éµå¾ªç‰©ç†å®šå¾‹å’Œéä¼ ç»Ÿç‰©ç†ç°è±¡çš„å›¾åƒæ¨ç†å¼•å…¥ç”Ÿæˆä»»åŠ¡ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¡ˆä¾‹ç ”ç©¶çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SRRLç®—æ³•ç”šè‡³ä¼˜äºGPT- 4oã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://jadenpan0.github.io/srrl.github.io/%E3%80%82">https://jadenpan0.github.io/srrl.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22407v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨é€»è¾‘å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ä»é¢ä¸´å›¾åƒæ¨ç†çš„å›°å¢ƒã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ€ç»´é“¾å’Œå¼ºåŒ–å­¦ä¹ æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºSRRLï¼Œä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„è‡ªåæ€å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡ç”Ÿæˆè½¨è¿¹çš„åæ€å’Œè¿­ä»£å®ç°é€»è¾‘å›¾åƒçš„æ¨ç†ç”Ÿæˆã€‚ä¸ºè§£å†³å»å™ªè¿‡ç¨‹ä¸­æ ·æœ¬æºå¸¦å™ªå£°å¯¼è‡´çš„å‡†ç¡®å¥–åŠ±è¯„ä¼°å›°éš¾çš„é—®é¢˜ï¼ŒSRRLå°†æ•´ä¸ªå»å™ªè½¨è¿¹è§†ä¸ºå¤šè½®åæ€å»å™ªè¿‡ç¨‹çš„æ€ç»´é“¾æ­¥éª¤ï¼Œå¹¶å¼•å…¥æ¡ä»¶å¼•å¯¼å‰å‘è¿‡ç¨‹ï¼Œå®ç°æ€ç»´é“¾æ­¥éª¤ä¹‹é—´çš„åæ€è¿­ä»£ã€‚åŸºäºSRRLçš„è¿­ä»£æ‰©æ•£è®­ç»ƒï¼Œæˆ‘ä»¬é¦–æ¬¡å°†éµå¾ªç‰©ç†å®šå¾‹å’Œéä¼ ç»Ÿç‰©ç†ç°è±¡çš„å›¾åƒæ¨ç†å¼•å…¥ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒæ¡ˆä¾‹ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSRRLç®—æ³•æ€§èƒ½ä¼˜å¼‚ï¼Œç”šè‡³ä¼˜äºGPT-4oã€‚é¡¹ç›®é¡µé¢ä¸º<a target="_blank" rel="noopener" href="https://jadenpan0.github.io/srrl.github.io/">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å±•ç°ä¼˜ç§€æ€§èƒ½ï¼Œä½†åœ¨é€»è¾‘å›¾åƒç”Ÿæˆä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å‡†ç¡®è¯„ä¼°å»å™ªè¿‡ç¨‹ä¸­æ ·æœ¬çš„å¥–åŠ±å€¼ã€‚</li>
<li>SRRLç®—æ³•é€šè¿‡å¼•å…¥è‡ªåæ€æœºåˆ¶è§£å†³æ­¤é—®é¢˜ï¼Œå°†å»å™ªè½¨è¿¹è§†ä¸ºå¤šè½®åæ€è¿‡ç¨‹ã€‚</li>
<li>æ¡ä»¶å¼•å¯¼å‰å‘è¿‡ç¨‹å…è®¸åœ¨æ€ç»´é“¾æ­¥éª¤ä¹‹é—´è¿›è¡Œåæ€è¿­ä»£ã€‚</li>
<li>SRRLé¦–æ¬¡å°†éµå¾ªç‰©ç†å®šå¾‹å’Œéä¼ ç»Ÿç‰©ç†ç°è±¡çš„å›¾åƒæ¨ç†å¼•å…¥ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜SRRLç®—æ³•æ€§èƒ½ä¼˜äºæŸäº›ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b25c0e480124a49353060154e0cce5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dbcd16baf94f2904bfce96839ce3291.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6892ca63a2e17ab60e2b223ea181c9c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f47fc2b0e80df153b528672ede552e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Pangu-Embedded-An-Efficient-Dual-system-LLM-Reasoner-with-Metacognition"><a href="#Pangu-Embedded-An-Efficient-Dual-system-LLM-Reasoner-with-Metacognition" class="headerlink" title="Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition"></a>Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition</h2><p><strong>Authors:Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang, Other Contributors</strong></p>
<p>This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a â€œfastâ€ mode for routine queries and a deeper â€œslowâ€ mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners. </p>
<blockquote>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Pangu Embeddedï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨Ascendç¥ç»ç½‘ç»œå¤„ç†å•å…ƒï¼ˆNPUsï¼‰ä¸Šå¼€å‘çš„é«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å™¨ï¼Œå…·å¤‡çµæ´»çš„å¿«æ€è€ƒå’Œæ…¢æ€è€ƒèƒ½åŠ›ã€‚Pangu Embeddedè§£å†³äº†ç°æœ‰ä¼˜åŒ–æ¨ç†çš„LLMä¸­æ™®éå­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜å’Œæ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬ä¸ºå…¶æ„å»ºæå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è¿­ä»£è’¸é¦è¿‡ç¨‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶ç»“åˆè·¨è¿­ä»£æ¨¡å‹åˆå¹¶ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆäº’è¡¥çŸ¥è¯†ã€‚éšåæ˜¯åœ¨Ascendé›†ç¾¤ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œç”±å»¶è¿Ÿå®¹å¿è°ƒåº¦å™¨è¿›è¡Œä¼˜åŒ–ï¼Œè¯¥è°ƒåº¦å™¨ç»“åˆäº†å»¶è¿ŸåŒæ­¥å¹¶è¡Œæ€§å’Œä¼˜å…ˆçº§æ•°æ®é˜Ÿåˆ—ã€‚å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ç”±å¤šæºè‡ªé€‚åº”å¥–åŠ±ç³»ç»Ÿï¼ˆMARSï¼‰å¼•å¯¼ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ç¡®å®šæ€§æŒ‡æ ‡å’Œè½»é‡çº§LLMè¯„ä¼°å™¨ä¸ºæ•°å­¦ã€ç¼–ç å’Œä¸€èˆ¬é—®é¢˜è§£å†³ä»»åŠ¡ç”ŸæˆåŠ¨æ€ã€ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±ä¿¡å·ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥äº†ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œèµ‹äºˆPangu Embeddedä¸€ç§ç”¨äºå¸¸è§„æŸ¥è¯¢çš„â€œå¿«é€Ÿâ€æ¨¡å¼å’Œä¸€ç§ç”¨äºå¤æ‚æ¨ç†çš„æ›´æ·±å±‚æ¬¡â€œæ…¢é€Ÿâ€æ¨¡å¼ã€‚è¯¥æ¡†æ¶æ—¢æä¾›æ‰‹åŠ¨æ¨¡å¼åˆ‡æ¢ä¾›ç”¨æˆ·æ§åˆ¶ï¼Œåˆæä¾›ä¸€ç§è‡ªåŠ¨çš„ã€æ„ŸçŸ¥å¤æ‚æ€§çš„æ¨¡å¼é€‰æ‹©æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºä»¥å¹³è¡¡å»¶è¿Ÿå’Œæ¨ç†æ·±åº¦ã€‚åœ¨AIME 2024ã€GPQAå’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPangu Embeddedå…·æœ‰7Bå‚æ•°ï¼Œä¼˜äºç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼ˆå¦‚Qwen3-8Bå’ŒGLM4-9Bï¼‰ã€‚å®ƒåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å†…å®ç°äº†å¿«é€Ÿå“åº”å’Œæœ€æ–°æ°´å¹³çš„æ¨ç†è´¨é‡ï¼Œä¸ºå¼€å‘å¼ºå¤§ä¸”å®é™…å¯éƒ¨ç½²çš„LLMæ¨ç†å™¨æŒ‡æ˜äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22375v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºAscendç¥ç»å¤„ç†å•å…ƒï¼ˆNPUsï¼‰ï¼Œå¼€å‘å‡ºäº†ä¸€æ¬¾é«˜æ•ˆçš„æ¨ç†å¤§è¯­è¨€æ¨¡å‹â€”â€”Pangu Embeddedã€‚å®ƒæ‹¥æœ‰çµæ´»çš„å¿«æ€è€ƒå’Œæ…¢æ€è€ƒèƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ¨ç†ä¼˜åŒ–å‹LLMä¸­å­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜å’Œæ¨ç†å»¶è¿Ÿç­‰æŒ‘æˆ˜ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶æ„å»ºï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡è¿­ä»£è’¸é¦è¿‡ç¨‹å¾®è°ƒæ¨¡å‹ï¼Œå¹¶ç»“åˆæ¨¡å‹åˆå¹¶æ¥æœ‰æ•ˆèšåˆäº’è¡¥çŸ¥è¯†ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶ç»“åˆå»¶è¿Ÿå®¹å¿è°ƒåº¦å™¨è¿›è¡Œä¼˜åŒ–ã€‚Pangu Embeddedå…·æœ‰å¿«é€Ÿæ¨¡å¼å’Œæ·±åº¦æ¨¡å¼ï¼Œå¯åœ¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œå¹¶ä¼˜äºç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pangu Embeddedæ˜¯ä¸€ä¸ªåŸºäºAscend NPUsçš„é«˜æ•ˆLLMæ¨ç†æ¨¡å‹ï¼Œå…·å¤‡å¿«æ…¢æ€è€ƒèƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶æ„å»ºï¼Œç¬¬ä¸€é˜¶æ®µé€šè¿‡è¿­ä»£è’¸é¦å’Œæ¨¡å‹åˆå¹¶æ¥ä¼˜åŒ–çŸ¥è¯†æ•´åˆã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œç»“åˆå»¶è¿Ÿå®¹å¿è°ƒåº¦å™¨è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>Pangu Embeddedå…·æœ‰å¿«é€Ÿå“åº”å’Œæ·±åº¦æ¨ç†æ¨¡å¼ï¼Œå¯æ‰‹åŠ¨åˆ‡æ¢æˆ–è‡ªåŠ¨æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å¼ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨AIME 2024ã€GPQAå’ŒLiveCodeBenchç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼ŒPangu Embeddedå…·æœ‰æ›´é«˜çš„æ¨ç†è´¨é‡å’Œå“åº”é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-002d2d74a8230e4302cb7cec42aae8fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30939b7363124d3bec7cd4f40d2facee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dab72efea4331e9c4dc6c0b47b351c39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-818c7a3f1ee99c1fbb95e4a993773278.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d46872bec61903266b3c1a5d7cc4e383.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text2Grad-Reinforcement-Learning-from-Natural-Language-Feedback"><a href="#Text2Grad-Reinforcement-Learning-from-Natural-Language-Feedback" class="headerlink" title="Text2Grad: Reinforcement Learning from Natural Language Feedback"></a>Text2Grad: Reinforcement Learning from Natural Language Feedback</h2><p><strong>Authors:Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</strong></p>
<p>Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the modelâ€™s policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/Text2Grad">https://github.com/microsoft/Text2Grad</a> </p>
<blockquote>
<p>ä¼ ç»ŸRLHFï¼ˆå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼‰é€šè¿‡ç²—ç•¥çš„æ ‡é‡å¥–åŠ±ä¼˜åŒ–è¯­è¨€æ¨¡å‹ï¼Œè¿™ç§å¥–åŠ±æ©ç›–äº†æˆåŠŸæˆ–å¤±è´¥èƒŒåçš„ç²¾ç»†åŸå› ï¼Œå¯¼è‡´å­¦ä¹ é€Ÿåº¦æ…¢ä¸”é€æ˜åº¦ä½ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡æç¤ºæˆ–åæ€ä¸ºRLå¢åŠ äº†æ–‡æœ¬è¯„ä»·ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ï¼Œä½†å¹¶æœªè§¦åŠæ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬å¼•å…¥äº†Text2Gradï¼Œä¸€ç§å°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬åé¦ˆè½¬åŒ–ä¸ºè·¨åº¦çº§æ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ã€‚ç»™å®šäººç±»ï¼ˆæˆ–ç¨‹åºåŒ–çš„ï¼‰è¯„ä»·ï¼ŒText2Gradå°†æ¯ä¸ªåé¦ˆçŸ­è¯­ä¸ç›¸å…³çš„ä»¤ç‰Œè·¨åº¦å¯¹é½ï¼Œå°†è¿™äº›å¯¹é½è½¬æ¢ä¸ºå¯å¾®åˆ†çš„å¥–åŠ±ä¿¡å·ï¼Œå¹¶æ‰§è¡Œæ¢¯åº¦æ›´æ–°ï¼Œç›´æ¥ä¼˜åŒ–æ¨¡å‹ç­–ç•¥ä¸­çš„ç›¸å…³éƒ¨åˆ†ã€‚è¿™äº§ç”Ÿäº†ç²¾ç¡®çš„ã€åŸºäºåé¦ˆçš„è°ƒæ•´ï¼Œè€Œä¸æ˜¯å…¨å±€çš„å¾®è°ƒã€‚Text2Gradé€šè¿‡ä¸‰ä¸ªç»„ä»¶å®ç°ï¼šï¼ˆ1ï¼‰é«˜è´¨é‡åé¦ˆæ³¨é‡Šç®¡é“ï¼Œå°†è¯„ä»·ä¸ä»¤ç‰Œè·¨åº¦é…å¯¹ï¼›ï¼ˆ2ï¼‰ç²¾ç»†å¥–åŠ±æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆçš„åŒæ—¶é¢„æµ‹è·¨åº¦çº§åˆ«çš„å¥–åŠ±å¹¶è§£é‡Šè¯„ä»·ï¼›ï¼ˆ3ï¼‰è·¨åº¦çº§ç­–ç•¥ä¼˜åŒ–å™¨ï¼Œåå‘ä¼ æ’­è‡ªç„¶è¯­è¨€æ¢¯åº¦ã€‚åœ¨æ‘˜è¦ã€ä»£ç ç”Ÿæˆå’Œé—®ç­”ç­‰ä»»åŠ¡ä¸­ï¼ŒText2Gradå§‹ç»ˆè¶…è¶Šäº†æ ‡é‡å¥–åŠ±RLå’Œä»…æç¤ºçš„åŸºçº¿ï¼Œæä¾›äº†æ›´é«˜çš„ä»»åŠ¡æŒ‡æ ‡å’Œæ›´ä¸°å¯Œçš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“è‡ªç„¶è¯­è¨€åé¦ˆè¢«è½¬åŒ–ä¸ºæ¢¯åº¦æ—¶ï¼Œå®ƒæ˜¯ç²¾ç»†ç­–ç•¥ä¼˜åŒ–çš„å¼ºå¤§ä¿¡å·ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/Text2Grad">https://github.com/microsoft/Text2Grad</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22338v1">PDF</a> The code for our method is available at   <a target="_blank" rel="noopener" href="https://github.com/microsoft/Text2Grad">https://github.com/microsoft/Text2Grad</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–è¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ä½¿ç”¨ç²—ç•¥çš„æ ‡é‡å¥–åŠ±å¯¼è‡´å­¦ä¹ é€Ÿåº¦æ…¢ä¸”ä¸å¤Ÿé€æ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ€è¿‘çš„å·¥ä½œå°è¯•å°†æ–‡æœ¬æ‰¹è¯„ä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä½†å¹¶æœªè§¦åŠæ¨¡å‹å‚æ•°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºText2Gradçš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå®ƒå°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬åé¦ˆè½¬åŒ–ä¸ºè·¨åº¦çš„æ¢¯åº¦ã€‚Text2Gradé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å®ç°ï¼šé«˜è´¨é‡çš„åé¦ˆæ³¨é‡Šç®¡é“ã€ç²¾ç»†å¥–åŠ±æ¨¡å‹å’Œè·¨åº¦çº§æ”¿ç­–ä¼˜åŒ–å™¨ã€‚è¯¥æ–¹æ³•åœ¨æ€»ç»“ã€ä»£ç ç”Ÿæˆå’Œé—®ç­”ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†ä½¿ç”¨æ ‡é‡å¥–åŠ±çš„RLå’Œä»…ä½¿ç”¨æç¤ºçš„åŸºçº¿ï¼Œæ—¢æé«˜äº†ä»»åŠ¡æŒ‡æ ‡åˆä¸°å¯Œäº†å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–è¯­è¨€æ¨¡å‹æ—¶å­˜åœ¨ç²—ç²’åº¦å¥–åŠ±çš„é—®é¢˜ï¼Œå¯¼è‡´å­¦ä¹ é€Ÿåº¦æ…¢ã€ä¸å¤Ÿé€æ˜ã€‚</li>
<li>Text2Gradæ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬åé¦ˆè½¬åŒ–ä¸ºè·¨åº¦çº§çš„æ¢¯åº¦ã€‚</li>
<li>Text2GradåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåé¦ˆæ³¨é‡Šç®¡é“ã€ç²¾ç»†å¥–åŠ±æ¨¡å‹å’Œè·¨åº¦çº§æ”¿ç­–ä¼˜åŒ–å™¨ã€‚</li>
<li>Text2Gradé€šè¿‡ç›´æ¥ä¼˜åŒ–æ¨¡å‹ç­–ç•¥çš„ç›¸å…³éƒ¨åˆ†ï¼Œå®ç°äº†ç²¾ç¡®ã€åŸºäºåé¦ˆçš„è°ƒæ•´ï¼Œè€Œéå…¨å±€å¾®è°ƒã€‚</li>
<li>Text2Gradåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬æ€»ç»“ã€ä»£ç ç”Ÿæˆå’Œé—®ç­”ï¼Œæ—¢æé«˜äº†ä»»åŠ¡æŒ‡æ ‡åˆå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>Text2Gradæ–¹æ³•å…·æœ‰å¼ºå¤§çš„å®é™…åº”ç”¨æ½œåŠ›ï¼Œå¯ä»¥å°†è‡ªç„¶è¯­è¨€åé¦ˆè½¬åŒ–ä¸ºæ¢¯åº¦ï¼Œè¿›è¡Œç²¾ç»†çš„æ”¿ç­–ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b54e563fba54427572e47ac744b9044b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f05781d2d25c85108c92519f8e95bc6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f880c4a5abc45cdb5adaf9ccdb6b439.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Advancing-Multimodal-Reasoning-via-Reinforcement-Learning-with-Cold-Start"><a href="#Advancing-Multimodal-Reasoning-via-Reinforcement-Learning-with-Cold-Start" class="headerlink" title="Advancing Multimodal Reasoning via Reinforcement Learning with Cold   Start"></a>Advancing Multimodal Reasoning via Reinforcement Learning with Cold   Start</h2><p><strong>Authors:Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, Weiran Huang</strong></p>
<p>Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While â€œaha momentâ€ patternsâ€“where models exhibit self-correction through reflectionâ€“are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/waltonfuture/RL-with-Cold-Start">https://github.com/waltonfuture/RL-with-Cold-Start</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å±•ç¤ºå‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€ç»´é“¾æ¨ç†èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚è™½ç„¶â€œå•Šå“ˆæ—¶åˆ»â€æ¨¡å¼â€”â€”æ¨¡å‹é€šè¿‡åæ€è¿›è¡Œè‡ªæˆ‘ä¿®æ­£â€”â€”é€šå¸¸è¢«å½’å› äºRLçš„çªå‘å±æ€§ï¼Œä½†æˆ‘ä»¬é¦–å…ˆè¯æ˜è¿™äº›æ¨¡å¼å­˜åœ¨äºå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰çš„RLè®­ç»ƒä¹‹å‰ï¼Œä½†ä¸ä¸€å®šä¸æé«˜çš„æ¨ç†æ€§èƒ½ç›¸å…³è”ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•å¢å¼ºå¤šæ¨¡æ€æ¨ç†çš„ç»¼åˆç ”ç©¶ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç»“æ„åŒ–æ€ç»´é“¾æ¨ç†æ¨¡å¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºå†·å¯åŠ¨ï¼›ï¼ˆ2ï¼‰é€šè¿‡GRPOè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥æ”¹è¿›è¿™äº›èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç»„åˆæ–¹æ³•åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨SFTæˆ–ä»…ä½¿ç”¨RLçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨3Bå’Œ7Bè§„æ¨¡ä¸Šå‡è¾¾åˆ°äº†å¼€æºMLLMçš„æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå…¶ä¸­æˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨MathVistaï¼ˆä»66.3%æé«˜åˆ°73.4%ï¼‰å’ŒWe-Mathï¼ˆä»62.9%æé«˜åˆ°70.4%ï¼‰ä¸Šå‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œä¸ºæ„å»ºå…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/waltonfuture/RL-with-Cold-Start%E3%80%82">https://github.com/waltonfuture/RL-with-Cold-Startã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22334v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿ç»­æ¨ç†èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œâ€œå•Šå“ˆæ—¶åˆ»â€ï¼ˆæ¨¡å‹é€šè¿‡åæ€è¿›è¡Œè‡ªæˆ‘çº æ­£çš„æ—¶åˆ»ï¼‰çš„æ¨¡å¼åœ¨RLè®­ç»ƒä¹‹å‰å°±å­˜åœ¨äºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ï¼Œä½†å¹¶ä¸ä¸€å®šä¸æ¨ç†æ€§èƒ½çš„æé«˜æœ‰å…³ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬ç ”ç©¶é€šè¿‡ä¸¤é˜¶æ®µçš„æ–¹æ³•æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼šé¦–å…ˆæ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å†·å¯åŠ¨é˜¶æ®µï¼Œå½¢æˆç»“æ„åŒ–çš„è¿ç»­æ¨ç†æ¨¡å¼ï¼Œéšåæ˜¯è¿›ä¸€æ­¥ä¼˜åŒ–é€šè¿‡å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç»“åˆçš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä»…ä½¿ç”¨SFTæˆ–ä»…ä½¿ç”¨RLçš„æ–¹æ³•ã€‚æ‰€å¾—åˆ°çš„æ¨¡å‹åœ¨å¼€æºçš„MLLMä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œä¾‹å¦‚åœ¨æ•°å­¦å¯è§†åŒ–é¢†åŸŸæå‡äº†6%ä»¥ä¸Šï¼Œä¸”åœ¨æ›´å¤æ‚çš„æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç ”ç©¶ä¸ºæ„å»ºå…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„è¿ç»­æ¨ç†èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨å…¶ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>â€œå•Šå“ˆæ—¶åˆ»â€æ¨¡å¼åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œä¸æ¨ç†æ€§èƒ½çš„æå‡ä¸å¿…ç„¶ç›¸å…³ã€‚</li>
<li>ä¸¤é˜¶æ®µæ–¹æ³•ç”¨äºå¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼šç›‘ç£å¾®è°ƒå†·å¯åŠ¨åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒã€‚</li>
<li>ç»“åˆæ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå•ä¸€æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå®ç°é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬æ•°å­¦å¯è§†åŒ–é¢†åŸŸå’Œå¤æ‚æ•°å­¦é—®é¢˜å¤„ç†ã€‚</li>
<li>ç ”ç©¶ä¸ºæ„å»ºå…ˆè¿›å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8726e3251f436563933a655b00f80ead.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-432d199317e2c401a1994b9af512ce70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7bdc4addaa492779fc214fd1976d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bbe5b8d0a04190c4801fad5267a4df7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-for-Large-Language-Model-empowered-Wireless-Communications"><a href="#Chain-of-Thought-for-Large-Language-Model-empowered-Wireless-Communications" class="headerlink" title="Chain-of-Thought for Large Language Model-empowered Wireless   Communications"></a>Chain-of-Thought for Large Language Model-empowered Wireless   Communications</h2><p><strong>Authors:Xudong Wang, Jian Zhu, Ruichen Zhang, Lei Feng, Dusit Niyato, Jiacheng Wang, Hongyang Du, Shiwen Mao, Zhu Han</strong></p>
<p>Recent advances in large language models (LLMs) have opened new possibilities for automated reasoning and decision-making in wireless networks. However, applying LLMs to wireless communications presents challenges such as limited capability in handling complex logic, generalization, and reasoning. Chain-of-Thought (CoT) prompting, which guides LLMs to generate explicit intermediate reasoning steps, has been shown to significantly improve LLM performance on complex tasks. Inspired by this, this paper explores the application potential of CoT-enhanced LLMs in wireless communications. Specifically, we first review the fundamental theory of CoT and summarize various types of CoT. We then survey key CoT and LLM techniques relevant to wireless communication and networking. Moreover, we introduce a multi-layer intent-driven CoT framework that bridges high-level user intent expressed in natural language with concrete wireless control actions. Our proposed framework sequentially parses and clusters intent, selects appropriate CoT reasoning modules via reinforcement learning, then generates interpretable control policies for system configuration. Using the unmanned aerial vehicle (UAV) network as a case study, we demonstrate that the proposed framework significantly outperforms a non-CoT baseline in both communication performance and quality of generated reasoning. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºæ— çº¿ç½‘ç»œçš„è‡ªåŠ¨åŒ–æ¨ç†å’Œå†³ç­–å¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œå°†LLMåº”ç”¨äºæ— çº¿é€šä¿¡ä¹Ÿå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¦‚å¤„ç†å¤æ‚é€»è¾‘ã€æ¦‚æ‹¬å’Œæ¨ç†çš„èƒ½åŠ›æœ‰é™ç­‰ã€‚æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå¯ä»¥å¼•å¯¼LLMç”Ÿæˆæ˜ç¡®çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå·²è¢«è¯æ˜å¯ä»¥æ˜¾è‘—æé«˜LLMåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡æ¢è®¨äº†å¢å¼ºå‹æ€ç»´é“¾LLMåœ¨æ— çº¿é€šä¿¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾äº†æ€ç»´é“¾çš„åŸºæœ¬ç†è®ºï¼Œå¹¶æ€»ç»“äº†å„ç§æ€ç»´é“¾çš„ç±»å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ä¸æ— çº¿é€šä¿¡å’Œç½‘ç»œç›¸å…³çš„å…³é”®æ€ç»´é“¾å’ŒLLMæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå¤šå±‚çš„æ„å›¾é©±åŠ¨æ€ç»´é“¾æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„é«˜çº§ç”¨æˆ·æ„å›¾ä¸å…·ä½“çš„æ— çº¿æ§åˆ¶åŠ¨ä½œè”ç³»èµ·æ¥ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶æŒ‰é¡ºåºè§£æå’Œèšç±»æ„å›¾ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é€‰æ‹©é€‚å½“çš„æ€ç»´é“¾æ¨ç†æ¨¡å—ï¼Œç„¶åä¸ºç³»ç»Ÿé…ç½®ç”Ÿæˆå¯è§£é‡Šçš„æ§åˆ¶ç­–ç•¥ã€‚ä»¥æ— äººæœºï¼ˆUAVï¼‰ç½‘ç»œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ¡†æ¶åœ¨é€šä¿¡æ€§èƒ½å’Œç”Ÿæˆçš„æ¨ç†è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºéæ€ç»´é“¾åŸºå‡†çº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22320v1">PDF</a> 7 pages, 5 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ— çº¿é€šè®¯é¢†åŸŸçš„åº”ç”¨å¸¦æ¥äº†æ–°çš„è‡ªåŠ¨åŒ–æ¨ç†å’Œå†³ç­–å¯èƒ½æ€§ï¼Œä½†å­˜åœ¨å¤„ç†å¤æ‚é€»è¾‘ã€æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›æœ‰é™ç­‰æŒ‘æˆ˜ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥æ˜¾è‘—æé«˜LLMåœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†åº”ç”¨CoTå¢å¼ºLLMåœ¨æ— çº¿é€šè®¯ä¸­çš„æ½œåŠ›ï¼Œå¹¶ä»‹ç»äº†å¤šå±‚æ¬¡æ„å›¾é©±åŠ¨CoTæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„é«˜å±‚æ¬¡ç”¨æˆ·æ„å›¾ä¸å…·ä½“çš„æ— çº¿æ§åˆ¶åŠ¨ä½œï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é€‰æ‹©é€‚å½“çš„CoTæ¨ç†æ¨¡å—ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ§åˆ¶ç­–ç•¥è¿›è¡Œç³»ç»Ÿé…ç½®ã€‚ä»¥æ— äººæœºï¼ˆUAVï¼‰ç½‘ç»œä¸ºä¾‹ï¼Œè¯¥æ¡†æ¶åœ¨é€šä¿¡æ€§èƒ½å’Œæ¨ç†è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºéCoTåŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ— çº¿é€šè®¯é¢†åŸŸçš„åº”ç”¨å¸¦æ¥è‡ªåŠ¨åŒ–æ¨ç†å’Œå†³ç­–çš„æ–°æœºä¼šã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯æ˜¾è‘—æé«˜LLMåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„æ€§èƒ½ã€‚</li>
<li>CoTå¢å¼ºLLMåœ¨æ— çº¿é€šè®¯ä¸­çš„æ½œåŠ›å¾—åˆ°æ¢è®¨ã€‚</li>
<li>ä»‹ç»äº†å¤šå±‚æ¬¡æ„å›¾é©±åŠ¨çš„CoTæ¡†æ¶ï¼Œç»“åˆé«˜å±‚æ¬¡ç”¨æˆ·æ„å›¾å’Œå…·ä½“æ— çº¿æ§åˆ¶åŠ¨ä½œã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ é€‰æ‹©é€‚å½“çš„CoTæ¨ç†æ¨¡å—ã€‚</li>
<li>ä»¥æ— äººæœºï¼ˆUAVï¼‰ç½‘ç»œä¸ºä¾‹ï¼ŒCoTæ¡†æ¶åœ¨é€šä¿¡æ€§èƒ½å’Œæ¨ç†è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºéCoTåŸºå‡†ã€‚</li>
<li>LLMåœ¨æ— çº¿é€šè®¯ä¸­çš„åº”ç”¨ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚é€»è¾‘ã€æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›çš„é™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be0dd0fed21ea1d8556b1daf792c6415.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e2819d016499d2375572233f7d49f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-459d186b82ab3bc2a502558da12ef580.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0fbc96e220eb1bdc42d9a60ca713f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67628f0fc2bb14f090bd6b5e716b89ba.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Revisiting-Group-Relative-Policy-Optimization-Insights-into-On-Policy-and-Off-Policy-Training"><a href="#Revisiting-Group-Relative-Policy-Optimization-Insights-into-On-Policy-and-Off-Policy-Training" class="headerlink" title="Revisiting Group Relative Policy Optimization: Insights into On-Policy   and Off-Policy Training"></a>Revisiting Group Relative Policy Optimization: Insights into On-Policy   and Off-Policy Training</h2><p><strong>Authors:Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios</strong></p>
<p>We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart. </p>
<blockquote>
<p>æˆ‘ä»¬é‡æ–°ç ”ç©¶äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰åœ¨on-policyå’Œoff-policyä¼˜åŒ–æœºåˆ¶ä¸‹çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„çµæ„Ÿæ¥è‡ªäºè¿‘æœŸå…³äºoff-policyè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰çš„ç ”ç©¶ï¼Œå®ƒæé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€é‡‡æ ·æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œå¯¹GRPOçš„æœ€æ–°åˆ†æè¡¨æ˜ï¼Œåˆ©ç”¨off-policyæ ·æœ¬ä¼°è®¡ä¼˜åŠ¿å‡½æ•°å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬å°†GRPOé€‚åº”äºoff-policyè®¾ç½®ã€‚æˆ‘ä»¬å±•ç¤ºäº†on-policyå’Œoff-policy GRPOç›®æ ‡éƒ½æé«˜äº†å¥–åŠ±ã€‚è¿™ä¸€ç»“æœæ¨åŠ¨äº†åœ¨GRPOçš„off-policyç‰ˆæœ¬ä¸­ä½¿ç”¨å‰ªåˆ‡æ›¿ä»£ç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒåä½¿ç”¨è¿™ä¸¤ç§GRPOå˜ä½“æ¯”è¾ƒäº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ çš„ç»éªŒæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œoff-policy GRPOè¦ä¹ˆæ˜¾è‘—ä¼˜äºå…¶on-policyå¯¹åº”ç‰ˆæœ¬ï¼Œè¦ä¹ˆä¸ä¹‹è¡¨ç°ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22257v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦æ¢è®¨äº†Group Relative Policy Optimization (GRPO)åœ¨on-policyå’Œoff-policyä¼˜åŒ–ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚æ–‡ç« å—åˆ°è¿‘æœŸPPOç®—æ³•çš„å¯å‘ï¼Œè¯¥ç®—æ³•æé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€é‡‡æ ·æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ–‡ç« å°†GRPOé€‚åº”åˆ°off-policyç¯å¢ƒä¸­ï¼Œå¹¶å‘ç°æ— è®ºæ˜¯on-policyè¿˜æ˜¯off-policyç‰ˆæœ¬çš„GRPOéƒ½èƒ½æé«˜å¥–åŠ±ï¼Œè¿™è¡¨æ˜ä½¿ç”¨å¯å‰ªè¾‘çš„æ›¿ä»£ç›®æ ‡åœ¨off-policyç‰ˆæœ¬çš„GRPOä¸­æ˜¯æœ‰æ•ˆçš„ã€‚æœ€åï¼Œæ–‡ç« é€šè¿‡å®è¯ç ”ç©¶æ¯”è¾ƒäº†å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±åœ¨è®­ç»ƒåçš„è¡¨ç°ï¼Œç»“æœè¡¨æ˜off-policy GRPOåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæˆ–ä¸on-policy GRPOä¸ç›¸ä¸Šä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ¢è®¨äº†Group Relative Policy Optimization (GRPO)åœ¨on-policyå’Œoff-policyä¸¤ç§ä¼˜åŒ–ç¯å¢ƒä¸‹çš„åº”ç”¨ã€‚</li>
<li>å—åˆ°è¿‘æœŸPPOç®—æ³•çš„å¯å‘ï¼Œè¯¥ç®—æ³•æé«˜äº†è®­ç»ƒç¨³å®šæ€§ã€é‡‡æ ·æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>æ–‡ç« å°†GRPOé€‚åº”åˆ°off-policyç¯å¢ƒä¸­ï¼Œå¹¶å‘ç°æ— è®ºæ˜¯å“ªç§ç‰ˆæœ¬çš„GRPOéƒ½èƒ½æé«˜å¥–åŠ±ã€‚</li>
<li>ä½¿ç”¨å¯å‰ªè¾‘çš„æ›¿ä»£ç›®æ ‡åœ¨off-policyç‰ˆæœ¬çš„GRPOä¸­æ˜¯æœ‰æ•ˆçš„ã€‚</li>
<li>æ–‡ç« æ¯”è¾ƒäº†å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±åœ¨è®­ç»ƒåçš„è¡¨ç°ã€‚</li>
<li>Off-policy GRPOåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæˆ–ä¸on-policy GRPOä¸ç›¸ä¸Šä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8538603b60e790d1c490e0c0b8f36d79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7732f3227734bb07c33c3518e413cbec.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BioHopR-A-Benchmark-for-Multi-Hop-Multi-Answer-Reasoning-in-Biomedical-Domain"><a href="#BioHopR-A-Benchmark-for-Multi-Hop-Multi-Answer-Reasoning-in-Biomedical-Domain" class="headerlink" title="BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical   Domain"></a>BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical   Domain</h2><p><strong>Authors:Yunsoo Kim, Yusuf Abdulle, Honghan Wu</strong></p>
<p>Biomedical reasoning often requires traversing interconnected relationships across entities such as drugs, diseases, and proteins. Despite the increasing prominence of large language models (LLMs), existing benchmarks lack the ability to evaluate multi-hop reasoning in the biomedical domain, particularly for queries involving one-to-many and many-to-many relationships. This gap leaves the critical challenges of biomedical multi-hop reasoning underexplored. To address this, we introduce BioHopR, a novel benchmark designed to evaluate multi-hop, multi-answer reasoning in structured biomedical knowledge graphs. Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop reasoning tasks that reflect real-world biomedical complexities.   Evaluations of state-of-the-art models reveal that O3-mini, a proprietary reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on 2-hop tasks, outperforming proprietary models such as GPT4O and open-source biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all models exhibit significant declines in multi-hop performance, underscoring the challenges of resolving implicit reasoning steps in the biomedical domain. By addressing the lack of benchmarks for multi-hop reasoning in biomedical domain, BioHopR sets a new standard for evaluating reasoning capabilities and highlights critical gaps between proprietary and open-source models while paving the way for future advancements in biomedical LLMs. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦æ¨ç†é€šå¸¸éœ€è¦éå†è¯ç‰©ã€ç–¾ç—…å’Œè›‹ç™½è´¨ç­‰å®ä½“ä¹‹é—´çš„äº’è”å…³ç³»ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çªå‡ºæ€§æ—¥ç›Šå¢å¼ºï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•è¯„ä¼°ç”Ÿç‰©åŒ»å­¦é¢†åŸŸä¸­çš„å¤šè·³æ¨ç†ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ¶‰åŠä¸€å¯¹ä¸€å’Œä¸€å¯¹å¤šå…³ç³»çš„æŸ¥è¯¢ã€‚è¿™ä¸€å·®è·ä½¿å¾—ç”Ÿç‰©åŒ»å­¦å¤šè·³æ¨ç†çš„å…³é”®æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†BioHopRï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°ç»“æ„åŒ–ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾è°±ä¸­å¤šè·³å¤šç­”æ¡ˆæ¨ç†çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚BioHopRå»ºç«‹åœ¨å…¨é¢çš„PrimeKGä¹‹ä¸Šï¼ŒåŒ…å«åæ˜ ç°å®ä¸–ç•Œç”Ÿç‰©åŒ»å­¦å¤æ‚æ€§çš„1è·³å’Œ2è·³æ¨ç†ä»»åŠ¡ã€‚å¯¹æœ€æ–°æ¨¡å‹çš„è¯„ä»·æ˜¾ç¤ºï¼ŒO3-miniï¼Œä¸€ä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„ä¸“æœ‰æ¨¡å‹ï¼Œåœ¨1è·³ä»»åŠ¡ä¸Šè¾¾åˆ°37.93%çš„ç²¾åº¦ï¼Œåœ¨2è·³ä»»åŠ¡ä¸Šè¾¾åˆ°14.57%ï¼Œè¶…è¶Šäº†GPT4Oç­‰ä¸“æœ‰æ¨¡å‹å’ŒåŒ…æ‹¬HuatuoGPT-o1-70Bå’ŒLlama-3.3-70Båœ¨å†…çš„å¼€æºç”Ÿç‰©åŒ»å­¦æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤šè·³æ€§èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—ä¸‹é™ï¼Œè¿™å¼ºè°ƒäº†è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸéšå¼æ¨ç†æ­¥éª¤çš„æŒ‘æˆ˜ã€‚é€šè¿‡è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•çš„ç¼ºä¹ï¼ŒBioHopRä¸ºè¯„ä¼°æ¨ç†èƒ½åŠ›è®¾å®šäº†æ–°æ ‡å‡†ï¼Œçªå‡ºäº†ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å…³é”®å·®è·ï¼ŒåŒæ—¶ä¸ºç”Ÿç‰©åŒ»å­¦LLMçš„æœªæ¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22240v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿç‰©åŒ»å­¦æ¨ç†ä¸­å¤šè·³æ¨ç†çš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•è¯„ä¼°ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå¤šè·³æ¨ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•BioHopRã€‚è¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°ç»“æ„åŒ–ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾ä¸­çš„å¤šè·³å¤šç­”æ¡ˆæ¨ç†èƒ½åŠ›ã€‚BioHopRåŒ…æ‹¬åæ˜ ç°å®ç”Ÿç‰©åŒ»å­¦å¤æ‚æ€§çš„1è·³å’Œ2è·³æ¨ç†ä»»åŠ¡ã€‚å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè‡ªä¸»å¼€å‘çš„æ¨ç†èšç„¦æ¨¡å‹O3-miniåœ¨1è·³ä»»åŠ¡ä¸Šè¾¾åˆ°37.93%çš„å‡†ç¡®ç‡ï¼Œåœ¨2è·³ä»»åŠ¡ä¸Šè¾¾åˆ°14.57%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºGPT4Oç­‰è‡ªä¸»æ¨¡å‹ä»¥åŠå¼€æºç”Ÿç‰©åŒ»å­¦æ¨¡å‹ï¼Œå¦‚HuatuoGPT-o1-70Bå’ŒLlama-3.3-70Bã€‚ç„¶è€Œï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤šè·³ä»»åŠ¡ä¸Šçš„æ€§èƒ½éƒ½æœ‰æ˜¾è‘—ä¸‹é™ï¼Œå‡¸æ˜¾äº†è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸéšå«æ¨ç†æ­¥éª¤çš„æŒ‘æˆ˜ã€‚BioHopRå¡«è¡¥äº†ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œä¸ºè¯„ä¼°æ¨ç†èƒ½åŠ›è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œæ­ç¤ºäº†è‡ªä¸»æ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å…³é”®å·®è·ï¼Œå¹¶ä¸ºç”Ÿç‰©åŒ»å­¦å¤§å‹è¯­è¨€æ¨¡å‹çš„æœªæ¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>ç”Ÿç‰©åŒ»å­¦æ¨ç†éœ€è¦è·¨å®ä½“ä¹‹é—´çš„å…³è”å…³ç³»è¿›è¡Œå¤šè·³æ¨ç†ï¼Œå¦‚è¯ç‰©ã€ç–¾ç—…å’Œè›‹ç™½è´¨ã€‚</li>
<li>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•è¯„ä¼°ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„å¤šè·³æ¨ç†ã€‚</li>
<li>å¼•å…¥BioHopRåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ç»“æ„åŒ–ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾ä¸­çš„å¤šè·³å¤šç­”æ¡ˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>BioHopRåŒ…æ‹¬åæ˜ ç°å®ç”Ÿç‰©åŒ»å­¦å¤æ‚æ€§çš„1è·³å’Œ2è·³æ¨ç†ä»»åŠ¡ã€‚</li>
<li>O3-miniæ¨¡å‹åœ¨å¤šè·³ä»»åŠ¡æ€§èƒ½è¯„ä¼°ä¸­è¡¨ç°å‡ºæœ€å¥½çš„æ€§èƒ½ï¼Œä½†æ‰€æœ‰æ¨¡å‹éƒ½é¢ä¸´è§£å†³ç”Ÿç‰©åŒ»å­¦é¢†åŸŸéšå«æ¨ç†æ­¥éª¤çš„æŒ‘æˆ˜ã€‚</li>
<li>BioHopRå¡«è¡¥äº†ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•çš„ç©ºç™½ï¼Œä¸ºè¯„ä¼°æ¨ç†èƒ½åŠ›è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d5a2c5d571e5be2e53e1e616df3df51c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb70bce08590313f3a2b2d48ffbb9a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-491aae62762819220ba830368248ee1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec9447d8db1ea572855e7e3e1363222.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7ad37677e8c5a82ebab86597cd231d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-224ec1d63e229de4ac1d70cac76cfdc3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="What-Makes-a-Good-Reasoning-Chain-Uncovering-Structural-Patterns-in-Long-Chain-of-Thought-Reasoning"><a href="#What-Makes-a-Good-Reasoning-Chain-Uncovering-Structural-Patterns-in-Long-Chain-of-Thought-Reasoning" class="headerlink" title="What Makes a Good Reasoning Chain? Uncovering Structural Patterns in   Long Chain-of-Thought Reasoning"></a>What Makes a Good Reasoning Chain? Uncovering Structural Patterns in   Long Chain-of-Thought Reasoning</h2><p><strong>Authors:Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, Defu Lian</strong></p>
<p>Recent advances in reasoning with large language models (LLMs) have popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate and step-by-step reasoning before producing a final answer. While LCoTs have enabled expert-level performance in complex tasks, how the internal structures of their reasoning chains drive, or even predict, the correctness of final answers remains a critical yet underexplored question. In this work, we present LCoT2Tree, an automated framework that converts sequential LCoTs into hierarchical tree structures and thus enables deeper structural analysis of LLM reasoning. Using graph neural networks (GNNs), we reveal that structural patterns extracted by LCoT2Tree, including exploration, backtracking, and verification, serve as stronger predictors of final performance across a wide range of tasks and models. Leveraging an explainability technique, we further identify critical thought patterns such as over-branching that account for failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree support practical applications, including improving Best-of-N decoding effectiveness. Overall, our results underscore the critical role of internal structures of reasoning chains, positioning LCoT2Tree as a powerful tool for diagnosing, interpreting, and improving reasoning in LLMs. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢çš„è¿›å±•ï¼Œé•¿é“¾æ€ç»´ï¼ˆLCoTï¼‰ç­–ç•¥é€æ¸æ™®åŠã€‚LCoTé¼“åŠ±åœ¨äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¿›è¡Œæ·±æ€ç†Ÿè™‘ã€å¾ªåºæ¸è¿›çš„æ¨ç†ã€‚è™½ç„¶LCoTå·²ç»åœ¨å¤æ‚ä»»åŠ¡ä¸­å®ç°äº†ä¸“å®¶çº§æ€§èƒ½ï¼Œä½†å…¶æ¨ç†é“¾çš„å†…éƒ¨ç»“æ„å¦‚ä½•é©±åŠ¨ç”šè‡³é¢„æµ‹æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®ä½†å°šæœªæ·±å…¥æ¢ç´¢çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22148v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿é“¾æ€ç»´ï¼ˆLCoTï¼‰ç­–ç•¥é¼“åŠ±é€æ­¥æ¨ç†ï¼Œèƒ½åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸“å®¶çº§æ°´å¹³ã€‚ç„¶è€Œï¼Œå…¶å†…éƒ¨æ¨ç†é“¾ç»“æ„å¦‚ä½•é©±åŠ¨ç”šè‡³é¢„æµ‹æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ä»æ˜¯å…³é”®ä¸”æœªè¢«å……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºLCoT2Treeæ¡†æ¶ï¼Œèƒ½å°†é¡ºåºçš„LCoTsè½¬åŒ–ä¸ºåˆ†å±‚æ ‘ç»“æ„ï¼Œä»è€Œå®ç°å¯¹LLMæ¨ç†çš„æ›´æ·±å±‚æ¬¡ç»“æ„åˆ†æã€‚åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ï¼Œæˆ‘ä»¬å‘ç°LCoT2Treeæå–çš„ç»“æ„æ¨¡å¼ï¼Œå¦‚æ¢ç´¢ã€å›æº¯å’ŒéªŒè¯ç­‰ï¼Œåœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œæ¨¡å‹ä¸­æˆä¸ºé¢„æµ‹æœ€ç»ˆæ€§èƒ½çš„æ›´å¯é æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©è§£é‡ŠæŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†å¯¼è‡´å¤±è´¥çš„å…³é”®æ€ç»´æ¨¡å¼ï¼Œå¦‚è¿‡åº¦åˆ†æ”¯ã€‚LCoT2Treeçš„ç»“æ„æ¨¡å¼ä¸ä»…æä¾›è¯Šæ–­æ´å¯Ÿï¼Œè¿˜æ”¯æŒå®é™…åº”ç”¨ï¼Œå¦‚æé«˜Best-of-Nè§£ç çš„æœ‰æ•ˆæ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶å¼ºè°ƒæ¨ç†é“¾å†…éƒ¨ç»“æ„çš„é‡è¦æ€§ï¼Œå¹¶å°†LCoT2Treeå®šä½ä¸ºè¯Šæ–­ã€è§£é‡Šå’Œæ”¹è¿›LLMæ¨ç†çš„å¼ºå¤§å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿é“¾æ€ç»´ï¼ˆLCoTï¼‰ç­–ç•¥èƒ½å¤Ÿä¿ƒè¿›é€æ­¥æ¨ç†ï¼Œåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸“å®¶çº§æ€§èƒ½ã€‚</li>
<li>LCoT2Treeæ¡†æ¶èƒ½å°†LCoTsè½¬åŒ–ä¸ºåˆ†å±‚æ ‘ç»“æ„ï¼Œä¾¿äºå¯¹LLMæ¨ç†è¿›è¡Œæ·±å±‚æ¬¡çš„ç»“æ„åˆ†æã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰è¢«ç”¨äºåˆ†æLCoT2Treeçš„ç»“æ„æ¨¡å¼ï¼Œå‘ç°æ¢ç´¢ã€å›æº¯å’ŒéªŒè¯ç­‰æ¨¡å¼å¯¹é¢„æµ‹æœ€ç»ˆæ€§èƒ½æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>LCoT2Treeèƒ½è¯†åˆ«å¯¼è‡´å¤±è´¥çš„å…³é”®æ€ç»´æ¨¡å¼ï¼Œå¦‚è¿‡åº¦åˆ†æ”¯ã€‚</li>
<li>LCoT2Treeçš„ç»“æ„æ¨¡å¼æä¾›è¯Šæ–­æ´å¯Ÿï¼Œå¹¶æ”¯æŒå®é™…åº”ç”¨ï¼Œå¦‚æé«˜Best-of-Nè§£ç çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨ç†é“¾å†…éƒ¨ç»“æ„çš„é‡è¦æ€§è¢«å¼ºè°ƒï¼Œå¯¹LLMæ¨ç†çš„è¯Šæ–­ã€è§£é‡Šå’Œæ”¹è¿›æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>LCoT2Treeå®šä½ä¸ºå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºè¯Šæ–­ã€è§£é‡Šå’Œæ”¹è¿›LLMæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a1e6452ab8d9957318612a14aef5622.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e51d0cde800caece79bbbe383f88298b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3680abbc5a69eaec1fad8570334733c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e426e8268996415d06d65bf3e9016ec3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e12070481c249eb44d61b233ca9cc7e3.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="EULER-Enhancing-the-Reasoning-Ability-of-Large-Language-Models-through-Error-Induced-Learning"><a href="#EULER-Enhancing-the-Reasoning-Ability-of-Large-Language-Models-through-Error-Induced-Learning" class="headerlink" title="EULER: Enhancing the Reasoning Ability of Large Language Models through   Error-Induced Learning"></a>EULER: Enhancing the Reasoning Ability of Large Language Models through   Error-Induced Learning</h2><p><strong>Authors:Zhuoyang Wu, Xinze Li, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Minghe Yu, Cheng Yang, Yu Gu, Ge Yu, Maosong Sun</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong reasoning capabilities and achieved promising results in mathematical problem-solving tasks. Learning from errors offers the potential to further enhance the performance of LLMs during Supervised Fine-Tuning (SFT). However, the errors in synthesized solutions are typically gathered from sampling trails, making it challenging to generate solution errors for each mathematical problem. This paper introduces the Error-IndUced LEaRning (EULER) model, which aims to develop an error exposure model that generates high-quality solution errors to enhance the mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the error exposure model to increase the generation probability of self-made solution errors while utilizing solutions produced by a superior LLM to regularize the generation quality. Our experiments across various mathematical problem datasets demonstrate the effectiveness of the EULER model, achieving an improvement of over 4% compared to all baseline models. Further analysis reveals that EULER is capable of synthesizing more challenging and educational solution errors, which facilitate both the training and inference processes of LLMs. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/EULER">https://github.com/NEUIR/EULER</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ä»é”™è¯¯ä¸­å­¦ä¹ å¯ä»¥ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æœŸé—´è¿›ä¸€æ­¥æ”¹å–„LLMçš„æ€§èƒ½æä¾›æ½œåŠ›ã€‚ç„¶è€Œï¼Œåˆæˆè§£å†³æ–¹æ¡ˆä¸­çš„é”™è¯¯é€šå¸¸æ˜¯ä»é‡‡æ ·è½¨è¿¹æ”¶é›†çš„ï¼Œè¿™ä½¿å¾—ä¸ºæ¯ä¸ªæ•°å­¦é—®é¢˜ç”Ÿæˆè§£å†³æ–¹æ¡ˆé”™è¯¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†è¯¯å·®è¯±å¯¼å­¦ä¹ ï¼ˆEULERï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§è¯¯å·®æš´éœ²æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆé”™è¯¯ï¼Œä»¥æé«˜LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒEULERä¼˜åŒ–è¯¯å·®æš´éœ²æ¨¡å‹ï¼Œä»¥æé«˜è‡ªåˆ¶è§£å†³æ–¹æ¡ˆé”™è¯¯çš„ç”Ÿæˆæ¦‚ç‡ï¼ŒåŒæ—¶ä½¿ç”¨ç”±é«˜çº§LLMäº§ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ¥è§„èŒƒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦é—®é¢˜è§£å†³æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†EULERæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä¸æ‰€æœ‰åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå®ç°äº†è¶…è¿‡4%çš„æ”¹è¿›ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒEULERèƒ½å¤Ÿåˆæˆæ›´å…·æŒ‘æˆ˜æ€§å’Œæ•™è‚²æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆé”™è¯¯ï¼Œè¿™æœ‰åŠ©äºLLMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚æ‰€æœ‰ä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/EULER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NEUIR/EULERæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22131v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è·å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚é€šè¿‡ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œå¯ä»¥æé«˜LLMåœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æœŸé—´çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»é‡‡æ ·è½¨è¿¹ä¸­æ”¶é›†çš„é”™è¯¯é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéš¾ä»¥ç”Ÿæˆæ¯ä¸ªæ•°å­¦é—®é¢˜çš„é”™è¯¯è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡ä»‹ç»äº†Error-IndUced LEaRningï¼ˆEULERï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é”™è¯¯æš´éœ²æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆé”™è¯¯ä»¥å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚EULERä¼˜åŒ–äº†é”™è¯¯æš´éœ²æ¨¡å‹ï¼Œæé«˜äº†è‡ªåˆ¶è§£å†³æ–¹æ¡ˆé”™è¯¯çš„ç”Ÿæˆæ¦‚ç‡ï¼Œå¹¶åˆ©ç”¨é«˜çº§LLMäº§ç”Ÿçš„è§£å†³æ–¹æ¡ˆæ¥è§„èŒƒç”Ÿæˆè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒEULERæ¨¡å‹åœ¨å„ç§æ•°å­¦é—®é¢˜æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºæ‰€æœ‰åŸºå‡†æ¨¡å‹æé«˜äº†è¶…è¿‡4%çš„æ€§èƒ½ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒEULERèƒ½å¤Ÿåˆæˆæ›´å…·æŒ‘æˆ˜æ€§å’Œæ•™è‚²æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆé”™è¯¯ï¼Œæœ‰åŠ©äºLLMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œå¯ä»¥æé«˜LLMçš„æ€§èƒ½ã€‚</li>
<li>æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä»é‡‡æ ·è½¨è¿¹ä¸­ç”Ÿæˆæ¯ä¸ªæ•°å­¦é—®é¢˜çš„é”™è¯¯è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼•å…¥Error-IndUced LEaRningï¼ˆEULERï¼‰æ¨¡å‹æ¥è§£å†³æ­¤æŒ‘æˆ˜ã€‚</li>
<li>EULERæ¨¡å‹æ—¨åœ¨é€šè¿‡ç”Ÿæˆé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆé”™è¯¯æ¥å¢å¼ºLLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>EULERä¼˜åŒ–äº†é”™è¯¯æš´éœ²æ¨¡å‹ï¼Œæé«˜è‡ªåˆ¶è§£å†³æ–¹æ¡ˆé”™è¯¯çš„ç”Ÿæˆæ¦‚ç‡ï¼Œå¹¶åˆ©ç”¨é«˜çº§LLMè§£å†³æ–¹æ¡ˆæ¥è§„èŒƒç”Ÿæˆè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a453890025cb73944472dd12dac038f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-899111db2af6ee5c7b4876da3e3cda65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1fe7e657d00417d77b47b68fe1c8a3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06ffa20a9a8e98ba1dfa6ef805f09abd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-to-Route-Queries-Across-Knowledge-Bases-for-Step-wise-Retrieval-Augmented-Reasoning"><a href="#Learning-to-Route-Queries-Across-Knowledge-Bases-for-Step-wise-Retrieval-Augmented-Reasoning" class="headerlink" title="Learning to Route Queries Across Knowledge Bases for Step-wise   Retrieval-Augmented Reasoning"></a>Learning to Route Queries Across Knowledge Bases for Step-wise   Retrieval-Augmented Reasoning</h2><p><strong>Authors:Chunyi Peng, Zhipeng Xu, Zhenghao Liu, Yishan Li, Yukun Yan, Shuo Wang, Zhiyuan Liu, Yu Gu, Minghe Yu, Ge Yu, Maosong Sun</strong></p>
<p>Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in mitigating hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge during generation. Existing MRAG methods typically adopt a static retrieval pipeline that fetches relevant information from multiple Knowledge Bases (KBs), followed by a refinement step. However, these approaches overlook the reasoning and planning capabilities of MLLMs to dynamically determine how to interact with different KBs during the reasoning process. To address this limitation, we propose R1-Router, a novel MRAG framework that learns to decide when and where to retrieve knowledge based on the evolving reasoning state. Specifically, R1-Router can generate follow-up queries according to the current reasoning step, routing these intermediate queries to the most suitable KB, and integrating external knowledge into a coherent reasoning trajectory to answer the original query. Furthermore, we introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored reinforcement learning algorithm that assigns step-specific rewards to optimize the reasoning behavior of MLLMs. Experimental results on various open-domain QA benchmarks across multiple modalities demonstrate that R1-Router outperforms baseline models by over 7%. Further analysis shows that R1-Router can adaptively and effectively leverage diverse KBs, reducing unnecessary retrievals and improving both efficiency and accuracy. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰é€šè¿‡ç”Ÿæˆè¿‡ç¨‹ä¸­èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œåœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­æ˜¾ç¤ºå‡ºç¼“è§£å¹»è§‰çš„æ½œåŠ›ã€‚ç°æœ‰çš„MRAGæ–¹æ³•é€šå¸¸é‡‡ç”¨é™æ€æ£€ç´¢ç®¡é“ï¼Œä»å¤šä¸ªçŸ¥è¯†åº“ï¼ˆKBsï¼‰ä¸­è·å–ç›¸å…³ä¿¡æ¯ï¼Œç„¶åè¿›è¡Œç»†åŒ–æ­¥éª¤ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½è§†äº†MLLMsçš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œæ— æ³•åŠ¨æ€ç¡®å®šåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•ä¸ä¸åŒçš„KBsè¿›è¡Œäº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†R1-Routerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MRAGæ¡†æ¶ï¼Œèƒ½å¤Ÿå­¦ä¹ æ ¹æ®ä¸æ–­å˜åŒ–çš„æ¨ç†çŠ¶æ€æ¥å†³å®šä½•æ—¶ä½•åœ°æ£€ç´¢çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼ŒR1-Routerå¯ä»¥æ ¹æ®å½“å‰çš„æ¨ç†æ­¥éª¤ç”Ÿæˆåç»­æŸ¥è¯¢ï¼Œå°†è¿™äº›ä¸­é—´æŸ¥è¯¢è·¯ç”±åˆ°æœ€åˆé€‚çš„çŸ¥è¯†åº“ï¼Œå¹¶å°†å¤–éƒ¨çŸ¥è¯†æ•´åˆåˆ°è¿è´¯çš„æ¨ç†è½¨è¿¹ä¸­ä»¥å›ç­”åŸå§‹æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€æ­¥ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆStep-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºåˆ†é…æ­¥éª¤ç‰¹å®šçš„å¥–åŠ±æ¥ä¼˜åŒ–MLLMsçš„æ¨ç†è¡Œä¸ºã€‚åœ¨ä¸åŒæ¨¡æ€çš„å¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Routeræ¯”åŸºçº¿æ¨¡å‹é«˜å‡º7%ä»¥ä¸Šçš„è¡¨ç°ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒR1-Routerèƒ½å¤Ÿè‡ªé€‚åº”å’Œæœ‰æ•ˆåœ°åˆ©ç”¨å„ç§KBsï¼Œå‡å°‘ä¸å¿…è¦çš„æ£€ç´¢ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22095v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰æŠ€æœ¯åœ¨ç¼“è§£å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­çš„å¹»è§‰é—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚æå‡ºä¸€ç§æ–°å‹MRAGæ¡†æ¶R1-Routerï¼Œèƒ½å¤Ÿæ ¹æ®ä¸æ–­å˜åŒ–çš„æ¨ç†çŠ¶æ€å†³å®šä½•æ—¶ä½•å¤„æ£€ç´¢çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†åˆ†æ­¥éª¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆStep-GRPOï¼‰ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–MLLMçš„æ¨ç†è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Routeråœ¨å¤šä¸ªæ¨¡æ€çš„å¼€æ”¾åŸŸé—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRAGæŠ€æœ¯é€šè¿‡èå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œåœ¨ç¼“è§£MLLMæ¨¡å‹å¹»è§‰æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>R1-Routeræ˜¯ä¸€ä¸ªæ–°é¢–çš„MRAGæ¡†æ¶ï¼Œèƒ½æ ¹æ®æ¨ç†çŠ¶æ€çš„æ¼”å˜æ¥å†³å®šä½•æ—¶ä½•åœ°æ£€ç´¢çŸ¥è¯†ã€‚</li>
<li>R1-Routerèƒ½ç”Ÿæˆåç»­æŸ¥è¯¢ï¼Œå¹¶å°†å…¶è·¯ç”±åˆ°æœ€åˆé€‚çš„æ•°æ®åº“ï¼Œå°†å¤–éƒ¨çŸ¥è¯†æ•´åˆåˆ°è¿è´¯çš„æ¨ç†è½¨è¿¹ä¸­ã€‚</li>
<li>Step-GRPOæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç”¨äºä¼˜åŒ–MLLMçš„æ¨ç†è¡Œä¸ºå¹¶èµ‹äºˆæ­¥éª¤ç‰¹å®šçš„å¥–åŠ±ã€‚</li>
<li>R1-Routeråœ¨è·¨æ¨¡æ€å¼€æ”¾é¢†åŸŸé—®ç­”æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºåŸºå‡†æ¨¡å‹7%ã€‚</li>
<li>R1-Routerèƒ½è‡ªé€‚åº”æœ‰æ•ˆåœ°åˆ©ç”¨ä¸åŒçš„æ•°æ®åº“ï¼Œå‡å°‘ä¸å¿…è¦çš„æ£€ç´¢ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba20cb08e3aa92a83a40d6c30342de89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800fdcb82ff7980b2ee06dc6b6de36b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd851817b0d7fdef601598e1284ae885.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Beyond-path-selection-Better-LLMs-for-Scientific-Information-Extraction-with-MimicSFT-and-Relevance-and-Rule-induced-R-2-GRPO"><a href="#Beyond-path-selection-Better-LLMs-for-Scientific-Information-Extraction-with-MimicSFT-and-Relevance-and-Rule-induced-R-2-GRPO" class="headerlink" title="Beyond path selection: Better LLMs for Scientific Information Extraction   with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO"></a>Beyond path selection: Better LLMs for Scientific Information Extraction   with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO</h2><p><strong>Authors:Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen</strong></p>
<p>Previous study suggest that powerful Large Language Models (LLMs) trained with Reinforcement Learning with Verifiable Rewards (RLVR) only refines reasoning path without improving the reasoning capacity in math tasks while supervised-finetuning(SFT) with distillation can. We study this from the view of Scientific information extraction (SciIE) where LLMs and reasoning LLMs underperforms small Bert-based models. SciIE require both the reasoning and memorization. We argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE. We propose two-stage training with 1. MimicSFT, using structured reasoning templates without needing high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and rule-induced rewards. Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ranlislz/R2GRPO">https://github.com/ranlislz/R2GRPO</a>. </p>
<blockquote>
<p>å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒçš„å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åªåœ¨æ•°å­¦ä»»åŠ¡ä¸­ç²¾ç‚¼æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸æé«˜æ¨ç†èƒ½åŠ›ï¼Œè€Œä½¿ç”¨è’¸é¦æŠ€æœ¯è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆ™å¯ä»¥ã€‚æˆ‘ä»¬ä»ç§‘å­¦ä¿¡æ¯æå–ï¼ˆSciIEï¼‰çš„è§’åº¦ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ¨ç†å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°éƒ½ä¸å¦‚åŸºäºå°å‹BERTçš„æ¨¡å‹ã€‚ç§‘å­¦ä¿¡æ¯æå–æ—¢éœ€è¦æ¨ç†èƒ½åŠ›ä¹Ÿéœ€è¦è®°å¿†èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒåŸºäºSciIEï¼ŒSFTå’ŒRLVRéƒ½èƒ½ä»¥ç®€å•çš„æ–¹å¼ç²¾ç‚¼æ¨ç†è·¯å¾„å¹¶æé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼šä¸€æ˜¯æ¨¡ä»¿SFTï¼Œä½¿ç”¨ç»“æ„åŒ–æ¨ç†æ¨¡æ¿è€Œæ— éœ€é«˜è´¨é‡çš„æ€è€ƒé“¾æ•°æ®ï¼›äºŒæ˜¯R$^2$GRPOä¸ç›¸å…³æ€§åŠè§„åˆ™è¯±å¯¼å¥–åŠ±ç›¸ç»“åˆã€‚åœ¨ç§‘å­¦ä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½æé«˜æ¨ç†èƒ½åŠ›ã€‚ç»“åˆäº†æ¨¡ä»¿SFTçš„R$^2$GRPOåœ¨å…³ç³»æå–ä¸Šè¶…è¶Šäº†åŸºçº¿LLMå’Œä¸“é—¨çš„ç›‘ç£æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/ranlislz/R2GRPO%E3%80%82">https://github.com/ranlislz/R2GRPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰ç ”ç©¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒä¸‹ï¼Œä»…ä¼˜åŒ–æ¨ç†è·¯å¾„è€Œä¸æé«˜æ•°å­¦ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé€šè¿‡è’¸é¦è¿›è¡Œçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯ä»¥æ”¹è¿›è¿™ä¸€ç‚¹ã€‚ä»ç§‘å­¦ä¿¡æ¯æå–ï¼ˆSciIEï¼‰çš„è§’åº¦ç ”ç©¶ï¼ŒLLMå’Œæ¨ç†å‹LLMçš„è¡¨ç°ä¸å¦‚åŸºäºå°å‹BERTçš„æ¨¡å‹ã€‚SciIEéœ€è¦æ¨ç†å’Œè®°å¿†èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºSFTå’ŒRLVRéƒ½å¯ä»¥åŸºäºSciIEä¼˜åŒ–æ¨ç†è·¯å¾„å¹¶æé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé˜¶æ®µæ€§çš„è®­ç»ƒç­–ç•¥ï¼š1ï¼‰MimicSFTï¼Œåˆ©ç”¨ç»“æ„åŒ–æ¨ç†æ¨¡æ¿æ— éœ€é«˜è´¨é‡çš„æ€è€ƒé“¾æ•°æ®ï¼›2ï¼‰RÂ²GRPOä¸ç›¸å…³æ€§åŠè§„åˆ™è¯±å¯¼å¥–åŠ±ã€‚åœ¨ç§‘å­¦ä¿¡æ¯æå–åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½æé«˜æ¨ç†èƒ½åŠ›ã€‚ç»“åˆäº†mimicSFTçš„RÂ²GRPOåœ¨å…³ç³»æå–ä¸Šè¶…è¶Šäº†åŸºçº¿LLMå’Œä¸“é—¨çš„ç›‘ç£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨RLVRè®­ç»ƒä¸‹ä»…ä¼˜åŒ–æ¨ç†è·¯å¾„è€Œä¸æé«˜æ•°å­¦ä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SFTç»“åˆè’¸é¦å¯ä»¥æé«˜LLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä»SciIEè§’åº¦çœ‹ï¼ŒLLMå’Œæ¨ç†å‹LLMè¡¨ç°ä¸å¦‚å°å‹BERTæ¨¡å‹ã€‚</li>
<li>SciIEéœ€è¦åŒæ—¶å…·å¤‡æ¨ç†å’Œè®°å¿†èƒ½åŠ›ã€‚</li>
<li>SFTå’ŒRLVRéƒ½èƒ½åŸºäºSciIEä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚</li>
<li>æå‡ºäº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šMimicSFTå’ŒRÂ²GRPOï¼Œå‰è€…åˆ©ç”¨ç»“æ„åŒ–æ¨ç†æ¨¡æ¿ï¼Œåè€…ç»“åˆç›¸å…³æ€§å’Œè§„åˆ™è¯±å¯¼å¥–åŠ±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a19d6e4ecb94201e693a4942683f988b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53c88ca59506e2af98afdec11cd79ed.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-89671461f58ca318de287f7fc336ee70.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  3DLLM-Mem Long-Term Spatial-Temporal Memory for Embodied 3D Large   Language Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-29/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-819d5efdc144be34311fc6a378eef33a.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-29  Def-DTS Deductive Reasoning for Open-domain Dialogue Topic Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
