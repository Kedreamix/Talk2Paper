<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-05-30  Tell me Habibi, is it Real or Fake?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d4c856b39137d632c5e621835da585e0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-30-更新"><a href="#2025-05-30-更新" class="headerlink" title="2025-05-30 更新"></a>2025-05-30 更新</h1><h2 id="Tell-me-Habibi-is-it-Real-or-Fake"><a href="#Tell-me-Habibi-is-it-Real-or-Fake" class="headerlink" title="Tell me Habibi, is it Real or Fake?"></a>Tell me Habibi, is it Real or Fake?</h2><p><strong>Authors:Kartik Kuckreja, Parul Gupta, Injy Hamed, Thamar Solorio, Muhammad Haris Khan, Abhinav Dhall</strong></p>
<p>Deepfake generation methods are evolving fast, making fake media harder to detect and raising serious societal concerns. Most deepfake detection and dataset creation research focuses on monolingual content, often overlooking the challenges of multilingual and code-switched speech, where multiple languages are mixed within the same discourse. Code-switching, especially between Arabic and English, is common in the Arab world and is widely used in digital communication. This linguistic mixing poses extra challenges for deepfake detection, as it can confuse models trained mostly on monolingual data. To address this, we introduce \textbf{ArEnAV}, the first large-scale Arabic-English audio-visual deepfake dataset featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic content. It \textbf{contains 387k videos and over 765 hours of real and fake videos}. Our dataset is generated using a novel pipeline integrating four Text-To-Speech and two lip-sync models, enabling comprehensive analysis of multilingual multimodal deepfake detection. We benchmark our dataset against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and a human evaluation, highlighting its potential to advance deepfake research. The dataset can be accessed \href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/kartik060702/ArEnAV-Full%7D%7Bhere%7D">https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}</a>. </p>
<blockquote>
<p>深度伪造生成方法正在快速发展，使得虚假媒体更难检测，并引发了社会的严重关注。大多数深度伪造检测和数据集创建研究都集中在单语内容上，经常忽视多语种和代码切换语音的挑战，同一话语中混合了多种语言。在阿拉伯世界，代码切换，特别是阿拉伯语和英语之间的切换，是常见的，并广泛用于数字通信。这种语言混合给深度伪造检测带来了额外的挑战，因为它可能会混淆主要基于单语数据的模型。为解决这一问题，我们推出了ArEnAV，这是第一个大规模阿拉伯语-英语视听深度伪造数据集，具有跨语码转换、方言变化和单语阿拉伯语内容的特点。它包含了38.7万多个视频和超过765小时的真实和伪造的视频内容。我们的数据集是通过一个新型管道生成的，该管道集成了四种文本到语音和两种唇同步模型，能够全面分析多语种多媒体深度伪造检测。我们将数据集与现有的单语种和多语种数据集、最先进的深度伪造检测模型以及人类评估进行了比较，凸显了其在推动深度伪造研究方面的潜力。数据集可通过<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/kartik060702/ArEnAV-Full">https://huggingface.co/datasets/kartik060702/ArEnAV-Full</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22581v1">PDF</a> 9 pages, 2 figures, 12 tables</p>
<p><strong>Summary</strong><br>深假技术生成方法发展迅速，使得假媒体更难检测并引发社会关注。当前大部分深度伪造检测和数据集创建研究都集中在单语内容上，忽略了多语种和混合语言带来的挑战。阿拉伯和英语的混合语言在阿拉伯世界普遍存在，给深度伪造检测带来了额外的挑战。为解决这一问题，我们推出了ArEnAV数据集，这是首个包含阿拉伯语和英语音视频深度伪造数据集，包含跨句混合语言、方言差异和单语阿拉伯语内容。数据集包含38.7万视频和超过765小时的真实和伪造视频。我们整合了四个文本转语音和两个唇同步模型来生成该数据集，以全面分析多语种模态深度伪造检测。我们对数据集进行了基准测试，与现有的单语种和多语种数据集以及最先进的深度伪造检测模型进行了比较，并进行了人类评估，突出了其对深度伪造研究的潜力。数据集可在此访问：[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深假技术快速发展带来社会担忧。</li>
<li>当前深度伪造检测和数据集研究主要关注单语内容。</li>
<li>多语言和混合语言挑战深度伪造检测。</li>
<li>ArEnAV数据集是首个包含阿拉伯语和英语音视频深度伪造数据集。</li>
<li>ArEnAV包含真实和伪造视频数据超过765小时。</li>
<li>ArEnAV数据集的生成使用了创新的集成管道技术处理多种模型和语言分析需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9bd422b0b90b10eb8114746dcf2c0f88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb3326fa46cc0b857a52f0ecb1b98ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e59ad237ae1b5b9e906994b4d2768169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07e29d028e4b88a84e427158a1842e2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d23ad51e5a765d361f7b0789df9c4850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbff075bcd913a845b34f78c5e444b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c05434e5b44358dabe2f835c5c918eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-242c760d3cb2eaa60758cf1e3e29939e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e2276c2f0543201e2e4f9d3be4dbb0d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection"><a href="#Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection" class="headerlink" title="Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection"></a>Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection</h2><p><strong>Authors:Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys – the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance. All data, models, and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys">https://github.com/Berkeley-Speech-Group/LLM-Dys</a>. </p>
<blockquote>
<p>语音流畅性检测对于临床诊断和治疗语言评估至关重要，但现有方法受到高质量标注数据稀缺的限制。尽管最近文本转语音模型的进步已经能够实现合成流畅性生成，但现有的合成数据集存在韵律不自然和上下文多样性有限的缺点。为了解决这些局限性，我们提出了LLM-Dys——一个由大型语言模型增强流畅性模拟的最全面的流畅性语音语料库。该数据集涵盖了涵盖单词和音素级别的18个流畅性问题类别。基于此资源，我们改进了端到端的流畅性检测框架。实验验证显示其达到了最新技术水平。所有数据和模型都在<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E4%B8%8A%E8%BF%9B%E8%A1%8C%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dys上进行开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22029v1">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音流畅性检测在临床诊断和治疗语言评估中的重要性。然而，高质量标注数据的稀缺限制了现有方法的性能。尽管最近的TTS模型进步已经能够实现合成流畅性生成，但现有合成数据集存在语音韵律不自然和上下文多样性有限的问题。为解决这些问题，我们提出了LLM-Dys——一个由大型语言模型增强模拟流畅性的最全面的流畅性语音语料库。该数据集涵盖了词和音素两个级别的11种流畅性问题类别。基于这一资源，我们改进了一个端到端的流畅性检测框架，并通过实验验证取得了业界最佳性能。所有数据、模型和代码均已开源共享在<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dys。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音流畅性检测在临床诊断和治疗语言评估中具有重要性。</li>
<li>现有方法受限于高质量标注数据的稀缺。</li>
<li>TTS模型的最新进展能够实现合成流畅性的生成。</li>
<li>现有合成数据集存在语音韵律不自然和上下文多样性有限的问题。</li>
<li>LLM-Dys是一个全面的流畅性语音语料库，由大型语言模型增强模拟流畅性。</li>
<li>该数据集涵盖了词和音素两个级别的11种流畅性问题类别。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22029">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-940820839a1fe5abc1d5ff4dd6770dc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f09ea430ad1b7c0b1ad598e4f7cab2fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b030629238cfa23ed174c5da4e1f06be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae4225d2805104617350cc554123266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4c856b39137d632c5e621835da585e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40f8c15c1bcb1f5c3aa3c4628cc2c9b9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions"><a href="#OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions" class="headerlink" title="OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions"></a>OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions</h2><p><strong>Authors:Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem</strong></p>
<p>In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker’s multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. </p>
<blockquote>
<p>本文介绍了在线多模态对话响应生成（OMCRG）这一新任务，该任务旨在根据说话者的多模态输入，在线生成同步的言语和非言语听众反馈。OMCRG反映了自然的二元交互，并在实现生成音频和听众面部响应之间的同步方面提出了新的挑战。为了应对这些挑战，我们创新性地引入文本作为中间模态来桥接音频和面部响应。因此，我们提出了OmniResponse，这是一种多模态大型语言模型（MLLM），可以自回归地生成高质量的多模态听众响应。OmniResponse利用预训练的大型语言模型，并配备了两个新型组件：Chrono-Text，它暂时锚定生成的文本令牌；以及TempoVoice，这是一个可控的在线文本转语音（TTS）模块，能够产生与面部反应同步的语音。为了支持进一步的OMCRG研究，我们推出了ResponseNet数据集，包含696个高质量的二元交互视频，每个视频都包含同步的分割屏幕视频、多通道音频、字幕和面部行为注释。在ResponseNet上进行的综合评估表明，OmniResponse在语义语音内容、音视频同步和生成质量方面均显著优于基准模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21724v1">PDF</a> 23 pages, 9 figures</p>
<p><strong>Summary</strong><br>在线多模态对话响应生成（OMCRG）是一项旨在根据说话者的多模态输入在线生成同步的言语和非言语听众反馈的新任务。为应对挑战，引入了文本作为连接音频和面部响应的中间模态，并提出了OmniResponse多任务大型语言模型（MLLM）。该模型可自动生成高质量的多模态听众响应，并利用带有两个新组件的预训练LLM：用于临时锚定生成文本令牌的Chrono-Text和用于产生与面部反应同步语音的TempoVoice可控在线TTS模块。为了支持OMCRG研究，推出ResponseNet数据集，包含696个高质量双人互动同步分屏视频、多通道音频、字幕和面部行为注释。在ResponseNet上的综合评估表明，OmniResponse在语义语音内容、视听同步和生成质量方面显著优于基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OMCRG任务旨在在线生成同步的言语和非言语听众反馈，基于说话者的多模态输入。</li>
<li>引入文本作为连接音频和面部响应的中间模态，以应对挑战。</li>
<li>提出OmniResponse多任务大型语言模型（MLLM），可自动生成高质量的多模态听众响应。</li>
<li>OmniResponse利用带有Chrono-Text和TempoVoice两个新组件的预训练LLM。</li>
<li>ResponseNet数据集包含同步分屏视频、多通道音频、字幕和面部行为注释，支持OMCRG研究。</li>
<li>综合评估显示，OmniResponse在语义语音内容、视听同步和生成质量方面优于基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21724">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7ee89439a08a800f7f2bc627e7c2eed8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c2f950db0761e9334fc4539985ab21a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f00d4b0043b1fde1c4f569f3780c9ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e3716546e46310ae6969f8875cfb7b4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Faster-and-Better-LLMs-via-Latency-Aware-Test-Time-Scaling"><a href="#Faster-and-Better-LLMs-via-Latency-Aware-Test-Time-Scaling" class="headerlink" title="Faster and Better LLMs via Latency-Aware Test-Time Scaling"></a>Faster and Better LLMs via Latency-Aware Test-Time Scaling</h2><p><strong>Authors:Zili Wang, Tianyu Zhang, Lei Zhu, Haoli Bai, Lu Hou, Shiming Xiang, Xianzhi Yu, Wulong Liu</strong></p>
<p>Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-aware evaluation of representative TTS methods, we demonstrate that a compute-optimal TTS does not always result in the lowest latency in scenarios where latency is critical. To address this gap and achieve latency-optimal TTS, we propose two key approaches by optimizing the concurrency configurations: (1) branch-wise parallelism, which leverages multiple concurrent inference branches, and (2) sequence-wise parallelism, enabled by speculative decoding. By integrating these two approaches and allocating computational resources properly to each, our latency-optimal TTS enables a 32B model to reach 82.3% accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4% within 10 seconds. Our work emphasizes the importance of latency-aware TTS and demonstrates its ability to deliver both speed and accuracy in latency-sensitive scenarios. </p>
<blockquote>
<p>测试时缩放（TTS）已证明在推理过程中能有效提高大语言模型（LLM）的性能。然而，现有研究从延迟敏感的角度忽视了TTS的效率。通过对代表性TTS方法的延迟感知评估，我们证明计算最优的TTS并不总是导致延迟最敏感场景中延迟最低。为了弥补这一差距并实现延迟优化的TTS，我们提出了两种通过优化并发配置的关键方法：（1）分支并行性，利用多个并发推理分支；（2）通过投机解码实现的序列并行性。通过整合这两种方法并为每种方法适当分配计算资源，我们的延迟优化TTS使32B模型在MATH-500上1分钟内达到82.3%的准确率，较小的3B模型在10秒内达到72.4%的准确率。我们的工作强调了延迟感知TTS的重要性，并展示了其在延迟敏感场景中实现速度和准确性的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19634v3">PDF</a> </p>
<p><strong>摘要</strong><br>    文本主要介绍了测试时间缩放（TTS）在提高大型语言模型（LLM）推理性能方面的有效性。然而，现有研究从延迟敏感的角度忽视了TTS的效率。通过延迟感知的代表性TTS方法的评估，我们证明了计算最优的TTS并不总是导致延迟最低的场景。为了弥补这一差距并实现延迟最优的TTS，我们提出了两种通过优化并发配置的关键方法：（1）分支并行性，利用多个并发推理分支；（2）序列并行性，通过投机解码实现。通过整合这两种方法并适当分配计算资源给每个部分，我们的延迟最优TTS使一个规模为32B的模型能在MATH-500测试中达到82.3%的准确率，并且只需一分钟；而一个更小的规模为3B的模型可以在10秒内达到72.4%的准确率。我们的工作强调了延迟感知TTS的重要性，并展示了其在需要即时反应的场景中能够同时实现速度和准确度的能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>测试时间缩放（TTS）能提高大型语言模型（LLM）的推理性能。</li>
<li>现有研究忽略了从延迟敏感的角度评估TTS的效率。</li>
<li>实现延迟最优的TTS需要优化并发配置，包括分支并行性和序列并行性。</li>
<li>通过整合这两种并行性方法，我们能有效地降低模型推理的延迟。</li>
<li>在延迟敏感的场景中，优化后的TTS能在保持高准确率的同时，显著提高推理速度。</li>
<li>规模为32B的模型能在MATH-500测试中达到82.3%的准确率，并且只需一分钟完成推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e291fd4b53af90917de07d21498d1c34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235616815750083229d9ac47ab79bf1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc11049b2a067213003b77acf7e59eda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fa7f64c34abc1a76feb38474c46ef11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb2ab025abe0288742ee07fedafd092.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9a08d1815a39687e6ed039c7f59d70.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ClonEval-An-Open-Voice-Cloning-Benchmark"><a href="#ClonEval-An-Open-Voice-Cloning-Benchmark" class="headerlink" title="ClonEval: An Open Voice Cloning Benchmark"></a>ClonEval: An Open Voice Cloning Benchmark</h2><p><strong>Authors:Iwona Christop, Tomasz Kuczyński, Marek Kubis</strong></p>
<p>We present a novel benchmark for voice cloning text-to-speech models. The benchmark consists of an evaluation protocol, an open-source library for assessing the performance of voice cloning models, and an accompanying leaderboard. The paper discusses design considerations and presents a detailed description of the evaluation procedure. The usage of the software library is explained, along with the organization of results on the leaderboard. </p>
<blockquote>
<p>我们为语音克隆文本到语音模型提出一个新的基准测试。该基准测试包括评估协议、评估语音克隆模型性能的开源库以及伴随的排行榜。论文讨论了设计考虑因素，并详细介绍了评估流程。同时解释了软件库的使用以及排行榜上结果的组织情况。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20581v2">PDF</a> Under review at NeurIPS</p>
<p><strong>Summary</strong><br>     本文介绍了一个用于语音克隆文本到语音模型的新型基准测试。该基准测试包括评估协议、开源库以及排行榜，用以评估语音克隆模型的性能。文章讨论了设计考量，并详细介绍了评估流程、软件库的使用及结果展示方式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了新型的语音克隆文本到语音模型基准测试，包括评估协议和开源库。</li>
<li>该基准测试提供了评估语音克隆模型的性能的工具和排行榜。</li>
<li>文章讨论了设计该基准测试时的考量因素。</li>
<li>详细介绍了评估流程的具体步骤和方法。</li>
<li>说明了如何使用开源库进行模型性能的评估。</li>
<li>文章解释了结果展示的方式和组织形式。</li>
<li>此基准测试对于提升语音克隆模型的性能具有重要的作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f21c079d59ae89a642d17ca069316342.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4466c9a4d0a7a1f49633879933036587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f588c699e20cf3e115cf5afff3fa0feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d44c465d993c71a26ea102c782838395.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-681bbf31dfb1895bc2f4f7e49e703c96.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GOAT-TTS-Expressive-and-Realistic-Speech-Generation-via-A-Dual-Branch-LLM"><a href="#GOAT-TTS-Expressive-and-Realistic-Speech-Generation-via-A-Dual-Branch-LLM" class="headerlink" title="GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch   LLM"></a>GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch   LLM</h2><p><strong>Authors:Yaodong Song, Hongjie Chen, Jie Lian, Yuxin Zhang, Guangmin Xia, Zehan Li, Genliang Zhao, Jian Kang, Jie Li, Yongxiang Li, Xuelong Li</strong></p>
<p>While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM’s native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data. </p>
<blockquote>
<p>虽然大型语言模型（LLM）通过离散标记化范式在文本到语音（TTS）合成领域带来了革命性的变革，但当前架构在三个关键维度之间表现出基本矛盾：1）由于语音提示的量化而导致的不可逆的声学特征损失；2）严格依赖于精确对齐的语音文本对，限制了其在现实世界的部署；3）在优化语音标记生成过程中，LLM对原生文本理解能力的灾难性遗忘。为了应对这些挑战，我们提出了一种基于LLM的文本到语音生成方法，通过一种新的双分支架构（GOAT-TTS）进行优化。我们的框架引入了两个关键创新点：（1）模态对齐分支结合了语音编码器和投影仪来捕获连续的声学嵌入，实现了副语言特征（语言、音色、情感）和语义文本表示之间的双向关联，无需转录依赖；（2）语音生成分支采用模块化微调，对LLM的前k层进行语音标记预测，同时冻结底部n层以保持基础语言知识。此外，还引入了多标记预测，以支持实时流式TTS合成。实验结果表明，我们的GOAT-TTS在达到最新TTS模型性能的同时，验证了合成方言语音数据的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12339v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的文本到语音（TTS）合成方法已经取得了显著进展。然而，当前架构在三个关键方面存在基本矛盾。为了解决这些问题，我们提出了一个优化的双分支架构（GOAT-TTS）。该框架包括两个关键创新点：模态对齐分支和语音生成分支。通过模态对齐分支实现无字幕依赖的跨语言特征双向关联；通过语音生成分支对LLM进行模块化微调以实现语音令牌预测。实验结果表明，GOAT-TTS性能可与最新TTS模型相比，并验证了合成方言语音数据的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在文本到语音（TTS）合成中已有显著进展，但仍存在三大挑战。</li>
<li>现有架构在量化语音提示时会导致不可逆的声学特征损失。</li>
<li>精确的语音-文本配对限制了现实世界的部署应用。</li>
<li>在优化语音令牌生成过程中，LLM会遗忘其原有的文本理解能力。</li>
<li>GOAT-TTS框架通过模态对齐分支实现了跨语言特征的双向关联，无需字幕依赖。</li>
<li>GOAT-TTS框架通过语音生成分支对LLM进行模块化微调，以支持实时流式传输TTS合成。</li>
<li>实验结果表明GOAT-TTS性能与最新TTS模型相当，并能有效合成方言语音数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12339">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ece75d2f0eeb436e5ad914455b47bc03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f34f2c4d8edab85c0cf5c33d19d59846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe1384de69f524679eb319e8e902826e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc9b90b6d0c199f1919fb1a047f39314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7776e022a5ff5d5e18015ed415f82d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71c4f455520eb725343096bf1cba2ddb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Mono-to-Binaural-Speech-Synthesis"><a href="#Zero-Shot-Mono-to-Binaural-Speech-Synthesis" class="headerlink" title="Zero-Shot Mono-to-Binaural Speech Synthesis"></a>Zero-Shot Mono-to-Binaural Speech Synthesis</h2><p><strong>Authors:Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn, Eliya Nachmani</strong></p>
<p>We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis. </p>
<blockquote>
<p>我们提出了ZeroBAS方法，这是一种从单声道音频录制和位置信息合成双声道音频的神经方法，无需在任何双声道数据上进行训练。据我们所知，这是首次发布的从零开始学习单声道到双声道音频合成的神经方法。具体来说，我们展示了基于源位置的参数化几何时间扭曲和振幅缩放足以获得初步的双声道合成，可以通过迭代应用预训练的降噪编码器进行改进。此外，我们发现这导致了跨房间条件的泛化，我们通过引入一个新的数据集TUT Mono-to-Binaural来衡量这一点，以评估最先进的单声道到双声道合成方法在未见条件下的表现。我们的零样本方法与标准单声道到双声道数据集上的有监督方法在感知上表现相当，甚至在我们的离群TUT Mono-to-Binaural数据集上超过了它们。我们的结果突显了预训练的生成音频模型和零样本学习在解锁稳健的双声道音频合成方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08356v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>无需训练即可合成双耳音频的新神经网络方法ZeroBAS。基于零数据学习和现有生成音频模型技术，将单声道音频转换为双耳音频，仅使用位置信息。通过几何时间扭曲和基于源位置的振幅缩放实现初步的双耳合成，再迭代应用预训练的降噪编解码器进行完善。在未见条件下的新数据集TUT Mono-to-Binaural上测试，零样本方法性能与标准单声道到双耳音频合成数据集上的监督方法相当，甚至超出其表现。突显了预训练生成音频模型和零样本学习的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无需训练即可合成双耳音频的新神经网络方法ZeroBAS被提出。</li>
<li>基于零数据学习和现有生成音频模型技术实现单声道到双耳音频的转换。</li>
<li>通过几何时间扭曲和基于源位置的振幅缩放实现初步的双耳合成。</li>
<li>使用预训练的降噪编解码器对初步合成结果进行完善。</li>
<li>在未见条件下的新数据集TUT Mono-to-Binaural上进行测试，验证了方法的泛化能力。</li>
<li>零样本方法在标准数据集上的性能与监督方法相当，甚至在未见条件下超出其表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4148be2f2fcbb735e45655624763a501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e424c252c3689826fd886f28bbee6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a11daf01d2014a55ab9323e1c582579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c95350f658f15e67b5a0b6c51212ece.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VQ-CTAP-Cross-Modal-Fine-Grained-Sequence-Representation-Learning-for-Speech-Processing"><a href="#VQ-CTAP-Cross-Modal-Fine-Grained-Sequence-Representation-Learning-for-Speech-Processing" class="headerlink" title="VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for   Speech Processing"></a>VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for   Speech Processing</h2><p><strong>Authors:Chunyu Qiang, Wang Geng, Yi Zhao, Ruibo Fu, Tao Wang, Cheng Gong, Tianrui Wang, Qiuyu Liu, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Hao Che, Longbiao Wang, Jianwu Dang, Jianhua Tao</strong></p>
<p>Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the speech modality. We propose a method called “Vector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)”, which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, learning how to connect text and speech at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and recognition tasks in speech processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression speech coding at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. The audio demo is available at <a target="_blank" rel="noopener" href="https://qiangchunyu.github.io/VQCTAP/">https://qiangchunyu.github.io/VQCTAP/</a> </p>
<blockquote>
<p>深度学习为跨模态表示学习领域带来了重大改进。对于文本转语音（TTS）、语音转换（VC）和自动语音识别（ASR）等任务，我们期望得到一种跨模态精细（帧级）序列表示，强调文本模态的语义内容，同时淡化语音模态的副语言信息。我们提出了一种名为“向量量化对比令牌声学预训练（VQ-CTAP）”的方法，该方法使用跨模态对齐序列转码器将文本和语音带入联合多模态空间，学习如何在帧级别连接文本和语音。提出的VQ-CTAP是跨模态序列表示学习的一种范式，为语音处理中的精细粒度生成和识别任务提供了有前景的解决方案。VQ-CTAP可直接应用于VC和ASR任务，无需微调或额外结构。我们提出了一种序列感知语义连接器，它将多个冻结的预训练模块连接到TTS任务中，展现出即插即用的能力。我们设计了一种步进优化策略，通过逐步注入和调整各种损失分量的影响，以确保模型的有效收敛。此外，我们提出了一种语义转移副语言一致性损失，以增强表示能力，使模型能够更好地推广到未见数据并捕捉副语言信息的细微差别。另外，VQ-CTAP实现了高压缩语音编码，以25Hz的速率从24kHz输入波形进行编码，采样率降低了960倍。音频演示可在<a target="_blank" rel="noopener" href="https://qiangchunyu.github.io/VQCTAP/%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://qiangchunyu.github.io/VQCTAP/进行查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05758v2">PDF</a> </p>
<p><strong>摘要</strong><br>    深度学习极大提升了跨模态表征学习领域的能力，特别是在文本转语音（TTS）、语音转换（VC）和自动语音识别（ASR）等任务中。本文提出一种名为“向量量化对比令牌声学预训练（VQ-CTAP）”的方法，使用跨模态对齐序列转码器将文本和语音引入联合多模态空间，在帧级别连接文本和语音。此方法为跨模态序列表征学习提供了有力解决方案，适用于精细粒度的生成和识别任务。VQ-CTAP可直接应用于VC和ASR任务，无需微调或额外结构。设计序列感知语义连接器，为TTS任务连接多个冻结的预训练模块，展现即插即用能力。通过逐步优化策略确保模型有效收敛，同时提出语义转移旁语一致性损失以增强表征能力，使模型更好地泛化到未见数据和捕捉旁语信息的细微差别。此外，VQ-CTAP实现高压缩语音编码，以25Hz的采样率从24kHz输入波形，达到960倍的采样率降低。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>深度学习在跨模态表征学习领域取得显著进步，尤其在TTS、VC和ASR任务中。</li>
<li>提出VQ-CTAP方法，使用跨模态对齐序列转码器在帧级别连接文本和语音。</li>
<li>VQ-CTAP是一种跨模态序列表征学习的范式，适用于精细粒度的生成和识别任务。</li>
<li>VQ-CTAP可直接应用于VC和ASR任务，无需额外调整或结构。</li>
<li>设计序列感知语义连接器，展现对TTS任务的即插即用能力。</li>
<li>通过逐步优化策略和语义转移旁语一致性损失提高模型效能和泛化能力。</li>
<li>VQ-CTAP实现高压缩语音编码，大幅降低采样率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05758">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b6370feb9110bfbbb3dfbeed3b9dabb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-deac6e1246f33a95a531e19b52ca2cf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba9ee7dcdedad1c0935aa97b057f44fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c4f1e3f74ca5f390c49bfa4ef9f5a7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6fce304b8432d8b73b31bde4d695f4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7e3716546e46310ae6969f8875cfb7b4.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-05-30  OmniResponse Online Multimodal Conversational Response Generation in   Dyadic Interactions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-384e35f008881080ac4f07969a28155e.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-30  Chest Disease Detection In X-Ray Images Using Deep Learning   Classification Method
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
