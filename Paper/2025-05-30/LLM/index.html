<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  3DLLM-Mem Long-Term Spatial-Temporal Memory for Embodied 3D Large   Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-89671461f58ca318de287f7fc336ee70.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="3DLLM-Mem-Long-Term-Spatial-Temporal-Memory-for-Embodied-3D-Large-Language-Model"><a href="#3DLLM-Mem-Long-Term-Spatial-Temporal-Memory-for-Embodied-3D-Large-Language-Model" class="headerlink" title="3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large   Language Model"></a>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large   Language Model</h2><p><strong>Authors:Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang</strong></p>
<p>Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agentâ€™s ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Benchâ€™s most challenging in-the-wild embodied tasks. </p>
<blockquote>
<p>äººç±»åœ¨åˆ©ç”¨é•¿æœŸè®°å¿†è¿›è¡Œè·¨æ—¶é—´å’Œç©ºé—´çš„ç»éªŒæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä»è€Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ¨æ€çš„å¤šæˆ¿é—´ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆè§„åˆ’å’Œè¡ŒåŠ¨æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§é™åˆ¶çš„ä¸€éƒ¨åˆ†æ˜¯ç”±äºLLMä¸­ç¼ºä¹é€‚å½“çš„ä¸‰ç»´æ—¶ç©ºè®°å¿†å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†3DMem-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡26,000ä¸ªè½¨è¿¹å’Œ2,892ä¸ªä»»åŠ¡çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é—®ç­”å’Œæè¿°ç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨ä¸‰ç»´ç¯å¢ƒä¸­åˆ©ç”¨é•¿æœŸè®°å¿†è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†3DLLM-Memï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŠ¨æ€å†…å­˜ç®¡ç†å’Œèåˆæ¨¡å‹ï¼Œç”¨äºLLMä¸­çš„å®ä½“æ—¶ç©ºæ¨ç†å’Œè¡ŒåŠ¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨å·¥ä½œè®°å¿†ä»¤ç‰Œæ¥è¡¨ç¤ºå½“å‰è§‚å¯Ÿï¼Œä½œä¸ºæŸ¥è¯¢æ¥æœ‰é€‰æ‹©åœ°å…³æ³¨å¹¶èåˆæ¥è‡ªæƒ…æ™¯è®°å¿†ä¸­çš„æœ€æœ‰ç”¨çš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæƒ…æ™¯è®°å¿†å­˜å‚¨è¿‡å»çš„è§‚å¯Ÿå’Œäº¤äº’ã€‚è¿™ç§æ–¹æ³•å…è®¸ä»£ç†ä¸“æ³¨äºä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨å¤æ‚çš„é•¿è¿œç¯å¢ƒä¸­ä¿æŒè®°å¿†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3DLLM-Memåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨3DMem-Benchæœ€å…·æŒ‘æˆ˜æ€§çš„å®é™…ä»»åŠ¡ä¸­ï¼ŒæˆåŠŸç‡æé«˜äº†16.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22657v1">PDF</a> demos at: <a target="_blank" rel="noopener" href="https://3dllm-mem.github.io/">https://3dllm-mem.github.io</a></p>
<p><strong>æ‘˜è¦</strong><br>äººç±»æ“…é•¿é€šè¿‡åˆ©ç”¨è·¨æ—¶é—´å’Œç©ºé—´çš„ç»éªŒä¸­çš„é•¿æœŸè®°å¿†æ¥æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŠ¨æ€ã€å¤šæˆ¿é—´çš„3Dç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’å’Œè¡ŒåŠ¨æ—¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ä¸€é™åˆ¶éƒ¨åˆ†æ˜¯ç”±äºLLMsä¸­é€‚å½“çš„3Dæ—¶ç©ºè®°å¿†å»ºæ¨¡çš„ç¼ºä¹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæ¨å‡ºäº†3DMem-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡26ï¼Œ000ä¸ªè½¨è¿¹å’Œ2ï¼Œ892ä¸ªä½“ç°ä»»åŠ¡ï¼ˆé—®ç­”å’Œæè¿°ï¼‰çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°ä»£ç†åœ¨3Dç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸæ¨ç†çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†3DLLM-Memï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºLLMsä¸­çš„åŠ¨æ€ç©ºé—´æ—¶é—´æ¨ç†å’Œè¡ŒåŠ¨çš„æ–°å‹åŠ¨æ€å†…å­˜ç®¡ç†å’Œèåˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨å·¥ä½œè®°å¿†ç¬¦å·ä½œä¸ºæŸ¥è¯¢ï¼Œæœ‰é€‰æ‹©åœ°å…³æ³¨å¹¶èåˆæ¥è‡ªæƒ…æ™¯è®°å¿†ä¸­çš„æœ€æœ‰ç”¨çš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼ˆå­˜å‚¨è¿‡å»çš„è§‚å¯Ÿå’Œäº’åŠ¨ï¼‰ã€‚è¿™ç§æ–¹æ³•å…è®¸ä»£ç†ä¸“æ³¨äºä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨å¤æ‚ã€é•¿æœŸç¯å¢ƒä¸­ä¿æŒè®°å¿†æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸Šï¼Œç›¸å¯¹äºæœ€å¼ºåŸºçº¿æ¨¡å‹ï¼Œ3DLLM-Memåœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸ŠæˆåŠŸç‡æé«˜äº†16.5%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„åŠ¨æ€ã€å¤šæˆ¿é—´ç¯å¢ƒä¸­è§„åˆ’å’Œè¡ŒåŠ¨èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>éƒ¨åˆ†åŸå› æ˜¯LLMsç¼ºä¹é€‚å½“çš„3Dæ—¶ç©ºè®°å¿†å»ºæ¨¡ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·ï¼š3DMem-Benchï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸæ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€å†…å­˜ç®¡ç†å’Œèåˆæ¨¡å‹ï¼š3DLLM-Memï¼Œç”¨äºLLMsä¸­çš„ç©ºé—´æ—¶é—´æ¨ç†å’Œè¡ŒåŠ¨ã€‚è¯¥æ¨¡å‹ä½¿ç”¨å·¥ä½œè®°å¿†ç¬¦å·ä½œä¸ºæŸ¥è¯¢æ¥å…³æ³¨ä»»åŠ¡ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿèåˆæƒ…æ™¯è®°å¿†ä¸­çš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œæé«˜ä»£ç†åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a84b3620e836198b12bd80b7af411a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dac7d04054be49688146803075d51e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401370dd4c61ddd8bd360b94cf28e2b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e16e8132355183ebc1c9c411e68d0d7f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Position-Uncertainty-Quantification-Needs-Reassessment-for-Large-language-Model-Agents"><a href="#Position-Uncertainty-Quantification-Needs-Reassessment-for-Large-language-Model-Agents" class="headerlink" title="Position: Uncertainty Quantification Needs Reassessment for   Large-language Model Agents"></a>Position: Uncertainty Quantification Needs Reassessment for   Large-language Model Agents</h2><p><strong>Authors:Michael Kirchhof, Gjergji Kasneci, Enkelejda Kasneci</strong></p>
<p>Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒèŠå¤©æœºå™¨äººä»£ç†æœ‰æ—¶ä¼šæä¾›é”™è¯¯çš„è¾“å‡ºï¼Œæœ€è¿‘çš„ç ”ç©¶å‘ç°è¿™ä¸å¯èƒ½å®Œå…¨é¿å…ã€‚å› æ­¤ï¼Œä¸ç¡®å®šæ€§é‡åŒ–æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œæ—¨åœ¨é‡åŒ–æ€»æ•°é‡æˆ–ä¸€ä¸ªæ•°å­—ä¸­å¶ç„¶æ€§ä¸ç¡®å®šæ€§å’ŒçŸ¥è¯†çš„ä¸ç¡®å®šæ€§è¿™ä¸¤ä¸ªä¸ç¡®å®šæ€§æ°´å¹³ã€‚æœ¬æ–‡ä¸»å¼ å¯¹äºåœ¨ç”¨æˆ·ä¸ä»£ç†é€šä¿¡çš„å¼€æ”¾å’Œäº¤äº’å¼è®¾ç½®å†…è¿è¡Œçš„LLMä»£ç†æ¥è¯´ï¼Œè¿™ç§ä¸ç¡®å®šæ€§çš„ä¼ ç»ŸäºŒåˆ†æ³•è¿‡äºæœ‰é™ã€‚æˆ‘ä»¬éœ€è¦ç ”ç©¶èƒ½å¤Ÿåœ¨è¿™ä¸€æ–°å‹åœºæ™¯ä¸­æ·»åŠ ä¸°å¯Œä¸ç¡®å®šæ€§çš„é€”å¾„ã€‚æˆ‘ä»¬å›é¡¾æ–‡çŒ®å‘ç°ï¼Œæµè¡Œçš„å¶ç„¶æ€§ä¸ç¡®å®šæ€§å’ŒçŸ¥è¯†ä¸ç¡®å®šæ€§çš„å®šä¹‰ç›´æ¥ç›¸äº’çŸ›ç›¾ï¼Œåœ¨äº¤äº’å¼LLMä»£ç†è®¾ç½®ä¸­å¤±å»å…¶æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³äºäººç±»è®¡ç®—æœºäº¤äº’ä¸­ä¸ç¡®å®šæ€§ç ”ç©¶çš„æ–°æ–¹å‘ï¼šå½“ç”¨æˆ·æ²¡æœ‰æä¾›æ‰€æœ‰ä¿¡æ¯æˆ–ç¬¬ä¸€æ¬¡æœªèƒ½å®šä¹‰ç¡®åˆ‡ä»»åŠ¡æ—¶çš„è§„èŒƒä¸è¶³çš„ä¸ç¡®å®šæ€§ã€é€šè¿‡æå‡ºåç»­é—®é¢˜å‡å°‘å½“å‰ä¸Šä¸‹æ–‡ä¸ç¡®å®šæ€§çš„äº¤äº’å­¦ä¹ ã€ä»¥åŠåˆ©ç”¨ä¸°å¯Œçš„è¯­è¨€å’Œè¯­éŸ³ç©ºé—´æ¥è¡¨è¾¾ä¸ä»…ä»…æ˜¯æ•°å­—çš„ä¸ç¡®å®šæ€§è¾“å‡ºä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬é¢„è®¡è¿™äº›æ–°çš„å¤„ç†ä¸ç¡®å®šæ€§å’Œäº¤æµä¸ç¡®å®šæ€§æ–¹å¼å°†å¸¦æ¥æ›´åŠ é€æ˜ã€å¯é å’Œç›´è§‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†äº¤äº’ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22655v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒèŠå¤©æœºå™¨äººä»£ç†äººæœ‰æ—¶ä¼šæä¾›é”™è¯¯çš„è¾“å‡ºï¼Œä¸”è¿™ç§æƒ…å†µæ— æ³•å®Œå…¨é¿å…ã€‚å› æ­¤ï¼Œä¸ç¡®å®šæ€§é‡åŒ–èµ·ç€å…³é”®ä½œç”¨ï¼Œæ—¨åœ¨é‡åŒ–æ€»ä½“æ•°å­—æˆ–ä¸¤ä¸ªæ•°å­—çš„ä¸ç¡®å®šæ€§ã€‚ä¼ ç»Ÿçš„ä¸ç¡®å®šæ€§çš„äºŒåˆ†æ³•å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ç”¨æˆ·äº¤äº’æ—¶çš„å¼€æ”¾å’Œäº’åŠ¨è®¾ç½®å¤ªè¿‡å±€é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸‰ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘ï¼Œä¸“æ³¨äºäººç±»è®¡ç®—æœºäº¤äº’ä¸­çš„ä¸ç¡®å®šæ€§ï¼šæœªæŒ‡å®šä¸ç¡®å®šæ€§ã€äº¤äº’å­¦ä¹ å’Œè¾“å‡ºä¸ç¡®å®šæ€§ã€‚æœŸæœ›è¿™äº›æ–°çš„å¤„ç†ä¸ç¡®å®šæ€§å’Œäº¤æµä¸ç¡®å®šæ€§æ–¹å¼èƒ½ä½¿LLMä»£ç†äº¤äº’æ›´åŠ é€æ˜ã€å¯ä¿¡å’Œç›´è§‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒèŠå¤©æœºå™¨äººæœ‰æ—¶æ— æ³•é¿å…æä¾›é”™è¯¯è¾“å‡ºã€‚</li>
<li>ä¸ç¡®å®šæ€§é‡åŒ–åœ¨LLMä¸­è‡³å…³é‡è¦ï¼Œéœ€è¦é‡åŒ–æ€»ä½“æ•°å­—çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>ä¼ ç»Ÿçš„ä¸ç¡®å®šæ€§äºŒåˆ†æ³•åœ¨LLMä¸ç”¨æˆ·äº¤äº’æ—¶æ˜¾å¾—è¿‡äºå±€é™ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§æ–°çš„ç ”ç©¶æ–¹å‘æ¥å¤„ç†ä¸ç¡®å®šæ€§ï¼šæœªæŒ‡å®šä¸ç¡®å®šæ€§ã€äº¤äº’å­¦ä¹ å’Œè¾“å‡ºä¸ç¡®å®šæ€§ã€‚</li>
<li>æœªæŒ‡å®šä¸ç¡®å®šæ€§æŒ‡ç”¨æˆ·æœªæä¾›æ‰€æœ‰ä¿¡æ¯æˆ–æœªæ˜ç¡®å®šä¹‰ä»»åŠ¡æ—¶çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>äº¤äº’å­¦ä¹ é€šè¿‡æå‡ºåç»­é—®é¢˜æ¥å‡å°‘å½“å‰ä¸Šä¸‹æ–‡çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7266bd01812f1fcbf5103ebd858a6790.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cd1cbb1418e22ca035c93e7a6fbea02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bca35510bc394b19ec3858d33979d00e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4007323d2cd7996885b5bfe9edf29ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96aba5bf41e8a4cceece2931a4a09452.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="The-Climb-Carves-Wisdom-Deeper-Than-the-Summit-On-the-Noisy-Rewards-in-Learning-to-Reason"><a href="#The-Climb-Carves-Wisdom-Deeper-Than-the-Summit-On-the-Noisy-Rewards-in-Learning-to-Reason" class="headerlink" title="The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in   Learning to Reason"></a>The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in   Learning to Reason</h2><p><strong>Authors:Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan</strong></p>
<p>Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward functionâ€™s outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as &#96;&#96;first, I need toâ€™â€™-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLMâ€™s performance on open-ended tasks. These findings suggest the importance of improving modelsâ€™ foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at <a target="_blank" rel="noopener" href="https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason">https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason</a>. </p>
<blockquote>
<p>è¿‘æœŸå…³äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè®­ç»ƒåç”¨äºæ¨ç†çš„ç ”ç©¶é€šå¸¸èšç„¦äºå¯ä»¥å‡†ç¡®éªŒè¯å’Œå¥–åŠ±çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è§£å†³æ•°å­¦é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†å¥–åŠ±å™ªå£°çš„å½±å“ï¼Œè¿™æ˜¯ä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹LLMè¿›è¡Œåè®­ç»ƒæ¶‰åŠç°å®ä¸–ç•Œåœºæ™¯æ—¶æ›´å®é™…çš„è€ƒè™‘ã€‚æˆ‘ä»¬å‘ç°LLMå¯¹å¤§é‡çš„å¥–åŠ±å™ªå£°è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚ä¾‹å¦‚ï¼Œæ‰‹åŠ¨ç¿»è½¬æ•°å­¦ä»»åŠ¡ä¸­å¥–åŠ±åŠŸèƒ½è¾“å‡ºçš„40%ï¼Œä»ç„¶å…è®¸Qwen-2.5-7Bæ¨¡å‹å¿«é€Ÿæ”¶æ•›ï¼Œå…¶æ•°å­¦ä»»åŠ¡æ€§èƒ½ä»5%æé«˜åˆ°72%ï¼Œè€Œä½¿ç”¨æ— å™ªå£°å¥–åŠ±è®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡ä¸º75%ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»…é€šè¿‡å¥–åŠ±å…³é”®æ¨ç†çŸ­è¯­çš„å‡ºç°ï¼ˆå³æ¨ç†æ¨¡å¼å¥–åŠ±ï¼ŒRPRï¼‰ï¼Œå¦‚â€œé¦–å…ˆï¼Œæˆ‘éœ€è¦â€â€”â€”è€Œä¸éªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œè¯¥æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½è¾¾åˆ°å³°å€¼ï¼ˆå¯¹äºQwen-2.5-7Bæ¨¡å‹å‡†ç¡®ç‡è¶…è¿‡70%ï¼‰ï¼Œä¸ç»è¿‡ä¸¥æ ¼æ­£ç¡®æ€§éªŒè¯å’Œå‡†ç¡®å¥–åŠ±è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ã€‚æˆ‘ä»¬è®¤è¯†åˆ°æ¨ç†è¿‡ç¨‹æ¯”æœ€ç»ˆç»“æœæ›´é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬å°†RPRä¸å™ªå£°å¥–åŠ±æ¨¡å‹ç›¸ç»“åˆã€‚RPRæœ‰åŠ©äºæ ¡å‡†å™ªå£°å¥–åŠ±æ¨¡å‹ï¼Œå‡å°‘æ½œåœ¨çš„å‡é˜´æ€§ï¼Œå¹¶å¢å¼ºLLMåœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜åœ¨é¢„è®­ç»ƒé˜¶æ®µæé«˜æ¨¡å‹çš„åŸºæœ¬èƒ½åŠ›çš„é‡è¦æ€§ï¼ŒåŒæ—¶ä¸ºæ”¹è¿›åè®­ç»ƒæŠ€æœ¯æä¾›äº†è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å’Œè„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reasonä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22653v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­è¿›è¡Œæ¨ç†è®­ç»ƒæ—¶çš„å¥–åŠ±å™ªå£°é—®é¢˜ã€‚ç ”ç©¶å‘ç°åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œå¥–åŠ±æ¨¡å‹ä¸­çš„å™ªå£°å¹¶ä¸ä¼šä¸¥é‡å½±å“LLMçš„ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹éƒ¨åˆ†å¥–åŠ±å‡½æ•°è¾“å‡ºçš„æ‰‹åŠ¨è°ƒæ•´ä»¥åŠå¢åŠ å…³é”®æ¨ç†è¯ç»„çš„å¥–åŠ±æœºåˆ¶ï¼Œå³ä½¿åœ¨æ²¡æœ‰å‡†ç¡®éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§çš„æƒ…å†µä¸‹ï¼ŒLLMä»èƒ½è·å¾—è¾ƒå¥½çš„æ€§èƒ½æå‡ã€‚è¿™äº›å‘ç°å¯¹äºæ”¹è¿›æ¨¡å‹çš„åŸºç¡€èƒ½åŠ›å’Œæ¨è¿›è®­ç»ƒæŠ€æœ¯å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­è¿›è¡Œæ¨ç†è®­ç»ƒæ—¶çš„å¥–åŠ±å™ªå£°é—®é¢˜ã€‚</li>
<li>LLMå¯¹å¥–åŠ±å™ªå£°è¡¨ç°å‡ºå¼ºçƒˆçš„ç¨³å¥æ€§ï¼Œå³ä½¿å¥–åŠ±å‡½æ•°è¾“å‡ºè¢«æ‰‹åŠ¨è°ƒæ•´40%ï¼Œæ¨¡å‹æ€§èƒ½ä»èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>é€šè¿‡ä»…å¥–åŠ±å…³é”®æ¨ç†è¯ç»„ï¼ˆå¦‚â€œé¦–å…ˆï¼Œæˆ‘éœ€è¦â€ï¼‰ï¼Œå³ä½¿ä¸éªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼Œä¹Ÿèƒ½è¾¾åˆ°å³°å€¼æ€§èƒ½ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹ä¸­çš„å™ªå£°å¯ä»¥é€šè¿‡å…³é”®æ¨ç†è¯ç»„å¥–åŠ±ï¼ˆRPRï¼‰è¿›è¡Œæ ¡å‡†ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>RPRæœ‰åŠ©äºå‡è½»æ½œåœ¨é”™è¯¯è´Ÿé¢æ•ˆåº”ï¼Œå¢å¼ºLLMåœ¨å¼€æ”¾å¼ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†æé«˜æ¨¡å‹åŸºç¡€èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é¢„è®­ç»ƒé˜¶æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16bcf8b564b64385335468dfa1d50af5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c46b0d65b4e54c65bc92cb4df2af7af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0238020e3779feba698a7924322fc023.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa53a4a3018cc97c89f6cf6c99ed7c37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a69874cbf294342a5eb20545791ead6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Characterizing-Bias-Benchmarking-Large-Language-Models-in-Simplified-versus-Traditional-Chinese"><a href="#Characterizing-Bias-Benchmarking-Large-Language-Models-in-Simplified-versus-Traditional-Chinese" class="headerlink" title="Characterizing Bias: Benchmarking Large Language Models in Simplified   versus Traditional Chinese"></a>Characterizing Bias: Benchmarking Large Language Models in Simplified   versus Traditional Chinese</h2><p><strong>Authors:Hanjia Lyu, Jiebo Luo, Jian Kang, Allison Koenecke</strong></p>
<p>While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models â€“ spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (<a target="_blank" rel="noopener" href="https://github.com/brucelyu17/SC-TC-Bench">https://github.com/brucelyu17/SC-TC-Bench</a>). </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®€ä½“å’Œç¹ä½“ä¸­æ–‡ä¸‹çš„èƒ½åŠ›å·²ç»å¾—åˆ°äº†ç ”ç©¶ï¼Œä½†åœ¨è¿™ä¸¤ç§ä¹¦é¢ä¸­æ–‡å½¢å¼ä¸‹ï¼ŒLLMæ˜¯å¦è¡¨ç°å‡ºä¸åŒçš„æ€§èƒ½ä»ç„¶ä¸æ¸…æ¥šã€‚è¿™ç§ç†è§£è‡³å…³é‡è¦ï¼Œå› ä¸ºLLMå›åº”çš„è´¨é‡å·®å¼‚å¯èƒ½ä¼šå¿½è§†ç®€ä½“å’Œç¹ä½“ä¸­æ–‡èƒŒåçš„ä¸åŒæ–‡åŒ–èƒŒæ™¯ï¼Œä»è€Œå»¶ç»­ä»£è¡¨æ€§ä¼¤å®³ï¼Œå¹¶å¯èƒ½åŠ å‰§æ•™è‚²æˆ–æ‹›è˜ç­‰é¢†åŸŸç”±LLMè¾…åŠ©å†³ç­–äº§ç”Ÿçš„ä¸‹æ¸¸å±å®³ã€‚ä¸ºäº†è°ƒæŸ¥LLMæ½œåœ¨çš„ç»©æ•ˆå·®å¼‚ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªåæ˜ çœŸå®ä¸–ç•Œæƒ…å†µçš„åŸºå‡†æµ‹è¯•ä»»åŠ¡ï¼šåŒºåŸŸæœ¯è¯­é€‰æ‹©ï¼ˆæç¤ºLLMå‘½åä¸€ä¸ªåœ¨ä¸åŒä¸­å›½å¤§é™†å’Œå°æ¹¾åœ°åŒºæœ‰ä¸åŒçš„ç§°å‘¼çš„ç‰©å“ï¼‰ï¼Œä»¥åŠåŒºåŸŸåç§°é€‰æ‹©ï¼ˆæç¤ºLLMä»åŒ…å«ç®€ä½“å’Œç¹ä½“ä¸­æ–‡çš„åå•ä¸­é€‰æ‹©é›‡ä½£è°ï¼‰ã€‚å¯¹äºè¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬å®¡æ ¸äº†11ä¸ªé¢†å…ˆçš„å•†ä¸šLLMæœåŠ¡å’Œå¼€æºæ¨¡å‹çš„è¡¨ç°â€”â€”è¿™äº›æ¨¡å‹ä¸»è¦æ¥å—è‹±è¯­ã€ç®€ä½“ä¸­æ–‡æˆ–ç¹ä½“ä¸­æ–‡çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMçš„å›åº”åè§å–å†³äºä»»åŠ¡å’Œæç¤ºè¯­è¨€ï¼šè™½ç„¶åœ¨åŒºåŸŸæœ¯è¯­é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œå¤§å¤šæ•°LLMåçˆ±ç®€ä½“ä¸­æ–‡å›åº”ï¼Œä½†åœ¨åŒºåŸŸåç§°é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬å´æ„å¤–åœ°åçˆ±ç¹ä½“ä¸­æ–‡åç§°ã€‚æˆ‘ä»¬å‘ç°è¿™äº›å·®å¼‚å¯èƒ½æºäºè®­ç»ƒæ•°æ®è¡¨ç¤ºã€ä¹¦é¢å­—ç¬¦åå¥½ä»¥åŠç®€ä½“å’Œç¹ä½“ä¸­æ–‡çš„æ ‡è®°åŒ–æ–¹é¢çš„å·®å¼‚ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è¿›ä¸€æ­¥åˆ†æLLMåè§çš„éœ€æ±‚ï¼›å› æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¯¹æœªæ¥LLMåœ¨ä¸­æ–‡è¯­è¨€å˜ä½“è¡Œä¸ºè¯„ä¼°çš„é‡å¤æ€§è¯„ä¼°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/brucelyu17/SC-TC-Bench%EF%BC%89%E3%80%82">https://github.com/brucelyu17/SC-TC-Benchï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22645v1">PDF</a> To appear in the 2025 ACM Conference on Fairness, Accountability, and   Transparency (FAccT â€˜25)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®€ä½“å’Œç¹ä½“ä¸­æ–‡ä¸‹çš„æ€§èƒ½å·®å¼‚ã€‚é€šè¿‡è®¾è®¡åæ˜ çœŸå®åœºæ™¯çš„ä¸¤ä¸ªåŸºå‡†ä»»åŠ¡ï¼Œå‘ç°LLMçš„å›åº”åè§å–å†³äºä»»åŠ¡å’Œæç¤ºè¯­è¨€ã€‚åœ¨åŒºåŸŸæœ¯è¯­é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œå¤§å¤šæ•°LLMå€¾å‘äºä½¿ç”¨ç®€ä½“ä¸­æ–‡å›åº”ï¼›è€Œåœ¨åŒºåŸŸåç§°é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œå´åå‘äºä½¿ç”¨ç¹ä½“ä¸­æ–‡åç§°ã€‚è¿™äº›å·®å¼‚å¯èƒ½æºäºç®€ä½“å’Œç¹ä½“ä¸­æ–‡åœ¨è®­ç»ƒæ•°æ®è¡¨ç¤ºã€ä¹¦é¢å­—ç¬¦åå¥½å’Œåˆ†è¯æ–¹é¢çš„å·®å¼‚ã€‚æœ¬æ–‡å¼ºè°ƒäº†å¯¹LLMåè§è¿›è¡Œè¿›ä¸€æ­¥åˆ†æçš„éœ€è¦ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†æ¥ä¿ƒè¿›å¯¹æœªæ¥LLMåœ¨ä¸­æ–‡è¯­è¨€å˜ä½“æ–¹é¢çš„è¡Œä¸ºè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç®€ä½“å’Œç¹ä½“ä¸­æ–‡ä¸‹çš„æ€§èƒ½å·®å¼‚ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯­è¨€å·®å¼‚å¯èƒ½å¯¼è‡´LLMå›åº”çš„åè§ã€‚</li>
<li>é€šè¿‡è®¾è®¡åæ˜ çœŸå®åœºæ™¯çš„åŒºåŸŸæœ¯è¯­å’ŒåŒºåŸŸåç§°é€‰æ‹©ä»»åŠ¡ï¼Œå‘ç°LLMå›åº”åè§ä¸ä»»åŠ¡å’Œæç¤ºè¯­è¨€æœ‰å…³ã€‚</li>
<li>åœ¨åŒºåŸŸæœ¯è¯­é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œå¤§å¤šæ•°LLMå€¾å‘äºä½¿ç”¨ç®€ä½“ä¸­æ–‡å›åº”ï¼›è€Œåœ¨åŒºåŸŸåç§°é€‰æ‹©ä»»åŠ¡ä¸­ï¼Œåå‘äºä½¿ç”¨ç¹ä½“ä¸­æ–‡åç§°ã€‚</li>
<li>LLMçš„å›åº”åè§å¯èƒ½æºäºè®­ç»ƒæ•°æ®è¡¨ç¤ºã€ä¹¦é¢å­—ç¬¦åå¥½å’Œåˆ†è¯æ–¹é¢çš„å·®å¼‚ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥åˆ†æLLMçš„åè§ï¼Œä»¥ç¡®ä¿å…¶åœ¨ä¸åŒè¯­è¨€èƒŒæ™¯ä¸‹çš„å…¬å¹³æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå¼€æºåŸºå‡†æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¯¹æœªæ¥LLMåœ¨ä¸­æ–‡è¯­è¨€å˜ä½“æ–¹é¢çš„è¡Œä¸ºè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc34fab51a3824a16508562cc1c2eaf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7dfa4442cfce5528cc116fb49e740d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddded45b363e020bac6c7b16c5608a59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c293256fa908a0c07b0d7416f651432.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-Composable-Chains-of-Thought"><a href="#Learning-Composable-Chains-of-Thought" class="headerlink" title="Learning Composable Chains-of-Thought"></a>Learning Composable Chains-of-Thought</h2><p><strong>Authors:Fangcong Yin, Zeyu Leo Liu, Liu Leqi, Xi Ye, Greg Durrett</strong></p>
<p>A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train â€œatomic CoTâ€ models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget. </p>
<blockquote>
<p>ç›®å‰å¸¸è§çš„æ•™æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•æ˜¯é€šè¿‡å¯¹å†…éƒ¨åˆ†å¸ƒå¼æ¨ç†é—®é¢˜çš„æ€è€ƒé“¾ï¼ˆCoTï¼‰è½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä½†å¯¹äºæ¯ä¸ªæ„Ÿå…´è¶£çš„é—®é¢˜æ¥è¯´ï¼Œè·å–æ­¤ç±»æ³¨é‡Šæ•°æ®æˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬å¸Œæœ›æ¨ç†æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šå…¶è®­ç»ƒåˆ†å¸ƒè¿›è¡Œæ¨å¹¿ï¼Œå¹¶ä¸”ç†æƒ³æƒ…å†µä¸‹èƒ½å¤Ÿç»„åˆæ¨å¹¿ï¼šç»“åˆåŸå­æ¨ç†æŠ€èƒ½æ¥è§£å†³æ›´å›°éš¾ã€æœªè§è¿‡çš„æ¨ç†ä»»åŠ¡ã€‚å½“è§£å†³æ²¡æœ‰æ ‡è®°CoTæ•°æ®çš„ç›®æ ‡ç»„åˆä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬æœç€æ¨ç†æŠ€èƒ½çš„ç»„åˆæ¨å¹¿è¿ˆå‡ºäº†ä¸€æ­¥ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…å¯¹åŸå­ä»»åŠ¡çš„CoTæ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒä¼šå¯¼è‡´æœ‰é™çš„æ¨å¹¿æ•ˆæœï¼Œä½†ç¨å¾®ä¿®æ”¹ç»„æˆåŸå­ä»»åŠ¡çš„CoTæ ¼å¼ä»¥ä½¿å…¶å…·æœ‰ç»„åˆæ€§ï¼Œå¯ä»¥å¯¼è‡´æ”¹è¿›ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç»„åˆCoTæ•°æ®å¯¹åŸå­ä»»åŠ¡ä¸Šçš„â€œåŸå­CoTâ€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸å¤šä»»åŠ¡å­¦ä¹ æˆ–æ¨¡å‹åˆå¹¶ç›¸ç»“åˆï¼Œä»¥åœ¨ç›®æ ‡ç»„åˆä»»åŠ¡ä¸Šè·å¾—æ›´å¥½çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚è¿™ç§ç»„åˆæ¨¡å‹å¯ä»¥ä½¿ç”¨æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰åœ¨å°é¢ç»„åˆæ•°æ®ä¸Šè¿›è¡Œè¿›ä¸€æ­¥å¼•å¯¼ã€‚åœ¨å­—ç¬¦ä¸²æ“ä½œå’Œè‡ªç„¶è¯­è¨€æŠ€èƒ½ç»„åˆæ–¹é¢çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç»™å®šè®­ç»ƒæ•°æ®é¢„ç®—å†…ï¼Œå¯¹LLMè¿›è¡Œç»„åˆCoTè®­ç»ƒä¼˜äºå¤šä»»åŠ¡å­¦ä¹ å’ŒæŒç»­å¾®è°ƒåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•™å­¦æ–¹æ³•ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹æ¨ç†èƒ½åŠ›çš„è®­ç»ƒã€‚å½“å‰å¸¸è§çš„åšæ³•æ˜¯å¯¹åˆ†å¸ƒå†…çš„æ¨ç†é—®é¢˜è¿›è¡Œæ€ç»´é“¾ï¼ˆCoTï¼‰ç—•è¿¹çš„è®­ç»ƒï¼Œä½†è¿™æ ·çš„æ ‡æ³¨æ•°æ®å¯¹äºæ¯ä¸ªæ„Ÿå…´è¶£çš„é—®é¢˜æ¥è¯´éƒ½æ˜¯æ˜‚è´µçš„ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°æ¨ç†æŠ€èƒ½çš„ç»„åˆæ³›åŒ–ï¼Œå³åœ¨è§£å†³æ²¡æœ‰æ ‡è®°CoTæ•°æ®çš„ç‰¹å®šç»„åˆä»»åŠ¡æ—¶ï¼Œé€šè¿‡å¯¹åŸºæœ¬ä»»åŠ¡çš„CoTæ ¼å¼è¿›è¡Œå¾®å°ä¿®æ”¹æ¥å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡è®­ç»ƒâ€œåŸå­CoTâ€æ¨¡å‹ï¼Œå¹¶ç»“åˆå¤šä»»åŠ¡å­¦ä¹ æˆ–æ¨¡å‹åˆå¹¶ï¼Œå¯ä»¥åœ¨ç›®æ ‡ç»„åˆä»»åŠ¡ä¸Šå®ç°é›¶æ ·æœ¬æ€§èƒ½çš„æ”¹è¿›ã€‚è¿›ä¸€æ­¥ä½¿ç”¨æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰åœ¨å°è§„æ¨¡ç»„åˆæ•°æ®ä¸Šä¼˜åŒ–æ¨¡å‹ï¼Œèƒ½æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å­—ç¬¦ä¸²æ“ä½œå’Œè‡ªç„¶è¯­è¨€æŠ€èƒ½ç»„åˆä»»åŠ¡ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œå¯¹LLMè¿›è¡Œç»„åˆCoTè®­ç»ƒä¼˜äºå¤šä»»åŠ¡å­¦ä¹ å’Œè¿ç»­å¾®è°ƒåŸºçº¿åœ¨ç»™å®šè®­ç»ƒæ•°æ®é¢„ç®—å†…çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šå¸¸é€šè¿‡è®­ç»ƒåˆ†å¸ƒå†…çš„æ¨ç†é—®é¢˜çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ç—•è¿¹è¿›è¡Œæ•™å­¦ï¼Œä½†æ ‡æ³¨æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æ–‡ç« çš„ç›®çš„æ˜¯å®ç°æ¨ç†æŠ€èƒ½çš„ç»„åˆæ³›åŒ–ï¼Œè§£å†³æ²¡æœ‰æ ‡è®°CoTæ•°æ®çš„ç‰¹å®šç»„åˆä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ç®€å•ä¿®æ”¹åŸºæœ¬ä»»åŠ¡çš„CoTæ ¼å¼ï¼Œå¯ä»¥å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒâ€œåŸå­CoTâ€æ¨¡å‹å¹¶ç»“åˆå¤šä»»åŠ¡å­¦ä¹ æˆ–æ¨¡å‹åˆå¹¶ï¼Œèƒ½æé«˜åœ¨ç›®æ ‡ç»„åˆä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å­—ç¬¦ä¸²æ“ä½œå’Œè‡ªç„¶è¯­è¨€æŠ€èƒ½ç»„åˆä»»åŠ¡ä¸Šï¼Œå¯¹LLMè¿›è¡Œç»„åˆCoTè®­ç»ƒä¼˜äºå…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-01ee954557d0cc4171dd2a6eb3706692.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-279f3102d044792d1907129b9c736c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16e4b648999336eb50c2ddd980f25967.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-946fc9be8908c344db7b6e0345340368.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RICO-Improving-Accuracy-and-Completeness-in-Image-Recaptioning-via-Visual-Reconstruction"><a href="#RICO-Improving-Accuracy-and-Completeness-in-Image-Recaptioning-via-Visual-Reconstruction" class="headerlink" title="RICO: Improving Accuracy and Completeness in Image Recaptioning via   Visual Reconstruction"></a>RICO: Improving Accuracy and Completeness in Image Recaptioning via   Visual Reconstruction</h2><p><strong>Authors:Yuchi Wang, Yishuo Cai, Shuhuai Ren, Sihan Yang, Linli Yao, Yuanxin Liu, Yuanxing Zhang, Pengfei Wan, Xu Sun</strong></p>
<p>Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at <a target="_blank" rel="noopener" href="https://github.com/wangyuchi369/RICO">https://github.com/wangyuchi369/RICO</a>. </p>
<blockquote>
<p>å›¾åƒé‡æ–°æè¿°å¹¿æ³›ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºå„ç§å¤šæ¨¡å¼ä»»åŠ¡ã€‚ç°æœ‰çš„é‡æ–°æè¿°æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå¤§çš„å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥æé«˜æ–‡æœ¬æè¿°ï¼Œä½†ç”±äºå¹»è§‰å’Œå› ç¼ºå°‘ç»†å¾®ç»†èŠ‚è€Œå¯¼è‡´çš„ä¸å®Œæ•´æ€§é—®é¢˜ï¼Œç»å¸¸å‡ºç°ä¸å‡†ç¡®çš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RICOï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è§†è§‰é‡å»ºæ¥ä¼˜åŒ–æè¿°çš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å°†æè¿°é‡å»ºä¸ºå‚è€ƒå›¾åƒï¼Œå¹¶æç¤ºMLLMè¯†åˆ«åŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ä¼˜åŒ–æè¿°ã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯è¿­ä»£çš„ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›äº†æ›´çœŸå®ã€æ›´å…¨é¢çš„æè¿°çš„ç”Ÿæˆã€‚ä¸ºäº†ç¼“è§£è¿­ä»£è¿‡ç¨‹å¸¦æ¥çš„é¢å¤–è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬å¼•å…¥äº†RICO-Flashï¼Œå®ƒå­¦ä¼šäº†åƒRICOä¸€æ ·ç”Ÿæˆæè¿°ï¼Œä½†ä½¿ç”¨çš„æ˜¯DPOã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œåœ¨CapsBenchå’ŒCompreCapä¸Šçš„è¡¨ç°ä¼˜äºå¤§å¤šæ•°åŸºçº¿æ–¹æ³•çº¦10%ã€‚ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wangyuchi369/RICO%E3%80%82">https://github.com/wangyuchi369/RICOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22613v1">PDF</a> code: <a target="_blank" rel="noopener" href="https://github.com/wangyuchi369/RICO">https://github.com/wangyuchi369/RICO</a></p>
<p><strong>æ‘˜è¦</strong><br>å›¾åƒé‡è¿°è¢«å¹¿æ³›ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºå„ç§å¤šæ¨¡æ€ä»»åŠ¡ã€‚ç°æœ‰é‡è¿°æ–¹æ³•é€šå¸¸ä¾èµ–å¼ºå¤§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¢å¼ºæ–‡æœ¬æè¿°ï¼Œä½†å—é™äºç”±äºç¼ºå¤±ç»†èŠ‚å¯¼è‡´çš„ä¸å‡†ç¡®ã€å¹»è±¡å’Œä¸å®Œå…¨ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºRICOï¼Œä¸€ç§é€šè¿‡è§†è§‰é‡å»ºç»†åŒ–å­—å¹•çš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å°†å­—å¹•é‡å»ºä¸ºå‚è€ƒå›¾åƒï¼Œå¹¶æç¤ºMLLMè¯†åˆ«åŸå§‹å’Œé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚ä»¥ä¼˜åŒ–å­—å¹•ã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯è¿­ä»£çš„ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›äº†æ›´çœŸå®å’Œå…¨é¢çš„æè¿°ç”Ÿæˆã€‚ä¸ºç¼“è§£è¿­ä»£è¿‡ç¨‹å¸¦æ¥çš„é¢å¤–è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RICO-Flashï¼Œå®ƒå­¦ä¹ åƒRICOä¸€æ ·ç”Ÿæˆå­—å¹•ï¼Œå¹¶ä½¿ç”¨DPOã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜å­—å¹•å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œåœ¨CapsBenchå’ŒCompreCapä¸Šçš„è¡¨ç°ä¼˜äºå¤§å¤šæ•°åŸºçº¿çº¦10%ã€‚ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wangyuchi369/RICO%E3%80%82">https://github.com/wangyuchi369/RICOã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å›¾åƒé‡è¿°æ˜¯ç”¨äºç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®é›†çš„ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºå¤šç§å¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰é‡è¿°æ–¹æ³•ä¾èµ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¢å¼ºæ–‡æœ¬æè¿°ï¼Œä½†å­˜åœ¨ä¸å‡†ç¡®ã€å¹»è±¡å’Œä¸å®Œå…¨çš„é—®é¢˜ã€‚</li>
<li>RICOæ¡†æ¶é€šè¿‡è§†è§‰é‡å»ºç»†åŒ–å­—å¹•ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹é‡å»ºå‚è€ƒå›¾åƒã€‚</li>
<li>RICOé€šè¿‡è¯†åˆ«åŸå§‹å’Œé‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚è¿­ä»£ä¼˜åŒ–å­—å¹•ã€‚</li>
<li>Ricoæ¡†æ¶å¯ä»¥ä¿ƒè¿›ç”Ÿæˆæ›´çœŸå®å’Œå…¨é¢çš„æè¿°ã€‚</li>
<li>ä¸ºå‡å°‘è¿­ä»£è¿‡ç¨‹çš„è®¡ç®—æˆæœ¬ï¼Œæ¨å‡ºäº†RICO-Flashã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRICOæ–¹æ³•åœ¨æé«˜å­—å¹•å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢è¡¨ç°æ˜¾è‘—ï¼Œä¼˜äºå¤§å¤šæ•°åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14e26f4819a145d82970761f1aa1d8ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa8e07a747370c6ac0b9b9ddb7f806e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8575887a9c1a7d1d1742eaa5553f85e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88778c6820e240596c882e676f44a71b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-238606c945bdd9ed0730205ca6442687.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Less-but-Better-Efficient-Multilingual-Expansion-for-LLMs-via-Layer-wise-Mixture-of-Experts"><a href="#Less-but-Better-Efficient-Multilingual-Expansion-for-LLMs-via-Layer-wise-Mixture-of-Experts" class="headerlink" title="Less, but Better: Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts"></a>Less, but Better: Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts</h2><p><strong>Authors:Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou</strong></p>
<p>Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs. The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages. To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts). Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages. To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer. Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts. Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens. Experimental results show that our method outperforms the previous state-of-the-art baseline with 60% fewer experts in the single-expansion setting and with 33.3% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method. </p>
<blockquote>
<p>å¯¹äºç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ï¼Œä¸æ–­æ‰©å±•æ–°çš„è¯­è¨€æ˜¯ä¸€ç§å……æ»¡å‰æ™¯ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºå¼ºå¤§çš„å¤šè¯­è¨€LLMã€‚æœ€å¤§çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•ä½¿æ¨¡å‹åœ¨æŒç»­å­¦ä¹ æ–°è¯­è¨€çš„åŒæ—¶ä¿æŒæ—§è¯­è¨€çš„ç†Ÿç»ƒèƒ½åŠ›ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæœ€è¿‘çš„å·¥ä½œåˆ©ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ¥æ‰©å±•æ–°è¯­è¨€ï¼Œé€šè¿‡æ·»åŠ æ–°ä¸“å®¶å¹¶é€šè¿‡å¯¹åº”ä»¤ç‰Œè·¯ç”±åˆ°åŸå§‹æ¨¡å‹ä¸»å¹²ï¼ˆæ—§ä¸“å®¶ï¼‰æ¥é¿å…æ—§è¯­è¨€çš„ç¾éš¾æ€§é—å¿˜ã€‚å°½ç®¡è¿™ç§ç›´è§‰æ€§çš„æ–¹æ³•å¾ˆç›´è§‚ï¼Œä½†åœ¨æ‰©å±•æ–°è¯­è¨€æ—¶æˆæœ¬è¾ƒé«˜ï¼Œå¹¶ä¸”ä»ç„¶ä¸å¯é¿å…åœ°ä¼šå½±å“æ—§è¯­è¨€çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬åˆ†æäº†LLMä¸­ä¸åŒå±‚çš„è¯­è¨€ç‰¹å¾ï¼Œå¹¶æå‡ºäº†åˆ†å±‚ä¸“å®¶åˆ†é…ç®—æ³•ï¼ˆLayerMoEï¼‰æ¥ç¡®å®šæ¯å±‚é€‚å½“çš„æ–°ä¸“å®¶æ•°é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°LLMä¸­çš„ä¸åŒå±‚åœ¨è¯­è¨€ä¹‹é—´è¡¨ç°å‡ºä¸åŒçš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œç„¶ååˆ©ç”¨è¿™ç§ç›¸ä¼¼æ€§ä½œä¸ºæŒ‡ç¤ºå™¨æ¥ä¸ºæ¯å±‚åˆ†é…ä¸“å®¶ï¼Œå³ç›¸ä¼¼æ€§è¶Šé«˜ï¼Œä¸“å®¶æ•°é‡è¶Šå°‘ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥å‡è½»æ—§è¯­è¨€çš„é—å¿˜ï¼Œæˆ‘ä»¬åœ¨å…·æœ‰è¾ƒé«˜ç›¸ä¼¼æ€§çš„å±‚ä¹‹å‰åœ¨è·¯ç”±å™¨ç½‘ç»œä¸­æ·»åŠ äº†ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œä»¥å¼•å¯¼æ—§è¯­è¨€ä»¤ç‰Œçš„è·¯ç”±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•æ‰©å±•è®¾ç½®å’Œå¤šç”Ÿæ‰©å±•è®¾ç½®ä¸­å‡ä¼˜äºä¹‹å‰çš„æœ€æ–°åŸºçº¿ï¼Œåˆ†åˆ«å‡å°‘äº†60%å’Œ33.3%çš„ä¸“å®¶æ•°é‡ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22582v1">PDF</a> ACL 2025 (Main), 16 pages, 5 figures, 11 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æŒç»­æ‰©å±•æ–°è¯­è¨€çš„æŒ‘æˆ˜å’Œæ–¹æ³•ã€‚åˆ©ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¿›è¡Œæ–°è¯­è¨€æ‰©å±•æ—¶ï¼Œå¦‚ä½•åœ¨æ·»åŠ æ–°ä¸“å®¶çš„åŒæ—¶é¿å…æ—§è¯­è¨€èƒ½åŠ›çš„ä¸§å¤±ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åˆ†å±‚ä¸“å®¶åˆ†é…ç®—æ³•ï¼ˆLayerMoEï¼‰ï¼Œé€šè¿‡åˆ†æLLMä¸­ä¸åŒè¯­è¨€ç‰¹æ€§çš„å±‚æ¬¡å…³ç³»ï¼Œç¡®å®šæ¯å±‚çš„é€‚å½“ä¸“å®¶æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•æ‰©å±•å’Œç»ˆèº«æ‰©å±•è®¾ç½®ä¸­å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä½¿ç”¨æ›´å°‘çš„ä¸“å®¶æ•°é‡å®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒç»­æ‰©å±•æ–°è¯­è¨€æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡æ–°è¯­è¨€å­¦ä¹ å’Œæ—§è¯­è¨€ä¿æŒèƒ½åŠ›ã€‚</li>
<li>æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¢«ç”¨äºæ–°è¯­è¨€æ‰©å±•ï¼Œä½†å­˜åœ¨å‚æ•°æˆæœ¬é«˜æ˜‚å’Œå¯¹æ—§è¯­è¨€æ€§èƒ½çš„å½±å“é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åˆ†å±‚ä¸“å®¶åˆ†é…ç®—æ³•ï¼ˆLayerMoEï¼‰ï¼Œæ ¹æ®è¯­è¨€ç‰¹æ€§ç¡®å®šæ¯å±‚çš„ä¸“å®¶æ•°é‡ã€‚</li>
<li>å‘ç°LLMçš„ä¸åŒå±‚åœ¨è¡¨ç¤ºä¸åŒè¯­è¨€æ—¶çš„ç›¸ä¼¼æ€§å·®å¼‚ï¼Œåˆ©ç”¨è¿™ç§ç›¸ä¼¼æ€§ä½œä¸ºåˆ†é…ä¸“å®¶çš„æŒ‡æ ‡ã€‚</li>
<li>åœ¨è·¯ç”±ç½‘ç»œä¸Šå¢åŠ åˆ†ç±»å™¨ï¼Œä»¥å¼•å¯¼æ—§è¯­è¨€ç¬¦å·çš„è·¯ç”±ï¼Œè¿›ä¸€æ­¥å‡è½»æ—§è¯­è¨€é—å¿˜é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLayerMoEæ–¹æ³•ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œåœ¨å•æ‰©å±•å’Œç»ˆèº«æ‰©å±•è®¾ç½®ä¸­å‡ä½¿ç”¨æ›´å°‘çš„ä¸“å®¶å®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8df6ca0fa329595005f8e23e8b6f5204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01c64eae794b57480e3769ca4d7a005f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53db2c6553e8ba50bd6b204d2a718035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2918f74633ac350544130267969109d2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ClaimPKG-Enhancing-Claim-Verification-via-Pseudo-Subgraph-Generation-with-Lightweight-Specialized-LLM"><a href="#ClaimPKG-Enhancing-Claim-Verification-via-Pseudo-Subgraph-Generation-with-Lightweight-Specialized-LLM" class="headerlink" title="ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation   with Lightweight Specialized LLM"></a>ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation   with Lightweight Specialized LLM</h2><p><strong>Authors:Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui</strong></p>
<p>Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones. </p>
<blockquote>
<p>å°†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œä»¥å¢å¼ºå…¶æ¨ç†èƒ½åŠ›ï¼Œæ˜¯å£°æ˜éªŒè¯é¢†åŸŸçš„æ–°å…´ç ”ç©¶æŒ‘æˆ˜ã€‚è™½ç„¶çŸ¥è¯†å›¾è°±æä¾›äº†ç»“æ„åŒ–å’Œè¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼ï¼Œéå¸¸é€‚åˆè¿›è¡Œæ¨ç†ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„éªŒè¯æ–¹æ³•éƒ½ä¾èµ–äºéç»“æ„åŒ–çš„æ–‡æœ¬è¯­æ–™åº“ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æœ‰æ•ˆåˆ©ç”¨çŸ¥è¯†å›¾è°±çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå°½ç®¡ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æ‹¥æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤šæ­¥éª¤æ¨¡å—åŒ–ç®¡é“å’Œé€‚åº”çŸ¥è¯†å›¾è°±çš„æ¨ç†æ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ClaimPKGï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œæ— ç¼é›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼ŒClaimPKGçš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨ä¸€ä¸ªè½»é‡çº§çš„ä¸“ä¸šå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¡¨ç¤ºè¾“å…¥å£°æ˜ä½œä¸ºä¼ªå­å›¾ï¼Œå¼•å¯¼ä¸“é—¨çš„å­å›¾æ£€ç´¢æ¨¡å—æ¥è¯†åˆ«ç›¸å…³çš„çŸ¥è¯†å›¾è°±å­å›¾ã€‚è¿™äº›æ£€ç´¢åˆ°çš„å­å›¾éšåè¢«é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†ï¼Œä»¥äº§ç”Ÿæœ€ç»ˆçš„è£å†³å’Œä¾æ®ã€‚åœ¨FactKGæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒClaimPKGè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨è¯¥ç ”ç©¶é¢†åŸŸçš„å¤šä¸ªç±»åˆ«ä¸­ï¼Œå…¶å‡†ç¡®æ€§è¶…è¿‡äº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹è¾¾9%-12%ã€‚æ­¤å¤–ï¼ŒClaimPKGåœ¨æ— ç»“æ„æ•°æ®é›†ï¼ˆå¦‚HoVerå’ŒFEVEROUSï¼‰ä¸Šè¡¨ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22552v1">PDF</a> Accepted by ACL 2025 findings</p>
<p><strong>Summary</strong>ï¼š<br>å°†çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥æå‡å…¶åœ¨å£°æ˜éªŒè¯ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¯ä¸€é¡¹æ–°å…´çš„ç ”ç©¶æŒ‘æˆ˜ã€‚è™½ç„¶çŸ¥è¯†å›¾è°±æä¾›äº†ç»“æ„åŒ–å’Œè¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼ï¼Œé€‚åˆè¿›è¡Œæ¨ç†ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„éªŒè¯æ–¹æ³•ä»ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨çŸ¥è¯†å›¾è°±ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ClaimPKGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼é›†æˆäº†LLMæ¨ç†ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ã€‚ClaimPKGé€šè¿‡é‡‡ç”¨è½»é‡åŒ–ã€ä¸“ä¸šåŒ–çš„LLMæ¥è¡¨ç¤ºè¾“å…¥å£°æ˜ä¸ºä¼ªå­å›¾ï¼Œå¼•å¯¼ä¸“ç”¨çš„å­å›¾æ£€ç´¢æ¨¡å—æ¥è¯†åˆ«ç›¸å…³çš„KGå­å›¾ã€‚è¿™äº›æ£€ç´¢åˆ°çš„å­å›¾ç„¶åè¢«ä¸€ä¸ªé€šç”¨çš„LLMå¤„ç†ï¼Œä»¥äº§ç”Ÿæœ€ç»ˆçš„è£å†³å’Œç†ç”±ã€‚åœ¨FactKGæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒClaimPKGå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨è¿™ä¸ªç ”ç©¶é¢†åŸŸçš„å¤šä¸ªç±»åˆ«ä¸­ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†9%-12%ã€‚æ­¤å¤–ï¼ŒClaimPKGåœ¨æ— ç»“æ„æ•°æ®é›†ï¼ˆå¦‚HoVerå’ŒFEVEROUSï¼‰ä¸Šå±•ç°äº†é›¶é•œå¤´æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ä¸å„ç§LLMéª¨å¹²ç½‘çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç»“åˆå¯¹äºæé«˜å£°æ˜éªŒè¯ä¸­çš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€ä¸ªæ–°å…´ç ”ç©¶æŒ‘æˆ˜ã€‚</li>
<li>è™½ç„¶çŸ¥è¯†å›¾è°±æä¾›äº†ç»“æ„åŒ–å’Œè¯­ä¹‰ä¸°å¯Œçš„ä¿¡æ¯ï¼Œä½†å¤§å¤šæ•°éªŒè¯æ–¹æ³•ä»ä¾èµ–éç»“æ„åŒ–æ–‡æœ¬è¯­æ–™åº“ã€‚</li>
<li>ClaimPKGæ¡†æ¶é›†æˆäº†LLMæ¨ç†ä¸çŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ï¼Œé‡‡ç”¨è½»é‡çº§ä¸“ä¸šLLMè¡¨ç¤ºè¾“å…¥å£°æ˜ä¸ºä¼ªå­å›¾ã€‚</li>
<li>ClaimPKGé€šè¿‡å­å›¾æ£€ç´¢æ¨¡å—è¯†åˆ«ç›¸å…³çš„KGå­å›¾ï¼Œå†äº¤ç”±é€šç”¨LLMå¤„ç†äº§ç”Ÿæœ€ç»ˆè£å†³å’Œç†ç”±ã€‚</li>
<li>åœ¨FactKGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒClaimPKGæ€§èƒ½ä¼˜è¶Šï¼Œè¾ƒåŸºçº¿æ¨¡å‹å‡†ç¡®ç‡æé«˜9%-12%ã€‚</li>
<li>ClaimPKGå±•ç°å‡ºé›¶é•œå¤´æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨æ— ç»“æ„æ•°æ®é›†ä¸Šæœ‰æ•ˆç»“åˆçŸ¥è¯†å›¾è°±çš„ç»“æ„åŒ–çŸ¥è¯†ä¸LLMæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb75a6d33f124191a082d239ceb1c258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c859465138917efe4971dab2fa0c923f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling"><a href="#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling" class="headerlink" title="Curse of High Dimensionality Issue in Transformer for Long-context   Modeling"></a>Curse of High Dimensionality Issue in Transformer for Long-context   Modeling</h2><p><strong>Authors:Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</strong></p>
<p>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at <a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢ä¸´é‡å¤§çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼šè™½ç„¶æ³¨æ„åŠ›æƒé‡é€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œä½†æ‰€æœ‰ä»¤ç‰Œéƒ½æ¶ˆè€—ç€å¹³ç­‰çš„è®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„æ¦‚ç‡åºåˆ—å»ºæ¨¡é‡æ–°è¡¨è¿°ä¸ºâ€œç›‘ç£å­¦ä¹ ä»»åŠ¡â€ï¼Œè¿™èƒ½å¤Ÿåˆ†ç¦»ç›¸å…³å’Œä¸ç›¸å…³çš„ä»¤ç‰Œï¼Œå¹¶æä¾›å¯¹å†—ä½™çš„æ›´æ¸…æ™°ç†è§£ã€‚åŸºäºè¿™ç§é‡æ–°è¡¨è¿°ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œæ­ç¤ºåªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹äº§ç”Ÿé‡å¤§è´¡çŒ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›ä¼˜åŒ–è¡¨è¿°ä¸ºçº¿æ€§ç¼–ç é—®é¢˜ï¼Œå¹¶æå‡ºâ€œåˆ†ç»„ç¼–ç ç­–ç•¥â€ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†å…¶æé«˜å¯¹æŠ—éšæœºå™ªå£°çš„é²æ£’æ€§å’Œæé«˜å­¦ä¹ æ•ˆç‡çš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåŠ¨æ€ç»„æ³¨æ„åŠ›â€ï¼ˆDGAï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†ç»„ç¼–ç æ¥é€šè¿‡èšåˆä¸å¤ªé‡è¦çš„ä»¤ç‰Œæ¥æ˜ç¡®å‡å°‘å†—ä½™çš„è®¡ç®—ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DGAåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22107v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ï¼Œå°¤å…¶åœ¨äºé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢ä¸´è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå°†ä¼ ç»Ÿæ¦‚ç‡åºåˆ—å»ºæ¨¡é‡æ–°å®šä¹‰ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä»¥åŒºåˆ†ç›¸å…³å’Œä¸ç›¸å…³çš„ä»¤ç‰Œï¼Œä»è€Œå‡å°‘å†—ä½™ã€‚æ–‡ç« è¿˜è¿›è¡Œäº†æ³¨æ„åŠ›ç¨€ç–æ€§çš„ç†è®ºåˆ†æï¼Œå¹¶è¯æ˜åªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹æœ‰æ˜¾è‘—è´¡çŒ®ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« æå‡ºäº†åŠ¨æ€ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ†ç»„ç¼–ç ç­–ç•¥æ¥é™ä½å†—ä½™ï¼Œå¹¶åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶èšåˆä¸é‡è¦çš„ä»¤ç‰Œã€‚å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒDGAåœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œä¿æŒäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMs excel in natural language processing by capturing long-range dependencies through self-attention.</li>
<li>é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å°†ä¼ ç»Ÿæ¦‚ç‡åºåˆ—å»ºæ¨¡é‡æ–°å®šä¹‰ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡æ¥å‡å°‘å†—ä½™ã€‚</li>
<li>æ–‡ç« è¿›è¡Œäº†æ³¨æ„åŠ›ç¨€ç–æ€§çš„ç†è®ºåˆ†æï¼Œå¹¶è¯æ˜åªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹æœ‰æ˜¾è‘—è´¡çŒ®ã€‚</li>
<li>æå‡ºäº†åŠ¨æ€ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ†ç»„ç¼–ç ç­–ç•¥é™ä½å†—ä½™ã€‚</li>
<li>DGAåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79b79c086893142ba74da0a562dc0651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eca6ec960b889a2e81e3ec88cc965ea2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Legal-Assist-AI-Leveraging-Transformer-Based-Model-for-Effective-Legal-Assistance"><a href="#Legal-Assist-AI-Leveraging-Transformer-Based-Model-for-Effective-Legal-Assistance" class="headerlink" title="Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal   Assistance"></a>Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal   Assistance</h2><p><strong>Authors:Jatin Gupta, Akhil Sharma, Saransh Singhania, Ali Imam Abidi</strong></p>
<p>Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the modelâ€™s applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well. </p>
<blockquote>
<p>åœ¨å°åº¦ï¼Œè·å–åˆæ³•æ³•å¾‹æ´åŠ©çš„è¿½æ±‚é¢ä¸´ç€ä¸€ä¸ªå…³é”®çš„å·®è·ï¼Œå› ä¸ºè®¸å¤šå…¬æ°‘ç”±äºå¯¹ç›¸å…³æ³•å¾‹ä¿¡æ¯çš„äº†è§£å’Œè·å–æœ‰é™ï¼Œéš¾ä»¥åˆ©ç”¨è‡ªå·±çš„æ³•å¾‹æƒåˆ©ã€‚æœ¬æ–‡ä»‹ç»äº†Legal Assist AIï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºtransformerçš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æœ‰æ•ˆçš„æ³•å¾‹æ´åŠ©ï¼Œä»¥å¼¥åˆè¿™ä¸€å·®è·ã€‚è¯¥ç³»ç»Ÿä»ç²¾é€‰çš„æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ³•å¾‹ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆå‡†ç¡®çš„å“åº”ï¼Œèƒ½å¤Ÿä¸ºåŒ…æ‹¬æ³•å¾‹ä¸“ä¸šäººå£«ã€å­¦è€…å’Œå…¬ä¼—åœ¨å†…çš„ä¸åŒç”¨æˆ·æä¾›æœ‰æ•ˆçš„å¸®åŠ©ã€‚è¯¥æ¨¡å‹ç»è¿‡å°åº¦æ³•å¾‹é¢†åŸŸçš„å¤§é‡æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ŒåŒ…æ‹¬å°åº¦å®ªæ³•ã€å°åº¦æ³•å…¸ï¼ˆBNSï¼‰ã€å°åº¦å…¬æ°‘å®‰å…¨æ³•æ¡ˆï¼ˆBNSSï¼‰ç­‰ç­‰ï¼Œå¯¹å°åº¦æ³•å¾‹çš„å¤æ‚æ€§æœ‰äº†ç¨³å¥çš„ç†è§£ã€‚é€šè¿‡èå…¥ç‰¹å®šé¢†åŸŸçš„æ³•å¾‹æ•°æ®é›†ï¼Œæ‰€æè®®çš„æ¨¡å‹åœ¨æ³•å¾‹é—®ç­”æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆç‡å’Œä¸“ä¸šåŒ–ã€‚è¯¥æ¨¡å‹ä¸æœ€å‰æ²¿çš„æ¨¡å‹å¦‚GPT-3.5 Turboå’ŒMistral 7Bè¿›è¡Œäº†è¯„ä¼°å¯¹æ¯”ï¼Œåœ¨æ³•å¾‹æ¨ç†å’Œå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†60.08%çš„AIBEè¯„åˆ†ï¼Œè¶…è¶Šäº†å…¶ç«äº‰å¯¹æ‰‹ã€‚ä¸å…¶ä»–æ¨¡å‹ä¸åŒçš„æ˜¯ï¼ŒLegal Assist AIé¿å…äº†è¯¸å¦‚å¹»è§‰ç­‰å¸¸è§é—®é¢˜ï¼Œä½¿å…¶æˆä¸ºå®é™…æ³•å¾‹åº”ç”¨çš„é«˜åº¦å¯é å·¥å…·ã€‚å®ƒå±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç°å®æ³•å¾‹åœºæ™¯ä¸­çš„åº”ç”¨æ€§ï¼Œæœªæ¥çš„ç‰ˆæœ¬æ—¨åœ¨æé«˜æ€§èƒ½ï¼Œå¹¶æ‰©å¤§æ•°æ®é›†ï¼Œä»¥è¦†ç›–æ›´å¹¿æ³›çš„å¤šç§è¯­è¨€å’Œç‰¹å®šæ¡ˆä¾‹æŸ¥è¯¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22003v1">PDF</a> 9 pages, 5 tables, 4 figures. This is a revised version of a preprint   previously available at this URL: <a target="_blank" rel="noopener" href="https://doi.org/10.21203/rs.3.rs-5351879/v1">https://doi.org/10.21203/rs.3.rs-5351879/v1</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>å°åº¦æ³•å¾‹æ´åŠ©çš„å¯åŠæ€§é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè®¸å¤šå…¬æ°‘å› å¯¹æ³•å¾‹ä¿¡æ¯ç¼ºä¹äº†è§£å’Œè·å–é€”å¾„è€Œæ— æ³•å……åˆ†åˆ©ç”¨è‡ªèº«æ³•å¾‹æƒç›Šã€‚æœ¬æ–‡ä»‹ç»äº†Legal Assist AIï¼Œä¸€æ¬¾åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æœ‰æ•ˆçš„æ³•å¾‹æ´åŠ©æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚è¯¥ç³»ç»Ÿä»ç²¾é€‰æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ³•å¾‹ä¿¡æ¯å¹¶ç”Ÿæˆå‡†ç¡®å“åº”ï¼Œä¸ºåŒ…æ‹¬æ³•å¾‹ä¸“ä¸šäººå£«ã€å­¦è€…å’Œå…¬ä¼—åœ¨å†…çš„å„ç±»ç”¨æˆ·æä¾›æœ‰æ•ˆæ´åŠ©ã€‚æ¨¡å‹ç»è¿‡å°åº¦æ³•å¾‹é¢†åŸŸçš„å¹¿æ³›æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼ŒåŒ…æ‹¬å°åº¦å®ªæ³•ã€å°åº¦æ³•å¾‹æ±‡ç¼–ç­‰ï¼Œå¯¹å°åº¦æ³•å¾‹çš„å¤æ‚æ€§æœ‰æ·±å…¥äº†è§£ã€‚é€šè¿‡å¼•å…¥é¢†åŸŸç‰¹å®šçš„æ³•å¾‹æ•°æ®é›†ï¼Œè¯¥æ¨¡å‹åœ¨æ³•å¾‹é—®ç­”æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆç‡å’Œä¸“ä¸šæ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLegal Assist AIåœ¨æ³•å¾‹æ¨ç†å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºGPT-3.5 Turboå’ŒMistral 7Bç­‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨å®é™…æ³•å¾‹åœºæ™¯ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶é€‚ç”¨æ€§ï¼Œæœªæ¥ç‰ˆæœ¬å°†è¿›ä¸€æ­¥æé«˜æ€§èƒ½å¹¶æ‰©å±•æ•°æ®é›†ï¼Œä»¥è¦†ç›–æ›´å¹¿æ³›çš„è·¨è¯­è¨€åŠç‰¹å®šæ¡ˆä¾‹æŸ¥è¯¢ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å°åº¦å…¬æ°‘åœ¨è·å–æ³•å¾‹æ´åŠ©æ–¹é¢é¢ä¸´å›°éš¾ï¼Œç¼ºä¹å¯¹ç›¸å…³æ³•å¾‹ä¿¡æ¯çš„äº†è§£å’Œè·å–é€”å¾„ã€‚</li>
<li>Legal Assist AIæ˜¯ä¸€æ¬¾åŸºäºLLMè®¾è®¡çš„æ¨¡å‹ï¼Œæ—¨åœ¨ç¼©å°è¿™ä¸€å·®è·ã€‚</li>
<li>è¯¥ç³»ç»Ÿå¯ä»æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ³•å¾‹ä¿¡æ¯å¹¶ç”Ÿæˆå‡†ç¡®å“åº”ã€‚</li>
<li>æ¨¡å‹ç»è¿‡å°åº¦æ³•å¾‹é¢†åŸŸçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„ä¸“ä¸šæ€§å’Œæ•ˆç‡ã€‚</li>
<li>Legal Assist AIåœ¨æ³•å¾‹æ¨ç†å’Œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é¿å…äº†å¸¸è§çš„å¹»è§‰é—®é¢˜ï¼Œå…·æœ‰é«˜åº¦å¯é æ€§ï¼Œé€‚ç”¨äºå®é™…æ³•å¾‹åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d44840295c3577fe4963886949255174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89671461f58ca318de287f7fc336ee70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a57a12ac14b0872a9c333f8f7c89990.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c0be24cc479d84946a3d6411dd71486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac36fc19b3e4a3dc1598dc1633641c94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb58bb5ad5c710e7a22157fbff33c0c2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation"><a href="#Distill-CLIP-DCLIP-Enhancing-Image-Text-Retrieval-via-Cross-Modal-Transformer-Distillation" class="headerlink" title="Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation"></a>Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal   Transformer Distillation</h2><p><strong>Authors:Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabeau, Michael Lu, Kevin Zhu, Sean Oâ€™Brien, Vasu Sharma</strong></p>
<p>We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original modelâ€™s strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIPâ€™s original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIPâ€™s zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md">https://anonymous.4open.science/r/DCLIP-B772/README.md</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Distill CLIPï¼ˆDCLIPï¼‰ï¼Œå®ƒæ˜¯CLIPæ¨¡å‹çš„ä¸€ç§å¾®è°ƒå˜ä½“ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ¨¡å‹å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚CLIPæ¨¡å‹é€šå¸¸å—åˆ°å›ºå®šå›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™ä¸Šä¸‹æ–‡çš„é™åˆ¶ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å®ƒä»¬åœ¨éœ€è¦ç²¾ç»†è·¨æ¨¡æ€ç†è§£çš„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆå­¦ç”Ÿè’¸é¦æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå…¶ä¸­è·¨æ¨¡æ€è½¬æ¢å™¨æ•™å¸ˆç»è¿‡å¾®è°ƒä»¥é€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ã€‚è¿™äº›è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„å…¨å±€è¡¨ç¤ºé€šè¿‡ä½¿ç”¨ç»“åˆå¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡çš„æ··åˆæŸå¤±æ¥æŒ‡å¯¼è½»é‡çº§å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚å°½ç®¡ä»…ä½¿ç”¨ä»MSCOCOã€Flickr30kå’ŒConceptual Captionsç²¾é€‰çš„çº¦67,500ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œåªå CLIPåŸå§‹æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPå¤§çº¦94%çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ä¸“ä¸šåŒ–å’Œé€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/DCLIP-B772/README.md%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/DCLIP-B772/README.mdæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21549v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Distill CLIPï¼ˆDCLIPï¼‰æ˜¯CLIPæ¨¡å‹çš„ç²¾ç»†è°ƒæ•´å˜ä½“ï¼Œå®ƒå¢å¼ºäº†å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ¨¡å‹å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚DCLIPé€šè¿‡å…ƒæ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³CLIPæ¨¡å‹åœ¨å›¾åƒåˆ†è¾¨ç‡å’Œä¸Šä¸‹æ–‡æ–¹é¢çš„é™åˆ¶ï¼Œé€šè¿‡è·¨æ¨¡æ€å˜å‹å™¨æ•™å¸ˆäº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ï¼Œé€šè¿‡YOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦ä¹‹é—´çš„åŒå‘äº¤å‰æ³¨æ„åŠ›è¿›è¡Œè®­ç»ƒã€‚ä½¿ç”¨æ··åˆæŸå¤±è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡ã€‚å°½ç®¡åªåœ¨çº¦67ï¼Œ500ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ ·æœ¬æ˜¯ä»MSCOCOã€Flickr30kå’ŒConceptual Captionsä¸­ç²¾é€‰å‡ºæ¥çš„ï¼Œä»…å CLIPåŸå§‹æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œä½†DCLIPæ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æŒ‡æ ‡ï¼ˆRecall@Kï¼ŒMAPï¼‰ï¼ŒåŒæ—¶ä¿ç•™äº†CLIPçš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½çš„çº¦94%ã€‚ç»“æœè¡¨æ˜ï¼ŒDCLIPæœ‰æ•ˆåœ°ç¼“è§£äº†ä»»åŠ¡ç‰¹åŒ–ä¸é€šç”¨åŒ–ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºé«˜çº§è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†èµ„æºé«˜æ•ˆã€é¢†åŸŸè‡ªé€‚åº”å’Œç»†èŠ‚æ•æ„Ÿçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCLIPæ˜¯CLIPæ¨¡å‹çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ£€ç´¢åŠŸèƒ½å¹¶ä¿ç•™å…¶é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>DCLIPé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ¡†æ¶è§£å†³CLIPæ¨¡å‹çš„é™åˆ¶ï¼ŒåŒ…æ‹¬å›ºå®šå›¾åƒåˆ†è¾¨ç‡å’Œæœ‰é™ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹é€šè¿‡åŒå‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿä¸°å¯Œçš„åµŒå…¥ï¼Œç»“åˆYOLOæå–çš„å›¾åƒåŒºåŸŸå’Œç›¸åº”æ–‡æœ¬è·¨åº¦è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹ä½¿ç”¨æ··åˆæŸå¤±è¿›è¡Œè®­ç»ƒï¼Œè¯¥æŸå¤±ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œä½™å¼¦ç›¸ä¼¼æ€§ç›®æ ‡ã€‚</li>
<li>DCLIPåœ¨æœ‰é™çš„æ ·æœ¬é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œä½†æ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬æ£€ç´¢æ€§èƒ½ï¼ˆRecall@Kï¼ŒMAPï¼‰ã€‚</li>
<li>DCLIPèƒ½å¤Ÿä¿ç•™CLIPå¤§çº¦94%çš„é›¶æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d39c40dc0621157ac2a952ec650748c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64fc51ed96f60f5dfbe42773329d3a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3c03365170299397c44c4faec81cd22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2a39fdf816a19d9a53b77d2690e9b54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46f8ddd452943fc59fedaf051f639e47.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Is-Attention-Required-for-Transformer-Inference-Explore-Function-preserving-Attention-Replacement"><a href="#Is-Attention-Required-for-Transformer-Inference-Explore-Function-preserving-Attention-Replacement" class="headerlink" title="Is Attention Required for Transformer Inference? Explore   Function-preserving Attention Replacement"></a>Is Attention Required for Transformer Inference? Explore   Function-preserving Attention Replacement</h2><p><strong>Authors:Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang</strong></p>
<p>While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually â€˜â€™simpleâ€™â€™ enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformerâ€™s attention module. </p>
<blockquote>
<p>è™½ç„¶Transformeråœ¨è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„ä¾èµ–å¯¹æ¨ç†æ•ˆç‡æå‡ºäº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰æœ‰é™å¹¶è¡Œæ€§å’Œå†…å­˜å¸¦å®½çš„è¾¹ç¼˜å’ŒåµŒå…¥å¼åŠ é€Ÿå™¨ä¸Šã€‚é€šè¿‡è§‚å¯Ÿæ¨ç†æ—¶æ³¨æ„åŠ›çš„å†—ä½™æ€§ï¼Œæˆ‘ä»¬å‡è®¾å°½ç®¡æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ å¤æ‚çš„ä»¤ç‰Œä¾èµ–æ€§ï¼Œä½†æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ¨ç†æ—¶é—´åºåˆ—åˆ°åºåˆ—çš„æ˜ å°„å®é™…ä¸Šå¯ä»¥ç”¨æ›´ä¾¿å®œçš„å‡½æ•°æ¥è¡¨ç¤ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†FARï¼ˆå‡½æ•°ä¿æŒæ³¨æ„åŠ›æ›¿æ¢æ¡†æ¶ï¼‰ï¼Œè¯¥æ¡†æ¶ç”¨å¯å­¦ä¹ çš„åºåˆ—åˆ°åºåˆ—æ¨¡å—æ›¿æ¢äº†é¢„è®­ç»ƒTransformerä¸­çš„æ‰€æœ‰æ³¨æ„åŠ›å—ï¼Œä»¥LSTMä¸ºä¾‹ã€‚FARä¼˜åŒ–äº†ä¸€ç§å¤šå¤´LSTMæ¶æ„ï¼Œé‡‡ç”¨å—çº§è’¸é¦ç›®æ ‡å’Œå…¨å±€ç»“æ„å‰ªææ¡†æ¶ï¼Œä»é¢„è®­ç»ƒçš„Transformerä¸­å®ç°äº†ä¸€ç³»åˆ—é«˜æ•ˆçš„LSTMæ¨¡å‹å®¶æ—ã€‚æˆ‘ä»¬åœ¨DeiTè§†è§‰Transformerå®¶æ—ä¸ŠéªŒè¯äº†FARï¼Œå¹¶è¯æ˜å®ƒåœ¨ImageNetå’Œå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸åŸå§‹æ¨¡å‹ç›¸åŒ¹é…ï¼ŒåŒæ—¶å‡å°‘äº†å‚æ•°å’Œå»¶è¿Ÿã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒFARä¿ç•™äº†Transformerçš„æ³¨æ„åŠ›æ¨¡å—ä¸­å­¦ä¹ çš„è¯­ä¹‰ä»¤ç‰Œå…³ç³»å’Œä»¤ç‰Œåˆ°ä»¤ç‰Œçš„å…³è”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21535v1">PDF</a> 12 pages main paper + 6 pages appendix, 14 figures; submitted to   NeurIPS 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å°†é¢„è®­ç»ƒTransformerä¸­çš„æ³¨æ„åŠ›å—æ›¿æ¢ä¸ºå¯å­¦ä¹ çš„åºåˆ—åˆ°åºåˆ—æ¨¡å—çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥LSTMä¸ºä¾‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ã€‚è¯¥ç ”ç©¶ä¼˜åŒ–äº†ä¸€ç§åŸºäºLSTMçš„å¤šå¤´æ¶æ„ï¼Œé‡‡ç”¨å—çº§è’¸é¦ç›®æ ‡å’Œå…¨å±€ç»“æ„å‰ªææ¡†æ¶ï¼Œä»é¢„è®­ç»ƒTransformerä¸­å®ç°äº†é«˜æ•ˆçš„LSTMæ¨¡å‹å®¶æ—ã€‚åœ¨DeiTè§†è§‰Transformerå®¶æ—ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œåœ¨ImageNetå’Œå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„ç²¾åº¦ï¼ŒåŒæ—¶é™ä½äº†å‚æ•°å’Œå»¶è¿Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformersåœ¨è§†è§‰å’Œè¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„ä¾èµ–ç»™æ¨ç†æ•ˆç‡å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¶è¡Œåº¦å’Œå†…å­˜å¸¦å®½æœ‰é™çš„è¾¹ç¼˜å’ŒåµŒå…¥å¼åŠ é€Ÿå™¨ä¸Šã€‚</li>
<li>è§‚å¯Ÿåˆ°æ¨ç†æ—¶çš„æ³¨æ„åŠ›å†—ä½™ï¼Œæ¨¡å‹è™½ç„¶åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ å¤æ‚çš„ä»¤ç‰Œä¾èµ–å…³ç³»ï¼Œä½†æ¨ç†æ—¶æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„åºåˆ—åˆ°åºåˆ—æ˜ å°„å®é™…ä¸Šå¯ä»¥ç”¨æ›´ç®€å•çš„å‡½æ•°æ¥è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºFARçš„åŠŸèƒ½ä¿æŒæ³¨æ„åŠ›æ›¿ä»£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨å¯å­¦ä¹ çš„åºåˆ—åˆ°åºåˆ—æ¨¡å—ï¼ˆå¦‚LSTMï¼‰æ›¿ä»£äº†é¢„è®­ç»ƒTransformerä¸­çš„æ‰€æœ‰æ³¨æ„åŠ›å—ã€‚</li>
<li>FARé€šè¿‡ä¼˜åŒ–å¤šå¤´LSTMæ¶æ„ã€é‡‡ç”¨å—çº§è’¸é¦ç›®æ ‡å’Œå…¨å±€ç»“æ„å‰ªææ¡†æ¶ï¼Œå®ç°äº†ä»é¢„è®­ç»ƒTransformerä¸­çš„é«˜æ•ˆLSTMæ¨¡å‹å®¶æ—ã€‚</li>
<li>åœ¨DeiTè§†è§‰Transformerå®¶æ—ä¸ŠéªŒè¯äº†FARï¼Œåœ¨ImageNetå’Œå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„ç²¾åº¦ï¼ŒåŒæ—¶é™ä½äº†å‚æ•°å’Œè®¡ç®—å»¶è¿Ÿã€‚</li>
<li>FARèƒ½å¤Ÿä¿ç•™Transformerä¸­æ³¨æ„åŠ›æ¨¡å—å­¦ä¹ çš„è¯­ä¹‰ä»¤ç‰Œå…³ç³»å’Œä»¤ç‰Œåˆ°ä»¤ç‰Œçš„ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-082f9dabfdbc2d1da05234bd151ddabe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-089a557d9a38af2ff72cdc176dc2d2c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c50ac0dc1230dfd7b82b062bf35e7a2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-536e5962c5e6820abd9bc70e4d7437e9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SynLogic-Synthesizing-Verifiable-Reasoning-Data-at-Scale-for-Learning-Logical-Reasoning-and-Beyond"><a href="#SynLogic-Synthesizing-Verifiable-Reasoning-Data-at-Scale-for-Learning-Logical-Reasoning-and-Beyond" class="headerlink" title="SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond"></a>SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond</h2><p><strong>Authors:Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He</strong></p>
<p>Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/SynLogic">https://github.com/MiniMax-AI/SynLogic</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¦‚OpenAI-o1å’ŒDeepSeek R1ç­‰è¿›å±•è¡¨æ˜å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶å¼€æºå¤åˆ¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œä½†å¼€å‘é€šç”¨æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å’Œèµ„æºä»ç„¶æ¢ç´¢ä¸è¶³ã€‚è¿™ä¸€å·®è·éƒ¨åˆ†æ˜¯ç”±äºæ”¶é›†é€‚åˆå¼ºåŒ–å­¦ä¹ çš„å¤šæ ·åŒ–å’Œå¯éªŒè¯çš„æ¨ç†æ•°æ®æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‡è®¾é€»è¾‘æ¨ç†å¯¹äºå‘å±•é€šç”¨æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå› ä¸ºé€»è¾‘æ˜¯æ¨ç†çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynLogicï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆå¤šæ ·åŒ–çš„é€»è¾‘æ¨ç†æ•°æ®ï¼Œæ¶µç›–35ç§ä¸åŒçš„é€»è¾‘æ¨ç†ä»»åŠ¡ã€‚SynLogicæ–¹æ³•èƒ½å¤Ÿæ§åˆ¶æ•°æ®å’Œéš¾åº¦å’Œæ•°é‡çš„åˆæˆã€‚é‡è¦çš„æ˜¯ï¼Œæ‰€æœ‰ä¾‹å­éƒ½å¯ä»¥é€šè¿‡ç®€å•çš„è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œä½¿å®ƒä»¬éå¸¸é€‚åˆå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨SynLogicæ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ•°æ®é›†åŸºäº7Bå’Œ32Bæ¨¡å‹ã€‚åœ¨å¼€æºæ•°æ®é›†ä¸­ï¼ŒSynLogicåœ¨é€»è¾‘æ¨ç†æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨BBEHä¸Šæ¯”DeepSeek-R1-Distill-Qwen-32Bé«˜å‡º6åˆ†ã€‚æ­¤å¤–ï¼Œå°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆåœ¨ä¸€èµ·ï¼Œå¯ä»¥æé«˜è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ··åˆè®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†DeepSeek-R1-Zero-Qwen-32Bã€‚è¿™äº›å‘ç°ä½¿SynLogicæˆä¸ºæ¨åŠ¨LLMæ›´å¹¿æ³›æ¨ç†èƒ½åŠ›çš„é‡è¦èµ„æºã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/SynLogic">https://github.com/MiniMax-AI/SynLogic</a>å…¬å¼€äº†æ•°æ®åˆæˆç®¡é“å’ŒSynLogicæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19641v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SynLogicæ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºç”Ÿæˆå¤§è§„æ¨¡çš„é€»è¾‘æ¨ç†æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¡«è¡¥äº†é€šç”¨æ¨ç†èƒ½åŠ›å‘å±•æ–¹æ³•å’Œèµ„æºçš„ç©ºç¼ºã€‚æœ¬æ–‡æˆåŠŸå¼€å‘å‡ºå¤šç§é€»è¾‘ä»»åŠ¡çš„åˆæˆæ•°æ®ï¼Œé€šè¿‡è°ƒæ•´éš¾åº¦å’Œæ•°é‡ï¼Œå®ç°å¯¹æ•°æ®çš„å¯æ§åˆæˆã€‚è¯¥æ•°æ®é›†éªŒè¯å¥–åŠ±çš„å¯éªŒè¯æ€§ä½¿å…¶æˆä¸ºå¼ºåŒ–å­¦ä¹ çš„ç†æƒ³é€‰æ‹©ã€‚å®éªŒè¡¨æ˜ï¼ŒSynLogicæ•°æ®é›†åœ¨å¼€æºæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é€»è¾‘æ¨ç†æ€§èƒ½ï¼Œæ··åˆæ•°å­¦å’Œç¼–ç ä»»åŠ¡çš„æ•°æ®å¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†å·²å¼€æºå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SynLogicæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¤§è§„æ¨¡é€»è¾‘æ¨ç†æ•°æ®çš„åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SynLogicå¡«è¡¥äº†é€šç”¨æ¨ç†èƒ½åŠ›å‘å±•æ–¹æ³•å’Œèµ„æºçš„ç©ºç¼ºã€‚</li>
<li>æ•°æ®é›†å…·æœ‰å¯æ§çš„åˆæˆæ–¹å¼ï¼Œå¯ä»¥è°ƒæ•´éš¾åº¦å’Œæ•°é‡ã€‚</li>
<li>æ•°æ®é›†é€‚ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œå› ä¸ºéªŒè¯å¥–åŠ±çš„å¯éªŒè¯æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSynLogicåœ¨å¼€æºæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„é€»è¾‘æ¨ç†æ€§èƒ½ã€‚</li>
<li>æ··åˆæ•°å­¦å’Œç¼–ç ä»»åŠ¡çš„æ•°æ®å¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ae1b7b3e50d2b64b7b82db2ac7d4f05d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae46e603d036b51c6aee01664b142f07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ee478aea4d7b9a46e198f0148d35776.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80d2d1aeb165ce18fca2291b8fbcf6cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce1ec7471c9c52fc4eb01fcfddb11526.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Moderating-Harm-Benchmarking-Large-Language-Models-for-Cyberbullying-Detection-in-YouTube-Comments"><a href="#Moderating-Harm-Benchmarking-Large-Language-Models-for-Cyberbullying-Detection-in-YouTube-Comments" class="headerlink" title="Moderating Harm: Benchmarking Large Language Models for Cyberbullying   Detection in YouTube Comments"></a>Moderating Harm: Benchmarking Large Language Models for Cyberbullying   Detection in YouTube Comments</h2><p><strong>Authors:Amel Muminovic</strong></p>
<p>As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohenâ€™s kappa &#x3D; 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall &#x3D; 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation. </p>
<blockquote>
<p>éšç€åœ¨çº¿å¹³å°çš„å¢é•¿ï¼Œè¯„è®ºåŒºè¶Šæ¥è¶Šå¤šåœ°å‡ºç°éªšæ‰°è¡Œä¸ºï¼Œè¿™ç ´åäº†ç”¨æˆ·ä½“éªŒå’Œç¦ç¥‰ã€‚æœ¬ç ”ç©¶ä»¥ä¸‰ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”OpenAI GPT-4.1ã€Google Gemini 1.5 Proå’ŒAnthropic Claude 3 Opusä¸ºåŸºå‡†ï¼Œå¯¹ä»æ¸¸æˆã€ç”Ÿæ´»æ–¹å¼ã€ç¾é£Ÿåšå®¢å’ŒéŸ³ä¹é¢‘é“ä¸­é«˜åº¦æ»¥ç”¨è¨€è®ºçš„çº¿ç¨‹ä¸­é‡‡é›†çš„5,080æ¡YouTubeè¯„è®ºè¿›è¡Œäº†è¯„ä¼°ã€‚è¯¥æ•°æ®é›†åŒ…å«ç”¨è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œå°åº¦å°¼è¥¿äºšè¯­ç¼–å†™çš„1,334æ¡æœ‰å®³å’Œ3,746æ¡æ— å®³æ¶ˆæ¯ï¼Œç”±ä¸¤åè¯„å®¡å‘˜ç‹¬ç«‹æ³¨é‡Šï¼Œä¸¤äººä¹‹é—´è¾¾æˆäº†å¤§é‡å…±è¯†ï¼ˆCohençš„kappaå€¼ä¸º0.83ï¼‰ã€‚ä½¿ç”¨ç»Ÿä¸€çš„æç¤ºå’Œç¡®å®šæ€§è®¾ç½®ï¼ŒGPT-4.1å–å¾—äº†æœ€ä½³çš„æ•´ä½“å¹³è¡¡ï¼ŒF1å¾—åˆ†ä¸º0.863ï¼Œç²¾ç¡®åº¦ä¸º0.887ï¼Œå¬å›ç‡ä¸º0.841ã€‚Geminiæ ‡è®°çš„æœ‰å®³å¸–å­æ¯”ä¾‹æœ€é«˜ï¼ˆå¬å›ç‡&#x3D; 0.875ï¼‰ï¼Œä½†ç”±äºé¢‘ç¹å‡ºç°è¯¯æŠ¥ï¼Œå…¶ç²¾ç¡®åº¦é™è‡³0.767ã€‚Claudeçš„ç²¾ç¡®åº¦æœ€é«˜ï¼Œè¾¾åˆ°0.920ï¼Œè¯¯æŠ¥ç‡æœ€ä½ï¼Œä¸º0.022ï¼Œä½†å¬å›ç‡é™è‡³0.720ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹åœ¨åº”å¯¹è®½åˆºã€ç¼–ç ä¾®è¾±å’Œæ··åˆè¯­è¨€ä¿šè¯­æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ã€‚è¿™äº›ç»“æœå¼ºè°ƒéœ€è¦é‡‡ç”¨ç»“åˆäº’è¡¥æ¨¡å‹ã€èå…¥å¯¹è¯è¯­å¢ƒã€é’ˆå¯¹ä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€å’Œéšæ€§æ»¥ç”¨è¿›è¡Œå¾®è°ƒçš„å†…å®¹è¿‡æ»¤ç®¡é“ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œåœ¨è‡ªåŠ¨å†…å®¹è¿‡æ»¤æ–¹é¢çš„è¿›ä¸€æ­¥è¿›å±•ï¼Œå‘å¸ƒäº†ä¸€ä¸ªåŒ¿ååŒ–çš„æ•°æ®é›†å’Œå®Œæ•´çš„æç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18927v2">PDF</a> Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a   journal. Feedback welcome</p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿å¹³å°å‘å±•è¿…çŒ›ï¼Œè¯„è®ºåŒºçš„éªšæ‰°è¡Œä¸ºæ—¥ç›Šå¢å¤šï¼Œå½±å“ç”¨æˆ·ä½“éªŒå’Œèº«å¿ƒå¥åº·ã€‚æœ¬ç ”ç©¶å¯¹ä¸‰å¤§ä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹OpenAI GPT-4.1ã€Google Gemini 1.5 Proå’ŒAnthropic Claude 3 Opusè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæµ‹è¯•æ ·æœ¬å–è‡ªYouTubeçš„é«˜æ»¥ç”¨è¯„è®ºçº¿ç¨‹ï¼ŒåŒ…æ‹¬æ¸¸æˆã€ç”Ÿæ´»æ–¹å¼ã€ç¾é£Ÿåšå®¢å’ŒéŸ³ä¹é¢‘é“ç­‰ã€‚æ•°æ®é›†åŒ…å«ç‹¬ç«‹æ ‡æ³¨çš„1334æ¡æœ‰å®³ä¿¡æ¯å’Œ3746æ¡æ— å®³ä¿¡æ¯ï¼Œæ¶‰åŠè‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œå°å°¼è¯­ç­‰è¯­ç§ï¼Œæ ‡æ³¨äººå‘˜é—´çš„å®è´¨ä¸€è‡´æ€§è¾ƒå¼ºï¼ˆCohençš„kappaç³»æ•°ä¸º0.83ï¼‰ã€‚é€šè¿‡ç»Ÿä¸€çš„æç¤ºå’Œç¡®å®šæ€§è®¾ç½®ï¼ŒGPT-4.1åœ¨æ€»ä½“å¹³è¡¡æ–¹é¢è¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°ä¸º0.863ï¼Œç²¾ç¡®åº¦ä¸º0.887ï¼Œå¬å›ç‡ä¸º0.841ã€‚Geminiæ£€æµ‹åˆ°æœ‰å®³å¸–å­çš„æ¯”ä¾‹æœ€é«˜ï¼ˆå¬å›ç‡ä¸º0.875ï¼‰ï¼Œä½†å› å…¶äº§ç”Ÿè¾ƒå¤šçš„å‡é˜³æ€§ç»“æœå¯¼è‡´ç²¾ç¡®åº¦é™è‡³0.767ã€‚Claudeæ‹¥æœ‰æœ€é«˜çš„ç²¾ç¡®åº¦ï¼ˆè¾¾åˆ°0.92ï¼‰ï¼Œå¹¶ä¸”è¯¯æŠ¥ç‡æœ€ä½ï¼ˆä»…ä¸º0.022ï¼‰ï¼Œä½†å…¶å¬å›ç‡é™è‡³æœ€ä½ä¸º0.72ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤„ç†è®½åˆºæ€§è¯­è¨€ã€ä¼ªè£…æ€§è¾±éª‚ä»¥åŠæ··åˆè¯­è¨€ä¿šè¯­æ—¶éƒ½æœ‰å›°éš¾ã€‚è¿™æé†’æˆ‘ä»¬éœ€è¦é‡‡å–ä¸€äº›ç®¡ç†ç­–ç•¥ï¼Œç»“åˆäº’è¡¥æ¨¡å‹ï¼Œèå…¥å¯¹è¯è¯­å¢ƒï¼Œå¹¶é’ˆå¯¹éšæ€§æ»¥ç”¨ä»¥åŠä½ä»£è¡¨æ€§è¯­è¨€è¿›è¡Œå¾®è°ƒã€‚æ•°æ®é›†çš„éæ ‡è¯†ç‰ˆæœ¬ä»¥åŠå®Œæ•´æç¤ºå·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œè‡ªåŠ¨åŒ–å†…å®¹ç®¡ç†çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿å¹³å°è¯„è®ºåŒºéªšæ‰°è¡Œä¸ºæ™®éï¼Œå½±å“ç”¨æˆ·ä½“éªŒå’Œèº«å¿ƒå¥åº·ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠä¸‰å¤§ä¸»æµå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œæµ‹è¯•æ ·æœ¬å–è‡ªé«˜æ»¥ç”¨è¯„è®ºçº¿ç¨‹ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§è¯­è¨€å’Œæ ‡æ³¨ç»“æœï¼Œæ ‡æ³¨äººå‘˜é—´å®è´¨ä¸€è‡´æ€§è¾ƒå¼ºã€‚</li>
<li>GPT-4.1åœ¨æ€»ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†å„æ¨¡å‹åœ¨ç‰¹å®šæ–¹é¢å­˜åœ¨å·®å¼‚ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹åœ¨å¤„ç†ç‰¹å®šè¯­è¨€ç‰¹å¾ï¼ˆå¦‚è®½åˆºæ€§è¯­è¨€ã€ä¼ªè£…æ€§è¾±éª‚å’Œæ··åˆè¯­è¨€ä¿šè¯­ï¼‰æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>éœ€è¦ç»“åˆäº’è¡¥æ¨¡å‹ã€èå…¥å¯¹è¯è¯­å¢ƒçš„ç­–ç•¥è¿›è¡Œå†…å®¹ç®¡ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-093985dc3c096be41f43b19f5bf0c3c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee25d339e7eb67fa70f70499aa5576d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e37b6e28bca8627d3e8f71145a8e004.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-916898ae48602ab97222a141fcdceb42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea7e89b0e74e7b4f58d41510ba8c0c25.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts"><a href="#Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts" class="headerlink" title="Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts"></a>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h2><p><strong>Authors:Qi Feng</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½ç„¶åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹äºå¤„ç†ç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„è§†è§‰ç©ºé—´è®¤çŸ¥ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸ç¼ºä¹å¿…è¦çš„æ¶æ„ç»„ä»¶å’Œç²¾ç»†ç©ºé—´ç†è§£æ‰€éœ€çš„ä¸“é—¨è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å¼•å…¥äº†ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MLLMï¼Œæ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ViCA2é‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆSigLIPè¿›è¡Œè¯­ä¹‰åˆ†æï¼Œå¹¶é‡‡ç”¨Hieraè¿›è¡Œç©ºé—´ç»“æ„å¤„ç†ï¼ŒåŒæ—¶é‡‡ç”¨ä»¤ç‰Œæ¯”ç‡æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ViCA-322Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡å¯¹ç©ºé—´å®šä½çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œç”¨äºæœ‰é’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒæ•´ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹è¾¾åˆ°äº†å¹³å‡å¾—åˆ†56.8çš„ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—è¶…è¿‡äº†å…¶ä»–å¤§å‹å¼€æºæ¨¡å‹ï¼ˆä¾‹å¦‚LLaVA-NeXT-Video-72Bï¼Œå¾—åˆ†ä¸º40.9ï¼‰å’Œé¢†å…ˆçš„ä¸“ä¸šæ¨¡å‹ï¼ˆGemini-1.5 Proï¼Œå¾—åˆ†ä¸º45.4ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°å¼ºå¤§è§†è§‰ç©ºé—´æ™ºèƒ½çš„ç´§å‡‘æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘å¸ƒViCA2ã€å…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12363v3">PDF</a> In version 1, Hidetoshi Shimodaira was included as a co-author   without their consent and has been removed from the author list</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸Šè¿°æ–‡æœ¬å†…å®¹ï¼Œå…¶æ ¸å¿ƒæ‘˜è¦ä¸ºï¼šå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„è§†è§‰ç©ºé—´è®¤çŸ¥ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ¨å‡ºäº†æ–°å‹MLLMâ€”â€”ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸“é—¨çš„æ¶æ„ç»„ä»¶å’Œæ•°æ®é›†ViCA-322Kæ¥å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒViCA2æ¨¡å‹è¡¨ç°å“è¶Šï¼Œå¹³å‡å¾—åˆ†è¾¾åˆ°äº†å…ˆè¿›çš„æ°´å¹³ã€‚å®ƒçš„ä»£ç å’ŒViCA-322Kæ•°æ®é›†å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›åç»­ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç©ºé—´è®¤çŸ¥æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†æ–°å‹MLLMâ€”â€”ViCA2ï¼Œç”¨äºå¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ViCA2å…·æœ‰åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œèåˆäº†SigLIPè¯­ä¹‰å’ŒHieraç©ºé—´ç»“æ„æŠ€æœ¯ã€‚</li>
<li>ViCA-322Kæ•°æ®é›†çš„æ¨å‡ºæ˜¯ä¸ºäº†æä¾›ä¸“é—¨è®­ç»ƒæ•°æ®ä»¥æ”¯æŒç²¾ç»†åŒ–ç©ºé—´ç†è§£ã€‚</li>
<li>ViCA2æ¨¡å‹åœ¨VSI-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå¹³å‡å¾—åˆ†è¶…è¿‡å…¶ä»–å¤§å‹å¼€æºæ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c0a60dc96f980780755856a47f5f70f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ddf487f5a21022b3d8cb8abb6ebc1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b49a1d2f8faf93399797eb2a9195519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94de87aa0b44a40c568d427a06ae798c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Beyond-â€˜Aha-â€™-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models"><a href="#Beyond-â€˜Aha-â€™-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models" class="headerlink" title="Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models"></a>Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models</h2><p><strong>Authors:Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li</strong></p>
<p>Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the modelâ€™s â€œaha momentâ€. However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMsâ€™ reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental â€œaha momentsâ€. Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional gain in performance ceiling for both 7B and 32B models across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment">https://github.com/zhiyuanhubj/Meta-Ability-Alignment</a> </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²ç»å…·å¤‡äº†æ½œåœ¨çš„è¿ç»­é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æ—©æœŸçš„ç ”ç©¶æ˜¾ç¤ºï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿæ„å¤–åœ°æ¿€å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£ã€å›æº¯å’ŒéªŒè¯ç°è±¡ï¼Œè¿™äº›å¸¸è¢«çœ‹ä½œæ˜¯æ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–°å…´è¡Œä¸ºçš„æ—¶æœºå’Œä¸€è‡´æ€§ä»ç„¶ä¸å¯é¢„æµ‹å’Œä¸å¯æ§åˆ¶ï¼Œé™åˆ¶äº†LRMsæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸å†ä¾èµ–æç¤ºå’Œå¶ç„¶çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç›¸åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ã€å¯è‡ªæˆ‘éªŒè¯çš„ä»»åŠ¡ï¼Œæ˜ç¡®åœ°ä½¿æ¨¡å‹ä¸ä¸‰ç§å…ƒèƒ½åŠ›ï¼ˆæ¼”ç»ã€å½’çº³å’Œæº¯å› ï¼‰å¯¹é½ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸è¾ƒäºæŒ‡ä»¤è°ƒæ•´åŸºå‡†çº¿ï¼Œæ€§èƒ½æå‡äº†è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œä»å¯¹é½æ£€æŸ¥ç‚¹è¿›è¡Œçš„é¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ä¸º7Bå’Œ32Bæ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸Šé™å¸¦æ¥äº†é¢å¤–æå‡ï¼Œè¿™è¯æ˜äº†æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯é çš„æ¨ç†åŸºç¡€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhiyuanhubj/Meta-Ability-Alignmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10554v2">PDF</a> In Progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²å…·å¤‡æ½œåœ¨çš„é•¿æœŸæ€ç»´æ¨ç†èƒ½åŠ›ã€‚å…ˆå‰çš„å·¥ä½œè¡¨æ˜ï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶æ¿€å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£ã€å›æº¯å’ŒéªŒè¯ç°è±¡ï¼Œè¿™äº›å¸¸è¢«çœ‹ä½œæ˜¯æ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›çªå‘è¡Œä¸ºçš„æ—¶æœºå’Œä¸€è‡´æ€§ä»ç„¶ä¸å¯é¢„æµ‹å’Œä¸å¯æ§åˆ¶ï¼Œé™åˆ¶äº†LRMsæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬ä¸å†ä¾èµ–æç¤ºå’Œå¶ç„¶çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œè€Œæ˜¯æ˜ç¡®åœ°å°†æ¨¡å‹ä¸ä¸‰ç§å…ƒèƒ½åŠ›ï¼ˆæ¼”ç»ã€å½’çº³å’Œæº¯å› ï¼‰å¯¹é½ï¼Œä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ã€å¯è‡ªæˆ‘éªŒè¯çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸è¾ƒäºæŒ‡ä»¤è°ƒæ•´åŸºå‡†çº¿ï¼Œæ€§èƒ½æå‡è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œä»å¯¹é½æ£€æŸ¥ç‚¹è¿›è¡Œçš„é¢†åŸŸç‰¹å®šRLä¸º7Bå’Œ32Bæ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸Šé™å¸¦æ¥äº†é¢å¤–æ”¶ç›Šï¼Œè¯æ˜æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å·²å…·å¤‡æ½œåœ¨é•¿æœŸæ€ç»´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ èƒ½æ¿€å‘æ¨¡å‹çš„é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’ŒéªŒè¯ã€‚</li>
<li>æ¨¡å‹çš„çªå‘æ¨ç†è¡Œä¸ºæ—¶æœºå’Œä¸€è‡´æ€§éš¾ä»¥é¢„æµ‹å’Œæ§åˆ¶ã€‚</li>
<li>é€šè¿‡æ˜ç¡®ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ç­‰å…ƒèƒ½åŠ›å¯¹é½ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ã€å¯è‡ªæˆ‘éªŒè¯çš„ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06ccda022f12fc421fd1d27803c963f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05c9191149f0e07ab5e9327a2ffddf0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67364ee229c668a9d57b2917a5199f2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e47eedfe9ea54088ffebaa4165e57f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa71ae42585f5d0c74b2f959e8c0f52a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Intra-Layer-Recurrence-in-Transformers-for-Language-Modeling"><a href="#Intra-Layer-Recurrence-in-Transformers-for-Language-Modeling" class="headerlink" title="Intra-Layer Recurrence in Transformers for Language Modeling"></a>Intra-Layer Recurrence in Transformers for Language Modeling</h2><p><strong>Authors:Anthony Nguyen, Wenjun Lin</strong></p>
<p>Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures. </p>
<blockquote>
<p>Transformeræ¨¡å‹å·²åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œå…¶æ·±åº¦å¢åŠ å¯¼è‡´å‚æ•°æ•°é‡å¤§å¹…å¢åŠ ã€‚ç°æœ‰çš„å¾ªç¯Transformeræ–¹æ³•é€šè¿‡å¤šæ¬¡é‡æ–°å¤„ç†å±‚æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¸¸å¸¸ä¸åŠ åŒºåˆ«åœ°åœ¨æ•´ä¸ªå±‚å—ä¸­åº”ç”¨å¾ªç¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å±‚å†…å¾ªç¯ï¼ˆILRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´æœ‰é’ˆå¯¹æ€§çš„æ–¹æ³•ï¼Œå®ƒé€‰æ‹©åœ¨å•ä¸ªå‰å‘ä¼ é€’è¿‡ç¨‹ä¸­çš„ä¸ªåˆ«å±‚ä¸Šåº”ç”¨å¾ªç¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¯¹æ—©æœŸå±‚åˆ†é…æ›´å¤šè¿­ä»£æ¬¡æ•°å¯ä»¥è·å¾—æœ€ä½³ç»“æœã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒILRä¸ºä¼˜åŒ–Transformeræ¶æ„ä¸­çš„å¾ªç¯ç»“æ„æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01855v2">PDF</a> Accepted at Canadian AI 2025. Code available at   <a target="_blank" rel="noopener" href="https://github.com/ant-8/Layer-Recurrent-Transformers">https://github.com/ant-8/Layer-Recurrent-Transformers</a></p>
<p><strong>Summary</strong><br>     å˜æ¢å™¨æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæ ‘ç«‹äº†æ–°çš„åŸºå‡†ï¼Œä½†å…¶æ·±åº¦å¢åŠ å¯¼è‡´å‚æ•°æ•°é‡å¤§å¹…å¢é•¿ã€‚ç°æœ‰å¾ªç¯å˜æ¢å™¨æ–¹æ³•é€šè¿‡å¤šæ¬¡é‡æ–°å¤„ç†å±‚æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¸¸å¸¸åœ¨æ•´ä¸ªå±‚å—ä¸Šç›²ç›®åº”ç”¨å¾ªç¯ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å•å‘å‰ä¼ ä¸­çš„é€‰æ‹©æ€§å±‚å†…å¾ªç¯ï¼ˆILRï¼‰ï¼Œæ›´æœ‰é’ˆå¯¹æ€§åœ°åº”ç”¨äºå•ä¸ªå±‚ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ºæ—©æœŸå±‚åˆ†é…æ›´å¤šè¿­ä»£æ¬¡æ•°å¯è·å¾—æœ€ä½³ç»“æœã€‚è¿™è¡¨æ˜ILRä¸ºä¼˜åŒ–å˜æ¢å™¨æ¶æ„ä¸­çš„å¾ªç¯ç»“æ„æä¾›äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜æ¢å™¨æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†æ·±åº¦å¢åŠ å¯¼è‡´å‚æ•°æ•°é‡å¢é•¿ã€‚</li>
<li>ç°æœ‰å¾ªç¯å˜æ¢å™¨æ–¹æ³•é‡æ–°å¤„ç†å±‚æ¥è§£å†³å‚æ•°å¢é•¿é—®é¢˜ï¼Œä½†æ–¹æ³•è¾ƒä¸ºç›²ç›®ã€‚</li>
<li>ç ”ç©¶æå‡ºå±‚å†…å¾ªç¯ï¼ˆILRï¼‰ä½œä¸ºæ›´æœ‰é’ˆå¯¹æ€§çš„æ–¹æ³•ï¼Œé€‰æ‹©æ€§åº”ç”¨äºå•ä¸ªå±‚ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¯¹æ—©æœŸå±‚åˆ†é…æ›´å¤šè¿­ä»£æ¬¡æ•°å¯ä»¥è·å¾—æœ€ä½³ç»“æœã€‚</li>
<li>ILRä¸ºä¼˜åŒ–å˜æ¢å™¨æ¶æ„ä¸­çš„å¾ªç¯ç»“æ„æä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹æ€§åº”ç”¨å¾ªç¯ç»“æ„çš„é‡è¦æ€§ï¼Œè€Œéç›²ç›®åœ°åº”ç”¨äºæ•´ä¸ªå±‚å—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4561951ec6f1f3151618cf42a1d9f61c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec57c9faa6aac953b6e7ef0c7649927b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4a5c98fe99c20bf118440d8de5d949.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3af3136c99ed2daebc8c17c3b6b1fcf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a7e2ddf51e5331a68a55fec76f64d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c874685dbe8f901a5c03c47446350852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3deb02988fd0fc6e06e1b543390bfd1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb879eecd7b40d13f50a85478ac86e54.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers"><a href="#When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers" class="headerlink" title="When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers"></a>When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers</h2><p><strong>Authors:Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen</strong></p>
<p>Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B). </p>
<blockquote>
<p>ä»»åŠ¡ç®—æœ¯æ˜¯æŒ‡é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡çš„åŠ æƒå’Œæ¥ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªä»»åŠ¡å‘é‡éƒ½æ˜¯é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹çš„æƒé‡æ›´æ–°ã€‚æœ€è¿‘ï¼Œè¿™ç§æ–¹æ³•ä½œä¸ºä¸€ç§è®¡ç®—é«˜æ•ˆçš„æ¨ç†æ–¹æ³•å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œä¾‹å¦‚ç”¨äºæ¨¡å‹ç¼–è¾‘çš„å¤šä»»åŠ¡å­¦ä¹ ã€é—å¿˜å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒTransformeræ¨¡å‹çš„é«˜åº¦éå‡¸æ€§ï¼Œå…³äºä»»åŠ¡å‘é‡å¦‚ä½•æ‰§è¡Œå„ç§æ¦‚å¿µæ“ä½œçš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡é¦–æ¬¡å¯¹ä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–ä¿è¯è¿›è¡Œäº†ç†è®ºæè¿°ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ¦‚å¿µå­¦ä¹ åœºæ™¯ï¼Œå…¶ä¸­æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯åŸºäºåˆ¤åˆ«æ¨¡å¼çš„äºŒåˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†åŒæ—¶å­¦ä¹ ä¸€ç»„ä¸ç›¸å…³æˆ–å¯¹é½çš„ä»»åŠ¡æ—¶æ·»åŠ ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠä»æ— å…³æˆ–çŸ›ç›¾çš„ä»»åŠ¡ä¸­é—å¿˜ä¸€ä¸ªä»»åŠ¡æ—¶å¦å®šä»»åŠ¡çš„æˆåŠŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸ºä»»åŠ¡ç®—æœ¯é€‰æ‹©åˆé€‚çš„çº¿æ€§ç³»æ•°ï¼Œä»¥å®ç°è·¨åŸŸä»»åŠ¡çš„ä¿è¯æ³›åŒ–ã€‚æˆ‘ä»¬æ‰€æœ‰çš„ç†è®ºç»“æœéƒ½é€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼ã€‚è™½ç„¶æ˜¯åœ¨æ¦‚å¿µä¸Šå»ºç«‹çš„ï¼Œä½†æˆ‘ä»¬çš„ç†è®ºå‘ç°å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ï¼ˆ1.3Bï¼‰çš„å®é™…æœºå™¨é—å¿˜ä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10957v3">PDF</a> Published at ICLR 2025 as an oral paper</p>
<p><strong>Summary</strong>ï¼š<br>ä»»åŠ¡ç®—æœ¯é€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç¼–è¾‘ï¼Œæ·»åŠ ä»»åŠ¡å‘é‡æ¥å®ç°å¤šä»»åŠ¡å­¦ä¹ ã€é—å¿˜å’Œè·¨åŸŸæ³›åŒ–ç­‰èƒ½åŠ›ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹ä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–ä¿è¯è¿›è¡Œäº†ç†è®ºè¡¨å¾ï¼Œè¯æ˜äº†ä»»åŠ¡æ·»åŠ å’Œå¦å®šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å®ç°äº†å¯¹åŸŸå¤–ä»»åŠ¡çš„æ³›åŒ–ä¿è¯ã€‚ç›¸å…³ç†è®ºç»“æœé€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼ã€‚è¿™äº›ç†è®ºå‘ç°å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»»åŠ¡ç®—æœ¯é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ç°è®¡ç®—é«˜æ•ˆçš„å¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å¯¹ä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–ä¿è¯è¿›è¡Œäº†ç†è®ºè¡¨å¾ã€‚</li>
<li>è¯æ˜äº†ä»»åŠ¡æ·»åŠ å’Œå¦å®šçš„æœ‰æ•ˆæ€§ï¼Œå¯ä»¥åŒæ—¶å­¦ä¹ ä¸€ç»„ä¸ç›¸å…³æˆ–å¯¹é½çš„ä»»åŠ¡ï¼Œå¹¶ä»ä¸ç›¸å…³æˆ–çŸ›ç›¾çš„ä»»åŠ¡ä¸­é—å¿˜ä¸€ä¸ªä»»åŠ¡ã€‚</li>
<li>é€šè¿‡é€‚å½“é€‰æ‹©çº¿æ€§ç³»æ•°ï¼Œå¯ä»¥å®ç°åŸŸå¤–ä»»åŠ¡çš„æ³›åŒ–ä¿è¯ã€‚</li>
<li>ç†è®ºç»“æœé€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼ã€‚</li>
<li>è¿™äº›ç†è®ºå‘ç°å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e7f7fb2532b3de9df20cc38fbaf06b18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9dae42c3036b8c360443b59a86d070.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0789da0de26cf4a5d5e3cd25d4ea2690.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible â€œdark patternsâ€ in LLMsâ€™ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local â€œsafety regionsâ€ in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shiftsâ€“a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨åŒ–çš„åŸºç¡€æ¢ç´¢ï¼Œç„¶è€Œï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å­¦ä¹ ä½¿å…¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½ä»…å®ç°äº†è¡¨é¢çš„åˆè§„æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¯æ˜é¢„è®­ç»ƒæœŸé—´åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€æŒç»­å­˜åœ¨äºLLMçš„å‚æ•°è®°å¿†ä¸­ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶é€šè¿‡æ•Œå¯¹è¯±å¯¼é‡æ–°å‡ºç°ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šåˆ†æå¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œè¯æ˜å½“å‰çš„å¯¹é½æ–¹æ³•åªèƒ½åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨çš„â€œå®‰å…¨åŒºåŸŸâ€ã€‚ç›¸åï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä»ç„¶ä¸æœ‰å®³æ¦‚å¿µå…¨çƒè¿æ¥ï¼Œé€šè¿‡é«˜æ¦‚ç‡çš„æ•Œå¯¹è½¨è¿¹ã€‚åŸºäºè¿™ä¸€ç†è®ºè§è§£ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯è¯±å¯¼æ¥å®è¯éªŒè¯æˆ‘ä»¬çš„å‘ç°â€”â€”è¿™æ˜¯ä¸€ç§é€šè¿‡ä¼˜åŒ–æ•Œå¯¹æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸçš„æ–¹æ³•ã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ä¸ªæœ€æ–°å¯¹é½LLMä¸­çš„19ä¸ªä¸Šå®ç°äº†100%çš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬DeepSeek-R1å’ŒLLaMA-3ï¼Œæ­ç¤ºäº†å…¶æ™®éå­˜åœ¨çš„è„†å¼±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨æ¢ç´¢çš„åŸºç¡€ï¼Œä½†å®ƒä»¬ä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ä»…é™äºè¡¨é¢ã€‚ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒæ—¶åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šåœ¨æ¨¡å‹å‚æ•°è®°å¿†ä¸­å½¢æˆéš¾ä»¥æ¶ˆé™¤çš„â€œæš—æ¨¡å¼â€ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶é‡æ–°æµ®ç°ã€‚ç†è®ºä¸Šåˆ†æè¡¨æ˜ï¼Œç°æœ‰å¯¹é½æ–¹æ³•åªèƒ½åœ¨çŸ¥è¯†æµå½¢ä¸­å½¢æˆå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ï¼Œè€Œé¢„è®­ç»ƒçŸ¥è¯†ä»ä¸æœ‰å®³æ¦‚å¿µå…¨çƒè¿æ¥ã€‚é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸçš„è¯­ä¹‰è¿è´¯è¯±å¯¼æ–¹æ³•ï¼Œå¯¹ç†è®ºè§è§£è¿›è¡Œå®è¯éªŒè¯ï¼ŒæˆåŠŸæ”»å‡»äº†æ‰€æœ‰23æ¬¾æœ€å…ˆè¿›çš„å¯¹é½LLMä¸­çš„åä¹æ¬¾ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ™®éè„†å¼±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¯¹æ¥ä»…é™äºè¡¨é¢ã€‚</li>
<li>é¢„è®­ç»ƒæ—¶åµŒå…¥çš„æœ‰å®³çŸ¥è¯†å½¢æˆéš¾ä»¥æ¶ˆé™¤çš„â€œæš—æ¨¡å¼â€ã€‚</li>
<li>è¿™äº›æš—æ¨¡å¼åœ¨åˆ†å¸ƒè½¬ç§»æ—¶å¯èƒ½é‡æ–°å‡ºç°å¹¶äº§ç”Ÿå½±å“ã€‚</li>
<li>å½“å‰çš„å¯¹é½æ–¹æ³•åªåœ¨çŸ¥è¯†æµå½¢ä¸­åˆ›å»ºå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ã€‚</li>
<li>é¢„è®­ç»ƒçš„çŸ¥è¯†ä¸æœ‰å®³æ¦‚å¿µåœ¨å…¨çƒèŒƒå›´å†…ä¿æŒè”ç³»ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æç¤ºç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸçš„æ–¹æ³•æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96061ab5e1ccac8a4754a137b3272741.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aa8dac1c9fd762afe6ad13233b81cc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8894cc97e6192748906cfca91b3ed446.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TLUE-A-Tibetan-Language-Understanding-Evaluation-Benchmark"><a href="#TLUE-A-Tibetan-Language-Understanding-Evaluation-Benchmark" class="headerlink" title="TLUE: A Tibetan Language Understanding Evaluation Benchmark"></a>TLUE: A Tibetan Language Understanding Evaluation Benchmark</h2><p><strong>Authors:Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Hao Wang Xiao Feng, Yongbin Yu</strong></p>
<p>Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchmark), the first large-scale benchmark for assessing LLMsâ€™ capabilities in Tibetan. TLUE comprises two major components: (1) a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a diverse set of state-of-the-art LLMs. Experimental results demonstrate that most LLMs perform below the random baseline, highlighting the considerable challenges LLMs face in processing Tibetan, a low-resource language. TLUE provides an essential foundation for driving future research and progress in Tibetan language understanding and underscores the need for greater inclusivity in LLM development. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†åœ¨å¯¹ä½èµ„æºè¯­è¨€å¦‚è—è¯­çš„è¯„ä¼°ä¸­ä»å­˜åœ¨æ˜¾è‘—çš„ä»£è¡¨æ€§ä¸è¶³ã€‚å°½ç®¡æœ‰è¶…è¿‡ä¸ƒç™¾ä¸‡äººåœ¨ä½¿ç”¨è—è¯­ï¼Œä½†åœ¨LLMçš„å¼€å‘å’Œè¯„ä¼°ä¸­ï¼Œè—è¯­å´è¢«å¤§å¤§å¿½è§†äº†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TLUEï¼ˆè—è¯­ç†è§£è¯„ä¼°åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯è¯„ä¼°LLMåœ¨è—è¯­èƒ½åŠ›æ–¹é¢çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚TLUEåŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰æ¶µç›–5ä¸ªé¢†åŸŸå’Œ67ä¸ªå­é¢†åŸŸçš„ç»¼åˆå¤šä»»åŠ¡ç†è§£åŸºå‡†ï¼›ï¼ˆ2ï¼‰æ¶µç›–7ä¸ªå­é¢†åŸŸçš„å®‰å…¨åŸºå‡†ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„LLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°LLMçš„è¡¨ç°ä½äºéšæœºåŸºçº¿æ°´å¹³ï¼Œè¿™çªæ˜¾äº†LLMåœ¨å¤„ç†ä½èµ„æºè¯­è¨€è—è¯­æ—¶æ‰€é¢ä¸´çš„å·¨å¤§æŒ‘æˆ˜ã€‚TLUEä¸ºè—è¯­ç†è§£çš„æœªæ¥ç ”ç©¶å’Œè¿›æ­¥æä¾›äº†é‡è¦çš„åŸºç¡€ï¼Œå¹¶å¼ºè°ƒäº†LLMå‘å±•ä¸­æ›´å¤§çš„åŒ…å®¹æ€§çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12051v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>LLMåœ¨è—è¯­ç­‰å¤§èµ„æºè¯­è¨€æ–¹é¢çš„è¿›å±•æœ‰é™ï¼Œä¸ºæ­¤æå‡ºTLUEåŸºå‡†æµ‹è¯•ä»¥è¯„ä¼°æ¨¡å‹åœ¨è—è¯­æ–¹é¢çš„èƒ½åŠ›ã€‚TLUEåŒ…å«å¤šä»»åŠ¡ç†è§£å’Œå®‰å…¨åŸºå‡†æµ‹è¯•ä¸¤éƒ¨åˆ†ï¼Œå¯¹å¤šä¸ªé¢†åŸŸçš„è—è¯­ç†è§£è¿›è¡Œè¯„ä¼°ã€‚å¤§éƒ¨åˆ†LLMè¡¨ç°ä¸ä½³ï¼Œå‡¸æ˜¾å…¶åœ¨å¤„ç†ä½èµ„æºè¯­è¨€æ—¶çš„æŒ‘æˆ˜ã€‚TLUEä¸ºè—è¯­ç†è§£çš„æœªæ¥ç ”ç©¶å’Œå‘å±•å¥ å®šåŸºç¡€ï¼Œå¹¶å¼ºè°ƒLLMå¼€å‘éœ€è¦æ›´å¤šåŒ…å®¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨è—è¯­ç­‰å¤§èµ„æºè¯­è¨€æ–¹é¢çš„è¿›å±•æœ‰é™ã€‚</li>
<li>æå‡ºTLUEåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è—è¯­æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>TLUEåŒ…å«å¤šä»»åŠ¡ç†è§£å’Œå®‰å…¨åŸºå‡†æµ‹è¯•ä¸¤éƒ¨åˆ†ã€‚</li>
<li>TLUEæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œè¯„ä¼°è—è¯­çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>å¤§éƒ¨åˆ†LLMåœ¨å¤„ç†è—è¯­æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¤„ç†ä½èµ„æºè¯­è¨€å¯¹LLMå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d92e4d4d2d8bcd2a428601fa7dea2ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aedd5b2cf3e5f3f2de16ee83938ad8d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9277c13317204bba1fa97b20e4b8302.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-bb307dc4e1461f88213a6d3aa7fc133c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Position Uncertainty Quantification Needs Reassessment for   Large-language Model Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3680abbc5a69eaec1fad8570334733c8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  The Climb Carves Wisdom Deeper Than the Summit On the Noisy Rewards in   Learning to Reason
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
