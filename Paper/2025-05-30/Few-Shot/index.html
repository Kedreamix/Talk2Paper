<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-30  Mastering Agile Tasks with Limited Trials">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8bf2b505a75d0ad66863efb87ec4357c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-30-更新"><a href="#2025-05-30-更新" class="headerlink" title="2025-05-30 更新"></a>2025-05-30 更新</h1><h2 id="Mastering-Agile-Tasks-with-Limited-Trials"><a href="#Mastering-Agile-Tasks-with-Limited-Trials" class="headerlink" title="Mastering Agile Tasks with Limited Trials"></a>Mastering Agile Tasks with Limited Trials</h2><p><strong>Authors:Yihang Hu, Pingyue Sheng, Shengjie Wang, Yang Gao</strong></p>
<p>Embodied robots nowadays can already handle many real-world manipulation tasks. However, certain other real-world tasks (e.g., shooting a basketball into a hoop) are highly agile and require high execution precision, presenting additional challenges for methods primarily designed for quasi-static manipulation tasks. This leads to increased efforts in costly data collection, laborious reward design, or complex motion planning. Such tasks, however, are far less challenging for humans. Say a novice basketball player typically needs only $\sim$10 attempts to make their first successful shot, by roughly imitating a motion prior and then iteratively adjusting their motion based on the past outcomes. Inspired by this human learning paradigm, we propose the Adaptive Diffusion Action Plannin (ADAP) algorithm, a simple &amp; scalable approach which iteratively refines its action plan by few real-world trials within a learned prior motion pattern, until reaching a specific goal. Experiments demonstrated that ADAP can learn and accomplish a wide range of goal-conditioned agile dynamic tasks with human-level precision and efficiency directly in real-world, such as throwing a basketball into the hoop in fewer than 10 trials. Project website:<a target="_blank" rel="noopener" href="https://adap-robotics.github.io/">https://adap-robotics.github.io/</a> . </p>
<blockquote>
<p>现今，嵌入式机器人已经能够处理许多现实世界的操作任务。然而，某些其他现实世界的任务（例如，将篮球投进篮筐）高度敏捷且需要高精度的执行，对于主要为静态操纵任务设计的方法而言，这些任务带来了额外的挑战。这导致了成本高昂的数据收集、繁琐的奖励设计和复杂的运动规划工作量的增加。然而，对于人类来说，这样的任务并不那么具有挑战性。例如，新手篮球运动员通常需要大约十次尝试才能打出第一个成功的投篮，通过大致模仿之前的动作并不断根据过去的结果调整动作。受人类学习模式的启发，我们提出了自适应扩散动作规划（ADAP）算法，这是一种简单且可扩展的方法，通过在少数现实世界的试验中学习到的先前动作模式来迭代优化行动计划，直至达到特定目标。实验表明，ADAP能够在现实世界中直接学习和完成一系列目标导向的敏捷动态任务，达到人类水平的精度和效率，如在不到十次的尝试中将篮球投进篮筐。项目网站：[<a target="_blank" rel="noopener" href="https://adap-robotics.github.io/]">https://adap-robotics.github.io/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21916v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了自适应扩散动作规划（ADAP）算法，该算法通过模仿人类学习模式实现机器人的高效动态任务执行。在少量真实世界尝试中，机器人能迭代优化动作计划以达到特定目标，完成一系列目标导向的敏捷动态任务，如投篮等，并实现人类级别的精度和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人处理现实世界的操作任务已具有相当能力，但对于需要高敏捷度和高精度的任务仍存在挑战。</li>
<li>人类学习新任务（如篮球投篮）通常通过模仿初步动作并基于过去的结果进行迭代调整，机器人可借鉴此模式。</li>
<li>ADAP算法允许机器人在少量真实世界尝试中迭代优化动作计划，达成特定目标。</li>
<li>ADAP算法可广泛应用于各种目标导向的敏捷动态任务。</li>
<li>ADAP算法使得机器人能在真实世界中直接学习并执行任务，具有人类级别的精度和效率。</li>
<li>该算法通过简单且可扩展的方式实现机器人动作的精细化调整，提高了机器人的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21916">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-36df0c7c05366e0d1851e17d6415324e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f0dfc7303fd7aa5a6c9ebd2576a9b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d34dcd0150de54b6b11a1f6ef490acb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f50e6ca6ffead098e73018a1a8d152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d997b182cbc4c71150c97f43dec8d8ca.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Concentrate-on-Weakness-Mining-Hard-Prototypes-for-Few-Shot-Medical-Image-Segmentation"><a href="#Concentrate-on-Weakness-Mining-Hard-Prototypes-for-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical   Image Segmentation"></a>Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical   Image Segmentation</h2><p><strong>Authors:Jianchao Jiang, Haofeng Zhang</strong></p>
<p>Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a model that can perform segmentation from only a few annotated images. However, most existing prototype-based FSMIS methods generate multiple prototypes from the support image solely by random sampling or local averaging, which can cause particularly severe boundary blurring due to the tendency for normal features accounting for the majority of features of a specific category. Consequently, we propose to focus more attention to those weaker features that are crucial for clear segmentation boundary. Specifically, we design a Support Self-Prediction (SSP) module to identify such weak features by comparing true support mask with one predicted by global support prototype. Then, a Hard Prototypes Generation (HPG) module is employed to generate multiple hard prototypes based on these weak features. Subsequently, a Multiple Similarity Maps Fusion (MSMF) module is devised to generate final segmenting mask in a dual-path fashion to mitigate the imbalance between foreground and background in medical images. Furthermore, we introduce a boundary loss to further constraint the edge of segmentation. Extensive experiments on three publicly available medical image datasets demonstrate that our method achieves state-of-the-art performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jcjiang99/CoW">https://github.com/jcjiang99/CoW</a>. </p>
<blockquote>
<p>小样本医学图像分割（FSMIS）已被广泛应用于训练仅从少量标注图像中执行分割的模型。然而，大多数现有的基于原型的FSMIS方法仅通过随机抽样或局部平均从支持图像中生成多个原型，这可能导致边界模糊特别严重，因为正常特征往往构成某一特定类别的主要特征。因此，我们提议更多地关注那些对于清晰分割边界至关重要的较弱特征。具体来说，我们设计了一个支持自我预测（SSP）模块，通过比较真实的支持掩膜与全局支持原型预测的掩膜来识别这些弱特征。然后，采用硬原型生成（HPG）模块基于这些弱特征生成多个硬原型。随后，采用多相似度图融合（MSMF）模块以双路径方式生成最终的分割掩膜，以缓解医学图像中前景和背景之间的不平衡。此外，我们引入了一种边界损失来进一步约束分割的边缘。在三个公开的医学图像数据集上的大量实验表明，我们的方法达到了最先进的性能。代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/jcjiang99/CoW">https://github.com/jcjiang99/CoW</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21897v1">PDF</a> 12 pages, 9 figures, 9 tables, accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>少数样本医学图像分割（FSMIS）方法广泛应用于训练模型，仅从少量标注图像中执行分割任务。现有原型方法主要从支持图像中生成多个原型，但可能导致边界模糊。本文提出关注关键弱特征以明确分割边界，设计支持自我预测（SSP）模块识别弱特征，并采用硬原型生成（HPG）模块生成多个硬原型。此外，采用多重相似图融合（MSMF）模块生成最终分割掩膜，并引入边界损失以进一步约束分割边缘。在三个公开医学图像数据集上的实验表明，该方法达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少数样本医学图像分割（FSMIS）是训练模型进行分割的有效方法，仅需少量标注图像。</li>
<li>现有原型方法主要通过随机采样或局部平均从支持图像中生成多个原型，可能导致边界模糊。</li>
<li>本文强调关注关键弱特征以明确分割边界，设计SSP模块识别这些特征。</li>
<li>采用HPG模块生成多个硬原型，基于识别出的弱特征。</li>
<li>使用MSMF模块以双重路径方式生成最终分割掩膜，解决医学图像中前景与背景的不平衡问题。</li>
<li>引入边界损失以进一步约束分割边缘。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-112f2b22a43492f47fed31a8b49d7eea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f78e1eb789f2e7daf17f649e6c93c11b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e41ca871ea0ef05483686ab1ca4545f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation"><a href="#SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation" class="headerlink" title="SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation"></a>SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</h2><p><strong>Authors:Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone</strong></p>
<p>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a>. </p>
<blockquote>
<p>少数样本分割旨在从少量的标注样本中对未见过的目标类别进行分割。这要求机制能够在图像中识别语义相关的对象，并准确生成分割掩膜。我们注意到，借助提示和扩展机制，Segment Anything 2（SAM2）既具有强大的分割能力，又具备内置的特征匹配过程。然而，我们展示其在优化对象跟踪的特定任务线索时，其表示与这些线索纠缠在一起，这损害了其在需要更高层次语义理解的任务中的使用。我们的关键见解是，尽管SAM2具有类别无关的预训练，但它已经在其特性中编码了丰富的语义结构。我们提出了SANSA（语义对齐的Segment Anything 2），这是一个使这种潜在结构明确化的框架，并通过最少的特定任务修改使SAM2用于少数样本分割。SANSA在专门设计用于评估泛化的少数样本分割基准测试中实现了最先进的性能，在流行上下文设置中的通用方法表现优异，支持通过点、框或涂鸦进行各种提示灵活交互，并且相较于先前的方法，其速度更快、更紧凑。代码可在<a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ClaudiaCuttano/SANSA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21795v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Few-shot segmentation的目标和方法。文章指出Segment Anything 2（SAM2）虽然具有强大的分割能力和特征匹配功能，但其表示与任务特定线索纠缠在一起，影响其在需要高级语义理解的任务中的使用。文章提出SANSA框架，通过使SAM2的潜在结构显性化并对其进行最小任务特定的修改，用于少样本分割。SANSA实现了专为评估泛化能力而设计的少样本分割基准测试的最佳性能，在流行的上下文设置内优于通用方法，并支持通过各种提示进行灵活交互，同时速度更快、更紧凑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot segmentation旨在从少量标注的示例中对未见过的对象类别进行分割。</li>
<li>Segment Anything 2（SAM2）具有强大的分割能力和特征匹配功能。</li>
<li>SAM2的表示与任务特定线索纠缠在一起，影响其在高级语义理解任务中的应用。</li>
<li>SANSA框架通过使SAM2的潜在结构显性化，并对其进行最小任务特定的修改，用于少样本分割。</li>
<li>SANSA实现了专为评估泛化能力设计的少样本分割基准测试的最佳性能。</li>
<li>SANSA在流行上下文设置内的性能优于通用方法。</li>
<li>SANSA支持通过各种提示（如点、框或涂鸦）进行灵活交互，并且速度更快、更紧凑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21795">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-776453b1b78a989e79af018be9145c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf2b505a75d0ad66863efb87ec4357c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-022dba601551ed556c01f471a95365d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84853eff613e8afa690dddf511e22788.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling"><a href="#Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling" class="headerlink" title="Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling"></a>Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling</h2><p><strong>Authors:Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi</strong></p>
<p>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual&#x2F;textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a> </p>
<blockquote>
<p>视觉语言模型（VLMs）最近已被纳入多实例学习（MIL）框架，以解决对全幻灯片图像（WSI）进行少量、弱监督分类的挑战。一种关键趋势是，利用多尺度信息来更好地表示层次化的组织结构。然而，现有方法通常面临两个主要局限性：（1）同一模态内不同尺度（例如，5倍和20倍）之间交互的建模不足；（2）同一尺度上视觉和文本模态之间的对齐不足。为了解决这些差距，我们提出了HiVE-MIL，这是一个层次化的视觉语言框架，它构建了一个统一图，包括（1）粗（5倍）和细（20倍）视觉&#x2F;文本节点之间的父子链接，以捕获层次关系，以及（2）在同一尺度上连接视觉和文本节点的异构图内边。为了进一步增强语义一致性，HiVE-MIL采用了一个两阶段的文本引导动态过滤机制，该机制消除了弱相关的补丁文本对，并引入了一种层次对比损失，以对齐不同尺度的文本语义。在TCGA乳腺癌、肺癌和肾癌数据集上的大量实验表明，HiVE-MIL始终优于传统的MIL和最新的基于VLM的MIL方法，在16次拍摄的宏观F1得分提高了高达4.1%。我们的结果证明了联合建模层次结构和多模态对齐对于从有限的病理学数据中实现高效和可扩展学习的价值。代码可在<a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17982v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何将视觉语言模型（VLMs）融入多实例学习（MIL）框架，以解决少样本、弱监督分类全幻灯片图像（WSIs）的挑战。文章强调了利用多尺度信息的趋势，并指出了现有方法的两个主要局限性。为解决这些问题，本文提出了HiVE-MIL框架，通过构建统一图来捕捉层次关系，并增强语义一致性。实验表明，HiVE-MIL在TCGA乳腺癌、肺癌和肾癌数据集上表现出优异的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs被集成到MIL框架中，用于解决少样本、弱监督分类的WSI挑战。</li>
<li>利用多尺度信息成为关键趋势，但现有方法存在局限性。</li>
<li>HiVE-MIL框架通过构建统一图来捕捉层次关系，包括父子链接和异质内尺度边缘。</li>
<li>HiVE-MIL采用两阶段文本引导的动态过滤机制，增强语义一致性。</li>
<li>HiVE-MIL引入层次对比损失，对齐同一尺度的文本语义。</li>
<li>实验表明HiVE-MIL在多种数据集上表现优异，尤其是TCGA数据集。</li>
<li>代码已公开，可供进一步研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ef61629f6dbf6eabef99acea72faa7e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97dcfab98be7f160215b7823cf69fa16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9450b1afc48fc6fa3f8e9bbffc4e4c52.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="C-LoRA-Contextual-Low-Rank-Adaptation-for-Uncertainty-Estimation-in-Large-Language-Models"><a href="#C-LoRA-Contextual-Low-Rank-Adaptation-for-Uncertainty-Estimation-in-Large-Language-Models" class="headerlink" title="C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in   Large Language Models"></a>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in   Large Language Models</h2><p><strong>Authors:Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian</strong></p>
<p>Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes. </p>
<blockquote>
<p>低秩适应（LoRA）为微调大型语言模型（LLM）提供了具有成本效益的解决方案，但在数据稀缺的少量样本环境中通常会产生过于自信的预测。为解决这一问题，几种经典的统计学习方法已被重新用于可扩展的具有不确定性的LoRA微调。然而，这些方法忽略了输入特征如何影响预测不确定性估计。为解决这一局限性，我们提出上下文低秩适应（C-LoRA）作为一种新型的具有不确定性的参数高效微调方法，通过开发针对每个输入数据样本的轻型LoRA模块来动态适应不确定性估计。通过将数据驱动上下文融入参数后验分布，C-LoRA缓解了过拟合问题，实现了校准良好的不确定性，并产生了稳健的预测。大量实验表明，在不确定度量模型和模型泛化方面，C-LoRA始终优于最新的具有不确定度的LoRA方法。消融研究进一步证实了我们的上下文模块在捕捉样本特定不确定性方面的关键作用。C-LoRA为少量样本环境下的稳健、具有不确定性的LLM微调设定了新的标准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17773v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于低秩适应（LoRA）的少量数据场景下的大型语言模型（LLM）微调方法虽然经济高效，但常常产生过于自信的预测结果。为解决这一问题，研究者提出一种新型的不确定性感知和参数高效的微调方法——上下文低秩适应（C-LoRA）。它通过开发针对每个输入数据样本的轻量化LoRA模块，动态适应不确定性估计，并将数据驱动上下文融入参数后验分布。C-LoRA能够缓解过拟合问题，实现良好校准的不确定性，产生稳健预测。实验表明，在不确定度量化与模型泛化方面，C-LoRA均优于现有不确定性感知的LoRA方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低秩适应（LoRA）是大型语言模型（LLM）微调的一种经济高效方法，但在少量数据场景下会产生过于自信的预测。</li>
<li>上下文低秩适应（C-LoRA）是一种新型的不确定性感知和参数高效的微调方法。</li>
<li>C-LoRA通过开发针对每个输入数据样本的轻量化LoRA模块，实现动态适应不确定性估计。</li>
<li>C-LoRA将数据驱动的上下文融入参数后验分布，以缓解过拟合问题并实现良好校准的不确定性。</li>
<li>C-LoRA能够产生稳健预测，并在不确定度量化与模型泛化方面优于现有方法。</li>
<li>广泛实验证明C-LoRA的有效性，包括与现有不确定性感知的LoRA方法的对比。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17773">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2695add100c5c9f9d673afc422490faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed2797f8477478a46c05b7085ed946e6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments"><a href="#AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments" class="headerlink" title="AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments"></a>AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments</h2><p><strong>Authors:Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold</strong></p>
<p>Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87% accuracy, 89% precision, and 83% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency. </p>
<blockquote>
<p>追踪气候适应领域的金融投资是一项复杂且需要专业技能的任务，特别是对于缺乏多边发展银行和基金标准化财务报告的早期预警系统（EWS）而言。为了应对这一挑战，我们引入了一个基于大型语言模型的智能AI系统，该系统结合了上下文检索、微调以及多步骤推理，以提取相关财务数据、分类投资并确保符合资金指导方针。我们的研究关注现实应用：追踪气候风险与早期预警系统（CREWS）基金中的EWS投资。我们分析了25份MDB项目文件，并评估了多种AI驱动的分类方法，包括零样本和少样本学习、微调基于转换器的分类器、链式思维（CoT）提示以及基于代理的检索增强生成（RAG）方法等。我们的结果表明，基于代理的RAG方法显著优于其他方法，达到了87%的准确率、89%的精确率和8 3%的召回率。此外，我们还贡献了一个基准数据集和专家注释语料库，为AI驱动的金融追踪和气候金融透明度的未来研究提供了有价值的资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05104v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个基于大型语言模型（LLM）的智能化AI系统，用于追踪气候适应的金融投资。该系统结合了语境检索、微调技术和多步骤推理，能够从多边发展银行的项目文件中提取相关数据，对投资进行分类，并确保符合资助指南的要求。研究重点是在气候风险和预警系统基金（CREWS）中对早期预警系统（EWS）投资的跟踪。研究评估了多种AI分类方法，最终发现基于代理的检索增强生成（RAG）方法表现最佳，准确率、精确度和召回率分别达到了87%、89%和83%。同时，本文还贡献了一个基准数据集和专家注释语料库，为未来AI驱动的金融追踪和气候金融透明度的研究提供了宝贵资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based AI系统被用于追踪气候适应的金融投资，解决了早期预警系统（EWS）缺乏标准化财务报告的挑战。</li>
<li>该系统通过结合语境检索、微调技术和多步骤推理，能够从多边发展银行的项目文档中提取相关金融数据。</li>
<li>研究焦点是气候风险和预警系统基金（CREWS）中的EWS投资跟踪。</li>
<li>多种AI分类方法被评估，包括零样本和少样本学习、微调过的基于变压器的分类器、思维链提示和基于代理的检索增强生成（RAG）方法。</li>
<li>基于代理的RAG方法表现最佳，准确率、精确度和召回率分别达到了87%、89%和83%。</li>
<li>研究贡献了一个基准数据集和专家注释语料库，为未来的AI金融追踪和气候金融透明度研究提供了资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05104">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19184a7ea6fdf4e28241d56e586ef998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baa2cf8ae15e9964dd8f8e8bfe075efa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09365514e6267da7826d5f8b5cde5eba.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Model-Diffusion-for-Certifiable-Few-shot-Transfer-Learning"><a href="#Model-Diffusion-for-Certifiable-Few-shot-Transfer-Learning" class="headerlink" title="Model Diffusion for Certifiable Few-shot Transfer Learning"></a>Model Diffusion for Certifiable Few-shot Transfer Learning</h2><p><strong>Authors:Fady Rezk, Royson Lee, Henry Gouk, Timothy Hospedales, Minyoung Kim</strong></p>
<p>In contemporary deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a distribution over PEFT parameters. We then learn the downstream task by a sample-and-evaluate procedure – sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a finite set of PEFT samples. In contrast to the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime. </p>
<blockquote>
<p>在当前的深度学习中，解决低数据问题的一个流行且有效的工作流程是通过参数有效的微调（PEFT）将强大的预训练基础模型（FMs）适应到新任务。然而，尽管经验上有效，但所得解决方案缺乏泛化保证来证明其准确性，而在部署到高优先级应用之前，可能出于道德或法律原因需要这种保证。在本文中，我们开发了一种新型迁移学习方法，旨在促进下游任务的非空洞学习理论泛化保证，即使在低射击状态下也是如此。具体来说，我们首先使用上游任务来训练PEFT参数的分布。然后，我们通过采样和评估程序来学习下游任务——从训练的扩散模型中采样合理的PEFTs，并选择在下游数据上可能性最高的一个。关键的是，这将我们的模型假设限制在PEFT样本的有限集合中。与神经网络权重的典型连续假设空间相比，这有助于更紧密的风险证书。我们实现了我们的界限，并显示出与非空洞泛化保证的现有学习方法相比，在低射击状态下具有非平凡泛化保证的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06970v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出了一种新的迁移学习方法，旨在便于下游任务在非空泛化的学习理论泛化保证，即使在低数据量情况下也能实现。通过上游任务训练参数扩散模型，采用采样评估程序学习下游任务，从训练的扩散模型中采样可能的微调参数，选择在下游数据上可能性最高的一个。通过将模型假设限制在有限的微调参数样本集上，与传统的神经网络权重连续假设空间相比，能提供更严格的风险证书。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当代深度学习中，使用预训练基础模型进行参数有效微调是解决低数据问题的一种流行而有效的方法。</li>
<li>然而，现有方法缺乏泛化保证，无法在道德或法律原因要求在重要应用中部署时证明其准确性。</li>
<li>本文提出了一种新的迁移学习方法，通过上游任务训练参数扩散模型以提供非空泛化的学习理论泛化保证。</li>
<li>方法采用采样评估程序，从训练的扩散模型中采样可能的微调参数，选择适应下游数据的最佳参数。</li>
<li>通过将模型假设限制在有限的微调参数样本集上，该方法能提供更严格的风险证书。</li>
<li>与现有学习方法相比，本文方法在低数据量情况下提供了非泛化的泛化保证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06970">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e314c1973a57720001bef4205474ac6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a290d69f485a0ae3558565b78575514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d6a3d3ef9e66308bf4084c32257d39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9add275847994a2c8dd1a1cc6e4d2f5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation"><a href="#FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation" class="headerlink" title="FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation"></a>FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation</h2><p><strong>Authors:Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt</strong></p>
<p>Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF’s core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction. </p>
<blockquote>
<p>在自然语言处理（NLP）中，反事实例子被广泛应用于改进模型的数据，同时也在可解释的人工智能（XAI）中被用来理解模型行为。尽管大型语言模型（LLM）在许多任务上表现出令人印象深刻的效果，但自动生成反事实例子仍然是一个具有挑战性的任务。在本文中，我们首先介绍了ZeroCF，这是一种忠实的方法，利用从特征归因方法派生出的重要单词，在无样本环境中生成反事实例子。其次，我们提出了一个新的框架FitCF，它通过标签翻转验证进一步验证了上述的反事实，然后将它们作为演示用于小样本提示，超越了两种最先进的基线方法。通过消融研究，我们确定了FitCF的每个核心组件在提高反事实质量方面的重要性，这通过翻转率、困惑度和相似性度量进行评估。此外，我们展示了LIME和集成梯度作为FitCF的骨干归因方法的有效性，并发现演示的数量对性能的影响最大。最后，我们揭示了特征归因分数忠实性与生成反事实质量之间的强烈相关性，我们希望这将成为未来研究的重要发现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00777v3">PDF</a> ACL 2025 Findings; camera-ready version</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在自然语言处理（NLP）和可解释人工智能（XAI）中广泛应用的反事实示例。文章提出了ZeroCF方法，利用特征归属方法得出的重要词汇在无样本环境下生成反事实示例。进一步，文章提出了FitCF框架，通过标签翻转验证前述反事实，并将其作为演示用于小样本提示，优于两个先进基线。文章还通过消融研究确定了FitCF核心组件在提高反事实质量方面的重要性，并展示了LIME和Integrated Gradients作为FitCF骨干归属方法的有效性。最后，文章揭示了特征归属分数忠实性与生成反事实质量之间的强相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Counterfactual examples are valuable in NLP and XAI.</li>
<li>ZeroCF方法利用特征归属方法的重要词汇生成反事实示例。</li>
<li>FitCF框架通过标签翻转验证反事实，并用于小样本提示。</li>
<li>FitCF优于其他先进基线，其有效性通过消融研究得到证实。</li>
<li>LIME和Integrated Gradients是有效的骨干归属方法。</li>
<li>演示的数量对性能影响最大。</li>
<li>特征归属分数的忠实性与生成的反事实质量之间存在强相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c53e4026e252ae806c528c0c257cda3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b345248bca34006338e17f33b5467a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a1f76aac418934b2c42a9ef942eacd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering"><a href="#FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering" class="headerlink" title="FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering"></a>FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering</h2><p><strong>Authors:Zheng Cheng, Rendong Wang, Zhicheng Wang</strong></p>
<p>Recently, multi-modal large language models have made significant progress. However, visual information lacking of guidance from the user’s intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos. To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the user’s prompt. In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively. The two branches are combined using the Spatial-Temporal Filtering Module (STFM). STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the user’s query. It lowers the essential number of visual tokens inputted into the LLM. FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied. It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data. </p>
<blockquote>
<p>近期，多模态大型语言模型取得了显著进展。然而，缺乏用户意图指导的视觉信息可能导致冗余计算并引入不必要的视觉噪音，特别是在长而无剪辑的视频中。为了解决这一问题，我们提出了FocusChat，这是一个受文本引导的多模态大型语言模型（LLM），它强调与用户提示相关的视觉信息。具体来说，我们的模型首先经过语义提取模块，该模块包括一个视觉语义分支和一个文本语义分支，分别提取图像和文本语义。这两个分支通过时空滤波模块（STFM）进行结合。STFM实现了显式的空间级信息滤波和隐式的特征级时间滤波，确保视觉令牌与用户查询紧密对齐。它降低了输入LLM的必要视觉令牌数量。FocusChat在零样本实验中显著优于Video-LLaMA，使用数量级的训练数据量更少，仅占用16个视觉令牌。在少量样本实验中，其达到的结果与最新技术相当，仅使用0.72M的预训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12833v2">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>文本描述了一种名为FocusChat的文本引导式多模态大型语言模型（LLM），该模型强调与用户提示相关的视觉信息。它通过语义提取模块和空间时间过滤模块，将图像和文本语义相结合，确保视觉令牌与用户查询紧密对齐，降低输入LLM的必需视觉令牌数量。FocusChat在零样本实验中显著优于Video-LLaMA，使用数量级较少的训练数据，仅有16个视觉令牌。在少量样本实验中，它达到了与最新技术相当的结果，只有0.72M的预训练数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FocusChat是一个文本引导的多模态大型语言模型，旨在解决在视频处理中由于用户意图缺乏指导而导致的冗余计算和不必要的视觉噪音问题。</li>
<li>FocusChat通过语义提取模块提取图像和文本语义，并通过空间时间过滤模块进行结合，确保视觉信息与用户查询对齐。</li>
<li>该模型降低了输入大型语言模型的必要视觉令牌数量。</li>
<li>在零样本实验中，FocusChat显著优于Video-LLaMA，使用了较少的训练数据。</li>
<li>FocusChat在少量样本实验中表现出与最新技术相当的性能。</li>
<li>FocusChat的预训练数据量较小，只有0.72M。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eff5fc66d84efc59a4158d53cc01bb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a731d522370a9647dfa7062a0505d69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8c5185d8c42e742825f63df215019bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f1f06fa00d3366c5b6739f379a7e110.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-85fb140468175d17f9cbfb75ebce09d6.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-30  UAVPairs A Challenging Benchmark for Match Pair Retrieval of   Large-scale UAV Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b65ecbcd674cfe162fe2029826adc661.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-30  MTVQA Benchmarking Multilingual Text-Centric Visual Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
