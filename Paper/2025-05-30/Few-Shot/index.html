<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Mastering Agile Tasks with Limited Trials">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8bf2b505a75d0ad66863efb87ec4357c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="Mastering-Agile-Tasks-with-Limited-Trials"><a href="#Mastering-Agile-Tasks-with-Limited-Trials" class="headerlink" title="Mastering Agile Tasks with Limited Trials"></a>Mastering Agile Tasks with Limited Trials</h2><p><strong>Authors:Yihang Hu, Pingyue Sheng, Shengjie Wang, Yang Gao</strong></p>
<p>Embodied robots nowadays can already handle many real-world manipulation tasks. However, certain other real-world tasks (e.g., shooting a basketball into a hoop) are highly agile and require high execution precision, presenting additional challenges for methods primarily designed for quasi-static manipulation tasks. This leads to increased efforts in costly data collection, laborious reward design, or complex motion planning. Such tasks, however, are far less challenging for humans. Say a novice basketball player typically needs only $\sim$10 attempts to make their first successful shot, by roughly imitating a motion prior and then iteratively adjusting their motion based on the past outcomes. Inspired by this human learning paradigm, we propose the Adaptive Diffusion Action Plannin (ADAP) algorithm, a simple &amp; scalable approach which iteratively refines its action plan by few real-world trials within a learned prior motion pattern, until reaching a specific goal. Experiments demonstrated that ADAP can learn and accomplish a wide range of goal-conditioned agile dynamic tasks with human-level precision and efficiency directly in real-world, such as throwing a basketball into the hoop in fewer than 10 trials. Project website:<a target="_blank" rel="noopener" href="https://adap-robotics.github.io/">https://adap-robotics.github.io/</a> . </p>
<blockquote>
<p>ç°ä»Šï¼ŒåµŒå…¥å¼æœºå™¨äººå·²ç»èƒ½å¤Ÿå¤„ç†è®¸å¤šç°å®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡ã€‚ç„¶è€Œï¼ŒæŸäº›å…¶ä»–ç°å®ä¸–ç•Œçš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œå°†ç¯®çƒæŠ•è¿›ç¯®ç­ï¼‰é«˜åº¦æ•æ·ä¸”éœ€è¦é«˜ç²¾åº¦çš„æ‰§è¡Œï¼Œå¯¹äºä¸»è¦ä¸ºé™æ€æ“çºµä»»åŠ¡è®¾è®¡çš„æ–¹æ³•è€Œè¨€ï¼Œè¿™äº›ä»»åŠ¡å¸¦æ¥äº†é¢å¤–çš„æŒ‘æˆ˜ã€‚è¿™å¯¼è‡´äº†æˆæœ¬é«˜æ˜‚çš„æ•°æ®æ”¶é›†ã€ç¹ççš„å¥–åŠ±è®¾è®¡å’Œå¤æ‚çš„è¿åŠ¨è§„åˆ’å·¥ä½œé‡çš„å¢åŠ ã€‚ç„¶è€Œï¼Œå¯¹äºäººç±»æ¥è¯´ï¼Œè¿™æ ·çš„ä»»åŠ¡å¹¶ä¸é‚£ä¹ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¾‹å¦‚ï¼Œæ–°æ‰‹ç¯®çƒè¿åŠ¨å‘˜é€šå¸¸éœ€è¦å¤§çº¦åæ¬¡å°è¯•æ‰èƒ½æ‰“å‡ºç¬¬ä¸€ä¸ªæˆåŠŸçš„æŠ•ç¯®ï¼Œé€šè¿‡å¤§è‡´æ¨¡ä»¿ä¹‹å‰çš„åŠ¨ä½œå¹¶ä¸æ–­æ ¹æ®è¿‡å»çš„ç»“æœè°ƒæ•´åŠ¨ä½œã€‚å—äººç±»å­¦ä¹ æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ‰©æ•£åŠ¨ä½œè§„åˆ’ï¼ˆADAPï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å°‘æ•°ç°å®ä¸–ç•Œçš„è¯•éªŒä¸­å­¦ä¹ åˆ°çš„å…ˆå‰åŠ¨ä½œæ¨¡å¼æ¥è¿­ä»£ä¼˜åŒ–è¡ŒåŠ¨è®¡åˆ’ï¼Œç›´è‡³è¾¾åˆ°ç‰¹å®šç›®æ ‡ã€‚å®éªŒè¡¨æ˜ï¼ŒADAPèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œä¸­ç›´æ¥å­¦ä¹ å’Œå®Œæˆä¸€ç³»åˆ—ç›®æ ‡å¯¼å‘çš„æ•æ·åŠ¨æ€ä»»åŠ¡ï¼Œè¾¾åˆ°äººç±»æ°´å¹³çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¦‚åœ¨ä¸åˆ°åæ¬¡çš„å°è¯•ä¸­å°†ç¯®çƒæŠ•è¿›ç¯®ç­ã€‚é¡¹ç›®ç½‘ç«™ï¼š[<a target="_blank" rel="noopener" href="https://adap-robotics.github.io/]">https://adap-robotics.github.io/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21916v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªé€‚åº”æ‰©æ•£åŠ¨ä½œè§„åˆ’ï¼ˆADAPï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡æ¨¡ä»¿äººç±»å­¦ä¹ æ¨¡å¼å®ç°æœºå™¨äººçš„é«˜æ•ˆåŠ¨æ€ä»»åŠ¡æ‰§è¡Œã€‚åœ¨å°‘é‡çœŸå®ä¸–ç•Œå°è¯•ä¸­ï¼Œæœºå™¨äººèƒ½è¿­ä»£ä¼˜åŒ–åŠ¨ä½œè®¡åˆ’ä»¥è¾¾åˆ°ç‰¹å®šç›®æ ‡ï¼Œå®Œæˆä¸€ç³»åˆ—ç›®æ ‡å¯¼å‘çš„æ•æ·åŠ¨æ€ä»»åŠ¡ï¼Œå¦‚æŠ•ç¯®ç­‰ï¼Œå¹¶å®ç°äººç±»çº§åˆ«çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººå¤„ç†ç°å®ä¸–ç•Œçš„æ“ä½œä»»åŠ¡å·²å…·æœ‰ç›¸å½“èƒ½åŠ›ï¼Œä½†å¯¹äºéœ€è¦é«˜æ•æ·åº¦å’Œé«˜ç²¾åº¦çš„ä»»åŠ¡ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>äººç±»å­¦ä¹ æ–°ä»»åŠ¡ï¼ˆå¦‚ç¯®çƒæŠ•ç¯®ï¼‰é€šå¸¸é€šè¿‡æ¨¡ä»¿åˆæ­¥åŠ¨ä½œå¹¶åŸºäºè¿‡å»çš„ç»“æœè¿›è¡Œè¿­ä»£è°ƒæ•´ï¼Œæœºå™¨äººå¯å€Ÿé‰´æ­¤æ¨¡å¼ã€‚</li>
<li>ADAPç®—æ³•å…è®¸æœºå™¨äººåœ¨å°‘é‡çœŸå®ä¸–ç•Œå°è¯•ä¸­è¿­ä»£ä¼˜åŒ–åŠ¨ä½œè®¡åˆ’ï¼Œè¾¾æˆç‰¹å®šç›®æ ‡ã€‚</li>
<li>ADAPç®—æ³•å¯å¹¿æ³›åº”ç”¨äºå„ç§ç›®æ ‡å¯¼å‘çš„æ•æ·åŠ¨æ€ä»»åŠ¡ã€‚</li>
<li>ADAPç®—æ³•ä½¿å¾—æœºå™¨äººèƒ½åœ¨çœŸå®ä¸–ç•Œä¸­ç›´æ¥å­¦ä¹ å¹¶æ‰§è¡Œä»»åŠ¡ï¼Œå…·æœ‰äººç±»çº§åˆ«çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>è¯¥ç®—æ³•é€šè¿‡ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹å¼å®ç°æœºå™¨äººåŠ¨ä½œçš„ç²¾ç»†åŒ–è°ƒæ•´ï¼Œæé«˜äº†æœºå™¨äººçš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36df0c7c05366e0d1851e17d6415324e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f0dfc7303fd7aa5a6c9ebd2576a9b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d34dcd0150de54b6b11a1f6ef490acb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f50e6ca6ffead098e73018a1a8d152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d997b182cbc4c71150c97f43dec8d8ca.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Concentrate-on-Weakness-Mining-Hard-Prototypes-for-Few-Shot-Medical-Image-Segmentation"><a href="#Concentrate-on-Weakness-Mining-Hard-Prototypes-for-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical   Image Segmentation"></a>Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical   Image Segmentation</h2><p><strong>Authors:Jianchao Jiang, Haofeng Zhang</strong></p>
<p>Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a model that can perform segmentation from only a few annotated images. However, most existing prototype-based FSMIS methods generate multiple prototypes from the support image solely by random sampling or local averaging, which can cause particularly severe boundary blurring due to the tendency for normal features accounting for the majority of features of a specific category. Consequently, we propose to focus more attention to those weaker features that are crucial for clear segmentation boundary. Specifically, we design a Support Self-Prediction (SSP) module to identify such weak features by comparing true support mask with one predicted by global support prototype. Then, a Hard Prototypes Generation (HPG) module is employed to generate multiple hard prototypes based on these weak features. Subsequently, a Multiple Similarity Maps Fusion (MSMF) module is devised to generate final segmenting mask in a dual-path fashion to mitigate the imbalance between foreground and background in medical images. Furthermore, we introduce a boundary loss to further constraint the edge of segmentation. Extensive experiments on three publicly available medical image datasets demonstrate that our method achieves state-of-the-art performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jcjiang99/CoW">https://github.com/jcjiang99/CoW</a>. </p>
<blockquote>
<p>å°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºè®­ç»ƒä»…ä»å°‘é‡æ ‡æ³¨å›¾åƒä¸­æ‰§è¡Œåˆ†å‰²çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºåŸå‹çš„FSMISæ–¹æ³•ä»…é€šè¿‡éšæœºæŠ½æ ·æˆ–å±€éƒ¨å¹³å‡ä»æ”¯æŒå›¾åƒä¸­ç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œè¿™å¯èƒ½å¯¼è‡´è¾¹ç•Œæ¨¡ç³Šç‰¹åˆ«ä¸¥é‡ï¼Œå› ä¸ºæ­£å¸¸ç‰¹å¾å¾€å¾€æ„æˆæŸä¸€ç‰¹å®šç±»åˆ«çš„ä¸»è¦ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æè®®æ›´å¤šåœ°å…³æ³¨é‚£äº›å¯¹äºæ¸…æ™°åˆ†å‰²è¾¹ç•Œè‡³å…³é‡è¦çš„è¾ƒå¼±ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ”¯æŒè‡ªæˆ‘é¢„æµ‹ï¼ˆSSPï¼‰æ¨¡å—ï¼Œé€šè¿‡æ¯”è¾ƒçœŸå®çš„æ”¯æŒæ©è†œä¸å…¨å±€æ”¯æŒåŸå‹é¢„æµ‹çš„æ©è†œæ¥è¯†åˆ«è¿™äº›å¼±ç‰¹å¾ã€‚ç„¶åï¼Œé‡‡ç”¨ç¡¬åŸå‹ç”Ÿæˆï¼ˆHPGï¼‰æ¨¡å—åŸºäºè¿™äº›å¼±ç‰¹å¾ç”Ÿæˆå¤šä¸ªç¡¬åŸå‹ã€‚éšåï¼Œé‡‡ç”¨å¤šç›¸ä¼¼åº¦å›¾èåˆï¼ˆMSMFï¼‰æ¨¡å—ä»¥åŒè·¯å¾„æ–¹å¼ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²æ©è†œï¼Œä»¥ç¼“è§£åŒ»å­¦å›¾åƒä¸­å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾¹ç•ŒæŸå¤±æ¥è¿›ä¸€æ­¥çº¦æŸåˆ†å‰²çš„è¾¹ç¼˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/jcjiang99/CoW">https://github.com/jcjiang99/CoW</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21897v1">PDF</a> 12 pages, 9 figures, 9 tables, accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>å°‘æ•°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ–¹æ³•å¹¿æ³›åº”ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œä»…ä»å°‘é‡æ ‡æ³¨å›¾åƒä¸­æ‰§è¡Œåˆ†å‰²ä»»åŠ¡ã€‚ç°æœ‰åŸå‹æ–¹æ³•ä¸»è¦ä»æ”¯æŒå›¾åƒä¸­ç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œä½†å¯èƒ½å¯¼è‡´è¾¹ç•Œæ¨¡ç³Šã€‚æœ¬æ–‡æå‡ºå…³æ³¨å…³é”®å¼±ç‰¹å¾ä»¥æ˜ç¡®åˆ†å‰²è¾¹ç•Œï¼Œè®¾è®¡æ”¯æŒè‡ªæˆ‘é¢„æµ‹ï¼ˆSSPï¼‰æ¨¡å—è¯†åˆ«å¼±ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨ç¡¬åŸå‹ç”Ÿæˆï¼ˆHPGï¼‰æ¨¡å—ç”Ÿæˆå¤šä¸ªç¡¬åŸå‹ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¤šé‡ç›¸ä¼¼å›¾èåˆï¼ˆMSMFï¼‰æ¨¡å—ç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œï¼Œå¹¶å¼•å…¥è¾¹ç•ŒæŸå¤±ä»¥è¿›ä¸€æ­¥çº¦æŸåˆ†å‰²è¾¹ç¼˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ•°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ˜¯è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†å‰²çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä»…éœ€å°‘é‡æ ‡æ³¨å›¾åƒã€‚</li>
<li>ç°æœ‰åŸå‹æ–¹æ³•ä¸»è¦é€šè¿‡éšæœºé‡‡æ ·æˆ–å±€éƒ¨å¹³å‡ä»æ”¯æŒå›¾åƒä¸­ç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œå¯èƒ½å¯¼è‡´è¾¹ç•Œæ¨¡ç³Šã€‚</li>
<li>æœ¬æ–‡å¼ºè°ƒå…³æ³¨å…³é”®å¼±ç‰¹å¾ä»¥æ˜ç¡®åˆ†å‰²è¾¹ç•Œï¼Œè®¾è®¡SSPæ¨¡å—è¯†åˆ«è¿™äº›ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨HPGæ¨¡å—ç”Ÿæˆå¤šä¸ªç¡¬åŸå‹ï¼ŒåŸºäºè¯†åˆ«å‡ºçš„å¼±ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨MSMFæ¨¡å—ä»¥åŒé‡è·¯å¾„æ–¹å¼ç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œï¼Œè§£å†³åŒ»å­¦å›¾åƒä¸­å‰æ™¯ä¸èƒŒæ™¯çš„ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥è¾¹ç•ŒæŸå¤±ä»¥è¿›ä¸€æ­¥çº¦æŸåˆ†å‰²è¾¹ç¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-112f2b22a43492f47fed31a8b49d7eea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f78e1eb789f2e7daf17f649e6c93c11b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e41ca871ea0ef05483686ab1ca4545f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation"><a href="#SANSA-Unleashing-the-Hidden-Semantics-in-SAM2-for-Few-Shot-Segmentation" class="headerlink" title="SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation"></a>SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</h2><p><strong>Authors:Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone</strong></p>
<p>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a>. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬åˆ†å‰²æ—¨åœ¨ä»å°‘é‡çš„æ ‡æ³¨æ ·æœ¬ä¸­å¯¹æœªè§è¿‡çš„ç›®æ ‡ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚è¿™è¦æ±‚æœºåˆ¶èƒ½å¤Ÿåœ¨å›¾åƒä¸­è¯†åˆ«è¯­ä¹‰ç›¸å…³çš„å¯¹è±¡ï¼Œå¹¶å‡†ç¡®ç”Ÿæˆåˆ†å‰²æ©è†œã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå€ŸåŠ©æç¤ºå’Œæ‰©å±•æœºåˆ¶ï¼ŒSegment Anything 2ï¼ˆSAM2ï¼‰æ—¢å…·æœ‰å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›ï¼Œåˆå…·å¤‡å†…ç½®çš„ç‰¹å¾åŒ¹é…è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å±•ç¤ºå…¶åœ¨ä¼˜åŒ–å¯¹è±¡è·Ÿè¸ªçš„ç‰¹å®šä»»åŠ¡çº¿ç´¢æ—¶ï¼Œå…¶è¡¨ç¤ºä¸è¿™äº›çº¿ç´¢çº ç¼ åœ¨ä¸€èµ·ï¼Œè¿™æŸå®³äº†å…¶åœ¨éœ€è¦æ›´é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­çš„ä½¿ç”¨ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡SAM2å…·æœ‰ç±»åˆ«æ— å…³çš„é¢„è®­ç»ƒï¼Œä½†å®ƒå·²ç»åœ¨å…¶ç‰¹æ€§ä¸­ç¼–ç äº†ä¸°å¯Œçš„è¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†SANSAï¼ˆè¯­ä¹‰å¯¹é½çš„Segment Anything 2ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿è¿™ç§æ½œåœ¨ç»“æ„æ˜ç¡®åŒ–çš„æ¡†æ¶ï¼Œå¹¶é€šè¿‡æœ€å°‘çš„ç‰¹å®šä»»åŠ¡ä¿®æ”¹ä½¿SAM2ç”¨äºå°‘æ•°æ ·æœ¬åˆ†å‰²ã€‚SANSAåœ¨ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ³›åŒ–çš„å°‘æ•°æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æµè¡Œä¸Šä¸‹æ–‡è®¾ç½®ä¸­çš„é€šç”¨æ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œæ”¯æŒé€šè¿‡ç‚¹ã€æ¡†æˆ–æ¶‚é¸¦è¿›è¡Œå„ç§æç¤ºçµæ´»äº¤äº’ï¼Œå¹¶ä¸”ç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•ï¼Œå…¶é€Ÿåº¦æ›´å¿«ã€æ›´ç´§å‡‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ClaudiaCuttano/SANSAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21795v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/ClaudiaCuttano/SANSA">https://github.com/ClaudiaCuttano/SANSA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Few-shot segmentationçš„ç›®æ ‡å’Œæ–¹æ³•ã€‚æ–‡ç« æŒ‡å‡ºSegment Anything 2ï¼ˆSAM2ï¼‰è™½ç„¶å…·æœ‰å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œç‰¹å¾åŒ¹é…åŠŸèƒ½ï¼Œä½†å…¶è¡¨ç¤ºä¸ä»»åŠ¡ç‰¹å®šçº¿ç´¢çº ç¼ åœ¨ä¸€èµ·ï¼Œå½±å“å…¶åœ¨éœ€è¦é«˜çº§è¯­ä¹‰ç†è§£çš„ä»»åŠ¡ä¸­çš„ä½¿ç”¨ã€‚æ–‡ç« æå‡ºSANSAæ¡†æ¶ï¼Œé€šè¿‡ä½¿SAM2çš„æ½œåœ¨ç»“æ„æ˜¾æ€§åŒ–å¹¶å¯¹å…¶è¿›è¡Œæœ€å°ä»»åŠ¡ç‰¹å®šçš„ä¿®æ”¹ï¼Œç”¨äºå°‘æ ·æœ¬åˆ†å‰²ã€‚SANSAå®ç°äº†ä¸“ä¸ºè¯„ä¼°æ³›åŒ–èƒ½åŠ›è€Œè®¾è®¡çš„å°‘æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•çš„æœ€ä½³æ€§èƒ½ï¼Œåœ¨æµè¡Œçš„ä¸Šä¸‹æ–‡è®¾ç½®å†…ä¼˜äºé€šç”¨æ–¹æ³•ï¼Œå¹¶æ”¯æŒé€šè¿‡å„ç§æç¤ºè¿›è¡Œçµæ´»äº¤äº’ï¼ŒåŒæ—¶é€Ÿåº¦æ›´å¿«ã€æ›´ç´§å‡‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot segmentationæ—¨åœ¨ä»å°‘é‡æ ‡æ³¨çš„ç¤ºä¾‹ä¸­å¯¹æœªè§è¿‡çš„å¯¹è±¡ç±»åˆ«è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>Segment Anything 2ï¼ˆSAM2ï¼‰å…·æœ‰å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œç‰¹å¾åŒ¹é…åŠŸèƒ½ã€‚</li>
<li>SAM2çš„è¡¨ç¤ºä¸ä»»åŠ¡ç‰¹å®šçº¿ç´¢çº ç¼ åœ¨ä¸€èµ·ï¼Œå½±å“å…¶åœ¨é«˜çº§è¯­ä¹‰ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>SANSAæ¡†æ¶é€šè¿‡ä½¿SAM2çš„æ½œåœ¨ç»“æ„æ˜¾æ€§åŒ–ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæœ€å°ä»»åŠ¡ç‰¹å®šçš„ä¿®æ”¹ï¼Œç”¨äºå°‘æ ·æœ¬åˆ†å‰²ã€‚</li>
<li>SANSAå®ç°äº†ä¸“ä¸ºè¯„ä¼°æ³›åŒ–èƒ½åŠ›è®¾è®¡çš„å°‘æ ·æœ¬åˆ†å‰²åŸºå‡†æµ‹è¯•çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>SANSAåœ¨æµè¡Œä¸Šä¸‹æ–‡è®¾ç½®å†…çš„æ€§èƒ½ä¼˜äºé€šç”¨æ–¹æ³•ã€‚</li>
<li>SANSAæ”¯æŒé€šè¿‡å„ç§æç¤ºï¼ˆå¦‚ç‚¹ã€æ¡†æˆ–æ¶‚é¸¦ï¼‰è¿›è¡Œçµæ´»äº¤äº’ï¼Œå¹¶ä¸”é€Ÿåº¦æ›´å¿«ã€æ›´ç´§å‡‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-776453b1b78a989e79af018be9145c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf2b505a75d0ad66863efb87ec4357c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-022dba601551ed556c01f471a95365d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84853eff613e8afa690dddf511e22788.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling"><a href="#Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling" class="headerlink" title="Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling"></a>Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling</h2><p><strong>Authors:Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi</strong></p>
<p>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual&#x2F;textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘å·²è¢«çº³å…¥å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œå°‘é‡ã€å¼±ç›‘ç£åˆ†ç±»çš„æŒ‘æˆ˜ã€‚ä¸€ç§å…³é”®è¶‹åŠ¿æ˜¯ï¼Œåˆ©ç”¨å¤šå°ºåº¦ä¿¡æ¯æ¥æ›´å¥½åœ°è¡¨ç¤ºå±‚æ¬¡åŒ–çš„ç»„ç»‡ç»“æ„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰åŒä¸€æ¨¡æ€å†…ä¸åŒå°ºåº¦ï¼ˆä¾‹å¦‚ï¼Œ5å€å’Œ20å€ï¼‰ä¹‹é—´äº¤äº’çš„å»ºæ¨¡ä¸è¶³ï¼›ï¼ˆ2ï¼‰åŒä¸€å°ºåº¦ä¸Šè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å¯¹é½ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†HiVE-MILï¼Œè¿™æ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€å›¾ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰ç²—ï¼ˆ5å€ï¼‰å’Œç»†ï¼ˆ20å€ï¼‰è§†è§‰&#x2F;æ–‡æœ¬èŠ‚ç‚¹ä¹‹é—´çš„çˆ¶å­é“¾æ¥ï¼Œä»¥æ•è·å±‚æ¬¡å…³ç³»ï¼Œä»¥åŠï¼ˆ2ï¼‰åœ¨åŒä¸€å°ºåº¦ä¸Šè¿æ¥è§†è§‰å’Œæ–‡æœ¬èŠ‚ç‚¹çš„å¼‚æ„å›¾å†…è¾¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ï¼ŒHiVE-MILé‡‡ç”¨äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ¶ˆé™¤äº†å¼±ç›¸å…³çš„è¡¥ä¸æ–‡æœ¬å¯¹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å±‚æ¬¡å¯¹æ¯”æŸå¤±ï¼Œä»¥å¯¹é½ä¸åŒå°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰ã€‚åœ¨TCGAä¹³è…ºç™Œã€è‚ºç™Œå’Œè‚¾ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHiVE-MILå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„MILå’Œæœ€æ–°çš„åŸºäºVLMçš„MILæ–¹æ³•ï¼Œåœ¨16æ¬¡æ‹æ‘„çš„å®è§‚F1å¾—åˆ†æé«˜äº†é«˜è¾¾4.1%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è”åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¯¹é½å¯¹äºä»æœ‰é™çš„ç—…ç†å­¦æ•°æ®ä¸­å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•å­¦ä¹ çš„ä»·å€¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17982v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èå…¥å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³å°‘æ ·æœ¬ã€å¼±ç›‘ç£åˆ†ç±»å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„æŒ‘æˆ˜ã€‚æ–‡ç« å¼ºè°ƒäº†åˆ©ç”¨å¤šå°ºåº¦ä¿¡æ¯çš„è¶‹åŠ¿ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†HiVE-MILæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç»Ÿä¸€å›¾æ¥æ•æ‰å±‚æ¬¡å…³ç³»ï¼Œå¹¶å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒHiVE-MILåœ¨TCGAä¹³è…ºç™Œã€è‚ºç™Œå’Œè‚¾ç™Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsè¢«é›†æˆåˆ°MILæ¡†æ¶ä¸­ï¼Œç”¨äºè§£å†³å°‘æ ·æœ¬ã€å¼±ç›‘ç£åˆ†ç±»çš„WSIæŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨å¤šå°ºåº¦ä¿¡æ¯æˆä¸ºå…³é”®è¶‹åŠ¿ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>HiVE-MILæ¡†æ¶é€šè¿‡æ„å»ºç»Ÿä¸€å›¾æ¥æ•æ‰å±‚æ¬¡å…³ç³»ï¼ŒåŒ…æ‹¬çˆ¶å­é“¾æ¥å’Œå¼‚è´¨å†…å°ºåº¦è¾¹ç¼˜ã€‚</li>
<li>HiVE-MILé‡‡ç”¨ä¸¤é˜¶æ®µæ–‡æœ¬å¼•å¯¼çš„åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>HiVE-MILå¼•å…¥å±‚æ¬¡å¯¹æ¯”æŸå¤±ï¼Œå¯¹é½åŒä¸€å°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰ã€‚</li>
<li>å®éªŒè¡¨æ˜HiVE-MILåœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯TCGAæ•°æ®é›†ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef61629f6dbf6eabef99acea72faa7e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97dcfab98be7f160215b7823cf69fa16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9450b1afc48fc6fa3f8e9bbffc4e4c52.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="C-LoRA-Contextual-Low-Rank-Adaptation-for-Uncertainty-Estimation-in-Large-Language-Models"><a href="#C-LoRA-Contextual-Low-Rank-Adaptation-for-Uncertainty-Estimation-in-Large-Language-Models" class="headerlink" title="C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in   Large Language Models"></a>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in   Large Language Models</h2><p><strong>Authors:Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian</strong></p>
<p>Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸ºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†å…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘é‡æ ·æœ¬ç¯å¢ƒä¸­é€šå¸¸ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå‡ ç§ç»å…¸çš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•å·²è¢«é‡æ–°ç”¨äºå¯æ‰©å±•çš„å…·æœ‰ä¸ç¡®å®šæ€§çš„LoRAå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†è¾“å…¥ç‰¹å¾å¦‚ä½•å½±å“é¢„æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºä¸Šä¸‹æ–‡ä½ç§©é€‚åº”ï¼ˆC-LoRAï¼‰ä½œä¸ºä¸€ç§æ–°å‹çš„å…·æœ‰ä¸ç¡®å®šæ€§çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å¼€å‘é’ˆå¯¹æ¯ä¸ªè¾“å…¥æ•°æ®æ ·æœ¬çš„è½»å‹LoRAæ¨¡å—æ¥åŠ¨æ€é€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚é€šè¿‡å°†æ•°æ®é©±åŠ¨ä¸Šä¸‹æ–‡èå…¥å‚æ•°åéªŒåˆ†å¸ƒï¼ŒC-LoRAç¼“è§£äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®ç°äº†æ ¡å‡†è‰¯å¥½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶äº§ç”Ÿäº†ç¨³å¥çš„é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ç¡®å®šåº¦é‡æ¨¡å‹å’Œæ¨¡å‹æ³›åŒ–æ–¹é¢ï¼ŒC-LoRAå§‹ç»ˆä¼˜äºæœ€æ–°çš„å…·æœ‰ä¸ç¡®å®šåº¦çš„LoRAæ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡æ¨¡å—åœ¨æ•æ‰æ ·æœ¬ç‰¹å®šä¸ç¡®å®šæ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚C-LoRAä¸ºå°‘é‡æ ·æœ¬ç¯å¢ƒä¸‹çš„ç¨³å¥ã€å…·æœ‰ä¸ç¡®å®šæ€§çš„LLMå¾®è°ƒè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17773v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å°‘é‡æ•°æ®åœºæ™¯ä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒæ–¹æ³•è™½ç„¶ç»æµé«˜æ•ˆï¼Œä½†å¸¸å¸¸äº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ç»“æœã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å’Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡ä½ç§©é€‚åº”ï¼ˆC-LoRAï¼‰ã€‚å®ƒé€šè¿‡å¼€å‘é’ˆå¯¹æ¯ä¸ªè¾“å…¥æ•°æ®æ ·æœ¬çš„è½»é‡åŒ–LoRAæ¨¡å—ï¼ŒåŠ¨æ€é€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶å°†æ•°æ®é©±åŠ¨ä¸Šä¸‹æ–‡èå…¥å‚æ•°åéªŒåˆ†å¸ƒã€‚C-LoRAèƒ½å¤Ÿç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®ç°è‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§ï¼Œäº§ç”Ÿç¨³å¥é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ç¡®å®šåº¦é‡åŒ–ä¸æ¨¡å‹æ³›åŒ–æ–¹é¢ï¼ŒC-LoRAå‡ä¼˜äºç°æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„LoRAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒçš„ä¸€ç§ç»æµé«˜æ•ˆæ–¹æ³•ï¼Œä½†åœ¨å°‘é‡æ•°æ®åœºæ™¯ä¸‹ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚</li>
<li>ä¸Šä¸‹æ–‡ä½ç§©é€‚åº”ï¼ˆC-LoRAï¼‰æ˜¯ä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å’Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>C-LoRAé€šè¿‡å¼€å‘é’ˆå¯¹æ¯ä¸ªè¾“å…¥æ•°æ®æ ·æœ¬çš„è½»é‡åŒ–LoRAæ¨¡å—ï¼Œå®ç°åŠ¨æ€é€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>C-LoRAå°†æ•°æ®é©±åŠ¨çš„ä¸Šä¸‹æ–‡èå…¥å‚æ•°åéªŒåˆ†å¸ƒï¼Œä»¥ç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜å¹¶å®ç°è‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>C-LoRAèƒ½å¤Ÿäº§ç”Ÿç¨³å¥é¢„æµ‹ï¼Œå¹¶åœ¨ä¸ç¡®å®šåº¦é‡åŒ–ä¸æ¨¡å‹æ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜C-LoRAçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä¸ç°æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„LoRAæ–¹æ³•çš„å¯¹æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2695add100c5c9f9d673afc422490faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed2797f8477478a46c05b7085ed946e6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments"><a href="#AI-for-Climate-Finance-Agentic-Retrieval-and-Multi-Step-Reasoning-for-Early-Warning-System-Investments" class="headerlink" title="AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments"></a>AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for   Early Warning System Investments</h2><p><strong>Authors:Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold</strong></p>
<p>Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87% accuracy, 89% precision, and 83% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency. </p>
<blockquote>
<p>è¿½è¸ªæ°”å€™é€‚åº”é¢†åŸŸçš„é‡‘èæŠ•èµ„æ˜¯ä¸€é¡¹å¤æ‚ä¸”éœ€è¦ä¸“ä¸šæŠ€èƒ½çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç¼ºä¹å¤šè¾¹å‘å±•é“¶è¡Œå’ŒåŸºé‡‘æ ‡å‡†åŒ–è´¢åŠ¡æŠ¥å‘Šçš„æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰è€Œè¨€ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½AIç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†ä¸Šä¸‹æ–‡æ£€ç´¢ã€å¾®è°ƒä»¥åŠå¤šæ­¥éª¤æ¨ç†ï¼Œä»¥æå–ç›¸å…³è´¢åŠ¡æ•°æ®ã€åˆ†ç±»æŠ•èµ„å¹¶ç¡®ä¿ç¬¦åˆèµ„é‡‘æŒ‡å¯¼æ–¹é’ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å…³æ³¨ç°å®åº”ç”¨ï¼šè¿½è¸ªæ°”å€™é£é™©ä¸æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆCREWSï¼‰åŸºé‡‘ä¸­çš„EWSæŠ•èµ„ã€‚æˆ‘ä»¬åˆ†æäº†25ä»½MDBé¡¹ç›®æ–‡ä»¶ï¼Œå¹¶è¯„ä¼°äº†å¤šç§AIé©±åŠ¨çš„åˆ†ç±»æ–¹æ³•ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€å¾®è°ƒåŸºäºè½¬æ¢å™¨çš„åˆ†ç±»å™¨ã€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºä»¥åŠåŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ç­‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºä»£ç†çš„RAGæ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¾¾åˆ°äº†87%çš„å‡†ç¡®ç‡ã€89%çš„ç²¾ç¡®ç‡å’Œ8 3%çš„å¬å›ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºAIé©±åŠ¨çš„é‡‘èè¿½è¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦çš„æœªæ¥ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05104v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½åŒ–AIç³»ç»Ÿï¼Œç”¨äºè¿½è¸ªæ°”å€™é€‚åº”çš„é‡‘èæŠ•èµ„ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è¯­å¢ƒæ£€ç´¢ã€å¾®è°ƒæŠ€æœ¯å’Œå¤šæ­¥éª¤æ¨ç†ï¼Œèƒ½å¤Ÿä»å¤šè¾¹å‘å±•é“¶è¡Œçš„é¡¹ç›®æ–‡ä»¶ä¸­æå–ç›¸å…³æ•°æ®ï¼Œå¯¹æŠ•èµ„è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ç¡®ä¿ç¬¦åˆèµ„åŠ©æŒ‡å—çš„è¦æ±‚ã€‚ç ”ç©¶é‡ç‚¹æ˜¯åœ¨æ°”å€™é£é™©å’Œé¢„è­¦ç³»ç»ŸåŸºé‡‘ï¼ˆCREWSï¼‰ä¸­å¯¹æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰æŠ•èµ„çš„è·Ÿè¸ªã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§AIåˆ†ç±»æ–¹æ³•ï¼Œæœ€ç»ˆå‘ç°åŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«è¾¾åˆ°äº†87%ã€89%å’Œ83%ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºæœªæ¥AIé©±åŠ¨çš„é‡‘èè¿½è¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦çš„ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based AIç³»ç»Ÿè¢«ç”¨äºè¿½è¸ªæ°”å€™é€‚åº”çš„é‡‘èæŠ•èµ„ï¼Œè§£å†³äº†æ—©æœŸé¢„è­¦ç³»ç»Ÿï¼ˆEWSï¼‰ç¼ºä¹æ ‡å‡†åŒ–è´¢åŠ¡æŠ¥å‘Šçš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡ç»“åˆè¯­å¢ƒæ£€ç´¢ã€å¾®è°ƒæŠ€æœ¯å’Œå¤šæ­¥éª¤æ¨ç†ï¼Œèƒ½å¤Ÿä»å¤šè¾¹å‘å±•é“¶è¡Œçš„é¡¹ç›®æ–‡æ¡£ä¸­æå–ç›¸å…³é‡‘èæ•°æ®ã€‚</li>
<li>ç ”ç©¶ç„¦ç‚¹æ˜¯æ°”å€™é£é™©å’Œé¢„è­¦ç³»ç»ŸåŸºé‡‘ï¼ˆCREWSï¼‰ä¸­çš„EWSæŠ•èµ„è·Ÿè¸ªã€‚</li>
<li>å¤šç§AIåˆ†ç±»æ–¹æ³•è¢«è¯„ä¼°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€å¾®è°ƒè¿‡çš„åŸºäºå˜å‹å™¨çš„åˆ†ç±»å™¨ã€æ€ç»´é“¾æç¤ºå’ŒåŸºäºä»£ç†çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚</li>
<li>åŸºäºä»£ç†çš„RAGæ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«è¾¾åˆ°äº†87%ã€89%å’Œ83%ã€‚</li>
<li>ç ”ç©¶è´¡çŒ®äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸“å®¶æ³¨é‡Šè¯­æ–™åº“ï¼Œä¸ºæœªæ¥çš„AIé‡‘èè¿½è¸ªå’Œæ°”å€™é‡‘èé€æ˜åº¦ç ”ç©¶æä¾›äº†èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19184a7ea6fdf4e28241d56e586ef998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baa2cf8ae15e9964dd8f8e8bfe075efa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09365514e6267da7826d5f8b5cde5eba.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Model-Diffusion-for-Certifiable-Few-shot-Transfer-Learning"><a href="#Model-Diffusion-for-Certifiable-Few-shot-Transfer-Learning" class="headerlink" title="Model Diffusion for Certifiable Few-shot Transfer Learning"></a>Model Diffusion for Certifiable Few-shot Transfer Learning</h2><p><strong>Authors:Fady Rezk, Royson Lee, Henry Gouk, Timothy Hospedales, Minyoung Kim</strong></p>
<p>In contemporary deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a distribution over PEFT parameters. We then learn the downstream task by a sample-and-evaluate procedure â€“ sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a finite set of PEFT samples. In contrast to the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime. </p>
<blockquote>
<p>åœ¨å½“å‰çš„æ·±åº¦å­¦ä¹ ä¸­ï¼Œè§£å†³ä½æ•°æ®é—®é¢˜çš„ä¸€ä¸ªæµè¡Œä¸”æœ‰æ•ˆçš„å·¥ä½œæµç¨‹æ˜¯é€šè¿‡å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰å°†å¼ºå¤§çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰é€‚åº”åˆ°æ–°ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°½ç®¡ç»éªŒä¸Šæœ‰æ•ˆï¼Œä½†æ‰€å¾—è§£å†³æ–¹æ¡ˆç¼ºä¹æ³›åŒ–ä¿è¯æ¥è¯æ˜å…¶å‡†ç¡®æ€§ï¼Œè€Œåœ¨éƒ¨ç½²åˆ°é«˜ä¼˜å…ˆçº§åº”ç”¨ä¹‹å‰ï¼Œå¯èƒ½å‡ºäºé“å¾·æˆ–æ³•å¾‹åŸå› éœ€è¦è¿™ç§ä¿è¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ƒè¿›ä¸‹æ¸¸ä»»åŠ¡çš„éç©ºæ´å­¦ä¹ ç†è®ºæ³›åŒ–ä¿è¯ï¼Œå³ä½¿åœ¨ä½å°„å‡»çŠ¶æ€ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¸Šæ¸¸ä»»åŠ¡æ¥è®­ç»ƒPEFTå‚æ•°çš„åˆ†å¸ƒã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡é‡‡æ ·å’Œè¯„ä¼°ç¨‹åºæ¥å­¦ä¹ ä¸‹æ¸¸ä»»åŠ¡â€”â€”ä»è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·åˆç†çš„PEFTsï¼Œå¹¶é€‰æ‹©åœ¨ä¸‹æ¸¸æ•°æ®ä¸Šå¯èƒ½æ€§æœ€é«˜çš„ä¸€ä¸ªã€‚å…³é”®çš„æ˜¯ï¼Œè¿™å°†æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾é™åˆ¶åœ¨PEFTæ ·æœ¬çš„æœ‰é™é›†åˆä¸­ã€‚ä¸ç¥ç»ç½‘ç»œæƒé‡çš„å…¸å‹è¿ç»­å‡è®¾ç©ºé—´ç›¸æ¯”ï¼Œè¿™æœ‰åŠ©äºæ›´ç´§å¯†çš„é£é™©è¯ä¹¦ã€‚æˆ‘ä»¬å®ç°äº†æˆ‘ä»¬çš„ç•Œé™ï¼Œå¹¶æ˜¾ç¤ºå‡ºä¸éç©ºæ´æ³›åŒ–ä¿è¯çš„ç°æœ‰å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ä½å°„å‡»çŠ¶æ€ä¸‹å…·æœ‰éå¹³å‡¡æ³›åŒ–ä¿è¯çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06970v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¾¿äºä¸‹æ¸¸ä»»åŠ¡åœ¨éç©ºæ³›åŒ–çš„å­¦ä¹ ç†è®ºæ³›åŒ–ä¿è¯ï¼Œå³ä½¿åœ¨ä½æ•°æ®é‡æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ã€‚é€šè¿‡ä¸Šæ¸¸ä»»åŠ¡è®­ç»ƒå‚æ•°æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨é‡‡æ ·è¯„ä¼°ç¨‹åºå­¦ä¹ ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·å¯èƒ½çš„å¾®è°ƒå‚æ•°ï¼Œé€‰æ‹©åœ¨ä¸‹æ¸¸æ•°æ®ä¸Šå¯èƒ½æ€§æœ€é«˜çš„ä¸€ä¸ªã€‚é€šè¿‡å°†æ¨¡å‹å‡è®¾é™åˆ¶åœ¨æœ‰é™çš„å¾®è°ƒå‚æ•°æ ·æœ¬é›†ä¸Šï¼Œä¸ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæƒé‡è¿ç»­å‡è®¾ç©ºé—´ç›¸æ¯”ï¼Œèƒ½æä¾›æ›´ä¸¥æ ¼çš„é£é™©è¯ä¹¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“ä»£æ·±åº¦å­¦ä¹ ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è¿›è¡Œå‚æ•°æœ‰æ•ˆå¾®è°ƒæ˜¯è§£å†³ä½æ•°æ®é—®é¢˜çš„ä¸€ç§æµè¡Œè€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹æ³›åŒ–ä¿è¯ï¼Œæ— æ³•åœ¨é“å¾·æˆ–æ³•å¾‹åŸå› è¦æ±‚åœ¨é‡è¦åº”ç”¨ä¸­éƒ¨ç½²æ—¶è¯æ˜å…¶å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä¸Šæ¸¸ä»»åŠ¡è®­ç»ƒå‚æ•°æ‰©æ•£æ¨¡å‹ä»¥æä¾›éç©ºæ³›åŒ–çš„å­¦ä¹ ç†è®ºæ³›åŒ–ä¿è¯ã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨é‡‡æ ·è¯„ä¼°ç¨‹åºï¼Œä»è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­é‡‡æ ·å¯èƒ½çš„å¾®è°ƒå‚æ•°ï¼Œé€‰æ‹©é€‚åº”ä¸‹æ¸¸æ•°æ®çš„æœ€ä½³å‚æ•°ã€‚</li>
<li>é€šè¿‡å°†æ¨¡å‹å‡è®¾é™åˆ¶åœ¨æœ‰é™çš„å¾®è°ƒå‚æ•°æ ·æœ¬é›†ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½æä¾›æ›´ä¸¥æ ¼çš„é£é™©è¯ä¹¦ã€‚</li>
<li>ä¸ç°æœ‰å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä½æ•°æ®é‡æƒ…å†µä¸‹æä¾›äº†éæ³›åŒ–çš„æ³›åŒ–ä¿è¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e314c1973a57720001bef4205474ac6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a290d69f485a0ae3558565b78575514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d6a3d3ef9e66308bf4084c32257d39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9add275847994a2c8dd1a1cc6e4d2f5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation"><a href="#FitCF-A-Framework-for-Automatic-Feature-Importance-guided-Counterfactual-Example-Generation" class="headerlink" title="FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation"></a>FitCF: A Framework for Automatic Feature Importance-guided   Counterfactual Example Generation</h2><p><strong>Authors:Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian MÃ¶ller, Vera Schmitt</strong></p>
<p>Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCFâ€™s core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction. </p>
<blockquote>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œåäº‹å®ä¾‹å­è¢«å¹¿æ³›åº”ç”¨äºæ”¹è¿›æ¨¡å‹çš„æ•°æ®ï¼ŒåŒæ—¶ä¹Ÿåœ¨å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­è¢«ç”¨æ¥ç†è§£æ¨¡å‹è¡Œä¸ºã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œä½†è‡ªåŠ¨ç”Ÿæˆåäº‹å®ä¾‹å­ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†ZeroCFï¼Œè¿™æ˜¯ä¸€ç§å¿ å®çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä»ç‰¹å¾å½’å› æ–¹æ³•æ´¾ç”Ÿå‡ºçš„é‡è¦å•è¯ï¼Œåœ¨æ— æ ·æœ¬ç¯å¢ƒä¸­ç”Ÿæˆåäº‹å®ä¾‹å­ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶FitCFï¼Œå®ƒé€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯è¿›ä¸€æ­¥éªŒè¯äº†ä¸Šè¿°çš„åäº‹å®ï¼Œç„¶åå°†å®ƒä»¬ä½œä¸ºæ¼”ç¤ºç”¨äºå°æ ·æœ¬æç¤ºï¼Œè¶…è¶Šäº†ä¸¤ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®å®šäº†FitCFçš„æ¯ä¸ªæ ¸å¿ƒç»„ä»¶åœ¨æé«˜åäº‹å®è´¨é‡æ–¹é¢çš„é‡è¦æ€§ï¼Œè¿™é€šè¿‡ç¿»è½¬ç‡ã€å›°æƒ‘åº¦å’Œç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LIMEå’Œé›†æˆæ¢¯åº¦ä½œä¸ºFitCFçš„éª¨å¹²å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°æ¼”ç¤ºçš„æ•°é‡å¯¹æ€§èƒ½çš„å½±å“æœ€å¤§ã€‚æœ€åï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç‰¹å¾å½’å› åˆ†æ•°å¿ å®æ€§ä¸ç”Ÿæˆåäº‹å®è´¨é‡ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™å°†æˆä¸ºæœªæ¥ç ”ç©¶çš„é‡è¦å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00777v3">PDF</a> ACL 2025 Findings; camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­å¹¿æ³›åº”ç”¨çš„åäº‹å®ç¤ºä¾‹ã€‚æ–‡ç« æå‡ºäº†ZeroCFæ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å¾å½’å±æ–¹æ³•å¾—å‡ºçš„é‡è¦è¯æ±‡åœ¨æ— æ ·æœ¬ç¯å¢ƒä¸‹ç”Ÿæˆåäº‹å®ç¤ºä¾‹ã€‚è¿›ä¸€æ­¥ï¼Œæ–‡ç« æå‡ºäº†FitCFæ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯å‰è¿°åäº‹å®ï¼Œå¹¶å°†å…¶ä½œä¸ºæ¼”ç¤ºç”¨äºå°æ ·æœ¬æç¤ºï¼Œä¼˜äºä¸¤ä¸ªå…ˆè¿›åŸºçº¿ã€‚æ–‡ç« è¿˜é€šè¿‡æ¶ˆèç ”ç©¶ç¡®å®šäº†FitCFæ ¸å¿ƒç»„ä»¶åœ¨æé«˜åäº‹å®è´¨é‡æ–¹é¢çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†LIMEå’ŒIntegrated Gradientsä½œä¸ºFitCFéª¨å¹²å½’å±æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæ–‡ç« æ­ç¤ºäº†ç‰¹å¾å½’å±åˆ†æ•°å¿ å®æ€§ä¸ç”Ÿæˆåäº‹å®è´¨é‡ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Counterfactual examples are valuable in NLP and XAI.</li>
<li>ZeroCFæ–¹æ³•åˆ©ç”¨ç‰¹å¾å½’å±æ–¹æ³•çš„é‡è¦è¯æ±‡ç”Ÿæˆåäº‹å®ç¤ºä¾‹ã€‚</li>
<li>FitCFæ¡†æ¶é€šè¿‡æ ‡ç­¾ç¿»è½¬éªŒè¯åäº‹å®ï¼Œå¹¶ç”¨äºå°æ ·æœ¬æç¤ºã€‚</li>
<li>FitCFä¼˜äºå…¶ä»–å…ˆè¿›åŸºçº¿ï¼Œå…¶æœ‰æ•ˆæ€§é€šè¿‡æ¶ˆèç ”ç©¶å¾—åˆ°è¯å®ã€‚</li>
<li>LIMEå’ŒIntegrated Gradientsæ˜¯æœ‰æ•ˆçš„éª¨å¹²å½’å±æ–¹æ³•ã€‚</li>
<li>æ¼”ç¤ºçš„æ•°é‡å¯¹æ€§èƒ½å½±å“æœ€å¤§ã€‚</li>
<li>ç‰¹å¾å½’å±åˆ†æ•°çš„å¿ å®æ€§ä¸ç”Ÿæˆçš„åäº‹å®è´¨é‡ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c53e4026e252ae806c528c0c257cda3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b345248bca34006338e17f33b5467a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a1f76aac418934b2c42a9ef942eacd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering"><a href="#FocusChat-Text-guided-Long-Video-Understanding-via-Spatiotemporal-Information-Filtering" class="headerlink" title="FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering"></a>FocusChat: Text-guided Long Video Understanding via Spatiotemporal   Information Filtering</h2><p><strong>Authors:Zheng Cheng, Rendong Wang, Zhicheng Wang</strong></p>
<p>Recently, multi-modal large language models have made significant progress. However, visual information lacking of guidance from the userâ€™s intention may lead to redundant computation and involve unnecessary visual noise, especially in long, untrimmed videos. To address this issue, we propose FocusChat, a text-guided multi-modal large language model (LLM) that emphasizes visual information correlated to the userâ€™s prompt. In detail, Our model first undergoes the semantic extraction module, which comprises a visual semantic branch and a text semantic branch to extract image and text semantics, respectively. The two branches are combined using the Spatial-Temporal Filtering Module (STFM). STFM enables explicit spatial-level information filtering and implicit temporal-level feature filtering, ensuring that the visual tokens are closely aligned with the userâ€™s query. It lowers the essential number of visual tokens inputted into the LLM. FocusChat significantly outperforms Video-LLaMA in zero-shot experiments, using an order of magnitude less training data with only 16 visual tokens occupied. It achieves results comparable to the state-of-the-art in few-shot experiments, with only 0.72M pre-training data. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç¼ºä¹ç”¨æˆ·æ„å›¾æŒ‡å¯¼çš„è§†è§‰ä¿¡æ¯å¯èƒ½å¯¼è‡´å†—ä½™è®¡ç®—å¹¶å¼•å…¥ä¸å¿…è¦çš„è§†è§‰å™ªéŸ³ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è€Œæ— å‰ªè¾‘çš„è§†é¢‘ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FocusChatï¼Œè¿™æ˜¯ä¸€ä¸ªå—æ–‡æœ¬å¼•å¯¼çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ƒå¼ºè°ƒä¸ç”¨æˆ·æç¤ºç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é¦–å…ˆç»è¿‡è¯­ä¹‰æå–æ¨¡å—ï¼Œè¯¥æ¨¡å—åŒ…æ‹¬ä¸€ä¸ªè§†è§‰è¯­ä¹‰åˆ†æ”¯å’Œä¸€ä¸ªæ–‡æœ¬è¯­ä¹‰åˆ†æ”¯ï¼Œåˆ†åˆ«æå–å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ã€‚è¿™ä¸¤ä¸ªåˆ†æ”¯é€šè¿‡æ—¶ç©ºæ»¤æ³¢æ¨¡å—ï¼ˆSTFMï¼‰è¿›è¡Œç»“åˆã€‚STFMå®ç°äº†æ˜¾å¼çš„ç©ºé—´çº§ä¿¡æ¯æ»¤æ³¢å’Œéšå¼çš„ç‰¹å¾çº§æ—¶é—´æ»¤æ³¢ï¼Œç¡®ä¿è§†è§‰ä»¤ç‰Œä¸ç”¨æˆ·æŸ¥è¯¢ç´§å¯†å¯¹é½ã€‚å®ƒé™ä½äº†è¾“å…¥LLMçš„å¿…è¦è§†è§‰ä»¤ç‰Œæ•°é‡ã€‚FocusChatåœ¨é›¶æ ·æœ¬å®éªŒä¸­æ˜¾è‘—ä¼˜äºVideo-LLaMAï¼Œä½¿ç”¨æ•°é‡çº§çš„è®­ç»ƒæ•°æ®é‡æ›´å°‘ï¼Œä»…å ç”¨16ä¸ªè§†è§‰ä»¤ç‰Œã€‚åœ¨å°‘é‡æ ·æœ¬å®éªŒä¸­ï¼Œå…¶è¾¾åˆ°çš„ç»“æœä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼Œä»…ä½¿ç”¨0.72Mçš„é¢„è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12833v2">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†ä¸€ç§åä¸ºFocusChatçš„æ–‡æœ¬å¼•å¯¼å¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯¥æ¨¡å‹å¼ºè°ƒä¸ç”¨æˆ·æç¤ºç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚å®ƒé€šè¿‡è¯­ä¹‰æå–æ¨¡å—å’Œç©ºé—´æ—¶é—´è¿‡æ»¤æ¨¡å—ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ç›¸ç»“åˆï¼Œç¡®ä¿è§†è§‰ä»¤ç‰Œä¸ç”¨æˆ·æŸ¥è¯¢ç´§å¯†å¯¹é½ï¼Œé™ä½è¾“å…¥LLMçš„å¿…éœ€è§†è§‰ä»¤ç‰Œæ•°é‡ã€‚FocusChatåœ¨é›¶æ ·æœ¬å®éªŒä¸­æ˜¾è‘—ä¼˜äºVideo-LLaMAï¼Œä½¿ç”¨æ•°é‡çº§è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ï¼Œä»…æœ‰16ä¸ªè§†è§‰ä»¤ç‰Œã€‚åœ¨å°‘é‡æ ·æœ¬å®éªŒä¸­ï¼Œå®ƒè¾¾åˆ°äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„ç»“æœï¼Œåªæœ‰0.72Mçš„é¢„è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FocusChatæ˜¯ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åœ¨è§†é¢‘å¤„ç†ä¸­ç”±äºç”¨æˆ·æ„å›¾ç¼ºä¹æŒ‡å¯¼è€Œå¯¼è‡´çš„å†—ä½™è®¡ç®—å’Œä¸å¿…è¦çš„è§†è§‰å™ªéŸ³é—®é¢˜ã€‚</li>
<li>FocusChaté€šè¿‡è¯­ä¹‰æå–æ¨¡å—æå–å›¾åƒå’Œæ–‡æœ¬è¯­ä¹‰ï¼Œå¹¶é€šè¿‡ç©ºé—´æ—¶é—´è¿‡æ»¤æ¨¡å—è¿›è¡Œç»“åˆï¼Œç¡®ä¿è§†è§‰ä¿¡æ¯ä¸ç”¨æˆ·æŸ¥è¯¢å¯¹é½ã€‚</li>
<li>è¯¥æ¨¡å‹é™ä½äº†è¾“å…¥å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿…è¦è§†è§‰ä»¤ç‰Œæ•°é‡ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬å®éªŒä¸­ï¼ŒFocusChatæ˜¾è‘—ä¼˜äºVideo-LLaMAï¼Œä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>FocusChatåœ¨å°‘é‡æ ·æœ¬å®éªŒä¸­è¡¨ç°å‡ºä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>FocusChatçš„é¢„è®­ç»ƒæ•°æ®é‡è¾ƒå°ï¼Œåªæœ‰0.72Mã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eff5fc66d84efc59a4158d53cc01bb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a731d522370a9647dfa7062a0505d69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8c5185d8c42e742825f63df215019bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f1f06fa00d3366c5b6739f379a7e110.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-85fb140468175d17f9cbfb75ebce09d6.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  UAVPairs A Challenging Benchmark for Match Pair Retrieval of   Large-scale UAV Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b65ecbcd674cfe162fe2029826adc661.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  MTVQA Benchmarking Multilingual Text-Centric Visual Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
