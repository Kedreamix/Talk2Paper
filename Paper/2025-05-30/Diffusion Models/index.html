<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  SPIRAL Semantic-Aware Progressive LiDAR Scene Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8743d83b99b208a00280e759a4cf6fcd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="SPIRAL-Semantic-Aware-Progressive-LiDAR-Scene-Generation"><a href="#SPIRAL-Semantic-Aware-Progressive-LiDAR-Scene-Generation" class="headerlink" title="SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation"></a>SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation</h2><p><strong>Authors:Dekai Zhu, Yixuan Hu, Youquan Liu, Dongyue Lu, Lingdong Kong, Slobodan Ilic</strong></p>
<p>Leveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, we propose Spiral, a novel range-view LiDAR diffusion model that simultaneously generates depth, reflectance images, and semantic maps. Furthermore, we introduce novel semantic-aware metrics to evaluate the quality of the generated labeled range-view data. Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, outperforming two-step methods that combine the generative and segmentation models. Additionally, we validate that range images generated by Spiral can be effectively used for synthetic data augmentation in the downstream segmentation training, significantly reducing the labeling effort on LiDAR data. </p>
<blockquote>
<p>åŸºäºæœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼Œæ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰çš„å¤§è§„æ¨¡ä¸‰ç»´åœºæ™¯ç”Ÿæˆå·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚è™½ç„¶åŸºäºä½“ç´ çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå‡ ä½•ç»“æ„å’Œè¯­ä¹‰æ ‡ç­¾ï¼Œä½†ç°æœ‰çš„èŒƒå›´è§†å›¾æ–¹æ³•ä»…é™äºç”Ÿæˆæ— æ ‡ç­¾çš„æ¿€å…‰é›·è¾¾åœºæ™¯ã€‚ä¾èµ–é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹æ¥é¢„æµ‹è¯­ä¹‰åœ°å›¾å¾€å¾€ä¼šå¯¼è‡´è·¨æ¨¡æ€ä¸€è‡´æ€§ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™èŒƒå›´è§†å›¾è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼Œå¦‚è®¡ç®—æ•ˆç‡å’Œç½‘ç»œè®¾è®¡ç®€åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†Spiralï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„èŒƒå›´è§†å›¾æ¿€å…‰é›·è¾¾æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆæ·±åº¦ã€åå°„å›¾åƒå’Œè¯­ä¹‰åœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°å‹çš„è¯­ä¹‰æ„ŸçŸ¥æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆçš„å¸¦æ ‡ç­¾èŒƒå›´è§†å›¾æ•°æ®çš„è´¨é‡ã€‚åœ¨SemanticKITTIå’ŒnuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpiralåœ¨å‚æ•°è§„æ¨¡æœ€å°çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œè¶…è¶Šäº†å°†ç”Ÿæˆæ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ç›¸ç»“åˆçš„ä¸¤æ­¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†Spiralç”Ÿæˆçš„èŒƒå›´å›¾åƒå¯ä»¥æœ‰æ•ˆåœ°ç”¨äºä¸‹æ¸¸åˆ†å‰²è®­ç»ƒä¸­çš„åˆæˆæ•°æ®å¢å¼ºï¼Œä»è€Œå¤§å¤§å‡å°‘æ¿€å…‰é›·è¾¾æ•°æ®çš„æ ‡æ³¨å·¥ä½œé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22643v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼ŒLiDARåŸºå¤§è§„æ¨¡3Dåœºæ™¯ç”Ÿæˆå·²å–å¾—äº†å·¨å¤§æˆåŠŸã€‚è™½ç„¶æœ€è¿‘çš„åŸºäºä½“ç´ çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå‡ ä½•ç»“æ„å’Œè¯­ä¹‰æ ‡ç­¾ï¼Œä½†ç°æœ‰çš„èŒƒå›´è§†å›¾æ–¹æ³•ä»…é™äºç”Ÿæˆæœªæ ‡è®°çš„LiDARåœºæ™¯ã€‚ä¾é é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹æ¥é¢„æµ‹è¯­ä¹‰åœ°å›¾å¾€å¾€ä¼šå¯¼è‡´è·¨æ¨¡æ€ä¸€è‡´æ€§è¾ƒå·®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼ŒåŒæ—¶ä¿ç•™èŒƒå›´è§†å›¾è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼ˆå¦‚è®¡ç®—æ•ˆç‡é«˜å’Œç®€åŒ–ç½‘ç»œè®¾è®¡ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†Spiralï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„èŒƒå›´è§†å›¾LiDARæ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆæ·±åº¦ã€åå°„å›¾åƒå’Œè¯­ä¹‰åœ°å›¾ã€‚å®éªŒè¡¨æ˜ï¼ŒSpiralåœ¨SemanticKITTIå’ŒnuScenesæ•°æ®é›†ä¸Šæ€§èƒ½å¤„äºé¢†å…ˆåœ°ä½ï¼Œå‚æ•°è§„æ¨¡æœ€å°ï¼Œä¸”ä¼˜äºç»“åˆç”Ÿæˆæ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹çš„ä¸¤æ­¥æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†Spiralç”Ÿæˆçš„èŒƒå›´å›¾åƒå¯ä»¥æœ‰æ•ˆåœ°ç”¨äºä¸‹æ¸¸åˆ†å‰²è®­ç»ƒä¸­çš„åˆæˆæ•°æ®å¢å¼ºï¼Œå¤§å¤§å‡å°‘LiDARæ•°æ®çš„æ ‡æ³¨å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼ŒLiDARåŸºå¤§è§„æ¨¡3Dåœºæ™¯ç”Ÿæˆå–å¾—äº†æˆåŠŸã€‚</li>
<li>ç°æœ‰çš„èŒƒå›´è§†å›¾æ–¹æ³•ä¸»è¦ç”Ÿæˆæœªæ ‡è®°çš„LiDARåœºæ™¯ã€‚</li>
<li>Spiralæ˜¯ä¸€ç§æ–°å‹çš„èŒƒå›´è§†å›¾LiDARæ‰©æ•£æ¨¡å‹ï¼Œèƒ½åŒæ—¶ç”Ÿæˆæ·±åº¦ã€åå°„å›¾åƒå’Œè¯­ä¹‰åœ°å›¾ã€‚</li>
<li>Spiralåœ¨è¯­ä¹‰åœ°å›¾ç”Ÿæˆæ–¹é¢æ€§èƒ½é¢†å…ˆï¼Œå‚æ•°è§„æ¨¡è¾ƒå°ã€‚</li>
<li>Spiralä¸ç»“åˆç”Ÿæˆæ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹çš„ä¸¤æ­¥æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>Spiralç”Ÿæˆçš„èŒƒå›´å›¾åƒå¯ç”¨äºä¸‹æ¸¸åˆ†å‰²è®­ç»ƒä¸­çš„åˆæˆæ•°æ®å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1eb20ae14d641e9192f14a8c9588345f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e3e2ad8d7b2f53398efdbbb4dbfec9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5814703a6e50ea96ec72bd631c8b4930.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead31c3ac1dc9a010dd362c1475b54fa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ImageReFL-Balancing-Quality-and-Diversity-in-Human-Aligned-Diffusion-Models"><a href="#ImageReFL-Balancing-Quality-and-Diversity-in-Human-Aligned-Diffusion-Models" class="headerlink" title="ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion   Models"></a>ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion   Models</h2><p><strong>Authors:Dmitrii Sorokin, Maksim Nakhodnov, Andrey Kuznetsov, Aibek Alanov</strong></p>
<p>Recent advances in diffusion models have led to impressive image generation capabilities, but aligning these models with human preferences remains challenging. Reward-based fine-tuning using models trained on human feedback improves alignment but often harms diversity, producing less varied outputs. In this work, we address this trade-off with two contributions. First, we introduce \textit{combined generation}, a novel sampling strategy that applies a reward-tuned diffusion model only in the later stages of the generation process, while preserving the base model for earlier steps. This approach mitigates early-stage overfitting and helps retain global structure and diversity. Second, we propose \textit{ImageReFL}, a fine-tuning method that improves image diversity with minimal loss in quality by training on real images and incorporating multiple regularizers, including diffusion and ReFL losses. Our approach outperforms conventional reward tuning methods on standard quality and diversity metrics. A user study further confirms that our method better balances human preference alignment and visual diversity. The source code can be found at <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/ImageReFL">https://github.com/ControlGenAI/ImageReFL</a> . </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä¸ºå›¾åƒç”Ÿæˆèƒ½åŠ›å¸¦æ¥äº†æ·±åˆ»å½±å“ï¼Œä½†æ˜¯å¦‚ä½•å°†è¿™äº›æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä½¿ç”¨ç»è¿‡äººç±»åé¦ˆè®­ç»ƒçš„æ¨¡å‹è¿›è¡ŒåŸºäºå¥–åŠ±çš„å¾®è°ƒï¼Œå¯ä»¥æé«˜å¯¹é½æ€§ï¼Œä½†å¾€å¾€ä¼šæŸå®³å¤šæ ·æ€§ï¼Œå¯¼è‡´è¾“å‡ºç»“æœè¾ƒä¸ºå•ä¸€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªè´¡çŒ®æ¥è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†<em>ç»„åˆç”Ÿæˆ</em>è¿™ä¸€æ–°å‹é‡‡æ ·ç­–ç•¥ï¼Œåªåœ¨ç”Ÿæˆè¿‡ç¨‹çš„åæœŸåº”ç”¨ç»è¿‡å¥–åŠ±è°ƒæ•´çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™æ—©æœŸæ­¥éª¤çš„åŸºå‡†æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å‡è½»äº†æ—©æœŸé˜¶æ®µçš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œæœ‰åŠ©äºä¿æŒå…¨å±€ç»“æ„å’Œå¤šæ ·æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º<em>ImageReFL</em>çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨å®é™…å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒå¹¶å¼•å…¥å¤šç§æ­£åˆ™åŒ–å™¨ï¼ˆåŒ…æ‹¬æ‰©æ•£å’ŒReFLæŸå¤±ï¼‰æ¥æé«˜å›¾åƒå¤šæ ·æ€§ï¼ŒåŒæ—¶å°½é‡å‡å°‘è´¨é‡æŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†è´¨é‡å’Œå¤šæ ·æ€§æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå¥–åŠ±è°ƒæ•´æ–¹æ³•ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ›´å¥½åœ°å¹³è¡¡äº†äººç±»åå¥½å¯¹é½å’Œè§†è§‰å¤šæ ·æ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/ImageReFL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ControlGenAI/ImageReFLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22569v1">PDF</a> The source code can be found at   <a target="_blank" rel="noopener" href="https://github.com/ControlGenAI/ImageReFL">https://github.com/ControlGenAI/ImageReFL</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•ä¸ºå›¾åƒç”Ÿæˆèƒ½åŠ›å¸¦æ¥äº†æ˜¾è‘—æå‡ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åŸºäºå¥–åŠ±çš„å¾®è°ƒä½¿ç”¨è®­ç»ƒåœ¨äººç±»åé¦ˆä¸Šçš„æ¨¡å‹æ”¹å–„äº†å¯¹é½æ€§ï¼Œä½†å¸¸å¸¸æŸå®³å¤šæ ·æ€§ï¼Œå¯¼è‡´è¾“å‡ºè¾ƒå°‘å˜åŒ–ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸¤ä¸ªè´¡çŒ®æ¥è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥â€œç»„åˆç”Ÿæˆâ€è¿™ä¸€æ–°é¢–é‡‡æ ·ç­–ç•¥ï¼Œä»…åœ¨ç”Ÿæˆè¿‡ç¨‹çš„åæœŸåº”ç”¨å¥–åŠ±è°ƒæ•´çš„æ‰©æ•£æ¨¡å‹ï¼Œè€Œåœ¨æ—©æœŸé˜¶æ®µä¿ç•™åŸºç¡€æ¨¡å‹ï¼Œä»¥å‡è½»æ—©æœŸé˜¶æ®µçš„è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶æœ‰åŠ©äºä¿ç•™å…¨å±€ç»“æ„å’Œå¤šæ ·æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºâ€œImageReFLâ€è¿™ä¸€å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒçœŸå®å›¾åƒå¹¶èå…¥åŒ…æ‹¬æ‰©æ•£å’ŒReFLæŸå¤±åœ¨å†…çš„å¤šä¸ªè§„åˆ™åŒ–å™¨ï¼Œåœ¨å‡ ä¹ä¸æŸå¤±è´¨é‡çš„æƒ…å†µä¸‹æé«˜å›¾åƒå¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†è´¨é‡å’Œå¤šæ ·æ€§æŒ‡æ ‡ä¸Šä¼˜äºä¼ ç»Ÿå¥–åŠ±è°ƒæ•´æ–¹æ³•ï¼Œç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®æˆ‘ä»¬çš„æ–¹æ³•æ›´å¥½åœ°å¹³è¡¡äº†äººç±»åå¥½å¯¹é½å’Œè§†è§‰å¤šæ ·æ€§ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹é½äººç±»åå¥½ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>å¥–åŠ±å¾®è°ƒèƒ½æé«˜æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ï¼Œä½†å¯èƒ½æŸå®³è¾“å‡ºå¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥â€œç»„åˆç”Ÿæˆâ€ç­–ç•¥ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹åæœŸåº”ç”¨å¥–åŠ±è°ƒæ•´çš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥ä¿ç•™å…¨å±€ç»“æ„å’Œå¤šæ ·æ€§ã€‚</li>
<li>æå‡ºâ€œImageReFLâ€å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒçœŸå®å›¾åƒå¹¶èå…¥å¤šä¸ªè§„åˆ™åŒ–å™¨ï¼Œæé«˜å›¾åƒå¤šæ ·æ€§åŒæ—¶å‡ ä¹ä¸æŸå¤±è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†è´¨é‡å’Œå¤šæ ·æ€§æŒ‡æ ‡ä¸Šè¶…è¶Šä¼ ç»Ÿå¥–åŠ±è°ƒæ•´æ–¹æ³•ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¯å®è¯¥æ–¹æ³•åœ¨å¹³è¡¡äººç±»åå¥½å¯¹é½å’Œè§†è§‰å¤šæ ·æ€§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9352a2053a942fc710647f2337a04a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08e5f4089ea79e9fbd45401029717fa4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35920a3a8b15af291e827fbc2f3123eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8bc8e99c71eb19bbf63d6dc31b54232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1913e3c529e519d8766eca969ba8776f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PrismLayers-Open-Data-for-High-Quality-Multi-Layer-Transparent-Image-Generative-Models"><a href="#PrismLayers-Open-Data-for-High-Quality-Multi-Layer-Transparent-Image-Generative-Models" class="headerlink" title="PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image   Generative Models"></a>PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image   Generative Models</h2><p><strong>Authors:Junwen Chen, Heyang Jiang, Yanbin Wang, Keming Wu, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, Yuhui Yuan</strong></p>
<p>Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery. </p>
<blockquote>
<p>ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡ã€å¤šå±‚é€æ˜å›¾åƒå¯ä»¥è§£é”ä¸€ä¸ªæ–°çš„åˆ›æ„æ§åˆ¶çº§åˆ«ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåƒç¼–è¾‘æ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºä¸€æ ·è½»æ¾åœ°ç¼–è¾‘æ¯ä¸ªå›¾å±‚ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘å¤§å‹é«˜è´¨é‡çš„å¤šå±‚é€æ˜æ•°æ®é›†ï¼Œå¤šå±‚ç”Ÿæˆæ¨¡å‹çš„å‘å±•æ»åäºä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³äº†è¿™ä¸€åŸºæœ¬æŒ‘æˆ˜ï¼š(i)å‘å¸ƒäº†ç¬¬ä¸€ä¸ªå¼€æ”¾ã€è¶…é«˜ä¿çœŸåº¦çš„PrismLayersï¼ˆPrismLayersProï¼‰æ•°æ®é›†ï¼ŒåŒ…å«20ä¸‡ä¸ªï¼ˆ2ä¸‡ä¸ªï¼‰å¸¦æœ‰ç²¾ç¡®alphaè’™ç‰ˆçš„å¤šå±‚é€æ˜å›¾åƒï¼›(ii)å¼•å…¥äº†ä¸€ä¸ªæŒ‰éœ€ç”Ÿæˆæ­¤ç±»æ•°æ®çš„æ— è®­ç»ƒåˆæˆç®¡é“ï¼Œè¯¥ç®¡é“ä½¿ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹ï¼›(iii)å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„å¼€æºå¤šå±‚ç”Ÿæˆæ¨¡å‹ART+ï¼Œè¯¥æ¨¡å‹ä¸ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®¡ç¾ç›¸åŒ¹é…ã€‚å…³é”®çš„æŠ€æœ¯è´¡çŒ®åŒ…æ‹¬ï¼šLayerFLUXï¼Œå®ƒæ“…é•¿ç”Ÿæˆå…·æœ‰ç²¾ç¡®alphaè’™ç‰ˆçš„é«˜è´¨é‡å•ä¸€é€æ˜å›¾å±‚ï¼›ä»¥åŠMultiLayerFLUXï¼Œå®ƒå°†å¤šä¸ªLayerFLUXè¾“å‡ºç»„åˆæˆå®Œæ•´çš„å›¾åƒï¼Œç”±äººç±»æ³¨é‡Šçš„è¯­ä¹‰å¸ƒå±€å¼•å¯¼ã€‚ä¸ºäº†ç¡®ä¿æ›´é«˜çš„è´¨é‡ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸€ä¸ªä¸¥æ ¼çš„è¿‡æ»¤é˜¶æ®µæ¥æ¶ˆé™¤ä¼ªå½±å’Œè¯­ä¹‰ä¸åŒ¹é…ï¼Œéšåè¿›è¡Œäººå·¥é€‰æ‹©ã€‚åœ¨æˆ‘ä»¬åˆæˆçš„PrismLayersProæ•°æ®é›†ä¸Šå¯¹æœ€å…ˆè¿›çš„ARTæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°äº†ART+ï¼Œåœ¨å¤´å¯¹å¤´çš„ç”¨æˆ·ç ”ç©¶æ¯”è¾ƒä¸­ï¼ŒART+åœ¨60%çš„æƒ…å†µä¸‹è¶…è¶Šäº†åŸå§‹ARTçš„è¡¨ç°ï¼Œç”šè‡³ä¸FLUXç”Ÿæˆçš„å›¾åƒè§†è§‰è´¨é‡ç›¸åŒ¹é…ã€‚æˆ‘ä»¬é¢„æœŸï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºå¤šå±‚é€æ˜å›¾åƒç”Ÿæˆä»»åŠ¡å»ºç«‹åšå®çš„æ•°æ®é›†åŸºç¡€ï¼Œä¸ºéœ€è¦ç²¾ç¡®ã€å¯ç¼–è¾‘å’Œè§†è§‰å¸å¼•äººçš„åˆ†å±‚å›¾åƒçš„ç ”ç©¶å’Œåº”ç”¨æä¾›æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22523v1">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://prism-layers.github.io/">https://prism-layers.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡è§£å†³äº†å¤šå±‚é€æ˜å›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡å»ºç«‹é¦–ä¸ªå…¬å¼€çš„ã€è¶…é«˜ä¿çœŸåº¦çš„PrismLayersï¼ˆPrismLayersProï¼‰æ•°æ®é›†ï¼Œå¼•å…¥äº†æ— éœ€è®­ç»ƒçš„åˆæˆç®¡é“ï¼Œå¹¶ä½¿ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹æŒ‰éœ€ç”Ÿæˆæ•°æ®ã€‚åŒæ—¶ï¼Œæ¨å‡ºå¼ºå¤§çš„å¼€æºå¤šå±‚ç”Ÿæˆæ¨¡å‹ART+ï¼Œä¸ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®¡ç¾ç›¸åŒ¹é…ã€‚ä¸»è¦æŠ€æœ¯è´¡çŒ®åŒ…æ‹¬LayerFLUXå’ŒMultiLayerFLUXï¼Œå‰è€…æ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„å•å±‚é€æ˜å›¾åƒï¼Œåè€…èƒ½å°†å¤šä¸ªLayerFLUXè¾“å‡ºç»„åˆæˆå®Œæ•´çš„å›¾åƒã€‚ç»è¿‡ä¸¥æ ¼ç­›é€‰å’Œäººå·¥é€‰æ‹©ï¼Œç¡®ä¿å›¾åƒè´¨é‡ã€‚åœ¨åˆæˆPrismLayersProæ•°æ®é›†ä¸Šå¯¹æœ€å…ˆè¿›çš„ARTæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°ART+ï¼Œåœ¨å¤´å¯¹å¤´ç”¨æˆ·ç ”ç©¶æ¯”è¾ƒä¸­ï¼ŒART+åœ¨60%çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºåŸå§‹ARTï¼Œç”šè‡³ä¸FLUX.1-[dev]æ¨¡å‹ç”Ÿæˆçš„å›¾åƒè§†è§‰è´¨é‡ç›¸åŒ¹é…ã€‚æœ¬æ–‡çš„å·¥ä½œå°†ä¸ºå¤šå±‚é€æ˜å›¾åƒç”Ÿæˆä»»åŠ¡å»ºç«‹åšå®çš„æ•°æ®é›†åŸºç¡€ï¼Œä¸ºéœ€è¦ç²¾ç¡®ã€å¯ç¼–è¾‘å’Œè§†è§‰å¸å¼•åŠ›å¼ºçš„åˆ†å±‚å›¾åƒçš„ç ”ç©¶å’Œåº”ç”¨æä¾›æ”¯æŒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡è§£å†³äº†å¤šå±‚é€æ˜å›¾åƒç”Ÿæˆé¢†åŸŸå› ç¼ºä¹å¤§å‹é«˜è´¨é‡æ•°æ®é›†è€Œæ»åçš„é—®é¢˜ï¼Œå‘å¸ƒäº†é¦–ä¸ªè¶…é«˜ä¿çœŸåº¦çš„PrismLayersï¼ˆPrismLayersProï¼‰æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åˆæˆç®¡é“ï¼Œåˆ©ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹æŒ‰éœ€ç”Ÿæˆæ•°æ®ã€‚</li>
<li>æ¨å‡ºäº†å¼ºå¤§çš„å¼€æºå¤šå±‚ç”Ÿæˆæ¨¡å‹ART+ï¼Œè¯¥æ¨¡å‹ä¸ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®¡ç¾ç›¸åŒ¹é…ã€‚</li>
<li>ä¸»è¦æŠ€æœ¯è´¡çŒ®åŒ…æ‹¬LayerFLUXå’ŒMultiLayerFLUXï¼Œå‰è€…èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å•å±‚é€æ˜å›¾åƒï¼Œåè€…èƒ½å°†å¤šä¸ªå›¾å±‚ç»„åˆæˆå®Œæ•´çš„å›¾åƒã€‚</li>
<li>é€šè¿‡ä¸¥æ ¼çš„ç­›é€‰é˜¶æ®µå’Œäººå·¥é€‰æ‹©ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚</li>
<li>ART+æ¨¡å‹åœ¨å¤´å¯¹å¤´ç”¨æˆ·ç ”ç©¶æ¯”è¾ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-afe6a1b01aec796d9df67380be1b82f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-419081e4a89832652e8c6d41d7dfdd95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-623341eaed4177f9c306774bb270843c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d7e026ab4c6005ebd268823882c4a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a29906f93f1a79a851dc9a0d0d113f18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50edafd65151ee23396170e2a0affc1f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cascaded-3D-Diffusion-Models-for-Whole-body-3D-18-F-FDG-PET-CT-synthesis-from-Demographics"><a href="#Cascaded-3D-Diffusion-Models-for-Whole-body-3D-18-F-FDG-PET-CT-synthesis-from-Demographics" class="headerlink" title="Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET&#x2F;CT synthesis   from Demographics"></a>Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET&#x2F;CT synthesis   from Demographics</h2><p><strong>Authors:Siyeop Yoon, Sifan Song, Pengfei Jin, Matthew Tivnan, Yujin Oh, Sekeun Kim, Dufan Wu, Xiang Li, Quanzheng Li</strong></p>
<p>We propose a cascaded 3D diffusion model framework to synthesize high-fidelity 3D PET&#x2F;CT volumes directly from demographic variables, addressing the growing need for realistic digital twins in oncologic imaging, virtual trials, and AI-driven data augmentation. Unlike deterministic phantoms, which rely on predefined anatomical and metabolic templates, our method employs a two-stage generative process. An initial score-based diffusion model synthesizes low-resolution PET&#x2F;CT volumes from demographic variables alone, providing global anatomical structures and approximate metabolic activity. This is followed by a super-resolution residual diffusion model that refines spatial resolution. Our framework was trained on 18-F FDG PET&#x2F;CT scans from the AutoPET dataset and evaluated using organ-wise volume and standardized uptake value (SUV) distributions, comparing synthetic and real data between demographic subgroups. The organ-wise comparison demonstrated strong concordance between synthetic and real images. In particular, most deviations in metabolic uptake values remained within 3-5% of the ground truth in subgroup analysis. These findings highlight the potential of cascaded 3D diffusion models to generate anatomically and metabolically accurate PET&#x2F;CT images, offering a robust alternative to traditional phantoms and enabling scalable, population-informed synthetic imaging for clinical and research applications. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§çº§è”çš„3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç›´æ¥ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆé«˜ä¿çœŸåº¦çš„3D PET&#x2F;CTä½“ç§¯å›¾åƒï¼Œä»è€Œæ»¡è¶³è‚¿ç˜¤æˆåƒã€è™šæ‹Ÿè¯•éªŒå’Œäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ•°æ®å¢å¼ºä¸­æ‰€éœ€é€¼çœŸçš„æ•°å­—åŒèƒèƒæ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚ä¸ä¾èµ–äºé¢„å®šä¹‰è§£å‰–å’Œä»£è°¢æ¨¡æ¿çš„ç¡®å®šæ€§å¹½çµï¼ˆphantomsï¼‰ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ã€‚åˆå§‹åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ä»…ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆä½åˆ†è¾¨ç‡çš„PET&#x2F;CTä½“ç§¯å›¾åƒï¼Œæä¾›å…¨çƒè§£å‰–ç»“æ„å’Œè¿‘ä¼¼ä»£è°¢æ´»åŠ¨ã€‚æ¥ä¸‹æ¥æ˜¯é«˜åˆ†è¾¨ç‡å‰©ä½™æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæé«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨AutoPETæ•°æ®é›†çš„18F FDG PET&#x2F;CTæ‰«æä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨å™¨å®˜ä½“ç§¯å’Œæ ‡å‡†æ‘„å–å€¼ï¼ˆSUVï¼‰åˆ†å¸ƒè¿›è¡Œè¯„ä¼°ã€‚åœ¨äººå£ç»Ÿè®¡å­¦åˆ†ç»„ä¸­æ¯”è¾ƒåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®çš„å™¨å®˜é—´å·®å¼‚ï¼Œåˆæˆå›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´æ˜¾ç¤ºå‡ºå¼ºçƒˆçš„ä¸€è‡´æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨äºšç»„åˆ†æä¸­ï¼Œå¤§å¤šæ•°ä»£è°¢æ‘„å–å€¼çš„åå·®ä»ä¿æŒåœ¨çœŸå®å€¼çš„3-5%ä»¥å†…ã€‚è¿™äº›å‘ç°çªå‡ºäº†çº§è”çš„3Dæ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆè§£å‰–å­¦å’Œä»£è°¢ä¸Šå‡†ç¡®çš„PET&#x2F;CTå›¾åƒï¼Œä¸ºä¼ ç»Ÿå¹»å½±æä¾›äº†ä¸€ä¸ªç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å®ç°ç”¨äºä¸´åºŠå’Œç ”ç©¶åº”ç”¨çš„è§„æ¨¡åŒ–ã€ä»¥äººç¾¤ä¸ºåŸºç¡€çš„åˆæˆæˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22489v1">PDF</a> MICCAI2025 Submitted version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§çº§è”çš„3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå¯ç›´æ¥ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆé«˜ä¿çœŸåº¦çš„3D PET&#x2F;CTä½“ç§¯ï¼Œæ»¡è¶³è‚¿ç˜¤æˆåƒã€è™šæ‹Ÿè¯•éªŒå’ŒAIé©±åŠ¨çš„æ•°æ®å¢å¼ºä¸­å¯¹çœŸå®æ•°å­—åŒèƒèƒçš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼Œåˆå§‹åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆä½åˆ†è¾¨ç‡çš„PET&#x2F;CTä½“ç§¯ï¼Œæä¾›å…¨çƒè§£å‰–ç»“æ„å’Œå¤§è‡´çš„ä»£è°¢æ´»åŠ¨ï¼Œæ¥ç€ç”±è¶…åˆ†è¾¨ç‡æ®‹å·®æ‰©æ•£æ¨¡å‹æé«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚è¯¥æ¡†æ¶åœ¨AutoPETæ•°æ®é›†ä¸Šçš„è®­ç»ƒå’Œå¯¹ä¸åŒäººå£å­¦äºšç»„çš„å™¨å®˜ä½“ç§¯å’Œæ ‡å‡†æ‘„å–å€¼ï¼ˆSUVï¼‰åˆ†å¸ƒçš„è¯„ä¼°è¡¨æ˜ï¼Œåˆæˆå›¾åƒä¸çœŸå®å›¾åƒé—´è¡¨ç°å‡ºå¼ºçƒˆçš„ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£è°¢æ‘„å–å€¼çš„åå·®åˆ†æä¸­ï¼Œå¤§å¤šæ•°åå·®ä¿æŒåœ¨çœŸå®å€¼çš„3-5%ä»¥å†…ã€‚è¿™çªæ˜¾äº†çº§è”3Dæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§£å‰–å’Œä»£è°¢å‡†ç¡®çš„PET&#x2F;CTå›¾åƒæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºä¼ ç»Ÿå¹»å½±æä¾›äº†ä¸€ç§ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸ºä¸´åºŠå’Œç ”ç©¶åº”ç”¨æä¾›äº†å¯æ‰©å±•çš„ã€ä»¥äººç¾¤ä¸ºåŸºç¡€çš„ç»¼åˆæˆåƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§çº§è”çš„3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºä»äººå£ç»Ÿè®¡å­¦å˜é‡ä¸­åˆæˆé«˜ä¿çœŸåº¦çš„3D PET&#x2F;CTä½“ç§¯ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼Œå…ˆåˆæˆä½åˆ†è¾¨ç‡å›¾åƒï¼Œå†æé«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚</li>
<li>æ¡†æ¶åœ¨AutoPETæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°å‡ºåˆæˆå›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´å¼ºçƒˆçš„ä¸€è‡´æ€§ã€‚</li>
<li>åˆæˆå›¾åƒçš„å™¨å®˜ä½“ç§¯å’Œæ ‡å‡†æ‘„å–å€¼ï¼ˆSUVï¼‰åˆ†å¸ƒä¸çœŸå®æ•°æ®åœ¨äººå£å­¦äºšç»„ä¸­çš„æ¯”è¾ƒæ˜¾ç¤ºå‡ºäº†è‰¯å¥½çš„ä¸€è‡´æ€§ã€‚</li>
<li>å¤§å¤šæ•°ä»£è°¢æ‘„å–å€¼çš„åå·®ä¿æŒåœ¨çœŸå®å€¼çš„3-5%ä»¥å†…ã€‚</li>
<li>çº§è”3Dæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§£å‰–å’Œä»£è°¢å‡†ç¡®çš„PET&#x2F;CTå›¾åƒæ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-282cdeeb8931a6ae49ba0f02d708f4c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f262c057354c0cd219b81044d630a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98f7ee32a48acaf45986c1e3ace9583.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Reflective-Reinforcement-Learning-for-Diffusion-based-Image-Reasoning-Generation"><a href="#Self-Reflective-Reinforcement-Learning-for-Diffusion-based-Image-Reasoning-Generation" class="headerlink" title="Self-Reflective Reinforcement Learning for Diffusion-based Image   Reasoning Generation"></a>Self-Reflective Reinforcement Learning for Diffusion-based Image   Reasoning Generation</h2><p><strong>Authors:Jiadong Pan, Zhiyuan Ma, Kaiyan Zhang, Ning Ding, Bowen Zhou</strong></p>
<p>Diffusion models have recently demonstrated exceptional performance in image generation task. However, existing image generation methods still significantly suffer from the dilemma of image reasoning, especially in logic-centered image generation tasks. Inspired by the success of Chain of Thought (CoT) and Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL algorithm for diffusion models to achieve reasoning generation of logical images by performing reflection and iteration across generation trajectories. The intermediate samples in the denoising process carry noise, making accurate reward evaluation difficult. To address this challenge, SRRL treats the entire denoising trajectory as a CoT step with multi-round reflective denoising process and introduces condition guided forward process, which allows for reflective iteration between CoT steps. Through SRRL-based iterative diffusion training, we introduce image reasoning through CoT into generation tasks adhering to physical laws and unconventional physical phenomena for the first time. Notably, experimental results of case study exhibit that the superior performance of our SRRL algorithm even compared with GPT-4o. The project page is <a target="_blank" rel="noopener" href="https://jadenpan0.github.io/srrl.github.io/">https://jadenpan0.github.io/srrl.github.io/</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒç”Ÿæˆæ–¹æ³•ä»ç„¶é¢ä¸´ç€å›¾åƒæ¨ç†çš„å›°å¢ƒï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥é€»è¾‘ä¸ºä¸­å¿ƒçš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ã€‚å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SRRLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„è‡ªåæ€RLç®—æ³•ï¼Œé€šè¿‡åæ€å’Œè¿­ä»£ç”Ÿæˆè½¨è¿¹æ¥å®ç°é€»è¾‘å›¾åƒçš„æ¨ç†ç”Ÿæˆã€‚å»å™ªè¿‡ç¨‹ä¸­çš„ä¸­é—´æ ·æœ¬å¸¦æœ‰å™ªå£°ï¼Œä½¿å¾—å‡†ç¡®çš„å¥–åŠ±è¯„ä¼°å˜å¾—å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒSRRLå°†æ•´ä¸ªå»å™ªè½¨è¿¹è§†ä¸ºå¸¦æœ‰å¤šè½®åæ€å»å™ªè¿‡ç¨‹çš„CoTæ­¥éª¤ï¼Œå¹¶å¼•å…¥æ¡ä»¶å¼•å¯¼å‰å‘è¿‡ç¨‹ï¼Œè¿™å…è®¸åœ¨CoTæ­¥éª¤ä¹‹é—´è¿›è¡Œåæ€è¿­ä»£ã€‚é€šè¿‡åŸºäºSRRLçš„è¿­ä»£æ‰©æ•£è®­ç»ƒï¼Œæˆ‘ä»¬é¦–æ¬¡å°†éµå¾ªç‰©ç†å®šå¾‹å’Œéä¼ ç»Ÿç‰©ç†ç°è±¡çš„å›¾åƒæ¨ç†å¼•å…¥ç”Ÿæˆä»»åŠ¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¡ˆä¾‹ç ”ç©¶çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SRRLç®—æ³•æ€§èƒ½å“è¶Šï¼Œç”šè‡³è¶…è¶Šäº†GPT-4oã€‚é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://jadenpan0.github.io/srrl.github.%E3%80%82%E3%80%90%E5%AE%BD%E5%AD%A6%E7%AF%87%E5%A4%96%E7%BD%AE%E9%AB%98%E7%BA%A7-%E4%B8%AD%E5%BC%A5/oracingyoho/">https://jadenpan0.github.io/srrl.github.io/ã€‚</a>.</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22407v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨é€»è¾‘å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ä»é¢ä¸´å›¾åƒæ¨ç†çš„å›°å¢ƒã€‚æœ¬ç ”ç©¶ç»“åˆChain of Thoughtï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¼˜ç‚¹ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æå‡ºè‡ªåå°„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆSRRLï¼‰ï¼Œå®ç°é€»è¾‘å›¾åƒçš„æ¨ç†ç”Ÿæˆã€‚é€šè¿‡åœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­å¼•å…¥å¤šè½®åå°„å»å™ªè¿‡ç¨‹å’Œæ¡ä»¶å¼•å¯¼å‰å‘è¿‡ç¨‹ï¼Œè§£å†³äº†ä¸­é—´æ ·æœ¬å»å™ªè¿‡ç¨‹ä¸­å­˜åœ¨çš„å™ªå£°é—®é¢˜ã€‚åŸºäºSRRLçš„è¿­ä»£æ‰©æ•£è®­ç»ƒï¼Œé¦–æ¬¡å°†å›¾åƒæ¨ç†é€šè¿‡CoTå¼•å…¥éµå¾ªç‰©ç†å®šå¾‹å’Œéä¼ ç»Ÿç‰©ç†ç°è±¡çš„ç”Ÿæˆä»»åŠ¡ä¸­ã€‚å®éªŒæ¡ˆä¾‹ç ”ç©¶ç»“æœæ˜¾ç¤ºSRRLç®—æ³•æ€§èƒ½å“è¶Šï¼Œç”šè‡³è¶…è¶ŠGPT-4oã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨é€»è¾‘å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨å›¾åƒæ¨ç†å›°å¢ƒã€‚</li>
<li>æå‡ºè‡ªåå°„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆSRRLï¼‰ç»“åˆChain of Thoughtï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜ç‚¹ï¼Œä¸ºæ‰©æ•£æ¨¡å‹å®ç°é€»è¾‘å›¾åƒçš„æ¨ç†ç”Ÿæˆã€‚</li>
<li>é€šè¿‡åœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­å¼•å…¥å¤šè½®åå°„å»å™ªè¿‡ç¨‹ï¼Œè§£å†³ä¸­é—´æ ·æœ¬å»å™ªè¿‡ç¨‹ä¸­çš„å™ªå£°é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ¡ä»¶å¼•å¯¼å‰å‘è¿‡ç¨‹ï¼Œå®ç°åå°„è¿­ä»£åœ¨CoTæ­¥éª¤ä¹‹é—´ã€‚</li>
<li>åŸºäºSRRLçš„è¿­ä»£æ‰©æ•£è®­ç»ƒé¦–æ¬¡å°†å›¾åƒæ¨ç†é€šè¿‡CoTå¼•å…¥ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œéµå¾ªç‰©ç†å®šå¾‹å’Œéä¼ ç»Ÿç‰©ç†ç°è±¡ã€‚</li>
<li>å®éªŒæ¡ˆä¾‹ç ”ç©¶æ˜¾ç¤ºSRRLç®—æ³•æ€§èƒ½å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b25c0e480124a49353060154e0cce5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dbcd16baf94f2904bfce96839ce3291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6892ca63a2e17ab60e2b223ea181c9c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43f47fc2b0e80df153b528672ede552e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="What-Makes-for-Text-to-360-degree-Panorama-Generation-with-Stable-Diffusion"><a href="#What-Makes-for-Text-to-360-degree-Panorama-Generation-with-Stable-Diffusion" class="headerlink" title="What Makes for Text to 360-degree Panorama Generation with Stable   Diffusion?"></a>What Makes for Text to 360-degree Panorama Generation with Stable   Diffusion?</h2><p><strong>Authors:Jinhong Ni, Chang-Bin Zhang, Qiang Zhang, Jing Zhang</strong></p>
<p>Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code will be released. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°ç¹è£ï¼Œä¾‹å¦‚Stable Diffusionï¼Œå·²ç»æ¿€å‘äº†å°†å…¶é€‚åº”360åº¦å…¨æ™¯å›¾ç”Ÿæˆçš„ç ”ç©¶ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¯æ˜ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„ä½é˜¶é€‚åº”æŠ€æœ¯åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸Šç”Ÿæˆå…¨æ™¯å›¾åƒæ˜¯å¯è¡Œçš„ã€‚ç„¶è€Œï¼Œé€è§†å›¾åƒå’Œå…¨æ™¯å›¾åƒä¹‹é—´çš„å·¨å¤§é¢†åŸŸå·®è·å¼•å‘äº†å…³äºå®ç°è¿™ç§ç»éªŒæˆåŠŸçš„æ½œåœ¨æœºåˆ¶çš„é—®é¢˜ã€‚æˆ‘ä»¬å‡è®¾å¹¶æ£€æŸ¥ï¼Œå¯è®­ç»ƒå¯¹åº”ç‰©åœ¨å…¨æ™¯æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ä¼šè¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºï¼Œè¿™ç§é€‚åº”æ©ç›–äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†åˆ©ç”¨çš„å†…åœ¨æœºåˆ¶ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼š1ï¼‰æ³¨æ„åŠ›æ¨¡å—ä¸­çš„æŸ¥è¯¢å’Œé”®çŸ©é˜µè´Ÿè´£å…¨æ™¯å’Œé€è§†åŸŸä¹‹é—´å¯ä»¥å…±äº«çš„å…±åŒä¿¡æ¯ï¼Œå› æ­¤ä¸å…¨æ™¯ç”Ÿæˆå…³ç³»ä¸å¤§ï¼›2ï¼‰å€¼çŸ©é˜µå’Œè¾“å‡ºæƒé‡çŸ©é˜µä¸“é—¨é€‚åº”é¢„è®­ç»ƒçŸ¥è¯†åˆ°å…¨æ™¯é¢†åŸŸï¼Œåœ¨å…¨æ™¯ç”Ÿæˆçš„å¾®è°ƒè¿‡ç¨‹ä¸­å‘æŒ¥æ›´å…³é”®çš„ä½œç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªåä¸ºUniPanoçš„ç®€å•æ¡†æ¶ï¼Œå®è¯éªŒè¯äº†è¿™äº›è§è§£ï¼Œå…¶ç›®æ ‡æ˜¯ä¸ºæœªæ¥ç ”ç©¶å»ºç«‹ä¼˜é›…çš„åŸºå‡†çº¿ã€‚UniPanoä¸ä»…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”ä¸å…ˆå‰çš„åŒåˆ†æ”¯æ–¹æ³•ç›¸æ¯”ï¼Œè¿˜å¤§å¤§é™ä½äº†å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ—¶é—´ï¼Œä½¿å…¶å¯ç”¨äºç«¯åˆ°ç«¯çš„é«˜åˆ†è¾¨ç‡å…¨æ™¯å›¾åƒç”Ÿæˆã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22129v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°æˆåŠŸï¼Œå¦‚Stable Diffusionï¼Œå·²ä¿ƒä½¿ç ”ç©¶å°†å…¶é€‚åº”å…¨æ™¯å›¾ç”Ÿæˆã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å¯¹å…¨æ™¯å›¾åƒè¿›è¡Œä½é˜¶é€‚åº”çš„æ–¹æ³•ï¼Œå¹¶åˆ†æäº†å…¶å†…åœ¨æœºåˆ¶ã€‚ç ”ç©¶å‘ç°æ³¨æ„åŠ›æ¨¡å—ä¸­çš„æŸ¥è¯¢å’Œå…³é”®çŸ©é˜µè´Ÿè´£åœ¨å…¨æ™¯å’Œé€è§†é¢†åŸŸä¹‹é—´å…±äº«é€šç”¨ä¿¡æ¯ï¼Œè€Œå€¼çŸ©é˜µå’Œè¾“å‡ºæƒé‡çŸ©é˜µåœ¨å…¨æ™¯ç”Ÿæˆä¸­èµ·åˆ°æ›´å…³é”®ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†UniPanoæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºæœªæ¥ç ”ç©¶æä¾›ä¼˜é›…çš„åŸºå‡†çº¿ï¼Œå®ƒä¸ä»…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”åœ¨å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ—¶é—´æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŒåˆ†æ”¯æ–¹æ³•ï¼Œå¯å®ç°ç«¯åˆ°ç«¯å…¨æ™¯å›¾ç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰å·²æˆåŠŸåº”ç”¨äºå…¨æ™¯å›¾ç”Ÿæˆã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œä½é˜¶é€‚åº”æ˜¯ä¸€ç§æœ‰æ•ˆçš„å…¨æ™¯å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>æ³¨æ„åŠ›æ¨¡å—ä¸­çš„æŸ¥è¯¢å’Œå…³é”®çŸ©é˜µè´Ÿè´£åœ¨å…¨æ™¯å’Œé€è§†å›¾åƒé¢†åŸŸä¹‹é—´å…±äº«é€šç”¨ä¿¡æ¯ã€‚</li>
<li>å€¼çŸ©é˜µå’Œè¾“å‡ºæƒé‡çŸ©é˜µåœ¨å…¨æ™¯å›¾åƒçš„ç”Ÿæˆä¸­èµ·åˆ°æ›´å…³é”®çš„ä½œç”¨ã€‚</li>
<li>UniPanoæ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå…¨æ™¯å›¾ç”Ÿæˆæä¾›äº†æ›´é«˜çš„åˆ†è¾¨ç‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>UniPanoæ¡†æ¶æ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ—¶é—´ï¼Œä¸å…ˆå‰çš„åŒåˆ†æ”¯æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-57c34dc39dc947373b44e1fbcdb341eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3afd04e6c1a8a687f666debcd3c4c20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5ee700186284afd89dcab37ad14b2c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-284310113b71536ad8bcbbdce2a566f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad40fdaa35d4f91c998405f52af3c1a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-969cd407081b86ad1b6a06e9d9aea29e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-Volume-Rate-3D-Ultrasound-Reconstruction-with-Diffusion-Models"><a href="#High-Volume-Rate-3D-Ultrasound-Reconstruction-with-Diffusion-Models" class="headerlink" title="High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models"></a>High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</h2><p><strong>Authors:Tristan S. W. Stevens, OisÃ­n Nolan, Oudom Somphone, Jean-Luc Robert, Ruud J. G. van Sloun</strong></p>
<p>Three-dimensional ultrasound enables real-time volumetric visualization of anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the reliance on precise probe orientation, potentially making ultrasound more accessible to clinicians with varying levels of experience and improving automated measurements and post-exam analysis. However, achieving both high volume rates and high image quality remains a significant challenge. While 3D diverging waves can provide high volume rates, they suffer from limited tissue harmonic generation and increased multipath effects, which degrade image quality. One compromise is to retain the focusing in elevation while leveraging unfocused diverging waves in the lateral direction to reduce the number of transmissions per elevation plane. Reaching the volume rates achieved by full 3D diverging waves, however, requires dramatically undersampling the number of elevation planes. Subsequently, to render the full volume, simple interpolation techniques are applied. This paper introduces a novel approach to 3D ultrasound reconstruction from a reduced set of elevation planes by employing diffusion models (DMs) to achieve increased spatial and temporal resolution. We compare both traditional and supervised deep learning-based interpolation methods on a 3D cardiac ultrasound dataset. Our results show that DM-based reconstruction consistently outperforms the baselines in image quality and downstream task performance. Additionally, we accelerate inference by leveraging the temporal consistency inherent to ultrasound sequences. Finally, we explore the robustness of the proposed method by exploiting the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrate improved recall on out-of-distribution data with synthetic anomalies under strong subsampling. </p>
<blockquote>
<p>ä¸‰ç»´è¶…å£°èƒ½å¤Ÿå®æ—¶å¯è§†åŒ–è§£å‰–ç»“æ„çš„ä¸‰ç»´å›¾åƒã€‚ä¸ä¼ ç»Ÿçš„äºŒç»´è¶…å£°ä¸åŒï¼Œä¸‰ç»´æˆåƒå‡å°‘äº†å¯¹äºç²¾ç¡®æ¢å¤´æ–¹å‘çš„ä¾èµ–ï¼Œä½¿å¾—ä¸åŒç»éªŒçš„ä¸´åºŠåŒ»ç”Ÿéƒ½èƒ½æ›´å®¹æ˜“åœ°ä½¿ç”¨è¶…å£°ï¼Œå¹¶æ”¹å–„äº†è‡ªåŠ¨æµ‹é‡å’Œè€ƒè¯•åçš„åˆ†æã€‚ç„¶è€Œï¼Œå®ç°é«˜ä½“ç§¯ç‡å’Œé«˜å›¾åƒè´¨é‡ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ä¸‰ç»´å‘æ•£æ³¢å¯ä»¥æä¾›é«˜ä½“ç§¯ç‡ï¼Œä½†å®ƒä»¬å­˜åœ¨ç»„ç»‡è°æ³¢ç”Ÿæˆæœ‰é™å’Œå¤šè·¯å¾„æ•ˆåº”å¢åŠ çš„é—®é¢˜ï¼Œä»è€Œé™ä½äº†å›¾åƒè´¨é‡ã€‚ä¸€ç§æŠ˜è¡·æ–¹æ¡ˆæ˜¯åœ¨å‚ç›´æ–¹å‘ä¸Šä¿æŒèšç„¦ï¼ŒåŒæ—¶åœ¨ä¾§å‘åˆ©ç”¨æœªèšç„¦çš„å‘æ•£æ³¢ï¼Œä»¥å‡å°‘æ¯ä¸ªå‚ç›´å¹³é¢çš„ä¼ è¾“æ•°é‡ã€‚ç„¶è€Œï¼Œè¦è¾¾åˆ°å®Œå…¨ä¸‰ç»´å‘æ•£æ³¢æ‰€è¾¾åˆ°çš„éŸ³é‡æ°´å¹³ï¼Œéœ€è¦å¯¹é«˜åº¦å‡å°å¹³é¢æ•°é‡è¿›è¡Œå¤§å¹…é™é‡‡æ ·å¤„ç†ã€‚éšåï¼Œé‡‡ç”¨ç®€å•çš„æ’å€¼æŠ€æœ¯è¿›è¡Œæ•´ä½“æ¸²æŸ“ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä»å‡å°‘çš„é«˜åº¦å¹³é¢ä¸Šé‡å»ºä¸‰ç»´è¶…å£°çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªä¸‰ç»´å¿ƒè„è¶…å£°æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†ä¼ ç»Ÿçš„å’Œç›‘ç£æ·±åº¦å­¦ä¹ æ’å€¼æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºDMçš„é‡å»ºåœ¨å›¾åƒè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è¶…å£°åºåˆ—å›ºæœ‰çš„æ—¶é—´ä¸€è‡´æ€§æ¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ‰©æ•£åé‡‡æ ·æ¦‚ç‡æ€§è´¨æ¥æ¢ç´¢æ‰€æå‡ºæ–¹æ³•çš„ç¨³å¥æ€§ï¼Œé‡åŒ–é‡å»ºçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨å¼ºé™é‡‡æ ·ä¸‹çš„åˆæˆå¼‚å¸¸æ•°æ®ä¸Šå±•ç¤ºäº†å¯¹å¼‚å¸¸æ•°æ®çš„æ”¹è¿›å¬å›ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22090v1">PDF</a> 10 pages, 10 figures, preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å¯¹ä¸‰ç»´è¶…å£°æ•°æ®è¿›è¡Œé‡å»ºçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å‡å°‘é‡‡æ ·å¹³é¢æ•°é‡å®ç°é«˜æ•ˆçš„ä¸‰ç»´è¶…å£°é‡å»ºï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹æé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚å¯¹æ¯”ä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ’å€¼æ–¹æ³•ï¼Œåœ¨ä¸‰ç»´å¿ƒè„è¶…å£°æ•°æ®é›†ä¸Šï¼ŒDMsé‡å»ºæ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ©ç”¨è¶…å£°åºåˆ—çš„å›ºæœ‰æ—¶é—´ä¸€è‡´æ€§ï¼Œæé«˜äº†æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†é€šè¿‡æ‰©æ•£æ¨¡å‹çš„åéªŒæ¦‚ç‡é‡‡æ ·æ¥é‡åŒ–é‡å»ºä¸ç¡®å®šæ€§çš„é²æ£’æ€§æ–¹æ³•ï¼Œå¹¶åœ¨å¼ºå­é‡‡æ ·æ¡ä»¶ä¸‹å¯¹åˆæˆå¼‚å¸¸æ•°æ®æé«˜äº†å¬å›ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´è¶…å£°èƒ½å¤Ÿå®ç°å®æ—¶ä½“ç§¯å¯è§†åŒ–è§£å‰–ç»“æ„ï¼Œç›¸è¾ƒäºä¼ ç»ŸäºŒç»´è¶…å£°ï¼Œå…¶é™ä½äº†å¯¹ç²¾ç¡®æ¢å¤´ä½ç½®çš„ä¾èµ–ã€‚</li>
<li>é«˜ä½“ç§¯ç‡å’Œé«˜è´¨é‡å›¾åƒæ˜¯ä¸‰ç»´è¶…å£°æˆåƒé¢ä¸´çš„æŒ‘æˆ˜ã€‚å‘æ•£æ³¢è™½ç„¶èƒ½å¤Ÿæä¾›é«˜ä½“ç§¯ç‡ä½†å›¾åƒè´¨é‡ä¸‹é™ã€‚</li>
<li>æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼šé€šè¿‡å‡å°‘é‡‡æ ·å¹³é¢æ•°é‡å¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°é«˜æ•ˆä¸‰ç»´è¶…å£°é‡å»ºï¼Œæé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚</li>
<li>å¯¹æ¯”ä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ’å€¼æ–¹æ³•ï¼ŒDMsåœ¨å›¾åƒè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</li>
<li>åˆ©ç”¨è¶…å£°åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡æ‰©æ•£æ¨¡å‹çš„åéªŒæ¦‚ç‡é‡‡æ ·é‡åŒ–é‡å»ºä¸ç¡®å®šæ€§ï¼Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8416fef48ed9de165777988ebb9d4f78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55388495e1a74de93d7275cd81150752.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9becc041abfdb0b06f7f812c9d37315.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb2d4ef54918532e2ae8adece00e0e75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b7be3d512aea8d434e9ec064100a630.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e42914303a323838ce3ffaa575f096e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="D-Fusion-Direct-Preference-Optimization-for-Aligning-Diffusion-Models-with-Visually-Consistent-Samples"><a href="#D-Fusion-Direct-Preference-Optimization-for-Aligning-Diffusion-Models-with-Visually-Consistent-Samples" class="headerlink" title="D-Fusion: Direct Preference Optimization for Aligning Diffusion Models   with Visually Consistent Samples"></a>D-Fusion: Direct Preference Optimization for Aligning Diffusion Models   with Visually Consistent Samples</h2><p><strong>Authors:Zijing Hu, Fengda Zhang, Kun Kuang</strong></p>
<p>The practical applications of diffusion models have been limited by the misalignment between generated images and corresponding text prompts. Recent studies have introduced direct preference optimization (DPO) to enhance the alignment of these models. However, the effectiveness of DPO is constrained by the issue of visual inconsistency, where the significant visual disparity between well-aligned and poorly-aligned images prevents diffusion models from identifying which factors contribute positively to alignment during fine-tuning. To address this issue, this paper introduces D-Fusion, a method to construct DPO-trainable visually consistent samples. On one hand, by performing mask-guided self-attention fusion, the resulting images are not only well-aligned, but also visually consistent with given poorly-aligned images. On the other hand, D-Fusion can retain the denoising trajectories of the resulting images, which are essential for DPO training. Extensive experiments demonstrate the effectiveness of D-Fusion in improving prompt-image alignment when applied to different reinforcement learning algorithms. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å®é™…åº”ç”¨å—åˆ°ç”Ÿæˆå›¾åƒä¸ç›¸åº”æ–‡æœ¬æç¤ºä¸åŒ¹é…çš„é™åˆ¶ã€‚è¿‘æœŸçš„ç ”ç©¶å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä»¥å¢å¼ºè¿™äº›æ¨¡å‹çš„å¯¹é½æ€§ã€‚ç„¶è€Œï¼ŒDPOçš„æœ‰æ•ˆæ€§å—åˆ°è§†è§‰ä¸ä¸€è‡´æ€§çš„çº¦æŸï¼Œå…¶ä¸­è‰¯å¥½å¯¹é½å’Œä¸è‰¯å¯¹é½å›¾åƒä¹‹é—´çš„é‡å¤§è§†è§‰å·®å¼‚é˜»ç¢äº†æ‰©æ•£æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¯†åˆ«å“ªäº›å› ç´ æ­£å‘è´¡çŒ®äºå¯¹é½ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†D-Fusionæ–¹æ³•ï¼Œæ„å»ºé€‚ç”¨äºDPOè®­ç»ƒçš„è§†è§‰ä¸€è‡´æ ·æœ¬ã€‚ä¸€æ–¹é¢ï¼Œé€šè¿‡æ‰§è¡Œæ©è†œå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›èåˆï¼Œç”Ÿæˆçš„å›¾åƒä¸ä»…å¯¹é½è‰¯å¥½ï¼Œè€Œä¸”åœ¨è§†è§‰ä¸Šè¿˜ä¸ç»™å®šçš„ä¸è‰¯å¯¹é½å›¾åƒä¿æŒä¸€è‡´ã€‚å¦ä¸€æ–¹é¢ï¼ŒD-Fusionèƒ½å¤Ÿä¿ç•™ç”Ÿæˆå›¾åƒçš„é™å™ªè½¨è¿¹ï¼Œè¿™å¯¹äºDPOè®­ç»ƒè‡³å…³é‡è¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“åº”ç”¨äºä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ—¶ï¼ŒD-Fusionåœ¨æ”¹è¿›æç¤ºå›¾åƒå¯¹é½æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22002v1">PDF</a> Accepted to ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œå³ç”Ÿæˆå›¾åƒä¸å¯¹åº”æ–‡æœ¬æç¤ºçš„ä¸å¯¹é½é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼ŒDPOå—åˆ°è§†è§‰ä¸ä¸€è‡´æ€§çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºD-Fusionçš„æ–¹æ³•ï¼Œç”¨äºæ„å»ºå¯è¿›è¡ŒDPOè®­ç»ƒçš„è§†è§‰ä¸€è‡´æ ·æœ¬ã€‚é€šè¿‡æ‰§è¡Œæ©è†œå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›èåˆï¼ŒD-Fusionä¸ä»…èƒ½ç”Ÿæˆä¸ç»™å®šä¸è‰¯å¯¹é½å›¾åƒè§†è§‰ä¸€è‡´çš„å›¾åƒï¼Œè¿˜èƒ½ä¿ç•™å›¾åƒçš„é™å™ªè½¨è¿¹ï¼Œè¿™å¯¹äºDPOè®­ç»ƒè‡³å…³é‡è¦ã€‚å®éªŒè¯æ˜ï¼ŒD-Fusionåœ¨åº”ç”¨äºä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ—¶ï¼Œèƒ½æœ‰æ•ˆæé«˜æç¤ºå›¾åƒçš„å¯¹é½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é¢ä¸´ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºä¸å¯¹é½çš„å®ç”¨é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¢«å¼•å…¥ä»¥æé«˜æ¨¡å‹å¯¹é½æ€§ï¼Œä½†å—é™äºè§†è§‰ä¸ä¸€è‡´æ€§ã€‚</li>
<li>D-Fusionæ–¹æ³•é€šè¿‡æ‰§è¡Œæ©è†œå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›èåˆæ¥è§£å†³è§†è§‰ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>D-Fusionä¸ä»…èƒ½ç”Ÿæˆè‰¯å¥½å¯¹é½çš„å›¾åƒï¼Œè¿˜èƒ½ä¿ç•™å›¾åƒçš„é™å™ªè½¨è¿¹ï¼Œå¯¹DPOè®­ç»ƒè‡³å…³é‡è¦ã€‚</li>
<li>D-Fusionå¯¹ä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•éƒ½æœ‰æ•ˆï¼Œèƒ½æ˜¾è‘—æé«˜æç¤ºå›¾åƒçš„å¯¹é½æ€§ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†D-Fusionçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-606fe845f50a1a2bafb223f50b4bf917.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8e1d84e5a50d8e500a9d0faf9a77a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2726069d4bc81ae5e46ab5d9b79adca6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Collaborative-Learning-for-Unsupervised-Multimodal-Remote-Sensing-Image-Registration-Integrating-Self-Supervision-and-MIM-Guided-Diffusion-Based-Image-Translation"><a href="#Collaborative-Learning-for-Unsupervised-Multimodal-Remote-Sensing-Image-Registration-Integrating-Self-Supervision-and-MIM-Guided-Diffusion-Based-Image-Translation" class="headerlink" title="Collaborative Learning for Unsupervised Multimodal Remote Sensing Image   Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based   Image Translation"></a>Collaborative Learning for Unsupervised Multimodal Remote Sensing Image   Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based   Image Translation</h2><p><strong>Authors:Xiaochen Wei, Weiwei Guo, Wenxian Yu</strong></p>
<p>The substantial modality-induced variations in radiometric, texture, and structural characteristics pose significant challenges for the accurate registration of multimodal images. While supervised deep learning methods have demonstrated strong performance, they often rely on large-scale annotated datasets, limiting their practical application. Traditional unsupervised methods usually optimize registration by minimizing differences in feature representations, yet often fail to robustly capture geometric discrepancies, particularly under substantial spatial and radiometric variations, thus hindering convergence stability. To address these challenges, we propose a Collaborative Learning framework for Unsupervised Multimodal Image Registration, named CoLReg, which reformulates unsupervised registration learning into a collaborative training paradigm comprising three components: (1) a cross-modal image translation network, MIMGCD, which employs a learnable Maximum Index Map (MIM) guided conditional diffusion model to synthesize modality-consistent image pairs; (2) a self-supervised intermediate registration network which learns to estimate geometric transformations using accurate displacement labels derived from MIMGCD outputs; (3) a distilled cross-modal registration network trained with pseudo-label predicted by the intermediate network. The three networks are jointly optimized through an alternating training strategy wherein each network enhances the performance of the others. This mutual collaboration progressively reduces modality discrepancies, enhances the quality of pseudo-labels, and improves registration accuracy. Extensive experimental results on multiple datasets demonstrate that our ColReg achieves competitive or superior performance compared to state-of-the-art unsupervised approaches and even surpasses several supervised baselines. </p>
<blockquote>
<p>å¤šæ¨¡æ€å›¾åƒåœ¨è¾å°„åº¦é‡ã€çº¹ç†å’Œç»“æ„ç‰¹æ€§ä¸Šçš„æ˜¾è‘—å·®å¼‚ä¸ºå‡†ç¡®çš„å¤šæ¨¡æ€å›¾åƒé…å‡†å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æœ‰ç›‘ç£çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å·²ç»å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œä»è€Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ— ç›‘ç£æ–¹æ³•é€šå¸¸é€šè¿‡æœ€å°åŒ–ç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚æ€§æ¥ä¼˜åŒ–é…å‡†ï¼Œä½†å¾€å¾€æ— æ³•ç¨³å¥åœ°æ•æ‰å‡ ä½•å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå¤§çš„ç©ºé—´å˜åŒ–å’Œè¾å°„åº¦é‡å˜åŒ–æƒ…å†µä¸‹ï¼Œä»è€Œå½±å“æ”¶æ•›ç¨³å®šæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCoLRegçš„åŸºäºæ— ç›‘ç£å­¦ä¹ çš„å¤šæ¨¡æ€å›¾åƒååŒé…å‡†æ¡†æ¶ã€‚å®ƒå°†æ— ç›‘ç£é…å‡†å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºä¸€ç§ååŒè®­ç»ƒæ¨¡å¼ï¼ŒåŒ…å«ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰è·¨æ¨¡æ€å›¾åƒç¿»è¯‘ç½‘ç»œMIMGCDï¼Œå®ƒé‡‡ç”¨åŸºäºå¯å­¦ä¹ æœ€å¤§ç´¢å¼•å›¾ï¼ˆMIMï¼‰å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ¨¡æ€ä¸€è‡´çš„å›¾åƒå¯¹ï¼›ï¼ˆ2ï¼‰è‡ªç›‘ç£ä¸­é—´é…å‡†ç½‘ç»œï¼Œè¯¥ç½‘ç»œå­¦ä¹ ä½¿ç”¨ç”±MIMGCDè¾“å‡ºå¾—å‡ºçš„å‡†ç¡®ä½ç§»æ ‡ç­¾æ¥ä¼°è®¡å‡ ä½•å˜æ¢ï¼›ï¼ˆ3ï¼‰é€šè¿‡ä¸­é—´ç½‘ç»œé¢„æµ‹çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒçš„è’¸é¦è·¨æ¨¡æ€é…å‡†ç½‘ç»œã€‚è¿™ä¸‰ä¸ªç½‘ç»œé€šè¿‡äº¤æ›¿è®­ç»ƒç­–ç•¥è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå…¶ä¸­æ¯ä¸ªç½‘ç»œéƒ½ä¼šå¢å¼ºå…¶ä»–ç½‘ç»œçš„æ€§èƒ½ã€‚è¿™ç§ç›¸äº’åä½œçš„æ–¹å¼é€æ­¥å‡å°‘äº†æ¨¡æ€å·®å¼‚ï¼Œæé«˜äº†ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶æé«˜äº†é…å‡†ç²¾åº¦ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ColRegä¸æœ€å…ˆè¿›çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”å–å¾—äº†ç«äº‰æˆ–ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†å¤šä¸ªæœ‰ç›‘ç£çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoLRegçš„ååŒå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤šæ¨¡æ€å›¾åƒæ³¨å†Œçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨ä¸‰ä¸ªåä½œç»„ä»¶ï¼šè·¨æ¨¡æ€å›¾åƒç¿»è¯‘ç½‘ç»œMIMGCDã€è‡ªç›‘ç£ä¸­é—´æ³¨å†Œç½‘ç»œå’Œè’¸é¦è·¨æ¨¡æ€æ³¨å†Œç½‘ç»œï¼ŒCoLRegèƒ½å¤Ÿå‡å°‘æ¨¡æ€å·®å¼‚ï¼Œæé«˜ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå¹¶æ”¹å–„æ³¨å†Œå‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒColRegç›¸è¾ƒäºå…ˆè¿›çš„æ— ç›‘ç£æ–¹æ³•å’Œä¸€äº›ç›‘ç£åŸºçº¿æ–¹æ³•è¡¨ç°å‡ºç«äº‰æ€§æˆ–ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å›¾åƒæ³¨å†Œé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒæ¨¡æ€çš„å›¾åƒåœ¨è¾å°„åº¦é‡ã€çº¹ç†å’Œç»“æ„ç‰¹æ€§ä¸Šå­˜åœ¨å·®å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚ç›‘ç£æ·±åº¦å­¦ä¹ å—é™äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œä¼ ç»Ÿæ— ç›‘ç£æ–¹æ³•éš¾ä»¥ç¨³å¥æ•è·å‡ ä½•å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§ç©ºé—´å˜åŒ–å’Œè¾å°„åº¦é‡å˜åŒ–çš„æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºçš„CoLRegæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªåä½œç»„ä»¶ï¼šè·¨æ¨¡æ€å›¾åƒç¿»è¯‘ç½‘ç»œMIMGCDã€è‡ªç›‘ç£ä¸­é—´æ³¨å†Œç½‘ç»œå’Œè’¸é¦è·¨æ¨¡æ€æ³¨å†Œç½‘ç»œã€‚</li>
<li>MIMGCDç½‘ç»œä½¿ç”¨å¯å­¦ä¹ çš„æœ€å¤§ç´¢å¼•å›¾ï¼ˆMIMï¼‰æŒ‡å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ¨¡æ€ä¸€è‡´çš„å›¾åƒå¯¹ã€‚</li>
<li>ä¸­é—´æ³¨å†Œç½‘ç»œé€šè¿‡åˆ©ç”¨MIMGCDè¾“å‡ºçš„å‡†ç¡®ä½ç§»æ ‡ç­¾è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œä¼°è®¡å‡ ä½•å˜æ¢ã€‚</li>
<li>CoLRegé€šè¿‡äº¤æ›¿è®­ç»ƒç­–ç•¥è”åˆä¼˜åŒ–è¿™ä¸‰ä¸ªç½‘ç»œï¼Œç›¸äº’åä½œæé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-052d423f7e4eed32ebdb5f9d605c3f0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c88735cc8a7679134e282797ffe07ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85fb140468175d17f9cbfb75ebce09d6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DvD-Unleashing-a-Generative-Paradigm-for-Document-Dewarping-via-Coordinates-based-Diffusion-Model"><a href="#DvD-Unleashing-a-Generative-Paradigm-for-Document-Dewarping-via-Coordinates-based-Diffusion-Model" class="headerlink" title="DvD: Unleashing a Generative Paradigm for Document Dewarping via   Coordinates-based Diffusion Model"></a>DvD: Unleashing a Generative Paradigm for Document Dewarping via   Coordinates-based Diffusion Model</h2><p><strong>Authors:Weiguang Zhang, Huangcheng Lu, Maizhen Ning, Xiaowei Huang, Wei Wang, Kaizhu Huang, Qiufeng Wang</strong></p>
<p>Document dewarping aims to rectify deformations in photographic document images, thus improving text readability, which has attracted much attention and made great progress, but it is still challenging to preserve document structures. Given recent advances in diffusion models, it is natural for us to consider their potential applicability to document dewarping. However, it is far from straightforward to adopt diffusion models in document dewarping due to their unfaithful control on highly complex document images (e.g., 2000$\times$3000 resolution). In this paper, we propose DvD, the first generative model to tackle document \textbf{D}ewarping \textbf{v}ia a \textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level denoising instead of typical pixel-level denoising, generating a mapping for deformation rectification. In addition, we further propose a time-variant condition refinement mechanism to enhance the preservation of document structures. In experiments, we find that current document dewarping benchmarks can not evaluate dewarping models comprehensively. To this end, we present AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark comprising 6,300 real image pairs across three distinct domains, enabling fine-grained evaluation of dewarping models. Comprehensive experiments demonstrate that our proposed DvD can achieve state-of-the-art performance with acceptable computational efficiency on multiple metrics across various benchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark and code will be publicly available. </p>
<blockquote>
<p>æ–‡æ¡£å»æ‰­æ›²æ—¨åœ¨çº æ­£æ‘„å½±æ–‡æ¡£å›¾åƒä¸­çš„å˜å½¢ï¼Œä»è€Œæé«˜æ–‡æœ¬çš„æ¸…æ™°åº¦ï¼Œè¿™å·²å¼•èµ·äº†å¾ˆå¤šå…³æ³¨å¹¶å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†ä¿ç•™æ–‡æ¡£ç»“æ„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œæˆ‘ä»¬è‡ªç„¶ä¼šè€ƒè™‘å…¶åº”ç”¨äºæ–‡æ¡£å»æ‰­æ›²çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹å¯¹é«˜åº¦å¤æ‚çš„æ–‡æ¡£å›¾åƒï¼ˆä¾‹å¦‚2000x3000åˆ†è¾¨ç‡ï¼‰æ§åˆ¶ä¸ç²¾ç¡®ï¼Œå› æ­¤åœ¨æ–‡æ¡£å»æ‰­æ›²ä¸­é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¹¶ä¸å®¹æ˜“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DvDï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡æ‰©æ•£æ¡†æ¶è§£å†³æ–‡æ¡£å»æ‰­æ›²çš„ç”Ÿæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒDvDå¼•å…¥äº†åæ ‡çº§å»å™ªï¼Œè€Œä¸æ˜¯å…¸å‹çš„åƒç´ çº§å»å™ªï¼Œä»è€Œäº§ç”Ÿç”¨äºå˜å½¢æ ¡æ­£çš„æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ—¶é—´å¯å˜æ¡ä»¶ç»†åŒ–æœºåˆ¶ï¼Œä»¥æé«˜æ–‡æ¡£ç»“æ„çš„ä¿ç•™ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AnyPhotoDoc6300ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¥æ ¼è®¾è®¡çš„å¤§è§„æ¨¡æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„6300å¼ çœŸå®å›¾åƒå¯¹ï¼Œèƒ½å¤Ÿå¯¹å»æ‰­æ›²æ¨¡å‹è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„DvDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬DocUNetã€DIR300å’ŒAnyPhotoDoc6300ï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡å¯æ¥å—ã€‚æ–°çš„åŸºå‡†æµ‹è¯•å’Œä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21975v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DvDï¼Œä¸€ä¸ªé€šè¿‡æ‰©æ•£æ¡†æ¶è§£å†³æ–‡æ¡£å»æ‰­æ›²çš„ç”Ÿæˆæ¨¡å‹ã€‚DvDå¼•å…¥åæ ‡çº§åˆ«çš„å»å™ªï¼Œè€Œä¸æ˜¯å…¸å‹çš„åƒç´ çº§åˆ«å»å™ªï¼Œä»¥ç”Ÿæˆç”¨äºå˜å½¢æ ¡æ­£çš„æ˜ å°„ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ—¶é—´å¯å˜æ¡ä»¶ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥æé«˜æ–‡æ¡£ç»“æ„çš„ä¿ç•™ã€‚å®éªŒå‘ç°ç°æœ‰æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°å»æ‰­æ›²æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AnyPhotoDoc6300ï¼Œä¸€ä¸ªä¸¥æ ¼è®¾è®¡çš„å¤§è§„æ¨¡æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«6300å¼ æ¥è‡ªä¸‰ä¸ªä¸åŒé¢†åŸŸçš„çœŸå®å›¾åƒå¯¹ï¼Œå¯å®ç°å»æ‰­æ›²æ¨¡å‹çš„ç²¾ç»†è¯„ä¼°ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DvDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬DocUNetã€DIR300å’ŒAnyPhotoDoc6300ï¼Œä¸”è®¡ç®—æ•ˆç‡å¯æ¥å—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æ¡£å»æ‰­æ›²æ—¨åœ¨çº æ­£æ‘„å½±æ–‡æ¡£å›¾åƒä¸­çš„å˜å½¢ï¼Œæé«˜æ–‡æœ¬å¯è¯»æ€§ï¼Œä½†ä¿ç•™æ–‡æ¡£ç»“æ„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æ¡£å»æ‰­æ›²ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>DvDæ˜¯é¦–ä¸ªé€šè¿‡æ‰©æ•£æ¡†æ¶è§£å†³æ–‡æ¡£å»æ‰­æ›²çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>DvDå¼•å…¥åæ ‡çº§åˆ«çš„å»å™ªæœºåˆ¶ï¼Œç”Ÿæˆç”¨äºå˜å½¢æ ¡æ­£çš„æ˜ å°„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ—¶é—´å¯å˜æ¡ä»¶ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥æé«˜æ–‡æ¡£ç»“æ„çš„ä¿ç•™ã€‚</li>
<li>ç°æœ‰æ–‡æ¡£å»æ‰­æ›²åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹ï¼Œå› æ­¤æ¨å‡ºäº†AnyPhotoDoc6300å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚</li>
<li>DvDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7224fdd55daf94ab4cd43966d17d4332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e3120f3b854ff65ab783a2d689d35ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3543b694b558eb8264b011e3bdaacb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b24e84529b8975d2d13157f8aa9b8b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25b66f24942d09a0857f2e062ad622e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfb6e994c30d84f39c4b5a9522b09fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3736f064be40f195a9ecb02ce380253.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="One-Way-Ticket-Time-Independent-Unified-Encoder-for-Distilling-Text-to-Image-Diffusion-Models"><a href="#One-Way-Ticket-Time-Independent-Unified-Encoder-for-Distilling-Text-to-Image-Diffusion-Models" class="headerlink" title="One-Way Ticket:Time-Independent Unified Encoder for Distilling   Text-to-Image Diffusion Models"></a>One-Way Ticket:Time-Independent Unified Encoder for Distilling   Text-to-Image Diffusion Models</h2><p><strong>Authors:Senmao Li, Lei Wang, Kai Wang, Tao Liu, Jiehang Xie, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang</strong></p>
<p>Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼›ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¨ç†é€Ÿåº¦ä¸å›¾åƒè´¨é‡ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œä¸ºæœ‰æ•ˆéƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰çš„è’¸é¦T2Iæ¨¡å‹å¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼Œä½†åœ¨å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢å¾€å¾€å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å•æ­¥æ¨¡å‹ä¸­ã€‚ä»æˆ‘ä»¬çš„åˆ†ææ¥çœ‹ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°UNetç¼–ç å™¨ä¸­å­˜åœ¨å†—ä½™è®¡ç®—ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå¯¹äºT2Iæ‰©æ•£æ¨¡å‹ï¼Œè§£ç å™¨æ›´æ“…é•¿æ•æ‰æ›´ä¸°å¯Œã€æ›´æ˜ç¡®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œç¼–ç å™¨å¯ä»¥æœ‰æ•ˆåœ°åœ¨æ¥è‡ªä¸åŒæ—¶é—´æ­¥çš„è§£ç å™¨ä¹‹é—´å…±äº«ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬ä¸ºå­¦ç”Ÿæ¨¡å‹UNetæ¶æ„å¼•å…¥äº†é¦–ä¸ªæ—¶é—´ç‹¬ç«‹ç»Ÿä¸€ç¼–ç å™¨TiUEï¼Œè¿™æ˜¯ä¸€ç§æ— å¾ªç¯çš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºè’¸é¦T2Iæ‰©æ•£æ¨¡å‹ã€‚TiUEä½¿ç”¨ä¸€æ¬¡é€šè¿‡æ–¹æ¡ˆï¼Œè·¨å¤šä¸ªè§£ç å™¨æ—¶é—´æ­¥å…±äº«ç¼–ç å™¨ç‰¹å¾ï¼Œå®ç°å¹¶è¡Œé‡‡æ ·ï¼Œå¹¶æ˜¾è‘—é™ä½æ¨ç†æ—¶é—´å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥KLæ•£åº¦é¡¹æ¥è§„èŒƒå™ªå£°é¢„æµ‹ï¼Œè¿™æé«˜äº†ç”Ÿæˆå›¾åƒçš„æ„ŸçŸ¥çœŸå®æ„Ÿå’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTiUEä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬LCMã€SD-Turboå’ŒSwiftBrushv2ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶äº§ç”Ÿæ›´å¤šæ ·åŒ–å’Œæ›´çœŸå®çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21960v1">PDF</a> Accepted at CVPR2025, Code: <a target="_blank" rel="noopener" href="https://github.com/sen-mao/Loopfree">https://github.com/sen-mao/Loopfree</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œä½†é¢ä¸´æ¨ç†é€Ÿåº¦ä¸å›¾åƒè´¨é‡ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿè§‚å¯Ÿåˆ°UNetç¼–ç å™¨çš„å†—ä½™è®¡ç®—ï¼Œå¹¶æå‡ºé¦–ä¸ªæ—¶é—´ç‹¬ç«‹ç»Ÿä¸€ç¼–ç å™¨TiUEï¼Œç”¨äºå­¦ç”Ÿæ¨¡å‹çš„UNetæ¶æ„ã€‚TiUEé‡‡ç”¨æ— å¾ªç¯å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡å…±äº«ç¼–ç å™¨ç‰¹å¾å®ç°å¹¶è¡Œé‡‡æ ·ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ—¶é—´å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥KLæ•£åº¦é¡¹æ¥è§„èŒƒå™ªå£°é¢„æµ‹ï¼Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„æ„ŸçŸ¥çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTiUEä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬LCMã€SD-Turboå’ŒSwiftBrushv2ï¼Œèƒ½åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶äº§ç”Ÿæ›´å¤šæ ·åŒ–å’Œé€¼çœŸçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå»ºæ¨¡æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†é¢ä¸´æ¨ç†é€Ÿåº¦ä¸å›¾åƒè´¨é‡ä¹‹é—´çš„æƒè¡¡æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è’¸é¦T2Iæ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼Œä½†åœ¨å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨å•æ­¥æ¨¡å‹ä¸­ã€‚</li>
<li>UNetç¼–ç å™¨ä¸­å­˜åœ¨å†—ä½™è®¡ç®—ã€‚</li>
<li>è§£ç å™¨æ›´èƒ½æ•æ‰ä¸°å¯Œä¸”æ˜ç¡®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œç¼–ç å™¨å¯è·¨ä¸åŒæ—¶é—´æ­¥æœ‰æ•ˆå…±äº«ã€‚</li>
<li>å¼•å…¥æ—¶é—´ç‹¬ç«‹ç»Ÿä¸€ç¼–ç å™¨TiUEï¼Œç”¨äºå­¦ç”Ÿæ¨¡å‹çš„UNetæ¶æ„ï¼Œé‡‡ç”¨æ— å¾ªç¯å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>TiUEé€šè¿‡å…±äº«ç¼–ç å™¨ç‰¹å¾å®ç°å¹¶è¡Œé‡‡æ ·ï¼Œæ˜¾è‘—é™ä½æ¨ç†æ—¶é—´å¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4234a4004dac65d732b3409d2f060250.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798a9b61cbc50db4e6232cdc195dc79c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea78a7dd0082f236ea713f3f3d41f6eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7af1c94d693eb651d1de87f50a79abf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed3f45cb831b0fc544a121e381822e6a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Hyperspectral-Gaussian-Splatting"><a href="#Hyperspectral-Gaussian-Splatting" class="headerlink" title="Hyperspectral Gaussian Splatting"></a>Hyperspectral Gaussian Splatting</h2><p><strong>Authors:Sunil Kumar Narayanan, Lingjun Zhao, Lu Gan, Yongsheng Chen</strong></p>
<p>Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise determination of nutritional elements in samples. Recently, 3D reconstruction methods have been used to create implicit neural representations of HSI scenes, which can help localize the target objectâ€™s nutrient composition spatially and spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit representation that can render hyperspectral channel compositions of each spatial location from any viewing direction. However, it faces limitations in training time and rendering speed. In this paper, we propose Hyperspectral Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of the hyperspectral scenes and novel view synthesis for the entire spectral range. To enhance the modelâ€™s ability to capture fine-grained reflectance variations across the light spectrum and leverage correlations between adjacent wavelengths for denoising, we introduce a wavelength encoder to generate wavelength-specific spherical harmonics offsets. We also introduce a novel Kullbackâ€“Leibler divergence-based loss to mitigate the spectral distribution gap between the rendered image and the ground truth. A diffusion model is further applied for denoising the rendered images and generating photorealistic hyperspectral images. We present extensive evaluations on five diverse hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of our proposed HS-GS framework. The results demonstrate that HS-GS achieves new state-of-the-art performance among all previously published methods. Code will be released upon publication. </p>
<blockquote>
<p>é«˜å…‰è°±æˆåƒï¼ˆHSIï¼‰åœ¨å†œä¸šåº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œç”¨äºéç ´åæ€§ä¼°è®¡æ¤ç‰©å…»åˆ†ç»„æˆå’Œç²¾ç¡®ç¡®å®šæ ·å“ä¸­çš„è¥å…»å…ƒç´ ã€‚æœ€è¿‘ï¼Œä¸‰ç»´é‡å»ºæ–¹æ³•è¢«ç”¨æ¥åˆ›å»ºHSIåœºæ™¯çš„éšå¼ç¥ç»è¡¨ç¤ºï¼Œè¿™æœ‰åŠ©äºåœ¨ç©ºé—´ä¸Šå…‰è°±ä¸Šå®šä½ç›®æ ‡å¯¹è±¡çš„å…»åˆ†ç»„æˆã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å‰æ²¿çš„éšå¼è¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥ä»ä»»ä½•è§‚çœ‹æ–¹å‘å‘ˆç°æ¯ä¸ªç©ºé—´ä½ç½®çš„é«˜å…‰è°±é€šé“ç»„åˆã€‚ç„¶è€Œï¼Œå®ƒåœ¨è®­ç»ƒæ—¶é—´å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜å…‰è°±é«˜æ–¯å±•å¸ƒï¼ˆHS-GSï¼‰ï¼Œå®ƒå°†æœ€æ–°çš„ä¸‰ç»´é«˜æ–¯å±•å¸ƒï¼ˆ3DGSï¼‰ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°å¯¹é«˜å…‰è°±åœºæ™¯çš„ä¸‰ç»´æ˜¾å¼é‡å»ºä»¥åŠæ•´ä¸ªå…‰è°±èŒƒå›´çš„å…¨æ–°è§†å›¾åˆæˆã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹æ•æ‰å…‰è°±ä¸­ç²¾ç»†åå°„ç‡å˜åŒ–çš„èƒ½åŠ›å¹¶åˆ©ç”¨ç›¸é‚»æ³¢é•¿ä¹‹é—´çš„ç›¸å…³æ€§è¿›è¡Œå»å™ªï¼Œæˆ‘ä»¬å¼•å…¥äº†æ³¢é•¿ç¼–ç å™¨æ¥ç”Ÿæˆç‰¹å®šæ³¢é•¿çš„çƒé¢è°æ³¢åç§»ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºKullback-Leibleræ•£åº¦æŸå¤±çš„æ–¹æ³•æ¥ç¼©å°æ¸²æŸ“å›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å…‰è°±åˆ†å¸ƒå·®è·ã€‚æ‰©æ•£æ¨¡å‹è¿›ä¸€æ­¥åº”ç”¨äºå¯¹æ¸²æŸ“å›¾åƒè¿›è¡Œå»å™ªï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„é«˜å…‰è°±å›¾åƒã€‚æˆ‘ä»¬åœ¨Hyper-NeRFæ•°æ®é›†ä¸Šçš„äº”ä¸ªä¸åŒé«˜å…‰è°±åœºæ™¯è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æå‡ºçš„HS-GSæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒHS-GSåœ¨æ‰€æœ‰å·²å‘å¸ƒçš„æ–¹æ³•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨å‘å¸ƒæ—¶å…¬å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21890v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHyperspectral Gaussian Splattingï¼ˆHS-GSï¼‰çš„æ–¹æ³•ï¼Œç»“åˆäº†ä¸‰ç»´é«˜æ–¯æº…å°„æŠ€æœ¯ä¸æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†å¯¹é«˜å…‰è°±åœºæ™¯çš„éšå¼é‡å»ºä¸æ–°å‹è§†åˆæˆï¼Œè¾¾åˆ°å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚HS-GSæ¨¡å‹å…·æœ‰å¿«é€Ÿæ¸²æŸ“çš„ç‰¹æ€§ï¼Œè§£å†³äº†ç°æœ‰çš„æ¨¡å‹æ¸²æŸ“èƒ½åŠ›ä½ä¸‹çš„é—®é¢˜ã€‚å®ƒé‡‡ç”¨æ³¢é•¿ç¼–ç æŠ€æœ¯æ¥æ•æ‰ä¸åŒæ³¢é•¿åå°„ç‰¹æ€§çš„å˜åŒ–å¹¶åˆ©ç”¨é‚»è¿‘æ³¢é•¿çš„ä¿¡æ¯å®ç°å»å™ªï¼Œå¹¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹æé«˜å›¾åƒå»å™ªæ•ˆæœå’Œç”Ÿæˆé€¼çœŸé«˜å…‰è°±å›¾åƒçš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºHS-GSæ¡†æ¶åœ¨æ‰€æœ‰å·²å‘å¸ƒçš„æ–¹æ³•ä¸­è¡¨ç°æœ€ä¼˜ã€‚ä»£ç å°†åœ¨å‡ºç‰ˆæ—¶å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡ä½¿ç”¨Hyperspectral Gaussian Splattingï¼ˆHS-GSï¼‰æ–¹æ³•ç»“åˆäº†ä¸‰ç»´é«˜æ–¯æº…å°„æŠ€æœ¯ä¸æ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†é«˜å…‰è°±åœºæ™¯çš„éšå¼é‡å»ºä¸æ–°å‹è§†åˆæˆã€‚</li>
<li>HS-GSæ¨¡å‹è§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨è®­ç»ƒæ—¶é—´å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šçš„é™åˆ¶ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ³¢é•¿ç¼–ç æŠ€æœ¯ï¼ŒHS-GSèƒ½å¤Ÿæ•æ‰ä¸åŒæ³¢é•¿åå°„ç‰¹æ€§çš„å˜åŒ–å¹¶åˆ©ç”¨é‚»è¿‘æ³¢é•¿çš„ä¿¡æ¯å®ç°å»å™ªã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹æé«˜äº†å›¾åƒå»å™ªæ•ˆæœå’Œç”Ÿæˆé€¼çœŸé«˜å…‰è°±å›¾åƒçš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4845ee215281ef71517601d760711042.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e7ffb7b9a355ce67fd869fce9e1cd45.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EPiC-Efficient-Video-Camera-Control-Learning-with-Precise-Anchor-Video-Guidance"><a href="#EPiC-Efficient-Video-Camera-Control-Learning-with-Precise-Anchor-Video-Guidance" class="headerlink" title="EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video   Guidance"></a>EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video   Guidance</h2><p><strong>Authors:Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, Mohit Bansal</strong></p>
<p>Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios. </p>
<blockquote>
<p>å…³äºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰ä¸­çš„3Dç›¸æœºæ§åˆ¶æ–¹æ³•ï¼Œè¿‘æœŸçš„ç ”ç©¶é€šå¸¸é€šè¿‡ä»ä¼°è®¡çš„ç‚¹äº‘æ¸²æŸ“æ¥åˆ›å»ºé”šè§†é¢‘ï¼Œä»¥ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„ç»“æ„åŒ–å…ˆéªŒæ¥å¼•å¯¼æ¨¡å‹ã€‚ç„¶è€Œï¼Œç‚¹äº‘ä¼°è®¡ä¸­çš„å›ºæœ‰è¯¯å·®å¾€å¾€å¯¼è‡´é”šè§†é¢‘ä¸å‡†ç¡®ã€‚æ­¤å¤–ï¼Œå¯¹å¤§é‡çš„ç›¸æœºè½¨è¿¹æ ‡æ³¨çš„éœ€æ±‚è¿›ä¸€æ­¥å¢åŠ äº†èµ„æºéœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†EPiCï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”ç²¾ç¡®çš„ç›¸æœºæ§åˆ¶å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ„å»ºé«˜è´¨é‡çš„é”šè§†é¢‘ï¼Œè€Œæ— éœ€æ˜‚è´µçš„ç›¸æœºè½¨è¿¹æ ‡æ³¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºç¬¬ä¸€å¸§å¯è§æ€§çš„æºè§†é¢‘é®ç½©æ¥åˆ›å»ºç”¨äºè®­ç»ƒçš„é«˜ç²¾åº¦é”šè§†é¢‘ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†é«˜å¯¹é½æ€§ï¼Œæ¶ˆé™¤äº†å¯¹ç›¸æœºè½¨è¿¹æ ‡æ³¨çš„éœ€æ±‚ï¼Œå› æ­¤å¯ä»¥è½»æ¾åœ°åº”ç”¨äºä»»ä½•é‡ç”Ÿè§†é¢‘ï¼Œç”Ÿæˆå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰çš„è®­ç»ƒå¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†Anchor-ControlNetï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è°ƒèŠ‚æ¨¡å—ï¼Œå®ƒå°†é”šè§†é¢‘æŒ‡å¯¼é›†æˆåˆ°å¯è§åŒºåŸŸä¸­ï¼Œä»¥é¢„è®­ç»ƒçš„VDMï¼Œå¹¶ä¸”åªéœ€è¦ä¸åˆ°1%çš„ä¸»æ¨¡å‹å‚æ•°ã€‚é€šè¿‡ç»“åˆæå‡ºçš„é”šè§†é¢‘æ•°æ®å’ŒControlNetæ¨¡å—ï¼ŒEPiCèƒ½å¤Ÿå®ç°é«˜æ•ˆçš„è®­ç»ƒï¼Œå¤§å¤§å‡å°‘å‚æ•°ã€è®­ç»ƒæ­¥éª¤å’Œæ•°æ®çš„éœ€æ±‚ï¼ŒåŒæ—¶æ— éœ€å¯¹æ‰©æ•£æ¨¡å‹çš„ä¸»æ¨¡å‹è¿›è¡Œæ”¹åŠ¨ï¼Œé€šå¸¸è¿™æ˜¯ä¸ºäº†ç¼“è§£æ¸²æŸ“ä¸å¯¹é½çš„é—®é¢˜ã€‚è™½ç„¶æ˜¯åœ¨åŸºäºé®ç½©çš„é”šè§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºæ¨æ–­æ—¶åˆ©ç”¨ç‚¹äº‘åˆ¶ä½œçš„é”šè§†é¢‘å…·æœ‰ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®çš„3Dä¿¡æ¯å¼•å¯¼çš„ç›¸æœºæ§åˆ¶ã€‚EPiCåœ¨RealEstate10Kå’ŒMiraDataä¸Šçš„I2Vç›¸æœºæ§åˆ¶ä»»åŠ¡ä¸Šè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œä»å®šé‡å’Œå®šæ€§ä¸¤ä¸ªæ–¹é¢éƒ½è¯æ˜äº†å…¶ç²¾ç¡®ä¸”ç¨³å¥çš„ç›¸æœºæ§åˆ¶èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒEPiCåœ¨è§†é¢‘åˆ°è§†é¢‘çš„åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21876v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://zunwang1.github.io/Epic">https://zunwang1.github.io/Epic</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„ç›¸æœºæ§åˆ¶å­¦ä¹ æ¡†æ¶EPiCï¼Œæ— éœ€æ˜‚è´µçš„ç›¸æœºè½¨è¿¹æ ‡æ³¨ï¼Œå³å¯è‡ªåŠ¨æ„å»ºé«˜è´¨é‡é”šè§†é¢‘ï¼Œç”¨äºè®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰ã€‚é€šè¿‡åŸºäºé¦–å¸§å¯è§æ€§çš„æºè§†é¢‘é®ç½©åˆ›å»ºç²¾ç¡®é”šè§†é¢‘ï¼Œå®ç°é«˜å¯¹é½ï¼Œæ¶ˆé™¤å¯¹ç›¸æœºè½¨è¿¹æ ‡æ³¨çš„éœ€æ±‚ï¼Œå¹¶å¯ç›´æ¥åº”ç”¨äºä»»ä½•é‡ç”Ÿè§†é¢‘ç”Ÿæˆå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰çš„è®­ç»ƒå¯¹ã€‚æ­¤å¤–ï¼Œå¼•å…¥Anchor-ControlNetæ¨¡å—ï¼Œåœ¨å¯è§åŒºåŸŸé›†æˆé”šè§†é¢‘æŒ‡å¯¼é¢„è®­ç»ƒçš„VDMï¼Œå‚æ•°å°‘äºä¸»å¹²æ¨¡å‹å‚æ•°çš„1%ã€‚ç»“åˆæå‡ºçš„é”šè§†é¢‘æ•°æ®å’ŒControlNetæ¨¡å—ï¼ŒEPiCå®ç°äº†é«˜æ•ˆè®­ç»ƒï¼Œå¤§å¹…å‡å°‘å‚æ•°ã€è®­ç»ƒæ­¥éª¤å’Œæ•°æ®éœ€æ±‚ï¼Œä¸”æ— éœ€ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„ä¸»å¹²æ¥ç¼“è§£æ¸²æŸ“é”™ä½é—®é¢˜ã€‚å°½ç®¡æ˜¯åœ¨åŸºäºé®ç½©çš„é”šè§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†æ—¶åˆ©ç”¨ç‚¹äº‘åˆ¶ä½œçš„é”šè§†é¢‘ä¹Ÿè¡¨ç°ç¨³å¥ï¼Œå®ç°äº†ç²¾ç¡®çš„3Dä¿¡æ¯å¼•å¯¼çš„ç›¸æœºæ§åˆ¶ã€‚EPiCåœ¨RealEstate10Kå’ŒMiraDataçš„I2Vç›¸æœºæ§åˆ¶ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜å…¶åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å…·å¤‡ç²¾ç¡®ä¸”ç¨³å¥çš„ç›¸æœºæ§åˆ¶èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒEPiCåœ¨è§†é¢‘åˆ°è§†é¢‘åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>EPiCæ¡†æ¶èƒ½è‡ªåŠ¨æ„å»ºé«˜è´¨é‡é”šè§†é¢‘ï¼Œæ— éœ€æ˜‚è´µçš„ç›¸æœºè½¨è¿¹æ ‡æ³¨ã€‚</li>
<li>é€šè¿‡åŸºäºé¦–å¸§å¯è§æ€§çš„æºè§†é¢‘é®ç½©åˆ›å»ºç²¾ç¡®é”šè§†é¢‘ï¼Œå®ç°é«˜å¯¹é½ã€‚</li>
<li>Anchor-ControlNetæ¨¡å—é›†æˆé”šè§†é¢‘æŒ‡å¯¼ï¼Œå‚æ•°éœ€æ±‚å°‘ã€‚</li>
<li>ç»“åˆé”šè§†é¢‘æ•°æ®å’ŒControlNetæ¨¡å—ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒï¼Œå‡å°‘å‚æ•°ã€è®­ç»ƒæ­¥éª¤å’Œæ•°æ®éœ€æ±‚ã€‚</li>
<li>æ–¹æ³•åœ¨åŸºäºç‚¹äº‘çš„é”šè§†é¢‘ä¸Šä¹Ÿè¡¨ç°ç¨³å¥ï¼Œå®ç°ç²¾ç¡®3Dä¿¡æ¯å¼•å¯¼çš„ç›¸æœºæ§åˆ¶ã€‚</li>
<li>åœ¨I2Vç›¸æœºæ§åˆ¶ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f27a9ac9e32097418934d573a4d56c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f96b602868d4e95b400fb1c3eba0eb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8743d83b99b208a00280e759a4cf6fcd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MagicTryOn-Harnessing-Diffusion-Transformer-for-Garment-Preserving-Video-Virtual-Try-on"><a href="#MagicTryOn-Harnessing-Diffusion-Transformer-for-Garment-Preserving-Video-Virtual-Try-on" class="headerlink" title="MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving   Video Virtual Try-on"></a>MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving   Video Virtual Try-on</h2><p><strong>Authors:Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang</strong></p>
<p>Video Virtual Try-On (VVT) aims to simulate the natural appearance of garments across consecutive video frames, capturing their dynamic variations and interactions with human body motion. However, current VVT methods still face challenges in terms of spatiotemporal consistency and garment content preservation. First, they use diffusion models based on the U-Net, which are limited in their expressive capability and struggle to reconstruct complex details. Second, they adopt a separative modeling approach for spatial and temporal attention, which hinders the effective capture of structural relationships and dynamic consistency across frames. Third, their expression of garment details remains insufficient, affecting the realism and stability of the overall synthesized results, especially during human motion. To address the above challenges, we propose MagicTryOn, a video virtual try-on framework built upon the large-scale video diffusion Transformer. We replace the U-Net architecture with a diffusion Transformer and combine full self-attention to jointly model the spatiotemporal consistency of videos. We design a coarse-to-fine garment preservation strategy. The coarse strategy integrates garment tokens during the embedding stage, while the fine strategy incorporates multiple garment-based conditions, such as semantics, textures, and contour lines during the denoising stage. Moreover, we introduce a mask-aware loss to further optimize garment region fidelity. Extensive experiments on both image and video try-on datasets demonstrate that our method outperforms existing SOTA methods in comprehensive evaluations and generalizes to in-the-wild scenarios. </p>
<blockquote>
<p>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æ—¨åœ¨æ¨¡æ‹Ÿè¡£ç‰©åœ¨è¿ç»­è§†é¢‘å¸§ä¸­çš„è‡ªç„¶å¤–è§‚ï¼Œæ•æ‰å…¶åŠ¨æ€å˜åŒ–å’Œä¸äººä½“è¿åŠ¨çš„äº¤äº’ã€‚ç„¶è€Œï¼Œå½“å‰çš„VVTæ–¹æ³•ä»é¢ä¸´æ—¶ç©ºä¸€è‡´æ€§å’Œæœè£…å†…å®¹ä¿ç•™æ–¹é¢çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒä»¬ä½¿ç”¨åŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹ï¼Œå…¶è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é‡å»ºå¤æ‚çš„ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬é‡‡ç”¨ç©ºé—´å’Œæ—¶é—´çš„åˆ†ç¦»å»ºæ¨¡æ–¹æ³•ï¼Œè¿™é˜»ç¢äº†è·¨å¸§çš„ç»“æ„å…³ç³»å’ŒåŠ¨æ€ä¸€è‡´æ€§çš„æœ‰æ•ˆæ•è·ã€‚ç¬¬ä¸‰ï¼Œå®ƒä»¬å¯¹æœè£…ç»†èŠ‚çš„è¡¨è¾¾ä»ç„¶ä¸è¶³ï¼Œå½±å“äº†åˆæˆç»“æœçš„ç°å®æ„Ÿå’Œç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äººä½“è¿åŠ¨æœŸé—´ã€‚é’ˆå¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MagicTryOnï¼Œä¸€ä¸ªåŸºäºå¤§è§„æ¨¡è§†é¢‘æ‰©æ•£Transformerçš„è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ¡†æ¶ã€‚æˆ‘ä»¬ç”¨æ‰©æ•£Transformeræ›¿æ¢U-Netæ¶æ„ï¼Œå¹¶ç»“åˆå…¨è‡ªæ³¨æ„åŠ›æ¥è”åˆå»ºæ¨¡è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æœè£…ä¿ç•™ç­–ç•¥ã€‚ç²—ç­–ç•¥åœ¨åµŒå…¥é˜¶æ®µé›†æˆæœè£…ä»¤ç‰Œï¼Œè€Œç»†ç­–ç•¥åœ¨é™å™ªé˜¶æ®µèå…¥å¤šç§åŸºäºæœè£…çš„æ¡ä»¶ï¼Œå¦‚è¯­ä¹‰ã€çº¹ç†å’Œè½®å»“çº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ©è†œæ„ŸçŸ¥æŸå¤±æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æœè£…åŒºåŸŸä¿çœŸåº¦ã€‚åœ¨å›¾åƒå’Œè§†é¢‘è¯•ç©¿æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»¼åˆè¯„ä¼°ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æ¨å¹¿åˆ°é‡å¤–åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21325v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æŠ€æœ¯æ—¨åœ¨æ¨¡æ‹Ÿè¡£ç‰©åœ¨è¿ç»­è§†é¢‘å¸§ä¸­çš„è‡ªç„¶å¤–è§‚ï¼Œæ•æ‰å…¶ä¸äººä½“åŠ¨ä½œçš„åŠ¨æ€å˜åŒ–å’Œäº¤äº’ã€‚ç„¶è€Œï¼Œå½“å‰VVTæ–¹æ³•ä»é¢ä¸´æ—¶ç©ºä¸€è‡´æ€§å’Œè¡£ç‰©å†…å®¹ä¿ç•™æ–¹é¢çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒä»¬ä½¿ç”¨åŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹ï¼Œå…¶è¡¨ç°èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é‡å»ºå¤æ‚ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬é‡‡ç”¨ç©ºé—´å’Œæ—¶é—´çš„åˆ†ç¦»å»ºæ¨¡æ–¹æ³•ï¼Œè¿™é˜»ç¢äº†å¸§é—´ç»“æ„å…³ç³»å’ŒåŠ¨æ€ä¸€è‡´æ€§çš„æœ‰æ•ˆæ•æ‰ã€‚æ­¤å¤–ï¼Œè¡£ç‰©ç»†èŠ‚çš„è¡¨è¾¾èƒ½åŠ›ä¸è¶³ï¼Œå½±å“äº†åˆæˆç»“æœçš„é€¼çœŸåº¦å’Œç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äººä½“è¿åŠ¨è¿‡ç¨‹ä¸­ã€‚ä¸ºè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MagicTryOnï¼Œä¸€ä¸ªåŸºäºå¤§è§„æ¨¡è§†é¢‘æ‰©æ•£Transformerçš„è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨æ‰©æ•£Transformeræ›¿ä»£U-Netæ¶æ„ï¼Œç»“åˆå…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§è¿›è¡Œè”åˆå»ºæ¨¡ã€‚è®¾è®¡äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„è¡£ç‰©ä¿ç•™ç­–ç•¥ã€‚ç²—ç­–ç•¥åœ¨åµŒå…¥é˜¶æ®µé›†æˆè¡£ç‰©ä»¤ç‰Œï¼Œè€Œç»†ç­–ç•¥åœ¨é™å™ªé˜¶æ®µèå…¥å¤šç§åŸºäºè¡£ç‰©çš„æ¡ä»¶ï¼Œå¦‚è¯­ä¹‰ã€çº¹ç†å’Œè½®å»“çº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ©ç æ„ŸçŸ¥æŸå¤±æ¥è¿›ä¸€æ­¥ä¼˜åŒ–è¡£ç‰©åŒºåŸŸçš„ä¿çœŸåº¦ã€‚åœ¨å›¾åƒå’Œè§†é¢‘è¯•ç©¿æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»¼åˆè¯„ä¼°ä¸­è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶é€‚ç”¨äºå®é™…åœºæ™¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æŠ€æœ¯æ—¨åœ¨æ¨¡æ‹Ÿè¡£ç‰©åœ¨è§†é¢‘ä¸­çš„è‡ªç„¶å¤–è§‚ï¼Œé¢ä¸´æ—¶ç©ºä¸€è‡´æ€§å’Œè¡£ç‰©å†…å®¹ä¿ç•™çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ä½¿ç”¨åŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹ï¼Œå…¶è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é‡å»ºå¤æ‚ç»†èŠ‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨åˆ†ç¦»å»ºæ¨¡æ—¶ç©ºæ³¨æ„åŠ›çš„æ–¹å¼ï¼Œå½±å“ç»“æ„å…³ç³»å’ŒåŠ¨æ€ä¸€è‡´æ€§çš„æœ‰æ•ˆæ•æ‰ã€‚</li>
<li>MagicTryOnæ¡†æ¶ä½¿ç”¨å¤§è§„æ¨¡è§†é¢‘æ‰©æ•£Transformerï¼Œç»“åˆå…¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡ã€‚</li>
<li>MagicTryOnè®¾è®¡äº†ä¸€ç§è¡£ç‰©ä¿ç•™ç­–ç•¥ï¼ŒåŒ…æ‹¬åœ¨åµŒå…¥é˜¶æ®µé›†æˆè¡£ç‰©ä»¤ç‰Œä»¥åŠåœ¨é™å™ªé˜¶æ®µèå…¥å¤šç§è¡£ç‰©æ¡ä»¶ã€‚</li>
<li>å¼•å…¥æ©ç æ„ŸçŸ¥æŸå¤±ä»¥ä¼˜åŒ–è¡£ç‰©åŒºåŸŸçš„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a79bbbf912c43019473c2710c6bb81c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fde2db8af9c328a1754379fb7af96df0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dc565d87e14ce3df5aad38a9baadf61.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Integrating-Intermediate-Layer-Optimization-and-Projected-Gradient-Descent-for-Solving-Inverse-Problems-with-Diffusion-Models"><a href="#Integrating-Intermediate-Layer-Optimization-and-Projected-Gradient-Descent-for-Solving-Inverse-Problems-with-Diffusion-Models" class="headerlink" title="Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models"></a>Integrating Intermediate Layer Optimization and Projected Gradient   Descent for Solving Inverse Problems with Diffusion Models</h2><p><strong>Authors:Yang Zheng, Wen Li, Zhaoqiang Liu</strong></p>
<p>Inverse problems (IPs) involve reconstructing signals from noisy observations. Recently, diffusion models (DMs) have emerged as a powerful framework for solving IPs, achieving remarkable reconstruction performance. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approaches under appropriate conditions and validate their superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers. </p>
<blockquote>
<p>é€†å‘é—®é¢˜ï¼ˆIPsï¼‰æ¶‰åŠä»å™ªå£°è§‚å¯Ÿä¸­é‡å»ºä¿¡å·ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºè§£å†³IPsçš„å¼ºå¤§æ¡†æ¶è€Œå‡ºç°ï¼Œå®ç°äº†æ˜¾è‘—çš„é‡å»ºæ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºDMçš„æ–¹æ³•ç»å¸¸é‡åˆ°è®¡ç®—éœ€æ±‚å¤§ã€æ”¶æ•›ä¸ä½³ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºæœ€è¿‘çš„å·¥ä½œDMPlugï¼Œæå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•DMILOå’ŒDMILO-PGDï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ç§æ–¹æ³•DMILOé‡‡ç”¨ä¸­é—´å±‚ä¼˜åŒ–ï¼ˆILOï¼‰æ¥ç¼“è§£DMPlugæ‰€å›ºæœ‰çš„å†…å­˜è´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ç¨€ç–åå·®ï¼Œæˆ‘ä»¬æ‰©å¤§äº†DMçš„èŒƒå›´ï¼Œèƒ½å¤Ÿæ¢ç´¢å¯èƒ½ä½äºæ‰©æ•£æ¨¡å‹èŒƒå›´ä¹‹å¤–çš„æ½œåœ¨ä¿¡å·ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†DMILO-PGDï¼Œå®ƒå°†ILOä¸æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ç›¸ç»“åˆï¼Œä»è€Œé™ä½äº†æ”¶æ•›ä¸ä½³çš„é£é™©ã€‚æˆ‘ä»¬åœ¨é€‚å½“æ¡ä»¶ä¸‹å¯¹æ–¹æ³•è¿›è¡Œäº†ç›´è§‚çš„ç†è®ºåˆ†æï¼Œå¹¶é€šè¿‡å¯¹å„ç§å›¾åƒæ•°æ®é›†çš„å¤§é‡å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œè¿™äº›å®éªŒæ¶µç›–äº†çº¿æ€§å’Œéçº¿æ€§IPsã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†DMILOå’ŒDMILO-PGDåœ¨è§£å†³åŸºäºDMçš„IPæ±‚è§£å™¨ä¸­çš„å¸¸è§é—®é¢˜æ—¶çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20789v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨è§£å†³åé—®é¢˜ï¼ˆIPï¼‰ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨è®¡ç®—é‡å¤§å’Œæ”¶æ•›æ€§ä¸ä½³ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸¤ç§æ–°æ–¹æ³•DMILOå’ŒDMILO-PGDï¼Œå‰è€…é€šè¿‡ä¸­é—´å±‚ä¼˜åŒ–ï¼ˆILOï¼‰å‡è½»å†…å­˜è´Ÿæ‹…ï¼Œå¹¶å¼•å…¥ç¨€ç–åå·®æ‰©å¤§æ¨¡å‹æ¢ç´¢èŒƒå›´ï¼›åè€…ç»“åˆILOå’ŒæŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ï¼Œé™ä½æ”¶æ•›é£é™©ã€‚ç†è®ºåˆ†æå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ–¹æ³•åœ¨å›¾åƒæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨åé—®é¢˜ï¼ˆIPï¼‰è§£å†³ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ç°æœ‰DMæ–¹æ³•å­˜åœ¨è®¡ç®—é‡å¤§å’Œæ”¶æ•›æ€§é—®é¢˜ã€‚</li>
<li>DMILOæ–¹æ³•é€šè¿‡ä¸­é—´å±‚ä¼˜åŒ–ï¼ˆILOï¼‰å‡è½»å†…å­˜è´Ÿæ‹…ã€‚</li>
<li>DMILOå¼•å…¥ç¨€ç–åå·®ä»¥æ‰©å¤§æ¨¡å‹çš„æ¢ç´¢èŒƒå›´ã€‚</li>
<li>DMILO-PGDç»“åˆILOå’ŒæŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ï¼Œæé«˜æ”¶æ•›æ€§ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•åœ¨å®éªŒä¸ŠéªŒè¯äº†å¯¹å¤šç§å›¾åƒæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eacc20dbb11b394480b5c3a1200dc55e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8440b4d1ceaf394f64337d49e0ff6204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608c1736489c1294ff5674ac5aeca7bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f249d2a2a4131a2012a96ee62e78474.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DIPO-Dual-State-Images-Controlled-Articulated-Object-Generation-Powered-by-Diverse-Data"><a href="#DIPO-Dual-State-Images-Controlled-Articulated-Object-Generation-Powered-by-Diverse-Data" class="headerlink" title="DIPO: Dual-State Images Controlled Articulated Object Generation Powered   by Diverse Data"></a>DIPO: Dual-State Images Controlled Articulated Object Generation Powered   by Diverse Data</h2><p><strong>Authors:Ruiqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, Ming-Ming Cheng</strong></p>
<p>We present DIPO, a novel framework for the controllable generation of articulated 3D objects from a pair of images: one depicting the object in a resting state and the other in an articulated state. Compared to the single-image approach, our dual-image input imposes only a modest overhead for data collection, but at the same time provides important motion information, which is a reliable guide for predicting kinematic relationships between parts. Specifically, we propose a dual-image diffusion model that captures relationships between the image pair to generate part layouts and joint parameters. In addition, we introduce a Chain-of-Thought (CoT) based graph reasoner that explicitly infers part connectivity relationships. To further improve robustness and generalization on complex articulated objects, we develop a fully automated dataset expansion pipeline, name LEGO-Art, that enriches the diversity and complexity of PartNet-Mobility dataset. We propose PM-X, a large-scale dataset of complex articulated 3D objects, accompanied by rendered images, URDF annotations, and textual descriptions. Extensive experiments demonstrate that DIPO significantly outperforms existing baselines in both the resting state and the articulated state, while the proposed PM-X dataset further enhances generalization to diverse and structurally complex articulated objects. Our code and dataset will be released to the community upon publication. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†DIPOï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¯æ§ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥ä»ä¸€å¯¹å›¾åƒä¸­ç”Ÿæˆç²¾ç»†çš„3Då¯¹è±¡ï¼šä¸€å¼ å›¾åƒæç»˜å¯¹è±¡åœ¨é™æ­¢çŠ¶æ€ï¼Œå¦ä¸€å¼ å›¾åƒæç»˜å¯¹è±¡åœ¨ç²¾ç»†è¿åŠ¨çŠ¶æ€ã€‚ä¸å•å›¾åƒæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŒå›¾åƒè¾“å…¥åªä¼šå¯¹æ•°æ®é‡‡é›†é€ æˆé€‚ä¸­çš„å¼€é”€ï¼Œä½†ä¸æ­¤åŒæ—¶æä¾›äº†é‡è¦çš„è¿åŠ¨ä¿¡æ¯ï¼Œè¿™å¯¹äºé¢„æµ‹éƒ¨ä»¶ä¹‹é—´çš„è¿åŠ¨å…³ç³»æ˜¯ä¸€ä¸ªå¯é çš„æŒ‡å—ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ•æ‰å›¾åƒå¯¹ä¹‹é—´çš„å…³ç³»ä»¥ç”Ÿæˆéƒ¨åˆ†å¸ƒå±€å’Œå…³èŠ‚å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰çš„å›¾æ¨ç†å™¨ï¼Œå¯ä»¥æ˜ç¡®æ¨æ–­éƒ¨åˆ†è¿æ¥å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åœ¨å¤æ‚ç²¾ç»†å¯¹è±¡ä¸Šçš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®é›†æ‰©å±•ç®¡é“ï¼Œåä¸ºLEGO-Artï¼Œå®ƒä¸°å¯Œäº†PartNet-Mobilityæ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†PM-Xï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡å¤æ‚ç²¾ç»†3Då¯¹è±¡æ•°æ®é›†ï¼Œé…æœ‰æ¸²æŸ“å›¾åƒã€URDFæ³¨é‡Šå’Œæ–‡æœ¬æè¿°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDIPOåœ¨é™æ­¢çŠ¶æ€å’Œç²¾ç»†è¿åŠ¨çŠ¶æ€æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè€Œæå‡ºçš„PM-Xæ•°æ®é›†è¿›ä¸€æ­¥å¢å¼ºäº†å¯¹å„ç§ç»“æ„å’Œå¤æ‚ç²¾ç»†å¯¹è±¡çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨å‘å¸ƒæ—¶å‘å…¬ä¼—å¼€æ”¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20460v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æå‡ºäº†DIPOæ¡†æ¶ï¼Œç”¨äºä»ä¸€å¯¹å›¾åƒå¯æ§åœ°ç”Ÿæˆå¤æ‚çš„ä¸‰ç»´ç‰©ä½“ã€‚ä¸å•å›¾åƒæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŒå›¾åƒè¾“å…¥å¸¦æ¥äº†è½»å¾®çš„æ•°æ®æ”¶é›†é¢å¤–è´Ÿæ‹…ï¼Œä½†åŒæ—¶æä¾›äº†é‡è¦çš„è¿åŠ¨ä¿¡æ¯ï¼Œå¯ä½œä¸ºé¢„æµ‹å„éƒ¨åˆ†é—´è¿åŠ¨å…³ç³»çš„å¯é æŒ‡å—ã€‚æˆ‘ä»¬ä½¿ç”¨åŒå›¾åƒæ‰©æ•£æ¨¡å‹æ¥æ•æ‰å›¾åƒé—´çš„å…³ç³»ï¼Œä»¥ç”Ÿæˆéƒ¨åˆ†å¸ƒå±€å’Œå…³èŠ‚å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰çš„å›¾å½¢æ¨ç†å™¨æ¥æ˜ç¡®æ¨æ–­éƒ¨åˆ†è¿æ¥å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åœ¨å¤æ‚å…³èŠ‚ç‰©ä½“çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨æ•°æ®é›†æ‰©å±•ç®¡é“LEGO-Artï¼Œä¸°å¯Œäº†PartNet-Mobilityæ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†PM-Xæ•°æ®é›†ï¼ŒåŒ…å«å¤æ‚å…³èŠ‚ä¸‰ç»´ç‰©ä½“çš„æ¸²æŸ“å›¾åƒã€URDFæ³¨é‡Šå’Œæ–‡æœ¬æè¿°ã€‚å®éªŒè¡¨æ˜ï¼ŒDIPOåœ¨é™æ­¢çŠ¶æ€å’Œå…³èŠ‚çŠ¶æ€æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè€ŒPM-Xæ•°æ®é›†è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å¯¹å¤šæ ·åŒ–å’Œç»“æ„å¤æ‚çš„å…³èŠ‚ç‰©ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨å‘å¸ƒæ—¶å‘å…¬ä¼—å¼€æ”¾ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>DIPOæ¡†æ¶é‡‡ç”¨åŒå›¾åƒè¾“å…¥ï¼Œç”¨äºç”Ÿæˆå¤æ‚çš„ä¸‰ç»´ç‰©ä½“ï¼ŒåŒ…æ‹¬é™æ­¢çŠ¶æ€å’Œå…³èŠ‚çŠ¶æ€çš„å¯¹è±¡å›¾åƒã€‚</li>
<li>åŒå›¾åƒæ‰©æ•£æ¨¡å‹ç”¨äºæ•æ‰å›¾åƒé—´çš„å†…åœ¨å…³ç³»ä»¥ç”Ÿæˆç‰©ä½“çš„éƒ¨åˆ†å¸ƒå±€å’Œå…³èŠ‚å‚æ•°ã€‚</li>
<li>å¼•å…¥Chain-of-Thoughtï¼ˆCoTï¼‰çš„å›¾å½¢æ¨ç†å™¨æ¥æ¨æ–­ç‰©ä½“éƒ¨åˆ†ä¹‹é—´çš„è¿æ¥å…³ç³»ã€‚</li>
<li>å¼€å‘LEGO-Artæ•°æ®é›†æ‰©å±•ç®¡é“ï¼Œå¢å¼ºæ•°æ®é›†çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚</li>
<li>æå‡ºPM-Xæ•°æ®é›†ï¼ŒåŒ…å«å¤æ‚å…³èŠ‚ä¸‰ç»´ç‰©ä½“çš„æ¸²æŸ“å›¾åƒã€URDFæ³¨é‡Šå’Œæ–‡æœ¬æè¿°ã€‚</li>
<li>DIPOæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå¤æ‚ä¸‰ç»´ç‰©ä½“çš„æ€§èƒ½è¡¨ç°ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b25d3e5206b57c1d7635e2c4347bf33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f681612f83e01091c43fee79524dcb1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e272565708f20ab61c06e29bb544a122.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-060221abf76271747c6fef6e540da2c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a9e04c362635c7ae6cea77331629ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-644176ee23b7ff0dda0c86dd2a46faa6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CreatiDesign-A-Unified-Multi-Conditional-Diffusion-Transformer-for-Creative-Graphic-Design"><a href="#CreatiDesign-A-Unified-Multi-Conditional-Diffusion-Transformer-for-Creative-Graphic-Design" class="headerlink" title="CreatiDesign: A Unified Multi-Conditional Diffusion Transformer for   Creative Graphic Design"></a>CreatiDesign: A Unified Multi-Conditional Diffusion Transformer for   Creative Graphic Design</h2><p><strong>Authors:Hui Zhang, Dexiang Hong, Maoke Yang, Yutao Cheng, Zhao Zhang, Jie Shao, Xinglong Wu, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>Graphic design plays a vital role in visual communication across advertising, marketing, and multimedia entertainment. Prior work has explored automated graphic design generation using diffusion models, aiming to streamline creative workflows and democratize design capabilities. However, complex graphic design scenarios require accurately adhering to design intent specified by multiple heterogeneous user-provided elements (\eg images, layouts, and texts), which pose multi-condition control challenges for existing methods. Specifically, previous single-condition control models demonstrate effectiveness only within their specialized domains but fail to generalize to other conditions, while existing multi-condition methods often lack fine-grained control over each sub-condition and compromise overall compositional harmony. To address these limitations, we introduce CreatiDesign, a systematic solution for automated graphic design covering both model architecture and dataset construction. First, we design a unified multi-condition driven architecture that enables flexible and precise integration of heterogeneous design elements with minimal architectural modifications to the base diffusion model. Furthermore, to ensure that each condition precisely controls its designated image region and to avoid interference between conditions, we propose a multimodal attention mask mechanism. Additionally, we develop a fully automated pipeline for constructing graphic design datasets, and introduce a new dataset with 400K samples featuring multi-condition annotations, along with a comprehensive benchmark. Experimental results show that CreatiDesign outperforms existing models by a clear margin in faithfully adhering to user intent. </p>
<blockquote>
<p>å¹³é¢è®¾è®¡åœ¨å¹¿å‘Šã€è¥é”€å’Œå¤šåª’ä½“å¨±ä¹ç­‰é¢†åŸŸçš„è§†è§‰äº¤æµä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè‡ªåŠ¨å¹³é¢è®¾è®¡ç”Ÿæˆï¼Œæ—¨åœ¨ç®€åŒ–åˆ›æ„å·¥ä½œæµç¨‹å¹¶æ™®åŠè®¾è®¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤æ‚çš„å¹³é¢è®¾è®¡åœºæ™¯éœ€è¦å‡†ç¡®éµå¾ªç”±å¤šä¸ªå¼‚æ„ç”¨æˆ·æä¾›çš„å…ƒç´ ï¼ˆä¾‹å¦‚å›¾åƒã€å¸ƒå±€å’Œæ–‡æœ¬ï¼‰æŒ‡å®šçš„è®¾è®¡æ„å›¾ï¼Œè¿™ä¸ºç°æœ‰æ–¹æ³•å¸¦æ¥äº†å¤šæ¡ä»¶æ§åˆ¶æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå…ˆå‰çš„å•æ¡ä»¶æ§åˆ¶æ¨¡å‹ä»…åœ¨å®ƒä»¬çš„ä¸“ä¸šé¢†åŸŸå†…æœ‰æ•ˆï¼Œä½†éš¾ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¡ä»¶ï¼Œè€Œç°æœ‰çš„å¤šæ¡ä»¶æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹æ¯ä¸ªå­æ¡ä»¶çš„ç²¾ç»†æ§åˆ¶ï¼Œå¹¶æŸå®³äº†æ•´ä½“çš„æ„å›¾å’Œè°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†CreatiDesignï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†æ„å»ºçš„è‡ªåŠ¨åŒ–å¹³é¢è®¾è®¡ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¡ä»¶é©±åŠ¨æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿçµæ´»ç²¾ç¡®åœ°é›†æˆå¼‚æ„è®¾è®¡å…ƒç´ ï¼Œå¹¶ä¸”åªéœ€å¯¹åŸºç¡€æ‰©æ•£æ¨¡å‹è¿›è¡Œæœ€å°çš„æ¶æ„ä¿®æ”¹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿æ¯ä¸ªæ¡ä»¶ç²¾ç¡®åœ°æ§åˆ¶å…¶æŒ‡å®šçš„å›¾åƒåŒºåŸŸå¹¶é¿å…æ¡ä»¶ä¹‹é—´çš„å¹²æ‰°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ³¨æ„åŠ›æ©ç æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†å…¨è‡ªåŠ¨çš„å¹³é¢è®¾è®¡æ•°æ®é›†æ„å»ºæµç¨‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«40ä¸‡æ ·æœ¬çš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å¤šæ¡ä»¶æ³¨é‡Šä»¥åŠå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCreatiDesignåœ¨å¿ å®äºç”¨æˆ·æ„å›¾æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19114v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¹¿å‘Šã€è¥é”€å’Œå¤šåª’ä½“å¨±ä¹ç­‰é¢†åŸŸä¸­ï¼Œå›¾å½¢è®¾è®¡åœ¨è§†è§‰æ²Ÿé€šä¸­çš„é‡è¦æ€§ã€‚é’ˆå¯¹è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡ç”Ÿæˆçš„é—®é¢˜ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¢ç´¢ï¼Œæ—¨åœ¨ä¼˜åŒ–åˆ›æ„å·¥ä½œæµç¨‹å¹¶æ™®åŠè®¾è®¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤æ‚å›¾å½¢è®¾è®¡åœºæ™¯éœ€è¦ç²¾å‡†åœ°éµå¾ªç”¨æˆ·æä¾›çš„å¤šä¸ªå¼‚æ„å…ƒç´ ï¼ˆå¦‚å›¾åƒã€å¸ƒå±€å’Œæ–‡æœ¬ï¼‰æ‰€æŒ‡å®šçš„è®¾è®¡æ„å›¾ï¼Œè¿™å¯¹ç°æœ‰æ–¹æ³•æå‡ºäº†å¤šæ¡ä»¶æ§åˆ¶çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CreatiDesignç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ¶µç›–äº†æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†æ„å»ºã€‚é€šè¿‡è®¾è®¡ç»Ÿä¸€çš„å¤šæ¡ä»¶é©±åŠ¨æ¶æ„ï¼Œå®ç°çµæ´»ç²¾ç¡®åœ°é›†æˆå¼‚æ„è®¾è®¡å…ƒç´ ï¼Œå¹¶å¼•å…¥å¤šæ¨¡æ€æ³¨æ„åŠ›å±è”½æœºåˆ¶ç¡®ä¿æ¯ä¸ªæ¡ä»¶ç²¾ç¡®æ§åˆ¶æŒ‡å®šçš„å›¾åƒåŒºåŸŸï¼Œé¿å…æ¡ä»¶é—´çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å»ºç«‹äº†å…¨è‡ªåŠ¨çš„å›¾å½¢è®¾è®¡æ•°æ®é›†æ„å»ºæµç¨‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«40ä¸‡æ ·æœ¬çš„æ–°æ•°æ®é›†ï¼Œè¿›è¡Œå¤šæ¡ä»¶æ³¨é‡Šå’Œå…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCreatiDesignåœ¨å¿ å®äºç”¨æˆ·æ„å›¾æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾å½¢è®¾è®¡åœ¨è§†è§‰æ²Ÿé€šé¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿å‘Šã€è¥é”€å’Œå¤šåª’ä½“å¨±ä¹æ–¹é¢ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºè‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡ç”Ÿæˆï¼Œæ—¨åœ¨ä¼˜åŒ–åˆ›æ„å·¥ä½œæµç¨‹å’Œæ™®åŠè®¾è®¡èƒ½åŠ›ã€‚</li>
<li>å¤æ‚å›¾å½¢è®¾è®¡åœºæ™¯éœ€è¦éµå¾ªå¤šä¸ªå¼‚æ„ç”¨æˆ·å…ƒç´ æŒ‡å®šçš„è®¾è®¡æ„å›¾ï¼Œè¿™å¯¹ç°æœ‰æ–¹æ³•æå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>å­˜åœ¨çš„æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸæœ‰æ•ˆï¼Œä½†éš¾ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¡ä»¶ï¼Œè€Œå¤šæ¡ä»¶æ–¹æ³•åˆ™ç¼ºä¹å¯¹æ¯ä¸ªå­æ¡ä»¶çš„ç²¾ç»†æ§åˆ¶å¹¶å¯èƒ½å½±å“æ•´ä½“å’Œè°ã€‚</li>
<li>å¼•å…¥çš„CreatiDesignç³»ç»Ÿè§£å†³æ–¹æ¡ˆåŒ…æ‹¬æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†æ„å»ºï¼Œå®ç°çµæ´»ç²¾ç¡®åœ°é›†æˆå¼‚æ„è®¾è®¡å…ƒç´ ã€‚</li>
<li>å¤šæ¨¡æ€æ³¨æ„åŠ›å±è”½æœºåˆ¶ç¡®ä¿æ¯ä¸ªæ¡ä»¶ç²¾ç¡®æ§åˆ¶æŒ‡å®šçš„å›¾åƒåŒºåŸŸï¼Œé¿å…æ¡ä»¶é—´çš„å¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1837d7c5f56ee8f1acd0d97cac2f529a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59ff18833c5df7b4239a9f213983bae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811e21237f15f4557c0fd2dc7612655e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-108a686ea25b99828c0d61433788027f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Latent-Beam-Diffusion-Models-for-Decoding-Image-Sequences"><a href="#Latent-Beam-Diffusion-Models-for-Decoding-Image-Sequences" class="headerlink" title="Latent Beam Diffusion Models for Decoding Image Sequences"></a>Latent Beam Diffusion Models for Decoding Image Sequences</h2><p><strong>Authors:Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, JoÃ£o MagalhÃ£es</strong></p>
<p>While diffusion models excel at generating high-quality images from text prompts, they struggle with visual consistency in image sequences. Existing methods generate each image independently, leading to disjointed narratives - a challenge further exacerbated in non-linear storytelling, where scenes must connect beyond adjacent frames. We introduce a novel beam search strategy for latent space exploration, enabling conditional generation of full image sequences with beam search decoding. Unlike prior approaches that use fixed latent priors, our method dynamically searches for an optimal sequence of latent representations, ensuring coherent visual transitions. As the latent denoising space is explored, the beam search graph is pruned with a cross-attention mechanism that efficiently scores search paths, prioritizing alignment with both textual prompts and visual context. Human and automatic evaluations confirm that BeamDiffusion outperforms other baseline methods, producing full sequences with superior coherence, visual continuity, and textual alignment. </p>
<blockquote>
<p>è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒåºåˆ—çš„è§†è§‰ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ–¹æ³•ç‹¬ç«‹ç”Ÿæˆæ¯å¼ å›¾åƒï¼Œå¯¼è‡´å™äº‹ä¸è¿è´¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éçº¿æ€§å™äº‹ä¸­ï¼Œåœºæ™¯å¿…é¡»åœ¨ç›¸é‚»å¸§ä¹‹å¤–å»ºç«‹è”ç³»ï¼Œè¿™ä¸€æŒ‘æˆ˜è¿›ä¸€æ­¥åŠ å‰§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ beam æœç´¢ç­–ç•¥ï¼Œç”¨äºæ½œåœ¨ç©ºé—´æ¢ç´¢ï¼Œé€šè¿‡ beam æœç´¢è§£ç å®ç°æœ‰æ¡ä»¶ç”Ÿæˆå®Œæ•´çš„å›¾åƒåºåˆ—ã€‚ä¸ä¹‹å‰ä½¿ç”¨å›ºå®šæ½œåœ¨å…ˆéªŒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€æœç´¢æœ€ä¼˜æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œç¡®ä¿è¿è´¯çš„è§†è§‰è¿‡æ¸¡ã€‚åœ¨æ½œåœ¨å»å™ªç©ºé—´æ¢ç´¢è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¿®å‰ª beam æœç´¢å›¾ï¼Œè¯¥æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°å¯¹æœç´¢è·¯å¾„è¿›è¡Œè¯„åˆ†ï¼Œä¼˜å…ˆä¸æ–‡æœ¬æç¤ºå’Œè§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€‚äººç±»å’Œè‡ªåŠ¨è¯„ä¼°ç»“æœè¯å®ï¼ŒBeamDiffusion ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œç”Ÿæˆçš„å…¨åºåˆ—å…·æœ‰æ›´é«˜çš„è¿è´¯æ€§ã€è§†è§‰è¿è´¯æ€§å’Œæ–‡æœ¬å¯¹é½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20429v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹å…‰æŸæœç´¢ç­–ç•¥ï¼Œç”¨äºæ¢ç´¢æ½œåœ¨ç©ºé—´ï¼Œå¯å®ç°åŸºäºæ–‡æœ¬æç¤ºçš„æ¡ä»¶å›¾åƒåºåˆ—ç”Ÿæˆã€‚è¯¥ç­–ç•¥é€šè¿‡åŠ¨æ€æœç´¢æœ€ä¼˜æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œç¡®ä¿è§†è§‰è¿‡æ¸¡è¿è´¯æ€§ï¼Œå¹¶é€šè¿‡å¯¹æ½œåœ¨å»å™ªç©ºé—´çš„æ¢ç´¢è¿›è¡Œå…‰æŸæœç´¢å›¾çš„ä¿®å‰ªï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆè¯„åˆ†æœç´¢è·¯å¾„ï¼Œä¼˜å…ˆä¸æ–‡æœ¬æç¤ºå’Œè§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€‚è¯„ä¼°å’Œè‡ªåŠ¨è¯„ä¼°å‡è¯å®ï¼ŒBeamDiffusionç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆè¿è´¯æ€§ã€è§†è§‰è¿ç»­æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒåºåˆ—çš„è§†è§‰è¿è´¯æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç‹¬ç«‹ç”Ÿæˆæ¯å¼ å›¾åƒï¼Œå¯¼è‡´å™äº‹ä¸è¿è´¯ï¼Œéçº¿æ€§å™äº‹ä¸­æŒ‘æˆ˜æ›´å¤§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å…‰æŸæœç´¢ç­–ç•¥ï¼Œç”¨äºæ¢ç´¢æ½œåœ¨ç©ºé—´ï¼Œå®ç°æ¡ä»¶å›¾åƒåºåˆ—ç”Ÿæˆã€‚</li>
<li>è¯¥ç­–ç•¥é€šè¿‡åŠ¨æ€æœç´¢æœ€ä¼˜æ½œåœ¨è¡¨ç¤ºåºåˆ—ï¼Œç¡®ä¿è§†è§‰è¿‡æ¸¡è¿è´¯ã€‚</li>
<li>å…‰æŸæœç´¢å›¾çš„ä¿®å‰ªé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œï¼Œæœ‰æ•ˆè¯„åˆ†æœç´¢è·¯å¾„ï¼Œä¸æ–‡æœ¬å’Œè§†è§‰ä¸Šä¸‹æ–‡å¯¹é½ã€‚</li>
<li>BeamDiffusionç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œåœ¨ç”Ÿæˆå›¾åƒåºåˆ—çš„è¿è´¯æ€§ã€è§†è§‰è¿ç»­æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ba9ea7767e8670a3e24e211953de03f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ab5a973d84e924916dfdc31e10cc854.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e75fc3cbd48b1968e8d3844548ed728.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing"><a href="#Inference-Time-Scaling-for-Flow-Models-via-Stochastic-Generation-and-Rollover-Budget-Forcing" class="headerlink" title="Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing"></a>Inference-Time Scaling for Flow Models via Stochastic Generation and   Rollover Budget Forcing</h2><p><strong>Authors:Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung</strong></p>
<p>We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion modelsâ€“offering faster generation and high-quality outputs in state-of-the-art image and video generative modelsâ€“efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºé¢„è®­ç»ƒçš„æµæ¨¡å‹æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•ã€‚æœ€è¿‘ï¼Œæ¨ç†æ—¶é—´å°ºåº¦åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ä½¿è¾“å‡ºä¸ç”¨æˆ·åå¥½å¯¹é½ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œç”±äºä¸­é—´å»å™ªæ­¥éª¤çš„éšæœºæ€§ï¼Œç²’å­é‡‡æ ·å…è®¸æ›´æœ‰æ•ˆçš„å°ºåº¦æ‰©å±•ã€‚ç›¸åï¼Œè™½ç„¶æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£å“è€Œå¹¿å—æ¬¢è¿ï¼Œä¸ºæœ€å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦å’Œé«˜è´¨é‡è¾“å‡ºï¼Œä½†ç”¨äºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´å°ºåº¦æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºæµæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰ç¡®å®šçš„ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†å®ç°æµæ¨¡å‹çš„æ¨ç†æ—¶é—´æœ‰æ•ˆå°ºåº¦æ‰©å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®æƒ³æ³•ï¼š1ï¼‰åŸºäºSDEçš„ç”Ÿæˆï¼Œä½¿æµæ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç²’å­é‡‡æ ·ï¼›2ï¼‰æ’å€¼è½¬æ¢ï¼Œæ‰©å¤§æœç´¢ç©ºé—´å¹¶å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ï¼›3ï¼‰æ»šåŠ¨é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”åœ°åœ¨æ—¶é—´æ­¥é•¿ä¹‹é—´åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºSDEçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºæ–¹å·®ä¿ç•™ï¼ˆVPï¼‰æ’å€¼çš„ç”Ÿæˆï¼Œæ”¹è¿›äº†æµæ¨¡å‹æ¨ç†æ—¶é—´å°ºåº¦æ‰©å±•çš„ç²’å­é‡‡æ ·æ–¹æ³•çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä½¿ç”¨VP-SDEçš„RBFå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºä»¥å‰æ‰€æœ‰çš„æ¨ç†æ—¶é—´å°ºåº¦æ‰©å±•æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19385v4">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://flow-inference-time-scaling.github.io/">https://flow-inference-time-scaling.github.io/</a></p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é¢„è®­ç»ƒçš„æµæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´æ–¹æ³•ã€‚å¯¹äºæµæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºä¸‰ä¸ªå…³é”®æƒ³æ³•ï¼šåŸºäºSDEçš„ç”Ÿæˆã€æ’å€¼è½¬æ¢å’Œæ»šå‹é¢„ç®—å¼ºåˆ¶ï¼ˆRBFï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºVPæ’å€¼çš„SDEç”Ÿæˆæé«˜äº†æµæ¨¡å‹ä¸­æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´çš„ç²’å­é‡‡æ ·æ–¹æ³•çš„æ€§èƒ½ã€‚ç»“åˆVP-SDEçš„RBFå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¶…è¶Šäº†ä»¥å‰çš„æ‰€æœ‰æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹çš„æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´æ–¹æ³•è¢«æå‡ºï¼Œæ”¹è¿›äº†æ ·æœ¬è´¨é‡ï¼Œå¹¶æ›´å¥½åœ°å¯¹é½äº†ç”¨æˆ·çš„è¾“å‡ºåå¥½ã€‚</li>
<li>æµæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶é—´å°ºåº¦è°ƒæ•´æ–¹é¢å­˜åœ¨åŒºåˆ«ï¼Œå› ä¸ºæµæ¨¡å‹å…·æœ‰ç¡®å®šçš„ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å¯¹äºæµæ¨¡å‹ï¼Œæå‡ºäº†ä¸‰ä¸ªå…³é”®æƒ³æ³•ï¼šåŸºäºSDEçš„ç”Ÿæˆã€æ’å€¼è½¬æ¢å’ŒRBFã€‚</li>
<li>åŸºäºSDEçš„ç”Ÿæˆæ–¹æ³•å…è®¸åœ¨æµæ¨¡å‹ä¸­è¿›è¡Œç²’å­é‡‡æ ·ã€‚</li>
<li>æ’å€¼è½¬æ¢å¯ä»¥æ‰©å¤§æœç´¢ç©ºé—´å¹¶æé«˜æ ·æœ¬å¤šæ ·æ€§ã€‚</li>
<li>RBFèƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥æœ€å¤§åŒ–é¢„ç®—åˆ©ç”¨ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24687caf3e80bcfc87acbc19d7f89fec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ab56c662782ce854fe7e239918aad6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CHATS-Combining-Human-Aligned-Optimization-and-Test-Time-Sampling-for-Text-to-Image-Generation"><a href="#CHATS-Combining-Human-Aligned-Optimization-and-Test-Time-Sampling-for-Text-to-Image-Generation" class="headerlink" title="CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for   Text-to-Image Generation"></a>CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for   Text-to-Image Generation</h2><p><strong>Authors:Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>Diffusion models have emerged as a dominant approach for text-to-image generation. Key components such as the human preference alignment and classifier-free guidance play a crucial role in ensuring generation quality. However, their independent application in current text-to-image models continues to face significant challenges in achieving strong text-image alignment, high generation quality, and consistency with human aesthetic standards. In this work, we for the first time, explore facilitating the collaboration of human performance alignment and test-time sampling to unlock the potential of text-to-image models. Consequently, we introduce CHATS (Combining Human-Aligned optimization and Test-time Sampling), a novel generative framework that separately models the preferred and dispreferred distributions and employs a proxy-prompt-based sampling strategy to utilize the useful information contained in both distributions. We observe that CHATS exhibits exceptional data efficiency, achieving strong performance with only a small, high-quality funetuning dataset. Extensive experiments demonstrate that CHATS surpasses traditional preference alignment methods, setting new state-of-the-art across various standard benchmarks. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»å¯¼æ–¹æ³•ã€‚å…³é”®ç»„ä»¶ï¼Œå¦‚äººç±»åå¥½å¯¹é½å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œåœ¨ç¡®ä¿ç”Ÿæˆè´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å½“å‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„ç‹¬ç«‹åº”ç”¨ï¼Œåœ¨å®ç°å¼ºå¤§çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€é«˜ç”Ÿæˆè´¨é‡ä»¥åŠä¸äººç±»å®¡ç¾æ ‡å‡†çš„ä¸€è‡´æ€§æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†äººç±»æ€§èƒ½å¯¹é½å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·çš„åä½œï¼Œä»¥è§£é”æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CHATSï¼ˆç»“åˆäººç±»å¯¹é½ä¼˜åŒ–å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒåˆ†åˆ«å»ºæ¨¡é¦–é€‰å’Œä¸å—æ¬¢è¿çš„åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨åŸºäºä»£ç†æç¤ºçš„é‡‡æ ·ç­–ç•¥ï¼Œåˆ©ç”¨è¿™ä¸¤ä¸ªåˆ†å¸ƒä¸­åŒ…å«çš„æœ‰ç”¨ä¿¡æ¯ã€‚æˆ‘ä»¬å‘ç°CHATSè¡¨ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªå°è€Œé«˜è´¨é‡å¾®è°ƒæ•°æ®é›†å°±èƒ½å®ç°å¼ºåŠ²è¡¨ç°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCHATSè¶…è¶Šäº†ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•ï¼Œåœ¨å¤šç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12579v2">PDF</a> ICML 2025. The code is publicly available at   <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/CHATS">https://github.com/AIDC-AI/CHATS</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ä¸»å¯¼åœ°ä½ï¼Œå¹¶æŒ‡å‡ºå…³é”®ç»„ä»¶å¦‚äººç±»åå¥½å¯¹é½å’Œåˆ†ç±»å™¨å…è´¹æŒ‡å¯¼åœ¨ç¡®ä¿ç”Ÿæˆè´¨é‡æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„ç‹¬ç«‹åº”ç”¨ä»ç„¶é¢ä¸´ç€å®ç°å¼ºå¤§çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€é«˜ç”Ÿæˆè´¨é‡å’Œç¬¦åˆäººç±»å®¡ç¾æ ‡å‡†çš„ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†äººç±»æ€§èƒ½å¯¹é½å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·çš„åä½œæ½œåŠ›ï¼Œå¹¶å¼•å…¥äº†CHATSï¼ˆç»“åˆäººç±»å¯¹é½ä¼˜åŒ–å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·ï¼‰è¿™ä¸€æ–°å‹ç”Ÿæˆæ¡†æ¶ã€‚å®ƒåˆ†åˆ«å»ºæ¨¡é¦–é€‰å’Œä¸å—æ¬¢è¿çš„åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨åŸºäºä»£ç†æç¤ºçš„é‡‡æ ·ç­–ç•¥æ¥åˆ©ç”¨è¿™ä¸¤ä¸ªåˆ†å¸ƒä¸­åŒ…å«çš„æœ‰ç”¨ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒCHATSå…·æœ‰å‡ºè‰²çš„æ•°æ®æ•ˆç‡ï¼Œä»…åœ¨å°å‹é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†ä¸Šå°±èƒ½å®ç°å‡ºè‰²æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•ï¼Œæ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»æµæ–¹æ³•ã€‚</li>
<li>æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç‹¬ç«‹åº”ç”¨é¢ä¸´æ–‡æœ¬å›¾åƒå¯¹é½ã€ç”Ÿæˆè´¨é‡å’Œç¬¦åˆäººç±»å®¡ç¾æ ‡å‡†ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>CHATSæ¡†æ¶é¦–æ¬¡æ¢ç´¢äº†äººç±»æ€§èƒ½å¯¹é½å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·çš„åä½œæ½œåŠ›ã€‚</li>
<li>CHATSé€šè¿‡åˆ†åˆ«å»ºæ¨¡é¦–é€‰å’Œä¸å—æ¬¢è¿çš„åˆ†å¸ƒå¹¶é‡‡ç”¨åŸºäºä»£ç†æç¤ºçš„é‡‡æ ·ç­–ç•¥æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>CHATSå±•ç°å‡ºå‡ºè‰²çš„æ•°æ®æ•ˆç‡ï¼Œä»…åœ¨å°å‹é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†ä¸Šå³å¯å®ç°å¼ºæ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜CHATSåœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf6e6f00efc7310a281589845c126b18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eae5c93b37b21e8b59b7ba989e49f3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e50f4898ea6ffb53653f528c87599146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d00924e448cf737de89d36229720f92c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-384e35f008881080ac4f07969a28155e.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Chest Disease Detection In X-Ray Images Using Deep Learning   Classification Method
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d4025cd89abbd1301a506eaec750b6f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade   Depth Loss
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
