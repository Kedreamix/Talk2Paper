<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-90dbd6d15ddbc7f55a8b0f4b2a726a0e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="Developing-a-Top-tier-Framework-in-Naturalistic-Conditions-Challenge-for-Categorized-Emotion-Prediction-From-Speech-Foundation-Models-and-Learning-Objective-to-Data-Augmentation-and-Engineering-Choices"><a href="#Developing-a-Top-tier-Framework-in-Naturalistic-Conditions-Challenge-for-Categorized-Emotion-Prediction-From-Speech-Foundation-Models-and-Learning-Objective-to-Data-Augmentation-and-Engineering-Choices" class="headerlink" title="Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices"></a>Developing a Top-tier Framework in Naturalistic Conditions Challenge for   Categorized Emotion Prediction: From Speech Foundation Models and Learning   Objective to Data Augmentation and Engineering Choices</h2><p><strong>Authors:Tiantian Feng, Thanathai Lertpetchpun, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>Speech emotion recognition (SER), particularly for naturally expressed emotions, remains a challenging computational task. Key challenges include the inherent subjectivity in emotion annotation and the imbalanced distribution of emotion labels in datasets. This paper introduces the \texttt{SAILER} system developed for participation in the INTERSPEECH 2025 Emotion Recognition Challenge (Task 1). The challenge dataset, which contains natural emotional speech from podcasts, serves as a valuable resource for studying imbalanced and subjective emotion annotations. Our system is designed to be simple, reproducible, and effective, highlighting critical choices in modeling, learning objectives, data augmentation, and engineering choices. Results show that even a single system (without ensembling) can outperform more than 95% of the submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of three systems further improves performance, achieving a competitively ranked score (top-3 performing team). Our model is at: <a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release">https://github.com/tiantiaf0627/vox-profile-release</a>. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ï¼Œå°¤å…¶æ˜¯è‡ªç„¶è¡¨è¾¾æƒ…æ„Ÿçš„è¯†åˆ«ï¼Œä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—ä»»åŠ¡ã€‚ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æƒ…æ„Ÿæ³¨é‡Šä¸­çš„å›ºæœ‰ä¸»è§‚æ€§å’Œæ•°æ®é›†ä¸­æƒ…æ„Ÿæ ‡ç­¾çš„ä¸å¹³è¡¡åˆ†å¸ƒã€‚æœ¬æ–‡ä»‹ç»äº†ä¸ºå‚ä¸INTERSPEECH 2025æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›ï¼ˆä»»åŠ¡1ï¼‰è€Œå¼€å‘çš„&#96;SAILERâ€™ç³»ç»Ÿã€‚æŒ‘æˆ˜èµ›æ•°æ®é›†åŒ…å«æ¥è‡ªæ’­å®¢çš„è‡ªç„¶æƒ…æ„Ÿè¯­éŸ³ï¼Œæ˜¯ç ”ç©¶ä¸å¹³è¡¡å’Œä¸»è§‚æƒ…æ„Ÿæ³¨é‡Šçš„å®è´µèµ„æºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè®¾è®¡ç®€å•ã€å¯å¤åˆ¶ã€æœ‰æ•ˆï¼Œå¼ºè°ƒå»ºæ¨¡ã€å­¦ä¹ ç›®æ ‡ã€æ•°æ®å¢å¼ºå’Œå·¥ç¨‹é€‰æ‹©ä¸­çš„å…³é”®é€‰æ‹©ã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ä¸€ä¸ªå•ä¸€çš„ç³»ç»Ÿï¼ˆä¸ç»„åˆï¼‰ä¹Ÿèƒ½è¡¨ç°å‡ºè¶…è¿‡95%æäº¤çš„æ€§èƒ½ï¼Œå®F1åˆ†æ•°è¶…è¿‡0.4ã€‚æ­¤å¤–ï¼Œä¸‰ä¸ªç³»ç»Ÿçš„ç»„åˆè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå®ç°äº†ç«äº‰æ’åï¼ˆå‰ä¸‰åé˜Ÿä¼ï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/vox-profile-release">https://github.com/tiantiaf0627/vox-profile-release</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22133v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æƒ…ç»ªæ ‡æ³¨çš„ä¸»è§‚æ€§å’Œæ•°æ®é›†æƒ…æ„Ÿæ ‡ç­¾åˆ†å¸ƒä¸å‡è¡¡ç­‰é—®é¢˜ï¼Œå¼•å…¥äº†ä¸ºå‚ä¸INTERSPEECH 2025æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›ï¼ˆä»»åŠ¡1ï¼‰è€Œå¼€å‘çš„<code>SAILER</code>ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ—¨åœ¨è®¾è®¡ç®€å•ã€å¯å¤åˆ¶å’Œé«˜æ•ˆï¼Œçªå‡ºå»ºæ¨¡ã€å­¦ä¹ ç›®æ ‡ã€æ•°æ®å¢å¼ºå’Œå·¥ç¨‹é€‰æ‹©çš„å…³é”®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯ä¸€ä¸ªå•ä¸€ç³»ç»Ÿä¹Ÿèƒ½è¶…è¶Šè¶…è¿‡95%çš„æäº¤ç»“æœï¼Œå®è§‚F1åˆ†æ•°è¶…è¿‡0.4ã€‚é€šè¿‡é›†æˆä¸‰ä¸ªç³»ç»Ÿï¼Œæ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼Œæˆä¸ºæ’åé å‰çš„ä¸‰æ”¯é˜Ÿä¼ä¹‹ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¾ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢ã€‚</li>
<li>æƒ…ç»ªæ ‡æ³¨çš„ä¸»è§‚æ€§å’Œæ•°æ®é›†æƒ…æ„Ÿæ ‡ç­¾åˆ†å¸ƒä¸å‡è¡¡æ˜¯SERçš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†<code>SAILER</code>ç³»ç»Ÿï¼Œä¸“ä¸ºINTERSPEECH 2025æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜èµ›è®¾è®¡ã€‚</li>
<li><code>SAILER</code>ç³»ç»Ÿçš„è®¾è®¡ç†å¿µæ˜¯ç®€å•ã€å¯å¤åˆ¶å’Œé«˜æ•ˆã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨å»ºæ¨¡ã€å­¦ä¹ ç›®æ ‡ã€æ•°æ®å¢å¼ºå’Œå·¥ç¨‹é€‰æ‹©æ–¹é¢åšå‡ºäº†å…³é”®æ€§å†³ç­–ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå•ä¸€ç³»ç»Ÿçš„æ€§èƒ½å·²è¶…è¶Šå¤§å¤šæ•°æäº¤çš„ç»“æœï¼Œå®è§‚F1åˆ†æ•°è¶…è¿‡0.4ã€‚</li>
<li>é€šè¿‡é›†æˆä¸‰ä¸ªç³»ç»Ÿï¼Œå–å¾—äº†ç«äº‰åŠ›çš„æ’åï¼Œæˆä¸ºå‰ä¸‰åé˜Ÿä¼ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d350d67069214959ed07ddd382eeb95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33138f2f85cfc98882eb1c1055f187d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50d9e86c044bd04e7b21bd934f595be5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-332e4df3ed207d3fa8994f0b1ff31299.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-873c7f87b75976e5ecd176c3c5e55f4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e4e22aad02f9f62d846dc4950bf40f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="On-the-fly-Routing-for-Zero-shot-MoE-Speaker-Adaptation-of-Speech-Foundation-Models-for-Dysarthric-Speech-Recognition"><a href="#On-the-fly-Routing-for-Zero-shot-MoE-Speaker-Adaptation-of-Speech-Foundation-Models-for-Dysarthric-Speech-Recognition" class="headerlink" title="On-the-fly Routing for Zero-shot MoE Speaker Adaptation of Speech   Foundation Models for Dysarthric Speech Recognition"></a>On-the-fly Routing for Zero-shot MoE Speaker Adaptation of Speech   Foundation Models for Dysarthric Speech Recognition</h2><p><strong>Authors:Shujie HU, Xurong Xie, Mengzhe Geng, Jiajun Deng, Huimeng Wang, Guinan Li, Chengxi Deng, Tianzi Wang, Mingyu Cui, Helen Meng, Xunying Liu</strong></p>
<p>This paper proposes a novel MoE-based speaker adaptation framework for foundation models based dysarthric speech recognition. This approach enables zero-shot adaptation and real-time processing while incorporating domain knowledge. Speech impairment severity and gender conditioned adapter experts are dynamically combined using on-the-fly predicted speaker-dependent routing parameters. KL-divergence is used to further enforce diversity among experts and their generalization to unseen speakers. Experimental results on the UASpeech corpus suggest that on-the-fly MoE-based adaptation produces statistically significant WER reductions of up to 1.34% absolute (6.36% relative) over the unadapted baseline HuBERT&#x2F;WavLM models. Consistent WER reductions of up to 2.55% absolute (11.44% relative) and RTF speedups of up to 7 times are obtained over batch-mode adaptation across varying speaker-level data quantities. The lowest published WER of 16.35% (46.77% on very low intelligibility) is obtained. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMoEçš„æ–°å‹è‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºåŸºäºåŸºç¡€æ¨¡å‹çš„å‘éŸ³éšœç¢è¯­éŸ³è¯†åˆ«ã€‚è¿™ç§æ–¹æ³•å®ç°äº†é›¶æ ·æœ¬è‡ªé€‚åº”å’Œå®æ—¶å¤„ç†ï¼ŒåŒæ—¶èå…¥äº†é¢†åŸŸçŸ¥è¯†ã€‚é€šè¿‡å®æ—¶é¢„æµ‹çš„è¯´è¯äººä¾èµ–è·¯ç”±å‚æ•°ï¼ŒåŠ¨æ€ç»“åˆäº†è¯­éŸ³éšœç¢ä¸¥é‡ç¨‹åº¦å’Œæ€§åˆ«æ¡ä»¶é€‚é…å™¨ä¸“å®¶ã€‚KLæ•£åº¦è¢«ç”¨æ¥è¿›ä¸€æ­¥å¼ºåŒ–ä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§ä»¥åŠå®ƒä»¬å¯¹æœªè§è¯´è¯äººçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨UASpeechè¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå®æ—¶MoEçš„è‡ªé€‚åº”ç›¸è¾ƒäºæœªé€‚åº”çš„åŸºçº¿HuBERT&#x2F;WavLMæ¨¡å‹ï¼Œäº§ç”Ÿäº†ç»å¯¹è¾¾1.34%ï¼ˆç›¸å¯¹å‡å°‘6.36%ï¼‰çš„æ˜¾è‘—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ã€‚ç›¸è¾ƒäºæ‰¹é‡æ¨¡å¼çš„è‡ªé€‚åº”ï¼Œåœ¨ä¸åŒè¯´è¯äººçº§åˆ«çš„æ•°æ®é‡ä¸Šï¼Œè·å¾—äº†æœ€é«˜è¾¾ç»å¯¹2.55%ï¼ˆç›¸å¯¹å‡å°‘11.44%ï¼‰çš„WERé™ä½å’Œæœ€é«˜è¾¾7å€çš„å®æ—¶å› å­ï¼ˆRTFï¼‰åŠ é€Ÿã€‚è·å¾—äº†æœ€ä½çš„å‘å¸ƒWERä¸º16.35%ï¼ˆæä½æ¸…æ™°åº¦ä¸Šä¸º46.77%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22072v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMoEï¼ˆMixture of Expertsï¼‰çš„è¯´è¯äººè‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºå¤„ç†åŸºäºåŸºç¡€æ¨¡å‹çš„å‘éŸ³éšœç¢è¯­éŸ³è¯†åˆ«ã€‚è¯¥æ–¹æ³•å®ç°äº†é›¶æ ·æœ¬è‡ªé€‚åº”å’Œå®æ—¶å¤„ç†ï¼ŒåŒæ—¶èå…¥äº†é¢†åŸŸçŸ¥è¯†ã€‚é€šè¿‡åŠ¨æ€ç»“åˆè¯­éŸ³éšœç¢ä¸¥é‡æ€§å’Œæ€§åˆ«è°ƒèŠ‚é€‚é…å™¨ä¸“å®¶ï¼Œå¹¶ä½¿ç”¨å³æ—¶é¢„æµ‹çš„è¯´è¯äººä¾èµ–è·¯ç”±å‚æ•°ï¼Œå®ç°äº†ä¸“å®¶ä¹‹é—´çš„å¤šæ ·æ€§å’Œå¯¹æœªè§è¯´è¯äººçš„æ³›åŒ–ã€‚åœ¨UASpeechè¯­æ–™åº“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³æ—¶MoEè‡ªé€‚åº”æ–¹æ³•è¾ƒæœªè‡ªé€‚åº”çš„HuBERT&#x2F;WavLMæ¨¡å‹åœ¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šæœ‰æ˜¾è‘—ä¸‹é™ï¼Œç›¸å¯¹é™ä½è¾¾6.36%ï¼Œç»å¯¹é™ä½è¾¾1.34%ã€‚åœ¨ä¸åŒè¯´è¯äººçº§åˆ«çš„æ•°æ®æ•°é‡ä¸‹ï¼Œä¸æ‰¹é‡æ¨¡å¼è‡ªé€‚åº”ç›¸æ¯”ï¼Œè·å¾—äº†æœ€å¤šè¾¾2.55%çš„ç»å¯¹WERé™ä½å’Œæœ€å¤šè¾¾7å€çš„å®æ—¶å¤„ç†é€Ÿåº¦æå‡ã€‚è·å¾—äº†è¿„ä»Šä¸ºæ­¢æœ€ä½çš„WERä¸º16.35%ï¼Œåœ¨æä½æ¸…æ™°åº¦ä¸Šçš„ç›¸å¯¹æ€§èƒ½ä¸º46.77%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMoEçš„è¯´è¯äººè‡ªé€‚åº”æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤„ç†å‘éŸ³éšœç¢çš„è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>è¯¥æ¡†æ¶å¯ä»¥å®ç°é›¶æ ·æœ¬è‡ªé€‚åº”å’Œå®æ—¶å¤„ç†ï¼Œå¹¶èå…¥é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡åŠ¨æ€ç»“åˆè¯­éŸ³éšœç¢ä¸¥é‡æ€§å’Œæ€§åˆ«è°ƒèŠ‚çš„é€‚é…å™¨ä¸“å®¶ï¼Œæé«˜äº†è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨KLæ•£åº¦å¼ºåŒ–äº†ä¸“å®¶é—´çš„å¤šæ ·æ€§ï¼Œå¹¶æé«˜äº†å¯¹æœªè§è¯´è¯äººçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨UASpeechè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥è‡ªé€‚åº”æ¡†æ¶åœ¨è¯é”™è¯¯ç‡ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ä¸æœªè‡ªé€‚åº”çš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶æä¾›äº†ç›¸å¯¹é™ä½çš„WERå’Œç»å¯¹é™ä½çš„WERå…·ä½“æ•°å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7a6755a18ca6438c96319516faf0604.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-352634edd41e5460f75a378de6a88d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef7f5fc60074ff5ba33eb60af86e397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a9762ac0ea5882e408cbd26c1bca74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7a38d9168d6f574c8501dd626e55470.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Weakly-Supervised-Data-Refinement-and-Flexible-Sequence-Compression-for-Efficient-Thai-LLM-based-ASR"><a href="#Weakly-Supervised-Data-Refinement-and-Flexible-Sequence-Compression-for-Efficient-Thai-LLM-based-ASR" class="headerlink" title="Weakly Supervised Data Refinement and Flexible Sequence Compression for   Efficient Thai LLM-based ASR"></a>Weakly Supervised Data Refinement and Flexible Sequence Compression for   Efficient Thai LLM-based ASR</h2><p><strong>Authors:Mingchen Shao, Xinfa Zhu, Chengyou Wang, Bingshen Mu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie</strong></p>
<p>Despite remarkable achievements, automatic speech recognition (ASR) in low-resource scenarios still faces two challenges: high-quality data scarcity and high computational demands. This paper proposes EThai-ASR, the first to apply large language models (LLMs) to Thai ASR and create an efficient LLM-based ASR system. EThai-ASR comprises a speech encoder, a connection module and a Thai LLM decoder. To address the data scarcity and obtain a powerful speech encoder, EThai-ASR introduces a self-evolving data refinement strategy to refine weak labels, yielding an enhanced speech encoder. Moreover, we propose a pluggable sequence compression module used in the connection module with three modes designed to reduce the sequence length, thus decreasing computational demands while maintaining decent performance. Extensive experiments demonstrate that EThai-ASR has achieved state-of-the-art accuracy in multiple datasets. We release our refined text transcripts to promote further research. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†åœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»ç„¶é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šé«˜è´¨é‡æ•°æ®çš„ç¨€ç¼ºå’Œé«˜è®¡ç®—éœ€æ±‚ã€‚æœ¬æ–‡é’ˆå¯¹æ³°è¯­ASRï¼Œé¦–æ¬¡åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºé«˜æ•ˆçš„LLM-based ASRç³»ç»Ÿï¼Œæå‡ºäº†EThai-ASRã€‚EThai-ASRåŒ…æ‹¬è¯­éŸ³ç¼–ç å™¨ã€è¿æ¥æ¨¡å—å’Œæ³°è¯­LLMè§£ç å™¨ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜å¹¶è·å¾—å¼ºå¤§çš„è¯­éŸ³ç¼–ç å™¨ï¼ŒEThai-ASRå¼•å…¥äº†ä¸€ç§è‡ªæˆ‘å®Œå–„çš„æ•°æ®ç²¾ç‚¼ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–å¼±æ ‡ç­¾ï¼Œä»è€Œäº§ç”Ÿå¢å¼ºçš„è¯­éŸ³ç¼–ç å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ’æ‹”çš„åºåˆ—å‹ç¼©æ¨¡å—ï¼Œç”¨äºè¿æ¥æ¨¡å—ä¸­ï¼Œè®¾è®¡äº†ä¸‰ç§æ¨¡å¼ä»¥å‡å°‘åºåˆ—é•¿åº¦ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½è®¡ç®—éœ€æ±‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEThai-ASRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬æ•´ç†è¿‡çš„æ–‡æœ¬è½¬å½•å†…å®¹ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22063v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¿™ç¯‡è®ºæ–‡é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„ä¸¤ä¸ªæŒ‘æˆ˜â€”â€”é«˜è´¨é‡æ•°æ®ç¨€ç¼ºå’Œè®¡ç®—éœ€æ±‚é«˜ï¼Œæå‡ºäº†EThai-ASRç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé¦–æ¬¡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºæ³°è¯­ASRï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„LLM-based ASRç³»ç»Ÿã€‚é€šè¿‡è‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç²¾ç‚¼ç­–ç•¥ï¼ŒEThai-ASRè§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶å¢å¼ºäº†è¯­éŸ³ç¼–ç å™¨çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªå¯æ’å…¥çš„åºåˆ—å‹ç¼©æ¨¡å—ï¼Œç”¨äºå‡å°‘åºåˆ—é•¿åº¦ï¼Œä»è€Œé™ä½è®¡ç®—éœ€æ±‚åŒæ—¶ä¿æŒä¸é”™çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒEThai-ASRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>EThai-ASRç³»ç»Ÿè§£å†³äº†ä½èµ„æºç¯å¢ƒä¸‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šé«˜è´¨é‡æ•°æ®ç¨€ç¼ºå’Œè®¡ç®—éœ€æ±‚é«˜ã€‚</li>
<li>EThai-ASRé¦–æ¬¡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºæ³°è¯­ASRã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘è¿›åŒ–çš„æ•°æ®ç²¾ç‚¼ç­–ç•¥ï¼ŒEThai-ASRå¢å¼ºäº†è¯­éŸ³ç¼–ç å™¨çš„æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå¯æ’å…¥çš„åºåˆ—å‹ç¼©æ¨¡å—ï¼Œç”¨äºå‡å°‘è®¡ç®—éœ€æ±‚åŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>EThai-ASRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ã€‚</li>
<li>è®ºæ–‡å…¬å¼€äº†ç²¾ç‚¼åçš„æ–‡æœ¬è½¬å½•ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90dbd6d15ddbc7f55a8b0f4b2a726a0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b085d4c207b962269fdcce7c9ed81ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b38bcd25801a7306efb837629cbe83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80825e6627f70ba81511ea2bc35b7d1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5a745f97e789f13a961cde31a0190a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection"><a href="#Analysis-and-Evaluation-of-Synthetic-Data-Generation-in-Speech-Dysfluency-Detection" class="headerlink" title="Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection"></a>Analysis and Evaluation of Synthetic Data Generation in Speech   Dysfluency Detection</h2><p><strong>Authors:Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli</strong></p>
<p>Speech dysfluency detection is crucial for clinical diagnosis and language assessment, but existing methods are limited by the scarcity of high-quality annotated data. Although recent advances in TTS model have enabled synthetic dysfluency generation, existing synthetic datasets suffer from unnatural prosody and limited contextual diversity. To address these limitations, we propose LLM-Dys â€“ the most comprehensive dysfluent speech corpus with LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency categories spanning both word and phoneme levels. Building upon this resource, we improve an end-to-end dysfluency detection framework. Experimental validation demonstrates state-of-the-art performance. All data, models, and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys">https://github.com/Berkeley-Speech-Group/LLM-Dys</a>. </p>
<blockquote>
<p>è¯­éŸ³æµç•…æ€§æ£€æµ‹å¯¹äºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è¯­è¨€è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•å—åˆ°é«˜è´¨é‡æ³¨é‡Šæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚å°½ç®¡æœ€è¿‘æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿå®ç°åˆæˆæµç•…æ€§ç”Ÿæˆï¼Œä½†ç°æœ‰åˆæˆæ•°æ®é›†å­˜åœ¨è¯­è°ƒä¸è‡ªç„¶å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-Dysâ€”â€”ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæµç•…æ€§æ¨¡æ‹Ÿçš„æœ€å…¨é¢çš„æµç•…æ€§è¯­éŸ³è¯­æ–™åº“ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è·¨è¶Šå•è¯å’ŒéŸ³ç´ çº§åˆ«çš„11ä¸ªæµç•…æ€§é—®é¢˜ç±»åˆ«ã€‚åŸºäºè¿™ä¸€èµ„æºï¼Œæˆ‘ä»¬æ”¹è¿›äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµç•…æ€§æ£€æµ‹æ¡†æ¶ã€‚å®éªŒéªŒè¯è¯æ˜äº†å…¶å¤„äºå‰æ²¿çš„æ€§èƒ½ã€‚æ‰€æœ‰æ•°æ®å’Œæ¨¡å‹éƒ½åœ¨<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dysä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22029v1">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³æµç•…æ€§æ£€æµ‹åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è¯­è¨€è¯„ä¼°ä¸­çš„é‡è¦æ€§ã€‚ç°æœ‰çš„æ–¹æ³•å—é™äºé«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚è™½ç„¶æœ€è¿‘çš„TTSæ¨¡å‹è¿›æ­¥èƒ½å¤Ÿå®ç°åˆæˆæµç•…æ€§ç”Ÿæˆï¼Œä½†ç°æœ‰åˆæˆæ•°æ®é›†å­˜åœ¨éŸµå¾‹ä¸è‡ªç„¶å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§æœ‰é™çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-Dysâ€”â€”æœ€å…¨é¢çš„æµç•…æ€§è¯­éŸ³è¯­æ–™åº“ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºæµç•…æ€§æ¨¡æ‹Ÿã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†è¯æ±‡å’ŒéŸ³ç´ ä¸¤ä¸ªå±‚é¢ä¸Šçš„11ç§æµç•…æ€§é—®é¢˜ç±»åˆ«ã€‚åŸºäºæ­¤èµ„æºï¼Œæˆ‘ä»¬æ”¹è¿›äº†ç«¯åˆ°ç«¯çš„æµç•…æ€§æ£€æµ‹æ¡†æ¶ã€‚å®éªŒéªŒè¯å±•ç¤ºäº†å…¶å¤„äºå‰æ²¿çš„æ€§èƒ½è¡¨ç°ã€‚æ‰€æœ‰ç›¸å…³æ•°æ®ã€æ¨¡å‹å’Œä»£ç å·²å¼€æºåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Berkeley-Speech-Group/LLM-Dys%E3%80%82">https://github.com/Berkeley-Speech-Group/LLM-Dysã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æµç•…æ€§æ£€æµ‹åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è¯­è¨€è¯„ä¼°ä¸­å…·æœ‰é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºé«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>TTSæ¨¡å‹çš„è¿›æ­¥èƒ½å¤Ÿå®ç°åˆæˆæµç•…æ€§ç”Ÿæˆï¼Œä½†å­˜åœ¨éŸµå¾‹ä¸è‡ªç„¶å’Œä¸Šä¸‹æ–‡å¤šæ ·æ€§æœ‰é™çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºLLM-Dysâ€”â€”æœ€å…¨é¢çš„æµç•…æ€§è¯­éŸ³è¯­æ–™åº“ï¼Œæ¶µç›–è¯æ±‡å’ŒéŸ³ç´ å±‚é¢ä¸Šçš„å¤šç§æµç•…æ€§é—®é¢˜ç±»åˆ«ã€‚</li>
<li>åˆ©ç”¨LLM-Dysæ”¹è¿›äº†ç«¯åˆ°ç«¯çš„æµç•…æ€§æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºè¯¥æ¡†æ¶å…·æœ‰å‰æ²¿æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-940820839a1fe5abc1d5ff4dd6770dc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f09ea430ad1b7c0b1ad598e4f7cab2fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b030629238cfa23ed174c5da4e1f06be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae4225d2805104617350cc554123266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4c856b39137d632c5e621835da585e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40f8c15c1bcb1f5c3aa3c4628cc2c9b9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Overlap-Adaptive-Hybrid-Speaker-Diarization-and-ASR-Aware-Observation-Addition-for-MISP-2025-Challenge"><a href="#Overlap-Adaptive-Hybrid-Speaker-Diarization-and-ASR-Aware-Observation-Addition-for-MISP-2025-Challenge" class="headerlink" title="Overlap-Adaptive Hybrid Speaker Diarization and ASR-Aware Observation   Addition for MISP 2025 Challenge"></a>Overlap-Adaptive Hybrid Speaker Diarization and ASR-Aware Observation   Addition for MISP 2025 Challenge</h2><p><strong>Authors:Shangkun Huang, Yuxuan Du, Jingwen Yang, Dejun Zhang, Xupeng Jia, Jing Deng, Jintao Kang, Rong Zheng</strong></p>
<p>This paper presents the system developed to address the MISP 2025 Challenge. For the diarization system, we proposed a hybrid approach combining a WavLM end-to-end segmentation method with a traditional multi-module clustering technique to adaptively select the appropriate model for handling varying degrees of overlapping speech. For the automatic speech recognition (ASR) system, we proposed an ASR-aware observation addition method that compensates for the performance limitations of Guided Source Separation (GSS) under low signal-to-noise ratio conditions. Finally, we integrated the speaker diarization and ASR systems in a cascaded architecture to address Track 3. Our system achieved character error rates (CER) of 9.48% on Track 2 and concatenated minimum permutation character error rate (cpCER) of 11.56% on Track 3, ultimately securing first place in both tracks and thereby demonstrating the effectiveness of the proposed methods in real-world meeting scenarios. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸ºåº”å¯¹MISP 2025æŒ‘æˆ˜è€Œå¼€å‘çš„ç³»ç»Ÿã€‚å¯¹äºè¯´è¯äººåˆ†ç±»ç³»ç»Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå°†WavLMç«¯åˆ°ç«¯åˆ†å‰²æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¤šæ¨¡å—èšç±»æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥è‡ªé€‚åº”åœ°é€‰æ‹©å¤„ç†ä¸åŒç¨‹åº¦é‡å è¯­éŸ³çš„åˆé€‚æ¨¡å‹ã€‚å¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ASRæ„ŸçŸ¥è§‚æµ‹å¢åŠ æ–¹æ³•ï¼Œä»¥å¼¥è¡¥åœ¨ä½å£°å™ªæ¯”æ¡ä»¶ä¸‹æŒ‡å¯¼æºåˆ†ç¦»ï¼ˆGSSï¼‰çš„æ€§èƒ½é™åˆ¶ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¯´è¯äººåˆ†ç±»å’ŒASRç³»ç»Ÿä»¥çº§è”æ¶æ„é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥è§£å†³ç¬¬3èµ›é“çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨èµ›é“2ä¸Šå–å¾—äº†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º9.48%ï¼Œèµ›é“3ä¸Šä¸²è”æœ€å°æ’åˆ—å­—ç¬¦é”™è¯¯ç‡ï¼ˆcpCERï¼‰ä¸º11.56%ï¼Œæœ€ç»ˆåœ¨ä¸¤é“èµ›é“ä¸Šå‡è·å¾—ç¬¬ä¸€åï¼Œä»è€Œè¯æ˜äº†æ‰€ææ–¹æ³•åœ¨çœŸå®ä¼šè®®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22013v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸ºåº”å¯¹MISP 2025æŒ‘æˆ˜è€Œå¼€å‘çš„ç³»ç»Ÿã€‚æ–‡ä¸­æå‡ºäº†ç»“åˆWavLMç«¯åˆ°ç«¯åˆ†å‰²æ–¹æ³•å’Œä¼ ç»Ÿå¤šæ¨¡å—èšç±»æŠ€æœ¯çš„æ··åˆæ–¹æ³•æ¥é€‚åº”ä¸åŒé‡å è¯­éŸ³ç¨‹åº¦çš„è¯´è¯äººè¯†åˆ«ã€‚é’ˆå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œæå‡ºäº†ASRæ„ŸçŸ¥è§‚æµ‹å¢åŠ æ–¹æ³•ï¼Œä»¥å¼¥è¡¥åœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹å¼•å¯¼æºåˆ†ç¦»ï¼ˆGSSï¼‰çš„æ€§èƒ½é™åˆ¶ã€‚æœ€åï¼Œå°†è¯´è¯äººè¯†åˆ«å’ŒASRç³»ç»Ÿä»¥çº§è”æ¶æ„æ•´åˆåœ¨ä¸€èµ·åº”å¯¹ç¬¬ä¸‰è½¨é“æŒ‘æˆ˜ã€‚ç³»ç»Ÿåœ¨ç¬¬äºŒè½¨é“ä¸Šå®ç°äº†äººç‰©é”™è¯¯è¯†åˆ«ç‡ä¸º9.48%ï¼Œåœ¨ç¬¬ä¸‰è½¨é“ä¸Šå®ç°äº†æœ€å°æ’åˆ—äººç‰©é”™è¯¯è¯†åˆ«ç‡ä¸º11.56%ï¼Œå¹¶è·å¾—äº†åŒè½¨é“çš„ç¬¬ä¸€åï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨çœŸå®ä¼šè®®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é‡‡ç”¨æ··åˆæ–¹æ³•åº”å¯¹è¯´è¯äººè¯†åˆ«æŒ‘æˆ˜ï¼Œç»“åˆWavLMç«¯åˆ°ç«¯åˆ†å‰²æ–¹æ³•å’Œä¼ ç»Ÿå¤šæ¨¡å—èšç±»æŠ€æœ¯ä»¥é€‚åº”ä¸åŒè¯­éŸ³é‡å ç¨‹åº¦ã€‚</li>
<li>é’ˆå¯¹ASRç³»ç»Ÿæå‡ºäº†ASRæ„ŸçŸ¥è§‚æµ‹å¢åŠ æ–¹æ³•ï¼Œä»¥æ”¹å–„ä½ä¿¡å™ªæ¯”ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡çº§è”æ¶æ„æ•´åˆè¯´è¯äººè¯†åˆ«å’ŒASRç³»ç»Ÿï¼Œåº”å¯¹ç¬¬ä¸‰è½¨é“çš„æŒ‘æˆ˜ã€‚</li>
<li>ç³»ç»Ÿåœ¨ç¬¬äºŒå’Œç¬¬ä¸‰è½¨é“çš„æ¯”èµ›ä¸­å‡å–å¾—äº†ç¬¬ä¸€åçš„æˆç»©ã€‚</li>
<li>æå‡ºäº†æœ‰æ•ˆçš„åº”å¯¹ç­–ç•¥ï¼Œé€‚ç”¨äºçœŸå®ä¼šè®®åœºæ™¯ã€‚</li>
<li>ç³»ç»Ÿçš„æ€§èƒ½é€šè¿‡äººç‰©é”™è¯¯è¯†åˆ«ç‡æ¥è¡¡é‡ï¼Œå…¶ä¸­ç¬¬äºŒè½¨é“çš„äººç‰©é”™è¯¯è¯†åˆ«ç‡ä¸º9.48%ï¼Œç¬¬ä¸‰è½¨é“çš„æœ€å°æ’åˆ—äººç‰©é”™è¯¯è¯†åˆ«ç‡ä¸º11.56%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-118c39230c4d07fcf9561c56b8c7e1a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-302a2255147b8fa4d856f6e6b29779de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be079f0e52556a1059a3756051bb5555.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58294cf9692ba4ab16f8fb4b666cd8a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dc65e73c4e34597152e8ef2759207ed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions"><a href="#OmniResponse-Online-Multimodal-Conversational-Response-Generation-in-Dyadic-Interactions" class="headerlink" title="OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions"></a>OmniResponse: Online Multimodal Conversational Response Generation in   Dyadic Interactions</h2><p><strong>Authors:Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem</strong></p>
<p>In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speakerâ€™s multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå®ƒçš„ç›®æ ‡æ˜¯æ ¹æ®è¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œåœ¨çº¿ç”ŸæˆåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚OMCRGåæ˜ äº†è‡ªç„¶çš„äºŒå…ƒäº¤äº’ï¼Œå¹¶å®ç°åœ¨ç”Ÿæˆçš„éŸ³é¢‘å’Œå¬ä¼—çš„é¢éƒ¨å“åº”ä¹‹é—´çš„åŒæ­¥æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°å¼•å…¥æ–‡æœ¬ä½œä¸ºä¸­é—´æ¨¡æ€ï¼Œä»¥æ¡¥æ¥éŸ³é¢‘å’Œé¢éƒ¨å“åº”ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†OmniResponseï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¯ä»¥è‡ªå›å½’åœ°ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚OmniResponseåˆ©ç”¨äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¢å¼ºäº†ä¸¤ä¸ªæ–°ç»„ä»¶ï¼šChrono-Textï¼Œå®ƒä¸´æ—¶é”šå®šç”Ÿæˆçš„æ–‡æœ¬æ ‡è®°ï¼›ä»¥åŠTempoVoiceï¼Œä¸€ä¸ªå¯æ§çš„åœ¨çº¿æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å—ï¼Œèƒ½å¤Ÿäº§ç”Ÿä¸é¢éƒ¨ååº”åŒæ­¥çš„è¯­éŸ³ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥çš„OMCRGç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ResponseNetï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«696ä¸ªé«˜è´¨é‡çš„äºŒå…ƒäº¤äº’ç‰¹å¾ï¼Œå…·æœ‰åŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡å­—è®°å½•å’Œé¢éƒ¨è¡Œä¸ºæ³¨é‡Šã€‚åœ¨ResponseNetä¸Šè¿›è¡Œçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21724v1">PDF</a> 23 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨æ ¹æ®è¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥ï¼Œåœ¨çº¿ç”ŸæˆåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­å¬ä¼—åé¦ˆã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œå¼•å…¥æ–‡æœ¬ä½œä¸ºä¸­é—´æ¨¡æ€ï¼Œæå‡ºOmniResponseå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚OmniResponseåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å¢åŠ ä¸¤ä¸ªæ–°ç»„ä»¶ï¼šChrono-Textå’ŒTempoVoiceï¼Œåˆ†åˆ«å®ç°æ–‡æœ¬ä»¤ç‰Œçš„æ—¶é—´é”šå®šå’Œä¸é¢éƒ¨ååº”åŒæ­¥çš„åœ¨çº¿è¯­éŸ³åˆæˆã€‚ä¸ºäº†æ”¯æŒOMCRGç ”ç©¶ï¼Œæ¨å‡ºResponseNetæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„åŒå‘äº’åŠ¨åŒæ­¥åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡æœ¬è®°å½•å’Œé¢éƒ¨è¡Œä¸ºæ³¨é‡Šã€‚è¯„ä¼°è¡¨æ˜ï¼ŒOmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†åœ¨çº¿å¤šæ¨¡æ€å¯¹è¯å“åº”ç”Ÿæˆï¼ˆOMCRGï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è¯´è¯è€…çš„å¤šæ¨¡æ€è¾“å…¥ç”ŸæˆåŒæ­¥çš„è¨€è¯­å’Œéè¨€è¯­åé¦ˆã€‚</li>
<li>OMCRGåæ˜ äº†è‡ªç„¶çš„åŒå‘äº’åŠ¨ï¼Œå¹¶å¸¦æ¥äº†ç”ŸæˆéŸ³é¢‘å’Œé¢éƒ¨å“åº”åŒæ­¥çš„æ–°æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºOmniResponseå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡å¼•å…¥æ–‡æœ¬ä½œä¸ºä¸­é—´æ¨¡æ€æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€å¬ä¼—å“åº”ã€‚</li>
<li>OmniResponseåŒ…å«ä¸¤ä¸ªæ–°ç»„ä»¶ï¼šChrono-Textå’ŒTempoVoiceï¼Œåˆ†åˆ«å®ç°æ–‡æœ¬çš„æ—¶é—´é”šå®šå’Œä¸é¢éƒ¨ååº”åŒæ­¥çš„è¯­éŸ³åˆæˆã€‚</li>
<li>ä¸ºäº†æ”¯æŒOMCRGç ”ç©¶ï¼Œæ¨å‡ºäº†ResponseNetæ•°æ®é›†ï¼ŒåŒ…å«åŒæ­¥çš„åˆ†å±è§†é¢‘ã€å¤šé€šé“éŸ³é¢‘ã€æ–‡æœ¬è®°å½•å’Œé¢éƒ¨è¡Œä¸ºæ³¨é‡Šã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒOmniResponseåœ¨è¯­ä¹‰è¯­éŸ³å†…å®¹ã€è§†å¬åŒæ­¥å’Œç”Ÿæˆè´¨é‡æ–¹é¢è¶…è¶ŠåŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ee89439a08a800f7f2bc627e7c2eed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2f950db0761e9334fc4539985ab21a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f00d4b0043b1fde1c4f569f3780c9ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e3716546e46310ae6969f8875cfb7b4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="In-context-Language-Learning-for-Endangered-Languages-in-Speech-Recognition"><a href="#In-context-Language-Learning-for-Endangered-Languages-in-Speech-Recognition" class="headerlink" title="In-context Language Learning for Endangered Languages in Speech   Recognition"></a>In-context Language Learning for Endangered Languages in Speech   Recognition</h2><p><strong>Authors:Zhaolin Li, Jan Niehues</strong></p>
<p>With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. </p>
<blockquote>
<p>å…¨ä¸–ç•Œå¤§çº¦æœ‰7000ç§è¯­è¨€ï¼Œè€Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åªæ”¯æŒä¸€å°éƒ¨åˆ†ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMå¯ä»¥åœ¨æ²¡æœ‰ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ä¸ºæŸäº›ä»»åŠ¡å­¦ä¹ æ–°çš„è¯­è¨€ã€‚æˆ‘ä»¬å°†å…¶è°ƒæŸ¥æ‰©å±•åˆ°è¯­éŸ³è¯†åˆ«ï¼Œç ”ç©¶LLMæ˜¯å¦å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥å­¦ä¹ æœªè§è¿‡çš„ä½èµ„æºè¯­è¨€ã€‚æˆ‘ä»¬åœ¨å››ç§LLMæœªç»è®­ç»ƒçš„æ¿’å±è¯­è¨€ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå‘ç°æä¾›ç›¸å…³çš„æ–‡æœ¬æ ·æœ¬æœ‰åŠ©äºå¢å¼ºè¯­è¨€å»ºæ¨¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜åŸºäºæ¦‚ç‡çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„åŸºäºæŒ‡ä»¤çš„å­¦ä¹ æ–¹æ³•åœ¨è¯­è¨€å­¦ä¹ æ–¹é¢ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†ICLä½¿LLMèƒ½å¤Ÿå®ç°çš„ASRæ€§èƒ½å¯ä¸æˆ–ç”šè‡³è¶…è¶Šé’ˆå¯¹è¿™äº›è¯­è¨€ä¸“é—¨è®­ç»ƒçš„ä¸“ç”¨è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™LLMçš„åŸå§‹åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20445v2">PDF</a> Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å­¦ä¹ æ–°è¯­è¨€ä»¥å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œæ— éœ€ç›‘ç£æ•°æ®ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†è¿™é¡¹æ¢ç©¶ï¼Œèšç„¦äºè¯­éŸ³è¯†åˆ«çš„é¢†åŸŸï¼Œæ¢è®¨LLMsæ˜¯å¦èƒ½é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å­¦ä¹ æœªè§è¿‡çš„ä½èµ„æºè¯­è¨€ã€‚åœ¨å››ç§ä¸åŒçš„æ¿’å±è¯­è¨€ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæä¾›ç›¸å…³çš„æ–‡æœ¬æ ·æœ¬èƒ½æé«˜è¯­è¨€å»ºæ¨¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæ¦‚ç‡æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„æŒ‡ä»¤æ–¹æ³•ç”¨äºè¯­è¨€å­¦ä¹ ã€‚åŒæ—¶ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ä½¿LLMsçš„ASRæ€§èƒ½ä¸é’ˆå¯¹è¿™äº›è¯­è¨€ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜ï¼ŒåŒæ—¶ä¿ç•™äº†LLMsçš„åŸå§‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ æ–°è¯­è¨€ä»¥å®Œæˆç‰¹å®šä»»åŠ¡ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å°†è¿™ä¸€å‘ç°åº”ç”¨äºè¯­éŸ³è¯†åˆ«çš„é¢†åŸŸã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼ŒLLMsèƒ½å¤Ÿå­¦ä¹ æœªè§è¿‡çš„ä½èµ„æºè¯­è¨€ã€‚</li>
<li>æä¾›ç›¸å…³æ–‡æœ¬æ ·æœ¬èƒ½å¤Ÿæé«˜è¯­è¨€å»ºæ¨¡å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è¡¨ç°ã€‚</li>
<li>åœ¨æ¦‚ç‡æ–¹æ³•çš„å¸®åŠ©ä¸‹ï¼ŒLLMsçš„è¯­è¨€å­¦ä¹ æ•ˆæœä¼˜äºä¼ ç»Ÿçš„æŒ‡ä»¤æ–¹æ³•ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ ä½¿LLMsçš„ASRæ€§èƒ½ä¸é’ˆå¯¹ç‰¹å®šè¯­è¨€è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77862324b9133baec235fab2ecc47977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a481ae22b6b4a7dbf577fac08bd0f10c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-524f4f6d3eecffdf8692b71393ebec5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-829fb446143474eb917e94b751fbab0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95834bfc7bc495782b9a4895d28942c9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models"><a href="#An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models" class="headerlink" title="An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models"></a>An Effective Training Framework for Light-Weight Automatic Speech   Recognition Models</h2><p><strong>Authors:Abdul Hannan, Alessio Brutti, Shah Nawaz, Mubashir Noman</strong></p>
<p>Recent advancement in deep learning encouraged developing large automatic speech recognition (ASR) models that achieve promising results while ignoring computational and memory constraints. However, deploying such models on low resource devices is impractical despite of their favorable performance. Existing approaches (pruning, distillation, layer skip etc.) transform the large models into smaller ones at the cost of significant performance degradation or require prolonged training of smaller models for better performance. To address these issues, we introduce an efficacious two-step representation learning based approach capable of producing several small sized models from a single large model ensuring considerably better performance in limited number of epochs. Comprehensive experimentation on ASR benchmarks reveals the efficacy of our approach, achieving three-fold training speed-up and up to 12.54% word error rate improvement. </p>
<blockquote>
<p>æœ€è¿‘æ·±åº¦å­¦ä¹ çš„å‘å±•æ¨åŠ¨äº†å¤§å‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å¼€å‘ï¼Œè¿™äº›æ¨¡å‹åœ¨å¿½ç•¥è®¡ç®—å’Œå†…å­˜é™åˆ¶çš„æƒ…å†µä¸‹å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šæ˜¯ä¸åˆ‡å®é™…çš„ï¼Œå°½ç®¡å…¶æ€§èƒ½è¡¨ç°è‰¯å¥½ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ä¿®å‰ªã€è’¸é¦ã€è·³è¿‡å±‚ç­‰ï¼‰é€šè¿‡å°†å¤§å‹æ¨¡å‹è½¬æ¢ä¸ºå°å‹æ¨¡å‹æ¥é™ä½æˆæœ¬ï¼Œä½†è¿™ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæˆ–è€…éœ€è¦å¯¹å°å‹æ¨¡å‹è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¸¤æ­¥è¡¨ç¤ºå­¦ä¹ æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»å•ä¸€çš„å¤§å‹æ¨¡å‹ä¸­ç”Ÿæˆå¤šä¸ªå°å‹æ¨¡å‹ï¼Œå¹¶åœ¨æœ‰é™çš„è¿­ä»£æ¬¡æ•°å†…ç¡®ä¿æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ASRåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡å’Œé«˜è¾¾12.54%çš„è¯é”™è¯¯ç‡æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16991v2">PDF</a> Accepted at InterSpeech 2025</p>
<p><strong>Summary</strong>ï¼š<br>æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†å¤§å‹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„å‘å±•ï¼Œè¿™äº›æ¨¡å‹åœ¨å¿½ç•¥è®¡ç®—å’Œå†…å­˜é™åˆ¶çš„æƒ…å†µä¸‹å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šå¹¶ä¸å®ç”¨ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚å‰ªæã€è’¸é¦ã€å±‚è·³è¿‡ç­‰ï¼‰å°†å¤§å‹æ¨¡å‹è½¬æ¢ä¸ºå°å‹æ¨¡å‹ï¼Œä½†ä»˜å‡ºäº†æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„ä»£ä»·ï¼Œæˆ–è€…éœ€è¦å»¶é•¿å°å‹æ¨¡å‹çš„è®­ç»ƒæ—¶é—´ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¸¤æ­¥è¡¨ç¤ºå­¦ä¹ æ³•ï¼Œèƒ½ä»å•ä¸€å¤§å‹æ¨¡å‹ä¸­ç”Ÿæˆå¤šä¸ªå°å‹æ¨¡å‹ï¼Œå¹¶åœ¨æœ‰é™çš„è®­ç»ƒå‘¨æœŸä¸­ç¡®ä¿æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ASRåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡å’Œæœ€é«˜è¾¾12.54%çš„è¯é”™è¯¯ç‡æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨åŠ¨å¤§å‹ASRæ¨¡å‹å‘å±•ï¼Œä½†èµ„æºæœ‰é™è®¾å¤‡çš„éƒ¨ç½²ä¸å®ç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è½¬æ¢å¤§å‹æ¨¡å‹ä¸ºå°å‹æ¨¡å‹æ—¶ï¼Œä¼šç‰ºç‰²æ€§èƒ½æˆ–éœ€è¦é•¿æ—¶é—´è®­ç»ƒã€‚</li>
<li>æå‡ºä¸€ç§æœ‰æ•ˆçš„ä¸¤æ­¥è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»å•ä¸€å¤§å‹æ¨¡å‹ç”Ÿæˆå¤šä¸ªå°å‹æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•åœ¨ASRåŸºå‡†æµ‹è¯•ä¸Šç»è¿‡ç»¼åˆå®éªŒéªŒè¯ï¼Œè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>æ–¹æ³•å®ç°äº†ä¸‰å€çš„è®­ç»ƒé€Ÿåº¦æå‡ã€‚</li>
<li>è¯é”™è¯¯ç‡æ”¹è¿›æœ€é«˜è¾¾12.54%ã€‚</li>
<li>æ­¤æ–¹æ³•ä¸ºè§£å†³å¤§å‹ASRæ¨¡å‹åœ¨èµ„æºæœ‰é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²é—®é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f18f9e7c813973a940935682adc014d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb91329d0e07f89231b6d115d64d0e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22cd0ed143f6d9d8295ee075f7ef8615.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79472fb0652ae372c2ba96b85f7fd9e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257c5646a5eef37e617df4c1e9dc4c86.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VoxEval-Benchmarking-the-Knowledge-Understanding-Capabilities-of-End-to-End-Spoken-Language-Models"><a href="#VoxEval-Benchmarking-the-Knowledge-Understanding-Capabilities-of-End-to-End-Spoken-Language-Models" class="headerlink" title="VoxEval: Benchmarking the Knowledge Understanding Capabilities of   End-to-End Spoken Language Models"></a>VoxEval: Benchmarking the Knowledge Understanding Capabilities of   End-to-End Spoken Language Models</h2><p><strong>Authors:Wenqian Cui, Xiaoqi Jiao, Ziqiao Meng, Irwin King</strong></p>
<p>With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMsâ€™ knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMsâ€™ knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/VoxEval">https://github.com/dreamtheater123/VoxEval</a> </p>
<blockquote>
<p>éšç€åŸºäºè¯­éŸ³çš„äº¤äº’æ¨¡å‹éœ€æ±‚çš„å¢é•¿ï¼Œç«¯åˆ°ç«¯çš„å£è¯­æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚è™½ç„¶è¿™äº›æ¨¡å‹éœ€è¦å…¨é¢çš„ä¸–ç•ŒçŸ¥è¯†æ¥è¿›è¡Œæœ‰æ„ä¹‰å’Œå¯é çš„äººç±»äº’åŠ¨ï¼Œä½†ç°æœ‰çš„é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£æ–¹é¢å´è¡¨ç°ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ”¯æŒç«¯åˆ°ç«¯çš„è¯­éŸ³è¯„ä¼°å¹¶è€ƒè™‘å„ç§è¾“å…¥éŸ³é¢‘æ¡ä»¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VoxEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯­éŸ³é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œå®ƒé€šè¿‡çº¯ç²¹çš„è¯­éŸ³äº’åŠ¨æ¥è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•1ï¼‰ç‹¬ç‰¹åœ°ä¿æŒè¯­éŸ³æ ¼å¼çš„è¾“å…¥å’Œè¾“å‡ºï¼Œ2ï¼‰è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œ3ï¼‰ç‡å…ˆè¯„ä¼°å£è¯­å½¢å¼çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚æ•°å­¦æ¨ç†ã€‚ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼ŒVoxEvalç»™å½“å‰çš„SLMå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬å¯¹å„ç§éŸ³é¢‘æ¡ä»¶çš„æ•æ„Ÿæ€§ï¼Œå¹¶å¼ºè°ƒæœªæ¥å¼€å‘æ—¶éœ€è¦å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªåŸºå‡†æµ‹è¯•èƒ½å¼•å¯¼æ›´ç²¾ç»†å’Œæ›´å¯é çš„SLMçš„å‘å±•ã€‚VoxEvalæ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/VoxEval%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dreamtheater123/VoxEvalè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04962v4">PDF</a> The Version of Record of this contribution is accepted to ACL 2025   main conference</p>
<p><strong>æ€»ç»“</strong><br>    éšç€åŸºäºè¯­éŸ³çš„äº¤äº’æ¨¡å‹çš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œç«¯åˆ°ç«¯çš„å£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰å·²æˆä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æ”¯æŒç«¯åˆ°ç«¯çš„è¯­éŸ³è¯„ä¼°å¹¶è€ƒè™‘å„ç§è¾“å…¥éŸ³é¢‘æ¡ä»¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VoxEvalï¼Œä¸€ä¸ªå…¨æ–°çš„SpeechQAåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡çº¯è¯­éŸ³äº¤äº’è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•ç‹¬ç‰¹ä¹‹å¤„åœ¨äº1ï¼‰ä¿æŒè¯­éŸ³è¾“å…¥è¾“å‡ºæ ¼å¼ï¼Œ2ï¼‰è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œ3ï¼‰ç‡å…ˆè¯„ä¼°å£è¯­å½¢å¼çš„å¤æ‚ä»»åŠ¡å¦‚æ•°å­¦æ¨ç†ã€‚ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼ŒVoxEvalå¯¹å½“å‰SLMæå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œçªæ˜¾äº†å…¶å¯¹ä¸åŒéŸ³é¢‘æ¡ä»¶çš„æ•æ„Ÿæ€§ï¼Œå¹¶å¼ºè°ƒæœªæ¥å‘å±•ä¸­å¢å¼ºæ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æœŸæœ›æ­¤åŸºå‡†æµ‹è¯•èƒ½æ¨åŠ¨æ›´å…ˆè¿›ã€æ›´å¯é çš„SLMçš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯çš„å£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨è¯­éŸ³äº¤äº’ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰é—®ç­”åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°SLMsçš„çŸ¥è¯†ç†è§£ã€‚</li>
<li>VoxEvalæ˜¯ä¸€ä¸ªæ–°çš„SpeechQAåŸºå‡†æµ‹è¯•ï¼Œä»¥çº¯è¯­éŸ³å½¢å¼è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ã€‚</li>
<li>VoxEvalä¿æŒè¯­éŸ³è¾“å…¥è¾“å‡ºæ ¼å¼ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>VoxEvalé¦–æ¬¡è¯„ä¼°å£è¯­å½¢å¼çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚æ•°å­¦æ¨ç†ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼ŒVoxEvalå¯¹å½“å‰SLMæå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e903fe42a9250c0c13410490716bcf9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-677e7b61ee6bf308ee6d9fefea9417c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5217712044257bea117168cd79c1f695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b714cd0061ba61046a429c33504341b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35d0581223e19567abcb9e6c603f3b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b8f3a83a7f96f40fe365b39c25947b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83b0c2bf2908447876cda96c1d11f10f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Align-SLM-Textless-Spoken-Language-Models-with-Reinforcement-Learning-from-AI-Feedback"><a href="#Align-SLM-Textless-Spoken-Language-Models-with-Reinforcement-Learning-from-AI-Feedback" class="headerlink" title="Align-SLM: Textless Spoken Language Models with Reinforcement Learning   from AI Feedback"></a>Align-SLM: Textless Spoken Language Models with Reinforcement Learning   from AI Feedback</h2><p><strong>Authors:Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko</strong></p>
<p>While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs. </p>
<blockquote>
<p>æ— æ–‡æœ¬Spoken Language Modelsï¼ˆSLMï¼‰åœ¨ç«¯åˆ°ç«¯è¯­éŸ³åˆ°è¯­éŸ³å»ºæ¨¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨è¯­ä¹‰è¿è´¯æ€§å’Œç›¸å…³æ€§æ–¹é¢ä»è½åäºåŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†Align-SLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å—å¼ºåŒ–å­¦ä¹ ä¸äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰å¯å‘çš„åå¥½ä¼˜åŒ–ï¼Œä»¥å¢å¼ºSLMçš„è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»ç»™å®šçš„æç¤ºç”Ÿæˆå¤šä¸ªè¯­éŸ³è¿ç»­ç‰‡æ®µï¼Œå¹¶ä½¿ç”¨è¯­ä¹‰åº¦é‡æ¥åˆ›å»ºç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„åå¥½æ•°æ®ã€‚æˆ‘ä»¬ä½¿ç”¨ZeroSpeech 2021è¯æ±‡å’Œå¥æ³•å»ºæ¨¡åŸºå‡†æµ‹è¯•ã€StoryClozeæ•°æ®é›†å£è¯­ç‰ˆå¯¹æ¡†æ¶è¿›è¡Œè¯­ä¹‰è¿è´¯æ€§è¯„ä¼°ä»¥åŠå…¶ä»–è¯­éŸ³ç”Ÿæˆåº¦é‡æ ‡å‡†ï¼ŒåŒ…æ‹¬GPT4-oå¾—åˆ†å’Œäººç±»è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†SLMçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œçªå‡ºäº†åå¥½ä¼˜åŒ–å¯¹æé«˜SLMè¯­ä¹‰çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01834v2">PDF</a> Accepted by ACL 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ— æ–‡æœ¬Spoken Language Modelsï¼ˆSLMsï¼‰åœ¨ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³å»ºæ¨¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨è¯­ä¹‰è¿è´¯æ€§å’Œç›¸å…³æ€§æ–¹é¢ä»è½åäºåŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Align-SLMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å—å¼ºåŒ–å­¦ä¹ ä¸äººå·¥æ™ºèƒ½åé¦ˆï¼ˆRLAIFï¼‰å¯å‘çš„åå¥½ä¼˜åŒ–ï¼Œä»¥æé«˜SLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆç»™å®šæç¤ºçš„å¤šä¸ªè¯­éŸ³è¿ç»­ç‰‡æ®µå¹¶ä½¿ç”¨è¯­ä¹‰åº¦é‡æ¥åˆ›å»ºç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰çš„åå¥½æ•°æ®ã€‚æˆ‘ä»¬åˆ©ç”¨ZeroSpeech 2021è¯æ±‡å’Œå¥æ³•å»ºæ¨¡åŸºå‡†æµ‹è¯•ã€å£è¯­ç‰ˆStoryClozeæ•°æ®é›†å¯¹æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬GPT4è¯„åˆ†å’Œäººç±»è¯„ä¼°åœ¨å†…çš„å…¶ä»–è¯­éŸ³ç”ŸæˆæŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†SLMçš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œçªæ˜¾äº†åå¥½ä¼˜åŒ–åœ¨æé«˜SLMè¯­ä¹‰æ–¹é¢çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ— æ–‡æœ¬Spoken Language Modelsï¼ˆSLMsï¼‰åœ¨ç«¯åˆ°ç«¯è¯­éŸ³å»ºæ¨¡ä¸­æœ‰æ½œåŠ›ï¼Œä½†è¯­ä¹‰è¿è´¯æ€§å’Œç›¸å…³æ€§æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>å¼•å…¥Align-SLMæ¡†æ¶ï¼Œé€šè¿‡åå¥½ä¼˜åŒ–æé«˜SLMçš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>ç”Ÿæˆå¤šä¸ªè¯­éŸ³è¿ç»­ç‰‡æ®µå¹¶ä½¿ç”¨è¯­ä¹‰åº¦é‡åˆ›å»ºåå¥½æ•°æ®ç”¨äºç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚</li>
<li>åˆ©ç”¨ZeroSpeech 2021åŸºå‡†æµ‹è¯•å’Œå…¶ä»–è¯­éŸ³ç”ŸæˆæŒ‡æ ‡å¯¹æ¡†æ¶è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æ–¹æ³•åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸Šå®ç°SLMçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>åå¥½ä¼˜åŒ–å¯¹æé«˜SLMè¯­ä¹‰çš„é‡è¦æ€§å¾—åˆ°å‡¸æ˜¾ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹è¿›SLMçš„è¯­ä¹‰æ€§èƒ½æä¾›äº†æ–°çš„æ–¹å‘å’Œç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-274d2eee64fb5608d263cb6d4bda404a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9eebf9e755b8be98cfc5c712b9d8425.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eef2d824d5d031e54b65b4f6cfd6f71a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eca2ebc39af141cc5885da11a0bb81b5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Faetar-Benchmark-Speech-Recognition-in-a-Very-Under-Resourced-Language"><a href="#The-Faetar-Benchmark-Speech-Recognition-in-a-Very-Under-Resourced-Language" class="headerlink" title="The Faetar Benchmark: Speech Recognition in a Very Under-Resourced   Language"></a>The Faetar Benchmark: Speech Recognition in a Very Under-Resourced   Language</h2><p><strong>Authors:Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar</strong></p>
<p>We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\c{c}al. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled speech. We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•è¯­æ–™åº“ï¼Œæ—¨åœ¨æ¨åŠ¨å½“å‰æ–¹æ³•åœ¨èµ„æºåŒ®ä¹è¯­éŸ³è¯†åˆ«æ–¹é¢çš„æé™ã€‚Faetaræ˜¯ä¸€ç§ä¸»è¦åœ¨æ„å¤§åˆ©ä½¿ç”¨çš„æ³•ç½—-æ™®ç½—æ—ºæ–¯æ–¹è¨€ï¼Œå®ƒæ²¡æœ‰æ ‡å‡†çš„æ­£å­—æ³•ï¼Œé™¤äº†åŸºå‡†æµ‹è¯•ä¸­åŒ…å«çš„å†…å®¹å¤–ï¼Œå‡ ä¹æ²¡æœ‰ç°æœ‰çš„æ–‡æœ¬æˆ–è¯­éŸ³èµ„æºï¼Œè€Œä¸”ä¸å…¶ä»–å½¢å¼çš„æ³•ç½—-æ™®ç½—æ—ºæ–¯æ–¹è¨€æœ‰å¾ˆå¤§ä¸åŒã€‚è¯¥è¯­æ–™åº“æ¥è‡ªç°åœºå½•éŸ³ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†å™ªéŸ³å¾ˆå¤§ï¼Œåªæœ‰5å°æ—¶çš„åŒ¹é…è½¬å½•ï¼Œå¹¶ä¸”å¼ºåˆ¶å¯¹é½çš„è´¨é‡å‚å·®ä¸é½ã€‚è¯­æ–™åº“è¿˜åŒ…å«é¢å¤–çš„20å°æ—¶æœªæ ‡æ³¨çš„è¯­éŸ³ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨æœ€å…ˆè¿›çš„è·¨è¯­è¨€è¯­éŸ³åŸºç¡€æ¨¡å‹çš„åŸºçº¿ç»“æœï¼Œæœ€ä½³è¯­éŸ³é”™è¯¯ç‡ä¸º3.04%ï¼Œä½¿ç”¨åœ¨åŸºç¡€æ¨¡å‹ä¸Šç»§ç»­ä½¿ç”¨æœªæ ‡æ³¨é›†è¿›è¡Œé¢„è®­ç»ƒçš„ç®¡é“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08103v4">PDF</a> To appear in INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“æ—¨åœ¨æ¨åŠ¨å½“å‰ä½èµ„æºè¯­éŸ³è¯†åˆ«æ–¹æ³•çš„å‰æ²¿ã€‚è¯¥è¯­æ–™åº“ä¸»è¦å…³æ³¨çš„æ˜¯ä¸€ç§åœ¨æ„å¤§åˆ©ä½¿ç”¨çš„è¢«ç§°ä¸ºFaetarçš„å¼—å…°ç§‘-æ™®ç½—æ—ºæ–¯æ–¹è¨€ï¼Œè¯¥æ–¹è¨€æ²¡æœ‰æ ‡å‡†çš„æ­£å­—æ³•ï¼Œå‡ ä¹æ²¡æœ‰ç°æœ‰çš„æ–‡æœ¬æˆ–è¯­éŸ³èµ„æºï¼Œå¹¶ä¸”ä¸å…¶ä»–å½¢å¼çš„å¼—å…°ç§‘-æ™®ç½—æ—ºæ–¯æ–¹è¨€æœ‰å¾ˆå¤§å·®å¼‚ã€‚è¯­æ–™åº“æ¥è‡ªç°åœºå½•éŸ³ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†å½•éŸ³ç¯å¢ƒå™ªéŸ³è¾ƒå¤§ï¼Œä»…æœ‰äº”å°æ—¶çš„å½•éŸ³æœ‰åŒ¹é…çš„è½¬å½•ï¼Œå¹¶ä¸”å¼ºåˆ¶å¯¹é½çš„è´¨é‡ä¸ä¸€ã€‚æ­¤å¤–ï¼Œè¯­æ–™åº“è¿˜åŒ…å«é¢å¤–çš„æœªæ ‡è®°çš„äºŒåå°æ—¶è¯­éŸ³ã€‚åˆ©ç”¨å¤šè¯­è¨€è¯­éŸ³åŸºå‡†æ¨¡å‹æŠ¥é“äº†æœ€ä½³å‘éŸ³è¯¯å·®ç‡ä¸ºç™¾åˆ†ä¹‹ä¸‰åçš„åˆæ­¥ç»“æœï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨æœªæ ‡è®°æ•°æ®é›†ç»§ç»­å¯¹åŸºå‡†æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒçš„æ–¹å¼ä¼˜åŒ–è¯†åˆ«æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŸºå‡†æµ‹è¯•è¯­æ–™åº“æ˜¯ä¸ºäº†æ¨åŠ¨ä½èµ„æºè¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„æé™è€Œè®¾è®¡çš„ã€‚</li>
<li>Faetaræ˜¯ä¸€ç§å‡ ä¹æ²¡æœ‰æ ‡å‡†æ­£å­—æ³•çš„å¼—å…°ç§‘-æ™®ç½—æ—ºæ–¯æ–¹è¨€ï¼Œä¸”ä¸å…¶ä»–å½¢å¼çš„å¼—å…°ç§‘-æ™®ç½—æ—ºæ–¯æ–¹è¨€å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>è¯¥è¯­æ–™åº“ä¸»è¦æ¥è‡ªç°åœºå½•éŸ³ï¼ŒåŒ…å«å¤§é‡çš„å™ªéŸ³ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†å¸¦æœ‰åŒ¹é…çš„è½¬å½•ã€‚</li>
<li>å¼ºåˆ¶å¯¹é½çš„è´¨é‡å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>é™¤äº†æœ‰è½¬å½•çš„è¯­éŸ³å¤–ï¼Œè¯­æ–™åº“è¿˜åŒ…å«é¢å¤–çš„æœªæ ‡è®°è¯­éŸ³æ•°æ®ã€‚</li>
<li>ç›®å‰çš„å¤šè¯­è¨€è¯­éŸ³åŸºå‡†æ¨¡å‹åœ¨Faetarè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸Šçš„æœ€ä½³å‘éŸ³è¯¯å·®ç‡ä¸ºç™¾åˆ†ä¹‹ä¸‰åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08103">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c52678075949cd296957c0835bf4e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5a4e4d492f12dad7a6f06680fc0d4b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11d84f054810116c1620e9069989e651.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c969f1e145ecc94c1e0972bae971b944.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VQ-CTAP-Cross-Modal-Fine-Grained-Sequence-Representation-Learning-for-Speech-Processing"><a href="#VQ-CTAP-Cross-Modal-Fine-Grained-Sequence-Representation-Learning-for-Speech-Processing" class="headerlink" title="VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for   Speech Processing"></a>VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for   Speech Processing</h2><p><strong>Authors:Chunyu Qiang, Wang Geng, Yi Zhao, Ruibo Fu, Tao Wang, Cheng Gong, Tianrui Wang, Qiuyu Liu, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Hao Che, Longbiao Wang, Jianwu Dang, Jianhua Tao</strong></p>
<p>Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the speech modality. We propose a method called â€œVector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)â€, which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, learning how to connect text and speech at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and recognition tasks in speech processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression speech coding at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. The audio demo is available at <a target="_blank" rel="noopener" href="https://qiangchunyu.github.io/VQCTAP/">https://qiangchunyu.github.io/VQCTAP/</a> </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ä¸ºè·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ é¢†åŸŸå¸¦æ¥äº†é‡å¤§æ”¹è¿›ã€‚å¯¹äºæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ã€è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰ä»»åŠ¡ï¼Œæˆ‘ä»¬æœŸæœ›å¾—åˆ°ä¸€ç§è·¨æ¨¡æ€ç²¾ç»†ï¼ˆå¸§çº§ï¼‰åºåˆ—è¡¨ç¤ºï¼Œå®ƒå¼ºè°ƒæ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰å†…å®¹ï¼ŒåŒæ—¶æ·¡åŒ–è¯­éŸ³æ¨¡æ€çš„å‰¯è¯­è¨€ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œå‘é‡é‡åŒ–å¯¹æ¯”ä»¤ç‰Œå£°å­¦é¢„è®­ç»ƒï¼ˆVQ-CTAPï¼‰â€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è·¨æ¨¡æ€å¯¹é½åºåˆ—è½¬ç å™¨å°†æ–‡æœ¬å’Œè¯­éŸ³å¸¦å…¥è”åˆå¤šæ¨¡æ€ç©ºé—´ï¼Œå­¦ä¹ å¦‚ä½•åœ¨å¸§çº§åˆ«è¿æ¥æ–‡æœ¬å’Œè¯­éŸ³ã€‚æ‰€æçš„VQ-CTAPæ˜¯è·¨æ¨¡æ€åºåˆ—è¡¨ç¤ºå­¦ä¹ çš„ä¸€ç§èŒƒå¼ï¼Œä¸ºè¯­éŸ³å¤„ç†ä¸­çš„ç²¾ç»†ç”Ÿæˆå’Œè¯†åˆ«ä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚VQ-CTAPå¯ç›´æ¥åº”ç”¨äºVCå’ŒASRä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒæˆ–é¢å¤–ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åºåˆ—æ„ŸçŸ¥è¯­ä¹‰è¿æ¥å™¨ï¼Œå®ƒå°†å¤šä¸ªå†»ç»“çš„é¢„è®­ç»ƒæ¨¡å—è¿æ¥åˆ°TTSä»»åŠ¡ä¸­ï¼Œå±•ç°å‡ºå³æ’å³ç”¨çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ­¥è¿›ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥æ³¨å…¥å’Œè°ƒæ•´å„ç§æŸå¤±æˆåˆ†çš„å½±å“ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ”¶æ•›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰è½¬ç§»å‰¯è¯­è¨€ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ¨å¹¿åˆ°æœªè§æ•°æ®å¹¶æ•æ‰å‰¯è¯­è¨€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ã€‚å¦å¤–ï¼ŒVQ-CTAPå®ç°äº†é«˜å‹ç¼©è¯­éŸ³ç¼–ç ï¼Œä»24kHzè¾“å…¥æ³¢å½¢ä»¥25Hzçš„é€Ÿç‡è¿›è¡Œç¼–ç ï¼Œé‡‡æ ·ç‡é™ä½äº†960å€ã€‚éŸ³é¢‘æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://qiangchunyu.github.io/VQCTAP/%E6%89%BE%E5%88%B0%E3%80%82">https://qiangchunyu.github.io/VQCTAP/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05758v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ·±åº¦å­¦ä¹ åœ¨è·¨æ¨¡æ€è¡¨å¾å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚å¯¹äºæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ã€è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç­‰ä»»åŠ¡ï¼Œéœ€è¦è·¨æ¨¡æ€ç²¾ç»†ï¼ˆå¸§çº§ï¼‰åºåˆ—è¡¨å¾ï¼Œå¼ºè°ƒæ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰å†…å®¹ï¼ŒåŒæ—¶æ·¡åŒ–è¯­éŸ³æ¨¡æ€çš„å‰¯è¯­è¨€ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œå‘é‡é‡åŒ–å¯¹æ¯”ä»¤ç‰Œå£°å­¦é¢„è®­ç»ƒï¼ˆVQ-CTAPï¼‰â€çš„æ–¹æ³•ï¼Œä½¿ç”¨è·¨æ¨¡æ€å¯¹é½åºåˆ—è½¬ç å™¨å°†æ–‡æœ¬å’Œè¯­éŸ³å¸¦å…¥è”åˆå¤šæ¨¡æ€ç©ºé—´ï¼Œåœ¨å¸§çº§åˆ«å­¦ä¹ å¦‚ä½•è¿æ¥æ–‡æœ¬å’Œè¯­éŸ³ã€‚VQ-CTAPä¸ºè·¨æ¨¡æ€åºåˆ—è¡¨å¾å­¦ä¹ æä¾›äº†ä¸€ç§èŒƒå¼ï¼Œä¸ºè¯­éŸ³å¤„ç†ä¸­çš„ç²¾ç»†ç”Ÿæˆå’Œè¯†åˆ«ä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å¯ç›´æ¥åº”ç”¨äºVCå’ŒASRä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒæˆ–é¢å¤–ç»“æ„ã€‚é’ˆå¯¹TTSä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åºåˆ—æ„ŸçŸ¥è¯­ä¹‰è¿æ¥å™¨ï¼Œå¯è¿æ¥å¤šä¸ªå†»ç»“çš„é¢„è®­ç»ƒæ¨¡å—ï¼Œå…·æœ‰å³æ’å³ç”¨åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ­¥è¿›ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥æ³¨å…¥å’Œè°ƒæ•´å„ç§æŸå¤±åˆ†é‡çš„å½±å“ï¼Œç¡®ä¿æ¨¡å‹çš„æœ‰æ•ˆæ”¶æ•›ã€‚è¿˜æå‡ºäº†ä¸€ç§è¯­ä¹‰è½¬ç§»å‰¯è¯­è¨€ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥æé«˜è¡¨å¾èƒ½åŠ›ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ¨å¹¿åˆ°æœªè§æ•°æ®å¹¶æ•æ‰å‰¯è¯­è¨€ä¿¡æ¯çš„ç»†å¾®å·®åˆ«ã€‚æ­¤å¤–ï¼ŒVQ-CTAPå®ç°äº†é«˜å‹ç¼©è¯­éŸ³ç¼–ç ï¼Œä»24kHzè¾“å…¥æ³¢å½¢ä»¥25Hzé€Ÿç‡è¿›è¡Œç¼–ç ï¼Œé‡‡æ ·ç‡é™ä½äº†960å€ã€‚éŸ³é¢‘æ¼”ç¤ºå¯åœ¨[<a target="_blank" rel="noopener" href="https://qiangchunyu.github.io/VQCTAP/]%E8%AE%BF%E9%97%AE%E3%80%82">https://qiangchunyu.github.io/VQCTAP/]è®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è·¨æ¨¡æ€è¡¨å¾å­¦ä¹ ä¸­å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå°¤å…¶åœ¨æ–‡æœ¬è½¬è¯­éŸ³ã€è¯­éŸ³è½¬æ¢å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ä¸­ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºVQ-CTAPçš„è·¨æ¨¡æ€ç²¾ç»†åºåˆ—è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼Œå°†æ–‡æœ¬å’Œè¯­éŸ³å¸¦å…¥è”åˆå¤šæ¨¡æ€ç©ºé—´ã€‚</li>
<li>VQ-CTAPå¯ç›´æ¥åº”ç”¨äºè¯­éŸ³è½¬æ¢å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ã€‚</li>
<li>é’ˆå¯¹æ–‡æœ¬è½¬è¯­éŸ³ä»»åŠ¡ï¼Œå¼•å…¥åºåˆ—æ„ŸçŸ¥è¯­ä¹‰è¿æ¥å™¨ï¼Œå®ç°æ¨¡å—å³æ’å³ç”¨ã€‚</li>
<li>é‡‡ç”¨æ­¥è¿›ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹æœ‰æ•ˆæ”¶æ•›ï¼Œå¹¶æå‡ºè¯­ä¹‰è½¬ç§»å‰¯è¯­è¨€ä¸€è‡´æ€§æŸå¤±ä»¥æé«˜æ¨¡å‹è¡¨å¾èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>VQ-CTAPå®ç°é«˜å‹ç¼©è¯­éŸ³ç¼–ç ï¼Œé‡‡æ ·ç‡å¤§å¹…é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b6370feb9110bfbbb3dfbeed3b9dabb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-deac6e1246f33a95a531e19b52ca2cf4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba9ee7dcdedad1c0935aa97b057f44fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c4f1e3f74ca5f390c49bfa4ef9f5a7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c6fce304b8432d8b73b31bde4d695f4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Sentiment-Reasoning-for-Healthcare"><a href="#Sentiment-Reasoning-for-Healthcare" class="headerlink" title="Sentiment Reasoning for Healthcare"></a>Sentiment Reasoning for Healthcare</h2><p><strong>Authors:Khai-Nguyen Nguyen, Khai Le-Duc, Bach Phan Tat, Duy Le, Long Vo-Dang, Truong-Son Hy</strong></p>
<p>Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)â€™s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the worldâ€™s largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving modelâ€™s classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/Sentiment-Reasoning">https://github.com/leduckhai/Sentiment-Reasoning</a> </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åŒ»ç–—å†³ç­–ä¸­çš„é€æ˜åº¦è‡³å…³é‡è¦ã€‚é€šè¿‡å°†æ¯ä¸ªé¢„æµ‹æ ‡ç­¾çš„ç†ç”±èå…¥å…¶ä¸­ï¼Œç”¨æˆ·å¯ä»¥ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ï¼Œä»¥åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºè¯­éŸ³å’Œæ–‡æœ¬æ¨¡æ€å¼•å…¥äº†ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”æƒ…æ„Ÿæ¨ç†ï¼Œä»¥åŠæˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€å¤šä»»åŠ¡æ¡†æ¶å’Œä¸–ç•Œä¸Šæœ€å¤§çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†ã€‚æƒ…æ„Ÿæ¨ç†æ˜¯æƒ…æ„Ÿåˆ†æä¸­çš„è¾…åŠ©ä»»åŠ¡ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥å†…å®¹é¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾å¹¶ç”Ÿæˆå…¶èƒŒåçš„ç†ç”±ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨äººå·¥è½¬å½•å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æ–‡æœ¬ä¸Šè¿›è¡Œï¼Œè¡¨æ˜æƒ…æ„Ÿæ¨ç†é€šè¿‡ä¸ºæ¨¡å‹é¢„æµ‹æä¾›ç†ç”±ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„é€æ˜åº¦ï¼Œå…¶è¯­ä¹‰è´¨é‡ä¸äººç±»ç›¸å½“ï¼ŒåŒæ—¶è¿˜å¯é€šè¿‡ç†ç”±å¢å¼ºå¾®è°ƒæé«˜æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ï¼ˆ+2%çš„å‡†ç¡®ç‡å’Œå®è§‚F1å€¼æé«˜ï¼‰ã€‚æ­¤å¤–ï¼Œäººå·¥è½¬å½•å’ŒASRè½¬å½•æ–‡æœ¬åœ¨ç”Ÿæˆç†ç”±çš„è¯­ä¹‰è´¨é‡ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®ï¼ˆäº”ç§è¯­è¨€â€”â€”è¶Šå—è¯­ã€è‹±è¯­ã€ä¸­æ–‡ã€å¾·è¯­å’Œæ³•è¯­ï¼‰å’Œæ¨¡å‹å‡åœ¨çº¿å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/leduckhai/Sentiment-Reasoning">https://github.com/leduckhai/Sentiment-Reasoning</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21054v4">PDF</a> ACL 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäººå·¥æ™ºèƒ½å¥åº·å†³ç­–ä¸­çš„é€æ˜åº¦è‡³å…³é‡è¦ã€‚é€šè¿‡èå…¥è§£é‡Šé¢„æµ‹æ ‡ç­¾çš„ç†ç”±ï¼Œç”¨æˆ·èƒ½æ›´å¥½åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†è¿‡ç¨‹ä»¥åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°çš„ä»»åŠ¡â€”â€”æƒ…æ„Ÿæ¨ç†ï¼Œé€‚ç”¨äºè¯­éŸ³å’Œæ–‡å­—æ¨¡æ€ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€å¤šä»»åŠ¡æ¡†æ¶å’Œå…¨çƒæœ€å¤§çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†ã€‚æƒ…æ„Ÿæ¨ç†æ˜¯æƒ…æ„Ÿåˆ†æä¸­çš„ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œæ¨¡å‹åŸºäºè¾“å…¥å†…å®¹é¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾å¹¶ç”ŸæˆèƒŒåçš„ç†ç”±ã€‚åœ¨äººå·¥è½¬å½•å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•çš„ç ”ç©¶ä¸­ï¼Œæƒ…æ„Ÿæ¨ç†é€šè¿‡æä¾›æ¨¡å‹é¢„æµ‹çš„ç†ç”±æé«˜äº†æ¨¡å‹çš„é€æ˜åº¦ï¼ŒåŒæ—¶ç”Ÿæˆçš„è¯­ä¹‰è´¨é‡ä¸äººç±»ç›¸å½“ï¼Œé€šè¿‡ç†ç”±å¢å¼ºå¾®è°ƒæé«˜äº†æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ï¼ˆå‡†ç¡®ç‡å’Œå®F1åˆ†åˆ«æé«˜2%ï¼‰ã€‚æ­¤å¤–ï¼Œäººå·¥å’ŒASRè½¬å½•ç”Ÿæˆçš„è§£é‡Šç†ç”±åœ¨è¯­ä¹‰è´¨é‡ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å·²åœ¨çº¿å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½å¥åº·å†³ç­–ä¸­çš„é€æ˜åº¦è‡³å…³é‡è¦ï¼Œèå…¥è§£é‡Šé¢„æµ‹æ ‡ç­¾çš„ç†ç”±æœ‰åŠ©äºç”¨æˆ·ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„ä»»åŠ¡â€”â€”æƒ…æ„Ÿæ¨ç†ï¼Œé€‚ç”¨äºè¯­éŸ³å’Œæ–‡å­—æ¨¡æ€ã€‚</li>
<li>æå‡ºäº†å¤šæ¨¡æ€å¤šä»»åŠ¡æ¡†æ¶å’Œå…¨çƒæœ€å¤§çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ•°æ®é›†ã€‚</li>
<li>æ¨¡å‹èƒ½åŸºäºè¾“å…¥å†…å®¹é¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾å¹¶ç”ŸæˆèƒŒåçš„ç†ç”±ã€‚</li>
<li>æƒ…æ„Ÿæ¨ç†æé«˜äº†æ¨¡å‹çš„é€æ˜åº¦ï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç†ç”±å¢å¼ºå¾®è°ƒï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡å’Œå®F1éƒ½æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bfdd169c9a0cfea2420f43dcfea08e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fab05b621b1f919e40cf07f0d4234d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddd2bffaeb4740293e5aa80114720b9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06149ba990da6ad868b667ad5d209c58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a1dfc35d5e6faed022546e07deb5a43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7196e463f0cfcf6c2b4ae45f2df2230.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Speech-Synthesis-without-Vector-Quantization"><a href="#Autoregressive-Speech-Synthesis-without-Vector-Quantization" class="headerlink" title="Autoregressive Speech Synthesis without Vector Quantization"></a>Autoregressive Speech Synthesis without Vector Quantization</h2><p><strong>Authors:Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, Furu Wei</strong></p>
<p>We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at <a target="_blank" rel="noopener" href="https://aka.ms/melle">https://aka.ms/melle</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MELLEï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¿ç»­å€¼æ ‡è®°çš„æ–°çš„æ–‡æœ¬è½¬è¯­éŸ³åˆæˆï¼ˆTTSï¼‰è¯­è¨€å»ºæ¨¡æ–¹æ³•ã€‚MELLEé€šè¿‡æ–‡æœ¬æ¡ä»¶ç›´æ¥è‡ªå›å½’ç”Ÿæˆè¿ç»­çš„æ¢…å°”é¢‘è°±å¸§ï¼Œé¿å…äº†å‘é‡é‡åŒ–çš„éœ€æ±‚ã€‚å‘é‡é‡åŒ–é€šå¸¸ç”¨äºéŸ³é¢‘å‹ç¼©ï¼Œä¸è¿ç»­è¡¨ç¤ºç›¸æ¯”ç‰ºç‰²äº†ä¿çœŸåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆiï¼‰æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼Œè€Œæ˜¯åº”ç”¨äº†å›å½’æŸå¤±å’Œæå‡ºçš„é¢‘è°±æµé‡æŸå¤±å‡½æ•°æ¥æ¨¡æ‹Ÿè¿ç»­å€¼æ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒï¼›ï¼ˆiiï¼‰æˆ‘ä»¬å°†å˜åˆ†æ¨æ–­èå…¥MELLEï¼Œä»¥ä¿ƒè¿›é‡‡æ ·æœºåˆ¶ï¼Œä»è€Œå¢å¼ºè¾“å‡ºå¤šæ ·æ€§å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¸¤é˜¶æ®µç¼–ç å™¨è§£ç å™¨è¯­è¨€æ¨¡å‹VALL-EåŠå…¶å˜ä½“ç›¸æ¯”ï¼Œå•é˜¶æ®µçš„MELLEé€šè¿‡é¿å…é‡‡æ ·å‘é‡é‡åŒ–ä»£ç çš„å›ºæœ‰ç¼ºé™·ï¼Œç¼“è§£äº†ç¨³å¥æ€§é—®é¢˜ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œè€Œä¸”æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒæä¾›äº†æ›´ç®€æ´çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„å·¥ä½œæ¼”ç¤ºè§<a target="_blank" rel="noopener" href="https://aka.ms/melle%E3%80%82">https://aka.ms/melleã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.08551v2">PDF</a> Accepted to ACL 2025 Main</p>
<p><strong>æ‘˜è¦</strong><br>MELLEæ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰çš„æ–°å‹è¿ç»­å€¼ä»¤ç‰ŒåŸºç¡€è¯­è¨€å»ºæ¨¡æ–¹æ³•ã€‚MELLEèƒ½å¤Ÿç›´æ¥ä»æ–‡æœ¬æ¡ä»¶ç”Ÿæˆè¿ç»­çš„æ¢…å°”é¢‘è°±å¸§ï¼Œä»è€Œç»•è¿‡äº†é€šå¸¸éœ€è¦å‘é‡é‡åŒ–çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•é€šè¿‡åº”ç”¨å›å½’æŸå¤±å’Œä¸€ä¸ªæå‡ºçš„é¢‘è°±æµé‡æŸå¤±å‡½æ•°æ¥å»ºæ¨¡è¿ç»­ä»¤ç‰Œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶å¼•å…¥äº†å˜åˆ†æ¨æ–­æ¥ä¿ƒè¿›é‡‡æ ·æœºåˆ¶ï¼Œä»è€Œæé«˜è¾“å‡ºå¤šæ ·æ€§å’Œæ¨¡å‹é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¸¤é˜¶æ®µç¼–ç è¯­è¨€æ¨¡å‹VALL-EåŠå…¶å˜ä½“ç›¸æ¯”ï¼Œå•é˜¶æ®µçš„MELLEé¿å…äº†é‡‡æ ·å‘é‡é‡åŒ–ä»£ç çš„å›ºæœ‰ç¼ºé™·ï¼Œæé«˜äº†å¤šä¸ªæŒ‡æ ‡çš„æ€§èƒ½ï¼Œå¹¶ä¸”æä¾›äº†ä¸€ä¸ªæ›´ç®€æ´çš„èŒƒå¼ã€‚æˆ‘ä»¬çš„å·¥ä½œæ¼”ç¤ºåœ°å€ä¸ºï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>MELLEæ˜¯ä¸€ç§æ–°å‹çš„è¿ç»­å€¼ä»¤ç‰ŒåŸºç¡€è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰ã€‚</li>
<li>MELLEç›´æ¥ç”Ÿæˆè¿ç»­çš„æ¢…å°”é¢‘è°±å¸§ï¼Œç»•è¿‡äº†å‘é‡é‡åŒ–çš„éœ€æ±‚ã€‚</li>
<li>MELLEåº”ç”¨å›å½’æŸå¤±å’Œé¢‘è°±æµé‡æŸå¤±å‡½æ•°æ¥å»ºæ¨¡è¿ç»­ä»¤ç‰Œçš„æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>MELLEå¼•å…¥äº†å˜åˆ†æ¨æ–­æ¥ä¿ƒè¿›é‡‡æ ·æœºåˆ¶ï¼Œæé«˜è¾“å‡ºå¤šæ ·æ€§å’Œæ¨¡å‹é²æ£’æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMELLEç›¸è¾ƒäºä¸¤é˜¶æ®µç¼–ç è¯­è¨€æ¨¡å‹VALL-EåŠå…¶å˜ä½“ï¼Œå…·æœ‰æ›´é«˜çš„æ€§èƒ½å’Œæ›´ç®€æ´çš„èŒƒå¼ã€‚</li>
<li>MELLEé¿å…äº†é‡‡æ ·å‘é‡é‡åŒ–ä»£ç çš„å›ºæœ‰ç¼ºé™·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.08551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea5639a13fcda3600c5da98d3fc1dd31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a2810e80e26ecc6acffc9a29bd8f08a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6462ac616b44e9d4884df4b30909698e.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  X-GAN A Generative AI-Powered Unsupervised Model for Main Vessel   Segmentation of Glaucoma Screening
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6e7815ecf4da49216c74c947f4a8ef96.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Single Domain Generalization for Alzheimer's Detection from 3D MRIs with   Pseudo-Morphological Augmentations and Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
