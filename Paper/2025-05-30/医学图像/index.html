<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Chest Disease Detection In X-Ray Images Using Deep Learning   Classification Method">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-384e35f008881080ac4f07969a28155e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-30-æ›´æ–°"><a href="#2025-05-30-æ›´æ–°" class="headerlink" title="2025-05-30 æ›´æ–°"></a>2025-05-30 æ›´æ–°</h1><h2 id="Chest-Disease-Detection-In-X-Ray-Images-Using-Deep-Learning-Classification-Method"><a href="#Chest-Disease-Detection-In-X-Ray-Images-Using-Deep-Learning-Classification-Method" class="headerlink" title="Chest Disease Detection In X-Ray Images Using Deep Learning   Classification Method"></a>Chest Disease Detection In X-Ray Images Using Deep Learning   Classification Method</h2><p><strong>Authors:Alanna Hazlett, Naomi Ohashi, Timothy Rodriguez, Sodiq Adewole</strong></p>
<p>In this work, we investigate the performance across multiple classification models to classify chest X-ray images into four categories of COVID-19, pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning techniques with state-of-the-art pre-trained Convolutional Neural Networks (CNNs) models. We fine-tuned these pre-trained architectures on a labeled medical x-ray images. The initial results are promising with high accuracy and strong performance in key classification metrics such as precision, recall, and F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for model interpretability to provide visual explanations for classification decisions, improving trust and transparency in clinical applications. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šä¸ªåˆ†ç±»æ¨¡å‹å¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œåˆ†ç±»çš„æ€§èƒ½ï¼Œåˆ†ä¸ºå››ç±»ï¼šCOVID-19ã€è‚ºç‚ã€ç»“æ ¸ç—…ï¼ˆTBï¼‰å’Œæ­£å¸¸ç—…ä¾‹ã€‚æˆ‘ä»¬åˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹é‡‡ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨æ ‡è®°çš„åŒ»å­¦Xå°„çº¿å›¾åƒä¸Šå¯¹é¢„è®­ç»ƒæ¶æ„è¿›è¡Œäº†å¾®è°ƒã€‚åˆæ­¥ç»“æœä»¤äººé¼“èˆï¼Œç²¾åº¦é«˜ï¼Œåœ¨å…³é”®åˆ†ç±»æŒ‡æ ‡ï¼ˆå¦‚ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ï¼‰æ–¹é¢è¡¨ç°å¼ºåŠ²ã€‚ä¸ºäº†æä¾›åˆ†ç±»å†³ç­–çš„è§†è§‰è§£é‡Šï¼Œæé«˜ä¸´åºŠåº”ç”¨ä¸­ä¿¡ä»»å’Œé€æ˜åº¦ï¼Œæˆ‘ä»¬åº”ç”¨äº†æ¢¯åº¦åŠ æƒç±»åˆ«æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰è¿›è¡Œæ¨¡å‹è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22609v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¤šç§åˆ†ç±»æ¨¡å‹åœ¨å°†èƒ¸éƒ¨Xå°„çº¿å›¾åƒåˆ†ç±»ä¸ºCOVID-19ã€è‚ºç‚ã€ç»“æ ¸ç—…ï¼ˆTBï¼‰å’Œæ­£å¸¸ç—…ä¾‹å››ç±»æ—¶çš„æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ æŠ€æœ¯ã€‚åœ¨æ ‡è®°çš„åŒ»å­¦Xå°„çº¿å›¾åƒä¸Šå¯¹é¢„è®­ç»ƒæ¶æ„è¿›è¡Œäº†å¾®è°ƒã€‚åˆæ­¥ç»“æœå…·æœ‰å‰æ™¯ï¼Œå…³é”®åˆ†ç±»æŒ‡æ ‡å¦‚ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œå¼ºåŠ²æ€§èƒ½ã€‚æˆ‘ä»¬åº”ç”¨äº†æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰æ¥æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä¸ºåˆ†ç±»å†³ç­–æä¾›è§†è§‰è§£é‡Šï¼Œæé«˜ä¸´åºŠåº”ç”¨çš„ä¿¡ä»»å’Œé€æ˜åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è€…æ¢ç©¶äº†å¤šç§åˆ†ç±»æ¨¡å‹åœ¨èƒ¸éƒ¨Xå°„çº¿å›¾åƒåˆ†ç±»ä¸­çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å…ˆè¿›çš„é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚</li>
<li>åœ¨æ ‡è®°çš„åŒ»å­¦Xå°„çº¿å›¾åƒä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œè·å¾—äº†æœ‰å‰æ™¯çš„åˆæ­¥ç»“æœã€‚</li>
<li>æ¨¡å‹åœ¨åˆ†ç±»ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰å…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œå¼ºåŠ²æ€§èƒ½ã€‚</li>
<li>åº”ç”¨äº†æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>æ¨¡å‹æä¾›è§†è§‰è§£é‡Šä¸ºåˆ†ç±»å†³ç­–æä¾›æ ¹æ®ï¼Œæé«˜äº†ä¸´åºŠåº”ç”¨çš„ä¿¡ä»»åº¦å’Œé€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a0285f551a3cc316f0168d357e099f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79fbdc79b05d497778881ea4fdd31492.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-384e35f008881080ac4f07969a28155e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-759b64475f7c2cfeb52599bb2b3d729d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3368beec37b7cbe9a9864d9811868c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2193407ba99da3cdc3120325498d8f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3a2e811d2adceb4c4bd949cf35a2e58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea01662d425ce3e467d21558e08dda3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c96f15eee2facdf5af4be6081c416ac.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prediction-and-Synthesis-of-Mg-4-Pt-3-H-6-A-Metallic-Complex-Transition-Metal-Hydride-Stabilized-at-Ambient-Pressure"><a href="#Prediction-and-Synthesis-of-Mg-4-Pt-3-H-6-A-Metallic-Complex-Transition-Metal-Hydride-Stabilized-at-Ambient-Pressure" class="headerlink" title="Prediction and Synthesis of Mg$_4$Pt$_3$H$_6$: A Metallic Complex   Transition Metal Hydride Stabilized at Ambient Pressure"></a>Prediction and Synthesis of Mg$_4$Pt$_3$H$_6$: A Metallic Complex   Transition Metal Hydride Stabilized at Ambient Pressure</h2><p><strong>Authors:Wencheng Lu, Michael J. Hutcheon, Mads F. Hansen, Kapildeb Dolui, Shubham Sinha, Mihir R. Sahoo, Chris J. Pickard, Christoph Heil, Anna Pakhomova, Mohamed Mezouar, Dominik Daisenberger, Stella Chariton, Vitali Prakapenka, Matthew N. Julian, Rohit P. Prasankumar, Timothy A. Strobel</strong></p>
<p>The low-pressure stabilization of superconducting hydrides with high critical temperatures ($T_c$s) remains a significant challenge, and experimentally verified superconducting hydrides are generally constrained to a limited number of structural prototypes. Ternary transition-metal complex hydrides (hydrido complexes)-typically regarded as hydrogen storage materials-exhibit a large range of compounds stabilized at low pressure with recent predictions for high-$T_c$ superconductivity. Motivated by this class of materials, we investigated complex hydride formation in the Mg-Pt-H system, which has no known ternary hydride compounds. Guided by ab initio structural predictions, we successfully synthesized a novel complex transition-metal hydride, Mg$_4$Pt$_3$H$_6$, using laser-heated diamond anvil cells. The compound forms in a body-centered cubic structural prototype at moderate pressures between 8-25 GPa. Unlike the majority of known hydrido complexes, Mg$_4$Pt$_3$H$_6$ is metallic, with formal charge described as 4[Mg]$^{2+}$.3[PtH$_2$]$^{2-}$. X-ray diffraction (XRD) measurements obtained during decompression reveal that Mg$_4$Pt$_3$H$_6$ remains stable upon quenching to ambient conditions. Magnetic-field and temperature-dependent electrical transport measurements indicate ambient-pressure superconductivity with $T_c$ (50%) &#x3D; 2.9 K, in reasonable agreement with theoretical calculations. These findings clarify the phase behavior in the Mg-Pt-H system and provide valuable insights for transition-metal complex hydrides as a new class of hydrogen-rich superconductors. </p>
<blockquote>
<p>è¶…å¯¼æ°¢åŒ–ç‰©ä½å‹ç¨³å®šæ€§çš„ç ”ç©¶ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œä¸”å®éªŒéªŒè¯çš„è¶…å¯¼æ°¢åŒ–ç‰©é€šå¸¸å±€é™äºæœ‰é™çš„ç»“æ„åŸå‹ã€‚ä¸‰å…ƒè¿‡æ¸¡é‡‘å±å¤åˆæ°¢åŒ–ç‰©ï¼ˆæ°¢åŒ–ç‰©å¤åˆç‰©ï¼‰é€šå¸¸è¢«è§†ä¸ºå‚¨æ°¢ææ–™ï¼Œå…¶èƒ½åœ¨ä½å‹ä¸‹ç¨³å®šå­˜åœ¨ä¸€å¤§ç±»åŒ–åˆç‰©ï¼Œæœ€è¿‘æœ‰é¢„æµ‹è¡¨æ˜å…¶å…·æœ‰é«˜æ¸©è¶…å¯¼æ€§ã€‚å—æ­¤ç±»ææ–™çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†Mg-Pt-Hä½“ç³»ä¸­å¤åˆæ°¢åŒ–ç‰©çš„å½¢æˆï¼Œè¯¥ä½“ç³»ä¸­å°šæ— å·²çŸ¥çš„ä¸‰å…ƒæ°¢åŒ–ç‰©åŒ–åˆç‰©ã€‚åœ¨åŸºäºä»å¤´ç®—ç»“æ„é¢„æµ‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æˆåŠŸä½¿ç”¨æ¿€å…‰åŠ çƒ­é‡‘åˆšçŸ³ç §é¢ç»†èƒåˆæˆäº†ä¸€ç§æ–°å‹è¿‡æ¸¡é‡‘å±æ°¢åŒ–ç‰©Mg4Pt3H6ã€‚è¯¥åŒ–åˆç‰©ä»¥ä½“å¿ƒç«‹æ–¹ç»“æ„åŸå‹å½¢æˆäº8-25 GPaçš„é€‚åº¦å‹åŠ›ä¸‹ã€‚ä¸å¤§å¤šæ•°å·²çŸ¥æ°¢åŒ–ç‰©å¤åˆç‰©ä¸åŒï¼ŒMg4Pt3H6å…·æœ‰é‡‘å±æ€§ï¼Œå½¢å¼ç”µè·æè¿°ä¸º4[Mg]2+.3[PtH2]2âˆ’ã€‚åœ¨å‡å‹è¿‡ç¨‹ä¸­è·å¾—çš„Xå°„çº¿è¡å°„ï¼ˆXRDï¼‰æµ‹é‡ç»“æœè¡¨æ˜ï¼ŒMg4Pt3H6åœ¨æ·¬ç«åˆ°ç¯å¢ƒæ¡ä»¶ä¸‹æ—¶ä¿æŒç¨³å®šã€‚ç£åœºå’Œæ¸©åº¦ç›¸å…³çš„ç”µè¾“è¿æµ‹é‡è¡¨æ˜ç¯å¢ƒå‹åŠ›ä¸‹å…·æœ‰è¶…å¯¼æ€§ï¼ŒåŠä¸´ç•Œæ¸©åº¦ï¼ˆTcï¼‰ï¼ˆ50%ï¼‰&#x3D; 2.9 Kï¼Œä¸ç†è®ºè®¡ç®—åŸºæœ¬ä¸€è‡´ã€‚è¿™äº›å‘ç°é˜æ˜äº†Mg-Pt-Hç³»ç»Ÿçš„ç›¸è¡Œä¸ºï¼Œå¹¶ä¸ºè¿‡æ¸¡é‡‘å±å¤åˆæ°¢åŒ–ç‰©ä½œä¸ºä¸€ç±»æ–°å‹å¯Œæ°¢è¶…å¯¼ææ–™æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22546v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<pre><code>é’ˆå¯¹å…·æœ‰é«˜è¶…å¯¼æ¸©åº¦ï¼ˆ$T_c$ï¼‰çš„è¶…å¯¼æ°¢åŒ–ç‰©åœ¨ä½å‹ä¸‹çš„ç¨³å®šæ€§ä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œä¸”å®éªŒéªŒè¯çš„è¶…å¯¼æ°¢åŒ–ç‰©é€šå¸¸å±€é™äºæœ‰é™çš„å‡ ç§ç»“æ„åŸå‹ã€‚æœ¬ç ”ç©¶å—ä¸‰å…ƒè¿‡æ¸¡é‡‘å±å¤åˆæ°¢åŒ–ç‰©ï¼ˆæ°¢åŒ–ç‰©å¤åˆä½“ï¼‰å¯å‘ï¼Œè¿™äº›ææ–™é€šå¸¸è¢«è§†ä¸ºå‚¨æ°¢ææ–™ï¼Œå¹¶åœ¨ä½å‹ä¸‹ç¨³å®šå­˜åœ¨å¤§é‡çš„åŒ–åˆç‰©ï¼Œæœ€è¿‘æœ‰é¢„æµ‹ç§°å…·æœ‰é«˜è¶…å¯¼æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†Mg-Pt-Hä½“ç³»ä¸­å¤åˆæ°¢åŒ–ç‰©çš„å½¢æˆï¼Œè¯¥ä½“ç³»ä¸­å°šæ— å·²çŸ¥çš„ä¸‰å…ƒæ°¢åŒ–ç‰©åŒ–åˆç‰©ã€‚åœ¨åŸºäºä»å¤´ç®—ç»“æ„é¢„æµ‹çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æˆåŠŸä½¿ç”¨æ¿€å…‰åŠ çƒ­é‡‘åˆšçŸ³ç §å®¤åˆæˆäº†ä¸€ç§æ–°å‹è¿‡æ¸¡é‡‘å±æ°¢åŒ–ç‰©Mg$_4$Pt$_3$H$_6$ã€‚è¯¥åŒ–åˆç‰©åœ¨8-25 GPaçš„é€‚åº¦å‹åŠ›ä¸‹å½¢æˆä½“å¿ƒç«‹æ–¹ç»“æ„åŸå‹ã€‚ä¸å¤§å¤šæ•°å·²çŸ¥çš„æ°¢åŒ–ç‰©å¤åˆä½“ä¸åŒï¼ŒMg$_4$Pt$_3$H$_6$æ˜¯é‡‘å±æ€§çš„ï¼Œå½¢å¼ç”µè·æè¿°ä¸º4[Mg]$^&#123;2+&#125;$.3[PtH$_2$]$^&#123;2-&#125;$ã€‚Xå°„çº¿è¡å°„ï¼ˆXRDï¼‰æµ‹é‡ç»“æœæ˜¾ç¤ºï¼Œåœ¨å‡å‹è¿‡ç¨‹ä¸­Mg$_4$Pt$_3$H$_6$åœ¨æ·¬ç«è‡³ç¯å¢ƒæ¡ä»¶ä¸‹ä¿æŒç¨³å®šã€‚ç£åœºå’Œæ¸©åº¦ç›¸å…³çš„è¾“è¿æµ‹é‡è¡¨æ˜ç¯å¢ƒå‹åŠ›ä¸‹å…·æœ‰è¶…å¯¼æ€§ï¼Œä¸”$T_c$ï¼ˆ50%ï¼‰= 2.9 Kï¼Œä¸ç†è®ºè®¡ç®—åŸºæœ¬ä¸€è‡´ã€‚è¿™äº›å‘ç°é˜æ˜äº†Mg-Pt-Hç³»ç»Ÿçš„ç›¸è¡Œä¸ºï¼Œå¹¶ä¸ºè¿‡æ¸¡é‡‘å±å¤åˆæ°¢åŒ–ç‰©ä½œä¸ºä¸€ç±»å¯Œæ°¢çš„è¶…å¯¼ä½“æä¾›äº†å®è´µçš„è§è§£ã€‚

**å…³é”®è§è§£**

1. ç ”ç©¶å‘ç°æ–°å‹è¿‡æ¸¡é‡‘å±æ°¢åŒ–ç‰©Mg$_4$Pt$_3$H$_6$åœ¨é€‚åº¦å‹åŠ›ä¸‹å½¢æˆä½“å¿ƒç«‹æ–¹ç»“æ„åŸå‹ã€‚
2. Mg$_4$Pt$_3$H$_6$æ˜¯é‡‘å±æ€§çš„ï¼Œå…·æœ‰ç‰¹å®šçš„å½¢å¼ç”µè·åˆ†å¸ƒã€‚
3. Xå°„çº¿è¡å°„æµ‹é‡æ˜¾ç¤ºMg$_4$Pt$_3$H$_6$åœ¨å‡å‹è‡³ç¯å¢ƒæ¡ä»¶ä¸‹ä¿æŒç¨³å®šã€‚
4. ç¯å¢ƒå‹åŠ›ä¸‹å…·æœ‰è¶…å¯¼æ€§ï¼Œä¸´ç•Œæ¸©åº¦ï¼ˆ$T_c$ï¼‰è¾¾åˆ°2.9 Kã€‚
5. è¿™äº›å‘ç°å¯¹è¿‡æ¸¡é‡‘å±å¤åˆæ°¢åŒ–ç‰©ä½œä¸ºå¯Œæ°¢è¶…å¯¼ä½“æä¾›äº†æ–°çš„è§†è§’ã€‚
6. ç ”ç©¶ç»“æœå¯¹ç†è§£è¶…å¯¼æ°¢åŒ–ç‰©çš„ç¨³å®šæ€§å’Œç›¸è¡Œä¸ºæœ‰é‡è¦æ„ä¹‰ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e64dee62936015e47db164892d05449.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ecb4dbb05c5635c53e0bb3e2d0094b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2c824501cc8d3593a432f25d70dc367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8093d3fd40b4804150f505529313e53f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-603118c9ea6cc3838614e0f63483b691.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions"><a href="#ConfLUNet-Multiple-sclerosis-lesion-instance-segmentation-in-presence-of-confluent-lesions" class="headerlink" title="ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions"></a>ConfLUNet: Multiple sclerosis lesion instance segmentation in presence   of confluent lesions</h2><p><strong>Authors:Maxence Wynen, Pedro M. Gordaliza, Maxime Istasse, Anna StÃ¶lting, Pietro Maggi, BenoÃ®t Macq, Meritxell Bach Cuadra</strong></p>
<p>Accurate lesion-level segmentation on MRI is critical for multiple sclerosis (MS) diagnosis, prognosis, and disease monitoring. However, current evaluation practices largely rely on semantic segmentation post-processed with connected components (CC), which cannot separate confluent lesions (aggregates of confluent lesion units, CLUs) due to reliance on spatial connectivity. To address this misalignment with clinical needs, we introduce formal definitions of CLUs and associated CLU-aware detection metrics, and include them in an exhaustive instance segmentation evaluation framework. Within this framework, we systematically evaluate CC and post-processing-based Automated Confluent Splitting (ACLS), the only existing methods for lesion instance segmentation in MS. Our analysis reveals that CC consistently underestimates CLU counts, while ACLS tends to oversplit lesions, leading to overestimated lesion counts and reduced precision. To overcome these limitations, we propose ConfLUNet, the first end-to-end instance segmentation framework for MS lesions. ConfLUNet jointly optimizes lesion detection and delineation from a single FLAIR image. Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the held-out test set (n&#x3D;13) in instance segmentation (Panoptic Quality: 42.0% vs. 37.5%&#x2F;36.8%; p &#x3D; 0.017&#x2F;0.005) and lesion detection (F1: 67.3% vs. 61.6%&#x2F;59.9%; p &#x3D; 0.028&#x2F;0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU] (81.5%), improving recall over CC (+12.5%, p &#x3D; 0.015) and precision over ACLS (+31.2%, p &#x3D; 0.003). By combining rigorous definitions, new CLU-aware metrics, a reproducible evaluation framework, and the first dedicated end-to-end model, this work lays the foundation for lesion instance segmentation in MS. </p>
<blockquote>
<p>åœ¨å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„è¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹ä¸­ï¼ŒMRIä¸Šçš„ç²¾ç¡®ç—…å˜çº§åˆ«åˆ†å‰²è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°å®è·µä¸»è¦ä¾èµ–äºé€šè¿‡è¿é€šç»„ä»¶ï¼ˆCCï¼‰è¿›è¡Œåå¤„ç†çš„è¯­ä¹‰åˆ†å‰²ï¼Œç”±äºä¾èµ–ç©ºé—´è¿é€šæ€§ï¼Œå®ƒæ— æ³•åˆ†ç¦»èåˆæ€§ç—…å˜ï¼ˆèåˆæ€§ç—…å˜å•ä½ï¼ˆCLUï¼‰çš„èšé›†ä½“ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å¹¶æ»¡è¶³ä¸´åºŠéœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†CLUsçš„æ­£å¼å®šä¹‰å’Œç›¸å…³çš„CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡ï¼Œå¹¶å°†å®ƒä»¬çº³å…¥è¯¦å°½çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ä¸­ã€‚åœ¨æ­¤æ¡†æ¶å†…ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†åŸºäºè¿é€šç»„ä»¶ï¼ˆCCï¼‰å’Œåå¤„ç†çš„è‡ªåŠ¨èåˆåˆ†è£‚ï¼ˆACLSï¼‰ï¼Œè¿™æ˜¯å¤šå‘æ€§ç¡¬åŒ–ç—‡ç—…å˜å®ä¾‹åˆ†å‰²ä¸­å”¯ä¸€ç°æœ‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒCCå§‹ç»ˆä½ä¼°CLUè®¡æ•°ï¼Œè€ŒACLSå€¾å‘äºè¿‡åº¦åˆ†å‰²ç—…å˜ï¼Œå¯¼è‡´ç—…å˜è®¡æ•°è¿‡é«˜å’Œç²¾åº¦é™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ConfLUNetï¼Œè¿™æ˜¯å¤šå‘æ€§ç¡¬åŒ–ç—‡çš„ç«¯åˆ°ç«¯ç—…å˜å®ä¾‹åˆ†å‰²æ¡†æ¶çš„é¦–åˆ›ã€‚ConfLUNetè”åˆä¼˜åŒ–ä»å•ä¸ªFLAIRå›¾åƒä¸­æ£€æµ‹å¹¶æç»˜ç—…å˜ã€‚åœ¨50åæ‚£è€…ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒConfLUNetåœ¨ä¿ç•™çš„æµ‹è¯•é›†ï¼ˆn&#x3D;13ï¼‰ä¸Šçš„å®ä¾‹åˆ†å‰²ï¼ˆå…¨æ™¯è´¨é‡ï¼š42.0%æ¯”37.5%&#x2F;36.8%ï¼›p&#x3D;0.017&#x2F;0.005ï¼‰å’Œç—…å˜æ£€æµ‹ï¼ˆF1ï¼š67.3%æ¯”61.6%&#x2F;59.9%ï¼›p&#x3D;0.028&#x2F;0.013ï¼‰æ–¹é¢å‡æ˜¾è‘—ä¼˜äºCCå’ŒACLSã€‚å¯¹äºCLUæ£€æµ‹ï¼ŒConfLUNetè¾¾åˆ°äº†æœ€é«˜çš„F1[CLU]ï¼ˆ81.5%ï¼‰ï¼Œåœ¨å¬å›æ–¹é¢è¶…è¿‡äº†CCï¼ˆ+12.5%ï¼Œp&#x3D;0.015ï¼‰ï¼Œå¹¶åœ¨ç²¾åº¦æ–¹é¢è¶…è¿‡äº†ACLSï¼ˆ+31.2%ï¼Œp&#x3D;0.003ï¼‰ã€‚é€šè¿‡ç»“åˆä¸¥æ ¼çš„å®šä¹‰ã€æ–°çš„CLUæ„ŸçŸ¥æŒ‡æ ‡ã€å¯å†ç°çš„è¯„ä¼°æ¡†æ¶å’Œé¦–ä¸ªä¸“ç”¨çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºå¤šå‘æ€§ç¡¬åŒ–ç—‡çš„ç—…å˜å®ä¾‹åˆ†å‰²å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22537v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„MRIå›¾åƒï¼Œç²¾ç¡®åˆ°ç—…å˜çº§åˆ«çš„åˆ†å‰²å¯¹äºè¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä»·å®è·µä¸»è¦ä¾èµ–äºè¿é€šç»„ä»¶ï¼ˆCCï¼‰è¿›è¡Œè¯­ä¹‰åˆ†å‰²åå¤„ç†ï¼Œæ— æ³•åˆ†ç¦»èåˆç—…ç¶ï¼ˆèšé›†çš„è¿ç»­ç—…ç¶å•ä½ï¼ŒCLUï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€ä¸ä¸´åºŠéœ€æ±‚çš„ä¸åŒ¹é…ï¼Œæœ¬æ–‡å¼•å…¥äº†CLUçš„æ­£å¼å®šä¹‰å’Œç›¸å…³CLUæ„ŸçŸ¥æ£€æµ‹æŒ‡æ ‡ï¼Œå¹¶å°†å…¶çº³å…¥è¯¦å°½çš„å®ä¾‹åˆ†å‰²è¯„ä¼°æ¡†æ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬å¯¹åŸºäºè¿é€šç»„ä»¶å’Œè‡ªåŠ¨èåˆåˆ†å‰²ï¼ˆACLSï¼‰çš„å®ä¾‹åˆ†å‰²æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè¯„ä»·ã€‚åˆ†æè¡¨æ˜ï¼ŒCCæŒç»­ä½ä¼°CLUè®¡æ•°ï¼Œè€ŒACLSå€¾å‘äºè¿‡åº¦åˆ†å‰²ç—…ç¶ï¼Œå¯¼è‡´ç—…ç¶è®¡æ•°è¿‡é«˜å’Œç²¾åº¦é™ä½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ConfLUNetï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹MSç—…ç¶çš„ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²æ¡†æ¶ã€‚ConfLUNetä»å•ä¸ªFLAIRå›¾åƒä¸­è”åˆä¼˜åŒ–ç—…ç¶æ£€æµ‹å’Œå‹¾å‹’ã€‚åœ¨50åæ‚£è€…ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒConfLUNetåœ¨ä¿ç•™çš„æµ‹è¯•é›†ï¼ˆn&#x3D;13ï¼‰ä¸Šçš„å®ä¾‹åˆ†å‰²å’Œç—…ç¶æ£€æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºCCå’ŒACLSï¼ˆå…¨æ™¯è´¨é‡ï¼š42.0% vs. 37.5%&#x2F;36.8%ï¼›p &#x3D; 0.017&#x2F;0.005ï¼›F1ï¼š67.3% vs. 61.6%&#x2F;59.9%ï¼›p &#x3D; 0.028&#x2F;0.013ï¼‰ã€‚å¯¹äºCLUæ£€æµ‹ï¼ŒConfLUNetå®ç°äº†æœ€é«˜çš„F1[CLU]ï¼ˆ81.5%ï¼‰ï¼Œåœ¨å¬å›ç‡ä¸Šè¾ƒCCæé«˜äº†ï¼ˆ+12.5%ï¼Œp &#x3D; 0.015ï¼‰ï¼Œå¹¶åœ¨ç²¾åº¦ä¸Šè¾ƒACLSæé«˜äº†ï¼ˆ+31.2%ï¼Œp &#x3D; 0.003ï¼‰ã€‚é€šè¿‡ç»“åˆä¸¥æ ¼å®šä¹‰ã€æ–°çš„CLUæ„ŸçŸ¥æŒ‡æ ‡ã€å¯é‡å¤çš„è¯„ä»·æ¡†æ¶å’Œé¦–ä¸ªç«¯åˆ°ç«¯çš„ä¸“ç”¨æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºMSçš„ç—…å˜å®ä¾‹åˆ†å‰²å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>å‡†ç¡®è¿›è¡ŒMRIçš„ç—…å˜çº§åˆ«åˆ†å‰²å¯¹MSçš„è¯Šæ–­ã€é¢„åå’Œç–¾ç—…ç›‘æµ‹éå¸¸é‡è¦ã€‚</li>
<li>å½“å‰çš„è¯„ä»·æ–¹æ³•ä¸»è¦ä¾èµ–è¿é€šç»„ä»¶è¿›è¡Œåå¤„ç†ï¼Œæ— æ³•æœ‰æ•ˆåˆ†ç¦»èåˆç—…ç¶ã€‚</li>
<li>æ–‡ä¸­æå‡ºäº†CLUçš„æ­£å¼å®šä¹‰å’Œç›¸å…³æ£€æµ‹æŒ‡æ ‡ï¼Œå¡«è¡¥äº†è¿™ä¸€ä¸´åºŠéœ€æ±‚çš„ç¼ºå£ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†åŸºäºè¿é€šç»„ä»¶å’Œè‡ªåŠ¨èåˆåˆ†å‰²çš„å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œå‘ç°å®ƒä»¬å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ConfLUNetï¼Œä¸€ä¸ªé’ˆå¯¹MSç—…ç¶çš„ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œä¼˜åŒ–äº†ç—…ç¶æ£€æµ‹å’Œå‹¾å‹’ã€‚</li>
<li>ConfLUNetåœ¨å®ä¾‹åˆ†å‰²ã€ç—…ç¶æ£€æµ‹å’ŒCLUæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea288273904119d417d16aaca76f63eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf79eb7b33b69d1b411bcf0f30d2cd5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf0b319fb695934d809adb4bdef3c31b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cascaded-3D-Diffusion-Models-for-Whole-body-3D-18-F-FDG-PET-CT-synthesis-from-Demographics"><a href="#Cascaded-3D-Diffusion-Models-for-Whole-body-3D-18-F-FDG-PET-CT-synthesis-from-Demographics" class="headerlink" title="Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET&#x2F;CT synthesis   from Demographics"></a>Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET&#x2F;CT synthesis   from Demographics</h2><p><strong>Authors:Siyeop Yoon, Sifan Song, Pengfei Jin, Matthew Tivnan, Yujin Oh, Sekeun Kim, Dufan Wu, Xiang Li, Quanzheng Li</strong></p>
<p>We propose a cascaded 3D diffusion model framework to synthesize high-fidelity 3D PET&#x2F;CT volumes directly from demographic variables, addressing the growing need for realistic digital twins in oncologic imaging, virtual trials, and AI-driven data augmentation. Unlike deterministic phantoms, which rely on predefined anatomical and metabolic templates, our method employs a two-stage generative process. An initial score-based diffusion model synthesizes low-resolution PET&#x2F;CT volumes from demographic variables alone, providing global anatomical structures and approximate metabolic activity. This is followed by a super-resolution residual diffusion model that refines spatial resolution. Our framework was trained on 18-F FDG PET&#x2F;CT scans from the AutoPET dataset and evaluated using organ-wise volume and standardized uptake value (SUV) distributions, comparing synthetic and real data between demographic subgroups. The organ-wise comparison demonstrated strong concordance between synthetic and real images. In particular, most deviations in metabolic uptake values remained within 3-5% of the ground truth in subgroup analysis. These findings highlight the potential of cascaded 3D diffusion models to generate anatomically and metabolically accurate PET&#x2F;CT images, offering a robust alternative to traditional phantoms and enabling scalable, population-informed synthetic imaging for clinical and research applications. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§çº§è”çš„3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç›´æ¥ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆé«˜ä¿çœŸåº¦çš„3D PET&#x2F;CTä½“ç§¯å›¾åƒï¼Œä»è€Œæ»¡è¶³è‚¿ç˜¤æˆåƒã€è™šæ‹Ÿè¯•éªŒå’Œäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ•°æ®å¢å¼ºä¸­å¯¹ç°å®æ•°å­—åŒèƒèƒçš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚ä¸åŒäºä¾èµ–äºé¢„å…ˆå®šä¹‰çš„è§£å‰–å­¦å’Œä»£è°¢æ¨¡æ¿çš„ç¡®å®šæ€§å¹»å½±ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ã€‚åˆå§‹çš„åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ä»…ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆä½åˆ†è¾¨ç‡çš„PET&#x2F;CTä½“ç§¯å›¾åƒï¼Œæä¾›å…¨çƒè§£å‰–å­¦ç»“æ„å’Œå¤§è‡´çš„ä»£è°¢æ´»åŠ¨ã€‚æ¥ä¸‹æ¥æ˜¯ç”±è¶…åˆ†è¾¨ç‡æ®‹å·®æ‰©æ•£æ¨¡å‹å¯¹ç©ºé—´åˆ†è¾¨ç‡è¿›è¡Œæ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨AutoPETæ•°æ®é›†çš„18F FDG PET&#x2F;CTæ‰«æä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨å™¨å®˜ä½“ç§¯å’Œæ ‡å‡†åŒ–æ‘„å–å€¼ï¼ˆSUVï¼‰åˆ†å¸ƒè¿›è¡Œè¯„ä¼°ï¼Œæ¯”è¾ƒä¸åŒäººå£ç»Ÿè®¡å­¦äºšç»„çš„åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ã€‚å™¨å®˜å±‚é¢çš„æ¯”è¾ƒè¡¨æ˜åˆæˆå›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´å…·æœ‰å¾ˆå¼ºçš„ä¸€è‡´æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨äºšç»„åˆ†æä¸­ï¼Œå¤§å¤šæ•°ä»£è°¢æ‘„å–å€¼çš„åå·®ä»ä¿æŒåœ¨çœŸå®å€¼çš„3-5%ä»¥å†…ã€‚è¿™äº›å‘ç°çªå‡ºäº†çº§è”3Dæ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆè§£å‰–å­¦å’Œä»£è°¢ä¸Šå‡†ç¡®çš„PET&#x2F;CTå›¾åƒï¼Œä¸ºä¼ ç»Ÿå¹»å½±æä¾›äº†ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶å®ç°äº†ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­å¯ä¼¸ç¼©çš„ã€ä»¥äººç¾¤ä¸ºåŸºç¡€çš„åˆæˆæˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22489v1">PDF</a> MICCAI2025 Submitted version</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§çº§è”çš„3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ç›´æ¥ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆé«˜ä¿çœŸåº¦çš„3D PET&#x2F;CTä½“ç§¯å›¾åƒï¼Œæ»¡è¶³è‚¿ç˜¤æˆåƒã€è™šæ‹Ÿè¯•éªŒå’ŒAIé©±åŠ¨çš„æ•°æ®å¢å¼ºä¸­å¯¹çœŸå®æ•°å­—åŒèƒèƒçš„éœ€æ±‚ã€‚è¯¥ç ”ç©¶é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼Œåˆå§‹çš„åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ä»äººå£ç»Ÿè®¡å­¦å˜é‡åˆæˆä½åˆ†è¾¨ç‡çš„PET&#x2F;CTä½“ç§¯å›¾åƒï¼Œæä¾›å…¨çƒè§£å‰–ç»“æ„å’Œå¤§è‡´çš„ä»£è°¢æ´»åŠ¨ï¼Œéšåæ˜¯è¶…åˆ†è¾¨ç‡å‰©ä½™æ‰©æ•£æ¨¡å‹ï¼Œæé«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚è¯¥æ¡†æ¶åœ¨AutoPETæ•°æ®é›†ä¸Šçš„18-F FDG PET&#x2F;CTæ‰«æä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡å™¨å®˜ä½“ç§¯å’Œæ ‡å‡†æ‘„å–å€¼ï¼ˆSUVï¼‰åˆ†å¸ƒè¿›è¡Œè¯„ä¼°ï¼Œæ¯”è¾ƒåˆæˆæ•°æ®ä¸ä¸åŒäººå£ç»Ÿè®¡å­¦ç‰¹å¾å°ç»„çš„å®é™…æ•°æ®ã€‚åˆæˆå›¾åƒä¸å®é™…å›¾åƒä¹‹é—´çš„å™¨å®˜æ¯”è¾ƒè¡¨ç°å‡ºå¼ºçƒˆçš„ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°ç»„åˆ†æä¸­ï¼Œå¤§å¤šæ•°ä»£è°¢æ‘„å–å€¼çš„åå·®ä¿æŒåœ¨çœŸå®å€¼çš„3-5%ä»¥å†…ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†çº§è”3Dæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§£å‰–å’Œä»£è°¢å‡†ç¡®çš„PET&#x2F;CTå›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§çº§è”çš„3Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œèƒ½ç›´æ¥åˆæˆé«˜ä¿çœŸåº¦çš„3D PET&#x2F;CTä½“ç§¯å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæ»¡è¶³è‚¿ç˜¤æˆåƒã€è™šæ‹Ÿè¯•éªŒå’ŒAIé©±åŠ¨çš„æ•°æ®å¢å¼ºä¸­å¯¹çœŸå®æ•°å­—åŒèƒèƒçš„éœ€æ±‚ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼šåˆå§‹é˜¶æ®µåˆæˆä½åˆ†è¾¨ç‡å›¾åƒï¼Œæä¾›å…¨çƒè§£å‰–ç»“æ„å’Œå¤§è‡´ä»£è°¢æ´»åŠ¨ï¼›è¶…åˆ†è¾¨ç‡é˜¶æ®µæé«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚</li>
<li>æ¡†æ¶åœ¨AutoPETæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨å™¨å®˜ä½“ç§¯å’Œæ ‡å‡†åŒ–æ‘„å–å€¼ï¼ˆSUVï¼‰åˆ†å¸ƒè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åˆæˆå›¾åƒä¸å®é™…å›¾åƒä¹‹é—´çš„å™¨å®˜æ¯”è¾ƒè¡¨ç°å‡ºå¼ºçƒˆçš„ä¸€è‡´æ€§ã€‚</li>
<li>çº§è”3Dæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè§£å‰–å’Œä»£è°¢å‡†ç¡®çš„PET&#x2F;CTå›¾åƒæ–¹é¢å…·æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-282cdeeb8931a6ae49ba0f02d708f4c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f262c057354c0cd219b81044d630a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98f7ee32a48acaf45986c1e3ace9583.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Single-Domain-Generalization-for-Alzheimerâ€™s-Detection-from-3D-MRIs-with-Pseudo-Morphological-Augmentations-and-Contrastive-Learning"><a href="#Single-Domain-Generalization-for-Alzheimerâ€™s-Detection-from-3D-MRIs-with-Pseudo-Morphological-Augmentations-and-Contrastive-Learning" class="headerlink" title="Single Domain Generalization for Alzheimerâ€™s Detection from 3D MRIs with   Pseudo-Morphological Augmentations and Contrastive Learning"></a>Single Domain Generalization for Alzheimerâ€™s Detection from 3D MRIs with   Pseudo-Morphological Augmentations and Contrastive Learning</h2><p><strong>Authors:Zobia Batool, Huseyin Ozkan, Erchan Aptoula</strong></p>
<p>Although Alzheimerâ€™s disease detection via MRIs has advanced significantly thanks to contemporary deep learning models, challenges such as class imbalance, protocol variations, and limited dataset diversity often hinder their generalization capacity. To address this issue, this article focuses on the single domain generalization setting, where given the data of one domain, a model is designed and developed with maximal performance w.r.t. an unseen domain of distinct distribution. Since brain morphology is known to play a crucial role in Alzheimerâ€™s diagnosis, we propose the use of learnable pseudo-morphological modules aimed at producing shape-aware, anatomically meaningful class-specific augmentations in combination with a supervised contrastive learning module to extract robust class-specific representations. Experiments conducted across three datasets show improved performance and generalization capacity, especially under class imbalance and imaging protocol variations. The source code will be made available upon acceptance at <a target="_blank" rel="noopener" href="https://github.com/zobia111/SDG-Alzheimer">https://github.com/zobia111/SDG-Alzheimer</a>. </p>
<blockquote>
<p>å°½ç®¡å½“ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é€šè¿‡æ ¸ç£å…±æŒ¯æˆåƒæ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç±»ä¸å¹³è¡¡ã€åè®®å˜åŒ–å’Œæ•°æ®é›†å¤šæ ·æ€§æœ‰é™ç­‰æŒ‘æˆ˜å¸¸å¸¸é˜»ç¢å…¶æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡ç€é‡äºå•ä¸€åŸŸæ³›åŒ–è®¾ç½®ï¼Œé’ˆå¯¹ä¸€ä¸ªåŸŸçš„æ•°æ®ï¼Œè®¾è®¡ä¸€ä¸ªæ¨¡å‹ï¼Œå…³äºæœªè§åŸŸçš„åˆ†å¸ƒå·®å¼‚ï¼Œå®ç°æœ€ä½³æ€§èƒ½ã€‚ç”±äºå·²çŸ¥å¤§è„‘å½¢æ€åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¯Šæ–­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨å¯å­¦ä¹ çš„ä¼ªå½¢æ€æ¨¡å—ï¼Œæ—¨åœ¨äº§ç”Ÿå½¢çŠ¶æ„ŸçŸ¥ã€è§£å‰–ä¸Šæ„ä¹‰é‡å¤§çš„ç±»ç‰¹å¼‚æ€§å¢å¼ºï¼Œå¹¶ç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œä»¥æå–ç¨³å¥çš„ç±»ç‰¹å¼‚æ€§è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»ä¸å¹³è¡¡å’Œæˆåƒåè®®å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æœ‰æ‰€æé«˜ã€‚æºä»£ç å°†åœ¨æ¥å—åå‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zobia111/SDG-Alzheimer%E3%80%82">https://github.com/zobia111/SDG-Alzheimerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22465v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½“ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é€šè¿‡MRIæ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ç±»ä¸å¹³è¡¡ã€åè®®å˜åŒ–å’Œæœ‰é™æ•°æ®é›†å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡å…³æ³¨å•åŸŸæ³›åŒ–è®¾ç½®ï¼Œæ—¨åœ¨è®¾è®¡ä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨æœªè§è¿‡çš„ä¸åŒé¢†åŸŸæ•°æ®ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ç ”ç©¶é‡‡ç”¨å¯å­¦ä¹ çš„ä¼ªå½¢æ€æ¨¡å—ï¼Œäº§ç”Ÿå½¢çŠ¶æ„ŸçŸ¥ã€è§£å‰–æ„ä¹‰æ˜ç¡®çš„ç±»ç‰¹å®šå¢å¼ºï¼Œç»“åˆç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å—ï¼Œæå–ç¨³å¥çš„ç±»ç‰¹å®šè¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç±»ä¸å¹³è¡¡å’Œæˆåƒåè®®å˜åŒ–çš„æƒ…å†µä¸‹å…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨MRIæ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç±»ä¸å¹³è¡¡ã€åè®®å˜åŒ–å’Œæœ‰é™æ•°æ®é›†å¤šæ ·æ€§ä»æ˜¯é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å…³æ³¨å•åŸŸæ³›åŒ–è®¾ç½®ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨æœªè§è¿‡é¢†åŸŸæ•°æ®ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä¼ªå½¢æ€æ¨¡å—ç”¨äºäº§ç”Ÿå½¢çŠ¶æ„ŸçŸ¥ã€è§£å‰–æ„ä¹‰æ˜ç¡®çš„ç±»ç‰¹å®šå¢å¼ºã€‚</li>
<li>ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å—ç”¨äºæå–ç¨³å¥çš„ç±»ç‰¹å®šè¡¨ç¤ºã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ff6c430db7c6b47b2cbd3791632447b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc83ad7e8f6b6e8f8bfd6bcb6ad711c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-845cb578604ab0464229952618f382b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e7815ecf4da49216c74c947f4a8ef96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daafcb5d1be4d81d86852d73086cef51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03d6f7eaa7f57a6483fe1970080bccc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-518b3995fb13ba6ce29c0c8a16890286.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CADReview-Automatically-Reviewing-CAD-Programs-with-Error-Detection-and-Correction"><a href="#CADReview-Automatically-Reviewing-CAD-Programs-with-Error-Detection-and-Correction" class="headerlink" title="CADReview: Automatically Reviewing CAD Programs with Error Detection and   Correction"></a>CADReview: Automatically Reviewing CAD Programs with Error Detection and   Correction</h2><p><strong>Authors:Jiali Chen, Xusen Hei, HongFei Liu, Yuancheng Wei, Zikun Deng, Jiayuan Xie, Yi Cai, Li Qing</strong></p>
<p>Computer-aided design (CAD) is crucial in prototyping 3D objects through geometric instructions (i.e., CAD programs). In practical design workflows, designers often engage in time-consuming reviews and refinements of these prototypes by comparing them with reference images. To bridge this gap, we introduce the CAD review task to automatically detect and correct potential errors, ensuring consistency between the constructed 3D objects and reference images. However, recent advanced multimodal large language models (MLLMs) struggle to recognize multiple geometric components and perform spatial geometric operations within the CAD program, leading to inaccurate reviews. In this paper, we propose the CAD program repairer (ReCAD) framework to effectively detect program errors and provide helpful feedback on error correction. Additionally, we create a dataset, CADReview, consisting of over 20K program-image pairs, with diverse errors for the CAD review task. Extensive experiments demonstrate that our ReCAD significantly outperforms existing MLLMs, which shows great potential in design applications. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨é€šè¿‡å‡ ä½•æŒ‡ä»¤ï¼ˆå³CADç¨‹åºï¼‰åˆ›å»º3Då¯¹è±¡åŸå‹æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨å®é™…è®¾è®¡æµç¨‹ä¸­ï¼Œè®¾è®¡å¸ˆç»å¸¸éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´å¯¹æ¯”å‚è€ƒå›¾åƒæ¥å®¡æŸ¥å’Œä¿®æ”¹è¿™äº›åŸå‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CADå®¡æŸ¥ä»»åŠ¡ï¼Œä»¥è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£æ½œåœ¨é”™è¯¯ï¼Œç¡®ä¿æ„å»ºçš„3Då¯¹è±¡ä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œæœ€è¿‘å…ˆè¿›çš„è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨CADç¨‹åºä¸­è¯†åˆ«å¤šä¸ªå‡ ä½•ç»„ä»¶å¹¶æ‰§è¡Œç©ºé—´å‡ ä½•æ“ä½œæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´å®¡æŸ¥ç»“æœä¸å‡†ç¡®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CADç¨‹åºä¿®å¤å™¨ï¼ˆReCADï¼‰æ¡†æ¶ï¼Œä»¥æœ‰æ•ˆåœ°æ£€æµ‹ç¨‹åºé”™è¯¯å¹¶æä¾›æœ‰å…³é”™è¯¯çº æ­£çš„æœ‰ç”¨åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡2ä¸‡ç»„ç¨‹åºå›¾åƒå¯¹çš„CADReviewæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å„ç§é”™è¯¯ï¼Œç”¨äºCADå®¡æŸ¥ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ReCADåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰MLLMsï¼Œåœ¨è®¾è®¡åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22304v1">PDF</a> ACL 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>CADç¨‹åºå®¡æŸ¥ä»»åŠ¡å¯¹äºè‡ªåŠ¨æ£€æµ‹å¹¶çº æ­£æ½œåœ¨é”™è¯¯è‡³å…³é‡è¦ï¼Œå¯ç¡®ä¿æ„å»ºçš„3Då¯¹è±¡ä¸å‚è€ƒå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CADç¨‹åºä¸­éš¾ä»¥è¯†åˆ«å¤šä¸ªå‡ ä½•ç»„ä»¶å¹¶æ‰§è¡Œç©ºé—´å‡ ä½•æ“ä½œï¼Œå¯¼è‡´å®¡æŸ¥ä¸å‡†ç¡®ã€‚æœ¬æ–‡æå‡ºçš„CADç¨‹åºä¿®å¤å™¨ï¼ˆReCADï¼‰æ¡†æ¶èƒ½æœ‰æ•ˆæ£€æµ‹ç¨‹åºé”™è¯¯ï¼Œå¹¶æä¾›æœ‰åŠ©äºé”™è¯¯çº æ­£çš„åé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADåœ¨è®¾è®¡3Då¯¹è±¡åŸå‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œé€šè¿‡å‡ ä½•æŒ‡ä»¤åˆ›å»ºæ¨¡å‹ã€‚</li>
<li>è®¾è®¡å¸ˆåœ¨æµç¨‹ä¸­éœ€èŠ±è´¹å¤§é‡æ—¶é—´å¯¹æ¯”å‚è€ƒå›¾åƒå¯¹åŸå‹è¿›è¡Œå®¡æŸ¥å’Œä¿®æ”¹ã€‚</li>
<li>CADå®¡æŸ¥ä»»åŠ¡æ—¨åœ¨è‡ªåŠ¨æ£€æµ‹å’Œçº æ­£æ½œåœ¨é”™è¯¯ï¼Œç¡®ä¿3Då¯¹è±¡ä¸å‚è€ƒå›¾åƒä¸€è‡´ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CADç¨‹åºä¸­å­˜åœ¨è¯†åˆ«å‡ ä½•ç»„ä»¶å’Œæ‰§è¡Œç©ºé—´æ“ä½œçš„å›°éš¾ï¼Œå¯¼è‡´å®¡æŸ¥ä¸å‡†ç¡®ã€‚</li>
<li>ReCADæ¡†æ¶èƒ½æœ‰æ•ˆæ£€æµ‹CADç¨‹åºé”™è¯¯ï¼Œå¹¶æä¾›çº æ­£åé¦ˆã€‚</li>
<li>åˆ›å»ºäº†åŒ…å«20Kä»¥ä¸Šç¨‹åº-å›¾åƒå¯¹çš„CADReviewæ•°æ®é›†ï¼Œç”¨äºCADå®¡æŸ¥ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1c6d54c4c4eedc894ad0fa7cae9ccf5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1ad0da511a16bc11763882be7cccbaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52ff0d3133ce6e495fb03dd1c79dec78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fd2b31b32000063de1566e369fe824b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Connecting-the-m-dots-accretion-rates-and-thermonuclear-burst-recurrence-times-on-neutron-stars-and-white-dwarfs"><a href="#Connecting-the-m-dots-accretion-rates-and-thermonuclear-burst-recurrence-times-on-neutron-stars-and-white-dwarfs" class="headerlink" title="Connecting the m-dots: accretion rates and thermonuclear burst   recurrence times on neutron stars and white dwarfs"></a>Connecting the m-dots: accretion rates and thermonuclear burst   recurrence times on neutron stars and white dwarfs</h2><p><strong>Authors:Triantafyllos Kormpakis, Manuel Linares, Jordi JosÃ©</strong></p>
<p>We present a compilation of observed recurrence times ($t_{\rm rec}$) and infer the corresponding local mass-accretion rates ($\dot m$) for type I X-ray bursts, milliHertz quasi-periodic oscillating sources and recurrent novae eruptions. We construct models of the $t_{\rm rec}-\dot m$ relation for accreting white dwarfs and neutron stars and find that both are roughly consistent with a global inverse linear relation, connecting for the first time thermonuclear runaways on neutron stars and white dwarfs. We find that theoretical models of pure He bursts are in agreement with the best $t_{\rm rec}$ measurements in ultra-compact X-ray binaries at low $\dot m$ (4U<del>$0614+09$ and 2S</del>0918-549).   We suggest that the transient Z source XTE~J1701-462 is a slow rotator, based on its mHz QPO properties. Finally, we discuss the implications for thermonuclear ignition and point out that the difference in eruption&#x2F;burst energy ($E_{b_{WD}}&#x2F;E_{b_{NS}}&#x3D;2\times 10^4$) is consistent with the difference in area between neutron stars and white dwarfs $\left((R_{WD}&#x2F;R_{NS})^2&#x3D;4\times 10^4\right)$. We conclude that ignitions of thermonuclear shell flashes on neutron stars and white dwarfs depend primarily on the specific mass accretion rate and do not depend on the nature of the underlying compact object. </p>
<blockquote>
<p>æˆ‘ä»¬æ±‡æ€»äº†è§‚å¯Ÿåˆ°çš„å¤å‘æ—¶é—´ï¼ˆ$t_{\rm rec}$ï¼‰ï¼Œå¹¶æ¨æ–­å‡ºç›¸åº”å±€éƒ¨è´¨é‡å¢ç‡ï¼ˆ$\dot m$ï¼‰å¯¹äºIå‹Xå°„çº¿çˆ†å‘ã€æ¯«èµ«å…¹å‡†å‘¨æœŸæŒ¯è¡æºä»¥åŠå¤å‘æ€§æ–°æ˜Ÿçˆ†å‘ã€‚æˆ‘ä»¬æ„å»ºäº†é€‚ç”¨äºç´¯ç§¯ç™½çŸ®æ˜Ÿå’Œä¸­å­æ˜Ÿçš„ç™½çŸ®æ˜Ÿå’Œä¸­å­æ˜Ÿçš„$t_{\rm rec}-\dot m$å…³ç³»æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬å¤§è‡´ç¬¦åˆå…¨å±€é€†çº¿æ€§å…³ç³»ï¼Œé¦–æ¬¡å°†ä¸­å­æ˜Ÿå’Œç™½çŸ®æ˜Ÿä¸Šçš„çƒ­æ ¸å¤±æ§è”ç³»åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬å‘ç°çº¯Heçˆ†å‘çš„ç†è®ºæ¨¡å‹ä¸è¶…ä½ç´§å‡‘å‹Xå°„çº¿åŒæ˜Ÿåœ¨ä½$\dot m$æ—¶çš„æœ€ä½³$t_{\rm rec}$æµ‹é‡ç»“æœï¼ˆä¾‹å¦‚ï¼Œé’ˆå¯¹è¶…ä½è´¨é‡ä¸­å­æ˜Ÿå¯¹åº”çš„éå…‰å­¦å½¢æ€æ ‡å¤´åˆ†ç¦»ç‚¹çš„åå°„å…‰çº¿ç³»ç»Ÿå’ŒAMçš„ä¸»è¦ç³»åˆ—å¯¹è±¡4U<del>0614+09å’ŒXå°„çº¿ç¬æ€2S</del>0918-549ä¸­çš„ä¸­æ€§ç”µå­èƒ½å¹³è¡¡åŠ¨åŠ›å­¦è¿›è¡Œçš„éªŒè¯æ¨¡å‹ä¸ç†è®ºè®¡ç®—ç»“æœå»åˆå¾—å¾ˆå¥½ã€‚æ ¹æ®å…¶mHz QPOç‰¹æ€§ï¼Œæˆ‘ä»¬å»ºè®®å°†æš‚æ€ZæºXTE J å¹¶ä¸æ˜¯æš—èƒŒæ™¯å¤–çš„æ”¾å°„ç‰¹ç‚¹ä¸åŒçš„ç±»åˆ«æ¯”è¾ƒä¸­å¿ƒé€šè¿‡å½“å‰çš„åº”ç”¨åˆ†æçš„ç‡ƒçƒ§åˆ†å¸ƒè¿ç»­å‡ºç°çš„é¢„æµ‹æ¯”å¯¹Xå°„çº¿å«æ˜Ÿæœ€æ–°è·å¾—çš„å¤§å‹è¿‘ç¼˜å…±æŒ¯æ©ç çš„å»¶æ—¶æ¼”å˜æµ‹å®šè¡¨ç¤ºç ”ç©¶å¦‚ä½•è‚¯å®šè¿™ä¸æ˜¯ä¸¤ç§å¹¶ä¸ä¸æ¶ˆå¤±çš„æ¨¡æ€è´¨é‡é‡å ç†”åŒ–æ‹¥æœ‰ç‹¬é—¨çš„å¼¯æ›²è¾ç…§åˆ†æçš„è°ƒæ•´ç”¨æˆ·å†…æ ¸ç­‰å€¼äºå…·æœ‰æ…¢æ—‹è½¬ç‰¹æ€§çš„å¤©ä½“ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†çƒ­æ ¸ç‚¹ç«çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºçˆ†å‘æˆ–çˆ†å‘èƒ½é‡å·®å¼‚ï¼ˆ$E_{b_{WD}}&#x2F;E_{b_{NS}}&#x3D;2\times 10^4$ï¼‰ä¸ä¸­å­æ˜Ÿå’Œç™½çŸ®æ˜Ÿä¹‹é—´çš„é¢ç§¯å·®å¼‚æ˜¯ä¸€è‡´çš„$\left((R_{WD}&#x2F;R_{NS})^2&#x3D;4\times 10^4\right)$ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œä¸­å­æ˜Ÿå’Œç™½çŸ®æ˜Ÿä¸Šçš„çƒ­æ ¸å£³å±‚é—ªå…‰ä¸»è¦ä¾èµ–äºç‰¹å®šçš„è´¨é‡å¢ç‡ï¼Œè€Œä¸ä¾èµ–äºåŸºç¡€è‡´å¯†å¤©ä½“çš„æ€§è´¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22302v1">PDF</a> 15 pages, 4 figures, Accepted in MNRAS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸åŒç±»å‹Xå°„çº¿çˆ†å‘çš„å¤å‘æ—¶é—´ï¼ˆ$t_{\rm rec}$ï¼‰å’Œç›¸åº”çš„å±€éƒ¨è´¨é‡å¢ç‡ï¼ˆ$\dot m$ï¼‰çš„å…³ç³»ï¼Œæ„å»ºäº†é€‚ç”¨äºç™½çŸ®æ˜Ÿå’Œä¸­å­æ˜Ÿçš„$t_{\rm rec}-\dot m$å…³ç³»æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ä¸¤è€…å‡ç¬¦åˆå…¨å±€é€†çº¿æ€§å…³ç³»ï¼Œé¦–æ¬¡å°†ä¸­å­æ˜Ÿå’Œç™½çŸ®æ˜Ÿä¸Šçš„çƒ­æ ¸é€ƒé€¸è”ç³»èµ·æ¥ã€‚ç†è®ºæ¨¡å‹ä¸è¶…ç´§å‡‘Xå°„çº¿åŒæ˜Ÿåœ¨ä½$\dot m$ä¸‹çš„æœ€ä½³$t_{\rm rec}$æµ‹é‡å€¼ä¸€è‡´ã€‚åŸºäºmHz QPOç‰¹æ€§ï¼Œæˆ‘ä»¬è®¤ä¸ºç¬æ€ZæºXTE J1701-462æ˜¯ä¸€é¢—æ…¢è‡ªè½¬æ˜Ÿã€‚æœ€åï¼Œæœ¬æ–‡è®¨è®ºäº†çƒ­æ ¸ç‚¹ç«çš„å«ä¹‰ï¼ŒæŒ‡å‡ºçˆ†å‘&#x2F;çˆ†å‘èƒ½é‡å·®å¼‚ï¼ˆ$E_{b_{WD}}&#x2F;E_{b_{NS}}&#x3D;2\times 10^4$ï¼‰ä¸ç™½çŸ®æ˜Ÿå’Œä¸­å­æ˜Ÿé¢ç§¯å·®å¼‚ä¸€è‡´ã€‚æ€»ç»“è®¤ä¸ºï¼Œä¸­å­æ˜Ÿå’Œç™½çŸ®æ˜Ÿä¸Šçš„çƒ­æ ¸å£³é—ªå…‰ç‚¹ç‡ƒä¸»è¦å–å†³äºç‰¹å®šè´¨é‡å¢ç‡ï¼Œè€Œä¸ä¾èµ–äºåº•å±‚è‡´å¯†å¤©ä½“çš„æ€§è´¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§‚å¯Ÿåˆ°ä¸åŒç±»å‹Xå°„çº¿çˆ†å‘çš„å¤å‘æ—¶é—´å’Œå±€éƒ¨è´¨é‡å¢ç‡ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ„å»ºé€‚ç”¨äºç™½çŸ®æ˜Ÿå’Œä¸­å­æ˜Ÿçš„$t_{\rm rec}-\dot m$å…³ç³»æ¨¡å‹ï¼Œå‘ç°ä¸¤è€…å‡ç¬¦åˆå…¨å±€é€†çº¿æ€§å…³ç³»ã€‚</li>
<li>ç†è®ºæ¨¡å‹ä¸è¶…ç´§å‡‘Xå°„çº¿åŒæ˜Ÿåœ¨ä½è´¨é‡å¢ç‡ä¸‹çš„æœ€ä½³å¤å‘æ—¶é—´æµ‹é‡å€¼ä¸€è‡´ã€‚</li>
<li>åŸºäºmHz QPOç‰¹æ€§ï¼Œæå‡ºç¬æ€ZæºXTE J1701-462æ˜¯ä¸€é¢—æ…¢è‡ªè½¬æ˜Ÿã€‚</li>
<li>è®¨è®ºäº†çƒ­æ ¸ç‚¹ç«çš„å«ä¹‰ï¼Œå¹¶æŒ‡å‡ºçˆ†å‘èƒ½é‡å·®å¼‚ä¸å¤©ä½“é¢ç§¯å·®å¼‚æœ‰å…³ã€‚</li>
<li>çƒ­æ ¸å£³é—ªå…‰ç‚¹ç‡ƒä¸»è¦å–å†³äºç‰¹å®šè´¨é‡å¢ç‡ï¼Œè€Œä¸æ˜¯åº•å±‚è‡´å¯†å¤©ä½“çš„æ€§è´¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ee6eb6a809bf32bb00d79e83ade1ff3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a807d7822c6165d2ee29a4b416fc253.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Look-Mark-Leveraging-Radiologist-Eye-Fixations-and-Bounding-boxes-in-Multimodal-Large-Language-Models-for-Chest-X-ray-Report-Generation"><a href="#Look-Mark-Leveraging-Radiologist-Eye-Fixations-and-Bounding-boxes-in-Multimodal-Large-Language-Models-for-Chest-X-ray-Report-Generation" class="headerlink" title="Look &amp; Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in   Multimodal Large Language Models for Chest X-ray Report Generation"></a>Look &amp; Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in   Multimodal Large Language Models for Chest X-ray Report Generation</h2><p><strong>Authors:Yunsoo Kim, Jinge Wu, Su-Hwan Kim, Pardeep Vasudev, Jiashu Shen, Honghan Wu</strong></p>
<p>Recent advancements in multimodal Large Language Models (LLMs) have significantly enhanced the automation of medical image analysis, particularly in generating radiology reports from chest X-rays (CXR). However, these models still suffer from hallucinations and clinically significant errors, limiting their reliability in real-world applications. In this study, we propose Look &amp; Mark (L&amp;M), a novel grounding fixation strategy that integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into the LLM prompting framework. Unlike conventional fine-tuning, L&amp;M leverages in-context learning to achieve substantial performance gains without retraining. When evaluated across multiple domain-specific and general-purpose models, L&amp;M demonstrates significant gains, including a 1.2% improvement in overall metrics (A.AVG) for CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for LLaVA-Med. General-purpose models also benefit from L&amp;M combined with in-context learning, with LLaVA-OV achieving an 87.3% clinical average performance (C.AVG)-the highest among all models, even surpassing those explicitly trained for CXR report generation. Expert evaluations further confirm that L&amp;M reduces clinically significant errors (by 0.43 average errors per report), such as false predictions and omissions, enhancing both accuracy and reliability. These findings highlight L&amp;Mâ€™s potential as a scalable and efficient solution for AI-assisted radiology, paving the way for improved diagnostic workflows in low-resource clinical settings. </p>
<blockquote>
<p>æœ€è¿‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨åŒ»å­¦å›¾åƒåˆ†æè‡ªåŠ¨åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰ç”ŸæˆæŠ¥å‘Šæ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶å­˜åœ¨ç€è™šæ„å’Œä¸´åºŠé‡è¦é”™è¯¯çš„é—®é¢˜ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯é æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Look &amp; Markï¼ˆL&amp;Mï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¥åœ°å›ºå®šç­–ç•¥ï¼Œå®ƒå°†æ”¾å°„ç§‘åŒ»ç”Ÿçœ¼çƒå›ºå®šï¼ˆLookï¼‰å’Œè¾¹ç•Œæ¡†æ³¨é‡Šï¼ˆMarkï¼‰é›†æˆåˆ°LLMæç¤ºæ¡†æ¶ä¸­ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒä¸åŒï¼ŒL&amp;Måˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œåœ¨ä¸è¿›è¡Œå†è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å¤šä¸ªç‰¹å®šé¢†åŸŸå’Œé€šç”¨æ¨¡å‹çš„è¯„ä»·ä¸­ï¼ŒL&amp;Mè¡¨ç°å‡ºäº†æ˜¾è‘—çš„å¢ç›Šï¼Œå…¶ä¸­CXR-LLaVAçš„æ•´ä½“æŒ‡æ ‡ï¼ˆA.AVGï¼‰ç›¸æ¯”åŸºçº¿æç¤ºæé«˜äº†1.2%ï¼ŒLLaVA-Medæé«˜äº†9.2%çš„æ˜¾è‘—å¢é•¿ã€‚é€šç”¨æ¨¡å‹ä¹Ÿå—ç›ŠäºL&amp;Mä¸ä¸Šä¸‹æ–‡å­¦ä¹ çš„ç»“åˆï¼ŒLLaVA-OVçš„ä¸´åºŠå¹³å‡æ€§èƒ½ï¼ˆC.AVGï¼‰è¾¾åˆ°äº†87.3%ï¼Œæˆä¸ºæ‰€æœ‰æ¨¡å‹ä¸­çš„æœ€é«˜ï¼Œç”šè‡³è¶…è¿‡äº†é‚£äº›ä¸“é—¨è®­ç»ƒç”¨äºCXRæŠ¥å‘Šç”Ÿæˆçš„æ¨¡å‹ã€‚ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒL&amp;Må‡å°‘äº†ä¸´åºŠé‡è¦é”™è¯¯ï¼ˆæ¯ä¸ªæŠ¥å‘Šå¹³å‡å‡å°‘0.43ä¸ªé”™è¯¯ï¼‰ï¼Œå¦‚è¯¯æŠ¥å’Œé—æ¼ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¿™äº›å‘ç°çªå‡ºäº†L&amp;Mä½œä¸ºå¯æ‰©å±•å’Œé«˜æ•ˆçš„AIè¾…åŠ©æ”¾å°„å­¦è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œä¸ºä½èµ„æºä¸´åºŠç¯å¢ƒä¸­çš„æ”¹è¿›è¯Šæ–­å·¥ä½œæµç¨‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22222v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æè‡ªåŠ¨åŒ–æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­ç”ŸæˆæŠ¥å‘Šæ–¹é¢ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨è™šæ„å’Œä¸´åºŠæ˜¾è‘—é”™è¯¯ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„å¯é æ€§åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºLook &amp; Markï¼ˆL&amp;Mï¼‰ç­–ç•¥ï¼Œå°†æ”¾å°„ç§‘åŒ»ç”Ÿçœ¼åŠ¨å’Œè¾¹ç•Œæ¡†æ³¨é‡Šèå…¥LLMæç¤ºæ¡†æ¶ä¸­ã€‚ä¸åŒäºä¼ ç»Ÿå¾®è°ƒï¼ŒL&amp;Måˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒL&amp;Måœ¨å¤šä¸ªç‰¹å®šé¢†åŸŸå’Œé€šç”¨æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸åŸºçº¿æç¤ºç›¸æ¯”ï¼ŒCXR-LLaVAæ•´ä½“æŒ‡æ ‡æ”¹å–„1.2%ï¼ŒLLaVA-Medæå‡æ˜¾è‘—è¾¾9.2%ã€‚é€šç”¨æ¨¡å‹ç»“åˆä¸Šä¸‹æ–‡å­¦ä¹ ä¸L&amp;Måè¡¨ç°æ›´ä½³ï¼ŒLLaVA-OVä¸´åºŠå¹³å‡æ€§èƒ½è¾¾87.3%ï¼Œæˆä¸ºæ‰€æœ‰æ¨¡å‹ä¸­æœ€ä¼˜ï¼Œç”šè‡³è¶…è¶Šä¸“ä¸ºç”ŸæˆCXRæŠ¥å‘Šè®­ç»ƒçš„æ¨¡å‹ã€‚ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒL&amp;Må‡å°‘äº†ä¸´åºŠæ˜¾è‘—é”™è¯¯ï¼ˆå¹³å‡æ¯æŠ¥å‘Šå‡å°‘0.43ä¸ªé”™è¯¯ï¼‰ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†L&amp;Måœ¨äººå·¥æ™ºèƒ½è¾…åŠ©æ”¾å°„å­¦ä¸­çš„æ½œåŠ›ï¼Œä¸ºä½èµ„æºä¸´åºŠç¯å¢ƒä¸­çš„è¯Šæ–­å·¥ä½œæµç¨‹æ”¹è¿›é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†æé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ </li>
<li>å½“å‰æ¨¡å‹å­˜åœ¨çš„è™šæ„å’Œä¸´åºŠé”™è¯¯é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„å¯é æ€§åº”ç”¨ã€‚</li>
<li>Look &amp; Markï¼ˆL&amp;Mï¼‰ç­–ç•¥æ•´åˆäº†æ”¾å°„ç§‘åŒ»ç”Ÿçœ¼åŠ¨å’Œè¾¹ç•Œæ¡†æ³¨é‡Šæ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚ </li>
<li>L&amp;Mé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æå‡æ¨¡å‹è¡¨ç°ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚ </li>
<li>L&amp;Måœ¨ç‰¹å®šé¢†åŸŸå’Œé€šç”¨æ¨¡å‹ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”åŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ã€‚ </li>
<li>L&amp;Mç­–ç•¥å‡å°‘ä¸´åºŠæ˜¾è‘—é”™è¯¯ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ </li>
<li>L&amp;Mç­–ç•¥å…·æœ‰æ½œåŠ›æ”¹å–„ä½èµ„æºä¸´åºŠç¯å¢ƒä¸­çš„è¯Šæ–­å·¥ä½œæµç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49f9a30b1867e36d35375acdf45a1849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef0c5100d2441ba1d4a0b0b752ca298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44b9ea78dfc469cd1709ef4d9b9948fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3dfb09aa06fe5d8190caef6ff558cc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a368832db2e4f3d9d70ef85dd9a6f546.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="High-Volume-Rate-3D-Ultrasound-Reconstruction-with-Diffusion-Models"><a href="#High-Volume-Rate-3D-Ultrasound-Reconstruction-with-Diffusion-Models" class="headerlink" title="High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models"></a>High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</h2><p><strong>Authors:Tristan S. W. Stevens, OisÃ­n Nolan, Oudom Somphone, Jean-Luc Robert, Ruud J. G. van Sloun</strong></p>
<p>Three-dimensional ultrasound enables real-time volumetric visualization of anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the reliance on precise probe orientation, potentially making ultrasound more accessible to clinicians with varying levels of experience and improving automated measurements and post-exam analysis. However, achieving both high volume rates and high image quality remains a significant challenge. While 3D diverging waves can provide high volume rates, they suffer from limited tissue harmonic generation and increased multipath effects, which degrade image quality. One compromise is to retain the focusing in elevation while leveraging unfocused diverging waves in the lateral direction to reduce the number of transmissions per elevation plane. Reaching the volume rates achieved by full 3D diverging waves, however, requires dramatically undersampling the number of elevation planes. Subsequently, to render the full volume, simple interpolation techniques are applied. This paper introduces a novel approach to 3D ultrasound reconstruction from a reduced set of elevation planes by employing diffusion models (DMs) to achieve increased spatial and temporal resolution. We compare both traditional and supervised deep learning-based interpolation methods on a 3D cardiac ultrasound dataset. Our results show that DM-based reconstruction consistently outperforms the baselines in image quality and downstream task performance. Additionally, we accelerate inference by leveraging the temporal consistency inherent to ultrasound sequences. Finally, we explore the robustness of the proposed method by exploiting the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrate improved recall on out-of-distribution data with synthetic anomalies under strong subsampling. </p>
<blockquote>
<p>ä¸‰ç»´è¶…å£°èƒ½å¤Ÿå®ç°è§£å‰–ç»“æ„çš„å®æ—¶ä½“ç§¯å¯è§†åŒ–ã€‚ä¸ä¼ ç»Ÿçš„äºŒç»´è¶…å£°ä¸åŒï¼Œä¸‰ç»´æˆåƒå‡å°‘äº†å¯¹äºç²¾ç¡®æ¢å¤´ä½ç½®çš„ä¾èµ–ï¼Œè¿™ä½¿å¾—ä¸åŒç»éªŒçš„ä¸´åºŠåŒ»ç”Ÿéƒ½èƒ½å¤Ÿæ›´è½»æ¾åœ°æ“ä½œè¶…å£°ï¼Œå¹¶æ”¹å–„äº†è‡ªåŠ¨åŒ–æµ‹é‡å’Œè€ƒè¯•ååˆ†æã€‚ç„¶è€Œï¼Œå®ç°é«˜ä½“ç§¯ç‡å’Œé«˜å›¾åƒè´¨é‡ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ä¸‰ç»´å‘æ•£æ³¢èƒ½å¤Ÿæä¾›é«˜ä½“ç§¯ç‡ï¼Œä½†å®ƒä»¬å­˜åœ¨ç»„ç»‡è°æ³¢ç”Ÿæˆå—é™å’Œå¤šè·¯å¾„æ•ˆåº”å¢åŠ çš„é—®é¢˜ï¼Œä»è€Œé™ä½äº†å›¾åƒè´¨é‡ã€‚ä¸€ç§æŠ˜è¡·æ–¹æ¡ˆæ˜¯åœ¨é«˜åº¦æ–¹å‘ä¸Šä¿æŒèšç„¦ï¼ŒåŒæ—¶åˆ©ç”¨ä¾§å‘çš„æœªèšç„¦å‘æ•£æ³¢æ¥å‡å°‘æ¯ä¸ªé«˜åº¦å¹³é¢çš„ä¼ è¾“æ¬¡æ•°ã€‚ç„¶è€Œï¼Œè¦è¾¾åˆ°å®Œå…¨ä¸‰ç»´å‘æ•£æ³¢æ‰€å®ç°çš„ä½“ç§¯ç‡ï¼Œéœ€è¦å¯¹é«˜åº¦å¹³é¢è¿›è¡Œå¤§å¹…åº¦æ¬ é‡‡æ ·ã€‚éšåï¼Œä¸ºäº†å‘ˆç°æ•´ä¸ªä½“ç§¯ï¼Œé‡‡ç”¨äº†ç®€å•çš„æ’å€¼æŠ€æœ¯ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä»å‡å°‘çš„é«˜åº¦å¹³é¢é›†è¿›è¡Œä¸‰ç»´è¶…å£°é‡å»ºçš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªä¸‰ç»´å¿ƒè„è¶…å£°æ•°æ®é›†ä¸Šæ¯”è¾ƒäº†ä¼ ç»Ÿå’ŒåŸºäºç›‘ç£æ·±åº¦å­¦ä¹ çš„æ’å€¼æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºDMçš„é‡å»ºåœ¨å›¾åƒè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡åˆ©ç”¨è¶…å£°åºåˆ—å›ºæœ‰çš„æ—¶é—´ä¸€è‡´æ€§æ¥åŠ é€Ÿæ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨æ‰©æ•£åéªŒé‡‡æ ·çš„æ¦‚ç‡æ€§è´¨æ¥æ¢ç´¢æ‰€æå‡ºæ–¹æ³•çš„ç¨³å¥æ€§ï¼Œé‡åŒ–é‡å»ºçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨å¼ºæ¬ é‡‡æ ·ä¸‹çš„åˆæˆå¼‚å¸¸æ•°æ®ä¸Šå±•ç¤ºäº†å¬å›ç‡çš„æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22090v1">PDF</a> 10 pages, 10 figures, preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç»´è¶…å£°æˆåƒæŠ€æœ¯çš„æ–°è¿›å±•ã€‚ä¸ä¼ ç»ŸäºŒç»´è¶…å£°ç›¸æ¯”ï¼Œä¸‰ç»´è¶…å£°èƒ½å¤Ÿå®ç°å®æ—¶ä½“ç§¯å¯è§†åŒ–ï¼Œå‡å°‘äº†å¯¹ç²¾ç¡®æ¢å¤´ä½ç½®çš„ä¾èµ–ã€‚æ–‡ç« è®¨è®ºäº†å®ç°é«˜ä½“ç§¯ç‡å’Œé«˜å›¾åƒè´¨é‡ä¹‹é—´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æ–°æ–¹æ³•æ¥å®ç°ä»å‡å°‘çš„ä¸€ç»„å¹³é¢è¿›è¡Œä¸‰ç»´è¶…å£°é‡å»ºï¼Œä»è€Œæé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚é€šè¿‡ä¸ä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ’å€¼æ–¹æ³•æ¯”è¾ƒï¼Œè¯¥æ–¹æ³•çš„å›¾åƒè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å‡è¡¨ç°æ›´ä½³ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ©ç”¨è¶…å£°åºåˆ—çš„å›ºæœ‰æ—¶é—´ä¸€è‡´æ€§æ¥åŠ é€Ÿæ¨æ–­ï¼Œä¸”é€šè¿‡åˆ©ç”¨æ‰©æ•£åéªŒé‡‡æ ·çš„æ¦‚ç‡æ€§è´¨æ¥é‡åŒ–é‡å»ºçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶åœ¨å¼ºäºšé‡‡æ ·æ¡ä»¶ä¸‹å¯¹åˆæˆå¼‚å¸¸æ•°æ®æé«˜äº†å¬å›ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸‰ç»´è¶…å£°èƒ½å¤Ÿå®æ—¶å¯è§†åŒ–è§£å‰–ç»“æ„ï¼Œç›¸è¾ƒäºä¼ ç»ŸäºŒç»´è¶…å£°ï¼Œå…¶å¯¹ä¸åŒç»éªŒæ°´å¹³çš„ä¸´åºŠåŒ»ç”Ÿæ›´ä¸ºå‹å¥½ï¼Œå¹¶æ”¹å–„äº†è‡ªåŠ¨åŒ–æµ‹é‡å’Œè€ƒè¯•ååˆ†æã€‚</li>
<li>å®ç°é«˜ä½“ç§¯ç‡å’Œé«˜å›¾åƒè´¨é‡ä»æ˜¯ä¸‰ç»´è¶…å£°æŠ€æœ¯çš„é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¢«ç”¨äºä»å‡å°‘çš„ä¸€ç»„å¹³é¢å®ç°ä¸‰ç»´è¶…å£°é‡å»ºï¼Œä»¥æé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ã€‚</li>
<li>DMsé‡å»ºæ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ’å€¼æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è¶…å£°åºåˆ—çš„å›ºæœ‰æ—¶é—´ä¸€è‡´æ€§æ¥åŠ é€Ÿæ¨æ–­ã€‚</li>
<li>æ‰©æ•£åéªŒé‡‡æ ·çš„æ¦‚ç‡æ€§è´¨è¢«ç”¨äºé‡åŒ–é‡å»ºçš„ä¸ç¡®å®šæ€§ï¼Œå¢å¼ºäº†æ–¹æ³•çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8416fef48ed9de165777988ebb9d4f78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55388495e1a74de93d7275cd81150752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9becc041abfdb0b06f7f812c9d37315.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb2d4ef54918532e2ae8adece00e0e75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b7be3d512aea8d434e9ec064100a630.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e42914303a323838ce3ffaa575f096e2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Bringing-CLIP-to-the-Clinic-Dynamic-Soft-Labels-and-Negation-Aware-Learning-for-Medical-Analysis"><a href="#Bringing-CLIP-to-the-Clinic-Dynamic-Soft-Labels-and-Negation-Aware-Learning-for-Medical-Analysis" class="headerlink" title="Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware   Learning for Medical Analysis"></a>Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware   Learning for Medical Analysis</h2><p><strong>Authors:Hanbin Ko, Chang-Min Park</strong></p>
<p>The development of large-scale image-text pair datasets has significantly advanced self-supervised learning in Vision-Language Processing (VLP). However, directly applying general-domain architectures such as CLIP to medical data presents challenges, particularly in handling negations and addressing the inherent data imbalance of medical datasets. To address these issues, we propose a novel approach that integrates clinically-enhanced dynamic soft labels and medical graphical alignment, thereby improving clinical comprehension and the applicability of contrastive loss in medical contexts. Furthermore, we introduce negation-based hard negatives to deepen the modelâ€™s understanding of the complexities of clinical language. Our approach is easily integrated into the medical CLIP training pipeline and achieves state-of-the-art performance across multiple tasks, including zero-shot, fine-tuned classification, and report retrieval. To comprehensively evaluate our modelâ€™s capacity for understanding clinical language, we introduce CXR-Align, a benchmark uniquely designed to evaluate the understanding of negation and clinical information within chest X-ray (CXR) datasets. Experimental results demonstrate that our proposed methods are straightforward to implement and generalize effectively across contrastive learning frameworks, enhancing medical VLP capabilities and advancing clinical language understanding in medical imaging. </p>
<blockquote>
<p>å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†è§†è§‰è¯­è¨€å¤„ç†ï¼ˆVLPï¼‰ä¸­çš„è‡ªç›‘ç£å­¦ä¹ ã€‚ç„¶è€Œï¼Œå°†é€šç”¨é¢†åŸŸçš„æ¶æ„ï¼ˆå¦‚CLIPï¼‰ç›´æ¥åº”ç”¨äºåŒ»ç–—æ•°æ®å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¦å®šè¯å’Œåº”å¯¹åŒ»ç–—æ•°æ®é›†å›ºæœ‰çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¸´åºŠå¢å¼ºçš„åŠ¨æ€è½¯æ ‡ç­¾å’ŒåŒ»ç–—å›¾å½¢å¯¹é½ï¼Œä»è€Œæé«˜äº†ä¸´åºŠç†è§£ä»¥åŠåœ¨åŒ»ç–—ç¯å¢ƒä¸­å¯¹æ¯”æŸå¤±çš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¦å®šçš„ç¡¬å¦å®šæ ·æœ¬ï¼Œä»¥åŠ æ·±æ¨¡å‹å¯¹ä¸´åºŠè¯­è¨€å¤æ‚æ€§çš„ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¾ˆå®¹æ˜“é›†æˆåˆ°åŒ»ç–—CLIPè®­ç»ƒç®¡é“ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å¾®è°ƒåˆ†ç±»å’ŒæŠ¥å‘Šæ£€ç´¢ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹å¯¹ä¸´åºŠè¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CXR-Alignï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¯¹èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸­çš„å¦å®šå’Œä¸´åºŠä¿¡æ¯çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜“äºå®ç°ï¼Œåœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­æœ‰æ•ˆæ¨å¹¿ï¼Œæé«˜äº†åŒ»ç–—VLPçš„èƒ½åŠ›ï¼Œå¹¶æ¨åŠ¨äº†åŒ»å­¦å½±åƒä¸­çš„ä¸´åºŠè¯­è¨€ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22079v1">PDF</a> 16 pages (8 main, 2 references, 6 appendix), 13 figures. Accepted to   CVPR 2025. This author-accepted manuscript includes an expanded ethics&#x2F;data   user agreement section. The final version will appear in the Proceedings of   CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†è§†è§‰è¯­è¨€å¤„ç†ï¼ˆVLPï¼‰é¢†åŸŸçš„è‡ªç›‘ç£å­¦ä¹ ã€‚ç„¶è€Œï¼Œå°†é€šç”¨åŸŸæ¶æ„ï¼ˆå¦‚CLIPï¼‰ç›´æ¥åº”ç”¨äºåŒ»ç–—æ•°æ®é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¦å®šå’Œè§£å†³åŒ»ç–—æ•°æ®é›†å†…åœ¨æ•°æ®ä¸å¹³è¡¡é—®é¢˜æ–¹é¢ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆä¸´åºŠå¢å¼ºåŠ¨æ€è½¯æ ‡ç­¾å’ŒåŒ»ç–—å›¾åƒå¯¹é½çš„æ–°æ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹å¯¹ä¸´åºŠè¯­å¢ƒçš„ç†è§£èƒ½åŠ›å’Œå¯¹æ¯”æŸå¤±çš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥åŸºäºå¦å®šçš„ç¡¬è´Ÿæ ·æœ¬ï¼Œæ·±åŒ–äº†æ¨¡å‹å¯¹å¤æ‚ä¸´åºŠè¯­è¨€çš„ç†è§£ã€‚è¯¥æ–¹æ³•æ˜“äºèå…¥åŒ»ç–—CLIPè®­ç»ƒç®¡é“ï¼Œå¹¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å¾®è°ƒåˆ†ç±»å’ŒæŠ¥å‘Šæ£€ç´¢ã€‚ä¸ºå…¨é¢è¯„ä¼°æ¨¡å‹å¯¹ä¸´åºŠè¯­è¨€çš„ç†è§£èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜æ¨å‡ºäº†CXR-AlignåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°èƒ¸é€æ•°æ®é›†å†…å¦å®šå’Œä¸´åºŠä¿¡æ¯çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜“äºå®ç°ï¼Œåœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜äº†åŒ»ç–—VLPçš„èƒ½åŠ›ï¼Œå¹¶æ¨åŠ¨äº†åŒ»ç–—å›¾åƒä¸­ä¸´åºŠè¯­è¨€çš„ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å›¾åƒæ–‡æœ¬å¯¹æ•°æ®é›†çš„å‘å±•æ¨åŠ¨äº†è§†è§‰è¯­è¨€å¤„ç†ï¼ˆVLPï¼‰çš„è‡ªç›‘ç£å­¦ä¹ ã€‚</li>
<li>å°†é€šç”¨æ¶æ„åº”ç”¨äºåŒ»ç–—æ•°æ®é¢ä¸´å¤„ç†å¦å®šå’Œæ•°æ®ä¸å¹³è¡¡çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆä¸´åºŠå¢å¼ºåŠ¨æ€è½¯æ ‡ç­¾å’ŒåŒ»ç–—å›¾åƒå¯¹é½çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹å¯¹åŒ»ç–—è¯­å¢ƒçš„ç†è§£ã€‚</li>
<li>å¼•å…¥åŸºäºå¦å®šçš„ç¡¬è´Ÿæ ·æœ¬ï¼Œæ·±åŒ–äº†æ¨¡å‹å¯¹å¤æ‚ä¸´åºŠè¯­è¨€çš„ç†è§£ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•æ˜“äºèå…¥åŒ»ç–—CLIPè®­ç»ƒç®¡é“ï¼Œå¹¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>æ¨å‡ºäº†CXR-AlignåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°èƒ¸é€æ•°æ®é›†å†…å¦å®šå’Œä¸´åºŠä¿¡æ¯çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87741934c51a43e7db770e75f80d00c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69e30855306ffdad3138fae809c9633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6c22d967b185a474a36b425fb267467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59797232de6f05731492268a26835970.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a484e724fa6a68afba1a8eeaa1e60c3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Collaborative-Learning-for-Unsupervised-Multimodal-Remote-Sensing-Image-Registration-Integrating-Self-Supervision-and-MIM-Guided-Diffusion-Based-Image-Translation"><a href="#Collaborative-Learning-for-Unsupervised-Multimodal-Remote-Sensing-Image-Registration-Integrating-Self-Supervision-and-MIM-Guided-Diffusion-Based-Image-Translation" class="headerlink" title="Collaborative Learning for Unsupervised Multimodal Remote Sensing Image   Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based   Image Translation"></a>Collaborative Learning for Unsupervised Multimodal Remote Sensing Image   Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based   Image Translation</h2><p><strong>Authors:Xiaochen Wei, Weiwei Guo, Wenxian Yu</strong></p>
<p>The substantial modality-induced variations in radiometric, texture, and structural characteristics pose significant challenges for the accurate registration of multimodal images. While supervised deep learning methods have demonstrated strong performance, they often rely on large-scale annotated datasets, limiting their practical application. Traditional unsupervised methods usually optimize registration by minimizing differences in feature representations, yet often fail to robustly capture geometric discrepancies, particularly under substantial spatial and radiometric variations, thus hindering convergence stability. To address these challenges, we propose a Collaborative Learning framework for Unsupervised Multimodal Image Registration, named CoLReg, which reformulates unsupervised registration learning into a collaborative training paradigm comprising three components: (1) a cross-modal image translation network, MIMGCD, which employs a learnable Maximum Index Map (MIM) guided conditional diffusion model to synthesize modality-consistent image pairs; (2) a self-supervised intermediate registration network which learns to estimate geometric transformations using accurate displacement labels derived from MIMGCD outputs; (3) a distilled cross-modal registration network trained with pseudo-label predicted by the intermediate network. The three networks are jointly optimized through an alternating training strategy wherein each network enhances the performance of the others. This mutual collaboration progressively reduces modality discrepancies, enhances the quality of pseudo-labels, and improves registration accuracy. Extensive experimental results on multiple datasets demonstrate that our ColReg achieves competitive or superior performance compared to state-of-the-art unsupervised approaches and even surpasses several supervised baselines. </p>
<blockquote>
<p>ç”±äºè¾å°„æµ‹é‡å­¦ã€çº¹ç†å’Œç»“æ„ç‰¹æ€§çš„æ˜¾è‘—å·®å¼‚å¯¼è‡´çš„æ¨¡æ€å›ºæœ‰å·®å¼‚ï¼Œç»™å¤šæ¨¡æ€å›¾åƒçš„å‡†ç¡®é…å‡†å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ç›‘ç£æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¼ ç»Ÿçš„æ— ç›‘ç£æ–¹æ³•é€šå¸¸é€šè¿‡æœ€å°åŒ–ç‰¹å¾è¡¨ç¤ºçš„å·®å¼‚æ€§æ¥ä¼˜åŒ–é…å‡†ï¼Œä½†å¾€å¾€ä¸èƒ½ç¨³å¥åœ°æ•æ‰å‡ ä½•å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´ä½ç½®å’Œè¾å°„æµ‹é‡å­¦ç‰¹æ€§å‘ç”Ÿæ˜¾è‘—å˜åŒ–æ—¶ï¼Œä»è€Œé˜»ç¢äº†æ”¶æ•›ç¨³å®šæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCoLRegçš„ç”¨äºæ— ç›‘ç£å¤šæ¨¡æ€å›¾åƒé…å‡†çš„åˆä½œå­¦ä¹ æ¡†æ¶ã€‚å®ƒå°†æ— ç›‘ç£é…å‡†å­¦ä¹ é‡æ–°å®šä¹‰ä¸ºåŒ…å«ä¸‰ä¸ªç»„ä»¶çš„åä½œè®­ç»ƒèŒƒå¼ï¼šä¸€æ˜¯è·¨æ¨¡æ€å›¾åƒç¿»è¯‘ç½‘ç»œMIMGCDï¼Œå®ƒé‡‡ç”¨å¯å­¦ä¹ çš„æœ€å¤§ç´¢å¼•æ˜ å°„ï¼ˆMaximum Index Mapï¼ŒMIMï¼‰å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ¨¡æ€ä¸€è‡´æ€§å›¾åƒå¯¹ï¼›äºŒæ˜¯è‡ªç›‘ç£ä¸­é—´é…å‡†ç½‘ç»œï¼Œå®ƒå­¦ä¹ ä½¿ç”¨ç”±MIMGCDè¾“å‡ºå¾—åˆ°çš„å‡†ç¡®ä½ç§»æ ‡ç­¾æ¥ä¼°è®¡å‡ ä½•å˜æ¢ï¼›ä¸‰æ˜¯è’¸é¦è·¨æ¨¡æ€é…å‡†ç½‘ç»œï¼Œé€šè¿‡ä¸­é—´ç½‘ç»œé¢„æµ‹çš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚è¿™ä¸‰ä¸ªç½‘ç»œé€šè¿‡äº¤æ›¿è®­ç»ƒç­–ç•¥è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå…¶ä¸­æ¯ä¸ªç½‘ç»œéƒ½èƒ½å¢å¼ºå…¶ä»–ç½‘ç»œçš„æ€§èƒ½ã€‚è¿™ç§ç›¸äº’åä½œçš„æ–¹å¼é€æ­¥å‡å°‘äº†æ¨¡æ€å·®å¼‚ï¼Œæé«˜äº†ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶æé«˜äº†é…å‡†çš„å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ColRegä¸æœ€å…ˆè¿›çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–è¡¨ç°æ›´ä¼˜è¶Šï¼Œç”šè‡³è¶…è¶Šäº†å‡ ä¸ªç›‘ç£åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCoLRegçš„åä½œå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ— ç›‘ç£å¤šæ¨¡æ€å›¾åƒé…å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªç»„ä»¶è¿›è¡ŒååŒè®­ç»ƒï¼šè·¨æ¨¡æ€å›¾åƒç¿»è¯‘ç½‘ç»œMIMGCDã€è‡ªç›‘ç£ä¸­é—´é…å‡†ç½‘ç»œå’Œè’¸é¦è·¨æ¨¡æ€é…å‡†ç½‘ç»œã€‚ä¸‰ä¸ªç½‘ç»œé€šè¿‡äº¤æ›¿è®­ç»ƒç­–ç•¥è”åˆä¼˜åŒ–ï¼Œé€æ­¥å‡å°‘æ¨¡æ€å·®å¼‚ï¼Œæé«˜ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå¹¶æ”¹å–„é…å‡†ç²¾åº¦ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒColRegä¸ç°æœ‰å…ˆè¿›æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä¸€äº›æœ‰ç›‘ç£åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å›¾åƒé…å‡†é¢ä¸´è¾å°„åº¦ã€çº¹ç†å’Œç»“æ„ç‰¹æ€§å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŒ…æ‹¬ç›‘ç£æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ— ç›‘ç£æ–¹æ³•ï¼Œä½†éƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„CoLRegåä½œå­¦ä¹ æ¡†æ¶é€šè¿‡ä¸‰ä¸ªç»„ä»¶è¿›è¡ŒååŒè®­ç»ƒï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€å›¾åƒç¿»è¯‘ç½‘ç»œMIMGCDã€è‡ªç›‘ç£ä¸­é—´é…å‡†ç½‘ç»œå’Œè’¸é¦è·¨æ¨¡æ€é…å‡†ç½‘ç»œã€‚</li>
<li>MIMGCDç½‘ç»œé‡‡ç”¨æœ€å¤§ç´¢å¼•å›¾ï¼ˆMIMï¼‰å¼•å¯¼æ¡ä»¶æ‰©æ•£æ¨¡å‹åˆæˆæ¨¡æ€ä¸€è‡´å›¾åƒå¯¹ã€‚</li>
<li>ä¸­é—´é…å‡†ç½‘ç»œé€šè¿‡ä»MIMGCDè¾“å‡ºä¸­æ´¾ç”Ÿå‡ºçš„å‡†ç¡®ä½ç§»æ ‡ç­¾å­¦ä¹ ä¼°è®¡å‡ ä½•å˜æ¢ã€‚</li>
<li>é€šè¿‡äº¤æ›¿è®­ç»ƒç­–ç•¥è”åˆä¼˜åŒ–ä¸‰ä¸ªç½‘ç»œï¼Œæé«˜äº†ä¼ªæ ‡ç­¾è´¨é‡å’Œé…å‡†ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-052d423f7e4eed32ebdb5f9d605c3f0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c88735cc8a7679134e282797ffe07ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85fb140468175d17f9cbfb75ebce09d6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Accretion-disc-Corona-Connection-in-NGC-6814-Insights-from-UV-and-X-ray-spectral-timing-studies"><a href="#Exploring-the-Accretion-disc-Corona-Connection-in-NGC-6814-Insights-from-UV-and-X-ray-spectral-timing-studies" class="headerlink" title="Exploring the Accretion disc&#x2F;Corona Connection in NGC 6814: Insights   from UV and X-ray spectral-timing studies"></a>Exploring the Accretion disc&#x2F;Corona Connection in NGC 6814: Insights   from UV and X-ray spectral-timing studies</h2><p><strong>Authors:Kavita Kumari, I. E. Papadakis, G. C. Dewangan</strong></p>
<p>We conducted a comprehensive spectral and timing analysis of NGC 6814 using AstroSatâ€™s 2019 and XMM-Newtonâ€™s 2021 observations. Cross-correlation analysis revealed a significant correlation between FUV (1541 \AA)&#x2F;X-ray and UVW1 (2910 \AA)&#x2F;X-ray variations, with delays of $\sim 15<del>\rm{ks}$ and $30</del>\rm{ks}$, respectively. We constructed four broadband SEDs after applying aperture correction (for the UVIT filter), subtracting host galaxy and emission line contributions from UV flux, and using mean X-ray spectra alongside selected UV data points. First, we fitted the SEDs with KYNSED model assuming various combinations of inclination, $\theta$, color correction factors, $f_{\rm col}$, and BH spins. Best-fit models were achieved for $\theta&#x3D;70^{\circ}$ (consistent with past estimates for this source) and for spin $\leq 0.5$, while $f_{\rm col}$ is not constrained. KYNSED provided satisfactory fit to all SEDs in the case when the corona is powered by the accretion process, with $\sim 10-20$% of the accretion power transferred to the corona, $\dot{m}&#x2F;\dot{m}_{\rm Edd}\sim 0.1$, corona radius of $\sim 6-10<del>r_g$, and height of $\sim7.5-35</del>r_g$. Model time-lags computed using the SED best-fit results are aligned well with the observed time-lags. Although some of the model parameters are not constrained, the important result of our work is that both the broadband X-ray&#x2F;UV spectra and the X-ray&#x2F;UV time-lags in NGC 6814 are consistent with the hypothesis of X-ray illumination of the disc in a lamp-post geometry framework. Within this model framework, we do not need to assume an outer or inner truncated disc. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ©ç”¨AstroSat 2019å¹´å’ŒXMM-Newton 2021å¹´çš„è§‚æµ‹æ•°æ®ï¼Œå¯¹NGC 6814è¿›è¡Œäº†å…¨é¢çš„å…‰è°±å’Œæ—¶é—´åˆ†æã€‚äº¤å‰å…³è”åˆ†ææ˜¾ç¤ºï¼ŒFUVï¼ˆ1541 \AAï¼‰&#x2F;Xå°„çº¿ä¸UVW1ï¼ˆ2910 \AAï¼‰&#x2F;Xå°„çº¿å˜åŒ–ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ï¼Œå»¶è¿Ÿæ—¶é—´åˆ†åˆ«ä¸ºçº¦15 kså’Œ30 ksã€‚åœ¨åº”ç”¨å­”å¾„æ ¡æ­£ï¼ˆç”¨äºUVITæ»¤æ³¢å™¨ï¼‰ã€ä»ç´«å¤–çº¿æµé‡ä¸­å‡å»å®¿ä¸»æ˜Ÿç³»å’Œå‘å°„çº¿è´¡çŒ®ã€ä½¿ç”¨å¹³å‡Xå°„çº¿å…‰è°±å’Œé€‰å®šçš„ç´«å¤–çº¿æ•°æ®ç‚¹åï¼Œæˆ‘ä»¬æ„å»ºäº†å››ä¸ªå®½é¢‘SEDã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨KYNSEDæ¨¡å‹æ‹ŸåˆSEDï¼Œå‡è®¾äº†å„ç§å€¾è§’Î¸ã€é¢œè‰²æ ¡æ­£å› å­fcolå’ŒBHè‡ªè½¬ç»„åˆã€‚æœ€ä½³æ‹Ÿåˆæ¨¡å‹æ˜¯åœ¨Î¸&#x3D;70Â°ï¼ˆä¸æ­¤æºçš„è¿‡å»ä¼°è®¡ä¸€è‡´ï¼‰å’Œè‡ªè½¬â‰¤0.5çš„æƒ…å†µä¸‹å®ç°çš„ï¼Œè€Œfcolæ²¡æœ‰çº¦æŸã€‚åœ¨æ—¥å†•ç”±å¸ç§¯è¿‡ç¨‹é©±åŠ¨çš„æƒ…å†µä¸‹ï¼ŒKYNSEDå¯¹æ‰€æœ‰SEDçš„æ‹Ÿåˆéƒ½å¾ˆæ»¡æ„ï¼Œå…¶ä¸­çº¦10-20%çš„å¸ç§¯åŠŸç‡è½¬ç§»åˆ°æ—¥å†•ä¸Šï¼Œmdot&#x2F;mEddË™â‰ˆ0.1ï¼Œæ—¥å†•åŠå¾„çº¦ä¸º6-10rgï¼Œé«˜åº¦çº¦ä¸º7.5-35rgã€‚ä½¿ç”¨SEDæœ€ä½³æ‹Ÿåˆç»“æœè®¡ç®—å‡ºçš„æ¨¡å‹æ—¶é—´å»¶è¿Ÿä¸è§‚æµ‹åˆ°çš„æ—¶é—´å»¶è¿Ÿå»åˆè‰¯å¥½ã€‚å°½ç®¡ä¸€äº›æ¨¡å‹å‚æ•°æ²¡æœ‰çº¦æŸï¼Œä½†æˆ‘ä»¬å·¥ä½œçš„é‡è¦ç»“æœæ˜¯ï¼ŒNGC 6814çš„å®½é¢‘Xå°„çº¿&#x2F;ç´«å¤–çº¿å…‰è°±å’ŒXå°„çº¿&#x2F;ç´«å¤–çº¿çš„æ—¶å»¶ä¸Xå°„çº¿ç…§å°„ç›˜çš„å‡è®¾ä¸€è‡´ï¼Œåœ¨ç¯æŸ±å‡ ä½•æ¡†æ¶ä¸‹ã€‚åœ¨æ­¤æ¨¡å‹æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦å‡è®¾å¤–éƒ¨æˆ–å†…éƒ¨æˆªæ–­ç›˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21953v1">PDF</a> Accepted for publication in ApJ, 20 Pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºAstroSatçš„2019å¹´å’ŒXMM-Newtonçš„2021å¹´è§‚æµ‹æ•°æ®ï¼Œå¯¹NGC 6814è¿›è¡Œäº†å…¨é¢çš„å…‰è°±å’Œæ—¶åºåˆ†æã€‚äº¤å‰ç›¸å…³åˆ†ææ˜¾ç¤ºFUVï¼ˆ1541 \AAï¼‰&#x2F;Xå°„çº¿ä¸UVW1ï¼ˆ2910 \AAï¼‰&#x2F;Xå°„çº¿å˜åŒ–ä¹‹é—´å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ï¼Œå»¶è¿Ÿæ—¶é—´åˆ†åˆ«ä¸ºçº¦15 kså’Œ30 ksã€‚æ„å»ºäº†å››ä¸ªå®½å¸¦SEDè°±ï¼Œå¹¶å‡è®¾ä¸åŒå€¾è§’ã€é¢œè‰²æ ¡æ­£å› å­å’Œé»‘æ´è‡ªè½¬ç­‰ç»„åˆï¼Œä½¿ç”¨KYNSEDæ¨¡å‹è¿›è¡Œæœ€ä½³æ‹Ÿåˆã€‚æœ€ä½³æ‹Ÿåˆæ¨¡å‹è¡¨æ˜ï¼Œå† å±‚å¯èƒ½ç”±å¸ç§¯è¿‡ç¨‹æä¾›èƒ½é‡ï¼Œå¸ç§¯åŠŸç‡çš„10-20%è¢«è½¬ç§»åˆ°å† å±‚ã€‚æ¨¡å‹æ—¶é—´æ»åä¸è§‚æµ‹æ—¶é—´æ»åä¸€è‡´ã€‚è™½ç„¶ä¸€äº›æ¨¡å‹å‚æ•°å°šæœªçº¦æŸï¼Œä½†é‡è¦çš„æ˜¯ï¼ŒNGC 6814çš„å®½å¸¦Xå°„çº¿&#x2F;ç´«å¤–çº¿å…‰è°±å’ŒXå°„çº¿&#x2F;ç´«å¤–çº¿æ—¶é—´æ»åä¸ç¯æŸ±å‡ ä½•æ¡†æ¶ä¸‹Xå°„çº¿ç…§å°„ç›˜çš„å‡è®¾ä¸€è‡´ã€‚åœ¨æ­¤æ¨¡å‹æ¡†æ¶å†…ï¼Œæ— éœ€å‡è®¾å¤–éƒ¨æˆ–å†…éƒ¨æˆªæ–­ç›˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨AstroSatå’ŒXMM-Newtonçš„è§‚æµ‹æ•°æ®å¯¹NGC 6814è¿›è¡Œäº†å…‰è°±å’Œæ—¶åºåˆ†æã€‚</li>
<li>å‘ç°äº†FUV&#x2F;Xå°„çº¿å’ŒUVW1&#x2F;Xå°„çº¿ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶æµ‹é‡äº†å¤§çº¦15kså’Œ30ksçš„æ—¶é—´å»¶è¿Ÿã€‚</li>
<li>é€šè¿‡KYNSEDæ¨¡å‹æ‹Ÿåˆäº†SEDè°±ï¼Œå‡è®¾äº†ä¸åŒçš„å€¾è§’ã€é¢œè‰²æ ¡æ­£å› å­å’Œé»‘æ´è‡ªè½¬ã€‚</li>
<li>æœ€ä½³æ‹Ÿåˆæ¨¡å‹è¡¨æ˜å† å±‚å¯èƒ½ç”±å¸ç§¯è¿‡ç¨‹æä¾›èƒ½é‡ï¼Œéƒ¨åˆ†å¸ç§¯åŠŸç‡è½¬ç§»åˆ°å† å±‚ã€‚</li>
<li>æ¨¡å‹æ—¶é—´æ»åä¸è§‚æµ‹æ—¶é—´æ»åä¸€è‡´ã€‚</li>
<li>NGC 6814çš„Xå°„çº¿&#x2F;ç´«å¤–çº¿ç‰¹å¾ä¸ç¯æŸ±å‡ ä½•æ¡†æ¶ä¸‹Xå°„çº¿ç…§å°„ç›˜çš„æ¨¡å‹ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4f4a5878414fcb2a0767aca88da77514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e0fd2924b8c3028aa1f2d010426715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8440f0918a7a730dcb682b27b141409d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59fd2b2867374e4d74b0504750c9ca85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa351b28279292a55f2cd76af03020eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-518f26d20187e21f172312f29b75451a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rise-Time-and-Charge-Collection-Efficiency-of-Graphene-Optimized-4H-SiC-PIN-Detector"><a href="#Rise-Time-and-Charge-Collection-Efficiency-of-Graphene-Optimized-4H-SiC-PIN-Detector" class="headerlink" title="Rise Time and Charge Collection Efficiency of Graphene-Optimized 4H-SiC   PIN Detector"></a>Rise Time and Charge Collection Efficiency of Graphene-Optimized 4H-SiC   PIN Detector</h2><p><strong>Authors:Zhenyu Jiang, Xuemei Lu, Congcong Wang, Yingjie Huang, Xiaoshen Kang, Suyu Xiao, Xiyuan Zhang, Xin Shi</strong></p>
<p>Silicon carbide detectors exhibit good detection performance and are being considered for detection applications. However, the presence of surface electrode of detector limits the application of low-penetration particle detectors, photodetectors and heavy-ion detection. A graphene-optimized 4H-SiC detector has been fabricated to expand the application of SiC detectors.Its electrical properties and the charge collection performance of {\alpha} particles are reported. The effective doping concentration of lightly doped 4H-SiC epitaxial layer is about 4.5\times10^{13}cm^{-3}, approaching the limit of the lowest doping level by the SiC epitaxial growth technique. The rise time of the graphene-optimized ring electrode detector is reduced by 24% at 200 V, compared to ring electrode detector. The charge collection efficiency (CCE) of graphene-optimized 4H-SiC PIN is 99.22%. When the irradiation dose is 2\times10^{11} n_{eq}&#x2F;cm^2, the irradiation has no significant impact on the rise time and uniformity of the rise time for the graphene-optimized 4H-SiC detectors. This study proves that graphene has a certain radiation resistance. Graphene-optimized 4H-SiC detectors can not only reduce the signal rise time, but also improve uniformity of signal rise time and stability of charge collection. This research will expand the application of graphene-based 4H-SiC detectors in fields such as low energy ions, X-ray, UV light detection, particle physics, medical dosimetry and heavy-ion detection. </p>
<blockquote>
<p>ç¢³åŒ–ç¡…æ¢æµ‹å™¨å…·æœ‰è‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ï¼Œæ­£åœ¨è¢«è€ƒè™‘ç”¨äºæ£€æµ‹åº”ç”¨ã€‚ç„¶è€Œï¼Œæ¢æµ‹å™¨çš„è¡¨é¢ç”µæçš„å­˜åœ¨é™åˆ¶äº†ä½ç©¿é€ç²’å­æ¢æµ‹å™¨ã€å…‰ç”µæ¢æµ‹å™¨å’Œé‡ç¦»å­æ£€æµ‹çš„åº”ç”¨ã€‚ä¸€ç§ä¼˜åŒ–çš„çŸ³å¢¨çƒ¯4H-SiCæ¢æµ‹å™¨å·²ç»è¢«åˆ¶é€ å‡ºæ¥ï¼Œä»¥æ‰©å¤§SiCæ£€æµ‹å™¨çš„åº”ç”¨ã€‚æœ¬æ–‡æŠ¥é“äº†å…¶ç”µå­¦æ€§èƒ½å’ŒÎ±ç²’å­çš„ç”µè·æ”¶é›†æ€§èƒ½ã€‚è½»æºæ‚çš„4H-SiCå¤–å»¶å±‚çš„æœ‰æ•ˆæºæ‚æµ“åº¦çº¦ä¸º4.5Ã—10^{13}cm^{-3}ï¼Œæ¥è¿‘SiCå¤–å»¶ç”Ÿé•¿æŠ€æœ¯çš„æœ€ä½æºæ‚æé™ã€‚ä¸ç¯å½¢ç”µææ¢æµ‹å™¨ç›¸æ¯”ï¼Œåœ¨200Væ—¶ï¼Œä¼˜åŒ–çŸ³å¢¨çƒ¯ç¯å½¢ç”µææ¢æµ‹å™¨çš„ä¸Šå‡æ—¶é—´ç¼©çŸ­äº†24%ã€‚ä¼˜åŒ–çŸ³å¢¨çƒ¯çš„4H-SiC PINçš„ç”µè·æ”¶é›†æ•ˆç‡ï¼ˆCCEï¼‰ä¸º99.22%ã€‚å½“è¾å°„å‰‚é‡ä¸º2Ã—10^{11} n_{eq}&#x2F;cm^2æ—¶ï¼Œè¾å°„å¯¹ä¼˜åŒ–çŸ³å¢¨çƒ¯çš„4H-SiCæ¢æµ‹å™¨çš„ä¸Šå‡æ—¶é—´å’Œä¸Šå‡æ—¶é—´çš„ä¸€è‡´æ€§å‡ ä¹æ²¡æœ‰å½±å“ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜çŸ³å¢¨çƒ¯å…·æœ‰ä¸€å®šçš„æŠ—è¾å°„æ€§ã€‚ä¼˜åŒ–çŸ³å¢¨çƒ¯çš„4H-SiCæ¢æµ‹å™¨ä¸ä»…èƒ½å‡å°‘ä¿¡å·ä¸Šå‡æ—¶é—´ï¼Œè¿˜èƒ½æé«˜ä¿¡å·ä¸Šå‡æ—¶é—´çš„ä¸€è‡´æ€§å’Œç”µè·æ”¶é›†çš„ç¨³å®šæ€§ã€‚è¿™é¡¹ç ”ç©¶å°†æ‰©å¤§åŸºäºçŸ³å¢¨çƒ¯çš„4H-SiCæ¢æµ‹å™¨åœ¨ä½èƒ½ç¦»å­ã€Xå°„çº¿ã€ç´«å¤–å…‰æ£€æµ‹ã€ç²’å­ç‰©ç†ã€åŒ»å­¦è®¡é‡å’Œé‡ç¦»å­æ£€æµ‹ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21902v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¢³åŒ–ç¡…æ¢æµ‹å™¨æ€§èƒ½ä¼˜è‰¯ï¼Œè¢«åº”ç”¨äºæ£€æµ‹é¢†åŸŸã€‚ä½†å…¶è¡¨é¢ç”µæé™åˆ¶äº†å…¶åœ¨ä½ç©¿é€ç²’å­æ£€æµ‹å™¨ã€å…‰ç”µæ£€æµ‹å™¨å’Œé‡ç¦»å­æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚ä¸ºæ‰©å±•ç¢³åŒ–ç¡…æ¢æµ‹å™¨çš„åº”ç”¨èŒƒå›´ï¼Œå·²ç ”åˆ¶å‡ºçŸ³å¢¨çƒ¯ä¼˜åŒ–çš„4H-SiCæ¢æµ‹å™¨ã€‚å…¶ç”µå­¦æ€§èƒ½å’ŒÎ±ç²’å­ç”µè·æ”¶é›†æ€§èƒ½å¾—åˆ°äº†æŠ¥é“ã€‚è¯¥æ¢æµ‹å™¨çš„æœ‰æ•ˆæºæ‚æµ“åº¦æ¥è¿‘SiCå¤–å»¶ç”Ÿé•¿æŠ€æœ¯çš„æœ€ä½æºæ‚æ°´å¹³ã€‚çŸ³å¢¨çƒ¯ä¼˜åŒ–ç¯å½¢ç”µææ¢æµ‹å™¨çš„ä¸Šå‡æ—¶é—´ç›¸è¾ƒäºç¯å½¢ç”µææ¢æµ‹å™¨å‡å°‘äº†24%ã€‚çŸ³å¢¨çƒ¯ä¼˜åŒ–çš„4H-SiC PINçš„ç”µè·æ”¶é›†æ•ˆç‡é«˜è¾¾99.22%ã€‚åœ¨è¾å°„å‰‚é‡ä¸º2Ã—10Â¹Â¹ neq&#x2F;cmÂ²æ—¶ï¼Œè¾å°„å¯¹çŸ³å¢¨çƒ¯ä¼˜åŒ–çš„4H-SiCæ¢æµ‹å™¨çš„ä¸Šå‡æ—¶é—´å’Œä¸Šå‡æ—¶é—´å‡åŒ€æ€§æ²¡æœ‰æ˜¾è‘—å½±å“ã€‚ç ”ç©¶è¯æ˜çŸ³å¢¨çƒ¯å…·æœ‰ä¸€å®šçš„æŠ—è¾å°„æ€§ã€‚è¯¥ç ”ç©¶æˆæœå°†æ‰©å¤§çŸ³å¢¨çƒ¯åŸº4H-SiCæ¢æµ‹å™¨åœ¨ä½èƒ½ç¦»å­ã€Xå°„çº¿ã€ç´«å¤–çº¿æ£€æµ‹ã€ç²’å­ç‰©ç†ã€åŒ»å­¦è®¡é‡å’Œé‡ç¦»å­æ£€æµ‹ç­‰é¢†åŸŸçš„åº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¢³åŒ–ç¡…æ¢æµ‹å™¨å…·æœ‰è‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ï¼Œä½†è¡¨é¢ç”µæé™åˆ¶äº†å…¶åº”ç”¨èŒƒå›´ã€‚</li>
<li>çŸ³å¢¨çƒ¯ä¼˜åŒ–çš„4H-SiCæ¢æµ‹å™¨è¢«ç ”åˆ¶å‡ºä»¥æ‰©å±•ç¢³åŒ–ç¡…æ¢æµ‹å™¨çš„åº”ç”¨èŒƒå›´ã€‚</li>
<li>è¯¥æ¢æµ‹å™¨çš„æœ‰æ•ˆæºæ‚æµ“åº¦æ¥è¿‘SiCå¤–å»¶ç”Ÿé•¿æŠ€æœ¯çš„æœ€ä½æºæ‚æ°´å¹³ã€‚</li>
<li>çŸ³å¢¨çƒ¯ä¼˜åŒ–ç¯å½¢ç”µææ¢æµ‹å™¨ç›¸è¾ƒäºä¼ ç»Ÿç¯å½¢ç”µææ¢æµ‹å™¨ï¼Œå…¶ä¿¡å·ä¸Šå‡æ—¶é—´å‡å°‘äº†24%ã€‚</li>
<li>çŸ³å¢¨çƒ¯ä¼˜åŒ–çš„4H-SiC PINçš„ç”µè·æ”¶é›†æ•ˆç‡é«˜è¾¾99.22%ã€‚</li>
<li>åœ¨ä¸€å®šè¾å°„å‰‚é‡ä¸‹ï¼Œè¾å°„å¯¹çŸ³å¢¨çƒ¯ä¼˜åŒ–çš„4H-SiCæ¢æµ‹å™¨çš„æ€§èƒ½æ²¡æœ‰æ˜¾è‘—å½±å“ï¼Œè¯æ˜çŸ³å¢¨çƒ¯å…·æœ‰ä¸€å®šçš„æŠ—è¾å°„æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b47bd3acce82082251be4612bc40660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bff709d118142060e89ebe89a3b06ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f04d1247d0673f83f672a80d5fe889.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d5c0749cd9cb1e24728c3989bdfc374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f488ec819589423c7077a4d324d2f29.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Concentrate-on-Weakness-Mining-Hard-Prototypes-for-Few-Shot-Medical-Image-Segmentation"><a href="#Concentrate-on-Weakness-Mining-Hard-Prototypes-for-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical   Image Segmentation"></a>Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical   Image Segmentation</h2><p><strong>Authors:Jianchao Jiang, Haofeng Zhang</strong></p>
<p>Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a model that can perform segmentation from only a few annotated images. However, most existing prototype-based FSMIS methods generate multiple prototypes from the support image solely by random sampling or local averaging, which can cause particularly severe boundary blurring due to the tendency for normal features accounting for the majority of features of a specific category. Consequently, we propose to focus more attention to those weaker features that are crucial for clear segmentation boundary. Specifically, we design a Support Self-Prediction (SSP) module to identify such weak features by comparing true support mask with one predicted by global support prototype. Then, a Hard Prototypes Generation (HPG) module is employed to generate multiple hard prototypes based on these weak features. Subsequently, a Multiple Similarity Maps Fusion (MSMF) module is devised to generate final segmenting mask in a dual-path fashion to mitigate the imbalance between foreground and background in medical images. Furthermore, we introduce a boundary loss to further constraint the edge of segmentation. Extensive experiments on three publicly available medical image datasets demonstrate that our method achieves state-of-the-art performance. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jcjiang99/CoW">https://github.com/jcjiang99/CoW</a>. </p>
<blockquote>
<p>å°æ ·åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…ä»å°‘é‡æ ‡æ³¨å›¾åƒä¸­å³å¯è¿›è¡Œåˆ†å‰²ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºåŸå‹çš„FSMISæ–¹æ³•ä»…é€šè¿‡éšæœºæŠ½æ ·æˆ–å±€éƒ¨å¹³å‡ä»æ”¯æŒå›¾åƒç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´è¾¹ç•Œæ¨¡ç³Šç‰¹åˆ«ä¸¥é‡ï¼Œå› ä¸ºæ­£å¸¸ç‰¹å¾å¾€å¾€æ„æˆæŸä¸€ç‰¹å®šç±»åˆ«çš„ä¸»è¦ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æè®®æ›´å¤šåœ°å…³æ³¨é‚£äº›å¯¹äºæ¸…æ™°åˆ†å‰²è¾¹ç•Œè‡³å…³é‡è¦çš„è¾ƒå¼±ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ”¯æŒè‡ªæˆ‘é¢„æµ‹ï¼ˆSSPï¼‰æ¨¡å—ï¼Œé€šè¿‡æ¯”è¾ƒçœŸå®çš„æ”¯æŒæ©è†œä¸ç”±å…¨å±€æ”¯æŒåŸå‹é¢„æµ‹çš„æ”¯æŒæ©è†œæ¥è¯†åˆ«è¿™äº›å¼±ç‰¹å¾ã€‚ç„¶åï¼Œé‡‡ç”¨ç¡¬åŸå‹ç”Ÿæˆï¼ˆHPGï¼‰æ¨¡å—åŸºäºè¿™äº›å¼±ç‰¹å¾ç”Ÿæˆå¤šä¸ªç¡¬åŸå‹ã€‚éšåï¼Œé‡‡ç”¨å¤šé‡ç›¸ä¼¼å›¾èåˆï¼ˆMSMFï¼‰æ¨¡å—ä»¥åŒè·¯å¾„æ–¹å¼ç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œï¼Œä»¥ç¼“è§£åŒ»å­¦å›¾åƒä¸­å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„ä¸å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥è¾¹ç•ŒæŸå¤±æ¥è¿›ä¸€æ­¥çº¦æŸåˆ†å‰²çš„è¾¹ç¼˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jcjiang99/CoW%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jcjiang99/CoWæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21897v1">PDF</a> 12 pages, 9 figures, 9 tables, accepted by IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åŒ»ç–—å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ”¯æŒè‡ªæˆ‘é¢„æµ‹ï¼ˆSSPï¼‰æ¨¡å—è¯†åˆ«å…³é”®å¼±ç‰¹å¾ï¼Œé‡‡ç”¨ç¡¬åŸå‹ç”Ÿæˆï¼ˆHPGï¼‰æ¨¡å—ç”Ÿæˆå¤šä¸ªç¡¬åŸå‹ï¼Œå¹¶é‡‡ç”¨å¤šé‡ç›¸ä¼¼æ€§æ˜ å°„èåˆï¼ˆMSMFï¼‰æ¨¡å—ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²æ©è†œã€‚åŒæ—¶ï¼Œå¼•å…¥è¾¹ç•ŒæŸå¤±ä»¥è¿›ä¸€æ­¥çº¦æŸåˆ†å‰²è¾¹ç¼˜ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Medical Image Segmentation (FSMIS) ç”¨äºä»å°‘é‡æ ‡æ³¨å›¾åƒè¿›è¡Œè®­ç»ƒæ¨¡å‹åˆ†å‰²ã€‚</li>
<li>ç°æœ‰åŸå‹åŸºç¡€çš„FSMISæ–¹æ³•ä¸»è¦é€šè¿‡éšæœºé‡‡æ ·æˆ–å±€éƒ¨å¹³å‡ä»æ”¯æŒå›¾åƒç”Ÿæˆå¤šä¸ªåŸå‹ï¼Œè¿™å¯èƒ½å¯¼è‡´è¾¹ç•Œæ¨¡ç³Šã€‚</li>
<li>å¼•å…¥æ”¯æŒè‡ªæˆ‘é¢„æµ‹ï¼ˆSSPï¼‰æ¨¡å—ï¼Œé€šè¿‡æ¯”è¾ƒçœŸå®æ”¯æŒæ©è†œä¸å…¨å±€æ”¯æŒåŸå‹é¢„æµ‹çš„æ©è†œæ¥è¯†åˆ«å…³é”®å¼±ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨ç¡¬åŸå‹ç”Ÿæˆï¼ˆHPGï¼‰æ¨¡å—ï¼ŒåŸºäºè¿™äº›å¼±ç‰¹å¾ç”Ÿæˆå¤šä¸ªç¡¬åŸå‹ã€‚</li>
<li>ä½¿ç”¨å¤šé‡ç›¸ä¼¼æ€§æ˜ å°„èåˆï¼ˆMSMFï¼‰æ¨¡å—ä»¥åŒé‡è·¯å¾„æ–¹å¼ç”Ÿæˆæœ€ç»ˆåˆ†å‰²æ©è†œï¼Œç¼“è§£åŒ»ç–—å›¾åƒä¸­å‰æ™¯å’ŒèƒŒæ™¯çš„ä¸å¹³è¡¡ã€‚</li>
<li>å¼•å…¥è¾¹ç•ŒæŸå¤±ä»¥è¿›ä¸€æ­¥çº¦æŸåˆ†å‰²è¾¹ç¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-112f2b22a43492f47fed31a8b49d7eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f78e1eb789f2e7daf17f649e6c93c11b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e41ca871ea0ef05483686ab1ca4545f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Targeted-Unlearning-Using-Perturbed-Sign-Gradient-Methods-With-Applications-On-Medical-Images"><a href="#Targeted-Unlearning-Using-Perturbed-Sign-Gradient-Methods-With-Applications-On-Medical-Images" class="headerlink" title="Targeted Unlearning Using Perturbed Sign Gradient Methods With   Applications On Medical Images"></a>Targeted Unlearning Using Perturbed Sign Gradient Methods With   Applications On Medical Images</h2><p><strong>Authors:George R. Nahass, Zhu Wang, Homa Rashidisabet, Won Hwa Kim, Sasha Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi, Sathya N. Ravi</strong></p>
<p>Machine unlearning aims to remove the influence of specific training samples from a trained model without full retraining. While prior work has largely focused on privacy-motivated settings, we recast unlearning as a general-purpose tool for post-deployment model revision. Specifically, we focus on utilizing unlearning in clinical contexts where data shifts, device deprecation, and policy changes are common. To this end, we propose a bilevel optimization formulation of boundary-based unlearning that can be solved using iterative algorithms. We provide convergence guarantees when first-order algorithms are used to unlearn. Our method introduces tunable loss design for controlling the forgetting-retention tradeoff and supports novel model composition strategies that merge the strengths of distinct unlearning runs. Across benchmark and real-world clinical imaging datasets, our approach outperforms baselines on both forgetting and retention metrics, including scenarios involving imaging devices and anatomical outliers. This work establishes machine unlearning as a modular, practical alternative to retraining for real-world model maintenance in clinical applications. </p>
<blockquote>
<p>æœºå™¨é—å¿˜æ—¨åœ¨ä»å·²è®­ç»ƒçš„æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šè®­ç»ƒæ ·æœ¬çš„å½±å“ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å°½ç®¡æ—©æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä»¥éšç§ä¸ºé©±åŠ¨çš„åœºæ™¯ä¸Šï¼Œæˆ‘ä»¬å°†é—å¿˜é‡å¡‘ä¸ºä¸€ç§ç”¨äºéƒ¨ç½²åæ¨¡å‹ä¿®æ­£çš„é€šç”¨å·¥å…·ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å…³æ³¨åœ¨æ•°æ®åç§»ã€è®¾å¤‡å¼ƒç”¨å’Œæ”¿ç­–å˜æ›´ç­‰æ™®éå­˜åœ¨çš„ä¸´åºŠç¯å¢ƒä¸­åˆ©ç”¨é—å¿˜æŠ€æœ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¾¹ç•Œçš„é—å¿˜çš„ä¸¤çº§ä¼˜åŒ–å…¬å¼ï¼Œå¯ä»¥ä½¿ç”¨è¿­ä»£ç®—æ³•æ¥è§£å†³ã€‚å½“ä½¿ç”¨ä¸€é˜¶ç®—æ³•è¿›è¡Œé—å¿˜æ—¶ï¼Œæˆ‘ä»¬æä¾›æ”¶æ•›æ€§ä¿è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†å¯è°ƒæŸå¤±è®¾è®¡ï¼Œä»¥æ§åˆ¶é—å¿˜ä¸ä¿ç•™ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ”¯æŒæ–°çš„æ¨¡å‹ç»„åˆç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥åˆå¹¶ä¸åŒé—å¿˜è¿è¡Œçš„ä¼˜åŠ¿ã€‚åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œä¸´åºŠæˆåƒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜å’Œä¿ç•™æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¶‰åŠæˆåƒè®¾å¤‡å’Œè§£å‰–å¼‚å¸¸å€¼ç­‰åœºæ™¯ã€‚è¿™é¡¹å·¥ä½œç¡®ç«‹äº†æœºå™¨é—å¿˜ä½œä¸ºä¸´åºŠåº”ç”¨ä¸­ç°å®ä¸–ç•Œæ¨¡å‹ç»´æŠ¤çš„ä¸€ç§æ¨¡å—åŒ–ã€å®ç”¨çš„æ›¿ä»£å†è®­ç»ƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21872v1">PDF</a> 39 pages, 12 figures, 11 tables, 3 algorithms</p>
<p><strong>Summary</strong><br>æœºå™¨é—å¿˜æ—¨åœ¨ä»å·²è®­ç»ƒçš„æ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šè®­ç»ƒæ ·æœ¬çš„å½±å“ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚æœ¬æ–‡å°†å…¶é‡æ–°å®šä½ä¸ºç”¨äºéƒ¨ç½²åæ¨¡å‹ä¿®è®¢çš„é€šç”¨å·¥å…·ï¼Œé‡ç‚¹å…³æ³¨ä¸´åºŠç¯å¢ƒä¸‹çš„åº”ç”¨ï¼Œå¦‚æ•°æ®å˜åŒ–ã€è®¾å¤‡æ·˜æ±°å’Œæ”¿ç­–å˜æ›´ç­‰åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œçš„é—å¿˜çš„ä¸¤çº§ä¼˜åŒ–å…¬å¼ï¼Œå¯ä»¥ä½¿ç”¨è¿­ä»£ç®—æ³•æ¥è§£å†³ã€‚å½“ä½¿ç”¨ä¸€é˜¶ç®—æ³•è¿›è¡Œé—å¿˜æ—¶ï¼Œæˆ‘ä»¬æä¾›äº†æ”¶æ•›æ€§ä¿è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†å¯è°ƒæŸå¤±è®¾è®¡ï¼Œä»¥æ§åˆ¶é—å¿˜ä¸ä¿ç•™ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ”¯æŒæ–°çš„æ¨¡å‹ç»„åˆç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥åˆå¹¶ä¸åŒé—å¿˜è¿è¡Œçš„ä¼˜ç‚¹ã€‚åœ¨åŸºå‡†å’ŒçœŸå®ä¸´åºŠæˆåƒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜å’Œä¿ç•™æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¶‰åŠæˆåƒè®¾å¤‡å’Œè§£å‰–å¼‚å¸¸æƒ…å†µçš„åœºæ™¯ã€‚æœ¬ç ”ç©¶ç¡®ç«‹äº†æœºå™¨é—å¿˜ä½œä¸ºä¸€ç§æ¨¡å—åŒ–ã€å®ç”¨çš„æ›¿ä»£æ–¹æ³•ï¼Œç”¨äºä¸´åºŠåº”ç”¨ä¸­ç°å®æ¨¡å‹çš„ç»´æŠ¤ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨é—å¿˜æ—¨åœ¨ç§»é™¤ç‰¹å®šè®­ç»ƒæ ·æœ¬å¯¹æ¨¡å‹çš„å½±å“ï¼Œè€Œæ— éœ€å…¨é¢é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡å°†æœºå™¨é—å¿˜é‡æ–°å®šä½ä¸ºç”¨äºéƒ¨ç½²åæ¨¡å‹ä¿®è®¢çš„é€šç”¨å·¥å…·ã€‚</li>
<li>åœ¨ä¸´åºŠç¯å¢ƒä¸‹åº”ç”¨æœºå™¨é—å¿˜ï¼Œè€ƒè™‘äº†æ•°æ®å˜åŒ–ã€è®¾å¤‡æ·˜æ±°å’Œæ”¿ç­–å˜æ›´ç­‰åœºæ™¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œçš„é—å¿˜çš„ä¸¤çº§ä¼˜åŒ–å…¬å¼ï¼Œå¹¶ä½¿ç”¨è¿­ä»£ç®—æ³•è§£å†³ã€‚</li>
<li>æ–¹æ³•å¼•å…¥äº†å¯è°ƒæŸå¤±è®¾è®¡ï¼Œä»¥æ§åˆ¶é—å¿˜ä¸ä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>æ”¯æŒæ–°çš„æ¨¡å‹ç»„åˆç­–ç•¥ï¼Œå¯ä»¥åˆå¹¶ä¸åŒé—å¿˜è¿è¡Œçš„ä¼˜ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16314f8043a451fea3805db0692bd27e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47768ee05a912b526ad4ee1e94a25069.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging"><a href="#Towards-Scalable-Language-Image-Pre-training-for-3D-Medical-Imaging" class="headerlink" title="Towards Scalable Language-Image Pre-training for 3D Medical Imaging"></a>Towards Scalable Language-Image Pre-training for 3D Medical Imaging</h2><p><strong>Authors:Chenhui Zhao, Yiwei Lyu, Asadur Chowdury, Edward Harake, Akhil Kondepudi, Akshay Rao, Xinhai Hou, Honglak Lee, Todd Hollon</strong></p>
<p>Language-image pre-training has demonstrated strong performance in 2D medical imaging, but its success in 3D modalities such as CT and MRI remains limited due to the high computational demands of volumetric data, which pose a significant barrier to training on large-scale, uncurated clinical studies. In this study, we introduce Hierarchical attention for Language-Image Pre-training (HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a lightweight hierarchical attention mechanism inspired by the natural hierarchy of radiology data: slice, scan, and study. This mechanism exhibits strong generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables direct training on uncurated datasets. Trained on 220K patients with 3.13 million scans for brain MRI and 240K patients with 1.44 million scans for head CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and +6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These results demonstrate that, with HLIP, directly pre-training on uncurated clinical datasets is a scalable and effective direction for language-image pre-training in 3D medical imaging. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zch0414/hlip">https://github.com/Zch0414/hlip</a> </p>
<blockquote>
<p>è¯­è¨€å›¾åƒé¢„è®­ç»ƒåœ¨2DåŒ»å­¦å½±åƒä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨CTå’ŒMRIç­‰3Dæ¨¡å¼ä¸­çš„æˆåŠŸåº”ç”¨ä»ç„¶æœ‰é™ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä½“ç§¯æ•°æ®çš„é«˜è®¡ç®—éœ€æ±‚æ„æˆäº†å¤§è§„æ¨¡éæ•´ç†ä¸´åºŠç ”ç©¶çš„è®­ç»ƒéšœç¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äº3DåŒ»å­¦å½±åƒçš„å¯æ‰©å±•é¢„è®­ç»ƒæ¡†æ¶â€”â€”åˆ†å±‚æ³¨æ„åŠ›è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆHLIPï¼‰ã€‚HLIPé‡‡ç”¨äº†ä¸€ç§è½»é‡çº§çš„åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å—åˆ°æ”¾å°„å­¦æ•°æ®çš„è‡ªç„¶å±‚æ¬¡ç»“æ„çš„å¯å‘ï¼šåˆ‡ç‰‡ã€æ‰«æå’Œç ”ç©¶ã€‚è¿™ç§æœºåˆ¶å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨CT-RATEä¸Šé¢„è®­ç»ƒååœ¨Rad-ChestCTåŸºå‡†æµ‹è¯•ä¸Šå®è§‚AUCæé«˜4.3%ã€‚æ­¤å¤–ï¼ŒHLIPçš„è®¡ç®—æ•ˆç‡èƒ½å¤Ÿå®ç°ç›´æ¥åœ¨éæ•´ç†æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨é’ˆå¯¹è„‘éƒ¨MRIçš„22ä¸‡æ‚£è€…å’Œ313ä¸‡æ¬¡æ‰«æä»¥åŠé’ˆå¯¹å¤´éƒ¨CTçš„24ä¸‡æ‚£è€…å’Œ144ä¸‡æ¬¡æ‰«æçš„è®­ç»ƒä¸‹ï¼ŒHLIPå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨æå‡ºçš„å…¬å¼€è„‘éƒ¨MRIåŸºå‡†æµ‹è¯•Pub-Brain-5ä¸Šçš„å¹³è¡¡å‡†ç¡®ç‡æé«˜32.4%ï¼›åœ¨å¤´éƒ¨CTåŸºå‡†æµ‹è¯•RSNAå’ŒCQ500ä¸Šçš„å®è§‚AUCåˆ†åˆ«æé«˜1.4%å’Œ6.9%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨HLIPç›´æ¥åœ¨éæ•´ç†çš„ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ˜¯3DåŒ»å­¦å½±åƒè¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„å¯æ‰©å±•å’Œæœ‰æ•ˆæ–¹å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zch0414/hlip%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Zch0414/hlipæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹3DåŒ»å­¦å½±åƒçš„é¢„è®­ç»ƒæ¡†æ¶â€”â€”åˆ†å±‚æ³¨æ„åŠ›è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆHLIPï¼‰ã€‚HLIPé‡‡ç”¨è½»é‡çº§åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æ”¾å°„å­¦æ•°æ®è‡ªç„¶å±‚æ¬¡ç»“æ„ï¼ˆåˆ‡ç‰‡ã€æ‰«æå’Œç ”ç©¶ï¼‰çš„å¯å‘ä¸‹è®¾è®¡ã€‚è¯¥æœºåˆ¶åœ¨Rad-ChestCTåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œé¢„è®­ç»ƒåå®AUCæé«˜4.3%ã€‚æ­¤å¤–ï¼ŒHLIPçš„è®¡ç®—æ•ˆç‡å¯ç›´æ¥åœ¨æœªç»å¤„ç†çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨å…¬å¼€çš„å¤§è„‘MRIåŸºå‡†æµ‹è¯•Pub-Brain-5ä¸Šï¼ŒHLIPå–å¾—äº†é¢†å…ˆæ°´å¹³ï¼Œå¹³è¡¡ç²¾åº¦æé«˜32.4%ã€‚åœ¨å¤´éƒ¨CTåŸºå‡†æµ‹è¯•RSNAå’ŒCQ500ä¸Šï¼Œå®AUCåˆ†åˆ«æé«˜1.4%å’Œ6.9%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç›´æ¥åœ¨æœªç»å¤„ç†çš„ä¸´åºŠæ•°æ®é›†ä¸Šä½¿ç”¨HLIPè¿›è¡Œé¢„è®­ç»ƒæ˜¯3DåŒ»å­¦å½±åƒè¯­è¨€å›¾åƒé¢„è®­ç»ƒçš„å¯æ‰©å±•å’Œæœ‰æ•ˆæ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HLIPæ˜¯ä¸€ä¸ªé’ˆå¯¹3DåŒ»å­¦å½±åƒçš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºå¤„ç†å¤§è§„æ¨¡æœªç»å¤„ç†çš„ä¸´åºŠæ•°æ®é›†è®¾è®¡ã€‚</li>
<li>HLIPé‡‡ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€‚åº”æ”¾å°„å­¦æ•°æ®çš„è‡ªç„¶å±‚æ¬¡ç»“æ„ã€‚</li>
<li>HLIPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>åœ¨Rad-ChestCTåŸºå‡†æµ‹è¯•ä¸Šï¼Œé¢„è®­ç»ƒåå®AUCæé«˜4.3%ã€‚</li>
<li>HLIPåœ¨å¤§è„‘MRIå’Œå¤´éƒ¨CTçš„å…¬å¼€åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>HLIPå¯ç›´æ¥åœ¨æœªç»å¤„ç†çš„ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-505fb763c77d92fc8d6d255c49ff39b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4387bc1b6c4ea37ff7499ad7477a21f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7d1db593cb4590fec51d1174fe735c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8922d8349633fb3458e59805f6a70829.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="STA-Risk-A-Deep-Dive-of-Spatio-Temporal-Asymmetries-for-Breast-Cancer-Risk-Prediction"><a href="#STA-Risk-A-Deep-Dive-of-Spatio-Temporal-Asymmetries-for-Breast-Cancer-Risk-Prediction" class="headerlink" title="STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer   Risk Prediction"></a>STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer   Risk Prediction</h2><p><strong>Authors:Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Jules Sumkin, Shandong Wu</strong></p>
<p>Predicting the risk of developing breast cancer is an important clinical tool to guide early intervention and tailoring personalized screening strategies. Early risk models have limited performance and recently machine learning-based analysis of mammogram images showed encouraging risk prediction effects. These models however are limited to the use of a single exam or tend to overlook nuanced breast tissue evolvement in spatial and temporal details of longitudinal imaging exams that are indicative of breast cancer risk. In this paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk Prediction), a novel Transformer-based model that captures fine-grained mammographic imaging evolution simultaneously from bilateral and longitudinal asymmetries for breast cancer risk prediction. STA-Risk is innovative by the side encoding and temporal encoding to learn spatial-temporal asymmetries, regulated by a customized asymmetry loss. We performed extensive experiments with two independent mammogram datasets and achieved superior performance than four representative SOTA models for 1- to 5-year future risk prediction. Source codes will be released upon publishing of the paper. </p>
<blockquote>
<p>é¢„æµ‹ä¹³è…ºç™Œå‘å±•çš„é£é™©æ˜¯æŒ‡å¯¼æ—©æœŸå¹²é¢„å’Œå®šåˆ¶ä¸ªæ€§åŒ–ç­›æŸ¥ç­–ç•¥çš„é‡è¦ä¸´åºŠå·¥å…·ã€‚æ—©æœŸçš„é£é™©æ¨¡å‹æ€§èƒ½æœ‰é™ï¼Œè€Œæœ€è¿‘åŸºäºæœºå™¨å­¦ä¹ å¯¹ä¹³æˆ¿Xå…‰å½±åƒçš„åˆ†ææ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„é£é™©é¢„æµ‹æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»…é™äºä½¿ç”¨å•æ¬¡æ£€æŸ¥ï¼Œæˆ–è€…å€¾å‘äºå¿½ç•¥çºµå‘æˆåƒæ£€æŸ¥çš„ç©ºé—´å’Œæ—¶é—´ç»†èŠ‚ä¸­å¾®å¦™çš„ä¹³è…ºç»„ç»‡æ¼”å˜ï¼Œè¿™äº›è¿¹è±¡è¡¨æ˜ä¸ä¹³è…ºç™Œé£é™©æœ‰å…³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†STA-Riskï¼ˆåŸºäºç©ºé—´å’Œæ—¶é—´ä¸å¯¹ç§°æ€§çš„é£é™©é¢„æµ‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºTransformerçš„æ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶æ•æ‰åŒä¾§å’Œçºµå‘ä¸å¯¹ç§°æ€§çš„ç»†å¾®ä¹³æˆ¿Xå…‰å½±åƒæ¼”å˜ï¼Œç”¨äºé¢„æµ‹ä¹³è…ºç™Œçš„é£é™©ã€‚STA-Riskçš„åˆ›æ–°ä¹‹å¤„åœ¨äºé€šè¿‡ä¾§ç¼–ç å’Œæ—¶é—´ç¼–ç æ¥å­¦ä¹ ç©ºé—´-æ—¶é—´ä¸å¯¹ç§°æ€§ï¼Œç”±å®šåˆ¶çš„ä¸å¯¹ç§°æŸå¤±è¿›è¡Œè°ƒæ§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªç‹¬ç«‹çš„ä¹³æˆ¿Xå…‰å½±åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶åœ¨æœªæ¥1è‡³5å¹´çš„é£é™©é¢„æµ‹æ–¹é¢å–å¾—äº†ä¼˜äºå››ä¸ªä»£è¡¨æ€§æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚è®ºæ–‡å‘è¡¨åå°†å…¬å¸ƒæºä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21699v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„æ–°å‹ä¹³è…ºç™Œé£é™©é¢„æµ‹æ¨¡å‹STA-Riskï¼Œè¯¥æ¨¡å‹é€šè¿‡æ•æ‰åŒä¾§ä¹³è…ºå›¾åƒçš„é•¿æœŸæ¼”åŒ–è¿‡ç¨‹ä¸­çš„ç©ºé—´å’Œæ—¶é—´ä¸å¯¹ç§°æ€§ï¼Œè¿›è¡Œç²¾ç»†çš„ä¹³è…ºå›¾åƒåˆ†æï¼Œä»¥é¢„æµ‹ä¹³è…ºç™Œçš„é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTA-Riskåœ¨ä¸¤ä¸ªç‹¬ç«‹çš„ä¹³è…ºå½±åƒæ•°æ®é›†ä¸Šçš„é¢„æµ‹æ€§èƒ½ä¼˜äºå…¶ä»–å››ç§å…ˆè¿›æ¨¡å‹ï¼Œæœªæ¥ä¸€è‡³äº”å¹´çš„é¢„æµ‹æ•ˆæœå°¤ä¸ºæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œé£é™©é¢„æµ‹åœ¨ä¸´åºŠä¸Šæ˜¯æ—©æœŸå¹²é¢„å’Œä¸ªæ€§åŒ–ç­›æŸ¥ç­–ç•¥çš„å…³é”®å·¥å…·ã€‚</li>
<li>æ—©æœŸçš„é£é™©é¢„æµ‹æ¨¡å‹æ€§èƒ½æœ‰é™ã€‚</li>
<li>æœºå™¨å­¦ä¹ åœ¨åˆ†æä¹³è…ºXå…‰å›¾åƒä»¥é¢„æµ‹ä¹³è…ºç™Œé£é™©æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>ç›®å‰æ¨¡å‹å¿½è§†äº†ä¹³è…ºç»„ç»‡åœ¨çºµå‘æˆåƒä¸­çš„ç©ºé—´å’Œæ—¶é—´å˜åŒ–ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚å¯èƒ½é¢„ç¤ºä¹³è…ºç™Œé£é™©ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºTransformerçš„ä¹³è…ºç™Œé£é™©é¢„æµ‹æ¨¡å‹STA-Riskã€‚</li>
<li>STA-Riské€šè¿‡æ•æ‰åŒä¾§ä¹³è…ºå›¾åƒçš„é•¿æœŸæ¼”åŒ–è¿‡ç¨‹ä¸­çš„ç©ºé—´å’Œæ—¶é—´ä¸å¯¹ç§°æ€§è¿›è¡Œé¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbf6492b5ae9187a502a3fb54c0ced14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c712915f68112702ccaaa6c977ee5a1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedBridge-Bridging-Foundation-Vision-Language-Models-to-Medical-Image-Diagnosis"><a href="#MedBridge-Bridging-Foundation-Vision-Language-Models-to-Medical-Image-Diagnosis" class="headerlink" title="MedBridge: Bridging Foundation Vision-Language Models to Medical Image   Diagnosis"></a>MedBridge: Bridging Foundation Vision-Language Models to Medical Image   Diagnosis</h2><p><strong>Authors:Yitong Li, Morteza Ghahremani, Christian Wachinger</strong></p>
<p>Recent vision-language foundation models deliver state-of-the-art results on natural image classification but falter on medical images due to pronounced domain shifts. At the same time, training a medical foundation model requires substantial resources, including extensive annotated data and high computational capacity. To bridge this gap with minimal overhead, we introduce MedBridge, a lightweight multimodal adaptation framework that re-purposes pretrained VLMs for accurate medical image diagnosis. MedBridge comprises three key components. First, a Focal Sampling module that extracts high-resolution local regions to capture subtle pathological features and compensate for the limited input resolution of general-purpose VLMs. Second, a Query Encoder (QEncoder) injects a small set of learnable queries that attend to the frozen feature maps of VLM, aligning them with medical semantics without retraining the entire backbone. Third, a Mixture of Experts mechanism, driven by learnable queries, harnesses the complementary strength of diverse VLMs to maximize diagnostic performance. We evaluate MedBridge on five medical imaging benchmarks across three key adaptation tasks, demonstrating its superior performance in both cross-domain and in-domain adaptation settings, even under varying levels of training data availability. Notably, MedBridge achieved over 6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in multi-label thoracic disease diagnosis, underscoring its effectiveness in leveraging foundation models for accurate and data-efficient medical diagnosis. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ai-med/MedBridge">https://github.com/ai-med/MedBridge</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†ç±»æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ï¼Œä½†ç”±äºæ˜æ˜¾çš„é¢†åŸŸå·®å¼‚ï¼Œåœ¨åŒ»å­¦å›¾åƒæ–¹é¢è¡¨ç°ä¸ä½³ã€‚åŒæ—¶ï¼Œè®­ç»ƒåŒ»å­¦åŸºç¡€æ¨¡å‹éœ€è¦å¤§é‡çš„èµ„æºï¼ŒåŒ…æ‹¬å¤§é‡çš„æ ‡æ³¨æ•°æ®å’Œå¼ºå¤§çš„è®¡ç®—èƒ½åŠ›ã€‚ä¸ºäº†ä»¥æœ€å°çš„é¢å¤–å¼€é”€å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†MedBridgeï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€é€‚åº”æ¡†æ¶ï¼Œç”¨äºå¯¹é¢„è®­ç»ƒçš„VLMè¿›è¡Œå†åˆ©ç”¨ï¼Œä»¥å®ç°å‡†ç¡®çš„åŒ»å­¦å›¾åƒè¯Šæ–­ã€‚MedBridgeåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆï¼ŒFocal Samplingæ¨¡å—ç”¨äºæå–é«˜åˆ†è¾¨ç‡çš„å±€éƒ¨åŒºåŸŸï¼Œä»¥æ•æ‰å¾®å¦™çš„ç—…ç†ç‰¹å¾ï¼Œå¹¶å¼¥è¡¥é€šç”¨VLMçš„æœ‰é™è¾“å…¥åˆ†è¾¨ç‡ã€‚å…¶æ¬¡ï¼ŒæŸ¥è¯¢ç¼–ç å™¨ï¼ˆQEncoderï¼‰æ³¨å…¥äº†ä¸€å°éƒ¨åˆ†å¯å­¦ä¹ çš„æŸ¥è¯¢é¡¹ï¼Œè¿™äº›æŸ¥è¯¢é¡¹å…³æ³¨VLMçš„å†»ç»“ç‰¹å¾å›¾ï¼Œå°†å…¶ä¸åŒ»å­¦è¯­ä¹‰å¯¹é½ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªä¸»å¹²ç½‘ç»œã€‚ç¬¬ä¸‰ï¼Œç”±å¯å­¦ä¹ æŸ¥è¯¢é©±åŠ¨çš„ä¸“å®¶æ··åˆæœºåˆ¶åˆ©ç”¨å¤šç§VLMçš„äº’è¡¥ä¼˜åŠ¿ï¼Œä»¥æœ€å¤§åŒ–è¯Šæ–­æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŒ»å­¦å½±åƒåŸºå‡†ä¸Šå¯¹MedBridgeè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†ä¸‰é¡¹å…³é”®é€‚åº”ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶åœ¨è·¨åŸŸå’ŒåŸŸå†…é€‚åº”è®¾ç½®ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œå³ä½¿åœ¨å„ç§è®­ç»ƒæ•°æ®å¯ç”¨æ€§ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨èƒ¸éƒ¨ç–¾ç—…çš„å¤šæ ‡ç­¾è¯Šæ–­ä¸­ï¼ŒMedBridgeä¸æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹é€‚åº”æ–¹æ³•ç›¸æ¯”ï¼ŒAUCæé«˜äº†6-15%ä»¥ä¸Šï¼Œè¿™çªæ˜¾äº†å…¶åœ¨åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå‡†ç¡®å’Œé«˜æ•ˆåŒ»å­¦è¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai-med/MedBridge%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ai-med/MedBridgeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21698v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„è·¨æ¨¡æ€é€‚åº”æ¡†æ¶MedBridgeï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”¨äºç²¾ç¡®åŒ»å­¦å›¾åƒè¯Šæ–­ã€‚å®ƒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šç”¨äºæ•æ‰ç»†å¾®ç—…ç†ç‰¹å¾çš„Focal Samplingæ¨¡å—ã€é€šè¿‡å…³æ³¨VLMçš„ç‰¹å¾æ˜ å°„ä¸åŒ»å­¦è¯­ä¹‰å¯¹é½çš„Query Encoderï¼ˆQEncoderï¼‰ä»¥åŠåˆ©ç”¨ä¸åŒVLMäº’è¡¥ä¼˜åŠ¿çš„Mixture of Expertsæœºåˆ¶ã€‚åœ¨äº”ä¸ªåŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMedBridgeåœ¨è·¨åŸŸå’ŒåŒåŸŸé€‚åº”è®¾ç½®ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³åœ¨è®­ç»ƒæ•°æ®ä¸åŒå¯ç”¨æ€§çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°è¶…è¿‡ç°æœ‰VLMé€‚åº”æ–¹æ³•çš„æ”¹è¿›ã€‚åœ¨èƒ¸éƒ¨ç–¾ç—…çš„å¤šæ ‡ç­¾è¯Šæ–­ä¸­ï¼ŒMedBridgeçš„AUCæé«˜äº†6-15%ï¼Œçªæ˜¾å…¶åœ¨åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå‡†ç¡®å’Œæ•°æ®é«˜æ•ˆåŒ»å­¦è¯Šæ–­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedBridgeæ˜¯ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€é€‚åº”æ¡†æ¶ï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç”¨äºåŒ»å­¦å›¾åƒè¯Šæ–­ã€‚</li>
<li>MedBridgeåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šFocal Samplingæ¨¡å—ã€Query Encoderï¼ˆQEncoderï¼‰å’ŒMixture of Expertsæœºåˆ¶ã€‚</li>
<li>Focal Samplingæ¨¡å—é€šè¿‡æå–é«˜åˆ†è¾¨ç‡å±€éƒ¨åŒºåŸŸæ¥æ•æ‰ç»†å¾®çš„ç—…ç†ç‰¹å¾ã€‚</li>
<li>Query Encoderï¼ˆQEncoderï¼‰é€šè¿‡å…³æ³¨VLMçš„ç‰¹å¾æ˜ å°„ä¸åŒ»å­¦è¯­ä¹‰å¯¹é½ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ•´ä¸ªåŸºç¡€æ¨¡å‹ã€‚</li>
<li>Mixture of Expertsæœºåˆ¶åˆ©ç”¨ä¸åŒVLMçš„äº’è¡¥ä¼˜åŠ¿ä»¥æé«˜è¯Šæ–­æ€§èƒ½ã€‚</li>
<li>MedBridgeåœ¨äº”ä¸ªåŒ»å­¦æˆåƒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†è·¨åŸŸå’ŒåŒåŸŸé€‚åº”è®¾ç½®ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0842b61daed43153b5d9a1e3755fa007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b35e4182fb1a9615dcad771a5fccb06d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e935ca6ed33a782cdf6e3214fc9c11cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0999e771fac3a192192a2fed30dc46be.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Geometric-Feature-Prompting-of-Image-Segmentation-Models"><a href="#Geometric-Feature-Prompting-of-Image-Segmentation-Models" class="headerlink" title="Geometric Feature Prompting of Image Segmentation Models"></a>Geometric Feature Prompting of Image Segmentation Models</h2><p><strong>Authors:Kenneth Ball, Erin Taylor, Nirav Patel, Andrew Bartels, Gary Koplik, James Polly, Jay Hineman</strong></p>
<p>Advances in machine learning, especially the introduction of transformer architectures and vision transformers, have led to the development of highly capable computer vision foundation models. The segment anything model (known colloquially as SAM and more recently SAM 2), is a highly capable foundation model for segmentation of natural images and has been further applied to medical and scientific image segmentation tasks. SAM relies on prompts â€“ points or regions of interest in an image â€“ to generate associated segmentations.   In this manuscript we propose the use of a geometrically motivated prompt generator to produce prompt points that are colocated with particular features of interest. Focused prompting enables the automatic generation of sensitive and specific segmentations in a scientific image analysis task using SAM with relatively few point prompts. The image analysis task examined is the segmentation of plant roots in rhizotron or minirhizotron images, which has historically been a difficult task to automate. Hand annotation of rhizotron images is laborious and often subjective; SAM, initialized with GeomPrompt local ridge prompts has the potential to dramatically improve rhizotron image processing.   The authors have concurrently released an open source software suite called geomprompt <a target="_blank" rel="noopener" href="https://pypi.org/project/geomprompt/">https://pypi.org/project/geomprompt/</a> that can produce point prompts in a format that enables direct integration with the segment-anything package. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯transformeræ¶æ„å’Œè§†è§‰transformerçš„å¼•å…¥ï¼Œæ¨åŠ¨äº†é«˜åº¦èƒ½åŠ¨çš„è®¡ç®—æœºè§†è§‰åŸºç¡€æ¨¡å‹çš„å¼€å‘ã€‚è¢«ç§°ä¸ºSAMï¼ˆæœ€è¿‘åˆç§°ä¸ºSAM 2ï¼‰çš„ä»»æ„åˆ†å‰²æ¨¡å‹æ˜¯è‡ªç„¶å›¾åƒåˆ†å‰²çš„é«˜åº¦åŸºç¡€æ¨¡å‹ï¼Œå¹¶å·²è¿›ä¸€æ­¥åº”ç”¨äºåŒ»å­¦å’Œç§‘å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚SAMä¾èµ–äºæç¤ºâ€”â€”å›¾åƒä¸­çš„ç‚¹æˆ–æ„Ÿå…´è¶£åŒºåŸŸâ€”â€”æ¥ç”Ÿæˆç›¸å…³çš„åˆ†å‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•åŠ¨æœºçš„æç¤ºç”Ÿæˆå™¨ï¼Œç”¨äºäº§ç”Ÿä¸å›¾åƒç‰¹å®šç‰¹å¾ç›¸å¯¹åº”çš„æç¤ºç‚¹ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æç¤ºï¼Œä½¿ç”¨SAMå’Œç›¸å¯¹è¾ƒå°‘çš„ç‚¹æç¤ºï¼Œå¯ä»¥åœ¨ç§‘å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­è‡ªåŠ¨äº§ç”Ÿæ•æ„Ÿå’Œç‰¹å®šçš„åˆ†å‰²ã€‚æœ¬æ–‡ç ”ç©¶çš„å›¾åƒåˆ†æä»»åŠ¡æ˜¯æ ¹å®¤æˆ–å¾®å‹æ ¹å®¤å›¾åƒçš„æ¤ç‰©æ ¹åˆ†å‰²ï¼Œè¿™å†æ¥æ˜¯è‡ªåŠ¨åŒ–éš¾åº¦è¾ƒå¤§çš„ä»»åŠ¡ã€‚æ ¹å®¤å›¾åƒçš„æ‰‹åŠ¨æ³¨é‡Šæ˜¯è´¹æ—¶ä¸”ä¸»è§‚çš„ï¼›ä½¿ç”¨GeomPromptåˆå§‹åŒ–çš„SAMæœ‰æ½œåŠ›æå¤§åœ°æ”¹å–„æ ¹å®¤å›¾åƒçš„å¤„ç†ã€‚ä½œè€…åŒæ—¶å‘å¸ƒäº†ä¸€ä¸ªåä¸ºgeompromptçš„å¼€æºè½¯ä»¶å¥—ä»¶ï¼Œè¯¥è½¯ä»¶å¥—ä»¶å¯ä»¥äº§ç”Ÿç‚¹æç¤ºï¼Œä»¥æ ¼å¼ç›´æ¥é›†æˆåˆ°åˆ†æ®µè½¯ä»¶åŒ…ä¸­ã€‚æ‚¨å¯ä»¥è®¿é—®geomprompt at <a target="_blank" rel="noopener" href="https://pypi.org/project/geomprompt/">https://pypi.org/project/geomprompt/</a> äº†è§£è¯¦æƒ…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21644v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯transformeræ¶æ„å’Œè§†è§‰transformerçš„å¼•å…¥ï¼Œæ¨åŠ¨äº†é«˜åº¦èƒ½å¹²çš„è®¡ç®—æœºè§†è§‰åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆä¿—ç§°SAMï¼Œæœ€è¿‘å‡çº§ä¸ºSAM 2ï¼‰æ˜¯å¯¹è‡ªç„¶å›¾åƒè¿›è¡Œåˆ†å‰²çš„é«˜åº¦èƒ½å¹²çš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶å·²è¿›ä¸€æ­¥åº”ç”¨äºåŒ»å­¦å’Œç§‘å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚SAMé€šè¿‡æç¤ºâ€”â€”å›¾åƒä¸­çš„ç‚¹æˆ–æ„Ÿå…´è¶£åŒºåŸŸâ€”â€”æ¥ç”Ÿæˆç›¸å…³çš„åˆ†å‰²ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å‡ ä½•åŠ¨æœºæç¤ºç”Ÿæˆå™¨æ¥äº§ç”Ÿä¸å›¾åƒç‰¹å®šç‰¹å¾å…±å®šä½çš„æç¤ºç‚¹ã€‚é€šè¿‡èšç„¦æç¤ºï¼Œä½¿ç”¨SAMå’Œç›¸å¯¹è¾ƒå°‘çš„ç‚¹æç¤ºï¼Œå¯ä»¥åœ¨ç§‘å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­è‡ªåŠ¨ç”Ÿæˆæ•æ„Ÿå’Œç‰¹å®šçš„åˆ†å‰²ã€‚æ‰€ç ”ç©¶çš„å›¾åƒåˆ†æä»»åŠ¡æ˜¯åˆ†å‰²æ¤ç‰©æ ¹åœ¨rhizotronæˆ–minirhizotronå›¾åƒä¸­çš„éƒ¨åˆ†ï¼Œè¿™å†æ¥æ˜¯è‡ªåŠ¨åŒ–å›°éš¾çš„ã€‚æ‰‹å·¥æ ‡æ³¨rhizotronå›¾åƒæ˜¯ç¹é‡ä¸”ä¸»è§‚çš„ï¼›ä½¿ç”¨ç”±GeomPromptæœ¬åœ°è„Šæç¤ºåˆå§‹åŒ–çš„SAMï¼Œæœ‰æœ›æ˜¾è‘—æ”¹å–„rhizotronå›¾åƒå¤„ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. æœºå™¨å­¦ä¹ è¿›æ­¥æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰åŸºç¡€æ¨¡å‹çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰åœ¨å›¾åƒåˆ†å‰²æ–¹é¢çš„èƒ½åŠ›æ˜¾è‘—ã€‚
 2. SAMé€šè¿‡æç¤ºç”Ÿæˆç›¸å…³åˆ†å‰²ï¼Œè¿™äº›æç¤ºå¯ä»¥æ˜¯å›¾åƒä¸­çš„ç‚¹æˆ–æ„Ÿå…´è¶£åŒºåŸŸã€‚
 3. å‡ ä½•åŠ¨æœºæç¤ºç”Ÿæˆå™¨èƒ½äº§ç”Ÿä¸å›¾åƒç‰¹å®šç‰¹å¾å…±å®šä½çš„æç¤ºç‚¹ï¼Œæé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚
 4. èšç„¦æç¤ºèƒ½ä½¿ç”¨ç›¸å¯¹è¾ƒå°‘çš„ç‚¹æç¤ºåœ¨ç§‘å­¦å›¾åƒåˆ†æä»»åŠ¡ä¸­è‡ªåŠ¨ç”Ÿæˆæ•æ„Ÿå’Œç‰¹å®šçš„åˆ†å‰²ã€‚
 5. æ¤ç‰©æ ¹åœ¨rhizotronæˆ–minirhizotronå›¾åƒä¸­çš„åˆ†å‰²æ˜¯ä¸€ä¸ªå›°éš¾çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚
 6. æ‰‹å·¥æ ‡æ³¨rhizotronå›¾åƒæ˜¯ç¹é‡ä¸”ä¸»è§‚çš„ï¼Œä½¿ç”¨SAMæœ‰æ½œåŠ›æ”¹å–„è¿™ä¸€æƒ…å†µã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba40a823c867f1b2c1bc1a1c03b78f3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a15fd989085d1a365f9b2e07d057b52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f68defd2cd65a4f11911b076f780a133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-830c91ed5e48bed6ea8bd22b1140fdb5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Optimizing-Deep-Learning-for-Skin-Cancer-Classification-A-Computationally-Efficient-CNN-with-Minimal-Accuracy-Trade-Off"><a href="#Optimizing-Deep-Learning-for-Skin-Cancer-Classification-A-Computationally-Efficient-CNN-with-Minimal-Accuracy-Trade-Off" class="headerlink" title="Optimizing Deep Learning for Skin Cancer Classification: A   Computationally Efficient CNN with Minimal Accuracy Trade-Off"></a>Optimizing Deep Learning for Skin Cancer Classification: A   Computationally Efficient CNN with Minimal Accuracy Trade-Off</h2><p><strong>Authors:Abdullah Al Mamun, Pollob Chandra Ray, Md Rahat Ul Nasib, Akash Das, Jia Uddin, Md Nurul Absur</strong></p>
<p>The rapid advancement of deep learning in medical image analysis has greatly enhanced the accuracy of skin cancer classification. However, current state-of-the-art models, especially those based on transfer learning like ResNet50, come with significant computational overhead, rendering them impractical for deployment in resource-constrained environments. This study proposes a custom CNN model that achieves a 96.7% reduction in parameters (from 23.9 million in ResNet50 to 692,000) while maintaining a classification accuracy deviation of less than 0.022%. Our empirical analysis of the HAM10000 dataset reveals that although transfer learning models provide a marginal accuracy improvement of approximately 0.022%, they result in a staggering 13,216.76% increase in FLOPs, considerably raising computational costs and inference latency. In contrast, our lightweight CNN architecture, which encompasses only 30.04 million FLOPs compared to ResNet50â€™s 4.00 billion, significantly reduces energy consumption, memory footprint, and inference time. These findings underscore the trade-off between the complexity of deep models and their real-world feasibility, positioning our optimized CNN as a practical solution for mobile and edge-based skin cancer diagnostics. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ·±åº¦å­¦ä¹ å¿«é€Ÿå‘å±•å¤§å¤§æé«˜äº†çš®è‚¤ç™Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åŸºäºè¿ç§»å­¦ä¹ ï¼ˆå¦‚ResNet50ï¼‰çš„æ¨¡å‹ï¼Œå­˜åœ¨å·¨å¤§çš„è®¡ç®—å¼€é”€ï¼Œä½¿å¾—å®ƒä»¬åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²ä¸åˆ‡å®é™…ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªå®šä¹‰çš„CNNæ¨¡å‹ï¼Œåœ¨ä¿æŒåˆ†ç±»ç²¾åº¦åå·®å°äº0.022%çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹å‚æ•°çš„96.7%çš„ç¼©å‡ï¼ˆä»ResNet50ä¸­çš„2390ä¸‡å‡å°‘åˆ°69ä¸‡ï¼‰ã€‚æˆ‘ä»¬å¯¹HAM10000æ•°æ®é›†çš„ç»éªŒåˆ†æè¡¨æ˜ï¼Œå°½ç®¡è¿ç§»å­¦ä¹ æ¨¡å‹æä¾›äº†çº¦0.022%çš„è½»å¾®ç²¾åº¦æ”¹è¿›ï¼Œä½†å®ƒä»¬å¯¼è‡´æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰æƒŠäººåœ°å¢åŠ äº†13216.76%ï¼Œå¤§å¤§æé«˜äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„è½»é‡çº§CNNæ¶æ„ä»…åŒ…å«3.00äº¿FLOPsï¼Œä¸ResNet50çš„4äº¿ç›¸æ¯”å¤§å¹…é™ä½ï¼Œæ˜¾è‘—å‡å°‘äº†èƒ½è€—ã€å†…å­˜å ç”¨å’Œæ¨ç†æ—¶é—´ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ·±åº¦æ¨¡å‹å¤æ‚æ€§ä¸å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„å¯è¡Œæ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä½¿ç»è¿‡ä¼˜åŒ–çš„CNNæˆä¸ºç§»åŠ¨å’Œè¾¹ç¼˜è®¡ç®—ä¸ºåŸºç¡€çš„çš®è‚¤ç™Œè¯Šæ–­çš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21597v1">PDF</a> 6 pages, &amp; 7 Images</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¿«é€Ÿå‘å±•å¤§å¤§æé«˜äº†çš®è‚¤ç™Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚åŸºäºè¿ç§»å­¦ä¹ çš„ResNet50ï¼Œå­˜åœ¨è¾ƒå¤§çš„è®¡ç®—å¼€é”€ï¼Œä¸é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªå®šä¹‰çš„CNNæ¨¡å‹ï¼Œåœ¨ä¿æŒåˆ†ç±»ç²¾åº¦åå·®å°äº0.022%çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹ResNet50æ¨¡å‹çš„96.7%çš„å‚æ•°ç¼©å‡ã€‚é€šè¿‡å¯¹HAM10000æ•°æ®é›†çš„ç»éªŒåˆ†æï¼Œå‘ç°è¿ç§»å­¦ä¹ æ¨¡å‹è™½ç„¶åªæé«˜äº†çº¦0.022%çš„ç²¾åº¦ï¼Œä½†å´å¯¼è‡´äº†FLOPså¢åŠ äº†æƒŠäººçš„13216.76%ï¼Œå¤§å¹…å¢åŠ äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚ç›¸åï¼Œæˆ‘ä»¬çš„è½»é‡çº§CNNæ¶æ„åªåŒ…å«30.04äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼Œä¸ResNet50çš„40äº¿æ¬¡ç›¸æ¯”ï¼Œæ˜¾è‘—å‡å°‘äº†èƒ½è€—ã€å†…å­˜å ç”¨å’Œæ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸæé«˜äº†çš®è‚¤ç™Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ¨¡å‹å¦‚ResNet50è™½ç„¶ç²¾åº¦é«˜ï¼Œä½†åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹ä¸é€‚ç”¨ï¼Œå­˜åœ¨è¾ƒå¤§çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>è‡ªå®šä¹‰çš„CNNæ¨¡å‹å®ç°äº†æ˜¾è‘—å‚æ•°ç¼©å‡ï¼ˆ96.7%ï¼‰ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„åˆ†ç±»ç²¾åº¦åå·®ï¼ˆ&lt; 0.022%ï¼‰ã€‚</li>
<li>è¿ç§»å­¦ä¹ æ¨¡å‹è™½èƒ½æé«˜ç²¾åº¦ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¯¼è‡´FLOPså¤§å¹…å¢åŠ ã€‚</li>
<li>è½»é‡çº§CNNæ¶æ„æ˜¾è‘—å‡å°‘èƒ½è€—ã€å†…å­˜å ç”¨å’Œæ¨ç†æ—¶é—´ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ·±åº¦æ¨¡å‹å¤æ‚æ€§ä¸å®é™…å¯è¡Œæ€§ä¹‹é—´çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6b872c724140b655c4d4f988bf3cb82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e07c59d3988033e0278f73b2719858.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bda764800be481c9803d2f04b058dc6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f622a2af181719a919157047f487528a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894751765f759957cfcdfbe71c1a9af2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b30cffcb3151e964fddd5e42da791582.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d4c856b39137d632c5e621835da585e0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  Tell me Habibi, is it Real or Fake?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-30/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8743d83b99b208a00280e759a4cf6fcd.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-30  SPIRAL Semantic-Aware Progressive LiDAR Scene Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
