<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  Dynamic Low-Rank Sparse Adaptation for Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ab40690d2dffabbf7fb8b5eb38abf2c0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-22-æ›´æ–°"><a href="#2025-02-22-æ›´æ–°" class="headerlink" title="2025-02-22 æ›´æ–°"></a>2025-02-22 æ›´æ–°</h1><h2 id="Dynamic-Low-Rank-Sparse-Adaptation-for-Large-Language-Models"><a href="#Dynamic-Low-Rank-Sparse-Adaptation-for-Large-Language-Models" class="headerlink" title="Dynamic Low-Rank Sparse Adaptation for Large Language Models"></a>Dynamic Low-Rank Sparse Adaptation for Large Language Models</h2><p><strong>Authors:Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji</strong></p>
<p>Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA), a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by 16.32$%$, achieving a 2.60$\times$ speedup on CPU and 2.23$\times$ speedup on GPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wzhuang-xmu/LoSA">https://github.com/wzhuang-xmu/LoSA</a>. </p>
<blockquote>
<p>å°½ç®¡ç½‘ç»œç¨€ç–æ€§åœ¨ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²å‹åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ€§èƒ½ä»æœ‰æ‰€é™ä½ã€‚å°†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åº”ç”¨äºå¾®è°ƒç¨€ç–LLMä¸ºè§£å†³è¿™ä¸€å›°å¢ƒæä¾›äº†ç›´è§‚çš„æ–¹æ³•ï¼Œä½†å®ƒä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼ŒåŒ…æ‹¬ï¼š1ï¼‰æ— æ³•åœ¨è®­ç»ƒåçš„ç¨€ç–LLMä¸­é›†æˆLoRAæƒé‡ï¼›2ï¼‰åœ¨é«˜ç¨€ç–æ¯”ç‡ä¸‹æ€§èƒ½æ¢å¤ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŠ¨æ€ä½ç§©ç¨€ç–é€‚åº”ï¼ˆLoSAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°†ä½ç§©é€‚åº”æ— ç¼é›†æˆåˆ°LLMç¨€ç–æ€§çš„ç»Ÿä¸€æ¡†æ¶ä¸­çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç¨€ç–LLMçš„æ€§èƒ½ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†å»¶è¿Ÿã€‚ç‰¹åˆ«æ˜¯ï¼ŒLoSAåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ ¹æ®ç›¸åº”çš„ç¨€ç–æƒé‡åŠ¨æ€åœ°ç¨€ç–åŒ–LoRAçš„ç»“æœï¼Œä»è€Œç¡®ä¿LoRAæ¨¡å—å¯ä»¥åœ¨è®­ç»ƒåçš„ç¨€ç–LLMä¸­é›†æˆã€‚æ­¤å¤–ï¼ŒLoSAåˆ©ç”¨è¡¨ç¤ºäº’ä¿¡æ¯ï¼ˆRMIï¼‰ä½œä¸ºæŒ‡æ ‡æ¥ç¡®å®šå±‚çš„é‡è¦æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¡®å®šå¾®è°ƒè¿‡ç¨‹ä¸­çš„é€å±‚ç¨€ç–ç‡ã€‚åŸºäºæ­¤ï¼ŒLoSAæ ¹æ®é€å±‚é‡å»ºè¯¯å·®çš„å˜åŒ–æ¥è°ƒæ•´LoRAæ¨¡å—çš„ç§©ï¼Œä¸ºæ¯å±‚åˆ†é…é€‚å½“çš„å¾®è°ƒï¼Œä»¥å‡å°‘å¯†é›†å’Œç¨€ç–LLMä¹‹é—´çš„è¾“å‡ºå·®å¼‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLoSAå¯ä»¥åœ¨å‡ å°æ—¶å†…æœ‰æ•ˆæé«˜ç¨€ç–LLMçš„æ•ˆç‡ï¼Œè€Œä¸ä¼šå¼•å…¥ä»»ä½•é¢å¤–çš„æ¨ç†è´Ÿæ‹…ã€‚ä¾‹å¦‚ï¼ŒLoSAå°†ç¨€ç–LLaMA-2-7Bçš„å›°æƒ‘åº¦é™ä½äº†68.73%ï¼Œé›¶å°„å‡†ç¡®åº¦æé«˜äº†16.32%ï¼Œåœ¨CPUä¸Šå®ç°äº†2.60å€çš„åŠ é€Ÿï¼Œåœ¨GPUä¸Šå®ç°äº†2.23å€çš„åŠ é€Ÿï¼Œåªéœ€åœ¨å•ä¸ªNVIDIA A100 80GB GPUä¸Šè¿›è¡Œ45åˆ†é’Ÿçš„å¾®è°ƒå³å¯ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/wzhuang-xmu/LoSA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wzhuang-xmu/LoSAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14816v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºåŠ¨æ€ä½ç§©ç¨€ç–é€‚é…ï¼ˆLoSAï¼‰æ–¹æ³•ï¼Œå°†ä½ç§©é€‚é…æŠ€æœ¯æ— ç¼é›†æˆåˆ°LLMç¨€ç–æ¨¡å‹ä¸­ï¼Œæå‡ç¨€ç–LLMæ€§èƒ½ä¸”ä¸ä¼šå¢åŠ æ¨ç†å»¶è¿Ÿã€‚LoSAé€šè¿‡åŠ¨æ€ç¨€ç–åŒ–LoRAç»“æœã€åˆ©ç”¨è¡¨ç¤ºäº’ä¿¡æ¯ï¼ˆRMIï¼‰ç¡®å®šå±‚é‡è¦æ€§åŠè°ƒæ•´LoRAæ¨¡å—ç§©ç­‰æ–¹æ³•ï¼Œä¼˜åŒ–äº†LoRAåœ¨ç¨€ç–LLMä¸­çš„é›†æˆæ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒLoSAèƒ½æ˜¾è‘—æé«˜ç¨€ç–LLMæ•ˆç‡ï¼Œå¦‚LLaMA-2-7Bçš„è¯è°œå‡å°‘68.73%ï¼Œé›¶æ ·æœ¬å‡†ç¡®ç‡æé«˜16.32%ï¼Œå¹¶åœ¨CPUå’ŒGPUä¸Šå®ç°åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘ç»œç¨€ç–æ€§è™½ç„¶èƒ½ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éƒ¨ç½²å‹åŠ›ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>LoSAæ–¹æ³•å°†ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æŠ€æœ¯é›†æˆåˆ°LLMç¨€ç–æ¨¡å‹ä¸­ï¼Œæé«˜äº†ç¨€ç–LLMçš„æ€§èƒ½ã€‚</li>
<li>LoSAé€šè¿‡åŠ¨æ€ç¨€ç–åŒ–LoRAç»“æœï¼Œä¿è¯äº†LoRAæ¨¡å—å¯ä»¥åœ¨ç¨€ç–LLMè®­ç»ƒåè¿›è¡Œé›†æˆã€‚</li>
<li>LoSAåˆ©ç”¨è¡¨ç¤ºäº’ä¿¡æ¯ï¼ˆRMIï¼‰æ¥ç¡®å®šå±‚çš„é‡è¦æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°ç¡®å®šå¾®è°ƒæœŸé—´çš„å±‚çº§ç¨€ç–ç‡ã€‚</li>
<li>LoSAæ ¹æ®å±‚é—´é‡å»ºè¯¯å·®çš„å˜å¼‚æ€§è°ƒæ•´LoRAæ¨¡å—çš„ç§©ï¼Œä¸ºæ¯å±‚åˆ†é…é€‚å½“çš„å¾®è°ƒï¼Œä»¥å‡å°‘å¯†é›†å’Œç¨€ç–LLMä¹‹é—´çš„è¾“å‡ºå·®å¼‚ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLoSAèƒ½æ˜¾è‘—æé«˜ç¨€ç–LLMçš„æ•ˆç‡ï¼Œå¦‚LLaMA-2-7Bçš„è¯è°œå‡å°‘å’Œé›¶æ ·æœ¬å‡†ç¡®ç‡çš„æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8ccb21d8d66b9b80f9f55438c682f61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f471e1e3bbc0f54f98ec2319512020a3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Similarity-Paradigm-Through-Textual-Regularization-Without-Forgetting"><a href="#A-Similarity-Paradigm-Through-Textual-Regularization-Without-Forgetting" class="headerlink" title="A Similarity Paradigm Through Textual Regularization Without Forgetting"></a>A Similarity Paradigm Through Textual Regularization Without Forgetting</h2><p><strong>Authors:Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu</strong></p>
<p>Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²æˆä¸ºå°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚è™½ç„¶ä¼˜åŒ–ä¸Šä¸‹æ–‡å¯¹äºæé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½å¯èƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒå¾€å¾€ä¼šå¯¼è‡´åœ¨æ¥è‡ªä¸åŒåˆ†å¸ƒçš„æœªè§ç±»åˆ«æˆ–æ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚è¿™å¯èƒ½æ˜¯ç”±äºæ–‡æœ¬æç¤ºå€¾å‘äºè¿‡åº¦æ‹Ÿåˆä¸‹æ¸¸æ•°æ®åˆ†å¸ƒï¼Œå¯¼è‡´å¿˜è®°æ‰‹å·¥åˆ¶ä½œçš„æç¤ºæ‰€è¡ç”Ÿçš„é€šç”¨çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå¸¦æ–‡æœ¬æ­£åˆ™åŒ–çš„ç›¸ä¼¼æ€§èŒƒå¼ï¼ˆSPTRï¼‰ï¼Œç”¨äºæ— é—å¿˜çš„æç¤ºå­¦ä¹ ã€‚SPTRæ˜¯ä¸€ä¸ªåŸºäºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºçš„ä¸¤é¢è®¾è®¡ï¼Œæ˜¯ä¸€ä¸ªä¸å¯åˆ†å‰²çš„æ¡†æ¶ã€‚1ï¼‰ä¸ºäº†é¿å…å¿˜è®°ä¸€èˆ¬çš„æ–‡æœ¬çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœ€ä¼˜ä¼ è¾“ä½œä¸ºæ–‡æœ¬æ­£åˆ™åŒ–ï¼Œä»¥ç²¾ç»†åœ°ç¡®ä¿ä¸æ‰‹å·¥åˆ¶ä½œçš„ç‰¹å¾å’Œè°ƒæ•´æ–‡æœ¬ç‰¹å¾çš„è¿‘ä¼¼ã€‚2ï¼‰ä¸ºäº†ä¸æ–­é‡Šæ”¾å¤šä¸ªæ‰‹å·¥åˆ¶ä½œçš„æç¤ºçš„é€šç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªç„¶å¯¹é½å¾—åˆ†å’Œå¯¹æŠ—æ€§å¯¹é½å¾—åˆ†çš„ç›¸ä¼¼æ€§èŒƒå¼ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸¤ä¸ªæ¨¡å—çš„å…±åŒç›®æ ‡æ˜¯è§£å†³æ³›åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä»å¤šä¸ªæ‰‹å·¥åˆ¶ä½œçš„æç¤ºä¸­è·å¾—çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨11ä¸ªæ•°æ®é›†çš„å››ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼ˆå³éæ³›åŒ–å°æ ·æœ¬å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†æ³›åŒ–å’ŒåŸŸæ³›åŒ–ï¼‰ä¸Šè¡¨æ˜ï¼ŒSPTRçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14376v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPTRçš„æ–°å‹æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨é€‚åº”ä¸‹æ¸¸ä»»åŠ¡æ—¶çš„æ³›åŒ–é—®é¢˜ã€‚SPTRé€šè¿‡å¼•å…¥æ–‡æœ¬æ­£åˆ™åŒ–å’Œç›¸ä¼¼æ€§èŒƒå¼ï¼Œç¡®ä¿æ¨¡å‹åœ¨åˆ©ç”¨æ‰‹å·¥æç¤ºçš„åŒæ—¶ï¼Œé¿å…é—å¿˜é€šç”¨æ–‡æœ¬çŸ¥è¯†ï¼Œå¹¶æé«˜æ¨¡å‹å¯¹ä¸åŒæ•°æ®é›†å’Œåˆ†å¸ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç¤ºå­¦ä¹ æ˜¯é€‚åº”é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>æ–‡æœ¬æç¤ºå¯èƒ½å¯¼è‡´æ¨¡å‹å¯¹ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ä¼˜åŒ–ï¼Œä½†æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚</li>
<li>SPTRæ˜¯ä¸€ç§åŸºäºæ‰‹å·¥æç¤ºçš„æ–°å‹æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ³›åŒ–é—®é¢˜ã€‚</li>
<li>SPTRä½¿ç”¨æ–‡æœ¬æ­£åˆ™åŒ–æ¥ç¡®ä¿æ¨¡å‹ä¸æ‰‹å·¥ç‰¹å¾çš„ç²¾ç»†å¯¹é½ï¼Œé¿å…é—å¿˜é€šç”¨æ–‡æœ¬çŸ¥è¯†ã€‚</li>
<li>SPTRå¼•å…¥ç›¸ä¼¼æ€§èŒƒå¼ï¼Œé€šè¿‡è‡ªç„¶å¯¹é½å¾—åˆ†å’Œå¯¹æŠ—æ€§å¯¹é½å¾—åˆ†æ¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SPTRåœ¨ä¸¤ä¸ªä¸»è¦ç›®æ ‡ä¸Šè§£å†³äº†æ³›åŒ–é—®é¢˜ï¼Œæ—¨åœ¨ä»å¤šä¸ªæ‰‹å·¥æç¤ºä¸­æœ€å¤§é™åº¦åœ°æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9a6cfe0ed935a2b230f4bbc63cd0f39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ca0670cbf65a815276bae63f283e896.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-850fe4ecb7969a76df28ea542cab15f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79c4d3f7f00da30844efae3481bd785f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26942adab36734fbc0c96371ae565df0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-131d0a7b7cd8e42c27e14d985dc229fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d438bb3c665833358ce8da713d9500a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Asymmetric-Co-Training-for-Source-Free-Few-Shot-Domain-Adaptation"><a href="#Asymmetric-Co-Training-for-Source-Free-Few-Shot-Domain-Adaptation" class="headerlink" title="Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation"></a>Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation</h2><p><strong>Authors:Gengxu Li, Yuan Wu</strong></p>
<p>Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data. However, SFUDA approaches come with inherent limitations that are frequently overlooked. These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the source domain, or when sufficient unlabeled target data is unavailable-a common situation in real-world applications. To address these issues, we propose an asymmetric co-training (ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a more practical alternative to SFUDA, as gathering a few labeled target instances is more feasible than acquiring large volumes of unlabeled target data in many real-world contexts. Our ACT method begins by employing a weak-strong augmentation to enhance data diversity. Then we use a two-step optimization process to train the target model. In the first step, we optimize the label smoothing cross-entropy loss, the entropy of the class-conditional distribution, and the reverse-entropy loss to bolster the modelâ€™s discriminative ability while mitigating overfitting. The second step focuses on reducing redundancy in the output space by minimizing classifier determinacy disparity. Extensive experiments across four benchmarks demonstrate the superiority of our ACT approach, which outperforms state-of-the-art SFUDA methods and transfer learning techniques. Our findings suggest that adapting a source pre-trained model using only a small amount of labeled target data offers a practical and dependable solution. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gengxuli/ACT">https://github.com/gengxuli/ACT</a>. </p>
<blockquote>
<p>æºæ•°æ®æ— å…³çš„æ— äººç›‘ç£é¢†åŸŸè‡ªé€‚åº”ï¼ˆSFUDAï¼‰ä½œä¸ºä¸€ç§æ›¿ä»£ä¼ ç»Ÿæ— äººç›‘ç£é¢†åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰çš„æ–¹æ³•ï¼Œå·²ç»å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ä¼ ç»Ÿæ— äººç›‘ç£é¢†åŸŸè‡ªé€‚åº”ä¾èµ–äºå§‹ç»ˆå¯ç”¨çš„æœ‰æ ‡ç­¾æºæ•°æ®ã€‚ç„¶è€Œï¼ŒSFUDAæ–¹æ³•å¸¦æœ‰ç»å¸¸è¢«å¿½è§†çš„å†…åœ¨å±€é™æ€§ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬åœ¨æ— æ ‡ç­¾ç›®æ ‡æ•°æ®æ— æ³•æ»¡è¶³å…³é”®å‡è®¾æ—¶æ€§èƒ½ä¸‹é™ï¼Œä¾‹å¦‚å½“å°é—­é›†æ ‡ç­¾åˆ†å¸ƒä¸æºåŸŸç›¸åŒæ—¶ï¼Œæˆ–è€…åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­ç»å¸¸å‡ºç°è¶³å¤Ÿçš„æ— æ ‡ç­¾ç›®æ ‡æ•°æ®ä¸å¯ç”¨çš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“ä¸ºSFFSDAåœºæ™¯è®¾è®¡çš„å¯¹ç§°ååŒè®­ç»ƒï¼ˆACTï¼‰æ–¹æ³•ã€‚SFFSDAä¸ºSFUDAæä¾›äº†ä¸€ä¸ªæ›´å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºåœ¨è®¸å¤šç°å®ä¸–ç•Œçš„æƒ…å¢ƒä¸­ï¼Œæ”¶é›†å°‘é‡æœ‰æ ‡ç­¾çš„ç›®æ ‡å®ä¾‹æ¯”è·å–å¤§é‡æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®æ›´ä¸ºå¯è¡Œã€‚æˆ‘ä»¬çš„ACTæ–¹æ³•é¦–å…ˆé‡‡ç”¨å¼±å¼ºå¢å¼ºæ³•æ¥æé«˜æ•°æ®å¤šæ ·æ€§ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ä¸¤æ­¥ä¼˜åŒ–è¿‡ç¨‹æ¥è®­ç»ƒç›®æ ‡æ¨¡å‹ã€‚ç¬¬ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä¼˜åŒ–æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±ã€ç±»æ¡ä»¶åˆ†å¸ƒçš„ç†µå’Œåå‘ç†µæŸå¤±ï¼Œä»¥æé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å¹¶å‡è½»è¿‡æ‹Ÿåˆç°è±¡ã€‚ç¬¬äºŒæ­¥ä¸“æ³¨äºé€šè¿‡æœ€å°åŒ–åˆ†ç±»å™¨ç¡®å®šæ€§å·®å¼‚æ¥å‡å°‘è¾“å‡ºç©ºé—´çš„å†—ä½™ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ACTæ–¹æ³•è¡¨ç°å“è¶Šï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„SFUDAæ–¹æ³•å’Œè¿ç§»å­¦ä¹ æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨å°‘é‡æœ‰æ ‡ç­¾çš„ç›®æ ‡æ•°æ®å¯¹æºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé€‚åº”è°ƒæ•´æä¾›äº†ä¸€ç§å®ç”¨å¯é çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/gengxuli/ACT">https://github.com/gengxuli/ACT</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14214v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†æ— æºæ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆSFUDAï¼‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹SFUDAçš„å¯¹ç§°ååŒè®­ç»ƒï¼ˆACTï¼‰æ–¹æ³•ã€‚ACTæ–¹æ³•é€šè¿‡å¼±å¼ºå¢å¼ºæ•°æ®å¤šæ ·æ€§ï¼Œé‡‡ç”¨ä¸¤æ­¥ä¼˜åŒ–è¿‡ç¨‹è®­ç»ƒç›®æ ‡æ¨¡å‹ï¼Œä¼˜åŒ–æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±ã€ç±»æ¡ä»¶åˆ†å¸ƒçš„ç†µå’Œåå‘ç†µæŸå¤±ï¼Œä»¥æé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒACTæ–¹æ³•ä¼˜äºæœ€æ–°çš„SFUDAæ–¹æ³•å’Œè¿ç§»å­¦ä¹ æŠ€æœ¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨å°‘é‡ç›®æ ‡æ•°æ®å¯¹æºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¯ä¸€ç§å®ç”¨å¯é çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æºè‡ªç”±æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆSFUDAï¼‰æ–¹æ³•å—åˆ°å…³æ³¨ï¼Œä½œä¸ºä¼ ç»Ÿæ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†SFUDAæ–¹æ³•å­˜åœ¨æ€§èƒ½ä¸‹é™çš„å±€é™æ€§ã€‚</li>
<li>å½“ç›®æ ‡æ•°æ®ä¸æ»¡è¶³å…³é”®å‡è®¾ï¼ˆå¦‚æ ‡ç­¾åˆ†å¸ƒä¸æºåŸŸç›¸åŒï¼‰æˆ–ç›®æ ‡æ•°æ®ä¸è¶³æ—¶ï¼ŒSFUDAæ€§èƒ½ä¼šå—åˆ°å½±å“ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹SFUDAåœºæ™¯çš„å¯¹ç§°ååŒè®­ç»ƒï¼ˆACTï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼±å¼ºå¢å¼ºæ•°æ®å¤šæ ·æ€§ï¼Œé‡‡ç”¨ä¸¤æ­¥ä¼˜åŒ–è¿‡ç¨‹è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ACTæ–¹æ³•ä¼˜åŒ–äº†æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±ã€ç±»æ¡ä»¶åˆ†å¸ƒçš„ç†µå’Œåå‘ç†µæŸå¤±ï¼Œæé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å¹¶å‡å°‘è¿‡æ‹Ÿåˆã€‚</li>
<li>ACTæ–¹æ³•åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºæœ€æ–°çš„SFUDAæ–¹æ³•å’Œè¿ç§»å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨å°‘é‡ç›®æ ‡æ•°æ®å¯¹æºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¯ä¸€ç§å®ç”¨å¯é çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7da82a5a7107720afc8fa95f6a5a8fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e3106e3e8d76d8af336bd408bdb9dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b1c1d06c5b226c6b2d195f12719bb4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dual-level-Mixup-for-Graph-Few-shot-Learning-with-Fewer-Tasks"><a href="#Dual-level-Mixup-for-Graph-Few-shot-Learning-with-Fewer-Tasks" class="headerlink" title="Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks"></a>Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks</h2><p><strong>Authors:Yonghao Liu, Mengyu Li, Fausto Giunchiglia, Lan Huang, Ximing Li, Xiaoyue Feng, Renchu Guan</strong></p>
<p>Graph neural networks have been demonstrated as a powerful paradigm for effectively learning graph-structured data on the web and mining content from it.Current leading graph models require a large number of labeled samples for training, which unavoidably leads to overfitting in few-shot scenarios. Recent research has sought to alleviate this issue by simultaneously leveraging graph learning and meta-learning paradigms. However, these graph meta-learning models assume the availability of numerous meta-training tasks to learn transferable meta-knowledge. Such assumption may not be feasible in the real world due to the difficulty of constructing tasks and the substantial costs involved. Therefore, we propose a SiMple yet effectIve approach for graph few-shot Learning with fEwer tasks, named SMILE. We introduce a dual-level mixup strategy, encompassing both within-task and across-task mixup, to simultaneously enrich the available nodes and tasks in meta-learning. Moreover, we explicitly leverage the prior information provided by the node degrees in the graph to encode expressive node representations. Theoretically, we demonstrate that SMILE can enhance the model generalization ability. Empirically, SMILE consistently outperforms other competitive models by a large margin across all evaluated datasets with in-domain and cross-domain settings. Our anonymous code can be found here. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œå·²è¢«è¯æ˜æ˜¯åœ¨Webä¸Šæœ‰æ•ˆå­¦ä¹ å›¾ç»“æ„æ•°æ®å’Œä»ä¸­æŒ–æ˜å†…å®¹çš„æœ‰åŠ›èŒƒå¼ã€‚å½“å‰é¢†å…ˆçš„å›¾æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡ç­¾æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨å°‘é‡æ ·æœ¬åœºæ™¯ä¸­ä¸å¯é¿å…åœ°å¯¼è‡´è¿‡æ‹Ÿåˆã€‚æœ€è¿‘çš„ç ”ç©¶è¯•å›¾é€šè¿‡åŒæ—¶åˆ©ç”¨å›¾å­¦ä¹ å’Œå…ƒå­¦ä¹ èŒƒå¼æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›å›¾å…ƒå­¦ä¹ æ¨¡å‹å‡è®¾å­˜åœ¨å¤§é‡çš„å…ƒè®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ å¯è¿ç§»çš„å…ƒçŸ¥è¯†ã€‚ç”±äºæ„å»ºä»»åŠ¡çš„éš¾åº¦å’Œæ¶‰åŠçš„å¤§é‡æˆæœ¬ï¼Œè¿™ç§å‡è®¾åœ¨ç°å®ä¸­å¯èƒ½ä¸å¯è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å›¾å°æ ·å­¦ä¹ æ–°æ–¹æ³•ï¼Œåä¸ºSMILEï¼ˆç”¨äºå°æ ·ä»»åŠ¡çš„å›¾å­¦ä¹ ï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒçº§æ··åˆç­–ç•¥ï¼ŒåŒ…æ‹¬ä»»åŠ¡å†…å’Œä»»åŠ¡é—´æ··åˆï¼Œä»¥åŒæ—¶ä¸°å¯Œå…ƒå­¦ä¹ ä¸­å¯ç”¨çš„èŠ‚ç‚¹å’Œä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜ç¡®åˆ©ç”¨å›¾ä¸­èŠ‚ç‚¹åº¦æä¾›çš„å…ˆéªŒä¿¡æ¯æ¥ç¼–ç è¡¨è¾¾æ€§èŠ‚ç‚¹è¡¨ç¤ºã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†SMILEå¯ä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å®è¯ä¸Šï¼ŒSMILEåœ¨æ‰€æœ‰è¯„ä¼°çš„åŸŸå†…å’Œè·¨åŸŸè®¾ç½®çš„æ•°æ®é›†ä¸Šå‡å¤§å¹…è¶…è¶Šå…¶ä»–ç«äº‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„åŒ¿åä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14158v1">PDF</a> WWW25</p>
<p><strong>Summary</strong></p>
<p>å›¾ç¥ç»ç½‘ç»œå·²æˆä¸ºå­¦ä¹ ç½‘ä¸Šå›¾ç»“æ„æ•°æ®å’Œä»ä¸­æŒ–æ˜å†…å®¹çš„æœ‰æ•ˆèŒƒå¼ã€‚å½“å‰é¢†å…ˆçš„å›¾æ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨å°æ ·æ•°æ®åœºæ™¯ä¸­ä¸å¯é¿å…åœ°å¯¼è‡´è¿‡æ‹Ÿåˆã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡ç»“åˆå›¾å­¦ä¹ å’Œå…ƒå­¦ä¹ èŒƒå¼æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›å›¾å…ƒå­¦ä¹ æ¨¡å‹å‡è®¾å¯ç”¨å¤§é‡çš„å…ƒè®­ç»ƒä»»åŠ¡æ¥å­¦ä¹ å¯è¿ç§»çš„å…ƒçŸ¥è¯†ï¼Œè¿™åœ¨ç°å®ä¸­å¯èƒ½ä¸å¯è¡Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å›¾å°æ ·å­¦ä¹ çš„æ–¹æ³•SMILEï¼Œé€šè¿‡åŒå±‚æ¬¡æ··åˆç­–ç•¥åŒæ—¶ä¸°å¯Œå¯ç”¨èŠ‚ç‚¹å’Œä»»åŠ¡ã€‚ç†è®ºä¸Šï¼ŒSMILEèƒ½æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›å®è¯ä¸Šï¼ŒSMILEåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾ç¥ç»ç½‘ç»œå·²æˆä¸ºå­¦ä¹ å’ŒæŒ–æ˜ç½‘ä¸Šå›¾ç»“æ„æ•°æ®çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å½“å‰å›¾æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>æœ€è¿‘ç ”ç©¶é€šè¿‡ç»“åˆå›¾å­¦ä¹ å’Œå…ƒå­¦ä¹ æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å…ƒå­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡å…ƒè®­ç»ƒä»»åŠ¡ï¼Œè¿™åœ¨ç°å®ä¸­å¯èƒ½éš¾ä»¥å®ç°ã€‚</li>
<li>SMILEæ–¹æ³•é€šè¿‡åŒå±‚æ¬¡æ··åˆç­–ç•¥ä¸°å¯Œå¯ç”¨èŠ‚ç‚¹å’Œä»»åŠ¡ï¼Œæœ‰æ•ˆåº”å¯¹å°æ ·æ•°æ®é—®é¢˜ã€‚</li>
<li>SMILEèƒ½æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f24314b7f9b31147e246c4003edd23e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c23b5a4ad7ba7c1c2cf39657b8261d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf8c7b878a4aa107bf115129ca2ac5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c28378d461a701039541828dc3e26b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Retrieving-Versus-Understanding-Extractive-Evidence-in-Few-Shot-Learning"><a href="#Retrieving-Versus-Understanding-Extractive-Evidence-in-Few-Shot-Learning" class="headerlink" title="Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning"></a>Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning</h2><p><strong>Authors:Karl Elbakian, Samuel Carton</strong></p>
<p>A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document evidence for large language model in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with evidence retrieval errors with respect to gold-standard human-annotated extractive evidence for five datasets, using two popular closed proprietary models. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and evidence retrieval error, but that evidence retrieval error is mostly not associated with evidence interpretation errorâ€“a hopeful sign for downstream applications built on this mechanism. </p>
<blockquote>
<p>å¯¹é½çš„å…³é”®åœ¨äºæ­£ç¡®ä½¿ç”¨æ–‡æ¡£å†…çš„è¯æ®æ¥æ„å»ºæ–‡æ¡£çº§åˆ«çš„å†³ç­–ã€‚æˆ‘ä»¬åœ¨å°æ ·æœ¬ç¯å¢ƒä¸‹ï¼Œåˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ£€ç´¢å’Œè§£é‡Šæ–‡æ¡£å†…è¯æ®ä¹‹é—´çš„å…³ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªæµè¡Œçš„å¤§å‹ä¸“æœ‰æ¨¡å‹ï¼Œé€šè¿‡è¡¡é‡æ¨¡å‹é¢„æµ‹è¯¯å·®ä¸å…³äºäº”ä¸ªæ•°æ®é›†çš„é»„é‡‘æ ‡å‡†äººç±»æ³¨é‡Šæå–è¯æ®çš„æ£€ç´¢è¯¯å·®ä¹‹é—´çš„å…³è”ç¨‹åº¦ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹æ¶ˆèç ”ç©¶ï¼Œä»¥è°ƒæŸ¥æ ‡ç­¾é¢„æµ‹å’Œè¯æ®æ£€ç´¢é”™è¯¯éƒ½å¯ä»¥å½’å› äºè¯æ®è´¨é‡çš„æƒ…å†µã€‚æˆ‘ä»¬å‘ç°æ¨¡å‹é¢„æµ‹å’Œè¯æ®æ£€ç´¢é”™è¯¯ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„å®è¯å…³ç³»ï¼Œä½†è¯æ®æ£€ç´¢é”™è¯¯ä¸»è¦ä¸è¯æ®è§£é‡Šé”™è¯¯ä¸ç›¸å…³â€”â€”è¿™å¯¹äºåŸºäºè¿™ç§æœºåˆ¶çš„ä¸‹æ¸¸åº”ç”¨æ˜¯ä¸€ä¸ªç§¯æçš„è¿¹è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14095v1">PDF</a> 9 pages, 8 figures, Accepted to AAAI 2025 Main Conference (AI   Alignment Track)</p>
<p><strong>Summary</strong>:</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„æ–‡æ¡£å†…è¯æ®æ£€ç´¢ä¸è§£è¯»çš„å…³ç³»åˆ†æã€‚ç ”ç©¶é€šè¿‡äº”ä¸ªæ•°æ®é›†æµ‹é‡æ¨¡å‹é¢„æµ‹é”™è¯¯ä¸è¯æ®æ£€ç´¢é”™è¯¯çš„ç›¸å…³æ€§ï¼Œå¹¶è¿›è¡Œä¸¤é¡¹æ¶ˆèç ”ç©¶ï¼Œå‘ç°æ¨¡å‹é¢„æµ‹ä¸è¯æ®æ£€ç´¢é”™è¯¯é—´å­˜åœ¨å¼ºå®è¯å…³ç³»ï¼Œä½†è¯æ®æ£€ç´¢é”™è¯¯å¤§å¤šä¸è¯æ®è§£è¯»é”™è¯¯æ— å…³ï¼Œä¸ºä¸‹æ¸¸åº”ç”¨æä¾›äº†ä¹è§‚çš„æŒ‡ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸‹ï¼Œè¯æ®æ£€ç´¢å¯¹äºæ–‡æ¡£çº§åˆ«çš„å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹é”™è¯¯ä¸è¯æ®æ£€ç´¢é”™è¯¯ä¹‹é—´å­˜åœ¨å¼ºå®è¯å…³ç³»ã€‚</li>
<li>é€šè¿‡äº”ä¸ªæ•°æ®é›†çš„ç ”ç©¶ï¼Œå‘ç°è¯æ®æ£€ç´¢é”™è¯¯å¹¶ä¸ä¸»è¦ä¸è¯æ®è§£è¯»é”™è¯¯ç›¸å…³ã€‚</li>
<li>è¿›è¡Œäº†ä¸¤é¡¹æ¶ˆèç ”ç©¶ï¼Œä»¥æ¢ç©¶æ ‡ç­¾é¢„æµ‹å’Œè¯æ®æ£€ç´¢é”™è¯¯çš„æ¥æºã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†ä¸¤ä¸ªæµè¡Œçš„å°é—­ä¸“æœ‰æ¨¡å‹è¿›è¡Œåˆ†æã€‚</li>
<li>å¯¹äºä¸‹æ¸¸åº”ç”¨ï¼Œè¯æ®æ£€ç´¢å’Œè§£è¯»çš„åˆ†ç¦»æä¾›äº†ä¸€ä¸ªä¹è§‚çš„æŒ‡ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09d7bc72401fce7a116faf72483c9e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86ef192aad73b7b5a85fdabecbb6fe8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0967916a388e89983d8257f98bf7f38b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-beda4ddec796bfd9f99032a6e4a32c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b648c8bc5ad6a999104181fc83f27eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01315f377fb5ed195ef4aa81463c3bf0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars"><a href="#RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars" class="headerlink" title="RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars"></a>RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars</h2><p><strong>Authors:Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari</strong></p>
<p>Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignmentâ€“factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at <a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE">https://github.com/AnonymousCode-ComputerScience/RIDE</a>. </p>
<blockquote>
<p>å¯¹é½è°ƒæ•´å¯¹äºç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥ä¼¦ç†å’Œæœ‰å¸®åŠ©çš„æ–¹å¼è¡Œä¸ºè‡³å…³é‡è¦ã€‚å½“å‰çš„å¯¹é½æ–¹æ³•éœ€è¦é«˜è´¨é‡æ ‡æ³¨å’Œå¤§é‡çš„è®­ç»ƒèµ„æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬ã€æ— éœ€è°ƒæ•´çš„æ–¹æ³•ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥å¢å¼ºLLMçš„å¯¹é½ã€‚é€šè¿‡å¯¹é«˜è´¨é‡ICLæ¼”ç¤ºå†…å®¹çš„åˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†é£æ ¼æ˜¯å½±å“LLMå¯¹é½èƒ½åŠ›çš„å…³é”®å› ç´ ï¼Œå¹¶åŸºäºæ­¤é£æ ¼æ¡†æ¶æ˜ç¡®åœ°å¯¹ICLç¤ºä¾‹è¿›è¡Œäº†é‡æ–°è®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†é‡æ–°è®¾è®¡çš„æ¼”ç¤ºå†…å®¹ç›¸ç»“åˆï¼Œä»¥å®ç°LLMå¯¹é½çš„ä¸¤ä¸ªæ–¹é¢ä¹‹é—´çš„å¹³è¡¡â€”â€”äº‹å®æ€§å’Œå®‰å…¨æ€§ã€‚æˆ‘ä»¬å°†é‡æ–°è®¾è®¡çš„ç¤ºä¾‹æ‰“åŒ…ä¸ºæç¤ºï¼Œä»¥è§¦å‘å°‘é‡å­¦ä¹ ï¼Œæé«˜LLMçš„å¯¹é½èƒ½åŠ›ã€‚ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ˆæœ€é«˜å¹³å‡å¾—åˆ†ä¸º5.00ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Alpacaä»»åŠ¡ä¸Šæœ€é«˜å¾—åˆ†å¢åŠ äº†0.10ï¼ˆä»4.50åˆ°4.60ï¼‰ï¼Œåœ¨Just-evalåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†0.22ï¼ˆä»4.34åˆ°4.56ï¼‰ï¼Œåœ¨MT-Benchæ•°æ®é›†ä¸Šæœ€é«˜æé«˜äº†0.32ï¼ˆä»3.53åˆ°3.85ï¼‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Anonymouscode-ComputerScience/RIDE%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E%E6%8D%AE%E3%80%82">https://github.com/AnonymousCode-ComputerScience/RIDEä¸Šå‘å¸ƒäº†ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11681v2">PDF</a> 38 pages, 2 figures, 20 tables; The paper is under review in ARR</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬ã€æ— éœ€è°ƒä¼˜çš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æŠ€æœ¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½æ€§èƒ½ã€‚ç ”ç©¶é€šè¿‡é«˜è´¨é‡ICLæ¼”ç¤ºåˆ†æï¼Œå‘ç°é£æ ¼æ˜¯å½±å“LLMå¯¹é½èƒ½åŠ›çš„é‡è¦å› ç´ ï¼Œå¹¶æ®æ­¤é‡æ–°è®¾è®¡äº†ICLèŒƒä¾‹ã€‚ç»“åˆè¿™äº›é‡æ–°è®¾è®¡çš„èŒƒä¾‹ï¼Œåœ¨LLMå¯¹é½çš„ä¸¤ä¸ªæ–¹é¢â€”â€”çœŸå®æ€§å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚æœ€ç»ˆé€šè¿‡è§¦å‘å°‘æ ·æœ¬å­¦ä¹ æå‡äº†LLMçš„å¯¹é½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¼ºè°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼¦ç†å’Œå®ç”¨å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ç§ä½æˆæœ¬çš„æ— éœ€è°ƒä¼˜çš„æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æŠ€æœ¯ï¼Œé€šè¿‡é«˜è´¨é‡ç¤ºèŒƒåˆ†æå‘ç°é£æ ¼æ˜¯å½±å“LLMå¯¹é½çš„å…³é”®å› ç´ ã€‚</li>
<li>è®ºæ–‡é‡æ–°è®¾è®¡äº†ICLèŒƒä¾‹ï¼Œå¹¶åŸºäºé£æ ¼æ¡†æ¶è¿›è¡Œå±•ç¤ºã€‚</li>
<li>é€šè¿‡ç»“åˆé‡æ–°è®¾è®¡çš„èŒƒä¾‹ï¼Œåœ¨LLMå¯¹é½çš„ä¸¤ä¸ªæ–¹é¢â€”â€”çœŸå®æ€§å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>è®ºæ–‡ä½¿ç”¨è¿™äº›é‡æ–°è®¾è®¡çš„ä¾‹å­ä½œä¸ºæç¤ºæ¥è§¦å‘å°‘æ ·æœ¬å­¦ä¹ ï¼Œæé«˜äº†LLMçš„å¯¹é½æ•ˆæœã€‚</li>
<li>ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè®ºæ–‡æå‡ºçš„æ–¹æ³•åœ¨Alpacaä»»åŠ¡ã€Just-evalåŸºå‡†æµ‹è¯•å’ŒMT-Benchæ•°æ®é›†ä¸Šå‡æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ad113c3a36a68b867a335cf66eb9c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d96c391855703fe107b5c5c4644169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f156d089c7759aa95d148939c38fa8ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab40690d2dffabbf7fb8b5eb38abf2c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-800b7715e2257f5db9d7e3af024cf51a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Patient-Comments-Multi-Label-Classification"><a href="#Large-Language-Models-for-Patient-Comments-Multi-Label-Classification" class="headerlink" title="Large Language Models for Patient Comments Multi-Label Classification"></a>Large Language Models for Patient Comments Multi-Label Classification</h2><p><strong>Authors:Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin</strong></p>
<p>Patient experience and care quality are crucial for a hospitalâ€™s sustainability and reputation. The analysis of patient feedback offers valuable insight into patient satisfaction and outcomes. However, the unstructured nature of these comments poses challenges for traditional machine learning methods following a supervised learning paradigm. This is due to the unavailability of labeled data and the nuances these texts encompass. This research explores leveraging Large Language Models (LLMs) in conducting Multi-label Text Classification (MLTC) of inpatient comments shared after a stay in the hospital. GPT-4 Turbo was leveraged to conduct the classification. However, given the sensitive nature of patientsâ€™ comments, a security layer is introduced before feeding the data to the LLM through a Protected Health Information (PHI) detection framework, which ensures patientsâ€™ de-identification. Additionally, using the prompt engineering framework, zero-shot learning, in-context learning, and chain-of-thought prompting were experimented with. Results demonstrate that GPT-4 Turbo, whether following a zero-shot or few-shot setting, outperforms traditional methods and Pre-trained Language Models (PLMs) and achieves the highest overall performance with an F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the few-shot learning results. Subsequently, the resultsâ€™ association with other patient experience structured variables (e.g., rating) was conducted. The study enhances MLTC through the application of LLMs, offering healthcare practitioners an efficient method to gain deeper insights into patient feedback and deliver prompt, appropriate responses. </p>
<blockquote>
<p>æ‚£è€…ä½“éªŒå’ŒæŠ¤ç†è´¨é‡å¯¹äºåŒ»é™¢çš„å¯æŒç»­æ€§å’Œå£°èª‰è‡³å…³é‡è¦ã€‚æ‚£è€…åé¦ˆçš„åˆ†æä¸ºæ‚£è€…æ»¡æ„åº¦å’Œç»“æœæä¾›äº†å®è´µçš„è§è§£ã€‚ç„¶è€Œï¼Œè¿™äº›è¯„è®ºçš„éç»“æ„åŒ–ç‰¹æ€§ç»™ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éµå¾ªç›‘ç£å­¦ä¹ èŒƒå¼çš„æƒ…å†µä¸‹ã€‚è¿™æ˜¯å› ä¸ºç¼ºä¹æ ‡è®°æ•°æ®ä»¥åŠè¿™äº›æ–‡æœ¬ä¸­è•´å«çš„ç»†èŠ‚å·®å¼‚ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä½é™¢æ‚£è€…è¯„è®ºçš„å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ï¼ˆMLTCï¼‰ã€‚GPT-4 Turboè¢«ç”¨äºè¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œé‰´äºæ‚£è€…è¯„è®ºçš„æ•æ„Ÿæ€§ï¼Œåœ¨é€šè¿‡PHIæ£€æµ‹æ¡†æ¶å°†æ•°æ®è¾“å…¥LLMä¹‹å‰ï¼Œå¼•å…¥äº†ä¸€ä¸ªå®‰å…¨å±‚ï¼Œä»¥ç¡®ä¿æ‚£è€…çš„åŒ¿åæ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹æ¡†æ¶ï¼Œå®éªŒäº†é›¶æ ·æœ¬å­¦ä¹ ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œé“¾å¼æ€ç»´æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨é›¶æ ·æœ¬è¿˜æ˜¯å°æ ·è®¾ç½®ä¸‹ï¼ŒGPT-4 Turboéƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œå¹¶åœ¨æœ€é«˜æ€»ä½“æ€§èƒ½ä¸Šè¾¾åˆ°äº†76.12%çš„F1åˆ†æ•°å’Œ73.61%çš„åŠ æƒF1åˆ†æ•°ï¼Œå…¶æ¬¡æ˜¯å°‘æ•°æ ·æœ¬å­¦ä¹ çš„ç»“æœã€‚éšåï¼Œå°†ç»“æœä¸æ‚£è€…ä½“éªŒçš„å…¶ä»–ç»“æ„åŒ–å˜é‡ï¼ˆå¦‚è¯„åˆ†ï¼‰è¿›è¡Œäº†å…³è”åˆ†æã€‚è¯¥ç ”ç©¶é€šè¿‡åº”ç”¨LLMå¢å¼ºäº†MLTCï¼Œä¸ºåŒ»ç–—ä»ä¸šè€…æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥æ›´æ·±å…¥åœ°äº†è§£æ‚£è€…åé¦ˆå¹¶åšå‡ºåŠæ—¶ã€é€‚å½“çš„å›åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23528v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ï¼ˆMLTCï¼‰åˆ†æä½é™¢æ‚£è€…è¯„è®ºçš„å¯è¡Œæ€§ã€‚é€šè¿‡GPT-4 Turboè¿›è¡Œåˆ†ç±»ï¼Œå¹¶åœ¨é¦ˆé€æ•°æ®ç»™LLMä¹‹å‰å¼•å…¥äº†ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰æ£€æµ‹æ¡†æ¶ï¼Œç¡®ä¿æ‚£è€…å»æ ‡è¯†åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒGPT-4 Turboåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ï¼Œæœ€é«˜æ€§èƒ½è¾¾åˆ°F1åˆ†æ•°ä¸º76.12%ï¼ŒåŠ æƒF1åˆ†æ•°ä¸º73.61%ã€‚è¯¥ç ”ç©¶æé«˜äº†é€šè¿‡LLMsè¿›è¡ŒMLTCçš„èƒ½åŠ›ï¼Œä¸ºåŒ»ç–—ä»ä¸šè€…æä¾›äº†ä¸€ç§æ·±å…¥äº†è§£æ‚£è€…åé¦ˆå’ŒåŠæ—¶ã€æ°å½“åœ°å›åº”æ‚£è€…åé¦ˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‚£è€…ä½“éªŒå’ŒæŠ¤ç†è´¨é‡å¯¹åŒ»é™¢çš„å¯æŒç»­æ€§å’Œå£°èª‰è‡³å…³é‡è¦ã€‚</li>
<li>æ‚£è€…åé¦ˆåˆ†æä¸ºæ‚£è€…æ»¡æ„åº¦å’Œç»“æœæä¾›äº†å®è´µçš„è§è§£ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†ä½é™¢æ‚£è€…è¯„è®ºçš„å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ï¼ˆMLTCï¼‰æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>GPT-4 Turboåœ¨åˆ†ç±»ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ã€‚</li>
<li>åœ¨å¤„ç†æ‚£è€…è¯„è®ºæ—¶ï¼Œå¼•å…¥äº†ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰æ£€æµ‹æ¡†æ¶ä»¥ç¡®ä¿æ‚£è€…å»æ ‡è¯†åŒ–ã€‚</li>
<li>é€šè¿‡å®éªŒï¼Œå±•ç¤ºäº†é›¶æ ·æœ¬å­¦ä¹ å’Œå°‘æ ·æœ¬å­¦ä¹ åœ¨LLMsä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cd0e54c65f2611383d41bef5f13e9f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-446e380dd223d4f6898807f988831dab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c56ddba2a33fa5be9cf3e8c83c669ea.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models"><a href="#Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models" class="headerlink" title="Revisiting In-context Learning Inference Circuit in Large Language   Models"></a>Revisiting In-context Learning Inference Circuit in Large Language   Models</h2><p><strong>Authors:Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (in the demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations of demonstrations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. Through careful measurements, the proposed inference circuit successfully captures and unifies many fragmented phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸Šæ–°å…´çš„ä¸€ç§å°æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå…¶å†…éƒ¨æœºåˆ¶å°šæœªè¢«æ¢ç´¢ã€‚è™½ç„¶å·²æœ‰å·¥ä½œæè¿°äº†ICLçš„å†…éƒ¨å¤„ç†è¿‡ç¨‹ï¼Œä½†å®ƒä»¬å¾ˆéš¾æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ‰€æœ‰æ¨ç†ç°è±¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»¼åˆç”µè·¯æ¥æ¨¡æ‹Ÿæ¨ç†åŠ¨æ€ï¼Œå¹¶è¯•å›¾è§£é‡Šè§‚å¯Ÿåˆ°çš„ICLç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ICLæ¨ç†è¿‡ç¨‹åˆ†ä¸º3ä¸ªä¸»è¦æ“ä½œï¼šï¼ˆ1ï¼‰è¾“å…¥æ–‡æœ¬ç¼–ç ï¼šè¯­è¨€æ¨¡å‹å°†æ¯ä¸ªè¾“å…¥æ–‡æœ¬ï¼ˆåœ¨æ¼”ç¤ºå’ŒæŸ¥è¯¢ä¸­ï¼‰ç¼–ç æˆéšè—çŠ¶æ€ä¸­çš„çº¿æ€§è¡¨ç¤ºï¼Œå…¶ä¸­åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯æ¥è§£å†³ICLä»»åŠ¡ã€‚ï¼ˆ2ï¼‰è¯­ä¹‰åˆå¹¶ï¼šè¯­è¨€æ¨¡å‹å°†æ¼”ç¤ºçš„ç¼–ç è¡¨ç¤ºä¸å…¶ç›¸åº”çš„æ ‡ç­¾æ ‡è®°åˆå¹¶ï¼Œä»¥äº§ç”Ÿæ ‡ç­¾å’Œæ¼”ç¤ºçš„è”åˆè¡¨ç¤ºã€‚ï¼ˆ3ï¼‰ç‰¹å¾æ£€ç´¢å’Œå¤åˆ¶ï¼šè¯­è¨€æ¨¡å‹ä¼šåœ¨ä»»åŠ¡å­ç©ºé—´ä¸­æœç´¢ä¸æŸ¥è¯¢è¡¨ç¤ºç›¸ä¼¼çš„æ¼”ç¤ºè”åˆè¡¨ç¤ºï¼Œå¹¶å°†æ‰€æœç´¢çš„è¡¨ç¤ºå¤åˆ¶åˆ°æŸ¥è¯¢ä¸­ã€‚ç„¶åï¼Œè¯­è¨€æ¨¡å‹å¤´éƒ¨ä¼šæ•è·è¿™äº›å¤åˆ¶çš„æ ‡ç­¾è¡¨ç¤ºï¼Œå¹¶å°†å…¶è§£ç ä¸ºé¢„æµ‹çš„æ ‡ç­¾ã€‚é€šè¿‡ç²¾å¿ƒæµ‹é‡ï¼Œæ‰€æå‡ºçš„æ¨ç†ç”µè·¯æˆåŠŸåœ°æ•è·å¹¶ç»Ÿä¸€äº†ICLè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„è®¸å¤šç¢ç‰‡åŒ–ç°è±¡ï¼Œæ˜¯å¯¹ICLæ¨ç†è¿‡ç¨‹çš„å…¨é¢å®ç”¨è§£é‡Šã€‚è€Œä¸”ï¼Œé€šè¿‡ç¦ç”¨æ‰€æå‡ºçš„æ­¥éª¤è¿›è¡Œçš„åˆ†æè¡¨æ˜ï¼Œè¿™ä¸¥é‡æŸå®³äº†ICLçš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜æ‰€æå‡ºçš„æ¨ç†ç”µè·¯æ˜¯ä¸»å¯¼æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®è®¤å¹¶åˆ—å‡ºäº†ä¸€äº›ä¸æ‰€æå‡ºçš„ç”µè·¯å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æ—è·¯æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04468v4">PDF</a> 37 pages, 41 figures, 8 tables. ICLR 2025 Accepted. Camera-ready   Version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–°å…´çš„åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„å°‘é‡å­¦ä¹ æ¨¡å¼â€”â€”ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å…¨é¢çš„ç”µè·¯æ¨¡å‹ï¼Œæ—¨åœ¨æ¨¡æ‹ŸICLçš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å°†å…¶åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ“ä½œï¼šè¾“å…¥æ–‡æœ¬ç¼–ç ã€è¯­ä¹‰åˆå¹¶å’Œç‰¹å¾æ£€ç´¢ä¸å¤åˆ¶ã€‚æ­¤æ¨¡å‹æˆåŠŸæ•æ‰å¹¶ç»Ÿä¸€äº†ICLè¿‡ç¨‹ä¸­çš„ç¢ç‰‡åŒ–ç°è±¡ï¼Œæä¾›äº†å¯¹å…¶è¿‡ç¨‹çš„å…¨é¢è€Œå®ç”¨çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œé€šè¿‡é€æ­¥æ‹†è§£åˆ†æï¼ŒéªŒè¯äº†è¯¥æ¨ç†ç”µè·¯åœ¨ICLä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚åŒæ—¶ï¼Œä¹Ÿç¡®è®¤äº†å…¶ä»–å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æ—è·¯æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è¯­è¨€æ¨¡å‹ä¸­çš„å°‘é‡å­¦ä¹ æ¨¡å¼ï¼Œå…¶å†…éƒ¨æœºåˆ¶å°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨æè¿°ICLçš„å†…éƒ¨å¤„ç†æ–¹é¢å·²å–å¾—ä¸€å®šæˆæœï¼Œä½†ä»éš¾ä»¥å…¨é¢æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ç°è±¡ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„ç”µè·¯æ¨¡å‹ï¼Œç”¨ä»¥æ¨¡æ‹ŸICLçš„æ¨ç†è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªä¸»è¦æ“ä½œï¼šè¾“å…¥æ–‡æœ¬ç¼–ç ã€è¯­ä¹‰åˆå¹¶å’Œç‰¹å¾æ£€ç´¢ä¸å¤åˆ¶ã€‚</li>
<li>è¯¥ç”µè·¯æ¨¡å‹æˆåŠŸæ•æ‰å¹¶ç»Ÿä¸€äº†ICLè¿‡ç¨‹ä¸­çš„ç¢ç‰‡åŒ–ç°è±¡ï¼Œä¸ºç†è§£è¿™ä¸€è¿‡ç¨‹æä¾›äº†å…¨é¢è€Œå®ç”¨çš„è§†è§’ã€‚</li>
<li>é€šè¿‡é€æ­¥æ‹†è§£åˆ†æï¼ŒéªŒè¯äº†è¯¥æ¨ç†ç”µè·¯åœ¨ICLä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚</li>
<li>é™¤äº†ä¸»è¦çš„æ¨ç†ç”µè·¯å¤–ï¼Œè¿˜å­˜åœ¨å…¶ä»–å¹¶è¡Œè§£å†³ICLä»»åŠ¡çš„æ—è·¯æœºåˆ¶ã€‚</li>
<li>é€šè¿‡å¯¹ICLçš„æ·±å…¥å‰–æï¼Œæœ¬æ–‡ä¸ºè¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ”¹è¿›æä¾›äº†ç†è®ºåŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1fb2cbc690cfe62632360b24cedc46cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adb2d74983103299ca22b732c6dd9846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb11aef9a2f00241404f5ca8a1a9a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a13f5c010bd41cb3db361a0b0b86a6ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a749688bf036aa0269712c360232ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30dc931cf6d35a7daceb94618dadb35e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM4TS-Aligning-Pre-Trained-LLMs-as-Data-Efficient-Time-Series-Forecasters"><a href="#LLM4TS-Aligning-Pre-Trained-LLMs-as-Data-Efficient-Time-Series-Forecasters" class="headerlink" title="LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series   Forecasters"></a>LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series   Forecasters</h2><p><strong>Authors:Ching Chang, Wei-Yao Wang, Wen-Chih Peng, Tien-Fu Chen</strong></p>
<p>Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of time-series data, and the forecasting fine-tuning stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across 7 time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios, and also achieves the highest rank in few-shot scenarios. In addition, evaluations compared with different unsupervised representation learning approaches highlight LLM4TSâ€™s effectiveness with representation learning in forecasting tasks. Ablation studies further validate each componentâ€™s contribution to LLM4TS and underscore the essential role of utilizing LLMâ€™s pre-trained weights for optimal performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/blacksnail789521/LLM4TS">https://github.com/blacksnail789521/LLM4TS</a>. </p>
<blockquote>
<p>å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹åœ¨ç»æµè§„åˆ’ã€å¤©æ°”é¢„æŠ¥ç­‰é¢†åŸŸå…·æœ‰è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶æ·±åº¦ä»å¤´è®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºæœ‰æ•ˆçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ€è¿‘ï¼Œç ”ç©¶è€…ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¡¨ç¤ºå­¦ä¹ è¿ç§»èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°å¤„ç†æœ‰é™çš„éè¯­è¨€æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå°†LLMsä¸æ—¶é—´åºåˆ—æ•°æ®ç›¸ç»“åˆé¢ä¸´ç€æœ‰é™çš„é€‚åº”æ€§æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºæ—¶é—´åºåˆ—æ•°æ®å’Œè¯­è¨€æ•°æ®ä¹‹é—´çš„ä¸åŒç»„æˆä»¥åŠæ— æ³•å¤„ç†å¤šå°ºåº¦æ—¶é—´ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLM4TSï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é¢„è®­ç»ƒLLMsè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹çš„æ¡†æ¶ã€‚LLM4TSç”±ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ç»„æˆï¼šæ—¶é—´åºåˆ—å¯¹é½é˜¶æ®µï¼Œä½¿LLMsä¸æ—¶é—´åºåˆ—æ•°æ®çš„ç»†å¾®å·®åˆ«å¯¹é½ï¼›ä»¥åŠç”¨äºä¸‹æ¸¸æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡çš„é¢„æµ‹å¾®è°ƒé˜¶æ®µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰æ–°é¢–çš„ä¸¤çº§èšåˆæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨é¢„è®­ç»ƒçš„LLMså†…é›†æˆå¤šå°ºåº¦æ—¶é—´æ•°æ®ï¼Œå¢å¼ºå®ƒä»¬è§£é‡Šæ—¶é—´ç‰¹å®šä¿¡æ¯çš„èƒ½åŠ›ã€‚åœ¨7ä¸ªæ—¶é—´åºåˆ—é¢„æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLM4TSåœ¨å…¨ç¨‹åœºæ™¯ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å°æ ·æœ¬åœºæ™¯ä¸­è·å¾—äº†æœ€é«˜æ’åã€‚æ­¤å¤–ï¼Œä¸ä¸åŒçš„æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•è¿›è¡Œçš„è¯„ä¼°çªå‡ºäº†LLM4TSåœ¨é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†LLM4TSæ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œå¹¶å¼ºè°ƒäº†åˆ©ç”¨LLMçš„é¢„è®­ç»ƒæƒé‡å¯¹äºå®ç°æœ€ä½³æ€§èƒ½çš„é‡è¦ä½œç”¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/blacksnail789521/LLM4TS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/blacksnail789521/LLM4TSä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08469v6">PDF</a> Accepted for publication in ACM Transactions on Intelligent Systems   and Technology (TIST) 2025. The final published version will be available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3719207">https://doi.org/10.1145/3719207</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLM4TSæ¡†æ¶åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰é™æ•°æ®ä¸‹å¤„ç†éè¯­è¨€æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå±€é™æ€§ï¼Œæå‡ºä¸€ç§åŒ…å«ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥çš„æ–°æ¡†æ¶ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¤šç§æ—¶é—´åºåˆ—é¢„æµ‹æ•°æ®é›†ä¸Šï¼ŒLLM4TSä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è®­ç»ƒè‡ªåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ä»¥åŠæ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ—¶ã€‚åŒæ—¶æå‡ºä¸€ä¸ªå¤šå±‚æ¬¡æ•°æ®èšåˆæ–¹æ³•ï¼Œæé«˜äº†é¢„è®­ç»ƒLLMsè§£é‡Šæ—¶é—´åºåˆ—ç‰¹å®šä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM4TSæ˜¯ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼ŒåŒ…æ‹¬æ—¶é—´åºåˆ—å¯¹é½å’Œé¢„æµ‹å¾®è°ƒé˜¶æ®µã€‚</li>
<li>LLM4TSæ¡†æ¶ä½¿ç”¨å¤šå±‚æ¬¡æ•°æ®èšåˆæ–¹æ³•ï¼Œå¯ä»¥å¤„ç†å¤šå°ºåº¦æ—¶é—´åºåˆ—æ•°æ®ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLLM4TSä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å°‘æ ·æœ¬æ•°æ®åœºæ™¯æ—¶ã€‚ç›¸è¾ƒäºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½æ›´åŠ ä¼˜è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.08469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04c8bb68123bb6293e2ac5825b894ca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d41ba37531ef9fad265bd24cf454e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8747398ff066159c656bfae393e5f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5101a9036f9d95d77f6c8fd620d82dde.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c72a4687c2ce6dfeaa7d819eab4c6fb8.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  d-Sketch Improving Visual Fidelity of Sketch-to-Image Translation with   Pretrained Latent Diffusion Models without Retraining
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f0c532212a2fd8648edb1bc73899e73.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  Building reliable sim driving agents by scaling self-play
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
