<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-22  Dynamic Low-Rank Sparse Adaptation for Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ab40690d2dffabbf7fb8b5eb38abf2c0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-22-更新"><a href="#2025-02-22-更新" class="headerlink" title="2025-02-22 更新"></a>2025-02-22 更新</h1><h2 id="Dynamic-Low-Rank-Sparse-Adaptation-for-Large-Language-Models"><a href="#Dynamic-Low-Rank-Sparse-Adaptation-for-Large-Language-Models" class="headerlink" title="Dynamic Low-Rank Sparse Adaptation for Large Language Models"></a>Dynamic Low-Rank Sparse Adaptation for Large Language Models</h2><p><strong>Authors:Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji</strong></p>
<p>Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA), a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by 16.32$%$, achieving a 2.60$\times$ speedup on CPU and 2.23$\times$ speedup on GPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wzhuang-xmu/LoSA">https://github.com/wzhuang-xmu/LoSA</a>. </p>
<blockquote>
<p>尽管网络稀疏性在缓解大型语言模型（LLM）部署压力方面表现出色，但其性能仍有所降低。将低秩适应（LoRA）应用于微调稀疏LLM为解决这一困境提供了直观的方法，但它也存在一些缺点，包括：1）无法在训练后的稀疏LLM中集成LoRA权重；2）在高稀疏比率下性能恢复不足。在本文中，我们介绍了动态低秩稀疏适应（LoSA）方法，这是一种将低秩适应无缝集成到LLM稀疏性的统一框架中的新方法，旨在提高稀疏LLM的性能，而不会增加推理延迟。特别是，LoSA在微调过程中根据相应的稀疏权重动态地稀疏化LoRA的结果，从而确保LoRA模块可以在训练后的稀疏LLM中集成。此外，LoSA利用表示互信息（RMI）作为指标来确定层的重要性，从而有效地确定微调过程中的逐层稀疏率。基于此，LoSA根据逐层重建误差的变化来调整LoRA模块的秩，为每层分配适当的微调，以减少密集和稀疏LLM之间的输出差异。大量实验表明，LoSA可以在几小时内有效提高稀疏LLM的效率，而不会引入任何额外的推理负担。例如，LoSA将稀疏LLaMA-2-7B的困惑度降低了68.73%，零射准确度提高了16.32%，在CPU上实现了2.60倍的加速，在GPU上实现了2.23倍的加速，只需在单个NVIDIA A100 80GB GPU上进行45分钟的微调即可。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/wzhuang-xmu/LoSA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wzhuang-xmu/LoSA获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14816v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong><br>     本文提出动态低秩稀疏适配（LoSA）方法，将低秩适配技术无缝集成到LLM稀疏模型中，提升稀疏LLM性能且不会增加推理延迟。LoSA通过动态稀疏化LoRA结果、利用表示互信息（RMI）确定层重要性及调整LoRA模块秩等方法，优化了LoRA在稀疏LLM中的集成效果。实验表明，LoSA能显著提高稀疏LLM效率，如LLaMA-2-7B的词谜减少68.73%，零样本准确率提高16.32%，并在CPU和GPU上实现加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>网络稀疏性虽然能缓解大型语言模型（LLM）的部署压力，但会导致性能显著下降。</li>
<li>LoSA方法将低秩适配（LoRA）技术集成到LLM稀疏模型中，提高了稀疏LLM的性能。</li>
<li>LoSA通过动态稀疏化LoRA结果，保证了LoRA模块可以在稀疏LLM训练后进行集成。</li>
<li>LoSA利用表示互信息（RMI）来确定层的重要性，从而有效地确定微调期间的层级稀疏率。</li>
<li>LoSA根据层间重建误差的变异性调整LoRA模块的秩，为每层分配适当的微调，以减少密集和稀疏LLM之间的输出差异。</li>
<li>实验表明，LoSA能显著提高稀疏LLM的效率，如LLaMA-2-7B的词谜减少和零样本准确率的提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8ccb21d8d66b9b80f9f55438c682f61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f471e1e3bbc0f54f98ec2319512020a3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Similarity-Paradigm-Through-Textual-Regularization-Without-Forgetting"><a href="#A-Similarity-Paradigm-Through-Textual-Regularization-Without-Forgetting" class="headerlink" title="A Similarity Paradigm Through Textual Regularization Without Forgetting"></a>A Similarity Paradigm Through Textual Regularization Without Forgetting</h2><p><strong>Authors:Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu</strong></p>
<p>Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods. </p>
<blockquote>
<p>提示学习已成为将预训练的视觉语言模型（VLM）适应多种下游任务的一种有前途的方法。虽然优化上下文对于提高特定任务的性能可能是有效的，但它往往会导致在来自不同分布的未见类别或数据集上的泛化性能较差。这可能是由于文本提示倾向于过度拟合下游数据分布，导致忘记手工制作的提示所衍生的通用知识。在本文中，我们提出了一种新的方法，称为带文本正则化的相似性范式（SPTR），用于无遗忘的提示学习。SPTR是一个基于手工制作的提示的两面设计，是一个不可分割的框架。1）为了避免忘记一般的文本知识，我们引入了最优传输作为文本正则化，以精细地确保与手工制作的特征和调整文本特征的近似。2）为了不断释放多个手工制作的提示的通用能力，我们提出了自然对齐得分和对抗性对齐得分的相似性范式，以提高模型的泛化能力。这两个模块的共同目标是解决泛化问题，旨在最大化从多个手工制作的提示中获得的泛化能力。在11个数据集的四个代表性任务（即非泛化小样本学习、基础到新颖的泛化、跨数据集泛化和域泛化）上表明，SPTR的性能优于现有的提示学习方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14376v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为SPTR的新型提示学习方法，旨在解决预训练视觉语言模型在适应下游任务时的泛化问题。SPTR通过引入文本正则化和相似性范式，确保模型在利用手工提示的同时，避免遗忘通用文本知识，并提高模型对不同数据集和分布的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提示学习是适应预训练视觉语言模型到下游任务的一种有前途的方法。</li>
<li>文本提示可能导致模型对特定任务的性能优化，但泛化性能较差。</li>
<li>SPTR是一种基于手工提示的新型提示学习方法，旨在解决泛化问题。</li>
<li>SPTR使用文本正则化来确保模型与手工特征的精细对齐，避免遗忘通用文本知识。</li>
<li>SPTR引入相似性范式，通过自然对齐得分和对抗性对齐得分来提高模型的稳健性和泛化能力。</li>
<li>SPTR在两个主要目标上解决了泛化问题，旨在从多个手工提示中最大限度地提高泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14376">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9a6cfe0ed935a2b230f4bbc63cd0f39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ca0670cbf65a815276bae63f283e896.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-850fe4ecb7969a76df28ea542cab15f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79c4d3f7f00da30844efae3481bd785f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26942adab36734fbc0c96371ae565df0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-131d0a7b7cd8e42c27e14d985dc229fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d438bb3c665833358ce8da713d9500a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Asymmetric-Co-Training-for-Source-Free-Few-Shot-Domain-Adaptation"><a href="#Asymmetric-Co-Training-for-Source-Free-Few-Shot-Domain-Adaptation" class="headerlink" title="Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation"></a>Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation</h2><p><strong>Authors:Gengxu Li, Yuan Wu</strong></p>
<p>Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data. However, SFUDA approaches come with inherent limitations that are frequently overlooked. These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the source domain, or when sufficient unlabeled target data is unavailable-a common situation in real-world applications. To address these issues, we propose an asymmetric co-training (ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a more practical alternative to SFUDA, as gathering a few labeled target instances is more feasible than acquiring large volumes of unlabeled target data in many real-world contexts. Our ACT method begins by employing a weak-strong augmentation to enhance data diversity. Then we use a two-step optimization process to train the target model. In the first step, we optimize the label smoothing cross-entropy loss, the entropy of the class-conditional distribution, and the reverse-entropy loss to bolster the model’s discriminative ability while mitigating overfitting. The second step focuses on reducing redundancy in the output space by minimizing classifier determinacy disparity. Extensive experiments across four benchmarks demonstrate the superiority of our ACT approach, which outperforms state-of-the-art SFUDA methods and transfer learning techniques. Our findings suggest that adapting a source pre-trained model using only a small amount of labeled target data offers a practical and dependable solution. The code is available at <a target="_blank" rel="noopener" href="https://github.com/gengxuli/ACT">https://github.com/gengxuli/ACT</a>. </p>
<blockquote>
<p>源数据无关的无人监督领域自适应（SFUDA）作为一种替代传统无人监督领域自适应（UDA）的方法，已经引起了人们的广泛关注。传统无人监督领域自适应依赖于始终可用的有标签源数据。然而，SFUDA方法带有经常被忽视的内在局限性。这些挑战包括在无标签目标数据无法满足关键假设时性能下降，例如当封闭集标签分布与源域相同时，或者在现实世界应用中经常出现足够的无标签目标数据不可用的情况。为了解决这些问题，我们提出了一种专为SFFSDA场景设计的对称协同训练（ACT）方法。SFFSDA为SFUDA提供了一个更实用的替代方案，因为在许多现实世界的情境中，收集少量有标签的目标实例比获取大量无标签的目标数据更为可行。我们的ACT方法首先采用弱强增强法来提高数据多样性。然后我们通过两步优化过程来训练目标模型。第一步中，我们优化标签平滑交叉熵损失、类条件分布的熵和反向熵损失，以提高模型的判别能力并减轻过拟合现象。第二步专注于通过最小化分类器确定性差异来减少输出空间的冗余。在四个基准测试上的大量实验表明，我们的ACT方法表现卓越，超越了最先进的SFUDA方法和迁移学习技术。我们的研究结果表明，仅使用少量有标签的目标数据对源预训练模型进行适应调整提供了一种实用可靠的解决方案。代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/gengxuli/ACT">https://github.com/gengxuli/ACT</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14214v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>该文介绍了无源无监督域自适应（SFUDA）方法面临的挑战，并提出了针对SFUDA的对称协同训练（ACT）方法。ACT方法通过弱强增强数据多样性，采用两步优化过程训练目标模型，优化标签平滑交叉熵损失、类条件分布的熵和反向熵损失，以提高模型的判别能力并减少过拟合。在四个基准测试上的实验表明，ACT方法优于最新的SFUDA方法和迁移学习技术。研究结果表明，仅使用少量目标数据对源预训练模型进行微调是一种实用可靠的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>源自由无监督域自适应（SFUDA）方法受到关注，作为传统无监督域自适应（UDA）的替代方案，但SFUDA方法存在性能下降的局限性。</li>
<li>当目标数据不满足关键假设（如标签分布与源域相同）或目标数据不足时，SFUDA性能会受到影响。</li>
<li>提出了针对SFUDA场景的对称协同训练（ACT）方法，通过弱强增强数据多样性，采用两步优化过程训练模型。</li>
<li>ACT方法优化了标签平滑交叉熵损失、类条件分布的熵和反向熵损失，提高模型的判别能力并减少过拟合。</li>
<li>ACT方法在四个基准测试上的实验表现优于最新的SFUDA方法和迁移学习技术。</li>
<li>使用少量目标数据对源预训练模型进行微调是一种实用可靠的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7da82a5a7107720afc8fa95f6a5a8fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e3106e3e8d76d8af336bd408bdb9dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b1c1d06c5b226c6b2d195f12719bb4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dual-level-Mixup-for-Graph-Few-shot-Learning-with-Fewer-Tasks"><a href="#Dual-level-Mixup-for-Graph-Few-shot-Learning-with-Fewer-Tasks" class="headerlink" title="Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks"></a>Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks</h2><p><strong>Authors:Yonghao Liu, Mengyu Li, Fausto Giunchiglia, Lan Huang, Ximing Li, Xiaoyue Feng, Renchu Guan</strong></p>
<p>Graph neural networks have been demonstrated as a powerful paradigm for effectively learning graph-structured data on the web and mining content from it.Current leading graph models require a large number of labeled samples for training, which unavoidably leads to overfitting in few-shot scenarios. Recent research has sought to alleviate this issue by simultaneously leveraging graph learning and meta-learning paradigms. However, these graph meta-learning models assume the availability of numerous meta-training tasks to learn transferable meta-knowledge. Such assumption may not be feasible in the real world due to the difficulty of constructing tasks and the substantial costs involved. Therefore, we propose a SiMple yet effectIve approach for graph few-shot Learning with fEwer tasks, named SMILE. We introduce a dual-level mixup strategy, encompassing both within-task and across-task mixup, to simultaneously enrich the available nodes and tasks in meta-learning. Moreover, we explicitly leverage the prior information provided by the node degrees in the graph to encode expressive node representations. Theoretically, we demonstrate that SMILE can enhance the model generalization ability. Empirically, SMILE consistently outperforms other competitive models by a large margin across all evaluated datasets with in-domain and cross-domain settings. Our anonymous code can be found here. </p>
<blockquote>
<p>图神经网络已被证明是在Web上有效学习图结构数据和从中挖掘内容的有力范式。当前领先的图模型需要大量的标签样本进行训练，这在少量样本场景中不可避免地导致过拟合。最近的研究试图通过同时利用图学习和元学习范式来缓解这个问题。然而，这些图元学习模型假设存在大量的元训练任务来学习可迁移的元知识。由于构建任务的难度和涉及的大量成本，这种假设在现实中可能不可行。因此，我们提出了一种简单有效的图小样学习新方法，名为SMILE（用于小样任务的图学习）。我们引入了一种双级混合策略，包括任务内和任务间混合，以同时丰富元学习中可用的节点和任务。此外，我们明确利用图中节点度提供的先验信息来编码表达性节点表示。理论上，我们证明了SMILE可以增强模型的泛化能力。在实证上，SMILE在所有评估的域内和跨域设置的数据集上均大幅超越其他竞争模型。我们的匿名代码可以在这里找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14158v1">PDF</a> WWW25</p>
<p><strong>Summary</strong></p>
<p>图神经网络已成为学习网上图结构数据和从中挖掘内容的有效范式。当前领先的图模型需要大量标注样本进行训练，这在小样数据场景中不可避免地导致过拟合。最近的研究通过结合图学习和元学习范式来缓解这一问题。然而，这些图元学习模型假设可用大量的元训练任务来学习可迁移的元知识，这在现实中可能不可行。因此，我们提出了一种简单有效的图小样学习的方法SMILE，通过双层次混合策略同时丰富可用节点和任务。理论上，SMILE能提升模型的泛化能力；实证上，SMILE在不同数据集上的表现优于其他模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图神经网络已成为学习和挖掘网上图结构数据的有效方法。</li>
<li>当前图模型在少量样本情况下易产生过拟合问题。</li>
<li>最近研究通过结合图学习和元学习来缓解这一问题。</li>
<li>元学习模型需要大量元训练任务，这在现实中可能难以实现。</li>
<li>SMILE方法通过双层次混合策略丰富可用节点和任务，有效应对小样数据问题。</li>
<li>SMILE能提升模型的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f24314b7f9b31147e246c4003edd23e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c23b5a4ad7ba7c1c2cf39657b8261d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf8c7b878a4aa107bf115129ca2ac5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c28378d461a701039541828dc3e26b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Retrieving-Versus-Understanding-Extractive-Evidence-in-Few-Shot-Learning"><a href="#Retrieving-Versus-Understanding-Extractive-Evidence-in-Few-Shot-Learning" class="headerlink" title="Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning"></a>Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning</h2><p><strong>Authors:Karl Elbakian, Samuel Carton</strong></p>
<p>A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document evidence for large language model in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with evidence retrieval errors with respect to gold-standard human-annotated extractive evidence for five datasets, using two popular closed proprietary models. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and evidence retrieval error, but that evidence retrieval error is mostly not associated with evidence interpretation error–a hopeful sign for downstream applications built on this mechanism. </p>
<blockquote>
<p>对齐的关键在于正确使用文档内的证据来构建文档级别的决策。我们在小样本环境下，分析了大型语言模型中检索和解释文档内证据之间的关系。具体来说，我们利用两个流行的大型专有模型，通过衡量模型预测误差与关于五个数据集的黄金标准人类注释提取证据的检索误差之间的关联程度。我们进行了两项消融研究，以调查标签预测和证据检索错误都可以归因于证据质量的情况。我们发现模型预测和证据检索错误之间存在强烈的实证关系，但证据检索错误主要与证据解释错误不相关——这对于基于这种机制的下游应用是一个积极的迹象。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14095v1">PDF</a> 9 pages, 8 figures, Accepted to AAAI 2025 Main Conference (AI   Alignment Track)</p>
<p><strong>Summary</strong>:</p>
<p>大型语言模型在少样本环境下的文档内证据检索与解读的关系分析。研究通过五个数据集测量模型预测错误与证据检索错误的相关性，并进行两项消融研究，发现模型预测与证据检索错误间存在强实证关系，但证据检索错误大多与证据解读错误无关，为下游应用提供了乐观的指示。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型在少样本环境下，证据检索对于文档级别的决策至关重要。</li>
<li>模型预测错误与证据检索错误之间存在强实证关系。</li>
<li>通过五个数据集的研究，发现证据检索错误并不主要与证据解读错误相关。</li>
<li>进行了两项消融研究，以探究标签预测和证据检索错误的来源。</li>
<li>研究使用了两个流行的封闭专有模型进行分析。</li>
<li>对于下游应用，证据检索和解读的分离提供了一个乐观的指示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-09d7bc72401fce7a116faf72483c9e66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86ef192aad73b7b5a85fdabecbb6fe8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0967916a388e89983d8257f98bf7f38b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-beda4ddec796bfd9f99032a6e4a32c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b648c8bc5ad6a999104181fc83f27eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01315f377fb5ed195ef4aa81463c3bf0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars"><a href="#RIDE-Enhancing-Large-Language-Model-Alignment-through-Restyled-In-Context-Learning-Demonstration-Exemplars" class="headerlink" title="RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars"></a>RIDE: Enhancing Large Language Model Alignment through Restyled   In-Context Learning Demonstration Exemplars</h2><p><strong>Authors:Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari</strong></p>
<p>Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment–factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at <a target="_blank" rel="noopener" href="https://github.com/AnonymousCode-ComputerScience/RIDE">https://github.com/AnonymousCode-ComputerScience/RIDE</a>. </p>
<blockquote>
<p>对齐调整对于确保大型语言模型（LLM）以伦理和有帮助的方式行为至关重要。当前的对齐方法需要高质量标注和大量的训练资源。本文提出了一种低成本、无需调整的方法，使用上下文学习（ICL）来增强LLM的对齐。通过对高质量ICL演示内容的分析，我们确定了风格是影响LLM对齐能力的关键因素，并基于此风格框架明确地对ICL示例进行了重新设计。此外，我们将重新设计的演示内容相结合，以实现LLM对齐的两个方面之间的平衡——事实性和安全性。我们将重新设计的示例打包为提示，以触发少量学习，提高LLM的对齐能力。与最佳基线方法相比（最高平均得分为5.00），我们的方法在Alpaca任务上最高得分增加了0.10（从4.50到4.60），在Just-eval基准测试上提高了0.22（从4.34到4.56），在MT-Bench数据集上最高提高了0.32（从3.53到3.85）。我们在<a target="_blank" rel="noopener" href="https://github.com/Anonymouscode-ComputerScience/RIDE%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E%E6%8D%AE%E3%80%82">https://github.com/AnonymousCode-ComputerScience/RIDE上发布了代码和数据。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11681v2">PDF</a> 38 pages, 2 figures, 20 tables; The paper is under review in ARR</p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种低成本、无需调优的方法，利用上下文学习（ICL）技术提升大型语言模型（LLM）的对齐性能。研究通过高质量ICL演示分析，发现风格是影响LLM对齐能力的重要因素，并据此重新设计了ICL范例。结合这些重新设计的范例，在LLM对齐的两个方面——真实性和安全性之间取得了平衡。最终通过触发少样本学习提升了LLM的对齐效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文强调大型语言模型（LLM）的伦理和实用对齐的重要性，并提出一种低成本的无需调优的方法。</li>
<li>利用上下文学习（ICL）技术，通过高质量示范分析发现风格是影响LLM对齐的关键因素。</li>
<li>论文重新设计了ICL范例，并基于风格框架进行展示。</li>
<li>通过结合重新设计的范例，在LLM对齐的两个方面——真实性和安全性之间取得平衡。</li>
<li>论文使用这些重新设计的例子作为提示来触发少样本学习，提高了LLM的对齐效果。</li>
<li>与最佳基线方法相比，论文提出的方法在Alpaca任务、Just-eval基准测试和MT-Bench数据集上均有所改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8ad113c3a36a68b867a335cf66eb9c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d96c391855703fe107b5c5c4644169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f156d089c7759aa95d148939c38fa8ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab40690d2dffabbf7fb8b5eb38abf2c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-800b7715e2257f5db9d7e3af024cf51a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Patient-Comments-Multi-Label-Classification"><a href="#Large-Language-Models-for-Patient-Comments-Multi-Label-Classification" class="headerlink" title="Large Language Models for Patient Comments Multi-Label Classification"></a>Large Language Models for Patient Comments Multi-Label Classification</h2><p><strong>Authors:Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin</strong></p>
<p>Patient experience and care quality are crucial for a hospital’s sustainability and reputation. The analysis of patient feedback offers valuable insight into patient satisfaction and outcomes. However, the unstructured nature of these comments poses challenges for traditional machine learning methods following a supervised learning paradigm. This is due to the unavailability of labeled data and the nuances these texts encompass. This research explores leveraging Large Language Models (LLMs) in conducting Multi-label Text Classification (MLTC) of inpatient comments shared after a stay in the hospital. GPT-4 Turbo was leveraged to conduct the classification. However, given the sensitive nature of patients’ comments, a security layer is introduced before feeding the data to the LLM through a Protected Health Information (PHI) detection framework, which ensures patients’ de-identification. Additionally, using the prompt engineering framework, zero-shot learning, in-context learning, and chain-of-thought prompting were experimented with. Results demonstrate that GPT-4 Turbo, whether following a zero-shot or few-shot setting, outperforms traditional methods and Pre-trained Language Models (PLMs) and achieves the highest overall performance with an F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the few-shot learning results. Subsequently, the results’ association with other patient experience structured variables (e.g., rating) was conducted. The study enhances MLTC through the application of LLMs, offering healthcare practitioners an efficient method to gain deeper insights into patient feedback and deliver prompt, appropriate responses. </p>
<blockquote>
<p>患者体验和护理质量对于医院的可持续性和声誉至关重要。患者反馈的分析为患者满意度和结果提供了宝贵的见解。然而，这些评论的非结构化特性给传统机器学习方法带来了挑战，尤其是在遵循监督学习范式的情况下。这是因为缺乏标记数据以及这些文本中蕴含的细节差异。本研究探讨了利用大型语言模型（LLM）进行住院患者评论的多标签文本分类（MLTC）。GPT-4 Turbo被用于进行分类。然而，鉴于患者评论的敏感性，在通过PHI检测框架将数据输入LLM之前，引入了一个安全层，以确保患者的匿名性。此外，通过提示工程框架，实验了零样本学习、上下文学习和链式思维提示。结果表明，无论是在零样本还是小样设置下，GPT-4 Turbo都优于传统方法和预训练语言模型（PLM），并在最高总体性能上达到了76.12%的F1分数和73.61%的加权F1分数，其次是少数样本学习的结果。随后，将结果与患者体验的其他结构化变量（如评分）进行了关联分析。该研究通过应用LLM增强了MLTC，为医疗从业者提供了一种有效的方法，可以更深入地了解患者反馈并做出及时、适当的回应。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23528v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用大型语言模型（LLMs）进行多标签文本分类（MLTC）分析住院患者评论的可行性。通过GPT-4 Turbo进行分类，并在馈送数据给LLM之前引入了保护健康信息（PHI）检测框架，确保患者去标识化。实验表明，GPT-4 Turbo在零样本和少样本设置下均优于传统方法和预训练语言模型（PLMs），最高性能达到F1分数为76.12%，加权F1分数为73.61%。该研究提高了通过LLMs进行MLTC的能力，为医疗从业者提供了一种深入了解患者反馈和及时、恰当地回应患者反馈的有效方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>患者体验和护理质量对医院的可持续性和声誉至关重要。</li>
<li>患者反馈分析为患者满意度和结果提供了宝贵的见解。</li>
<li>大型语言模型（LLMs）在处理住院患者评论的多标签文本分类（MLTC）方面具有潜力。</li>
<li>GPT-4 Turbo在分类中的表现优于传统方法和预训练语言模型（PLMs）。</li>
<li>在处理患者评论时，引入了保护健康信息（PHI）检测框架以确保患者去标识化。</li>
<li>通过实验，展示了零样本学习和少样本学习在LLMs中的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23528">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cd0e54c65f2611383d41bef5f13e9f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-446e380dd223d4f6898807f988831dab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c56ddba2a33fa5be9cf3e8c83c669ea.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models"><a href="#Revisiting-In-context-Learning-Inference-Circuit-in-Large-Language-Models" class="headerlink" title="Revisiting In-context Learning Inference Circuit in Large Language   Models"></a>Revisiting In-context Learning Inference Circuit in Large Language   Models</h2><p><strong>Authors:Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</strong></p>
<p>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (in the demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations of demonstrations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. Through careful measurements, the proposed inference circuit successfully captures and unifies many fragmented phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit. </p>
<blockquote>
<p>上下文学习（ICL）是语言模型（LM）上新兴的一种小样本学习范式，其内部机制尚未被探索。虽然已有工作描述了ICL的内部处理过程，但它们很难捕捉大型语言模型中的所有推理现象。因此，本文提出了一个综合电路来模拟推理动态，并试图解释观察到的ICL现象。具体来说，我们将ICL推理过程分为3个主要操作：（1）输入文本编码：语言模型将每个输入文本（在演示和查询中）编码成隐藏状态中的线性表示，其中包含足够的信息来解决ICL任务。（2）语义合并：语言模型将演示的编码表示与其相应的标签标记合并，以产生标签和演示的联合表示。（3）特征检索和复制：语言模型会在任务子空间中搜索与查询表示相似的演示联合表示，并将所搜索的表示复制到查询中。然后，语言模型头部会捕获这些复制的标签表示，并将其解码为预测的标签。通过精心测量，所提出的推理电路成功地捕获并统一了ICL过程中观察到的许多碎片化现象，是对ICL推理过程的全面实用解释。而且，通过禁用所提出的步骤进行的分析表明，这严重损害了ICL的性能，这表明所提出的推理电路是主导机制。此外，我们确认并列出了一些与所提出的电路并行解决ICL任务的旁路机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04468v4">PDF</a> 37 pages, 41 figures, 8 tables. ICLR 2025 Accepted. Camera-ready   Version</p>
<p><strong>Summary</strong></p>
<p>本文探讨了新兴的在语言模型中的少量学习模式——上下文学习（ICL）。文章提出了一种全面的电路模型，旨在模拟ICL的推理过程，并将其分为三个主要操作：输入文本编码、语义合并和特征检索与复制。此模型成功捕捉并统一了ICL过程中的碎片化现象，提供了对其过程的全面而实用的解释。此外，通过逐步拆解分析，验证了该推理电路在ICL中的核心作用。同时，也确认了其他并行解决ICL任务的旁路机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>上下文学习（ICL）是一种新兴的语言模型中的少量学习模式，其内部机制尚未被完全探索。</li>
<li>现有研究在描述ICL的内部处理方面已取得一定成果，但仍难以全面捕捉大型语言模型的推理现象。</li>
<li>本文提出了一个全面的电路模型，用以模拟ICL的推理过程，该过程分为三个主要操作：输入文本编码、语义合并和特征检索与复制。</li>
<li>该电路模型成功捕捉并统一了ICL过程中的碎片化现象，为理解这一过程提供了全面而实用的视角。</li>
<li>通过逐步拆解分析，验证了该推理电路在ICL中的核心作用。</li>
<li>除了主要的推理电路外，还存在其他并行解决ICL任务的旁路机制。</li>
<li>通过对ICL的深入剖析，本文为语言模型的进一步优化和改进提供了理论基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1fb2cbc690cfe62632360b24cedc46cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adb2d74983103299ca22b732c6dd9846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb11aef9a2f00241404f5ca8a1a9a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a13f5c010bd41cb3db361a0b0b86a6ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a749688bf036aa0269712c360232ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30dc931cf6d35a7daceb94618dadb35e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM4TS-Aligning-Pre-Trained-LLMs-as-Data-Efficient-Time-Series-Forecasters"><a href="#LLM4TS-Aligning-Pre-Trained-LLMs-as-Data-Efficient-Time-Series-Forecasters" class="headerlink" title="LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series   Forecasters"></a>LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series   Forecasters</h2><p><strong>Authors:Ching Chang, Wei-Yao Wang, Wen-Chih Peng, Tien-Fu Chen</strong></p>
<p>Multivariate time-series forecasting is vital in various domains, e.g., economic planning and weather prediction. Deep train-from-scratch models have exhibited effective performance yet require large amounts of data, which limits real-world applicability. Recently, researchers have leveraged the representation learning transferability of pre-trained Large Language Models (LLMs) to handle limited non-linguistic datasets effectively. However, incorporating LLMs with time-series data presents challenges of limited adaptation due to different compositions between time-series and linguistic data, and the inability to process multi-scale temporal information. To tackle these challenges, we propose LLM4TS, a framework for time-series forecasting with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the time-series alignment stage to align LLMs with the nuances of time-series data, and the forecasting fine-tuning stage for downstream time-series forecasting tasks. Furthermore, our framework features a novel two-level aggregation method that integrates multi-scale temporal data within pre-trained LLMs, enhancing their ability to interpret time-specific information. In experiments across 7 time-series forecasting datasets, LLM4TS is superior to existing state-of-the-art methods compared with trained-from-scratch models in full-shot scenarios, and also achieves the highest rank in few-shot scenarios. In addition, evaluations compared with different unsupervised representation learning approaches highlight LLM4TS’s effectiveness with representation learning in forecasting tasks. Ablation studies further validate each component’s contribution to LLM4TS and underscore the essential role of utilizing LLM’s pre-trained weights for optimal performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/blacksnail789521/LLM4TS">https://github.com/blacksnail789521/LLM4TS</a>. </p>
<blockquote>
<p>多元时间序列预测在经济规划、天气预报等领域具有至关重要的作用。虽然深度从头训练模型表现出有效的性能，但它们需要大量的数据，从而限制了其在现实世界中的应用。最近，研究者们利用预训练的大型语言模型（LLMs）的表示学习迁移能力，有效地处理有限的非语言数据集。然而，将LLMs与时间序列数据相结合面临着有限的适应性挑战，这是由于时间序列数据和语言数据之间的不同组成以及无法处理多尺度时间信息。为了应对这些挑战，我们提出了LLM4TS，这是一个利用预训练LLMs进行时间序列预测的框架。LLM4TS由两阶段微调策略组成：时间序列对齐阶段，使LLMs与时间序列数据的细微差别对齐；以及用于下游时间序列预测任务的预测微调阶段。此外，我们的框架具有新颖的两级聚合方法，能够在预训练的LLMs内集成多尺度时间数据，增强它们解释时间特定信息的能力。在7个时间序列预测数据集上的实验表明，LLM4TS在全程场景中的表现优于现有的最先进方法，并且在小样本场景中获得了最高排名。此外，与不同的无监督表示学习方法进行的评估突出了LLM4TS在预测任务中表示学习的有效性。消融研究进一步验证了LLM4TS每个组件的贡献，并强调了利用LLM的预训练权重对于实现最佳性能的重要作用。代码可在<a target="_blank" rel="noopener" href="https://github.com/blacksnail789521/LLM4TS%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/blacksnail789521/LLM4TS上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08469v6">PDF</a> Accepted for publication in ACM Transactions on Intelligent Systems   and Technology (TIST) 2025. The final published version will be available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3719207">https://doi.org/10.1145/3719207</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了LLM4TS框架在多变量时间序列预测中的应用。针对预训练的大型语言模型（LLMs）在有限数据下处理非语言数据集的有效性和局限性，提出一种包含两阶段微调策略的新框架。实验证明，在多种时间序列预测数据集上，LLM4TS优于现有方法，特别是针对训练自基础模型的方法以及无监督表示学习方法时。同时提出一个多层次数据聚合方法，提高了预训练LLMs解释时间序列特定信息的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM4TS是一个基于预训练的大型语言模型（LLMs）的时间序列预测框架。</li>
<li>该框架采用两阶段微调策略，包括时间序列对齐和预测微调阶段。</li>
<li>LLM4TS框架使用多层次数据聚合方法，可以处理多尺度时间序列数据。</li>
<li>在多个数据集上的实验表明，LLM4TS优于其他方法，特别是处理少样本数据场景时。相较于从头开始训练的模型，其性能更加优越。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.08469">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04c8bb68123bb6293e2ac5825b894ca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d41ba37531ef9fad265bd24cf454e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8747398ff066159c656bfae393e5f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5101a9036f9d95d77f6c8fd620d82dde.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c72a4687c2ce6dfeaa7d819eab4c6fb8.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-22  d-Sketch Improving Visual Fidelity of Sketch-to-Image Translation with   Pretrained Latent Diffusion Models without Retraining
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f0c532212a2fd8648edb1bc73899e73.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-22  Building reliable sim driving agents by scaling self-play
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
