<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  MedVAE Efficient Automated Interpretation of Medical Images with   Large-Scale Generalizable Autoencoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-66847e975d9f58abc0b6078652e5f7d0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-22-æ›´æ–°"><a href="#2025-02-22-æ›´æ–°" class="headerlink" title="2025-02-22 æ›´æ–°"></a>2025-02-22 æ›´æ–°</h1><h2 id="MedVAE-Efficient-Automated-Interpretation-of-Medical-Images-with-Large-Scale-Generalizable-Autoencoders"><a href="#MedVAE-Efficient-Automated-Interpretation-of-Medical-Images-with-Large-Scale-Generalizable-Autoencoders" class="headerlink" title="MedVAE: Efficient Automated Interpretation of Medical Images with   Large-Scale Generalizable Autoencoders"></a>MedVAE: Efficient Automated Interpretation of Medical Images with   Large-Scale Generalizable Autoencoders</h2><p><strong>Authors:Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari</strong></p>
<p>Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/StanfordMIMI/MedVAE">https://github.com/StanfordMIMI/MedVAE</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä»¥é«˜åˆ†è¾¨ç‡å’Œå¤§è§†é‡è·å–ï¼Œä»¥æ•æ‰ä¸´åºŠå†³ç­–æ‰€éœ€çš„å°ç‰¹å¾ã€‚å› æ­¤ï¼Œåœ¨åŒ»å­¦å›¾åƒä¸Šè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†ç¼©å°åŒ»å­¦å›¾åƒå°ºå¯¸çš„æŒ‘æˆ˜ï¼Œä»¥æé«˜ä¸‹æ¸¸è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™ä¸ä¸´åºŠç›¸å…³çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä»‹ç»äº†MedVAEï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¤§å‹äºŒç»´å’Œä¸‰ç»´è‡ªç¼–ç å™¨å®¶æ—ï¼Œèƒ½å¤Ÿå°†åŒ»å­¦å›¾åƒç¼–ç ä¸ºç¼©å°çš„æ½œåœ¨è¡¨ç¤ºå½¢å¼ï¼Œå¹¶å°†æ½œåœ¨è¡¨ç¤ºå½¢å¼è§£ç å›é«˜åˆ†è¾¨ç‡å›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œ1052730å¼ åŒ»å­¦å›¾åƒæ¥è®­ç»ƒMedVAEè‡ªç¼–ç å™¨ã€‚ä»20ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸­è·å¾—çš„å„ç§ä»»åŠ¡è¡¨æ˜ï¼šï¼ˆ1ï¼‰åœ¨è®­ç»ƒä¸‹æ¸¸æ¨¡å‹æ—¶ä½¿ç”¨MedVAEæ½œåœ¨è¡¨ç¤ºå½¢å¼ä»£æ›¿é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¯ä»¥åœ¨ä¿ç•™ä¸ä¸´åºŠç›¸å…³ç‰¹å¾çš„åŒæ—¶å¸¦æ¥æ•ˆç‡æ•ˆç›Šï¼ˆååé‡æœ€å¤šæé«˜70å€ï¼‰ï¼›ï¼ˆ2ï¼‰MedVAEå¯ä»¥å°†æ½œåœ¨è¡¨ç¤ºå½¢å¼è§£ç å›é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä¿çœŸåº¦å¾ˆé«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§è§„æ¨¡ã€å¯æ¨å¹¿çš„è‡ªç¼–ç å™¨æœ‰åŠ©äºè§£å†³åŒ»å­¦é¢†åŸŸçš„å…³é”®æ•ˆç‡æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/StanfordMIMI/MedVAE%E3%80%82">https://github.com/StanfordMIMI/MedVAEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å›¾åƒçš„é«˜åˆ†è¾¨ç‡å’Œå¤§è§†é‡å¸¦æ¥äº†åºå¤§çš„è®¡ç®—æˆæœ¬ï¼Œå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒæ„æˆæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºMedVAEï¼Œä¸€ç§èƒ½å¤Ÿç¼–ç åŒ»ç–—å›¾åƒä¸ºç¼©å°æ½œåœ¨è¡¨å¾å¹¶è§£ç å›é«˜åˆ†è¾¨ç‡å›¾åƒçš„äºŒç»´å’Œä¸‰ç»´è‡ªç¼–ç å™¨å®¶æ—ã€‚é€šè¿‡é‡‡ç”¨æ–°å‹ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œå¤§é‡åŒ»ç–—å›¾åƒæ•°æ®ï¼Œç ”ç©¶è¯æ˜MedVAEåœ¨æé«˜ä¸‹æ¸¸æ¨¡å‹è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œèƒ½ä¿ç•™ä¸´åºŠç›¸å…³ç‰¹å¾ï¼Œå¹¶å®ç°é«˜è¾¾70å€çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒMedVAEè¿˜èƒ½é«˜è´¨é‡åœ°è§£ç æ½œåœ¨è¡¨å¾è‡³é«˜åˆ†è¾¨ç‡å›¾åƒã€‚ç ”ç©¶å±•ç¤ºäº†å¤§è§„æ¨¡è‡ªç¼–ç å™¨åœ¨åŒ»ç–—é¢†åŸŸè§£å†³æ•ˆç‡é—®é¢˜çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒçš„é«˜åˆ†è¾¨ç‡å’Œå¤§è§†é‡å¸¦æ¥è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>MedVAEæ˜¯ä¸€ç§ç”¨äºåŒ»ç–—å›¾åƒå¤„ç†çš„è‡ªç¼–ç å™¨å®¶æ—ï¼Œèƒ½ç¼–ç å›¾åƒä¸ºç¼©å°æ½œåœ¨è¡¨å¾ã€‚</li>
<li>MedVAEé‡‡ç”¨æ–°å‹ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œå¤§é‡åŒ»ç–—å›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>MedVAEèƒ½æé«˜ä¸‹æ¸¸æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™ä¸´åºŠç›¸å…³ç‰¹å¾ã€‚</li>
<li>MedVAEèƒ½å®ç°é«˜è¾¾70å€çš„æ€§èƒ½æå‡ã€‚</li>
<li>MedVAEèƒ½å°†æ½œåœ¨è¡¨å¾è§£ç å›é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œä¸”è´¨é‡é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2605a0e186129a817db2ba17bf46d2f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26eb210ab033769bcacfbfd825617c48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a488ecffc67cb06621b354f9babdc78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e48a163156bed070723f4bad107d233a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5031b4bbfb37e8e60748ae9d32207a2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TRUSWorthy-Toward-Clinically-Applicable-Deep-Learning-for-Confident-Detection-of-Prostate-Cancer-in-Micro-Ultrasound"><a href="#TRUSWorthy-Toward-Clinically-Applicable-Deep-Learning-for-Confident-Detection-of-Prostate-Cancer-in-Micro-Ultrasound" class="headerlink" title="TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident   Detection of Prostate Cancer in Micro-Ultrasound"></a>TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident   Detection of Prostate Cancer in Micro-Ultrasound</h2><p><strong>Authors:Mohamed Harmanani, Paul F. R. Wilson, Minh Nguyen Nhat To, Mahdi Gilany, Amoon Jamzad, Fahimeh Fooladgar, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi</strong></p>
<p>While deep learning methods have shown great promise in improving the effectiveness of prostate cancer (PCa) diagnosis by detecting suspicious lesions from trans-rectal ultrasound (TRUS), they must overcome multiple simultaneous challenges. There is high heterogeneity in tissue appearance, significant class imbalance in favor of benign examples, and scarcity in the number and quality of ground truth annotations available to train models. Failure to address even a single one of these problems can result in unacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed, tuned, and integrated system for reliable PCa detection. Our pipeline integrates self-supervised learning, multiple-instance learning aggregation using transformers, random-undersampled boosting and ensembling: these address label scarcity, weak labels, class imbalance, and overconfidence, respectively. We train and rigorously evaluate our method using a large, multi-center dataset of micro-ultrasound data. Our method outperforms previous state-of-the-art deep learning methods in terms of accuracy and uncertainty calibration, with AUROC and balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20% of predictions with the highest confidence, we can achieve a balanced accuracy of up to 91%. The success of TRUSWorthy demonstrates the potential of integrated deep learning solutions to meet clinical needs in a highly challenging deployment setting, and is a significant step towards creating a trustworthy system for computer-assisted PCa diagnosis. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨é€šè¿‡ç»ç›´è‚ è¶…å£°ï¼ˆTRUSï¼‰æ£€æµ‹å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰å¯ç–‘ç—…ç¶ä»è€Œæé«˜è¯Šæ–­æ•ˆç‡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬å¿…é¡»å…‹æœå¤šä¸ªåŒæ—¶å‡ºç°çš„æŒ‘æˆ˜ã€‚ç»„ç»‡å¤–è§‚å­˜åœ¨é«˜åº¦å¼‚è´¨æ€§ï¼Œè‰¯æ€§æ ·æœ¬çš„ç±»åˆ«ä¸å¹³è¡¡æ˜¾è‘—ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹çš„çœŸå®æ³¨é‡Šçš„æ•°é‡å’Œè´¨é‡ç¨€ç¼ºã€‚å³ä½¿ä¸èƒ½è§£å†³å…¶ä¸­ä»»ä½•ä¸€ä¸ªé—®é¢˜ï¼Œä¹Ÿå¯èƒ½ä¼šå¯¼è‡´ä¸´åºŠç»“æœæ— æ³•æ¥å—ã€‚æˆ‘ä»¬æå‡ºäº†TRUSWorthyï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡ã€è°ƒæ•´å’Œé›†æˆçš„å¯é PCaæ£€æµ‹ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç®¡é“é›†æˆäº†è‡ªç›‘ç£å­¦ä¹ ã€ä½¿ç”¨å˜å‹å™¨çš„å¤šå®ä¾‹å­¦ä¹ èšåˆã€éšæœºæ¬ é‡‡æ ·å¢å¼ºå’Œé›†æˆï¼šè¿™äº›åˆ†åˆ«è§£å†³äº†æ ‡ç­¾ç¨€ç¼ºã€å¼±æ ‡ç­¾ã€ç±»åˆ«ä¸å¹³è¡¡å’Œè¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§è§„æ¨¡å¤šä¸­å¿ƒè¶…å£°å¾®æ•°æ®é›†è®­å’Œä¸¥æ ¼è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®åº¦å’Œä¸ç¡®å®šæ€§æ ¡å‡†æ–¹é¢è¶…è¶Šäº†ä¹‹å‰çš„æœ€æ–°æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œæ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰å’Œå¹³è¡¡å‡†ç¡®åº¦åˆ†åˆ«ä¸º79.9%å’Œ71.5%ã€‚åœ¨é¢„æµ‹ç½®ä¿¡åº¦æœ€é«˜çš„å‰20%ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°é«˜è¾¾91%çš„å¹³è¡¡å‡†ç¡®ç‡ã€‚TRUSWorthyçš„æˆåŠŸå±•ç¤ºäº†åœ¨é«˜åº¦æŒ‘æˆ˜çš„éƒ¨ç½²ç¯å¢ƒä¸­ï¼Œé›†æˆæ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆæ»¡è¶³ä¸´åºŠéœ€æ±‚çš„æ½œåŠ›ï¼Œå¹¶ä¸”æ˜¯æœç€åˆ›å»ºå¯ä¿¡çš„è®¡ç®—æœºè¾…åŠ©PCaè¯Šæ–­ç³»ç»Ÿè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14707v1">PDF</a> accepted to IJCARS. This preprint has not undergone post-submission   improvements or corrections. To access the Version of Record of this article,   see the journal reference below</p>
<p><strong>Summary</strong></p>
<p>åœ¨ç¶“ç›´è‚ è¶…è²æ³¢ï¼ˆTRUSï¼‰ä¸­æª¢æ¸¬å¯ç–‘ç—…è®Šä»¥æ”¹å–„å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰è¨ºæ–·æ•ˆæœçš„æ·±åº¦å­¸ç¿’æ–¹æ³•å±•ç¾å‡ºå·¨å¤§æ½›åŠ›ï¼Œä½†åŒæ™‚ä¹Ÿéœ€å…‹æœå¤šé‡å•é¡Œã€‚æœ¬æ–‡æå‡ºTRUSWorthyç³»çµ±ï¼Œæ•´åˆè‡ªç›£ç£å­¸ç¿’ã€å¤šå¯¦ä¾‹å­¸ç¿’èšåˆä½¿ç”¨è®Šæ›å™¨ã€éš¨æ©Ÿæ¬ æ¨£æœ¬å¢å¼·å’Œé›†æˆç­‰æ–¹æ³•ï¼Œè§£æ±ºæ¨™ç±¤ç¼ºä¹ã€å¼±æ¨™ç±¤ã€é¡åˆ¥ä¸å¹³è¡¡å’Œéåº¦è‡ªä¿¡ç­‰å•é¡Œã€‚åœ¨å¤§å‹å¤šä¸­å¿ƒå¾®è¶…è²æ³¢æ•¸æ“šé›†ä¸Šé€²è¡Œè¨“ç·´å’Œåš´æ ¼è©•ä¼°ï¼Œè©²æ–¹æ³•æ€§èƒ½å„ªæ–¼å…ˆå‰æœ€å…ˆé€²çš„æ·±åº¦å­¸ç¿’æ–¹æ³•ï¼Œæº–ç¢ºæ€§å’Œä¸ç¢ºå®šåº¦æ ¡æ­£çš„AUROCå’Œå¹³è¡¡æº–ç¢ºåº¦åˆ†åˆ¥é”åˆ°79.9%å’Œ71.5%ï¼Œåœ¨é æ¸¬ç½®ä¿¡åº¦æœ€é«˜çš„å‰20%ä¸­å¯å¯¦ç¾æœ€é«˜é”91%çš„å¹³è¡¡æº–ç¢ºåº¦ã€‚TRUSWorthyçš„æˆåŠŸé¡¯ç¤ºå‡ºé›†æˆæ·±åº¦å­¸ç¿’è§£æ±ºæ–¹æ¡ˆåœ¨é«˜åº¦æŒ‘æˆ°æ€§çš„éƒ¨ç½²ç’°å¢ƒä¸­æ»¿è¶³è¨ºæ–·éœ€æ±‚çš„æ½›åŠ›ï¼Œæ˜¯å»ºç«‹å¯ä¿¡çš„é›»è„ˆè¨ºæ–·ç³»çµ±çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å‰åˆ—è…ºç™Œè¯Šæ–­ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´ç»„ç»‡å¤–è§‚é«˜åº¦å¼‚è´¨æ€§ã€è‰¯æ€§ç¤ºä¾‹æ˜¾è‘—ç±»åˆ«ä¸å¹³è¡¡å’Œå¯ç”¨æ–¼è®­ç»ƒæ¨¡å‹çš„åœ°é¢çœŸç›¸æ³¨é‡Šæ•°é‡å’Œè³ªé‡ç¨€ç¼ºç­‰å¤šé‡æŒ‘æˆ˜ã€‚</li>
<li>TRUSWorthyç³»ç»Ÿé€šè¿‡æ•´åˆå¤šç§æŠ€æœ¯æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‡ªç›‘ç£å­¦ä¹ ã€å¤šå®ä¾‹å­¦ä¹ èšåˆä½¿ç”¨å˜æ¢å™¨ã€éšæœºæ¬ é‡‡æ ·å¢å¼ºå’Œé›†æˆã€‚</li>
<li>TRUSWorthyç³»ç»Ÿåœ¨å¤§å‹å¤šä¸­å¿ƒå¾®è¶…å£°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå’Œä¸¥æ ¼è¯„ä¼°ï¼Œæ€§èƒ½ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨å‡†ç¡®åº¦å’Œä¸ç¡®å®šæ€§æ ¡å‡†æ–¹é¢çš„è¡¨ç°ä¼˜å¼‚ï¼ŒAUROCå’Œå¹³è¡¡å‡†ç¡®åº¦åˆ†åˆ«ä¸º79.9%å’Œ71.5%ã€‚</li>
<li>åœ¨é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„å‰20%ä¸­ï¼Œå¹³è¡¡æº–ç¢ºåº¦å¯é”åˆ°91%ï¼Œé¡¯ç¤ºå‡ºè©²ç³»çµ±çš„å¯é æ€§ã€‚</li>
<li>TRUSWorthyçš„æˆåŠŸè¯æ˜äº†é›†æˆæ·±åº¦å­¸ç¿’æ–¹æ¡ˆåœ¨æå…·æŒ‘æˆ˜æ€§çš„éƒ¨ç½²ç¯å¢ƒä¸­æ»¡è¶³ä¸´åºŠè¯Šæ–­éœ€æ±‚çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2133c36c087e9f7af7a36e2650ebabf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0491aeae8754121100d087e0f6640d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a6ca0cc1dbab5d206a5bcdea1e7bb43.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges"><a href="#Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges" class="headerlink" title="Vision Foundation Models in Medical Image Analysis: Advances and   Challenges"></a>Vision Foundation Models in Medical Image Analysis: Advances and   Challenges</h2><p><strong>Authors:Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang</strong></p>
<p>The rapid development of Vision Foundation Models (VFMs), particularly Vision Transformers (ViT) and Segment Anything Model (SAM), has sparked significant advances in the field of medical image analysis. These models have demonstrated exceptional capabilities in capturing long-range dependencies and achieving high generalization in segmentation tasks. However, adapting these large models to medical image analysis presents several challenges, including domain differences between medical and natural images, the need for efficient model adaptation strategies, and the limitations of small-scale medical datasets. This paper reviews the state-of-the-art research on the adaptation of VFMs to medical image segmentation, focusing on the challenges of domain adaptation, model compression, and federated learning. We discuss the latest developments in adapter-based improvements, knowledge distillation techniques, and multi-scale contextual feature modeling, and propose future directions to overcome these bottlenecks. Our analysis highlights the potential of VFMs, along with emerging methodologies such as federated learning and model compression, to revolutionize medical image analysis and enhance clinical applications. The goal of this work is to provide a comprehensive overview of current approaches and suggest key areas for future research that can drive the next wave of innovation in medical image segmentation. </p>
<blockquote>
<p>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œåˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå¼•å‘äº†é‡å¤§çªç ´ã€‚è¿™äº›æ¨¡å‹åœ¨æ•æ‰é•¿æœŸä¾èµ–å…³ç³»å’Œå®ç°åˆ†å‰²ä»»åŠ¡çš„é«˜æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›å¤§å‹æ¨¡å‹é€‚åº”åŒ»å­¦å›¾åƒåˆ†æé¢ä¸´å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒ»å­¦å›¾åƒå’Œè‡ªç„¶å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®å¼‚ã€éœ€è¦æœ‰æ•ˆçš„æ¨¡å‹é€‚åº”ç­–ç•¥ä»¥åŠå°è§„æ¨¡åŒ»å­¦æ•°æ®é›†çš„å±€é™æ€§ã€‚æœ¬æ–‡ç»¼è¿°äº†å°†VFMsé€‚åº”åŒ»å­¦å›¾åƒåˆ†å‰²çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨é¢†åŸŸé€‚åº”ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è®¨è®ºäº†åŸºäºé€‚é…å™¨çš„æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡çš„æœ€æ–°å‘å±•ï¼Œå¹¶æå‡ºäº†å…‹æœè¿™äº›ç“¶é¢ˆçš„æœªæ¥å‘å±•æ–¹å‘ã€‚æˆ‘ä»¬çš„åˆ†æå¼ºè°ƒäº†VFMsçš„æ½œåŠ›ï¼Œä»¥åŠä¸è”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•ç›¸ç»“åˆï¼Œå°†é©å‘½åŒ»å­¦å›¾åƒåˆ†æå¹¶å¢å¼ºä¸´åºŠåº”ç”¨ã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯æä¾›å½“å‰æ–¹æ³•è®ºçš„å…¨é¢æ¦‚è¿°ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æå‡ºå…³é”®é¢†åŸŸï¼Œä»¥æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„ä¸‹ä¸€æ³¢åˆ›æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14584v1">PDF</a> 17 pages, 1 figure</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå› Vision Foundation Modelsï¼ˆVFMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯Vision Transformersï¼ˆViTï¼‰å’ŒSegment Anything Modelï¼ˆSAMï¼‰çš„å´›èµ·è€Œå–å¾—æ˜¾è‘—è¿›æ­¥ã€‚è¿™äº›æ¨¡å‹åœ¨æ•æ‰é•¿ç¨‹ä¾èµ–æ€§å’Œå®ç°é«˜æ³›åŒ–åˆ†å‰²ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›å¤§å‹æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŒ»å­¦ä¸è‡ªç„¶å›¾åƒé¢†åŸŸä¹‹é—´çš„å·®å¼‚ã€å¯¹é«˜æ•ˆæ¨¡å‹é€‚é…ç­–ç•¥çš„éœ€æ±‚ä»¥åŠå°è§„æ¨¡åŒ»å­¦æ•°æ®é›†çš„å±€é™æ€§ã€‚æœ¬æ–‡ç»¼è¿°äº†VFMsåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€æ–°ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨é¢†åŸŸé€‚é…ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ çš„æŒ‘æˆ˜ã€‚é€šè¿‡è®¨è®ºåŸºäºé€‚é…å™¨çš„æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†å…‹æœè¿™äº›ç“¶é¢ˆçš„æœªæ¥æ–¹å‘ã€‚æœ¬æ–‡å¼ºè°ƒäº†VFMsçš„æ½œåŠ›ï¼Œä»¥åŠè”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç­‰æ–°å…´æ–¹æ³•ï¼Œè¿™äº›æœ‰æœ›é©æ–°åŒ»å­¦å›¾åƒåˆ†æï¼Œæé«˜ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Foundation Models (VFMs) åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯Vision Transformers (ViT) å’Œ Segment Anything Model (SAM)ã€‚</li>
<li>VFMs åœ¨æ•æ‰é•¿ç¨‹ä¾èµ–æ€§å’Œå®ç°é«˜æ³›åŒ–åˆ†å‰²ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ã€‚</li>
<li>å°†VFMsåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†æé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é¢†åŸŸå·®å¼‚ã€æ¨¡å‹é€‚é…ç­–ç•¥éœ€æ±‚å’ŒåŒ»å­¦æ•°æ®é›†å±€é™æ€§ã€‚</li>
<li>è®ºæ–‡ç»¼è¿°äº†VFMsåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€æ–°ç ”ç©¶ï¼Œå…³æ³¨é¢†åŸŸé€‚é…ã€æ¨¡å‹å‹ç¼©å’Œè”é‚¦å­¦ä¹ çš„æŒ‘æˆ˜ã€‚</li>
<li>é€‚é…å™¨æ”¹è¿›ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šå°ºåº¦ä¸Šä¸‹æ–‡ç‰¹å¾å»ºæ¨¡æ˜¯å…‹æœæŒ‘æˆ˜çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>VFMs çš„æ½œåŠ›å·¨å¤§ï¼Œä¸æ–°å…´æ–¹æ³•å¦‚è”é‚¦å­¦ä¹ å’Œæ¨¡å‹å‹ç¼©ç»“åˆï¼Œæœ‰æœ›é©æ–°åŒ»å­¦å›¾åƒåˆ†æï¼Œæé«˜ä¸´åºŠåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5fca45c9874048949ef49b737f38287.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a807514980741b462b8353a810dc5e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Mobile-Robotic-Approach-to-Autonomous-Surface-Scanning-in-Legal-Medicine"><a href="#A-Mobile-Robotic-Approach-to-Autonomous-Surface-Scanning-in-Legal-Medicine" class="headerlink" title="A Mobile Robotic Approach to Autonomous Surface Scanning in Legal   Medicine"></a>A Mobile Robotic Approach to Autonomous Surface Scanning in Legal   Medicine</h2><p><strong>Authors:Sarah Grube, Sarah Latus, Martin Fischer, Vidas Raudonis, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer</strong></p>
<p>Purpose: Comprehensive legal medicine documentation includes both an internal but also an external examination of the corpse. Typically, this documentation is conducted manually during conventional autopsy. A systematic digital documentation would be desirable, especially for the external examination of wounds, which is becoming more relevant for legal medicine analysis. For this purpose, RGB surface scanning has been introduced. While a manual full surface scan using a handheld camera is timeconsuming and operator dependent, floor or ceiling mounted robotic systems require substantial space and a dedicated room. Hence, we consider whether a mobile robotic system can be used for external documentation. Methods: We develop a mobile robotic system that enables full-body RGB-D surface scanning. Our work includes a detailed configuration space analysis to identify the environmental parameters that need to be considered to successfully perform a surface scan. We validate our findings through an experimental study in the lab and demonstrate the systemâ€™s application in a legal medicine environment. Results: Our configuration space analysis shows that a good trade-off between coverage and time is reached with three robot base positions, leading to a coverage of 94.96 %. Experiments validate the effectiveness of the system in accurately capturing body surface geometry with an average surface coverage of 96.90 +- 3.16 % and 92.45 +- 1.43 % for a body phantom and actual corpses, respectively. Conclusion: This work demonstrates the potential of a mobile robotic system to automate RGB-D surface scanning in legal medicine, complementing the use of post-mortem CT scans for inner documentation. Our results indicate that the proposed system can contribute to more efficient and autonomous legal medicine documentation, reducing the need for manual intervention. </p>
<blockquote>
<p>ç›®çš„ï¼šå…¨é¢çš„æ³•å¾‹åŒ»å­¦æ–‡æ¡£è®°å½•åŒ…æ‹¬å†…éƒ¨å’Œå¤–éƒ¨çš„å°¸ä½“æ£€æŸ¥ã€‚é€šå¸¸ï¼Œè¿™ç§æ–‡æ¡£è®°å½•æ˜¯åœ¨ä¼ ç»Ÿå°¸æ£€è¿‡ç¨‹ä¸­æ‰‹åŠ¨è¿›è¡Œçš„ã€‚å¯¹äºä¼¤å£çš„å¤–éƒ¨æ£€æŸ¥ï¼Œå°¤å…¶éœ€è¦ç³»ç»Ÿçš„æ•°å­—åŒ–è®°å½•ï¼Œè¿™åœ¨æ³•å¾‹åŒ»å­¦åˆ†æä¸­å…·æœ‰è¶Šæ¥è¶Šé‡è¦çš„æ„ä¹‰ã€‚ä¸ºæ­¤ï¼Œå·²ç»å¼•å…¥äº†RGBè¡¨é¢æ‰«ææŠ€æœ¯ã€‚ä½¿ç”¨æ‰‹æŒç›¸æœºè¿›è¡Œæ‰‹åŠ¨å…¨é¢è¡¨é¢æ‰«ææ—¢è€—æ—¶åˆä¾èµ–äºæ“ä½œäººå‘˜ï¼Œè€Œåœ°é¢æˆ–å¤©èŠ±æ¿å®‰è£…çš„æœºå™¨äººç³»ç»Ÿåˆ™éœ€è¦å¤§é‡ç©ºé—´å’Œä¸€ä¸ªä¸“é—¨çš„æˆ¿é—´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘æ˜¯å¦å¯ä»¥ä½¿ç”¨ç§»åŠ¨æœºå™¨äººç³»ç»Ÿè¿›è¡Œå¤–éƒ¨è®°å½•ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç§»åŠ¨æœºå™¨äººç³»ç»Ÿï¼Œèƒ½å¤Ÿè¿›è¡Œå…¨èº«RGB-Dè¡¨é¢æ‰«æã€‚æˆ‘ä»¬çš„å·¥ä½œåŒ…æ‹¬è¯¦ç»†çš„é…ç½®ç©ºé—´åˆ†æï¼Œä»¥è¯†åˆ«æˆåŠŸæ‰§è¡Œè¡¨é¢æ‰«æéœ€è¦è€ƒè™‘çš„ç¯å¢ƒå‚æ•°ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå®¤çš„å®éªŒç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ï¼Œå¹¶å±•ç¤ºäº†è¯¥ç³»ç»Ÿåœ¨æ³•å¾‹åŒ»å­¦ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚ç»“æœï¼šæˆ‘ä»¬çš„é…ç½®ç©ºé—´åˆ†æè¡¨æ˜ï¼Œé€šè¿‡ä¸‰ä¸ªæœºå™¨äººåŸºåº§ä½ç½®å¯ä»¥è¾¾åˆ°è¦†ç›–ç‡å’Œæ—¶é—´ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ï¼Œè¦†ç›–ç‡ä¸º94.96%ã€‚å®éªŒéªŒè¯äº†è¯¥ç³»ç»Ÿåœ¨å‡†ç¡®æ•æ‰äººä½“è¡¨é¢å‡ ä½•ç»“æ„æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¯¹äºäººä½“å‡ä½“å’Œå®é™…å°¸ä½“ï¼Œå¹³å‡è¡¨é¢è¦†ç›–ç‡åˆ†åˆ«ä¸º96.90 Â± 3.16%å’Œ92.45 Â± 1.43%ã€‚ç»“è®ºï¼šè¿™é¡¹å·¥ä½œè¯æ˜äº†ç§»åŠ¨æœºå™¨äººç³»ç»Ÿåœ¨æ³•å¾‹åŒ»å­¦ä¸­è‡ªåŠ¨è¿›è¡ŒRGB-Dè¡¨é¢æ‰«æçš„æ½œåŠ›ï¼Œå¯ä»¥é…åˆæ­»åCTæ‰«æç”¨äºå†…éƒ¨è®°å½•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿå¯ä»¥æé«˜æ³•å¾‹åŒ»å­¦è®°å½•çš„æ•ˆç‡å’Œè‡ªä¸»æ€§ï¼Œå‡å°‘äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14514v1">PDF</a> Submitted and accepted for presentation at CARS 2025. This preprint   has not undergone peer review or post-submission revisions. The final version   of this work will appear in the official CARS 2025 proceedings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºæ³•å¾‹åŒ»å­¦æ–‡æ¡£è®°å½•çš„ç§»åŠ¨æœºå™¨äººç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œå…¨èº«RGB-Dè¡¨é¢æ‰«æã€‚é€šè¿‡é…ç½®ç©ºé—´åˆ†æï¼Œç¡®å®šäº†æˆåŠŸè¿›è¡Œè¡¨é¢æ‰«ææ‰€éœ€è€ƒè™‘çš„ç¯å¢ƒå‚æ•°ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®æ•æ‰äººä½“è¡¨é¢å‡ ä½•ç»“æ„ï¼Œå¯¹å°¸ä½“è¿›è¡Œè‡ªåŠ¨RGBè¡¨é¢æ‰«æå…·æœ‰æ½œåŠ›ï¼Œæœ‰åŠ©äºæ›´æœ‰æ•ˆç‡ä¸”è‡ªä¸»çš„åŒ»å­¦æ–‡æ¡£è®°å½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æœºå™¨äººç³»ç»Ÿè¢«å¼€å‘ç”¨äºæ³•å¾‹åŒ»å­¦ä¸­çš„å…¨èº«RGB-Dè¡¨é¢æ‰«æã€‚</li>
<li>é…ç½®ç©ºé—´åˆ†æç¡®å®šäº†æˆåŠŸè¿›è¡Œè¡¨é¢æ‰«ææ‰€éœ€è€ƒè™‘çš„ç¯å¢ƒå‚æ•°ã€‚</li>
<li>å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå‡†ç¡®æ•æ‰äººä½“è¡¨é¢å‡ ä½•ç»“æ„ã€‚</li>
<li>ç³»ç»Ÿåœ¨å°¸ä½“è¡¨é¢æ‰«æçš„è¦†ç›–ç‡é«˜ï¼Œä¸º94.96%ã€‚</li>
<li>ä¸å°¸ä½“å¹»å½±å’ŒçœŸå®å°¸ä½“çš„å®éªŒè¡¨æ˜ï¼Œç³»ç»Ÿè¡¨é¢è¦†ç›–ç‡ä¸º96.90%Â±3.16%å’Œ92.45%Â±1.43%ã€‚</li>
<li>è¯¥ç³»ç»Ÿæœ‰åŠ©äºæ›´æœ‰æ•ˆç‡ä¸”è‡ªä¸»çš„åŒ»å­¦æ–‡æ¡£è®°å½•ï¼Œå‡å°‘äººå·¥å¹²é¢„çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1c8a84cf0b88a7cc85cf4b1fcedfb72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efd5176a3bbb679f8ec42ad6951ad94a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f555ff71126b4d4596dd3c8c2f78856.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Role-of-the-Pretraining-and-the-Adaptation-data-sizes-for-low-resource-real-time-MRI-video-segmentation"><a href="#Role-of-the-Pretraining-and-the-Adaptation-data-sizes-for-low-resource-real-time-MRI-video-segmentation" class="headerlink" title="Role of the Pretraining and the Adaptation data sizes for low-resource   real-time MRI video segmentation"></a>Role of the Pretraining and the Adaptation data sizes for low-resource   real-time MRI video segmentation</h2><p><strong>Authors:Masoud Thajudeen Tholan, Vinayaka Hegde, Chetan Sharma, Prasanta Kumar Ghosh</strong></p>
<p>Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech production studies as it provides a complete view of the vocal tract during articulation. This study investigates the effectiveness of rtMRI in analyzing vocal tract movements by employing the SegNet and UNet models for Air-Tissue Boundary (ATB)segmentation tasks. We conducted pretraining of a few base models using increasing numbers of subjects and videos, to assess performance on two datasets. First, consisting of unseen subjects with unseen videos from the same data source, achieving 0.33% and 0.91% (Pixel-wise Classification Accuracy (PCA) and Dice Coefficient respectively) better than its matched condition. Second, comprising unseen videos from a new data source, where we obtained an accuracy of 99.63% and 98.09% (PCA and Dice Coefficient respectively) of its matched condition performance. Here, matched condition performance refers to the performance of a model trained only on the test subjects which was set as a benchmark for the other models. Our findings highlight the significance of fine-tuning and adapting models with limited data. Notably, we demonstrated that effective model adaptation can be achieved with as few as 15 rtMRI frames from any new dataset. </p>
<blockquote>
<p>å®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰åœ¨è¯­éŸ³ç”Ÿäº§ç ”ç©¶ä¸­ç»å¸¸è¢«ä½¿ç”¨ï¼Œå› ä¸ºå®ƒèƒ½æä¾›å‘éŸ³æ—¶æ•´ä¸ªå£°é“çš„è§†å›¾ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†rtMRIåœ¨åˆ†æå£°é“è¿åŠ¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé‡‡ç”¨SegNetå’ŒUNetæ¨¡å‹è¿›è¡Œç©ºæ°”-ç»„ç»‡è¾¹ç•Œï¼ˆATBï¼‰åˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å¤šä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨ä¸æ–­å¢åŠ çš„å—è¯•è€…å’Œè§†é¢‘æ•°é‡æ¥è¯„ä¼°ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªåŒä¸€æ•°æ®æºçš„æœªè§è¿‡çš„å—è¯•è€…åŠå…¶è§†é¢‘ï¼Œç›¸è¾ƒäºåŒ¹é…æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œå–å¾—äº†0.33%ï¼ˆåƒç´ çº§åˆ†ç±»å‡†ç¡®åº¦ï¼‰å’Œ0.91%ï¼ˆDiceç³»æ•°ï¼‰çš„æ”¹è¿›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªæ–°æ•°æ®æºçš„æœªè§è¿‡çš„è§†é¢‘ï¼Œç›¸è¾ƒäºåŒ¹é…æ¡ä»¶ä¸‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬è·å¾—äº†99.63%ï¼ˆPCAï¼‰å’Œ98.09%ï¼ˆDiceç³»æ•°ï¼‰çš„å‡†ç¡®ç‡ã€‚è¿™é‡Œï¼ŒåŒ¹é…æ¡ä»¶ä¸‹çš„æ€§èƒ½æ˜¯æŒ‡ä»…å¯¹æµ‹è¯•å—è¯•è€…è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹çš„è¡¨ç°ï¼Œå®ƒè¢«è®¾å®šä¸ºå…¶ä»–æ¨¡å‹æ€§èƒ½çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†å¾®è°ƒå¹¶é€‚åº”æœ‰é™æ•°æ®çš„æ¨¡å‹çš„é‡è¦æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜ä»…ä½¿ç”¨ä»»ä½•æ–°æ•°æ®é›†çš„15ä¸ªrtMRIå¸§å³å¯å®ç°æœ‰æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14418v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨rtMRIå®æ—¶ç£å…±æŒ¯æˆåƒæŠ€æœ¯ï¼Œé€šè¿‡SegNetå’ŒUNetæ¨¡å‹å¯¹ç©ºæ°”ç»„ç»‡è¾¹ç•Œï¼ˆATBï¼‰åˆ†å‰²ä»»åŠ¡è¿›è¡Œåˆ†æï¼Œæ¢è®¨å…¶åœ¨ç ”ç©¶è¯­éŸ³äº§ç”Ÿè¿‡ç¨‹ä¸­å£°å¸¦è¿åŠ¨çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶é€šè¿‡åœ¨ä¸åŒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¹¶å¢åŠ å—è¯•è€…å’Œè§†é¢‘æ•°é‡æ¥è¯„ä¼°æ€§èƒ½ã€‚åœ¨æœªè§å—è¯•è€…å’Œæœªè§è§†é¢‘çš„æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ï¼Œä¸”åœ¨æ–°æ•°æ®æºæœªè§è§†é¢‘çš„æµ‹è¯•ä¸­ï¼Œæ¨¡å‹æ€§èƒ½è¾¾åˆ°äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°ç²¾ç»†è°ƒæ•´æ¨¡å‹å¹¶é€‚åº”æœ‰é™æ•°æ®è‡³å…³é‡è¦ï¼Œä»…ä½¿ç”¨æ–°æ•°æ®é›†çš„å°‘é‡rtMRIå¸§å³å¯å®ç°æœ‰æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>rtMRIåœ¨è¯­éŸ³äº§ç”Ÿç ”ç©¶ä¸­è¢«å¹¿æ³›åº”ç”¨äºè§‚å¯Ÿå£°å¸¦è¿åŠ¨ã€‚</li>
<li>SegNetå’ŒUNetæ¨¡å‹è¢«ç”¨äºè¿›è¡Œç©ºæ°”ç»„ç»‡è¾¹ç•Œï¼ˆATBï¼‰åˆ†å‰²ä»»åŠ¡çš„åˆ†æã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å¹¶å¢åŠ å—è¯•è€…å’Œè§†é¢‘æ•°é‡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨æœªè§å—è¯•è€…å’Œæœªè§è§†é¢‘çš„æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>åœ¨æ–°æ•°æ®æºæœªè§è§†é¢‘çš„æµ‹è¯•ä¸­ï¼Œæ¨¡å‹æ€§èƒ½è¾¾åˆ°äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´æ¨¡å‹å¹¶é€‚åº”æœ‰é™æ•°æ®è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9c090acae09eb44ce9337cd00ad33d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66847e975d9f58abc0b6078652e5f7d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1b0c3a72fe7b91cc415e6e7da6664aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-485f79e1227004bc012f8a82d62dba82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc86ff5507c3905cb21d164d9f74d9c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedFuncta-Modality-Agnostic-Representations-Based-on-Efficient-Neural-Fields"><a href="#MedFuncta-Modality-Agnostic-Representations-Based-on-Efficient-Neural-Fields" class="headerlink" title="MedFuncta: Modality-Agnostic Representations Based on Efficient Neural   Fields"></a>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural   Fields</h2><p><strong>Authors:Paul Friedrich, Florentin Bieder, Phlippe C. Cattin</strong></p>
<p>Recent research in medical image analysis with deep learning almost exclusively focuses on grid- or voxel-based data representations. We challenge this common choice by introducing MedFuncta, a modality-agnostic continuous data representation based on neural fields. We demonstrate how to scale neural fields from single instances to large datasets by exploiting redundancy in medical signals and by applying an efficient meta-learning approach with a context reduction scheme. We further address the spectral bias in commonly used SIREN activations, by introducing an $\omega_0$-schedule, improving reconstruction quality and convergence speed. We validate our proposed approach on a large variety of medical signals of different dimensions and modalities (1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, Colon Histopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfully demonstrate that we can solve relevant downstream tasks on these representations. We additionally release a large-scale dataset of &gt; 550k annotated neural fields to promote research in this direction. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„ç ”ç©¶å‡ ä¹å®Œå…¨é›†ä¸­åœ¨åŸºäºç½‘æ ¼æˆ–ä½“ç´ çš„æ•°æ®è¡¨ç¤ºä¸Šã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥MedFunctaè¿™ä¸€åŸºäºç¥ç»åœºçš„æ¨¡æ€æ— å…³è¿ç»­æ•°æ®è¡¨ç¤ºæ–¹å¼ï¼Œå¯¹è¿™ä¸€å¸¸è§é€‰æ‹©æå‡ºäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨åŒ»å­¦ä¿¡å·ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œé€šè¿‡é‡‡ç”¨å¸¦æœ‰ä¸Šä¸‹æ–‡ç¼©å‡æ–¹æ¡ˆçš„é«˜æ•ˆå…ƒå­¦ä¹ æ–¹æ³•ï¼Œå°†ç¥ç»åœºä»å•ä¸ªå®ä¾‹æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥Ï‰0è°ƒåº¦æ–¹æ³•è§£å†³äº†å¸¸ç”¨SIRENæ¿€æ´»å‡½æ•°ä¸­çš„é¢‘è°±åå·®é—®é¢˜ï¼Œæé«˜äº†é‡å»ºè´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚æˆ‘ä»¬åœ¨å¤šç§ä¸åŒç»´åº¦å’Œæ¨¡æ€çš„åŒ»å­¦ä¿¡å·ï¼ˆ1Dï¼šå¿ƒç”µå›¾ï¼›2Dï¼šèƒ¸éƒ¨Xå°„çº¿ã€è§†ç½‘è†œOCTã€çœ¼åº•ç›¸æœºã€çš®è‚¤æ˜¾å¾®é•œã€ç»“è‚ ç»„ç»‡ç—…ç†å­¦ã€ç»†èƒæ˜¾å¾®é•œï¼›3Dï¼šè„‘éƒ¨MRIã€è‚ºéƒ¨CTï¼‰ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶æˆåŠŸè¯æ˜æˆ‘ä»¬èƒ½å¤Ÿåœ¨è¿™äº›è¡¨ç¤ºä¸Šè§£å†³ç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡55ä¸‡ä¸ªæ³¨é‡Šç¥ç»åœºçš„å¤§å‹æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14401v1">PDF</a> Code and Dataset: <a target="_blank" rel="noopener" href="https://github.com/pfriedri/medfuncta">https://github.com/pfriedri/medfuncta</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„ç ”ç©¶ä¸»è¦èšç„¦äºç½‘æ ¼æˆ–ä½“ç´ æ•°æ®è¡¨ç¤ºã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥MedFunctaè¿™ä¸€æ¨¡æ€æ— å…³çš„è¿ç»­æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œå¯¹è¿™ä¸€ç°è±¡æå‡ºæŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶é€šè¿‡åˆ©ç”¨åŒ»å­¦ä¿¡å·çš„å†—ä½™ä¿¡æ¯å’Œé‡‡ç”¨ä¸€ç§å¸¦æœ‰ä¸Šä¸‹æ–‡ç¼©å‡æ–¹æ¡ˆçš„é«˜æ•ˆå…ƒå­¦ä¹ æ–¹æ³•å®ç°äº†ç¥ç»ç½‘ç»œä»å•ä¸€å®ä¾‹åˆ°å¤§æ•°æ®é›†çš„æ‰©å±•ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶é’ˆå¯¹å¸¸ç”¨SIRENæ¿€æ´»å‡½æ•°ä¸­çš„é¢‘è°±åå·®é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥Ï‰0è°ƒåº¦æ–¹æ¡ˆï¼Œæé«˜äº†é‡å»ºè´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚æœ¬ç ”ç©¶åœ¨ä¸åŒç»´åº¦å’Œæ¨¡æ€çš„åŒ»å­¦ä¿¡å·ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æˆåŠŸå±•ç¤ºäº†å…¶åœ¨ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å‘å¸ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡ç¥ç»åœºæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¯¥æ–¹å‘çš„ç ”ç©¶å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æŒ‘æˆ˜äº†æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸»æµæ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œå¼•å…¥äº†æ¨¡æ€æ— å…³çš„è¿ç»­æ•°æ®è¡¨ç¤ºæ–¹æ³•MedFunctaã€‚</li>
<li>é€šè¿‡åˆ©ç”¨åŒ»å­¦ä¿¡å·çš„å†—ä½™ä¿¡æ¯å’Œé«˜æ•ˆå…ƒå­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†ç¥ç»ç½‘ç»œåœ¨å¤§æ•°æ®é›†ä¸Šçš„åº”ç”¨ã€‚</li>
<li>é’ˆå¯¹SIRENæ¿€æ´»å‡½æ•°çš„é¢‘è°±åå·®é—®é¢˜ï¼Œæå‡ºäº†Ï‰0è°ƒåº¦æ–¹æ¡ˆï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šç§ç»´åº¦å’Œæ¨¡æ€çš„åŒ»å­¦ä¿¡å·ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æˆåŠŸå±•ç¤ºäº†æ–¹æ³•åœ¨ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>å‘å¸ƒäº†å¤§è§„æ¨¡ç¥ç»åœºæ•°æ®é›†ï¼Œä¿ƒè¿›ç ”ç©¶æ–¹å‘çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17d37ef73e80fc02799bfc0a99dcc26b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-224e9a496dbd279ec7035cec96e51ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c24efe4411892193811c180690f30b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f749ffc2792cd29ab3642220857fb703.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a8c50f74233fbcb6a7b7b09aa8b5aab.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SegAnyPET-Universal-Promptable-Segmentation-from-Positron-Emission-Tomography-Images"><a href="#SegAnyPET-Universal-Promptable-Segmentation-from-Positron-Emission-Tomography-Images" class="headerlink" title="SegAnyPET: Universal Promptable Segmentation from Positron Emission   Tomography Images"></a>SegAnyPET: Universal Promptable Segmentation from Positron Emission   Tomography Images</h2><p><strong>Authors:Yichi Zhang, Le Xue, Wenbo Zhang, Lanlan Li, Yuchen Liu, Chen Jiang, Yuan Cheng, Yuan Qi</strong></p>
<p>Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patientâ€™s body, which is essential for quantification of therapy response and monitoring treatment progress. However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities. Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data. Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒåœ¨ç°ä»£åŒ»å­¦è¯Šæ–­ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå®ƒèƒ½å¤Ÿæ­ç¤ºæ‚£è€…ä½“å†…çš„ä»£è°¢è¿‡ç¨‹ï¼Œè¿™å¯¹äºé‡åŒ–æ²»ç–—ååº”å’Œç›‘æµ‹æ²»ç–—è¿›å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºPETå›¾åƒçš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¾¹ç•Œä¸å¤Ÿæ¸…æ™°ï¼Œä¸å…¶ä»–ç»“æ„åŒ»å­¦å›¾åƒç›¸æ¯”ï¼Œå…¶åˆ†å‰²é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ€è¿‘çš„åˆ†å‰²åŸºç¡€æ¨¡å‹çš„å‘å±•è¡¨æ˜ï¼Œå®ƒä»¬åœ¨å„ç§è‡ªç„¶å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„å¤šåŠŸèƒ½æ€§ã€‚å°½ç®¡è¿›è¡Œäº†åŒ»å­¦é€‚åº”æ€§çš„åŠªåŠ›ï¼Œä½†è¿™äº›å·¥ä½œä¸»è¦é›†ä¸­åœ¨å…·æœ‰è¯¦ç»†ç”Ÿç†ç»“æ„ä¿¡æ¯çš„ç»“æ„åŒ»å­¦å›¾åƒä¸Šï¼Œè€Œåœ¨é€‚åº”åˆ†å­PETæˆåƒæ—¶è¡¨ç°å‡ºè¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†å’Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„PETåˆ†å‰²æ•°æ®é›†PETS-5kï¼ŒåŒ…å«5731ä¸ªä¸‰ç»´å…¨èº«PETå›¾åƒå’Œè¶…è¿‡130ä¸‡ä¸ªäºŒç»´å›¾åƒã€‚åŸºäºå»ºç«‹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç‰¹å®šæ¨¡æ€çš„ä¸‰ç»´åŸºç¡€æ¨¡å‹SegAnyPETï¼Œç”¨äºä»PETå›¾åƒè¿›è¡Œé€šç”¨å³æ—¶åˆ†å‰²ã€‚ä¸ºäº†è§£å†³PETå›¾åƒæ ‡æ³¨è´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å¸¦æœ‰ä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹çš„äº¤å‰æç¤ºç½®ä¿¡å­¦ä¹ ï¼ˆCPCLï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å¥åœ°ä»é«˜è´¨é‡æ ‡æ³¨æ•°æ®å’Œä½è´¨é‡å™ªå£°æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSegAnyPETä»…ä½¿ç”¨ä¸€ä¸ªæˆ–å°‘æ•°æç¤ºç‚¹å°±èƒ½æ­£ç¡®åˆ†å‰²å·²çŸ¥å’ŒæœªçŸ¥ç›®æ ‡ï¼Œä¼˜äºæœ€æ–°çš„åŸºç¡€æ¨¡å‹å’Œä»»åŠ¡ç‰¹å®šå…¨ç›‘ç£æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›è¿›è¡Œé€šç”¨åˆ†å‰²ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªç”¨äºPETå›¾åƒçš„åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬ç›¸ä¿¡SegAnyPETå°†æ¨åŠ¨å…¶åœ¨åˆ†å­æˆåƒçš„å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒåœ¨ç°ä»£åŒ»å­¦è¯Šæ–­ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†PETå›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„PETåˆ†å‰²æ•°æ®é›†PETS-5kï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘äº†ä¸€ç§é€‚ç”¨äºPETå›¾åƒçš„é€šç”¨å³æ—¶åˆ†å‰²æ¨¡å‹SegAnyPETã€‚è¯¥æ¨¡å‹é‡‡ç”¨è·¨æç¤ºç½®ä¿¡å­¦ä¹ ï¼ˆCPCLï¼‰ç­–ç•¥ï¼Œå¯ç¨³å¥åœ°ä»é«˜è´¨é‡æ ‡æ³¨æ•°æ®å’Œä½è´¨é‡å™ªå£°æ ‡æ³¨æ•°æ®ä¸­å­¦ä¹ åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSegAnyPETèƒ½å¤Ÿæ­£ç¡®åˆ†å‰²å·²è§å’Œæœªè§ç›®æ ‡ï¼Œä¼˜äºæœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹å’Œç‰¹å®šä»»åŠ¡çš„å®Œå…¨ç›‘ç£æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¼ºå¤§çš„é€šç”¨åˆ†å‰²èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETæˆåƒåœ¨ç°ä»£åŒ»å­¦è¯Šæ–­ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½æ­ç¤ºæ‚£è€…ä½“å†…çš„ä»£è°¢è¿‡ç¨‹ï¼Œå¯¹æ²»ç–—ååº”çš„é‡åŒ–å’Œæ²»ç–—è¿›å±•çš„ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>PETå›¾åƒåˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼Œå› å…¶å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¾¹ç•Œè¾ƒæ¨¡ç³Šã€‚</li>
<li>æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„PETåˆ†å‰²æ•°æ®é›†PETS-5kï¼ŒåŒ…å«5731ä¸ªä¸‰ç»´å…¨èº«PETå›¾åƒå’Œè¶…è¿‡130ä¸‡ä¸ªäºŒç»´å›¾åƒã€‚</li>
<li>å¼€å‘äº†é€‚ç”¨äºPETå›¾åƒçš„é€šç”¨å³æ—¶åˆ†å‰²æ¨¡å‹SegAnyPETã€‚</li>
<li>SegAnyPETé‡‡ç”¨è·¨æç¤ºç½®ä¿¡å­¦ä¹ ï¼ˆCPCLï¼‰ç­–ç•¥ï¼Œå¯ä»ä¸åŒè´¨é‡æ ‡æ³¨æ•°æ®ä¸­ç¨³å¥å­¦ä¹ åˆ†å‰²ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜SegAnyPETæ€§èƒ½ä¼˜è¶Šï¼Œèƒ½å¤Ÿæ­£ç¡®åˆ†å‰²å·²è§å’Œæœªè§ç›®æ ‡ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é€šç”¨åˆ†å‰²èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-288550d062d4aba970482dcf75b83a74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05270e8e4cddc0876e187d4eb2511c30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae2d74ff268df4180ff80ef656c65d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca952e0bf8821a340720d50a7355840a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging"><a href="#H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging" class="headerlink" title="H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging"></a>H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging</h2><p><strong>Authors:Zhen Huang, Ronghao Xu, Xiaoqian Zhou, Yangbo Wei, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao</strong></p>
<p>3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.   In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection \textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights. </p>
<blockquote>
<p>ä¸‰ç»´ï¼ˆ3Dï¼‰åœ°æ ‡æ£€æµ‹æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œè€Œå‡†ç¡®æ£€æµ‹è§£å‰–åœ°æ ‡å¯¹äºåç»­åŒ»å­¦æˆåƒä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸçš„ä¸»æµæ·±åº¦å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨æ•æ‰ç²¾ç»†å±€éƒ¨ç‰¹å¾ã€å¯¹å…¨å±€ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡çš„åŒæ—¶ï¼Œä¿æŒå‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚å±€éƒ¨ç‰¹å¾æå–éœ€è¦æ•æ‰ç²¾ç»†çš„è§£å‰–ç»†èŠ‚ï¼Œè€Œå…¨å±€å»ºæ¨¡åˆ™éœ€è¦ç†è§£å¤æ‚è§£å‰–ç»“æ„å†…çš„ç©ºé—´å…³ç³»ã€‚ç”±äºåœ°æ ‡åˆ†å¸ƒç¨€ç–ï¼Œä¸‰ç»´ä½“ç§¯çš„é«˜ç»´æ€§è´¨è¿›ä¸€æ­¥åŠ å‰§äº†è¿™äº›æŒ‘æˆ˜ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚å› æ­¤ï¼Œå®ç°é«˜æ•ˆä¸”ç²¾ç¡®çš„3Dåœ°æ ‡æ£€æµ‹ä»ç„¶æ˜¯åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„ä¸€ä¸ªç´§è¿«æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¸‰ç»´æ£€æµ‹ç½‘ç»œï¼ˆHybrid 3D Detection Netï¼Œç®€ç§°H3DE-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç”¨äºå±€éƒ¨ç‰¹å¾æå–å’Œä¸€ä¸ªè½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨é«˜æ•ˆæ•æ‰ä¸‰ç»´ä½“ç§¯æ•°æ®ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚è¯¥æœºåˆ¶é‡‡ç”¨åˆ†å±‚è·¯ç”±ç­–ç•¥æ¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒH3DE-Netæ˜¯é¦–ä¸ªå°†è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ä¸CNNç»“åˆç”¨äºä¸‰ç»´åœ°æ ‡æ£€æµ‹çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦ç‰¹å¾èåˆæŠ€æœ¯è¿›ä¸€æ­¥æé«˜äº†æ£€æµ‹ç²¾åº¦å’Œé²æ£’æ€§ã€‚åœ¨å…¬å…±CTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒH3DE-Netè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨åœ°æ ‡ç¼ºå¤±æˆ–è§£å‰–ç»“æ„å¤æ‚çš„åœºæ™¯ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬å·²ç»å¼€æºäº†æˆ‘ä»¬çš„é¡¹ç›®ï¼ŒåŒ…æ‹¬ä»£ç ã€æ•°æ®å’Œæ¨¡å‹æƒé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ··åˆä¸‰ç»´æ£€æµ‹ç½‘ç»œï¼ˆH3DE-Netï¼‰ï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶ä»¥é«˜æ•ˆæ•æ‰ä¸‰ç»´ä½“ç§¯æ•°æ®ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚è·¯ç”±ç­–ç•¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚åœ¨å…¬å¼€CTæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒH3DE-Netè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºå¤±åœ°æ ‡æˆ–å¤æ‚è§£å‰–å˜å¼‚æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D landmark detectionåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­æ˜¯å…³é”®ä»»åŠ¡ã€‚</li>
<li>ä¸»æµæ·±åº¦å­¦ä¹ æ–¹æ³•éš¾ä»¥åœ¨æ•æ‰å±€éƒ¨ç²¾ç»†ç‰¹å¾å’Œå»ºæ¨¡å…¨å±€ç©ºé—´å…³ç³»ä¹‹é—´ä¿æŒå¹³è¡¡ã€‚</li>
<li>H3DE-Netç»“åˆCNNè¿›è¡Œå±€éƒ¨ç‰¹å¾æå–ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å…¨å±€ä¾èµ–ã€‚</li>
<li>åˆ†å±‚è·¯ç”±ç­–ç•¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</li>
<li>H3DE-Netæ˜¯é¦–ä¸ªç»“åˆè½»é‡çº§æ³¨æ„åŠ›æœºåˆ¶å’ŒCNNçš„3Dåœ°æ ‡æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>å¤šå°ºåº¦ç‰¹å¾èåˆè¿›ä¸€æ­¥æé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78603d55fa0b8a64302b114e590066fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb185c11e0b8d790e5c9577256dd6498.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation"><a href="#Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation" class="headerlink" title="Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation"></a>Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation</h2><p><strong>Authors:Behzad Hejrati, Soumyanil Banerjee, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Diffusion models have been used extensively for high quality image and video generation tasks. In this paper, we propose a novel conditional diffusion model with spatial attention and latent embedding (cDAL) for medical image segmentation. In cDAL, a convolutional neural network (CNN) based discriminator is used at every time-step of the diffusion process to distinguish between the generated labels and the real ones. A spatial attention map is computed based on the features learned by the discriminator to help cDAL generate more accurate segmentation of discriminative regions in an input image. Additionally, we incorporated a random latent embedding into each layer of our model to significantly reduce the number of training and sampling time-steps, thereby making it much faster than other diffusion models for image segmentation. We applied cDAL on 3 publicly available medical image segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed significant qualitative and quantitative improvements with higher Dice scores and mIoU over the state-of-the-art algorithms. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/">https://github.com/Hejrati/cDAL/</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨äºé«˜è´¨é‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰ç©ºé—´æ³¨æ„åŠ›å’Œæ½œåœ¨åµŒå…¥ï¼ˆcDALï¼‰çš„æ–°å‹æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚åœ¨cDALä¸­ï¼Œæ‰©æ•£è¿‡ç¨‹çš„æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä½¿ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„åˆ¤åˆ«å™¨æ¥åŒºåˆ†ç”Ÿæˆçš„æ ‡ç­¾å’ŒçœŸå®çš„æ ‡ç­¾ã€‚åŸºäºåˆ¤åˆ«å™¨å­¦ä¹ çš„ç‰¹å¾è®¡ç®—ç©ºé—´æ³¨æ„åŠ›å›¾ï¼Œä»¥å¸®åŠ©cDALç”Ÿæˆè¾“å…¥å›¾åƒä¸­åˆ¤åˆ«åŒºåŸŸçš„æ›´å‡†ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†éšæœºæ½œåœ¨åµŒå…¥èå…¥æ¨¡å‹ä¸­çš„æ¯ä¸€å±‚ï¼Œä»¥å¤§å¤§å‡å°‘è®­ç»ƒå’Œé‡‡æ ·æ—¶é—´æ­¥çš„æ•°é‡ï¼Œä»è€Œä½¿å®ƒæ¯”å…¶ä»–å›¾åƒåˆ†å‰²æ‰©æ•£æ¨¡å‹æ›´å¿«ã€‚æˆ‘ä»¬åœ¨3ä¸ªå…¬å¼€å¯ç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ˆMoNuSegã€Chest X-rayå’ŒHippocampusï¼‰ä¸Šåº”ç”¨äº†cDALï¼Œå¹¶åœ¨æœ€å…ˆè¿›çš„ç®—æ³•ä¸Šè§‚å¯Ÿåˆ°æ˜¾è‘—çš„å®šæ€§å’Œå®šé‡æ”¹è¿›ï¼Œå…·æœ‰æ›´é«˜çš„Diceåˆ†æ•°å’ŒmIoUã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Hejrati/cDAL/å¤„è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06997v2">PDF</a> 13 pages, 5 figures, 3 tables, Accepted in MICCAI 2024</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œæå‡ºä¸€ç§æ–°å‹çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹cDALï¼Œç»“åˆç©ºé—´æ³¨æ„åŠ›å’Œæ½œåœ¨åµŒå…¥ã€‚ä½¿ç”¨CNNåˆ¤åˆ«å™¨åŒºåˆ†ç”Ÿæˆæ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾ï¼Œè®¡ç®—ç©ºé—´æ³¨æ„å›¾ä»¥æé«˜è¾“å…¥å›¾åƒä¸­åˆ¤åˆ«åŒºåŸŸçš„åˆ†å‰²å‡†ç¡®æ€§ã€‚èå…¥éšæœºæ½œåœ¨åµŒå…¥ï¼Œå‡å°‘è®­ç»ƒå’Œé‡‡æ ·æ—¶é—´æ­¥æ•°ï¼Œè¾ƒå…¶ä»–å›¾åƒåˆ†å‰²æ‰©æ•£æ¨¡å‹æ›´å¿«ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šåº”ç”¨ï¼Œè·å¾—è¾ƒé«˜çš„Diceåˆ†æ•°å’ŒmIoUï¼Œä¼˜äºç°æœ‰ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºæ–°å‹æ¡ä»¶æ‰©æ•£æ¨¡å‹cDALç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>ç»“åˆç©ºé—´æ³¨æ„åŠ›å’Œæ½œåœ¨åµŒå…¥æŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨CNNåˆ¤åˆ«å™¨ä»¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>ç©ºé—´æ³¨æ„å›¾å¯å¸®åŠ©cDALç”Ÿæˆæ›´å‡†ç¡®çš„ç»“æœã€‚</li>
<li>èå…¥éšæœºæ½œåœ¨åµŒå…¥ä»¥åŠ é€Ÿè®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å¼€åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69d3b4fed0d242f0ec8d4a7984619fda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a77ce743221a1e323f776121fce2cc5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Finite-Element-Analysis-Model-for-Magnetomotive-Ultrasound-Elastometry-Magnet-Design-with-Experimental-Validation"><a href="#A-Finite-Element-Analysis-Model-for-Magnetomotive-Ultrasound-Elastometry-Magnet-Design-with-Experimental-Validation" class="headerlink" title="A Finite Element Analysis Model for Magnetomotive Ultrasound Elastometry   Magnet Design with Experimental Validation"></a>A Finite Element Analysis Model for Magnetomotive Ultrasound Elastometry   Magnet Design with Experimental Validation</h2><p><strong>Authors:Jacquelline Nyakunu, Christopher T. Piatnichouk, Henry C. Russell, Niels J. van Duijnhoven, Benjamin E. Levy</strong></p>
<p>Magnetomotive ultrasound (MMUS) using magnetic nanoparticle contrast agents has shown promise for thrombosis imaging and quantitative elastometry via magnetomotive resonant acoustic spectroscopy (MRAS). Youngâ€™s modulus measurements of smaller, stiffer thrombi require an MRAS system capable of generating forces at higher temporal frequencies. Solenoids with fewer turns, and thus less inductance, could improve high frequency performance, but the reduced force may compromise results. In this work, a computational model capable of assessing the effectiveness of MRAS elastometry magnet configurations is presented and validated. Finite element analysis (FEA) was used to model the force and inductance of MRAS systems. The simulations incorporated both solenoid electromagnets and permanent magnets in three-dimensional steady-state, frequency domain, and time domain studies. The model successfully predicted that a configuration in which permanent magnets were added to an existing MRAS system could be used to increase the force supplied. Accordingly, the displacement measured in a magnetically labeled validation phantom increased by a factor of $2.2 \pm 0.3$ when the force was predicted to increase by a factor of $2.2 \pm 0.2$. The model additionally identified a new solenoid configuration consisting of four smaller coils capable of providing sufficient force at higher driving frequencies. These results indicate two methods by which MRAS systems could be designed to deliver higher frequency magnetic forces without the need for experimental trial and error. Either the number of turns within each solenoid could be reduced while permanent magnets are added at precise locations, or a larger number of smaller solenoids could be used. These findings overcome a key challenge toward the goal of MMUS thrombosis elastometry, and simulation files are provided online for broader experimentation. </p>
<blockquote>
<p>ç£åŠ¨è¶…å£°ï¼ˆMMUSï¼‰åˆ©ç”¨ç£æ€§çº³ç±³ç²’å­é€ å½±å‰‚åœ¨è¡€æ “æˆåƒå’Œå®šé‡å¼¹æ€§æµ‹é‡æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œè¿™é€šè¿‡ç£åŠ¨å…±æŒ¯å£°å­¦å…‰è°±æ³•ï¼ˆMRASï¼‰å®ç°ã€‚å¯¹è¾ƒå°ã€è¾ƒç¡¬çš„è¡€æ “çš„æ¨æ°æ¨¡é‡æµ‹é‡éœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨è¾ƒé«˜æ—¶é—´é¢‘ç‡ä¸‹äº§ç”ŸåŠ›çš„MRASç³»ç»Ÿã€‚å…·æœ‰è¾ƒå°‘åŒæ•°ã€å› æ­¤ç”µæ„Ÿè¾ƒå°çš„èºçº¿ç®¡å¯ä»¥æé«˜é«˜é¢‘æ€§èƒ½ï¼Œä½†å‡å°çš„ä½œç”¨åŠ›å¯èƒ½ä¼šæŸå®³ç»“æœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ç§èƒ½å¤Ÿè¯„ä¼°MRASå¼¹æ€§æµ‹é‡ç£æ„å‹çš„æœ‰æ•ˆæ€§çš„è®¡ç®—æ¨¡å‹ï¼Œå¹¶è¿›è¡ŒéªŒè¯ã€‚æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰ç”¨äºæ¨¡æ‹ŸMRASç³»ç»Ÿçš„åŠ›å’Œç”µæ„Ÿã€‚æ¨¡æ‹Ÿç»“åˆäº†èºçº¿ç®¡ç”µç£é“å’Œæ°¸ä¹…ç£é“çš„ä¸‰ç»´ç¨³æ€ã€é¢‘åŸŸå’Œæ—¶åŸŸç ”ç©¶ã€‚è¯¥æ¨¡å‹æˆåŠŸé¢„æµ‹ï¼Œå‘ç°æœ‰MRASç³»ç»Ÿæ·»åŠ æ°¸ä¹…ç£é“çš„é…ç½®å¯ç”¨äºå¢åŠ æ‰€æä¾›çš„åŠ›ã€‚å› æ­¤ï¼Œåœ¨é¢„æµ‹åŠ›å¢åŠ äº†ä¸€ä¸ªå› å­$2.2Â±0.2$çš„æƒ…å†µä¸‹ï¼Œç£æ€§æ ‡è®°éªŒè¯æ¨¡å‹ä¸­çš„ä½ç§»å¢åŠ äº†$2.2Â±0.3$å€ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ç¡®å®šäº†ä¸€ç§æ–°çš„èºçº¿ç®¡é…ç½®ï¼Œç”±å››ä¸ªè¾ƒå°çš„çº¿åœˆç»„æˆï¼Œèƒ½å¤Ÿåœ¨è¾ƒé«˜çš„é©±åŠ¨é¢‘ç‡ä¸‹æä¾›è¶³å¤Ÿçš„åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡ä¸¤ç§æ–¹æ³•è®¾è®¡MRASç³»ç»Ÿä»¥äº§ç”Ÿè¾ƒé«˜é¢‘ç‡çš„ç£åŠ›ï¼Œæ— éœ€é€šè¿‡å®éªŒåå¤è¯•é”™ã€‚ä¸€ç§æ–¹æ³•æ˜¯å‡å°‘æ¯ä¸ªèºçº¿ç®¡ä¸­çš„åŒæ•°ï¼ŒåŒæ—¶åœ¨ç²¾ç¡®ä½ç½®æ·»åŠ æ°¸ä¹…ç£é“ï¼›å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨æ›´å¤šè¾ƒå°çš„èºçº¿ç®¡ã€‚è¿™äº›å‘ç°ä¸ºå®ç°MMUSè¡€æ “å¼¹æ€§æµ‹é‡çš„ç›®æ ‡å…‹æœäº†å…³é”®æŒ‘æˆ˜ï¼Œå¹¶ä¸”ä»¿çœŸæ–‡ä»¶å·²åœ¨çº¿æä¾›ä¾›è¿›ä¸€æ­¥å®éªŒä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07737v2">PDF</a> 12 pages, 8 figures. This manuscript has been revised from version v1   via the peer review process. It has been accepted in its current form and is   awaiting publication at Biomedical Physics and Engineering Express</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç£åŠ¨æœºè¶…å£°ï¼ˆMMUSï¼‰åˆ©ç”¨ç£æ€§çº³ç±³ç²’å­é€ å½±å‰‚åœ¨è¡€æ “æˆåƒå’Œå®šé‡å¼¹æ€§æµ‹é‡æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºæé«˜å¯¹è¾ƒå°ã€è¾ƒç¡¬è¡€æ “çš„æ¨æ°æ¨¡é‡æµ‹é‡èƒ½åŠ›ï¼Œéœ€è¦èƒ½åœ¨æ›´é«˜æ—¶é—´é¢‘ç‡ä¸‹äº§ç”ŸåŠ›çš„MRASç³»ç»Ÿã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§èƒ½å¤Ÿè¯„ä¼°MRASå¼¹æ€§æµ‹é‡ç£é…ç½®çš„æœ‰æ•ˆæ€§çš„è®¡ç®—æ¨¡å‹ï¼Œå¹¶é€šè¿‡æœ‰é™å…ƒåˆ†æï¼ˆFEAï¼‰è¿›è¡Œæ¨¡æ‹ŸéªŒè¯ã€‚æ¨¡å‹æˆåŠŸé¢„æµ‹äº†æ·»åŠ æ°¸ä¹…ç£é“åˆ°ç°æœ‰MRASç³»ç»Ÿä¸­å¯å¢åŠ æä¾›çš„åŠ›çš„é…ç½®ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜è¯†åˆ«äº†ä¸€ç§æ–°çš„ç”±å››ä¸ªå°å‹çº¿åœˆç»„æˆçš„å¤©çº¿é…ç½®ï¼Œèƒ½å¤Ÿåœ¨è¾ƒé«˜çš„é©±åŠ¨é¢‘ç‡ä¸‹æä¾›è¶³å¤Ÿçš„åŠ›é‡ã€‚è¿™æ ‡å¿—ç€è§£å†³MRASç³»ç»Ÿåœ¨è®¾è®¡ä¸Šæœç€æ›´é«˜é¢‘ç‡ç£åœºåŠ›å‘å±•çš„ä¸¤å¤§çªç ´ï¼Œæ— éœ€å®éªŒæ€§çš„åå¤è¯•é”™ã€‚è¿™äº›å‘ç°å…‹æœäº†å®ç°MMUSè¡€æ “å¼¹æ€§æµ‹é‡çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸”æ¨¡æ‹Ÿæ–‡ä»¶å·²åœ¨çº¿æä¾›ä»¥ä¾›æ›´å¹¿æ³›çš„å®éªŒä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMUSä½¿ç”¨ç£æ€§çº³ç±³ç²’å­é€ å½±å‰‚åœ¨è¡€æ “æˆåƒå’Œå®šé‡å¼¹æ€§æµ‹é‡æ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>é«˜é¢‘æ€§èƒ½æå‡éœ€è¦èƒ½åœ¨æ›´é«˜æ—¶é—´é¢‘ç‡ä¸‹äº§ç”ŸåŠ›çš„MRASç³»ç»Ÿã€‚</li>
<li>è®¡ç®—æ¨¡å‹é€šè¿‡æœ‰é™å…ƒåˆ†æè¯„ä¼°ä¸åŒç£é…ç½®çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ·»åŠ æ°¸ä¹…ç£é“åˆ°MRASç³»ç»Ÿä¸­å¯å¢åŠ æä¾›çš„åŠ›ã€‚</li>
<li>æ–°è¯†åˆ«çš„ä¸€ç§ç”±å››ä¸ªå°å‹çº¿åœˆç»„æˆçš„é…ç½®èƒ½åœ¨é«˜é©±åŠ¨é¢‘ç‡ä¸‹æä¾›è¶³å¤ŸåŠ›é‡ã€‚</li>
<li>è¿™äº›å‘ç°è§£å†³äº†è®¾è®¡MRASç³»ç»Ÿä»¥å®ç°æ›´é«˜é¢‘ç‡ç£åœºåŠ›çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb87733106393cf49e8b41fb28bdbc4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d76a711e0def95004a6abf5bac039b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6176e674851207efae218da24a8d48d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a718043e0e50662fbd0b990ae50e1f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33a8f3a22677bfb5a8c6b08bea420ef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Robust-Tumor-Segmentation-with-Hyperspectral-Imaging-and-Graph-Neural-Networks"><a href="#Robust-Tumor-Segmentation-with-Hyperspectral-Imaging-and-Graph-Neural-Networks" class="headerlink" title="Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural   Networks"></a>Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural   Networks</h2><p><strong>Authors:Mayar Lotfy Mostafa, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler</strong></p>
<p>Segmenting the boundary between tumor and healthy tissue during surgical cancer resection poses a significant challenge. In recent years, Hyperspectral Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising solution. However, due to the extensive information contained within the spectral domain, most ML approaches primarily classify individual HSI (super-)pixels, or tiles, without taking into account their spatial context. In this paper, we propose an improved methodology that leverages the spatial context of tiles for more robust and smoother segmentation. To address the irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate context information across neighboring regions. The features for each tile within the graph are extracted using a Convolutional Neural Network (CNN), which is trained simultaneously with the subsequent GNN. Moreover, we incorporate local image quality metrics into the loss function to enhance the training procedureâ€™s robustness against low-quality regions in the training images. We demonstrate the superiority of our proposed method using a clinical ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the limited dataset, the GNN-based model significantly outperforms context-agnostic approaches, accurately distinguishing between healthy and tumor tissues, even in images from previously unseen patients. Furthermore, we show that our carefully designed loss function, accounting for local image quality, results in additional improvements. Our findings demonstrate that context-aware GNN algorithms can robustly find tumor demarcations on HSI images, ultimately contributing to better surgery success and patient outcome. </p>
<blockquote>
<p>åœ¨æ‰‹æœ¯åˆ‡é™¤ç™Œç—‡çš„è¿‡ç¨‹ä¸­ï¼ŒåŒºåˆ†è‚¿ç˜¤å’Œå¥åº·ç»„ç»‡çš„è¾¹ç•Œæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥ï¼Œé«˜å…‰è°±æˆåƒï¼ˆHSIï¼‰ç»“åˆæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·²æˆä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå…‰è°±åŸŸå†…åŒ…å«çš„å¤§é‡ä¿¡æ¯ï¼Œå¤§å¤šæ•°æœºå™¨å­¦ä¹ çš„æ–¹æ³•ä¸»è¦å¯¹å•ä¸ªHSIï¼ˆè¶…çº§ï¼‰åƒç´ æˆ–ç“¦ç‰‡è¿›è¡Œåˆ†ç±»ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°å®ƒä»¬çš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç“¦ç‰‡çš„ç©ºé—´ä¸Šä¸‹æ–‡æ¥å®ç°æ›´ç¨³å¥ã€æ›´å¹³æ»‘çš„åˆ†å‰²ã€‚ä¸ºäº†è§£å†³ç“¦ç‰‡å½¢çŠ¶ä¸è§„åˆ™çš„é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨é‚»è¿‘åŒºåŸŸä¹‹é—´ä¼ æ’­ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å›¾ä¸­æ¯ä¸ªç“¦ç‰‡çš„ç‰¹å¾æ˜¯ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–çš„ï¼Œè¯¥ç½‘ç»œæ˜¯ä¸éšåçš„GNNåŒæ—¶è®­ç»ƒçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å±€éƒ¨å›¾åƒè´¨é‡æŒ‡æ ‡çº³å…¥æŸå¤±å‡½æ•°ä¸­ï¼Œä»¥æé«˜è®­ç»ƒè¿‡ç¨‹å¯¹è®­ç»ƒå›¾åƒä¸­ä½è´¨é‡åŒºåŸŸçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨ç”±30åæ‚£è€…çš„51å¼ HSIå›¾åƒç»„æˆçš„ä¸´åºŠç¦»ä½“æ•°æ®é›†æ¥è¯æ˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å°½ç®¡æ•°æ®é›†æœ‰é™ï¼Œä½†åŸºäºGNNçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºä¸Šä¸‹æ–‡æ— å…³çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå‡†ç¡®åŒºåˆ†å¥åº·ç»„ç»‡å’Œè‚¿ç˜¤ç»„ç»‡ï¼Œå³ä½¿åœ¨ä»¥å‰æœªè§è¿‡çš„æ‚£è€…çš„å›¾åƒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„è€ƒè™‘å±€éƒ¨å›¾åƒè´¨é‡çš„æŸå¤±å‡½æ•°ä¼šå¯¼è‡´é¢å¤–çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„GNNç®—æ³•å¯ä»¥ç¨³å¥åœ°åœ¨HSIå›¾åƒä¸Šæ‰¾åˆ°è‚¿ç˜¤è¾¹ç•Œï¼Œæœ€ç»ˆæœ‰åŠ©äºæé«˜æ‰‹æœ¯æˆåŠŸç‡å’Œæ‚£è€…æ²»ç–—æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11782v2">PDF</a> 18 pages, 5 figures, The German Conference on Pattern Recognition   (GCPR) 2024</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡è€ƒè™‘è¶…å…‰è°±æˆåƒï¼ˆHSIï¼‰ä¸­åƒç´ çš„ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®ç°å¯¹è‚¿ç˜¤ä¸å¥åº·ç»„ç»‡çš„æ›´ç¨³å¥ã€å¹³æ»‘çš„åˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨æœ‰é™çš„ä¸´åºŠç¦»ä½“æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å‡†ç¡®åŒºåˆ†å¥åº·ç»„ç»‡å’Œè‚¿ç˜¤ç»„ç»‡ï¼Œç”šè‡³åœ¨å¯¹æœªè§æ‚£è€…çš„æ–°å›¾åƒä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°å±€éƒ¨å›¾åƒè´¨é‡çš„æŸå¤±å‡½æ•°è®¾è®¡è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å…‰è°±æˆåƒï¼ˆHSIï¼‰ç»“åˆæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨è‚¿ç˜¤åˆ‡é™¤æ‰‹æœ¯ä¸­åŒºåˆ†è‚¿ç˜¤å’Œå¥åº·ç»„ç»‡æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å¤§å¤šæ•°MLæ–¹æ³•ä¸»è¦å¯¹HSIåƒç´ æˆ–ç“¦ç‰‡è¿›è¡Œåˆ†ç±»ï¼Œå¿½ç•¥äº†å…¶ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„æ–¹æ³•ï¼Œè€ƒè™‘ç“¦ç‰‡çš„ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå®ç°æ›´ç¨³å¥å’Œå¹³æ»‘çš„åˆ†å‰²ã€‚</li>
<li>é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–ç“¦ç‰‡ç‰¹å¾ï¼Œå¹¶ä¸éšåçš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚</li>
<li>çº³å…¥å±€éƒ¨å›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œå¢å¼ºè®­ç»ƒç¨‹åºå¯¹ä½è´¨é‡åŒºåŸŸçš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨åŒ…å«51å¼ HSIå›¾åƒçš„ä¸´åºŠç¦»ä½“æ•°æ®é›†ä¸ŠéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œè¯¥æ•°æ®é›†æ¥è‡ª30åæ‚£è€…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.11782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f41364e93bdd741561e5328ce3ac222c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a197bfa99d623443cf1c49c6826648.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f637ebaece3e1eebfbad74cf00d2000.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0426fb81106a34846c470965b5963a29.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  Speech to Speech Translation with Translatotron A State of the Art   Review
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a5f906428dc7327be918f0b7e1b6dc75.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  A Survey on Text-Driven 360-Degree Panorama Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
