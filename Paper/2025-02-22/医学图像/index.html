<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-22  MedVAE Efficient Automated Interpretation of Medical Images with   Large-Scale Generalizable Autoencoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-66847e975d9f58abc0b6078652e5f7d0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-22-更新"><a href="#2025-02-22-更新" class="headerlink" title="2025-02-22 更新"></a>2025-02-22 更新</h1><h2 id="MedVAE-Efficient-Automated-Interpretation-of-Medical-Images-with-Large-Scale-Generalizable-Autoencoders"><a href="#MedVAE-Efficient-Automated-Interpretation-of-Medical-Images-with-Large-Scale-Generalizable-Autoencoders" class="headerlink" title="MedVAE: Efficient Automated Interpretation of Medical Images with   Large-Scale Generalizable Autoencoders"></a>MedVAE: Efficient Automated Interpretation of Medical Images with   Large-Scale Generalizable Autoencoders</h2><p><strong>Authors:Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari</strong></p>
<p>Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/StanfordMIMI/MedVAE">https://github.com/StanfordMIMI/MedVAE</a>. </p>
<blockquote>
<p>医学图像以高分辨率和大视野获取，以捕捉临床决策所需的小特征。因此，在医学图像上训练深度学习模型可能会产生巨大的计算成本。在这项工作中，我们解决了缩小医学图像尺寸的挑战，以提高下游计算效率，同时保留与临床相关的特征。我们介绍了MedVAE，这是一系列大型二维和三维自编码器家族，能够将医学图像编码为缩小的潜在表示形式，并将潜在表示形式解码回高分辨率图像。我们使用一种新型的两阶段训练方法和1052730张医学图像来训练MedVAE自编码器。从20个医学图像数据集中获得的各种任务表明：（1）在训练下游模型时使用MedVAE潜在表示形式代替高分辨率图像，可以在保留与临床相关特征的同时带来效率效益（吞吐量最多提高70倍）；（2）MedVAE可以将潜在表示形式解码回高分辨率图像，保真度很高。我们的研究表明，大规模、可推广的自编码器有助于解决医学领域的关键效率挑战。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/StanfordMIMI/MedVAE%E3%80%82">https://github.com/StanfordMIMI/MedVAE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>医疗图像的高分辨率和大视野带来了庞大的计算成本，对深度学习模型的训练构成挑战。本研究提出MedVAE，一种能够编码医疗图像为缩小潜在表征并解码回高分辨率图像的二维和三维自编码器家族。通过采用新型两阶段训练方法和大量医疗图像数据，研究证明MedVAE在提高下游模型计算效率的同时，能保留临床相关特征，并实现高达70倍的性能提升。此外，MedVAE还能高质量地解码潜在表征至高分辨率图像。研究展示了大规模自编码器在医疗领域解决效率问题的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像的高分辨率和大视野带来计算成本挑战。</li>
<li>MedVAE是一种用于医疗图像处理的自编码器家族，能编码图像为缩小潜在表征。</li>
<li>MedVAE采用新型两阶段训练方法和大量医疗图像数据进行训练。</li>
<li>MedVAE能提高下游模型的计算效率，同时保留临床相关特征。</li>
<li>MedVAE能实现高达70倍的性能提升。</li>
<li>MedVAE能将潜在表征解码回高分辨率图像，且质量高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2605a0e186129a817db2ba17bf46d2f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26eb210ab033769bcacfbfd825617c48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a488ecffc67cb06621b354f9babdc78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e48a163156bed070723f4bad107d233a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5031b4bbfb37e8e60748ae9d32207a2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TRUSWorthy-Toward-Clinically-Applicable-Deep-Learning-for-Confident-Detection-of-Prostate-Cancer-in-Micro-Ultrasound"><a href="#TRUSWorthy-Toward-Clinically-Applicable-Deep-Learning-for-Confident-Detection-of-Prostate-Cancer-in-Micro-Ultrasound" class="headerlink" title="TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident   Detection of Prostate Cancer in Micro-Ultrasound"></a>TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident   Detection of Prostate Cancer in Micro-Ultrasound</h2><p><strong>Authors:Mohamed Harmanani, Paul F. R. Wilson, Minh Nguyen Nhat To, Mahdi Gilany, Amoon Jamzad, Fahimeh Fooladgar, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi</strong></p>
<p>While deep learning methods have shown great promise in improving the effectiveness of prostate cancer (PCa) diagnosis by detecting suspicious lesions from trans-rectal ultrasound (TRUS), they must overcome multiple simultaneous challenges. There is high heterogeneity in tissue appearance, significant class imbalance in favor of benign examples, and scarcity in the number and quality of ground truth annotations available to train models. Failure to address even a single one of these problems can result in unacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed, tuned, and integrated system for reliable PCa detection. Our pipeline integrates self-supervised learning, multiple-instance learning aggregation using transformers, random-undersampled boosting and ensembling: these address label scarcity, weak labels, class imbalance, and overconfidence, respectively. We train and rigorously evaluate our method using a large, multi-center dataset of micro-ultrasound data. Our method outperforms previous state-of-the-art deep learning methods in terms of accuracy and uncertainty calibration, with AUROC and balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20% of predictions with the highest confidence, we can achieve a balanced accuracy of up to 91%. The success of TRUSWorthy demonstrates the potential of integrated deep learning solutions to meet clinical needs in a highly challenging deployment setting, and is a significant step towards creating a trustworthy system for computer-assisted PCa diagnosis. </p>
<blockquote>
<p>深度学习技术在通过经直肠超声（TRUS）检测前列腺癌（PCa）可疑病灶从而提高诊断效率方面显示出巨大潜力，但它们必须克服多个同时出现的挑战。组织外观存在高度异质性，良性样本的类别不平衡显著，用于训练模型的真实注释的数量和质量稀缺。即使不能解决其中任何一个问题，也可能会导致临床结果无法接受。我们提出了TRUSWorthy，这是一个精心设计、调整和集成的可靠PCa检测系统。我们的管道集成了自监督学习、使用变压器的多实例学习聚合、随机欠采样增强和集成：这些分别解决了标签稀缺、弱标签、类别不平衡和过度自信的问题。我们使用大规模多中心超声微数据集训和严格评估了我们的方法。我们的方法在准确度和不确定性校准方面超越了之前的最新深度学习技术，曲线下面积（AUROC）和平衡准确度分别为79.9%和71.5%。在预测置信度最高的前20%中，我们可以实现高达91%的平衡准确率。TRUSWorthy的成功展示了在高度挑战的部署环境中，集成深度学习解决方案满足临床需求的潜力，并且是朝着创建可信的计算机辅助PCa诊断系统迈出的重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14707v1">PDF</a> accepted to IJCARS. This preprint has not undergone post-submission   improvements or corrections. To access the Version of Record of this article,   see the journal reference below</p>
<p><strong>Summary</strong></p>
<p>在經直肠超聲波（TRUS）中檢測可疑病變以改善前列腺癌（PCa）診斷效果的深度學習方法展現出巨大潛力，但同時也需克服多重問題。本文提出TRUSWorthy系統，整合自監督學習、多實例學習聚合使用變換器、隨機欠樣本增強和集成等方法，解決標籤缺乏、弱標籤、類別不平衡和過度自信等問題。在大型多中心微超聲波數據集上進行訓練和嚴格評估，該方法性能優於先前最先進的深度學習方法，準確性和不確定度校正的AUROC和平衡準確度分別達到79.9%和71.5%，在預測置信度最高的前20%中可實現最高達91%的平衡準確度。TRUSWorthy的成功顯示出集成深度學習解決方案在高度挑戰性的部署環境中滿足診斷需求的潛力，是建立可信的電脈診斷系統的重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习方法在前列腺癌诊断中具有巨大潜力，但面临组织外观高度异质性、良性示例显著类别不平衡和可用於训练模型的地面真相注释数量和質量稀缺等多重挑战。</li>
<li>TRUSWorthy系统通过整合多种技术来解决这些挑战，包括自监督学习、多实例学习聚合使用变换器、随机欠采样增强和集成。</li>
<li>TRUSWorthy系统在大型多中心微超声数据上进行训练和严格评估，性能优于先前的最先进深度学习方法。</li>
<li>该系统在准确度和不确定性校准方面的表现优异，AUROC和平衡准确度分别为79.9%和71.5%。</li>
<li>在高置信度预测的前20%中，平衡準確度可達到91%，顯示出該系統的可靠性。</li>
<li>TRUSWorthy的成功证明了集成深度學習方案在极具挑战性的部署环境中满足临床诊断需求的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14707">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c2133c36c087e9f7af7a36e2650ebabf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0491aeae8754121100d087e0f6640d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a6ca0cc1dbab5d206a5bcdea1e7bb43.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges"><a href="#Vision-Foundation-Models-in-Medical-Image-Analysis-Advances-and-Challenges" class="headerlink" title="Vision Foundation Models in Medical Image Analysis: Advances and   Challenges"></a>Vision Foundation Models in Medical Image Analysis: Advances and   Challenges</h2><p><strong>Authors:Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang</strong></p>
<p>The rapid development of Vision Foundation Models (VFMs), particularly Vision Transformers (ViT) and Segment Anything Model (SAM), has sparked significant advances in the field of medical image analysis. These models have demonstrated exceptional capabilities in capturing long-range dependencies and achieving high generalization in segmentation tasks. However, adapting these large models to medical image analysis presents several challenges, including domain differences between medical and natural images, the need for efficient model adaptation strategies, and the limitations of small-scale medical datasets. This paper reviews the state-of-the-art research on the adaptation of VFMs to medical image segmentation, focusing on the challenges of domain adaptation, model compression, and federated learning. We discuss the latest developments in adapter-based improvements, knowledge distillation techniques, and multi-scale contextual feature modeling, and propose future directions to overcome these bottlenecks. Our analysis highlights the potential of VFMs, along with emerging methodologies such as federated learning and model compression, to revolutionize medical image analysis and enhance clinical applications. The goal of this work is to provide a comprehensive overview of current approaches and suggest key areas for future research that can drive the next wave of innovation in medical image segmentation. </p>
<blockquote>
<p>视觉基础模型（VFMs）的快速发展，特别是视觉转换器（ViT）和分段任何模型（SAM），在医学图像分析领域引发了重大突破。这些模型在捕捉长期依赖关系和实现分割任务的高泛化方面表现出卓越的能力。然而，将这些大型模型适应医学图像分析面临几个挑战，包括医学图像和自然图像领域之间的差异、需要有效的模型适应策略以及小规模医学数据集的局限性。本文综述了将VFMs适应医学图像分割的最新研究，重点关注领域适应、模型压缩和联邦学习的挑战。我们讨论了基于适配器的改进、知识蒸馏技术和多尺度上下文特征建模的最新发展，并提出了克服这些瓶颈的未来发展方向。我们的分析强调了VFMs的潜力，以及与联邦学习和模型压缩等新兴方法相结合，将革命医学图像分析并增强临床应用。本工作的目标是提供当前方法论的全面概述，并为未来的研究提出关键领域，以推动医学图像分割领域的下一波创新。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14584v1">PDF</a> 17 pages, 1 figure</p>
<p><strong>Summary</strong><br>     医学图像分析领域因Vision Foundation Models（VFMs）的快速发展，尤其是Vision Transformers（ViT）和Segment Anything Model（SAM）的崛起而取得显著进步。这些模型在捕捉长程依赖性和实现高泛化分割任务方面表现出卓越的能力。然而，将这些大型模型应用于医学图像分析面临诸多挑战，包括医学与自然图像领域之间的差异、对高效模型适配策略的需求以及小规模医学数据集的局限性。本文综述了VFMs在医学图像分割中的最新研究，重点关注领域适配、模型压缩和联邦学习的挑战。通过讨论基于适配器的改进、知识蒸馏技术和多尺度上下文特征建模的最新进展，提出了克服这些瓶颈的未来方向。本文强调了VFMs的潜力，以及联邦学习和模型压缩等新兴方法，这些有望革新医学图像分析，提高临床应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Foundation Models (VFMs) 在医学图像分析领域取得显著进展，特别是Vision Transformers (ViT) 和 Segment Anything Model (SAM)。</li>
<li>VFMs 在捕捉长程依赖性和实现高泛化分割任务方面表现出卓越能力。</li>
<li>将VFMs应用于医学图像分析面临诸多挑战，包括领域差异、模型适配策略需求和医学数据集局限性。</li>
<li>论文综述了VFMs在医学图像分割中的最新研究，关注领域适配、模型压缩和联邦学习的挑战。</li>
<li>适配器改进、知识蒸馏技术和多尺度上下文特征建模是克服挑战的最新进展。</li>
<li>VFMs 的潜力巨大，与新兴方法如联邦学习和模型压缩结合，有望革新医学图像分析，提高临床应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14584">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e5fca45c9874048949ef49b737f38287.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a807514980741b462b8353a810dc5e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Mobile-Robotic-Approach-to-Autonomous-Surface-Scanning-in-Legal-Medicine"><a href="#A-Mobile-Robotic-Approach-to-Autonomous-Surface-Scanning-in-Legal-Medicine" class="headerlink" title="A Mobile Robotic Approach to Autonomous Surface Scanning in Legal   Medicine"></a>A Mobile Robotic Approach to Autonomous Surface Scanning in Legal   Medicine</h2><p><strong>Authors:Sarah Grube, Sarah Latus, Martin Fischer, Vidas Raudonis, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer</strong></p>
<p>Purpose: Comprehensive legal medicine documentation includes both an internal but also an external examination of the corpse. Typically, this documentation is conducted manually during conventional autopsy. A systematic digital documentation would be desirable, especially for the external examination of wounds, which is becoming more relevant for legal medicine analysis. For this purpose, RGB surface scanning has been introduced. While a manual full surface scan using a handheld camera is timeconsuming and operator dependent, floor or ceiling mounted robotic systems require substantial space and a dedicated room. Hence, we consider whether a mobile robotic system can be used for external documentation. Methods: We develop a mobile robotic system that enables full-body RGB-D surface scanning. Our work includes a detailed configuration space analysis to identify the environmental parameters that need to be considered to successfully perform a surface scan. We validate our findings through an experimental study in the lab and demonstrate the system’s application in a legal medicine environment. Results: Our configuration space analysis shows that a good trade-off between coverage and time is reached with three robot base positions, leading to a coverage of 94.96 %. Experiments validate the effectiveness of the system in accurately capturing body surface geometry with an average surface coverage of 96.90 +- 3.16 % and 92.45 +- 1.43 % for a body phantom and actual corpses, respectively. Conclusion: This work demonstrates the potential of a mobile robotic system to automate RGB-D surface scanning in legal medicine, complementing the use of post-mortem CT scans for inner documentation. Our results indicate that the proposed system can contribute to more efficient and autonomous legal medicine documentation, reducing the need for manual intervention. </p>
<blockquote>
<p>目的：全面的法律医学文档记录包括内部和外部的尸体检查。通常，这种文档记录是在传统尸检过程中手动进行的。对于伤口的外部检查，尤其需要系统的数字化记录，这在法律医学分析中具有越来越重要的意义。为此，已经引入了RGB表面扫描技术。使用手持相机进行手动全面表面扫描既耗时又依赖于操作人员，而地面或天花板安装的机器人系统则需要大量空间和一个专门的房间。因此，我们考虑是否可以使用移动机器人系统进行外部记录。方法：我们开发了一种移动机器人系统，能够进行全身RGB-D表面扫描。我们的工作包括详细的配置空间分析，以识别成功执行表面扫描需要考虑的环境参数。我们通过实验室的实验研究验证了我们的发现，并展示了该系统在法律医学环境中的实际应用。结果：我们的配置空间分析表明，通过三个机器人基座位置可以达到覆盖率和时间之间的良好平衡，覆盖率为94.96%。实验验证了该系统在准确捕捉人体表面几何结构方面的有效性，对于人体假体和实际尸体，平均表面覆盖率分别为96.90 ± 3.16%和92.45 ± 1.43%。结论：这项工作证明了移动机器人系统在法律医学中自动进行RGB-D表面扫描的潜力，可以配合死后CT扫描用于内部记录。我们的结果表明，所提出的系统可以提高法律医学记录的效率和自主性，减少人工干预的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14514v1">PDF</a> Submitted and accepted for presentation at CARS 2025. This preprint   has not undergone peer review or post-submission revisions. The final version   of this work will appear in the official CARS 2025 proceedings</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种用于法律医学文档记录的移动机器人系统，该系统能够进行全身RGB-D表面扫描。通过配置空间分析，确定了成功进行表面扫描所需考虑的环境参数。实验验证表明，该系统能够准确捕捉人体表面几何结构，对尸体进行自动RGB表面扫描具有潜力，有助于更有效率且自主的医学文档记录。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>移动机器人系统被开发用于法律医学中的全身RGB-D表面扫描。</li>
<li>配置空间分析确定了成功进行表面扫描所需考虑的环境参数。</li>
<li>实验验证表明，该系统能够准确捕捉人体表面几何结构。</li>
<li>系统在尸体表面扫描的覆盖率高，为94.96%。</li>
<li>与尸体幻影和真实尸体的实验表明，系统表面覆盖率为96.90%±3.16%和92.45%±1.43%。</li>
<li>该系统有助于更有效率且自主的医学文档记录，减少人工干预的需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c1c8a84cf0b88a7cc85cf4b1fcedfb72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efd5176a3bbb679f8ec42ad6951ad94a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f555ff71126b4d4596dd3c8c2f78856.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Role-of-the-Pretraining-and-the-Adaptation-data-sizes-for-low-resource-real-time-MRI-video-segmentation"><a href="#Role-of-the-Pretraining-and-the-Adaptation-data-sizes-for-low-resource-real-time-MRI-video-segmentation" class="headerlink" title="Role of the Pretraining and the Adaptation data sizes for low-resource   real-time MRI video segmentation"></a>Role of the Pretraining and the Adaptation data sizes for low-resource   real-time MRI video segmentation</h2><p><strong>Authors:Masoud Thajudeen Tholan, Vinayaka Hegde, Chetan Sharma, Prasanta Kumar Ghosh</strong></p>
<p>Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech production studies as it provides a complete view of the vocal tract during articulation. This study investigates the effectiveness of rtMRI in analyzing vocal tract movements by employing the SegNet and UNet models for Air-Tissue Boundary (ATB)segmentation tasks. We conducted pretraining of a few base models using increasing numbers of subjects and videos, to assess performance on two datasets. First, consisting of unseen subjects with unseen videos from the same data source, achieving 0.33% and 0.91% (Pixel-wise Classification Accuracy (PCA) and Dice Coefficient respectively) better than its matched condition. Second, comprising unseen videos from a new data source, where we obtained an accuracy of 99.63% and 98.09% (PCA and Dice Coefficient respectively) of its matched condition performance. Here, matched condition performance refers to the performance of a model trained only on the test subjects which was set as a benchmark for the other models. Our findings highlight the significance of fine-tuning and adapting models with limited data. Notably, we demonstrated that effective model adaptation can be achieved with as few as 15 rtMRI frames from any new dataset. </p>
<blockquote>
<p>实时磁共振成像（rtMRI）在语音生产研究中经常被使用，因为它能提供发音时整个声道的视图。本研究探讨了rtMRI在分析声道运动中的有效性，采用SegNet和UNet模型进行空气-组织边界（ATB）分割任务。我们通过对多个基础模型进行预训练，并使用不断增加的受试者和视频数量来评估两个数据集上的性能。首先，我们使用了来自同一数据源的未见过的受试者及其视频，相较于匹配条件下的性能，取得了0.33%（像素级分类准确度）和0.91%（Dice系数）的改进。其次，我们使用了来自新数据源的未见过的视频，相较于匹配条件下的性能，我们获得了99.63%（PCA）和98.09%（Dice系数）的准确率。这里，匹配条件下的性能是指仅对测试受试者进行训练的模型的表现，它被设定为其他模型性能的基准。我们的研究结果表明了微调并适应有限数据的模型的重要性。值得注意的是，我们证明仅使用任何新数据集的15个rtMRI帧即可实现有效的模型适应。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14418v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     本研究利用rtMRI实时磁共振成像技术，通过SegNet和UNet模型对空气组织边界（ATB）分割任务进行分析，探讨其在研究语音产生过程中声带运动的有效性。研究通过在不同数据集上预训练基础模型并增加受试者和视频数量来评估性能。在未见受试者和未见视频的数据集上取得了优异的表现，且在新数据源未见视频的测试中，模型性能达到了很高的准确率。此外，研究发现精细调整模型并适应有限数据至关重要，仅使用新数据集的少量rtMRI帧即可实现有效的模型适应。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>rtMRI在语音产生研究中被广泛应用于观察声带运动。</li>
<li>SegNet和UNet模型被用于进行空气组织边界（ATB）分割任务的分析。</li>
<li>通过预训练基础模型并增加受试者和视频数量来评估模型性能。</li>
<li>在未见受试者和未见视频的数据集上取得了优异表现。</li>
<li>在新数据源未见视频的测试中，模型性能达到了很高的准确率。</li>
<li>精细调整模型并适应有限数据至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c9c090acae09eb44ce9337cd00ad33d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66847e975d9f58abc0b6078652e5f7d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1b0c3a72fe7b91cc415e6e7da6664aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-485f79e1227004bc012f8a82d62dba82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fc86ff5507c3905cb21d164d9f74d9c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedFuncta-Modality-Agnostic-Representations-Based-on-Efficient-Neural-Fields"><a href="#MedFuncta-Modality-Agnostic-Representations-Based-on-Efficient-Neural-Fields" class="headerlink" title="MedFuncta: Modality-Agnostic Representations Based on Efficient Neural   Fields"></a>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural   Fields</h2><p><strong>Authors:Paul Friedrich, Florentin Bieder, Phlippe C. Cattin</strong></p>
<p>Recent research in medical image analysis with deep learning almost exclusively focuses on grid- or voxel-based data representations. We challenge this common choice by introducing MedFuncta, a modality-agnostic continuous data representation based on neural fields. We demonstrate how to scale neural fields from single instances to large datasets by exploiting redundancy in medical signals and by applying an efficient meta-learning approach with a context reduction scheme. We further address the spectral bias in commonly used SIREN activations, by introducing an $\omega_0$-schedule, improving reconstruction quality and convergence speed. We validate our proposed approach on a large variety of medical signals of different dimensions and modalities (1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, Colon Histopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfully demonstrate that we can solve relevant downstream tasks on these representations. We additionally release a large-scale dataset of &gt; 550k annotated neural fields to promote research in this direction. </p>
<blockquote>
<p>近期深度学习在医学图像分析领域的研究几乎完全集中在基于网格或体素的数据表示上。我们通过引入MedFuncta这一基于神经场的模态无关连续数据表示方式，对这一常见选择提出了挑战。我们展示了如何利用医学信号中的冗余信息，通过采用带有上下文缩减方案的高效元学习方法，将神经场从单个实例扩展到大型数据集。此外，我们通过引入ω0调度方法解决了常用SIREN激活函数中的频谱偏差问题，提高了重建质量和收敛速度。我们在多种不同维度和模态的医学信号（1D：心电图；2D：胸部X射线、视网膜OCT、眼底相机、皮肤显微镜、结肠组织病理学、细胞显微镜；3D：脑部MRI、肺部CT）上验证了我们的方法，并成功证明我们能够在这些表示上解决相关的下游任务。此外，我们还发布了一个包含超过55万个注释神经场的大型数据集，以促进这一方向的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14401v1">PDF</a> Code and Dataset: <a target="_blank" rel="noopener" href="https://github.com/pfriedri/medfuncta">https://github.com/pfriedri/medfuncta</a></p>
<p><strong>Summary</strong></p>
<p>基于深度学习在医学图像分析领域的研究主要聚焦于网格或体素数据表示。本研究通过引入MedFuncta这一模态无关的连续数据表示方法，对这一现象提出挑战。该研究通过利用医学信号的冗余信息和采用一种带有上下文缩减方案的高效元学习方法实现了神经网络从单一实例到大数据集的扩展。同时，该研究针对常用SIREN激活函数中的频谱偏差问题，通过引入ω0调度方案，提高了重建质量和收敛速度。本研究在不同维度和模态的医学信号上验证了所提出方法的有效性，并成功展示了其在相关下游任务中的应用潜力。此外，该研究还发布了一个大规模神经场数据集，以促进该方向的研究发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究挑战了深度学习在医学图像分析中的主流数据表示方法，引入了模态无关的连续数据表示方法MedFuncta。</li>
<li>通过利用医学信号的冗余信息和高效元学习方法，实现了神经网络在大数据集上的应用。</li>
<li>针对SIREN激活函数的频谱偏差问题，提出了ω0调度方案，提高了模型性能。</li>
<li>在多种维度和模态的医学信号上验证了所提出方法的有效性。</li>
<li>成功展示了方法在相关下游任务中的应用潜力。</li>
<li>发布了大规模神经场数据集，促进研究方向的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17d37ef73e80fc02799bfc0a99dcc26b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-224e9a496dbd279ec7035cec96e51ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c24efe4411892193811c180690f30b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f749ffc2792cd29ab3642220857fb703.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a8c50f74233fbcb6a7b7b09aa8b5aab.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SegAnyPET-Universal-Promptable-Segmentation-from-Positron-Emission-Tomography-Images"><a href="#SegAnyPET-Universal-Promptable-Segmentation-from-Positron-Emission-Tomography-Images" class="headerlink" title="SegAnyPET: Universal Promptable Segmentation from Positron Emission   Tomography Images"></a>SegAnyPET: Universal Promptable Segmentation from Positron Emission   Tomography Images</h2><p><strong>Authors:Yichi Zhang, Le Xue, Wenbo Zhang, Lanlan Li, Yuchen Liu, Chen Jiang, Yuan Cheng, Yuan Qi</strong></p>
<p>Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patient’s body, which is essential for quantification of therapy response and monitoring treatment progress. However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities. Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data. Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging. </p>
<blockquote>
<p>正电子发射断层扫描（PET）成像在现代医学诊断中发挥着关键作用，它能够揭示患者体内的代谢过程，这对于量化治疗反应和监测治疗进展至关重要。然而，由于PET图像的对比度较低，边界不够清晰，与其他结构医学图像相比，其分割面临独特挑战。最近的分割基础模型的发展表明，它们在各种自然图像分割任务中具有出色的多功能性。尽管进行了医学适应性的努力，但这些工作主要集中在具有详细生理结构信息的结构医学图像上，而在适应分子PET成像时表现出较差的泛化能力。在本文中，我们收集和构建了迄今为止最大的PET分割数据集PETS-5k，包含5731个三维全身PET图像和超过130万个二维图像。基于建立的数据集，我们开发了一种特定模态的三维基础模型SegAnyPET，用于从PET图像进行通用即时分割。为了解决PET图像标注质量不一致的问题，我们采用带有不确定性引导的自我修正过程的交叉提示置信学习（CPCL）策略，以稳健地从高质量标注数据和低质量噪声标注数据中学习分割。实验结果表明，SegAnyPET仅使用一个或少数提示点就能正确分割已知和未知目标，优于最新的基础模型和任务特定全监督模型，具有更高的准确性和强大的泛化能力进行通用分割。作为第一个用于PET图像的基础模型，我们相信SegAnyPET将推动其在分子成像的各种下游任务中的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了正电子发射断层扫描（PET）成像在现代医学诊断中的重要性，并指出了PET图像分割面临的挑战。为此，作者构建了迄今为止最大的PET分割数据集PETS-5k，并在此基础上开发了一种适用于PET图像的通用即时分割模型SegAnyPET。该模型采用跨提示置信学习（CPCL）策略，可稳健地从高质量标注数据和低质量噪声标注数据中学习分割。实验结果表明，SegAnyPET能够正确分割已见和未见目标，优于最先进的基础模型和特定任务的完全监督模型，具有更高的准确性和强大的通用分割能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PET成像在现代医学诊断中起关键作用，能揭示患者体内的代谢过程，对治疗反应的量化和治疗进展的监测至关重要。</li>
<li>PET图像分割面临挑战，因其对比度较低，边界较模糊。</li>
<li>构建了迄今为止最大的PET分割数据集PETS-5k，包含5731个三维全身PET图像和超过130万个二维图像。</li>
<li>开发了适用于PET图像的通用即时分割模型SegAnyPET。</li>
<li>SegAnyPET采用跨提示置信学习（CPCL）策略，可从不同质量标注数据中稳健学习分割。</li>
<li>实验结果表明SegAnyPET性能优越，能够正确分割已见和未见目标，并具备强大的通用分割能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-288550d062d4aba970482dcf75b83a74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05270e8e4cddc0876e187d4eb2511c30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae2d74ff268df4180ff80ef656c65d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca952e0bf8821a340720d50a7355840a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging"><a href="#H3DE-Net-Efficient-and-Accurate-3D-Landmark-Detection-in-Medical-Imaging" class="headerlink" title="H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging"></a>H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical   Imaging</h2><p><strong>Authors:Zhen Huang, Ronghao Xu, Xiaoqian Zhou, Yangbo Wei, Suhua Wang, Xiaoxin Sun, Han Li, Qingsong Yao</strong></p>
<p>3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.   In this work, We propose a \textbf{H}ybrid \textbf{3}D \textbf{DE}tection \textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights. </p>
<blockquote>
<p>三维（3D）地标检测是医学图像分析中的一项关键任务，而准确检测解剖地标对于后续医学成像任务至关重要。然而，该领域的主流深度学习方法难以在捕捉精细局部特征、对全局空间关系进行建模的同时，保持准确性和计算效率之间的平衡。局部特征提取需要捕捉精细的解剖细节，而全局建模则需要理解复杂解剖结构内的空间关系。由于地标分布稀疏，三维体积的高维性质进一步加剧了这些挑战，导致计算成本显著增加。因此，实现高效且精确的3D地标检测仍然是医学图像分析领域的一个紧迫挑战。在本研究中，我们提出了一种混合三维检测网络（Hybrid 3D Detection Net，简称H3DE-Net），这是一个新颖框架，结合了卷积神经网络（CNN）用于局部特征提取和一个轻量级注意力机制，旨在高效捕捉三维体积数据中的全局依赖关系。该机制采用分层路由策略来降低计算成本，同时保持全局上下文建模。据我们所知，H3DE-Net是首个将轻量级注意力机制与CNN结合用于三维地标检测的模型。此外，多尺度特征融合技术进一步提高了检测精度和鲁棒性。在公共CT数据集上的实验结果表明，H3DE-Net达到了最新的性能水平，特别是在地标缺失或解剖结构复杂的场景中，显著提高了准确性和鲁棒性。我们已经开源了我们的项目，包括代码、数据和模型权重。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种混合三维检测网络（H3DE-Net），结合卷积神经网络（CNN）进行局部特征提取，并设计了一种轻量级注意力机制以高效捕捉三维体积数据中的全局依赖关系。该方法采用分层路由策略降低计算成本，同时保持全局上下文建模。在公开CT数据集上的实验结果表明，H3DE-Net达到最先进的性能，特别是在缺失地标或复杂解剖变异情况下，显著提高了准确性和鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D landmark detection在医学图像分析中是关键任务。</li>
<li>主流深度学习方法难以在捕捉局部精细特征和建模全局空间关系之间保持平衡。</li>
<li>H3DE-Net结合CNN进行局部特征提取，并采用轻量级注意力机制捕捉全局依赖。</li>
<li>分层路由策略降低计算成本，同时保持全局上下文建模。</li>
<li>H3DE-Net是首个结合轻量级注意力机制和CNN的3D地标检测模型。</li>
<li>多尺度特征融合进一步提高检测准确性和鲁棒性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-78603d55fa0b8a64302b114e590066fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb185c11e0b8d790e5c9577256dd6498.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation"><a href="#Conditional-diffusion-model-with-spatial-attention-and-latent-embedding-for-medical-image-segmentation" class="headerlink" title="Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation"></a>Conditional diffusion model with spatial attention and latent embedding   for medical image segmentation</h2><p><strong>Authors:Behzad Hejrati, Soumyanil Banerjee, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Diffusion models have been used extensively for high quality image and video generation tasks. In this paper, we propose a novel conditional diffusion model with spatial attention and latent embedding (cDAL) for medical image segmentation. In cDAL, a convolutional neural network (CNN) based discriminator is used at every time-step of the diffusion process to distinguish between the generated labels and the real ones. A spatial attention map is computed based on the features learned by the discriminator to help cDAL generate more accurate segmentation of discriminative regions in an input image. Additionally, we incorporated a random latent embedding into each layer of our model to significantly reduce the number of training and sampling time-steps, thereby making it much faster than other diffusion models for image segmentation. We applied cDAL on 3 publicly available medical image segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed significant qualitative and quantitative improvements with higher Dice scores and mIoU over the state-of-the-art algorithms. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/">https://github.com/Hejrati/cDAL/</a>. </p>
<blockquote>
<p>扩散模型已被广泛应用于高质量图像和视频生成任务。在本文中，我们提出了一种具有空间注意力和潜在嵌入（cDAL）的新型条件扩散模型，用于医学图像分割。在cDAL中，扩散过程的每个时间步都使用基于卷积神经网络（CNN）的判别器来区分生成的标签和真实的标签。基于判别器学习的特征计算空间注意力图，以帮助cDAL生成输入图像中判别区域的更准确分割。此外，我们将随机潜在嵌入融入模型中的每一层，以大大减少训练和采样时间步的数量，从而使它比其他图像分割扩散模型更快。我们在3个公开可用的医学图像分割数据集（MoNuSeg、Chest X-ray和Hippocampus）上应用了cDAL，并在最先进的算法上观察到显著的定性和定量改进，具有更高的Dice分数和mIoU。源代码可在<a target="_blank" rel="noopener" href="https://github.com/Hejrati/cDAL/%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Hejrati/cDAL/处获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06997v2">PDF</a> 13 pages, 5 figures, 3 tables, Accepted in MICCAI 2024</p>
<p><strong>Summary</strong><br>医学图像分割中，提出一种新型的条件扩散模型cDAL，结合空间注意力和潜在嵌入。使用CNN判别器区分生成标签和真实标签，计算空间注意图以提高输入图像中判别区域的分割准确性。融入随机潜在嵌入，减少训练和采样时间步数，较其他图像分割扩散模型更快。在三个公开医学图像分割数据集上应用，获得较高的Dice分数和mIoU，优于现有算法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出新型条件扩散模型cDAL用于医学图像分割。</li>
<li>结合空间注意力和潜在嵌入技术。</li>
<li>使用CNN判别器以提高分割准确性。</li>
<li>空间注意图可帮助cDAL生成更准确的结果。</li>
<li>融入随机潜在嵌入以加速训练和采样过程。</li>
<li>在三个公开医学图像分割数据集上实现显著效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06997">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69d3b4fed0d242f0ec8d4a7984619fda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a77ce743221a1e323f776121fce2cc5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Finite-Element-Analysis-Model-for-Magnetomotive-Ultrasound-Elastometry-Magnet-Design-with-Experimental-Validation"><a href="#A-Finite-Element-Analysis-Model-for-Magnetomotive-Ultrasound-Elastometry-Magnet-Design-with-Experimental-Validation" class="headerlink" title="A Finite Element Analysis Model for Magnetomotive Ultrasound Elastometry   Magnet Design with Experimental Validation"></a>A Finite Element Analysis Model for Magnetomotive Ultrasound Elastometry   Magnet Design with Experimental Validation</h2><p><strong>Authors:Jacquelline Nyakunu, Christopher T. Piatnichouk, Henry C. Russell, Niels J. van Duijnhoven, Benjamin E. Levy</strong></p>
<p>Magnetomotive ultrasound (MMUS) using magnetic nanoparticle contrast agents has shown promise for thrombosis imaging and quantitative elastometry via magnetomotive resonant acoustic spectroscopy (MRAS). Young’s modulus measurements of smaller, stiffer thrombi require an MRAS system capable of generating forces at higher temporal frequencies. Solenoids with fewer turns, and thus less inductance, could improve high frequency performance, but the reduced force may compromise results. In this work, a computational model capable of assessing the effectiveness of MRAS elastometry magnet configurations is presented and validated. Finite element analysis (FEA) was used to model the force and inductance of MRAS systems. The simulations incorporated both solenoid electromagnets and permanent magnets in three-dimensional steady-state, frequency domain, and time domain studies. The model successfully predicted that a configuration in which permanent magnets were added to an existing MRAS system could be used to increase the force supplied. Accordingly, the displacement measured in a magnetically labeled validation phantom increased by a factor of $2.2 \pm 0.3$ when the force was predicted to increase by a factor of $2.2 \pm 0.2$. The model additionally identified a new solenoid configuration consisting of four smaller coils capable of providing sufficient force at higher driving frequencies. These results indicate two methods by which MRAS systems could be designed to deliver higher frequency magnetic forces without the need for experimental trial and error. Either the number of turns within each solenoid could be reduced while permanent magnets are added at precise locations, or a larger number of smaller solenoids could be used. These findings overcome a key challenge toward the goal of MMUS thrombosis elastometry, and simulation files are provided online for broader experimentation. </p>
<blockquote>
<p>磁动超声（MMUS）利用磁性纳米粒子造影剂在血栓成像和定量弹性测量方面表现出巨大潜力，这通过磁动共振声学光谱法（MRAS）实现。对较小、较硬的血栓的杨氏模量测量需要一种能够在较高时间频率下产生力的MRAS系统。具有较少匝数、因此电感较小的螺线管可以提高高频性能，但减小的作用力可能会损害结果。在这项工作中，提出了一种能够评估MRAS弹性测量磁构型的有效性的计算模型，并进行验证。有限元分析（FEA）用于模拟MRAS系统的力和电感。模拟结合了螺线管电磁铁和永久磁铁的三维稳态、频域和时域研究。该模型成功预测，向现有MRAS系统添加永久磁铁的配置可用于增加所提供的力。因此，在预测力增加了一个因子$2.2±0.2$的情况下，磁性标记验证模型中的位移增加了$2.2±0.3$倍。此外，该模型还确定了一种新的螺线管配置，由四个较小的线圈组成，能够在较高的驱动频率下提供足够的力。这些结果表明，可以通过两种方法设计MRAS系统以产生较高频率的磁力，无需通过实验反复试错。一种方法是减少每个螺线管中的匝数，同时在精确位置添加永久磁铁；另一种方法是使用更多较小的螺线管。这些发现为实现MMUS血栓弹性测量的目标克服了关键挑战，并且仿真文件已在线提供供进一步实验使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07737v2">PDF</a> 12 pages, 8 figures. This manuscript has been revised from version v1   via the peer review process. It has been accepted in its current form and is   awaiting publication at Biomedical Physics and Engineering Express</p>
<p><strong>Summary</strong></p>
<p>本文介绍了磁动机超声（MMUS）利用磁性纳米粒子造影剂在血栓成像和定量弹性测量方面的潜力。为提高对较小、较硬血栓的杨氏模量测量能力，需要能在更高时间频率下产生力的MRAS系统。研究提出了一种能够评估MRAS弹性测量磁配置的有效性的计算模型，并通过有限元分析（FEA）进行模拟验证。模型成功预测了添加永久磁铁到现有MRAS系统中可增加提供的力的配置。此外，模型还识别了一种新的由四个小型线圈组成的天线配置，能够在较高的驱动频率下提供足够的力量。这标志着解决MRAS系统在设计上朝着更高频率磁场力发展的两大突破，无需实验性的反复试错。这些发现克服了实现MMUS血栓弹性测量的关键挑战，且模拟文件已在线提供以供更广泛的实验使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMUS使用磁性纳米粒子造影剂在血栓成像和定量弹性测量方面展现潜力。</li>
<li>高频性能提升需要能在更高时间频率下产生力的MRAS系统。</li>
<li>计算模型通过有限元分析评估不同磁配置的有效性。</li>
<li>添加永久磁铁到MRAS系统中可增加提供的力。</li>
<li>新识别的一种由四个小型线圈组成的配置能在高驱动频率下提供足够力量。</li>
<li>这些发现解决了设计MRAS系统以实现更高频率磁场力的关键挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb87733106393cf49e8b41fb28bdbc4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d76a711e0def95004a6abf5bac039b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6176e674851207efae218da24a8d48d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a718043e0e50662fbd0b990ae50e1f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33a8f3a22677bfb5a8c6b08bea420ef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Robust-Tumor-Segmentation-with-Hyperspectral-Imaging-and-Graph-Neural-Networks"><a href="#Robust-Tumor-Segmentation-with-Hyperspectral-Imaging-and-Graph-Neural-Networks" class="headerlink" title="Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural   Networks"></a>Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural   Networks</h2><p><strong>Authors:Mayar Lotfy Mostafa, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler</strong></p>
<p>Segmenting the boundary between tumor and healthy tissue during surgical cancer resection poses a significant challenge. In recent years, Hyperspectral Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising solution. However, due to the extensive information contained within the spectral domain, most ML approaches primarily classify individual HSI (super-)pixels, or tiles, without taking into account their spatial context. In this paper, we propose an improved methodology that leverages the spatial context of tiles for more robust and smoother segmentation. To address the irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate context information across neighboring regions. The features for each tile within the graph are extracted using a Convolutional Neural Network (CNN), which is trained simultaneously with the subsequent GNN. Moreover, we incorporate local image quality metrics into the loss function to enhance the training procedure’s robustness against low-quality regions in the training images. We demonstrate the superiority of our proposed method using a clinical ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the limited dataset, the GNN-based model significantly outperforms context-agnostic approaches, accurately distinguishing between healthy and tumor tissues, even in images from previously unseen patients. Furthermore, we show that our carefully designed loss function, accounting for local image quality, results in additional improvements. Our findings demonstrate that context-aware GNN algorithms can robustly find tumor demarcations on HSI images, ultimately contributing to better surgery success and patient outcome. </p>
<blockquote>
<p>在手术切除癌症的过程中，区分肿瘤和健康组织的边界是一个巨大的挑战。近年来，高光谱成像（HSI）结合机器学习（ML）已成为一种前景广阔的解决方案。然而，由于光谱域内包含的大量信息，大多数机器学习的方法主要对单个HSI（超级）像素或瓦片进行分类，而没有考虑到它们的空间上下文。在本文中，我们提出了一种改进的方法，该方法利用瓦片的空间上下文来实现更稳健、更平滑的分割。为了解决瓦片形状不规则的问题，我们利用图神经网络（GNNs）在邻近区域之间传播上下文信息。图中每个瓦片的特征是使用卷积神经网络（CNN）提取的，该网络是与随后的GNN同时训练的。此外，我们将局部图像质量指标纳入损失函数中，以提高训练过程对训练图像中低质量区域的稳健性。我们使用由30名患者的51张HSI图像组成的临床离体数据集来证明我们提出的方法的优越性。尽管数据集有限，但基于GNN的模型显著优于上下文无关的方法，能够准确区分健康组织和肿瘤组织，即使在以前未见过的患者的图像中也是如此。此外，我们表明，我们精心设计的考虑局部图像质量的损失函数会导致额外的改进。我们的研究结果表明，上下文感知的GNN算法可以稳健地在HSI图像上找到肿瘤边界，最终有助于提高手术成功率和患者治疗效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11782v2">PDF</a> 18 pages, 5 figures, The German Conference on Pattern Recognition   (GCPR) 2024</p>
<p><strong>Summary</strong><br>     本论文提出了一种利用图神经网络（GNN）结合卷积神经网络（CNN）的方法，通过考虑超光谱成像（HSI）中像素的空间上下文信息，实现对肿瘤与健康组织的更稳健、平滑的分割。该方法在有限的临床离体数据集上表现出显著优势，能准确区分健康组织和肿瘤组织，甚至在对未见患者的新图像上也有良好表现。同时，考虑到局部图像质量的损失函数设计进一步提高模型的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超光谱成像（HSI）结合机器学习（ML）在肿瘤切除手术中区分肿瘤和健康组织方面具有潜力。</li>
<li>大多数ML方法主要对HSI像素或瓦片进行分类，忽略了其空间上下文信息。</li>
<li>本研究提出了一种利用图神经网络（GNN）的方法，考虑瓦片的空间上下文信息，实现更稳健和平滑的分割。</li>
<li>通过卷积神经网络（CNN）提取瓦片特征，并与随后的图神经网络（GNN）一起进行训练。</li>
<li>纳入局部图像质量指标，增强训练程序对低质量区域的稳健性。</li>
<li>在包含51张HSI图像的临床离体数据集上验证了所提出方法的优越性，该数据集来自30名患者。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.11782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f41364e93bdd741561e5328ce3ac222c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5a197bfa99d623443cf1c49c6826648.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f637ebaece3e1eebfbad74cf00d2000.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0426fb81106a34846c470965b5963a29.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-02-22  Speech to Speech Translation with Translatotron A State of the Art   Review
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a5f906428dc7327be918f0b7e1b6dc75.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-22  A Survey on Text-Driven 360-Degree Panorama Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
