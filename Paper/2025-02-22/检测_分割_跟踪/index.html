<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-02-22  Monocular Depth Estimation and Segmentation for Transparent Object with   Iterative Semantic and Geometric Fusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f1f51db6dcce32bab82abcd113417182.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-22-更新"><a href="#2025-02-22-更新" class="headerlink" title="2025-02-22 更新"></a>2025-02-22 更新</h1><h2 id="Monocular-Depth-Estimation-and-Segmentation-for-Transparent-Object-with-Iterative-Semantic-and-Geometric-Fusion"><a href="#Monocular-Depth-Estimation-and-Segmentation-for-Transparent-Object-with-Iterative-Semantic-and-Geometric-Fusion" class="headerlink" title="Monocular Depth Estimation and Segmentation for Transparent Object with   Iterative Semantic and Geometric Fusion"></a>Monocular Depth Estimation and Segmentation for Transparent Object with   Iterative Semantic and Geometric Fusion</h2><p><strong>Authors:Jiangyuan Liu, Hongxuan Ma, Yuxin Guo, Yuhao Zhao, Chi Zhang, Wei Sui, Wei Zou</strong></p>
<p>Transparent object perception is indispensable for numerous robotic tasks. However, accurately segmenting and estimating the depth of transparent objects remain challenging due to complex optical properties. Existing methods primarily delve into only one task using extra inputs or specialized sensors, neglecting the valuable interactions among tasks and the subsequent refinement process, leading to suboptimal and blurry predictions. To address these issues, we propose a monocular framework, which is the first to excel in both segmentation and depth estimation of transparent objects, with only a single-image input. Specifically, we devise a novel semantic and geometric fusion module, effectively integrating the multi-scale information between tasks. In addition, drawing inspiration from human perception of objects, we further incorporate an iterative strategy, which progressively refines initial features for clearer results. Experiments on two challenging synthetic and real-world datasets demonstrate that our model surpasses state-of-the-art monocular, stereo, and multi-view methods by a large margin of about 38.8%-46.2% with only a single RGB input. Codes and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/L-J-Yuan/MODEST">https://github.com/L-J-Yuan/MODEST</a>. </p>
<blockquote>
<p>透明物体的感知对于许多机器人任务来说是不可或缺的。然而，由于透明物体的光学属性复杂，准确地对透明物体进行分割和深度估计仍然是一个挑战。现有的方法主要专注于使用额外的输入或专用传感器来解决单一任务，忽视了任务间的有价值的交互以及随后的优化过程，导致预测结果不佳且模糊。为了解决这些问题，我们提出了一种单目相机框架，该框架首次在仅使用单张图像输入的情况下，出色地完成了透明物体的分割和深度估计任务。具体来说，我们设计了一种新型语义和几何融合模块，有效地集成了任务之间的多尺度信息。此外，从人类对物体的感知中汲取灵感，我们进一步采用了一种迭代策略，该策略逐步优化初始特征以获得更清晰的结果。在两个具有挑战性的合成数据集和真实世界数据集上的实验表明，我们的模型在仅使用单一RGB输入的情况下，大大超越了最先进的单目、立体和多视角方法，准确率高出了大约38.8%-46.2%。模型和代码公开可访问<a target="_blank" rel="noopener" href="https://github.com/L-J-Yuan/MODEST%E3%80%82">https://github.com/L-J-Yuan/MODEST。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14616v1">PDF</a> Accepted by ICRA(2025). The code is accessible through:   <a target="_blank" rel="noopener" href="https://github.com/L-J-Yuan/MODEST">https://github.com/L-J-Yuan/MODEST</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种全新的单眼框架，能够在仅使用单张图像的情况下出色地完成透明物体的分割和深度估计。通过构建新颖的语义和几何融合模块，实现任务间的多尺度信息有效整合。同时，借鉴人类感知物体的方式，采用迭代策略逐步优化初始特征，以获得更清晰的结果。在具有挑战性的合成和真实数据集上的实验表明，该模型在单眼、立体和多视角方法中均表现出显著优势，相对提升了约38.8%-46.2%。模型和代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>透明物体感知在机器人任务中至关重要，但准确分割和估计其深度具有挑战性。</li>
<li>当前方法主要关注单一任务，缺乏任务间交互和随后的优化过程，导致预测结果不佳。</li>
<li>提出的单眼框架可同时进行透明物体的分割和深度估计，仅使用单张图像。</li>
<li>通过语义和几何融合模块实现多尺度信息整合。</li>
<li>借鉴人类感知物体方式，采用迭代策略优化初始特征，获得更清晰结果。</li>
<li>在多个数据集上的实验表明，该模型显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d863ae640bd5e0d60175c80ccb5ef3d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56334e6ff3f0945f8c2fbc780858d8bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f85b51d08aa8b93446a1db7808f86f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-866a2212cbf32043c6a6b3d59076b795.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0b1cc2f6661c61640534acefbe5ba56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81955abf0b7baa3c203e58d732def4da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53acd2bf4d1628a4711428b0b864aa77.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LXLv2-Enhanced-LiDAR-Excluded-Lean-3D-Object-Detection-with-Fusion-of-4D-Radar-and-Camera"><a href="#LXLv2-Enhanced-LiDAR-Excluded-Lean-3D-Object-Detection-with-Fusion-of-4D-Radar-and-Camera" class="headerlink" title="LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of   4D Radar and Camera"></a>LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of   4D Radar and Camera</h2><p><strong>Authors:Weiyi Xiong, Zean Zou, Qiuchi Zhao, Fengchun He, Bing Zhu</strong></p>
<p>As the previous state-of-the-art 4D radar-camera fusion-based 3D object detection method, LXL utilizes the predicted image depth distribution maps and radar 3D occupancy grids to assist the sampling-based image view transformation. However, the depth prediction lacks accuracy and consistency, and the concatenation-based fusion in LXL impedes the model robustness. In this work, we propose LXLv2, where modifications are made to overcome the limitations and improve the performance. Specifically, considering the position error in radar measurements, we devise a one-to-many depth supervision strategy via radar points, where the radar cross section (RCS) value is further exploited to adjust the supervision area for object-level depth consistency. Additionally, a channel and spatial attention-based fusion module named CSAFusion is introduced to improve feature adaptiveness. Experimental results on the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can outperform LXL in detection accuracy, inference speed and robustness, demonstrating the effectiveness of the model. </p>
<blockquote>
<p>作为之前最先进的基于4D雷达摄像头融合技术的3D目标检测方法，LXL利用预测的图像深度分布图和雷达3D占用网格来辅助基于采样的图像视图转换。然而，深度预测缺乏准确性和一致性，并且LXL中的基于拼接的融合方法阻碍了模型的稳健性。在这项工作中，我们提出了LXLv2，对其进行了修改以克服局限性并提高性能。具体来说，考虑到雷达测量中的位置误差，我们设计了一种基于雷达点的一对一多深度监督策略，其中进一步利用雷达横截面（RCS）值来调整监督区域以实现对象级别的深度一致性。此外，引入了一个名为CSAFusion的基于通道和空间注意力的融合模块，以提高特征适应性。在Delft视角和TJ4DRadSet数据集上的实验结果表明，所提出的LXLv2在检测精度、推理速度和稳健性方面优于LXL，证明了该模型的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14503v1">PDF</a> Accepted by IEEE Robotics and Automation Letters</p>
<p><strong>Summary</strong></p>
<p>基于雷达与摄像头融合技术的三维物体检测算法LXLv2，针对前序算法LXL在深度预测及融合稳健性上的不足进行了改进。通过采用雷达点的深度监督策略以及融入CSAFusion模块等方法，提升了模型的检测精度、推理速度和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LXL算法在深度预测方面存在精度和一致性问题。</li>
<li>LXLv2算法旨在克服LXL算法的局限性并提升性能。</li>
<li>LXLv2采用雷达点的深度监督策略，通过雷达横截面（RCS）值调整监督区域，以提高深度一致性。</li>
<li>引入CSAFusion模块提升特征适应性。</li>
<li>在View-of-Delft和TJ4DRadSet数据集上的实验结果显示，LXLv2在检测精度、推理速度和稳健性方面优于LXL。</li>
<li>LXLv2改进了采样过程并提高了数据效率，进而提高了模型的总体性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14503">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-af91e31adb17edd70b0e30043c2abd9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e2492abead050b14fd08683e3f5e306.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c577ae5f1a896b641beb0fad9bcdc8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4822659f1ed0cf2de7bef199ec4d7336.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8130a297f923b642c564a0b9af30785a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b076a01e3e42bdb6993461735a5fcba2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Bayesian-SegNet-for-Semantic-Segmentation-with-Improved-Interpretation-of-Microstructural-Evolution-During-Irradiation-of-Materials"><a href="#Bayesian-SegNet-for-Semantic-Segmentation-with-Improved-Interpretation-of-Microstructural-Evolution-During-Irradiation-of-Materials" class="headerlink" title="Bayesian SegNet for Semantic Segmentation with Improved Interpretation   of Microstructural Evolution During Irradiation of Materials"></a>Bayesian SegNet for Semantic Segmentation with Improved Interpretation   of Microstructural Evolution During Irradiation of Materials</h2><p><strong>Authors:Marjolein Oostrom, Alex Hagen, Nicole LaHaye, Karl Pazdernik</strong></p>
<p>Understanding the relationship between the evolution of microstructures of irradiated LiAlO2 pellets and tritium diffusion, retention and release could improve predictions of tritium-producing burnable absorber rod performance. Given expert-labeled segmented images of irradiated and unirradiated pellets, we trained Deep Convolutional Neural Networks to segment images into defect, grain, and boundary classes. Qualitative microstructural information was calculated from these segmented images to facilitate the comparison of unirradiated and irradiated pellets. We tested modifications to improve the sensitivity of the model, including incorporating meta-data into the model and utilizing uncertainty quantification. The predicted segmentation was similar to the expert-labeled segmentation for most methods of microstructural qualification, including pixel proportion, defect area, and defect density. Overall, the high performance metrics for the best models for both irradiated and unirradiated images shows that utilizing neural network models is a viable alternative to expert-labeled images. </p>
<blockquote>
<p>理解辐射后的LiAlO2颗粒微观结构与氚扩散、滞留和释放之间的关系，有助于预测产氚可燃吸收棒的性能。基于专家标注的辐射和非辐射颗粒的分割图像，我们训练了深度卷积神经网络来对图像进行缺陷、晶粒和边界类别的分割。从分割图像中计算定性微观结构信息，以便比较未辐射和辐射颗粒。我们测试了一些改进模型灵敏度的修改方法，包括将元数据纳入模型和利用不确定性量化。对于大多数微观结构鉴定方法，预测分割与专家标注的分割相似，包括像素比例、缺陷面积和缺陷密度。总体而言，对于辐射和非辐射图像的最佳模型的高性能指标表明，利用神经网络模型是专家标注图像的可行替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14184v1">PDF</a> </p>
<p><strong>Summary</strong><br>     通过理解辐射LiAlO2颗粒微观结构与氚扩散、滞留与释放之间的关系，能够提升预测氚发生可熔烧吸收体杆的性能。采用深度学习卷积神经网络对辐射与非辐射颗粒图像进行分割，包括缺陷、晶粒和边界类别。从分割图像中计算定性微观结构信息，以便对比非辐射与辐射颗粒。测试了改进模型敏感性的方法，包括将元数据纳入模型和利用不确定性量化。预测分割与专家标记分割在多数微观结构鉴定方法上相似，包括像素比例、缺陷面积和缺陷密度。总体而言，最佳模型的高性能表明，利用神经网络模型是替代专家标记图像的可行选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>理解微观结构与氚扩散、滞留和释放的关系对预测氚发生可熔烧吸收体杆性能至关重要。</li>
<li>采用深度学习卷积神经网络对辐射与非辐射颗粒图像进行分割，涉及缺陷、晶粒和边界的识别。</li>
<li>从分割图像中提取定性微观结构信息，用于对比非辐射和辐射颗粒。</li>
<li>测试了改进模型的方法，包括纳入元数据和利用不确定性量化。</li>
<li>预测分割与专家标记的分割在多数微观结构鉴定方法上相似。</li>
<li>神经网络模型的高性能表明其是替代专家标记图像的可行选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f1f51db6dcce32bab82abcd113417182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3badb1b89953623748d157b727e8ce9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-442587f08a5ed8aa10085f757e4f6f27.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SegRet-An-Efficient-Design-for-Semantic-Segmentation-with-Retentive-Network"><a href="#SegRet-An-Efficient-Design-for-Semantic-Segmentation-with-Retentive-Network" class="headerlink" title="SegRet: An Efficient Design for Semantic Segmentation with Retentive   Network"></a>SegRet: An Efficient Design for Semantic Segmentation with Retentive   Network</h2><p><strong>Authors:Zhiyuan Li, Yi Chang, Yuan Wu</strong></p>
<p>With the ongoing advancement of autonomous driving technology and intelligent transportation systems, research into semantic segmentation has become increasingly pivotal. Accurate understanding and analysis of real-world scenarios are now essential for these emerging fields. However, traditional semantic segmentation methods often struggle to balance high model accuracy with computational efficiency, particularly in terms of parameter count. To address this challenge, we introduce SegRet, a novel approach that leverages the Retentive Network (RetNet) architecture and integrates a lightweight residual decoder featuring zero-initialization. SegRet exhibits three key characteristics: (1) Lightweight Residual Decoder: We incorporate a zero-initialization layer within the residual network framework, ensuring that the decoder remains computationally efficient while preserving critical information flow; (2) Robust Feature Extraction: Utilizing RetNet as the backbone, our model adeptly extracts hierarchical features from input images, thereby enhancing the depth and breadth of feature representation; (3) Parameter Efficiency: SegRet achieves state-of-the-art performance while significantly reducing the number of parameters, maintaining high accuracy without compromising on computational resources. Empirical evaluations on benchmark datasets such as ADE20K, Cityscapes, and COCO-Stuff10K demonstrate the efficacy of our approach. SegRet delivers impressive results, achieving an mIoU of 52.23% on ADE20K with only 95.81M parameters, 83.36% on Cityscapes, and 46.63% on COCO-Stuff. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/ZhiyuanLi218/segret">https://github.com/ZhiyuanLi218/segret</a>. </p>
<blockquote>
<p>随着自动驾驶技术和智能交通系统的不断发展，语义分割的研究变得越来越重要。准确理解和分析真实场景对这些新兴领域至关重要。然而，传统的语义分割方法往往难以在模型准确性和计算效率之间取得平衡，尤其是在参数计数方面。为了解决这一挑战，我们引入了SegRet，这是一种利用Retentive Network（RetNet）架构的新方法，并集成了一个具有零初始化的轻量级残差解码器。SegRet具有三个关键特点：</p>
</blockquote>
<p>（1）轻量级残差解码器：我们在残差网络框架中引入了一个零初始化层，确保解码器在计算效率的同时保持关键信息的流动；</p>
<p>（2）稳健的特征提取：利用RetNet作为骨干网，我们的模型能够从输入图像中熟练地提取层次化特征，从而增强特征表示的深度和广度；</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14014v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>随着自动驾驶技术与智能交通系统的不断进步，语义分割研究的重要性日益凸显。针对传统语义分割方法难以在模型精度与计算效率之间取得平衡的问题，提出了一种新型方法SegRet。该方法结合Retentive Network（RetNet）架构与轻量级残差解码器，具有零初始化特性。SegRet具备三大特点：计算效率高、特征提取能力强、参数效率高。在ADE20K、Cityscapes和COCO-Stuff等基准数据集上的实证评估证明了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割在自动驾驶与智能交通系统中至关重要。</li>
<li>传统语义分割方法在模型精度与计算效率之间存在平衡难题。</li>
<li>SegRet方法结合RetNet架构与轻量级残差解码器，具备零初始化特性。</li>
<li>SegRet具有计算效率高、特征提取能力强、参数效率高等三大特点。</li>
<li>SegRet在ADE20K、Cityscapes和COCO-Stuff等数据集上表现出卓越性能。</li>
<li>SegRet在ADE20K上实现了52.23%的mIoU，参数仅95.81M。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14014">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-763b12521f9bf79a9f2c33be700bddb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d71637967b769d196e7182f4e6a4036.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c85f7f23380f998949bfd28791e1c86d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5decb4c4b24cab8923da5cc83104e14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34f1a48daac6ef5a935cfd6bb263db39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5921d5afb8f73f9114a922a1ff979d0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e21908b469a204748ffaa8ab859a54c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Remote-Sensing-Semantic-Segmentation-Quality-Assessment-based-on-Vision-Language-Model"><a href="#Remote-Sensing-Semantic-Segmentation-Quality-Assessment-based-on-Vision-Language-Model" class="headerlink" title="Remote Sensing Semantic Segmentation Quality Assessment based on Vision   Language Model"></a>Remote Sensing Semantic Segmentation Quality Assessment based on Vision   Language Model</h2><p><strong>Authors:Huiying Shi, Zhihong Tan, Zhihan Zhang, Hongchen Wei, Yaosi Hu, Yingxue Zhang, Zhenzhong Chen</strong></p>
<p>The complexity of scenes and variations in image quality result in significant variability in the performance of semantic segmentation methods of remote sensing imagery (RSI) in supervised real-world scenarios. This makes the evaluation of semantic segmentation quality in such scenarios an issue to be resolved. However, most of the existing evaluation metrics are developed based on expert-labeled object-level annotations, which are not applicable in such scenarios. To address this issue, we propose RS-SQA, an unsupervised quality assessment model for RSI semantic segmentation based on vision language model (VLM). This framework leverages a pre-trained RS VLM for semantic understanding and utilizes intermediate features from segmentation methods to extract implicit information about segmentation quality. Specifically, we introduce CLIP-RS, a large-scale pre-trained VLM trained with purified text to reduce textual noise and capture robust semantic information in the RS domain. Feature visualizations confirm that CLIP-RS can effectively differentiate between various levels of segmentation quality. Semantic features and low-level segmentation features are effectively integrated through a semantic-guided approach to enhance evaluation accuracy. To further support the development of RS semantic segmentation quality assessment, we present RS-SQED, a dedicated dataset sampled from four major RS semantic segmentation datasets and annotated with segmentation accuracy derived from the inference results of 8 representative segmentation methods. Experimental results on the established dataset demonstrate that RS-SQA significantly outperforms state-of-the-art quality assessment models. This provides essential support for predicting segmentation accuracy and high-quality semantic segmentation interpretation, offering substantial practical value. </p>
<blockquote>
<p>场景复杂度和图像质量的变化导致遥感图像（RSI）语义分割方法在监督现实场景中的性能存在重大差异。这使得在此类场景中评估语义分割质量成为一个需要解决的问题。然而，大多数现有的评估指标都是基于专家标注的对象级注释开发的，不适用于此类场景。为了解决这一问题，我们提出了RS-SQA，这是一种基于视觉语言模型（VLM）的遥感图像语义分割无监督质量评估模型。该框架利用预训练的RS VLM进行语义理解，并利用分割方法的中间特征提取分割质量隐式信息。具体来说，我们引入了CLIP-RS，这是一个用纯净文本大规模预训练的VLM，旨在减少文本噪声，捕获RS领域的稳健语义信息。特征可视化证实，CLIP-RS可以有效地区分不同级别的分割质量。通过语义引导方法有效地整合语义特征和低级分割特征，以提高评估精度。为了进一步支持遥感语义分割质量评估的发展，我们推出了RS-SQED，这是一个从四个主要遥感语义分割数据集中采样而成的专用数据集，并用8种代表性分割方法的推理结果来标注分割精度。在建立的数据集上的实验结果表明，RS-SQA显著优于最新质量评估模型。这为预测分割精度和高质量语义分割解释提供了重要支持，具有巨大的实用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13990v1">PDF</a> 16 pages,6 figures</p>
<p><strong>Summary</strong><br>    本文提出了一个基于视觉语言模型（VLM）的无监督质量评估模型RS-SQA，用于遥感图像（RSI）语义分割的质量评估。该模型利用预训练的RS VLM进行语义理解，并利用分割方法的中间特征提取隐含的分割质量信息。引入CLIP-RS大型预训练VLM，减少文本噪声，捕获RS领域的稳健语义信息。通过语义引导和特征整合提高评估准确性。同时，推出RS-SQED数据集，支持RS语义分割质量评估的发展。实验结果表明，RS-SQA显著优于现有质量评估模型，为预测分割准确性和高质量语义分割解释提供了重要支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感图像语义分割方法在真实世界场景中的性能存在显著变异性，评价语义分割质量成为需要解决的问题。</li>
<li>现有评价度量主要基于专家标注的对象级注解，不适用于真实世界场景。</li>
<li>提出无监督质量评估模型RS-SQA，基于视觉语言模型（VLM）进行遥感图像语义分割的质量评估。</li>
<li>使用预训练的RS VLM进行语义理解，利用分割方法的中间特征提取隐含的分割质量信息。</li>
<li>引入CLIP-RS大型预训练VLM，减少文本噪声，提高语义信息的捕获。</li>
<li>通过语义引导和特征整合，提高评估准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13990">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f873e851313f98f5842611b05ae3bd1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb2988fcea27c57840042578ec7664cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cb1861df062bb97dee60704d6bb4a2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e82b6012e4e6581263e0b70301520bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428e3b746e2a37047ccdd1463ed1a417.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Visible-Thermal-Tiny-Object-Detection-A-Benchmark-Dataset-and-Baselines"><a href="#Visible-Thermal-Tiny-Object-Detection-A-Benchmark-Dataset-and-Baselines" class="headerlink" title="Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines"></a>Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines</h2><p><strong>Authors:Xinyi Ying, Chao Xiao, Ruojing Li, Xu He, Boyang Li, Xu Cao, Zhaoxu Li, Yingqian Wang, Mingyuan Hu, Qingyu Xu, Zaiping Lin, Miao Li, Shilin Zhou, Wei An, Weidong Sheng, Li Liu</strong></p>
<p>Small object detection (SOD) has been a longstanding yet challenging task for decades, with numerous datasets and algorithms being developed. However, they mainly focus on either visible or thermal modality, while visible-thermal (RGBT) bimodality is rarely explored. Although some RGBT datasets have been developed recently, the insufficient quantity, limited category, misaligned images and large target size cannot provide an impartial benchmark to evaluate multi-category visible-thermal small object detection (RGBT SOD) algorithms. In this paper, we build the first large-scale benchmark with high diversity for RGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93K frames and 1.2M manual annotations. RGBT-Tiny contains abundant targets (7 categories) and high-diversity scenes (8 types that cover different illumination and density variations). Note that, over 81% of targets are smaller than 16x16, and we provide paired bounding box annotations with tracking ID to offer an extremely challenging benchmark with wide-range applications, such as RGBT fusion, detection and tracking. In addition, we propose a scale adaptive fitness (SAFit) measure that exhibits high robustness on both small and large targets. The proposed SAFit can provide reasonable performance evaluation and promote detection performance. Based on the proposed RGBT-Tiny dataset and SAFit measure, extensive evaluations have been conducted, including 23 recent state-of-the-art algorithms that cover four different types (i.e., visible generic detection, visible SOD, thermal SOD and RGBT object detection). Project is available at <a target="_blank" rel="noopener" href="https://github.com/XinyiYing/RGBT-Tiny">https://github.com/XinyiYing/RGBT-Tiny</a>. </p>
<blockquote>
<p>小目标检测（SOD）几十年来一直是一项长期且具挑战性的任务，已经开发了许多数据集和算法。然而，它们主要集中在可见光或热模态，而可见光-热（RGBT）双模态很少被探索。尽管最近已经开发了一些RGBT数据集，但由于图像数量不足、类别有限、图像不对齐以及目标尺寸较大等问题，它们无法为多类别可见光-热小目标检测（RGBT SOD）算法提供公正的基准测试。在本文中，我们建立了RGBT-Tiny，这是一个RGBT多目标检测的第一个大规模基准数据集和丰富的工具包（具有高多样性），包含115个配对序列、93K帧和手动标注的近百万数据。RGBT-Tiny包含丰富的目标类别（共七类）和高多样性的场景类型（涵盖不同照明和密度变化的八种类型）。值得注意的是，超过百分之八十一的目标小于十六乘十六像素大小。我们提供了配对的边界框标注以及跟踪ID，以提供极具挑战性的基准测试，具有广泛的应用范围，如RGBT融合、检测和跟踪等。此外，我们提出了一种自适应尺度的拟合度度量标准（SAFit），对于大目标和中等大小的目标都具有高度的鲁棒性。所提出的SAFit能够为性能评估提供合理的依据并促进检测性能的提升。基于提出的RGBT-Tiny数据集和SAFit度量标准，进行了广泛的评估实验，包括涵盖四种不同类型的最新前沿算法共二十三套算法。该项目的源代码和相关数据资源可通过链接<a target="_blank" rel="noopener" href="https://github.com/XinyiYing/RGBT-Tiny%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/XinyiYing/RGBT-Tiny获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14482v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>针对可见光和热成像（RGBT）双模态的小目标检测（SOD）任务，现有数据集存在数量不足、类别有限、图像不对准和目标过大等问题。本文构建了首个大规模、高多样性的RGBT-Tiny数据集，包含115个配对序列、93K帧和120万手动标注。该数据集包含丰富的目标类别（7类）和高多样性的场景类型（覆盖不同的照明和密度变化）。此外，本文提出了尺度自适应拟合（SAFit）度量标准，该标准对小目标和大型目标均表现出高鲁棒性，可合理评估检测性能并提升检测效果。基于RGBT-Tiny数据集和SAFit度量标准，本文评估了涵盖四种类型的最新先进算法共23种。该项目在GitHub上公开访问。</p>
<p><strong>Key Takeaways</strong>:</p>
<p>一、RGBT小目标检测面临的挑战在于数据集稀缺、多样性不足、目标大小和照明变化等问题。<br>二、本文构建了首个大规模RGBT-Tiny数据集，包含丰富的目标类别和高多样性的场景类型。超过81%的目标小于16x16像素大小。<br>三、数据集提供了配对的边界框标注和跟踪ID，适用于RGBT融合、检测和跟踪等多种应用。<br>四、提出了尺度自适应拟合（SAFit）度量标准，该标准对小目标和大型目标都具有鲁棒性，能够合理评估检测性能。<br>五、对四种不同类型的最新先进算法进行了基于RGBT-Tiny数据集和SAFit度量标准的广泛评估。这些算法包括可见通用检测、可见SOD、热SOD和RGBT对象检测等。<br>六、该数据集及研究方法为RGBT小目标检测提供了新的评价标准和推进方向。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14482">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2fe1b3ea629f6c9cdb9a63ac44f89252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f659093b2f0e6b668ca1f78e89cbef63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb02aef868b5cbabcbe11f513c096cd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6e7779d85b50c2a0fc31a5454a9487.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bd9708f3d5afc8e4f0ee2f53f25ced2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d60a8e0ee2d7070e16d528d81673c0a5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-15e67e0c5ed3d31f2504af5082533e69.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech 方向最新论文已更新，请持续关注 Update in 2025-02-22  WavRAG Audio-Integrated Retrieval Augmented Generation for Spoken   Dialogue Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-55a807514980741b462b8353a810dc5e.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-02-22  Vision Foundation Models in Medical Image Analysis Advances and   Challenges
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">13921.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
