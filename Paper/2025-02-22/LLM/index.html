<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  LServe Efficient Long-sequence LLM Serving with Unified Sparse   Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e8ccb21d8d66b9b80f9f55438c682f61.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-22-æ›´æ–°"><a href="#2025-02-22-æ›´æ–°" class="headerlink" title="2025-02-22 æ›´æ–°"></a>2025-02-22 æ›´æ–°</h1><h2 id="LServe-Efficient-Long-sequence-LLM-Serving-with-Unified-Sparse-Attention"><a href="#LServe-Efficient-Long-sequence-LLM-Serving-with-Unified-Sparse-Attention" class="headerlink" title="LServe: Efficient Long-sequence LLM Serving with Unified Sparse   Attention"></a>LServe: Efficient Long-sequence LLM Serving with Unified Sparse   Attention</h2><p><strong>Authors:Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han</strong></p>
<p>Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/omniserve">https://github.com/mit-han-lab/omniserve</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿åºåˆ—æ—¶è¡¨ç°å‡ºäº†æ˜¾è‘—æ½œåŠ›ï¼Œä½†ç”±äºé¢„å¡«å……é˜¶æ®µçš„æ³¨æ„åŠ›è®¡ç®—å…·æœ‰äºŒæ¬¡å¤æ‚æ€§å’Œè§£ç é˜¶æ®µçš„KVç¼“å­˜å ç”¨å¤§é‡å†…å­˜ï¼Œæœ‰æ•ˆåœ°æœåŠ¡è¿™äº›é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LServeç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ··åˆç¨€ç–æ³¨æ„åŠ›åŠ é€Ÿé•¿åºåˆ—LLMæœåŠ¡ã€‚æ­¤æ–¹æ³•å°†é¢„å¡«å……å’Œè§£ç æ³¨æ„åŠ›çš„ä¸åŒç¡¬ä»¶å‹å¥½ç»“æ„åŒ–ç¨€ç–æ¨¡å¼ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€æ¡†æ¶ä¸­ï¼Œå…¶ä¸­å¯¹æ¬¡è¦æ ‡è®°çš„è®¡ç®—æŒ‰å—è·³è¿‡ã€‚LServeè¯æ˜äº†é™æ€å’ŒåŠ¨æ€ç¨€ç–åœ¨é•¿ä¸Šä¸‹æ–‡LLMæ³¨æ„åŠ›ä¸­çš„å…¼å®¹æ€§ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿé€šè¿‡ç»“åˆè¿™äº›ä¼˜åŒ–å®ç°ä¹˜æ³•çš„é€Ÿåº¦æå‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä¸€åŠçš„æ³¨æ„åŠ›å¤´è½¬æ¢ä¸ºé¢„å¡«å……å’Œè§£ç é˜¶æ®µçš„è¿‘ä¼¼å…è´¹æµå¼å¤´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ— è®ºä¸Šä¸‹æ–‡é•¿åº¦å¦‚ä½•ï¼Œåªéœ€ä¿æŒä¸€å®šæ•°é‡çš„KVé¡µé¢å³å¯ä¿æŒé•¿ä¸Šä¸‹æ–‡åŠŸèƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæŸ¥è¯¢ä¸­å¿ƒç›¸ä¼¼æ€§çš„åˆ†å±‚KVé¡µé¢é€‰æ‹©ç­–ç•¥ï¼ŒåŠ¨æ€åˆ é™¤KVé¡µé¢ã€‚å¹³å‡è€Œè¨€ï¼ŒLServeç›¸å¯¹äºvLLMåŠ é€Ÿäº†LLMçš„é¢„å¡«å……é«˜è¾¾2.9å€ï¼Œè§£ç åŠ é€Ÿäº†1.3-2.1å€ï¼ŒåŒæ—¶ä¿æŒäº†é•¿ä¸Šä¸‹æ–‡çš„å‡†ç¡®æ€§ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/omniserve%E4%B8%8A%E3%80%82">https://github.com/mit-han-lab/omniserveä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14866v1">PDF</a> Accepted by MLSys 2025. Code available at:   <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/omniserve">https://github.com/mit-han-lab/omniserve</a></p>
<p><strong>Summary</strong></p>
<p>LLMå¤„ç†é•¿åºåˆ—æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸ºæ­¤æå‡ºäº†LServeç³»ç»Ÿï¼Œé€šè¿‡æ··åˆç¨€ç–æ³¨æ„åŠ›åŠ é€Ÿé•¿åºåˆ—LLMæœåŠ¡ã€‚LServeé‡‡ç”¨ç¡¬ä»¶å‹å¥½ã€ç»“æ„åŒ–çš„ç¨€ç–æ¨¡å¼ï¼Œè·³è¿‡ä¸é‡è¦ç¬¦å·å—è®¡ç®—ï¼Œå®ç°äº†é™æ€å’ŒåŠ¨æ€ç¨€ç–æ€§çš„å…¼å®¹æ€§ã€‚å®ƒèƒ½å®ç°ä¹˜æ³•çš„åŠ é€Ÿæ•ˆæœï¼Œå¹¶æé«˜å†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLServeå¯åŠ å¿«LLMçš„é¢„å¡«å……é€Ÿåº¦æœ€å¤šè¾¾2.9å€ï¼Œè§£ç é€Ÿåº¦æœ€å¤šè¾¾1.3è‡³2å€ï¼ŒåŒæ—¶ä¿æŒé•¿ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬æ³¨æ„åŠ›é¢„å¡«å……é˜¶æ®µçš„äºŒæ¬¡è®¡ç®—å¤æ‚æ€§å’Œè§£ç é˜¶æ®µKVç¼“å­˜çš„å¤§å†…å­˜å ç”¨ã€‚</li>
<li>LServeç³»ç»Ÿé€šè¿‡æ··åˆç¨€ç–æ³¨æ„åŠ›è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°é«˜æ•ˆçš„é•¿åºåˆ—LLMæœåŠ¡ã€‚</li>
<li>LServeç»“åˆäº†é™æ€å’ŒåŠ¨æ€ç¨€ç–æ€§ï¼Œé‡‡ç”¨ç¡¬ä»¶å‹å¥½ã€ç»“æ„åŒ–çš„ç¨€ç–æ¨¡å¼è¿›è¡Œè®¡ç®—ä¼˜åŒ–ã€‚</li>
<li>LServeé€šè¿‡å°†éƒ¨åˆ†æ³¨æ„åŠ›å¤´è½¬æ¢ä¸ºå‡ ä¹å…è´¹çš„æµå¤´ï¼Œå®ç°äº†åŠ é€Ÿæ•ˆæœã€‚</li>
<li>LServeå‘ç°ä¿æŒé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ‰€éœ€çš„KVé¡µé¢æ•°é‡æ˜¯æ’å®šçš„ï¼Œæ— è®ºä¸Šä¸‹æ–‡é•¿åº¦å¦‚ä½•ã€‚</li>
<li>LServeè®¾è®¡äº†ä¸€ç§åŸºäºæŸ¥è¯¢ä¸­å¿ƒç›¸ä¼¼æ€§çš„åˆ†å±‚KVé¡µé¢é€‰æ‹©ç­–ç•¥ï¼ŒåŠ¨æ€åˆ é™¤KVé¡µé¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7ce65a1e944cfb7b1a5007991155fcf4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b8987f102a317ac60b996e6f97ad3c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d6bebc5f6d583bf6529aa16a811aeed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-967d623055c1c9dc038fff746750a648.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93b0b5dc30ec1790006926ff567398d4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Aligning-LLMs-to-Ask-Good-Questions-A-Case-Study-in-Clinical-Reasoning"><a href="#Aligning-LLMs-to-Ask-Good-Questions-A-Case-Study-in-Clinical-Reasoning" class="headerlink" title="Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning"></a>Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning</h2><p><strong>Authors:Shuyue Stella Li, Jimin Mun, Faeze Brahman, Jonathan S. Ilgen, Yulia Tsvetkov, Maarten Sap</strong></p>
<p>Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decisionmaking. We present ALFA, a framework that improves LLM question-asking by (i) decomposing the notion of a â€œgoodâ€ question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ç¡®å®šæƒ…å†µä¸‹å¾€å¾€æ— æ³•æå‡ºæœ‰æ•ˆé—®é¢˜ï¼Œä½¿å…¶åœ¨å†³ç­–ä¸­éœ€è¦ä¸»åŠ¨æ”¶é›†ä¿¡æ¯çš„é¢†åŸŸä¸å¯é ã€‚æˆ‘ä»¬æå‡ºäº†ALFAæ¡†æ¶ï¼Œå®ƒé€šè¿‡ï¼ˆiï¼‰å°†â€œå¥½é—®é¢˜â€çš„æ¦‚å¿µåˆ†è§£ä¸ºä¸€ç³»åˆ—åŸºäºç†è®ºçš„å±æ€§ï¼ˆä¾‹å¦‚æ¸…æ™°åº¦ã€ç›¸å…³æ€§ç­‰ï¼‰ï¼Œï¼ˆiiï¼‰å¯æ§åœ°åˆæˆç‰¹å®šå±æ€§çš„é—®é¢˜å˜ä½“ï¼Œï¼ˆiiiï¼‰é€šè¿‡åŸºäºåå¥½çš„ä¼˜åŒ–æ¥å¯¹é½æ¨¡å‹ï¼Œä»¥æ˜ç¡®å­¦ä¹ åœ¨è¿™äº›ç²¾ç»†å±æ€§ä¸Šæå‡ºæ›´å¥½çš„é—®é¢˜ï¼Œä»è€Œæ”¹è¿›LLMçš„æé—®èƒ½åŠ›ã€‚ä»¥ä¸´åºŠæ¨ç†ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬ä»‹ç»äº†MediQ-AskDocsæ•°æ®é›†ï¼Œå®ƒç”±1.7ä¸‡ä¸ªçœŸå®ä¸–ç•Œä¸´åºŠäº’åŠ¨ç»„æˆï¼Œè¾…ä»¥8ä¸‡ä¸ªç‰¹å®šå±æ€§åå¥½å¯¹çš„åç»­é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªç”¨äºè¯„ä¼°æé—®èƒ½åŠ›çš„æ–°å‹ä¸“å®¶æ³¨é‡Šäº¤äº’å¼åŒ»ç–—é—®ç­”ä»»åŠ¡ã€‚ä¸æœ€æ–°æŠ€æœ¯çš„æŒ‡ä»¤å¾®è°ƒLLMç›¸æ¯”ï¼Œä½¿ç”¨ALFAå¯¹é½çš„æ¨¡å‹åœ¨MediQ-AskDocsä¸Šå°†è¯Šæ–­é”™è¯¯å‡å°‘äº†56.6%ï¼Œé—®é¢˜çº§åˆ«çš„èƒœç‡ä¸º64.4%ï¼Œå¹¶ä¸”å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç»“æ„åŒ–ã€ç²¾ç»†å±æ€§çš„æ˜ç¡®æŒ‡å¯¼æé—®ï¼Œä¸ºæ”¹è¿›LLMæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“å®¶åº”ç”¨é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14860v1">PDF</a> 22 pages, 8 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹æé—®èƒ½åŠ›æœ‰é™ï¼Œè¿™å¯¼è‡´å®ƒä»¬åœ¨éœ€è¦ä¸»åŠ¨æ”¶é›†ä¿¡æ¯çš„å†³ç­–é¢†åŸŸä¸å¯é ã€‚æœ¬æ–‡æå‡ºALFAæ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£â€œå¥½é—®é¢˜â€çš„æ¦‚å¿µã€å¯æ§åœ°åˆæˆç‰¹å®šå±æ€§é—®é¢˜å˜ç§ä»¥åŠé€šè¿‡åå¥½ä¼˜åŒ–æ¥å¯¹é½æ¨¡å‹ï¼Œä»¥æé«˜LLMçš„æé—®èƒ½åŠ›ã€‚ä»¥ä¸´åºŠæ¨ç†ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œä»‹ç»äº†MediQ-AskDocsæ•°æ®é›†å’Œä¸€ç§æ–°çš„ä¸“å®¶æ³¨é‡Šäº¤äº’å¼åŒ»ç–—é—®ç­”ä»»åŠ¡ã€‚ä¸æœ€æ–°æŒ‡ä»¤å¾®è°ƒLLMç›¸æ¯”ï¼Œä½¿ç”¨ALFAå¯¹é½çš„æ¨¡å‹åœ¨MediQ-AskDocsä¸Šå°†è¯Šæ–­é”™è¯¯å‡å°‘äº†56.6%ï¼Œé—®é¢˜çº§åˆ«èƒœç‡ä¸º64.4%ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç»“æ„åŒ–ã€ç²¾ç»†åŒ–çš„å±æ€§æŒ‡å¯¼æé—®ï¼Œå¯ä¸ºæé«˜LLMæä¾›ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸“å®¶åº”ç”¨é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä¸ç¡®å®šæ€§ç¯å¢ƒä¸‹æé—®èƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶åœ¨éœ€è¦ä¸»åŠ¨ä¿¡æ¯æ”¶é›†çš„å†³ç­–é¢†åŸŸã€‚</li>
<li>ALFAæ¡†æ¶é€šè¿‡åˆ†è§£â€œå¥½é—®é¢˜â€å±æ€§ã€åˆæˆç‰¹å®šé—®é¢˜å¹¶ä¼˜åŒ–æ¨¡å‹æ¥æé«˜LLMçš„æé—®èƒ½åŠ›ã€‚</li>
<li>ä»‹ç»äº†MediQ-AskDocsæ•°æ®é›†å’Œæ–°çš„ä¸“å®¶æ³¨é‡Šäº¤äº’å¼åŒ»ç–—é—®ç­”ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°æé—®èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨ALFAå¯¹é½çš„æ¨¡å‹ç›¸æ¯”æœ€æ–°æŒ‡ä»¤å¾®è°ƒLLMï¼Œè¯Šæ–­é”™è¯¯å‡å°‘56.6%ï¼Œé—®é¢˜çº§åˆ«èƒœç‡ä¸º64.4%ã€‚</li>
<li>ALFAæ–¹æ³•å…·æœ‰å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»“æ„åŒ–ã€ç²¾ç»†åŒ–çš„å±æ€§æŒ‡å¯¼æé—®ï¼Œå¯æé«˜LLMçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70679f992068e07eb19124455874dd18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1c086cb06ee6150f95149557b3b7f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-818ca33114bddfbca638c1881b816242.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dab3d19bb6ddfaddffbd4e5b9ff6ddb3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FR-Spec-Accelerating-Large-Vocabulary-Language-Models-via-Frequency-Ranked-Speculative-Sampling"><a href="#FR-Spec-Accelerating-Large-Vocabulary-Language-Models-via-Frequency-Ranked-Speculative-Sampling" class="headerlink" title="FR-Spec: Accelerating Large-Vocabulary Language Models via   Frequency-Ranked Speculative Sampling"></a>FR-Spec: Accelerating Large-Vocabulary Language Models via   Frequency-Ranked Speculative Sampling</h2><p><strong>Authors:Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang, Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jianyong Wang, Zhiyuan Liu, Maosong Sun</strong></p>
<p>Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2. </p>
<blockquote>
<p>æ¨æµ‹æ€§é‡‡æ ·ä½œä¸ºä¸€ç§é‡è¦æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨è‰ç¨¿-éªŒè¯æœºåˆ¶ï¼Œæ¯æ¬¡å‰å‘ä¼ é€’äº§ç”Ÿå¤šä¸ªä»¤ç‰Œï¼Œä»è€ŒåŠ é€Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨å›å½’ç”Ÿæˆè¿‡ç¨‹ã€‚è™½ç„¶æœ€å…ˆè¿›çš„æ¨æµ‹é‡‡æ ·æ–¹æ³•ä»…ä½¿ç”¨ä¸€å±‚å’Œè¯­è¨€å»ºæ¨¡ï¼ˆLMï¼‰å¤´ä½œä¸ºè‰ç¨¿æ¨¡å‹ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„å±‚å‹ç¼©ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤§å‹è¯æ±‡è¡¨LLMï¼ˆå¦‚æ‹¥æœ‰12.8ä¸‡ä»¤ç‰Œçš„Llama-3-8Bï¼‰æ—¶çš„æ•ˆç‡æå‡å¤§å¹…å‡å°‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FR-Specï¼Œè¿™æ˜¯ä¸€ä¸ªé¢‘ç‡æ’åæ¨æµ‹é‡‡æ ·æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–è¯æ±‡ç©ºé—´å‹ç¼©æ¥æ”¹è¿›è‰ç¨¿å€™é€‰é€‰æ‹©ã€‚é€šè¿‡å°†è‰ç¨¿æœç´¢é™åˆ¶åœ¨ä¼˜å…ˆé¢‘ç‡çš„ä»¤ç‰Œå­é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†LM Headè®¡ç®—å¼€é”€å‡å°‘äº†75%ï¼ŒåŒæ—¶ç¡®ä¿æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒçš„ç­‰æ•ˆæ€§ã€‚åœ¨å¤šæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¨æµ‹é‡‡æ ·æ–¹æ³•EAGLE-2ç›¸æ¯”ï¼Œå…¶å¹³å‡åŠ é€Ÿæ•ˆæœæå‡äº†çº¦1.12å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14856v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é‡‡ç”¨é¢‘ç‡ä¼˜å…ˆçš„æŠ•æœºé‡‡æ ·æ¡†æ¶FR-Specï¼Œé€šè¿‡è¯æ±‡ç©ºé—´å‹ç¼©ä¼˜åŒ–è‰æ¡ˆå€™é€‰é€‰æ‹©ï¼Œå‡å°‘LM Headè®¡ç®—å¼€é”€ï¼Œç¡®ä¿æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒç­‰æ•ˆæ€§ï¼Œæå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ•æœºé‡‡æ ·æ˜¯ä¸€ç§åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹çš„é‡è¦æŠ€æœ¯ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æŠ•æœºé‡‡æ ·æ–¹æ³•ä¸»è¦ä½¿ç”¨å•ä¸€å±‚å’Œè¯­è¨€å»ºæ¨¡å¤´ä½œä¸ºè‰æ¡ˆæ¨¡å‹ï¼Œä½†åœ¨å¤„ç†å¤§è¯æ±‡é‡LLMæ—¶æ•ˆç‡å¢ç›Šé™ä½ã€‚</li>
<li>FR-Specæ¡†æ¶é€šè¿‡è¯æ±‡ç©ºé—´å‹ç¼©ä¼˜åŒ–è‰æ¡ˆå€™é€‰é€‰æ‹©ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ã€‚</li>
<li>FR-Specé€šè¿‡çº¦æŸè‰æ¡ˆæœç´¢è‡³é¢‘ç‡ä¼˜å…ˆçš„ä»¤ç‰Œå­é›†ï¼Œå‡å°‘LM Headè®¡ç®—å¼€é”€75%ã€‚</li>
<li>FR-Specç¡®ä¿æœ€ç»ˆè¾“å‡ºåˆ†å¸ƒçš„ç­‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFR-Specç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æŠ•æœºé‡‡æ ·æ–¹æ³•EAGLE-2å¹³å‡æé€Ÿ1.12å€ã€‚</li>
<li>FR-Specæœ‰åŠ©äºè§£å†³å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆæ•ˆç‡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd9715226ec1b6ae7bec540f092b780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38260ce15b741d9f0534644f3442675b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec01bb2f93fe5c0ad7c51e862ed2c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c41d7fa6582cea3a6fabf4c6c5a837ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f966b277f00ca21db87a4540a5d91dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d56552f54f15453d9bd7df5a70c3de5e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt-to-Leaderboard"><a href="#Prompt-to-Leaderboard" class="headerlink" title="Prompt-to-Leaderboard"></a>Prompt-to-Leaderboard</h2><p><strong>Authors:Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N. Angelopoulos, Ion Stoica</strong></p>
<p>Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2Lâ€™s ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the #1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: <a target="_blank" rel="noopener" href="https://github.com/lmarena/p2l">https://github.com/lmarena/p2l</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°é€šå¸¸ä¾èµ–äºå‡†ç¡®åº¦æˆ–äººç±»åå¥½çš„èšåˆæŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ä¼šå¹³å‡è€ƒè™‘ç”¨æˆ·å’Œæç¤ºã€‚è¿™ç§å¹³å‡ä¼šæ©ç›–æ¨¡å‹å’Œæç¤ºç‰¹å®šçš„æ€§èƒ½å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Prompt-to-Leaderboardï¼ˆP2Lï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šæç¤ºç”Ÿæˆæ’è¡Œæ¦œã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è‡ªç„¶è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥æ¥è¾“å‡ºBradley-Terryç³»æ•°å‘é‡ï¼Œç„¶åä½¿ç”¨è¿™äº›ç³»æ•°é¢„æµ‹äººç±»åå¥½æŠ•ç¥¨ã€‚ç»“æœæç¤ºç›¸å…³çš„æ’è¡Œæ¦œå…è®¸è¿›è¡Œæ— ç›‘ç£çš„ç‰¹å®šä»»åŠ¡è¯„ä¼°ã€æŸ¥è¯¢åˆ°æ¨¡å‹çš„æœ€ä¼˜è·¯ç”±ã€ä¸ªæ€§åŒ–ä»¥åŠæ¨¡å‹çš„ä¼˜ç‚¹å’Œç¼ºç‚¹çš„è‡ªåŠ¨è¯„ä¼°ã€‚æ¥è‡ªChatbot Arenaçš„æ•°æ®è¡¨æ˜ï¼Œç›¸å¯¹äºå¹³å‡æ’è¡Œæ¦œï¼ŒP2Lèƒ½æ›´å¥½åœ°æ•æ‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¾®å¦™æ ¼å±€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒP2Lç”Ÿæˆç‰¹å®šæç¤ºè¯„ä¼°çš„èƒ½åŠ›éµå¾ªå¹‚å¾‹ç¼©æ”¾ï¼Œè¿™ä¸åœ¨LLMæœ¬èº«ä¸­æ‰€è§‚å¯Ÿåˆ°çš„ç›¸ä¼¼ã€‚åœ¨2025å¹´1æœˆï¼Œæˆ‘ä»¬åŸºäºè¿™ç§æ–¹æ³•è®­ç»ƒçš„è·¯ç”±å™¨åœ¨Chatbot Arenaæ’è¡Œæ¦œä¸Šè·å¾—äº†ç¬¬ä¸€åã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨GitHubé“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/lmarena/p2l%E3%80%82">https://github.com/lmarena/p2lã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14855v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°çš„ä¸€ç§æ–°æ–¹æ³•â€”â€”Prompt-to-Leaderboardï¼ˆP2Lï¼‰ã€‚ä¼ ç»ŸLLMè¯„ä¼°ä¾èµ–äºå‡†ç¡®æ€§æˆ–äººç±»åå¥½çš„èšåˆæŒ‡æ ‡ï¼Œè¿™ç§æ–¹æ³•å¿½è§†äº†ç”¨æˆ·å’Œæç¤ºå¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚è€ŒP2Låˆ™èƒ½å¤Ÿç”Ÿæˆç‰¹å®šäºæç¤ºçš„æ’è¡Œæ¦œã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è‡ªç„¶è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥æ¥è¾“å‡ºBradley-Terryç³»æ•°å‘é‡ï¼Œé¢„æµ‹äººç±»åå¥½æŠ•ç¥¨ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿçš„æç¤ºä¾èµ–æ’è¡Œæ¦œå¯å®ç°æ— ç›‘ç£çš„ä»»åŠ¡ç‰¹å®šè¯„ä¼°ã€æŸ¥è¯¢çš„æœ€ä¼˜è·¯ç”±ã€ä¸ªæ€§åŒ–ä»¥åŠæ¨¡å‹çš„ä¼˜ç¼ºç‚¹è‡ªåŠ¨è¯„ä¼°ã€‚æ¥è‡ªChatbot Arenaçš„æ•°æ®è¡¨æ˜ï¼Œç›¸è¾ƒäºå¹³å‡æ’è¡Œæ¦œï¼ŒP2Læ›´èƒ½æ•æ‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¾®å¦™å·®å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°P2Lç”Ÿæˆæç¤ºç‰¹å®šè¯„ä¼°çš„èƒ½åŠ›éµå¾ªå¹‚å¾‹ç¼©æ”¾ï¼Œä¸è§‚å¯Ÿåˆ°çš„LLMè‡ªèº«ç±»ä¼¼ã€‚è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŸºäºè¯¥æ–¹æ³•çš„è·¯ç”±å™¨åœ¨Chatbot Arenaæ’è¡Œæ¦œä¸Šè£ç™»æ¦œé¦–ã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¯„ä¼°é€šå¸¸ä½¿ç”¨èšåˆæŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®æ€§å’Œäººç±»åå¥½ï¼Œè¿™å¿½ç•¥äº†ç”¨æˆ·å’Œæç¤ºå¯¹æ¨¡å‹æ€§èƒ½çš„å…·ä½“å½±å“ã€‚</li>
<li>Prompt-to-Leaderboardï¼ˆP2Lï¼‰æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆç‰¹å®šäºæç¤ºçš„æ’è¡Œæ¦œï¼Œä»¥æ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>P2Lé€šè¿‡è®­ç»ƒLLMä»¥è‡ªç„¶è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥æ¥é¢„æµ‹äººç±»åå¥½æŠ•ç¥¨ï¼Œè¾“å‡ºBradley-Terryç³»æ•°å‘é‡ã€‚</li>
<li>P2Lå¯æ”¯æŒæ— ç›‘ç£çš„ä»»åŠ¡ç‰¹å®šè¯„ä¼°ã€æŸ¥è¯¢çš„æœ€ä¼˜è·¯ç”±ã€ä¸ªæ€§åŒ–ä»¥åŠæ¨¡å‹çš„è‡ªåŠ¨è¯„ä¼°ã€‚</li>
<li>ä¸å¹³å‡æ’è¡Œæ¦œç›¸æ¯”ï¼ŒP2Læ›´èƒ½æ•æ‰è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ç»†å¾®å·®å¼‚ã€‚</li>
<li>P2Lç”Ÿæˆæç¤ºç‰¹å®šè¯„ä¼°çš„èƒ½åŠ›éµå¾ªå¹‚å¾‹ç¼©æ”¾ï¼Œä¸LLMçš„ç¼©æ”¾ç±»ä¼¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-952119cac1b6f82dc2fd2ff4949a3aab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4609397af92a4ee436a4ee26c2e773.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GATE-Graph-based-Adaptive-Tool-Evolution-Across-Diverse-Tasks"><a href="#GATE-Graph-based-Adaptive-Tool-Evolution-Across-Diverse-Tasks" class="headerlink" title="GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks"></a>GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks</h2><p><strong>Authors:Jianwen Luo, Yiming Huang, Jinxiang Meng, Fangyu Lei, Shizhu He, Xiao Liu, Shanshan Jiang, Bin Dong, Jun Zhao, Kang Liu</strong></p>
<p>Large Language Models (LLMs) have shown great promise in tool-making, yet existing frameworks often struggle to efficiently construct reliable toolsets and are limited to single-task settings. To address these challenges, we propose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that dynamically constructs and evolves a hierarchical graph of reusable tools across multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft), agent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date, TabMWP). Our results show that GATE achieves up to 4.3x faster milestone completion in Minecraft compared to the previous SOTA, and provides an average improvement of 9.23% over existing tool-making methods in code generation tasks and 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution, balancing tool quantity, complexity, and functionality while maintaining high efficiency. Code and data are available at \url{<a target="_blank" rel="noopener" href="https://github.com/ayanami2003/GATE%7D">https://github.com/ayanami2003/GATE}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å·¥å…·åˆ¶ä½œæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç„¶è€Œç°æœ‰çš„æ¡†æ¶å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°æ„å»ºå¯é çš„å·¥å…·é›†ï¼Œå¹¶ä¸”ä»…é™äºå•ä»»åŠ¡ç¯å¢ƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›¾çš„è‡ªé€‚åº”å·¥å…·è¿›åŒ–ï¼ˆGATEï¼‰è¿™ä¸€è‡ªé€‚åº”æ¡†æ¶ï¼Œå®ƒèƒ½åŠ¨æ€æ„å»ºå¹¶åœ¨å¤šä¸ªåœºæ™¯ä¸­è¿›åŒ–å¯é‡å¤ä½¿ç”¨çš„å·¥å…·çš„åˆ†å±‚æ¬¡å›¾ã€‚æˆ‘ä»¬åœ¨å¼€æ”¾å¼ä»»åŠ¡ï¼ˆMinecraftï¼‰ã€åŸºäºä»£ç†çš„ä»»åŠ¡ï¼ˆTextCraftã€DABenchï¼‰å’Œä»£ç ç”Ÿæˆä»»åŠ¡ï¼ˆMATHã€Dateã€TabMWPï¼‰ä¸Šè¯„ä¼°äº†GATEã€‚ç»“æœè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æœ€ä½³æŠ€æœ¯ç›¸æ¯”ï¼ŒGATEåœ¨Minecraftä¸­çš„é‡Œç¨‹ç¢‘å®Œæˆé€Ÿåº¦æœ€å¿«å¯è¾¾4.3å€ï¼Œåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡å’Œä»£ç†ä»»åŠ¡ä¸Šçš„å¹³å‡æ”¹è¿›åˆ†åˆ«ä¸º9.23%å’Œ10.03%ã€‚GATEå±•ç¤ºäº†è‡ªé€‚åº”è¿›åŒ–çš„åŠ›é‡ï¼Œåœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶ï¼Œå¹³è¡¡äº†å·¥å…·çš„æ•°é‡ã€å¤æ‚åº¦å’ŒåŠŸèƒ½ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ayanami2003/GATE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ayanami2003/GATEä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14848v1">PDF</a> 8 pages of main text, 38 pages of appendices</p>
<p><strong>Summary</strong></p>
<p>LLMå·¥å…·åˆ¶é€ ä¸­å­˜åœ¨æ•ˆç‡ä¸å¯é æ€§çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ¡†æ¶éš¾ä»¥é€‚åº”å¤šä»»åŠ¡åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºåŸºäºå›¾è‡ªé€‚åº”è¿›åŒ–ï¼ˆGATEï¼‰çš„æ¡†æ¶ï¼ŒåŠ¨æ€æ„å»ºä¸è¿›åŒ–å±‚æ¬¡åŒ–å·¥å…·å›¾ï¼Œé€‚ç”¨äºå¤šç§åœºæ™¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGATEåœ¨å¼€æ”¾ä»»åŠ¡ï¼ˆå¦‚Minecraftï¼‰ä¸­å®Œæˆé‡Œç¨‹ç¢‘ä»»åŠ¡çš„é€Ÿåº¦æœ€å¿«å¯è¾¾ä¹‹å‰çš„æœ€ä½³è§£çš„4.3å€ï¼Œåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡å’Œä»£ç†ä»»åŠ¡ä¸­å¹³å‡æ”¹è¿›äº†ç°æœ‰å·¥å…·åˆ¶é€ æ–¹æ³•çš„9.23%å’Œ10.03%ã€‚GATEå±•ç°äº†è‡ªé€‚åº”è¿›åŒ–çš„åŠ›é‡ï¼Œåœ¨å¹³è¡¡å·¥å…·æ•°é‡ã€å¤æ‚æ€§å’ŒåŠŸèƒ½æ€§çš„åŒæ—¶ï¼Œç»´æŒäº†é«˜æ•ˆç‡ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ayanami2003/GATE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ayanami2003/GATEæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå·¥å…·åˆ¶é€ é¢ä¸´æ•ˆç‡å’Œå¯é æ€§çš„æŒ‘æˆ˜ï¼Œéœ€è¦èƒ½å¤Ÿé€‚åº”å¤šä»»åŠ¡åœºæ™¯çš„æ¡†æ¶ã€‚</li>
<li>GATEæ¡†æ¶è¢«æå‡ºä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤ŸåŠ¨æ€æ„å»ºå’Œè¿›åŒ–å±‚æ¬¡åŒ–çš„å·¥å…·å›¾ã€‚</li>
<li>GATEåœ¨å¼€æ”¾ä»»åŠ¡ï¼ˆå¦‚Minecraftï¼‰ä¸­çš„æ€§èƒ½æ˜¾è‘—ï¼Œå®Œæˆé‡Œç¨‹ç¢‘ä»»åŠ¡çš„é€Ÿåº¦å¤§å¹…æå‡ã€‚</li>
<li>GATEåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡å’Œä»£ç†ä»»åŠ¡ä¸­ä¹Ÿæœ‰è‰¯å¥½çš„è¡¨ç°ï¼Œå¯¹ç°æœ‰å·¥å…·åˆ¶é€ æ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>GATEé€šè¿‡å¹³è¡¡å·¥å…·æ•°é‡ã€å¤æ‚æ€§å’ŒåŠŸèƒ½æ€§ï¼Œå±•ç¤ºäº†è‡ªé€‚åº”è¿›åŒ–çš„ä¼˜åŠ¿ã€‚</li>
<li>GATEæ¡†æ¶å…·æœ‰é«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿåº”å¯¹å¤æ‚çš„ä»»åŠ¡éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccd6f4556fe91d98b6037d48eb9d484e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f214ee4ea704d573e8b1dee724d5f0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e5d990477e1d12410f654976722fbe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a27ff18a8f6ba2df652b06b9fccd123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ddafe3fdeb35f694fb7db7f96c1f051.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Scaling-Text-Rich-Image-Understanding-via-Code-Guided-Synthetic-Multimodal-Data-Generation"><a href="#Scaling-Text-Rich-Image-Understanding-via-Code-Guided-Synthetic-Multimodal-Data-Generation" class="headerlink" title="Scaling Text-Rich Image Understanding via Code-Guided Synthetic   Multimodal Data Generation"></a>Scaling Text-Rich Image Understanding via Code-Guided Synthetic   Multimodal Data Generation</h2><p><strong>Authors:Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark</strong></p>
<p>Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., â€œnutrition fact labelsâ€), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments. </p>
<blockquote>
<p>å¤„ç†åŒ…å«å›¾è¡¨å’Œæ–‡æ¡£ç­‰ä¸°å¯Œæ–‡æœ¬çš„å›¾ç‰‡æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å…³é”®åº”ç”¨ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤šæ ·åŒ–çš„ä¸°å¯Œæ–‡æœ¬è§†è§‰è¯­è¨€æ•°æ®ï¼ŒVLMåœ¨è¿™äº›é¢†åŸŸç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoSynæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨çº¯æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼–ç èƒ½åŠ›æ¥è‡ªåŠ¨åˆ›å»ºåˆæˆä¸°å¯Œæ–‡æœ¬çš„å¤šæ¨¡æ€æ•°æ®ã€‚ç»™å®šæè¿°ç›®æ ‡åŸŸçš„è¾“å…¥æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œâ€œè¥å…»äº‹å®æ ‡ç­¾â€ï¼‰ï¼ŒCoSynæç¤ºLLMç”Ÿæˆç”¨äºå‘ˆç°åˆæˆå›¾åƒçš„ä»£ç ï¼ˆPythonã€HTMLã€LaTeXç­‰ï¼‰ã€‚CoSynä»¥åº•å±‚ä»£ç ä½œä¸ºåˆæˆå›¾åƒçš„æ–‡æœ¬è¡¨ç¤ºï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦è°ƒæ•´æ•°æ®ï¼Œè¿™åŒæ ·ä¾èµ–äºçº¯æ–‡æœ¬LLMã€‚ä½¿ç”¨CoSynï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«40ä¸‡å¼ å›¾ç‰‡å’Œ270ä¸‡è¡Œè§†è§‰è¯­è¨€æ•™å­¦è°ƒæ•´æ•°æ®çš„æ•°æ®é›†ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬åˆæˆçš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨åŒ…æ‹¬Llama 3.2åœ¨å†…çš„å¼€æºç«äº‰æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”è¶…è¶Šäº†ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4Vå’ŒGemini 1.5 Flashã€‚æ­¤å¤–ï¼ŒCoSynå¯ä»¥ç”ŸæˆåˆæˆæŒ‡å‘æ•°æ®ï¼Œä½¿VLMèƒ½å¤Ÿåœ¨è¾“å…¥å›¾ç‰‡ä¸­å®šä½ä¿¡æ¯ï¼Œå±•ç¤ºäº†å…¶å¼€å‘èƒ½åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿è¡Œçš„å¤šåª’ä½“ä»£ç†çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14846v1">PDF</a> 20 pages, 19 figures, 9 tables, website:   <a target="_blank" rel="noopener" href="https://yueyang1996.github.io/cosyn/">https://yueyang1996.github.io/cosyn/</a></p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¤„ç†å›¾åƒä¸­ä¸°å¯Œçš„æ–‡æœ¬ï¼ˆå¦‚å›¾è¡¨å’Œæ–‡æ¡£ï¼‰æ˜¯ä¸€å¤§å…³é”®åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤šæ ·çš„æ–‡æœ¬ä¸°å¯Œè§†è§‰è¯­è¨€æ•°æ®ï¼ŒVLMså¾€å¾€åœ¨è¿™äº›é¢†åŸŸè¡¨ç°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºCoSynæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä»…æ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç¼–ç èƒ½åŠ›ï¼Œè‡ªåŠ¨åˆ›å»ºåˆæˆæ–‡æœ¬ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ã€‚ç»™å®šæè¿°ç›®æ ‡åŸŸçš„è¾“å…¥æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œâ€œè¥å…»äº‹å®æ ‡ç­¾â€ï¼‰ï¼ŒCoSynæç¤ºLLMç”Ÿæˆä»£ç ï¼ˆPythonã€HTMLã€LaTeXç­‰ï¼‰ä»¥å‘ˆç°åˆæˆå›¾åƒã€‚åŸºäºåº•å±‚ä»£ç ä½œä¸ºåˆæˆå›¾åƒçš„æ–‡æœ¬è¡¨ç¤ºï¼ŒCoSynå¯ä»¥ç”Ÿæˆé«˜è´¨é‡æŒ‡ä»¤è®­ç»ƒæ•°æ®ï¼ŒåŒæ ·ä¾èµ–äºæ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡CoSynæ„å»ºçš„æ•°æ®é›†åŒ…å«å››åä¸‡å¼ å›¾åƒå’Œä¸¤ç™¾ä¸ƒåä¸‡è¡Œè§†è§‰è¯­è¨€æŒ‡ä»¤è®­ç»ƒæ•°æ®ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç»è¿‡åˆæˆæ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨ç«äº‰æ€§å¼€æºæ¨¡å‹ä¸­è¡¨ç°å¤„äºé¢†å…ˆåœ°ä½ï¼ŒåŒ…æ‹¬Llama 3.2åœ¨å†…ï¼Œå¹¶ä¸”è¶…è¿‡äº†ä¸“æœ‰æ¨¡å‹å¦‚GPT-4Vå’ŒGemini 1.5 Flashã€‚æ­¤å¤–ï¼ŒCoSynè¿˜èƒ½ç”ŸæˆåˆæˆæŒ‡å‘æ•°æ®ï¼Œä½¿VLMsèƒ½åœ¨è¾“å…¥å›¾åƒä¸­æ‰¾åˆ°ä¿¡æ¯ä½ç½®å…³ç³»ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼€å‘èƒ½åœ¨ç°å®ä¸–ç•Œä¸­è¡ŒåŠ¨çš„å¤šåª’ä½“æ™ºèƒ½ä½“çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>CoSynæ¡†æ¶åˆ©ç”¨æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¼–ç èƒ½åŠ›è‡ªåŠ¨ç”Ÿæˆåˆæˆæ–‡æœ¬ä¸°å¯Œçš„å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>é€šè¿‡æè¿°ç›®æ ‡åŸŸçš„è¾“å…¥æ–‡æœ¬ç”Ÿæˆåˆæˆå›¾åƒçš„ä»£ç è¡¨ç¤ºã€‚</li>
<li>CoSynåˆ›å»ºçš„æ•°æ®é›†åŒ…å«å››åä¸‡å¼ å›¾åƒå’Œå¤§é‡è§†è§‰è¯­è¨€æŒ‡ä»¤è®­ç»ƒæ•°æ®ã€‚</li>
<li>å®éªŒè¯æ˜åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç«äº‰æ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-782e5d0f049b19d50ac11f2edc51107c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1902c9dbb01be553738f672099d5797a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38f709a3c992b0c536b5cdd702df3c0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb961054f6c410cdcb6b2093cf03de13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3b9d9333ee0dd58f88aec6d2e1ec5bf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Middle-Layer-Representation-Alignment-for-Cross-Lingual-Transfer-in-Fine-Tuned-LLMs"><a href="#Middle-Layer-Representation-Alignment-for-Cross-Lingual-Transfer-in-Fine-Tuned-LLMs" class="headerlink" title="Middle-Layer Representation Alignment for Cross-Lingual Transfer in   Fine-Tuned LLMs"></a>Middle-Layer Representation Alignment for Cross-Lingual Transfer in   Fine-Tuned LLMs</h2><p><strong>Authors:Danni Liu, Jan Niehues</strong></p>
<p>While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (<a target="_blank" rel="noopener" href="https://github.com/dannigt/mid-align">https://github.com/dannigt/mid-align</a>). </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¾®è°ƒåœ¨ç‰¹å®šä»»åŠ¡åº”ç”¨ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤šç§è¯­è¨€é—´æ‰©å±•è¿™äº›ä¼˜åŠ¿å¯¹äºå¹¿æ³›çš„å¯è®¿é—®æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆçš„è·¨è¯­è¨€è¿ç§»å—åˆ°è¯­è¨€é—´å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½å·®è·ä»¥åŠè®¸å¤šè¯­è¨€ä¸­å¾®è°ƒæ•°æ®ç¨€ç¼ºçš„é˜»ç¢ã€‚é€šè¿‡å¯¹æ¥è‡ªè¶…è¿‡1000ç§è¯­è¨€å¯¹çš„LLMå†…éƒ¨è¡¨ç¤ºçš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¸­å±‚å…·æœ‰æœ€å¼ºçš„è·¨è¯­è¨€å¯¹é½æ½œåŠ›ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé›†æˆåˆ°ç‰¹å®šä»»åŠ¡è®­ç»ƒä¸­çš„ä¸­å±‚å¯¹é½ç›®æ ‡ã€‚æˆ‘ä»¬åœ¨æ’æ§½å¡«å……ã€æœºå™¨ç¿»è¯‘å’Œç»“æ„åŒ–æ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„å®éªŒæ˜¾ç¤ºï¼Œåœ¨è·¨è¯­è¨€è¿ç§»æ–¹é¢ï¼Œå°¤å…¶æ˜¯åˆ°èµ„æºè¾ƒå°‘çš„è¯­è¨€æ–¹é¢ï¼Œéƒ½å–å¾—äº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚è¯¥æ–¹æ³•å¯¹äºé€‰æ‹©çš„å¯¹é½è¯­è¨€å…·æœ‰ç¨³å¥æ€§ï¼Œå¹¶ä¸”å¯ä»¥æ¨å¹¿åˆ°å¯¹é½è¿‡ç¨‹ä¸­æœªè§çš„è¯­è¨€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¯ä»¥åˆå¹¶å•ç‹¬è®­ç»ƒçš„å¯¹æ¥æ¨¡å—ä¸ç°æœ‰çš„ç‰¹å®šä»»åŠ¡æ¨¡å—ï¼Œä»è€Œæé«˜è·¨è¯­è¨€çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢çš„å†è®­ç»ƒã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/dannigt/mid-align%EF%BC%89%E3%80%82">https://github.com/dannigt/mid-alignï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14830v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¾®è°ƒåœ¨ç‰¹å®šä»»åŠ¡åº”ç”¨ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†è¦åœ¨å¤šç§è¯­è¨€é—´å®ç°å¹¿æ³›çš„å¯è®¿é—®æ€§ï¼Œä»éœ€å°†è¿™äº›ç›Šå¤„æ‰©å±•åˆ°å¤šç§è¯­è¨€ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆè·¨è¯­è¨€è¿ç§»å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­è¨€é—´æ€§èƒ½å·®å¼‚åŠè®¸å¤šè¯­è¨€å¾®è°ƒæ•°æ®ç¨€ç¼ºæ€§çš„é˜»ç¢ã€‚é€šè¿‡åˆ†æè¶…è¿‡1000ç§è¯­è¨€å¯¹çš„å†…éƒ¨è¡¨ç¤ºï¼Œæˆ‘ä»¬å‘ç°ä¸­å±‚å…·æœ‰æœ€å¼ºçš„è·¨è¯­è¨€å¯¹é½æ½œåŠ›ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬æå‡ºé›†æˆåˆ°ä»»åŠ¡ç‰¹å®šè®­ç»ƒä¸­çš„ä¸­å±‚å¯¹é½ç›®æ ‡ã€‚æˆ‘ä»¬åœ¨æ§½å¡«å……ã€æœºå™¨ç¿»è¯‘å’Œç»“æ„åŒ–æ–‡æœ¬ç”Ÿæˆä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œè·¨è¯­è¨€è¿ç§»èƒ½åŠ›å¾—åˆ°äº†ä¸€è‡´æå‡ï¼Œå°¤å…¶æ˜¯å¯¹ä½èµ„æºè¯­è¨€çš„è¿ç§»ã€‚è¯¥æ–¹æ³•å¯¹é€‰æ‹©çš„å¯¹é½è¯­è¨€å…·æœ‰ç¨³å¥æ€§ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°å¯¹é½æ—¶æœªè§çš„è¯­è¨€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¯ä»¥åˆå¹¶å•ç‹¬è®­ç»ƒçš„å¯¹é½æ¨¡å—ä¸ç°æœ‰çš„ä»»åŠ¡ç‰¹å®šæ¨¡å—ï¼Œæ— éœ€å…¨é¢é‡æ–°è®­ç»ƒå³å¯æé«˜è·¨è¯­è¨€èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­é€šè¿‡å¾®è°ƒå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œè·¨è¯­è¨€æ‰©å±•å¯¹å®ç°å¹¿æ³›å¯è®¿é—®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>è·¨è¯­è¨€è¿ç§»é¢ä¸´å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­è¨€é—´æ€§èƒ½å·®å¼‚å’Œå¾®è°ƒæ•°æ®ç¨€ç¼ºæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡åˆ†æè¶…è¿‡1000ç§è¯­è¨€å¯¹çš„å†…éƒ¨è¡¨ç¤ºï¼Œå‘ç°ä¸­å±‚å…·æœ‰æœ€å¼ºçš„è·¨è¯­è¨€å¯¹é½æ½œåŠ›ã€‚</li>
<li>æå‡ºé›†æˆåˆ°ä»»åŠ¡ç‰¹å®šè®­ç»ƒä¸­çš„ä¸­å±‚å¯¹é½ç›®æ ‡ï¼Œå®éªŒè¯æ˜åœ¨å¤šç§ä»»åŠ¡ä¸Šæå‡äº†è·¨è¯­è¨€è¿ç§»èƒ½åŠ›ã€‚</li>
<li>å¯¹é½æ–¹æ³•å¯¹é€‰æ‹©çš„å¯¹é½è¯­è¨€å…·æœ‰ç¨³å¥æ€§ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°æœªè§çš„è¯­è¨€ã€‚</li>
<li>å•ç‹¬è®­ç»ƒçš„å¯¹é½æ¨¡å—å¯ä»¥ä¸ä»»åŠ¡ç‰¹å®šæ¨¡å—åˆå¹¶ï¼Œæ— éœ€å…¨é¢é‡æ–°è®­ç»ƒå³å¯æå‡è·¨è¯­è¨€èƒ½åŠ›ã€‚</li>
<li>å…¬å¼€å¯ç”¨ä»£ç ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/dannigt/mid-align%EF%BC%89%E3%80%82">https://github.com/dannigt/mid-alignï¼‰ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5bc90dd9168bb6e3f9f9e2ecb501c406.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-973f2a6164dd518ea22810c71e322078.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fffb7cddd68e41cf4360e1165fbd35ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f243279a0739c48f24e7a3144d995084.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9c6039919d6896ddcd21e696a75082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6adb857845d594bb0489814ec7498ab4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20b361a68699910de801e08b8e46d127.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fundamental-Limitations-in-Defending-LLM-Finetuning-APIs"><a href="#Fundamental-Limitations-in-Defending-LLM-Finetuning-APIs" class="headerlink" title="Fundamental Limitations in Defending LLM Finetuning APIs"></a>Fundamental Limitations in Defending LLM Finetuning APIs</h2><p><strong>Authors:Xander Davies, Eric Winsor, Tomek Korbak, Alexandra Souly, Robert Kirk, Christian Schroeder de Witt, Yarin Gal</strong></p>
<p>LLM developers have imposed technical interventions to prevent fine-tuning misuse attacks, attacks where adversaries evade safeguards by fine-tuning the model using a public API. Previous work has established several successful attacks against specific fine-tuning API defences. In this work, we show that defences of fine-tuning APIs that seek to detect individual harmful training or inference samples (â€˜pointwiseâ€™ detection) are fundamentally limited in their ability to prevent fine-tuning attacks. We construct â€˜pointwise-undetectableâ€™ attacks that repurpose entropy in benign model outputs (e.g. semantic or syntactic variations) to covertly transmit dangerous knowledge. Our attacks are composed solely of unsuspicious benign samples that can be collected from the model before fine-tuning, meaning training and inference samples are all individually benign and low-perplexity. We test our attacks against the OpenAI fine-tuning API, finding they succeed in eliciting answers to harmful multiple-choice questions, and that they evade an enhanced monitoring system we design that successfully detects other fine-tuning attacks. We encourage the community to develop defences that tackle the fundamental limitations we uncover in pointwise fine-tuning API defences. </p>
<blockquote>
<p>LLMå¼€å‘è€…å·²ç»é‡‡å–äº†æŠ€æœ¯å¹²é¢„æªæ–½ï¼Œä»¥é˜²æ­¢å¾®è°ƒæ»¥ç”¨æ”»å‡»ï¼Œå³å¯¹æ‰‹é€šè¿‡ä½¿ç”¨å…¬å…±APIæ¥å¾®è°ƒæ¨¡å‹ä»¥è§„é¿å®‰å…¨ä¿æŠ¤ã€‚ä¹‹å‰çš„å·¥ä½œå·²ç»é’ˆå¯¹ç‰¹å®šçš„å¾®è°ƒAPIé˜²å¾¡æˆåŠŸå®æ–½äº†å‡ æ¬¡æ”»å‡»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œè¯•å›¾æ£€æµ‹ä¸ªåˆ«æœ‰å®³è®­ç»ƒæˆ–æ¨ç†æ ·æœ¬ï¼ˆå³â€œç‚¹çŠ¶â€æ£€æµ‹ï¼‰çš„å¾®è°ƒAPIé˜²å¾¡æªæ–½ï¼Œåœ¨é˜²æ­¢å¾®è°ƒæ”»å‡»æ–¹é¢çš„èƒ½åŠ›å­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚æˆ‘ä»¬æ„å»ºäº†â€œç‚¹çŠ¶ä¸å¯æ£€æµ‹â€çš„æ”»å‡»ï¼Œå®ƒé‡æ–°åˆ©ç”¨è‰¯æ€§æ¨¡å‹è¾“å‡ºä¸­çš„ç†µï¼ˆä¾‹å¦‚è¯­ä¹‰æˆ–å¥æ³•å˜åŒ–ï¼‰æ¥æš—ä¸­ä¼ è¾“å±é™©çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ”»å‡»ä»…ç”±åœ¨å¾®è°ƒæ¨¡å‹ä¹‹å‰å°±å¯ä»¥æ”¶é›†çš„æ— å¯ç–‘è‰¯æ€§æ ·æœ¬ç»„æˆï¼Œè¿™æ„å‘³ç€è®­ç»ƒå’Œæ¨ç†æ ·æœ¬éƒ½æ˜¯ä¸ªåˆ«çš„è‰¯æ€§ä¸”ä½å›°æƒ‘åº¦ã€‚æˆ‘ä»¬é’ˆå¯¹OpenAIçš„å¾®è°ƒAPIæµ‹è¯•äº†æˆ‘ä»¬çš„æ”»å‡»ï¼Œå‘ç°å®ƒä»¬åœ¨å¼•å‘å¯¹æœ‰å®³å¤šé¡¹é€‰æ‹©é—®é¢˜çš„å›ç­”æ–¹é¢å–å¾—æˆåŠŸï¼Œå¹¶ä¸”ç»•è¿‡äº†æˆ‘ä»¬è®¾è®¡çš„å¢å¼ºç›‘æ§ç³»ç»Ÿï¼Œè¯¥ç›‘æ§ç³»ç»ŸæˆåŠŸæ£€æµ‹åˆ°äº†å…¶ä»–å¾®è°ƒæ”»å‡»ã€‚æˆ‘ä»¬é¼“åŠ±ç¤¾åŒºå¼€å‘è§£å†³æˆ‘ä»¬å‘ç°çš„ç‚¹å¯¹ç‚¹å¾®è°ƒAPIé˜²å¾¡ä¸­çš„æ ¹æœ¬æ€§é™åˆ¶çš„é˜²å¾¡æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMå¼€å‘å›¢é˜Ÿä¸ºé˜²æ­¢æ»¥ç”¨æ”»å‡»é‡‡å–äº†æŠ€æœ¯å¹²é¢„æªæ–½ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é€šè¿‡å…¬å…±APIå¾®è°ƒæ¨¡å‹æ¥è§„é¿ä¿éšœæªæ–½çš„å¯¹æ‰‹æ”»å‡»ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶æŒ‡å‡ºï¼Œè¯•å›¾æ£€æµ‹ä¸ªåˆ«æœ‰å®³è®­ç»ƒæˆ–æ¨ç†æ ·æœ¬ï¼ˆâ€œç‚¹çŠ¶â€æ£€æµ‹ï¼‰çš„å¾®è°ƒAPIé˜²å¾¡æªæ–½åœ¨é˜²æ­¢å¾®è°ƒæ”»å‡»æ–¹é¢çš„èƒ½åŠ›å­˜åœ¨æ ¹æœ¬å±€é™æ€§ã€‚ç ”ç©¶æ„å»ºäº†â€œç‚¹çŠ¶ä¸å¯æ£€æµ‹â€çš„æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è‰¯æ€§æ¨¡å‹è¾“å‡ºä¸­çš„ç†µï¼ˆå¦‚è¯­ä¹‰æˆ–å¥æ³•å˜åŒ–ï¼‰æ¥ç§˜å¯†ä¼ è¾“å±é™©çŸ¥è¯†ã€‚æ”»å‡»ç”±å¾®è°ƒæ¨¡å‹ä¹‹å‰æ”¶é›†çš„ä»…åŒ…å«æ™®é€šæ ·æœ¬ç»„æˆï¼Œè¿™æ„å‘³ç€è®­ç»ƒå’Œæ¨ç†æ ·æœ¬éƒ½æ˜¯ä¸ªåˆ«çš„è‰¯æ€§ä¸”ä½å›°æƒ‘åº¦ã€‚æµ‹è¯•é’ˆå¯¹OpenAIå¾®è°ƒAPIçš„æ”»å‡»å‘ç°ï¼Œå®ƒä»¬æˆåŠŸå¼•å‘äº†æœ‰å®³å¤šé€‰é¢˜ç­”æ¡ˆï¼Œå¹¶ç»•è¿‡äº†æˆ‘ä»¬è®¾è®¡çš„å¢å¼ºç›‘æ§ç³»ç»Ÿï¼Œè¯¥ç›‘æµ‹ç³»ç»Ÿèƒ½å¤ŸæˆåŠŸæ£€æµ‹å…¶ä»–å¾®è°ƒæ”»å‡»ã€‚é¼“åŠ±ç¤¾åŒºå¼€å‘åº”å¯¹æˆ‘ä»¬å‘ç°çš„â€œç‚¹çŠ¶â€å¾®è°ƒAPIé˜²å¾¡æ ¹æœ¬é™åˆ¶çš„é˜²å¾¡æªæ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¼€å‘è€…é‡‡å–æªæ–½é˜²æ­¢æ»¥ç”¨æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½¿ç”¨å…¬å…±APIè¿›è¡Œå¾®è°ƒæ¨¡å‹çš„æ”»å‡»ã€‚</li>
<li>â€œç‚¹çŠ¶â€æ£€æµ‹ï¼ˆæ£€æµ‹ä¸ªåˆ«æœ‰å®³è®­ç»ƒæˆ–æ¨ç†æ ·æœ¬ï¼‰åœ¨é¢„é˜²å¾®è°ƒæ”»å‡»æ–¹é¢çš„èƒ½åŠ›å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹å¼â€”â€”â€œç‚¹çŠ¶ä¸å¯æ£€æµ‹â€çš„æ”»å‡»ï¼Œåˆ©ç”¨è‰¯æ€§æ¨¡å‹è¾“å‡ºä¸­çš„ç†µæ¥ç§˜å¯†ä¼ è¾“å±é™©çŸ¥è¯†ã€‚</li>
<li>æ”»å‡»æ ·æœ¬ä»…ä¸ºæ™®é€šæ ·æœ¬ï¼Œä¸ä¼šåœ¨å¾®è°ƒå‰å¼•å‘è­¦è§‰ã€‚</li>
<li>æµ‹è¯•é’ˆå¯¹OpenAIå¾®è°ƒAPIçš„æ”»å‡»æˆåŠŸå¼•å‘æœ‰å®³ç­”æ¡ˆã€‚</li>
<li>è®¾è®¡çš„å¢å¼ºç›‘æ§ç³»ç»Ÿæ— æ³•æ£€æµ‹åˆ°æ–°å‹æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87a222c84a9fc133f3e5d24929e86f7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc7ab7256bed0925d42466ee5b698cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0076aa217ef1900021e0a90c845e0872.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dynamic-Low-Rank-Sparse-Adaptation-for-Large-Language-Models"><a href="#Dynamic-Low-Rank-Sparse-Adaptation-for-Large-Language-Models" class="headerlink" title="Dynamic Low-Rank Sparse Adaptation for Large Language Models"></a>Dynamic Low-Rank Sparse Adaptation for Large Language Models</h2><p><strong>Authors:Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji</strong></p>
<p>Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA), a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by 16.32$%$, achieving a 2.60$\times$ speedup on CPU and 2.23$\times$ speedup on GPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wzhuang-xmu/LoSA">https://github.com/wzhuang-xmu/LoSA</a>. </p>
<blockquote>
<p>å°½ç®¡ç½‘ç»œç¨€ç–æ€§åœ¨ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éƒ¨ç½²å‹åŠ›æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»ç„¶é¢ä¸´æ€§èƒ½ä¸¥é‡ä¸‹é™çš„é—®é¢˜ã€‚å°†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åº”ç”¨äºå¾®è°ƒç¨€ç–LLMä¸ºè§£å†³è¿™ä¸€å›°å¢ƒæä¾›äº†ç›´è§‚çš„æ–¹æ³•ï¼Œä½†å®ƒä»å­˜åœ¨ä»¥ä¸‹ç¼ºç‚¹ï¼š1ï¼‰æ— æ³•å°†LoRAæƒé‡é›†æˆåˆ°ç¨€ç–LLMçš„åæœŸè®­ç»ƒä¸­ï¼›2ï¼‰åœ¨é«˜ç¨€ç–ç‡ä¸‹çš„æ€§èƒ½æ¢å¤ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŠ¨æ€ä½ç§©ç¨€ç–é€‚åº”ï¼ˆLoSAï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨ç»Ÿä¸€æ¡†æ¶å†…æ— ç¼åœ°å°†ä½ç§©é€‚åº”é›†æˆåˆ°LLMç¨€ç–æ€§ä¸­ï¼Œä»è€Œæé«˜ç¨€ç–LLMçš„æ€§èƒ½ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†å»¶è¿Ÿã€‚ç‰¹åˆ«æ˜¯ï¼ŒLoSAæ ¹æ®å¾®è°ƒæœŸé—´çš„ç›¸åº”ç¨€ç–æƒé‡åŠ¨æ€åœ°ç¨€ç–åŒ–LoRAçš„ç»“æœï¼Œä»è€Œç¡®ä¿LoRAæ¨¡å—å¯ä»¥é›†æˆåˆ°ç¨€ç–LLMçš„åæœŸè®­ç»ƒä¸­ã€‚æ­¤å¤–ï¼ŒLoSAåˆ©ç”¨è¡¨ç¤ºäº’ä¿¡æ¯ï¼ˆRMIï¼‰ä½œä¸ºæŒ‡æ ‡æ¥ç¡®å®šå±‚çš„é‡è¦æ€§ï¼Œä»è€Œåœ¨å¾®è°ƒæœŸé—´æœ‰æ•ˆåœ°ç¡®å®šé€å±‚ç¨€ç–ç‡ã€‚åŸºäºæ­¤ï¼ŒLoSAæ ¹æ®é€å±‚é‡å»ºè¯¯å·®çš„å˜å¼‚æ€§è°ƒæ•´LoRAæ¨¡å—çš„ç­‰çº§ï¼Œä¸ºæ¯å±‚åˆ†é…é€‚å½“çš„å¾®è°ƒï¼Œä»¥å‡å°‘å¯†é›†å’Œç¨€ç–LLMä¹‹é—´çš„è¾“å‡ºå·®å¼‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLoSAå¯ä»¥åœ¨æ•°å°æ—¶å†…æœ‰æ•ˆæé«˜ç¨€ç–LLMçš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šå¼•å…¥ä»»ä½•é¢å¤–çš„æ¨ç†è´Ÿæ‹…ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨LoSAï¼Œç¨€ç–LLaMA-2-7Bçš„å›°æƒ‘åº¦é™ä½äº†68.73%ï¼Œé›¶æ ·æœ¬å‡†ç¡®ç‡æé«˜äº†16.32%ï¼Œåœ¨CPUä¸Šå®ç°äº†2.60å€çš„åŠ é€Ÿï¼Œåœ¨GPUä¸Šå®ç°äº†2.23å€çš„åŠ é€Ÿï¼Œåªéœ€åœ¨å•ä¸ªNVIDIA A100 80GB GPUä¸Šè¿›è¡Œ45åˆ†é’Ÿçš„å¾®è°ƒå³å¯ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/wzhuang-xmu/LoSA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wzhuang-xmu/LoSAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14816v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç½‘ç»œç¨€ç–æ€§è™½ç„¶èƒ½å¤Ÿç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²æ—¶çš„å‹åŠ›ï¼Œä½†å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚æœ¬æ–‡å°†ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰åº”ç”¨äºç¨€ç–LLMçš„å¾®è°ƒä»¥åº”å¯¹è¿™ä¸€å›°å¢ƒï¼Œä½†ä»å­˜åœ¨æ— æ³•åœ¨ç¨€ç–LLMè®­ç»ƒåé›†æˆLoRAæƒé‡ä»¥åŠé«˜ç¨€ç–æ¯”ç‡ä¸‹æ€§èƒ½æ¢å¤ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€ä½ç§©ç¨€ç–è‡ªé€‚åº”ï¼ˆLoSAï¼‰æ–¹æ³•ï¼Œå°†ä½ç§©è‡ªé€‚åº”æ— ç¼é›†æˆåˆ°LLMç¨€ç–æ€§ä¸­ï¼Œæé«˜äº†ç¨€ç–LLMçš„æ€§èƒ½ï¼Œä¸”ä¸å¢åŠ æ¨ç†å»¶è¿Ÿã€‚å…·ä½“è€Œè¨€ï¼ŒLoSAæ ¹æ®ç²¾ç»†è°ƒèŠ‚è¿‡ç¨‹ä¸­çš„ç¨€ç–æƒé‡åŠ¨æ€è°ƒæ•´LoRAç»“æœï¼Œç¡®ä¿äº†LoRAæ¨¡å—èƒ½å¤Ÿåœ¨ç¨€ç–LLMè®­ç»ƒåè¿›è¡Œé›†æˆã€‚æ­¤å¤–ï¼ŒLoSAåˆ©ç”¨è¡¨ç¤ºäº’ä¿¡æ¯ï¼ˆRMIï¼‰ä½œä¸ºæŒ‡æ ‡æ¥ç¡®å®šå±‚çš„é‡è¦æ€§ï¼Œä»è€Œåœ¨ç²¾ç»†è°ƒèŠ‚è¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°ç¡®å®šå±‚çº§çš„ç¨€ç–ç‡ã€‚åŸºäºæ­¤ï¼ŒLoSAæ ¹æ®å±‚é‡å»ºè¯¯å·®çš„å˜å¼‚æ€§è°ƒæ•´LoRAæ¨¡å—çš„ç­‰çº§ï¼Œä¸ºæ¯å±‚åˆ†é…é€‚å½“çš„å¾®è°ƒï¼Œä»¥å‡å°‘å¯†é›†å’Œç¨€ç–LLMä¹‹é—´çš„è¾“å‡ºå·®å¼‚ã€‚å®éªŒè¯æ˜ï¼ŒLoSAå¯ä»¥åœ¨å‡ å°æ—¶å†…æœ‰æ•ˆæé«˜ç¨€ç–LLMçš„æ•ˆç‡ï¼Œä¸”ä¸ä¼šå¼•å…¥é¢å¤–çš„æ¨ç†è´Ÿæ‹…ã€‚ä¾‹å¦‚ï¼Œå¯¹äºç¨€ç–LLaMA-2-7Bæ¨¡å‹ï¼ŒLoSAå‡å°‘äº†å›°æƒ‘åº¦å¹¶æé«˜äº†é›¶æ ·æœ¬å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨CPUå’ŒGPUä¸Šå®ç°äº†åŠ é€Ÿæ•ˆæœã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç½‘ç»œç¨€ç–æ€§è™½ç„¶æœ‰åŠ©äºå‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éƒ¨ç½²å‹åŠ›ï¼Œä½†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰è¢«ç”¨æ¥æé«˜ç¨€ç–LLMçš„æ€§èƒ½ï¼Œä½†åœ¨é›†æˆåˆ°è®­ç»ƒåçš„æ¨¡å‹å’Œé«˜ç¨€ç–æ¯”ç‡ä¸‹çš„æ€§èƒ½æ¢å¤æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŠ¨æ€ä½ç§©ç¨€ç–è‡ªé€‚åº”ï¼ˆLoSAï¼‰æ–¹æ³•ï¼Œå°†ä½ç§©è‡ªé€‚åº”æ— ç¼é›†æˆåˆ°LLMçš„ç¨€ç–æ€§ä¸­ï¼Œæé«˜äº†æ€§èƒ½ä¸”ä¸ä¼šå¢åŠ æ¨ç†å»¶è¿Ÿã€‚</li>
<li>LoSAæ ¹æ®ç²¾ç»†è°ƒèŠ‚è¿‡ç¨‹ä¸­çš„ç¨€ç–æƒé‡åŠ¨æ€è°ƒæ•´LoRAç»“æœï¼Œç¡®ä¿åœ¨è®­ç»ƒåçš„ç¨€ç–LLMä¸­é›†æˆLoRAæ¨¡å—ã€‚</li>
<li>LoSAåˆ©ç”¨è¡¨ç¤ºäº’ä¿¡æ¯ï¼ˆRMIï¼‰æ¥ç¡®å®šå±‚çš„é‡è¦æ€§ï¼Œå¹¶æ®æ­¤ç¡®å®šå±‚çº§çš„ç¨€ç–ç‡ã€‚</li>
<li>LoSAæ ¹æ®å±‚é‡å»ºè¯¯å·®è°ƒæ•´LoRAæ¨¡å—çš„ç­‰çº§ï¼Œä»¥å‡å°‘å¯†é›†å’Œç¨€ç–LLMä¹‹é—´çš„è¾“å‡ºå·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8ccb21d8d66b9b80f9f55438c682f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f471e1e3bbc0f54f98ec2319512020a3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-RAG-to-Memory-Non-Parametric-Continual-Learning-for-Large-Language-Models"><a href="#From-RAG-to-Memory-Non-Parametric-Continual-Learning-for-Large-Language-Models" class="headerlink" title="From RAG to Memory: Non-Parametric Continual Learning for Large Language   Models"></a>From RAG to Memory: Non-Parametric Continual Learning for Large Language   Models</h2><p><strong>Authors:Bernal JimÃ©nez GutiÃ©rrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su</strong></p>
<p>Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at <a target="_blank" rel="noopener" href="https://github.com/OSU-NLP-Group/HippoRAG">https://github.com/OSU-NLP-Group/HippoRAG</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æŒç»­è·å–ã€ç»„ç»‡å’Œåˆ©ç”¨çŸ¥è¯†çš„èƒ½åŠ›æ˜¯äººç±»æ™ºèƒ½çš„å…³é”®ç‰¹å¾ï¼Œäººå·¥æ™ºèƒ½ç³»ç»Ÿå¿…é¡»æ¨¡æ‹Ÿè¿™ä¸€ç‰¹å¾ä»¥å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ã€‚é‰´äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æŒç»­å­¦ä¹ æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒåŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºå¼•å…¥æ–°ä¿¡æ¯çš„ä¸»è¦æ–¹å¼ã€‚ç„¶è€Œï¼Œå®ƒå¯¹å‘é‡æ£€ç´¢çš„ä¾èµ–é˜»ç¢äº†å…¶æ¨¡ä»¿äººç±»é•¿æœŸè®°å¿†çš„åŠ¨æ€å’Œäº’è”æ€§è´¨çš„èƒ½åŠ›ã€‚æœ€è¿‘çš„RAGæ–¹æ³•é€šè¿‡çŸ¥è¯†å›¾è°±ç­‰ç»“æ„å¢å¼ºå‘é‡åµŒå…¥ï¼Œä»¥è§£å†³å…¶ä¸­ä¸€äº›å·®è·ï¼Œå³ç†è§£å’Œå…³è”æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ›´åŸºæœ¬çš„è®°å¿†ä»»åŠ¡ä¸Šçš„è¡¨ç°å´å¤§å¤§ä½äºæ ‡å‡†RAGã€‚æˆ‘ä»¬è§£å†³äº†è¿™ç§æ„å¤–çš„é€€åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºäº†HippoRAG 2ï¼Œå®ƒåœ¨äº‹å®ã€ç†è§£å’Œå…³è”è®°å¿†ä»»åŠ¡ä¸Šå…¨é¢è¶…è¶Šäº†æ ‡å‡†RAGã€‚HippoRAG 2ä»¥HippoRAGä¸­ä½¿ç”¨çš„ä¸ªæ€§åŒ–PageRankç®—æ³•ä¸ºåŸºç¡€ï¼Œé€šè¿‡æ›´æ·±å…¥çš„æ®µè½é›†æˆå’Œæ›´æœ‰æ•ˆçš„åœ¨çº¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨æ¥å¢å¼ºå…¶æ€§èƒ½ã€‚è¿™ç§ç»“åˆä½¿RAGç³»ç»Ÿæ›´æ¥è¿‘äººç±»é•¿æœŸè®°å¿†çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å…³è”è®°å¿†ä»»åŠ¡ä¸Šè¾ƒæœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹æé«˜äº†7%ï¼ŒåŒæ—¶å±•ç°å‡ºå“è¶Šçš„äº‹å®çŸ¥è¯†å’Œç†è§£è®°å¿†èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºéå‚æ•°åŒ–æŒç»­å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/OSU-NLP-Group/HippoRAG%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/OSU-NLP-Group/HippoRAGä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14802v1">PDF</a> Code and data to be released at:   <a target="_blank" rel="noopener" href="https://github.com/OSU-NLP-Group/HippoRAG">https://github.com/OSU-NLP-Group/HippoRAG</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿæ¨¡ä»¿äººç±»æŒç»­å­¦ä¹ èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒæŒç»­å­¦ä¹ æ—¶çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹åŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•åœ¨æ¨¡æ‹Ÿäººç±»é•¿æœŸè®°å¿†æ–¹é¢çš„å±€é™æ€§ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºHippoRAG 2çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä¸ªæ€§åŒ–PageRankç®—æ³•ï¼Œé€šè¿‡æ›´æ·±å…¥çš„æ®µè½é›†æˆå’Œæ›´æœ‰æ•ˆçš„åœ¨çº¿å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨ï¼Œå…¨é¢è¶…è¶Šäº†æ ‡å‡†RAGåœ¨äº‹å®æ€§ã€æ„ä¹‰ç”Ÿæˆå’Œå…³è”è®°å¿†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚HippoRAG 2å®ç°äº†å…³è”è®°å¿†ä»»åŠ¡çš„æœ€æ–°åµŒå…¥æ¨¡å‹7%çš„æå‡ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„äº‹å®çŸ¥è¯†å’Œæ„ä¹‰ç”Ÿæˆè®°å¿†èƒ½åŠ›ã€‚è¿™ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„éå‚æ•°æŒç»­å­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ç³»ç»Ÿéœ€è¦æ¨¡æ‹Ÿäººç±»çš„æŒç»­å­¦ä¹ èƒ½åŠ›ä»¥é‡Šæ”¾å…¶å…¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯å¼•å…¥æ–°ä¿¡æ¯çš„ä¸»æµæ–¹å¼ï¼Œä½†å…¶ä¾èµ–å‘é‡æ£€ç´¢é™åˆ¶äº†æ¨¡æ‹Ÿäººç±»é•¿æœŸè®°å¿†çš„åŠ¨æ€å’Œäº’è”æ€§è´¨ã€‚</li>
<li>ä¸ºè§£å†³RAGåœ¨æ¨¡æ‹Ÿäººç±»é•¿æœŸè®°å¿†æ–¹é¢çš„ç¼ºé™·ï¼Œç»“åˆäº†çŸ¥è¯†å›¾è°±ç­‰ç»“æ„çš„æœ€æ–°RAGæ–¹æ³•è¯•å›¾å¼¥è¡¥ä¸€äº›å·®è·ï¼Œå¦‚æ„ä¹‰ç”Ÿæˆå’Œå…³è”æ€§ã€‚</li>
<li>ç„¶è€Œï¼Œè¿™äº›æ–°æ–¹æ³•åœ¨åŸºæœ¬çš„äº‹å®è®°å¿†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>HippoRAG 2æ¡†æ¶é€šè¿‡ç»“åˆä¸ªæ€§åŒ–PageRankç®—æ³•ã€æ›´æ·±å…¥çš„æ®µè½é›†æˆå’Œæœ‰æ•ˆçš„åœ¨çº¿å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨ï¼Œå…¨é¢è¶…è¶Šäº†æ ‡å‡†RAGåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>HippoRAG 2å®ç°äº†å…³è”è®°å¿†ä»»åŠ¡çš„æœ€æ–°åµŒå…¥æ¨¡å‹7%çš„æå‡ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„äº‹å®çŸ¥è¯†å’Œæ„ä¹‰ç”Ÿæˆè®°å¿†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-183a6669ae6e93aeb0cb1bbe24be4021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb52a6980ce3694a831cf83386d10e68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4fd33bb37da03b16f4ef11903bea654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b71d68965f822b2e07f704b6b4403fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca8cba1e10dfb7081c02c17f909ffbef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SurveyX-Academic-Survey-Automation-via-Large-Language-Models"><a href="#SurveyX-Academic-Survey-Automation-via-Large-Language-Models" class="headerlink" title="SurveyX: Academic Survey Automation via Large Language Models"></a>SurveyX: Academic Survey Automation via Large Language Models</h2><p><strong>Authors:Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Simin Niu, Shichao Song, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, Zhiyu li</strong></p>
<p>Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on <a target="_blank" rel="noopener" href="http://www.surveyx.cn/">www.surveyx.cn</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºäº†å‡ºè‰²çš„ç†è§£èƒ½åŠ›å’Œåºå¤§çš„çŸ¥è¯†åº“ï¼Œè¿™è¡¨æ˜LLMå¯ä»¥ä½œä¸ºè‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆçš„æ•ˆç‡å·¥å…·ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„æœ‰å…³è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆçš„ç ”ç©¶ä»ç„¶å—åˆ°ä¸€äº›å…³é”®é™åˆ¶ï¼Œä¾‹å¦‚æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€ç¼ºä¹æ·±åº¦å†…å®¹è®¨è®ºå’Œç¼ºä¹ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚å—äººç±»å†™ä½œè¿‡ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SurveyXï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”æœ‰æ¡ç†çš„è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆç³»ç»Ÿï¼Œå®ƒå°†é—®å·åˆ›ä½œè¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå‡†å¤‡é˜¶æ®µå’Œç”Ÿæˆé˜¶æ®µã€‚é€šè¿‡åˆ›æ–°æ€§åœ°å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€ä¸€ç§åä¸ºAttributeTreeçš„é¢„å¤„ç†æ–¹æ³•ä»¥åŠæ¶¦è‰²è¿‡ç¨‹ï¼ŒSurveyXæ˜¾è‘—æé«˜äº†é—®å·åˆ›ä½œçš„æ•ˆç‡ã€‚å®éªŒè¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨å†…å®¹è´¨é‡ï¼ˆæé«˜0.259ï¼‰å’Œå¼•ç”¨è´¨é‡ï¼ˆæé«˜1.76ï¼‰æ–¹é¢ï¼ŒSurveyXä¼˜äºç°æœ‰çš„è‡ªåŠ¨åŒ–é—®å·ç”Ÿæˆç³»ç»Ÿï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°ç»´åº¦ä¸Šæ¥è¿‘äººç±»ä¸“å®¶çš„æ€§èƒ½ã€‚SurveyXç”Ÿæˆçš„é—®å·ç¤ºä¾‹å¯åœ¨<a target="_blank" rel="noopener" href="http://www.surveyx.cnä¸ŠæŸ¥çœ‹./">www.surveyx.cnä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14776v1">PDF</a> 15 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>LLMæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ç†è§£å’ŒçŸ¥è¯†åº“èƒ½åŠ›ï¼Œå¯ç”¨äºè‡ªåŠ¨åŒ–è°ƒæŸ¥ç”Ÿæˆã€‚ä½†ç”±äºä¸Šä¸‹æ–‡çª—å£æœ‰é™åˆ¶ç­‰é™åˆ¶å› ç´ ï¼Œä»å­˜åœ¨ä¸è¶³ä¹‹å¤„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè°ƒç ”å·¥å…·SurveyXï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿäººç±»å†™ä½œè¿‡ç¨‹å’Œå¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ç­‰åˆ›æ–°æŠ€æœ¯æ¥æå‡è°ƒç ”çš„æ•ˆç‡å’Œç»“æ„è´¨é‡ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSurveyXåœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰è‡ªåŠ¨åŒ–è°ƒç ”ç”Ÿæˆç³»ç»Ÿï¼Œå¹¶åœ¨å¤šä¸ªç»´åº¦æ¥è¿‘äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚æ›´å¤šè°ƒç ”ç¤ºä¾‹å¯è®¿é—®<a target="_blank" rel="noopener" href="http://www.surveyx.cn./">www.surveyx.cnã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMæ¨¡å‹å¯ç”¨äºè‡ªåŠ¨åŒ–è°ƒæŸ¥ç”Ÿæˆå·¥å…·ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–è°ƒç ”ç”Ÿæˆå­˜åœ¨ä¸Šä¸‹æ–‡çª—å£æœ‰é™ç­‰é™åˆ¶å› ç´ ã€‚</li>
<li>SurveyXé€šè¿‡æ¨¡æ‹Ÿäººç±»å†™ä½œè¿‡ç¨‹æ¥æ”¹è¿›è°ƒç ”ç”Ÿæˆç³»ç»Ÿã€‚</li>
<li>SurveyXå¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢å’Œé¢„å¤„ç†æ–¹æ³•AttributeTreeç­‰æŠ€æœ¯æ¥æå‡è°ƒç ”è´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜SurveyXåœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¥è¿‘äººç±»ä¸“å®¶æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-864f2574a698635da63cff82876b8fc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ef6084b0a86097560b916bd8e020f09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44f747933006395ed652a423e967af6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05d205b3925cb755810f8e025c9bd469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7465049d529a1599c0a49ba1766a608f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f2581db7b0741c71ab4338848f6f06e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EquivaMap-Leveraging-LLMs-for-Automatic-Equivalence-Checking-of-Optimization-Formulations"><a href="#EquivaMap-Leveraging-LLMs-for-Automatic-Equivalence-Checking-of-Optimization-Formulations" class="headerlink" title="EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of   Optimization Formulations"></a>EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of   Optimization Formulations</h2><p><strong>Authors:Haotian Zhai, Connor Lawless, Ellen Vitercik, Liu Leqi</strong></p>
<p>A fundamental problem in combinatorial optimization is identifying equivalent formulations, which can lead to more efficient solution strategies and deeper insights into a problemâ€™s computational complexity. The need to automatically identify equivalence between problem formulations has grown as optimization copilotsâ€“systems that generate problem formulations from natural language descriptionsâ€“have proliferated. However, existing approaches to checking formulation equivalence lack grounding, relying on simple heuristics which are insufficient for rigorous validation. Inspired by Karp reductions, in this work we introduce quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings, enabling scalable and reliable equivalence verification. To evaluate our approach, we construct the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence. </p>
<blockquote>
<p>ç»„åˆä¼˜åŒ–ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜æ˜¯è¯†åˆ«ç­‰æ•ˆå…¬å¼ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆç­–ç•¥å’Œå¯¹é—®é¢˜è®¡ç®—å¤æ‚æ€§çš„æ·±å…¥äº†è§£ã€‚éšç€ä¼˜åŒ–å…±é©¾é©¶å‘˜ï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆé—®é¢˜å…¬å¼çš„ç³»ç»Ÿï¼‰çš„æ™®åŠï¼Œè‡ªåŠ¨è¯†åˆ«é—®é¢˜å…¬å¼ä¹‹é—´ç­‰ä»·æ€§çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€æŸ¥å…¬å¼ç­‰ä»·æ€§çš„æ–¹æ³•ç¼ºä¹åŸºç¡€ï¼Œå®ƒä»¬ä¾èµ–äºç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼Œè¿™å¯¹äºä¸¥æ ¼éªŒè¯æ˜¯ä¸å¤Ÿçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å—åˆ°Karpç®€åŒ–çš„å¯å‘ï¼Œå¼•å…¥äº†å‡†Karpç­‰ä»·æ€§è¿™ä¸€æ­£å¼æ ‡å‡†ï¼Œç”¨äºç¡®å®šä¸¤ä¸ªä¼˜åŒ–å…¬å¼ä½•æ—¶åŸºäºå…¶å†³ç­–å˜é‡ä¹‹é—´çš„æ˜ å°„è€Œç­‰ä»·ã€‚æˆ‘ä»¬æå‡ºäº†EquivaMapæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨å‘ç°è¿™æ ·çš„æ˜ å°„ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œå¯é çš„ç­‰ä»·æ€§éªŒè¯ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªå¼€æºçš„ç­‰æ•ˆä¼˜åŒ–å…¬å¼æ•°æ®é›†ï¼Œé€šè¿‡åº”ç”¨æ·»åŠ æ¾å¼›å˜é‡æˆ–ç°æœ‰å…¬å¼çš„æœ‰æ•ˆä¸ç­‰å¼ç­‰è½¬æ¢æ¥ç”Ÿæˆæ•°æ®é›†ã€‚ä»å®è¯ä¸Šçœ‹ï¼ŒEquivaMapæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ­£ç¡®è¯†åˆ«å…¬å¼ç­‰ä»·æ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14760v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¼˜åŒ–é—®é¢˜ç­‰ä»·æ€§çš„é‡è¦æ€§åŠå…¶è‡ªåŠ¨è¯†åˆ«çš„æŒ‘æˆ˜ã€‚å¼•å…¥åŸºäºKarpå½’çº¦çš„å‡†Karpç­‰ä»·æ€§æ¦‚å¿µï¼Œæå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºEquivaMapæ¡†æ¶ï¼Œå®ç°å†³ç­–å˜é‡ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå®ç°ç­‰ä»·æ€§çš„è‡ªåŠ¨å‘ç°ä¸éªŒè¯ã€‚æ„å»ºé¦–ä¸ªå¼€æ”¾æºä»£ç çš„ç­‰ä»·ä¼˜åŒ–é—®é¢˜å…¬å¼æ•°æ®é›†ä»¥è¯„ä¼°æ–¹æ³•æ€§èƒ½ï¼Œå®è¯ç»“æœæ˜¾ç¤ºEquivaMapæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»„åˆä¼˜åŒ–ä¸­çš„å…³é”®é—®é¢˜ä¹‹ä¸€æ˜¯è¯†åˆ«ç­‰æ•ˆé—®é¢˜å…¬å¼ï¼Œæœ‰åŠ©äºæé«˜è§£å†³ç­–ç•¥æ•ˆç‡å’Œæ·±å…¥ç†è§£é—®é¢˜çš„è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>éšç€è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆé—®é¢˜å…¬å¼çš„ä¼˜åŒ–ååŒç³»ç»Ÿçš„æ™®åŠï¼Œè‡ªåŠ¨è¯†åˆ«é—®é¢˜å…¬å¼ç­‰ä»·çš„å¿…è¦æ€§æ—¥ç›Šå¢å¼ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹éªŒè¯å…¬å¼ç­‰ä»·æ€§çš„ä¸¥è°¨æ ‡å‡†ï¼Œä¸»è¦ä¾èµ–ç®€å•å¯å‘å¼æ–¹æ³•ï¼Œä¸è¶³ä»¥è¿›è¡Œä¸¥æ ¼çš„éªŒè¯ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥å‡†Karpç­‰ä»·æ€§æ¦‚å¿µï¼ŒåŸºäºå†³ç­–å˜é‡ä¹‹é—´çš„æ˜ å°„å…³ç³»ç¡®å®šä¸¤ä¸ªä¼˜åŒ–å…¬å¼æ˜¯å¦ç­‰ä»·ã€‚</li>
<li>æå‡ºEquivaMapæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨å‘ç°è¿™ç§æ˜ å°„å…³ç³»ï¼Œå®ç°å¯æ‰©å±•å’Œå¯é çš„ç­‰ä»·æ€§éªŒè¯ã€‚</li>
<li>æ„å»ºé¦–ä¸ªå¼€æ”¾æºä»£ç çš„ç­‰ä»·ä¼˜åŒ–é—®é¢˜å…¬å¼æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ–¹æ³•æ€§èƒ½ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼ŒEquivaMapåœ¨æ­£ç¡®è¯†åˆ«é—®é¢˜å…¬å¼ç­‰ä»·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5425c90bc7e07ae63c5c71758dea146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8047a20ca6cec835b782790b836eb329.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d55cec58b6d2045eb2eba1127854ec7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b3cc7c2f26fe2a1eb34f6416657abaa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TritonBench-Benchmarking-Large-Language-Model-Capabilities-for-Generating-Triton-Operators"><a href="#TritonBench-Benchmarking-Large-Language-Model-Capabilities-for-Generating-Triton-Operators" class="headerlink" title="TritonBench: Benchmarking Large Language Model Capabilities for   Generating Triton Operators"></a>TritonBench: Benchmarking Large Language Model Capabilities for   Generating Triton Operators</h2><p><strong>Authors:Jianling Li, Shangzhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, Haojie Wang, Jianrong Wang, Xu Han, Zhiyuan Liu, Maosong Sun</strong></p>
<p>Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. TritonBench will be available at <a target="_blank" rel="noopener" href="https://github.com/thunlp/TritonBench">https://github.com/thunlp/TritonBench</a>. </p>
<blockquote>
<p>Tritonæ˜¯ä¸€ç§ä¸ºæ„å»ºé«˜æ•ˆGPUå†…æ ¸è€Œè®¾è®¡çš„Pythonç±»é«˜çº§è¯­è¨€ï¼Œç”±äºå…¶å¯ç§»æ¤æ€§ã€çµæ´»æ€§å’Œå¯è®¿é—®æ€§è€Œåœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç¼–ç¨‹å’Œå¹¶è¡Œä¼˜åŒ–ä»ç„¶éœ€è¦Tritonå¼€å‘è€…è¿›è¡Œå¤§é‡çš„è¯•éªŒå’Œé”™è¯¯æ‘¸ç´¢ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¸¸è§„ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®ã€æ€§èƒ½ä¼˜åŒ–çš„Tritonä»£ç æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯¹è¯¥è¯­è¨€çš„ç‰¹æ€§å’ŒGPUç¼–ç¨‹çš„å¤æ‚æ€§äº†è§£ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè¿«åˆ‡éœ€è¦é’ˆå¯¹Tritonè¿›è¡Œç³»ç»Ÿçš„è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TritonBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹Tritonæ“ä½œç”Ÿæˆçš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚TritonBenchå…·æœ‰ä¸¤ä¸ªè¯„ä¼°é€šé“ï¼šä¸€ä¸ªæ˜¯æ¥è‡ªGitHubçš„ç²¾é€‰ç°å®ä¸–ç•Œæ“ä½œç¬¦é›†ï¼ˆå…±åŒ…å«184ä¸ªæ“ä½œç¬¦ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯ä¸PyTorchæ¥å£å¯¹é½çš„æ“ä½œç¬¦é›†åˆã€‚ä¸ä¼ ç»Ÿçš„ä»£ç åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒTritonBenchä¸ä»…é‡è§†åŠŸèƒ½æ­£ç¡®æ€§ï¼Œè¿˜ä¾§é‡äºåœ¨å¹¿æ³›éƒ¨ç½²çš„GPUä¸Šæ ¹æ®è¡Œä¸šåº”ç”¨è¿›è¡Œæ•ˆç‡æ€§èƒ½åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„ä»£ç LLMåœ¨ç”Ÿæˆé«˜æ•ˆçš„Tritonæ“ä½œç¬¦æ–¹é¢é¢ä¸´å›°éš¾ï¼Œè¿™çªæ˜¾äº†é«˜æ€§èƒ½ä»£ç ç”Ÿæˆæ–¹é¢çš„å·¨å¤§å·®è·ã€‚TritonBenchå°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/thunlp/TritonBench%E3%80%82">https://github.com/thunlp/TritonBenchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14752v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Tritonè¯­è¨€å› å…¶é«˜æ•ˆæ€§ã€å¯ç§»æ¤æ€§å’Œçµæ´»æ€§åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç¼–ç¨‹å’Œå¹³è¡Œä¼˜åŒ–éœ€è¦å¤§é‡å°è¯•å’Œé”™è¯¯ã€‚å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå‡†ç¡®çš„Tritonä»£ç æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç¼ºä¹å¯¹å…¶ç‰¹æ€§å’ŒGPUç¼–ç¨‹å¤æ‚æ€§çš„äº†è§£ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†TritonBenchï¼Œå®ƒæ˜¯é’ˆå¯¹Tritonæ“ä½œç”Ÿæˆçš„é¦–ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ã€‚TritonBenchå…·æœ‰ä¸¤ä¸ªè¯„ä¼°é€šé“ï¼ŒåŒ…æ‹¬GitHubä¸Šçš„çœŸå®æ“ä½œå™¨å’Œä¸PyTorchæ¥å£å¯¹é½çš„æ“ä½œå™¨é›†åˆã€‚é™¤äº†å¼ºè°ƒåŠŸèƒ½æ­£ç¡®æ€§å¤–ï¼ŒTritonBenchè¿˜åœ¨å¹¿æ³›éƒ¨ç½²çš„GPUä¸Šè¯„ä¼°æ•ˆç‡æ€§èƒ½ï¼Œä»¥ç¬¦åˆè¡Œä¸šåº”ç”¨éœ€æ±‚ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„ä»£ç LLMéš¾ä»¥ç”Ÿæˆé«˜æ•ˆçš„Tritonæ“ä½œå™¨ï¼Œçªæ˜¾é«˜æ€§èƒ½ä»£ç ç”Ÿæˆæ–¹é¢çš„å·¨å¤§å·®è·ã€‚TritonBenchå°†æä¾›å…¬å…±è®¿é—®é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tritonæ˜¯ä¸€ç§ç”¨äºæ„å»ºé«˜æ•ˆGPUå†…æ ¸çš„Pythonç±»ä¼¼è¯­è¨€ï¼Œå¹¿æ³›åº”ç”¨äºæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ã€‚</li>
<li>Tritonç¼–ç¨‹å’Œå¹³è¡Œä¼˜åŒ–éœ€è¦æ˜¾è‘—çš„åŠªåŠ›å’Œè¯•é”™è¿‡ç¨‹ã€‚</li>
<li>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå‡†ç¡®çš„Tritonä»£ç æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¸äº†è§£Tritonçš„ç‰¹æ€§å’ŒGPUç¼–ç¨‹çš„å¤æ‚æ€§ã€‚</li>
<li>å¼•å…¥TritonBenchä½œä¸ºé¦–ä¸ªé’ˆå¯¹Tritonæ“ä½œç”Ÿæˆçš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>TritonBenchåŒ…å«ä¸¤ä¸ªè¯„ä¼°é€šé“ï¼šGitHubä¸Šçš„çœŸå®æ“ä½œå™¨å’Œä¸PyTorchæ¥å£å¯¹é½çš„æ“ä½œå™¨é›†åˆã€‚</li>
<li>TritonBenchä¸ä»…å¼ºè°ƒåŠŸèƒ½æ­£ç¡®æ€§ï¼Œè¿˜è¯„ä¼°æ•ˆç‡æ€§èƒ½ï¼Œä»¥ç¬¦åˆè¡Œä¸šåº”ç”¨éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c7422ddda545f78c3805117fa715f3e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29090b5efedcca3bcd371e45779eea90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40cb8843f3de4f77950b2d1a44f9db24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55d3e81dc0b9a858ed021f2aba603a40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b661e7e1fb65f9d92adfa5c452efb08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdf7035f6c326cc1022a5b2f0f45d1b3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-Transforming-Natural-Language-Questions-into-SQL-Queries-via-Abstract-Query-Pattern-and-Contextual-Schema-Markup"><a href="#Bridging-the-Gap-Transforming-Natural-Language-Questions-into-SQL-Queries-via-Abstract-Query-Pattern-and-Contextual-Schema-Markup" class="headerlink" title="Bridging the Gap: Transforming Natural Language Questions into SQL   Queries via Abstract Query Pattern and Contextual Schema Markup"></a>Bridging the Gap: Transforming Natural Language Questions into SQL   Queries via Abstract Query Pattern and Contextual Schema Markup</h2><p><strong>Authors:Yonghui Kong, Hongbing Hu, Dan Zhang, Siyuan Chai, Fan Zhang, Wei Wang</strong></p>
<p>Large language models have demonstrated excellent performance in many tasks, including Text-to-SQL, due to their powerful in-context learning capabilities. They are becoming the mainstream approach for Text-to-SQL. However, these methods still have a significant gap compared to human performance, especially on complex questions. As the complexity of questions increases, the gap between questions and SQLs increases. We identify two important gaps: the structural mapping gap and the lexical mapping gap. To tackle these two gaps, we propose PAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates gaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM). AQP aims to obtain the structural pattern of the question by removing database-related information, which enables us to find structurally similar demonstrations. CSM aims to associate database-related text span in the question with specific tables or columns in the database, which alleviates the lexical mapping gap. Experimental results on the Spider and BIRD datasets demonstrate the effectiveness of our proposed method. Specifically, PAS-SQL + GPT-4o sets a new state-of-the-art on the Spider benchmark with an execution accuracy of 87.9%, and achieves leading results on the BIRD dataset with an execution accuracy of 64.67%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ç”±äºå…·æœ‰å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨è®¸å¤šä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°SQLçš„è½¬æ¢ã€‚å®ƒä»¬æ­£æˆä¸ºæ–‡æœ¬åˆ°SQLçš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸äººç±»æ€§èƒ½ç›¸æ¯”ä»å­˜åœ¨è¾ƒå¤§å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚é—®é¢˜ä¸Šã€‚éšç€é—®é¢˜å¤æ‚æ€§çš„å¢åŠ ï¼Œé—®é¢˜ä¸SQLä¹‹é—´çš„å·®è·ä¹Ÿåœ¨å¢åŠ ã€‚æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªé‡è¦çš„å·®è·ï¼šç»“æ„æ˜ å°„å·®è·å’Œè¯æ±‡æ˜ å°„å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLLMçš„æœ‰æ•ˆSQLç”Ÿæˆç®¡é“PAS-SQLï¼Œé€šè¿‡æŠ½è±¡æŸ¥è¯¢æ¨¡å¼ï¼ˆAQPï¼‰å’Œä¸Šä¸‹æ–‡æ¨¡å¼æ ‡è®°ï¼ˆCSMï¼‰æ¥ç¼“è§£å·®è·ã€‚AQPæ—¨åœ¨é€šè¿‡å»é™¤ä¸æ•°æ®åº“ç›¸å…³çš„ä¿¡æ¯æ¥è·å–é—®é¢˜çš„ç»“æ„æ¨¡å¼ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ‰¾åˆ°ç»“æ„ç›¸ä¼¼çš„ç¤ºä¾‹ã€‚CSMæ—¨åœ¨å°†é—®é¢˜ä¸­ä¸æ•°æ®åº“ç›¸å…³çš„æ–‡æœ¬è·¨åº¦ä¸æ•°æ®åº“ä¸­çš„ç‰¹å®šè¡¨æˆ–åˆ—ç›¸å…³è”ï¼Œä»è€Œç¼“è§£è¯æ±‡æ˜ å°„çš„å·®è·ã€‚åœ¨Spiderå’ŒBIRDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒPAS-SQL+GPT-4oåœ¨SpideråŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°çš„å“è¶Šè¡¨ç°ï¼Œæ‰§è¡Œå‡†ç¡®åº¦è¾¾åˆ°87.9%ï¼Œå¹¶åœ¨BIRDæ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆçš„æ‰§è¡Œå‡†ç¡®åº¦ä¸º64.67%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14682v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Text-to-SQLä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¾—ç›Šäºå…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæˆä¸ºä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼Œä¸äººçš„è¡¨ç°ç›¸æ¯”ä»å­˜åœ¨å·®è·ï¼Œå¤æ‚é—®é¢˜ä¸Šçš„å·®è·æ›´å¤§ã€‚æœ¬æ–‡æå‡ºPAS-SQLæ–¹æ³•ï¼Œé€šè¿‡æŠ½è±¡æŸ¥è¯¢æ¨¡å¼ï¼ˆAQPï¼‰å’Œä¸Šä¸‹æ–‡æ¨¡å¼æ ‡è®°ï¼ˆCSMï¼‰ç¼“è§£ç»“æ„æ˜ å°„å’Œè¯æ±‡æ˜ å°„çš„å·®è·ï¼Œæé«˜SQLç”Ÿæˆæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPAS-SQL+GPT-4oåœ¨Spiderå’ŒBIRDæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Text-to-SQLä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºä¸»æµæ–¹æ³•ã€‚</li>
<li>ä¸äººç±»è¡¨ç°ç›¸æ¯”ï¼Œå¤æ‚é—®é¢˜ä¸Šçš„å·®è·ä»ç„¶æ˜¾è‘—ã€‚</li>
<li>å­˜åœ¨ç»“æ„æ˜ å°„å’Œè¯æ±‡æ˜ å°„ä¸¤ä¸ªé‡è¦å·®è·ã€‚</li>
<li>æå‡ºPAS-SQLæ–¹æ³•ï¼Œé€šè¿‡AQPå’ŒCSMç¼“è§£ä¸Šè¿°å·®è·ã€‚</li>
<li>AQPé€šè¿‡å»é™¤æ•°æ®åº“ç›¸å…³ä¿¡æ¯è·å–é—®é¢˜ç»“æ„æ¨¡å¼ã€‚</li>
<li>CSMå°†é—®é¢˜ä¸­çš„æ•°æ®åº“ç›¸å…³æ–‡æœ¬ä¸æ•°æ®åº“ä¸­çš„ç‰¹å®šè¡¨æˆ–åˆ—ç›¸å…³è”ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPAS-SQL+GPT-4oåœ¨Spiderå’ŒBIRDæ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b46c9a01691cdaa5acae819f640b4fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef8b795e3b8114f67b1f40a25e544284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd15b7a8a8dce702aba61a1e8c56119.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5578074a6c98742ad9cd2ae1f2279e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8273139883080c95ebfa6c2a7ccc2962.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Edit-Once-Update-Everywhere-A-Simple-Framework-for-Cross-Lingual-Knowledge-Synchronization-in-LLMs"><a href="#Edit-Once-Update-Everywhere-A-Simple-Framework-for-Cross-Lingual-Knowledge-Synchronization-in-LLMs" class="headerlink" title="Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual   Knowledge Synchronization in LLMs"></a>Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual   Knowledge Synchronization in LLMs</h2><p><strong>Authors:Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao</strong></p>
<p>Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings. </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘å…è®¸é«˜æ•ˆåœ°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–°ä¿¡æ¯æˆ–ä¿®æ­£çš„é€‚åº”ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢å†è®­ç»ƒã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•é€šå¸¸ä¾§é‡äºå•è¯­è¨€ç¼–è¾‘æˆ–åŸºæœ¬çš„å¤šè¯­è¨€ç¼–è¾‘ï¼Œæ— æ³•å®ç°çœŸæ­£çš„è·¨è¯­è¨€çŸ¥è¯†åŒæ­¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•å®ç”¨çš„æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰é…æ–¹â€”â€”è·¨è¯­è¨€çŸ¥è¯†æ°‘ä¸»ç¼–è¾‘ï¼ˆX-KDEï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»ä¸»å¯¼è¯­è¨€ä¼ æ’­åˆ°å…¶ä»–è¯­è¨€ã€‚æˆ‘ä»¬çš„X-KDEåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰è·¨è¯­è¨€ç¼–è¾‘æŒ‡ä»¤è°ƒæ•´ï¼ˆXE-ITï¼‰ï¼Œè¯¥é˜¶æ®µåœ¨ä¸€ä¸ªç²¾é€‰çš„å¹¶è¡Œæ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¿®æ”¹èŒƒå›´å†…çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™ä¸ç›¸å…³çš„ä¿¡æ¯ï¼›ï¼ˆiiï¼‰ç›®æ ‡è¯­è¨€åå¥½ä¼˜åŒ–ï¼ˆTL-POï¼‰ï¼Œè¯¥é˜¶æ®µåº”ç”¨å…ˆè¿›çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥ç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§ï¼Œä¿ƒè¿›æ›´æ–°çš„è½¬ç§»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡ã€è·¨è¯­è¨€çš„ä¸“é—¨è®¾è®¡çš„æ•°æ®é›†ï¼Œä»¥æé«˜è·¨è¯­è¨€çš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨Bi-ZsREå’ŒMzsREåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒX-KDEæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†+8.19%ï¼ŒåŒæ—¶åœ¨å•è¯­è¨€ç¯å¢ƒä¸­ä¹Ÿä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14645v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†ç¼–è¾‘èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–°ä¿¡æ¯æˆ–ä¿®æ­£çš„é€‚åº”ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢å†è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•è¯­è¨€ç¼–è¾‘æˆ–åŸºæœ¬çš„å¤šè¯­è¨€ç¼–è¾‘ï¼Œæ— æ³•å®ç°çœŸæ­£çš„è·¨è¯­è¨€çŸ¥è¯†åŒæ­¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç®€å•å®ç”¨çš„æœ€æ–°é…æ–¹â€”â€”è·¨è¯­è¨€çŸ¥è¯†æ°‘ä¸»ç¼–è¾‘ï¼ˆX-KDEï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»ä¸»å¯¼è¯­è¨€ä¼ æ’­åˆ°å…¶ä»–è¯­è¨€ã€‚X-KDEåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šä¸€æ˜¯è·¨è¯­è¨€ç¼–è¾‘æŒ‡ä»¤å¾®è°ƒï¼ˆXE-ITï¼‰ï¼Œé€šè¿‡å¾®è°ƒæ¨¡å‹åœ¨ç²¾é€‰çš„å¹³è¡Œæ•°æ®é›†ä¸Šä¿®æ”¹èŒƒå›´å†…çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™æ— å…³ä¿¡æ¯ï¼›äºŒæ˜¯ç›®æ ‡è¯­è¨€åå¥½ä¼˜åŒ–ï¼ˆTL-POï¼‰ï¼Œé‡‡ç”¨å…ˆè¿›çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§ï¼Œä¿ƒè¿›æ›´æ–°çš„è½¬ç§»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡ã€è·¨è¯­è¨€çš„æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå¢å¼ºè·¨è¯­è¨€çš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨Bi-ZsREå’ŒMzsREåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒX-KDEæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€æ€§èƒ½ï¼Œå¹³å‡æ”¹è¿›äº†+8.19%ï¼ŒåŒæ—¶åœ¨å•è¯­è¨€è®¾ç½®ä¸­ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†ç¼–è¾‘èƒ½å¤Ÿä¸ç»è¿‡å…¨é¢å†è®­ç»ƒè€Œé€‚åº”æ–°ä¿¡æ¯æˆ–ä¿®æ­£çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è¿›è¡Œè·¨è¯­è¨€ç¼–è¾‘æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å®ç°çœŸæ­£çš„è·¨è¯­è¨€çŸ¥è¯†åŒæ­¥ã€‚</li>
<li>æå‡ºäº†è·¨è¯­è¨€çŸ¥è¯†æ°‘ä¸»ç¼–è¾‘ï¼ˆX-KDEï¼‰æ–¹æ³•ï¼ŒåŒ…å«è·¨è¯­è¨€ç¼–è¾‘æŒ‡ä»¤å¾®è°ƒï¼ˆXE-ITï¼‰å’Œç›®æ ‡è¯­è¨€åå¥½ä¼˜åŒ–ï¼ˆTL-POï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>X-KDEèƒ½æœ‰æ•ˆå°†ä»ä¸»å¯¼è¯­è¨€ä¼ æ’­çš„çŸ¥è¯†è½¬ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚</li>
<li>è´¡çŒ®äº†ä¸€ä¸ªä¸“é—¨ç”¨äºå¢å¼ºè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»çš„é«˜è´¨é‡ã€è·¨è¯­è¨€æ•°æ®é›†ã€‚</li>
<li>åœ¨Bi-ZsREå’ŒMzsREåŸºå‡†æµ‹è¯•ä¸Šï¼ŒX-KDEæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ab6dca703dc1f8e917f498b4d7765d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd735473e9388f77456786a0013c3428.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac59dff14ea7c5a5401dfbdd9fdc07d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6e5b6c885205a3086c5cee86a5bd058.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9327fdf51790883ef225935406fb3740.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PEARL-Towards-Permutation-Resilient-LLMs"><a href="#PEARL-Towards-Permutation-Resilient-LLMs" class="headerlink" title="PEARL: Towards Permutation-Resilient LLMs"></a>PEARL: Towards Permutation-Resilient LLMs</h2><p><strong>Authors:Liang Chen, Li Shen, Yang Deng, Xiaoyan Zhao, Bin Liang, Kam-Fai Wong</strong></p>
<p>The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the modelâ€™s inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLMâ€™s robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›è®©å®ƒä»¬èƒ½å¤Ÿåˆ©ç”¨æä¾›çš„æ¼”ç¤ºæ¥å®Œæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒICLå¯¹æ¼”ç¤ºçš„é¡ºåºé«˜åº¦æ•æ„Ÿï¼Œå¯¼è‡´é¢„æµ‹ç»“æœä¸ç¨³å®šã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™ä¸€æ¼æ´è®¾è®¡ä¸€ç§è‡ªç„¶çš„æ”»å‡»â€”â€”è¿™ç§æ”»å‡»å¾ˆéš¾è¢«æ¨¡å‹æä¾›å•†æ£€æµ‹åˆ°â€”â€”é€šè¿‡ç®€å•åœ°æ”¹å˜æ¼”ç¤ºé¡ºåºï¼Œåœ¨LLaMA-3ä¸Šå®ç°äº†è¿‘80%çš„æˆåŠŸç‡ã€‚ç°æœ‰çš„ç¼“è§£æ–¹æ³•ä¸»è¦ä¾èµ–äºåå¤„ç†ï¼Œæœªèƒ½æé«˜æ¨¡å‹å¯¹è¾“å…¥æ’åˆ—çš„å›ºæœ‰é²æ£’æ€§ï¼Œå¼•å‘äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§å’Œå¯é æ€§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆDROï¼‰çš„æ’åˆ—éŸ§æ€§å­¦ä¹ ï¼ˆPEARLï¼‰æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯¹æŠ—æœ€åæƒ…å†µè¾“å…¥æ’åˆ—æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒPEARLåŒ…æ‹¬ä¸€ä¸ªæ’åˆ—æè®®ç½‘ç»œï¼ˆP-Netï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚P-Netå°†é—®é¢˜è§†ä¸ºæœ€ä¼˜ä¼ è¾“é—®é¢˜ï¼Œé€šè¿‡è§£å†³å¸¦ç†µçº¦æŸçš„Sinkhornç®—æ³•ç”Ÿæˆæœ€å…·æŒ‘æˆ˜æ€§çš„æ’åˆ—ã€‚é€šè¿‡æå°æå¤§ä¼˜åŒ–ï¼ŒP-Netå’Œå¤§å‹è¯­è¨€æ¨¡å‹ç›¸äº’è¿­ä»£ä¼˜åŒ–ï¼Œé€æ­¥æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨åˆæˆé¢„è®­ç»ƒä»»åŠ¡å’Œç°å®ä¸–ç•ŒæŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPEARLæœ‰æ•ˆåœ°ç¼“è§£äº†æ’åˆ—æ”»å‡»ï¼Œæé«˜äº†æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨æ›´å°‘çš„å°„å‡»å’Œè¾ƒçŸ­çš„ä¸Šä¸‹æ–‡æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼ŒPEARLåœ¨åº”ç”¨åˆ°å¤šæ¬¡å°„å‡»å’Œé•¿ä¸Šä¸‹æ–‡åœºæ™¯æ—¶ä»èƒ½å®ç°é«˜è¾¾40%çš„æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†å…¶æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14628v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿé€šè¿‡æä¾›çš„æ¼”ç¤ºæ‰§è¡Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒICLé«˜åº¦ä¾èµ–äºæ¼”ç¤ºçš„é¡ºåºï¼Œå¯¼è‡´é¢„æµ‹ç»“æœä¸ç¨³å®šã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™ä¸€æ¼æ´è®¾è®¡ä¸€ç§è‡ªç„¶æ”»å‡»ï¼Œé€šè¿‡ç®€å•è°ƒæ•´æ¼”ç¤ºçš„é¡ºåºå³å¯è¾¾åˆ°æ”»å‡»æ•ˆæœï¼Œå¹¶å¯¹LLaMA-3æ¨¡å‹å®ç°äº†è¿‘80%çš„æˆåŠŸç‡ã€‚ç°æœ‰çš„ç¼“è§£æ–¹æ³•ä¸»è¦ä¾èµ–åå¤„ç†ï¼Œæœªèƒ½æé«˜æ¨¡å‹å¯¹è¾“å…¥æ’åˆ—çš„å›ºæœ‰ç¨³å¥æ€§ï¼Œå¼•å‘äº†äººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§å’Œå¯é æ€§çš„æ‹…å¿§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåŸºäºåˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆDROï¼‰çš„æ’åˆ—éŸ§æ€§å­¦ä¹ ï¼ˆPEARLï¼‰æ¡†æ¶ï¼Œä¼˜åŒ–æ¨¡å‹åº”å¯¹æœ€åæƒ…å†µä¸‹è¾“å…¥æ’åˆ—çš„æ€§èƒ½ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼ŒPEARLæœ‰æ•ˆç¼“è§£æ’åˆ—æ”»å‡»ï¼Œæé«˜æ€§èƒ½ï¼Œä¸”åœ¨å°‘æ ·æœ¬å’ŒçŸ­ä¸Šä¸‹æ–‡åœºæ™¯ä¸­æ€§èƒ½æå‡æ˜¾è‘—ã€‚è¿™è¡¨æ˜PEARLå…·æœ‰è¾ƒé«˜çš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä½¿å…¶èƒ½æ‰§è¡Œå¤æ‚ä»»åŠ¡ï¼Œä½†æ¼”ç¤ºé¡ºåºå¯¹å…¶å½±å“å¤§ï¼Œå¯¼è‡´é¢„æµ‹ä¸ç¨³å®šã€‚</li>
<li>é€šè¿‡è°ƒæ•´æ¼”ç¤ºé¡ºåºå¯è¿›è¡Œè‡ªç„¶æ”»å‡»ï¼Œå¯¹ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹æ„æˆå¨èƒã€‚</li>
<li>å½“å‰ç¼“è§£æ–¹æ³•ä¸»è¦ä¾èµ–åå¤„ç†ï¼Œæ— æ³•æé«˜æ¨¡å‹å¯¹è¾“å…¥æ’åˆ—çš„ç¨³å¥æ€§ã€‚</li>
<li>æå‡ºåŸºäºåˆ†å¸ƒé²æ£’ä¼˜åŒ–çš„æ’åˆ—éŸ§æ€§å­¦ä¹ ï¼ˆPEARLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–æ¨¡å‹åº”å¯¹æœ€åæƒ…å†µä¸‹è¾“å…¥æ’åˆ—çš„æ€§èƒ½æ¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>PEARLæ¡†æ¶åŒ…æ‹¬æ’åˆ—ææ¡ˆç½‘ç»œï¼ˆP-Netï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒP-Neté€šè¿‡è§£å†³æœ€ä¼˜ä¼ è¾“é—®é¢˜ç”Ÿæˆæœ€å…·æŒ‘æˆ˜æ€§çš„æ’åˆ—ã€‚</li>
<li>PEARLæ¡†æ¶é€šè¿‡æœ€å°æœ€å¤§åŒ–ä¼˜åŒ–ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¯¹æ’åˆ—æ”»å‡»å…·æœ‰æŠµå¾¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b31dadc6a4c69312682b41c7cb340a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e557ea986a1eb8fa00bef23f810feae6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a284efcfb36d1b6752727cff75c03e6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6379df616284a91f90df58d988e92af1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Smaller-But-Better-Unifying-Layout-Generation-with-Smaller-Large-Language-Models"><a href="#Smaller-But-Better-Unifying-Layout-Generation-with-Smaller-Large-Language-Models" class="headerlink" title="Smaller But Better: Unifying Layout Generation with Smaller Large   Language Models"></a>Smaller But Better: Unifying Layout Generation with Smaller Large   Language Models</h2><p><strong>Authors:Peirong Zhang, Jiaxin Zhang, Jiahuan Cao, Hongliang Li, Lianwen Jin</strong></p>
<p>We propose LGGPT, an LLM-based model tailored for unified layout generation. First, we propose Arbitrary Layout Instruction (ALI) and Universal Layout Response (ULR) as the uniform I&#x2F;O template. ALI accommodates arbitrary layout generation task inputs across multiple layout domains, enabling LGGPT to unify both task-generic and domain-generic layout generation hitherto unexplored. Collectively, ALI and ULR boast a succinct structure that forgoes superfluous tokens typically found in existing HTML-based formats, facilitating efficient instruction tuning and boosting unified generation performance. In addition, we propose an Interval Quantization Encoding (IQE) strategy that compresses ALI into a more condensed structure. IQE precisely preserves valid layout clues while eliminating the less informative placeholders, facilitating LGGPT to capture complex and variable layout generation conditions during the unified training process. Experimental results demonstrate that LGGPT achieves superior or on par performance compared to existing methods. Notably, LGGPT strikes a prominent balance between proficiency and efficiency with a compact 1.5B parameter LLM, which beats prior 7B or 175B models even in the most extensive and challenging unified scenario. Furthermore, we underscore the necessity of employing LLMs for unified layout generation and suggest that 1.5B could be an optimal parameter size by comparing LLMs of varying scales. Code is available at <a target="_blank" rel="noopener" href="https://github.com/NiceRingNode/LGGPT">https://github.com/NiceRingNode/LGGPT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†LGGPTæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®šåˆ¶ç»Ÿä¸€å¸ƒå±€ç”Ÿæˆæ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä»»æ„å¸ƒå±€æŒ‡ä»¤ï¼ˆALIï¼‰å’Œé€šç”¨å¸ƒå±€å“åº”ï¼ˆULRï¼‰ä½œä¸ºç»Ÿä¸€çš„è¾“å…¥è¾“å‡ºæ¨¡æ¿ã€‚ALIå¯ä»¥å®¹çº³å¤šä¸ªå¸ƒå±€åŸŸçš„ä»»æ„å¸ƒå±€ç”Ÿæˆä»»åŠ¡è¾“å…¥ï¼Œä½¿å¾—LGGPTèƒ½å¤Ÿç»Ÿä¸€è¿„ä»Šä¸ºæ­¢å°šæœªæ¢ç´¢çš„ä»»åŠ¡é€šç”¨å’ŒåŸŸé€šç”¨çš„å¸ƒå±€ç”Ÿæˆã€‚æ€»çš„æ¥è¯´ï¼ŒALIå’ŒULRæ‹¥æœ‰ç®€æ´çš„ç»“æ„ï¼Œæ‘’å¼ƒäº†ç°æœ‰HTMLæ ¼å¼ä¸­é€šå¸¸å­˜åœ¨çš„å¤šä½™æ ‡è®°ï¼Œä¾¿äºé«˜æ•ˆçš„æŒ‡ä»¤è°ƒæ•´ï¼Œå¹¶æé«˜äº†ç»Ÿä¸€çš„ç”Ÿæˆæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é—´éš”é‡åŒ–ç¼–ç ï¼ˆIQEï¼‰ç­–ç•¥ï¼Œå°†ALIå‹ç¼©æˆæ›´ç´§å‡‘çš„ç»“æ„ã€‚IQEèƒ½å¤Ÿç²¾ç¡®ä¿ç•™æœ‰æ•ˆçš„å¸ƒå±€çº¿ç´¢ï¼ŒåŒæ—¶æ¶ˆé™¤ä¿¡æ¯é‡è¾ƒå°‘çš„å ä½ç¬¦ï¼Œæœ‰åŠ©äºLGGPTåœ¨ç»Ÿä¸€è®­ç»ƒè¿‡ç¨‹ä¸­æ•æ‰å¤æ‚ä¸”å¯å˜çš„å¸ƒå±€ç”Ÿæˆæ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLGGPTçš„æ€§èƒ½ä¼˜äºæˆ–ç›¸å½“äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLGGPTåœ¨ç†Ÿç»ƒå’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†çªå‡ºçš„å¹³è¡¡ï¼Œä½¿ç”¨ç´§å‡‘çš„1.5Bå‚æ•°å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”šè‡³åœ¨æœ€å¹¿æ³›å’Œæœ€å…·æŒ‘æˆ˜æ€§çš„ç»Ÿä¸€åœºæ™¯ä¸­å‡»è´¥äº†å…ˆå‰çš„7Bæˆ–175Bæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¼ºè°ƒäº†åœ¨ç»Ÿä¸€å¸ƒå±€ç”Ÿæˆä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿…è¦æ€§ï¼Œå¹¶å»ºè®®1.5Bå¯èƒ½æ˜¯æœ€ä½³å‚æ•°è§„æ¨¡ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/NiceRingNode/LGGPT%E3%80%82">https://github.com/NiceRingNode/LGGPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14005v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºLLMçš„æ¨¡å‹LGGPTï¼Œç”¨äºç»Ÿä¸€çš„å¸ƒå±€ç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥ä»»æ„å¸ƒå±€æŒ‡ä»¤ï¼ˆALIï¼‰å’Œé€šç”¨å¸ƒå±€å“åº”ï¼ˆULRï¼‰ä½œä¸ºç»Ÿä¸€è¾“å…¥è¾“å‡ºæ¨¡æ¿ï¼Œå®ç°äº†è·¨å¤šä¸ªå¸ƒå±€é¢†åŸŸçš„ç»Ÿä¸€å¸ƒå±€ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŒºé—´é‡åŒ–ç¼–ç ï¼ˆIQEï¼‰ç­–ç•¥ï¼Œç”¨äºå‹ç¼©ALIä¸ºæ›´ç´§å‡‘çš„ç»“æ„ï¼Œå¹¶ä¿ç•™æœ‰æ•ˆçš„å¸ƒå±€çº¿ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLGGPTåœ¨ç»Ÿä¸€è®­ç»ƒè¿‡ç¨‹ä¸­å–å¾—äº†ä¼˜äºæˆ–ç›¸å½“äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä½¿ç”¨ç´§å‡‘çš„1.5Bå‚æ•°LLMæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡å¹³è¡¡ï¼Œç”šè‡³åœ¨æœ€å…·æŒ‘æˆ˜çš„ç»Ÿä¸€åœºæ™¯ä¸­å‡»è´¥äº†ä¹‹å‰çš„7Bæˆ–175Bæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¼ºè°ƒäº†é‡‡ç”¨LLMè¿›è¡Œç»Ÿä¸€å¸ƒå±€ç”Ÿæˆçš„å¿…è¦æ€§ï¼Œå¹¶å»ºè®®å°†ä½¿ç”¨è§„æ¨¡ä¸º1.5Bå‚æ•°çš„LLMä½œä¸ºæœ€ä½³é€‰æ‹©ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LGGPTæ˜¯ä¸€ä¸ªåŸºäºLLMçš„æ¨¡å‹ï¼Œç”¨äºç»Ÿä¸€çš„å¸ƒå±€ç”Ÿæˆã€‚</li>
<li>ALIå’ŒULRä½œä¸ºç»Ÿä¸€è¾“å…¥è¾“å‡ºæ¨¡æ¿ï¼Œå®ç°äº†è·¨å¤šä¸ªå¸ƒå±€é¢†åŸŸçš„ç»Ÿä¸€å¸ƒå±€ç”Ÿæˆã€‚</li>
<li>IQEç­–ç•¥ç”¨äºå‹ç¼©ALIç»“æ„å¹¶ä¿ç•™æœ‰æ•ˆå¸ƒå±€çº¿ç´¢ã€‚</li>
<li>LGGPTæ€§èƒ½ä¼˜äºæˆ–ç›¸å½“äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ä½¿ç”¨ç´§å‡‘çš„1.5Bå‚æ•°LLMæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡å¹³è¡¡ã€‚</li>
<li>LGGPTåœ¨å„ç§è§„æ¨¡LLMä¹‹é—´çš„æ€§èƒ½å¯¹æ¯”ä¸­è¡¨ç°ä¼˜è¶Šï¼Œçªæ˜¾äº†ä½¿ç”¨LLMè¿›è¡Œç»Ÿä¸€å¸ƒå±€ç”Ÿæˆçš„é‡è¦æ€§ã€‚</li>
<li>LGGPTä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed122382b6c695b8152e9a1a4cc9a0a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b200081b1cc0d4a62f3378d41bb977e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fb461df8a7f0e21a549a07ff32e89c7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Optimizer-Design-for-LLM-via-Structured-Fisher-Approximation-with-a-Low-Rank-Extension"><a href="#Towards-Efficient-Optimizer-Design-for-LLM-via-Structured-Fisher-Approximation-with-a-Low-Rank-Extension" class="headerlink" title="Towards Efficient Optimizer Design for LLM via Structured Fisher   Approximation with a Low-Rank Extension"></a>Towards Efficient Optimizer Design for LLM via Structured Fisher   Approximation with a Low-Rank Extension</h2><p><strong>Authors:Wenbo Gong, Meyer Scetbon, Chao Ma, Edward Meeds</strong></p>
<p>Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2x faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory. </p>
<blockquote>
<p>è®¾è®¡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é«˜æ•ˆä¼˜åŒ–å™¨ï¼Œè¦æ±‚å†…å­˜éœ€æ±‚ä½ä¸”æ”¶æ•›é€Ÿåº¦å¿«ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡ç»“æ„åŒ–çš„Fisherä¿¡æ¯çŸ©é˜µï¼ˆFIMï¼‰è¿‘ä¼¼é€é•œï¼Œæœç€ç³»ç»Ÿåœ°è®¾è®¡æ­¤ç±»ä¼˜åŒ–å™¨è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„ä¼˜åŒ–å™¨éƒ½å¯ä»¥è¢«è§†ä¸ºæ»¡è¶³FrobeniusèŒƒæ•°ä¸‹çš„FIMè¿‘ä¼¼è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¸¦æœ‰ç‰¹å®šçš„ç»“æ„å‡è®¾ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹LLMçš„å®é™…é«˜æ•ˆä¼˜åŒ–å™¨çš„ä¸¤é¡¹è®¾è®¡å»ºè®®ï¼ŒåŒ…æ‹¬è°¨æ…é€‰æ‹©ç»“æ„å‡è®¾ä»¥å¹³è¡¡é€šç”¨æ€§å’Œæ•ˆç‡ï¼Œå¹¶é€šè¿‡æ–°å‹çš„ä½ç§©æ‰©å±•æ¡†æ¶æé«˜ä¼˜åŒ–å™¨çš„å†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬é€šè¿‡æ¨å¯¼æ–°çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨æ¥å±•ç¤ºæ¯ç§è®¾è®¡æ–¹æ³•ï¼šè¡Œå’Œåˆ—ç¼©æ”¾SGDï¼ˆRACSï¼‰å’Œè‡ªé€‚åº”ä½ç»´å­ç©ºé—´ä¼°è®¡ï¼ˆAliceï¼‰ã€‚åœ¨LLaMAé¢„è®­ç»ƒï¼ˆæœ€å¤š1äº¿ä¸ªå‚æ•°ï¼‰ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºæ¯”ç°æœ‰å†…å­˜é«˜æ•ˆçš„åŸºå‡†æµ‹è¯•å’ŒAdamæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAliceå®ç°äº†è¶…è¿‡Adamçš„2å€æ”¶æ•›é€Ÿåº¦æå‡ï¼Œè€ŒRACSåœ¨å…·æœ‰SGDå†…å­˜çš„1äº¿æ¨¡å‹ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07752v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»“æ„åŒ–Fisherä¿¡æ¯çŸ©é˜µï¼ˆFIMï¼‰è¿‘ä¼¼è§†è§’ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡é«˜æ•ˆä¼˜åŒ–å™¨çš„é—®é¢˜ã€‚æ–‡ç« å±•ç¤ºäº†è®¸å¤šå…ˆè¿›çš„ä¼˜åŒ–å™¨å¯è§†ä¸ºç‰¹å®šç»“æ„å‡è®¾ä¸‹FIMè¿‘ä¼¼è§£å†³æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†ä¸¤ç§é’ˆå¯¹LLMå®ç”¨ä¼˜åŒ–å™¨çš„è®¾è®¡å»ºè®®ã€‚é€šè¿‡æ¨å¯¼æ–°çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨Rowå’ŒColumn Scaled SGDï¼ˆRACSï¼‰å’Œè‡ªé€‚åº”ä½ç»´å­ç©ºé—´ä¼°è®¡ï¼ˆAliceï¼‰ï¼ŒéªŒè¯äº†æ‰€æè®¾è®¡æ–¹æ³•çš„å®ç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰çš„å†…å­˜é«˜æ•ˆåŸºçº¿åŠAdamä¼˜åŒ–å™¨ï¼Œè¿™äº›æ–¹æ³•æ›´åŠ æœ‰æ•ˆå¹¶å…·æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚ç‰¹åˆ«æ˜¯Aliceå®ç°äº†è¶…è¿‡Adamä¸¤å€ä»¥ä¸Šçš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥è®ºæ–‡ç ”ç©¶å¦‚ä½•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡å…·æœ‰ä½å†…å­˜è¦æ±‚å’Œå¿«é€Ÿæ”¶æ•›çš„ä¼˜åŒ–å™¨ã€‚</li>
<li>æ–‡ç« é€šè¿‡ç»“æ„åŒ–Fisherä¿¡æ¯çŸ©é˜µï¼ˆFIMï¼‰è¿‘ä¼¼è§†è§’è¿›è¡Œç³»ç»Ÿæ€§çš„ä¼˜åŒ–å™¨è®¾è®¡ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†è®¸å¤šå…ˆè¿›çš„ä¼˜åŒ–å™¨å¯ä»¥çœ‹ä½œæ˜¯ç‰¹å®šç»“æ„å‡è®¾ä¸‹çš„FIMè¿‘ä¼¼è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸¤ç§é’ˆå¯¹LLMçš„ä¼˜åŒ–å™¨è®¾è®¡å»ºè®®ï¼ŒåŒ…æ‹¬å¹³è¡¡é€šç”¨æ€§å’Œæ•ˆç‡çš„å‡è®¾é€‰æ‹©ä»¥åŠé€šè¿‡æ–°å‹ä½ç§©æ‰©å±•æ¡†æ¶æé«˜ä¼˜åŒ–å™¨çš„å†…å­˜æ•ˆç‡ã€‚</li>
<li>æ–‡ç« é€šè¿‡æ¨å¯¼æ–°ä¼˜åŒ–å™¨RACSå’ŒAliceéªŒè¯äº†è®¾è®¡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3922d82b946659dc308e3087601f6ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-248d009526d50f30cba7e12b9bfa2f1d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Simplifying-Formal-Proof-Generating-Models-with-ChatGPT-and-Basic-Searching-Techniques"><a href="#Simplifying-Formal-Proof-Generating-Models-with-ChatGPT-and-Basic-Searching-Techniques" class="headerlink" title="Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques"></a>Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques</h2><p><strong>Authors:Sangjun Han, Taeil Hur, Youngmi Hur, Kathy Sangkyung Lee, Myungyoon Lee, Hyojae Lim</strong></p>
<p>The challenge of formal proof generation has a rich history, but with modern techniques, we may finally be at the stage of making actual progress in real-life mathematical problems. This paper explores the integration of ChatGPT and basic searching techniques to simplify generating formal proofs, with a particular focus on the miniF2F dataset. We demonstrate how combining a large language model like ChatGPT with a formal language such as Lean, which has the added advantage of being verifiable, enhances the efficiency and accessibility of formal proof generation. Despite its simplicity, our best-performing Lean-based model surpasses all known benchmarks with a 31.15% pass rate. We extend our experiments to include other datasets and employ alternative language models, showcasing our modelsâ€™ comparable performance in diverse settings and allowing for a more nuanced analysis of our results. Our findings offer insights into AI-assisted formal proof generation, suggesting a promising direction for future research in formal mathematical proof. </p>
<blockquote>
<p>å½¢å¼åŒ–è¯æ˜ç”Ÿæˆè¿™ä¸€æŒ‘æˆ˜æœ‰ç€æ‚ ä¹…çš„å†å²èƒŒæ™¯ï¼Œä½†éšç€ç°ä»£æŠ€æœ¯çš„å‘å±•ï¼Œæˆ‘ä»¬å¯èƒ½ç»ˆäºè¿æ¥äº†è§£å†³ç°å®ç”Ÿæ´»ä¸­çš„æ•°å­¦é—®é¢˜æ–¹é¢å–å¾—å®é™…è¿›å±•çš„é˜¶æ®µã€‚æœ¬æ–‡æ¢è®¨äº†å°†ChatGPTå’ŒåŸºæœ¬æœç´¢æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥ç®€åŒ–å½¢å¼åŒ–è¯æ˜çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œé‡ç‚¹é›†ä¸­åœ¨miniF2Fæ•°æ®é›†ä¸Šã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç²¾ç›Šï¼ˆLeanï¼‰ç­‰å¯éªŒè¯çš„å½¢å¼è¯­è¨€ç›¸ç»“åˆï¼Œä»¥æé«˜å½¢å¼åŒ–è¯æ˜çš„ç”Ÿæˆæ•ˆç‡å’Œå¯åŠæ€§ã€‚å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹ç®€å•ï¼Œä½†åŸºäºç²¾ç›Šçš„æœ€ä½³æ¨¡å‹ä»¥31.15%çš„é€šè¿‡ç‡è¶…è¶Šäº†æ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±•å®éªŒï¼ŒåŒ…æ‹¬ä½¿ç”¨å…¶ä»–æ•°æ®é›†å’Œæ›¿ä»£è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸‹çš„ç›¸å½“æ€§èƒ½ï¼Œå¹¶å…è®¸å¯¹ç»“æœè¿›è¡Œæ›´ç»†è‡´çš„åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæä¾›äº†è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å½¢å¼åŒ–æ•°å­¦è¯æ˜çš„ç ”ç©¶æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03321v3">PDF</a> This manuscript was accepted for publication in the proceedings of   the Computing Conference 2025 (Springer LNNS). The Version of Record (VoR)   has not yet been published. This Accepted Manuscript does not reflect any   post-acceptance improvements or corrections. Use of this version is subject   to Springer Natureâ€™s Accepted Manuscript terms of use</p>
<p><strong>Summary</strong><br>ç°ä»£æŠ€æœ¯å¯èƒ½ä½¿æˆ‘ä»¬åœ¨å®é™…çš„æ•°å­¦é—®é¢˜ä¸Šå–å¾—è¿›å±•ã€‚æœ¬æ–‡æ¢è®¨äº†å°†ChatGPTå’ŒåŸºæœ¬æœç´¢æŠ€æœ¯ç›¸ç»“åˆï¼Œä»¥ç®€åŒ–ç”Ÿæˆå½¢å¼è¯æ˜çš„è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨miniF2Fæ•°æ®é›†ã€‚ç»“åˆChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¯éªŒè¯çš„Leanå½¢å¼è¯­è¨€ï¼Œæé«˜äº†å½¢å¼è¯æ˜çš„æ•ˆç‡å’Œå¯åŠæ€§ã€‚åŸºäºLeançš„æœ€ä½³æ¨¡å‹è¡¨ç°è¶…è¶Šæ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ç‡ä¸º31.15%ã€‚å®éªŒæ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†å’Œä½¿ç”¨å…¶ä»–è¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒç¯å¢ƒä¸­çš„å¯æ¯”æ€§ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œäº†æ›´ç»†è‡´çš„åˆ†æã€‚æœ¬æ–‡ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©å½¢å¼è¯æ˜ç”Ÿæˆæä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£æŠ€æœ¯æœ‰åŠ©äºç®€åŒ–ç”Ÿæˆå½¢å¼è¯æ˜çš„è¿‡ç¨‹ã€‚</li>
<li>ChatGPTå’ŒLeançš„ç»“åˆæé«˜äº†å½¢å¼è¯æ˜çš„æ•ˆç‡å’Œå¯åŠæ€§ã€‚</li>
<li>åŸºäºLeançš„æœ€ä½³æ¨¡å‹è¡¨ç°è¶…è¶Šäº†æ‰€æœ‰å·²çŸ¥åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®éªŒæ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†å’Œè¯­è¨€æ¨¡å‹ï¼Œå±•ç¤ºäº†æ¨¡å‹çš„é€‚ç”¨æ€§ã€‚</li>
<li>AIæŠ€æœ¯åœ¨å½¢å¼æ•°å­¦è¯æ˜ä¸­æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œå½¢å¼è¯­è¨€ï¼Œå¯ä»¥å®ç°æ›´é«˜æ•ˆçš„å½¢å¼è¯æ˜ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-834d52dce112d7cb6b5af5842257ed43.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9f0c532212a2fd8648edb1bc73899e73.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-22  Building reliable sim driving agents by scaling self-play
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4b11dab7dee3f9c995aa6f275a7b7d1b.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-21  FlexDuo A Pluggable System for Enabling Full-Duplex Capabilities in   Speech Dialogue Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
