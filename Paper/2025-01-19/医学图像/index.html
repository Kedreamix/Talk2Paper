<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-19  On the optimal prediction of extreme events in heavy-tailed time series   with applications to solar flare forecasting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3f7d1e18027fbbee053a2f950228f8c5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-19-æ›´æ–°"><a href="#2025-01-19-æ›´æ–°" class="headerlink" title="2025-01-19 æ›´æ–°"></a>2025-01-19 æ›´æ–°</h1><h2 id="On-the-optimal-prediction-of-extreme-events-in-heavy-tailed-time-series-with-applications-to-solar-flare-forecasting"><a href="#On-the-optimal-prediction-of-extreme-events-in-heavy-tailed-time-series-with-applications-to-solar-flare-forecasting" class="headerlink" title="On the optimal prediction of extreme events in heavy-tailed time series   with applications to solar flare forecasting"></a>On the optimal prediction of extreme events in heavy-tailed time series   with applications to solar flare forecasting</h2><p><strong>Authors:Victor Verma, Stilian Stoev, Yang Chen</strong></p>
<p>The prediction of extreme events in time series is a fundamental problem arising in many financial, scientific, engineering, and other applications. We begin by establishing a general Neyman-Pearson-type characterization of optimal extreme event predictors in terms of density ratios. This yields new insights and several closed-form optimal extreme event predictors for additive models. These results naturally extend to time series, where we study optimal extreme event prediction for both light- and heavy-tailed autoregressive and moving average models. Using a uniform law of large numbers for ergodic time series, we establish the asymptotic optimality of an empirical version of the optimal predictor for autoregressive models. Using multivariate regular variation, we obtain an expression for the optimal extremal precision in heavy-tailed infinite moving averages, which provides theoretical bounds on the ability to predict extremes in this general class of models. We address the important problem of predicting solar flares by applying our theory and methodology to a state-of-the-art time series consisting of solar soft X-ray flux measurements. Our results demonstrate the success and limitations in solar flare forecasting of long-memory autoregressive models and long-range-dependent, heavy-tailed FARIMA models. </p>
<blockquote>
<p>æç«¯äº‹ä»¶æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯é‡‘èã€ç§‘å­¦ã€å·¥ç¨‹ç­‰è®¸å¤šåº”ç”¨é¢†åŸŸä¸­ä¸€ä¸ªåŸºæœ¬çš„é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆé’ˆå¯¹å¯†åº¦æ¯”å»ºç«‹äº†ä¸€ç§ä¸€èˆ¬æ€§çš„å¥ˆæ›¼-çš®å°”é€Šå‹æç«¯äº‹ä»¶æœ€ä¼˜é¢„æµ‹å™¨çš„è¡¨å¾ã€‚è¿™ä¸ºåŠ æ³•æ¨¡å‹æä¾›äº†æ–°çš„è§è§£å’Œå‡ ä¸ªå°é—­å½¢å¼çš„æœ€ä½³æç«¯äº‹ä»¶é¢„æµ‹å™¨ã€‚è¿™äº›ç»“æœè‡ªç„¶åœ°æ‰©å±•åˆ°æ—¶é—´åºåˆ—ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è½»å°¾å’Œé‡å°¾è‡ªå›å½’å’Œç§»åŠ¨å¹³å‡æ¨¡å‹çš„æœ€ä½³æç«¯äº‹ä»¶é¢„æµ‹ã€‚åˆ©ç”¨éå†æ—¶é—´åºåˆ—çš„å¤§æ•°å®šå¾‹ï¼Œæˆ‘ä»¬å»ºç«‹äº†è‡ªå›å½’æ¨¡å‹æœ€ä¼˜é¢„æµ‹å™¨ç»éªŒç‰ˆæœ¬çš„æ¸è¿‘æœ€ä¼˜æ€§ã€‚åˆ©ç”¨å¤šå…ƒæ­£åˆ™å˜åŒ–ï¼Œæˆ‘ä»¬å¾—åˆ°äº†é‡å°¾æ— é™ç§»åŠ¨å¹³å‡ä¸­çš„æœ€ä¼˜æå€¼ç²¾åº¦çš„è¡¨è¾¾å¼ï¼Œè¿™ä¸ºé¢„æµ‹æ­¤ç±»ä¸€èˆ¬æ¨¡å‹ä¸­æç«¯äº‹ä»¶çš„èƒ½åŠ›æä¾›äº†ç†è®ºç•Œé™ã€‚æˆ‘ä»¬é€šè¿‡å°†ç†è®ºå’Œæ–¹æ³•è®ºåº”ç”¨äºæœ€å…ˆè¿›çš„å¤ªé˜³è½¯Xå°„çº¿æµé‡æµ‹é‡æ—¶é—´åºåˆ—æ¥è§£å†³é¢„æµ‹å¤ªé˜³è€€æ–‘è¿™ä¸€é‡è¦é—®é¢˜ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†é•¿è®°å¿†è‡ªå›å½’æ¨¡å‹å’Œé•¿ç¨‹ä¾èµ–é‡å°¾FARIMAæ¨¡å‹åœ¨å¤ªé˜³è€€æ–‘é¢„æŠ¥ä¸­çš„æˆåŠŸå’Œå±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11887v2">PDF</a> 62 pages, 6 figures. Revised version accepted for publication in the   Journal of Time Series Analysis</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ—¶é—´åºåˆ—ä¸­æç«¯äº‹ä»¶çš„é¢„æµ‹é—®é¢˜ï¼Œåœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚æ–‡ç« å»ºç«‹äº†æœ€ä¼˜æç«¯äº‹ä»¶é¢„æµ‹å™¨çš„Neyman-Pearsonç±»å‹è¡¨å¾ï¼Œä¸ºåŠ æ³•æ¨¡å‹æä¾›äº†å°é—­å½¢å¼çš„æœ€ä¼˜æç«¯äº‹ä»¶é¢„æµ‹å™¨ã€‚æ­¤å¤–ï¼Œæ–‡ç« ç ”ç©¶äº†è‡ªå›å½’å’Œç§»åŠ¨å¹³å‡æ¨¡å‹çš„è½»é‡å°¾æç«¯äº‹ä»¶é¢„æµ‹ï¼Œå¹¶å»ºç«‹äº†è‡ªå›å½’æ¨¡å‹é¢„æµ‹å™¨çš„ç»éªŒç‰ˆæœ¬æ¸è¿‘æœ€ä¼˜æ€§ã€‚å¯¹äºé‡å°¾æ— é™ç§»åŠ¨å¹³å‡æ¨¡å‹ï¼Œæ–‡ç« è·å¾—äº†æœ€ä¼˜æç«¯ç²¾åº¦è¡¨è¾¾å¼ï¼Œä¸ºé¢„æµ‹æç«¯äº‹ä»¶æä¾›äº†ç†è®ºç•Œé™ã€‚æœ€åï¼Œæ–‡ç« å°†ç†è®ºå’Œæ–¹æ³•åº”ç”¨äºå¤ªé˜³è½¯Xå°„çº¿æµé‡æµ‹é‡æ—¶é—´åºåˆ—ï¼Œå±•ç¤ºäº†é•¿æœŸè®°å¿†è‡ªå›å½’æ¨¡å‹å’Œé•¿æœŸä¾èµ–é‡å°¾FARIMAæ¨¡å‹åœ¨å¤ªé˜³è€€æ–‘é¢„æµ‹ä¸­çš„æˆåŠŸå’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»ºç«‹äº†æœ€ä¼˜æç«¯äº‹ä»¶é¢„æµ‹å™¨çš„Neyman-Pearsonç±»å‹è¡¨å¾ã€‚</li>
<li>ä¸ºåŠ æ³•æ¨¡å‹æä¾›äº†å°é—­å½¢å¼çš„æœ€ä¼˜æç«¯äº‹ä»¶é¢„æµ‹å™¨ã€‚</li>
<li>ç ”ç©¶äº†è‡ªå›å½’å’Œç§»åŠ¨å¹³å‡æ¨¡å‹çš„è½»é‡å°¾æç«¯äº‹ä»¶é¢„æµ‹ã€‚</li>
<li>å»ºç«‹äº†è‡ªå›å½’æ¨¡å‹é¢„æµ‹å™¨çš„ç»éªŒç‰ˆæœ¬æ¸è¿‘æœ€ä¼˜æ€§ã€‚</li>
<li>å¯¹äºé‡å°¾æ— é™ç§»åŠ¨å¹³å‡æ¨¡å‹ï¼Œè·å¾—äº†æœ€ä¼˜æç«¯ç²¾åº¦è¡¨è¾¾å¼ã€‚</li>
<li>å°†ç†è®ºå’Œæ–¹æ³•åº”ç”¨äºå¤ªé˜³è½¯Xå°„çº¿æµé‡æµ‹é‡æ—¶é—´åºåˆ—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9b1f07dca5f9e98fa1ab08acb3f73f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d520dbca9da43f6b6342dde229ddfff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f7d1e18027fbbee053a2f950228f8c5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhanced-Masked-Image-Modeling-to-Avoid-Model-Collapse-on-Multi-modal-MRI-Datasets"><a href="#Enhanced-Masked-Image-Modeling-to-Avoid-Model-Collapse-on-Multi-modal-MRI-Datasets" class="headerlink" title="Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal   MRI Datasets"></a>Enhanced Masked Image Modeling to Avoid Model Collapse on Multi-modal   MRI Datasets</h2><p><strong>Authors:Linxuan Han, Sa Xiao, Zimeng Li, Haidong Li, Xiuchao Zhao, Yeqing Han, Fumin Guo, Xin Zhou</strong></p>
<p>Multi-modal magnetic resonance imaging (MRI) provides information of lesions for computer-aided diagnosis from different views. Deep learning algorithms are suitable for identifying specific anatomical structures, segmenting lesions, and classifying diseases. Manual labels are limited due to the high expense, which hinders further improvement of accuracy. Self-supervised learning, particularly masked image modeling (MIM), has shown promise in utilizing unlabeled data. However, we spot model collapse when applying MIM to multi-modal MRI datasets. The performance of downstream tasks does not see any improvement following the collapsed model. To solve model collapse, we analyze and address it in two types: complete collapse and dimensional collapse. We find complete collapse occurs because the collapsed loss value in multi-modal MRI datasets falls below the normally converged loss value. Based on this, the hybrid mask pattern (HMP) masking strategy is introduced to elevate the collapsed loss above the normally converged loss value and avoid complete collapse. Additionally, we reveal that dimensional collapse stems from insufficient feature uniformity in MIM. We mitigate dimensional collapse by introducing the pyramid barlow twins (PBT) module as an explicit regularization method. Overall, we construct the enhanced MIM (E-MIM) with HMP and PBT module to avoid model collapse multi-modal MRI. Experiments are conducted on three multi-modal MRI datasets to validate the effectiveness of our approach in preventing both types of model collapse. By preventing model collapse, the training of the model becomes more stable, resulting in a decent improvement in performance for segmentation and classification tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LinxuanHan/E-MIM">https://github.com/LinxuanHan/E-MIM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä»ä¸åŒè§†è§’ä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­æä¾›ç—…å˜ä¿¡æ¯ã€‚æ·±åº¦å­¦ä¹ ç®—æ³•é€‚ç”¨äºè¯†åˆ«ç‰¹å®šè§£å‰–ç»“æ„ã€åˆ†å‰²ç—…å˜å’Œåˆ†ç±»ç–¾ç—…ã€‚ç”±äºé«˜æ˜‚çš„è´¹ç”¨ï¼Œæ‰‹åŠ¨æ ‡ç­¾å—åˆ°é™åˆ¶ï¼Œè¿™é˜»ç¢äº†å‡†ç¡®æ€§çš„è¿›ä¸€æ­¥æé«˜ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼Œå°¤å…¶æ˜¯æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰åœ¨åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨å°†MIMåº”ç”¨äºå¤šæ¨¡æ€MRIæ•°æ®é›†æ—¶å‘ç°äº†æ¨¡å‹å´©æºƒçš„æƒ…å†µã€‚å´©æºƒçš„æ¨¡å‹å¹¶æœªæ”¹å–„ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³æ¨¡å‹å´©æºƒé—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æå’Œè§£å†³äº†ä¸¤ç§ç±»å‹çš„å´©æºƒï¼šå®Œå…¨å´©æºƒå’Œç»´åº¦å´©æºƒã€‚æˆ‘ä»¬å‘ç°å®Œå…¨å´©æºƒæ˜¯ç”±äºå¤šæ¨¡æ€MRIæ•°æ®é›†ä¸­çš„æŸå¤±å€¼å´©æºƒä½äºæ­£å¸¸æ”¶æ•›çš„æŸå¤±å€¼ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆæ©è†œæ¨¡å¼ï¼ˆHMPï¼‰æ©è†œç­–ç•¥ï¼Œå°†å´©æºƒçš„æŸå¤±å€¼æå‡åˆ°æ­£å¸¸æ”¶æ•›çš„æŸå¤±å€¼ä¹‹ä¸Šï¼Œé¿å…å®Œå…¨å´©æºƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç»´åº¦å´©æºƒæºäºMIMä¸­ç‰¹å¾å‡åŒ€æ€§ä¸è¶³ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥é‡‘å­—å¡”å·´æ´›å­ªç”Ÿï¼ˆPBTï¼‰æ¨¡å—ä½œä¸ºæ˜¾å¼æ­£åˆ™åŒ–æ–¹æ³•æ¥ç¼“è§£ç»´åº¦å´©æºƒã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ç»“åˆäº†HMPå’ŒPBTæ¨¡å—æ„å»ºäº†å¢å¼ºå‹MIMï¼ˆE-MIMï¼‰æ¥é¿å…å¤šæ¨¡æ€MRIçš„æ¨¡å‹å´©æºƒã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•åœ¨é˜²æ­¢ä¸¤ç§æ¨¡å‹å´©æºƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡é˜²æ­¢æ¨¡å‹å´©æºƒï¼Œæ¨¡å‹çš„è®­ç»ƒå˜å¾—æ›´åŠ ç¨³å®šï¼Œåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LinxuanHan/E-MIM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LinxuanHan/E-MIMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10377v4">PDF</a> This work has been submitted to the lEEE for possible publication.   copyright may be transferred without notice, after which this version may no   longer be accessible</p>
<p><strong>Summary</strong><br>    å¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒç»“åˆæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡ä¸åŒè§†è§’ä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­æä¾›ç—…å˜ä¿¡æ¯ã€‚è‡ªç›‘ç£å­¦ä¹ ä¸­çš„æ©è†œå›¾åƒå»ºæ¨¡åœ¨åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åº”ç”¨äºå¤šæ¨¡æ€MRIæ•°æ®é›†æ—¶ä¼šå‡ºç°æ¨¡å‹å´©æºƒé—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ··åˆæ©è†œç­–ç•¥å’Œé‡‘å­—å¡”BarlowåŒç”Ÿæ¨¡å—ï¼Œå¢å¼ºäº†æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆE-MIMï¼‰ï¼Œæœ‰æ•ˆé¿å…äº†æ¨¡å‹å´©æºƒï¼Œæé«˜äº†åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒä»ä¸åŒè§†è§’ä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­æä¾›ç—…å˜ä¿¡æ¯ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨è¯†åˆ«ç‰¹å®šè§£å‰–ç»“æ„ã€åˆ†å‰²ç—…å˜å’Œç–¾ç—…åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ ä¸­çš„æ©è†œå›¾åƒå»ºæ¨¡åœ¨åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>æ©è†œå›¾åƒå»ºæ¨¡åº”ç”¨äºå¤šæ¨¡æ€MRIæ•°æ®é›†æ—¶å¯èƒ½å‡ºç°æ¨¡å‹å´©æºƒé—®é¢˜ã€‚</li>
<li>æ¨¡å‹å´©æºƒåˆ†ä¸ºå®Œå…¨å´©æºƒå’Œç»´åº¦å´©æºƒä¸¤ç§ç±»å‹ï¼Œéœ€åˆ†åˆ«åº”å¯¹ã€‚</li>
<li>å¼•å…¥æ··åˆæ©è†œç­–ç•¥å’Œé‡‘å­—å¡”BarlowåŒç”Ÿæ¨¡å—å¢å¼ºæ©è†œå›¾åƒå»ºæ¨¡ï¼Œè§£å†³æ¨¡å‹å´©æºƒé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.10377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36044f2ea4f159b542a1f71cab96a3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f16b906b38fc97da4b122b02d87547c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b3b6deb2f613c5cf5e79e3d7f485617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba6c54733130ddf61363b4cac937adb0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="reBEN-Refined-BigEarthNet-Dataset-for-Remote-Sensing-Image-Analysis"><a href="#reBEN-Refined-BigEarthNet-Dataset-for-Remote-Sensing-Image-Analysis" class="headerlink" title="reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis"></a>reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis</h2><p><strong>Authors:Kai Norman Clasen, Leonard Hackel, Tom Burgert, Gencer Sumbul, BegÃ¼m Demir, Volker Markl</strong></p>
<p>This paper presents refined BigEarthNet (reBEN) that is a large-scale, multi-modal remote sensing dataset constructed to support deep learning (DL) studies for remote sensing image analysis. The reBEN dataset consists of 549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN, we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m. We apply atmospheric correction to the Sentinel-2 patches using the latest version of the sen2cor tool, resulting in higher-quality patches compared to those present in BigEarthNet. Each patch is then associated with a pixel-level reference map and scene-level multi-labels. This makes reBEN suitable for pixel- and scene-based learning tasks. The labels are derived from the most recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class nomenclature as in BigEarthNet. The use of the most recent CLC map results in overcoming the label noise present in BigEarthNet. Furthermore, we introduce a new geographical-based split assignment algorithm that significantly reduces the spatial correlation among the train, validation, and test sets with respect to those present in BigEarthNet. This increases the reliability of the evaluation of DL models. To minimize the DL model training time, we introduce software tools that convert the reBEN dataset into a DL-optimized data format. In our experiments, we show the potential of reBEN for multi-modal multi-label image classification problems by considering several state-of-the-art DL models. The pre-trained model weights, associated code, and complete dataset are available at <a target="_blank" rel="noopener" href="https://bigearth.net/">https://bigearth.net</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç²¾ç»†åŒ–çš„BigEarthNetï¼ˆreBENï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæ”¯æŒé¥æ„Ÿå›¾åƒåˆ†æçš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ç ”ç©¶è€Œæ„å»ºçš„å¤§è§„æ¨¡ã€å¤šæ¨¡æ€é¥æ„Ÿæ•°æ®é›†ã€‚reBENæ•°æ®é›†åŒ…å«549,488å¯¹Sentinel-1å’ŒSentinel-2å›¾åƒè¡¥ä¸ã€‚ä¸ºäº†æ„å»ºreBENï¼Œæˆ‘ä»¬é¦–å…ˆè€ƒè™‘äº†ç”¨äºæ„å»ºBigEarthNetæ•°æ®é›†çš„Sentinel-1å’ŒSentinel-2ç“¦ç‰‡ï¼Œç„¶åå°†å…¶åˆ’åˆ†ä¸ºå¤§å°ä¸º1200ç±³x 1200ç±³çš„è¡¥ä¸ã€‚æˆ‘ä»¬å¯¹Sentinel-2è¡¥ä¸åº”ç”¨äº†ä½¿ç”¨sen2corå·¥å…·æœ€æ–°ç‰ˆæœ¬çš„å¤§æ°”æ ¡æ­£ï¼Œç»“æœäº§ç”Ÿäº†ä¸BigEarthNetä¸­ç°æœ‰çš„è¡¥ä¸ç›¸æ¯”è´¨é‡æ›´é«˜çš„è¡¥ä¸ã€‚ç„¶åï¼Œæ¯ä¸ªè¡¥ä¸éƒ½ä¸åƒç´ çº§å‚è€ƒåœ°å›¾å’Œåœºæ™¯çº§å¤šæ ‡ç­¾ç›¸å…³è”ã€‚è¿™ä½¿å¾—reBENé€‚åˆç”¨äºåƒç´ å’Œåœºæ™¯åŸºç¡€çš„å­¦ä¹ ä»»åŠ¡ã€‚æ ‡ç­¾æ˜¯é€šè¿‡åˆ©ç”¨BigEarthNetä¸­çš„19ç±»å‘½åæ³•ï¼Œä»æœ€æ–°çš„2018å¹´CORINEåœŸåœ°è¦†ç›–ï¼ˆCLCï¼‰åœ°å›¾ä¸­å¾—å‡ºçš„ã€‚ä½¿ç”¨æœ€æ–°çš„CLCåœ°å›¾å…‹æœäº†BigEarthNetä¸­å­˜åœ¨çš„æ ‡ç­¾å™ªå£°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºåœ°ç†çš„åˆ†å‰²åˆ†é…ç®—æ³•ï¼Œè¯¥ç®—æ³•æ˜¾è‘—å‡å°‘äº†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§ï¼Œä¸BigEarthNetä¸­çš„ç›¸å…³é›†ç›¸æ¯”ã€‚è¿™å¢åŠ äº†æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºäº†æœ€å°åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ—¶é—´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å°†reBENæ•°æ®é›†è½¬æ¢ä¸ºæ·±åº¦å­¦ä¹ ä¼˜åŒ–æ•°æ®æ ¼å¼çš„è½¯ä»¶å·¥å…·ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è€ƒè™‘ä¸€äº›æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå±•ç¤ºäº†reBENåœ¨å¤šæ¨¡æ€å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»é—®é¢˜ä¸Šçš„æ½œåŠ›ã€‚é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€ç›¸å…³ä»£ç å’Œå®Œæ•´æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://bigearth.netä¸Šæ‰¾åˆ°./">https://bigearth.netä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03653v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡æ¨å‡ºç²¾ç»†åŒ–çš„BigEarthNetï¼ˆreBENï¼‰ï¼Œè¿™æ˜¯ä¸€å¥—å¤§å‹ã€å¤šæ¨¡å¼é¥æ„Ÿæ•°æ®é›†ï¼Œä¸“ä¸ºæ”¯æŒé¥æ„Ÿå›¾åƒåˆ†æçš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ç ”ç©¶è€Œæ„å»ºã€‚reBENæ•°æ®é›†åŒ…å«549,488å¯¹Sentinel-1å’ŒSentinel-2å›¾åƒè¡¥ä¸ã€‚ç›¸è¾ƒäºBigEarthNetï¼Œé€šè¿‡å¯¹Sentinel-2è¡¥ä¸åº”ç”¨å¤§æ°”æ ¡æ­£åŠé‡‡ç”¨æœ€æ–°sen2corå·¥å…·ç‰ˆæœ¬ï¼ŒreBENçš„å›¾åƒè´¨é‡æ›´é«˜ã€‚æ¯ä¸ªè¡¥ä¸éƒ½ä¸åƒç´ çº§å‚è€ƒåœ°å›¾å’Œåœºæ™¯çº§å¤šæ ‡ç­¾ç›¸å…³è”ï¼Œä½¿å…¶é€‚åˆåƒç´ å’Œåœºæ™¯åŸºç¡€çš„å­¦ä¹ ä»»åŠ¡ã€‚æ ‡ç­¾ç”±æœ€æ–°çš„2018å¹´CORINEåœŸåœ°è¦†ç›–ï¼ˆCLCï¼‰åœ°å›¾æ´¾ç”Ÿè€Œæ¥ï¼Œå…‹æœäº†BigEarthNetä¸­çš„æ ‡ç­¾å™ªå£°é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥æ–°çš„åŸºäºåœ°ç†çš„åˆ†å‰²åˆ†é…ç®—æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§ï¼Œæé«˜äº†æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºå‡å°‘æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒæ—¶é—´ï¼Œæˆ‘ä»¬æä¾›è½¯ä»¶å·¥å…·å°†reBENæ•°æ®é›†è½¬æ¢ä¸ºæ·±åº¦å­¦ä¹ ä¼˜åŒ–æ ¼å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒreBENåœ¨å¤šæ¨¡å¼å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»é—®é¢˜ä¸Šå…·æœ‰æ½œåŠ›ï¼Œå¯é€šè¿‡è€ƒè™‘è‹¥å¹²æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡ŒéªŒè¯ã€‚é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€ç›¸å…³ä»£ç å’Œå®Œæ•´æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://bigearth.netè®¿é—®./">https://bigearth.netè®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>reBENæ˜¯ä¸€ä¸ªå¤§å‹ã€å¤šæ¨¡å¼é¥æ„Ÿæ•°æ®é›†ï¼Œç”¨äºæ”¯æŒæ·±åº¦å­¦ä¹ ç ”ç©¶ã€‚</li>
<li>reBENåŒ…å«é«˜è´¨é‡å›¾åƒè¡¥ä¸ï¼Œé€šè¿‡åº”ç”¨å¤§æ°”æ ¡æ­£å’Œé‡‡ç”¨æœ€æ–°sen2corå·¥å…·ç‰ˆæœ¬æ”¹è¿›äº†å›¾åƒè´¨é‡ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æ¯ä¸ªè¡¥ä¸éƒ½ä¸åƒç´ çº§å‚è€ƒåœ°å›¾å’Œåœºæ™¯çº§å¤šæ ‡ç­¾å…³è”ï¼Œé€‚åˆå¤šç§å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨æœ€æ–°çš„CORINEåœŸåœ°è¦†ç›–ï¼ˆCLCï¼‰åœ°å›¾ï¼Œå‡å°‘äº†æ ‡ç­¾å™ªå£°é—®é¢˜ã€‚</li>
<li>æ–°çš„åŸºäºåœ°ç†çš„åˆ†å‰²åˆ†é…ç®—æ³•é™ä½äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§ï¼Œæé«˜äº†æ¨¡å‹è¯„ä¼°çš„å¯é æ€§ã€‚</li>
<li>æä¾›è½¯ä»¶å·¥å…·å°†æ•°æ®é›†è½¬æ¢ä¸ºæ·±åº¦å­¦ä¹ ä¼˜åŒ–æ ¼å¼ï¼Œä»¥ç¼©çŸ­æ¨¡å‹è®­ç»ƒæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa1782d19e842b27eafbbae49f84e062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80c3481a01281067a3e2b2d96adf9c9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5907d632d449e0b3f3852416e0868bf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15cf0faee643cbfa1cbb75a0357ff760.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CMRxRecon2024-A-Multi-Modality-Multi-View-K-Space-Dataset-Boosting-Universal-Machine-Learning-for-Accelerated-Cardiac-MRI"><a href="#CMRxRecon2024-A-Multi-Modality-Multi-View-K-Space-Dataset-Boosting-Universal-Machine-Learning-for-Accelerated-Cardiac-MRI" class="headerlink" title="CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting   Universal Machine Learning for Accelerated Cardiac MRI"></a>CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting   Universal Machine Learning for Accelerated Cardiac MRI</h2><p><strong>Authors:Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Cheng Ouyang, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang</strong></p>
<p>Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability to provide diverse information with multiple modalities and anatomical views. Accelerated cardiac MRI is highly expected to achieve time-efficient and patient-friendly imaging, and then advanced image reconstruction approaches are required to recover high-quality, clinically interpretable images from undersampled measurements. However, the lack of publicly available cardiac MRI k-space dataset in terms of both quantity and diversity has severely hindered substantial technological progress, particularly for data-driven artificial intelligence. Here, we provide a standardized, diverse, and high-quality CMRxRecon2024 dataset to facilitate the technical development, fair evaluation, and clinical transfer of cardiac MRI reconstruction approaches, towards promoting the universal frameworks that enable fast and robust reconstructions across different cardiac MRI protocols in clinical practice. To the best of our knowledge, the CMRxRecon2024 dataset is the largest and most protocal-diverse publicly available cardiac k-space dataset. It is acquired from 330 healthy volunteers, covering commonly used modalities, anatomical views, and acquisition trajectories in clinical cardiac MRI workflows. Besides, an open platform with tutorials, benchmarks, and data processing tools is provided to facilitate data usage, advanced method development, and fair performance evaluation. </p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å·²æˆä¸ºä¸´åºŠè¯Šæ–­å¿ƒè„ç–¾ç—…çš„é‡‘æ ‡å‡†æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿæä¾›å¤šç§æ¨¡æ€å’Œè§£å‰–è§†è§’çš„å¤šæ ·åŒ–ä¿¡æ¯ã€‚åŠ é€Ÿå¿ƒè„MRIè¢«å¯„äºˆåšæœ›ï¼Œä»¥å®ç°é«˜æ•ˆä¸”æ‚£è€…å‹å¥½çš„æˆåƒï¼Œå› æ­¤éœ€è¦å…ˆè¿›çš„å›¾åƒé‡å»ºæ–¹æ³•ä»æ¬ é‡‡æ ·çš„æµ‹é‡ä¸­æ¢å¤é«˜è´¨é‡ã€å¯ä¸´åºŠè§£è¯»çš„å›¾åƒã€‚ç„¶è€Œï¼Œç¼ºä¹å…¬å¼€å¯ç”¨çš„å¿ƒè„MRI k-spaceæ•°æ®é›†ï¼Œæ— è®ºæ˜¯åœ¨æ•°é‡è¿˜æ˜¯å¤šæ ·æ€§æ–¹é¢ï¼Œéƒ½ä¸¥é‡é˜»ç¢äº†å®è´¨æ€§çš„æŠ€æœ¯è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ•°æ®é©±åŠ¨çš„äººå·¥æ™ºèƒ½ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†æ ‡å‡†åŒ–ã€å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„CMRxRecon2024æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¿ƒè„MRIé‡å»ºæ–¹æ³•çš„æŠ€æœ¯å‘å±•ã€å…¬å¹³è¯„ä¼°ä»¥åŠä¸´åºŠè½¬åŒ–ï¼Œæ¨åŠ¨åœ¨ä¸´åºŠå®è·µä¸­ä¸åŒå¿ƒè„MRIåè®®éƒ½èƒ½å¿«é€Ÿç¨³å¥é‡å»ºçš„é€šç”¨æ¡†æ¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCMRxRecon2024æ•°æ®é›†æ˜¯æœ€å¤§ä¸”åè®®æœ€å¤šæ ·åŒ–çš„å…¬å¼€å¯ç”¨çš„å¿ƒè„k-spaceæ•°æ®é›†ã€‚å®ƒæ¥è‡ª330åå¥åº·å¿—æ„¿è€…çš„æ•°æ®ï¼Œæ¶µç›–äº†ä¸´åºŠå¿ƒè„MRIå·¥ä½œæµç¨‹ä¸­å¸¸ç”¨çš„æ¨¡æ€ã€è§£å‰–è§†è§’å’Œé‡‡é›†è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æ”¾å¹³å°ï¼Œæä¾›æ•™ç¨‹ã€åŸºå‡†æµ‹è¯•å’Œæ•°æ®å¤„ç†å·¥å…·ï¼Œä»¥ä¿ƒè¿›æ•°æ®ä½¿ç”¨ã€å…ˆè¿›æ–¹æ³•å¼€å‘å’Œå…¬å¹³æ€§èƒ½è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.19043v2">PDF</a> 23 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä½œä¸ºè¯Šæ–­å¿ƒè„ç–¾ç—…çš„ä¸´åºŠé‡‘æ ‡å‡†æŠ€æœ¯ï¼Œå…¶èƒ½å¤Ÿæä¾›å¤šç§æ¨¡æ€å’Œè§£å‰–è§†å›¾çš„ä¿¡æ¯ã€‚ä¸ºäº†ä»æ¬ é‡‡æ ·çš„æµ‹é‡ä¸­æ¢å¤é«˜è´¨é‡ã€å¯ä¸´åºŠè§£è¯»çš„å›¾åƒï¼Œéœ€è¦å…ˆè¿›çš„å›¾åƒé‡å»ºæ–¹æ³•ã€‚ç”±äºç¼ºä¹å…¬å¼€å¯ç”¨çš„å¿ƒè„MRI k-spaceæ•°æ®é›†ï¼Œåœ¨æ•°é‡å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é©±åŠ¨çš„äººå·¥æ™ºèƒ½æ–¹é¢ï¼Œé˜»ç¢äº†é‡å¤§æŠ€æœ¯è¿›æ­¥ã€‚ä¸ºæ­¤ï¼Œæä¾›äº†æ ‡å‡†åŒ–ã€å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„CMRxRecon2024æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¿ƒè„MRIé‡å»ºæ–¹æ³•çš„æŠ€æœ¯å‘å±•ã€å…¬å¹³è¯„ä¼°å’Œä¸´åºŠè½¬åŒ–ï¼Œæ¨åŠ¨åœ¨ä¸´åºŠå®è·µä¸­ä¸åŒå¿ƒè„MRIåè®®å®ç°å¿«é€Ÿç¨³å¥é‡å»ºçš„é€šç”¨æ¡†æ¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCMRxRecon2024æ•°æ®é›†æ˜¯æœ€å¤§ä¸”åè®®æœ€å¤šå…ƒçš„å¿ƒè„k-spaceå…¬å¼€æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„MRIæ˜¯è¯Šæ–­å¿ƒè„ç–¾ç—…çš„é‡‘æ ‡å‡†æŠ€æœ¯ï¼Œå…·æœ‰æä¾›å¤šç§æ¨¡æ€å’Œè§£å‰–è§†å›¾ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>åŠ é€Ÿå¿ƒè„MRIéœ€è¦å…ˆè¿›çš„å›¾åƒé‡å»ºæ–¹æ³•æ¥æ¢å¤é«˜è´¨é‡çš„ä¸´åºŠå›¾åƒã€‚</li>
<li>ç¼ºä¹å…¬å¼€å¯ç”¨çš„å¿ƒè„MRI k-spaceæ•°æ®é›†åœ¨æ•°é‡å’Œå¤šæ ·æ€§æ–¹é¢é™åˆ¶äº†æŠ€æœ¯è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®é©±åŠ¨çš„AIæ–¹é¢ã€‚</li>
<li>CMRxRecon2024æ•°æ®é›†æ˜¯æœ€å¤§ä¸”åè®®æœ€å¤šå…ƒçš„å¿ƒè„k-spaceå…¬å¼€æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–äº†ä¸´åºŠå¿ƒè„MRIå·¥ä½œæµç¨‹ä¸­å¸¸ç”¨çš„æ¨¡æ€ã€è§£å‰–è§†å›¾å’Œé‡‡é›†è½¨è¿¹ã€‚</li>
<li>æ•°æ®é›†ä»330åå¥åº·å¿—æ„¿è€…ä¸­è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.19043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c05bfb63ae1809a52f93dff907c8f7f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VIS-MAE-An-Efficient-Self-supervised-Learning-Approach-on-Medical-Image-Segmentation-and-Classification"><a href="#VIS-MAE-An-Efficient-Self-supervised-Learning-Approach-on-Medical-Image-Segmentation-and-Classification" class="headerlink" title="VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image   Segmentation and Classification"></a>VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image   Segmentation and Classification</h2><p><strong>Authors:Zelong Liu, Andrew Tieu, Nikhil Patel, Georgios Soultanidis, Louisa Deyer, Ying Wang, Sean Huver, Alexander Zhou, Yunhao Mei, Zahi A. Fayad, Timothy Deyer, Xueyan Mei</strong></p>
<p>Artificial Intelligence (AI) has the potential to revolutionize diagnosis and segmentation in medical imaging. However, development and clinical implementation face multiple challenges including limited data availability, lack of generalizability, and the necessity to incorporate multi-modal data effectively. A foundation model, which is a large-scale pre-trained AI model, offers a versatile base that can be adapted to a variety of specific tasks and contexts. Here, we present VIsualization and Segmentation Masked AutoEncoder (VIS-MAE), novel model weights specifically designed for medical imaging. Specifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images from various modalities (CT, MR, PET,X-rays, and ultrasound), using self-supervised learning techniques. It is then adapted to classification and segmentation tasks using explicit labels. VIS-MAE has high label efficiency, outperforming several benchmark models in both in-domain and out-of-domain applications. In addition, VIS-MAE has improved label efficiency as it can achieve similar performance to other models with a reduced amount of labeled training data (50% or 80%) compared to other pre-trained weights. VIS-MAE represents a significant advancement in medical imaging AI, offering a generalizable and robust solution for improving segmentation and classification tasks while reducing the data annotation workload. The source code of this work is available at <a target="_blank" rel="noopener" href="https://github.com/lzl199704/VIS-MAE">https://github.com/lzl199704/VIS-MAE</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æœ‰æ½œåŠ›åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå®ç°è¯Šæ–­å’Œåˆ†å‰²çš„é©å‘½æ€§å˜é©ã€‚ç„¶è€Œï¼Œå…¶åœ¨å¼€å‘å’Œä¸´åºŠåº”ç”¨è¿‡ç¨‹ä¸­é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®æœ‰é™ã€ç¼ºä¹é€šç”¨æ€§ä»¥åŠå¿…é¡»æœ‰æ•ˆåœ°æ•´åˆå¤šæ¨¡æ€æ•°æ®ç­‰ã€‚åŸºç¡€æ¨¡å‹æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒçš„AIæ¨¡å‹ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„åŸºç¡€ï¼Œå¯ä»¥é€‚åº”å„ç§ç‰¹å®šä»»åŠ¡å’Œä¸Šä¸‹æ–‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¨å‡ºVIsualization and Segmentation Masked AutoEncoderï¼ˆVIS-MAEï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨é’ˆå¯¹åŒ»å­¦æˆåƒè®¾è®¡çš„å…¨æ–°æ¨¡å‹æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼ŒVIS-MAEåœ¨åŒ…å«æ¥è‡ªå„ç§æ¨¡æ€ï¼ˆCTã€MRã€PETã€Xå°„çº¿å’Œè¶…å£°æ³¢ï¼‰çš„250ä¸‡å¼ æ— æ ‡ç­¾å›¾åƒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚ç„¶åï¼Œå®ƒé€‚åº”äºä½¿ç”¨æ˜ç¡®æ ‡ç­¾çš„åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚VIS-MAEå…·æœ‰å¾ˆé«˜çš„æ ‡ç­¾æ•ˆç‡ï¼Œåœ¨é¢†åŸŸå†…å¤–åº”ç”¨ä¸­å‡ä¼˜äºå¤šä¸ªåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVIS-MAEæ”¹è¿›äº†æ ‡ç­¾æ•ˆç‡ï¼Œä¸å…¶ä»–é¢„è®­ç»ƒæƒé‡ç›¸æ¯”ï¼Œå®ƒå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„æ ‡è®°è®­ç»ƒæ•°æ®ï¼ˆ50%æˆ–80%ï¼‰å®ç°ä¸å…¶ä»–æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚VIS-MAEåœ¨åŒ»å­¦æˆåƒAIä¸­ä»£è¡¨äº†é‡å¤§è¿›å±•ï¼Œä¸ºæ”¹å–„åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡åŒæ—¶å‡å°‘æ•°æ®æ³¨é‡Šå·¥ä½œé‡æä¾›äº†é€šç”¨ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥å·¥ä½œçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lzl199704/VIS-MAE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lzl199704/VIS-MAEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.01034v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œå¯æ¨åŠ¨è¯Šæ–­å’Œåˆ†å‰²çš„é©æ–°ã€‚é¢ä¸´æœ‰é™æ•°æ®å¯ç”¨æ€§ã€ç¼ºä¹é€šç”¨æ€§å’Œå¤šæ¨¡æ€æ•°æ®çš„æœ‰æ•ˆèåˆç­‰æŒ‘æˆ˜ã€‚VIS-MAEæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦æˆåƒçš„æ–°å‹åŸºç¡€æ¨¡å‹æƒé‡ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯åœ¨å¤§é‡æœªæ ‡è®°å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œé€‚åº”åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¨¡å‹æ ‡ç­¾æ•ˆç‡é«˜ï¼Œå…·æœ‰æ”¹å–„é¢†åŸŸå†…å¤–åº”ç”¨è¡¨ç°çš„èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨è¾ƒå°‘çš„æ ‡è®°è®­ç»ƒæ•°æ®å³å¯è¾¾åˆ°ä¸å…¶ä»–é¢„è®­ç»ƒæƒé‡ç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ã€‚VIS-MAEåœ¨åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½é¢†åŸŸä»£è¡¨äº†é‡è¦è¿›å±•ï¼Œä¸ºè§£å†³åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡æä¾›äº†é€šç”¨æ€§å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶é™ä½äº†æ•°æ®æ³¨é‡Šå·¥ä½œé‡ã€‚æ›´å¤šä¿¡æ¯å¯è®¿é—®ç›¸å…³ä»£ç ä»“åº“é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/lzl199704/VIS-MAE%E3%80%82">https://github.com/lzl199704/VIS-MAEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦æˆåƒä¸­æœ‰æ½œåŠ›æ¨åŠ¨è¯Šæ–­å’Œåˆ†å‰²çš„è¿›æ­¥ã€‚</li>
<li>VIS-MAEæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦æˆåƒçš„æ–°å‹åŸºç¡€æ¨¡å‹æƒé‡ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯è®­ç»ƒã€‚</li>
<li>VIS-MAEå¯é€‚åº”å¤šç§åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ï¼Œå…·æœ‰é«˜æ ‡ç­¾æ•ˆç‡ã€‚</li>
<li>VIS-MAEåœ¨å‡å°‘æ ‡è®°è®­ç»ƒæ•°æ®é‡çš„æƒ…å†µä¸‹å³å¯è¾¾åˆ°ä¸å…¶ä»–é¢„è®­ç»ƒæƒé‡ç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>VIS-MAEæå‡äº†æ¨¡å‹åœ¨é¢†åŸŸå†…å¤–åº”ç”¨çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>VIS-MAEä¸ºåŒ»å­¦æˆåƒé¢†åŸŸæä¾›äº†é€šç”¨æ€§å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.01034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20ad2525bd7264743278bc6c3e9cf97c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df6b67162cbef9ffcca1f64a4660f352.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RAD-DINO-Exploring-Scalable-Medical-Image-Encoders-Beyond-Text-Supervision"><a href="#RAD-DINO-Exploring-Scalable-Medical-Image-Encoders-Beyond-Text-Supervision" class="headerlink" title="RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text   Supervision"></a>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text   Supervision</h2><p><strong>Authors:Fernando PÃ©rez-GarcÃ­a, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C. Castro, Anton Schwaighofer, Matthew P. Lungren, Maria Wetscherek, Noel Codella, Stephanie L. Hyland, Javier Alvarez-Valle, Ozan Oktay</strong></p>
<p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, the computed features are limited by the information contained in the text, which is particularly problematic in medical imaging, where the findings described by radiologists focus on specific observations. This challenge is compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general-purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language-supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINOâ€™s performance; notably, we observe that RAD-DINOâ€™s downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder. Model weights of RAD-DINO trained on publicly available datasets are available at <a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/rad-dino">https://huggingface.co/microsoft/rad-dino</a>. </p>
<blockquote>
<p>è¯­è¨€ç›‘ç£çš„é¢„è®­ç»ƒå·²è¢«è¯æ˜æ˜¯ä»å›¾åƒä¸­æå–è¯­ä¹‰ç‰¹å¾çš„ä¸€ç§æœ‰ä»·å€¼çš„æ–¹æ³•ï¼Œå®ƒä½œä¸ºè®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒé¢†åŸŸä¸­çš„å¤šæ¨¡æ€ç³»ç»Ÿçš„åŸºç¡€å…ƒç´ ã€‚ç„¶è€Œï¼Œè®¡ç®—å‡ºçš„ç‰¹å¾å—é™äºæ–‡æœ¬ä¸­æ‰€åŒ…å«çš„ä¿¡æ¯ï¼Œè¿™åœ¨åŒ»å­¦æˆåƒä¸­å°¤å…¶æˆé—®é¢˜ï¼Œå› ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæ‰€æè¿°çš„å‘ç°é‡ç‚¹å…³æ³¨ç‰¹å®šçš„è§‚å¯Ÿç»“æœã€‚è¿™ä¸€æŒ‘æˆ˜è¿˜å› é…å¯¹æˆåƒ-æ–‡æœ¬æ•°æ®çš„ç¨€ç¼ºè€ŒåŠ å‰§ï¼Œè¿™æ˜¯å‡ºäºå¯¹æ³„éœ²ä¸ªäººå¥åº·ä¿¡æ¯çš„æ‹…å¿§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æ ¹æœ¬ä¸Šè´¨ç–‘å¯¹è¯­è¨€ç›‘ç£å­¦ä¹ é€šç”¨ç”Ÿç‰©åŒ»å­¦æˆåƒç¼–ç å™¨çš„ä¾èµ–ã€‚æˆ‘ä»¬å¼•å…¥äº†RAD-DINOï¼Œè¿™æ˜¯ä¸€ç§ä»…åœ¨ä¸€æ¨¡æ€ç”Ÿç‰©åŒ»å­¦æˆåƒæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒç¼–ç å™¨ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒè·å¾—ä¸æœ€æ–°è¯­è¨€ç›‘ç£æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ ‡å‡†æˆåƒä»»åŠ¡ï¼ˆåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä»¥åŠè§†è§‰è¯­è¨€å¯¹é½ä»»åŠ¡ï¼ˆä»å›¾åƒç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šï¼‰æ¥è¯„ä¼°æ‰€å­¦ä¹ è¡¨ç¤ºçš„è´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯æ˜è¯­è¨€ç›‘ç£çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å±•ç¤ºäº†RAD-DINOçš„ç‰¹å¾ä¸å…¶ä»–åŒ»ç–—è®°å½•ï¼ˆå¦‚æ€§åˆ«æˆ–å¹´é¾„ï¼‰çš„ç›¸å…³æ€§ä¼˜äºè¯­è¨€ç›‘ç£æ¨¡å‹ï¼Œè¿™äº›é€šå¸¸åœ¨æ”¾å°„å­¦æŠ¥å‘Šä¸­å¹¶æœªæåŠã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—æµ‹è¯•ä»¥ç¡®å®šRAD-DINOæ€§èƒ½çš„å› ç´ ï¼›å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°RAD-DINOçš„ä¸‹æ¸¸æ€§èƒ½éšç€è®­ç»ƒæ•°æ®æ•°é‡å’Œå¤šæ ·æ€§çš„å¢åŠ è€Œæé«˜ï¼Œè¿™è¡¨æ˜ä»…ä½¿ç”¨å›¾åƒç›‘ç£æ˜¯ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•æ¥è®­ç»ƒåŸºç¡€ç”Ÿç‰©åŒ»å­¦å›¾åƒç¼–ç å™¨ã€‚RAD-DINOåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æƒé‡å¯åœ¨[<a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/rad-dino%E8%8E%B7%E5%8F%96%E3%80%82]">https://huggingface.co/microsoft/rad-dinoè·å–ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.10815v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†RAD-DINOæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ä»…ä¾èµ–ç”Ÿç‰©åŒ»å­¦å½±åƒæ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„ç”Ÿç‰©åŒ»å­¦å½±åƒç¼–ç å™¨ã€‚è¯¥æ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¸å…ˆè¿›çš„è¯­è¨€ç›‘ç£æ¨¡å‹ç›¸ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ï¼Œä¸”æ›´é€‚ç”¨äºå›¾åƒç”ŸæˆæŠ¥å‘Šç­‰è·¨æ¨¡æ€ä»»åŠ¡ã€‚RAD-DINOçš„ç‰¹å¾ä¸åŒ»ç–—è®°å½•å…³è”æ›´å¼ºï¼Œå°¤å…¶åœ¨æ€§åˆ«å’Œå¹´é¾„é¢„æµ‹ç­‰ä»»åŠ¡ä¸­ã€‚å…¶è¡¨ç°è‰¯å¥½ä¸”å¯æ‰©å±•ï¼Œéšç€è®­ç»ƒæ•°æ®é‡å’Œå¤šæ ·æ€§çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šè¿›ä¸€æ­¥æå‡ã€‚æ¨¡å‹æƒé‡å·²å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAD-DINOæ˜¯ä¸€ä¸ªç”Ÿç‰©åŒ»å­¦å½±åƒç¼–ç å™¨ï¼Œä»…ä¾èµ–ç”Ÿç‰©åŒ»å­¦å½±åƒæ•°æ®è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAD-DINOæ€§èƒ½ä¸å…ˆè¿›çš„è¯­è¨€ç›‘ç£æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>RAD-DINOé€‚ç”¨äºè·¨æ¨¡æ€ä»»åŠ¡ï¼Œå¦‚å›¾åƒç”ŸæˆæŠ¥å‘Šã€‚</li>
<li>RAD-DINOå­¦åˆ°çš„ç‰¹å¾ä¸åŒ»ç–—è®°å½•å…³è”æ›´å¼ºï¼Œå°¤å…¶åœ¨æ€§åˆ«å’Œå¹´é¾„é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</li>
<li>RAD-DINOæ¨¡å‹è¡¨ç°è‰¯å¥½ä¸”å¯æ‰©å±•ï¼Œéšç€è®­ç»ƒæ•°æ®é‡å’Œå¤šæ ·æ€§çš„å¢åŠ ï¼Œå…¶æ€§èƒ½ä¼šè¿›ä¸€æ­¥æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.10815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e8a3ef93e2d33ca3ca2dbed4e23aa94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c98567d71c2a82688f23ddb2ccd9eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c33078168e75d8a43b69c51cb3c8fad7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ba7cb63a246cd84ab24def442b4ddc8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PeFoMed-Parameter-Efficient-Fine-tuning-of-Multimodal-Large-Language-Models-for-Medical-Imaging"><a href="#PeFoMed-Parameter-Efficient-Fine-tuning-of-Multimodal-Large-Language-Models-for-Medical-Imaging" class="headerlink" title="PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language   Models for Medical Imaging"></a>PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language   Models for Medical Imaging</h2><p><strong>Authors:Jinlong He, Pengfei Li, Gang Liu, Genrong He, Zhaolin Chen, Shenjun Zhong</strong></p>
<p>Multimodal large language models (MLLMs) represent an evolutionary expansion in the capabilities of traditional large language models, enabling them to tackle challenges that surpass the scope of purely text-based applications. It leverages the knowledge previously encoded within these language models, thereby enhancing their applicability and functionality in the reign of multimodal contexts. Recent works investigate the adaptation of MLLMs as a universal solution to address medical multi-modal problems as a generative task. In this paper, we propose a parameter efficient framework for fine-tuning MLLMs, specifically validated on medical visual question answering (Med-VQA) and medical report generation (MRG) tasks, using public benchmark datasets. We also introduce an evaluation metric using the 5-point Likert scale and its weighted average value to measure the quality of the generated reports for MRG tasks, where the scale ratings are labelled by both humans manually and the GPT-4 model. We further assess the consistency of performance metrics across traditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The results indicate that semantic similarity assessments using GPT-4 align closely with human annotators and provide greater stability, yet they reveal a discrepancy when compared to conventional lexical similarity measurements. This questions the reliability of lexical similarity metrics for evaluating the performance of generative models in Med-VQA and report generation tasks. Besides, our fine-tuned model significantly outperforms GPT-4v. This indicates that without additional fine-tuning, multi-modal models like GPT-4v do not perform effectively on medical imaging tasks. The code will be available here: <a target="_blank" rel="noopener" href="https://github.com/jinlHe/PeFoMed">https://github.com/jinlHe/PeFoMed</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»£è¡¨äº†ä¼ ç»Ÿå¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›ä¸Šçš„è¿›åŒ–æ‰©å±•ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåº”å¯¹è¶…è¶Šçº¯æ–‡æœ¬åº”ç”¨ç¨‹åºèŒƒå›´çš„æŒ‘æˆ˜ã€‚å®ƒåˆ©ç”¨è¿™äº›è¯­è¨€æ¨¡å‹å†…éƒ¨å…ˆå‰ç¼–ç çš„çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºå…¶åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„é€‚ç”¨æ€§å’ŒåŠŸèƒ½ã€‚è¿‘æœŸçš„ç ”ç©¶æ¢è®¨äº†å°†MLLMsä½œä¸ºè§£å†³åŒ»å­¦å¤šæ¨¡æ€é—®é¢˜çš„é€šç”¨è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå‚æ•°é«˜æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå¾®è°ƒMLLMsï¼Œè¯¥æ¡†æ¶åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰å’ŒåŒ»ç–—æŠ¥å‘Šç”Ÿæˆï¼ˆMRGï¼‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œä½¿ç”¨çš„æ˜¯å…¬å…±åŸºå‡†æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§ä½¿ç”¨5ç‚¹åˆ©å…‹ç‰¹é‡è¡¨åŠå…¶åŠ æƒå¹³å‡å€¼ä½œä¸ºè¯„ä¼°ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡çš„è¯„ä»·æŒ‡æ ‡ï¼Œç”¨äºæµ‹é‡MRGä»»åŠ¡çš„æŠ¥å‘Šè´¨é‡ï¼Œé‡è¡¨è¯„åˆ†ç”±äººå·¥å’ŒGPT-4æ¨¡å‹æ ‡è®°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†ä¼ ç»Ÿåº¦é‡æ ‡å‡†ã€GPT-4å’Œäººç±»è¯„åˆ†åœ¨VQAå’ŒMRGä»»åŠ¡ä¸Šçš„æ€§èƒ½æŒ‡æ ‡çš„ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨GPT-4çš„è¯­ä¹‰ç›¸ä¼¼æ€§è¯„ä¼°ä¸äººç±»æ³¨é‡Šè€…ç´§å¯†å¯¹é½ï¼Œå¹¶æä¾›æ›´å¤§çš„ç¨³å®šæ€§ï¼Œç„¶è€Œï¼Œä¸ä¼ ç»Ÿçš„è¯æ±‡ç›¸ä¼¼æ€§åº¦é‡ç›¸æ¯”ï¼Œå®ƒä»¬æ˜¾ç¤ºå‡ºå·®å¼‚ã€‚è¿™è´¨ç–‘äº†è¯æ±‡ç›¸ä¼¼æ€§æŒ‡æ ‡åœ¨è¯„ä¼°Med-VQAå’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡çš„ç”Ÿæˆæ¨¡å‹æ€§èƒ½æ–¹é¢çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºGPT-4vã€‚è¿™è¡¨æ˜æœªç»é¢å¤–å¾®è°ƒçš„å¤šæ¨¡æ€æ¨¡å‹å¦‚GPT-4våœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸æœ‰æ•ˆã€‚ä»£ç å°†åœ¨æ­¤å¤„æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/jinlHe/PeFoMed">https://github.com/jinlHe/PeFoMed</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.02797v3">PDF</a> 12 pages, 8 figures, 12 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯ä¼ ç»Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰©å±•ï¼Œèƒ½å¤Ÿåº”å¯¹å•çº¯æ–‡æœ¬åº”ç”¨ç¨‹åºæ— æ³•åº”å¯¹çš„æŒ‘æˆ˜ã€‚å®ƒåˆ©ç”¨å·²ç¼–ç åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†ï¼Œå¢å¼ºå…¶åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„é€‚ç”¨æ€§å’ŒåŠŸèƒ½ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢è®¨äº†å°†MLLMsä½œä¸ºè§£å†³åŒ»å­¦å¤šæ¨¡æ€é—®é¢˜çš„é€šç”¨è§£å†³æ–¹æ¡ˆçš„ç”Ÿæˆä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒMLLMsæ¡†æ¶ï¼Œå¹¶åœ¨åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰å’ŒåŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMRGï¼‰ä»»åŠ¡ä¸Šè¿›è¡ŒéªŒè¯ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§ä½¿ç”¨äº”ç‚¹æå…‹ç‰¹é‡è¡¨åŠå…¶åŠ æƒå¹³å‡å€¼ä½œä¸ºè¯„ä»·ç”ŸæˆæŠ¥å‘Šè´¨é‡çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥é‡è¡¨çš„è¯„åˆ†ç”±äººå·¥å’ŒGPT-4æ¨¡å‹æ ‡æ³¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†ä¼ ç»Ÿåº¦é‡æ ‡å‡†ã€GPT-4å’Œäººç±»è¯„åˆ†åœ¨VQAå’ŒMRGä»»åŠ¡ä¸Šçš„æ€§èƒ½æŒ‡æ ‡çš„ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨GPT-4è¿›è¡Œçš„è¯­ä¹‰ç›¸ä¼¼æ€§è¯„ä¼°ä¸äººç±»æ³¨é‡Šè€…å¯†åˆ‡ç›¸å…³ï¼Œå¹¶æä¾›æ›´å¤§çš„ç¨³å®šæ€§ï¼Œä½†ä¸ä¼ ç»Ÿçš„è¯æ±‡ç›¸ä¼¼æ€§åº¦é‡ç›¸æ¯”ï¼Œå®ƒä»¬æ˜¾ç¤ºå‡ºå·®å¼‚ã€‚è¿™è´¨ç–‘äº†è¯æ±‡ç›¸ä¼¼æ€§åº¦é‡åœ¨è¯„ä¼°Med-VQAå’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡çš„ç”Ÿæˆæ¨¡å‹æ€§èƒ½æ–¹é¢çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ˜¾è‘—ä¼˜äºGPT-4vã€‚è¿™è¡¨æ˜æœªç»é¢å¤–å¾®è°ƒçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¦‚GPT-4vï¼Œåœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸æœ‰æ•ˆã€‚ä»£ç å°†åœ¨æ­¤å¤„æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/jinlHe/PeFoMed">https://github.com/jinlHe/PeFoMed</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿåº”å¯¹ä¼ ç»Ÿè¯­è¨€æ¨¡å‹æ— æ³•å¤„ç†çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒMLLMsæ¡†æ¶ï¼Œé€‚ç”¨äºåŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰å’ŒåŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆMRGï¼‰ä»»åŠ¡ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç»“åˆäº”ç‚¹æå…‹ç‰¹é‡è¡¨åŠå…¶åŠ æƒå¹³å‡å€¼æ¥è¯„ä»·ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡ï¼Œå…¶ä¸­è¯„åˆ†ç”±äººå·¥å’ŒGPT-4æ¨¡å‹å…±åŒå®Œæˆã€‚</li>
<li>GPT-4åœ¨è¯­ä¹‰ç›¸ä¼¼æ€§è¯„ä¼°æ–¹é¢ä¸äººç±»æ³¨é‡Šè€…å¯†åˆ‡ç›¸å…³ï¼Œä½†ä¸ä¼ ç»Ÿè¯æ±‡ç›¸ä¼¼æ€§åº¦é‡ç›¸æ¯”å­˜åœ¨å·®å¼‚ã€‚</li>
<li>è®ºæ–‡è´¨ç–‘äº†è¯æ±‡ç›¸ä¼¼æ€§åº¦é‡åœ¨è¯„ä¼°ç”Ÿæˆæ¨¡å‹åœ¨Med-VQAå’ŒæŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½æ—¶çš„å¯é æ€§ã€‚</li>
<li>è®ºæ–‡çš„å¾®è°ƒæ¨¡å‹åœ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºGPT-4vï¼Œè¡¨æ˜æœªç»é¢å¤–è°ƒå‚çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰é™ã€‚</li>
<li>è®ºæ–‡æä¾›çš„ä»£ç å°†å…¬å¼€ï¼Œä¾›å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.02797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77bbc228b180775d1e4562982e167df1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7fad07118e8368146e9b6c482515892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ad5752dd44ebabc00c94d13bc501f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e18bf87c27612df275b46fe7c2f4dcdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9127808b1f2ba9a5390edb5d51e3d273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c565f25213cde220b63738be03f3c1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70b3dfc42f75042382b7d5012f643985.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a9b3929f400dd861e57bad3fed550363.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-21  FaceXBench Evaluating Multimodal LLMs on Face Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-19/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96942639a987da31c16db1b96f06eddd.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-19  Quilt-LLaVA Visual Instruction Tuning by Extracting Localized   Narratives from Open-Source Histopathology Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
