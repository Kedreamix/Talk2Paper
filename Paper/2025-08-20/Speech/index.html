<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-08-20  Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with   Transformer-Based Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-30e5c45217a2c470230c1e3173c6aefe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-20-更新"><a href="#2025-08-20-更新" class="headerlink" title="2025-08-20 更新"></a>2025-08-20 更新</h1><h2 id="Arabic-ASR-on-the-SADA-Large-Scale-Arabic-Speech-Corpus-with-Transformer-Based-Models"><a href="#Arabic-ASR-on-the-SADA-Large-Scale-Arabic-Speech-Corpus-with-Transformer-Based-Models" class="headerlink" title="Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with   Transformer-Based Models"></a>Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with   Transformer-Based Models</h2><p><strong>Authors:Branislav Gerazov, Marcello Politi, Sébastien Bratières</strong></p>
<p>We explore the performance of several state-of-the-art automatic speech recognition (ASR) models on a large-scale Arabic speech dataset, the SADA (Saudi Audio Dataset for Arabic), which contains 668 hours of high-quality audio from Saudi television shows. The dataset includes multiple dialects and environments, specifically a noisy subset that makes it particularly challenging for ASR. We evaluate the performance of the models on the SADA test set, and we explore the impact of fine-tuning, language models, as well as noise and denoising on their performance. We find that the best performing model is the MMS 1B model finetuned on SADA with a 4-gram language model that achieves a WER of 40.9% and a CER of 17.6% on the SADA test clean set. </p>
<blockquote>
<p>我们对多个最先进的自动语音识别（ASR）模型在大型阿拉伯语语音数据集SADA（沙特阿拉伯阿拉伯语数据集）上的表现进行了探索。该数据集包含来自沙特阿拉伯电视剧的668小时高质量音频。该数据集包含多种方言和环境，特别是一个嘈杂的子集，这使得自动语音识别特别具有挑战性。我们在SADA测试集上评估模型的性能，并探讨了微调、语言模型以及噪声和降噪对性能的影响。我们发现表现最好的模型是在SADA上微调过的MMS 1B模型，它使用了一个4元语言模型，在SADA测试集干净集上实现了词错误率（WER）为40.9%，字符错误率（CER）为17.6%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12968v1">PDF</a> </p>
<p><strong>Summary</strong>：我们对多个前沿的自动语音识别（ASR）模型在大型阿拉伯语语音数据集SADA上的表现进行了探索。该数据集包含来自沙特电视节目的高质量音频，时长668小时，包含多种方言和环境，尤其是噪音子集，对ASR来说极具挑战性。我们在SADA测试集上评估了这些模型的表现，并探讨了微调、语言模型以及噪声和去噪对性能的影响。研究发现，在SADA的清洁集上，经SADA微调过的MMS 1B模型配合4-gram语言模型表现最佳，字错误率（WER）达到40.9%，字符错误率（CER）为17.6%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>论文研究了多个前沿自动语音识别（ASR）模型在沙特阿拉伯音频数据集（SADA）上的表现。</li>
<li>SADA数据集包含多种方言和环境，对ASR系统来说是一个挑战。</li>
<li>研究者评估了模型在SADA测试集上的性能。</li>
<li>最好的模型是MMS 1B模型，经过在SADA数据集上的微调，并配合使用4-gram语言模型。</li>
<li>该最佳模型在SADA清洁测试集上实现了较低的字错误率（WER）和字符错误率（CER）。</li>
<li>研究发现噪声和去噪对ASR模型性能有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12968">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-efb218af5ef688b1277eb241015d160b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-589048573219f060235a44c47863da62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6133da511eae3b6d1f3d71d0fd79343.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CarelessWhisper-Turning-Whisper-into-a-Causal-Streaming-Model"><a href="#CarelessWhisper-Turning-Whisper-into-a-Causal-Streaming-Model" class="headerlink" title="CarelessWhisper: Turning Whisper into a Causal Streaming Model"></a>CarelessWhisper: Turning Whisper into a Causal Streaming Model</h2><p><strong>Authors:Tomer Krichli, Bhiksha Raj, Joseph Keshet</strong></p>
<p>Automatic Speech Recognition (ASR) has seen remarkable progress, with models like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA) performance in offline transcription. However, these models are not designed for streaming (online or real-time) transcription, due to limitations in their architecture and training methodology. We propose a method to turn the transformer encoder-decoder model into a low-latency streaming model that is careless about future context. We present an analysis explaining why it is not straightforward to convert an encoder-decoder transformer to a low-latency streaming model. Our proposed method modifies the existing (non-causal) encoder to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated inference mechanism that utilizes the fine-tune causal encoder and decoder to yield greedy and beam-search decoding, and is shown to be locally optimal. Experiments on low-latency chunk sizes (less than 300 msec) show that our fine-tuned model outperforms existing non-fine-tuned streaming approaches in most cases, while using a lower complexity. Additionally, we observe that our training process yields better alignment, enabling a simple method for extracting word-level timestamps. We release our training and inference code, along with the fine-tuned models, to support further research and development in streaming ASR. </p>
<blockquote>
<p>自动语音识别（ASR）已经取得了显著的进步，例如OpenAIwhisper和NVIDIAcanary等模型在离线转录方面达到了最新技术水平。然而，这些模型并不适用于流式（在线或实时）转录，这是由于它们的架构和训练方法的局限性所导致的。我们提出了一种将转换器编码器-解码器模型转化为低延迟流式模型的方法，该模型不关心未来上下文。我们进行了分析，解释了为什么将编码器-解码器转换器转换为低延迟流式模型并不简单。我们提出的方法通过将现有（非因果）编码器微调为因果编码器，同时利用低秩适应（LoRA）和弱对齐数据集对编码器和解码器进行微调。然后，我们提出了一种更新的推理机制，该机制利用微调的因果编码器和解码器来产生贪心和解码束搜索，并被认为是局部最优的。在低延迟块大小（小于300毫秒）的实验中，表明我们的微调模型在大多数情况下都优于现有的未微调流式方法，同时使用更低的复杂性。此外，我们观察到我们的训练过程产生了更好的对齐效果，这为实现简单的提取词级时间戳方法提供了可能。我们发布我们的训练和推理代码以及微调模型，以支持流式ASR的进一步研究和开发。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12301v1">PDF</a> 17 pages, 7 Figures, This work has been submitted to the IEEE for   possible publication</p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动语音识别（ASR）领域的最新进展，特别是针对流式转码模型的研究。文章指出，虽然OpenAI Whisper和NVIDIA Canary等模型在离线转录方面表现出卓越的性能，但它们并不适用于流式转录。为此，文章提出了一种将转换器编码器-解码器模型转换为低延迟流式模型的方法，并详细解释了转换的难点。通过微调编码器和解码器，并使用低秩适应（LoRA）和弱对齐数据集，文章提出了一种新的因果编码器。更新的推理机制利用精细调整的因果编码器和解码器，实现贪婪和集束搜索解码，并证明其局部最优。实验表明，在低声延迟块大小下，精细调整后的模型在大多数情况下优于现有的非精细调整流式方法，同时降低了复杂性。此外，该训练过程还产生了更好的对齐效果，为提取单词级时间戳提供了一种简单方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章指出自动语音识别（ASR）的在线或实时转录仍面临挑战，尽管已有模型在离线转录方面表现出卓越性能。</li>
<li>文章提出了一种将转换器编码器-解码器模型转换为低延迟流式模型的方法。</li>
<li>转换的难点在于如何将模型的架构和训练方法进行调整以适应流式处理的需求。</li>
<li>通过对编码器和解码器的微调，以及使用低秩适应（LoRA）和弱对齐数据集，提出了一种新的因果编码器。</li>
<li>更新后的推理机制可以实现贪婪和集束搜索解码，并具有局部最优性。</li>
<li>实验结果表明，在低延迟条件下，精细调整后的模型性能优于大多数现有方法，且计算复杂度较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0666f0ebb8e0fae15e8b87792f92656d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b9c1cc8fdf256726ab8c1857d6626e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30e5c45217a2c470230c1e3173c6aefe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a8207d4869d4728fabd1013e9a21bab.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Self-Supervised-Audio-Models-for-Generalized-Anomalous-Sound-Detection"><a href="#Exploring-Self-Supervised-Audio-Models-for-Generalized-Anomalous-Sound-Detection" class="headerlink" title="Exploring Self-Supervised Audio Models for Generalized Anomalous Sound   Detection"></a>Exploring Self-Supervised Audio Models for Generalized Anomalous Sound   Detection</h2><p><strong>Authors:Bing Han, Anbai Jiang, Xinhu Zheng, Wei-Qiang Zhang, Jia Liu, Pingyi Fan, Yanmin Qian</strong></p>
<p>Machine anomalous sound detection (ASD) is a valuable technique across various applications. However, its generalization performance is often limited due to challenges in data collection and the complexity of acoustic environments. Inspired by the success of large pre-trained models in numerous fields, this paper introduces a robust ASD model that leverages self-supervised pre-trained models trained on large-scale speech and audio datasets. Although there are inconsistencies between the pre-training datasets and the ASD task, our findings indicate that pre-training still provides substantial benefits for ASD. To mitigate overfitting and retain learned knowledge when fine-tuning with limited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as an alternative to full fine-tuning. Additionally, we propose a Machine-aware Group Adapter module, which enables the model to capture differences between various machines within a unified framework, thereby enhancing the generalization performance of ASD systems. To address the challenge of missing attribute labels, we design a novel objective function that dynamically clusters unattributed data using vector quantization and optimizes through a dual-level contrastive learning loss. The proposed methods are evaluated on all benchmark datasets, including the DCASE 2020-2024 five ASD challenges, and the experimental results show significant improvements of our new approach and demonstrate the effectiveness of our proposed strategies. </p>
<blockquote>
<p>机器异常声音检测（ASD）在多种应用中是一项有价值的技术。然而，由于其数据收集的挑战和声音环境的复杂性，其泛化性能通常受到限制。受大型预训练模型在众多领域成功的启发，本文引入了一个稳健的ASD模型，该模型利用在大型语音和音频数据集上训练的自我监督预训练模型。尽管预训练数据集与ASD任务之间存在不一致，但我们的研究结果表明，预训练仍然对ASD提供重大益处。为了缓解过度拟合问题并在使用有限数据进行微调时保留所学知识，我们探索了全连接低秩适配（LoRA）作为全微调的一种替代方法。此外，我们提出了机器感知组适配器模块，使模型能够在统一框架内捕捉各种机器之间的差异，从而提高ASD系统的泛化性能。为了解决缺失属性标签的挑战，我们设计了一种新的目标函数，该函数通过向量量化动态聚类未标记数据，并通过双重对比学习损失进行优化。所提出的方法在所有基准数据集上进行了评估，包括DCASE 2020-2024五个ASD挑战，实验结果表明我们的新方法取得了显著改进，并证明了我们提出的策略的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12230v1">PDF</a> Accepted by TASLP. 15 pages, 7 figures;</p>
<p><strong>Summary</strong>：机器异常声音检测（ASD）是一项在多领域具有应用价值的检测技术。本文通过借鉴预训练模型在其他领域的成功经验，引入了一种基于大规模语音和音频数据集的预训练ASD模型。针对预训练数据集与ASD任务间的不一致性，研究发现预训练仍为ASD带来实质效益。为缓解过拟合问题并在有限数据上保留学习到的知识，本文探索了全连接低秩适配（LoRA）作为传统微调的替代方案。此外，还提出了机器感知组适配器模块，能在统一框架内捕捉不同机器之间的差异，提高了ASD系统的泛化性能。针对缺少属性标签的挑战，设计了一种新的目标函数，通过向量量化和双层次对比学习损失来动态聚类无属性数据并进行优化。在DCASE 2020-2024五个ASD挑战数据集上的实验结果表明，新方法显著提高了检测性能，验证了所提策略的有效性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>预训练模型对于机器异常声音检测（ASD）有实质性益处，即使预训练数据集与ASD任务存在不一致性。</li>
<li>采用全连接低秩适配（LoRA）能有效缓解ASD模型过拟合问题，并在有限数据上保留学习到的知识。</li>
<li>提出的机器感知组适配器模块能够捕捉不同机器之间的差异，提高了ASD系统的泛化性能。</li>
<li>针对缺少属性标签的挑战，通过设计新的目标函数实现了无属性数据的动态聚类。</li>
<li>采用向量量化和双层次对比学习损失进行优化，增强了模型的检测性能。</li>
<li>在DCASE 2020-2024五个ASD挑战数据集上的实验结果表明，新方法显著提高了检测效果。</li>
<li>本文策略的有效性得到了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12230">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0542758bccb57972f3635accca7756a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfff545015ee693a44f88a9a6a3f3e72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b18f9334bbdbd93cc1e7d8b2036464a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5de30dcccf57a899740665e76bdd0ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1297fa74e46b73eddd1c015d51f5ebe6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FNH-TTS-A-Fast-Natural-and-Human-Like-Speech-Synthesis-System-with-advanced-prosodic-modeling-based-on-Mixture-of-Experts"><a href="#FNH-TTS-A-Fast-Natural-and-Human-Like-Speech-Synthesis-System-with-advanced-prosodic-modeling-based-on-Mixture-of-Experts" class="headerlink" title="FNH-TTS: A Fast, Natural, and Human-Like Speech Synthesis System with   advanced prosodic modeling based on Mixture of Experts"></a>FNH-TTS: A Fast, Natural, and Human-Like Speech Synthesis System with   advanced prosodic modeling based on Mixture of Experts</h2><p><strong>Authors:Qingliang Meng, Luogeng Xiong, Wei Liang, Limei Yu, Huizhi Liang, Tian Li</strong></p>
<p>Achieving natural and human-like speech synthesis with low inference costs remains a major challenge in speech synthesis research. This study focuses on human prosodic patterns and synthesized spectrum harmony, addressing the challenges of prosody modeling and artifact issues in non-autoregressive models. To enhance prosody modeling and synthesis quality, we introduce a new Duration Predictor based on the Mixture of Experts alongside a new Vocoder with two advanced multi-scale discriminators. We integrated the these new modules into the VITS system, forming our FNH-TTS system. Our experiments on LJSpeech, VCTK, and LibriTTS demonstrate the system’s superiority in synthesis quality, phoneme duration prediction, Vocoder results, and synthesis speed. Our prosody visualization results show that FNH-TTS produces duration predictions that more closely align with natural human beings than other systems. </p>
<blockquote>
<p>在语音合成研究中，实现低成本、自然、拟人化的语音合成仍然是一个重大挑战。本研究关注人类语调模式和合成频谱和谐性，解决语调建模和非自回归模型中的伪影问题的挑战。为了增强语调建模和合成质量，我们引入了一种基于专家混合的新持续时间预测器，以及一种具有两个先进多尺度鉴别器的新Vocoder。我们将这些新模块集成到VITS系统中，形成了我们的FNH-TTS系统。我们在LJSpeech、VCTK和LibriTTS上的实验证明了该系统在合成质量、音素持续时间预测、Vocoder结果和合成速度上的优越性。我们的语调可视化结果显示，FNH-TTS产生的持续时间预测与其他系统相比，更接近自然人类的水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12001v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究关注人类语调模式和合成频谱和谐性，旨在解决非自回归模型中的语调建模和伪迹问题。通过引入基于专家混合的新持续时间预测器和带有两个先进多尺度鉴别器的新的声码器，提高了语调建模和合成质量。集成到新VITS系统中，形成了我们的FNH-TTS系统。实验证明，该系统在合成质量、音素持续时间预测、声码器结果和合成速度方面均优于其他系统，产生的语调可视化结果更接近自然人类。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注自然和人类化的语音合成，挑战在于实现低推理成本的语音合成。</li>
<li>研究重点之一是理解和模拟人类语调模式以及合成频谱和谐性。</li>
<li>为了提高语调建模和合成质量，引入了新的持续时间预测器和声码器。</li>
<li>新的模块是基于专家混合和先进的多尺度鉴别器设计的。</li>
<li>这些新模块被集成到VITS系统中，形成了FNH-TTS系统。</li>
<li>在多个数据集上的实验证明，FNH-TTS系统在合成质量、音素持续时间预测、声码器结果和合成速度上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12001">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-367770a9fcc7e391c1c05254d8668432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b87d647c08768811ba838a389ecb82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb8293187e68dbe3b98515438382d6fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2765146d8e9ba422ab874481d39df87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de212c5e25c77b537dc513df982ccb9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aabfb140e32162546fd1d2ea0b491f08.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection"><a href="#TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection" class="headerlink" title="TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection"></a>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection</h2><p><strong>Authors:Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</strong></p>
<p>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real&#x2F;synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at <a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>. </p>
<blockquote>
<p>电信欺诈检测面临着巨大的挑战，这是由于缺乏高质量的多模式训练数据，无法将音频信号与面向推理的文本分析相结合。为了解决这一空白，我们推出了TeleAntiFraud-28k，这是专门为电信欺诈自动化分析设计的第一个开源音频文本慢思考数据集。我们的数据集通过以下三种策略构建：（1）使用自动语音识别（ASR）转录的通话录音生成隐私保护的文本真实样本（带有匿名原始音频），并通过文本到语音（TTS）模型再生确保现实一致性；（2）通过基于大型语言模型（LLM）的自我指令采样对真实的ASR输出进行语义增强，以扩大场景覆盖；（3）模拟新兴欺诈策略的多代理对抗合成通过预设的通信场景和欺诈类型。生成的数据集包含经过严格处理的28511个语音文本对，带有详细的欺诈推理注释。数据集分为三个任务：场景分类、欺诈检测、欺诈类型分类。此外，我们构建了TeleAntiFraud-Bench，一个标准化的评估基准，包含从数据集中按比例采样的实例，以促进电信欺诈检测任务上模型性能的系统测试。我们还为混合真实&#x2F;合成数据训练的生产优化监督微调（SFT）模型做出了贡献，同时开源数据处理框架以推动社区驱动的数据集扩展。这项工作为跨模式反欺诈研究建立了基础框架，同时解决了数据隐私和场景多样性方面的关键挑战。该项目将在<a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24115v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对电信欺诈检测领域所面临的挑战，提出了一种新的开放源代码音频文本慢思考数据集TeleAntiFraud-28k。该数据集通过三种策略构建，包括隐私保护的文本真实样本生成、语义增强的大语言模型自我指令采样以及多智能体对抗合成。数据集包含经过严格处理的28,511个语音文本对，具有详细的欺诈推理注释，分为场景分类、欺诈检测和欺诈类型分类三个任务。此外，还构建了TeleAntiFraud-Bench标准化评估基准，以系统化测试电信欺诈检测任务的模型性能。本工作奠定了多模式抗欺诈研究的基础框架，并解决了数据隐私和场景多样性等关键挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeleAntiFraud-28k是专门为电信欺诈分析设计的首个开放源代码音频文本慢思考数据集。</li>
<li>数据集通过隐私保护的文本真实样本生成、语义增强及多智能体对抗合成等三种策略构建。</li>
<li>数据集包含28,511个语音文本对，具备详细的欺诈推理注释，分为三个任务：场景分类、欺诈检测、欺诈类型分类。</li>
<li>建立了TeleAntiFraud-Bench评估基准，以便系统化测试电信欺诈检测模型的性能。</li>
<li>该工作解决了电信欺诈检测中的数据隐私和场景多样性等关键挑战。</li>
<li>公开了数据处理框架，便于社区进行数据集扩展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d3b36609c9ab9a09876a9207f9b7bce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f7f634e6142e39dd5314cbbc507d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-253fa7d6973f26d24d2c81d519deac01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e8ff0522c821f7aed3553f25af23621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d40fdd3e5088aee9979809c6fd59961.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39efe517225eb10b12914a9414a2343d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d70a53acc5fdca0b1a3370df29b3b8e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3a36244324f42384a7e1ed1581aedb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4440e8a7f2508cb3f2258cf2944964e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VisualSpeech-Enhancing-Prosody-Modeling-in-TTS-Using-Video"><a href="#VisualSpeech-Enhancing-Prosody-Modeling-in-TTS-Using-Video" class="headerlink" title="VisualSpeech: Enhancing Prosody Modeling in TTS Using Video"></a>VisualSpeech: Enhancing Prosody Modeling in TTS Using Video</h2><p><strong>Authors:Shumin Que, Anton Ragni</strong></p>
<p>Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody given a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as video, remains under-utilized despite being available in many applications. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates visual and textual information for improving prosody generation in TTS. Empirical results indicate that incorporating visual features improves prosodic modeling, enhancing the expressiveness of the synthesized speech. Audio samples are available at <a target="_blank" rel="noopener" href="https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/">https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/</a>. </p>
<blockquote>
<p>文本转语音（TTS）合成面临一个固有的挑战，即如何根据单一的文本输入生成具有不同韵律的多个语音输出。尽管先前的研究已经通过从文本和语音中预测韵律信息来解决这个问题，但在许多应用程序中都可获得的其他上下文信息（如视频）仍未得到充分利用。本文探讨了整合视觉上下文以增强韵律预测的可能性。我们提出了一种新型模型VisualSpeech，该模型结合了视觉和文本信息，旨在改善TTS中的韵律生成。经验结果表明，引入视觉特征可改善韵律建模，提高合成语音的表达力。音频样本可通过以下网址获取：<a target="_blank" rel="noopener" href="https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/">https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19258v2">PDF</a> </p>
<p><strong>摘要</strong><br>随着文本到语音合成（TTS）的发展，对于给定单一文本输入产生多种不同语调输出的挑战愈发显著。先前的研究通过预测文本和语音中的韵律信息来解决这一问题，但忽略了可利用的额外上下文信息，如视频信息。本文探索整合视觉上下文信息以提高韵律预测能力。提出了融合视觉和文本信息的全新模型VisualSpeech，用于改进TTS中的韵律生成。实证结果表明，融入视觉特征有助于改善韵律建模，提高合成语音的表达力。音频样本可在<a target="_blank" rel="noopener" href="https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/">https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/</a>获取。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>TTS合成面临单一文本输入产生多样语音输出的挑战。</li>
<li>VisualSpeech模型整合视觉和文本信息以改进韵律生成。</li>
<li>视觉特征的融入提高了韵律建模的准确度。</li>
<li>视觉上下文信息在增强语音表达力方面发挥重要作用。</li>
<li>VisualSpeech模型可提高TTS系统的性能，使其更自然、更富有表现力。</li>
<li>音频样本展示了改进后的语音合成效果。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4f3a3f18a3f09ce2fe085d82303e900e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6958f7bb32b2c3c23364206e03e226e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-679468f62b17ba913467ad7c51454110.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da9e3db4ef45450b3abb7461f194a0c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-437f0974eda4de52c76f56d65ebc5910.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e83cfbcfc54ef6020ccbcdd7582e46ed.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-20  WP-CLIP Leveraging CLIP to Predict Wölfflin's Principles in Visual   Art
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-857881371a0aa747f6cc48c760feaa2f.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-08-20  Single-Reference Text-to-Image Manipulation with Dual Contrastive   Denoising Score
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
