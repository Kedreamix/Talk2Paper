<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-08-20  TiP4GEN Text to Immersive Panorama 4D Scene Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-11054e273d5c29a6bad15193edac95b7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-20-更新"><a href="#2025-08-20-更新" class="headerlink" title="2025-08-20 更新"></a>2025-08-20 更新</h1><h2 id="TiP4GEN-Text-to-Immersive-Panorama-4D-Scene-Generation"><a href="#TiP4GEN-Text-to-Immersive-Panorama-4D-Scene-Generation" class="headerlink" title="TiP4GEN: Text to Immersive Panorama 4D Scene Generation"></a>TiP4GEN: Text to Immersive Panorama 4D Scene Generation</h2><p><strong>Authors:Ke Xing, Hanwen Liang, Dejia Xu, Yuyang Yin, Konstantinos N. Plataniotis, Yao Zhao, Yunchao Wei</strong></p>
<p>With the rapid advancement and widespread adoption of VR&#x2F;AR technologies, there is a growing demand for the creation of high-quality, immersive dynamic scenes. However, existing generation works predominantly concentrate on the creation of static scenes or narrow perspective-view dynamic scenes, falling short of delivering a truly 360-degree immersive experience from any viewpoint. In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic panorama scene generation framework that enables fine-grained content control and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN integrates panorama video generation and dynamic scene reconstruction to create 360-degree immersive virtual environments. For video generation, we introduce a \textbf{Dual-branch Generation Model} consisting of a panorama branch and a perspective branch, responsible for global and local view generation, respectively. A bidirectional cross-attention mechanism facilitates comprehensive information exchange between the branches. For scene reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model} based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using metric depth maps and initializing scene cameras with estimated poses, our method ensures geometric consistency and temporal coherence for the reconstructed scenes. Extensive experiments demonstrate the effectiveness of our proposed designs and the superiority of TiP4GEN in generating visually compelling and motion-coherent dynamic panoramic scenes. Our project page is at <a target="_blank" rel="noopener" href="https://ke-xing.github.io/TiP4GEN/">https://ke-xing.github.io/TiP4GEN/</a>. </p>
<blockquote>
<p>随着虚拟现实&#x2F;增强现实技术的迅速发展和广泛应用，对高质量沉浸式动态场景的创作需求日益增长。然而，现有的工作主要集中在创建静态场景或有限视角的动态场景，无法提供从任何视角的真正360度沉浸式体验。在本文中，我们介绍了<strong>TiP4GEN</strong>，这是一个先进的文本到动态全景场景生成框架，它能够实现精细的内容控制，并合成运动丰富、几何一致的全景4D场景。TiP4GEN集成了全景视频生成和动态场景重建，以创建360度沉浸式虚拟环境。对于视频生成，我们引入了一个<strong>双分支生成模型</strong>，包括全景分支和透视分支，分别负责全局和局部视图生成。双向交叉注意力机制促进了分支之间的全面信息交流。对于场景重建，我们提出了一个基于3D高斯拼贴的<strong>几何对齐重建模型</strong>。通过利用度量深度图对齐时空点云，并用估计的姿态初始化场景相机，我们的方法确保了重建场景的空间几何一致性和时间连贯性。大量实验证明了我们所提出设计的有效性以及TiP4GEN在生成视觉上吸引人且运动连贯的动态全景场景方面的优越性。我们的项目页面是<a target="_blank" rel="noopener" href="https://ke-xing.github.io/TiP4GEN/%E3%80%82">https://ke-xing.github.io/TiP4GEN/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12415v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着VR&#x2F;AR技术的迅速发展和广泛应用，对高质量、沉浸式动态场景的创作需求日益增长。然而，当前主要工作主要集中在静态场景或窄视角动态场景的创作上，无法提供真正的360度沉浸式体验。本文介绍了一种先进的文本到动态全景场景生成框架——TiP4GEN，它能够实现精细的内容控制，并合成运动丰富、几何一致的全景4D场景。TiP4GEN结合了全景视频生成和动态场景重建，创建出全方位的沉浸式虚拟环境。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TiP4GEN框架能够实现高质量的动态全景场景生成，满足VR&#x2F;AR技术的需求。</li>
<li>该框架通过结合全景视频生成和动态场景重建，创建出全方位的沉浸式虚拟环境。</li>
<li>提出了Dual-branch Generation Model，包括全景分支和透视分支，分别负责全局和局部视图生成。</li>
<li>双向交叉注意机制促进了分支之间的全面信息交流。</li>
<li>提出了基于3D高斯拼贴的Geometry-aligned Reconstruction Model，确保场景重建的几何一致性和时间连贯性。</li>
<li>实验证明，所提出的设计方案有效，TiP4GEN在生成视觉吸引、运动连贯的动态全景场景上具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12415">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f5912cf3260767ee37fd9627145f66c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-325dd3a48624bb4fa0b897fc22ba11b9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Densification-in-3D-Gaussian-Splatting-for-High-Fidelity-Rendering"><a href="#Improving-Densification-in-3D-Gaussian-Splatting-for-High-Fidelity-Rendering" class="headerlink" title="Improving Densification in 3D Gaussian Splatting for High-Fidelity   Rendering"></a>Improving Densification in 3D Gaussian Splatting for High-Fidelity   Rendering</h2><p><strong>Authors:Xiaobin Deng, Changyu Diao, Min Li, Ruohan Yu, Duanqing Xu</strong></p>
<p>Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in real-time rendering, its densification strategy often results in suboptimal reconstruction quality. In this work, we present a comprehensive improvement to the densification pipeline of 3DGS from three perspectives: when to densify, how to densify, and how to mitigate overfitting. Specifically, we propose an Edge-Aware Score to effectively select candidate Gaussians for splitting. We further introduce a Long-Axis Split strategy that reduces geometric distortions introduced by clone and split operations. To address overfitting, we design a set of techniques, including Recovery-Aware Pruning, Multi-step Update, and Growth Control. Our method enhances rendering fidelity without introducing additional training or inference overhead, achieving state-of-the-art performance with fewer Gaussians. </p>
<blockquote>
<p>尽管三维高斯斑点法（3DGS）在实时渲染方面取得了令人印象深刻的性能表现，但其稠化策略往往导致重建质量不佳。在这项工作中，我们从三个方面对3DGS的稠化流程进行了全面的改进：何时进行稠化、如何进行稠化以及如何缓解过度拟合问题。具体来说，我们提出了一种边缘感知得分（Edge-Aware Score），以有效地选择用于分割的高斯候选者。我们还引入了长轴分割策略（Long-Axis Split），以减少克隆和分割操作引入的几何失真。为解决过度拟合问题，我们设计了一系列技术，包括恢复感知修剪（Recovery-Aware Pruning）、多步更新（Multi-step Update）和生长控制（Growth Control）。我们的方法在提高了渲染保真度的同时，没有引入额外的训练或推理开销，使用更少的高斯实现了业界领先的性能表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12313v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://xiaobin2001.github.io/improved-gs-web">https://xiaobin2001.github.io/improved-gs-web</a></p>
<p><strong>Summary</strong></p>
<p>本文提出对3D高斯映射（3DGS）的密度增强流程的全面改进，从何时进行密度增强、如何进行密度增强以及如何缓解过拟合三个方面入手。通过引入边缘感知评分有效地选择候选高斯进行分割，采用长轴分割策略减少克隆和分割操作引起的几何失真。为解决过拟合问题，设计了一系列技术，包括恢复感知裁剪、多步更新和增长控制。该方法在不引入额外训练或推理开销的情况下提高了渲染保真度，实现了使用更少高斯数的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入Edge-Aware Score选择候选高斯进行分割，提高密度增强效率。</li>
<li>提出Long-Axis Split策略，减少克隆和分割操作引起的几何失真。</li>
<li>设计一系列技术解决过拟合问题，包括Recovery-Aware Pruning、Multi-step Update和Growth Control。</li>
<li>改进后的方法在渲染保真度上有所提升。</li>
<li>无需额外训练和推理开销。</li>
<li>使用更少的高斯数实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0fe19a5b24a7cda7fc351254448a851c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3877b5dbf339efdce63cb1b53fedbb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ada4628c00b673e77f5c0f6acef5e802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87cd4e0661f1d69a48468af682b46fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63390cfe03c7cba15b18892858fda8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7201f84844d8d262d78133e0b34df9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InstDrive-Instance-Aware-3D-Gaussian-Splatting-for-Driving-Scenes"><a href="#InstDrive-Instance-Aware-3D-Gaussian-Splatting-for-Driving-Scenes" class="headerlink" title="InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes"></a>InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes</h2><p><strong>Authors:Hongyuan Liu, Haochen Yu, Jianfei Jiang, Qiankun Liu, Jiansheng Chen, Huimin Ma</strong></p>
<p>Reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, we introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization. Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive, and to the best of our knowledge, it is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.More visualizations are available at our project page. </p>
<blockquote>
<p>从行车记录仪视频重建动态驾驶场景已经引起了越来越多的关注，这在自动驾驶和场景理解方面具有重要意义。虽然最近的进步已经取得了令人印象深刻的进展，但大多数方法仍然将所有背景元素统一到一个单一表示中，这阻碍了实例级别的理解和灵活的场景编辑。一些方法试图将2D分割提升到3D空间，但通常依赖于预处理的实例ID或复杂的管道来将连续特征映射到离散身份。此外，这些方法通常针对室内场景设计，视点丰富，使其不太适合室外驾驶场景。在本文中，我们提出了专为动态驾驶场景的交互式重建量身定制的实例感知3D高斯展平框架——InstDrive。我们使用SAM生成的蒙版作为伪真实标签，通过对比损失和伪监督目标来指导2D特征学习。在3D级别，我们引入正则化来隐式编码实例身份并通过基于体素的损失强制执行一致性。一个轻量级的静态编码本进一步连接连续特征和离散身份，无需数据预处理或复杂的优化。定量和定性实验证明了InstDrive的有效性，据我们所知，它是第一个实现在动态、开放世界驾驶场景中3D实例分割的框架。更多的可视化内容可在我们的项目页面查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12015v1">PDF</a> </p>
<p><strong>摘要</strong><br>    本文针对动态驾驶场景的重建展开研究，提出一个名为InstDrive的实例感知三维高斯溅泼框架，用于交互式重建动态驾驶场景。该研究利用SAM生成的掩膜作为伪真实标签，指导二维特征学习，并在三维层面引入正则化隐性编码实例身份，通过基于体素的损失进行一致性执行。框架适用于动态开放世界驾驶场景的3D实例分割，填补了现有技术的空白。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>动态驾驶场景的重建在自动驾驶和场景理解中具有重要性，吸引了越来越多的关注。</li>
<li>当前的方法在重建时将所有背景元素合并为一个单一表示，这影响了实例级别的理解和灵活的场景编辑。</li>
<li>本文提出的InstDrive框架利用实例感知进行重建，这有助于提高驾驶场景的理解和编辑能力。</li>
<li>使用SAM生成的掩膜作为伪真实标签，用于指导二维特征学习。</li>
<li>在三维层面引入正则化隐性编码实例身份，增强了框架的实例感知能力。</li>
<li>通过基于体素的损失进行一致性执行，提高了重建结果的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12015">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-673653db46723c5c6215e3bed41b0712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71b30a80b2e0730822ec28e391cc0f7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f66b2dfb1f5e6db1642b7f8aafbf33bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0ef0cca1e091f733368655984bfc7a0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ComplicitSplat-Downstream-Models-are-Vulnerable-to-Blackbox-Attacks-by-3D-Gaussian-Splat-Camouflages"><a href="#ComplicitSplat-Downstream-Models-are-Vulnerable-to-Blackbox-Attacks-by-3D-Gaussian-Splat-Camouflages" class="headerlink" title="ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by   3D Gaussian Splat Camouflages"></a>ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by   3D Gaussian Splat Camouflages</h2><p><strong>Authors:Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haorang Wang, Matthew Lau, Wenke Lee, Wilian Lunardi, Martin Andreoni, Polo Chau</strong></p>
<p>As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks for efficient novel-view synthesis from static images, how might an adversary tamper images to cause harm? We introduce ComplicitSplat, the first attack that exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle - to embed adversarial content in scene objects that are visible only from specific viewpoints and without requiring access to model architecture or weights. Our extensive experiments show that ComplicitSplat generalizes to successfully attack a variety of popular detector - both single-stage, multi-stage, and transformer-based models on both real-world capture of physical objects and synthetic scenes. To our knowledge, this is the first black-box attack on downstream object detectors using 3DGS, exposing a novel safety risk for applications like autonomous navigation and other mission-critical robotic systems. </p>
<blockquote>
<p>随着3D高斯贴图技术（3DGS）在静态图像高效合成新型视角的安全关键任务中得到迅速采用，对手如何篡改图像以造成危害？我们引入了ComplicitSplat技术，这是一种首次利用标准3DGS着色方法来创建视点特定迷彩的攻击技术——颜色和纹理会随着观察角度而改变——在场景对象中嵌入对抗内容，只有在特定视点才能看到这些场景对象，并且不需要访问模型架构或权重。我们的广泛实验表明，ComplicitSplat能够推广到攻击各种流行的检测器，包括单阶段、多阶段和基于变压器的模型，无论是在现实世界中对物理对象的捕获还是在合成场景中。据我们所知，这是利用3DGS对下游目标检测器的首次黑箱攻击，揭示了自动驾驶导航和其他关键任务机器人系统的应用中存在的新型安全风险。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11854v1">PDF</a> 7 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了ComplicitSplat攻击，这是一种针对使用3D高斯拼贴（3DGS）技术的安全关键任务的新型攻击方法。该攻击利用标准3DGS着色方法创建视角特定伪装，在场景对象中添加对抗性内容，这些内容只在特定视角下可见。该攻击无需获取模型架构或权重信息即可实现攻击。实验表明，ComplicitSplat攻击能够成功攻击各种流行的检测器模型，包括单阶段、多阶段和基于转换器的模型，对现实世界捕获的物理对象和合成场景具有通用性。这是首次针对使用3DGS的下游对象检测器的黑箱攻击，暴露出自动驾驶和关键任务机器人系统等应用的全新安全风险。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ComplicitSplat是一种针对3DGS技术的攻击方法，能够在场景对象中嵌入对抗性内容。</li>
<li>该攻击利用标准3DGS着色方法创建视角特定伪装，使得对抗性内容只在特定视角下可见。</li>
<li>ComplicitSplat攻击无需获取模型架构或权重信息即可实现攻击。</li>
<li>该攻击方法能够成功攻击各种流行的检测器模型，包括单阶段、多阶段和基于转换器的模型。</li>
<li>ComplicitSplat攻击具有在现实世界捕获的物理对象和合成场景中的通用性。</li>
<li>这是首次针对使用3DGS的下游对象检测器的黑箱攻击。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11854">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-23c9c9d4f92d22560fce1d1da2510dc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92e79b1ca976e559acea90cb16b68cf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9eb355b3dc81cbd50fe9a186d70a885.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56c12919f4f89ce79b9fc6a1d4b7501b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5edcd21ce6a3830da6c440fe93cabb81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33c8b19f6a255cb9dfd1912f3df3bd35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cbe2c377b5dd883a4d7b6d756303242.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SLAG-Scalable-Language-Augmented-Gaussian-Splatting"><a href="#SLAG-Scalable-Language-Augmented-Gaussian-Splatting" class="headerlink" title="SLAG: Scalable Language-Augmented Gaussian Splatting"></a>SLAG: Scalable Language-Augmented Gaussian Splatting</h2><p><strong>Authors:Laszlo Szilagyi, Francis Engelmann, Jeannette Bohg</strong></p>
<p>Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: <a target="_blank" rel="noopener" href="https://slag-project.github.io/">https://slag-project.github.io/</a>. </p>
<blockquote>
<p>语言增强的场景表示在大型机器人应用方面，如搜索和救援、智能城市和采矿等领域具有巨大潜力。许多这些场景都是时间敏感型的，需要快速场景编码，同时也是数据密集型的，需要可扩展的解决方案。在具有有限计算资源的机器人上部署这些表示形式进一步增加了挑战。为了解决这个问题，我们引入了SLAG，这是一个用于语言增强高斯拼接的多GPU框架，它提高了嵌入大型场景的速度和可扩展性。我们的方法通过将2D视觉语言模型特征集成到3D场景中，使用SAM和CLIP。与以前的方法不同，SLAG不需要使用损失函数来计算每个高斯语言嵌入。相反，它通过归一化加权平均从3D高斯场景参数中提取嵌入，从而实现高度并行的场景编码。此外，我们还引入了一个向量数据库，用于有效地存储和检索嵌入。我们的实验表明，与OpenGaussian相比，SLAG在16 GPU设置上实现了嵌入计算的速度提升18倍，同时在ScanNet和LERF数据集上保持了嵌入质量。更多细节，请访问我们的项目网站：<a target="_blank" rel="noopener" href="https://slag-project.github.io./">https://slag-project.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08124v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>语言增强的场景表示在大型机器人应用，如搜救、智能城市和采矿等领域具有巨大潜力。为应对这些场景的时间敏感性和数据密集型特点，提出SLAG多GPU框架，采用语言增强的高斯点云技术，提高大场景嵌入的速度和可扩展性。该方法通过SAM和CLIP整合2D视觉语言模型特征到3D场景，无需损失函数计算每个高斯语言的嵌入。SLAG通过归一化加权平均从3D高斯场景参数中导出嵌入，实现高度并行的场景编码。此外，引入向量数据库，实现高效的嵌入存储和检索。实验表明，SLAG在16 GPU设置上实现了相对于OpenGaussian的18倍加速嵌入计算，同时在ScanNet和LERF数据集上保持了嵌入质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言增强的场景表示在机器人应用中具有巨大潜力，特别是在大型、时间敏感、数据密集型的场景中。</li>
<li>SLAG是一个多GPU框架，用于处理语言增强的高斯点云技术，提高大场景嵌入的速度和可扩展性。</li>
<li>SLAG整合了2D视觉语言模型特征到3D场景，通过归一化加权平均导出嵌入，实现高度并行的场景编码。</li>
<li>与现有方法不同，SLAG无需损失函数计算语言嵌入。</li>
<li>SLAG引入了向量数据库，实现了高效的嵌入存储和检索。</li>
<li>实验表明，SLAG在加速嵌入计算方面相对于OpenGaussian有显著优势，同时保持了嵌入质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08124">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-587725383adbd7c163db472951cec056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ea49185cc170cc55bc071d4ce73eec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9326af7c2a731b287dc9fce9a764255e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04ad81d3376b530538b88b37f7efb256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf357db645c262d098e3feb01272ee8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Casual3DHDR-Deblurring-High-Dynamic-Range-3D-Gaussian-Splatting-from-Casually-Captured-Videos"><a href="#Casual3DHDR-Deblurring-High-Dynamic-Range-3D-Gaussian-Splatting-from-Casually-Captured-Videos" class="headerlink" title="Casual3DHDR: Deblurring High Dynamic Range 3D Gaussian Splatting from   Casually Captured Videos"></a>Casual3DHDR: Deblurring High Dynamic Range 3D Gaussian Splatting from   Casually Captured Videos</h2><p><strong>Authors:Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, Peidong Liu</strong></p>
<p>Photo-realistic novel view synthesis from multi-view images, such as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), has gained significant attention for its superior performance. However, most existing methods rely on low dynamic range (LDR) images, limiting their ability to capture detailed scenes in high-contrast environments. While some prior works address high dynamic range (HDR) scene reconstruction, they typically require multi-view sharp images with varying exposure times captured at fixed camera positions, which is time-consuming and impractical. To make data acquisition more flexible, we propose \textbf{Casual3DHDR}, a robust one-stage method that reconstructs 3D HDR scenes from casually-captured auto-exposure (AE) videos, even under severe motion blur and unknown, varying exposure times. Our approach integrates a continuous-time camera trajectory into a unified physical imaging model, jointly optimizing exposure times, camera trajectory, and the camera response function (CRF). Extensive experiments on synthetic and real-world datasets demonstrate that \textbf{Casual3DHDR} outperforms existing methods in robustness and rendering quality. Our source code and dataset will be available at <a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/CasualHDRSplat/">https://lingzhezhao.github.io/CasualHDRSplat/</a> </p>
<blockquote>
<p>基于多视角图像的光照真实感新型视图合成，例如神经辐射场（NeRF）和3D高斯拼贴（3DGS），因其卓越性能而受到广泛关注。然而，大多数现有方法依赖于低动态范围（LDR）图像，限制了它们在高对比度环境中捕捉精细场景的能力。虽然一些早期的工作解决了高动态范围（HDR）场景重建问题，但它们通常需要固定相机位置拍摄的多视角清晰图像，这些图像具有不同的曝光时间，既耗时又不切实际。为了使数据采集更加灵活，我们提出了一种稳健的一次性方法\textbf{Casual3DHDR}，可从随意捕获的自动曝光（AE）视频中重建3D HDR场景，即使在严重运动模糊和未知、变化的曝光时间下也可实现。我们的方法将连续时间相机轨迹集成到一个统一的物理成像模型中，联合优化曝光时间、相机轨迹和相机响应函数（CRF）。在合成和真实世界数据集上的大量实验表明，\textbf{Casual3DHDR}在鲁棒性和渲染质量方面优于现有方法。我们的源代码和数据集将在<a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/CasualHDRSplat/%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://lingzhezhao.github.io/CasualHDRSplat/上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17728v3">PDF</a> Accepted to ACM Multimedia 2025. Project page:   <a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/CasualHDRSplat/">https://lingzhezhao.github.io/CasualHDRSplat/</a></p>
<p><strong>Summary</strong></p>
<p>该文介绍了基于神经辐射场和3D高斯贴图技术的真实小说视角合成技术。针对现有方法在低动态范围图像上的局限性，提出一种名为Casual3DHDR的新方法，能够从非专业拍摄的自拍视频重建出三维高动态范围场景，并在严重运动模糊和未知、变化曝光时间下表现优异。该方法整合连续时间相机轨迹至统一物理成像模型，联合优化曝光时间、相机轨迹和相机响应函数。实验证明Casual3DHDR在稳健性和渲染质量上超越现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实小说视角合成技术结合神经辐射场和3D高斯贴图技术受到关注。</li>
<li>现有方法主要依赖低动态范围图像，难以捕捉高对比场景细节。</li>
<li>提出Casual3DHDR方法，能从非专业拍摄的自拍视频重建三维高动态范围场景。</li>
<li>Casual3DHDR在严重运动模糊和未知、变化曝光时间下表现优异。</li>
<li>方法整合连续时间相机轨迹至物理成像模型，优化曝光时间、相机轨迹和相机响应函数。</li>
<li>实验证明Casual3DHDR在稳健性和渲染质量上超越现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d19000adc2656b75d1c85bfae5aec2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa46f4327a0325b05f2a15d45a8d111a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af240a8d155299044775b22ac9f30a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8051e98ea27320a0fa26c634f7bbad87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf5ab129892cbee6b7b051e90d197c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54ea4593628d745597d43f098551c054.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SLGaussian-Fast-Language-Gaussian-Splatting-in-Sparse-Views"><a href="#SLGaussian-Fast-Language-Gaussian-Splatting-in-Sparse-Views" class="headerlink" title="SLGaussian: Fast Language Gaussian Splatting in Sparse Views"></a>SLGaussian: Fast Language Gaussian Splatting in Sparse Views</h2><p><strong>Authors:Kangjie Chen, BingQuan Dai, Minghan Qin, Dongbin Zhang, Peihao Li, Yingshuang Zou, Haoqian Wang</strong></p>
<p>3D semantic field learning is crucial for applications like autonomous navigation, AR&#x2F;VR, and robotics, where accurate comprehension of 3D scenes from limited viewpoints is essential. Existing methods struggle under sparse view conditions, relying on inefficient per-scene multi-view optimizations, which are impractical for many real-world tasks. To address this, we propose SLGaussian, a feed-forward method for constructing 3D semantic fields from sparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring consistent SAM segmentations through video tracking and using low-dimensional indexing for high-dimensional CLIP features, SLGaussian efficiently embeds language information in 3D space, offering a robust solution for accurate 3D scene understanding under sparse view conditions. In experiments on two-view sparse 3D object querying and segmentation in the LERF and 3D-OVS datasets, SLGaussian outperforms existing methods in chosen IoU, Localization Accuracy, and mIoU. Moreover, our model achieves scene inference in under 30 seconds and open-vocabulary querying in just 0.011 seconds per query. </p>
<blockquote>
<p>三维语义场学习对于自主导航、AR&#x2F;VR和机器人等应用至关重要，这些应用需要准确理解从有限视角观察到的三维场景。现有方法在稀疏视角条件下表现不佳，依赖于不切实际的场景多视角优化，这在许多现实世界任务中并不实用。为了解决这个问题，我们提出了SLGaussian方法，这是一种前馈方法，可以从稀疏视角构建三维语义场，允许直接推断基于3DGS的场景。通过确保通过视频跟踪的一致性SAM分割，并使用低维索引处理高维CLIP特征，SLGaussian有效地将语言信息嵌入三维空间，为稀疏视角条件下的准确三维场景理解提供了稳健的解决方案。在LERF和3D-OVS数据集上的两视角稀疏三维对象查询和分割实验表明，SLGaussian在选定的IoU、定位精度和mIoU方面优于现有方法。此外，我们的模型能够在不到30秒内实现场景推断，每次查询的开放词汇查询时间仅为0.011秒。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08331v3">PDF</a> Accepted by ACM MM 2025. Project page:   <a target="_blank" rel="noopener" href="https://chenkangjie1123.github.io/SLGaussian.github.io/">https://chenkangjie1123.github.io/SLGaussian.github.io/</a></p>
<p><strong>摘要</strong><br>    SLGaussian能够从稀疏视角构建三维语义场，解决真实场景中有限视角下的三维场景理解问题。该方法将语言信息嵌入三维空间，并通过视频跟踪确保SAM分割的一致性，采用高维CLIP特征的低维索引实现高效嵌入。在稀疏两视图三维物体查询和分割实验中，SLGaussian在IoU、定位精度和mIoU等方面优于现有方法，场景推理时间小于30秒，开放词汇查询每秒可达0.01次。该方法对自主导航、AR&#x2F;VR和机器人等应用具有重要意义。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SLGaussian解决了现有方法在稀疏视角条件下对三维场景理解的困难。</li>
<li>通过高效嵌入语言信息至三维空间实现了精准的三维场景理解。</li>
<li>利用视频跟踪确保SAM分割的一致性，增强模型性能。</li>
<li>采用低维索引处理高维CLIP特征，提升效率。</li>
<li>在LERF和3D-OVS数据集上的实验显示，SLGaussian在IoU、定位精度和mIoU等方面优于现有方法。</li>
<li>SLGaussian可实现快速场景推理（小于30秒）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f3f93d7e7810f810545100464bfc6827.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7e8c06c535616d3d87dc6465ac0f68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d00da06ec060ddd742494c28d1833ecc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DGNS-Deformable-Gaussian-Splatting-and-Dynamic-Neural-Surface-for-Monocular-Dynamic-3D-Reconstruction"><a href="#DGNS-Deformable-Gaussian-Splatting-and-Dynamic-Neural-Surface-for-Monocular-Dynamic-3D-Reconstruction" class="headerlink" title="DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for   Monocular Dynamic 3D Reconstruction"></a>DGNS: Deformable Gaussian Splatting and Dynamic Neural Surface for   Monocular Dynamic 3D Reconstruction</h2><p><strong>Authors:Xuesong Li, Jinguang Tong, Jie Hong, Vivien Rolland, Lars Petersson</strong></p>
<p>Dynamic scene reconstruction from monocular video is essential for real-world applications. We introduce DGNS, a hybrid framework integrating \underline{D}eformable \underline{G}aussian Splatting and Dynamic \underline{N}eural \underline{S}urfaces, effectively addressing dynamic novel-view synthesis and 3D geometry reconstruction simultaneously. During training, depth maps generated by the deformable Gaussian splatting module guide the ray sampling for faster processing and provide depth supervision within the dynamic neural surface module to improve geometry reconstruction. Conversely, the dynamic neural surface directs the distribution of Gaussian primitives around the surface, enhancing rendering quality. In addition, we propose a depth-filtering approach to further refine depth supervision. Extensive experiments conducted on public datasets demonstrate that DGNS achieves state-of-the-art performance in 3D reconstruction, along with competitive results in novel-view synthesis. </p>
<blockquote>
<p>从单目视频中重建动态场景对于实际应用至关重要。我们引入了DGNS，这是一个混合框架，集成了可变形的高斯喷绘（Deformable Gaussian Splatting）和动态神经网络表面（Dynamic Neural Surfaces），可以有效地同时处理动态新颖视图合成和3D几何重建。在训练过程中，由可变形高斯喷绘模块生成的深度图引导光线采样以实现更快的处理速度，并在动态神经网络模块内部提供深度监督，以提高几何重建的效果。相反，动态神经网络表面指导高斯原始元素在表面周围的分布，从而提高渲染质量。此外，我们还提出了一种深度过滤方法来进一步优化深度监督。在公共数据集上进行的广泛实验表明，DGNS在3D重建方面达到了最新技术水平，同时在新颖视图合成方面也取得了具有竞争力的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03910v3">PDF</a> </p>
<p><strong>Summary</strong><br>新一代动态场景重建技术DGNS融合变形高斯喷溅和动态神经网络表面，能有效实现动态新视角合成和3D几何重建。DGNS通过深度图指导光线采样以提高处理速度，同时改进几何重建；动态神经网络表面则能提升渲染质量。采用深度过滤方法进一步优化深度监督，在公共数据集上取得最先进的3D重建性能及新视角合成竞争力表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DGNS是一个融合变形高斯喷溅和动态神经网络表面的混合框架，用于动态场景重建。</li>
<li>DGNS能有效实现动态新视角合成和3D几何重建。</li>
<li>深度图在DGNS中起到重要作用，指导光线采样以提高处理速度，并改进几何重建。</li>
<li>动态神经网络表面能提升渲染质量，通过引导高斯原始数据的分布来实现。</li>
<li>采用深度过滤方法进一步优化深度监督，提高性能。</li>
<li>DGNS在公共数据集上实现了先进的3D重建性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03910">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e6ccf2732b13436362f7f53ca5c3f481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99661dd56eae70326c32f9951d63052e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88b934437b170a88a3b95bed74fc89ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df6228dcdf5ae0528afbefd79a303e93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7c769102004bfb5a68586a199ae57aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3baecbdbacb7f26cf68c9d22cd0c02ed.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Speedy-Splat-Fast-3D-Gaussian-Splatting-with-Sparse-Pixels-and-Sparse-Primitives"><a href="#Speedy-Splat-Fast-3D-Gaussian-Splatting-with-Sparse-Pixels-and-Sparse-Primitives" class="headerlink" title="Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse   Primitives"></a>Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse   Primitives</h2><p><strong>Authors:Alex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, Tom Goldstein</strong></p>
<p>3D Gaussian Splatting (3D-GS) is a recent 3D scene reconstruction technique that enables real-time rendering of novel views by modeling scenes as parametric point clouds of differentiable 3D Gaussians. However, its rendering speed and model size still present bottlenecks, especially in resource-constrained settings. In this paper, we identify and address two key inefficiencies in 3D-GS to substantially improve rendering speed. These improvements also yield the ancillary benefits of reduced model size and training time. First, we optimize the rendering pipeline to precisely localize Gaussians in the scene, boosting rendering speed without altering visual fidelity. Second, we introduce a novel pruning technique and integrate it into the training pipeline, significantly reducing model size and training time while further raising rendering speed. Our Speedy-Splat approach combines these techniques to accelerate average rendering speed by a drastic $\mathit{6.71\times}$ across scenes from the Mip-NeRF 360, Tanks &amp; Temples, and Deep Blending datasets. </p>
<blockquote>
<p>3D高斯展开技术（3D-GS）是一种最新的三维场景重建技术，它通过把场景建模为可微分的三维高斯参数点云来实现实时渲染新颖视角。然而，其渲染速度和模型大小仍然面临瓶颈，特别是在资源受限的环境中。在这篇论文中，我们识别并解决了在3D-GS中的两个关键低效问题，以显著提高渲染速度。这些改进还带来了模型大小和训练时间减少的辅助效益。首先，我们优化了渲染流程，精确地将高斯定位在场景中，以提高渲染速度而不影响视觉保真度。其次，我们引入了一种新型修剪技术并将其集成到训练流程中，在进一步提高了渲染速度的同时显著减少了模型大小和训练时间。我们的Speedy-Splat方法结合了这些技术，在Mip-NeRF 360、Tanks＆Temples和Deep Blending数据集的场景中平均渲染速度提高了惊人的6.71倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00578v3">PDF</a> CVPR 2025, Project Page: <a target="_blank" rel="noopener" href="https://speedysplat.github.io/">https://speedysplat.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对实时渲染瓶颈的改进版三维高斯点云渲染技术。通过优化渲染管道和引入新型修剪技术，该技术显著提高了渲染速度，同时减小了模型大小并缩短了训练时间。这些改进使场景渲染速度大幅提升，平均提升幅度达到$\times 6.71$倍。该技术的实施对场景重建领域具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了新的三维高斯点云渲染技术改进版本。此技术提高了实时渲染性能。</li>
<li>优化渲染管道以提高渲染速度，且不影响视觉保真度。这种优化能大幅提高渲染效率。</li>
<li>引入了一种新型修剪技术并将其集成到训练管道中，显著减小模型大小并缩短训练时间。这种修剪技术进一步提高了渲染速度。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00578">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b016c6d0d038f79d8168b04288af656d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e4aa5af078a50a81b2fa7a82fb96f0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0779e7661672a9a13f06c4514c1211c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48bca12f73986c1407d0764da3620e60.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Quadratic-Gaussian-Splatting-High-Quality-Surface-Reconstruction-with-Second-order-Geometric-Primitives"><a href="#Quadratic-Gaussian-Splatting-High-Quality-Surface-Reconstruction-with-Second-order-Geometric-Primitives" class="headerlink" title="Quadratic Gaussian Splatting: High Quality Surface Reconstruction with   Second-order Geometric Primitives"></a>Quadratic Gaussian Splatting: High Quality Surface Reconstruction with   Second-order Geometric Primitives</h2><p><strong>Authors:Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, Shunhan Shen</strong></p>
<p>We propose Quadratic Gaussian Splatting (QGS), a novel representation that replaces static primitives with deformable quadric surfaces (e.g., ellipse, paraboloids) to capture intricate geometry. Unlike prior works that rely on Euclidean distance for primitive density modeling–a metric misaligned with surface geometry under deformation–QGS introduces geodesic distance-based density distributions. This innovation ensures that density weights adapt intrinsically to the primitive curvature, preserving consistency during shape changes (e.g., from planar disks to curved paraboloids). By solving geodesic distances in closed form on quadric surfaces, QGS enables surface-aware splatting, where a single primitive can represent complex curvature that previously required dozens of planar surfels, potentially reducing memory usage while maintaining efficient rendering via fast ray-quadric intersection. Experiments on DTU, Tanks and Temples, and MipNeRF360 datasets demonstrate state-of-the-art surface reconstruction, with QGS reducing geometric error (chamfer distance) by 33% over 2DGS and 27% over GOF on the DTU dataset. Crucially, QGS retains competitive appearance quality, bridging the gap between geometric precision and visual fidelity for applications like robotics and immersive reality. </p>
<blockquote>
<p>我们提出了二次高斯贴图（Quadratic Gaussian Splatting，简称QGS）这一新型表达方式，它以可变形的二次曲面（如椭圆、抛物线等）替代静态的原始图形，以捕捉复杂的几何结构。不同于先前依赖欧几里得距离进行原始密度建模的作品——这一度量方式与变形下的表面几何结构不匹配——QGS引入了基于测地距离（geodesic distance）的密度分布。这一创新确保了密度权重能够自适应于原始曲率，在形状变化（例如从平面圆盘到弯曲的抛物线）过程中保持一致性。通过在二次曲面上解决封闭形式的测地距离问题，QGS能够实现表面感知贴图，其中单个原始图形可以代表之前需要数十个平面表面的复杂曲率，从而在保持高效渲染（通过快速的射线与二次曲面相交）的同时，可能减少内存使用。在DTU、Tanks and Temples以及MipNeRF360数据集上的实验表明，QGS在表面重建方面达到了最先进的水平，相对于二维几何贴图（2DGS）减少了33%的几何误差（混错距离），相对于GOF在DTU数据集上减少了27%。关键的是，QGS保持了竞争力的外观质量，在机器人和沉浸式现实等应用中，在几何精度和视觉保真度之间架起了桥梁。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16392v4">PDF</a> 16pages,18figures</p>
<p><strong>Summary</strong></p>
<p>一种名为Quadratic Gaussian Splatting（QGS）的新型表示方法，通过引入可变形的二次曲面（如椭圆、抛物线等）来捕捉复杂的几何形状，替换静态的原始模型。该方法创新性地采用基于测地距离（geodesic distance）的密度分布，使密度权重能够自适应原始曲率的变形，保证形状变化时的一致性。通过解决二次曲面上的测地距离问题，QGS实现了表面感知的拼接技术，单个原始模型可以代表复杂的曲面，可能大大减少内存使用，同时保持高效的渲染速度。实验证明，QGS在表面重建上具有最佳状态，减少了几何误差，并在某些数据集上超越了其他方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>QGS引入可变形二次曲面捕捉复杂几何形状，替换静态原始模型。</li>
<li>采用基于测地距离的密度分布，确保形状变化时密度权重的一致性。</li>
<li>QGS实现表面感知的拼接技术，提高内存效率和渲染速度。</li>
<li>QGS在表面重建上表现最佳状态，减少几何误差。</li>
<li>QGS在DTU数据集上相比其他方法减少了几何错误约33%。</li>
<li>QGS在保持几何精度的同时，具有优秀的视觉效果，适用于机器人和沉浸式现实等应用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16392">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ee8ccfdd5c72cbe6e3c43d36db81692.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ede5b9c089abb208a02ee275b01e52e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55e6fc5608092b0b34200ce0e8040ff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11054e273d5c29a6bad15193edac95b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137b819c6fa0fba8a41ab5e17cbd3073.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfc003dc297cee101b45241bb357f823.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reconstructing-Satellites-in-3D-from-Amateur-Telescope-Images"><a href="#Reconstructing-Satellites-in-3D-from-Amateur-Telescope-Images" class="headerlink" title="Reconstructing Satellites in 3D from Amateur Telescope Images"></a>Reconstructing Satellites in 3D from Amateur Telescope Images</h2><p><strong>Authors:Zhiming Chang, Boyang Liu, Yifei Xia, Youming Guo, Boxin Shi, He Sun</strong></p>
<p>Monitoring space objects is crucial for space situational awareness, yet reconstructing 3D satellite models from ground-based telescope images is challenging due to atmospheric turbulence, long observation distances, limited viewpoints, and low signal-to-noise ratios. In this paper, we propose a novel computational imaging framework that overcomes these obstacles by integrating a hybrid image pre-processing pipeline with a joint pose estimation and 3D reconstruction module based on controlled Gaussian Splatting (GS) and Branch-and-Bound (BnB) search. We validate our approach on both synthetic satellite datasets and on-sky observations of China’s Tiangong Space Station and the International Space Station, achieving robust 3D reconstructions of low-Earth orbit satellites from ground-based data. Quantitative evaluations using SSIM, PSNR, LPIPS, and Chamfer Distance demonstrate that our method outperforms state-of-the-art NeRF-based approaches, and ablation studies confirm the critical role of each component. Our framework enables high-fidelity 3D satellite monitoring from Earth, offering a cost-effective alternative for space situational awareness. Project page: <a target="_blank" rel="noopener" href="https://ai4scientificimaging.org/ReconstructingSatellites">https://ai4scientificimaging.org/ReconstructingSatellites</a> </p>
<blockquote>
<p>对空间目标进行监测是获取太空态势感知的关键，然而，由于大气湍流、长观测距离、有限的观测视角以及信噪比低等因素，从地面望远镜图像重建卫星三维模型是一项挑战。在本文中，我们提出了一种新型的计算成像框架，通过集成混合图像预处理管道和基于受控的高斯Splatting（GS）和分支界定（BnB）搜索的联合姿态估计和三维重建模块来克服这些障碍。我们在合成卫星数据集和中国天宫空间站以及国际空间站的实时天文观测上验证了我们的方法，实现了从地面数据对低地球轨道卫星的稳健三维重建。使用结构相似性度量（SSIM）、峰值信噪比（PSNR）、局部感知图像相似性（LPIPS）和Chamfer距离进行的定量评估表明，我们的方法优于最新的基于NeRF的方法，并且消融研究证实了每个组件的关键作用。我们的框架能够实现从地球的高保真三维卫星监测，为太空态势感知提供经济高效的替代方案。项目页面：<a target="_blank" rel="noopener" href="https://ai4scientificimaging.org/ReconstructingSatellites">网站链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18394v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的计算成像框架，通过整合混合图像预处理管道与基于受控高斯散斑（GS）和分支定界（BnB）搜索的联合姿态估计和三维重建模块，克服了从地面望远镜图像重建卫星三维模型的诸多挑战。该研究对合成卫星数据集和中国天宫空间站及国际空间站的实地观测进行了验证，实现了从地面数据稳健重建低地球轨道卫星的三维模型。定量评估表明，该方法优于现有的NeRF技术，各组件的作用至关重要。该研究为从地球进行的高保真三维卫星监测提供了成本效益高的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>监测空间物体对于空间态势感知至关重要，但从地面望远镜图像重建卫星三维模型具有挑战性。</li>
<li>本文提出了一种新型计算成像框架，整合了混合图像预处理管道和基于受控高斯散斑与分支定界搜索的联合姿态估计和三维重建模块。</li>
<li>该方法实现了对合成卫星数据集和真实卫星观测的稳健三维重建。</li>
<li>定量评估表明，该方法优于现有的NeRF技术。</li>
<li>消融研究证实了框架中每个组件的关键作用。</li>
<li>该框架为从地球进行的高保真三维卫星监测提供了可能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.18394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9b62f14cc9829fb42b3c0f0f63b2d49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff14a3def413dd089c2f5950aba389a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8930004fa7f91273b3dc909aee3c624d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3cc9e69bc827aa2620018535b4fdb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018982e6735d7f300c3030b152fd3bd3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-54ea4593628d745597d43f098551c054.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-20  Transfer Learning for Neutrino Scattering Domain Adaptation with GANs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e83cfbcfc54ef6020ccbcdd7582e46ed.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-20  WP-CLIP Leveraging CLIP to Predict Wölfflin's Principles in Visual   Art
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
