<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  RepreGuard Detecting LLM-Generated Text by Revealing Hidden   Representation Patterns">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-14e939ae0e1c5761564f4fa7243d1ef3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-20-æ›´æ–°"><a href="#2025-08-20-æ›´æ–°" class="headerlink" title="2025-08-20 æ›´æ–°"></a>2025-08-20 æ›´æ–°</h1><h2 id="RepreGuard-Detecting-LLM-Generated-Text-by-Revealing-Hidden-Representation-Patterns"><a href="#RepreGuard-Detecting-LLM-Generated-Text-by-Revealing-Hidden-Representation-Patterns" class="headerlink" title="RepreGuard: Detecting LLM-Generated Text by Revealing Hidden   Representation Patterns"></a>RepreGuard: Detecting LLM-Generated Text by Revealing Hidden   Representation Patterns</h2><p><strong>Authors:Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong</strong></p>
<p>Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/NLP2CT/RepreGuard">https://github.com/NLP2CT/RepreGuard</a> </p>
<blockquote>
<p>æ£€æµ‹ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„å†…å®¹å¯¹äºé˜²æ­¢æ»¥ç”¨å’Œæ„å»ºå¯ä¿¡çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰çš„æ£€æµ‹æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç¦»ç¾¤åˆ†å¸ƒåœºæ™¯ä¸­çš„ç¨³å¥æ€§ä»ç„¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ä¸ç°æœ‰æ£€æµ‹æ–¹æ³•ä½¿ç”¨çš„ç‰¹å¾ç›¸æ¯”ï¼ŒLLMçš„å†…éƒ¨è¡¨ç¤ºåŒ…å«æ›´å…¨é¢å’ŒåŸå§‹çš„ç‰¹å¾ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æ•è·å’ŒåŒºåˆ†LLMç”Ÿæˆæ–‡æœ¬ï¼ˆLGTï¼‰å’Œäººç±»æ’°å†™æ–‡æœ¬ï¼ˆHWTï¼‰ä¹‹é—´çš„ç»Ÿè®¡æ¨¡å¼å·®å¼‚ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„LLMä¸ŠéªŒè¯äº†è¿™ä¸€å‡è®¾ï¼Œå¹¶è§‚å¯Ÿåˆ°åœ¨å¤„ç†è¿™ä¸¤ç§ç±»å‹çš„æ–‡æœ¬æ—¶ï¼Œç¥ç»æ¿€æ´»æ¨¡å¼å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RepreGuardï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç»Ÿè®¡çš„é«˜æ•ˆæ£€æµ‹æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ›¿ä»£æ¨¡å‹æ¥æ”¶é›†LGTå’ŒHWTçš„è¡¨ç¤ºï¼Œå¹¶æå–å‡ºèƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†LGTçš„ç‹¬ç‰¹æ¿€æ´»ç‰¹å¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—æ–‡æœ¬è¡¨ç¤ºæ²¿æ­¤ç‰¹å¾æ–¹å‘çš„æŠ•å½±åˆ†æ•°ï¼Œå¹¶ä¸é¢„å…ˆè®¡ç®—çš„é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒæ¥åˆ†ç±»æ–‡æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRepreGuardåœ¨æ‰€æœ‰åŸºçº¿æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œåœ¨å†…å¤–åˆ†å¸ƒåœºæ™¯ä¸‹çš„å¹³å‡AUROCè¾¾åˆ°94.92%ï¼ŒåŒæ—¶å¯¹äºå„ç§æ–‡æœ¬å¤§å°å’Œä¸»æµæ”»å‡»ä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æŠµå¾¡èƒ½åŠ›ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NLP2CT/RepreGuard%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NLP2CT/RepreGuardæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13152v1">PDF</a> Accepted to TACL 2025. This version is a pre-MIT Press publication   version</p>
<p><strong>Summary</strong></p>
<p>LLMå†…å®¹ç”Ÿæˆç‰©çš„æ£€æµ‹å¯¹äºé˜²æ­¢æ»¥ç”¨å’Œæ„å»ºå¯ä¿¡çš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•çš„é²æ£’æ€§åœ¨éå¸¸è§„åœºæ™¯ä¸‹ä»æœ‰æ‰€æ¬ ç¼ºã€‚æœ¬ç ”ç©¶å‡è®¾LLMçš„å†…éƒ¨è¡¨å¾åŒ…å«äº†æ›´å…¨é¢å’ŒåŸå§‹çš„ç‰¹å¾ï¼Œæ›´æœ‰æ•ˆåœ°æ•æ‰å’ŒåŒºåˆ†LLMç”Ÿæˆæ–‡æœ¬ï¼ˆLGTï¼‰å’Œäººç±»ä¹¦å†™æ–‡æœ¬ï¼ˆHWTï¼‰ä¹‹é—´çš„ç»Ÿè®¡æ¨¡å¼å·®å¼‚ã€‚åŸºäºæ­¤å‡è®¾ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†RepreGuardè¿™ä¸€åŸºäºç»Ÿè®¡çš„é«˜æ•ˆæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¡ç®—æ–‡æœ¬è¡¨å¾æ²¿ç‰¹å®šç‰¹å¾æ–¹å‘çš„æŠ•å½±åˆ†æ•°å¹¶ä¸é¢„è®¡ç®—é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒæ¥åˆ†ç±»æ–‡æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRepreGuardåœ¨å¸¸è§„åœºæ™¯å’Œéå¸¸è§„åœºæ™¯ä¸‹çš„å¹³å‡AUROCè¾¾åˆ°94.92%ï¼Œå¯¹å„ç§æ–‡æœ¬å¤§å°å’Œä¸»æµæ”»å‡»è¡¨ç°å‡ºç¨³å¥çš„æŠµæŠ—åŠ›ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç”Ÿæˆå†…å®¹çš„æ£€æµ‹å¯¹äºç¡®ä¿AIç³»ç»Ÿçš„å¯é æ€§å’Œé˜²æ­¢æ»¥ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•åœ¨éå¸¸è§„åœºæ™¯ä¸‹çš„é²æ£’æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>LLMçš„å†…éƒ¨è¡¨å¾åŒ…å«æ›´å…¨é¢å’ŒåŸå§‹çš„ç‰¹å¾ï¼Œæœ‰åŠ©äºåŒºåˆ†LLMç”Ÿæˆæ–‡æœ¬å’Œäººç±»ä¹¦å†™æ–‡æœ¬ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºç»Ÿè®¡çš„RepreGuardæ£€æµ‹æ–¹æ³•ã€‚</li>
<li>RepreGuardé€šè¿‡è®¡ç®—æ–‡æœ¬è¡¨å¾çš„æŠ•å½±åˆ†æ•°è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªé¢„è®¡ç®—çš„é˜ˆå€¼è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRepreGuardåœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½å’Œç¨³å¥çš„æŠµæŠ—åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d555b5912d9a4771559b137a350e870.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32d83eb34af2ed32a9b992393a7c3be0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3ee319284051fe0fdc7b339b7962a04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8566da9a048259514ca09c7ec8dbdf2c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAJIC-Markovian-Adaptive-Jailbreaking-via-Iterative-Composition-of-Diverse-Innovative-Strategies"><a href="#MAJIC-Markovian-Adaptive-Jailbreaking-via-Iterative-Composition-of-Diverse-Innovative-Strategies" class="headerlink" title="MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of   Diverse Innovative Strategies"></a>MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of   Diverse Innovative Strategies</h2><p><strong>Authors:Weiwei Qi, Shuo Shao, Wei Gu, Tianhang Zheng, Puning Zhao, Zhan Qin, Kui Ren</strong></p>
<p>Large Language Models (LLMs) have exhibited remarkable capabilities but remain vulnerable to jailbreaking attacks, which can elicit harmful content from the models by manipulating the input prompts. Existing black-box jailbreaking techniques primarily rely on static prompts crafted with a single, non-adaptive strategy, or employ rigid combinations of several underperforming attack methods, which limits their adaptability and generalization. To address these limitations, we propose MAJIC, a Markovian adaptive jailbreaking framework that attacks black-box LLMs by iteratively combining diverse innovative disguise strategies. MAJIC first establishes a &#96;&#96;Disguise Strategy Poolâ€™â€™ by refining existing strategies and introducing several innovative approaches. To further improve the attack performance and efficiency, MAJIC formulate the sequential selection and fusion of strategies in the pool as a Markov chain. Under this formulation, MAJIC initializes and employs a Markov matrix to guide the strategy composition, where transition probabilities between strategies are dynamically adapted based on attack outcomes, thereby enabling MAJIC to learn and discover effective attack pathways tailored to the target model. Our empirical results demonstrate that MAJIC significantly outperforms existing jailbreak methods on prominent models such as GPT-4o and Gemini-2.0-flash, achieving over 90% attack success rate with fewer than 15 queries per attempt on average. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´è¶Šç‹±æ”»å‡»ï¼ˆjailbreaking attacksï¼‰çš„å¨èƒï¼Œè¿™äº›æ”»å‡»é€šè¿‡æ“çºµè¾“å…¥æç¤ºæ¥æ¿€å‘æ¨¡å‹ä¸­çš„æœ‰å®³å†…å®¹ã€‚ç°æœ‰çš„é»‘ç›’è¶Šç‹±æŠ€æœ¯ä¸»è¦ä¾èµ–äºä½¿ç”¨å•ä¸€ã€éè‡ªé€‚åº”ç­–ç•¥æ„å»ºçš„é™æ€æç¤ºï¼Œæˆ–è€…é‡‡ç”¨å‡ ç§è¡¨ç°ä¸ä½³çš„æ”»å‡»æ–¹æ³•çš„åƒµåŒ–ç»„åˆï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MAJICï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé©¬å°”å¯å¤«çš„è‡ªé€‚åº”è¶Šç‹±æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿­ä»£ç»“åˆå¤šç§åˆ›æ–°çš„ä¼ªè£…ç­–ç•¥æ¥æ”»å‡»é»‘ç›’LLMã€‚MAJICé¦–å…ˆé€šè¿‡ç²¾ç‚¼ç°æœ‰ç­–ç•¥å¹¶å¼•å…¥å¤šç§åˆ›æ–°æ–¹æ³•å»ºç«‹â€œä¼ªè£…ç­–ç•¥æ± â€ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ”»å‡»æ€§èƒ½å’Œæ•ˆç‡ï¼ŒMAJICå°†æ± ä¸­ç­–ç•¥çš„é¡ºåºé€‰æ‹©å’Œèåˆåˆ¶å®šä¸ºé©¬å°”å¯å¤«é“¾ã€‚åœ¨è¿™ç§åˆ¶å®šä¸‹ï¼ŒMAJICåˆå§‹åŒ–å’Œä½¿ç”¨ä¸€ä¸ªé©¬å°”å¯å¤«çŸ©é˜µæ¥æŒ‡å¯¼ç­–ç•¥ç»„åˆï¼Œå…¶ä¸­ç­–ç•¥ä¹‹é—´çš„è½¬ç§»æ¦‚ç‡æ ¹æ®æ”»å‡»ç»“æœåŠ¨æ€è°ƒæ•´ï¼Œä½¿MAJICèƒ½å¤Ÿå­¦ä¹ å’Œå‘ç°é’ˆå¯¹ç›®æ ‡æ¨¡å‹é‡èº«å®šåˆ¶çš„æœ‰æ•ˆæ”»å‡»é€”å¾„ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒMAJICåœ¨GPT-4oå’ŒGemini-2.0-flashç­‰ä¸»æµæ¨¡å‹ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰è¶Šç‹±æ–¹æ³•ï¼Œå¹³å‡æ¯æ¬¡å°è¯•æŸ¥è¯¢æ¬¡æ•°å°‘äº15æ¬¡çš„æƒ…å†µä¸‹ï¼Œæ”»å‡»æˆåŠŸç‡è¶…è¿‡90%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13048v1">PDF</a> 7 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>LLMé¢ä¸´é»‘ç›’æ”»å‡»é£é™©ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æå‡ºMAJICæ¡†æ¶ï¼Œé€šè¿‡Markové“¾è‡ªé€‚åº”ç»„åˆå¤šç§ä¼ªè£…ç­–ç•¥è¿›è¡Œæ”»å‡»ï¼Œæé«˜æ”»å‡»æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´é»‘ç›’æ”»å‡»é£é™©ï¼Œå¯èƒ½å¯¼è‡´è¾“å‡ºæœ‰å®³å†…å®¹ã€‚</li>
<li>ç°æœ‰é»‘ç›’è¶Šç‹±æŠ€æœ¯ä¸»è¦ä¾èµ–é™æ€æç¤ºæˆ–å¤šç§æ”»å‡»æ–¹æ³•çš„ç»„åˆï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MAJICæ¡†æ¶é€šè¿‡Markové“¾è‡ªé€‚åº”ç»„åˆå¤šç§ä¼ªè£…ç­–ç•¥è¿›è¡Œæ”»å‡»ã€‚</li>
<li>MAJICå»ºç«‹äº†ä¼ªè£…ç­–ç•¥æ± ï¼ŒåŒ…å«ç°æœ‰ç­–ç•¥å’Œæ–°ç­–ç•¥ã€‚</li>
<li>MAJICé‡‡ç”¨MarkovçŸ©é˜µæŒ‡å¯¼ç­–ç•¥ç»„åˆï¼ŒåŠ¨æ€é€‚åº”æ”»å‡»ç»“æœï¼Œä»¥å‘ç°é’ˆå¯¹ç›®æ ‡æ¨¡å‹çš„æœ‰æ•ˆæ”»å‡»è·¯å¾„ã€‚</li>
<li>MAJICæ˜¾è‘—æé«˜äº†æ”»å‡»æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-863e08dff9c65911d5f0de55875d117b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7a3ab45178836df618756e822209c78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01a29bbf15e53e79e78afc038e0089d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-893c2c9813986996e6cb943e06d58c2b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Using-AI-for-User-Representation-An-Analysis-of-83-Persona-Prompts"><a href="#Using-AI-for-User-Representation-An-Analysis-of-83-Persona-Prompts" class="headerlink" title="Using AI for User Representation: An Analysis of 83 Persona Prompts"></a>Using AI for User Representation: An Analysis of 83 Persona Prompts</h2><p><strong>Authors:Joni Salminen, Danial Amin, Bernard Jansen</strong></p>
<p>We analyzed 83 persona prompts from 27 research articles that used large language models (LLMs) to generate user personas. Findings show that the prompts predominantly generate single personas. Several prompts express a desire for short or concise persona descriptions, which deviates from the tradition of creating rich, informative, and rounded persona profiles. Text is the most common format for generated persona attributes, followed by numbers. Text and numbers are often generated together, and demographic attributes are included in nearly all generated personas. Researchers use up to 12 prompts in a single study, though most research uses a small number of prompts. Comparison and testing multiple LLMs is rare. More than half of the prompts require the persona output in a structured format, such as JSON, and 74% of the prompts insert data or dynamic variables. We discuss the implications of increased use of computational personas for user representation. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ†æäº†27ç¯‡ç ”ç©¶æ–‡ç« ä¸­çš„83ä¸ªè§’è‰²æç¤ºï¼Œè¿™äº›æ–‡ç« ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆç”¨æˆ·è§’è‰²ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æç¤ºä¸»è¦ç”Ÿæˆå•ä¸€è§’è‰²ã€‚è®¸å¤šæç¤ºè¡¨è¾¾äº†å¯¹äºç®€çŸ­æˆ–ç®€æ´çš„è§’è‰²æè¿°çš„æ¸´æœ›ï¼Œè¿™ä¸åˆ›å»ºä¸°å¯Œã€æœ‰ä¿¡æ¯é‡å’Œå…¨é¢çš„è§’è‰²èµ„æ–™çš„ä¼ ç»Ÿç›¸æ‚–ã€‚æ–‡æœ¬æ˜¯ç”Ÿæˆè§’è‰²å±æ€§æœ€å¸¸è§çš„æ ¼å¼ï¼Œå…¶æ¬¡æ˜¯æ•°å­—ã€‚æ–‡æœ¬å’Œæ•°å­—é€šå¸¸ä¸€èµ·ç”Ÿæˆï¼Œå‡ ä¹æ‰€æœ‰ç”Ÿæˆçš„è§’è‰²ä¸­éƒ½åŒ…å«äººå£ç»Ÿè®¡å±æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ä¸€ä¸ªç ”ç©¶ä¸­æœ€å¤šä½¿ç”¨å¤šè¾¾12ä¸ªæç¤ºï¼Œä½†å¤§å¤šæ•°ç ”ç©¶ä½¿ç”¨çš„æç¤ºæ•°é‡è¾ƒå°‘ã€‚æ¯”è¾ƒå’Œæµ‹è¯•å¤šä¸ªLLMçš„æƒ…å†µå¾ˆå°‘è§ã€‚è¶…è¿‡ä¸€åŠçš„æç¤ºè¦æ±‚è§’è‰²è¾“å‡ºé‡‡ç”¨ç»“æ„åŒ–æ ¼å¼ï¼Œå¦‚JSONï¼Œ74%çš„æç¤ºæ’å…¥æ•°æ®æˆ–åŠ¨æ€å˜é‡ã€‚æˆ‘ä»¬æ¢è®¨äº†è®¡ç®—è§’è‰²åœ¨ç”¨æˆ·ä»£è¡¨æ–¹é¢è¶Šæ¥è¶Šå¤šçš„ä½¿ç”¨çš„å«ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13047v1">PDF</a> Accepted at AICCSA-2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç”¨æˆ·äººè®¾çš„83ä¸ªæç¤ºåˆ†ææ˜¾ç¤ºï¼Œè¿™äº›æç¤ºä¸»è¦ç”Ÿæˆå•ä¸€äººè®¾ã€‚ç ”ç©¶äººå‘˜å€¾å‘äºç®€çŸ­æè¿°äººè®¾ï¼Œä¸ä¼ ç»Ÿåˆ›å»ºä¸°å¯Œã€å…¨é¢çš„äººè®¾æè¿°æœ‰æ‰€ä¸åŒã€‚æ–‡æœ¬æ˜¯æœ€å¸¸è§çš„äººè®¾å±æ€§å½¢å¼ï¼Œå…¶æ¬¡æ˜¯æ•°å­—ï¼Œä¸¤è€…ç»å¸¸ä¸€èµ·ç”Ÿæˆã€‚å¤§å¤šæ•°ç ”ç©¶ä½¿ç”¨çš„æç¤ºæ•°é‡è¾ƒå°‘ï¼Œè¶…è¿‡ä¸€åŠçš„æç¤ºè¦æ±‚äººè®¾è¾“å‡ºä¸ºç»“æ„åŒ–æ ¼å¼å¦‚JSONï¼Œä¸”74%çš„æç¤ºä¼šæ’å…¥æ•°æ®æˆ–åŠ¨æ€å˜é‡ã€‚æœ¬æ–‡æ¢è®¨äº†è®¡ç®—äººè®¾åœ¨ç”¨æˆ·ä»£è¡¨æ–¹é¢çš„åº”ç”¨åŠå…¶å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æç¤ºä¸»è¦ç”¨äºåˆ›å»ºå•ä¸€äººè®¾ã€‚</li>
<li>ç ”ç©¶å€¾å‘äºç®€çŸ­æè¿°äººè®¾ï¼Œè€Œéä¼ ç»Ÿä¸°å¯Œçš„æè¿°æ–¹å¼ã€‚</li>
<li>æ–‡æœ¬å’Œæ•°å­—æ˜¯æœ€å¸¸è§çš„äººè®¾å±æ€§å½¢å¼ï¼Œç»å¸¸ç»“åˆç”Ÿæˆã€‚</li>
<li>å¤§å¤šæ•°ç ”ç©¶ä½¿ç”¨è¾ƒå°‘çš„æç¤ºæ•°é‡ã€‚</li>
<li>è¶…è¿‡ä¸€åŠçš„æç¤ºè¦æ±‚äººè®¾è¾“å‡ºä¸ºç»“æ„åŒ–æ ¼å¼å¦‚JSONã€‚</li>
<li>å¤§éƒ¨åˆ†æç¤ºä¼šæ’å…¥æ•°æ®æˆ–åŠ¨æ€å˜é‡ã€‚</li>
<li>ä½¿ç”¨è®¡ç®—äººè®¾åœ¨ç”Ÿæˆç”¨æˆ·ä»£è¡¨æ–¹é¢çš„åº”ç”¨æ—¥ç›Šæ™®åŠã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e48c7224af06a0604db05edace9cdd91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15a8e45ee668164a04f1e57819e948e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8330ddd519ab9f4a9d5cbe0b114d10d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51a50854d63bd72fb44796a52eaa78a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Buyuk-Dil-Modelleri-icin-TR-MMLU-Benchmarki-Performans-Degerlendirmesi-Zorluklar-ve-Iyilestirme-Firsatlari"><a href="#Buyuk-Dil-Modelleri-icin-TR-MMLU-Benchmarki-Performans-Degerlendirmesi-Zorluklar-ve-Iyilestirme-Firsatlari" class="headerlink" title="BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans   DeÄŸerlendirmesi, Zorluklar ve Ä°yileÅŸtirme FÄ±rsatlarÄ±"></a>BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans   DeÄŸerlendirmesi, Zorluklar ve Ä°yileÅŸtirme FÄ±rsatlarÄ±</h2><p><strong>Authors:M. Ali Bayram, Ali Arda Fincan, Ahmet Semih GÃ¼mÃ¼ÅŸ, Banu Diri, SavaÅŸ YÄ±ldÄ±rÄ±m, Ã–ner AytaÅŸ</strong></p>
<p>Language models have made significant advancements in understanding and generating human language, achieving remarkable success in various applications. However, evaluating these models remains a challenge, particularly for resource-limited languages like Turkish. To address this issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a meticulously curated dataset comprising 6,200 multiple-choice questions across 62 sections within the Turkish education system. This benchmark provides a standard framework for Turkish NLP research, enabling detailed analyses of LLMsâ€™ capabilities in processing Turkish text. In this study, we evaluated state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model design. TR-MMLU sets a new standard for advancing Turkish NLP research and inspiring future innovations. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†ä»¤äººç©ç›®çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåœŸè€³å…¶è¯­ç­‰èµ„æºæœ‰é™çš„è¯­è¨€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åœŸè€³å…¶MMLUï¼ˆTR-MMLUï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åœŸè€³å…¶è¯­ä¸­çš„è¯­è¨€å’Œæ¦‚å¿µèƒ½åŠ›ã€‚TR-MMLUåŸºäºç²¾å¿ƒç­›é€‰çš„æ•°æ®é›†ï¼ŒåŒ…å«åœŸè€³å…¶æ•™è‚²ä½“ç³»ä¸­62ä¸ªé¢†åŸŸçš„6200ä¸ªé€‰æ‹©é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºåœŸè€³å…¶NLPç ”ç©¶æä¾›äº†ä¸€ä¸ªæ ‡å‡†æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹LLMå¤„ç†åœŸè€³å…¶æ–‡æœ¬çš„èƒ½åŠ›è¿›è¡Œè¯¦ç»†åˆ†æã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨TR-MMLUä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMï¼Œçªå‡ºäº†æ¨¡å‹è®¾è®¡éœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚TR-MMLUä¸ºæ¨è¿›åœŸè€³å…¶NLPç ”ç©¶è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå¹¶æ¿€å‘äº†æœªæ¥çš„åˆ›æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13044v1">PDF</a> 10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd   Signal Processing and Communications Applications Conference (SIU), 25â€“28   June 2025, Sile, Istanbul, T&quot;urkiye</p>
<p><strong>Summary</strong><br>     è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†éå‡¡çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒåœŸè€³å…¶è¯­è¿™æ ·çš„èµ„æºæœ‰é™çš„è¯­è¨€ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åœŸè€³å…¶MMLUï¼ˆTR-MMLUï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åœŸè€³å…¶è¯­ä¸­çš„è¯­è¨€å’Œç†è§£èƒ½åŠ›çš„å…¨é¢è¯„ä¼°æ¡†æ¶ã€‚TR-MMLUåŸºäºç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†ï¼ŒåŒ…å«åœŸè€³å…¶æ•™è‚²ç³»ç»Ÿä¸­çš„6200ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ã€‚å®ƒä¸ºåœŸè€³å…¶NLPç ”ç©¶æä¾›äº†æ ‡å‡†æ¡†æ¶ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿè¯¦ç»†åˆ†æLLMå¤„ç†åœŸè€³å…¶æ–‡æœ¬çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶åœ¨TR-MMLUä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMï¼Œå¹¶æŒ‡å‡ºäº†æ¨¡å‹è®¾è®¡éœ€è¦æ”¹è¿›çš„é¢†åŸŸã€‚TR-MMLUä¸ºæ¨è¿›åœŸè€³å…¶NLPç ”ç©¶è®¾ç«‹äº†æ–°æ ‡å‡†ï¼Œå¹¶æ¿€å‘äº†æœªæ¥çš„åˆ›æ–°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºä¸åŒé¢†åŸŸã€‚</li>
<li>è¯„ä¼°è¯­è¨€æ¨¡å‹å¯¹èµ„æºæœ‰é™çš„è¯­è¨€ï¼ˆå¦‚åœŸè€³å…¶è¯­ï¼‰ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥TR-MMLUåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åœŸè€³å…¶è¯­ä¸­çš„è¯­è¨€å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>TR-MMLUåŸºäºåŒ…å«6200ä¸ªå¤šé¡¹é€‰æ‹©é¢˜çš„ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†æ„å»ºã€‚</li>
<li>TR-MMLUä¸ºåœŸè€³å…¶NLPç ”ç©¶æä¾›äº†æ ‡å‡†æ¡†æ¶ï¼Œä¾¿äºåˆ†æLLMå¤„ç†åœŸè€³å…¶æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨TR-MMLUä¸Šè¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMï¼Œå‘ç°äº†æ¨¡å‹è®¾è®¡çš„æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11c268099d94bb40bea0475141fb7104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2af3e5454a3f0385cb67bd15a6ae933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e28104f76475e3b88d447d1684397c85.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-Large-Models-Teach-Student-Models-to-Solve-Mathematical-Problems-Like-Human-Beings-A-Reasoning-Distillation-Method-via-Multi-LoRA-Interaction"><a href="#Can-Large-Models-Teach-Student-Models-to-Solve-Mathematical-Problems-Like-Human-Beings-A-Reasoning-Distillation-Method-via-Multi-LoRA-Interaction" class="headerlink" title="Can Large Models Teach Student Models to Solve Mathematical Problems   Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction"></a>Can Large Models Teach Student Models to Solve Mathematical Problems   Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</h2><p><strong>Authors:Xinhe Li, Jiajun Liu, Peng Wang</strong></p>
<p>Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¾ˆå¼ºçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†éœ€è¦ä¾èµ–æ•°ç™¾äº¿å‚æ•°ã€‚ä¸ºäº†è§£å†³å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ¨ç†èƒ½åŠ›è¾ƒå·®çš„æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤§é‡æ•°æ®è¿›è¡Œå¡«é¸­å¼è®­ç»ƒã€‚åœ¨å¿ƒç†å­¦ä¸­ï¼Œè¿™ä¸ç³»ç»Ÿ1æ€ç»´ç±»ä¼¼ï¼Œå³åŸºäºç»éªŒå’Œç›´è§‰å¿«é€Ÿè§£å†³æ¨ç†é—®é¢˜ã€‚ç„¶è€Œï¼Œäººç±»å­¦ä¹ è¿˜éœ€è¦ç³»ç»Ÿ2æ€ç»´ï¼Œå³å…ˆè·å–çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å®è·µè¿›è¡Œå·©å›ºã€‚å—è¿™ä¸¤ç§ä¸åŒæ€ç»´æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šLoRAäº¤äº’çš„æ•°å­¦æ¨ç†è’¸é¦ï¼ˆLoRIDï¼‰çš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ ·æœ¬çš„é—®é¢˜å’Œæ¨ç†è¾“å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºçŸ¥è¯†å¢å¼ºæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å­¦ç”Ÿæ¨¡å‹ä¸Šè®­ç»ƒLoRAå—ä½œä¸ºç›´è§‰æ¨ç†è€…ï¼ˆIRï¼‰ï¼Œç›´æ¥ç”Ÿæˆè§£å†³é—®é¢˜çš„æ€ç»´é“¾ã€‚æ¥ç€ï¼Œä¸ºäº†æ¨¡ä»¿ç³»ç»Ÿ2æ€ç»´ï¼Œæˆ‘ä»¬åˆ†åˆ«è®­ç»ƒçŸ¥è¯†ç”Ÿæˆå™¨ï¼ˆKGï¼‰å’Œæ·±åº¦æ¨ç†å™¨ï¼ˆDRï¼‰ã€‚å‰è€…åœ¨æ¥æ”¶åˆ°é—®é¢˜ååªè¾“å‡ºçŸ¥è¯†ï¼Œè€Œåè€…åˆ™åˆ©ç”¨è¿™äº›çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚æœ€åï¼Œä¸ºäº†è§£å†³IRå’ŒDRç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšæœºæ€§ï¼Œæˆ‘ä»¬è¯„ä¼°ä»–ä»¬çš„è¾“å‡ºæ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´ï¼Œåˆ™éœ€è¦è¿­ä»£æ¨ç†è¿‡ç¨‹ã€‚è¿™ä¸€æ­¥å¯ä»¥é€šè¿‡ç›¸äº’åé¦ˆå¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRIDè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨GSM8Kæ•°æ®é›†ä¸Šï¼Œä¸ç¬¬äºŒåæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†2.3%ã€16.1%ã€2.4%ã€12.3%å’Œ1.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13037v1">PDF</a> Accepted by IJCAI2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†ä¾èµ–ç™¾äº¿å‚æ•°ã€‚ä¸ºè§£å†³å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ¨ç†èƒ½åŠ›å¼±çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•å¸¸åˆ©ç”¨LLMç”Ÿæˆå¤§é‡æ•°æ®è¿›è¡Œå¡«é¸­å¼è®­ç»ƒã€‚æœ¬æ–‡å—å¿ƒç†å­¦ä¸­ä¸¤ç§æ€è€ƒæ¨¡å¼å¯å‘ï¼Œæå‡ºåŸºäºå¤šLoRAäº¤äº’çš„æ•°å­¦æ¨ç†è’¸é¦ï¼ˆLoRIDï¼‰æ–°æ–¹æ³•ã€‚é¦–å…ˆåˆ©ç”¨LLMåˆ›å»ºçŸ¥è¯†å¢å¼ºæ•°æ®é›†ï¼Œè®­ç»ƒç›´è§‰æ¨ç†å™¨ï¼ˆIRï¼‰ï¼Œæ¨¡ä»¿åŸºäºç»éªŒå’Œç›´è§‰çš„å¿«é€Ÿæ¨ç†ã€‚æ¥ç€è®­ç»ƒçŸ¥è¯†ç”Ÿæˆå™¨ï¼ˆKGï¼‰å’Œæ·±åº¦æ¨ç†å™¨ï¼ˆDRï¼‰ï¼Œåˆ†åˆ«è¾“å‡ºçŸ¥è¯†å’Œè¿›è¡Œæ¨ç†ã€‚é€šè¿‡è¯„ä¼°ä¸¤è€…è¾“å‡ºçš„ä¸€è‡´æ€§ï¼Œè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæå‡SLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRIDåœ¨GSM8Kæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œåœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹ä¸Šåˆ†åˆ«è¶…è¶Šç¬¬äºŒåæ–¹æ³•2.3%ã€16.1%ã€2.4%ã€12.3%å’Œ1.8%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†ä¾èµ–å¤§é‡å‚æ•°ã€‚</li>
<li>SLMæ¨ç†èƒ½åŠ›è¾ƒå¼±ï¼Œç°æœ‰æ–¹æ³•å€ŸåŠ©LLMç”Ÿæˆæ•°æ®å¯¹å…¶è¿›è¡Œè®­ç»ƒæå‡ã€‚</li>
<li>å—å¿ƒç†å­¦å¯å‘ï¼Œæå‡ºåŸºäºä¸¤ç§æ€è€ƒæ¨¡å¼çš„LoRIDæ–¹æ³•ï¼Œæ¨¡ä»¿äººç±»çš„å¿«é€Ÿæ¨ç†å’ŒçŸ¥è¯†åº”ç”¨ã€‚</li>
<li>LoRIDé€šè¿‡åˆ›å»ºçŸ¥è¯†å¢å¼ºæ•°æ®é›†ã€è®­ç»ƒIRã€KGå’ŒDRä¸‰ä¸ªç»„ä»¶æ¥æå‡SLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LoRIDæ–¹æ³•é€šè¿‡è¯„ä¼°IRå’ŒDRè¾“å‡ºçš„ä¸€è‡´æ€§è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLoRIDåœ¨GSM8Kæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10318d949e17ceb3f0253078487e21c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd7078e96ffae7bb81cac64b8ff6acc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b05bf9b146a20753809be6504cb03b86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-565da1b7129bd2e13c0c89a8fbb6551b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-Application-of-Transformer-Based-Models-for-Predicting-Consequences-of-Cyber-Attacks"><a href="#The-Application-of-Transformer-Based-Models-for-Predicting-Consequences-of-Cyber-Attacks" class="headerlink" title="The Application of Transformer-Based Models for Predicting Consequences   of Cyber Attacks"></a>The Application of Transformer-Based Models for Predicting Consequences   of Cyber Attacks</h2><p><strong>Authors:Bipin Chhetri, Akbar Siami Namin</strong></p>
<p>Cyberattacks are increasing, and securing against such threats is costing industries billions of dollars annually. Threat Modeling, that is, comprehending the consequences of these attacks, can provide critical support to cybersecurity professionals, enabling them to take timely action and allocate resources that could be used elsewhere. Cybersecurity is heavily dependent on threat modeling, as it assists security experts in assessing and mitigating risks related to identifying vulnerabilities and threats. Recently, there has been a pressing need for automated methods to assess attack descriptions and forecast the future consequences of the increasing complexity of cyberattacks. This study examines how Natural Language Processing (NLP) and deep learning can be applied to analyze the potential impact of cyberattacks by leveraging textual descriptions from the MITRE Common Weakness Enumeration (CWE) database. We emphasize classifying attack consequences into five principal categories: Availability, Access Control, Confidentiality, Integrity, and Other. This paper investigates the use of Bidirectional Encoder Representations from Transformers (BERT) in combination with Hierarchical Attention Networks (HANs) for Multi-label classification, evaluating their performance in comparison with conventional CNN and LSTM-based models. Experimental findings show that BERT achieves an overall accuracy of $0.972$, far higher than conventional deep learning models in multi-label classification. HAN outperforms baseline forms of CNN and LSTM-based models on specific cybersecurity labels. However, BERT consistently achieves better precision and recall, making it more suitable for predicting the consequences of a cyberattack. </p>
<blockquote>
<p>ç½‘ç»œæ”»å‡»æ—¥ç›Šå¢å¤šï¼Œå¯¹æŠ—è¿™äº›å¨èƒçš„å®‰å…¨æªæ–½æ¯å¹´ç»™å„è¡Œä¸šå¸¦æ¥æ•°åäº¿ç¾å…ƒçš„æ”¯å‡ºã€‚å¨èƒå»ºæ¨¡ï¼Œå³ç†è§£è¿™äº›æ”»å‡»çš„åæœï¼Œå¯ä»¥ä¸ºç½‘ç»œå®‰å…¨ä¸“å®¶æä¾›å…³é”®æ”¯æŒï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿé‡‡å–åŠæ—¶è¡ŒåŠ¨å¹¶åˆ†é…å¯èƒ½ç”¨äºå…¶ä»–åœ°æ–¹çš„èµ„æºã€‚ç½‘ç»œå®‰å…¨ä¸¥é‡ä¾èµ–äºå¨èƒå»ºæ¨¡ï¼Œå› ä¸ºå®ƒå¯ä»¥å¸®åŠ©å®‰å…¨ä¸“å®¶è¯„ä¼°å’Œå‡è½»ä¸è¯†åˆ«æ¼æ´å’Œå¨èƒç›¸å…³çš„é£é™©ã€‚æœ€è¿‘ï¼Œéšç€ç½‘ç»œæ”»å‡»å¤æ‚æ€§çš„å¢åŠ ï¼Œæ€¥éœ€è‡ªåŠ¨æ–¹æ³•æ¥è¯„ä¼°æ”»å‡»æè¿°å¹¶é¢„æµ‹æœªæ¥çš„åæœã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼Œé€šè¿‡åˆ©ç”¨MITREå¸¸è§å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰æ•°æ®åº“ä¸­çš„æ–‡æœ¬æè¿°æ¥åˆ†æç½‘ç»œæ”»å‡»æ½œåœ¨å½±å“çš„åˆ†ç±»æ–¹æ³•ã€‚æˆ‘ä»¬å¼ºè°ƒå°†æ”»å‡»åæœåˆ†ä¸ºäº”ä¸ªä¸»è¦ç±»åˆ«ï¼šå¯ç”¨æ€§ã€è®¿é—®æ§åˆ¶ã€ä¿å¯†æ€§ã€å®Œæ•´æ€§å’Œå…¶ä»–ç±»åˆ«ã€‚æœ¬æ–‡ç ”ç©¶äº†ç»“åˆä½¿ç”¨åŸºäºå˜å‹å™¨çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼ˆBERTï¼‰å’Œåˆ†å±‚æ³¨æ„åŠ›ç½‘ç»œï¼ˆHANï¼‰è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»çš„æ–¹æ³•ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ä¼ ç»Ÿçš„CNNå’ŒLSTMæ¨¡å‹ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBERTçš„æ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°0.972ï¼Œè¿œé«˜äºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤šæ ‡ç­¾åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚HANåœ¨ç‰¹å®šçš„ç½‘ç»œå®‰å…¨æ ‡ç­¾ä¸Šä¼˜äºCNNå’ŒLSTMçš„åŸºçº¿æ¨¡å‹ã€‚ç„¶è€Œï¼ŒBERTåœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡æ–¹é¢å§‹ç»ˆè¡¨ç°æ›´å¥½ï¼Œä½¿å…¶æˆä¸ºé¢„æµ‹ç½‘ç»œæ”»å‡»åæœçš„æ›´åˆé€‚çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13030v1">PDF</a> 21 pages, 6 figures,Proceedings of the IEEE International Conference   on Computers, Software, &amp; Applications (COMPSAC), EATA Symposium, Toronto,   Canada, July 8-11, 2025</p>
<p><strong>æ‘˜è¦</strong><br>    ç½‘ç»œæ”»å‡»æ—¥ç›Šå¢å¤šï¼Œå¯¹è¡Œä¸šçš„ç½‘ç»œå®‰å…¨é˜²æŠ¤æˆæœ¬æ¯å¹´è¾¾æ•°åäº¿ç¾å…ƒã€‚å¨èƒå»ºæ¨¡èƒ½å¤Ÿæ·±å…¥ç†è§£è¿™äº›æ”»å‡»çš„åæœï¼Œä¸ºç½‘ç»œå®‰å…¨ä¸“å®¶æä¾›å…³é”®æ”¯æŒï¼Œä½¿å…¶èƒ½å¤ŸåŠæ—¶é‡‡å–è¡ŒåŠ¨å¹¶ä¸ºå¯èƒ½ç”¨äºå…¶ä»–åœ°æ–¹çš„èµ„æºåˆ†é…æä¾›å‚è€ƒã€‚ç½‘ç»œå®‰å…¨ä¸¥é‡ä¾èµ–äºå¨èƒå»ºæ¨¡ï¼Œå› ä¸ºå®ƒå¯ä»¥å¸®åŠ©å®‰å…¨ä¸“å®¶è¯„ä¼°å’Œç¼“è§£ä¸è¯†åˆ«æ¼æ´å’Œå¨èƒæœ‰å…³çš„é£é™©ã€‚è¿‘æœŸè¿«åˆ‡éœ€è¦è‡ªåŠ¨åŒ–æ–¹æ³•æ¥è¯„ä¼°æ”»å‡»æè¿°å¹¶é¢„æµ‹æ—¥ç›Šå¤æ‚çš„ç½‘ç»œæ”»å‡»çš„æœªæ¥åæœã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œæ·±åº¦å­¦ä¹ æ¥åˆ†æç½‘ç»œæ”»å‡»æ½œåœ¨å½±å“çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨MITREé€šç”¨å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰æ•°æ®åº“ä¸­çš„æ–‡æœ¬æè¿°æ¥å®ç°ã€‚æœ¬ç ”ç©¶å¼ºè°ƒå°†æ”»å‡»åæœåˆ†ç±»ä¸ºäº”ä¸ªä¸»è¦ç±»åˆ«ï¼šå¯ç”¨æ€§ã€è®¿é—®æ§åˆ¶ã€ä¿å¯†æ€§ã€å®Œæ•´æ€§å’Œå…¶ä»–ç±»åˆ«ã€‚æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•ä½¿ç”¨ç»“åˆäº†å±‚æ¬¡æ³¨æ„åŠ›ç½‘ç»œï¼ˆHANsï¼‰çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºå˜æ¢å™¨ï¼ˆBERTï¼‰è¿›è¡Œå¤šæ ‡ç­¾åˆ†ç±»ï¼Œè¯„ä¼°å…¶åœ¨ä¼ ç»ŸCNNå’ŒLSTMæ¨¡å‹ä¸­çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBERTçš„æ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°0.972ï¼Œåœ¨å¤šæ ‡ç­¾åˆ†ç±»ä¸­çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚HANåœ¨ç‰¹å®šç½‘ç»œå®‰å…¨æ ‡ç­¾ä¸Šä¼˜äºåŸºäºCNNå’ŒLSTMçš„åŸºçº¿æ¨¡å‹ã€‚ç„¶è€Œï¼ŒBERTåœ¨ç²¾ç¡®åº¦å’Œå¬å›ç‡æ–¹é¢å§‹ç»ˆè¡¨ç°æ›´å¥½ï¼Œå› æ­¤æ›´é€‚åˆé¢„æµ‹ç½‘ç»œæ”»å‡»çš„åæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç½‘ç»œæ”»å‡»é¢‘ç‡ä¸Šå‡ï¼Œå¹´åº¦å®‰å…¨æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>å¨èƒå»ºæ¨¡åœ¨ç½‘ç»œå®‰å…¨ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œå¸®åŠ©ä¸“å®¶è¯„ä¼°å¹¶ç¼“è§£é£é™©ã€‚</li>
<li>å­˜åœ¨å¯¹ç½‘ç»œæ”»å‡»åæœè‡ªåŠ¨åŒ–è¯„ä¼°çš„éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨MITRE CWEæ•°æ®åº“ä¸­çš„æ–‡æœ¬æè¿°è¿›è¡Œåˆ†æã€‚</li>
<li>æ”»å‡»åæœè¢«åˆ†ç±»ä¸ºäº”ä¸ªä¸»è¦ç±»åˆ«ã€‚</li>
<li>BERTä¸HANç»“åˆä½¿ç”¨åœ¨å¤šæ ‡ç­¾åˆ†ç±»ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®åº¦è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be1e04941290c624c1463f48b9163e90.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WebMall-â€“-A-Multi-Shop-Benchmark-for-Evaluating-Web-Agents"><a href="#WebMall-â€“-A-Multi-Shop-Benchmark-for-Evaluating-Web-Agents" class="headerlink" title="WebMall â€“ A Multi-Shop Benchmark for Evaluating Web Agents"></a>WebMall â€“ A Multi-Shop Benchmark for Evaluating Web Agents</h2><p><strong>Authors:Ralph Peeters, Aaron Steiner, Luca Schwarz, Julian Yuya Caspary, Christian Bizer</strong></p>
<p>LLM-based web agents have the potential to automate long-running web tasks, such as finding offers for specific products in multiple online shops and subsequently ordering the cheapest products that meet the users needs. This paper introduces WebMall, a multi-shop online shopping benchmark for evaluating the effectiveness and efficiency of web agents for comparison-shopping. WebMall consists of four simulated online shops populated with authentic product offers sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These tasks include basic tasks such as finding specific products in multiple shops, performing price comparisons, adding items to the shopping cart, and completing checkout. Advanced tasks involve searching for products based on vague requirements, identifying suitable substitutes, and finding compatible products. Compared to existing e-commerce benchmarks, such as WebShop or ShoppingBench, WebMall introduces comparison-shopping tasks across multiple shops. Furthermore, the product offers are more heterogeneous, as they originate from hundreds of distinct real-world shops. The tasks in WebMall require longer interaction trajectories than those in WebShop, while remaining representative of real-world shopping behaviors. We evaluate eight baseline agents on WebMall, varying in observation modality, memory utilization, and underlying large language model (GPT 4.1 and Claude Sonnet 4). The best-performing configurations achieve completion rates of 75% and 53%, and F1 scores of 87% and 63%, on the basic and advanced task sets, respectively. WebMall is publicly released to facilitate research on web agents and to promote advancements in navigation, reasoning, and efficiency within e-commerce scenarios. </p>
<blockquote>
<p>åŸºäºLLMçš„Webä»£ç†å…·æœ‰è‡ªåŠ¨åŒ–é•¿æœŸWebä»»åŠ¡çš„æ½œåŠ›ï¼Œä¾‹å¦‚åœ¨å¤šé—´ç½‘ä¸Šå•†åº—å¯»æ‰¾ç‰¹å®šäº§å“çš„ä¼˜æƒ ï¼Œç„¶åè®¢è´­ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„æœ€ä½ä»·æ ¼äº§å“ã€‚æœ¬æ–‡ä»‹ç»äº†WebMallï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°Webä»£ç†åœ¨æ¯”è¾ƒè´­ç‰©ä¸­çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡çš„è·¨åº—åœ¨çº¿è´­ç‰©åŸºå‡†æµ‹è¯•ã€‚WebMallç”±å››ä¸ªæ¨¡æ‹Ÿåœ¨çº¿å•†åº—ç»„æˆï¼Œè¿™äº›å•†åº—å……æ–¥ç€æ¥è‡ªCommon Crawlçš„çœŸå®äº§å“ä¼˜æƒ ä¿¡æ¯ï¼Œä»¥åŠä¸€å¥—91ä¸ªè·¨åº—ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬åŸºæœ¬ä»»åŠ¡ï¼Œå¦‚åœ¨å¤šé—´å•†åº—å¯»æ‰¾ç‰¹å®šäº§å“ã€æ¯”è¾ƒä»·æ ¼ã€å°†å•†å“æ·»åŠ åˆ°è´­ç‰©è½¦å¹¶å®Œæˆç»“è´¦ã€‚é«˜çº§ä»»åŠ¡åŒ…æ‹¬æ ¹æ®æ¨¡ç³Šéœ€æ±‚æœç´¢äº§å“ã€è¯†åˆ«åˆé€‚æ›¿ä»£å“å’ŒæŸ¥æ‰¾å…¼å®¹äº§å“ã€‚ä¸ç°æœ‰çš„ç”µå­å•†åŠ¡åŸºå‡†æµ‹è¯•ï¼ˆå¦‚WebShopæˆ–ShoppingBenchï¼‰ç›¸æ¯”ï¼ŒWebMallå¼•å…¥äº†è·¨å¤šä¸ªå•†åº—çš„æ¯”è¾ƒè´­ç‰©ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç”±äºäº§å“ä¼˜æƒ æ¥è‡ªæ•°ç™¾ä¸ªä¸åŒçš„çœŸå®å•†åº—ï¼Œå› æ­¤äº§å“ä¼˜æƒ æ›´åŠ å¤šæ ·åŒ–ã€‚WebMallä¸­çš„ä»»åŠ¡éœ€è¦æ¯”WebShopæ›´é•¿çš„äº¤äº’è½¨è¿¹ï¼ŒåŒæ—¶ä»ä»£è¡¨çœŸå®ä¸–ç•Œçš„è´­ç‰©è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨WebMallä¸Šè¯„ä¼°äº†å…«ç§åŸºçº¿ä»£ç†ï¼Œå®ƒä»¬åœ¨ä¸åŒè§‚å¯Ÿæ¨¡å¼ã€å†…å­˜åˆ©ç”¨å’Œåº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT 4.1å’ŒClaude Sonnet 4ï¼‰æ–¹é¢æœ‰æ‰€ä¸åŒã€‚è¡¨ç°æœ€ä½³çš„é…ç½®åœ¨åŸºæœ¬å’Œé«˜çº§ä»»åŠ¡é›†ä¸Šçš„å®Œæˆç‡åˆ†åˆ«ä¸º75%å’Œ53%ï¼ŒF1åˆ†æ•°åˆ†åˆ«ä¸º87%å’Œ63%ã€‚WebMallå…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯¹Webä»£ç†çš„ç ”ç©¶ï¼Œå¹¶æ¨åŠ¨ç”µå­å•†åŠ¡åœºæ™¯ä¸­çš„å¯¼èˆªã€æ¨ç†å’Œæ•ˆç‡æ–¹é¢çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13024v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLMé©±åŠ¨çš„Webä»£ç†åœ¨è‡ªåŠ¨åŒ–é•¿æœŸWebä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ï¼Œå¦‚è·¨å¤šä¸ªåœ¨çº¿å•†åº—æ¯”è¾ƒäº§å“å¹¶è®¢è´­æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„æœ€ä½ä»·æ ¼äº§å“ã€‚è®ºæ–‡æå‡ºWebMallåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¨¡æ‹Ÿåœ¨çº¿è´­ç‰©åœºæ™¯ä»¥è¯„ä¼°ä»£ç†åœ¨å¤„ç†å¯¹æ¯”è´­ç‰©æ—¶çš„æ•ˆèƒ½ä¸æ•ˆç‡ã€‚WebMallåŒ…å«å››ä¸ªæ¨¡æ‹Ÿåœ¨çº¿å•†åº—åŠä¸€å¥—è·¨åº—ä»»åŠ¡ï¼Œæ¶‰åŠåŸºç¡€ä»»åŠ¡å¦‚äº§å“å¯»æ‰¾ã€ä»·æ ¼å¯¹æ¯”å’Œè´­ç‰©è½¦ç®¡ç†ç­‰ï¼Œè¿˜åŒ…æ‹¬é«˜çº§ä»»åŠ¡å¦‚åŸºäºæ¨¡ç³Šè¦æ±‚æœç´¢äº§å“ã€å¯»æ‰¾æ›¿ä»£äº§å“ç­‰ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒWebMallæ¶µç›–æ›´å¤šçœŸå®å•†åº—çš„äº§å“ä¿¡æ¯ï¼Œä»»åŠ¡è½¨è¿¹æ›´é•¿ä¸”ä»£è¡¨çœŸå®è´­ç‰©è¡Œä¸ºã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³é…ç½®çš„ä»£ç†åœ¨åŸºç¡€ä»»åŠ¡é›†ä¸Šå®Œæˆç‡é«˜è¾¾75%ï¼Œé«˜çº§ä»»åŠ¡é›†ä¸Šå®Œæˆç‡ä¸º53%ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚WebMallå¹³å°å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå¯¹ä»£ç†è¿›è¡Œç ”ç©¶å¹¶æ¨åŠ¨ç”µå­å•†åŠ¡åœºæ™¯ä¸­å¯¼èˆªã€æ¨ç†å’Œæ•ˆç‡æ–¹é¢çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé©±åŠ¨çš„Webä»£ç†å¯ä»¥è‡ªåŠ¨åŒ–é•¿æœŸWebä»»åŠ¡ï¼Œå¦‚è·¨å¤šä¸ªåœ¨çº¿å•†åº—è¿›è¡Œäº§å“æ¯”è¾ƒå’Œè´­ä¹°æ»¡è¶³ç”¨æˆ·éœ€æ±‚çš„æœ€ä½ä»·æ ¼äº§å“ã€‚</li>
<li>WebMallæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿåœ¨çº¿è´­ç‰©åœºæ™¯çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°ä»£ç†åœ¨å¤„ç†å¯¹æ¯”è´­ç‰©æ—¶çš„æ•ˆèƒ½ä¸æ•ˆç‡ã€‚</li>
<li>WebMallåŒ…å«å››ä¸ªæ¨¡æ‹Ÿåœ¨çº¿å•†åº—å’Œä¸€å¥—åŒ…å«åŸºç¡€ä»»åŠ¡å’Œé«˜çº§ä»»åŠ¡çš„è·¨åº—ä»»åŠ¡é›†ã€‚</li>
<li>ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒWebMallçš„äº§å“ä¿¡æ¯æ›´ä¸°å¯Œä¸”çœŸå®ï¼Œä»»åŠ¡è½¨è¿¹æ›´é•¿ï¼Œæ›´æ¥è¿‘çœŸå®çš„è´­ç‰©è¡Œä¸ºã€‚</li>
<li>æœ€ä½³é…ç½®çš„ä»£ç†åœ¨åŸºç¡€ä»»åŠ¡é›†ä¸Šçš„å®Œæˆç‡é«˜è¾¾75%ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ½œåŠ›ã€‚è€Œåœ¨é«˜çº§ä»»åŠ¡é›†ä¸Šçš„å®Œæˆç‡ä¸º53%ï¼Œä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>WebMallå¹³å°å·²å…¬å¼€å‘å¸ƒï¼Œä¸ºä»£ç†ç ”ç©¶æä¾›äº†ä¾¿åˆ©çš„å·¥å…·ï¼Œæœ‰åŠ©äºæ¨åŠ¨ç”µå­å•†åŠ¡åœºæ™¯ä¸­å¯¼èˆªã€æ¨ç†å’Œæ•ˆç‡æ–¹é¢çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-657e1cef1c71a394c94080848a152520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7800447470596b895e61b5098ce3e00f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa6432ab39fb1752d487694831927264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3981b850f02ef59c49ccbdd1a265ff1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e55febebe0147ff03bdee88f9c9d38a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14e939ae0e1c5761564f4fa7243d1ef3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65d4d8fa0232c9d98845645e86265828.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="G-2-RPO-A-Guided-Group-Relative-Policy-Optimization-with-Adaptive-Guidance"><a href="#G-2-RPO-A-Guided-Group-Relative-Policy-Optimization-with-Adaptive-Guidance" class="headerlink" title="G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive   Guidance"></a>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive   Guidance</h2><p><strong>Authors:Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced the reasoning abilities of large language models (LLMs). Its success, however, largely depends on strong base models with rich world knowledge, yielding only modest improvements for small-size language models (SLMs). To address this limitation, we investigate Guided GRPO, which injects ground-truth reasoning steps into roll-out trajectories to compensate for SLMsâ€™ inherent weaknesses. Through a comprehensive study of various guidance configurations, we find that naively adding guidance delivers limited gains. These insights motivate G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength in response to the modelâ€™s evolving training dynamics. Experiments on mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A substantially outperforms vanilla GRPO. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/T-Lab-CUHKSZ/G2RPO-A">https://github.com/T-Lab-CUHKSZ/G2RPO-A</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå…·æœ‰ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œå¯¹äºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ”¹è¿›ä½œç”¨ç”šå¾®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¼•å¯¼GRPOæ–¹æ³•ï¼Œå®ƒé€šè¿‡å‘æ¨æ¼”è½¨è¿¹æ³¨å…¥çœŸå®æ¨ç†æ­¥éª¤æ¥å¼¥è¡¥SLMçš„å›ºæœ‰å¼±ç‚¹ã€‚é€šè¿‡å¯¹å„ç§å¼•å¯¼é…ç½®çš„ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ç®€å•åœ°æ·»åŠ å¼•å¯¼å¸¦æ¥çš„å¢ç›Šæœ‰é™ã€‚è¿™äº›è§è§£ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†G$^2$RPO-Aï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”ç®—æ³•ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ ¹æ®æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€è°ƒæ•´æŒ‡å¯¼å¼ºåº¦ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯å®ï¼ŒG$^2$RPO-Aå¤§å¹…ä¼˜äºæ ‡å‡†GRPOæ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/T-Lab-CUHKSZ/G2RPO-A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/T-Lab-CUHKSZ/G2RPO-Aæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13023v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå…·æœ‰ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œå¯¹äºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ”¹è¿›ä½œç”¨æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¼•å¯¼GRPOæ–¹æ³•ï¼Œé€šè¿‡å°†çœŸå®æ¨ç†æ­¥éª¤æ³¨å…¥roll-outè½¨è¿¹æ¥å¼¥è¡¥SLMçš„å›ºæœ‰å¼±ç‚¹ã€‚é€šè¿‡å…¨é¢ç ”ç©¶å„ç§æŒ‡å¯¼é…ç½®ï¼Œæˆ‘ä»¬å‘ç°ç®€å•åœ°æ·»åŠ æŒ‡å¯¼å¸¦æ¥çš„æ”¶ç›Šæœ‰é™ã€‚è¿™äº›è§è§£å‚¬ç”Ÿäº†è‡ªé€‚åº”ç®—æ³•G^2RPO-Aï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€è‡ªåŠ¨è°ƒæ•´æŒ‡å¯¼å¼ºåº¦ã€‚åœ¨æ•°ç†æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¯å®ï¼ŒG^2RPO-Aå¤§å¹…ä¼˜äºæ ‡å‡†GRPOã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/T-Lab-CUHKSZ/G2RPO-A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/T-Lab-CUHKSZ/G2RPO-Aè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹çš„æ”¹è¿›ç›¸å¯¹æœ‰é™ã€‚</li>
<li>å¼•å¯¼GRPOæ–¹æ³•é€šè¿‡æ³¨å…¥çœŸå®æ¨ç†æ­¥éª¤æ¥å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç®€å•åœ°æ·»åŠ æŒ‡å¯¼åœ¨æå‡æ€§èƒ½ä¸Šæ•ˆæœæœ‰é™ã€‚</li>
<li>G^2RPO-Aæ˜¯ä¸€ç§è‡ªé€‚åº”ç®—æ³•ï¼Œå¯æ ¹æ®æ¨¡å‹è®­ç»ƒåŠ¨æ€è°ƒæ•´æŒ‡å¯¼å¼ºåº¦ã€‚</li>
<li>G^2RPO-Aåœ¨æ•°ç†æ¨ç†å’Œä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜äºæ ‡å‡†GRPOã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-baed517d2979aa469fb66430da3ed8a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a74731f080797d4034de8f8344ad5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e2d1a05a3a29abc57f71576e1d4a325.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a186ba1289e23df00f40322bd5fc68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a82b7f03c6eb184cf4b9bebb67c505a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d235fea4f86730c000e678472fd3bc76.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Arabic-ASR-on-the-SADA-Large-Scale-Arabic-Speech-Corpus-with-Transformer-Based-Models"><a href="#Arabic-ASR-on-the-SADA-Large-Scale-Arabic-Speech-Corpus-with-Transformer-Based-Models" class="headerlink" title="Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with   Transformer-Based Models"></a>Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with   Transformer-Based Models</h2><p><strong>Authors:Branislav Gerazov, Marcello Politi, SÃ©bastien BratiÃ¨res</strong></p>
<p>We explore the performance of several state-of-the-art automatic speech recognition (ASR) models on a large-scale Arabic speech dataset, the SADA (Saudi Audio Dataset for Arabic), which contains 668 hours of high-quality audio from Saudi television shows. The dataset includes multiple dialects and environments, specifically a noisy subset that makes it particularly challenging for ASR. We evaluate the performance of the models on the SADA test set, and we explore the impact of fine-tuning, language models, as well as noise and denoising on their performance. We find that the best performing model is the MMS 1B model finetuned on SADA with a 4-gram language model that achieves a WER of 40.9% and a CER of 17.6% on the SADA test clean set. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†å‡ ç§æœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨å¤§å‹é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³æ•°æ®é›†SADAï¼ˆç”¨äºé˜¿æ‹‰ä¼¯è¯­çš„æ²™ç‰¹éŸ³é¢‘æ•°æ®é›†ï¼‰ä¸Šçš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªæ²™ç‰¹ç”µè§†èŠ‚ç›®çš„é«˜è´¨é‡éŸ³é¢‘ï¼Œæ—¶é•¿668å°æ—¶ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤šç§æ–¹è¨€å’Œç¯å¢ƒï¼Œå…¶ä¸­ç‰¹åˆ«åŒ…å«ä¸€ä¸ªå˜ˆæ‚çš„å­é›†ï¼Œè¿™å¯¹ASRæ¥è¯´å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬åœ¨SADAæµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æ¢è®¨äº†å¾®è°ƒã€è¯­è¨€æ¨¡å‹ä»¥åŠå™ªå£°å’Œå»å™ªå¯¹å…¶æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°è¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯åœ¨SADAä¸Šç»è¿‡å¾®è°ƒMMS 1Bæ¨¡å‹ï¼Œä½¿ç”¨4å…ƒè¯­è¨€æ¨¡å‹ï¼Œåœ¨SADAæµ‹è¯•æ¸…æ´é›†ä¸Šå®ç°äº†40.9ï¼…çš„WERå’Œ17.6ï¼…çš„CERã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12968v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨SADAæ•°æ®é›†ä¸Šçš„æ€§èƒ½ç ”ç©¶ã€‚æ–‡ç« æ¢ç´¢äº†å¤šä¸ªå…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨å¤§å‹é˜¿æ‹‰ä¼¯è¯­éŸ³æ•°æ®é›†SADAä¸Šçš„è¡¨ç°ã€‚SADAæ•°æ®é›†åŒ…å«æ¥è‡ªæ²™ç‰¹ç”µè§†èŠ‚ç›®çš„é«˜è´¨é‡éŸ³é¢‘ï¼Œæ—¶é•¿668å°æ—¶ï¼ŒåŒ…å«å¤šç§æ–¹è¨€å’Œç¯å¢ƒï¼Œå…¶ä¸­å™ªå£°å­é›†å°¤ä¸ºæŒ‘æˆ˜ASRã€‚ç ”ç©¶è¯„ä¼°äº†æ¨¡å‹åœ¨SADAæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶æ¢è®¨äº†å¾®è°ƒã€è¯­è¨€æ¨¡å‹ä»¥åŠå™ªå£°å’Œå»å™ªå¯¹æ€§èƒ½çš„å½±å“ã€‚æœ€ä½³æ¨¡å‹ä¸ºåœ¨SADAä¸Šå¾®è°ƒè¿‡çš„MMS 1Bæ¨¡å‹ï¼Œä½¿ç”¨4-gramè¯­è¨€æ¨¡å‹ï¼Œåœ¨SADAæµ‹è¯•æ¸…æ´é›†ä¸Šå®ç°å­—é”™è¯¯ç‡ï¼ˆWERï¼‰40.9%å’Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰17.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨äº†å…ˆè¿›çš„ASRæ¨¡å‹åœ¨å¤§å‹é˜¿æ‹‰ä¼¯è¯­éŸ³æ•°æ®é›†SADAä¸Šè¿›è¡Œæ€§èƒ½ç ”ç©¶ã€‚</li>
<li>SADAæ•°æ®é›†åŒ…å«å¤šç§æ–¹è¨€å’Œç¯å¢ƒçš„éŸ³é¢‘ï¼Œå…¶ä¸­å™ªå£°å­é›†å¢åŠ äº†ASRçš„æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†æ¨¡å‹åœ¨SADAæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æœ€ä½³çš„ASRæ¨¡å‹æ˜¯MMS 1Bæ¨¡å‹ï¼Œç»è¿‡åœ¨SADAä¸Šçš„å¾®è°ƒï¼Œå¹¶ä½¿ç”¨4-gramè¯­è¨€æ¨¡å‹ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨SADAæµ‹è¯•æ¸…æ´é›†ä¸Šå®ç°äº†è¾ƒä½çš„WERå’ŒCERã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†å¾®è°ƒã€è¯­è¨€æ¨¡å‹ã€å™ªå£°å’Œå»å™ªå¯¹ASRæ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-efb218af5ef688b1277eb241015d160b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-589048573219f060235a44c47863da62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6133da511eae3b6d1f3d71d0fd79343.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Is-GPT-OSS-Good-A-Comprehensive-Evaluation-of-OpenAIâ€™s-Latest-Open-Source-Models"><a href="#Is-GPT-OSS-Good-A-Comprehensive-Evaluation-of-OpenAIâ€™s-Latest-Open-Source-Models" class="headerlink" title="Is GPT-OSS Good? A Comprehensive Evaluation of OpenAIâ€™s Latest Open   Source Models"></a>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAIâ€™s Latest Open   Source Models</h2><p><strong>Authors:Ziqian Bi, Keyu Chen, Chiung-Yi Tseng, Danyang Zhang, Tianyang Wang, Hongying Luo, Lu Chen, Junming Huang, Jibin Guan, Junfeng Hao, Junhao Song</strong></p>
<p>In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments. </p>
<blockquote>
<p>åœ¨2025å¹´8æœˆï¼ŒOpenAIå‘å¸ƒäº†GPT-OSSæ¨¡å‹ï¼Œè¿™æ˜¯è‡ª2019å¹´GPT-2ä»¥æ¥å…¶é¦–ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸¤ç§æ··åˆä¸“å®¶æ¶æ„ï¼Œåˆ†åˆ«å¸¦æœ‰120Bå’Œ20Bçš„å‚æ•°ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸¤ç§å˜ä½“ï¼Œä¸å…­ç§å½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå‚æ•°èŒƒå›´ä»14.7Båˆ°235Bï¼‰è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¿™äº›æ¨¡å‹ä»£è¡¨äº†å¯†é›†å’Œç¨€ç–ä¸¤ç§è®¾è®¡ï¼Œæ¶µç›–äº†åä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é€šç”¨çŸ¥è¯†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šè¯­ç§ç†è§£å’Œå¯¹è¯èƒ½åŠ›ç­‰æ–¹é¢ã€‚æ‰€æœ‰æ¨¡å‹éƒ½åœ¨æ ‡å‡†åŒ–çš„æ¨ç†è®¾ç½®ä¸‹ä»¥éé‡åŒ–å½¢å¼è¿›è¡Œæµ‹è¯•ï¼Œå¹¶ä½¿ç”¨McNemarsæµ‹è¯•å’Œæ•ˆåº”é‡åˆ†æè¿›è¡Œç»Ÿè®¡éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡gpt-oss-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆå¦‚HumanEvalå’ŒMMLUï¼‰ä¸Šè¡¨ç°ä¼˜äºgpt-oss-120Bï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªå›ç­”æ‰€éœ€çš„å†…å­˜å’Œèƒ½é‡æ–¹é¢å¤§å¹…é™ä½ï¼Œä½†gpt-oss-20Bè¿˜æ˜¯å±•ç°äº†åœ¨å½“å‰å¼€æºç¯å¢ƒä¸­çš„ä¸­ç«¯æ•´ä½“æ€§èƒ½ï¼Œå…¶åœ¨ä»£ç ç”Ÿæˆæ–¹é¢ç›¸å¯¹è¾ƒå¼ºï¼Œè€Œåœ¨å¤šè¯­ç§ä»»åŠ¡ä¸­å­˜åœ¨æ˜æ˜¾å¼±ç‚¹ã€‚è¿™äº›å‘ç°æä¾›äº†å®è¯è¯æ®è¡¨æ˜ï¼Œåœ¨ç¨€ç–æ¶æ„ä¸­è¿›è¡Œæ‰©å±•å¯èƒ½ä¸ä¼šäº§ç”Ÿæ¯”ä¾‹çš„æ€§èƒ½æå‡ï¼Œè¿™å¼ºè°ƒäº†è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥ç ”ç©¶çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥å¼€æºéƒ¨ç½²æä¾›æ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©ä¾æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12461v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GPT-OSSæ¨¡å‹æ˜¯OpenAIç»§GPT-2ä¹‹åçš„é¦–æ‰¹å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«ä¸¤ä¸ªæ··åˆä¸“å®¶æ¶æ„ï¼Œåˆ†åˆ«æœ‰120Bå’Œ20Bå‚æ•°ã€‚æœ¬æ–‡é€šè¿‡å¯¹æ¯”åŒ…æ‹¬GPT-OSSåœ¨å†…çš„å…­ä¸ªå½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶µç›–é€šè¯†çŸ¥è¯†ã€æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆã€å¤šè¯­è¨€ç†è§£å’Œå¯¹è¯èƒ½åŠ›ç­‰å¤šä¸ªæ–¹é¢çš„åé¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå‘ç°GPT-OSS-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºGPT-OSS-120Bï¼Œå°½ç®¡å…¶å†…å­˜å’Œèƒ½æºéœ€æ±‚æ›´å°‘ã€‚ä¸¤è€…å‡åœ¨ç°æœ‰å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½å±…äºä¸­æ¸¸ï¼Œä½†åœ¨ä»£ç ç”Ÿæˆæ–¹é¢ç›¸å¯¹å‡ºè‰²ï¼Œè€Œåœ¨å¤šè¯­è¨€ä»»åŠ¡æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚æœ¬ç ”ç©¶ä¸ºä¼˜åŒ–ç­–ç•¥å’Œæœªæ¥å¼€æºéƒ¨ç½²çš„æ¨¡å‹é€‰æ‹©æä¾›äº†å®è¯ä¾æ®å’Œå‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-OSSæ˜¯OpenAIæ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ··åˆä¸“å®¶æ¶æ„çš„ç‰ˆæœ¬ï¼Œåˆ†åˆ«æ˜¯GPT-OSS-120Bå’ŒGPT-OSS-20Bã€‚</li>
<li>å¯¹æ¯”äº†å…­ä¸ªå½“ä»£å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
<li>GPT-OSS-20Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè¶…è¶ŠGPT-OSS-120Bçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨HumanEvalå’ŒMMLUç­‰æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ã€‚</li>
<li>GPT-OSSæ¨¡å‹å†…å­˜å’Œèƒ½æºæ•ˆç‡è¾ƒé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å“åº”æ¬¡æ•°æ–¹é¢ã€‚</li>
<li>GPT-OSSæ¨¡å‹æ•´ä½“æ€§èƒ½åœ¨ç°æœ‰å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å±…äºä¸­æ¸¸æ°´å¹³ã€‚</li>
<li>GPT-OSSæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å…·å¤‡ç›¸å¯¹ä¼˜åŠ¿ï¼Œä½†åœ¨å¤šè¯­è¨€ä»»åŠ¡æ–¹é¢å­˜åœ¨æ˜æ˜¾å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12461">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7c22d6b2d2cc0b29184aa2ae3105de8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae180d879e97efdc5d9d6df8437a167d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c908ec54acfd39f906674762ca139a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfa487b706651fdb4291bdf141a84739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6e708b938eb1ddb5e8b41d2c9b840f1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bi-Axial-Transformers-Addressing-the-Increasing-Complexity-of-EHR-Classification"><a href="#Bi-Axial-Transformers-Addressing-the-Increasing-Complexity-of-EHR-Classification" class="headerlink" title="Bi-Axial Transformers: Addressing the Increasing Complexity of EHR   Classification"></a>Bi-Axial Transformers: Addressing the Increasing Complexity of EHR   Classification</h2><p><strong>Authors:Rachael DeVries, Casper Christensen, Marie Lisandra Zepeda Mendoza, Ole Winther</strong></p>
<p>Electronic Health Records (EHRs), the digital representation of a patientâ€™s medical history, are a valuable resource for epidemiological and clinical research. They are also becoming increasingly complex, with recent trends indicating larger datasets, longer time series, and multi-modal integrations. Transformers, which have rapidly gained popularity due to their success in natural language processing and other domains, are well-suited to address these challenges due to their ability to model long-range dependencies and process data in parallel. But their application to EHR classification remains limited by data representations, which can reduce performance or fail to capture informative missingness. In this paper, we present the Bi-Axial Transformer (BAT), which attends to both the clinical variable and time point axes of EHR data to learn richer data relationships and address the difficulties of data sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is competitive to top methods for mortality classification. In comparison to other transformers, BAT demonstrates increased robustness to data missingness, and learns unique sensor embeddings which can be used in transfer learning. Baseline models, which were previously located across multiple repositories or utilized deprecated libraries, were re-implemented with PyTorch and made available for reproduction and future benchmarking. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä½œä¸ºæ‚£è€…åŒ»ç–—å†å²çš„æ•°å­—åŒ–è¡¨ç¤ºï¼Œæ˜¯æµè¡Œç—…å­¦å’Œä¸´åºŠç ”ç©¶ä¸­çš„å®è´µèµ„æºã€‚å®ƒä»¬ä¹Ÿå˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œæœ€è¿‘çš„è¶‹åŠ¿è¡¨æ˜æ•°æ®é›†æ›´å¤§ï¼Œæ—¶é—´åºåˆ—æ›´é•¿ï¼Œå¹¶ä¸”å…·æœ‰å¤šæ¨¡å¼é›†æˆã€‚ç”±äºå…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸçš„æˆåŠŸï¼ŒTransformerè¿…é€Ÿæµè¡Œèµ·æ¥ï¼Œç”±äºå…¶èƒ½å¤Ÿå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶å¹¶è¡Œå¤„ç†æ•°æ®çš„èƒ½åŠ›ï¼Œéå¸¸é€‚åˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ä½†æ˜¯å®ƒä»¬åœ¨ç”µå­å¥åº·è®°å½•åˆ†ç±»ä¸­çš„åº”ç”¨å—åˆ°æ•°æ®è¡¨ç¤ºçš„å±€é™ï¼Œè¿™å¯èƒ½ä¼šé™ä½æ€§èƒ½æˆ–æ— æ³•æ•è·ä¿¡æ¯ç¼ºå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒè½´å˜å‹å™¨ï¼ˆBATï¼‰ï¼Œå®ƒå…³æ³¨ç”µå­å¥åº·è®°å½•æ•°æ®ä¸­çš„ä¸´åºŠå˜é‡å’Œæ—¶é—´ç‚¹è½´ï¼Œä»¥å­¦ä¹ æ›´ä¸°å¯Œçš„æ•°æ®å…³ç³»å¹¶è§£å†³æ•°æ®ç¨€ç–æ€§çš„å›°éš¾ã€‚BATåœ¨è„“æ¯’ç—‡é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æ­»äº¡ç‡åˆ†ç±»æ–¹é¢ä¸é¡¶çº§æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚ä¸å…¶ä»–å˜å‹å™¨ç›¸æ¯”ï¼ŒBATå¯¹ç¼ºå¤±æ•°æ®çš„ç¨³å¥æ€§æœ‰æ‰€æé«˜ï¼Œå¹¶ä¸”å­¦ä¹ äº†å¯åœ¨è¿ç§»å­¦ä¹ ä¸­ä½¿ç”¨çš„å”¯ä¸€ä¼ æ„Ÿå™¨åµŒå…¥ã€‚ä¹‹å‰ä½äºå¤šä¸ªå­˜å‚¨åº“ä¸­çš„åŸºçº¿æ¨¡å‹æˆ–ä½¿ç”¨å·²å¼ƒç”¨çš„åº“ï¼Œç°å·²é‡æ–°ä½¿ç”¨PyTorchå®ç°å¹¶å¯ä¾›å¤åˆ¶å’Œæœªæ¥çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12418v1">PDF</a> 18 pages, 7 figures. Submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰çš„ä»·å€¼å’Œé¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤§æ•°æ®é›†ã€é•¿æ—¶é—´åºåˆ—å’Œå¤šæ¨¡å¼é›†æˆç­‰ã€‚ä½œè€…æå‡ºäº†åŒå‘è½´å˜å‹å™¨ï¼ˆBATï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…³æ³¨EHRæ•°æ®çš„ä¸´åºŠå˜é‡å’Œæ—¶é—´ç‚¹è½´ï¼Œä»¥å­¦ä¹ æ›´ä¸°å¯Œæ•°æ®å…³ç³»å¹¶è§£å†³æ•°æ®ç¨€ç–æ€§é—®é¢˜ã€‚BATåœ¨è„“æ¯’ç—‡é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æ­»äº¡ç‡åˆ†ç±»æ–¹é¢ä¸å…¶ä»–é¡¶çº§æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚ä¸å…¶ä»–å˜å‹å™¨ç›¸æ¯”ï¼ŒBATå¯¹ç¼ºå¤±æ•°æ®çš„ç¨³å¥æ€§æ›´é«˜ï¼Œå¹¶ä¸”å¯ä»¥å­¦ä¹ ç‹¬ç‰¹çš„ä¼ æ„Ÿå™¨åµŒå…¥ï¼Œå¯ç”¨äºè¿ç§»å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰æ˜¯å®è´µçš„åŒ»ç–—èµ„æºï¼Œç”¨äºæµè¡Œç—…å­¦å’Œä¸´åºŠç ”ç©¶ã€‚</li>
<li>EHRs æ•°æ®æ­£å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼ŒåŒ…æ‹¬æ›´å¤§çš„æ•°æ®é›†ã€æ›´é•¿æ—¶é—´åºåˆ—å’Œå¤šæ¨¡å¼é›†æˆç­‰è¶‹åŠ¿ã€‚</li>
<li>å˜å‹å™¨æ¨¡å‹ç”±äºå…¶èƒ½å¤Ÿå»ºæ¨¡é•¿è·ç¦»ä¾èµ–æ€§å’Œå¹¶è¡Œå¤„ç†æ•°æ®çš„èƒ½åŠ›ï¼Œéå¸¸é€‚åˆåº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>Bi-Axial Transformer (BAT) æ¨¡å‹è¢«æå‡ºæ¥è§£å†³EHRåˆ†ç±»çš„æŒ‘æˆ˜ï¼Œå®ƒå…³æ³¨ä¸´åºŠå˜é‡å’Œæ—¶é—´ç‚¹è½´ã€‚</li>
<li>BATåœ¨è„“æ¯’ç—‡é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æ­»äº¡ç‡åˆ†ç±»æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>BATæ¨¡å‹å¯¹ç¼ºå¤±æ•°æ®çš„ç¨³å¥æ€§è¾ƒé«˜ï¼Œå¹¶èƒ½å¤Ÿå­¦ä¹ ç‹¬ç‰¹çš„ä¼ æ„Ÿå™¨åµŒå…¥ï¼Œé€‚ç”¨äºè¿ç§»å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-babb6bf67ecb480e214d5623031dadd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2555e35d5a4ecc851d99218c7af5d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d9e5654b8f5e14dbca07b1638b93826.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Region-Level-Context-Aware-Multimodal-Understanding"><a href="#Region-Level-Context-Aware-Multimodal-Understanding" class="headerlink" title="Region-Level Context-Aware Multimodal Understanding"></a>Region-Level Context-Aware Multimodal Understanding</h2><p><strong>Authors:Hongliang Wei, Xianqi Zhang, Xingtao Wang, Xiaopeng Fan, Debin Zhao</strong></p>
<p>Despite significant progress, existing research on Multimodal Large Language Models (MLLMs) mainly focuses on general visual understanding, overlooking the ability to integrate textual context associated with objects for a more context-aware multimodal understanding â€“ an ability we refer to as Region-level Context-aware Multimodal Understanding (RCMU). To address this limitation, we first formulate the RCMU task, which requires models to respond to user instructions by integrating both image content and textual information of regions or objects. To equip MLLMs with RCMU capabilities, we propose Region-level Context-aware Visual Instruction Tuning (RCVIT), which incorporates object information into the model input and enables the model to utilize bounding box coordinates to effectively associate objectsâ€™ visual content with their textual information. To address the lack of datasets, we introduce the RCMU dataset, a large-scale visual instruction tuning dataset that covers multiple RCMU tasks. We also propose RC&amp;P-Bench, a comprehensive benchmark that can evaluate the performance of MLLMs in RCMU and multimodal personalized understanding tasks. Additionally, we propose a reference-free evaluation metric to perform a comprehensive and fine-grained evaluation of the region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental results indicate that RC-Qwen2-VL models not only achieve outstanding performance on multiple RCMU tasks but also demonstrate successful applications in multimodal RAG and personalized conversation. Our data, model and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/hongliang-wei/RC-MLLM">https://github.com/hongliang-wei/RC-MLLM</a> </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…³äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸€èˆ¬çš„è§†è§‰ç†è§£ä¸Šï¼Œå¿½ç•¥äº†æ•´åˆä¸å¯¹è±¡ç›¸å…³çš„æ–‡æœ¬ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œä»¥å®ç°æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†çš„å¤šæ¨¡æ€ç†è§£â€”â€”æˆ‘ä»¬å°†è¿™ç§èƒ½åŠ›ç§°ä¸ºåŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ï¼ˆRCMUï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šäº†RCMUä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹é€šè¿‡æ•´åˆå›¾åƒå†…å®¹å’ŒåŒºåŸŸæˆ–å¯¹è±¡çš„æ–‡æœ¬ä¿¡æ¯æ¥å“åº”ç”¨æˆ·æŒ‡ä»¤ã€‚ä¸ºäº†èµ‹äºˆMLLMs RCMUèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆRCVITï¼‰ï¼Œå®ƒå°†å¯¹è±¡ä¿¡æ¯çº³å…¥æ¨¡å‹è¾“å…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¾¹ç•Œæ¡†åæ ‡æœ‰æ•ˆåœ°å°†å¯¹è±¡çš„è§†è§‰å†…å®¹ä¸æ–‡æœ¬ä¿¡æ¯å…³è”èµ·æ¥ã€‚ä¸ºäº†è§£å†³æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RCMUæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªRCMUä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†RC&amp;P-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥è¯„ä¼°MLLMåœ¨RCMUå’Œå¤šæ¨¡æ€ä¸ªæ€§åŒ–ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥å¯¹åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒæè¿°è¿›è¡Œæ›´å…¨é¢ã€æ›´ç²¾ç»†çš„è¯„ä¼°ã€‚é€šè¿‡å¯¹Qwen2-VLæ¨¡å‹è¿›è¡ŒRCVITè®­ç»ƒå¹¶ä½¿ç”¨RCMUæ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†RC-Qwen2-VLæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRC-Qwen2-VLæ¨¡å‹ä¸ä»…åœ¨å¤šä¸ªRCMUä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¤šæ¨¡æ€RAGå’Œä¸ªæ€§åŒ–å¯¹è¯ä¸­ä¹Ÿå–å¾—äº†æˆåŠŸçš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ•°æ®ã€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hongliang-wei/RC-MLLM%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/hongliang-wei/RC-MLLMä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12263v1">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç ”ç©¶è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰ç ”ç©¶çš„å±€é™æ€§ï¼Œä¸»è¦é›†ä¸­åœ¨ä¸€èˆ¬è§†è§‰ç†è§£ä¸Šï¼Œå¿½è§†äº†å°†å¯¹è±¡ä¸æ–‡æœ¬ä¸Šä¸‹æ–‡ç›¸ç»“åˆçš„èƒ½åŠ›ï¼Œå³åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ï¼ˆRCMUï¼‰ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆRCVITï¼‰çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†RCMUæ•°æ®é›†å’ŒRC&amp;P-BenchåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRC-Qwen2-VLæ¨¡å‹ä¸ä»…åœ¨å¤šä¸ªRCMUä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè€Œä¸”åœ¨å¤šæ¨¡æ€RAGå’Œä¸ªæ€§åŒ–å¯¹è¯ä¸­ä¹Ÿå–å¾—äº†æˆåŠŸåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ï¼ˆRCMUï¼‰æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>RCMUè¦æ±‚æ¨¡å‹åœ¨å“åº”ç”¨æˆ·æŒ‡ä»¤æ—¶ï¼Œç»“åˆå›¾åƒå†…å®¹å’ŒåŒºåŸŸæˆ–å¯¹è±¡çš„æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>RCVITæ–¹æ³•é€šè¿‡å°†å¯¹è±¡ä¿¡æ¯çº³å…¥æ¨¡å‹è¾“å…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¾¹ç•Œæ¡†åæ ‡æœ‰æ•ˆå…³è”å¯¹è±¡çš„è§†è§‰å†…å®¹ä¸æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥RCMUæ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªRCMUä»»åŠ¡çš„å¤§å‹è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚</li>
<li>æå‡ºRC&amp;P-BenchåŸºå‡†æµ‹è¯•ï¼Œå¯è¯„ä¼°MLLMsåœ¨RCMUå’Œå¤šæ¨¡æ€ä¸ªæ€§åŒ–ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒæè¿°è¿›è¡Œæ›´å…¨é¢ã€ç²¾ç»†çš„è¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRC-Qwen2-VLæ¨¡å‹åœ¨å¤šä¸ªRCMUä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå¹¶åœ¨å¤šæ¨¡æ€å’Œä¸ªæ€§åŒ–å¯¹è¯ä¸­æœ‰æˆåŠŸåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86a1f48f24c67a186fc254f121d39815.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a11c0772fc55ef95d0f1d1e4d0dede6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ebacfbfee8813340589acc052060ae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46dc5464d81a124f0f6edd238651e022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0a007adce0162027bb8a44ac0a5a7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12d85d66f60657ce48e74a92f33b14e6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learning-Wisdom-from-Errors-Promoting-LLMâ€™s-Continual-Relation-Learning-through-Exploiting-Error-Cases"><a href="#Learning-Wisdom-from-Errors-Promoting-LLMâ€™s-Continual-Relation-Learning-through-Exploiting-Error-Cases" class="headerlink" title="Learning Wisdom from Errors: Promoting LLMâ€™s Continual Relation Learning   through Exploiting Error Cases"></a>Learning Wisdom from Errors: Promoting LLMâ€™s Continual Relation Learning   through Exploiting Error Cases</h2><p><strong>Authors:Shaozhe Yin, Jinyu Guo, Kai Shuang, Xia Liu, Ruize Ou</strong></p>
<p>Continual Relation Extraction (CRE) aims to continually learn new emerging relations while avoiding catastrophic forgetting. Existing CRE methods mainly use memory replay and contrastive learning to mitigate catastrophic forgetting. However, these methods do not attach importance to the error cases that can reveal the modelâ€™s cognitive biases more effectively. To address this issue, we propose an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in CRE. Different from existing CRE methods that typically handle the training and memory data in a unified manner, this approach splits the training and memory data of each task into two parts respectively based on the correctness of the initial responses and treats them differently through dual-task fine-tuning. In addition, leveraging the advantages of LLMâ€™s instruction-following ability, we propose a novel instruction-based contrastive tuning strategy for LLM to continuously correct current cognitive biases with the guidance of previous data in an instruction-tuning manner, which mitigates the gap between old and new relations in a more suitable way for LLMs. We experimentally evaluate our model on TACRED and FewRel, and the results show that our model achieves new state-of-the-art CRE performance with significant improvements, demonstrating the importance of specializing in exploiting error cases. </p>
<blockquote>
<p>æŒç»­å…³ç³»æŠ½å–ï¼ˆCREï¼‰æ—¨åœ¨æŒç»­å­¦ä¹ æ–°å…´å…³ç³»ï¼ŒåŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰çš„CREæ–¹æ³•ä¸»è¦ä½¿ç”¨è®°å¿†å›æ”¾å’Œå¯¹æ¯”å­¦ä¹ æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶ä¸é‡è§†é”™è¯¯æ¡ˆä¾‹ï¼Œè¿™äº›æ¡ˆä¾‹èƒ½æ›´æœ‰æ•ˆåœ°æ­ç¤ºæ¨¡å‹çš„è®¤çŸ¥åè§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºæŒ‡ä»¤çš„æŒç»­å¯¹æ¯”è°ƒæ•´æ–¹æ³•ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„CREã€‚ä¸åŒäºç°æœ‰çš„CREæ–¹æ³•é€šå¸¸ç»Ÿä¸€å¤„ç†è®­ç»ƒå’Œè®°å¿†æ•°æ®çš„æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒå’Œè®°å¿†æ•°æ®æ ¹æ®åˆå§‹å“åº”çš„æ­£ç¡®æ€§åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡åŒä»»åŠ¡å¾®è°ƒçš„æ–¹å¼åŒºåˆ«å¯¹å¾…å®ƒä»¬ã€‚æ­¤å¤–ï¼Œå€ŸåŠ©LLMéµå¾ªæŒ‡ä»¤çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæŒ‡ä»¤çš„å¯¹æ¯”è°ƒæ•´ç­–ç•¥ï¼Œä½¿LLMèƒ½å¤Ÿåœ¨æŒ‡ä»¤è°ƒæ•´æ–¹å¼çš„æŒ‡å¯¼ä¸‹ï¼Œåˆ©ç”¨ä»¥å¾€æ•°æ®çš„æŒ‡å¯¼ä¸æ–­çº æ­£å½“å‰çš„è®¤çŸ¥åè§ï¼Œä»¥æ›´é€‚åˆLLMçš„æ–¹å¼ç¼©å°æ–°æ—§å…³ç³»ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬åœ¨TACREDå’ŒFewRelä¸Šå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº†å®éªŒè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„CREæ€§èƒ½ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¿™è¯æ˜äº†ä¸“æ³¨äºåˆ©ç”¨é”™è¯¯æ¡ˆä¾‹çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12031v1">PDF</a> </p>
<p><strong>Summary</strong><br>æŒç»­å…³ç³»æŠ½å–ï¼ˆCREï¼‰æ—¨åœ¨æŒç»­å­¦ä¹ æ–°çš„å…³ç³»ï¼ŒåŒæ—¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰CREæ–¹æ³•ä¸»è¦ä½¿ç”¨è®°å¿†å›æ”¾å’Œå¯¹æ¯”å­¦ä¹ æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¹¶ä¸é‡è§†é”™è¯¯æ¡ˆä¾‹ï¼Œè¿™äº›æ¡ˆä¾‹æ›´èƒ½æ­ç¤ºæ¨¡å‹çš„è®¤çŸ¥åè§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºæŒ‡ä»¤çš„æŒç»­å¯¹æ¯”è°ƒä¼˜æ–¹æ³•ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„CREã€‚ä¸åŒäºç°æœ‰çš„CREæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªä»»åŠ¡çš„è®­ç»ƒå’Œè®°å¿†æ•°æ®æ ¹æ®åˆå§‹å›ç­”çš„æ­£ç¡®æ€§è¿›è¡Œæ‹†åˆ†ï¼Œå¹¶é€šè¿‡åŒä»»åŠ¡å¾®è°ƒæ¥åŒºåˆ«å¯¹å¾…å®ƒä»¬ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæŒ‡ä»¤çš„å¯¹æ¯”è°ƒä¼˜ç­–ç•¥ï¼Œä»¥æŒ‡å¯¼LLMåœ¨æŒ‡ä»¤è°ƒä¼˜æ–¹å¼ä¸‹ä¸æ–­çº æ­£å½“å‰è®¤çŸ¥åè§ï¼Œè¿™æœ‰åŠ©äºç¼©å°æ–°æ—§å…³ç³»ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæ›´é€‚åˆLLMã€‚å®éªŒåœ¨TACREDå’ŒFewRelä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†æ–°çš„æœ€å…ˆè¿›çš„CREæ€§èƒ½ï¼Œå±•ç¤ºäº†ä¸“æ³¨äºåˆ©ç”¨é”™è¯¯æ¡ˆä¾‹çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CREçš„ç›®æ ‡æ˜¯æŒç»­å­¦ä¹ æ–°çš„å…³ç³»å¹¶é¿å…ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>ç°æœ‰CREæ–¹æ³•ä¸»è¦ä½¿ç”¨è®°å¿†å›æ”¾å’Œå¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>é”™è¯¯æ¡ˆä¾‹å¯¹äºæ­ç¤ºæ¨¡å‹çš„è®¤çŸ¥åè§æ›´æœ‰æ•ˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤çš„æŒç»­å¯¹æ¯”è°ƒä¼˜æ–¹æ³•ï¼Œç”¨äºLLMçš„CREã€‚</li>
<li>è¯¥æ–¹æ³•å°†è®­ç»ƒå’Œè®°å¿†æ•°æ®æ ¹æ®åˆå§‹å›åº”çš„æ­£ç¡®æ€§è¿›è¡Œæ‹†åˆ†å¤„ç†ã€‚</li>
<li>åˆ©ç”¨LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒä¼˜æ–¹å¼çº æ­£è®¤çŸ¥åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b20de22678633e47a5189c7f9cb60104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24f0e8d24e4290da4f934180c9a83a08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9041e0001394323cf1d62fac4d0c8436.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="In-Context-Examples-Matter-Improving-Emotion-Recognition-in-Conversation-with-Instruction-Tuning"><a href="#In-Context-Examples-Matter-Improving-Emotion-Recognition-in-Conversation-with-Instruction-Tuning" class="headerlink" title="In-Context Examples Matter: Improving Emotion Recognition in   Conversation with Instruction Tuning"></a>In-Context Examples Matter: Improving Emotion Recognition in   Conversation with Instruction Tuning</h2><p><strong>Authors:Hui Ma, Bo Zhang, Jinpeng Hu, Zenglin Shi</strong></p>
<p>Emotion recognition in conversation (ERC) aims to identify the emotion of each utterance in a conversation, playing a vital role in empathetic artificial intelligence. With the growing of large language models (LLMs), instruction tuning has emerged as a critical paradigm for ERC. Existing studies mainly focus on multi-stage instruction tuning, which first endows LLMs with speaker characteristics, and then conducts context-aware instruction tuning to comprehend emotional states. However, these methods inherently constrains the capacity to jointly capture the dynamic interaction between speaker characteristics and conversational context, resulting in weak alignment among speaker identity, contextual cues, and emotion states within a unified framework. In this paper, we propose InitERC, a simple yet effective one-stage in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn speaker-context-emotion alignment from context examples via in-context instruction tuning. Specifically, InitERC comprises four components, i.e., demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. To explore the impact of in-context examples, we conduct a comprehensive study on three key factors: retrieval strategy, example ordering, and the number of examples. Extensive experiments on three widely used datasets demonstrate that our proposed InitERC achieves substantial improvements over the state-of-the-art baselines. </p>
<blockquote>
<p>å¯¹è¯ä¸­çš„æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰æ—¨åœ¨è¯†åˆ«å¯¹è¯ä¸­æ¯ä¸ªå¥å­çš„æƒ…æ„Ÿï¼Œå¯¹äºå¯Œæœ‰åŒæƒ…å¿ƒçš„äººå·¥æ™ºèƒ½èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¢é•¿ï¼ŒæŒ‡ä»¤å¾®è°ƒå·²æˆä¸ºERCçš„å…³é”®èŒƒå¼ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤šé˜¶æ®µæŒ‡ä»¤å¾®è°ƒä¸Šï¼Œé¦–å…ˆèµ‹äºˆLLMè¯´è¯è€…ç‰¹å¾ï¼Œç„¶åè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æŒ‡ä»¤å¾®è°ƒä»¥ç†è§£æƒ…æ„ŸçŠ¶æ€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å›ºæœ‰åœ°é™åˆ¶äº†åŒæ—¶æ•æ‰è¯´è¯è€…ç‰¹å¾å’Œå¯¹è¯ä¸Šä¸‹æ–‡ä¹‹é—´åŠ¨æ€äº¤äº’çš„èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨ç»Ÿä¸€æ¡†æ¶å†…è¯´è¯è€…èº«ä»½ã€ä¸Šä¸‹æ–‡çº¿ç´¢å’Œæƒ…æ„ŸçŠ¶æ€ä¹‹é—´çš„å¯¹é½è¾ƒå¼±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†InitERCï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„ç”¨äºERCçš„ä¸€é˜¶æ®µä¸Šä¸‹æ–‡æŒ‡ä»¤å¾®è°ƒæ¡†æ¶ã€‚InitERCä½¿LLMèƒ½å¤Ÿé€‚åº”ä»ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ è¯´è¯è€…-ä¸Šä¸‹æ–‡-æƒ…æ„Ÿå¯¹é½çš„æŒ‡ä»¤å¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼ŒInitERCåŒ…æ‹¬å››ä¸ªç»„ä»¶ï¼Œå³æ¼”ç¤ºæ± æ„å»ºã€ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©ã€æç¤ºæ¨¡æ¿è®¾è®¡å’Œä¸Šä¸‹æ–‡æŒ‡ä»¤å¾®è°ƒã€‚ä¸ºäº†æ¢ç©¶ä¸Šä¸‹æ–‡å®ä¾‹çš„å½±å“ï¼Œæˆ‘ä»¬å¯¹ä¸‰ä¸ªå…³é”®å› ç´ è¿›è¡Œäº†ç»¼åˆç ”ç©¶ï¼šæ£€ç´¢ç­–ç•¥ã€ç¤ºä¾‹æ’åºå’Œç¤ºä¾‹æ•°é‡ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„InitERCåœ¨æœ€æ–°åŸºçº¿æŠ€æœ¯ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11889v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¯¹è¯ä¸­çš„æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰æ—¨åœ¨è¯†åˆ«æ¯ä¸ªå‘è¨€çš„æƒ…æ„Ÿï¼Œå¯¹äºå¯Œæœ‰åŒæƒ…å¿ƒçš„äººå·¥æ™ºèƒ½èµ·åˆ°å…³é”®ä½œç”¨ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼ŒæŒ‡ä»¤å¾®è°ƒæˆä¸ºERCçš„å…³é”®èŒƒå¼ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤šé˜¶æ®µæŒ‡ä»¤å¾®è°ƒä¸Šï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨æ— æ³•è”åˆæ•æ‰è¯´è¯äººç‰¹æ€§å’Œå¯¹è¯ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºInitERCï¼Œä¸€ä¸ªç®€å•æœ‰æ•ˆçš„ä¸€é˜¶æ®µä¸Šä¸‹æ–‡æŒ‡ä»¤å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ERCé—®é¢˜ã€‚InitERCé€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå­¦ä¹ è¯´è¯äºº-ä¸Šä¸‹æ–‡-æƒ…æ„Ÿçš„åŒ¹é…ã€‚é€šè¿‡å…¨é¢ç ”ç©¶ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„å½±å“ï¼Œå®éªŒç»“æœè¡¨æ˜InitERCåœ¨ä¸‰ä¸ªå¸¸ç”¨æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ERCåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ—¨åœ¨è¯†åˆ«å¯¹è¯ä¸­æ¯ä¸ªå‘è¨€çš„æƒ…æ„Ÿã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†ERCçš„è¿›æ­¥ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡å¤šé˜¶æ®µæŒ‡ä»¤å¾®è°ƒæ¥å¤„ç†ERCï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>InitERCæ¡†æ¶æ—¨åœ¨è§£å†³ERCé—®é¢˜ï¼Œé‡‡ç”¨ä¸€é˜¶æ®µä¸Šä¸‹æ–‡æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>InitERCé€šè¿‡ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå­¦ä¹ è¯´è¯äºº-ä¸Šä¸‹æ–‡-æƒ…æ„Ÿçš„åŒ¹é…ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¤ºä¾‹çš„é€‰æ‹©å¯¹InitERCæ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œç ”ç©¶ä¸­è€ƒè™‘äº†æ£€ç´¢ç­–ç•¥ã€ç¤ºä¾‹æ’åºå’Œç¤ºä¾‹æ•°é‡ä¸‰ä¸ªå…³é”®å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-671002ce9dc0dcfd5e26e5ce5a42c2c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afea20081b38feac505681f140a22cce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be03d6fd154eeb0c766ca0e92e8ad679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04522237e6cb69a31b598cc02dde70c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5624ea18d26a5cae9ba3bf7f0abab48c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f74b43bf9430070e2e0f08348a929d4d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Ovis2-5-Technical-Report"><a href="#Ovis2-5-Technical-Report" class="headerlink" title="Ovis2.5 Technical Report"></a>Ovis2.5 Technical Report</h2><p><strong>Authors:Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, Yibo Wang, Hui Sun, Jinlong Huang, Yuping He, Shengze Shi, Weihong Zhang, Guodong Zheng, Junpeng Jiang, Sensen Gao, Yi-Feng Wu, Sijia Chen, Yuhui Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout â€“ crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection â€“ including self-checking and revision. This advanced capability is exposed as an optional â€œthinking modeâ€ at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the â€œsmall model, big performanceâ€ philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Ovis2.5ï¼Œå®ƒæ˜¯Ovis2çš„å‡çº§ç‰ˆï¼Œä¸“ä¸ºåŸç”Ÿåˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥å’Œå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†è€Œè®¾è®¡ã€‚Ovis2.5é›†æˆäº†ä¸€ä¸ªåŸç”Ÿåˆ†è¾¨ç‡è§†è§‰è½¬æ¢å™¨ï¼Œè¯¥è½¬æ¢å™¨ä»¥å›¾åƒçš„åŸå§‹å¯å˜åˆ†è¾¨ç‡å¤„ç†å›¾åƒï¼Œé¿å…äº†å›ºå®šåˆ†è¾¨ç‡åˆ†å—æ‰€å¯¼è‡´çš„ç”»è´¨é™ä½ï¼ŒåŒæ—¶ä¿ç•™äº†ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å¸ƒå±€ï¼Œè¿™å¯¹äºå¤æ‚å›¾è¡¨ç­‰è§†è§‰å¯†é›†å†…å®¹è‡³å…³é‡è¦ã€‚ä¸ºäº†åŠ å¼ºæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹è¶…è¶Šçº¿æ€§æ€ç»´é“¾ï¼Œè¿›è¡Œåæ€ï¼ŒåŒ…æ‹¬è‡ªæˆ‘æ£€æŸ¥å’Œä¿®è®¢ã€‚è¿™ç§é«˜çº§åŠŸèƒ½åœ¨æ¨ç†æ—¶ä»¥å¯é€‰çš„â€œæ€è€ƒæ¨¡å¼â€å‘ˆç°ï¼Œå…è®¸ç”¨æˆ·å»¶è¿Ÿä»¥è·å¾—å¯¹å›°éš¾è¾“å…¥çš„å¢å¼ºå‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡åŒ…å«äº”ä¸ªé˜¶æ®µçš„ç»¼åˆè¯¾ç¨‹è¿›è¡Œè®­ç»ƒï¼Œä»¥é€æ­¥å»ºç«‹å…¶æŠ€èƒ½ã€‚è¿™ä¸ªè¿‡ç¨‹ä»åŸºæœ¬çš„è§†è§‰å’Œå¤šæ¨¡æ€é¢„è®­ç»ƒå¼€å§‹ï¼Œé€šè¿‡å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´å–å¾—è¿›å±•ï¼Œæœ€ç»ˆä½¿ç”¨DPOå’ŒGRPOè¿›è¡Œå¯¹é½å’Œæ¨ç†å¢å¼ºã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ‰©å±•è¿™äº›å‡çº§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ¨¡æ€æ•°æ®æ‰“åŒ…å’Œæ··åˆå¹¶è¡Œæ€§ï¼Œä»è€Œå®ç°äº†ç«¯åˆ°ç«¯çš„æ˜¾è‘—åŠ é€Ÿã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªå¼€æºæ¨¡å‹ï¼šOvis2.5-9Bå’ŒOvis2.5-2Bã€‚åè€…ç»§ç»­ç§‰æ‰¿Ovis2çš„â€œå°æ¨¡å‹ã€å¤§æ€§èƒ½â€ç†å¿µï¼Œä½¿å…¶æˆä¸ºèµ„æºå—é™ã€è®¾å¤‡ç«¯çš„ç†æƒ³é€‰æ‹©ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ’è¡Œæ¦œä¸Šï¼ŒOvis2.5-9Bçš„å¹³å‡åˆ†ä¸º78.3ï¼Œç›¸è¾ƒäºå…¶å‰èº«Ovis2-8Bæœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ï¼Œå¹¶åœ¨å¼€æºMLLMsä¸­è¾¾åˆ°äº†æ¬¡40Bå‚æ•°èŒƒå›´å†…çš„é¢†å…ˆæ°´å¹³ï¼›Ovis2.5-2Bå¾—åˆ†ä¸º73.9ï¼Œåœ¨å…¶è§„æ¨¡å†…å»ºç«‹äº†å¼€æºæœ€ä½³çºªå½•ã€‚é™¤äº†æ€»ä½“å¾—åˆ†å¤–ï¼ŒOvis2.5åœ¨STEMåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œåœ¨æ¥åœ°å’Œè§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¤æ‚å›¾è¡¨åˆ†ææ–¹é¢è¾¾åˆ°äº†å…¶è§„æ¨¡çš„å¼€æºæœ€ä½³æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11737v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>Ovis2.5æ˜¯Ovis2çš„å‡çº§ç‰ˆï¼Œå…·æœ‰åŸç”Ÿåˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥å’Œå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡åŸç”Ÿåˆ†è¾¨ç‡è§†è§‰è½¬æ¢å™¨å¤„ç†å›¾åƒï¼Œé¿å…å›ºå®šåˆ†è¾¨ç‡åˆ†å—å¸¦æ¥çš„é™çº§é—®é¢˜ï¼Œå¹¶ä¿ç•™ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å¸ƒå±€ã€‚ä¸ºåŠ å¼ºæ¨ç†èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹æ¥å—äº†è®­ç»ƒï¼Œä»¥è¶…è¶Šçº¿æ€§æ€ç»´é“¾å¹¶è¿›è¡Œåæ€ï¼ŒåŒ…æ‹¬è‡ªæ£€å’Œä¿®è®¢ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†å¯é€‰çš„â€œæ€è€ƒæ¨¡å¼â€ï¼Œå¯åœ¨æ¨ç†æ—¶å…è®¸ç”¨æˆ·å»¶è¿Ÿä»¥è·å¾—å¢å¼ºçš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº”é˜¶æ®µç»¼åˆè¯¾ç¨‹è¿›è¡Œè®­ç»ƒï¼Œé€æ­¥å»ºç«‹æŠ€èƒ½ã€‚å®ƒè¿˜åŒ…æ‹¬å¤šæ¨¡æ€æ•°æ®æ‰“åŒ…å’Œæ··åˆå¹¶è¡Œæ€§ï¼Œä»¥æœ‰æ•ˆæ‰©å±•å‡çº§ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æ˜¾è‘—åŠ é€Ÿã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªå¼€æºæ¨¡å‹ï¼šOvis2.5-9Bå’ŒOvis2.5-2Bã€‚åè€…ç»§ç»­ç§‰æ‰¿Ovis2çš„â€œå°å‹æ¨¡å‹ï¼Œé«˜æ€§èƒ½â€ç†å¿µï¼Œé€‚åˆèµ„æºå—é™çš„è®¾å¤‡å’Œåœºæ™¯ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ’è¡Œæ¦œä¸Šï¼ŒOvis2.5-9Bçš„å¹³å‡åˆ†ä¸º78.3ï¼Œè¾ƒå…¶å‰èº«Ovis2-8Bæœ‰äº†æ˜¾è‘—æé«˜ï¼Œå¹¶åœ¨å¼€æºMLLMsçš„å­40Bå‚æ•°èŒƒå›´å†…è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼›Ovis2.5-2Bå¾—åˆ†ä¸º73.9ï¼Œåœ¨å…¶è§„æ¨¡ä¸Šå»ºç«‹äº†æœ€æ–°æ°´å¹³ã€‚é™¤äº†æ€»ä½“å¾—åˆ†å¤–ï¼ŒOvis2.5è¿˜åœ¨STEMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é¢†å…ˆåœ°ä½ï¼Œå¹¶åœ¨æ¥åœ°å’Œè§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å¤æ‚å›¾è¡¨åˆ†ææ–¹é¢è¾¾åˆ°äº†å¼€æºçš„æœ€æ–°æ°´å¹³ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ovis2.5æ˜¯Ovis2çš„å‡çº§ç‰ˆï¼Œå…·æœ‰åŸç”Ÿåˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œèƒ½å¤„ç†å›¾åƒçš„ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å¸ƒå±€ã€‚</li>
<li>Ovis2.5æ‹¥æœ‰å¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¶…è¶Šçº¿æ€§æ€ç»´é“¾è¿›è¡Œåæ€ï¼ŒåŒ…æ‹¬è‡ªæ£€å’Œä¿®è®¢ã€‚</li>
<li>Ovis2.5æä¾›äº†å¯é€‰çš„â€œæ€è€ƒæ¨¡å¼â€ï¼Œå¯åœ¨æ¨ç†æ—¶å¹³è¡¡å»¶è¿Ÿå’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨äº”é˜¶æ®µç»¼åˆè¯¾ç¨‹è¿›è¡Œè®­ç»ƒï¼Œé€æ­¥å»ºç«‹æŠ€èƒ½ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€æ•°æ®æ‰“åŒ…å’Œæ··åˆå¹¶è¡Œæ€§ï¼ŒOvis2.5å®ç°äº†é«˜æ•ˆçš„å‡çº§å’Œç«¯åˆ°ç«¯çš„æ˜¾è‘—åŠ é€Ÿã€‚</li>
<li>Ovis2.5åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬STEMåŸºå‡†æµ‹è¯•ã€æ¥åœ°ä»»åŠ¡ã€è§†é¢‘ä»»åŠ¡å’Œå¤æ‚å›¾è¡¨åˆ†æç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d878db4ab62bbb2a770e756fbff75be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85ba831cc6386aff7392e72fc9defe33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda7bb4aaea9fbd68ee9af5bbf7af400.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de7f3e024ee70fcbce51b4e1b72063fb.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TADT-CSA-Temporal-Advantage-Decision-Transformer-with-Contrastive-State-Abstraction-for-Generative-Recommendation"><a href="#TADT-CSA-Temporal-Advantage-Decision-Transformer-with-Contrastive-State-Abstraction-for-Generative-Recommendation" class="headerlink" title="TADT-CSA: Temporal Advantage Decision Transformer with Contrastive State   Abstraction for Generative Recommendation"></a>TADT-CSA: Temporal Advantage Decision Transformer with Contrastive State   Abstraction for Generative Recommendation</h2><p><strong>Authors:Xiang Gao, Tianyuan Liu, Yisha Li, Jingxin Liu, Lexi Gao, Xin Li, Haiyang Lu, Liyin Hong</strong></p>
<p>With the rapid advancement of Transformer-based Large Language Models (LLMs), generative recommendation has shown great potential in enhancing both the accuracy and semantic understanding of modern recommender systems. Compared to LLMs, the Decision Transformer (DT) is a lightweight generative model applied to sequential recommendation tasks. However, DT faces challenges in trajectory stitching, often producing suboptimal trajectories. Moreover, due to the high dimensionality of user states and the vast state space inherent in recommendation scenarios, DT can incur significant computational costs and struggle to learn effective state representations. To overcome these issues, we propose a novel Temporal Advantage Decision Transformer with Contrastive State Abstraction (TADT-CSA) model. Specifically, we combine the conventional Return-To-Go (RTG) signal with a novel temporal advantage (TA) signal that encourages the model to capture both long-term returns and their sequential trend. Furthermore, we integrate a contrastive state abstraction module into the DT framework to learn more effective and expressive state representations. Within this module, we introduce a TA-conditioned State Vector Quantization (TAC-SVQ) strategy, where the TA score guides the state codebooks to incorporate contextual token information. Additionally, a reward prediction network and a contrastive transition prediction (CTP) network are employed to ensure the state codebook preserves both the reward information of the current state and the transition information between adjacent states. Empirical results on both public datasets and an online recommendation system demonstrate the effectiveness of the TADT-CSA model and its superiority over baseline methods. </p>
<blockquote>
<p>éšç€åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆå¼æ¨èåœ¨æé«˜ç°ä»£æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸LLMç›¸æ¯”ï¼Œå†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰æ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ¨èä»»åŠ¡çš„è½»é‡çº§ç”Ÿæˆæ¨¡å‹ã€‚ç„¶è€Œï¼ŒDTåœ¨è½¨è¿¹æ‹¼æ¥æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç»å¸¸äº§ç”Ÿæ¬¡ä¼˜è½¨è¿¹ã€‚æ­¤å¤–ï¼Œç”±äºç”¨æˆ·çŠ¶æ€çš„é«˜ç»´æ€§å’Œæ¨èåœºæ™¯ä¸­çš„å›ºæœ‰çŠ¶æ€ç©ºé—´çš„å¹¿æ³›æ€§ï¼ŒDTå¯èƒ½ä¼šäº§ç”Ÿæ˜¾è‘—çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”éš¾ä»¥å­¦ä¹ æœ‰æ•ˆçš„çŠ¶æ€è¡¨ç¤ºã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹çš„å¸¦æœ‰å¯¹æ¯”çŠ¶æ€æŠ½è±¡çš„æ—¶é—´ä¼˜åŠ¿å†³ç­–å˜å‹å™¨ï¼ˆTADT-CSAï¼‰æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„è¿”å›ç›®æ ‡ï¼ˆRTGï¼‰ä¿¡å·ä¸æ–°å‹çš„æ—¶é—´ä¼˜åŠ¿ï¼ˆTAï¼‰ä¿¡å·ç›¸ç»“åˆï¼Œé¼“åŠ±æ¨¡å‹æ•æ‰é•¿æœŸå›æŠ¥åŠå…¶åºåˆ—è¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å—é›†æˆåˆ°DTæ¡†æ¶ä¸­ï¼Œå­¦ä¹ æ›´æœ‰æ•ˆå’Œæ›´å…·è¡¨ç°åŠ›çš„çŠ¶æ€è¡¨ç¤ºã€‚åœ¨è¯¥æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶é—´ä¼˜åŠ¿æ¡ä»¶çŠ¶æ€å‘é‡é‡åŒ–ï¼ˆTAC-SVQï¼‰ç­–ç•¥ï¼Œå…¶ä¸­æ—¶é—´ä¼˜åŠ¿è¯„åˆ†æŒ‡å¯¼çŠ¶æ€ç æœ¬èå…¥ä¸Šä¸‹æ–‡æ ‡è®°ä¿¡æ¯ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å¥–åŠ±é¢„æµ‹ç½‘ç»œå’Œå¯¹æ¯”è¿‡æ¸¡é¢„æµ‹ï¼ˆCTPï¼‰ç½‘ç»œï¼Œä»¥ç¡®ä¿çŠ¶æ€ç æœ¬ä¿ç•™å½“å‰çŠ¶æ€çš„å¥–åŠ±ä¿¡æ¯å’Œç›¸é‚»çŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡ä¿¡æ¯ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œåœ¨çº¿æ¨èç³»ç»Ÿä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒTADT-CSAæ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶å¯¹åŸºå‡†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿè¿›æ­¥ä¸ºæ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£å¸¦æ¥äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”¨äºåºåˆ—æ¨èä»»åŠ¡çš„å†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰é¢ä¸´è½¨è¿¹æ‹¼æ¥çš„æŒ‘æˆ˜ï¼Œå¯èƒ½äº§ç”Ÿæ¬¡ä¼˜è½¨è¿¹ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜å¹¶æå‡æ¨¡å‹æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆä¼ ç»Ÿå›å½’ç›®æ ‡ä¿¡å·ä¸æ–°å‹æ—¶åºä¼˜åŠ¿ä¿¡å·çš„æ—¶ç©ºä¼˜åŠ¿å†³ç­–å˜å‹å™¨ç»“åˆå¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å‹ï¼ˆTADT-CSAï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å—å’Œæ—¶åºä¼˜åŠ¿æ¡ä»¶çŠ¶æ€å‘é‡é‡åŒ–ç­–ç•¥ï¼Œå­¦ä¹ æ›´æœ‰æ•ˆã€æ›´å…·è¡¨ç°åŠ›çš„çŠ¶æ€è¡¨ç¤ºï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œåœ¨çº¿æ¨èç³»ç»Ÿä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒTADT-CSAæ¨¡å‹ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ¨åŠ¨äº†ç”Ÿæˆå¼æ¨èç³»ç»Ÿçš„å‘å±•ï¼Œæé«˜äº†æ¨èå‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£ã€‚</li>
<li>DTåœ¨è½¨è¿¹æ‹¼æ¥æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜è½¨è¿¹ç”Ÿæˆã€‚</li>
<li>TADT-CSAæ¨¡å‹é€šè¿‡ç»“åˆRTGä¿¡å·ä¸æ–°å‹æ—¶åºä¼˜åŠ¿ä¿¡å·åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>å¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å—å¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´æœ‰æ•ˆçš„çŠ¶æ€è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†TAC-SVQç­–ç•¥æ¥èåˆæ—¶åºä¼˜åŠ¿ä¿¡æ¯ä¸çŠ¶æ€ç æœ¬ã€‚</li>
<li>å¥–åŠ±é¢„æµ‹ç½‘ç»œå’Œå¯¹æ¯”è¿‡æ¸¡é¢„æµ‹ç½‘ç»œç¡®ä¿çŠ¶æ€ç æœ¬ä¿å­˜å½“å‰çŠ¶æ€çš„å¥–åŠ±ä¿¡æ¯å’Œç›¸é‚»çŠ¶æ€é—´çš„è¿‡æ¸¡ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e26025cebd4e8fe0c21107218e25b91c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04acdcb71131c1216db48ec81a6dd9bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4867d1a3d3304db2bb77ba527fa015dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-831b9ae5321420d74c32092d3cfdd907.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="NoCode-bench-A-Benchmark-for-Evaluating-Natural-Language-Driven-Feature-Addition"><a href="#NoCode-bench-A-Benchmark-for-Evaluating-Natural-Language-Driven-Feature-Addition" class="headerlink" title="NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature   Addition"></a>NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature   Addition</h2><p><strong>Authors:Le Deng, Zhonghao Jiang, Jialun Cao, Michael Pradel, Zhongxin Liu</strong></p>
<p>Natural language-driven no-code development allows users to specify software functionality using natural language (NL) instead of editing source code, promising increased productivity and democratized development. Large language models (LLMs) show potential in enabling this paradigm. In this context, software documentation acts as an NL specification for functionality. This work introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world NL-driven feature addition tasks, consisting of 634 tasks across 10 projects and 114k code changes. Each task pairs documentation updates with corresponding code implementations, validated by developer-written test cases. A subset of 114 high-quality, human-verified instances, NoCode-bench Verified, ensures reliable evaluation. Our experiments reveal that, despite high token usage, the best LLMs achieve a task success rate of only 28.07%, highlighting challenges in cross-file editing, codebase understanding, and tool calling. These findings indicate that LLMs are not yet ready for fully NL-driven no-code development. NoCode-bench lays the foundation for future advances in this area. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘å…è®¸ç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰æ¥æŒ‡å®šè½¯ä»¶åŠŸèƒ½ï¼Œè€Œä¸æ˜¯ç¼–è¾‘æºä»£ç ï¼Œä»è€Œæé«˜äº†ç”Ÿäº§åŠ›å¹¶å®ç°äº†å¼€å‘çš„æ°‘ä¸»åŒ–ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™ç§æ¨¡å¼ä¸‹æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè½¯ä»¶æ–‡æ¡£ä½œä¸ºåŠŸèƒ½æ€§çš„è‡ªç„¶è¯­è¨€è§„èŒƒã€‚æœ¬æ–‡ä»‹ç»äº†NoCode-benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMåœ¨ç°å®ä¸–ç•Œçš„è‡ªç„¶è¯­è¨€é©±åŠ¨åŠŸèƒ½æ·»åŠ ä»»åŠ¡ä¸­çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«10ä¸ªé¡¹ç›®çš„634ä¸ªä»»åŠ¡å’Œ11ä¸‡å¤šæ¬¡ä»£ç æ›´æ”¹ã€‚æ¯ä¸ªä»»åŠ¡éƒ½å°†æ–‡æ¡£æ›´æ–°ä¸ç›¸åº”çš„ä»£ç å®ç°é…å¯¹ï¼Œå¹¶é€šè¿‡å¼€å‘äººå‘˜ç¼–å†™çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯ã€‚å…¶ä¸­ä¸€éƒ¨åˆ†ä¸ºç»è¿‡äººå·¥éªŒè¯çš„é«˜è´¨é‡å®ä¾‹NoCode-bench Verifiedï¼Œç¡®ä¿äº†è¯„ä¼°çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ä½¿ç”¨äº†å¤§é‡çš„ä»¤ç‰Œï¼Œä½†æœ€å¥½çš„LLMçš„ä»»åŠ¡æˆåŠŸç‡ä»…ä¸º28.07%ï¼Œè¿™çªæ˜¾äº†åœ¨è·¨æ–‡ä»¶ç¼–è¾‘ã€ä»£ç åº“ç†è§£å’Œå·¥å…·è°ƒç”¨æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒLLMå°šæœªå‡†å¤‡å¥½è¿›è¡Œå®Œå…¨çš„è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘ã€‚NoCode-benchä¸ºæœªæ¥çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18130v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘é€šè¿‡è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰è€Œéç¼–è¾‘æºä»£ç æ¥å®ç°è½¯ä»¶åŠŸèƒ½çš„æŒ‡å®šï¼Œæé«˜äº†ç”Ÿäº§åŠ›å¹¶å®ç°äº†å¼€å‘çš„æ°‘ä¸»åŒ–ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤é¢†åŸŸå…·æœ‰æ½œåŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œè½¯ä»¶æ–‡æ¡£ä½œä¸ºåŠŸèƒ½æ€§çš„è‡ªç„¶è¯­è¨€è§„èŒƒã€‚æœ¬ç ”ç©¶å¼•å…¥äº†NoCode-benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMsåœ¨ç°å®ä¸–ç•Œçš„è‡ªç„¶è¯­è¨€é©±åŠ¨åŠŸèƒ½æ·»åŠ ä»»åŠ¡ä¸Šçš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«634ä¸ªä»»åŠ¡ï¼Œæ¶µç›–10ä¸ªé¡¹ç›®å’Œ11ä¸‡å¤šæ¬¡ä»£ç æ›´æ”¹ã€‚æ¯ä¸ªä»»åŠ¡éƒ½å°†æ–‡æ¡£æ›´æ–°ä¸ç›¸åº”çš„ä»£ç å®ç°é…å¯¹ï¼Œå¹¶é€šè¿‡å¼€å‘äººå‘˜ç¼–å†™çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯ã€‚NoCode-bench Verifiedçš„é«˜è´¨é‡çš„å®ä¾‹ç¡®ä¿äº†å¯é çš„è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡LLMsä½¿ç”¨äº†å¤§é‡çš„ä»¤ç‰Œï¼Œä½†æœ€ä½³LLMsçš„ä»»åŠ¡æˆåŠŸç‡ä»…ä¸º28.07%ï¼Œè¿™çªæ˜¾å‡ºåœ¨è·¨æ–‡ä»¶ç¼–è¾‘ã€ä»£ç åº“ç†è§£å’Œå·¥å…·è°ƒç”¨æ–¹é¢çš„æŒ‘æˆ˜ã€‚è¿™è¡¨æ˜LLMså°šæœªå‡†å¤‡å¥½è¿›è¡Œå®Œå…¨çš„è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘ã€‚NoCode-benchä¸ºæœªæ¥çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘å¯æé«˜ç”Ÿäº§åŠ›å’Œå®ç°å¼€å‘æ°‘ä¸»åŒ–ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰è€Œéç¼–è¾‘æºä»£ç æ¥æŒ‡å®šè½¯ä»¶åŠŸèƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘é¢†åŸŸå…·æœ‰æ½œåŠ›ã€‚</li>
<li>NoCode-benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMsåœ¨ç°å®ä¸–ç•Œçš„è‡ªç„¶è¯­è¨€é©±åŠ¨åŠŸèƒ½æ·»åŠ ä»»åŠ¡ä¸Šçš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>NoCode-benchåŒ…å«634ä¸ªä»»åŠ¡ï¼Œæ¶µç›–å¤šä¸ªé¡¹ç›®å’Œå¤§é‡ä»£ç æ›´æ”¹ï¼Œä»¥è¯„ä¼°LLMsçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLLMsåœ¨è·¨æ–‡ä»¶ç¼–è¾‘ã€ä»£ç åº“ç†è§£å’Œå·¥å…·è°ƒç”¨æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰LLMså°šæœªå‡†å¤‡å¥½è¿›è¡Œå®Œå…¨çš„è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ— ä»£ç å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ade0c0574e615eeaac1d28b7cf0dec2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71266f93a6b203bf7e85e73f26f261ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f50518cb94d7379c37605a3754022f42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da327ae22555c8fc57db9e094816ed30.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Quantization-Hurts-Reasoning-An-Empirical-Study-on-Quantized-Reasoning-Models"><a href="#Quantization-Hurts-Reasoning-An-Empirical-Study-on-Quantized-Reasoning-Models" class="headerlink" title="Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models"></a>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models</h2><p><strong>Authors:Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou</strong></p>
<p>Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in <a target="_blank" rel="noopener" href="https://github.com/ruikangliu/Quantized-Reasoning-Models">https://github.com/ruikangliu/Quantized-Reasoning-Models</a>. </p>
<blockquote>
<p>è¿‘æœŸæ¨ç†è¯­è¨€æ¨¡å‹çš„è¿›å±•åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶æ‰©å±•çš„é“¾å¼æ¨ç†è¿‡ç¨‹å¢åŠ äº†æ¨ç†å¼€é”€ã€‚è™½ç„¶é‡åŒ–å·²å¹¿æ³›åº”ç”¨äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æˆæœ¬ï¼Œä½†å¯¹æ¨ç†æ¨¡å‹çš„å½±å“ä»ç ”ç©¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹é‡åŒ–æ¨ç†æ¨¡å‹è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ï¼Œè¯„ä¼°äº†å¼€æºçš„DeepSeek-R1-Distilled Qwenå’ŒLLaMAç³»åˆ—ï¼Œå‚æ•°èŒƒå›´ä»1.5Båˆ°70Bï¼ŒåŒ…æ‹¬Qwq-32Bå’ŒQwen3-8Bã€‚æˆ‘ä»¬çš„è°ƒæŸ¥æ¶µç›–äº†ä½¿ç”¨æœ€æ–°ç®—æ³•çš„æƒé‡ã€KVç¼“å­˜å’Œæ¿€æ´»é‡åŒ–ï¼Œå¹¶åœ¨æ•°å­¦ï¼ˆAIMEï¼ŒMATH-500ï¼‰ã€ç§‘å­¦ï¼ˆGPQAï¼‰å’Œç¼–ç¨‹ï¼ˆLiveCodeBenchï¼‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨W8A8æˆ–W4A16é‡åŒ–å¯ä»¥å®ç°æ— æŸé‡åŒ–ï¼Œä½†è¾ƒä½çš„ä½å®½ä¼šå¼•å…¥æ˜¾è‘—å‡†ç¡®æ€§é£é™©ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†æ¨¡å‹å¤§å°ã€æ¨¡å‹æ¥æºå’Œä»»åŠ¡éš¾åº¦æ˜¯æ€§èƒ½çš„å…³é”®å†³å®šå› ç´ ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œé‡åŒ–æ¨¡å‹çš„è¾“å‡ºé•¿åº¦å¹¶æ²¡æœ‰å¢åŠ ã€‚æ­¤å¤–ï¼Œæˆ˜ç•¥æ€§åœ°è°ƒæ•´æ¨¡å‹å¤§å°æˆ–æ¨ç†æ­¥éª¤å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ã€‚æ‰€æœ‰é‡åŒ–æ¨¡å‹å’Œä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/ruikangliu/Quantized-Reasoning-Models%E3%80%82">https://github.com/ruikangliu/Quantized-Reasoning-Modelsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04823v2">PDF</a> COLM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é‡åŒ–æ¨ç†æ¨¡å‹çš„æ•ˆæœå’Œæ€§èƒ½ã€‚å®éªŒæ¶µç›–äº†å¤šç§æ¨¡å‹å’Œç®—æ³•ï¼ŒåŒ…æ‹¬DeepSeek-R1-Distilled Qwenç³»åˆ—å’ŒLLaMAå®¶æ—ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½æµ‹è¯•è¯æ˜äº†é‡åŒ–çš„åˆç†æ€§å¯¹æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆæœå½±å“ä¸å¤§ï¼Œä½†å¯¹äºä½ä½å®½è®¾è®¡æœ‰ä¸€å®šé£é™©ã€‚æ¨¡å‹å¤§å°ã€æ¥æºå’Œä»»åŠ¡éš¾åº¦å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚é‡åŒ–æ¨¡å‹çš„è¾“å‡ºé•¿åº¦å¹¶æœªå¢åŠ ã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´æ¨¡å‹è§„æ¨¡æˆ–æ¨ç†æ­¥éª¤å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ã€‚æ¨¡å‹å’Œä»£ç å‡å·²å¼€æºåˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜å¯¹å¤šç§è¯­è¨€æ¨ç†æ¨¡å‹è¿›è¡Œäº†é‡åŒ–æµ‹è¯•ï¼Œè¿™äº›æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æµ‹è¯•åŒ…æ‹¬æƒé‡ã€KVç¼“å­˜å’Œæ¿€æ´»é‡åŒ–ï¼Œä½¿ç”¨å…ˆè¿›çš„ç®—æ³•åœ¨ä¸åŒä½å®½ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æµ‹è¯•èŒƒå›´æ¶µç›–äº†æ•°å­¦ã€ç§‘å­¦å’Œç¼–ç¨‹ç­‰å¤šä¸ªé¢†åŸŸçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶å‘ç°æ— æŸé‡åŒ–å¯ä»¥åœ¨W8A8æˆ–W4A16é‡åŒ–ä¸­å®ç°ï¼Œä½†ä½ä½å®½è®¾è®¡ä¼šå¸¦æ¥æ˜¾è‘—ç²¾åº¦é£é™©ã€‚</li>
<li>æ¨¡å‹å¤§å°ã€æ¥æºå’Œä»»åŠ¡éš¾åº¦å¯¹é‡åŒ–æ¨ç†æ¨¡å‹çš„æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>é‡åŒ–æ¨¡å‹çš„è¾“å‡ºé•¿åº¦å¹¶æœªå¦‚é¢„æœŸå¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b7195a06e32797f43ee51bd1abf26c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63fe6848e53b2b0de7760865188a8236.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection"><a href="#TeleAntiFraud-28k-An-Audio-Text-Slow-Thinking-Dataset-for-Telecom-Fraud-Detection" class="headerlink" title="TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection"></a>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud   Detection</h2><p><strong>Authors:Zhiming Ma, Peidong Wang, Minhua Huang, Jingpeng Wang, Kai Wu, Xiangzhao Lv, Yachun Pang, Yin Yang, Wenjie Tang, Yuchen Kang</strong></p>
<p>The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real&#x2F;synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at <a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud">https://github.com/JimmyMa99/TeleAntiFraud</a>. </p>
<blockquote>
<p>ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡å¼è®­ç»ƒæ•°æ®ï¼Œæ— æ³•å°†éŸ³é¢‘ä¿¡å·ä¸é¢å‘æ¨ç†çš„æ–‡æœ¬åˆ†æç›¸ç»“åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TeleAntiFraud-28kï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºç”µä¿¡æ¬ºè¯ˆè‡ªåŠ¨åŒ–åˆ†æè®¾è®¡çš„ç¬¬ä¸€ä¸ªå¼€æºéŸ³é¢‘æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†é€šè¿‡ä»¥ä¸‹ä¸‰ç§ç­–ç•¥æ„å»ºï¼šï¼ˆ1ï¼‰ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•çš„å½•éŸ³ç”Ÿæˆéšç§ä¿æŠ¤çš„æ–‡æœ¬çœŸå®æ ·æœ¬ï¼ˆå¸¦æœ‰åŒ¿ååŸå§‹éŸ³é¢‘ï¼‰ï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å†ç”Ÿç¡®ä¿ç°å®ä¸–ç•Œçš„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰é€šè¿‡åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªæˆ‘æŒ‡ä»¤é‡‡æ ·å¯¹çœŸå®çš„ASRè¾“å‡ºè¿›è¡Œè¯­ä¹‰å¢å¼ºï¼Œä»¥æ‰©å¤§åœºæ™¯è¦†ç›–ï¼›ï¼ˆ3ï¼‰æ¨¡æ‹Ÿæ–°å…´æ¬ºè¯ˆç­–ç•¥çš„å¤šä»£ç†å¯¹æŠ—åˆæˆé€šè¿‡é¢„è®¾çš„é€šä¿¡åœºæ™¯å’Œæ¬ºè¯ˆç±»å‹ã€‚ç”Ÿæˆçš„æ•°æ®é›†åŒ…å«ç»è¿‡ä¸¥æ ¼å¤„ç†çš„28511ä¸ªè¯­éŸ³æ–‡æœ¬å¯¹ï¼Œå¸¦æœ‰è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šã€‚æ•°æ®é›†åˆ†ä¸ºä¸‰ä¸ªä»»åŠ¡ï¼šåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹ã€æ¬ºè¯ˆç±»å‹åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†TeleAntiFraud-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«ä»æ•°æ®é›†ä¸­æŒ‰æ¯”ä¾‹é‡‡æ ·çš„å®ä¾‹ï¼Œä»¥ä¾¿äºå¯¹ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ä»»åŠ¡ä¸Šæ¨¡å‹æ€§èƒ½çš„ç³»ç»Ÿæ€§æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜ä¸ºæ··åˆç°å®&#x2F;åˆæˆæ•°æ®è®­ç»ƒçš„ç”Ÿäº§ä¼˜åŒ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¨¡å‹åšå‡ºäº†è´¡çŒ®ï¼ŒåŒæ—¶å¼€æºæ•°æ®å¤„ç†æ¡†æ¶ä»¥æ¨åŠ¨æ•°æ®é›†æ‰©å±•ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤šåª’ä½“åæ¬ºè¯ˆç ”ç©¶å»ºç«‹äº†åŸºç¡€æ¡†æ¶ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ã€‚è¯¥é¡¹ç›®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JimmyMa99/TeleAntiFraud%E4%B8%8A%E5%BC%95%E5%A4%BF%E3%80%82">https://github.com/JimmyMa99/TeleAntiFraudä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24115v4">PDF</a> </p>
<p><strong>Summary</strong><br>    é’ˆå¯¹ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹é¢†åŸŸé¢ä¸´çš„é«˜è´¨é‡å¤šæ¨¡å¼è®­ç»ƒæ•°æ®ç¼ºå¤±çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºTeleAntiFraud-28kï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºè‡ªåŠ¨åŒ–ç”µä¿¡æ¬ºè¯ˆåˆ†æè®¾è®¡çš„éŸ³é¢‘æ–‡æœ¬æ…¢æ€è€ƒæ•°æ®é›†ã€‚é€šè¿‡éšç§ä¿æŠ¤çš„çœŸå®æ–‡æœ¬ç”Ÿæˆã€è¯­ä¹‰å¢å¼ºå’Œå¤§è¯­è¨€æ¨¡å‹è‡ªæˆ‘æŒ‡å¯¼é‡‡æ ·ä»¥åŠæ¨¡æ‹Ÿæ–°å…´æ¬ºè¯ˆç­–ç•¥çš„å¤šæ™ºèƒ½ä½“å¯¹æŠ—åˆæˆä¸‰ç§ç­–ç•¥æ„å»ºæ•°æ®é›†ã€‚åŒ…å«28,511ä¸ªç»è¿‡ä¸¥æ ¼å¤„ç†çš„è¯­éŸ³æ–‡æœ¬å¯¹ï¼Œå…·æœ‰è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šï¼Œåˆ†ä¸ºåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹å’Œæ¬ºè¯ˆç±»å‹åˆ†ç±»ä¸‰ä¸ªä»»åŠ¡ã€‚åŒæ—¶æ„å»ºTeleAntiFraud-Benchæ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†ï¼Œå¹¶å¼€æºæ•°æ®å¤„ç†æ¡†æ¶ï¼Œä»¥æ¨åŠ¨æ•°æ®é›†æ‰©å±•ã€‚ä¸ºè·¨æ¨¡æ€åæ¬ºè¯ˆç ”ç©¶æä¾›åŸºç¡€æ¡†æ¶ï¼Œè§£å†³æ•°æ®éšç§å’Œåœºæ™¯å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TeleAntiFraud-28kæ˜¯é¦–ä¸ªä¸“ä¸ºç”µä¿¡æ¬ºè¯ˆåˆ†æè®¾è®¡çš„éŸ³é¢‘æ–‡æœ¬æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡éšç§ä¿æŠ¤çš„çœŸå®æ–‡æœ¬ç”Ÿæˆã€è¯­ä¹‰å¢å¼ºå’Œå¤šæ™ºèƒ½ä½“å¯¹æŠ—åˆæˆæ„å»ºã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¯¦ç»†çš„æ¬ºè¯ˆæ¨ç†æ³¨é‡Šï¼Œåˆ†ä¸ºåœºæ™¯åˆ†ç±»ã€æ¬ºè¯ˆæ£€æµ‹å’Œæ¬ºè¯ˆç±»å‹åˆ†ç±»ä¸‰ä¸ªä»»åŠ¡ã€‚</li>
<li>æ„å»ºäº†TeleAntiFraud-Benchæ ‡å‡†åŒ–è¯„ä¼°åŸºå‡†ä»¥æµ‹è¯•æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼€æºæ•°æ®å¤„ç†æ¡†æ¶ï¼Œä¿ƒè¿›æ•°æ®é›†æ‰©å±•å’Œç¤¾åŒºå‚ä¸ã€‚</li>
<li>è¯¥å·¥ä½œä¸ºè·¨æ¨¡æ€åæ¬ºè¯ˆç ”ç©¶æä¾›åŸºç¡€æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8d3b36609c9ab9a09876a9207f9b7bce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f7f634e6142e39dd5314cbbc507d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-253fa7d6973f26d24d2c81d519deac01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e8ff0522c821f7aed3553f25af23621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d40fdd3e5088aee9979809c6fd59961.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39efe517225eb10b12914a9414a2343d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70a53acc5fdca0b1a3370df29b3b8e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3a36244324f42384a7e1ed1581aedb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4440e8a7f2508cb3f2258cf2944964e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1cd588f7ea33dc92654d2f58f9f73c0e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-71e9fe80666be9123890181df165ef03.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  MuDRiC Multi-Dialect Reasoning for Arabic Commonsense Validation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
