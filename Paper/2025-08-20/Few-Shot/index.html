<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-20  MAGNeT Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1cd6df32adbbf757cd796674a163d818.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-20-更新"><a href="#2025-08-20-更新" class="headerlink" title="2025-08-20 更新"></a>2025-08-20 更新</h1><h2 id="MAGNeT-Multimodal-Adaptive-Gaussian-Networks-for-Intent-Inference-in-Moving-Target-Selection-across-Complex-Scenarios"><a href="#MAGNeT-Multimodal-Adaptive-Gaussian-Networks-for-Intent-Inference-in-Moving-Target-Selection-across-Complex-Scenarios" class="headerlink" title="MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios"></a>MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in   Moving Target Selection across Complex Scenarios</h2><p><strong>Authors:Xiangxian Li, Yawen Zheng, Baiqiao Zhang, Yijia Ma, XianhuiCao XianhuiCao, Juan Liu, Yulong Bian, Jin Huang, Chenglei Yang</strong></p>
<p>Moving target selection in multimedia interactive systems faces unprecedented challenges as users increasingly interact across diverse and dynamic contexts-from live streaming in moving vehicles to VR gaming in varying environments. Existing approaches rely on probabilistic models that relate endpoint distribution to target properties such as size and speed. However, these methods require substantial training data for each new context and lack transferability across scenarios, limiting their practical deployment in diverse multimedia environments where rich multimodal contextual information is readily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian Networks), which addresses these problems by combining classical statistical modeling with a context-aware multimodal method. MAGNeT dynamically fuses pre-fitted Ternary-Gaussian models from various scenarios based on real-time contextual cues, enabling effective adaptation with minimal training data while preserving model interpretability. We conduct experiments on self-constructed 2D and 3D moving target selection datasets under in-vehicle vibration conditions. Extensive experiments demonstrate that MAGNeT achieves lower error rates with few-shot samples by applying context-aware fusion of Gaussian experts from multi-factor conditions. </p>
<blockquote>
<p>多媒体交互系统中的动态目标选择面临着前所未有的挑战，因为用户在不同的动态上下文中的交互越来越频繁，从移动车辆中的直播到不同环境中的VR游戏。现有方法依赖于将端点分布与目标属性（如大小和速度）相关的概率模型。然而，这些方法需要针对每种新上下文的大量训练数据，并且在跨场景之间的迁移能力方面存在不足，从而限制了它们在丰富的多媒体环境中的实际部署，这些环境中丰富的多模态上下文信息很容易获得。本文介绍了MAGNeT（多模态自适应高斯网络），它通过结合经典统计建模和上下文感知的多模态方法来解决这些问题。MAGNeT根据实时上下文线索动态融合来自各种场景的预先配置的三元高斯模型，通过最小的训练数据实现有效的适应，同时保持模型的可解释性。我们在自行构建的二维和三维移动目标选择数据集上进行了车内振动条件下的实验。大量实验表明，MAGNeT通过应用多因素条件下高斯专家的上下文感知融合，在少量样本的情况下实现了更低的错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12992v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>多媒体交互系统中移动目标选择面临前所未有的挑战，特别是在用户在不同动态环境中进行交互时。现有方法依赖于概率模型，将端点分布与目标属性关联，如大小和速度。但它们在面对新的上下文时需要大量训练数据，并且缺乏跨场景的迁移能力，限制了它们在丰富多媒体环境中的实际应用，这些环境中存在丰富的多模态上下文信息。本文提出MAGNeT（多模态自适应高斯网络），结合经典统计建模和上下文感知的多模态方法来解决这些问题。MAGNeT根据实时上下文线索动态融合各种场景的预拟合三元高斯模型，在有限的训练数据基础上实现有效适应，同时保持模型的可解释性。通过实验验证了MAGNeT在低样本情况下实现较低错误率的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多媒体交互系统中移动目标选择面临挑战，尤其是在动态环境中。</li>
<li>现有方法依赖于需要大量训练数据的概率模型，缺乏跨场景迁移能力。</li>
<li>MAGNeT结合了经典统计建模和上下文感知的多模态方法。</li>
<li>MAGNeT根据实时上下文线索动态融合预拟合的三元高斯模型。</li>
<li>MAGNeT在有限的训练数据上实现了有效适应，同时保持了模型的可解释性。</li>
<li>实验表明，MAGNeT在低样本情况下能够实现较低的错误率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12992">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-de36d0ae779c58364daa3cdfd3bae784.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-480c5c368a498f6117db7d7ee05573dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-776c785c045799ea8cf882f11466556d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ddd83def8b8af37b56935c69766ff23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deba86eab4f3a5091d01aca9bf1113eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-099e3f2ea114f18b2df3b57e3e684988.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cross-Domain-Few-Shot-Learning-via-Multi-View-Collaborative-Optimization-with-Vision-Language-Models"><a href="#Cross-Domain-Few-Shot-Learning-via-Multi-View-Collaborative-Optimization-with-Vision-Language-Models" class="headerlink" title="Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization   with Vision-Language Models"></a>Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization   with Vision-Language Models</h2><p><strong>Authors:Dexia Chen, Wentao Zhang, Qianjie Zhu, Ping Hu, Weibing Li, Tong Zhang, Ruixuan Wang</strong></p>
<p>Vision-language models (VLMs) pre-trained on natural image and language data, such as CLIP, have exhibited significant potential in few-shot image recognition tasks, leading to development of various efficient transfer learning methods. These methods exploit inherent pre-learned knowledge in VLMs and have achieved strong performance on standard image datasets. However, their effectiveness is often limited when confronted with cross-domain tasks where imaging domains differ from natural images. To address this limitation, we propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a novel fine-tuning strategy for VLMs. This strategy employs two functionally complementary expert modules to extract multi-view features, while incorporating prior knowledge-based consistency constraints and information geometry-based consensus mechanisms to enhance the robustness of feature learning. Additionally, a new cross-domain few-shot benchmark is established to help comprehensively evaluate methods on imaging domains distinct from natural images. Extensive empirical evaluations on both existing and newly proposed benchmarks suggest CoMuCo consistently outperforms current methods in few-shot tasks. The code and benchmark will be released. </p>
<blockquote>
<p>预训练于自然图像和语言数据上的视觉语言模型（如CLIP）在少样本图像识别任务中展现出显著潜力，推动了各种高效的迁移学习方法的发展。这些方法利用视觉语言模型中的固有预学习知识，并在标准图像数据集上取得了强大的性能。然而，当面对成像域不同于自然图像的跨域任务时，其有效性往往受到限制。为了解决这一局限性，我们提出了基于一致性引导的多视角协同优化（CoMuCo），这是一种用于视觉语言模型的新型微调策略。该策略采用两个功能上互补的专家模块来提取多视角特征，同时结合基于先验知识的一致性约束和基于信息几何的共识机制，以提高特征学习的稳健性。此外，还建立了一个新的跨域少样本基准测试，以帮助全面评估在不同于自然图像的成像域上的方法。对现有基准测试和新提出的基准测试的大量实证评估表明，CoMuCo在少样本任务中始终优于当前方法。代码和基准测试将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12861v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练在天然图像和语言数据上的视觉语言模型（VLMs），如CLIP，在少量图像识别任务中展现出巨大潜力，并衍生出多种有效的迁移学习方法。这些少量样本学习方法利用VLMs中的预学习固有知识，在标准图像数据集上取得了强大的性能。然而，当面对成像域与天然图像不同的跨域任务时，其效果往往受限。为此，我们提出了名为“一致性导向多视角协同优化”（CoMuCo）的新微调策略。此策略利用两个功能上互补的专家模块提取多视角特征，并结合基于先验知识的一致性约束和基于信息几何的共识机制，以提高特征学习的稳健性。此外，我们还建立了一个新的跨域小样本基准测试，以全面评估在不同于自然图像的成像领域上的方法表现。对既有和新提出的基准测试进行的广泛实证评估表明，CoMuCo在少样本任务中的表现始终优于当前方法。代码和基准测试将公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在少量图像识别任务中具有显著潜力。</li>
<li>衍生出多种迁移学习方法，利用VLMs中的预学习固有知识。</li>
<li>跨域任务中VLMs性能受限，需要新方法应对成像域差异。</li>
<li>提出名为CoMuCo的新微调策略，利用专家模块和一致性约束提高特征学习稳健性。</li>
<li>建立新的跨域小样本基准测试，评估在不同于自然图像的成像领域上的方法表现。</li>
<li>CoMuCo在少样本任务中的表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12861">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a3c937eb468095f3ff7d7179329fadfd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebbc4302be0327d8ece8994c7f94f9e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6731882901ee184a9ddff9cae1585672.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65f4465fba590623952a69f06e8abafa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71aa425c54a0773b8f1e499e1383c58b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe9c6936b824f3058d00a6bd517f1e35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-372cf54664b9727310b20f39b5e2ec34.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="E3RG-Building-Explicit-Emotion-driven-Empathetic-Response-Generation-System-with-Multimodal-Large-Language-Model"><a href="#E3RG-Building-Explicit-Emotion-driven-Empathetic-Response-Generation-System-with-Multimodal-Large-Language-Model" class="headerlink" title="E3RG: Building Explicit Emotion-driven Empathetic Response Generation   System with Multimodal Large Language Model"></a>E3RG: Building Explicit Emotion-driven Empathetic Response Generation   System with Multimodal Large Language Model</h2><p><strong>Authors:Ronghao Lin, Shuai Shen, Weipeng Hu, Qiaolin He, Aolin Xiong, Li Huang, Haifeng Hu, Yap-peng Tan</strong></p>
<p>Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/RH-Lin/E3RG">https://github.com/RH-Lin/E3RG</a>. </p>
<blockquote>
<p>多模态共情响应生成（MERG）对于构建具有情感智能的人机交互至关重要。尽管大型语言模型（LLM）已经改进了基于文本的ERG，但在处理多模态情感内容和保持身份一致性方面仍存在挑战。因此，我们提出了基于多模态LLM的显式情感驱动共情响应生成系统E3RG，它将MERG任务分解为三部分：多模态共情理解、共情记忆检索和多模态响应生成。通过集成先进的表达性语音和视频生成模型，E3RG无需额外训练即可提供自然、情感丰富且符合身份一致的响应。实验验证了我们系统在零样本和少样本设置上的优越性，在ACM MM 25的基于Avatar的多模态共情挑战中获得了第一名。我们的代码可通过<a target="_blank" rel="noopener" href="https://github.com/RH-Lin/E3RG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RH-Lin/E3RG获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12854v1">PDF</a> Accepted at ACM MM 2025 Grand Challenge</p>
<p><strong>Summary</strong></p>
<p>多模态共情反应生成（MERG）对于构建具有情感智能的人机交互至关重要。尽管大型语言模型（LLM）已经提高了基于文本的ERG能力，但在处理多模态情感内容和保持身份一致性方面仍存在挑战。因此，我们提出了基于多模态LLM的显式情感驱动共情反应生成系统E3RG，它将MERG任务分解为三个部分：多模态共情理解、共情记忆检索和多模态反应生成。通过整合先进的表达性语音和视频生成模型，E3RG能够在无需额外训练的情况下产生自然、情感丰富且身份一致的反应。实验验证了我们系统在零样本和少样本设置上的优越性，并在ACM MM 25的基于Avatar的多模态共情挑战中获得第一名。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/RH-Lin/E3RG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RH-Lin/E3RG获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MERG（多模态共情反应生成）是构建情感智能人机交互的核心。</li>
<li>大型语言模型（LLM）在文本基础的ERG上已有显著提升，但在处理多模态情感内容和维持身份一致性上仍有挑战。</li>
<li>E3RG系统是一个显式情感驱动的共情反应生成系统，基于多模态LLM。</li>
<li>E3RG将MERG任务分为三个核心部分：多模态共情理解、共情记忆检索和多模态反应生成。</li>
<li>E3RG集成了先进的表达性语音和视频生成模型，以产生自然、情感丰富和身份一致的反应。</li>
<li>实验证明，E3RG在零样本和少样本设置上表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12854">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a1f17f289393e81b34cbd3f10f873c69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757f5e69586b72becf87424870d4378c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2970e89718c0b43017efc3c230fbd5b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c213bba2a3e899ca2900bf4cbc293e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-796596ae4b339997027f23c836ec5034.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ddff4c92d2008432ad863da9497ce45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe78d9955cb33e0fe60823a6e58e69be.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Embodied-Long-Horizon-Manipulation-with-Closed-loop-Code-Generation-and-Incremental-Few-shot-Adaptation"><a href="#Embodied-Long-Horizon-Manipulation-with-Closed-loop-Code-Generation-and-Incremental-Few-shot-Adaptation" class="headerlink" title="Embodied Long Horizon Manipulation with Closed-loop Code Generation and   Incremental Few-shot Adaptation"></a>Embodied Long Horizon Manipulation with Closed-loop Code Generation and   Incremental Few-shot Adaptation</h2><p><strong>Authors:Yuan Meng, Xiangtong Yao, Haihui Ye, Yirui Zhou, Shengqiang Zhang, Zhenguo Sun, Zhenshan Bing, Alois Knoll</strong></p>
<p>Embodied long-horizon manipulation requires robotic systems to process multimodal inputs-such as vision and natural language-and translate them into executable actions. However, existing learning-based approaches often depend on large, task-specific datasets and struggle to generalize to unseen scenarios. Recent methods have explored using large language models (LLMs) as high-level planners that decompose tasks into subtasks using natural language and guide pretrained low-level controllers. Yet, these approaches assume perfect execution from low-level policies, which is unrealistic in real-world environments with noise or suboptimal behaviors. To overcome this, we fully discard the pretrained low-level policy and instead use the LLM to directly generate executable code plans within a closed-loop framework. Our planner employs chain-of-thought (CoT)-guided few-shot learning with incrementally structured examples to produce robust and generalizable task plans. Complementing this, a reporter evaluates outcomes using RGB-D and delivers structured feedback, enabling recovery from misalignment and replanning under partial observability. This design eliminates per-step inference, reduces computational overhead, and limits error accumulation that was observed in previous methods. Our framework achieves state-of-the-art performance on 30+ diverse seen and unseen long-horizon tasks across LoHoRavens, CALVIN, Franka Kitchen, and cluttered real-world settings. </p>
<blockquote>
<p>具有长期视野的操作需要机器人系统处理多模式输入，如视觉和自然语言，并将其转化为可执行的行动。然而，现有的基于学习的方法通常依赖于大型、特定的任务数据集，在未见场景中的泛化能力较差。最近的方法已经尝试使用大型语言模型（LLM）作为高级规划器，利用自然语言将任务分解为子任务，并引导预训练的低级控制器。然而，这些方法假设低级策略的执行是完美的，这在充满噪声或次优行为的现实环境中是不现实的。为了克服这一问题，我们完全抛弃了预训练的低级策略，而是使用LLM直接在闭环框架内生成可执行代码计划。我们的规划器采用增量结构化示例引导的思维链（CoT）小样本学习，生成稳健且可泛化的任务计划。作为补充，一个报告者使用RGB-D评估结果并提供结构化反馈，实现在部分可观察性下的偏差恢复和重新规划。这种设计消除了逐步推理，减少了计算开销，并限制了之前在方法中观察到的误差累积。我们的框架在LoHoRavens、CALVIN、Frank Kitchen和杂乱的真实世界环境中实现了超过30种多样化和未见过的长期任务的最佳性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21969v2">PDF</a> update ICRA 6 page</p>
<p><strong>Summary</strong></p>
<p>本论文探讨了使用大型语言模型（LLMs）作为高层次的计划制定者来解决长视野操纵的问题。鉴于真实世界中的噪音或次优行为问题，论文提出了一种全新的方法，即利用LLM直接生成可执行代码计划，而不是依赖预先训练的低层次策略。论文通过思维链引导少数实例学习来制定稳健且通用的任务计划，并使用RGB-D评估结果，以实现偏差恢复和不完全可观测下的重新规划。该方法避免了分步推理的复杂性和误差累积的问题，实现了跨多个场景的长远任务的出色表现。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是该文本的关键见解要点：</p>
<ul>
<li>论文聚焦于使用大型语言模型（LLMs）作为机器人系统的任务规划器来解决长视野操纵问题。</li>
<li>针对真实世界中的噪音和次优行为问题，论文提出了一种新方法，即不使用预先训练的低层次策略，直接使用LLM生成可执行代码计划。</li>
<li>论文提出通过思维链（CoT）引导少数实例学习的方法，使得机器人能够在各种场景中稳健地制定任务计划。</li>
<li>RGB-D用于评估任务执行的结果，这有助于系统对可能的偏差进行反馈并调整规划。这种反馈机制可以在不完全可观测的条件下提高系统的恢复能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-384dd051dad49af6e958719b8d4d1b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d34edef9b411ac4c1c28be81f232fe2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8b11308af9cce233e856bbc878d26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-506702e2a4341bcdd7004138af3c3667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d516f7bc4a2ca9c12ad57a136bebea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a6b9f2350360c95e1ce5bfd4a7db9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77846ad7794efa874f08b1c4773e28f0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Does-Prior-Data-Matter-Exploring-Joint-Training-in-the-Context-of-Few-Shot-Class-Incremental-Learning"><a href="#Does-Prior-Data-Matter-Exploring-Joint-Training-in-the-Context-of-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Does Prior Data Matter? Exploring Joint Training in the Context of   Few-Shot Class-Incremental Learning"></a>Does Prior Data Matter? Exploring Joint Training in the Context of   Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Shiwon Kim, Dongjun Hwang, Sungwon Woo, Rita Singh</strong></p>
<p>Class-incremental learning (CIL) aims to adapt to continuously emerging new classes while preserving knowledge of previously learned ones. Few-shot class-incremental learning (FSCIL) presents a greater challenge that requires the model to learn new classes from only a limited number of samples per class. While incremental learning typically assumes restricted access to past data, it often remains available in many real-world scenarios. This raises a practical question: should one retrain the model on the full dataset (i.e., joint training), or continue updating it solely with new data? In CIL, joint training is considered an ideal benchmark that provides a reference for evaluating the trade-offs between performance and computational cost. However, in FSCIL, joint training becomes less reliable due to severe imbalance between base and incremental classes. This results in the absence of a practical baseline, making it unclear which strategy is preferable for practitioners. To this end, we revisit joint training in the context of FSCIL by incorporating imbalance mitigation techniques, and suggest a new imbalance-aware joint training benchmark for FSCIL. We then conduct extensive comparisons between this benchmark and FSCIL methods to analyze which approach is most suitable when prior data is accessible. Our analysis offers realistic insights and guidance for selecting training strategies in real-world FSCIL scenarios. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/shiwonkim/Joint_FSCIL">https://github.com/shiwonkim/Joint_FSCIL</a> </p>
<blockquote>
<p>类增量学习（CIL）旨在适应不断涌现的新类，同时保留对先前学习知识的记忆。小样本类增量学习（FSCIL）呈现出一个更大的挑战，需要模型仅从每类的有限样本中学习新类别。虽然增量学习通常假设对过去数据的访问受到限制，但在许多现实场景中，它通常仍然可用。这引发了一个实际问题：是否应在整个数据集上重新训练模型（即联合训练），还是继续使用新数据进行更新？在CIL中，联合训练被认为是一个理想的基准，为评估性能和计算成本之间的权衡提供了参考。然而，在FSCIL中，由于基础类和增量类之间的严重不平衡，联合训练变得不那么可靠。这导致缺乏实用的基准，使得从业者不清楚哪种策略更为可取。为此，我们通过融入不平衡缓解技术，重新审视了FSCIL中的联合训练，并提出了一个新的不平衡感知联合训练基准用于FSCIL。然后，我们对此基准和FSCIL方法进行了广泛的比较，分析了当可以访问先前数据时，哪种方法最为适合。我们的分析为现实世界的FSCIL场景中选择训练策略提供了切实的见解和指导。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/shiwonkim/Joint_FSCIL">https://github.com/shiwonkim/Joint_FSCIL</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10003v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇文本讨论了类增量学习（CIL）和少样本类增量学习（FSCIL）的挑战，其中FSCIL需要从有限的样本中学习新类别。文本探讨了在实际场景中是否应该使用联合训练（即重新训练整个数据集）或仅使用新数据进行更新。对于FSCIL，由于基础类和增量类之间的严重不平衡，联合训练变得不那么可靠。为此，文本引入了不平衡感知的联合训练基准测试，并与FSCIL方法进行了广泛的比较，以分析在可访问先验数据的情况下哪种方法最适合实际应用场景。为此提供了对选择现实世界中FSCIL场景的训练策略的实际见解和指导。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是从文本中提取的关键要点，以简化和清晰的方式呈现：</p>
<ol>
<li>类增量学习（CIL）旨在适应不断出现的新类别，同时保留对先前学习知识的记忆。</li>
<li>少样本类增量学习（FSCIL）是一个更大挑战，要求模型从每个类别的有限样本中学习新类别。</li>
<li>联合训练在CIL中被视为理想基准，但在FSCIL中变得不那么可靠，因为基础类和增量类之间存在严重不平衡。</li>
<li>引入了一种新的不平衡感知联合训练基准测试，以应对FSCIL的挑战。</li>
<li>对比分析了新的不平衡感知联合训练基准与FSCIL方法，以了解在可访问先验数据的情况下哪种方法最有效。</li>
<li>分析提供了对选择现实世界中FSCIL场景的训练策略的实际见解和指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-045ad467441bead437a0479113056afd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cd6df32adbbf757cd796674a163d818.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc40591461944426f87b8809fa69fb21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-809ceefa4dc9a703828efb5ee793d9f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56f6f9e6025fd45e285493aa7d0300b8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SGPT-Few-Shot-Prompt-Tuning-for-Signed-Graphs"><a href="#SGPT-Few-Shot-Prompt-Tuning-for-Signed-Graphs" class="headerlink" title="SGPT: Few-Shot Prompt Tuning for Signed Graphs"></a>SGPT: Few-Shot Prompt Tuning for Signed Graphs</h2><p><strong>Authors:Zian Zhai, Sima Qing, Xiaoyang Wang, Wenjie Zhang</strong></p>
<p>Signed Graph Neural Networks (SGNNs) are effective in learning expressive representations for signed graphs but typically require substantial task-specific labels, limiting their applicability in label-scarce industrial scenarios. In contrast, unsigned graph structures are abundant and can be readily leveraged to pre-train Graph Neural Networks (GNNs), offering a promising solution to reduce supervision requirements in downstream signed graph tasks. However, transferring knowledge from unsigned to signed graphs is non-trivial due to the fundamental discrepancies in graph types and task objectives between pre-training and downstream phases. To address this challenge, we propose Signed Graph Prompt Tuning (SGPT), a novel graph prompting framework that adapts pre-trained unsigned GNNs to few-shot signed graph tasks. We first design a graph template based on balance theory to disentangle mixed node relationships introduced by negative links, mitigating the structural mismatches between unsigned and signed graphs. We further introduce a task template that reformulates downstream signed tasks into a unified link prediction objective, aligning their optimization goals with the pre-training task. Furthermore, we develop feature prompts that align downstream semantic spaces with the feature spaces learned during pre-training, and semantic prompts to integrate link sign semantics in a task-aware manner. We conduct extensive experiments on seven benchmark signed graph datasets, demonstrating that SGPT significantly outperforms existing state-of-the-art methods, establishing a powerful and generalizable solution for few-shot signed graph learning. </p>
<blockquote>
<p>有符号图神经网络（SGNNs）在学习有符号图的表现力表示方面非常有效，但通常需要大量的特定任务标签，这在标签稀缺的工业场景中限制了其应用。相比之下，无符号图结构非常丰富，可以很容易地用于预训练图神经网络（GNNs），为解决下游有符号图任务中的监督需求减少提供了有前途的解决方案。然而，从无符号图到签名图的知识转移并不简单，因为预训练阶段和下游阶段之间的图形类型和任务目标之间存在基本差异。为了解决这一挑战，我们提出了有符号图提示调整（SGPT），这是一种新型的图提示框架，用于将预训练的无符号GNNs适应于少量有符号图任务。我们首先根据平衡理论设计了一个图模板，以解开由负链接引入的混合节点关系，减轻无符号图和有符号图之间的结构不匹配问题。我们还引入了任务模板，将下游有符号任务重新制定为统一的链接预测目标，使它们的优化目标与预训练任务的目标保持一致。此外，我们开发了特征提示，使下游语义空间与预训练过程中学习的特征空间保持一致，以及语义提示，以任务感知的方式整合链接符号语义。我们在七个基准有符号图数据集上进行了广泛实验，结果表明SGPT显著优于现有最先进的方法，为少量有符号图学习建立了强大且通用的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12155v2">PDF</a> CIKM’25</p>
<p><strong>Summary</strong></p>
<p>本文提出了Signed Graph Prompt Tuning（SGPT）框架，该框架适应于预训练的unsigned GNNs并应用于少量的signed graph任务。通过设计基于平衡理论的图模板解决结构不匹配问题，引入任务模板统一下游signed任务为链接预测目标，同时开发特征提示和语义提示来增强预训练和下游任务的语义对齐。在七个基准signed graph数据集上的实验表明，SGPT显著优于现有先进技术，为少量signed graph学习提供了强大且通用的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Signed Graph Neural Networks (SGNNs) 在学习signed图表示时有效，但在标签稀缺的工业场景中适用性受限。</li>
<li>Unsigned图结构丰富，可预训练Graph Neural Networks（GNNs），为解决下游signed图任务的监督要求提供有前途的解决方案。</li>
<li>从unsigned图到signed图的知诺转移是非平凡的，因为预训练和下流阶段在图形类型和任务目标上的根本差异。</li>
<li>提出了Signed Graph Prompt Tuning (SGPT) 框架，该框架通过图模板、任务模板、特征提示和语义提示来解决这一问题。</li>
<li>图模板基于平衡理论设计，以解决unsigned和signed图之间的结构不匹配问题。</li>
<li>任务模板将下游signed任务重新制定为统一的链接预测目标，与预训练任务优化目标对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-57c91f17df4a079b74f16c100d7eb875.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4798691f072b84a551ba8a5cfa04dcac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9714550fd76ba2fb0d3eb4bdde0cb0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2893b7d926370c800ef0f2a4049196d0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8161e18e93cc54b4a5c45c627185b61a.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-20  Single-Reference Text-to-Image Manipulation with Dual Contrastive   Denoising Score
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1cd588f7ea33dc92654d2f58f9f73c0e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-20  WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
