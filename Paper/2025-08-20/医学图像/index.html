<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  HierAdaptMR Cross-Center Cardiac MRI Reconstruction with Hierarchical   Feature Adapters">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ba2e11ac23e5ddc048437d554ad4ef6f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-20-æ›´æ–°"><a href="#2025-08-20-æ›´æ–°" class="headerlink" title="2025-08-20 æ›´æ–°"></a>2025-08-20 æ›´æ–°</h1><h2 id="HierAdaptMR-Cross-Center-Cardiac-MRI-Reconstruction-with-Hierarchical-Feature-Adapters"><a href="#HierAdaptMR-Cross-Center-Cardiac-MRI-Reconstruction-with-Hierarchical-Feature-Adapters" class="headerlink" title="HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical   Feature Adapters"></a>HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical   Feature Adapters</h2><p><strong>Authors:Ruru Xu, Ilkay Oksuz</strong></p>
<p>Deep learning-based cardiac MRI reconstruction faces significant domain shift challenges when deployed across multiple clinical centers with heterogeneous scanner configurations and imaging protocols. We propose HierAdaptMR, a hierarchical feature adaptation framework that addresses multi-level domain variations through parameter-efficient adapters. Our method employs Protocol-Level Adapters for sequence-specific characteristics and Center-Level Adapters for scanner-dependent variations, built upon a variational unrolling backbone. A Universal Adapter enables generalization to entirely unseen centers through stochastic training that learns center-invariant adaptations. The framework utilizes multi-scale SSIM loss with frequency domain enhancement and contrast-adaptive weighting for robust optimization. Comprehensive evaluation on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9 modalities demonstrates superior cross-center generalization while maintaining reconstruction quality. code: <a target="_blank" rel="noopener" href="https://github.com/Ruru-Xu/HierAdaptMR">https://github.com/Ruru-Xu/HierAdaptMR</a> </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å¿ƒè„MRIé‡å»ºåœ¨è·¨å¤šä¸ªä¸´åºŠä¸­å¿ƒéƒ¨ç½²æ—¶ï¼Œé¢ä¸´å› æ‰«æä»ªé…ç½®å’Œæˆåƒåè®®ä¸åŒè€Œå¯¼è‡´çš„æ˜¾è‘—é¢†åŸŸåç§»æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†HierAdaptMRï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚ç‰¹å¾é€‚åº”æ¡†æ¶ï¼Œå®ƒé€šè¿‡å‚æ•°é«˜æ•ˆçš„é€‚é…å™¨è§£å†³å¤šçº§é¢†åŸŸå˜åŒ–é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åè®®çº§é€‚é…å™¨å¤„ç†åºåˆ—ç‰¹å®šç‰¹å¾ï¼Œé‡‡ç”¨ä¸­å¿ƒçº§é€‚é…å™¨å¤„ç†æ‰«æä»ªç›¸å…³å˜åŒ–ï¼Œå»ºç«‹åœ¨å¯å˜æ»šåŠ¨éª¨å¹²ç½‘ç»œä¸Šã€‚é€šç”¨é€‚é…å™¨é€šè¿‡éšæœºè®­ç»ƒå­¦ä¹ ä¸­å¿ƒä¸å˜é€‚åº”ï¼Œå®ç°å¯¹å…¨æ–°ä¸­å¿ƒçš„æ³›åŒ–ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šå°ºåº¦SSIMæŸå¤±ã€é¢‘ç‡åŸŸå¢å¼ºå’Œå¯¹æ¯”åº¦è‡ªé€‚åº”åŠ æƒè¿›è¡Œç¨³å¥ä¼˜åŒ–ã€‚åœ¨æ¶µç›–5ä¸ªä¸­å¿ƒã€10å¤šä¸ªæ‰«æä»ªå’Œ9ç§æ¨¡æ€çš„CMRxRecon2025æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œå…·æœ‰å‡ºè‰²çš„è·¨ä¸­å¿ƒæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ruru-Xu/HierAdaptMR">https://github.com/Ruru-Xu/HierAdaptMR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13026v1">PDF</a> MICCAI 2025, CMRxRecon2025 Challenge paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªé€‚åº”å¿ƒè„MRIé‡å»ºæ¡†æ¶HierAdaptMRï¼Œç”¨äºè§£å†³è·¨å¤šä¸ªä¸´åºŠä¸­å¿ƒéƒ¨ç½²æ—¶é¢ä¸´çš„é¢†åŸŸå·®å¼‚é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å‚æ•°é«˜æ•ˆçš„é€‚é…å™¨è§£å†³å¤šå±‚æ¬¡é¢†åŸŸå˜åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬é’ˆå¯¹åºåˆ—ç‰¹å®šç‰¹æ€§çš„åè®®çº§é€‚é…å™¨å’Œé’ˆå¯¹æ‰«æä»ªç›¸å…³å˜åŒ–çš„ä¸­å¿ƒçº§é€‚é…å™¨ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªé€šç”¨é€‚é…å™¨é€šè¿‡éšæœºè®­ç»ƒå­¦ä¹ ä¸­å¿ƒä¸å˜é€‚åº”ï¼Œèƒ½å¤Ÿæ¨å¹¿åˆ°å®Œå…¨æœªè§è¿‡çš„ä¸­å¿ƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŸå¤±ç»“åˆé¢‘åŸŸå¢å¼ºå’Œå¯¹æ¯”åº¦è‡ªé€‚åº”åŠ æƒè¿›è¡Œä¼˜åŒ–ï¼Œå¹¶åœ¨è·¨è¶Šå¤šä¸ªä¸­å¿ƒã€æ‰«æä»ªå’Œæ¨¡æ€çš„CMRxRecon2025æ•°æ®é›†ä¸Šè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå±•ç°å‡ºå“è¶Šçš„è·¨ä¸­å¿ƒæ³›åŒ–èƒ½åŠ›å’Œé‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HierAdaptMRæ˜¯ä¸€ä¸ªç”¨äºå¿ƒè„MRIé‡å»ºçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿè§£å†³è·¨å¤šä¸ªä¸´åºŠä¸­å¿ƒçš„é¢†åŸŸå·®å¼‚é—®é¢˜ã€‚</li>
<li>é€šè¿‡å‚æ•°é«˜æ•ˆçš„é€‚é…å™¨å®ç°å¤šå±‚æ¬¡é¢†åŸŸé€‚åº”ï¼ŒåŒ…æ‹¬åè®®çº§é€‚é…å™¨å’Œä¸­å¿ƒçº§é€‚é…å™¨ã€‚</li>
<li>é€šç”¨é€‚é…å™¨é€šè¿‡éšæœºè®­ç»ƒå­¦ä¹ ä¸­å¿ƒä¸å˜é€‚åº”ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŸå¤±ã€é¢‘åŸŸå¢å¼ºå’Œå¯¹æ¯”åº¦è‡ªé€‚åº”åŠ æƒï¼Œä»¥å®ç°ç¨³å¥ä¼˜åŒ–ã€‚</li>
<li>åœ¨è·¨è¶Šå¤šä¸ªä¸­å¿ƒã€æ‰«æä»ªå’Œæ¨¡æ€çš„CMRxRecon2025æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>HierAdaptMRåœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶å±•ç°å‡ºå“è¶Šçš„è·¨ä¸­å¿ƒæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05299d8f6871ef4864f5df29886b379e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78fcddbfa0a9c18f758dd5ad3fe9b12f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fede0ffcad0f4e3165c16b7b07e190f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fully-Automated-Segmentation-of-Fiber-Bundles-in-Anatomic-Tracing-Data"><a href="#Fully-Automated-Segmentation-of-Fiber-Bundles-in-Anatomic-Tracing-Data" class="headerlink" title="Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data"></a>Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data</h2><p><strong>Authors:Kyriaki-Margarita Bintsi, YaÃ«l Balbastre, Jingjing Wu, Julia F. Lehman, Suzanne N. Haber, Anastasia Yendiki</strong></p>
<p>Anatomic tracer studies are critical for validating and improving diffusion MRI (dMRI) tractography. However, large-scale analysis of data from such studies is hampered by the labor-intensive process of annotating fiber bundles manually on histological slides. Existing automated methods often miss sparse bundles or require complex post-processing across consecutive sections, limiting their flexibility and generalizability. We present a streamlined, fully automated framework for fiber bundle segmentation in macaque tracer data, based on a U-Net architecture with large patch sizes, foreground aware sampling, and semisupervised pre-training. Our approach eliminates common errors such as mislabeling terminals as bundles, improves detection of sparse bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared to the state-of-the-art, all while enabling analysis of standalone slices. This new framework will facilitate the automated analysis of anatomic tracing data at a large scale, generating more ground-truth data that can be used to validate and optimize dMRI tractography methods. </p>
<blockquote>
<p>è§£å‰–è¿½è¸ªç ”ç©¶å¯¹äºéªŒè¯å’Œæ”¹è¿›æ‰©æ•£MRIï¼ˆdMRIï¼‰å›¾è°±æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç ”ç©¶çš„æ•°æ®å¤§è§„æ¨¡åˆ†æå—åˆ°æ‰‹åŠ¨åœ¨ç»„ç»‡åˆ‡ç‰‡ä¸Šæ³¨é‡Šçº¤ç»´æŸçš„åŠ³åŠ¨å¼ºåº¦å¤§çš„é˜»ç¢ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç»å¸¸ä¼šé—æ¼ç¨€ç–çš„æŸæˆ–éœ€è¦åœ¨è¿ç»­çš„åˆ‡ç‰‡ä¸Šè¿›è¡Œå¤æ‚çš„åå¤„ç†ï¼Œè¿™é™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€åŒ–çš„ã€å®Œå…¨è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç”¨äºæ’æ²³çŒ´è¿½è¸ªæ•°æ®ä¸­çš„çº¤ç»´æŸåˆ†å‰²ï¼Œè¯¥æ¡†æ¶åŸºäºå¤§è¡¥ä¸å°ºå¯¸ã€å‰æ™¯æ„ŸçŸ¥é‡‡æ ·å’ŒåŠç›‘ç£é¢„è®­ç»ƒçš„U-Netæ¶æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†è¯¯æ ‡è®°ç»ˆç«¯ä¸ºæŸçš„å¸¸è§é”™è¯¯ï¼Œæé«˜äº†å¯¹ç¨€ç–æŸçš„æ£€æµ‹ç²¾åº¦è¶…è¿‡20%ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œé™ä½äº†40%çš„è¯¯æŠ¥ç‡ï¼ˆFDRï¼‰ï¼ŒåŒæ—¶æ”¯æŒç‹¬ç«‹åˆ‡ç‰‡çš„åˆ†æã€‚è¿™ä¸€æ–°æ¡†æ¶å°†ä¿ƒè¿›è§£å‰–è¿½è¸ªæ•°æ®çš„è‡ªåŠ¨åŒ–å¤§è§„æ¨¡åˆ†æï¼Œç”Ÿæˆæ›´å¤šçš„çœŸå®æ•°æ®ï¼Œå¯ç”¨äºéªŒè¯å’Œä¼˜åŒ–dMRIå›¾è°±æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12942v1">PDF</a> Accepted at CDMRI, MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºU-Netæ¶æ„çš„è‡ªåŠ¨åŒ–çº¤ç»´æŸåˆ†å‰²æ¡†æ¶ï¼Œé€‚ç”¨äºçŒ•çŒ´è¿½è¸ªæ•°æ®ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤§è¡¥ä¸å°ºå¯¸ã€å‰æ™¯æ„ŸçŸ¥é‡‡æ ·å’ŒåŠç›‘ç£é¢„è®­ç»ƒï¼Œå¯æ¶ˆé™¤è¯¯æ ‡è®°ç»ˆç«¯ä¸ºæŸçš„é”™è¯¯ï¼Œæé«˜ç¨€ç–æŸæ£€æµ‹ç‡è¶…è¿‡20%ï¼Œå¹¶é™ä½é”™è¯¯å‘ç°ç‡ï¼ˆFDRï¼‰è¾¾40%ï¼ŒåŒæ—¶æ”¯æŒå¯¹ç‹¬ç«‹åˆ‡ç‰‡çš„åˆ†æã€‚è¿™ä¸€æ–°æ¡†æ¶å°†ä¿ƒè¿›å¤§è§„æ¨¡è‡ªåŠ¨åˆ†æè§£å‰–è¿½è¸ªæ•°æ®ï¼Œç”Ÿæˆæ›´å¤šå¯ç”¨äºéªŒè¯å’Œä¼˜åŒ–dMRIè¿½è¸ªæ–¹æ³•çš„åœ°é¢çœŸå®æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å‰–è¿½è¸ªç ”ç©¶å¯¹äºéªŒè¯å’Œæ”¹è¿›æ‰©æ•£MRIï¼ˆdMRIï¼‰è½¨è¿¹ç»˜åˆ¶è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ‰‹åŠ¨æ³¨é‡Šçº¤ç»´æŸçš„æ–¹æ³•å­˜åœ¨åŠ³åŠ¨å¼ºåº¦å¤§ï¼Œéš¾ä»¥åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®åˆ†æçš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºU-Netæ¶æ„çš„è‡ªåŠ¨åŒ–çº¤ç»´æŸåˆ†å‰²æ¡†æ¶ï¼Œå…·æœ‰å¤§è¡¥ä¸å°ºå¯¸ã€å‰æ™¯æ„ŸçŸ¥é‡‡æ ·å’ŒåŠç›‘ç£é¢„è®­ç»ƒçš„ç‰¹ç‚¹ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½å¤Ÿæ¶ˆé™¤è¯¯æ ‡è®°ç»ˆç«¯ä¸ºæŸçš„é”™è¯¯ï¼Œæé«˜ç¨€ç–æŸæ£€æµ‹ç‡è¶…è¿‡20%ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ¡†æ¶å¯é™ä½é”™è¯¯å‘ç°ç‡ï¼ˆFDRï¼‰è¾¾40%ã€‚</li>
<li>æ–°æ¡†æ¶æ”¯æŒå¯¹ç‹¬ç«‹åˆ‡ç‰‡çš„åˆ†æï¼Œå¢å¼ºäº†å…¶çµæ´»æ€§å’Œé€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75ed1eb29a3d1e0e4e297fb1f84098a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0326875ea5dd1df30492175b513c6aba.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CTFlow-Video-Inspired-Latent-Flow-Matching-for-3D-CT-Synthesis"><a href="#CTFlow-Video-Inspired-Latent-Flow-Matching-for-3D-CT-Synthesis" class="headerlink" title="CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis"></a>CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis</h2><p><strong>Authors:Jiayi Wang, Hadrien Reynaud, Franciskus Xaverius Erick, Bernhard Kainz</strong></p>
<p>Generative modelling of entire CT volumes conditioned on clinical reports has the potential to accelerate research through data augmentation, privacy-preserving synthesis and reducing regulator-constraints on patient data while preserving diagnostic signals. With the recent release of CT-RATE, a large-scale collection of 3D CT volumes paired with their respective clinical reports, training large text-conditioned CT volume generation models has become achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching transformer model, conditioned on clinical reports. We leverage the A-VAE from FLUX to define our latent space, and rely on the CT-Clip text encoder to encode the clinical reports. To generate consistent whole CT volumes while keeping the memory constraints tractable, we rely on a custom autoregressive approach, where the model predicts the first sequence of slices of the volume from text-only, and then relies on the previously generated sequence of slices and the text, to predict the following sequence. We evaluate our results against state-of-the-art generative CT model, and demonstrate the superiority of our approach in terms of temporal coherence, image diversity and text-image alignment, with FID, FVD, IS scores and CLIP score. </p>
<blockquote>
<p>ç”Ÿæˆæ•´ä¸ªCTä½“ç§¯çš„æ¨¡å‹ä»¥ä¸´åºŠæŠ¥å‘Šä¸ºæ¡ä»¶ï¼Œå…·æœ‰é€šè¿‡æ•°æ®å¢å¼ºã€éšç§ä¿æŠ¤åˆæˆä»¥åŠå‡å°‘æ‚£è€…æ•°æ®çš„ç›‘ç®¡çº¦æŸæ¥åŠ é€Ÿç ”ç©¶çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¿ç•™è¯Šæ–­ä¿¡å·ã€‚éšç€å¤§å‹ä¸‰ç»´CTä½“ç§¯ä¸å…¶ç›¸åº”ä¸´åºŠæŠ¥å‘Šçš„é›†åˆCT-RATEçš„å‘å¸ƒï¼Œä»¥æ–‡æœ¬ä¸ºæ¡ä»¶çš„CTä½“ç§¯ç”Ÿæˆæ¨¡å‹è®­ç»ƒå˜å¾—å¯è¡Œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CTFlowï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥ä¸´åºŠæŠ¥å‘Šä¸ºæ¡ä»¶çš„0.5Bæ½œåœ¨æµåŒ¹é…å˜å‹å™¨æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨FLUXä¸­çš„A-VAEæ¥å®šä¹‰æˆ‘ä»¬çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä¾èµ–CT-Clipæ–‡æœ¬ç¼–ç å™¨å¯¹ä¸´åºŠæŠ¥å‘Šè¿›è¡Œç¼–ç ã€‚ä¸ºäº†åœ¨ä¿æŒå†…å­˜çº¦æŸçš„åŒæ—¶ç”Ÿæˆä¸€è‡´å®Œæ•´çš„CTä½“ç§¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è‡ªå®šä¹‰çš„è‡ªå›å½’æ–¹æ³•ï¼Œæ¨¡å‹ä»…æ ¹æ®æ–‡æœ¬é¢„æµ‹ä½“ç§¯çš„ç¬¬ä¸€åºåˆ—åˆ‡ç‰‡ï¼Œç„¶åä¾èµ–äºå…ˆå‰ç”Ÿæˆçš„åˆ‡ç‰‡åºåˆ—å’Œæ–‡æœ¬æ¥é¢„æµ‹åç»­çš„åºåˆ—ã€‚æˆ‘ä»¬ä¸æœ€å…ˆè¿›çš„ç”Ÿæˆå¼CTæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°å¯¹æ¯”ï¼Œé€šè¿‡FIDã€FVDã€ISå¾—åˆ†å’ŒCLIPå¾—åˆ†ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ—¶åºä¸€è‡´æ€§ã€å›¾åƒå¤šæ ·æ€§å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12900v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸´åºŠæŠ¥å‘Šçš„æ•´ä¸ªCTä½“ç§¯ç”Ÿæˆæ¨¡å‹å…·æœ‰åŠ é€Ÿç ”ç©¶ã€ä¿æŠ¤éšç§ã€åˆæˆæ•°æ®ä»¥åŠå‡å°‘ç›‘ç®¡å¯¹æ‚£è€…æ•°æ®çš„é™åˆ¶ç­‰æ½œåŠ›ï¼ŒåŒæ—¶ä¿ç•™è¯Šæ–­ä¿¡å·ã€‚å€ŸåŠ©æ–°å‘å¸ƒçš„CT-RATEå¤§è§„æ¨¡æ•°æ®é›†â€”â€”åŒ…å«é…å¯¹çš„3D CTä½“ç§¯åŠå…¶ä¸´åºŠæŠ¥å‘Šï¼Œç°åœ¨å¯å®ç°å¤§å‹æ–‡æœ¬æ¡ä»¶CTä½“ç§¯ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CTFlowï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸´åºŠæŠ¥å‘Šçš„0.5Bæ½œåœ¨æµåŒ¹é…è½¬æ¢å™¨æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨FLUXä¸­çš„A-VAEæ¥å®šä¹‰æˆ‘ä»¬çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä¾èµ–CT-Clipæ–‡æœ¬ç¼–ç å™¨å¯¹ä¸´åºŠæŠ¥å‘Šè¿›è¡Œç¼–ç ã€‚ä¸ºäº†åœ¨ä¿æŒå†…å­˜çº¦æŸçš„åŒæ—¶ç”Ÿæˆä¸€è‡´çš„å®Œæ•´CTä½“ç§¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è‡ªå®šä¹‰çš„è‡ªå›å½’æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹ä»…ä¾é æ–‡æœ¬é¢„æµ‹ä½“ç§¯çš„ç¬¬ä¸€åºåˆ—åˆ‡ç‰‡ï¼Œç„¶åä¾èµ–äºå…ˆå‰ç”Ÿæˆçš„åˆ‡ç‰‡åºåˆ—å’Œæ–‡æœ¬æ¥é¢„æµ‹åç»­çš„åˆ‡ç‰‡ã€‚æˆ‘ä»¬çš„ç»“æœç»è¿‡å…ˆè¿›çš„ç”Ÿæˆå¼CTæ¨¡å‹è¯„ä¼°ï¼Œåœ¨æ—¶åºè¿è´¯æ€§ã€å›¾åƒå¤šæ ·æ€§å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œé€šè¿‡FIDã€FVDã€ISå¾—åˆ†å’ŒCLIPå¾—åˆ†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼å»ºæ¨¡å¯ä»¥åŸºäºä¸´åºŠæŠ¥å‘Šç”Ÿæˆæ•´ä¸ªCTä½“ç§¯ï¼Œæœ‰åŠ©äºåŠ é€Ÿç ”ç©¶ã€ä¿æŠ¤éšç§å’Œå‡å°‘ç›‘ç®¡çº¦æŸã€‚</li>
<li>CT-RATEæ•°æ®é›†çš„å‘å¸ƒä½¿å¾—è®­ç»ƒå¤§å‹æ–‡æœ¬æ¡ä»¶CTä½“ç§¯ç”Ÿæˆæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚</li>
<li>CTFlowæ¨¡å‹åˆ©ç”¨æ½œåœ¨ç©ºé—´å®šä¹‰å’Œæ–‡æœ¬ç¼–ç æŠ€æœ¯ï¼Œç”Ÿæˆä¸ä¸´åºŠæŠ¥å‘Šç›¸åŒ¹é…çš„CTä½“ç§¯ã€‚</li>
<li>é‡‡ç”¨è‡ªå›å½’æ–¹æ³•ç”Ÿæˆä¸€è‡´çš„å®Œæ•´CTä½“ç§¯ï¼Œç»“åˆæ–‡æœ¬ä¿¡æ¯å’Œå·²ç”Ÿæˆçš„åˆ‡ç‰‡åºåˆ—è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ä¸å…ˆè¿›ç”Ÿæˆå¼CTæ¨¡å‹ç›¸æ¯”ï¼ŒCTFlowåœ¨æ—¶åºè¿è´¯æ€§ã€å›¾åƒå¤šæ ·æ€§å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25dd3c69a9a856291edc64332211501c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1349acb7a288de8658e644ad537aac5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11448326b9dd6629c2b3b6ba003ba5c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b7f081721c812b55109aff3d8acfb8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce9f93cf2f53a4e2353480fe2f028d02.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mechanism-of-Quercetin-in-Inhibiting-Triple-Negative-Breast-Cancer-by-Regulating-T-Cell-Related-Targets-An-Analysis-Based-on-Single-Cell-Sequencing-and-Network-Pharmacology"><a href="#Mechanism-of-Quercetin-in-Inhibiting-Triple-Negative-Breast-Cancer-by-Regulating-T-Cell-Related-Targets-An-Analysis-Based-on-Single-Cell-Sequencing-and-Network-Pharmacology" class="headerlink" title="Mechanism of Quercetin in Inhibiting Triple-Negative Breast Cancer by   Regulating T Cell-Related Targets: An Analysis Based on Single-Cell   Sequencing and Network Pharmacology"></a>Mechanism of Quercetin in Inhibiting Triple-Negative Breast Cancer by   Regulating T Cell-Related Targets: An Analysis Based on Single-Cell   Sequencing and Network Pharmacology</h2><p><strong>Authors:Ruiqi Chen, Liang Hang, Fengyun Wang</strong></p>
<p>Objective: To investigate the mechanism by which quercetin inhibits triple-negative breast cancer (TNBC) through regulating T-cell-related targets, providing a novel strategy for TNBC immunotherapy.Methods: Single-cell RNA sequencing (GSE161529 dataset) and network pharmacology were integrated. PCA and UMAP clustering identified T-cell subsets and differentially expressed genes in TNBC microenvironment. TNBC-related targets were screened via CTD and OMIM databases, with functional pathways analyzed by GO&#x2F;KEGG enrichment. Molecular docking and PPI networks validated interactions between quercetin and core targets.Results: Quercetin intersected with 79 TNBC targets, including AKT1, EGFR, and MMP9, enriched in EGFR inhibitor resistance and endocrine resistance pathways. Molecular docking revealed the highest affinity between quercetin and GSK3B (-13.2 kJ&#x2F;mol). AKT1 and MMP9 expression correlated with patient survival.Conclusion: Quercetin may reverse TNBC immunosuppression by multi-target modulation of T-cell function, but clinical application requires solutions for its low bioavailability, such as delivery systems or combination therapies. </p>
<blockquote>
<p>ç›®æ ‡ï¼šé€šè¿‡è°ƒèŠ‚Tç»†èƒç›¸å…³é¶ç‚¹æ¥ç ”ç©¶æ§²çš®ç´ æŠ‘åˆ¶ä¸‰é˜´æ€§ä¹³è…ºç™Œï¼ˆTNBCï¼‰çš„æœºåˆ¶ï¼Œä¸ºTNBCå…ç–«æ²»ç–—æä¾›æ–°çš„ç­–ç•¥ã€‚æ–¹æ³•ï¼šæ•´åˆå•ç»†èƒRNAæµ‹åºï¼ˆGSE161529æ•°æ®é›†ï¼‰å’Œç½‘ç»œè¯ç†å­¦ã€‚ä¸»æˆåˆ†åˆ†æå’ŒUMAPèšç±»æŠ€æœ¯ç¡®å®šäº†Tç»†èƒäºšç¾¤å’ŒTNBCå¾®ç¯å¢ƒä¸­å·®å¼‚è¡¨è¾¾çš„åŸºå› ã€‚é€šè¿‡CTDå’ŒOMIMæ•°æ®åº“ç­›é€‰TNBCç›¸å…³é¶ç‚¹ï¼Œåˆ©ç”¨GO&#x2F;KEGGå¯Œé›†åˆ†æåŠŸèƒ½é€”å¾„ã€‚åˆ†å­å¯¹æ¥å’Œè›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œéªŒè¯äº†æ§²çš®ç´ ä¸æ ¸å¿ƒé¶ç‚¹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ç»“æœï¼šæ§²çš®ç´ ä¸TNBCçš„79ä¸ªé¶ç‚¹ç›¸äº’ä½œç”¨ï¼ŒåŒ…æ‹¬AKT1ã€EGFRå’ŒMMP9ç­‰ï¼Œè¿™äº›é¶ç‚¹å¯Œé›†äºEGFRæŠ‘åˆ¶å‰‚è€è¯å’Œå†…åˆ†æ³Œè€è¯é€”å¾„ã€‚åˆ†å­å¯¹æ¥æ˜¾ç¤ºæ§²çš®ç´ ä¸GSK3Bä¹‹é—´çš„äº²å’ŒåŠ›æœ€é«˜ï¼ˆ-13.2kJ&#x2F;molï¼‰ã€‚AKT1å’ŒMMP9çš„è¡¨è¾¾ä¸æ‚£è€…ç”Ÿå­˜ç›¸å…³ã€‚ç»“è®ºï¼šæ§²çš®ç´ å¯èƒ½é€šè¿‡å¤šé¶ç‚¹è°ƒèŠ‚Tç»†èƒåŠŸèƒ½é€†è½¬TNBCå…ç–«æŠ‘åˆ¶ï¼Œä½†ä¸´åºŠåº”ç”¨éœ€è¦è§£å†³å…¶ç”Ÿç‰©åˆ©ç”¨åº¦ä½çš„é—®é¢˜ï¼Œå¦‚é‡‡ç”¨ç»™è¯ç³»ç»Ÿæˆ–è”åˆç–—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12731v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ§²çš®ç´ å¯é€šè¿‡è°ƒèŠ‚Tç»†èƒç›¸å…³é¶ç‚¹æŠ‘åˆ¶ä¸‰é˜´æ€§ä¹³è…ºç™Œï¼ˆTNBCï¼‰ï¼Œä¸ºTNBCå…ç–«æ²»ç–—æä¾›æ–°ç­–ç•¥ã€‚ç ”ç©¶é€šè¿‡å•ç»†èƒRNAæµ‹åºå’Œç½‘ç»œè¯ç†å­¦æ•´åˆæ–¹æ³•ï¼Œè¯†åˆ«Tç»†èƒäºšç¾¤å’ŒTNBCå¾®ç¯å¢ƒä¸­çš„å·®å¼‚è¡¨è¾¾åŸºå› ï¼Œç­›é€‰TNBCç›¸å…³é¶ç‚¹ï¼Œå¹¶éªŒè¯æ§²çš®ç´ ä¸æ ¸å¿ƒé¶ç‚¹çš„ç›¸äº’ä½œç”¨ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ§²çš®ç´ ä¸79ä¸ªTNBCé¶ç‚¹æœ‰äº¤é›†ï¼ŒåŒ…æ‹¬AKT1ã€EGFRå’ŒMMP9ç­‰ï¼Œä¸»è¦å¯Œé›†åœ¨EGFRæŠ‘åˆ¶å‰‚æŠ—æ€§å’Œå†…åˆ†æ³ŒæŠ—æ€§é€”å¾„ä¸­ã€‚åˆ†å­å¯¹æ¥æ˜¾ç¤ºæ§²çš®ç´ ä¸GSK3Bçš„äº²å’ŒåŠ›æœ€é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ§²çš®ç´ èƒ½å¤ŸæŠ‘åˆ¶ä¸‰é˜´æ€§ä¹³è…ºç™Œï¼ˆTNBCï¼‰çš„å‘å±•ï¼Œè¿™ä¸€ä½œç”¨é€šè¿‡è°ƒèŠ‚Tç»†èƒç›¸å…³é¶ç‚¹å®ç°ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å•ç»†èƒRNAæµ‹åºå’Œç½‘ç»œè¯ç†å­¦æ–¹æ³•ï¼Œç»¼åˆåˆ†æäº†Tç»†èƒäºšç¾¤å’ŒTNBCå¾®ç¯å¢ƒä¸­çš„åŸºå› è¡¨è¾¾å·®å¼‚ã€‚</li>
<li>é€šè¿‡CTDå’ŒOMIMæ•°æ®åº“ç­›é€‰äº†TNBCç›¸å…³é¶ç‚¹ï¼Œå¹¶è¿›è¡Œäº†GO&#x2F;KEGGå¯Œé›†åˆ†æï¼Œæ˜ç¡®äº†ç›¸å…³åŠŸèƒ½é€”å¾„ã€‚</li>
<li>åˆ†å­å¯¹æ¥å®éªŒæ˜¾ç¤ºï¼Œæ§²çš®ç´ ä¸æ ¸å¿ƒé¶ç‚¹å¦‚GSK3Bä¹‹é—´çš„äº²å’ŒåŠ›è¾ƒå¼ºã€‚</li>
<li>æ§²çš®ç´ ä¸å¤šä¸ªTNBCé¶ç‚¹æœ‰äº¤é›†ï¼ŒåŒ…æ‹¬AKT1ã€EGFRå’ŒMMP9ç­‰ã€‚</li>
<li>AKT1å’ŒMMP9çš„è¡¨è¾¾ä¸æ‚£è€…ç”Ÿå­˜ç›¸å…³ï¼Œè¿™æç¤ºäº†æ§²çš®ç´ æ²»ç–—çš„å¯èƒ½ä¸´åºŠæ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbafd1e6ff14652382c942aa93108170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00237ae9eac64ce10bdf1df036732e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b4aae8e29b48183b060c2e6c557ea3e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Accurate-and-Realistic-T1-weighted-Contrast-Enhanced-MR-Images-using-Posterior-Mean-Rectified-Flow"><a href="#Synthesizing-Accurate-and-Realistic-T1-weighted-Contrast-Enhanced-MR-Images-using-Posterior-Mean-Rectified-Flow" class="headerlink" title="Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR   Images using Posterior-Mean Rectified Flow"></a>Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR   Images using Posterior-Mean Rectified Flow</h2><p><strong>Authors:Bastian BrandstÃ¶tter, Erich Kobler</strong></p>
<p>Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic diagnosis but requires gadolinium-based agents, which add cost and scan time, raise environmental concerns, and may pose risks to patients. In this work, we propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for synthesizing volumetric CE brain MRI from non-contrast inputs. First, a patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE). Then, this initial estimate is refined by a time-conditioned 3D rectified flow to incorporate realistic textures without compromising structural fidelity. We train this model on a multi-institutional collection of paired pre- and post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360 diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and KID of $0.007$ ($\sim 68.7%$ lower FID than the posterior mean) while maintaining low volumetric MSE of $0.057$ ($\sim 27%$ higher than the posterior mean). Qualitative comparisons confirm that our method restores lesion margins and vascular details realistically, effectively navigating the perception-distortion trade-off for clinical deployment. </p>
<blockquote>
<p>åœ¨ç¥ç»è‚¿ç˜¤å­¦è¯Šæ–­ä¸­ï¼Œå¯¹æ¯”å¢å¼ºï¼ˆCEï¼‰T1åŠ æƒMRIæ˜¯å…³é”®æŠ€æœ¯ï¼Œä½†éœ€è¦ä¾èµ–é’†åŸºé€ å½±å‰‚ã€‚è¿™å¢åŠ äº†æˆæœ¬å’Œæ‰«ææ—¶é—´ï¼Œå¼•å‘äº†ç¯å¢ƒæ‹…å¿§ï¼Œå¹¶å¯èƒ½å¯¹æ‚£è€…æ„æˆé£é™©ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åå‡å€¼æ ¡æ­£æµï¼ˆPMRFï¼‰ç®¡é“ï¼Œç”¨äºä»éå¯¹æ¯”è¾“å…¥ä¸­åˆæˆä½“ç§¯CEè„‘MRIã€‚é¦–å…ˆï¼ŒåŸºäºè¡¥ä¸çš„3D U-Neté¢„æµ‹ä½“ç´ çš„åå‡å€¼ï¼ˆæœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼‰ã€‚ç„¶åï¼Œé€šè¿‡æ—¶é—´è°ƒèŠ‚çš„3Dæ ¡æ­£æµå¯¹è¿™ä¸ªåˆæ­¥ä¼°è®¡è¿›è¡Œç»†åŒ–ï¼Œä»¥èå…¥é€¼çœŸçš„çº¹ç†è€Œä¸æŸå®³ç»“æ„ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯åœ¨å¤šä¸ªæœºæ„æ”¶é›†çš„é…å¯¹çš„å‰åå¯¹æ¯”T1wä½“ç§¯é›†ï¼ˆBraTS 2023-2025ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚åœ¨ç”±360ä¸ªä¸åŒä½“ç§¯ç»„æˆçš„ä¿ç•™æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬æœ€ä½³ç»†åŒ–è¾“å‡ºå®ç°äº†è½´å‘FIDä¸º12.46å’ŒKIDä¸º0.007ï¼ˆåå‡å€¼FIDé™ä½äº†çº¦68.7%ï¼‰ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„ä½“ç§¯å‡æ–¹è¯¯å·®ä¸º0.057ï¼ˆæ¯”åå‡å€¼é«˜å¤§çº¦27%ï¼‰ã€‚å®šæ€§æ¯”è¾ƒè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé€¼çœŸåœ°æ¢å¤ç—…ç¶è¾¹ç¼˜å’Œè¡€ç®¡ç»†èŠ‚ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸´åºŠéƒ¨ç½²ä¸­çš„æ„ŸçŸ¥å¤±çœŸæƒè¡¡é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12640v1">PDF</a> 12 pages, 3 figures, MICCAI workshops (SASHIMI) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸¤é˜¶æ®µçš„åˆæˆå¯¹æ¯”å¢å¼ºï¼ˆCEï¼‰T1åŠ æƒMRIçš„æ–¹æ³•ï¼Œä½¿ç”¨éå¯¹æ¯”å¢å¼ºè¾“å…¥æ•°æ®ã€‚é¦–å…ˆï¼ŒåŸºäºè¡¥ä¸çš„3D U-Neté¢„æµ‹ä½“ç´ çº§åéªŒå‡å€¼ï¼ˆæœ€å°åŒ–å‡æ–¹è¯¯å·®ï¼‰ã€‚ç„¶åï¼Œé€šè¿‡æ—¶é—´è°ƒèŠ‚çš„3Dæ ¡æ­£æµå¯¹åˆå§‹ä¼°è®¡è¿›è¡Œç»†åŒ–ï¼Œä»¥èå…¥é€¼çœŸçš„çº¹ç†è€Œä¸æŸå®³ç»“æ„ä¿çœŸåº¦ã€‚è¯¥æ¨¡å‹åœ¨å¤šå®¶æœºæ„çš„é…å¯¹é¢„å¯¹æ¯”å’Œåå¯¹æ¯”T1wä½“ç§¯é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆBraTS 2023-2025ï¼‰ã€‚åœ¨ç”±360ä¸ªä¸åŒä½“ç§¯ç»„æˆçš„æµ‹è¯•é›†ä¸Šï¼Œæœ€ä½³ç»†åŒ–è¾“å‡ºè¾¾åˆ°è½´å‘FIDä¸º12.46å’ŒKIDä¸º0.007ï¼ˆä¸åéªŒå‡å€¼ç›¸æ¯”é™ä½äº†çº¦68.7%ï¼‰ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„ä½“ç§¯MSEä¸º0.057ï¼ˆæ¯”åéªŒå‡å€¼é«˜çº¦27%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åˆæˆå¯¹æ¯”å¢å¼ºï¼ˆCEï¼‰T1åŠ æƒMRIçš„æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨éå¯¹æ¯”å¢å¼ºè¾“å…¥æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä½¿ç”¨åŸºäºè¡¥ä¸çš„3D U-Neté¢„æµ‹ä½“ç´ çº§åéªŒå‡å€¼ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µä½¿ç”¨æ—¶é—´è°ƒèŠ‚çš„3Dæ ¡æ­£æµå¯¹åˆå§‹ä¼°è®¡è¿›è¡Œç»†åŒ–ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒæ•°æ®æ¥æºäºå¤šæœºæ„çš„é…å¯¹é¢„å’Œåå¯¹æ¯”T1wä½“ç§¯é›†ï¼ˆBraTS 2023-2025ï¼‰ã€‚</li>
<li>æµ‹è¯•ç»“æœæ˜¾ç¤ºæœ€ä½³ç»†åŒ–è¾“å‡ºçš„FIDå’ŒKIDç›¸è¾ƒäºåéªŒå‡å€¼æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½çš„MSEã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3a5cdea1de311b31ff7871cad01c33a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dbd1d72a7b4dbb1686fd33895969d4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6a8260cc1f02907f3653a59c22b1a7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FractMorph-A-Fractional-Fourier-Based-Multi-Domain-Transformer-for-Deformable-Image-Registration"><a href="#FractMorph-A-Fractional-Fourier-Based-Multi-Domain-Transformer-for-Deformable-Image-Registration" class="headerlink" title="FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for   Deformable Image Registration"></a>FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for   Deformable Image Registration</h2><p><strong>Authors:Shayan Kebriti, Shahabedin Nabavi, Ali Gooya</strong></p>
<p>Deformable image registration (DIR) is a crucial and challenging technique for aligning anatomical structures in medical images and is widely applied in diverse clinical applications. However, existing approaches often struggle to capture fine-grained local deformations and large-scale global deformations simultaneously within a unified framework. We present FractMorph, a novel 3D dual-parallel transformer-based architecture that enhances cross-image feature matching through multi-domain fractional Fourier transform (FrFT) branches. Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at fractional angles of 0{\deg}, 45{\deg}, 90{\deg}, along with a log-magnitude branch, to effectively extract local, semi-global, and global features at the same time. These features are fused via cross-attention between the fixed and moving image streams. A lightweight U-Net style network then predicts a dense deformation field from the transformer-enriched features. On the ACDC cardiac MRI dataset, FractMorph achieves state-of-the-art performance with an overall Dice Similarity Coefficient (DSC) of 86.45%, an average per-structure DSC of 75.15%, and a 95th-percentile Hausdorff distance (HD95) of 1.54 mm on our data split. We also introduce FractMorph-Light, a lightweight variant of our model with only 29.6M parameters, which maintains the superior accuracy of the main model while using approximately half the memory. Our results demonstrate that multi-domain spectral-spatial attention in transformers can robustly and efficiently model complex non-rigid deformations in medical images using a single end-to-end network, without the need for scenario-specific tuning or hierarchical multi-scale networks. The source code of our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/shayankebriti/FractMorph">https://github.com/shayankebriti/FractMorph</a>. </p>
<blockquote>
<p>å¯å˜å½¢å›¾åƒé…å‡†ï¼ˆDIRï¼‰æ˜¯åŒ»å­¦å›¾åƒä¸­å¯¹é½è§£å‰–ç»“æ„çš„å…³é”®ä¸”å…·æŒ‘æˆ˜æ€§çš„æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºå„ç§ä¸´åºŠåº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…åŒæ—¶æ•è·ç²¾ç»†çš„å±€éƒ¨å˜å½¢å’Œå¤§è§„æ¨¡çš„å…¨å±€å˜å½¢ã€‚æˆ‘ä»¬æå‡ºäº†FractMorphï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3DåŒå¹³è¡Œtransformerçš„æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡å¤šåŸŸåˆ†æ•°é˜¶å‚…é‡Œå¶å˜æ¢ï¼ˆFrFTï¼‰åˆ†æ”¯å¢å¼ºäº†è·¨å›¾åƒç‰¹å¾åŒ¹é…ã€‚æ¯ä¸ªåˆ†æ•°äº¤å‰æ³¨æ„ï¼ˆFCAï¼‰å—åœ¨0Â°ã€45Â°ã€90Â°çš„åˆ†æ•°è§’åº¦åº”ç”¨å¹¶è¡ŒFrFTï¼Œä»¥åŠä¸€ä¸ªå¯¹æ•°å¹…åº¦åˆ†æ”¯ï¼Œä»¥æœ‰æ•ˆåœ°åŒæ—¶æå–å±€éƒ¨ã€åŠå…¨å±€å’Œå…¨å±€ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾é€šè¿‡å›ºå®šå›¾åƒæµå’Œç§»åŠ¨å›¾åƒæµä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›è¿›è¡Œèåˆã€‚ç„¶åï¼Œä¸€ä¸ªè½»é‡çº§çš„U-Neté£æ ¼çš„ç½‘ç»œä»ä¸°å¯Œçš„transformerç‰¹å¾ä¸­é¢„æµ‹å¯†é›†çš„å˜å½¢åœºã€‚åœ¨ACDCå¿ƒè„MRIæ•°æ®é›†ä¸Šï¼ŒFractMorphè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ•´ä½“Diceç›¸ä¼¼åº¦ç³»æ•°ï¼ˆDSCï¼‰ä¸º86.45%ï¼Œå¹³å‡æ¯ä¸ªç»“æ„çš„DSCä¸º75.15%ï¼Œåœ¨æˆ‘ä»¬çš„æ•°æ®æ‹†åˆ†ä¸Šï¼Œç¬¬95ä¸ªç™¾åˆ†ä½çš„Hausdorffè·ç¦»ï¼ˆHD95ï¼‰ä¸º1.54mmã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†FractMorph-Lightï¼Œè¿™æ˜¯æˆ‘ä»¬æ¨¡å‹çš„è½»é‡çº§å˜ä½“ï¼Œä»…æœ‰29.6Må‚æ•°ï¼Œå®ƒä¿æŒäº†ä¸»æ¨¡å‹çš„ä¼˜è¶Šå‡†ç¡®æ€§ï¼ŒåŒæ—¶å¤§çº¦ä½¿ç”¨äº†ä¸€åŠçš„å†…å­˜ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨transformerä¸­çš„å¤šåŸŸè°±ç©ºé—´æ³¨æ„åŠ›å¯ä»¥ç¨³å¥è€Œæœ‰æ•ˆåœ°ä½¿ç”¨å•ä¸ªç«¯åˆ°ç«¯ç½‘ç»œå¯¹åŒ»å­¦å›¾åƒä¸­çš„å¤æ‚éåˆšæ€§å˜å½¢è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ— éœ€é’ˆå¯¹åœºæ™¯è¿›è¡Œç‰¹å®šè°ƒæ•´æˆ–åˆ†å±‚å¤šå°ºåº¦ç½‘ç»œã€‚æˆ‘ä»¬çš„å®ç°æºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/shayankebriti/FractMorph%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/shayankebriti/FractMorphä¸Šæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12445v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯â€”â€”FractMorphï¼Œå®ƒé‡‡ç”¨3DåŒå¹³è¡Œå˜å‹å™¨æ¶æ„ï¼Œé€šè¿‡å¤šåŸŸåˆ†æ•°å‚…ç«‹å¶å˜æ¢ï¼ˆFrFTï¼‰åˆ†æ”¯å¢å¼ºè·¨å›¾åƒç‰¹å¾åŒ¹é…ã€‚FractMorphèƒ½æœ‰æ•ˆæå–å±€éƒ¨ã€åŠå…¨å±€å’Œå…¨å±€ç‰¹å¾ï¼Œå¹¶åœ¨å•ä¸€ç½‘ç»œä¸­å®ç°å¤æ‚éåˆšæ€§å˜å½¢çš„ç¨³å¥å’Œé«˜æ•ˆå»ºæ¨¡ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œè°ƒæ•´æˆ–åˆ†å±‚å¤šå°ºåº¦ç½‘ç»œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FractMorphæ˜¯ä¸€ç§åŸºäº3DåŒå¹³è¡Œå˜å‹å™¨çš„æ–°å‹åŒ»å­¦å›¾åƒé…å‡†æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡å¤šåŸŸåˆ†æ•°å‚…ç«‹å¶å˜æ¢ï¼ˆFrFTï¼‰åˆ†æ”¯ï¼Œå¢å¼ºè·¨å›¾åƒç‰¹å¾åŒ¹é…ã€‚</li>
<li>FractMorphèƒ½åŒæ—¶æå–å±€éƒ¨ã€åŠå…¨å±€å’Œå…¨å±€ç‰¹å¾ã€‚</li>
<li>FractMorphåœ¨ACDCå¿ƒè„MRIæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>FractMorph-Lightæ˜¯æ¨¡å‹çš„è½»é‡çº§å˜ä½“ï¼Œå‚æ•°å°‘ï¼ŒèŠ‚çœå†…å­˜ï¼ŒåŒæ—¶ä¿æŒäº†ä¸»æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>FractMorphé‡‡ç”¨å•ä¸€ç«¯å¯¹ç«¯ç½‘ç»œï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœºæ™¯è°ƒæ•´æˆ–åˆ†å±‚å¤šå°ºåº¦ç½‘ç»œï¼Œå³å¯å¯¹å¤æ‚éåˆšæ€§å˜å½¢è¿›è¡Œç¨³å¥å’Œé«˜æ•ˆå»ºæ¨¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-133f07b4f672ae02ecd594c6799d26dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8adf8e0e3fa0dace403e9daf20a5da82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba2e11ac23e5ddc048437d554ad4ef6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0c7afd056318a2bdbbb997e499606ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-341caec025542f02bfade6b50bc97547.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SRMA-Mamba-Spatial-Reverse-Mamba-Attention-Network-for-Pathological-Liver-Segmentation-in-MRI-Volumes"><a href="#SRMA-Mamba-Spatial-Reverse-Mamba-Attention-Network-for-Pathological-Liver-Segmentation-in-MRI-Volumes" class="headerlink" title="SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological   Liver Segmentation in MRI Volumes"></a>SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological   Liver Segmentation in MRI Volumes</h2><p><strong>Authors:Jun Zeng, Yannan Huang, Elif Keles, Halil Ertugrul Aktas, Gorkem Durak, Nikhil Kumar Tomar, Quoc-Huy Trinh, Deepak Ranjan Nayak, Ulas Bagci, Debesh Jha</strong></p>
<p>Liver Cirrhosis plays a critical role in the prognosis of chronic liver disease. Early detection and timely intervention are critical in significantly reducing mortality rates. However, the intricate anatomical architecture and diverse pathological changes of liver tissue complicate the accurate detection and characterization of lesions in clinical settings. Existing methods underutilize the spatial anatomical details in volumetric MRI data, thereby hindering their clinical effectiveness and explainability. To address this challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to model the spatial relationships within the complex anatomical structures of MRI volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba), SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and combines anatomical information from the sagittal, coronal, and axial planes to construct a global spatial context representation, enabling efficient volumetric segmentation of pathological liver structures. Furthermore, we introduce the Spatial Reverse Attention module (SRMA), designed to progressively refine cirrhotic details in the segmentation map, utilizing both the coarse segmentation map and hierarchical encoding features. Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation. Our code is available for public: {\color{blue}{<a target="_blank" rel="noopener" href="https://github.com/JunZengz/SRMA-Mamba%7D%7D">https://github.com/JunZengz/SRMA-Mamba}}</a>. </p>
<blockquote>
<p>è‚ç¡¬åŒ–åœ¨æ…¢æ€§è‚ç—…çš„é¢„åä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„æ˜¯æ˜¾è‘—é™ä½æ­»äº¡ç‡çš„å…³é”®ã€‚ç„¶è€Œï¼Œè‚è„ç»„ç»‡çš„å¤æ‚è§£å‰–ç»“æ„å’Œå¤šæ ·åŒ–çš„ç—…ç†å˜åŒ–ä½¿ä¸´åºŠç¯å¢ƒä¸­ç—…å˜çš„å‡†ç¡®æ£€æµ‹å’Œç‰¹å¾æè¿°å˜å¾—å¤æ‚ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä½“ç§¯MRIæ•°æ®ä¸­çš„ç©ºé—´è§£å‰–ç»†èŠ‚ï¼Œä»è€Œå½±å“äº†å®ƒä»¬çš„ä¸´åºŠæ•ˆæœå’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºMambaç½‘ç»œçš„æ–°æ–¹æ³•ï¼Œå³SRMA-Mambaï¼Œæ—¨åœ¨æ¨¡æ‹ŸMRIä½“ç§¯ä¸­å¤æ‚è§£å‰–ç»“æ„å†…çš„ç©ºé—´å…³ç³»ã€‚é€šè¿‡é›†æˆåŸºäºç©ºé—´è§£å‰–çš„Mambaæ¨¡å—ï¼ˆSABMambaï¼‰ï¼ŒSRMA-Mambaåœ¨è‚ç¡¬åŒ–ç»„ç»‡å†…è¿›è¡Œé€‰æ‹©æ€§Mambaæ‰«æï¼Œå¹¶ç»“åˆæ¥è‡ªçŸ¢çŠ¶é¢ã€å† çŠ¶é¢å’Œè½´é¢çš„è§£å‰–ä¿¡æ¯ï¼Œæ„å»ºå…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œä»è€Œå®ç°ç—…ç†æ€§è‚ç»“æ„çš„æœ‰æ•ˆä½“ç§¯åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ç©ºé—´åå‘æ³¨æ„æ¨¡å—ï¼ˆSRMAï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨ç²—åˆ†å‰²å›¾å’Œåˆ†å±‚ç¼–ç ç‰¹å¾ï¼Œé€æ­¥ç²¾ç»†è°ƒæ•´è‚ç¡¬åŒ–ç»†èŠ‚åœ¨åˆ†å‰²å›¾ä¸Šçš„è¡¨ç°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSRMA-Mambaè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨3Dç—…ç†æ€§è‚è„åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š{\color{blue}{<a target="_blank" rel="noopener" href="https://github.com/JunZengz/SRMA-Mamba%7D%7D%E3%80%82">https://github.com/JunZengz/SRMA-Mamba}}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12410v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>    åŸºäºMambaç½‘ç»œçš„æ–°å‹SRMA-Mambaæ¨¡å‹åœ¨è‚è„è‚ç¡¬åŒ–ç»„ç»‡ç—…å˜çš„åˆ†å‰²ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å®ƒé€šè¿‡æ•´åˆç©ºé—´è§£å‰–ç»“æ„ä¿¡æ¯ï¼Œå»ºç«‹å…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸‰ç»´è‚è„ç—…å˜åˆ†å‰²ã€‚åŒæ—¶å¼•å…¥ç©ºé—´åå‘æ³¨æ„åŠ›æ¨¡å—SRMAï¼Œå¯¹ç¡¬åŒ–ç»†èŠ‚è¿›è¡Œæ¸è¿›ç²¾ç»†åŒ–å¤„ç†ã€‚SRMA-Mambaæ€§èƒ½è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œä»£ç å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‚è„è‚ç¡¬åŒ–åœ¨æ…¢æ€§è‚ç—…é¢„åä¸­èµ·å…³é”®ä½œç”¨ï¼Œæ—©æœŸæ£€æµ‹å’ŒåŠæ—¶å¹²é¢„èƒ½æ˜¾è‘—é™ä½æ­»äº¡ç‡ã€‚</li>
<li>è‚è„ç»„ç»‡çš„å¤æ‚è§£å‰–ç»“æ„å’Œå¤šå˜ç—…ç†å˜åŒ–ç»™ä¸´åºŠå‡†ç¡®æ£€æµ‹ä¸è¡¨å¾ç—…å˜å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨MRIæ•°æ®çš„ç©ºé—´è§£å‰–ç»†èŠ‚ï¼Œå½±å“å…¶åœ¨ä¸´åºŠçš„æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>SRMA-Mambaç½‘ç»œé€šè¿‡SABMambaæ¨¡å—å¯¹MRIä½“ç§¯å†…çš„å¤æ‚è§£å‰–ç»“æ„è¿›è¡Œç©ºé—´å…³ç³»å»ºæ¨¡ã€‚</li>
<li>SRMA-Mambaç»“åˆä¸‰ä¸ªå¹³é¢ï¼ˆçŸ¢çŠ¶é¢ã€å† çŠ¶é¢å’Œè½´é¢ï¼‰çš„è§£å‰–å­¦ä¿¡æ¯ï¼Œæ„å»ºå…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œå®ç°è‚è„ç—…å˜ç»“æ„çš„é«˜æ•ˆä¸‰ç»´åˆ†å‰²ã€‚</li>
<li>Spatial Reverse Attentionæ¨¡å—ï¼ˆSRMAï¼‰ç”¨äºé€æ­¥ä¼˜åŒ–ç¡¬åŒ–ç»†èŠ‚ï¼Œç»“åˆç²—åˆ†å‰²å›¾å’Œå±‚æ¬¡ç¼–ç ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜SRMA-Mambaæ€§èƒ½è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œåœ¨ä¸‰ç»´è‚è„ç—…å˜åˆ†å‰²ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbcc3b44ee41f1a271614c38eb31f580.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69186425c99836b908c282b775ed1704.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf0a33f0fe572d94e274670bd493564.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d42d54ab668328775ad1bef64db6d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b430d57bfe2cf3cd17c0e8f41e904f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-interpretable-prediction-of-recurrence-risk-in-breast-cancer-using-pathology-foundation-models"><a href="#Towards-interpretable-prediction-of-recurrence-risk-in-breast-cancer-using-pathology-foundation-models" class="headerlink" title="Towards interpretable prediction of recurrence risk in breast cancer   using pathology foundation models"></a>Towards interpretable prediction of recurrence risk in breast cancer   using pathology foundation models</h2><p><strong>Authors:Jakub R. Kaczmarzyk, Sarah C. Van Alsten, Alyssa J. Cozzo, Rajarsi Gupta, Peter K. Koo, Melissa A. Troester, Katherine A. Hoadley, Joel H. Saltz</strong></p>
<p>Transcriptomic assays such as the PAM50-based ROR-P score guide recurrence risk stratification in non-metastatic, ER-positive, HER2-negative breast cancer but are not universally accessible. Histopathology is routinely available and may offer a scalable alternative. We introduce MAKO, a benchmarking framework evaluating 12 pathology foundation models and two non-pathology baselines for predicting ROR-P scores from H&amp;E-stained whole slide images using attention-based multiple instance learning. Models were trained and validated on the Carolina Breast Cancer Study and externally tested on TCGA BRCA. Several foundation models outperformed baselines across classification, regression, and survival tasks. CONCH achieved the highest ROC AUC, while H-optimus-0 and Virchow2 showed top correlation with continuous ROR-P scores. All pathology models stratified CBCS participants by recurrence similarly to transcriptomic ROR-P. Tumor regions were necessary and sufficient for high-risk predictions, and we identified candidate tissue biomarkers of recurrence. These results highlight the promise of interpretable, histology-based risk models in precision oncology. </p>
<blockquote>
<p>åŸºäºPAM50çš„ROR-Pè¯„åˆ†ç­‰è½¬å½•ç»„æ£€æµ‹å¯¹éè½¬ç§»æ€§ã€ERé˜³æ€§ã€HER2é˜´æ€§ä¹³è…ºç™Œçš„å¤å‘é£é™©åˆ†å±‚å…·æœ‰æŒ‡å¯¼æ„ä¹‰ï¼Œä½†å¹¶éæ™®éé€‚ç”¨ã€‚ç»„ç»‡ç—…ç†å­¦æ˜¯å¸¸è§„å¯ç”¨çš„ï¼Œå¹¶å¯èƒ½æä¾›ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬ä»‹ç»äº†MAKOï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°åŸºå‡†æ¡†æ¶ï¼Œå¯¹12ä¸ªç—…ç†å­¦åŸºç¡€æ¨¡å‹ä»¥åŠä¸¤ä¸ªéç—…ç†å­¦åŸºçº¿è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå®ƒä»¬ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„å¤šå®ä¾‹å­¦ä¹ ä»H&amp;EæŸ“è‰²çš„å…¨å¹»ç¯ç‰‡å›¾åƒä¸­é¢„æµ‹ROR-Pè¯„åˆ†ã€‚è¿™äº›æ¨¡å‹åœ¨å¡ç½—è±çº³ä¹³è…ºç™Œç ”ç©¶ï¼ˆCarolina Breast Cancer Studyï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨TCGA BRCAä¸Šè¿›è¡Œäº†å¤–éƒ¨æµ‹è¯•ã€‚åœ¨åˆ†ç±»ã€å›å½’å’Œç”Ÿå­˜ä»»åŠ¡æ–¹é¢ï¼Œå¤šä¸ªåŸºç¡€æ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚CONCHè·å¾—äº†æœ€é«˜çš„ROC AUCå€¼ï¼Œè€ŒH-optimus-0å’ŒVirchow2ä¸è¿ç»­çš„ROR-Pè¯„åˆ†è¡¨ç°å‡ºè¾ƒé«˜çš„ç›¸å…³æ€§ã€‚æ‰€æœ‰ç—…ç†å­¦æ¨¡å‹å‡èƒ½å°†CBCSå‚ä¸è€…çš„å¤å‘æƒ…å†µä¸è½¬å½•ç»„ROR-Pç›¸ä¼¼åˆ†å±‚ã€‚è‚¿ç˜¤åŒºåŸŸå¯¹äºé«˜é£é™©é¢„æµ‹æ˜¯å¿…è¦ä¸”å……åˆ†çš„ï¼Œæˆ‘ä»¬ç¡®å®šäº†å¤å‘çš„å€™é€‰ç»„ç»‡ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚è¿™äº›ç»“æœçªå‡ºäº†åŸºäºå¯è§£é‡Šç»„ç»‡å­¦çš„é£é™©æ¨¡å‹åœ¨ç²¾å‡†è‚¿ç˜¤å­¦ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12025v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MAKOè¿™ä¸€è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä»·åˆ©ç”¨è‹æœ¨ç²¾å’Œä¼Šçº¢æŸ“è‰²å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹ROR-På¾—åˆ†çš„ç—…ç†åŸºç¡€æ¨¡å‹ã€‚è¯¥ç ”ç©¶åœ¨å¡ç½—è±çº³ä¹³è…ºç™Œç ”ç©¶ä¸­è¿›è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨TCGA BRCAä¸Šè¿›è¡Œå¤–éƒ¨æµ‹è¯•ã€‚ä¸€äº›ç—…ç†åŸºç¡€æ¨¡å‹åœ¨åˆ†ç±»ã€å›å½’å’Œç”Ÿå­˜ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡éç—…ç†åŸºçº¿ã€‚å…¶ä¸­ï¼ŒCONCHæ¨¡å‹ROC AUCæœ€é«˜ï¼ŒH-optimus-0å’ŒVirchow2ä¸è¿ç»­ROR-På¾—åˆ†çš„ç›¸å…³æ€§æœ€é«˜ã€‚è¿™äº›ç—…ç†æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‚¿ç˜¤åŒºåŸŸé¢„æµ‹é«˜é£é™©ï¼Œå¹¶è¯†åˆ«å‡ºå¤å‘å€™é€‰ç»„ç»‡ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†å¯è§£é‡Šçš„ã€åŸºäºç»„ç»‡å­¦çš„é£é™©æ¨¡å‹åœ¨ç²¾å‡†è‚¿ç˜¤å­¦ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨MAKOè¯„ä¼°æ¡†æ¶è¯„ä»·äº†é¢„æµ‹ROR-På¾—åˆ†çš„ç—…ç†åŸºç¡€æ¨¡å‹è¡¨ç°ã€‚</li>
<li>ç ”ç©¶åœ¨å¡ç½—è±çº³ä¹³è…ºç™Œç ”ç©¶ä¸­å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œå¹¶åœ¨TCGA BRCAä¸Šè¿›è¡Œå¤–éƒ¨æµ‹è¯•ã€‚</li>
<li>æŸäº›ç—…ç†åŸºç¡€æ¨¡å‹åœ¨åˆ†ç±»ã€å›å½’å’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºéç—…ç†åŸºçº¿ã€‚</li>
<li>CONCHæ¨¡å‹ROC AUCæœ€é«˜ï¼ŒH-optimus-0å’ŒVirchow2ä¸ROR-På¾—åˆ†çš„è¿ç»­æ€§ç›¸å…³æ€§æœ€å¼ºã€‚</li>
<li>ç—…ç†æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è‚¿ç˜¤åŒºåŸŸè¿›è¡Œé«˜é£é™©é¢„æµ‹ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„é¢„æµ‹ä¾æ®ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†ä¸è‚¿ç˜¤å¤å‘ç›¸å…³çš„ç»„ç»‡ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-395a0ef292f0fa5e681de9575aedaa63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6152cdb56bf0762779779546b0eddf9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TimeMachine-Fine-Grained-Facial-Age-Editing-with-Identity-Preservation"><a href="#TimeMachine-Fine-Grained-Facial-Age-Editing-with-Identity-Preservation" class="headerlink" title="TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation"></a>TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation</h2><p><strong>Authors:Yilin Mi, Qixin Yan, Zheng-Peng Duan, Chunle Guo, Hubery Yin, Hao Liu, Chen Li, Chongyi Li</strong></p>
<p>With the advancement of generative models, facial image editing has made significant progress. However, achieving fine-grained age editing while preserving personal identity remains a challenging task. In this paper, we propose TimeMachine, a novel diffusion-based framework that achieves accurate age editing while keeping identity features unchanged. To enable fine-grained age editing, we inject high-precision age information into the multi-cross attention module, which explicitly separates age-related and identity-related features. This design facilitates more accurate disentanglement of age attributes, thereby allowing precise and controllable manipulation of facial aging. Furthermore, we propose an Age Classifier Guidance (ACG) module that predicts age directly in the latent space, instead of performing denoising image reconstruction during training. By employing a lightweight module to incorporate age constraints, this design enhances age editing accuracy by modest increasing training cost. Additionally, to address the lack of large-scale, high-quality facial age datasets, we construct a HFFA dataset (High-quality Fine-grained Facial-Age dataset) which contains one million high-resolution images labeled with identity and facial attributes. Experimental results demonstrate that TimeMachine achieves state-of-the-art performance in fine-grained age editing while preserving identity consistency. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œé¢éƒ¨å›¾åƒç¼–è¾‘å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨ä¿æŒä¸ªäººèº«ä»½çš„åŒæ—¶å®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TimeMachineï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒèº«ä»½ç‰¹å¾ä¸å˜çš„åŒæ—¶å®ç°ç²¾ç¡®çš„å¹´é¾„ç¼–è¾‘ã€‚ä¸ºäº†å®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ï¼Œæˆ‘ä»¬å°†é«˜ç²¾åº¦å¹´é¾„ä¿¡æ¯æ³¨å…¥å¤šäº¤å‰æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—æ˜¾å¼åœ°åˆ†ç¦»å¹´é¾„ç›¸å…³å’Œèº«ä»½ç›¸å…³çš„ç‰¹å¾ã€‚è¿™ç§è®¾è®¡ä¿ƒè¿›äº†å¹´é¾„å±æ€§çš„æ›´ç²¾ç¡®åˆ†ç¦»ï¼Œä»è€Œå…è®¸ç²¾ç¡®ä¸”å¯æ§çš„é¢éƒ¨è¡°è€æ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¹´é¾„åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆACGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å¹´é¾„ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå»å™ªå›¾åƒé‡å»ºã€‚é€šè¿‡é‡‡ç”¨è½»é‡çº§æ¨¡å—æ¥èå…¥å¹´é¾„çº¦æŸï¼Œè¿™ç§è®¾è®¡åœ¨é€‚åº¦å¢åŠ è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹æé«˜äº†å¹´é¾„ç¼–è¾‘çš„å‡†ç¡®æ€§ã€‚å¦å¤–ï¼Œä¸ºäº†è§£å†³ç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡é¢éƒ¨å¹´é¾„æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†HFFAæ•°æ®é›†ï¼ˆé«˜è´¨é‡ç²¾ç»†é¢éƒ¨å¹´é¾„æ•°æ®é›†ï¼‰ï¼Œå…¶ä¸­åŒ…å«ä¸€åƒä¸‡å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶æ ‡æœ‰èº«ä»½å’Œé¢éƒ¨å±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeMachineåœ¨ç²¾ç»†å¹´é¾„ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11284v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œé¢éƒ¨å›¾åƒç¼–è¾‘å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä¿æŒä¸ªäººèº«ä»½çš„åŒæ—¶å®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†TimeMachineï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹æ¡†æ¶ï¼Œå¯åœ¨å‡†ç¡®ç¼–è¾‘å¹´é¾„çš„åŒæ—¶ä¿æŒèº«ä»½ç‰¹å¾ä¸å˜ã€‚é€šè¿‡å‘å¤šäº¤å‰æ³¨æ„æ¨¡å—æ³¨å…¥é«˜ç²¾åº¦å¹´é¾„ä¿¡æ¯ï¼Œå®ç°å¹´é¾„å’Œèº«ä»½ç‰¹å¾çš„æ˜¾å¼åˆ†ç¦»ï¼Œä»è€Œä¿ƒè¿›å¹´é¾„å±æ€§çš„ç²¾ç¡®è§£è€¦ï¼Œä»è€Œå®ç°é¢éƒ¨è¡°è€çš„ç²¾ç¡®å¯æ§æ“ä½œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¹´é¾„åˆ†ç±»å™¨å¼•å¯¼ï¼ˆACGï¼‰æ¨¡å—ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å¹´é¾„ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå»å™ªå›¾åƒé‡å»ºã€‚é€šè¿‡é‡‡ç”¨è½»é‡çº§æ¨¡å—æ¥èå…¥å¹´é¾„çº¦æŸï¼Œè¿™ç§è®¾è®¡åœ¨é€‚åº¦å¢åŠ è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹æé«˜äº†å¹´é¾„ç¼–è¾‘çš„å‡†ç¡®æ€§ã€‚å¦å¤–ï¼Œä¸ºè§£å†³ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡é¢éƒ¨å¹´é¾„æ•°æ®é›†çš„é—®é¢˜ï¼Œæ„å»ºäº†HFFAæ•°æ®é›†ï¼ˆé«˜è´¨é‡ç²¾ç»†é¢éƒ¨å¹´é¾„æ•°æ®é›†ï¼‰ï¼ŒåŒ…å«ä¸€åƒä¸‡å¼ å¸¦æœ‰èº«ä»½å’Œé¢éƒ¨å±æ€§æ ‡ç­¾çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeMachineåœ¨ç²¾ç»†å¹´é¾„ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†é¢éƒ¨å›¾åƒç¼–è¾‘çš„å‘å±•ã€‚</li>
<li>TimeMachineæ¡†æ¶å®ç°äº†åœ¨ä¿æŒèº«ä»½ç‰¹å¾ä¸å˜çš„æƒ…å†µä¸‹å‡†ç¡®ç¼–è¾‘å¹´é¾„ã€‚</li>
<li>é€šè¿‡å¤šäº¤å‰æ³¨æ„æ¨¡å—æ³¨å…¥é«˜ç²¾åº¦å¹´é¾„ä¿¡æ¯ï¼Œå®ç°å¹´é¾„å’Œèº«ä»½ç‰¹å¾çš„æ˜¾å¼åˆ†ç¦»ã€‚</li>
<li>æå‡ºAge Classifier Guidanceï¼ˆACGï¼‰æ¨¡å—ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å¹´é¾„ã€‚</li>
<li>ACGæ¨¡å—çš„è®¾è®¡é€šè¿‡é€‚åº¦å¢åŠ è®­ç»ƒæˆæœ¬æé«˜äº†å¹´é¾„ç¼–è¾‘çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ„å»ºäº†HFFAæ•°æ®é›†ï¼ŒåŒ…å«ä¸€åƒä¸‡å¼ å¸¦æœ‰èº«ä»½å’Œé¢éƒ¨å±æ€§æ ‡ç­¾çš„é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œç”¨äºé¢éƒ¨å¹´é¾„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-516f3b0bbb636e92f4fb2a524e97f58e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32ea9c7b09d4b6b4501b99e88fea929c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bebdea02a1b36091dd26db7a47b9c7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e45314c4c0f5782bade086b4dbc73806.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c77e555dbc8477c2532120987686f1b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation"><a href="#From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation" class="headerlink" title="From Intent to Execution: Multimodal Chain-of-Thought Reinforcement   Learning for Precise CAD Code Generation"></a>From Intent to Execution: Multimodal Chain-of-Thought Reinforcement   Learning for Precise CAD Code Generation</h2><p><strong>Authors:Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue</strong></p>
<p>Computer-Aided Design (CAD) plays a vital role in engineering and manufacturing, yet current CAD workflows require extensive domain expertise and manual modeling effort. Recent advances in large language models (LLMs) have made it possible to generate code from natural language, opening new opportunities for automating parametric 3D modeling. However, directly translating human design intent into executable CAD code remains highly challenging, due to the need for logical reasoning, syntactic correctness, and numerical precision. In this work, we propose CAD-RL, a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework for CAD modeling code generation. Our method combines CoT-based Cold Start with goal-driven reinforcement learning post training using three task-specific rewards: executability reward, geometric accuracy reward, and external evaluation reward. To ensure stable policy learning under sparse and high-variance reward conditions, we introduce three targeted optimization strategies: Trust Region Stretch for improved exploration, Precision Token Loss for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce noisy supervision. To support training and benchmarking, we release ExeCAD, a noval dataset comprising 16,540 real-world CAD examples with paired natural language and structured design language descriptions, executable CADQuery scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç„¶è€Œå½“å‰çš„CADå·¥ä½œæµç¨‹éœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨å»ºæ¨¡å·¥ä½œã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆä»£ç æˆä¸ºå¯èƒ½ï¼Œä¸ºè‡ªåŠ¨åŒ–å‚æ•°åŒ–3Då»ºæ¨¡å¸¦æ¥äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œç›´æ¥å°†äººç±»çš„è®¾è®¡æ„å›¾è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„CADä»£ç ä»ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™éœ€è¦è¿›è¡Œé€»è¾‘æ¨ç†ã€è¯­æ³•æ­£ç¡®æ€§å’Œæ•°å€¼ç²¾ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-RLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºCADå»ºæ¨¡ä»£ç ç”Ÿæˆçš„å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰å¼•å¯¼å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºCoTçš„å†·å¯åŠ¨å’ŒåŸºäºç›®æ ‡å¯¼å‘çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œä½¿ç”¨ä¸‰ç§ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±ï¼šå¯æ‰§è¡Œæ€§å¥–åŠ±ã€å‡ ä½•ç²¾åº¦å¥–åŠ±å’Œå¤–éƒ¨è¯„ä¼°å¥–åŠ±ã€‚ä¸ºäº†ç¡®ä¿åœ¨ç¨€ç–å’Œé«˜æ–¹å·®å¥–åŠ±æ¡ä»¶ä¸‹çš„ç­–ç•¥å­¦ä¹ ç¨³å®šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ç­–ç•¥ï¼šä¿¡ä»»åŒºåŸŸæ‰©å±•ä»¥æ”¹å–„æ¢ç´¢èƒ½åŠ›ï¼Œç²¾ç¡®ä»¤ç‰Œä¸¢å¤±ä»¥å¢å¼ºç»´åº¦å‚æ•°ç²¾åº¦ï¼Œä»¥åŠé•¿è¿‡æ»¤ä»¥å‡å°‘å™ªå£°ç›‘ç£ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ExeCADæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«16540ä¸ªç°å®ä¸–ç•Œçš„CADç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½é…æœ‰è‡ªç„¶è¯­è¨€æè¿°å’Œç»“æ„åŒ–è®¾è®¡è¯­è¨€æè¿°ã€å¯æ‰§è¡Œçš„CADQueryè„šæœ¬ä»¥åŠæ¸²æŸ“çš„3Dæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾åº¦å’Œä»£ç å¯æ‰§è¡Œæ€§æ–¹é¢ï¼ŒCAD-RLç›¸æ¯”ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10118v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚éœ€è¦æ·±åšçš„é¢†åŸŸçŸ¥è¯†å’Œæ‰‹åŠ¨å»ºæ¨¡ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºè‡ªåŠ¨åŒ–å‚æ•°åŒ–ä¸‰ç»´å»ºæ¨¡æä¾›äº†æ–°çš„æœºä¼šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„CADå»ºæ¨¡ä»£ç ç”Ÿæˆæ–¹æ³•â€”â€”CAD-RLï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰çš„å†·å¯åŠ¨å’Œé¢å‘ç›®æ ‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œé‡‡ç”¨ä¸‰ç§ç‰¹å®šä»»åŠ¡å¥–åŠ±æ¥ç¡®ä¿ä»£ç çš„å¯æ‰§è¡Œæ€§ã€å‡ ä½•ç²¾åº¦å’Œå¤–éƒ¨è¯„ä»·ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¨³å®šç­–ç•¥å­¦ä¹ å¹¶å¼•å…¥ä¸‰é¡¹æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ç­–ç•¥æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ”¯æŒè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ï¼Œå‘å¸ƒäº†ExeCADæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒCAD-RLåœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾åº¦å’Œä»£ç æ‰§è¡Œæ–¹é¢ç›¸è¾ƒäºç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADè®¾è®¡åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸­èµ·é‡è¦ä½œç”¨ï¼Œä½†æ‰‹åŠ¨å»ºæ¨¡å’Œæ·±åšçš„é¢†åŸŸçŸ¥è¯†éœ€æ±‚ä½¿å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºè‡ªåŠ¨åŒ–å‚æ•°åŒ–ä¸‰ç»´å»ºæ¨¡æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>CAD-RLæ–¹æ³•ç»“åˆäº†åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰çš„å†·å¯åŠ¨å’Œé¢å‘ç›®æ ‡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>CAD-RLé‡‡ç”¨ä¸‰ç§ç‰¹å®šä»»åŠ¡å¥–åŠ±ç¡®ä¿ä»£ç çš„å¯æ‰§è¡Œæ€§ã€å‡ ä½•ç²¾åº¦å’Œå¤–éƒ¨è¯„ä»·ã€‚</li>
<li>ä¸ºç¨³å®šç­–ç•¥å­¦ä¹ ï¼Œå¼•å…¥äº†ä¸‰é¡¹ä¼˜åŒ–ç­–ç•¥ï¼šTrust Region Stretchã€Precision Token Losså’ŒOverlong Filteringã€‚</li>
<li>å‘å¸ƒäº†ä¸€ä¸ªåä¸ºExeCADçš„æ–°æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«é…å¯¹è‡ªç„¶è¯­è¨€æè¿°å’Œç»“æ„åŒ–è®¾è®¡è¯­è¨€çš„çœŸå®CADç¤ºä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d85443476044b4ab68d36d4fd5db0c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ff1bbd8251ef77aa63e82f40ea57363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edf1b249ffe6b6397d7803fa47ff2769.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f4e6552499ff5ba0a1a1295218a5604.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bridging-AI-Innovation-and-Healthcare-Needs-Lessons-Learned-from-Incorporating-Modern-NLP-at-The-BC-Cancer-Registry"><a href="#Bridging-AI-Innovation-and-Healthcare-Needs-Lessons-Learned-from-Incorporating-Modern-NLP-at-The-BC-Cancer-Registry" class="headerlink" title="Bridging AI Innovation and Healthcare Needs: Lessons Learned from   Incorporating Modern NLP at The BC Cancer Registry"></a>Bridging AI Innovation and Healthcare Needs: Lessons Learned from   Incorporating Modern NLP at The BC Cancer Registry</h2><p><strong>Authors:Lovedeep Gondara, Gregory Arbour, Raymond Ng, Jonathan Simkin, Shebnum Devji</strong></p>
<p>Automating data extraction from clinical documents offers significant potential to improve efficiency in healthcare settings, yet deploying Natural Language Processing (NLP) solutions presents practical challenges. Drawing upon our experience implementing various NLP models for information extraction and classification tasks at the British Columbia Cancer Registry (BCCR), this paper shares key lessons learned throughout the project lifecycle. We emphasize the critical importance of defining problems based on clear business objectives rather than solely technical accuracy, adopting an iterative approach to development, and fostering deep interdisciplinary collaboration and co-design involving domain experts, end-users, and ML specialists from inception. Further insights highlight the need for pragmatic model selection (including hybrid approaches and simpler methods where appropriate), rigorous attention to data quality (representativeness, drift, annotation), robust error mitigation strategies involving human-in-the-loop validation and ongoing audits, and building organizational AI literacy. These practical considerations, generalizable beyond cancer registries, provide guidance for healthcare organizations seeking to successfully implement AI&#x2F;NLP solutions to enhance data management processes and ultimately improve patient care and public health outcomes. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ä»ä¸´åºŠæ–‡æ¡£ä¸­æå–æ•°æ®ï¼Œåœ¨åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­å…·æœ‰æ˜¾è‘—æé«˜æ•ˆç‡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œéƒ¨ç½²è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è§£å†³æ–¹æ¡ˆå´å­˜åœ¨å®é™…æŒ‘æˆ˜ã€‚æœ¬æ–‡åŸºäºæˆ‘ä»¬åœ¨ä¸åˆ—é¢ å“¥ä¼¦æ¯”äºšçœç™Œç—‡ç™»è®°å¤„ï¼ˆBCCRï¼‰ä¸ºä¿¡æ¯æå–å’Œåˆ†ç±»ä»»åŠ¡å®æ–½å„ç§NLPæ¨¡å‹çš„å®è·µç»éªŒï¼Œåˆ†äº«é¡¹ç›®ç”Ÿå‘½å‘¨æœŸä¸­å¸å–çš„å…³é”®æ•™è®­ã€‚æˆ‘ä»¬å¼ºè°ƒï¼Œæ ¹æ®æ˜ç¡®çš„ä¸šåŠ¡ç›®æ ‡è€Œä¸æ˜¯å•çº¯çš„æŠ€æœ¯å‡†ç¡®æ€§æ¥å®šä¹‰é—®é¢˜è‡³å…³é‡è¦ï¼Œé‡‡ç”¨è¿­ä»£å¼€å‘æ–¹æ³•ï¼Œå¹¶ä»ä¸€å¼€å§‹å°±ä¿ƒè¿›è·¨å­¦ç§‘çš„æ·±åº¦åä½œå’ŒååŒè®¾è®¡ï¼Œæ¶‰åŠé¢†åŸŸä¸“å®¶ã€æœ€ç»ˆç”¨æˆ·å’Œæœºå™¨å­¦ä¹ ä¸“å®¶ã€‚è¿›ä¸€æ­¥çš„è§è§£å¼ºè°ƒäº†å®ç”¨æ¨¡å‹é€‰æ‹©çš„éœ€è¦ï¼ˆåŒ…æ‹¬æ··åˆæ–¹æ³•å’Œé€‚å½“æƒ…å†µä¸‹çš„ç®€åŒ–æ–¹æ³•ï¼‰ï¼Œå¯¹æ•°æ®è´¨é‡ï¼ˆä»£è¡¨æ€§ã€æ¼‚ç§»ã€æ³¨é‡Šï¼‰çš„ä¸¥æ ¼å…³æ³¨ï¼Œæ¶‰åŠäººæœºå¾ªç¯éªŒè¯å’ŒæŒç»­å®¡è®¡çš„ç¨³å¥é”™è¯¯ç¼“è§£ç­–ç•¥ï¼Œä»¥åŠæé«˜ç»„ç»‡çš„äººå·¥æ™ºèƒ½ç´ å…»ã€‚è¿™äº›å®é™…è€ƒè™‘å› ç´ ï¼Œå¯æ¨å¹¿åº”ç”¨äºç™Œç—‡ç™»è®°ä»¥å¤–çš„é¢†åŸŸï¼Œä¸ºå¯»æ±‚æˆåŠŸå®æ–½äººå·¥æ™ºèƒ½&#x2F;è‡ªç„¶è¯­è¨€å¤„ç†è§£å†³æ–¹æ¡ˆä»¥æ”¹è¿›æ•°æ®ç®¡ç†æµç¨‹å¹¶æœ€ç»ˆæ”¹å–„æ‚£è€…æŠ¤ç†å’Œå…¬å…±å«ç”Ÿç»“æœçš„åŒ»ç–—ä¿å¥ç»„ç»‡æä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09991v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨åŒ–ä¸´åºŠæ–‡æ¡£æ•°æ®æå–åœ¨æå‡åŒ»ç–—æ•ˆç‡æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨éƒ¨ç½²è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è§£å†³æ–¹æ¡ˆæ—¶é¢ä¸´å®é™…æŒ‘æˆ˜ã€‚æœ¬æ–‡ä¾æ‰˜åœ¨åŠ æ‹¿å¤§ä¸åˆ—é¢ å“¥ä¼¦æ¯”äºšçœç™Œç—‡ç™»è®°å¤„ï¼ˆBCCRï¼‰å®æ–½å„ç§NLPæ¨¡å‹è¿›è¡Œä¿¡æ¯æå–å’Œåˆ†ç±»ä»»åŠ¡çš„ç»éªŒï¼Œåˆ†äº«é¡¹ç›®ç”Ÿå‘½å‘¨æœŸä¸­çš„é‡è¦æ•™è®­ã€‚å¼ºè°ƒä»¥æ˜ç¡®çš„ä¸šåŠ¡ç›®æ ‡æ¥å®šä¹‰é—®é¢˜çš„é‡è¦æ€§è€Œéå•çº¯è¿½æ±‚æŠ€æœ¯å‡†ç¡®æ€§ï¼Œé‡‡å–è¿­ä»£å¼å¼€å‘æ–¹æ³•ï¼Œå¹¶ä»ä¸€å¼€å§‹å°±ä¿ƒè¿›è·¨å­¦ç§‘åˆä½œå’Œè®¾è®¡ï¼ŒåŒ…æ‹¬é¢†åŸŸä¸“å®¶ã€ç»ˆç«¯ç”¨æˆ·å’Œæœºå™¨å­¦ä¹ ä¸“å®¶çš„å‚ä¸ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼ºè°ƒäº†é€‰æ‹©å®ç”¨æ¨¡å‹ï¼ˆåŒ…æ‹¬æ··åˆæ–¹æ³•å’Œé€‚å½“ç®€åŒ–æ–¹æ³•ï¼‰ã€ä¸¥æ ¼å…³æ³¨æ•°æ®è´¨é‡ï¼ˆä»£è¡¨æ€§ã€æ¼‚ç§»ã€æ³¨é‡Šï¼‰ã€æ„å»ºåŒ…å«äººä¸ºå¾ªç¯éªŒè¯å’ŒæŒç»­å®¡è®¡çš„é”™è¯¯ç¼“è§£ç­–ç•¥ä»¥åŠæé«˜ç»„ç»‡çš„äººå·¥æ™ºèƒ½ç´ å…»çš„é‡è¦æ€§ã€‚è¿™äº›å®é™…è€ƒé‡å› ç´ å¯¹äºå¯»æ±‚æˆåŠŸå®æ–½äººå·¥æ™ºèƒ½&#x2F;è‡ªç„¶è¯­è¨€å¤„ç†è§£å†³æ–¹æ¡ˆä»¥æ”¹å–„æ•°æ®ç®¡ç†æµç¨‹å¹¶æœ€ç»ˆæ”¹å–„æ‚£è€…æŠ¤ç†å’Œå…¬å…±å«ç”Ÿç»“æœçš„åŒ»ç–—æœºæ„å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®šä¹‰é—®é¢˜æ—¶éœ€ç»“åˆæ˜ç¡®çš„ä¸šåŠ¡ç›®æ ‡ï¼Œè€Œéä»…è¿½æ±‚æŠ€æœ¯å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨è¿­ä»£å¼å¼€å‘æ–¹æ³•ï¼Œä¿ƒè¿›è·¨å­¦ç§‘åˆä½œå’Œå‚ä¸ã€‚</li>
<li>éœ€è¦æ ¹æ®å®é™…é—®é¢˜å’Œç¯å¢ƒé€‰æ‹©é€‚åˆçš„NLPæ¨¡å‹å’Œæ–¹æ³•ã€‚</li>
<li>ä¸¥æ ¼å…³æ³¨æ•°æ®è´¨é‡ï¼ŒåŒ…æ‹¬æ•°æ®çš„ä»£è¡¨æ€§ã€æ¼‚ç§»å’Œæ³¨é‡Šã€‚</li>
<li>å®æ–½åŒ…å«äººä¸ºå¾ªç¯éªŒè¯å’ŒæŒç»­å®¡è®¡çš„é”™è¯¯ç¼“è§£ç­–ç•¥ã€‚</li>
<li>å¼ºè°ƒäººå·¥æ™ºèƒ½åœ¨ç»„ç»‡ä¸­çš„åº”ç”¨å’Œæ¨å¹¿çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b416a71bab06c2ae0686d53cce8540a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NEUBORN-The-Neurodevelopmental-Evolution-framework-Using-BiOmechanical-RemodelliNg"><a href="#NEUBORN-The-Neurodevelopmental-Evolution-framework-Using-BiOmechanical-RemodelliNg" class="headerlink" title="NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical   RemodelliNg"></a>NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical   RemodelliNg</h2><p><strong>Authors:Nashira Baena, Mariana da Silva, Irina Grigorescu, Aakash Saboo, Saga Masui, Jaques-Donald Tournier, Emma C. Robinson</strong></p>
<p>Understanding individual cortical development is essential for identifying deviations linked to neurodevelopmental disorders. However, current normative modelling frameworks struggle to capture fine-scale anatomical details due to their reliance on modelling data within a population-average reference space. Here, we present a novel framework for learning individual growth trajectories from biomechanically constrained, longitudinal, diffeomorphic image registration, implemented via a hierarchical network architecture. Trained on neonatal MRI data from the Developing Human Connectome Project, the method improves the biological plausibility of warps, generating growth trajectories that better follow population-level trends while generating smoother warps, with fewer negative Jacobians, relative to state-of-the-art baselines. The resulting subject-specific deformations provide interpretable, biologically grounded mappings of development. This framework opens new possibilities for predictive modeling of brain maturation and early identification of malformations of cortical development. </p>
<blockquote>
<p>äº†è§£ä¸ªä½“çš®å±‚å‘è‚²å¯¹äºè¯†åˆ«ä¸ç¥ç»å‘è‚²éšœç¢ç›¸å…³çš„åå·®è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§„èŒƒæ€§å»ºæ¨¡æ¡†æ¶ç”±äºä¾èµ–äºåœ¨äººç¾¤å¹³å‡å‚è€ƒç©ºé—´å†…è¿›è¡Œå»ºæ¨¡æ•°æ®ï¼Œéš¾ä»¥æ•è·ç²¾ç»†çš„è§£å‰–ç»“æ„ç»†èŠ‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å±‚æ¬¡ç½‘ç»œæ¶æ„å®ç°ï¼Œé€šè¿‡ç”Ÿç‰©åŠ›å­¦çº¦æŸã€çºµå‘å’Œå½¢æ€å›¾åƒé…å‡†æ¥å­¦ä¹ ä¸ªä½“ç”Ÿé•¿è½¨è¿¹ã€‚è¯¥æ–¹æ³•åœ¨å‘å±•ä¸­å›½å®¶äººç±»è¿æ¥ç»„è®¡åˆ’çš„æ–°ç”Ÿå„¿MRIæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†å˜å½¢åœºçš„ç”Ÿç‰©å­¦åˆç†æ€§ï¼Œç”Ÿæˆçš„ç”Ÿé•¿è½¨è¿¹èƒ½æ›´å¥½åœ°éµå¾ªäººç¾¤æ°´å¹³è¶‹åŠ¿ï¼ŒåŒæ—¶ç”Ÿæˆæ›´å¹³æ»‘çš„å˜å½¢åœºï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”å…·æœ‰æ›´å°‘çš„è´Ÿé›…å¯æ¯”è¡Œåˆ—å¼ã€‚è¿™äº›ç‰¹å®šçš„ä¸»ä½“å˜å½¢æä¾›äº†å¯è§£é‡Šçš„ã€ä»¥ç”Ÿç‰©å­¦ä¸ºåŸºç¡€çš„å‘è‚²æ˜ å°„ã€‚è¯¥æ¡†æ¶ä¸ºé¢„æµ‹å¤§è„‘æˆç†Ÿæ¨¡å‹å’Œæ—©æœŸè¯†åˆ«çš®å±‚å‘è‚²ç•¸å½¢æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09757v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºé€šè¿‡åŸºäºç”Ÿç‰©åŠ›å­¦çº¦æŸçš„çºµå‘å¾®åˆ†å›¾åƒé…å‡†æŠ€æœ¯å­¦ä¹ ä¸ªä½“ç”Ÿé•¿è½¨è¿¹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å±‚æ¬¡ç½‘ç»œæ¶æ„å®ç°ï¼Œç»è¿‡å¯¹å‘è‚²äººç±»è¿æ¥ç»„é¡¹ç›®ä¸­çš„æ–°ç”Ÿå„¿MRIæ•°æ®çš„è®­ç»ƒï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆäººå£æ°´å¹³è¶‹åŠ¿çš„å¹³æ»‘å˜å½¢æ˜ å°„ï¼Œæé«˜å˜å½¢æ˜ å°„çš„ç”Ÿç‰©åˆç†æ€§ã€‚è¿™ä¸ºé¢„æµ‹å¤§è„‘æˆç†Ÿå’Œæ—©æœŸè¯†åˆ«çš®è´¨å‘è‚²ç•¸å½¢æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§„èŒƒæ€§å»ºæ¨¡æ¡†æ¶å› ä¾èµ–ç¾¤ä½“å¹³å‡å‚è€ƒç©ºé—´è€Œæ— æ³•æ•æ‰ç²¾ç»†çš„è§£å‰–ç»†èŠ‚ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼ŒåŸºäºç”Ÿç‰©åŠ›å­¦çº¦æŸçš„çºµå‘å¾®åˆ†å›¾åƒé…å‡†æŠ€æœ¯æ¥å­¦ä¹ ä¸ªä½“ç”Ÿé•¿è½¨è¿¹ã€‚</li>
<li>æ–°æ¡†æ¶é‡‡ç”¨å±‚æ¬¡ç½‘ç»œæ¶æ„å®ç°ï¼Œç»è¿‡æ–°ç”Ÿå„¿MRIæ•°æ®çš„è®­ç»ƒã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•ç”Ÿæˆçš„å˜å½¢æ˜ å°„æ›´åŠ ç¬¦åˆäººå£æ°´å¹³è¶‹åŠ¿ï¼Œå¹¶ä¸”æ›´åŠ å¹³æ»‘ã€‚</li>
<li>æ–°æ–¹æ³•æé«˜äº†å˜å½¢æ˜ å°„çš„ç”Ÿç‰©åˆç†æ€§ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šçš„ã€åŸºäºç”Ÿç‰©å­¦çš„å‘è‚²æ˜ å°„ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºé¢„æµ‹å¤§è„‘æˆç†Ÿæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e63bf86ba2dfbf8fe3a72b2bcedae1a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5730fc88cb58e17dcac11263bcd80dd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8beb64cbc98a4740bb8a70171764b1b6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation"><a href="#Unified-and-Semantically-Grounded-Domain-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="Unified and Semantically Grounded Domain Adaptation for Medical Image   Segmentation"></a>Unified and Semantically Grounded Domain Adaptation for Medical Image   Segmentation</h2><p><strong>Authors:Xin Wang, Yin Guo, Jiamin Xia, Kaiyu Zhang, Niranjan Balu, Mahmud Mossa-Basha, Linda Shapiro, Chun Yuan</strong></p>
<p>Most prior unsupervised domain adaptation approaches for medical image segmentation are narrowly tailored to either the source-accessible setting, where adaptation is guided by source-target alignment, or the source-free setting, which typically resorts to implicit supervision mechanisms such as pseudo-labeling and model distillation. This substantial divergence in methodological designs between the two settings reveals an inherent flaw: the lack of an explicit, structured construction of anatomical knowledge that naturally generalizes across domains and settings. To bridge this longstanding divide, we introduce a unified, semantically grounded framework that supports both source-accessible and source-free adaptation. Fundamentally distinct from all prior works, our frameworkâ€™s adaptability emerges naturally as a direct consequence of the model architecture, without the need for any handcrafted adaptation strategies. Specifically, our model learns a domain-agnostic probabilistic manifold as a global space of anatomical regularities, mirroring how humans establish visual understanding. Thus, the structural content in each image can be interpreted as a canonical anatomy retrieved from the manifold and a spatial transformation capturing individual-specific geometry. This disentangled, interpretable formulation enables semantically meaningful prediction with intrinsic adaptability. Extensive experiments on challenging cardiac and abdominal datasets show that our framework achieves state-of-the-art results in both settings, with source-free performance closely approaching its source-accessible counterpart, a level of consistency rarely observed in prior works. Beyond quantitative improvement, we demonstrate strong interpretability of the proposed framework via manifold traversal for smooth shape manipulation. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œå¤§å¤šæ•°å…ˆå‰çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•ä¸»è¦å±€é™äºæºå¯è®¿é—®è®¾ç½®ï¼Œå…¶ä¸­è‡ªé€‚åº”ç”±æºç›®æ ‡å¯¹é½å¼•å¯¼ï¼Œæˆ–è€…æ— æºçš„è‡ªç”±è®¾ç½®ï¼Œè¿™é€šå¸¸ä¾èµ–äºä¼ªæ ‡ç­¾å’Œæ¨¡å‹è’¸é¦ç­‰éšå¼ç›‘ç£æœºåˆ¶ã€‚è¿™ä¸¤ç§è®¾ç½®ä¸­çš„æ–¹æ³•è®¾è®¡å­˜åœ¨æ˜æ˜¾çš„åˆ†æ­§ï¼Œæš´éœ²å‡ºå†…åœ¨ç¼ºé™·ï¼šç¼ºä¹æ˜ç¡®çš„ã€ç»“æ„åŒ–æ„å»ºçš„è§£å‰–å­¦çŸ¥è¯†ï¼Œè¿™ç§çŸ¥è¯†èƒ½å¤Ÿè‡ªç„¶åœ°è·¨åŸŸå’Œè®¾ç½®è¿›è¡Œæ¨å¹¿ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€é•¿æœŸå­˜åœ¨çš„åˆ†æ­§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„ã€è¯­ä¹‰åŸºç¡€æ¡†æ¶ï¼Œæ”¯æŒæºå¯è®¿é—®å’Œæ— æºè‡ªé€‚åº”ã€‚ä¸æ‰€æœ‰å…ˆå‰çš„å·¥ä½œæ ¹æœ¬ä¸åŒï¼Œæˆ‘ä»¬æ¡†æ¶çš„é€‚åº”æ€§æ˜¯æ¨¡å‹æ¶æ„çš„ç›´æ¥ç»“æœï¼Œæ— éœ€ä»»ä½•æ‰‹å·¥å®šåˆ¶çš„è‡ªé€‚åº”ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ ä¸€ä¸ªåŸŸæ— å…³çš„æ¦‚ç‡æµå½¢ä½œä¸ºè§£å‰–è§„å¾‹çš„å…¨å±€ç©ºé—´ï¼Œåæ˜ äººç±»å»ºç«‹è§†è§‰ç†è§£çš„æ–¹å¼ã€‚å› æ­¤ï¼Œæ¯ä¸ªå›¾åƒçš„ç»“æ„å†…å®¹å¯ä»¥è¢«è§£é‡Šä¸ºä»æµå½¢ä¸­æ£€ç´¢çš„è§„èŒƒè§£å‰–å­¦ä»¥åŠæ•è·ä¸ªä½“ç‰¹å®šå‡ ä½•çš„ç©ºé—´å˜æ¢ã€‚è¿™ç§è§£è€¦ã€å¯è§£é‡Šçš„è¡¨è¾¾å®ç°äº†å…·æœ‰å†…åœ¨é€‚åº”æ€§çš„è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„é¢„æµ‹ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¿ƒè„å’Œè…¹éƒ¨æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨è¿™ä¸¤ç§è®¾ç½®ä¸­å‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œæ— æºæ€§èƒ½æ¥è¿‘å…¶æºå¯è®¿é—®çš„å¯¹åº”ç‰©ï¼Œè¿™åœ¨ä»¥å‰çš„å·¥ä½œä¸­å¾ˆå°‘è§‚å¯Ÿåˆ°çš„ä¸€è‡´æ€§ã€‚é™¤äº†å®šé‡æ”¹è¿›ä¹‹å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æµå½¢éå†å±•ç¤ºäº†æ‰€æå‡ºæ¡†æ¶çš„å¼ºå¤§å¯è§£é‡Šæ€§ï¼Œä»¥å®ç°å¹³æ»‘çš„å½¢çŠ¶æ“çºµã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08660v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€ã€è¯­ä¹‰åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒæœ‰æºå¯è®¿é—®å’Œæ— æºè‡ªé€‚åº”çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¡†æ¶æ— éœ€ä»»ä½•æ‰‹å·¥å®šåˆ¶çš„è‡ªé€‚åº”ç­–ç•¥ï¼Œé€šè¿‡æ„å»ºé¢†åŸŸæ— å…³çš„æ¦‚ç•¥æµå½¢ä½œä¸ºè§£å‰–è§„å¾‹çš„å…¨å±€ç©ºé—´ï¼Œå®ç°ç»“æ„å†…å®¹çš„è§£è€¦å’Œå¯è§£é‡Šé¢„æµ‹ï¼Œå…·æœ‰å†…åœ¨çš„è‡ªé€‚åº”æ€§ã€‚åœ¨å¿ƒè„å’Œè…¹éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°äº†ä¸¤ç§è®¾ç½®ä¸‹çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”æ— æºæ€§èƒ½æ¥è¿‘å…¶æœ‰æºå¯¹åº”ç‰©ï¼Œè¿™åœ¨ä»¥å‰çš„å·¥ä½œä¸­å¾ˆå°‘è§‚å¯Ÿåˆ°ã€‚æ­¤å¤–ï¼Œé€šè¿‡æµå½¢éå†è¿›è¡Œå¹³æ»‘å½¢çŠ¶æ“çºµï¼Œå±•ç¤ºå‡ºäº†è¯¥æ¡†æ¶çš„å¼ºå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€ã€è¯­ä¹‰åŒ–çš„æ¡†æ¶ï¼Œæ”¯æŒæºå¯è®¿é—®å’Œæºè‡ªç”±ä¸¤ç§è‡ªé€‚åº”åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚</li>
<li>æ¡†æ¶é€šè¿‡æ„å»ºé¢†åŸŸæ— å…³çš„æ¦‚ç•¥æµå½¢ä½œä¸ºå…¨å±€ç©ºé—´ï¼Œå®ç°è§£å‰–çŸ¥è¯†çš„ç»“æ„åŒ–æ„å»ºã€‚</li>
<li>æ¨¡å‹æ¶æ„è‡ªç„¶äº§ç”Ÿé€‚åº”æ€§ï¼Œæ— éœ€ä»»ä½•æ‰‹å·¥å®šåˆ¶çš„è‡ªé€‚åº”ç­–ç•¥ã€‚</li>
<li>ç»“æ„å†…å®¹è¢«è§£è€¦å¹¶è¡¨ç¤ºä¸ºå¯è§£é‡Šçš„é¢„æµ‹ï¼ŒåŒ…æ‹¬é€šç”¨è§£å‰–ç»“æ„å’Œä¸ªä½“ç‰¹å®šå‡ ä½•çš„ç©ºé—´è½¬æ¢ã€‚</li>
<li>åœ¨å¿ƒè„å’Œè…¹éƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸¤ç§è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>æºè‡ªç”±æ€§èƒ½æ¥è¿‘æºå¯è®¿é—®æ€§èƒ½ï¼Œå±•ç°äº†æ¡†æ¶çš„é«˜é€‚åº”æ€§å’Œå†…åœ¨ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-32bd3d7486bf5b85202ec1275de4f934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b4640183fbdc032e975ff5c067a8f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ccfd1fa3a2dd9592d22299ee43387d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e70aa2f5d33771be96602acc2dbaf167.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="S2-UniSeg-Fast-Universal-Agglomerative-Pooling-for-Scalable-Segment-Anything-without-Supervision"><a href="#S2-UniSeg-Fast-Universal-Agglomerative-Pooling-for-Scalable-Segment-Anything-without-Supervision" class="headerlink" title="S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment   Anything without Supervision"></a>S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment   Anything without Supervision</h2><p><strong>Authors:Huihui Xu, Jin Ye, Hongqiu Wang, Changkai Ji, Jiashi Lin, Ming Hu, Ziyan Huang, Ying Chen, Chenglong Ma, Tianbin Li, Lihao Liu, Junjun He, Lei Zhu</strong></p>
<p>Recent self-supervised image segmentation models have achieved promising performance on semantic segmentation and class-agnostic instance segmentation. However, their pretraining schedule is multi-stage, requiring a time-consuming pseudo-masks generation process between each training epoch. This time-consuming offline process not only makes it difficult to scale with training dataset size, but also leads to sub-optimal solutions due to its discontinuous optimization routine. To solve these, we first present a novel pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer of UniAP can identify groups of similar nodes in parallel, allowing to generate both semantic-level and instance-level and multi-granular pseudo-masks within ens of milliseconds for one image. Based on the fast UniAP, we propose the Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a student and a momentum teacher for continuous pretraining. A novel segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is proposed to pretrain S2-UniSeg to learn the local-to-global correspondences. Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image subset of SA-1B, S2-UniSeg further achieves performance gains on all four benchmarks. Our code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/bio-mlhui/S2-UniSeg">https://github.com/bio-mlhui/S2-UniSeg</a> </p>
<blockquote>
<p>è¿‘æœŸï¼Œè‡ªç›‘ç£å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²å’Œç±»åˆ«æ— å…³çš„å®ä¾‹åˆ†å‰²æ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½è¡¨ç°ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é¢„è®­ç»ƒè®¡åˆ’æ˜¯å¤šé˜¶æ®µçš„ï¼Œéœ€è¦åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¹‹é—´è¿›è¡Œè€—æ—¶çš„ä¼ªæ©ç ç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§è€—æ—¶çš„ç¦»çº¿è¿‡ç¨‹ä¸ä»…éš¾ä»¥éšç€è®­ç»ƒæ•°æ®é›†çš„å¤§å°è¿›è¡Œæ‰©å±•ï¼Œè€Œä¸”ç”±äºå…¶ä¸è¿ç»­çš„ä¼˜åŒ–æµç¨‹ï¼Œè¿˜ä¼šå¯¼è‡´æ¬¡ä¼˜è§£ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°å‹çš„ä¼ªæ©ç ç®—æ³•â€”â€”å¿«é€Ÿé€šç”¨èšåˆæ± åŒ–ï¼ˆUniAPï¼‰ã€‚UniAPçš„æ¯ä¸€å±‚éƒ½èƒ½å¹¶è¡Œè¯†åˆ«å‡ºç›¸ä¼¼çš„èŠ‚ç‚¹ç»„ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…ä¸ºä¸€å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰çº§åˆ«å’Œå®ä¾‹çº§åˆ«çš„å¤šç²’åº¦ä¼ªæ©ç ã€‚åŸºäºå¿«é€Ÿçš„UniAPï¼Œæˆ‘ä»¬æå‡ºäº†å¯æ‰©å±•çš„è‡ªç›‘ç£é€šç”¨åˆ†å‰²ï¼ˆS2-UniSegï¼‰ï¼Œå®ƒé‡‡ç”¨å­¦ç”Ÿå’ŒåŠ¨é‡æ•™å¸ˆè¿›è¡Œè¿ç»­é¢„è®­ç»ƒã€‚è¿˜æå‡ºäº†ä¸€ç§é¢å‘åˆ†å‰²çš„é¢„æ–‡æœ¬ä»»åŠ¡â€”â€”æŸ¥è¯¢çº§è‡ªè’¸é¦ï¼ˆQuerySDï¼‰ï¼Œç”¨äºé¢„è®­ç»ƒSSTrackSegæ¥å­¦ä¹ å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ã€‚åœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼ŒS2-UniSegä¼˜äºUnSAMæ¨¡å‹ï¼Œåœ¨COCOä¸Šæé«˜äº†AP+6.9ï¼Œåœ¨UVOä¸Šæé«˜äº†AR+11.1ï¼Œåœ¨COCOStuff-27ä¸Šæé«˜äº†PixelAcc+4.5å’Œåœ¨Cityscapesä¸Šæé«˜äº†RQ+8.0ã€‚å½“æ‰©å¤§åˆ°æ›´å¤§çš„SA-1Bçš„2Må›¾åƒå­é›†æ—¶ï¼ŒS2-UniSegåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½å–å¾—äº†æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bio-mlhui/S2-UniSeg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bio-mlhui/S2-UniSegæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06995v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ä¼ªæ©è†œç®—æ³•â€”â€”å¿«é€Ÿé€šç”¨èåˆæ± åŒ–ï¼ˆUniAPï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰çº§åˆ«å’Œå®ä¾‹çº§åˆ«çš„å¤šç²’åº¦ä¼ªæ©è†œï¼Œå¤§å¤§æé«˜äº†è‡ªç›‘ç£å›¾åƒåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚åŸºäºå¿«é€ŸUniAPï¼Œæå‡ºäº†å¯æ‰©å±•è‡ªç›‘ç£é€šç”¨åˆ†å‰²ï¼ˆS2-UniSegï¼‰æ¨¡å‹ï¼Œé‡‡ç”¨å­¦ç”Ÿå’ŒåŠ¨é‡æ•™å¸ˆè¿›è¡Œè¿ç»­é¢„è®­ç»ƒã€‚æå‡ºäº†ä¸€ç§æ–°çš„é¢å‘åˆ†å‰²çš„é¢„æ–‡æœ¬ä»»åŠ¡â€”â€”æŸ¥è¯¢çº§è‡ªè’¸é¦ï¼ˆQuerySDï¼‰ï¼Œç”¨äºé¢„è®­ç»ƒS2-UniSegå­¦ä¹ å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ã€‚åœ¨ç›¸åŒè®¾ç½®ä¸‹ï¼ŒS2-UniSegä¼˜äºå½“å‰æœ€ä½³æ¨¡å‹UnSAMï¼Œåœ¨COCOã€UVOã€COCOStuff-27å’ŒCityscapesä¸Šçš„è¡¨ç°å‡æœ‰æ˜¾è‘—æé«˜ã€‚åœ¨æ‰©å±•åˆ°æ›´å¤§çš„SA-1Bçš„2Må›¾åƒå­é›†åï¼ŒS2-UniSegåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¼ªæ©è†œç®—æ³•â€”â€”å¿«é€Ÿé€šç”¨èåˆæ± åŒ–ï¼ˆUniAPï¼‰ï¼Œå¯å¿«é€Ÿç”Ÿæˆè¯­ä¹‰å’Œå®ä¾‹çº§åˆ«çš„ä¼ªæ©è†œã€‚</li>
<li>åŸºäºUniAPï¼Œæå‡ºäº†å¯æ‰©å±•è‡ªç›‘ç£é€šç”¨åˆ†å‰²æ¨¡å‹ï¼ˆS2-UniSegï¼‰ã€‚</li>
<li>S2-UniSegé‡‡ç”¨å­¦ç”Ÿå’ŒåŠ¨é‡æ•™å¸ˆçš„è¿ç»­é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢å‘åˆ†å‰²çš„é¢„æ–‡æœ¬ä»»åŠ¡â€”â€”æŸ¥è¯¢çº§è‡ªè’¸é¦ï¼ˆQuerySDï¼‰ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ å±€éƒ¨åˆ°å…¨å±€çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>S2-UniSegç›¸è¾ƒäºå½“å‰æœ€ä½³æ¨¡å‹UnSAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>S2-UniSegåœ¨æ‰©å±•åˆ°æ›´å¤§çš„æ•°æ®é›†åæ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a7df8d051cfe32e485dd4ad4c97368a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95cfe3c37c5c83dcacba4850e549c6e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbfaa24e3b90c884cde84841ffd24eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee308862fc61b601a8e55f226efac5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-414606c27aac876ea790a480f2615208.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Latent-Expression-Generation-for-Referring-Image-Segmentation-and-Grounding"><a href="#Latent-Expression-Generation-for-Referring-Image-Segmentation-and-Grounding" class="headerlink" title="Latent Expression Generation for Referring Image Segmentation and   Grounding"></a>Latent Expression Generation for Referring Image Segmentation and   Grounding</h2><p><strong>Authors:Seonghoon Yu, Junbeom Hong, Joonseok Lee, Jeany Son</strong></p>
<p>Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark. </p>
<blockquote>
<p>è§†è§‰å®šä½ä»»åŠ¡ï¼Œå¦‚å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰å’Œå¼•ç”¨è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°å®šä½ç›®æ ‡å¯¹è±¡ã€‚å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡å¯ä»¥ç”¨å¤šç§æ–¹å¼æè¿°ï¼Œåæ˜ è¯¸å¦‚é¢œè‰²ã€ä½ç½®ç­‰å¤šæ ·åŒ–çš„å±æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå•ä¸ªæ–‡æœ¬è¾“å…¥ï¼Œè¿™åªèƒ½æ•è·è§†è§‰åŸŸä¸­ä¸°å¯Œä¿¡æ¯çš„ä¸€éƒ¨åˆ†ã€‚ä¸°å¯Œè§†è§‰ç»†èŠ‚å’Œç¨€ç–æ–‡æœ¬çº¿ç´¢ä¹‹é—´çš„ä¸åŒ¹é…å¯èƒ½å¯¼è‡´ç›¸ä¼¼å¯¹è±¡çš„è¯¯è¯†åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å•ä¸ªæ–‡æœ¬è¾“å…¥äº§ç”Ÿçš„å¤šä¸ªæ½œåœ¨è¡¨è¾¾å¼ï¼Œå¹¶èå…¥åŸå§‹æè¿°ä¸­ç¼ºå¤±çš„äº’è¡¥è§†è§‰ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸»ä½“åˆ†é…å™¨å’Œè§†è§‰æ¦‚å¿µæ³¨å…¥æ¨¡å—ï¼Œå°†å…±äº«ä¸»ä½“å’Œç‹¬ç‰¹å±æ€§æ¦‚å¿µåµŒå…¥åˆ°æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œä»è€Œæ•è·ç‹¬ç‰¹ä¸”ç›®æ ‡ç‰¹å®šçš„è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ­£è¾¹è·å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå°†æ‰€æœ‰æ½œåœ¨è¡¨è¾¾å¼ä¸åŸå§‹æ–‡æœ¬å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™ç»†å¾®å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„RISå’ŒRECæ–¹æ³•ï¼Œè€Œä¸”åœ¨å¹¿ä¹‰å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05123v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºè§†è§‰å®šä½ä»»åŠ¡çš„ç ”ç©¶è®ºæ–‡ï¼Œä¸»è¦ä»‹ç»äº†å¦‚ä½•é€šè¿‡ç»“åˆå¤šç§æ½œåœ¨è¡¨è¾¾å’Œè§†è§‰ç»†èŠ‚æ¥è§£å†³å•ä¸€æ–‡æœ¬è¾“å…¥å¯¼è‡´çš„è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯ä¸åŒ¹é…çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ä¸»ä½“åˆ†é…å™¨å’Œè§†è§‰æ¦‚å¿µæ³¨å…¥å™¨æ¨¡å—æ¥åµŒå…¥å…±äº«ä¸»ä½“å’Œç‹¬ç‰¹å±æ€§æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨æ­£é¢é—´éš”å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥å¯¹é½æ‰€æœ‰æ½œåœ¨è¡¨è¾¾å’ŒåŸå§‹æ–‡æœ¬ã€‚è¯¥æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºèƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œå¹¶ä¸”åœ¨å¹¿ä¹‰å‚ç…§è¡¨è¾¾å¼åˆ†å‰²ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å®šä½ä»»åŠ¡åŒ…æ‹¬å‚ç…§å›¾åƒåˆ†å‰²å’Œå‚ç…§è¡¨è¾¾ç†è§£ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°å®šä½ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå•ä¸€çš„æ–‡æœ¬è¾“å…¥ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è§†è§‰é¢†åŸŸçš„ä¸°å¯Œä¿¡æ¯ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œç»“åˆäº†æ¥è‡ªå•ä¸€æ–‡æœ¬è¾“å…¥çš„å¤šç§æ½œåœ¨è¡¨è¾¾ï¼Œå¹¶å¼•å…¥äº†è§†è§‰ç»†èŠ‚æ¥è¡¥å……åŸå§‹æè¿°çš„ä¸è¶³ã€‚</li>
<li>å¼•å…¥ä¸»ä½“åˆ†é…å™¨å’Œè§†è§‰æ¦‚å¿µæ³¨å…¥å™¨æ¨¡å—ï¼Œä»¥åµŒå…¥å…±äº«ä¸»ä½“å’Œç‹¬ç‰¹å±æ€§æ¦‚å¿µã€‚</li>
<li>ä½¿ç”¨æ­£é¢é—´éš”å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥å¯¹é½æ‰€æœ‰æ½œåœ¨è¡¨è¾¾å’ŒåŸå§‹æ–‡æœ¬ï¼ŒåŒæ—¶ä¿ç•™ç»†å¾®å˜åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å‚ç…§å›¾åƒåˆ†å‰²å’Œå‚ç…§è¡¨è¾¾ç†è§£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b119cbd3661d81113e20a504b6cc958f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93230b615fbc5169c6979697497918dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f2cb9397e789fb3011b3a508ef3b593.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Mammo-SAE-Interpreting-Breast-Cancer-Concept-Learning-with-Sparse-Autoencoders"><a href="#Mammo-SAE-Interpreting-Breast-Cancer-Concept-Learning-with-Sparse-Autoencoders" class="headerlink" title="Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse   Autoencoders"></a>Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse   Autoencoders</h2><p><strong>Authors:Krishna Kanth Nakka</strong></p>
<p>Interpretability is critical in high-stakes domains such as medical imaging, where understanding model decisions is essential for clinical adoption. In this work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast imaging by analyzing {Mammo-CLIP}, a visionâ€“language foundation model pretrained on large-scale mammogram imageâ€“report pairs. We train a patch-level \texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features associated with clinically relevant breast concepts such as \textit{mass} and \textit{suspicious calcification}. Our findings reveal that top activated class level latent neurons in the SAE latent space often tend to align with ground truth regions, and also uncover several confounding factors influencing the modelâ€™s decision-making process. Additionally, we analyze which latent neurons the model relies on during downstream finetuning for improving the breast concept prediction. This study highlights the promise of interpretable SAE latent representations in providing deeper insight into the internal workings of foundation models at every layer for breast imaging. The code will be released at <a target="_blank" rel="noopener" href="https://krishnakanthnakka.github.io/MammoSAE/">https://krishnakanthnakka.github.io/MammoSAE/</a> </p>
<blockquote>
<p>åœ¨åŒ»å­¦å½±åƒç­‰é«˜é£é™©é¢†åŸŸï¼Œå¯è§£é‡Šæ€§å¯¹äºæ¨¡å‹çš„å†³ç­–ç†è§£è‡³å…³é‡è¦ï¼Œè¿™å¯¹äºä¸´åºŠé‡‡çº³è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æåŸºäºå¤§è§„æ¨¡ä¹³è…ºXå…‰å›¾åƒæŠ¥å‘Šå¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹Mammo-CLIPï¼Œå¼•å…¥åŸºäºç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰çš„å¯è§£é‡Šæ€§æ¥å¯¹ä¹³è…ºå½±åƒè¿›è¡Œè§£è¯»ã€‚æˆ‘ä»¬åœ¨Mammo-CLIPä¸Šè®­ç»ƒäº†ä¸€ä¸ªåŸºäºè¡¥ä¸çº§åˆ«çš„Mammo-SAEï¼Œä»¥è¯†åˆ«å’Œæ¢æµ‹ä¸ä¸´åºŠç›¸å…³çš„ä¹³è…ºæ¦‚å¿µï¼ˆå¦‚è‚¿å—å’Œå¯ç–‘é’™åŒ–ï¼‰ç›¸å…³çš„æ½œåœ¨ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSAEæ½œåœ¨ç©ºé—´ä¸­é¡¶éƒ¨æ¿€æ´»çš„ç±»åˆ«çº§åˆ«æ½œåœ¨ç¥ç»å…ƒå¾€å¾€ä¸çœŸå®åŒºåŸŸå¯¹é½ï¼Œå¹¶æ­ç¤ºäº†å½±å“æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„å‡ ä¸ªæ··æ·†å› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†æ¨¡å‹åœ¨è¿›è¡Œä¸‹æ¸¸å¾®è°ƒä»¥æ”¹å–„ä¹³è…ºæ¦‚å¿µé¢„æµ‹æ—¶ä¾èµ–å“ªäº›æ½œåœ¨ç¥ç»å…ƒã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†å¯è§£é‡Šçš„SAEæ½œåœ¨è¡¨ç¤ºåœ¨æ·±å…¥äº†è§£æ¯ä¸€å±‚åŸºç¡€æ¨¡å‹å†…éƒ¨å·¥ä½œåŸç†æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºæˆåƒæ–¹é¢ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://krishnakanthnakka.github.io/MammoSAE/%E5%8F%91%E5%B8%83%E3%80%82">https://krishnakanthnakka.github.io/MammoSAE/å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15227v2">PDF</a> Accepted at Deep Breast Imaging workshop, MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦æˆåƒç­‰é«˜é£é™©é¢†åŸŸä¸­è§£é‡Šæ€§çš„é‡è¦æ€§ï¼Œå¹¶é’ˆå¯¹ä¹³è…ºæˆåƒå¼•å…¥äº†åŸºäºç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰çš„è§£é‡Šæ€§æ–¹æ³•ã€‚é€šè¿‡å¯¹Mammo-CLIPçš„çš„åˆ†æå’Œç ”ç©¶ï¼Œå‘ç°SAEæ½œåœ¨ç©ºé—´ä¸­çš„é¡¶å±‚æ¿€æ´»ç±»æ½œåœ¨ç¥ç»å…ƒå¾€å¾€ä¸çœŸå®åŒºåŸŸå¯¹é½ï¼Œå¹¶ä¸”æ­ç¤ºäº†å½±å“æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„å‡ ä¸ªæ··æ‚å› ç´ ã€‚æ­¤å¤–ï¼Œè¿˜åˆ†æäº†æ¨¡å‹åœ¨è¿›è¡Œä¸‹æ¸¸å¾®è°ƒä»¥æ”¹å–„ä¹³è…ºç™Œæ¦‚å¿µé¢„æµ‹æ—¶ä¾èµ–å“ªäº›æ½œåœ¨ç¥ç»å…ƒã€‚æ­¤ç ”ç©¶å¼ºè°ƒäº†å¯è§£é‡Šçš„SAEæ½œåœ¨è¡¨è¾¾åœ¨ä¹³è…ºæˆåƒä¸­å¯¹åŸºç¡€æ¨¡å‹å†…éƒ¨å·¥ä½œçš„æ·±å…¥äº†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£é‡Šæ€§åœ¨åŒ»å­¦æˆåƒé¢†åŸŸè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©æƒ…å¢ƒå¦‚ä¹³è…ºæˆåƒä¸­ã€‚</li>
<li>ç ”ç©¶è€…å¼•å…¥äº†åŸºäºç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰çš„è§£é‡Šæ€§æ–¹æ³•ï¼Œä»¥å¢å¼ºå¯¹æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„ç†è§£ã€‚</li>
<li>å¯¹Mammo-CLIPæ¨¡å‹çš„åˆ†ææ˜¾ç¤ºï¼ŒSAEæ½œåœ¨ç©ºé—´ä¸­çš„é¡¶å±‚æ¿€æ´»ç±»æ½œåœ¨ç¥ç»å…ƒä¸çœŸå®åŒºåŸŸå¯¹é½ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å½±å“æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„æ··æ‚å› ç´ ã€‚</li>
<li>åˆ†æäº†æ¨¡å‹åœ¨è¿›è¡Œä¸‹æ¸¸å¾®è°ƒæ—¶ä¾èµ–çš„æ½œåœ¨ç¥ç»å…ƒï¼Œä»¥æé«˜ä¹³è…ºç™Œæ¦‚å¿µé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†å¯è§£é‡Šçš„SAEæ½œåœ¨è¡¨è¾¾å¯¹æ·±å…¥äº†è§£ä¹³è…ºæˆåƒåŸºç¡€æ¨¡å‹å†…éƒ¨å·¥ä½œçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1ddfd357adb40c1fd6dce74ac13dbb41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2ac83b02a84b45bdaa235e50e03bd3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2da3cac8f202df9db6e74f7118f473.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-920af8dc43005928088af3cfa03ce51a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Calibrated-Self-supervised-Vision-Transformers-Improve-Intracranial-Arterial-Calcification-Segmentation-from-Clinical-CT-Head-Scans"><a href="#Calibrated-Self-supervised-Vision-Transformers-Improve-Intracranial-Arterial-Calcification-Segmentation-from-Clinical-CT-Head-Scans" class="headerlink" title="Calibrated Self-supervised Vision Transformers Improve Intracranial   Arterial Calcification Segmentation from Clinical CT Head Scans"></a>Calibrated Self-supervised Vision Transformers Improve Intracranial   Arterial Calcification Segmentation from Clinical CT Head Scans</h2><p><strong>Authors:Benjamin Jin, Grant Mair, Joanna M. Wardlaw, Maria del C. ValdÃ©s HernÃ¡ndez</strong></p>
<p>Vision Transformers (ViTs) have gained significant popularity in the natural image domain but have been less successful in 3D medical image segmentation. Nevertheless, 3D ViTs are particularly interesting for large medical imaging volumes due to their efficient self-supervised training within the masked autoencoder (MAE) framework, which enables the use of imaging data without the need for expensive manual annotations. Intracranial arterial calcification (IAC) is an imaging biomarker visible on routinely acquired CT scans linked to neurovascular diseases such as stroke and dementia, and automated IAC quantification could enable their large-scale risk assessment. We pre-train ViTs with MAE and fine-tune them for IAC segmentation for the first time. To develop our models, we use highly heterogeneous data from a large clinical trial, the third International Stroke Trial (IST-3). We evaluate key aspects of MAE pre-trained ViTs in IAC segmentation, and analyse the clinical implications. We show: 1) our calibrated self-supervised ViT beats a strong supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial for ViTs for IAC segmentation and interpolation upsampling with regular convolutions is preferable to transposed convolutions for ViT-based models, and 3) our ViTs increase robustness to higher slice thicknesses and improve risk group classification in a clinical scenario by 46%. Our code is available online. </p>
<blockquote>
<p>Vision Transformersï¼ˆViTsï¼‰åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸå·²ç»è·å¾—äº†æå¤§çš„å—æ¬¢è¿ç¨‹åº¦ï¼Œä½†åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„æˆåŠŸè¾ƒå°ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å‹åŒ»å­¦æˆåƒä½“ç§¯è€Œè¨€ï¼Œ3D ViTsç‰¹åˆ«æœ‰è¶£ï¼Œå› ä¸ºå®ƒä»¬èƒ½åœ¨masked autoencoderï¼ˆMAEï¼‰æ¡†æ¶å†…è¿›è¡Œé«˜æ•ˆçš„è‡ªæˆ‘ç›‘ç£è®­ç»ƒï¼Œè¿™å…è®¸ä½¿ç”¨æˆåƒæ•°æ®è€Œæ— éœ€æ˜‚è´µçš„æ‰‹åŠ¨æ³¨é‡Šã€‚é¢…å†…åŠ¨è„‰é’™åŒ–ï¼ˆIACï¼‰æ˜¯ä¸€ä¸ªåœ¨å¸¸è§„è·å¾—çš„CTæ‰«æä¸Šå¯è§çš„æˆåƒç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä¸ä¸­é£å’Œç—´å‘†ç­‰ç¥ç»è¡€ç®¡ç–¾ç—…æœ‰å…³ï¼Œè‡ªåŠ¨åŒ–IACå®šé‡èƒ½å¤Ÿè¿›è¡Œå¤§è§„æ¨¡é£é™©è¯„ä¼°ã€‚æˆ‘ä»¬é¦–æ¬¡ä½¿ç”¨MAEå¯¹ViTsè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥è¿›è¡ŒIACåˆ†å‰²ã€‚ä¸ºäº†å¼€å‘æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªå¤§å‹ä¸´åºŠè¯•éªŒçš„å…·æœ‰é«˜åº¦å¼‚è´¨æ€§çš„æ•°æ®â€”â€”ç¬¬ä¸‰æ¬¡å›½é™…ä¸­é£è¯•éªŒï¼ˆIST-3ï¼‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†MAEé¢„è®­ç»ƒçš„ViTsåœ¨IACåˆ†å‰²æ–¹é¢çš„å…³é”®æ–¹é¢ï¼Œå¹¶åˆ†æäº†å…¶ä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼š1ï¼‰æˆ‘ä»¬çš„æ ¡å‡†è‡ªç›‘ç£ViTæ¯”å¼ºå¤§çš„ç›‘ç£å‹nnU-NetåŸºçº¿é«˜å‡º3.2 Diceç‚¹ï¼›2ï¼‰å¯¹äºIACåˆ†å‰²çš„ViTsè€Œè¨€ï¼Œå°è¡¥ä¸å°ºå¯¸è‡³å…³é‡è¦ï¼Œå¯¹äºåŸºäºViTçš„æ¨¡å‹ï¼Œä½¿ç”¨å¸¸è§„å·ç§¯è¿›è¡Œæ’å€¼ä¸Šé‡‡æ ·æ¯”è½¬ç½®å·ç§¯æ›´å¯å–ï¼›3ï¼‰æˆ‘ä»¬çš„ViTsæé«˜äº†å¯¹è¾ƒåšåˆ‡ç‰‡çš„é²æ£’æ€§ï¼Œå¹¶åœ¨ä¸´åºŠåœºæ™¯ä¸­æé«˜äº†é£é™©ç»„åˆ†ç±»çš„å‡†ç¡®ç‡ï¼Œè¾¾åˆ°46%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01744v2">PDF</a> Accepted at the 3rd Data Engineering in Medical Imaging workshop @   MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision Transformersï¼ˆViTsï¼‰åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä»¥åŠåœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿé¦–æ¬¡ä½¿ç”¨Masked Autoencoderï¼ˆMAEï¼‰æ¡†æ¶é¢„è®­ç»ƒViTsï¼Œå¹¶å°†å…¶å¾®è°ƒç”¨äºé¢…å†…åŠ¨è„‰é’™åŒ–ï¼ˆIACï¼‰åˆ†å‰²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡æ ¡å‡†çš„è‡ªæˆ‘ç›‘ç£ViTåœ¨IACåˆ†å‰²æ–¹é¢ä¼˜äºå¼ºå¤§çš„ç›‘ç£å‹nnU-NetåŸºå‡†æ¨¡å‹ï¼Œä¸”ä½è¡¥ä¸å¤§å°å’Œæ’å€¼ä¸Šé‡‡æ ·å¯¹äºViTåœ¨IACåˆ†å‰²ä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼ŒViTsèƒ½æé«˜å¯¹è¾ƒé«˜åˆ‡ç‰‡åšåº¦çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨ä¸´åºŠç¯å¢ƒä¸­æé«˜é£é™©åˆ†ç»„åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨è‡ªç„¶å›¾åƒé¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­åº”ç”¨ç›¸å¯¹è¾ƒå°‘ã€‚</li>
<li>ViTs åœ¨å¤§å‹åŒ»ç–—å½±åƒä½“ç§¯ä¸­ç‰¹åˆ«å—æ¬¢è¿ï¼Œå…¶åŸºäºMasked Autoencoder (MAE)æ¡†æ¶çš„è‡ªæˆ‘ç›‘ç£è®­ç»ƒèƒ½å¤Ÿå®ç°å½±åƒæ•°æ®çš„ä½¿ç”¨ï¼Œæ— éœ€æ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>é¦–æ¬¡ä½¿ç”¨MAEæ¡†æ¶é¢„è®­ç»ƒViTså¹¶å°†å…¶å¾®è°ƒç”¨äºé¢…å†…åŠ¨è„‰é’™åŒ–ï¼ˆIACï¼‰åˆ†å‰²ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡æ ¡å‡†çš„è‡ªæˆ‘ç›‘ç£ViTåœ¨IACåˆ†å‰²æ–¹é¢ä¼˜äºnnU-NetåŸºå‡†æ¨¡å‹ã€‚</li>
<li>ä½è¡¥ä¸å¤§å°å’Œæ’å€¼ä¸Šé‡‡æ ·å¯¹äºViTåœ¨IACåˆ†å‰²ä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚</li>
<li>ViTsèƒ½æé«˜å¯¹è¾ƒé«˜åˆ‡ç‰‡åšåº¦çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e726a17ee8dd91c5be7f0257702a8198.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335c39ff98765dc1910d40d5a0d93ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1da436980b5adefc8b6a51c0cea99a9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ae879be75d1c2b8c514666adaaded1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2525aa2c1471a186ed786f33ff586c31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3561540d9bfe9c674119444e86d4574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cc980406a9efa35e389237bd0154996.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Foundation-Models-for-Zero-Shot-Segmentation-of-Scientific-Images-without-AI-Ready-Data"><a href="#Foundation-Models-for-Zero-Shot-Segmentation-of-Scientific-Images-without-AI-Ready-Data" class="headerlink" title="Foundation Models for Zero-Shot Segmentation of Scientific Images   without AI-Ready Data"></a>Foundation Models for Zero-Shot Segmentation of Scientific Images   without AI-Ready Data</h2><p><strong>Authors:Shubhabrata Mukherjee, Jack Lang, Obeen Kwon, Iryna Zenyuk, Valerie Brogden, Adam Weber, Daniela Ushizima</strong></p>
<p>Zero-shot and prompt-based models have excelled at visual reasoning tasks by leveraging large-scale natural image corpora, but they often fail on sparse and domain-specific scientific image data. We introduce Zenesis, a no-code interactive computer vision platform designed to reduce data readiness bottlenecks in scientific imaging workflows. Zenesis integrates lightweight multimodal adaptation for zero-shot inference on raw scientific data, human-in-the-loop refinement, and heuristic-based temporal enhancement. We validate our approach on Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) datasets of catalyst-loaded membranes. Zenesis outperforms baselines, achieving an average accuracy of 0.947, Intersection over Union (IoU) of 0.858, and Dice score of 0.923 on amorphous catalyst samples; and 0.987 accuracy, 0.857 IoU, and 0.923 Dice on crystalline samples. These results represent a significant performance gain over conventional methods such as Otsu thresholding and standalone models like the Segment Anything Model (SAM). Zenesis enables effective image segmentation in domains where annotated datasets are limited, offering a scalable solution for scientific discovery. </p>
<blockquote>
<p>é›¶æ ·æœ¬å’ŒåŸºäºæç¤ºçš„æ¨¡å‹é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡çš„è‡ªç„¶å›¾åƒè¯­æ–™åº“åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨ç¨€ç–å’Œç‰¹å®šé¢†åŸŸçš„ç§‘å­¦å›¾åƒæ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æ¨å‡ºäº†Zenesisï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ä»£ç äº¤äº’å¼è®¡ç®—æœºè§†è§‰å¹³å°ï¼Œæ—¨åœ¨å‡å°‘ç§‘å­¦æˆåƒå·¥ä½œæµç¨‹ä¸­çš„æ•°æ®å‡†å¤‡ç“¶é¢ˆã€‚Zenesisé›†æˆäº†è½»é‡çº§çš„è·¨æ¨¡æ€é€‚åº”ï¼Œç”¨äºåœ¨åŸå§‹ç§‘å­¦æ•°æ®ä¸Šè¿›è¡Œé›¶æ ·æœ¬æ¨ç†ï¼Œè¿˜æœ‰äººä¸ºé©±åŠ¨çš„å¾ªç¯ä¼˜åŒ–å’ŒåŸºäºå¯å‘å¼çš„ä¸´æ—¶å¢å¼ºåŠŸèƒ½ã€‚æˆ‘ä»¬åœ¨å‚¬åŒ–å‰‚è´Ÿè½½è†œçš„èšç„¦ç¦»å­æŸæ‰«æç”µå­æ˜¾å¾®é•œï¼ˆFIB-SEMï¼‰æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚Zenesisçš„è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œåœ¨æ— åºå‚¬åŒ–å‰‚æ ·æœ¬ä¸Šè¾¾åˆ°äº†å¹³å‡å‡†ç¡®ç‡0.947ã€äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ä¸º0.858ä»¥åŠDiceç³»æ•°ä¸º0.923ï¼›åœ¨ç»“æ™¶æ ·æœ¬ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†0.987ã€IoUä¸º0.857ä»¥åŠDiceç³»æ•°ä¸º0.923ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚Otsué˜ˆå€¼æ³•ï¼‰å’Œç‹¬ç«‹æ¨¡å‹ï¼ˆå¦‚Anythingæ¨¡å‹ï¼‰ç­‰ï¼Œè¿™äº›ç»“æœä»£è¡¨äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚Zenesisåœ¨æ ‡æ³¨æ•°æ®é›†æœ‰é™çš„é¢†åŸŸå®ç°äº†æœ‰æ•ˆçš„å›¾åƒåˆ†å‰²ï¼Œä¸ºç§‘å­¦å‘ç°æä¾›äº†å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24039v2">PDF</a> This paper has been accepted for presentation at the 59th   International Conference on Parallel Processing (ICPP 2025), DRAI workshop</p>
<p><strong>Summary</strong></p>
<p>Zenesisæ˜¯ä¸€ä¸ªæ— ä»£ç çš„äº¤äº’å¼è®¡ç®—æœºè§†è§‰å¹³å°ï¼Œä¸“ä¸ºè§£å†³ç§‘å­¦æˆåƒå·¥ä½œæµç¨‹ä¸­çš„æ•°æ®å‡†å¤‡ç“¶é¢ˆè€Œè®¾è®¡ã€‚å®ƒé›†æˆäº†è½»é‡çº§çš„å¤šæ¨¡å¼é€‚åº”æŠ€æœ¯ï¼Œç”¨äºåœ¨åŸå§‹ç§‘å­¦æ•°æ®ä¸Šè¿›è¡Œé›¶å°„å‡»æ¨æ–­ï¼Œä»¥åŠäººæœºäº¤äº’ä¼˜åŒ–å’ŒåŸºäºå¯å‘å¼çš„æ—¶é—´å¢å¼ºåŠŸèƒ½ã€‚åœ¨Focused Ion Beam Scanning Electron Microscopy (FIB-SEM)æ•°æ®é›†ä¸ŠéªŒè¯ï¼ŒZenesisåœ¨å‚¬åŒ–å‰‚è´Ÿè½½è†œçš„æ— å®šå½¢å’Œç»“æ™¶æ ·å“ä¸Šå–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œå•ä¸€æ¨¡å‹ã€‚å®ƒä¸ºç§‘å­¦å‘ç°æä¾›äº†æœ‰æ•ˆçš„å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡æ³¨æ•°æ®é›†æœ‰é™çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Zenesisæ˜¯ä¸€ä¸ªé’ˆå¯¹ç§‘å­¦æˆåƒçš„è®¡ç®—æœºè§†è§‰å¹³å°ï¼Œæ—¨åœ¨è§£å†³æ•°æ®å‡†å¤‡ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>å¹³å°é›†æˆäº†è½»é‡çº§çš„å¤šæ¨¡å¼é€‚åº”æŠ€æœ¯ï¼Œç”¨äºåœ¨åŸå§‹ç§‘å­¦æ•°æ®ä¸Šè¿›è¡Œé›¶å°„å‡»æ¨æ–­ã€‚</li>
<li>Zenesisæ”¯æŒäººæœºäº¤äº’ä¼˜åŒ–å’ŒåŸºäºå¯å‘å¼çš„æ—¶é—´å¢å¼ºåŠŸèƒ½ã€‚</li>
<li>åœ¨FIB-SEMæ•°æ®é›†ä¸Šï¼ŒZenesiså¯¹å‚¬åŒ–å‰‚è´Ÿè½½è†œçš„æ— å®šå½¢å’Œç»“æ™¶æ ·å“è¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•å’Œå•ä¸€æ¨¡å‹ç›¸æ¯”ï¼ŒZenesisè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>è¯¥å¹³å°ç‰¹åˆ«é€‚ç”¨äºæ ‡æ³¨æ•°æ®é›†æœ‰é™çš„æƒ…å†µä¸‹çš„å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-752b2e489bcc5d1a4690e0242cd54f4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c173359076c1228cfa1f52e2c5b243c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92b86957cbf71f38f02042c0ce855633.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15d0bc6f0260a0cd04a13743e1e55a6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c9384ce8f340d6bec3148dc5029d326.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a62f33c3f5e7cf19c4396b058b99e61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24be618f4f968a86f754767ab28216fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29dd5bc062eabac8f6eaee4a99e79848.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Patient-Specific-Deep-Reinforcement-Learning-for-Automatic-Replanning-in-Head-and-Neck-Cancer-Proton-Therapy"><a href="#Patient-Specific-Deep-Reinforcement-Learning-for-Automatic-Replanning-in-Head-and-Neck-Cancer-Proton-Therapy" class="headerlink" title="Patient-Specific Deep Reinforcement Learning for Automatic Replanning in   Head-and-Neck Cancer Proton Therapy"></a>Patient-Specific Deep Reinforcement Learning for Automatic Replanning in   Head-and-Neck Cancer Proton Therapy</h2><p><strong>Authors:Malvern Madondo, Yuan Shao, Yingzi Liu, Jun Zhou, Xiaofeng Yang, Zhen Tian</strong></p>
<p>Anatomical changes during intensity-modulated proton therapy (IMPT) for head-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and organ-at-risk overdosing. Treatment replanning is often required to maintain clinically acceptable treatment quality. However, current manual replanning processes are resource-intensive and time-consuming. We propose a patient-specific deep reinforcement learning (DRL) framework for automated IMPT replanning, with a reward-shaping mechanism based on a $150$-point plan quality score addressing competing clinical objectives. We formulate the planning process as a reinforcement learning problem where agents learn control policies to adjust optimization priorities, maximizing plan quality. Unlike population-based approaches, our framework trains agents for each patient using their planning Computed Tomography (CT) and augmented anatomies simulating anatomical changes (tumor progression and regression). This patient-specific approach leverages anatomical similarities along the treatment course, enabling effective plan adaptation. We implemented two DRL algorithms, Deep Q-Network and Proximal Policy Optimization, using dose-volume histograms (DVHs) as state representations and a $22$-dimensional action space of priority adjustments. Evaluation on eight HNC patients using actual replanning CT data showed that both agents improved initial plan scores from $120.78 \pm 17.18$ to $139.59 \pm 5.50$ (DQN) and $141.50 \pm 4.69$ (PPO), surpassing the replans manually generated by a human planner ($136.32 \pm 4.79$). Clinical validation confirms that improvements translate to better tumor coverage and OAR sparing across diverse anatomical changes. This work highlights DRLâ€™s potential in addressing geometric and dosimetric complexities of adaptive proton therapy, offering efficient offline adaptation solutions and advancing online adaptive proton therapy. </p>
<blockquote>
<p>åœ¨å¤´é¢ˆç™Œï¼ˆHNCï¼‰çš„å¼ºåº¦è°ƒåˆ¶è´¨å­ç–—æ³•ï¼ˆIMPTï¼‰è¿‡ç¨‹ä¸­ï¼Œè§£å‰–ç»“æ„çš„å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´å¸ƒæ‹‰æ ¼å³°åç§»ï¼Œä»è€Œå­˜åœ¨è‚¿ç˜¤å‰‚é‡ä¸è¶³å’Œå±åŠå™¨å®˜å‰‚é‡è¿‡å¤§çš„é£é™©ã€‚ä¸ºäº†ä¿æŒä¸´åºŠä¸Šå¯æ¥å—çš„æ²»ç–—è´¨é‡ï¼Œç»å¸¸éœ€è¦è¿›è¡Œæ²»ç–—å†è®¡åˆ’ã€‚ç„¶è€Œï¼Œå½“å‰çš„å†è®¡åˆ’æµç¨‹éœ€è¦å¤§é‡èµ„æºå’Œæ—¶é—´ã€‚é’ˆå¯¹æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„æ‚£è€…ç‰¹å¼‚æ€§è‡ªåŠ¨IMPTå†è®¡åˆ’æ¡†æ¶ã€‚è¯¥æ¡†æ¶å…·æœ‰åŸºäº$150$ç‚¹è®¡åˆ’è´¨é‡åˆ†æ•°çš„å¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œä»¥è§£å†³ç›¸äº’ç«äº‰çš„ä¸´åºŠç›®æ ‡ã€‚æˆ‘ä»¬å°†è§„åˆ’è¿‡ç¨‹åˆ¶å®šä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­æ™ºèƒ½ä½“å­¦ä¹ æ§åˆ¶ç­–ç•¥æ¥è°ƒæ•´ä¼˜åŒ–ä¼˜å…ˆçº§ï¼Œä»¥æœ€å¤§åŒ–è®¡åˆ’è´¨é‡ã€‚ä¸åŒäºåŸºäºäººç¾¤çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨æ‚£è€…çš„è§„åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œæ¨¡æ‹Ÿè§£å‰–ç»“æ„å˜åŒ–çš„å¢å¼ºè§£å‰–ç»“æ„æ¥è®­ç»ƒæ¯ä¸ªæ‚£è€…çš„æ™ºèƒ½ä½“ï¼ˆå¦‚è‚¿ç˜¤è¿›å±•å’Œæ¶ˆé€€ï¼‰ã€‚è¿™ç§æ‚£è€…ç‰¹å¼‚æ€§æ–¹æ³•åˆ©ç”¨äº†æ²»ç–—è¿‡ç¨‹ä¸­çš„è§£å‰–ç»“æ„ç›¸ä¼¼æ€§ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„è®¡åˆ’é€‚åº”ã€‚æˆ‘ä»¬å®ç°äº†ä¸¤ç§DRLç®—æ³•ï¼Œæ·±åº¦Qç½‘ç»œå’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿ç”¨å‰‚é‡ä½“ç§¯ç›´æ–¹å›¾ä½œä¸ºçŠ¶æ€è¡¨ç¤ºå’Œ$22$ç»´çš„ä¼˜å…ˆçº§è°ƒæ•´åŠ¨ä½œç©ºé—´ã€‚å¯¹ä½¿ç”¨å®é™…å†è®¡åˆ’CTæ•°æ®çš„å…«åHNCæ‚£è€…çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸¤ç§æ™ºèƒ½ä½“éƒ½å°†åˆå§‹è®¡åˆ’å¾—åˆ†ä»$120.78 \pm 17.18$æé«˜åˆ°DQNçš„$139.59 \pm 5.50$å’ŒPPOçš„$141.50 \pm 4.69$ï¼Œè¶…è¿‡äº†äººå·¥è§„åˆ’å¸ˆæ‰‹åŠ¨ç”Ÿæˆçš„å†è®¡åˆ’ï¼ˆ$136.32 \pm 4.79$ï¼‰ã€‚ä¸´åºŠéªŒè¯è¯å®ï¼Œè¿™äº›æ”¹è¿›è½¬åŒ–ä¸ºæ›´å¥½çš„è‚¿ç˜¤è¦†ç›–ç‡å’ŒOARä¿æŠ¤ï¼Œè·¨è¶Šå„ç§è§£å‰–ç»“æ„å˜åŒ–ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†DRLåœ¨è§£å†³è‡ªé€‚åº”è´¨å­ç–—æ³•çš„å‡ ä½•å’Œå‰‚é‡å­¦å¤æ‚æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºåœ¨çº¿è‡ªé€‚åº”è´¨å­ç–—æ³•æä¾›æœ‰æ•ˆçš„ç¦»çº¿é€‚åº”è§£å†³æ–¹æ¡ˆå¹¶æ¨åŠ¨å…¶å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10073v2">PDF</a> Published in Proceedings of Machine Learning Research (PMLR) 298;   accepted at Machine Learning for Healthcare Conference (MLHC) 2025</p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è‡ªé€‚åº”è´¨å­æ²»ç–—è®¡åˆ’è°ƒæ•´æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿè§£å‰–å˜åŒ–è®­ç»ƒä¸ªä½“ï¼Œä»¥ä¼˜åŒ–æ²»ç–—è®¡åˆ’å¹¶åº”å¯¹è§£å‰–å˜åŒ–å¸¦æ¥çš„é£é™©ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ·±åº¦Qç½‘ç»œå’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨çŠ¶æ€è¡¨ç¤ºå’ŒåŠ¨ä½œç©ºé—´æ–¹é¢è¿›è¡Œäº†åˆ›æ–°è®¾è®¡ï¼Œæé«˜äº†æ²»ç–—è®¡åˆ’çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¼ºåº¦è°ƒåˆ¶è´¨å­ç–—æ³•ï¼ˆIMPTï¼‰ä¸­ï¼Œè§£å‰–å˜åŒ–å¯èƒ½å¯¼è‡´å¸ƒæ‹‰æ ¼å³°åç§»ï¼Œå¢åŠ è‚¿ç˜¤æ¬ å‰‚é‡å’Œå™¨å®˜é£é™©è¶…å‰‚é‡çš„é£é™©ã€‚</li>
<li>æ²»ç–—å†è®¡åˆ’æ˜¯ç»´æŒä¸´åºŠæ²»ç–—è´¨é‡çš„å…³é”®ï¼Œä½†å½“å‰çš„æ‰‹åŠ¨å†è®¡åˆ’è¿‡ç¨‹èµ„æºå¯†é›†ä¸”è€—æ—¶ã€‚</li>
<li>æè®®ä½¿ç”¨åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰çš„è‡ªåŠ¨åŒ–IMPTå†è®¡åˆ’æ¡†æ¶ï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶è°ƒæ•´ä¼˜åŒ–ä¼˜å…ˆçº§ï¼Œä»¥æœ€å¤§åŒ–è®¡åˆ’è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨æ‚£è€…ç‰¹å®šçš„è®­ç»ƒæ–¹å¼ï¼Œåˆ©ç”¨è§„åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œæ¨¡æ‹Ÿè§£å‰–å˜åŒ–è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®æ–½äº†ä¸¤ç§DRLç®—æ³•ï¼šæ·±åº¦Qç½‘ç»œå’Œè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼Œä½¿ç”¨å‰‚é‡ä½“ç§¯ç›´æ–¹å›¾ä½œä¸ºçŠ¶æ€è¡¨ç¤ºå’Œå…·æœ‰22ç»´åŠ¨ä½œç©ºé—´çš„ä¼˜å…ˆçº§è°ƒæ•´ã€‚</li>
<li>åœ¨å®é™…å†è®¡åˆ’CTæ•°æ®ä¸Šå¯¹8åå¤´é¢ˆç™Œæ‚£è€…çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†åˆå§‹è®¡åˆ’è´¨é‡ï¼Œè¶…è¿‡äº†äººç±»è§„åˆ’å¸ˆæ‰‹åŠ¨ç”Ÿæˆçš„é‡è®¡åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf794ca967f50aab98504909fc27bd9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0303a419ae30d85530c4fc791591d48.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Point-or-Line-Using-Line-based-Representation-for-Panoptic-Symbol-Spotting-in-CAD-Drawings"><a href="#Point-or-Line-Using-Line-based-Representation-for-Panoptic-Symbol-Spotting-in-CAD-Drawings" class="headerlink" title="Point or Line? Using Line-based Representation for Panoptic Symbol   Spotting in CAD Drawings"></a>Point or Line? Using Line-based Representation for Panoptic Symbol   Spotting in CAD Drawings</h2><p><strong>Authors:Xingguang Wei, Haomin Wang, Shenglong Ye, Ruifeng Luo, Yanting Zhang, Lixin Gu, Jifeng Dai, Yu Qiao, Wenhai Wang, Hongjie Zhang</strong></p>
<p>We study the task of panoptic symbol spotting, which involves identifying both individual instances of countable things and the semantic regions of uncountable stuff in computer-aided design (CAD) drawings composed of vector graphical primitives. Existing methods typically rely on image rasterization, graph construction, or point-based representation, but these approaches often suffer from high computational costs, limited generality, and loss of geometric structural information. In this paper, we propose VecFormer, a novel method that addresses these challenges through line-based representation of primitives. This design preserves the geometric continuity of the original primitive, enabling more accurate shape representation while maintaining a computation-friendly structure, making it well-suited for vector graphic understanding tasks. To further enhance prediction reliability, we introduce a Branch Fusion Refinement module that effectively integrates instance and semantic predictions, resolving their inconsistencies for more coherent panoptic outputs. Extensive experiments demonstrate that our method establishes a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and 21.2 points over the second-best results under settings with and without prior information, respectively, highlighting the strong potential of line-based representation as a foundation for vector graphic understanding. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ¶‰åŠè¯†åˆ«è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç»˜å›¾ä¸­çš„å¯è®¡æ•°äº‹ç‰©çš„å•ä¸ªå®ä¾‹å’Œä¸å¯è®¡æ•°å†…å®¹çš„è¯­ä¹‰åŒºåŸŸï¼Œè¿™äº›ç»˜å›¾ç”±çŸ¢é‡å›¾å½¢åŸå§‹å…ƒç´ ç»„æˆã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå›¾åƒçŸ¢é‡åŒ–ã€å›¾æ„å»ºæˆ–åŸºäºç‚¹çš„è¡¨ç¤ºï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§æœ‰é™ä»¥åŠå‡ ä½•ç»“æ„ä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VecFormerè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åŸºäºçº¿æ¡çš„åŸå§‹å…ƒç´ è¡¨ç¤ºæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¿™ç§è®¾è®¡ä¿ç•™äº†åŸå§‹å…ƒç´ çš„å‡ ä½•è¿ç»­æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè®¡ç®—å‹å¥½çš„ç»“æ„çš„åŒæ—¶å®ç°æ›´å‡†ç¡®çš„å½¢çŠ¶è¡¨ç¤ºï¼Œä½¿å…¶éå¸¸é€‚åˆçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é¢„æµ‹å¯é æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†æ”¯èåˆç»†åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—æœ‰æ•ˆåœ°é›†æˆäº†å®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ï¼Œè§£å†³å®ƒä»¬çš„ä¸ä¸€è‡´æ€§ï¼Œä»¥è·å¾—æ›´è¿è´¯çš„å…¨æ™¯è¾“å‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ›é€ äº†æ–°çš„æœ€ä½³æ°´å¹³ï¼Œå®ç°äº†91.1ä¸ªPQå€¼ï¼Œå…¶ä¸­Stuff-PQåœ¨æœ‰æ— å…ˆéªŒä¿¡æ¯çš„è®¾ç½®ä¸‹åˆ†åˆ«æé«˜äº†9.6å’Œ21.2ä¸ªç‚¹ï¼Œè¿™çªæ˜¾äº†åŸºäºçº¿æ¡çš„è¡¨ç¤ºä½œä¸ºçŸ¢é‡å›¾å½¢ç†è§£åŸºç¡€çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23395v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç»˜å›¾ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§æœ‰é™ä»¥åŠå‡ ä½•ç»“æ„ä¿¡æ¯ä¸¢å¤±ç­‰æ–¹é¢çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºçº¿æ¡è¡¨ç¤ºçš„VecFormeræ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¿ç•™åŸå§‹å‡ ä½•è¿ç»­æ€§ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„å½¢çŠ¶è¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—å‹å¥½çš„ç»“æ„ï¼Œéå¸¸é€‚åˆçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜äº†é¢„æµ‹å¯é æ€§ï¼Œå¼•å…¥äº†åˆ†æ”¯èåˆç»†åŒ–æ¨¡å—ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†å®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ï¼Œè§£å†³äº†å®ƒä»¬çš„ä¸ä¸€è‡´æ€§ï¼Œä¸ºå…¨æ™¯è¾“å‡ºæä¾›äº†æ›´è¿è´¯çš„ç»“æœã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å»ºç«‹äº†ä¸€ä¸ªæ–°çš„ä¸–ç•Œçºªå½•ï¼Œåœ¨æœ‰&#x2F;æ— å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†è¶…è¿‡ç¬¬äºŒåç»“æœé«˜è¾¾9.6å’Œ21.2ç‚¹çš„Stuff-PQæå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VecFormerè§£å†³äº†CADç»˜å›¾å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ä¸­è®¡ç®—æˆæœ¬é«˜ã€é€šç”¨æ€§æœ‰é™å’Œä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ã€‚</li>
<li>VecFormeré‡‡ç”¨åŸºäºçº¿æ¡çš„è¡¨ç¤ºæ–¹æ³•ï¼Œä¿ç•™äº†åŸå§‹å‡ ä½•è¿ç»­æ€§ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„å½¢çŠ¶è¡¨ç¤ºã€‚</li>
<li>VecFormerå…·æœ‰è®¡ç®—å‹å¥½çš„ç»“æ„ï¼Œé€‚åˆçŸ¢é‡å›¾å½¢ç†è§£ä»»åŠ¡ã€‚</li>
<li>åˆ†æ”¯èåˆç»†åŒ–æ¨¡å—ç»“åˆäº†å®ä¾‹å’Œè¯­ä¹‰é¢„æµ‹ï¼Œè§£å†³äº†é¢„æµ‹ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>VecFormeråœ¨å…¨æ™¯ç¬¦å·è¯†åˆ«ä»»åŠ¡ä¸Šå–å¾—äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚</li>
<li>åœ¨æœ‰å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒVecFormerçš„Stuff-PQæå‡äº†9.6ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d66a5d9883b79cdd6bbd9ed032f0014.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e103ae8a0c8b56686a26f3c399667775.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b284b97584f8e009b8fd34cacef6124a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-07b87d647c08768811ba838a389ecb82.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  FNH-TTS A Fast, Natural, and Human-Like Speech Synthesis System with   advanced prosodic modeling based on Mixture of Experts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0ff877a25991c88c19217a0fbd26950f.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a   Lightweight Auto3DSeg and SegResNet Implementation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
