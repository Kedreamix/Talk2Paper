<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  MuDRiC Multi-Dialect Reasoning for Arabic Commonsense Validation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-71e9fe80666be9123890181df165ef03.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-20-æ›´æ–°"><a href="#2025-08-20-æ›´æ–°" class="headerlink" title="2025-08-20 æ›´æ–°"></a>2025-08-20 æ›´æ–°</h1><h2 id="MuDRiC-Multi-Dialect-Reasoning-for-Arabic-Commonsense-Validation"><a href="#MuDRiC-Multi-Dialect-Reasoning-for-Arabic-Commonsense-Validation" class="headerlink" title="MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation"></a>MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation</h2><p><strong>Authors:Kareem Elozeiri, Mervat Abassy, Preslav Nakov, Yuxia Wang</strong></p>
<p>Commonsense validation evaluates whether a sentence aligns with everyday human understanding, a critical capability for developing robust natural language understanding systems. While substantial progress has been made in English, the task remains underexplored in Arabic, particularly given its rich linguistic diversity. Existing Arabic resources have primarily focused on Modern Standard Arabic (MSA), leaving regional dialects underrepresented despite their prevalence in spoken contexts. To bridge this gap, we present two key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense dataset incorporating multiple dialects, and (ii) a novel method adapting Graph Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances semantic relationship modeling for improved commonsense validation. Our experimental results demonstrate that this approach achieves superior performance in Arabic commonsense validation. Our work enhances Arabic natural language understanding by providing both a foundational dataset and a novel method for handling its complex variations. To the best of our knowledge, we release the first Arabic multi-dialect commonsense reasoning dataset. </p>
<blockquote>
<p>å¸¸è¯†éªŒè¯æ—¨åœ¨è¯„ä¼°å¥å­æ˜¯å¦ç¬¦åˆæ—¥å¸¸äººç±»ç†è§£ï¼Œè¿™æ˜¯å¼€å‘ç¨³å¥çš„è‡ªç„¶è¯­è¨€ç†è§£ç³»ç»Ÿçš„å…³é”®èƒ½åŠ›ã€‚è™½ç„¶è‹±è¯­æ–¹é¢å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†é˜¿æ‹‰ä¼¯è¯­çš„ä»»åŠ¡ä»ç„¶è¢«å¿½è§†ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°å…¶ä¸°å¯Œçš„è¯­è¨€å¤šæ ·æ€§ã€‚ç°æœ‰çš„é˜¿æ‹‰ä¼¯èµ„æºä¸»è¦é›†ä¸­åœ¨ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰ï¼Œå°½ç®¡å…¶åœ¨å£è¯­è¯­å¢ƒä¸­æ™®éå­˜åœ¨ï¼Œä½†æ–¹è¨€çš„ä½¿ç”¨å´é²œæœ‰æ¶‰åŠã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆiï¼‰æˆ‘ä»¬æ¨å‡ºäº†MuDRiCï¼Œä¸€ä¸ªèå…¥å¤šç§æ–¹è¨€çš„æ‰©å±•é˜¿æ‹‰ä¼¯è¯­å¸¸è¯†æ•°æ®é›†ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€‚åº”é˜¿æ‹‰ä¼¯å¸¸è¯†æ¨ç†çš„å›¾å·ç§¯ç½‘ç»œï¼ˆGCNï¼‰æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¢å¼ºäº†è¯­ä¹‰å…³ç³»å»ºæ¨¡ï¼Œæé«˜äº†å¸¸è¯†éªŒè¯çš„æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é˜¿æ‹‰ä¼¯å¸¸è¯†éªŒè¯æ–¹é¢å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æä¾›åŸºç¡€æ•°æ®é›†å’Œåº”å¯¹å…¶å¤æ‚å˜åŒ–çš„æ–°æ–¹æ³•ï¼Œå¢å¼ºäº†é˜¿æ‹‰ä¼¯è‡ªç„¶è¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ç¬¬ä¸€ä¸ªé˜¿æ‹‰ä¼¯å¤šæ–¹è¨€å¸¸è¯†æ¨ç†æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13130v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é˜¿æ‹‰ä¼¯è¯­çš„å¸¸è¯†éªŒè¯ç ”ç©¶ã€‚è¯¥ç ”ç©¶é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­çš„å¤šè¯­ç§ç‰¹æ€§ï¼Œæå‡ºä¸¤ä¸ªé‡è¦è´¡çŒ®ï¼šä¸€æ˜¯æ¨å‡ºäº†åŒ…å«å¤šç§æ–¹è¨€çš„MuDRiCé˜¿æ‹‰ä¼¯è¯­å¸¸è¯†æ•°æ®é›†ï¼›äºŒæ˜¯åˆ›æ–°åœ°é‡‡ç”¨å›¾å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰è¿›è¡Œé˜¿æ‹‰ä¼¯è¯­å¸¸è¯†æ¨ç†ï¼Œæå‡äº†è¯­ä¹‰å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå®ç°é˜¿æ‹‰ä¼¯è¯­çš„å¸¸è¯†éªŒè¯çš„ä¼˜è¶Šæ€§èƒ½ã€‚è¯¥ç ”ç©¶å¢å¼ºäº†é˜¿æ‹‰ä¼¯è¯­çš„è‡ªç„¶è¯­è¨€ç†è§£ï¼Œå¡«è¡¥äº†é˜¿æ‹‰ä¼¯å¤šç§æ–¹è¨€å¤„ç†é¢†åŸŸçš„ç©ºç™½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¸¸è¯†éªŒè¯æ˜¯è¯„ä¼°å¥å­æ˜¯å¦ç¬¦åˆæ—¥å¸¸äººç±»ç†è§£çš„é‡è¦èƒ½åŠ›ï¼Œå¯¹äºå¼€å‘ç¨³å¥çš„è‡ªç„¶è¯­è¨€ç†è§£ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>è‹±è¯­é¢†åŸŸçš„å¸¸è¯†éªŒè¯å·²ç»å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>ç°æœ‰é˜¿æ‹‰ä¼¯è¯­èµ„æºä¸»è¦é›†ä¸­åœ¨ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰ï¼Œå¿½è§†äº†åœ°åŒºæ–¹è¨€çš„ä¸°å¯Œæ€§ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæ¨å‡ºäº†åŒ…å«å¤šç§æ–¹è¨€çš„MuDRiCé˜¿æ‹‰ä¼¯è¯­å¸¸è¯†æ•°æ®é›†ã€‚</li>
<li>åˆ›æ–°åœ°ä½¿ç”¨å›¾å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰è¿›è¡Œé˜¿æ‹‰ä¼¯è¯­å¸¸è¯†æ¨ç†ï¼Œæå‡è¯­ä¹‰å…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•åœ¨é˜¿æ‹‰ä¼¯è¯­å¸¸è¯†éªŒè¯ä¸­å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6366277618d722380bf1f30b80aaaf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ff60c1a82fc2a11b481918756b933bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63377d8fd801587fe3022442a544eef2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3510ff0e1ffe8430cf003c1bf88128.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f9d6ab5f699f3ea8b31f0565aab1c48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e74bcc86dd17cf2b8827b174bb48fbdc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Contrastive-Representations-for-Temporal-Reasoning"><a href="#Contrastive-Representations-for-Temporal-Reasoning" class="headerlink" title="Contrastive Representations for Temporal Reasoning"></a>Contrastive Representations for Temporal Reasoning</h2><p><strong>Authors:Alicja Ziarko, Michal Bortkiewicz, Michal Zawalski, Benjamin Eysenbach, Piotr Milos</strong></p>
<p>In classical AI, perception relies on learning state-based representations, while planning, which can be thought of as temporal reasoning over action sequences, is typically achieved through search. We study whether such reasoning can instead emerge from representations that capture both perceptual and temporal structure. We show that standard temporal contrastive learning, despite its popularity, often fails to capture temporal structure due to its reliance on spurious features. To address this, we introduce Combinatorial Representations for Temporal Reasoning (CRTR), a method that uses a negative sampling scheme to provably remove these spurious features and facilitate temporal reasoning. CRTR achieves strong results on domains with complex temporal structure, such as Sokoban and Rubikâ€™s Cube. In particular, for the Rubikâ€™s Cube, CRTR learns representations that generalize across all initial states and allow it to solve the puzzle using fewer search steps than BestFS, though with longer solutions. To our knowledge, this is the first method that efficiently solves arbitrary Cube states using only learned representations, without relying on an external search algorithm. </p>
<blockquote>
<p>åœ¨ç»å…¸äººå·¥æ™ºèƒ½ä¸­ï¼Œæ„ŸçŸ¥ä¾èµ–äºåŸºäºçŠ¶æ€çš„å­¦ä¹ è¡¨ç¤ºï¼Œè€Œè§„åˆ’å¯ä»¥è¢«è§†ä¸ºå¯¹åŠ¨ä½œåºåˆ—çš„æ—¶é—´æ¨ç†ï¼Œé€šå¸¸é€šè¿‡æœç´¢æ¥å®ç°ã€‚æˆ‘ä»¬ç ”ç©¶æ˜¯å¦è¿™ç§æ¨ç†å¯ä»¥æ¥æºäºåŒæ—¶æ•æ‰æ„ŸçŸ¥å’Œæ—¶é—´ç»“æ„çš„è¡¨ç¤ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå°½ç®¡æ ‡å‡†çš„æ—¶é—´å¯¹æ¯”å­¦ä¹ å¾ˆå—æ¬¢è¿ï¼Œä½†ç”±äºå®ƒä¾èµ–äºè™šå‡ç‰¹å¾ï¼Œé€šå¸¸æ— æ³•æ•æ‰æ—¶é—´ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºæ—¶é—´æ¨ç†çš„ç»„åˆè¡¨ç¤ºï¼ˆCRTRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è´Ÿé‡‡æ ·æ–¹æ¡ˆæ¥è¯å®æ¶ˆé™¤è¿™äº›è™šå‡ç‰¹å¾å¹¶ä¿ƒè¿›æ—¶é—´æ¨ç†ã€‚CRTRåœ¨å…·æœ‰å¤æ‚æ—¶é—´ç»“æ„çš„é¢†åŸŸï¼ˆå¦‚ Sokoban å’Œé­”æ–¹ï¼‰å–å¾—äº†å¾ˆå¥½çš„ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹äºé­”æ–¹é—®é¢˜ï¼ŒCRTRå­¦ä¹ çš„è¡¨ç¤ºå¯ä»¥æ¦‚æ‹¬æ‰€æœ‰åˆå§‹çŠ¶æ€ï¼Œå¹¶å…è®¸å®ƒä½¿ç”¨æ¯”BestFSæ›´å°‘çš„æœç´¢æ­¥éª¤æ¥è§£å†³è°œé¢˜ï¼Œå°½ç®¡è§£å†³æ–¹æ¡ˆæ›´é•¿ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§ä»…ä½¿ç”¨å­¦ä¹ åˆ°çš„è¡¨ç¤ºæ¥æœ‰æ•ˆåœ°è§£å†³ä»»æ„é­”æ–¹çŠ¶æ€çš„æ–¹æ³•ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æœç´¢ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13113v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://princeton-rl.github.io/CRTR/">https://princeton-rl.github.io/CRTR/</a></p>
<p><strong>Summary</strong><br>     ç»å…¸äººå·¥æ™ºèƒ½ä¸­ï¼Œæ„ŸçŸ¥ä¾èµ–äºåŸºäºçŠ¶æ€è¡¨ç¤ºçš„å­¦ä¹ ï¼Œè€Œè§„åˆ’é€šå¸¸é€šè¿‡æœç´¢å®ç°ï¼Œè¢«è§†ä¸ºåŠ¨ä½œåºåˆ—çš„æ—¶é—´æ¨ç†ã€‚æœ¬ç ”ç©¶æ¢ç´¢æ˜¯å¦å¯ä»åŒæ—¶æ•æ‰æ„ŸçŸ¥å’Œæ—¶é—´ç»“æ„çš„è¡¨ç¤ºä¸­æ¶Œç°å‡ºè¿™ç§æ¨ç†ã€‚æ ‡å‡†æ—¶é—´å¯¹æ¯”å­¦ä¹ è™½å—æ¬¢è¿ï¼Œä½†ç”±äºä¾èµ–å¶ç„¶ç‰¹å¾ï¼Œå¾€å¾€æ— æ³•æ•æ‰æ—¶é—´ç»“æ„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç»„åˆè¡¨ç¤ºæ³•ç”¨äºæ—¶é—´æ¨ç†ï¼ˆCRTRï¼‰ï¼Œé€šè¿‡è´Ÿé‡‡æ ·æ–¹æ¡ˆç§»é™¤è¿™äº›å¶ç„¶ç‰¹å¾ï¼Œä¿ƒè¿›æ—¶é—´æ¨ç†ã€‚CRTRåœ¨å…·æœ‰å¤æ‚æ—¶é—´ç»“æ„çš„é¢†åŸŸï¼ˆå¦‚ç´¢è´å…‹å’Œé­”æ–¹ï¼‰å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹äºé­”æ–¹é—®é¢˜ï¼ŒCRTRå­¦ä¹ çš„è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæ³›åŒ–æ‰€æœ‰åˆå§‹çŠ¶æ€ï¼Œå‡å°‘æœç´¢æ­¥éª¤ï¼Œè™½è§£å†³æ–¹æ¡ˆè¾ƒé•¿ï¼Œä½†ä»ä¼˜äºBestFSã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ä»…é€šè¿‡å­¦åˆ°çš„è¡¨ç¤ºï¼Œä¸ä¾èµ–å¤–éƒ¨æœç´¢ç®—æ³•ï¼Œå°±èƒ½é«˜æ•ˆè§£å†³ä»»æ„çš„é­”æ–¹çŠ¶æ€çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»å…¸äººå·¥æ™ºèƒ½ä¸­æ„ŸçŸ¥å’Œè§„åˆ’æ˜¯åˆ†ç¦»çš„ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†é€šè¿‡ç»Ÿä¸€è¡¨ç¤ºæ³•å®ç°æ„ŸçŸ¥å’Œæ—¶é—´æ¨ç†çš„æ•´åˆã€‚</li>
<li>æ ‡å‡†æ—¶é—´å¯¹æ¯”å­¦ä¹ å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•æœ‰æ•ˆæ•æ‰æ—¶é—´ç»“æ„ï¼Œæœ¬ç ”ç©¶æå‡ºçš„CRTRæ–¹æ³•èƒ½å¤Ÿå…‹æœè¿™ä¸€ç¼ºé™·ã€‚</li>
<li>CRTRåˆ©ç”¨è´Ÿé‡‡æ ·æ–¹æ¡ˆç§»é™¤å¶ç„¶ç‰¹å¾ï¼Œä¿ƒè¿›æ—¶é—´æ¨ç†ã€‚</li>
<li>CRTRåœ¨è§£å†³å…·æœ‰å¤æ‚æ—¶é—´ç»“æ„çš„ä»»åŠ¡ï¼ˆå¦‚ç´¢è´å…‹å’Œé­”æ–¹ï¼‰æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>å¯¹äºé­”æ–¹é—®é¢˜ï¼ŒCRTRå­¦ä¹ çš„è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿæ³›åŒ–æ‰€æœ‰åˆå§‹çŠ¶æ€ï¼Œå‡å°‘æœç´¢æ­¥éª¤ã€‚</li>
<li>CRTRæ˜¯é¦–ä¸ªä»…é€šè¿‡å­¦åˆ°çš„è¡¨ç¤ºè§£å†³ä»»æ„çš„é­”æ–¹çŠ¶æ€çš„æ–¹æ³•ï¼Œä¸ä¾èµ–å¤–éƒ¨æœç´¢ç®—æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ•´åˆæ„ŸçŸ¥å’Œæ—¶é—´æ¨ç†æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61b79adf6524fe6f244d51629c34e0b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-617662a69939b2ef783dbb104fc300e5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Buyuk-Dil-Modelleri-icin-TR-MMLU-Benchmarki-Performans-Degerlendirmesi-Zorluklar-ve-Iyilestirme-Firsatlari"><a href="#Buyuk-Dil-Modelleri-icin-TR-MMLU-Benchmarki-Performans-Degerlendirmesi-Zorluklar-ve-Iyilestirme-Firsatlari" class="headerlink" title="BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans   DeÄŸerlendirmesi, Zorluklar ve Ä°yileÅŸtirme FÄ±rsatlarÄ±"></a>BÃ¼yÃ¼k Dil Modelleri iÃ§in TR-MMLU BenchmarkÄ±: Performans   DeÄŸerlendirmesi, Zorluklar ve Ä°yileÅŸtirme FÄ±rsatlarÄ±</h2><p><strong>Authors:M. Ali Bayram, Ali Arda Fincan, Ahmet Semih GÃ¼mÃ¼ÅŸ, Banu Diri, SavaÅŸ YÄ±ldÄ±rÄ±m, Ã–ner AytaÅŸ</strong></p>
<p>Language models have made significant advancements in understanding and generating human language, achieving remarkable success in various applications. However, evaluating these models remains a challenge, particularly for resource-limited languages like Turkish. To address this issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a meticulously curated dataset comprising 6,200 multiple-choice questions across 62 sections within the Turkish education system. This benchmark provides a standard framework for Turkish NLP research, enabling detailed analyses of LLMsâ€™ capabilities in processing Turkish text. In this study, we evaluated state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model design. TR-MMLU sets a new standard for advancing Turkish NLP research and inspiring future innovations. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†éå‡¡çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒåœŸè€³å…¶è¯­è¿™æ ·çš„èµ„æºå—é™è¯­è¨€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åœŸè€³å…¶MMLUï¼ˆTR-MMLUï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åœŸè€³å…¶è¯­ä¸­çš„è¯­è¨€å’Œæ¦‚å¿µèƒ½åŠ›ã€‚TR-MMLUåŸºäºç²¾å¿ƒç­›é€‰çš„æ•°æ®é›†æ„å»ºï¼ŒåŒ…å«åœŸè€³å…¶æ•™è‚²ä½“ç³»ä¸­62ä¸ªéƒ¨åˆ†çš„6200ä¸ªé€‰æ‹©é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºåœŸè€³å…¶NLPç ”ç©¶æä¾›äº†ä¸€ä¸ªæ ‡å‡†æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹LLMå¤„ç†åœŸè€³å…¶æ–‡æœ¬çš„èƒ½åŠ›è¿›è¡Œè¯¦ç»†åˆ†æã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨TR-MMLUä¸Šè¯„ä¼°äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œçªå‡ºäº†æ¨¡å‹è®¾è®¡æ–¹é¢çš„æ”¹è¿›é¢†åŸŸã€‚TR-MMLUä¸ºæ¨è¿›åœŸè€³å…¶NLPç ”ç©¶è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå¹¶æ¿€åŠ±äº†æœªæ¥çš„åˆ›æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13044v1">PDF</a> 10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd   Signal Processing and Communications Applications Conference (SIU), 25â€“28   June 2025, Sile, Istanbul, T&quot;urkiye</p>
<p><strong>Summary</strong></p>
<p>è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†ä»¤äººç©ç›®çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œå°¤å…¶æ˜¯èµ„æºæœ‰é™çš„åœŸè€³å…¶è¯­è¯„ä¼°ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åœŸè€³å…¶MMLUï¼ˆTR-MMLUï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åœŸè€³å…¶è¯­è¨€ä¸Šçš„è¯­è¨€èƒ½åŠ›å’Œæ¦‚å¿µèƒ½åŠ›ã€‚åŸºäºç²¾å¿ƒæ„å»ºçš„åŒ…å«åœŸè€³å…¶æ•™è‚²ä½“ç³»ä¸­æ¶µç›–çš„å…­åå¤šä¸ªé¢†åŸŸçš„å…­åƒå¤šä¸ªé€‰æ‹©é¢˜çš„æ•°æ®é›†ï¼ŒTR-MMLUä¸ºåœŸè€³å…¶NLPç ”ç©¶æä¾›äº†ä¸€ä¸ªæ ‡å‡†æ¡†æ¶ï¼Œèƒ½å¤Ÿè¯¦ç»†åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†åœŸè€³å…¶æ–‡æœ¬çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„LLMåœ¨TR-MMLUä¸Šçš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ¨¡å‹è®¾è®¡çš„é‡ç‚¹æ–¹å‘ã€‚TR-MMLUä¸ºæ¨è¿›åœŸè€³å…¶NLPç ”ç©¶å¹¶æ¿€å‘æœªæ¥åˆ›æ–°è®¾ç«‹äº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>å¯¹äºèµ„æºæœ‰é™çš„åœŸè€³å…¶è¯­çš„è¯„ä¼°ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åœŸè€³å…¶è¯­è¨€ä¸Šçš„è¡¨ç°ï¼Œå¼•å…¥äº†TR-MMLUåŸºå‡†æµ‹è¯•ã€‚</li>
<li>TR-MMLUæ˜¯åŸºäºæ¶µç›–åœŸè€³å…¶æ•™è‚²ä½“ç³»ä¸­å…­åå¤šä¸ªé¢†åŸŸçš„ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†æ„å»ºçš„ã€‚</li>
<li>TR-MMLUæä¾›äº†ä¸€ä¸ªæ ‡å‡†æ¡†æ¶æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†åœŸè€³å…¶æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>æœ€å…ˆè¿›çš„LLMåœ¨TR-MMLUä¸Šçš„è¡¨ç°å·²ç»å¾—åˆ°è¯„ä¼°ï¼Œçªå‡ºäº†æ¨¡å‹è®¾è®¡ä¸­çš„è–„å¼±ç¯èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11c268099d94bb40bea0475141fb7104.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2af3e5454a3f0385cb67bd15a6ae933.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e28104f76475e3b88d447d1684397c85.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Can-Large-Models-Teach-Student-Models-to-Solve-Mathematical-Problems-Like-Human-Beings-A-Reasoning-Distillation-Method-via-Multi-LoRA-Interaction"><a href="#Can-Large-Models-Teach-Student-Models-to-Solve-Mathematical-Problems-Like-Human-Beings-A-Reasoning-Distillation-Method-via-Multi-LoRA-Interaction" class="headerlink" title="Can Large Models Teach Student Models to Solve Mathematical Problems   Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction"></a>Can Large Models Teach Student Models to Solve Mathematical Problems   Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</h2><p><strong>Authors:Xinhe Li, Jiajun Liu, Peng Wang</strong></p>
<p>Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¾ˆå¼ºçš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†è¿™ä¾èµ–äºå…¶æ•°ç™¾äº¿çš„å‚æ•°ã€‚ä¸ºäº†è§£å†³å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ¨ç†èƒ½åŠ›è–„å¼±çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤§é‡æ•°æ®è¿›è¡Œå¡«é¸­å¼è®­ç»ƒã€‚åœ¨å¿ƒç†å­¦ä¸­ï¼Œè¿™ä¸ç³»ç»Ÿ1æ€ç»´ç±»ä¼¼ï¼Œå³åŸºäºç»éªŒå’Œç›´è§‰å¿«é€Ÿè§£å†³æ¨ç†é—®é¢˜ã€‚ç„¶è€Œï¼Œäººç±»å­¦ä¹ è¿˜éœ€è¦ç³»ç»Ÿ2æ€ç»´ï¼Œå…ˆè·å–çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å®è·µåŠ ä»¥å·©å›ºã€‚å—è¿™ä¸¤ç§ä¸åŒæ€ç»´æ¨¡å¼çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šLoRAäº¤äº’çš„æ•°å­¦æ¨ç†è’¸é¦ï¼ˆLoRIDï¼‰çš„æ–°æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ ·æœ¬çš„é—®é¢˜å’Œæ¨ç†è¾“å…¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºçŸ¥è¯†å¢å¼ºæ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å­¦ç”Ÿæ¨¡å‹ä¸Šè®­ç»ƒLoRAå—ä½œä¸ºç›´è§‰æ¨ç†å™¨ï¼ˆIRï¼‰ï¼Œç›´æ¥ç”Ÿæˆè§£å†³é—®é¢˜çš„æ€ç»´é“¾ã€‚æ¥ç€ï¼Œä¸ºäº†æ¨¡ä»¿ç³»ç»Ÿ2æ€ç»´ï¼Œæˆ‘ä»¬åˆ†åˆ«è®­ç»ƒçŸ¥è¯†ç”Ÿæˆå™¨ï¼ˆKGï¼‰å’Œæ·±åº¦æ¨ç†å™¨ï¼ˆDRï¼‰ã€‚å‰è€…åœ¨æ¥æ”¶åˆ°é—®é¢˜ååªè¾“å‡ºçŸ¥è¯†ï¼Œåè€…åˆ™åˆ©ç”¨è¿™äº›çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚æœ€åï¼Œä¸ºäº†è§£å†³IRå’ŒDRç”Ÿæˆè¿‡ç¨‹ä¸­çš„éšæœºæ€§ï¼Œæˆ‘ä»¬è¯„ä¼°ä»–ä»¬çš„è¾“å‡ºæ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™éœ€è¦è¿­ä»£æ¨ç†è¿‡ç¨‹ã€‚è¿™ä¸€æ­¥å¯ä»¥é€šè¿‡ç›¸äº’åé¦ˆå¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRIDè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨GSM8Kæ•°æ®é›†ä¸Šï¼Œä¸ç¬¬äºŒåæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨äº”ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†2.3%ã€16.1%ã€2.4%ã€12.3%å’Œ1.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13037v1">PDF</a> Accepted by IJCAI2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„æ¨ç†èƒ½åŠ›è¾ƒå¼±ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åˆ©ç”¨LLMsç”Ÿæˆå¤§é‡æ•°æ®è¿›è¡Œå¡«é¸­å¼è®­ç»ƒæ¥æå‡SLMsçš„æ¨ç†èƒ½åŠ›ã€‚å—å¿ƒç†å­¦ä¸­çš„ç³»ç»Ÿ1å’Œç³»ç»Ÿ2æ€ç»´çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šLoRAäº¤äº’çš„æ•°å­¦æ¨ç†è’¸é¦ï¼ˆLoRIDï¼‰æ–°æ–¹æ³•ã€‚LoRIDé€šè¿‡LLMåˆ›å»ºçŸ¥è¯†å¢å¼ºæ•°æ®é›†ï¼Œè®­ç»ƒç›´è§‰æ¨ç†å™¨ï¼ˆIRï¼‰ï¼Œå¹¶æ¨¡ä»¿ç³»ç»Ÿ2æ€ç»´è¿›è¡ŒçŸ¥è¯†ç”Ÿæˆå’Œæ·±åº¦æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRIDåœ¨GSM8Kæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œç›¸è¾ƒäºç¬¬äºŒåæ–¹æ³•ï¼Œå‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†2.3%ã€16.1%ã€2.4%ã€12.3%å’Œ1.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å€ŸåŠ©LLMsç”Ÿæˆæ•°æ®ä»¥è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹(SLMs)ï¼Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>äººè„‘åŒ…å«å¿«é€Ÿè§£å†³é—®é¢˜çš„ç³»ç»Ÿ1æ€ç»´ï¼ˆåŸºäºç»éªŒå’Œç›´è§‰ï¼‰å’ŒçŸ¥è¯†è·å–å¹¶é€šè¿‡å®è·µå¼ºåŒ–çš„ç³»ç»Ÿ2æ€ç»´ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºLoRIDçš„æ–°æ–¹æ³•ï¼Œç»“åˆç³»ç»Ÿ1å’Œç³»ç»Ÿ2æ€ç»´çš„æ¨¡å¼æ¥æå‡SLMsçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LoRIDé€šè¿‡LLMåˆ›å»ºçŸ¥è¯†å¢å¼ºæ•°æ®é›†ï¼Œå¹¶è®­ç»ƒç›´è§‰æ¨ç†å™¨ï¼ˆIRï¼‰ã€çŸ¥è¯†ç”Ÿæˆå™¨ï¼ˆKGï¼‰å’Œæ·±åº¦æ¨ç†å™¨ï¼ˆDRï¼‰ã€‚</li>
<li>LoRIDåœ¨GSM8Kæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®ç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10318d949e17ceb3f0253078487e21c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd7078e96ffae7bb81cac64b8ff6acc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b05bf9b146a20753809be6504cb03b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565da1b7129bd2e13c0c89a8fbb6551b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="G-2-RPO-A-Guided-Group-Relative-Policy-Optimization-with-Adaptive-Guidance"><a href="#G-2-RPO-A-Guided-Group-Relative-Policy-Optimization-with-Adaptive-Guidance" class="headerlink" title="G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive   Guidance"></a>G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive   Guidance</h2><p><strong>Authors:Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced the reasoning abilities of large language models (LLMs). Its success, however, largely depends on strong base models with rich world knowledge, yielding only modest improvements for small-size language models (SLMs). To address this limitation, we investigate Guided GRPO, which injects ground-truth reasoning steps into roll-out trajectories to compensate for SLMsâ€™ inherent weaknesses. Through a comprehensive study of various guidance configurations, we find that naively adding guidance delivers limited gains. These insights motivate G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength in response to the modelâ€™s evolving training dynamics. Experiments on mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A substantially outperforms vanilla GRPO. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/T-Lab-CUHKSZ/G2RPO-A">https://github.com/T-Lab-CUHKSZ/G2RPO-A</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå…·æœ‰ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†çš„å¼ºå¤§åŸºç¡€æ¨¡å‹ï¼Œå¯¹äºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ”¹è¿›ä½œç”¨æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¼•å¯¼GRPOæ–¹æ³•ï¼Œå®ƒé€šè¿‡å‘æ»šåŠ¨è½¨è¿¹ä¸­æ³¨å…¥çœŸå®æ¨ç†æ­¥éª¤æ¥å¼¥è¡¥SLMçš„å›ºæœ‰å¼±ç‚¹ã€‚é€šè¿‡å¯¹å„ç§å¼•å¯¼é…ç½®çš„ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ç®€å•åœ°æ·»åŠ å¼•å¯¼å¸¦æ¥çš„å¢ç›Šæœ‰é™ã€‚è¿™äº›è§è§£ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†G$^2$RPO-Aç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”ç®—æ³•ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ ¹æ®æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€å˜åŒ–è°ƒæ•´æŒ‡å¯¼åŠ›åº¦ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯å®ï¼ŒG$^2$RPO-Aæ˜¾è‘—ä¼˜äºæ™®é€šGRPOã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/T-Lab-CUHKSZ/G2RPO-A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/T-Lab-CUHKSZ/G2RPO-Aè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13023v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå…·æœ‰ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†çš„å¼ºåŸºæ¨¡å‹ï¼Œå¯¹äºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ”¹è¿›å¹…åº¦è¾ƒå°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¼•å¯¼GRPOæ–¹æ³•ï¼Œé€šè¿‡å‘è½¨è¿¹ä¸­æ·»åŠ çœŸå®æ¨ç†æ­¥éª¤æ¥å¼¥è¡¥SLMçš„å›ºæœ‰å¼±ç‚¹ã€‚é€šè¿‡å¯¹ä¸åŒå¼•å¯¼é…ç½®çš„ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ç›²ç›®æ·»åŠ å¼•å¯¼äº§ç”Ÿçš„æ”¶ç›Šæœ‰é™ã€‚è¿™äº›è§è§£å‚¬ç”Ÿäº†G$^2$RPO-Aè‡ªé€‚åº”ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯è‡ªåŠ¨æ ¹æ®æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€è°ƒæ•´å¼•å¯¼å¼ºåº¦ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¯å®ï¼ŒG$^2$RPO-Aå¤§å¹…ä¼˜äºæ ‡å‡†GRPOã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/T-Lab-CUHKSZ/G2RPO-A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/T-Lab-CUHKSZ/G2RPO-Aè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæŠ€æœ¯æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¯¹äºå°å‹è¯­è¨€æ¨¡å‹ï¼ŒRLVRçš„æ”¹è¿›æ•ˆæœæœ‰é™ã€‚</li>
<li>å¼•å¯¼GRPOæ–¹æ³•é€šè¿‡æ·»åŠ çœŸå®æ¨ç†æ­¥éª¤æ¥å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç›²ç›®æ·»åŠ å¼•å¯¼ä¸ä¼šäº§ç”Ÿæ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>G$^2$RPO-Aç®—æ³•èƒ½æ ¹æ®æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€è‡ªåŠ¨è°ƒæ•´å¼•å¯¼å¼ºåº¦ã€‚</li>
<li>G$^2$RPO-Aåœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ä¼˜äºæ ‡å‡†GRPOã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-baed517d2979aa469fb66430da3ed8a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a74731f080797d4034de8f8344ad5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e2d1a05a3a29abc57f71576e1d4a325.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a186ba1289e23df00f40322bd5fc68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a82b7f03c6eb184cf4b9bebb67c505a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d235fea4f86730c000e678472fd3bc76.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PC-Sampler-Position-Aware-Calibration-of-Decoding-Bias-in-Masked-Diffusion-Models"><a href="#PC-Sampler-Position-Aware-Calibration-of-Decoding-Bias-in-Masked-Diffusion-Models" class="headerlink" title="PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked   Diffusion Models"></a>PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked   Diffusion Models</h2><p><strong>Authors:Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Tong Xiao</strong></p>
<p>Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/NEUIR/PC-Sampler">https://github.com/NEUIR/PC-Sampler</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ©è†œæ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰çš„è¿›å±•ä¸ºåºåˆ—ç”Ÿæˆæä¾›äº†å¼ºå¤§çš„éè‡ªå›å½’æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼ŒMDMsçš„ç”Ÿæˆè´¨é‡å¯¹è§£ç ç­–ç•¥çš„é€‰æ‹©ä»ç„¶é«˜åº¦æ•æ„Ÿã€‚ç‰¹åˆ«æ˜¯å¹¿æ³›é‡‡ç”¨çš„åŸºäºä¸ç¡®å®šæ€§çš„é‡‡æ ·å™¨å­˜åœ¨ä¸¤å¤§å±€é™ï¼šç¼ºä¹å…¨å±€è½¨è¿¹æ§åˆ¶å’Œåœ¨è§£ç æ—©æœŸé˜¶æ®µå¯¹å¹³å‡¡ç¬¦å·çš„æ˜æ˜¾åå‘ã€‚è¿™äº›ç¼ºç‚¹é™åˆ¶äº†MDMsçš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½ç½®æ„ŸçŸ¥ç½®ä¿¡æ ¡å‡†é‡‡æ ·ï¼ˆPC-Samplerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§£ç ç­–ç•¥ï¼Œå®ƒå°†å…¨å±€è½¨è¿¹è§„åˆ’ä¸å†…å®¹æ„ŸçŸ¥çš„ä¿¡æ¯æœ€å¤§åŒ–ç›¸ç»“åˆã€‚PC-Sampleré‡‡ç”¨ä½ç½®æ„ŸçŸ¥åŠ æƒæœºåˆ¶æ¥è°ƒèŠ‚è§£ç è·¯å¾„ï¼Œå¹¶ä½¿ç”¨æ ¡å‡†ç½®ä¿¡åº¦æ¥æŠ‘åˆ¶å¹³å‡¡ç¬¦å·çš„è¿‡æ—©é€‰æ‹©ã€‚åœ¨ä¸‰ä¸ªå…ˆè¿›çš„MDMså’Œä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒâ€”â€”åŒ…æ‹¬é€»è¾‘æ¨ç†å’Œä»»åŠ¡è§„åˆ’â€”â€”è¯æ˜ï¼ŒPC-Samplerå¹³å‡è¶…å‡ºç°æœ‰MDMè§£ç ç­–ç•¥è¶…è¿‡10%çš„æ€§èƒ½ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸æœ€æ–°è‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NEUIR/PC-Sampler%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NEUIR/PC-Sampleræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13021v1">PDF</a> 17 pages,13 figures</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸéè‡ªå›å½’åºåˆ—ç”Ÿæˆæ¨¡å‹ä¸­çš„è¿›å±•å±•ç¤ºäº†Masked Diffusion Modelsï¼ˆMDMsï¼‰çš„å¼ºå¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåˆæ­¥å®éªŒå‘ç°å…¶ç”Ÿæˆè´¨é‡å¯¹è§£ç ç­–ç•¥çš„é€‰æ‹©éå¸¸æ•æ„Ÿã€‚å½“å‰å¹¿æ³›é‡‡ç”¨çš„åŸºäºä¸ç¡®å®šæ€§çš„é‡‡æ ·å™¨å­˜åœ¨å…¨å±€è½¨è¿¹æ§åˆ¶ç¼ºå¤±ä»¥åŠåœ¨è§£ç æ—©æœŸé˜¶æ®µæ˜æ˜¾åå‘äºå¹³å‡¡ç¬¦å·çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPosition-Aware Confidence-Calibrated Samplingï¼ˆPC-Samplerï¼‰çš„æ–°å‹è§£ç ç­–ç•¥ï¼Œå®ƒç»“åˆäº†å…¨å±€è½¨è¿¹è§„åˆ’å’Œå†…å®¹æ„ŸçŸ¥ä¿¡æ¯æœ€å¤§åŒ–ã€‚PC-Sampleré€šè¿‡ä½ç½®æ„ŸçŸ¥åŠ æƒæœºåˆ¶è°ƒæ§è§£ç è·¯å¾„ï¼Œå¹¶ä½¿ç”¨æ ¡å‡†ç½®ä¿¡åº¦å¾—åˆ†æŠ‘åˆ¶å¹³å‡¡ç¬¦å·çš„è¿‡æ—©é€‰æ‹©ã€‚åœ¨å¤šä¸ªé«˜çº§MDMå’Œä¸ƒä¸ªæŒ‘æˆ˜æ€§åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPC-Samplerå¹³å‡æ€§èƒ½ä¼˜äºç°æœ‰MDMè§£ç ç­–ç•¥è¶…è¿‡10%ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸æœ€æ–°è‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Masked Diffusion Models (MDMs) æ˜¯å¼ºå¤§çš„éè‡ªå›å½’åºåˆ—ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>åˆæ­¥å®éªŒå‘ç°MDMsçš„ç”Ÿæˆè´¨é‡å¯¹è§£ç ç­–ç•¥æ•æ„Ÿã€‚</li>
<li>ç°æœ‰ä¸ç¡®å®šæ€§é‡‡æ ·å™¨å­˜åœ¨å…¨å±€è½¨è¿¹æ§åˆ¶ç¼ºå¤±å’Œå¯¹å¹³å‡¡ç¬¦å·çš„åè§é—®é¢˜ã€‚</li>
<li>æå‡ºæ–°å‹è§£ç ç­–ç•¥PC-Samplerï¼Œç»“åˆå…¨å±€è½¨è¿¹è§„åˆ’å’Œå†…å®¹æ„ŸçŸ¥ä¿¡æ¯æœ€å¤§åŒ–ã€‚</li>
<li>PC-Sampleré€šè¿‡ä½ç½®æ„ŸçŸ¥åŠ æƒå’Œæ ¡å‡†ç½®ä¿¡åº¦å¾—åˆ†æœºåˆ¶ä¼˜åŒ–è§£ç ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜PC-Sampleræ˜¾è‘—æé«˜äº†MDMsçš„æ€§èƒ½ï¼Œå¹³å‡ä¼˜äºç°æœ‰è§£ç ç­–ç•¥è¶…è¿‡10%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bd85db9a531d99dd49041d661e850ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0fcdede9b1ce32e1aad6d6b444c35b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fcaef394c2d41cc71d9fa6cf3c30d7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64ddf32fddca51d55ca49c6852785166.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EvolMathEval-Towards-Evolvable-Benchmarks-for-Mathematical-Reasoning-via-Evolutionary-Testing"><a href="#EvolMathEval-Towards-Evolvable-Benchmarks-for-Mathematical-Reasoning-via-Evolutionary-Testing" class="headerlink" title="EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning   via Evolutionary Testing"></a>EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning   via Evolutionary Testing</h2><p><strong>Authors:Shengbo Wang, Mingwei Liu, Zike Li, Anji Li, Yanlin Wang, Xin Peng, Zibin Zheng</strong></p>
<p>The rapid advancement of LLMs poses a significant challenge to existing mathematical reasoning benchmarks. These benchmarks commonly suffer from issues such as score saturation, temporal decay, and data contamination. To address this challenge, this paper introduces EvolMathEval, an automated mathematical benchmark generation and evolution framework based on evolutionary testing. By dynamically generating unique evaluation instances ab initio, the framework fundamentally eliminates the risk of data contamination, and ensuring the benchmark remains perpetually challenging for future models.The core mechanisms of EvolMathEval include: seed problem generation based on reverse engineering with algebraic guarantees; multi-dimensional genetic operators designed to inject diverse cognitive challenges; and a composite fitness function that can rapidly and accurately assess problem difficulty. Experimental results demonstrate that the proposed composite fitness function can efficiently and precisely quantify the difficulty of mathematical problems. Furthermore, EvolMathEval can not only generate a large volume of high-difficulty problems through continuous self-iteration, but it can also significantly enhance the complexity of public datasets like GSM8K through evolution, reducing model accuracy by an average of 48%. Deeper investigation reveals that when solving these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to bypass complex multi-step logical reasoning, consequently leading to incorrect solutions. We define this phenomenon as â€œPseudo Aha Momentâ€. This finding uncovers a cognitive shortcut-taking behavior in the deep reasoning processes of current LLMs, which we find accounts for 77% to 100% of errors on targeted problems. Code and resources are available at:<a target="_blank" rel="noopener" href="https://github.com/SYSUSELab/EvolMathEval">https://github.com/SYSUSELab/EvolMathEval</a>. </p>
<blockquote>
<p>LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„å¿«é€Ÿå‘å±•å¯¹ç°æœ‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›åŸºå‡†æµ‹è¯•é€šå¸¸é¢ä¸´è¯¸å¦‚åˆ†æ•°é¥±å’Œã€æ—¶é—´è¡°å‡å’Œæ•°æ®æ±¡æŸ“ç­‰é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†EvolMathEvalï¼Œä¸€ä¸ªåŸºäºè¿›åŒ–æµ‹è¯•çš„è‡ªåŠ¨åŒ–æ•°å­¦åŸºå‡†æµ‹è¯•ç”Ÿæˆå’Œè¿›åŒ–æ¡†æ¶ã€‚å®ƒé€šè¿‡ä»æœ€åˆå°±åŠ¨æ€ç”Ÿæˆç‹¬ç‰¹çš„è¯„ä¼°å®ä¾‹ï¼Œä»æ ¹æœ¬ä¸Šæ¶ˆé™¤äº†æ•°æ®æ±¡æŸ“çš„é£é™©ï¼Œå¹¶ç¡®ä¿åŸºå‡†æµ‹è¯•å¯¹æœªæ¥æ¨¡å‹å§‹ç»ˆä¿æŒæŒ‘æˆ˜æ€§ã€‚EvolMathEvalçš„æ ¸å¿ƒæœºåˆ¶åŒ…æ‹¬ï¼šåŸºäºé€†å‘å·¥ç¨‹ç”Ÿæˆç§å­é—®é¢˜ï¼Œå¹¶å¸¦æœ‰ä»£æ•°ä¿è¯ï¼›è®¾è®¡å¤šç»´é—ä¼ ç®—å­ä»¥æ³¨å…¥å¤šæ ·åŒ–çš„è®¤çŸ¥æŒ‘æˆ˜ï¼›ä»¥åŠèƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®è¯„ä¼°é—®é¢˜éš¾åº¦çš„ç»„åˆé€‚åº”åº¦å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç»„åˆé€‚åº”åº¦å‡½æ•°èƒ½å¤Ÿé«˜æ•ˆä¸”ç²¾ç¡®åœ°é‡åŒ–æ•°å­¦é—®é¢˜çš„éš¾åº¦ã€‚æ­¤å¤–ï¼ŒEvolMathEvalä¸ä»…å¯ä»¥é€šè¿‡è¿ç»­è‡ªæˆ‘è¿­ä»£ç”Ÿæˆå¤§é‡é«˜éš¾åº¦é—®é¢˜ï¼Œè¿˜å¯ä»¥é€šè¿‡è¿›åŒ–æ˜¾è‘—æé«˜GSM8Kç­‰å…¬å…±æ•°æ®é›†å¤æ‚åº¦ï¼Œå¹³å‡é™ä½æ¨¡å‹å‡†ç¡®ç‡48%ã€‚æ›´æ·±å…¥çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è§£å†³è¿™äº›ç»è¿‡è¿›åŒ–çš„å¤æ‚é—®é¢˜æ—¶ï¼ŒLLMå€¾å‘äºä½¿ç”¨éä¸¥æ ¼çš„å¯å‘å¼æ–¹æ³•æ¥è§„é¿å¤æ‚çš„å¤šæ­¥éª¤é€»è¾‘æ¨ç†ï¼Œä»è€Œå¯¼è‡´é”™è¯¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡å®šä¹‰ä¸ºâ€œä¼ªé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å½“å‰LLMæ·±åº¦æ¨ç†è¿‡ç¨‹ä¸­çš„è®¤çŸ¥æ·å¾„è¡Œä¸ºï¼Œæˆ‘ä»¬å‘ç°è¿™ç§ç°è±¡åœ¨ç›®æ ‡é—®é¢˜ä¸Šçš„é”™è¯¯ç‡é«˜è¾¾77%è‡³100%ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SYSUSELab/EvolMathEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SYSUSELab/EvolMathEvalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13003v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†EvolMathEvalæ¡†æ¶ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å¸¦æ¥çš„æŒ‘æˆ˜ã€‚åŸºå‡†æµ‹è¯•é¢ä¸´çš„é—®é¢˜åŒ…æ‹¬æˆç»©é¥±å’Œã€æ—¶æ•ˆæ€§å’Œæ•°æ®æ±¡æŸ“ã€‚EvolMathEvalé€šè¿‡åŸºäºè¿›åŒ–æµ‹è¯•çš„åŠ¨æ€ç”Ÿæˆç‹¬ç‰¹çš„è¯„ä¼°å®ä¾‹ï¼Œä»æ ¹æœ¬ä¸Šæ¶ˆé™¤äº†æ•°æ®æ±¡æŸ“çš„é£é™©ï¼Œç¡®ä¿åŸºå‡†æµ‹è¯•å¯¹æœªæ¥æ¨¡å‹å§‹ç»ˆä¿æŒæŒ‘æˆ˜æ€§ã€‚å…¶æ ¸å¿ƒæœºåˆ¶åŒ…æ‹¬åŸºäºé€†å‘å·¥ç¨‹çš„ç§å­é—®é¢˜ç”Ÿæˆã€è®¾è®¡å¤šå…ƒé—ä¼ æ“ä½œä»¥æ³¨å…¥å„ç§è®¤çŸ¥æŒ‘æˆ˜ä»¥åŠå¿«é€Ÿå‡†ç¡®è¯„ä¼°é—®é¢˜éš¾åº¦çš„å¤åˆé€‚åº”åº¦å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆç”Ÿæˆå¤§é‡é«˜éš¾åº¦é—®é¢˜ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜å…¬å…±æ•°æ®é›†å¦‚GSM8Kçš„å¤æ‚æ€§ã€‚åŒæ—¶ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶å€¾å‘äºä½¿ç”¨éä¸¥è°¨å¯å‘å¼æ¥ç»•è¿‡å¤æ‚çš„å¤šæ­¥é€»è¾‘æ¨ç†ï¼Œå¯¼è‡´é”™è¯¯è§£å†³æ–¹æ¡ˆçš„ç°è±¡â€”â€”â€œä¼ªé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EvolMathEvalæ¡†æ¶è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•é¢ä¸´æˆç»©é¥±å’Œã€æ—¶æ•ˆæ€§å’Œæ•°æ®æ±¡æŸ“ç­‰é—®é¢˜ã€‚</li>
<li>EvolMathEvalé€šè¿‡åŠ¨æ€ç”Ÿæˆç‹¬ç‰¹çš„è¯„ä¼°å®ä¾‹æ¶ˆé™¤æ•°æ®æ±¡æŸ“é£é™©ã€‚</li>
<li>æ ¸å¿ƒæœºåˆ¶åŒ…æ‹¬ç§å­é—®é¢˜ç”Ÿæˆã€å¤šå…ƒé—ä¼ æ“ä½œå’Œå¤åˆé€‚åº”åº¦å‡½æ•°ã€‚</li>
<li>å¤åˆé€‚åº”åº¦å‡½æ•°èƒ½å¿«é€Ÿå‡†ç¡®è¯„ä¼°é—®é¢˜éš¾åº¦ã€‚</li>
<li>EvolMathEvalèƒ½è‡ªæˆ‘è¿­ä»£ç”Ÿæˆå¤§é‡é«˜éš¾åº¦é—®é¢˜ï¼Œæé«˜å…¬å…±æ•°æ®é›†å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66f51d7a692731cf81598067eee4e023.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-507421e22366003663b8da0576f6f27d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a05a0c05d2a3a7d3b4b499d82f426a7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bea6a7cb6b30adc3a3f40574a4f1bb00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a5dbea48d006f6f08b2998069616591.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Open-Ended-Emotional-Support-Conversations-in-LLMs-via-Reinforcement-Learning-with-Future-Oriented-Rewards"><a href="#Towards-Open-Ended-Emotional-Support-Conversations-in-LLMs-via-Reinforcement-Learning-with-Future-Oriented-Rewards" class="headerlink" title="Towards Open-Ended Emotional Support Conversations in LLMs via   Reinforcement Learning with Future-Oriented Rewards"></a>Towards Open-Ended Emotional Support Conversations in LLMs via   Reinforcement Learning with Future-Oriented Rewards</h2><p><strong>Authors:Ting Yang, Li Chen, Huimin Wang</strong></p>
<p>Emotional Support Conversation (ESC) systems aim to alleviate usersâ€™ emotional difficulties and provide long-term, systematic support for emotional well-being. However, most large language model (LLM)-based ESC systems rely on predefined strategies, which limits their effectiveness in complex, real-life scenarios. To enable flexible responses to diverse emotional problem scenarios, this paper introduces a novel end-to-end framework (RLFF-ESC) that directly learns enduring emotionally supportive response skills using reinforcement learning. For sustained emotional support, we first employ an LLM-based multi-agent mechanism to simulate future dialogue trajectories and collect future-oriented rewards. We then train a future-oriented reward model, which is subsequently used to train the emotional support policy model. Additionally, we incorporate an explicit reasoning process during response generation to further enhance the quality, relevance, and contextual appropriateness of the systemâ€™s responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two public ESC datasets. Experimental results demonstrate that RLFF-ESC consistently outperforms existing baselines in terms of goal completion and response quality. </p>
<blockquote>
<p>æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ç³»ç»Ÿçš„ç›®æ ‡æ˜¯ç¼“è§£ç”¨æˆ·çš„æƒ…æ„Ÿå›°éš¾ï¼Œä¸ºæƒ…æ„Ÿå¥åº·æä¾›é•¿æœŸã€ç³»ç»Ÿçš„æ”¯æŒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ESCç³»ç»Ÿä¾èµ–äºé¢„è®¾çš„ç­–ç•¥ï¼Œè¿™åœ¨å¤æ‚ã€çœŸå®çš„åœºæ™¯ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºäº†çµæ´»åº”å¯¹å„ç§æƒ…æ„Ÿé—®é¢˜åœºæ™¯ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶ï¼ˆRLFF-ESCï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥å­¦ä¹ æŒä¹…çš„æƒ…æ„Ÿæ”¯æŒå›åº”æŠ€èƒ½ã€‚ä¸ºäº†æŒç»­çš„æƒ…æ„Ÿæ”¯æŒï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æœºåˆ¶æ¨¡æ‹Ÿæœªæ¥çš„å¯¹è¯è½¨è¿¹å¹¶æ”¶é›†é¢å‘æœªæ¥çš„å¥–åŠ±ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒé¢å‘æœªæ¥çš„å¥–åŠ±æ¨¡å‹ï¼Œéšåç”¨äºè®­ç»ƒæƒ…æ„Ÿæ”¯æŒç­–ç•¥æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆå“åº”æ—¶åŠ å…¥æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç³»ç»Ÿå“åº”çš„è´¨é‡ã€ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡æ°å½“æ€§ã€‚æˆ‘ä»¬åœ¨Qwen2.5-7B-Instruct-1Må’ŒLLaMA3.1-8B-Instructæ¨¡å‹ä¸Šè¯„ä¼°äº†æ ¸å¿ƒç­–ç•¥æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¤ä¸ªå…¬å…±ESCæ•°æ®é›†ä¸Šæµ‹è¯•äº†æå‡ºçš„RLFF-ESCæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLFF-ESCåœ¨ç›®æ ‡å®Œæˆå’Œå“åº”è´¨é‡æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºé¢„è®¾ç­–ç•¥ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„çœŸå®åœºæ™¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ¡†æ¶RLFF-ESCï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥å­¦ä¹ æŒä¹…çš„æƒ…æ„Ÿæ”¯æŒå“åº”æŠ€èƒ½ã€‚RLFF-ESCé‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿæœªæ¥å¯¹è¯è½¨è¿¹å¹¶æ”¶é›†é¢å‘æœªæ¥çš„å¥–åŠ±ï¼Œè®­ç»ƒæœªæ¥å¯¼å‘å¥–åŠ±æ¨¡å‹ï¼Œè¿›è€Œè®­ç»ƒæƒ…æ„Ÿæ”¯æŒç­–ç•¥æ¨¡å‹ã€‚åŒæ—¶ï¼Œå“åº”ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ å…¥æ˜¾æ€§æ¨ç†ï¼Œä»¥æé«˜å“åº”è´¨é‡å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLFF-ESCåœ¨ç›®æ ‡å®Œæˆå’Œå“åº”è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESCç³»ç»Ÿæ—¨åœ¨ç¼“è§£ç”¨æˆ·çš„æƒ…æ„Ÿå›°éš¾ï¼Œå¹¶æä¾›é•¿æœŸç³»ç»Ÿçš„æƒ…æ„Ÿæ”¯æŒã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„ESCç³»ç»Ÿä¾èµ–äºé¢„è®¾ç­–ç•¥ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>RLFF-ESCæ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥å­¦ä¹ æŒä¹…çš„æƒ…æ„Ÿæ”¯æŒå“åº”æŠ€èƒ½ã€‚</li>
<li>RLFF-ESCé‡‡ç”¨å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿæœªæ¥å¯¹è¯è½¨è¿¹å¹¶æ”¶é›†é¢å‘æœªæ¥çš„å¥–åŠ±ã€‚</li>
<li>æœªæ¥å¯¼å‘å¥–åŠ±æ¨¡å‹ç”¨äºè®­ç»ƒæƒ…æ„Ÿæ”¯æŒç­–ç•¥æ¨¡å‹ã€‚</li>
<li>åœ¨å“åº”ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ å…¥æ˜¾æ€§æ¨ç†ï¼Œæé«˜å“åº”çš„è´¨é‡å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0393ecfb87c251f722198e552584394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e866e9b94c07a669e270e8d85011d30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ba970797ad5d799a06ca7b89360b1d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FuSaR-A-Fuzzification-Based-Method-for-LRM-Safety-Reasoning-Balance"><a href="#FuSaR-A-Fuzzification-Based-Method-for-LRM-Safety-Reasoning-Balance" class="headerlink" title="FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance"></a>FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance</h2><p><strong>Authors:Jianhao Chen, Mayi Xu, Xiaohu Li, Yongqi Li, Xiangyu Zhang, Jianjie Huang, Tieyun Qian</strong></p>
<p>Large Reasoning Models (LRMs) have demonstrated impressive performance across various tasks due to their powerful reasoning capabilities. However, their safety performance remains a significant concern. In this paper, we explore the reasons behind the vulnerability of LRMs. Based on this, we propose a novel method to improve the safety of LLMs without sacrificing their reasoning capability. Specifically, we exploit the competition between LRMâ€™s reasoning ability and safety ability, and achieve jailbreak by improving LRMâ€™s reasoning performance to reduce its safety performance. We then introduce an alignment strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by detoxifying the harmful reasoning process, where both the dangerous entities and the dangerous procedures in the reasoning steps are hidden. FuSaR successfully mitigates safety risks while preserving core reasoning information. We validate this strategy through alignment experiments on several open-source LRMs using detoxified reasoning data. The results compared with existing baselines conclusively show that FuSaR is an efficient alignment strategy to simultaneously enhance both the reasoning capability and safety of LRMs. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ç”±äºå…¶å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å®‰å…¨æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§å…³æ³¨ç‚¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†LRMè„†å¼±æ€§çš„åŸå› ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›LLMså®‰å…¨æ€§çš„æ–°æ–¹æ³•ï¼Œè€Œæ— éœ€ç‰ºç‰²å…¶æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LRMçš„æ¨ç†èƒ½åŠ›ä¸å®‰å…¨èƒ½åŠ›ä¹‹é—´çš„ç«äº‰ï¼Œé€šè¿‡æé«˜LRMçš„æ¨ç†æ€§èƒ½æ¥é™ä½å…¶å®‰å…¨æ€§èƒ½ï¼Œä»è€Œå®ç°çªç ´ã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºæ¨¡ç³ŠåŒ–æŠ€æœ¯å¼•å…¥äº†ä¸€ç§å¹³è¡¡å®‰å…¨æ¨ç†ï¼ˆFuSaRï¼‰çš„å¯¹é½ç­–ç•¥ï¼Œé€šè¿‡å‡€åŒ–æœ‰å®³çš„æ¨ç†è¿‡ç¨‹ï¼Œéšè—æ¨ç†æ­¥éª¤ä¸­çš„å±é™©å®ä½“å’Œå±é™©ç¨‹åºã€‚FuSaRæˆåŠŸå‡è½»äº†å®‰å…¨é£é™©ï¼ŒåŒæ—¶ä¿ç•™äº†æ ¸å¿ƒæ¨ç†ä¿¡æ¯ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å‡€åŒ–åçš„æ¨ç†æ•°æ®å¯¹å‡ ä¸ªå¼€æºLRMè¿›è¡Œå¯¹é½å®éªŒï¼ŒéªŒè¯äº†è¯¥ç­–ç•¥ã€‚ä¸ç°æœ‰åŸºå‡†çº¿çš„æ¯”è¾ƒç»“æœæ˜ç¡®è¡¨æ˜ï¼ŒFuSaRæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¯¹é½ç­–ç•¥ï¼Œå¯ä»¥åŒæ—¶æé«˜LRMçš„æ¨ç†èƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12897v1">PDF</a> 14pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä½†å…¶å®‰å…¨æ€§èƒ½ä»æ˜¯å…³æ³¨é‡ç‚¹ã€‚æœ¬æ–‡æ¢è®¨äº†LRMsè„†å¼±æ€§çš„åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§æé«˜LLMså®‰å…¨æ€§çš„æ–°æ–¹æ³•ï¼Œæ— éœ€ç‰ºç‰²å…¶æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨LRMçš„æ¨ç†èƒ½åŠ›ä¸å®‰å…¨èƒ½åŠ›ä¹‹é—´çš„ç«äº‰ï¼Œæˆ‘ä»¬æ”¹å–„äº†LRMçš„æ¨ç†æ€§èƒ½ä»¥é™ä½å…¶å®‰å…¨æ€§èƒ½ï¼Œå¹¶å¼•å…¥åŸºäºFuzzificationçš„å¯¹é½ç­–ç•¥æ¥å¹³è¡¡å®‰å…¨æ¨ç†ï¼ˆFuSaRï¼‰ã€‚FuSaRèƒ½å¤ŸæˆåŠŸé™ä½å®‰å…¨é£é™©åŒæ—¶ä¿ç•™æ ¸å¿ƒæ¨ç†ä¿¡æ¯ã€‚é€šè¿‡å¼€æºLRMçš„å¯¹é½å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºFuSaRæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¯¹é½ç­–ç•¥ï¼Œèƒ½åŒæ—¶æé«˜LRMçš„æ¨ç†èƒ½åŠ›å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMsè™½åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶å®‰å…¨æ€§èƒ½ä»æ˜¯å…³æ³¨é‡ç‚¹ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†LRMsçš„è„†å¼±æ€§åŸå› ï¼Œå¹¶æå‡ºäº†æé«˜LLMså®‰å…¨æ€§çš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ”¹å–„LRMçš„æ¨ç†æ€§èƒ½ä»¥é™ä½å…¶å®‰å…¨æ€§èƒ½ï¼Œå†é€šè¿‡å¯¹é½ç­–ç•¥æ¥å¹³è¡¡å®‰å…¨æ¨ç†ã€‚</li>
<li>å¼•å…¥äº†åŸºäºFuzzificationçš„å¯¹é½ç­–ç•¥FuSaRï¼Œèƒ½å¤ŸæˆåŠŸé™ä½å®‰å…¨é£é™©å¹¶ä¿ç•™æ ¸å¿ƒæ¨ç†ä¿¡æ¯ã€‚</li>
<li>FuSaRé€šè¿‡éšè—æ¨ç†æ­¥éª¤ä¸­çš„å±é™©å®ä½“å’Œå±é™©ç¨‹åºæ¥è§£æ¯’æœ‰å®³æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡å¼€æºLRMçš„å¯¹é½å®éªŒéªŒè¯ï¼ŒFuSaRç­–ç•¥çš„æ•ˆæœä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ef81553df529f4b9330a52708a97696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ad1ec145316567d8c1bbc7eda74a786.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8135f8d1c25ddec03d7421d166142d66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6d9198c2081abff9bb5feeca5f8a47b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Atom-Searcher-Enhancing-Agentic-Deep-Research-via-Fine-Grained-Atomic-Thought-Reward"><a href="#Atom-Searcher-Enhancing-Agentic-Deep-Research-via-Fine-Grained-Atomic-Thought-Reward" class="headerlink" title="Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic   Thought Reward"></a>Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic   Thought Reward</h2><p><strong>Authors:Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Changhua Meng</strong></p>
<p>Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä»¤äººç©ç›®çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œä½†ç”±äºé™æ€å†…éƒ¨çŸ¥è¯†è€Œåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é‡åˆ°å›°éš¾ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¢å¼ºäº†è®¿é—®å¤–éƒ¨ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä½†ç”±äºå·¥ä½œæµåƒµåŒ–ï¼Œåœ¨è·¨æ­¥æ¨ç†å’Œç­–ç•¥æœç´¢æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚æœ€è¿‘çš„ä»£ç†æ·±åº¦ç ”ç©¶çš„è¿›å±•ä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ¨ç†ã€æœç´¢å’Œåˆæˆä¿¡æ¯ã€‚ç„¶è€Œï¼Œç›®å‰ä¾èµ–ç»“æœåŸºç¡€ä¸Šçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•é¢ä¸´å…³é”®æ€§é—®é¢˜ï¼Œå¦‚æ¢¯åº¦å†²çªå’Œå¥–åŠ±ç¨€ç–ï¼Œè¿™é™åˆ¶äº†æ€§èƒ½æå‡å’Œè®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12800v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å‡ºè‰²çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ä»»åŠ¡æ–¹é¢å­˜åœ¨é™æ€çŸ¥è¯†é™åˆ¶ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æé«˜äº†å¯¹å¤–éƒ¨ä¿¡æ¯çš„è®¿é—®èƒ½åŠ›ï¼Œä½†åœ¨å¤šè·³æ¨ç†å’Œç­–ç•¥æœç´¢æ–¹é¢ä»å­˜åœ¨åˆšæ€§å·¥ä½œæµç¨‹çš„é™åˆ¶ã€‚æœ€è¿‘çš„ç ”ç©¶è¿›å±•ä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ¨ç†ã€æœç´¢å’Œåˆæˆä¿¡æ¯ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ–¹æ³•ä¾èµ–äºåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œé¢ä¸´ç€æ¢¯åº¦å†²çªå’Œå¥–åŠ±ç¨€ç–ç­‰å…³é”®é—®é¢˜ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡å’Œè®­ç»ƒæ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸå­æ€ç»´è¿™ä¸€æ–°å‹LLMæ€ç»´èŒƒå¼å’ŒAtom-Searcherè¿™ä¸€RLæ¡†æ¶ã€‚åŸå­æ€ç»´å°†æ¨ç†åˆ†è§£ä¸ºç²¾ç»†çš„åŠŸèƒ½å•å…ƒï¼Œå—åˆ°æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰çš„ç›‘ç£ï¼Œä¸ºç²¾ç»†æŒ‡å¯¼æä¾›åŸå­æ€ç»´å¥–åŠ±ï¼ˆATRï¼‰ã€‚Atom-Searcherç»“åˆäº†åŸå­æ€ç»´å’ŒATRï¼Œé‡‡ç”¨è¯¾ç¨‹å¼å¥–åŠ±æ—¶é—´è¡¨ï¼Œæ—©æœŸä¼˜å…ˆè¿‡ç¨‹çº§ATRï¼Œé€æ¸è¿‡æ¸¡åˆ°ç»“æœå¥–åŠ±ï¼ŒåŠ é€Ÿåœ¨æœ‰æ•ˆæ¨ç†è·¯å¾„ä¸Šçš„æ”¶æ•›ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å…·æœ‰ä¸€è‡´çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡æ–¹é¢å­˜åœ¨é™æ€çŸ¥è¯†é™åˆ¶ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èƒ½æé«˜å¯¹å¤–éƒ¨ä¿¡æ¯çš„è®¿é—®ï¼Œä½†åœ¨å¤šè·³æ¨ç†å’Œç­–ç•¥æœç´¢ä¸­æœ‰åˆšæ€§å·¥ä½œæµç¨‹çš„é™åˆ¶ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­é¢ä¸´æ¢¯åº¦å†²çªå’Œå¥–åŠ±ç¨€ç–çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸå­æ€ç»´è¿™ä¸€æ–°å‹LLMæ€ç»´èŒƒå¼ï¼Œå°†æ¨ç†åˆ†è§£ä¸ºç²¾ç»†çš„åŠŸèƒ½å•å…ƒã€‚</li>
<li>æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰ä¸ºåŸå­æ€ç»´æä¾›ç›‘ç£ï¼Œé€šè¿‡åŸå­æ€ç»´å¥–åŠ±ï¼ˆATRï¼‰è¿›è¡Œç²¾ç»†æŒ‡å¯¼ã€‚</li>
<li>Atom-Searcheræ˜¯ç»“åˆåŸå­æ€ç»´å’ŒATRçš„æ–°å‹RLæ¡†æ¶ï¼Œé‡‡ç”¨è¯¾ç¨‹å¼å¥–åŠ±æ—¶é—´è¡¨ä»¥åŠ é€Ÿæœ‰æ•ˆæ¨ç†è·¯å¾„çš„æ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75ef40e8940d1c4fd2b3b0a6b4a96736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d71e84c2ed3b992e40dd7cbadded5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-847fe97327e36bfe692c937f850560ee.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds"><a href="#HeroBench-A-Benchmark-for-Long-Horizon-Planning-and-Structured-Reasoning-in-Virtual-Worlds" class="headerlink" title="HeroBench: A Benchmark for Long-Horizon Planning and Structured   Reasoning in Virtual Worlds"></a>HeroBench: A Benchmark for Long-Horizon Planning and Structured   Reasoning in Virtual Worlds</h2><p><strong>Authors:Petr Anokhin, Roman Khalikov, Stefan Rebrikov, Viktor Volkov, Artyom Sorokin, Vincent Bissonnette</strong></p>
<p>Large language models (LLMs) have shown remarkable capabilities in isolated step-by-step reasoning tasks such as mathematics and programming, but their proficiency in long-horizon planning, where solutions require extended, structured sequences of interdependent actions, remains underexplored. Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments. We introduce HeroBench, a novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds. HeroBench provides a rigorously constructed dataset of tasks covering a wide range of difficulties, a simulated environment to execute and validate agent plans, and detailed analytical tools for evaluating model performance. Tasks challenge models to formulate strategic plans, efficiently gather resources, master necessary skills, craft equipment, and defeat adversaries, reflecting practical scenariosâ€™ layered dependencies and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning both open-source and proprietary models, including the GPT-5 family, reveals substantial performance disparities rarely observed in conventional reasoning benchmarks. Detailed error analysis further uncovers specific weaknesses in current modelsâ€™ abilities to generate robust high-level plans and reliably execute structured actions. HeroBench thus not only significantly advances the evaluation of LLM reasoning but also provides a flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å­¤ç«‹çš„é€æ­¥æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨é•¿æœŸè§„åˆ’æ–¹é¢çš„ç†Ÿç»ƒç¨‹åº¦ï¼Œå³è§£å†³æ–¹æ¡ˆéœ€è¦ä¸€ç³»åˆ—ç›¸äº’ä¾èµ–çš„è¡ŒåŠ¨ï¼Œä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸é€šè¿‡æŠ½è±¡æˆ–ä½ç»´ç®—æ³•ä»»åŠ¡æ¥è¯„ä¼°LLMï¼Œæ— æ³•æ•æ‰ç°å®è§„åˆ’ç¯å¢ƒçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†HeroBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°å¤æ‚RPGå¼è™šæ‹Ÿä¸–ç•Œä¸­çš„é•¿æœŸè§„åˆ’å’Œç»“æ„åŒ–æ¨ç†è€Œè®¾è®¡çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚HeroBenchæä¾›äº†ä¸€ä¸ªä¸¥æ ¼æ„å»ºçš„ä»»åŠ¡æ•°æ®é›†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„éš¾åº¦èŒƒå›´ï¼Œä¸€ä¸ªæ¨¡æ‹Ÿç¯å¢ƒæ¥æ‰§è¡Œå’ŒéªŒè¯ä»£ç†è®¡åˆ’ï¼Œä»¥åŠè¯¦ç»†çš„åˆ†æå·¥å…·æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚ä»»åŠ¡æŒ‘æˆ˜æ¨¡å‹åˆ¶å®šæˆ˜ç•¥è®¡åˆ’ï¼Œé«˜æ•ˆæ”¶é›†èµ„æºï¼ŒæŒæ¡å¿…è¦æŠ€èƒ½ï¼Œåˆ¶ä½œè£…å¤‡ï¼Œå¹¶å‡»è´¥æ•Œäººï¼Œåæ˜ äº†å®é™…åœºæ™¯çš„åˆ†å±‚ä¾èµ–æ€§å’Œçº¦æŸã€‚æˆ‘ä»¬å¯¹25ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œä»¥åŠGPT-5ç³»åˆ—æ¨¡å‹ï¼Œæ­ç¤ºäº†ä¸ä¼ ç»Ÿæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¾ˆå°‘è§‚å¯Ÿåˆ°çš„æ˜¾è‘—æ€§èƒ½å·®å¼‚ã€‚è¯¦ç»†çš„é”™è¯¯åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç”Ÿæˆç¨³å¥çš„é«˜çº§è®¡åˆ’å’Œå¯é æ‰§è¡Œç»“æ„åŒ–åŠ¨ä½œæ–¹é¢çš„ç‰¹å®šå¼±ç‚¹ã€‚å› æ­¤ï¼ŒHeroBenchä¸ä»…æ˜¾è‘—æ¨è¿›äº†LLMæ¨ç†çš„è¯„ä¼°ï¼Œè¿˜ä¸ºæœªæ¥è™šæ‹Ÿç¯å¢ƒä¸­å…ˆè¿›è‡ªä¸»è§„åˆ’çš„ç ”ç©¶æä¾›äº†çµæ´»ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12782v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/stefanrer/HeroBench">https://github.com/stefanrer/HeroBench</a></p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€æ­¥æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ã€‚ä½†å¯¹äºéœ€è¦ä¸€ç³»åˆ—ç›¸äº’ä¾èµ–çš„è¡ŒåŠ¨æ¥è§£å†³é—®é¢˜çš„é•¿æœŸè§„åˆ’ï¼Œå…¶èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚å½“å‰è¯„ä¼°LLMsçš„åŸºå‡†æµ‹è¯•é€šå¸¸æ˜¯æŠ½è±¡çš„æˆ–ä½ç»´åº¦çš„ç®—æ³•ä»»åŠ¡ï¼Œæ— æ³•æ•æ‰çœŸå®è§„åˆ’ç¯å¢ƒçš„å¤æ‚æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†HeroBenchï¼Œä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å¤æ‚RPGå¼è™šæ‹Ÿä¸–ç•Œä¸­çš„é•¿æœŸè§„åˆ’å’Œç»“æ„åŒ–æ¨ç†è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚HeroBenchæä¾›äº†ä¸€ä¸ªæ¶µç›–å¹¿æ³›éš¾åº¦èŒƒå›´çš„ä»»åŠ¡æ•°æ®é›†ã€æ¨¡æ‹Ÿæ‰§è¡Œå’ŒéªŒè¯ä»£ç†è®¡åˆ’çš„ç¯å¢ƒï¼Œä»¥åŠè¯¦ç»†çš„è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„åˆ†æå·¥å…·ã€‚è¯¥æµ‹è¯•æŒ‘æˆ˜æ¨¡å‹åˆ¶å®šæˆ˜ç•¥è®¡åˆ’ã€é«˜æ•ˆæ”¶é›†èµ„æºã€æŒæ¡å¿…è¦æŠ€èƒ½ã€åˆ¶ä½œè£…å¤‡å’Œå‡»è´¥æ•Œäººï¼Œåæ˜ äº†å®é™…åœºæ™¯çš„å¤æ‚ä¾èµ–æ€§å’Œçº¦æŸæ¡ä»¶ã€‚å¯¹ç°æœ‰æœ€æ–°LLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨é•¿æœŸè§„åˆ’æ–¹é¢çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€æ­¥æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œä½†åœ¨é•¿æœŸè§„åˆ’æ–¹é¢å°šå¾…æ¢ç´¢ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°LLMsåœ¨çœŸå®è§„åˆ’ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚</li>
<li>HeroBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨å¤æ‚RPGå¼è™šæ‹Ÿä¸–ç•Œä¸­çš„é•¿æœŸè§„åˆ’å’Œç»“æ„åŒ–æ¨ç†èƒ½åŠ›ã€‚</li>
<li>HeroBenchæä¾›äº†æ¶µç›–ä¸åŒéš¾åº¦èŒƒå›´çš„ä»»åŠ¡æ•°æ®é›†ã€æ¨¡æ‹Ÿç¯å¢ƒå’Œåˆ†æå·¥å…·ã€‚</li>
<li>æ¨¡å‹éœ€åˆ¶å®šæˆ˜ç•¥è®¡åˆ’ã€æ”¶é›†èµ„æºã€æŒæ¡æŠ€èƒ½å’Œåˆ¶ä½œè£…å¤‡ç­‰ï¼Œåæ˜ å®é™…åœºæ™¯çš„å¤æ‚æ€§å’Œçº¦æŸã€‚</li>
<li>å¯¹25ä¸ªæœ€æ–°LLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨é•¿æœŸè§„åˆ’æ–¹é¢çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2e9a803c13dae044ec92dadc5f71601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4429ec2910b79ab72b046ce1fe2777e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0e43385be815c54cd4b6d61e158cbfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3e47d5a5bc6d897210ae88b6da23d50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-387efa6a70973742af634869b92ab7d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b2e625c0b73aa08ef26d85dc0ad5ab.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DESIGNER-Design-Logic-Guided-Multidisciplinary-Data-Synthesis-for-LLM-Reasoning"><a href="#DESIGNER-Design-Logic-Guided-Multidisciplinary-Data-Synthesis-for-LLM-Reasoning" class="headerlink" title="DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM   Reasoning"></a>DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM   Reasoning</h2><p><strong>Authors:Weize Liu, Yongchi Zhao, Yijia Luo, Mingyu Xu, Jiaheng Liu, Yanan Li, Xiguo Hu, Yuchi Xu, Wenbo Su, Bo Zheng</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary breadth or the structural depth necessary to elicit robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (book corpus and web corpus) to generate multidisciplinary challenging questions. A core innovation of our approach is the introduction of a Design Logic concept, which mimics the question-creation process of human educators. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with disciplinary source materials, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Based on this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book), containing 3.04 million challenging questions synthesized from the book corpus, and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging questions from the web corpus. Our data analysis demonstrates that the questions synthesized by our method exhibit substantially greater difficulty and diversity than those in the baseline datasets. We validate the effectiveness of these datasets by conducting SFT experiments on the Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset significantly outperforms existing multidisciplinary datasets of the same volume. Training with the full datasets further enables the models to surpass the multidisciplinary reasoning performance of the official Qwen3-8B and Qwen3-4B models. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨å¤æ‚çš„å¤šæ­¥æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨å­¦ç§‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨ç†æ•°æ®é›†è¦ä¹ˆç¼ºä¹å­¦ç§‘å¹¿åº¦ï¼Œè¦ä¹ˆç¼ºä¹å¿…è¦çš„ç»“æ„æ·±åº¦ï¼Œæ— æ³•æ¿€å‘ç¨³å¥çš„æ¨ç†è¡Œä¸ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œDESIGNERâ€çš„è®¾è®¡é€»è¾‘å¼•å¯¼æ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨è‡ªç„¶å¯ç”¨çš„ä¸°å¯ŒåŸå§‹æ–‡æ¡£ï¼ˆå›¾ä¹¦è¯­æ–™åº“å’Œç½‘ç»œè¯­æ–™åº“ï¼‰æ¥ç”Ÿæˆè·¨å­¦ç§‘éš¾é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå¼•å…¥äº†è®¾è®¡é€»è¾‘æ¦‚å¿µï¼Œè¿™ä¸€æ¦‚å¿µæ¨¡ä»¿äº†äººç±»æ•™è‚²è€…çš„é—®é¢˜åˆ›å»ºè¿‡ç¨‹ã€‚æˆ‘ä»¬ä½¿ç”¨LLMsé€†å‘å·¥ç¨‹å’ŒæŠ½è±¡åŒ–æ¥è‡ªä¸åŒå­¦ç§‘ç°æœ‰é—®é¢˜çš„è®¾è®¡é€»è¾‘ï¼Œè¶…è¿‡12ä¸‡ä¸ªã€‚é€šè¿‡å°†è¿™äº›è®¾è®¡é€»è¾‘ä¸å­¦ç§‘æ¥æºææ–™ç›¸åŒ¹é…ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ›å»ºå‡ºè¿œè¶…ç°æœ‰æ•°æ®é›†éš¾åº¦å’Œå¤šæ ·æ€§çš„æ¨ç†é—®é¢˜ã€‚åŸºäºæ­¤ç®¡é“ï¼Œæˆ‘ä»¬ç»¼åˆäº†ä¸¤ä¸ªè·¨è¶Š75ä¸ªå­¦ç§‘çš„å¤§è§„æ¨¡æ¨ç†æ•°æ®é›†ï¼šè®¾è®¡é€»è¾‘æ¨ç†å›¾ä¹¦æ•°æ®é›†ï¼ˆDLR-Bookï¼‰ï¼ŒåŒ…å«ä»å›¾ä¹¦è¯­æ–™åº“ä¸­åˆæˆçš„304ä¸‡ä¸ªéš¾é¢˜ï¼›è®¾è®¡é€»è¾‘æ¨ç†ç½‘ç»œæ•°æ®é›†ï¼ˆDLR-Webï¼‰ï¼ŒåŒ…å«ä»ç½‘ç»œè¯­æ–™åº“ä¸­æå–çš„166ä¸‡ä¸ªéš¾é¢˜ã€‚æˆ‘ä»¬çš„æ•°æ®åˆ†æè¡¨æ˜ï¼Œé€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•åˆæˆçš„é—®é¢˜åœ¨éš¾åº¦å’Œå¤šæ ·æ€§ä¸Šæ˜¾è‘—é«˜äºåŸºå‡†æ•°æ®é›†çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨Qwen3-8B-Baseå’ŒQwen3-4B-Baseæ¨¡å‹è¿›è¡ŒSFTå®éªŒæ¥éªŒè¯è¿™äº›æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åœ¨ç›¸åŒè§„æ¨¡ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„è·¨å­¦ç§‘æ•°æ®é›†ã€‚ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒè¿›ä¸€æ­¥ä½¿æ¨¡å‹è¶…è¶Šäº†å®˜æ–¹Qwen3-8Bå’ŒQwen3-4Bæ¨¡å‹çš„å¤šå­¦ç§‘æ¨ç†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ï¼Œç‰¹åˆ«æ˜¯è·¨å­¦ç§‘æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨ç†æ•°æ®é›†ç¼ºä¹å­¦ç§‘å¹¿åº¦æˆ–ç»“æ„æ·±åº¦ï¼Œæ— æ³•æ¿€å‘ç¨³å¥çš„æ¨ç†è¡Œä¸ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDESIGNERçš„æ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œå®ƒåˆ©ç”¨è‡ªç„¶å¯ç”¨çš„ä¸°å¯ŒåŸå§‹æ–‡æ¡£ï¼ˆä¹¦ç±è¯­æ–™åº“å’Œç½‘ç»œè¯­æ–™åº“ï¼‰æ¥ç”Ÿæˆè·¨å­¦ç§‘æŒ‘æˆ˜æ€§é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå¼•å…¥äº†è®¾è®¡é€»è¾‘æ¦‚å¿µï¼Œè¿™ä¸€æ¦‚å¿µæ¨¡ä»¿äº†äººç±»æ•™è‚²è€…çš„é—®é¢˜åˆ›å»ºè¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€ç®¡é“ï¼Œæˆ‘ä»¬åˆæˆäº†ä¸¤ä¸ªå¤§è§„æ¨¡è·¨å­¦ç§‘çš„æ¨ç†æ•°æ®é›†â€”â€”DLR-Bookå’ŒDLR-Webã€‚æ•°æ®åˆ†ææ˜¾ç¤ºï¼Œä¸æˆ‘ä»¬æ–¹æ³•åˆæˆçš„é¢˜ç›®ç›¸æ¯”ï¼ŒåŸºçº¿æ•°æ®é›†çš„é¢˜ç›®éš¾åº¦å’Œå¤šæ ·æ€§å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚é€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†è¿™äº›æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚è®­ç»ƒå®Œæ•´æ•°æ®é›†å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹è·¨å­¦ç§‘æ¨ç†èƒ½åŠ›ï¼Œå¹¶è¶…è¿‡å®˜æ–¹æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚è¿™æ˜¯ä¸€é¡¹é¢å‘å¤§è§„æ¨¡è·¨å­¦ç§‘çš„å¤æ‚å¤šæ­¥éª¤æ¨ç†çš„é‡è¦è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤æ‚å¤šæ­¥éª¤ã€è·¨å­¦ç§‘æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨ç†æ•°æ®é›†ç¼ºä¹å­¦ç§‘å¹¿åº¦æˆ–ç»“æ„æ·±åº¦ï¼Œéš¾ä»¥æœ‰æ•ˆä¿ƒè¿›æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDESIGNERçš„æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨ä¹¦ç±å’Œç½‘ç»œè¯­æ–™åº“ç”Ÿæˆè·¨å­¦ç§‘æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥è®¾è®¡é€»è¾‘æ¦‚å¿µï¼Œæ¨¡æ‹Ÿäººç±»æ•™è‚²è€…çš„é—®é¢˜åˆ›å»ºè¿‡ç¨‹ã€‚</li>
<li>åˆæˆä¸¤ä¸ªå¤§è§„æ¨¡è·¨å­¦ç§‘æ¨ç†æ•°æ®é›†â€”â€”DLR-Bookå’ŒDLR-Webï¼Œéš¾åº¦å’Œå¤šæ ·æ€§è¶…è¿‡ç°æœ‰æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†çš„æœ‰æ•ˆæ€§é€šè¿‡å¯¹æ¯”å®éªŒå¾—åˆ°éªŒè¯ï¼Œè®­ç»ƒå®Œæ•´æ•°æ®é›†æ˜¾è‘—æå‡æ¨¡å‹è·¨å­¦ç§‘æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38d1c8c1635784d411adcaf4814667af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98983e014cc7f23136fb47b076fdc1fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-370b70e55409fab536cc4a8856e51fd8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Vision-G1-Towards-General-Vision-Language-Reasoning-with-Multi-Domain-Data-Curation"><a href="#Vision-G1-Towards-General-Vision-Language-Reasoning-with-Multi-Domain-Data-Curation" class="headerlink" title="Vision-G1: Towards General Vision Language Reasoning with Multi-Domain   Data Curation"></a>Vision-G1: Towards General Vision Language Reasoning with Multi-Domain   Data Curation</h2><p><strong>Authors:Yuheng Zha, Kun Zhou, Yujia Wu, Yushu Wang, Jie Feng, Zhi Xu, Shibo Hao, Zhengzhong Liu, Eric P. Xing, Zhiting Hu</strong></p>
<p>Despite their success, current training pipelines for reasoning VLMs focus on a limited range of tasks, such as mathematical and logical reasoning. As a result, these models face difficulties in generalizing their reasoning capabilities to a wide range of domains, primarily due to the scarcity of readily available and verifiable reward data beyond these narrowly defined areas. Moreover, integrating data from multiple domains is challenging, as the compatibility between domain-specific datasets remains uncertain. To address these limitations, we build a comprehensive RL-ready visual reasoning dataset from 46 data sources across 8 dimensions, covering a wide range of tasks such as infographic, mathematical, spatial, cross-image, graphic user interface, medical, common sense and general science. We propose an influence function based data selection and difficulty based filtering strategy to identify high-quality training samples from this dataset. Subsequently, we train the VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to iteratively improve its visual reasoning capabilities. Our model achieves state-of-the-art performance across various visual reasoning benchmarks, outperforming similar-sized VLMs and even proprietary models like GPT-4o and Gemini-1.5 Flash. The model, code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/yuh-zha/Vision-G1">https://github.com/yuh-zha/Vision-G1</a>. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†å½“å‰ç”¨äºæ¨ç†VLMçš„è®­ç»ƒæµç¨‹ä¸»è¦é›†ä¸­åœ¨æœ‰é™çš„èŒƒå›´å†…ä»»åŠ¡ä¸Šï¼Œå¦‚æ•°å­¦å’Œé€»è¾‘æ¨ç†ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹åœ¨å°†æ¨ç†èƒ½åŠ›æ¨å¹¿åˆ°å¹¿æ³›é¢†åŸŸæ—¶é¢ä¸´å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè¶…å‡ºè¿™äº›ç‹­ä¹‰å®šä¹‰é¢†åŸŸä¹‹å¤–çš„å¯ç”¨å’Œå¯éªŒè¯çš„å¥–åŠ±æ•°æ®ç¨€ç¼ºã€‚æ­¤å¤–ï¼Œç”±äºç‰¹å®šæ•°æ®é›†ä¹‹é—´çš„å…¼å®¹æ€§ä¸ç¡®å®šï¼Œå› æ­¤ä»å¤šä¸ªé¢†åŸŸæ•´åˆæ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä»8ä¸ªç»´åº¦çš„46ä¸ªæ•°æ®æºæ„å»ºäº†å…¨é¢çš„RLå°±ç»ªè§†è§‰æ¨ç†æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡ï¼Œå¦‚ä¿¡æ¯å›¾è¡¨ã€æ•°å­¦ã€ç©ºé—´ã€è·¨å›¾åƒã€å›¾å½¢ç”¨æˆ·ç•Œé¢ã€åŒ»å­¦ã€å¸¸è¯†å’Œæ™®é€šç§‘å­¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå½±å“å‡½æ•°çš„æ•°æ®é€‰æ‹©å’ŒåŸºäºéš¾åº¦çš„è¿‡æ»¤ç­–ç•¥ï¼Œä»¥ä»æ•°æ®é›†ä¸­è¯†åˆ«é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œä¸€ä»½æ•™å­¦å¤§çº²æ¥è®­ç»ƒç§°ä¸ºVision-G1çš„VLMæ¨¡å‹ï¼Œä»¥è¿­ä»£åœ°æé«˜å…¶è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„VLMæ¨¡å‹ä»¥åŠä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒGemini-1.5 Flashã€‚æ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘ç«™å…¬å¼€è®¿é—®ï¼š[ç½‘å€é“¾æ¥]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12680v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç›®å‰å¤§å¤šåªå…³æ³¨æœ‰é™çš„æ¨ç†ä»»åŠ¡ï¼Œå¦‚æ•°å­¦å’Œé€»è¾‘æ¨ç†ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ›´å¹¿æ³›çš„é¢†åŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ç”¨è§†è§‰æ¨ç†æ•°æ®é›†ï¼Œæ¶µç›–8ä¸ªç»´åº¦çš„46ä¸ªæ•°æ®æºï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå½±å“å‡½æ•°çš„æ•°æ®é€‰æ‹©å’ŒåŸºäºéš¾åº¦çš„è¿‡æ»¤ç­–ç•¥æ¥è¯†åˆ«é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚ä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®è¯¾ç¨‹è®­ç»ƒå‡ºçš„Vision-G1æ¨¡å‹ï¼Œåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºåŒç±»å¤§å°çš„VLMå’Œä¸“æœ‰æ¨¡å‹å¦‚GPT-4oå’ŒGemini-1.5 Flashã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰VLMçš„è®­ç»ƒç®¡é“ä¸»è¦å…³æ³¨æœ‰é™çš„æ¨ç†ä»»åŠ¡ï¼Œå¦‚æ•°å­¦å’Œé€»è¾‘ï¼Œå¯¼è‡´æ¨¡å‹åœ¨æ³›åŒ–åˆ°å¤šä¸ªé¢†åŸŸæ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>ç¼ºä¹å¹¿æ³›å¯ç”¨çš„å¯éªŒè¯å¥–åŠ±æ•°æ®æ˜¯é™åˆ¶VLMæ³›åŒ–èƒ½åŠ›çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„RL-readyè§†è§‰æ¨ç†æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸçš„æ•°æ®æºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå½±å“å‡½æ•°çš„æ•°æ®é€‰æ‹©å’ŒåŸºäºéš¾åº¦çš„è¿‡æ»¤ç­–ç•¥æ¥è¯†åˆ«é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>ä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®è¯¾ç¨‹è®­ç»ƒçš„Vision-G1æ¨¡å‹åœ¨å„ç§è§†è§‰æ¨ç†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>Vision-G1æ¨¡å‹ä¼˜äºåŒç±»å¤§å°çš„VLMå’Œå…¶ä»–ä¸“æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbce3f627937e10650a111c3d9dbf9b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a281e8fe7fb26e28af436535fe518b6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a9a135d8057e8dec6f10c58a24f3217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87b558404ed98c1f161bf0917e873930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d78d50c885286ae9bea6e19c0c8ccf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319ba580b9e83f078e8f4b1c83bb9c80.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Multimodal-Chain-of-Continuous-Thought-for-Latent-Space-Reasoning-in-Vision-Language-Models"><a href="#Multimodal-Chain-of-Continuous-Thought-for-Latent-Space-Reasoning-in-Vision-Language-Models" class="headerlink" title="Multimodal Chain of Continuous Thought for Latent-Space Reasoning in   Vision-Language Models"></a>Multimodal Chain of Continuous Thought for Latent-Space Reasoning in   Vision-Language Models</h2><p><strong>Authors:Tan-Hanh Pham, Chris Ngo</strong></p>
<p>Many reasoning techniques for large multimodal models adapt language model approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning as word sequences. While effective for text, these methods are suboptimal for multimodal contexts, struggling to align audio, visual, and textual information dynamically. To explore an alternative paradigm, we propose the Multimodal Chain of Continuous Thought (MCOUT), which enables reasoning directly in a joint latent space rather than in natural language. In MCOUT, the reasoning state is represented as a continuous hidden vector, iteratively refined and aligned with visual and textual embeddings, inspired by human reflective cognition. We develop two variants: MCOUT-Base, which reuses the language model&#96;s last hidden state as the continuous thought for iterative reasoning, and MCOUT-Multi, which integrates multimodal latent attention to strengthen cross-modal alignment between visual and textual features. Experiments on benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong baselines and improving BLEU scores up to 8.27% across multiple-choice and open-ended tasks. These findings highlight latent continuous reasoning as a promising direction for advancing LMMs beyond language-bound CoT, offering a scalable framework for human-like reflective multimodal inference. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hanhpt23/OmniMod">https://github.com/Hanhpt23/OmniMod</a>. </p>
<blockquote>
<p>è®¸å¤šé’ˆå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ¨ç†æŠ€æœ¯éƒ½é‡‡ç”¨äº†è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºï¼Œå®ƒå°†æ¨ç†è¡¨è¾¾ä¸ºå•è¯åºåˆ—ã€‚è™½ç„¶è¿™åœ¨æ–‡æœ¬ä¸­å¾ˆæœ‰æ•ˆï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­å¹¶ä¸ç†æƒ³ï¼Œéš¾ä»¥åŠ¨æ€å¯¹é½éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚ä¸ºäº†æ¢ç´¢ä¸€ç§æ›¿ä»£çš„èŒƒå¼ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è¿ç»­æ€ç»´é“¾ï¼ˆMCOUTï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç›´æ¥æ¨ç†ï¼Œè€Œä¸æ˜¯åœ¨è‡ªç„¶è¯­è¨€ä¸­ã€‚åœ¨MCOUTä¸­ï¼Œæ¨ç†çŠ¶æ€è¢«è¡¨ç¤ºä¸ºè¿ç»­çš„éšè—å‘é‡ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œä¸è§†è§‰å’Œæ–‡æœ¬åµŒå…¥çš„å¯¹é½æ¥ä½“ç°ï¼Œè¿™å—åˆ°äººç±»åæ€è®¤çŸ¥çš„å¯å‘ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªå˜ä½“ï¼šMCOUT-Baseï¼Œå®ƒåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºè¿ç»­æ€ç»´æ¥è¿›è¡Œè¿­ä»£æ¨ç†ï¼›ä»¥åŠMCOUT-Multiï¼Œå®ƒæ•´åˆå¤šæ¨¡æ€æ½œåœ¨æ³¨æ„åŠ›ï¼Œä»¥åŠ å¼ºè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ã€‚åœ¨åŒ…æ‹¬MMMUã€ScienceQAå’ŒMMStarç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMCOUTåœ¨å¤šä¸ªé€‰æ‹©ä»»åŠ¡å’Œå¼€æ”¾ä»»åŠ¡ä¸­ï¼Œä¸æ–­æé«˜å¤šæ¨¡æ€æ¨ç†çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹æœ‰é«˜è¾¾8.23%çš„å‡†ç¡®ç‡æå‡å’Œé«˜è¾¾8.27%çš„BLEUå¾—åˆ†æå‡ã€‚è¿™äº›å‘ç°çªæ˜¾å‡ºæ½œåœ¨è¿ç»­æ¨ç†ä½œä¸ºä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ï¼Œæ¨åŠ¨è¯­è¨€æ¨¡å‹è¶…è¶Šè¯­è¨€ç•Œé™çš„æ€ç»´é“¾ï¼Œä¸ºç±»äººç±»åæ€å¤šæ¨¡æ€æ¨æ–­æä¾›å¯æ‰©å±•çš„æ¡†æ¶ã€‚ç›¸å…³ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/Hanhpt23/OmniMod">https://github.com/Hanhpt23/OmniMod</a> äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12587v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•â€”â€”è¿ç»­å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆMCOUTï¼‰ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ä¸ä½¿ç”¨åŸºäºè‡ªç„¶è¯­è¨€æ¨¡å‹çš„æ¨ç†æ–¹å¼ï¼Œå¦‚æ€ç»´é“¾æç¤ºç­‰ã€‚ç›¸åï¼Œå®ƒåœ¨è”åˆæ½œåœ¨ç©ºé—´ä¸­ç›´æ¥è¿›è¡Œæ¨ç†ï¼Œå€Ÿé‰´äººç±»åæ€è®¤çŸ¥çš„ç‰¹æ€§ï¼Œé€šè¿‡è¿ç»­éšè—å‘é‡è¡¨ç¤ºæ¨ç†çŠ¶æ€ï¼Œå¹¶è¿­ä»£åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–å’Œä¸è§†è§‰å’Œæ–‡æœ¬åµŒå…¥å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCOUTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡æé«˜äº†å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œä¸ç°æœ‰å¼ºåŸºçº¿ç›¸æ¯”ï¼Œå‡†ç¡®ç‡æé«˜äº†æœ€å¤š8.23%ï¼Œå¹¶åœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾ä»»åŠ¡ä¸­BLEUå¾—åˆ†æé«˜äº†æœ€å¤š8.27%ã€‚è¿™è¡¨æ˜è¿ç»­æ½œåœ¨æ¨ç†æ˜¯æ¨è¿›è¯­è¨€æ¨¡å‹è¶…è¶Šè¯­è¨€é™åˆ¶æ€ç»´é“¾çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€åœºæ™¯ä¸­çš„ç°æœ‰æ¨ç†æŠ€æœ¯å¦‚æ€ç»´é“¾æç¤ºä¸»è¦åŸºäºè‡ªç„¶è¯­è¨€æ¨¡å‹ï¼Œä¸é€‚ç”¨äºå¤šæ¨¡æ€åœºæ™¯ã€‚</li>
<li>MCOUTæ–¹æ³•åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸­ç›´æ¥è¿›è¡Œè”åˆæ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†ï¼Œä¸ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¨¡å‹çš„æ–¹å¼ã€‚</li>
<li>MCOUTé€šè¿‡è¿ç»­éšè—å‘é‡è¡¨ç¤ºæ¨ç†çŠ¶æ€ï¼Œå¹¶è¿­ä»£åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–å’Œä¸è§†è§‰å’Œæ–‡æœ¬åµŒå…¥å¯¹é½ã€‚</li>
<li>MCOUTæœ‰ä¸¤ç§å˜ä½“ï¼šMCOUT-Baseå’ŒMCOUT-Multiï¼Œåˆ†åˆ«ä¾§é‡äºä¸åŒçš„å®ç°æ–¹å¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMCOUTåœ¨å¤šæ¨¡æ€æ¨ç†ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿åœ¨å‡†ç¡®ç‡ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a78c2f8d00b0184b27f5dbc1c82c63c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c8ee56db15999fbf6525ea12173c2b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e4b6fd9de0c71a0013925fb3daf827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1132827507a71b4cf1f9c08ddb68445e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b68abed025f8c259203c277a9b16abd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-181898579d681ad3b08b56c927b1ce96.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OS-R1-Agentic-Operating-System-Kernel-Tuning-with-Reinforcement-Learning"><a href="#OS-R1-Agentic-Operating-System-Kernel-Tuning-with-Reinforcement-Learning" class="headerlink" title="OS-R1: Agentic Operating System Kernel Tuning with Reinforcement   Learning"></a>OS-R1: Agentic Operating System Kernel Tuning with Reinforcement   Learning</h2><p><strong>Authors:Hongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu</strong></p>
<p>Linux kernel tuning is essential for optimizing operating system (OS) performance. However, existing methods often face challenges in terms of efficiency, scalability, and generalization. This paper introduces OS-R1, an agentic Linux kernel tuning framework powered by rule-based reinforcement learning (RL). By abstracting the kernel configuration space as an RL environment, OS-R1 facilitates efficient exploration by large language models (LLMs) and ensures accurate configuration modifications. Additionally, custom reward functions are designed to enhance reasoning standardization, configuration modification accuracy, and system performance awareness of the LLMs. Furthermore, we propose a two-phase training process that accelerates convergence and minimizes retraining across diverse tuning scenarios. Experimental results show that OS-R1 significantly outperforms existing baseline methods, achieving up to 5.6% performance improvement over heuristic tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across various real-world applications, demonstrating its potential for practical deployment in diverse environments. Our dataset and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/LHY-24/OS-R1">https://github.com/LHY-24/OS-R1</a>. </p>
<blockquote>
<p>Linuxå†…æ ¸è°ƒä¼˜å¯¹äºä¼˜åŒ–æ“ä½œç³»ç»Ÿï¼ˆOSï¼‰æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç»å¸¸åœ¨æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†OS-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é©±åŠ¨çš„Linuxå†…æ ¸æ™ºèƒ½è°ƒä¼˜æ¡†æ¶ã€‚é€šè¿‡å°†å†…æ ¸é…ç½®ç©ºé—´æŠ½è±¡ä¸ºRLç¯å¢ƒï¼ŒOS-R1ä¾¿äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé«˜æ•ˆæ¢ç´¢ï¼Œå¹¶ç¡®ä¿å‡†ç¡®çš„é…ç½®ä¿®æ”¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œä»¥æé«˜LLMçš„æ¨ç†æ ‡å‡†åŒ–ã€é…ç½®ä¿®æ”¹å‡†ç¡®æ€§å’Œç³»ç»Ÿæ€§èƒ½æ„è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘ä¸åŒè°ƒä¼˜åœºæ™¯ä¸‹çš„å†è®­ç»ƒæ—¶é—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOS-R1æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨å¯å‘å¼è°ƒä¼˜çš„åŸºç¡€ä¸Šå®ç°äº†é«˜è¾¾5.6%çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¿æŒäº†è¾ƒé«˜çš„æ•°æ®æ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒOS-R1å¯ä»¥é€‚åº”å„ç§å®é™…åº”ç”¨åœºæ™¯ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ä¸åŒç¯å¢ƒä¸­å®é™…éƒ¨ç½²çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/LHY-24/OS-R">https://github.com/LHY-24/OS-R</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„Linuxå†…æ ¸è°ƒä¼˜æ¡†æ¶OS-R1ï¼Œç”¨äºä¼˜åŒ–æ“ä½œç³»ç»Ÿï¼ˆOSï¼‰æ€§èƒ½ã€‚OS-R1é€šè¿‡æŠ½è±¡å†…æ ¸é…ç½®ç©ºé—´ä½œä¸ºRLç¯å¢ƒï¼Œä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ‰æ•ˆæ¢ç´¢ï¼Œå¹¶è®¾è®¡è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°æé«˜LLMçš„æ¨ç†æ ‡å‡†åŒ–ã€é…ç½®ä¿®æ”¹å‡†ç¡®æ€§å’Œç³»ç»Ÿæ€§èƒ½æ„è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOS-R1æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨æ€§èƒ½ä¸Šæœ€å¤šå¯æé«˜5.6%ï¼ŒåŒæ—¶ä¿æŒé«˜æ•°æ®æ•ˆç‡ï¼Œå¹¶é€‚åº”å„ç§å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OS-R1æ˜¯ä¸€ä¸ªåŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ çš„Linuxå†…æ ¸è°ƒä¼˜æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–æ“ä½œç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>OS-R1é€šè¿‡æŠ½è±¡å†…æ ¸é…ç½®ç©ºé—´ä½œä¸ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ¢ç´¢ã€‚</li>
<li>è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°çš„è®¾è®¡æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ ‡å‡†åŒ–ã€é…ç½®ä¿®æ”¹å‡†ç¡®æ€§å’Œç³»ç»Ÿæ€§èƒ½æ„è¯†ã€‚</li>
<li>OS-R1é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘ä¸åŒè°ƒä¼˜åœºæ™¯ä¸‹çš„é‡æ–°è®­ç»ƒæ—¶é—´ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOS-R1åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€å¤§å¯æé«˜5.6%çš„æ€§èƒ½ã€‚</li>
<li>OS-R1å…·æœ‰è‰¯å¥½çš„æ•°æ®æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-715461a7028b754f26b2d23c4821ad76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-609575a06a8826fcfad7acebadf92c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b16a43a3726c9e87ef1afc1e1e32249f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cd588f7ea33dc92654d2f58f9f73c0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae885579eff528b54f34d2484a7f12a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-153fec1be038821acecfe27a6b0ce823.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="REVEAL-â€“-Reasoning-and-Evaluation-of-Visual-Evidence-through-Aligned-Language"><a href="#REVEAL-â€“-Reasoning-and-Evaluation-of-Visual-Evidence-through-Aligned-Language" class="headerlink" title="REVEAL â€“ Reasoning and Evaluation of Visual Evidence through Aligned   Language"></a>REVEAL â€“ Reasoning and Evaluation of Visual Evidence through Aligned   Language</h2><p><strong>Authors:Ipsita Praharaj, Yukta Butala, Yash Butala</strong></p>
<p>The rapid advancement of generative models has intensified the challenge of detecting and interpreting visual forgeries, necessitating robust frameworks for image forgery detection while providing reasoning as well as localization. While existing works approach this problem using supervised training for specific manipulation or anomaly detection in the embedding space, generalization across domains remains a challenge. We frame this problem of forgery detection as a prompt-driven visual reasoning task, leveraging the semantic alignment capabilities of large vision-language models. We propose a framework, <code>REVEAL</code> (Reasoning and Evaluation of Visual Evidence through Aligned Language), that incorporates generalized guidelines. We propose two tangential approaches - (1) Holistic Scene-level Evaluation that relies on the physics, semantics, perspective, and realism of the image as a whole and (2) Region-wise anomaly detection that splits the image into multiple regions and analyzes each of them. We conduct experiments over datasets from different domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language Models against competitive baselines and analyze the reasoning provided by them. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†æ£€æµ‹å’Œè§£é‡Šè§†è§‰ä¼ªé€ å“çš„æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨æä¾›æ¨ç†å’Œå®šä½çš„åŒæ—¶ï¼Œæ„å»ºç¨³å¥çš„å›¾åƒä¼ªé€ æ£€æµ‹æ¡†æ¶ã€‚è™½ç„¶ç°æœ‰å·¥ä½œé€šè¿‡é’ˆå¯¹ç‰¹å®šæ“ä½œæˆ–åµŒå…¥ç©ºé—´ä¸­çš„å¼‚å¸¸æ£€æµ‹è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è·¨é¢†åŸŸçš„æ³›åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬å°†ä¼ªé€ æ£€æµ‹é—®é¢˜æ¡†æ¶åŒ–ä¸ºä¸€ç§æç¤ºé©±åŠ¨çš„è§†è§‰æ¨ç†ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å¯¹é½åŠŸèƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨æ€§å¼ºçš„æ¡†æ¶<code>REVEAL</code>ï¼ˆé€šè¿‡è¯­è¨€å¯¹é½è¿›è¡Œè§†è§‰è¯æ®æ¨ç†ä¸è¯„ä¼°ï¼‰ï¼Œå¹¶åœ¨æ­¤æ¡†æ¶ä¸‹æå‡ºäº†ä¸¤ç§åˆ‡åˆ†æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ•´ä½“åœºæ™¯çº§è¯„ä¼°ä¾èµ–äºå›¾åƒæ•´ä½“çš„ç‰©ç†æ€§ã€è¯­ä¹‰æ€§ã€é€è§†æ€§å’ŒçœŸå®æ€§ï¼›ï¼ˆ2ï¼‰åŒºåŸŸçº§å¼‚å¸¸æ£€æµ‹å°†å›¾åƒåˆ†å‰²æˆå¤šä¸ªåŒºåŸŸï¼Œå¹¶å¯¹æ¯ä¸ªåŒºåŸŸè¿›è¡Œåˆ†æã€‚æˆ‘ä»¬åœ¨æ¥è‡ªä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ˆå¦‚Photoshopã€DeepFakeå’ŒAIGCç¼–è¾‘ï¼‰ã€‚æˆ‘ä»¬å°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸ç«äº‰åŸºçº¿è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å¯¹å…¶æä¾›çš„æ¨ç†è¿›è¡Œäº†è¯„ä¼°åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12543v1">PDF</a> 4 pages, 6 figures, International Conference on Computer Vision, ICCV   2025</p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå›¾åƒä¼ªé€ æ£€æµ‹ä¸è§£è¯»çš„æŒ‘æˆ˜æ—¥ç›ŠåŠ å‰§ã€‚ç°æœ‰æ–¹æ³•å¤šé‡‡ç”¨ç‰¹å®šæ“ä½œçš„ç›‘ç£è®­ç»ƒæˆ–åœ¨åµŒå…¥ç©ºé—´ä¸­çš„å¼‚å¸¸æ£€æµ‹ï¼Œä½†è·¨åŸŸæ³›åŒ–ä»æœ‰å›°éš¾ã€‚æœ¬ç ”ç©¶å°†ä¼ªé€ æ£€æµ‹é—®é¢˜è§†ä¸ºæç¤ºé©±åŠ¨çš„è§†è§‰æ¨ç†ä»»åŠ¡ï¼Œå€ŸåŠ©å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰å¯¹é½èƒ½åŠ›ï¼Œæå‡ºä¸€ç§åä¸ºâ€œREVEALâ€çš„æ¡†æ¶ï¼Œé‡‡ç”¨æ³›åŒ–çš„å‡†åˆ™è¿›è¡Œæ£€æµ‹ä¸æ¨ç†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§å¹¶è¡Œçš„æ–¹æ³•ï¼šåŸºäºå›¾åƒæ•´ä½“çš„åœºæ™¯çº§åˆ«è¯„ä¼°å’ŒåŒºåŸŸçº§åˆ«çš„å¼‚å¸¸æ£€æµ‹ã€‚é€šè¿‡å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•åœ¨Photoshopã€DeepFakeå’ŒAIGCç¼–è¾‘ç­‰ä¸åŒé¢†åŸŸæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ç«äº‰åŸºçº¿è¿›è¡Œäº†å¯¹æ¯”å’Œåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†è§†è§‰ä¼ªé€ æ£€æµ‹ä¸è§£è¯»çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨ç›‘ç£è®­ç»ƒå’Œç‰¹å®šæ“ä½œæ£€æµ‹ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æç¤ºé©±åŠ¨çš„è§†è§‰æ¨ç†ä»»åŠ¡æ¥è§£å†³ä¼ªé€ æ£€æµ‹é—®é¢˜ã€‚</li>
<li>æå‡ºåä¸ºâ€œREVEALâ€çš„æ¡†æ¶ï¼Œç»“åˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹è¿›è¡Œå›¾åƒä¼ªé€ æ£€æµ‹ä¸æ¨ç†ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ç§å¹¶è¡Œæ–¹æ³•ï¼šåŸºäºæ•´ä½“çš„åœºæ™¯çº§åˆ«è¯„ä¼°å’ŒåŸºäºåŒºåŸŸçš„å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>å®éªŒéªŒè¯äº†REVEALæ¡†æ¶åœ¨ä¸åŒé¢†åŸŸæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b762070d4e83738d8d8b05062dc0b2f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d236b9aa70801e6fb76174ce02846e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6bb57ea74eb2c2bf041f676ae10dc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00a3d22e89efee953822c4fbad61a904.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Mitigating-Hallucinations-in-Large-Language-Models-via-Causal-Reasoning"><a href="#Mitigating-Hallucinations-in-Large-Language-Models-via-Causal-Reasoning" class="headerlink" title="Mitigating Hallucinations in Large Language Models via Causal Reasoning"></a>Mitigating Hallucinations in Large Language Models via Causal Reasoning</h2><p><strong>Authors:Yuangang Li, Yiqing Shen, Yi Nian, Jiechao Gao, Ziyi Wang, Chenxiao Yu, Shawn Li, Jie Wang, Xiyang Hu, Yue Zhao</strong></p>
<p>Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MrLYG/CDCR-SFT">https://github.com/MrLYG/CDCR-SFT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼šå‡ºç°é€»è¾‘ä¸ä¸€è‡´çš„å¹»è§‰ï¼Œè¿™äº›å¹»è§‰çœ‹ä¼¼è¿è´¯ï¼Œä½†å´è¿åäº†æ¨ç†åŸåˆ™ã€‚æœ€æ–°ç ”ç©¶æš—ç¤ºï¼Œå› æœæ¨ç†èƒ½åŠ›ä¸è¿™ç§å¹»è§‰ä¹‹é—´å­˜åœ¨é€†å‘å…³ç³»ã€‚ç„¶è€Œï¼ŒLLMsä¸­ç°æœ‰çš„æ¨ç†æ–¹æ³•ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰åŠå…¶åŸºäºå›¾çš„å˜ä½“ï¼Œéƒ½æ˜¯åœ¨è¯­è¨€ç¬¦å·å±‚é¢è¿›è¡Œæ“ä½œï¼Œè€Œéå¯¹å˜é‡ä¹‹é—´çš„åŸºæœ¬å› æœå…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å®ƒä»¬æ— æ³•è¡¨ç¤ºæ¡ä»¶ç‹¬ç«‹æ€§ï¼Œä¹Ÿæ— æ³•æ»¡è¶³å› æœè¯†åˆ«å‡è®¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†å› æœæœ‰å‘æ— ç¯å›¾æ„å»ºå’Œæ¨ç†ï¼ˆCDCR-SFTï¼‰ç›‘ç£å¾®è°ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è®­ç»ƒLLMsæ˜ç¡®æ„å»ºå˜é‡çº§åˆ«çš„æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«25,368ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼ˆCausalDRï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…æ‹¬è¾“å…¥é—®é¢˜ã€æ˜ç¡®çš„å› æœDAGã€åŸºäºå›¾çš„æ¨ç†è½¨è¿¹å’ŒéªŒè¯åçš„ç­”æ¡ˆã€‚åœ¨å››ä¸ªLLMså’Œå…«ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCDCR-SFTæé«˜äº†å› æœæ¨ç†èƒ½åŠ›ï¼Œåœ¨CLADDERä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„95.33%çš„å‡†ç¡®ç‡ï¼ˆé¦–æ¬¡è¶…è¿‡äººç±»æ€§èƒ½çš„94.8%ï¼‰ï¼Œå¹¶åœ¨HaluEvalä¸Šå‡å°‘äº†å¹»è§‰ï¼Œæé«˜äº†10%çš„æ•ˆæœã€‚è¿™è¡¨æ˜åœ¨LLMsä¸­è¿›è¡Œæ˜ç¡®çš„å› æœç»“æ„å»ºæ¨¡å¯ä»¥æœ‰æ•ˆåœ°å‡è½»LLMè¾“å‡ºä¸­çš„é€»è¾‘ä¸ä¸€è‡´æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MrLYG/CDCR-SFT">https://github.com/MrLYG/CDCR-SFT</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12495v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€»è¾‘ä¸Šä¼šå‡ºç°è¿è´¯æ€§å¹»è§‰ï¼Œå³æ¨¡å‹äº§ç”Ÿçš„è¾“å‡ºè™½ç„¶çœ‹ä¼¼è¿è´¯ï¼Œä½†å´è¿åäº†æ¨ç†åŸåˆ™ã€‚ç°æœ‰æ¨ç†æ–¹æ³•å¦‚Chain-of-Thoughtï¼ˆCoTï¼‰ä¸»è¦å…³æ³¨è¯­è¨€ç¬¦å·å±‚é¢ï¼Œæ— æ³•å¯¹å˜é‡é—´çš„å› æœå…³è”è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæœ¬ç ”ç©¶æå‡ºäº†åŸºäºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰çš„å› æœç»“æ„æ„å»ºä¸æ¨ç†ï¼ˆCDCR-SFTï¼‰æ¡†æ¶ï¼Œå¹¶å…¬å¼€äº†åŒ…å«å› æœDAGã€å›¾å½¢æ¨ç†è½¨è¿¹å’ŒéªŒè¯ç­”æ¡ˆçš„CausalDRæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒCDCR-SFTæ¡†æ¶æå‡äº†LLMçš„å› æœæ¨ç†èƒ½åŠ›ï¼Œåœ¨CLADDERä»»åŠ¡ä¸Šè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„95.33%å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†å¹»è§‰è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨é€»è¾‘ä¸ä¸€è‡´çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ¨ç†æ–¹æ³•å¦‚Chain-of-Thoughtä¸»è¦å…³æ³¨è¯­è¨€ç¬¦å·å±‚é¢ï¼Œç¼ºä¹å¯¹å˜é‡é—´å› æœå…³ç³»çš„å»ºæ¨¡ã€‚</li>
<li>CDCR-SFTæ¡†æ¶æ—¨åœ¨è®­ç»ƒLLMæ„å»ºå˜é‡çº§çš„DAGå¹¶è¿›è¡Œå›¾å½¢æ¨ç†ã€‚</li>
<li>CausalDRæ•°æ®é›†åŒ…å«è¾“å…¥é—®é¢˜ã€æ˜ç¡®çš„å› æœDAGã€å›¾å½¢æ¨ç†è½¨è¿¹å’ŒéªŒè¯ç­”æ¡ˆã€‚</li>
<li>CDCR-SFTæ¡†æ¶æå‡äº†LLMçš„å› æœæ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>CDCR-SFTæ¡†æ¶èƒ½å‡å°‘LLMçš„å¹»è§‰è¾“å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a0d1e1f0a863c0e15c3e3629953c2027.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-813d871193cb764b52a99d97a9ebc50e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37a826f5e04bce82cc96354a2fdc72db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9af823c1df24fcea9fe424af97a39d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe511eb14b4d5d4de5614c1450c3c059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fba7a9468e089c470d77fcd01a45fef0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67a9501d7ac001f304b645eb37d818aa.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="The-Yokai-Learning-Environment-Tracking-Beliefs-Over-Space-and-Time"><a href="#The-Yokai-Learning-Environment-Tracking-Beliefs-Over-Space-and-Time" class="headerlink" title="The Yokai Learning Environment: Tracking Beliefs Over Space and Time"></a>The Yokai Learning Environment: Tracking Beliefs Over Space and Time</h2><p><strong>Authors:Constantin Ruhdorfer, Matteo Bortoletto, Andreas Bulling</strong></p>
<p>Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to reason about the beliefs of others to build and maintain common ground. Existing ToM benchmarks, however, are restricted to passive observer settings or lack an assessment of how agents establish and maintain common ground over time. To address these gaps, we introduce the Yokai Learning Environment (YLE) - a multi-agent reinforcement learning (RL) environment based on the cooperative card game Yokai. In the YLE, agents take turns peeking at hidden cards and moving them to form clusters based on colour. Success requires tracking evolving beliefs, remembering past observations, using hints as grounded communication, and maintaining common ground with teammates. Our evaluation yields two key findings: First, current RL agents struggle to solve the YLE, even when given access to perfect memory. Second, while belief modelling improves performance, agents are still unable to effectively generalise to unseen partners or form accurate beliefs over longer games, exposing a reliance on brittle conventions rather than robust belief tracking. We use the YLE to investigate research questions in belief modelling, memory, partner generalisation, and scaling to higher-order ToM. </p>
<blockquote>
<p>å‘å±•åä½œäººå·¥æ™ºèƒ½ä¾èµ–äºå¿ƒæ™ºç†è®ºï¼ˆToMï¼‰â€”â€”æ¨ç†ä»–äººä¿¡å¿µä»¥å»ºç«‹å’Œç»´æŠ¤å…±è¯†çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¿ƒæ™ºç†è®ºåŸºå‡†æµ‹è¯•ä»…é™äºè¢«åŠ¨è§‚å¯Ÿè€…è®¾ç½®ï¼Œæˆ–ç¼ºä¹è¯„ä¼°ä»£ç†å¦‚ä½•éšæ—¶é—´å»ºç«‹å’Œç»´æŒå…±è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¦–æ€ªå­¦ä¹ ç¯å¢ƒï¼ˆYLEï¼‰â€”â€”ä¸€ä¸ªåŸºäºåˆä½œå¡ç‰Œæ¸¸æˆå¦–æ€ªçš„å¤šä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒã€‚åœ¨YLEä¸­ï¼Œä»£ç†è½®æµå·çœ‹éšè—çš„å¡ç‰Œï¼Œå¹¶æ ¹æ®é¢œè‰²å°†å®ƒä»¬ç§»åŠ¨ä»¥å½¢æˆé›†ç¾¤ã€‚æˆåŠŸéœ€è¦è¿½è¸ªä¸æ–­æ¼”å˜çš„ä¿¡å¿µï¼Œè®°ä½è¿‡å»çš„è§‚å¯Ÿï¼Œå°†çº¿ç´¢ä½œä¸ºæœ‰æ ¹æ®çš„äº¤æµï¼Œå¹¶ä¸é˜Ÿå‹ä¿æŒå…±åŒçš„åŸºç¡€ã€‚æˆ‘ä»¬çš„è¯„ä¼°å¾—å‡ºä¸¤ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œå³ä½¿å½“å‰çš„RLä»£ç†å¯ä»¥è®¿é—®å®Œç¾è®°å¿†ï¼Œä¹Ÿå¾ˆéš¾è§£å†³YLEé—®é¢˜ã€‚å…¶æ¬¡ï¼Œè™½ç„¶ä¿¡å¿µå»ºæ¨¡æé«˜äº†æ€§èƒ½ï¼Œä½†ä»£ç†ä»ç„¶æ— æ³•æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ä¼™ä¼´æˆ–åœ¨æ›´é•¿çš„æ¸¸æˆä¸­å½¢æˆå‡†ç¡®çš„ä¿¡å¿µï¼Œè¿™æš´éœ²å‡ºå¯¹è„†å¼±è§„èŒƒçš„ä¾èµ–ï¼Œè€Œä¸æ˜¯å¯¹ç¨³å¥ä¿¡å¿µè·Ÿè¸ªçš„ä¾èµ–ã€‚æˆ‘ä»¬ä½¿ç”¨YLEæ¥ç ”ç©¶ä¿¡å¿µå»ºæ¨¡ã€è®°å¿†ã€ä¼™ä¼´æ¨å¹¿å’Œæ‰©å±•åˆ°æ›´é«˜é˜¶å¿ƒæ™ºç†è®ºçš„ç ”ç©¶é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12480v1">PDF</a> Presented at the the ToM IJCAI 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼€å‘åä½œå¼äººå·¥æ™ºèƒ½æ—¶ï¼Œç†è®ºä¸­çš„å¿ƒæ™ºï¼ˆToMï¼‰èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå®ƒå…³ä¹å¦‚ä½•ç†è§£ä»–äººçš„ä¿¡å¿µä»¥å»ºç«‹å’Œç»´æŠ¤å…±åŒåŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ToMåŸºå‡†æµ‹è¯•ä»…é™äºè¢«åŠ¨è§‚å¯Ÿè€…ç¯å¢ƒï¼Œæˆ–æ— æ³•è¯„ä¼°ä»£ç†å¦‚ä½•åœ¨ä¸€æ®µæ—¶é—´å†…å»ºç«‹å’Œç»´æŒå…±åŒåŸºç¡€ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¦–æ€ªå­¦ä¹ ç¯å¢ƒï¼ˆYLEï¼‰â€”â€”ä¸€ä¸ªåŸºäºåˆä½œå¡ç‰Œæ¸¸æˆå¦–æ€ªçš„å¤šä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç¯å¢ƒã€‚åœ¨YLEä¸­ï¼Œä»£ç†è½®æµæŸ¥çœ‹éšè—å¡ç‰Œå¹¶æŒ‰é¢œè‰²å½¢æˆé›†ç¾¤ã€‚æˆåŠŸéœ€è¦è¿½è¸ªä¸æ–­å˜åŒ–çš„ä¿¡å¿µã€è®°ä½è¿‡å»çš„è§‚å¯Ÿç»“æœã€åˆ©ç”¨çº¿ç´¢ä½œä¸ºæ¥åœ°é€šä¿¡ä»¥åŠä¸ç»´æŠ¤å›¢é˜Ÿå…±åŒåŸºç¡€ã€‚æˆ‘ä»¬çš„è¯„ä¼°å¾—å‡ºä¸¤ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œå³ä½¿ç»™å½“å‰çš„RLä»£ç†æä¾›å®Œç¾è®°å¿†ï¼Œä»–ä»¬åœ¨è§£å†³YLEæ—¶ä¹Ÿè¡¨ç°æŒ£æ‰ï¼›å…¶æ¬¡ï¼Œè™½ç„¶ä¿¡å¿µå»ºæ¨¡èƒ½æé«˜æ€§èƒ½ï¼Œä½†ä»£ç†ä»ç„¶éš¾ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ä¼™ä¼´æˆ–åœ¨æ›´é•¿çš„æ¸¸æˆä¸­å½¢æˆå‡†ç¡®çš„ä¿¡å¿µï¼Œè¿™æš´éœ²å‡ºä»–ä»¬å¯¹è„†å¼±çš„æƒ¯ä¾‹çš„ä¾èµ–ï¼Œè€Œéç¨³å¥çš„ä¿¡å¿µè¿½è¸ªã€‚æˆ‘ä»¬ä½¿ç”¨YLEæ¥ç ”ç©¶ä¿¡å¿µå»ºæ¨¡ã€è®°å¿†ã€ä¼™ä¼´ä¸€èˆ¬åŒ–å’Œé«˜é˜¶ToMæ‰©å±•çš„ç ”ç©¶é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è®ºä¸­çš„å¿ƒæ™ºï¼ˆToMï¼‰åœ¨åä½œAIçš„å‘å±•ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œæ¶‰åŠç†è§£ä»–äººä¿¡å¿µä»¥å»ºç«‹å’Œç»´æŠ¤å…±åŒåŸºç¡€ã€‚</li>
<li>ç°æœ‰çš„ToMåŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å…¨é¢è¯„ä¼°ä»£ç†åœ¨åä½œè¿‡ç¨‹ä¸­çš„å¿ƒæ™ºèƒ½åŠ›ã€‚</li>
<li>æ¨å‡ºäº†å¦–æ€ªå­¦ä¹ ç¯å¢ƒï¼ˆYLEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ŒåŸºäºåˆä½œå¡ç‰Œæ¸¸æˆä»¥è¯„ä¼°å¿ƒæ™ºèƒ½åŠ›ã€‚</li>
<li>åœ¨YLEç¯å¢ƒä¸­ï¼ŒæˆåŠŸéœ€è¦è¿½è¸ªä¿¡å¿µã€è®°å¿†è§‚å¯Ÿç»“æœã€åˆ©ç”¨çº¿ç´¢è¿›è¡Œé€šä¿¡ä»¥åŠç»´æŠ¤å›¢é˜Ÿå…±åŒåŸºç¡€ã€‚</li>
<li>å½“å‰RLä»£ç†åœ¨è§£å†³YLEæ—¶é¢ä¸´å›°éš¾ï¼Œå³ä½¿å…·å¤‡å®Œç¾è®°å¿†ã€‚</li>
<li>ä¿¡å¿µå»ºæ¨¡è™½èƒ½æé«˜æ€§èƒ½ï¼Œä½†ä»£ç†åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„ä¼™ä¼´æˆ–é•¿æ—¶é—´æ¸¸æˆä¸­çš„ä¿¡å¿µå½¢æˆæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9611c43595cccd455f09034b854389b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ca408a08af890db2cc8dbd39e4e0568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4622c67bf676d1e3b4580b93946dbcd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34e819d664c703f88daf227358100332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f309b71c49e2056647fc66fe6621e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a190b22fe72adfe10cd81da7464cb39.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning"><a href="#Non-Iterative-Symbolic-Aided-Chain-of-Thought-for-Logical-Reasoning" class="headerlink" title="Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning"></a>Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</h2><p><strong>Authors:Phuong Minh Nguyen, Tien Huu Dang, Naoya Inoue</strong></p>
<p>This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process. By incorporating these symbolic structures, our method preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks â€“ ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning scenarios â€“ demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMsâ€™ reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç¬¦å·è¾…åŠ©æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ ‡å‡†æ€ç»´é“¾æ–¹æ³•çš„æ”¹è¿›ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é€»è¾‘æ¨ç†ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†è½»é‡çº§ç¬¦å·è¡¨ç¤ºé›†æˆåˆ°å°‘é‡æç¤ºä¸­ï¼Œä½¿ç”¨ä¸€è‡´çš„ç­–ç•¥æ¥ç»“æ„åŒ–æ¨ç†æ­¥éª¤ï¼Œåœ¨ä¸€ä¸ªéè¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­ä½¿æ¨ç†æ¨¡å¼æ›´åŠ æ˜ç¡®ã€‚é€šè¿‡èå…¥è¿™äº›ç¬¦å·ç»“æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒæ ‡å‡†æç¤ºæŠ€æœ¯æ™®éæ€§çš„åŒæ—¶ï¼Œæé«˜äº†LLMé€»è¾‘æ¨ç†çš„é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œå¯åˆ†ææ€§ã€‚åœ¨æ¶µç›–å¤šç§æ¨ç†åœºæ™¯çš„å››ä¸ªçŸ¥åé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•â€”â€”ProofWriterã€FOLIOã€ProntoQAå’ŒLogicalDeductionä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¯¼èˆªå¤šä¸ªçº¦æŸæˆ–è§„åˆ™çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¬¦å·è¾…åŠ©CoTæ–¹æ³•åœ¨å„ç§æ¨¡å‹å¤§å°ä¸­å‡æŒç»­æé«˜äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å››ä¸ªæ•°æ®é›†ä¸­çš„ä¸‰ä¸ªï¼ˆProofWriterã€ProntoQAå’ŒLogicalDeductionï¼‰ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸCoTæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12425v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¬¦å·è¾…åŠ©æ€ç»´é“¾ï¼ˆSymbolic-Aided Chain-of-Thoughtï¼Œç®€ç§°SACoTï¼‰æ˜¯ä¸€ç§æ”¹è¿›çš„æ ‡å‡†æ€ç»´é“¾æ–¹æ³•ï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†ã€‚è¯¥æ–¹æ³•å°†è½»é‡çº§ç¬¦å·è¡¨ç¤ºé›†æˆåˆ°å°‘é‡æç¤ºä¸­ï¼Œé€šè¿‡ä¸€è‡´çš„ç­–ç•¥ç»“æ„åŒ–æ¨ç†æ­¥éª¤ï¼Œä½¿éè¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­çš„æ¨ç†æ¨¡å¼æ›´åŠ æ˜ç¡®ã€‚SACoTæ–¹æ³•ä¿ç•™äº†æ ‡å‡†æç¤ºæŠ€æœ¯çš„é€šç”¨æ€§ï¼ŒåŒæ—¶æé«˜äº†LLMé€»è¾‘æ¨ç†çš„é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œå¯åˆ†ææ€§ã€‚åœ¨æ¶µç›–å„ç§æ¨ç†åœºæ™¯çš„å››ä¸ªé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éœ€è¦å¤„ç†å¤šä¸ªçº¦æŸæˆ–è§„åˆ™çš„å¤æ‚ä»»åŠ¡ä¸­ç‰¹åˆ«æœ‰æ•ˆã€‚ç¬¦å·è¾…åŠ©æ€ç»´é“¾å¯æ”¹å–„LLMåœ¨ä¸åŒæ¨¡å‹å°ºå¯¸ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Symbolic-Aided Chain-of-Thought (SACoT)æ˜¯å¯¹æ ‡å‡†æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•çš„æ”¹è¿›ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SACoTé€šè¿‡å°†è½»é‡çº§ç¬¦å·è¡¨ç¤ºé›†æˆåˆ°å°‘é‡æç¤ºä¸­ï¼Œå®ç°æ¨ç†æ­¥éª¤çš„ç»“æ„åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†éè¿­ä»£æ¨ç†è¿‡ç¨‹ä¸­çš„æ¨ç†æ¨¡å¼çš„æ˜ç¡®æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†æ ‡å‡†æç¤ºæŠ€æœ¯çš„é€šç”¨æ€§ã€‚</li>
<li>SACoTå¢å¼ºäº†LLMé€»è¾‘æ¨ç†çš„é€æ˜åº¦ã€å¯è§£é‡Šæ€§å’Œå¯åˆ†ææ€§ã€‚</li>
<li>åœ¨å¤šä¸ªé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSACoTè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¦‚éœ€è¦å¤„ç†å¤šä¸ªçº¦æŸæˆ–è§„åˆ™çš„æƒ…å¢ƒã€‚</li>
<li>SACoTåœ¨å„ç§æ¨¡å‹å°ºå¯¸ä¸Šéƒ½èƒ½æ”¹å–„LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a61175ccf3c29757a7c230adc1c4ee90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565a2f3cf199c6e909864cdeadaf57b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62211285c0f33f0f2f2df7a2c2e8c463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984a24c27dde6ee82dd5c6b55db4db1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd8c4a3b464e51d4bf1a247a25254577.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Where-to-Start-Alignment-Diffusion-Large-Language-Model-May-Demand-a-Distinct-Position"><a href="#Where-to-Start-Alignment-Diffusion-Large-Language-Model-May-Demand-a-Distinct-Position" class="headerlink" title="Where to Start Alignment? Diffusion Large Language Model May Demand a   Distinct Position"></a>Where to Start Alignment? Diffusion Large Language Model May Demand a   Distinct Position</h2><p><strong>Authors:Zhixin Xie, Xurui Song, Jun Luo</strong></p>
<p>Diffusion Large Language Models (dLLMs) have recently emerged as a competitive non-autoregressive paradigm due to their unique training and inference approach. However, there is currently a lack of safety study on this novel architecture. In this paper, we present the first analysis of dLLMsâ€™ safety performance and propose a novel safety alignment method tailored to their unique generation characteristics. Specifically, we identify a critical asymmetry between the defender and attacker in terms of security. For the defender, we reveal that the middle tokens of the response, rather than the initial ones, are more critical to the overall safety of dLLM outputs; this seems to suggest that aligning middle tokens can be more beneficial to the defender. The attacker, on the contrary, may have limited power to manipulate middle tokens, as we find dLLMs have a strong tendency towards a sequential generation order in practice, forcing the attack to meet this distribution and diverting it from influencing the critical middle tokens. Building on this asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method that directly aligns the modelâ€™s middle generation with safe refusals exploiting reinforcement learning. We implement MOSA and compare its security performance against eight attack methods on two benchmarks. We also test the utility of MOSA-aligned dLLM on coding, math, and general reasoning. The results strongly prove the superiority of MOSA. </p>
<blockquote>
<p>æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ç”±äºå…¶ç‹¬ç‰¹çš„è®­ç»ƒå’Œæ¨ç†æ–¹æ³•ï¼Œæœ€è¿‘æ¶Œç°ä¸ºä¸€ç§å…·æœ‰ç«äº‰åŠ›çš„éè‡ªå›å½’èŒƒå¼ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºè¿™ç§æ–°å‹æ¶æ„çš„å®‰å…¨ç ”ç©¶å°šæ˜¾ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹dLLMsçš„å®‰å…¨æ€§èƒ½è¿›è¡Œäº†é¦–æ¬¡åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å…¶ç‹¬ç‰¹ç”Ÿæˆç‰¹æ€§å®šåˆ¶çš„æ–°å‹å®‰å…¨å¯¹é½æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºåœ¨å®‰å…¨æ€§æ–¹é¢ï¼Œé˜²å¾¡è€…æ”»å‡»è€…ä¹‹é—´å­˜åœ¨å…³é”®ä¸å¯¹ç§°æ€§ã€‚å¯¹äºé˜²å¾¡è€…è€Œè¨€ï¼Œæˆ‘ä»¬å‘ç°å“åº”çš„ä¸­é—´ä»¤ç‰Œç›¸æ¯”åˆå§‹ä»¤ç‰Œï¼Œå¯¹dLLMè¾“å‡ºçš„æ•´ä½“å®‰å…¨æ€§æ›´ä¸ºå…³é”®ï¼›è¿™ä¼¼ä¹è¡¨æ˜å¯¹é½ä¸­é—´ä»¤ç‰Œå¯èƒ½å¯¹é˜²å¾¡è€…æ›´ä¸ºæœ‰ç›Šã€‚ç›¸åï¼Œæ”»å‡»è€…å¯èƒ½éš¾ä»¥æ“çºµä¸­é—´ä»¤ç‰Œï¼Œå› ä¸ºæˆ‘ä»¬å‘ç°dLLMsåœ¨å®è·µä¸­å…·æœ‰å¼ºçƒˆçš„é¡ºåºç”Ÿæˆå€¾å‘ï¼Œè¿™ä½¿å¾—æ”»å‡»å¿…é¡»éµå¾ªè¿™ç§åˆ†å¸ƒï¼Œè€Œæ— æ³•å½±å“å…³é”®ä¸­é—´ä»¤ç‰Œã€‚åŸºäºè¿™ç§ä¸å¯¹ç§°æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Middle-tOken Safety Alignmentï¼ˆMOSAï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥å¯¹é½æ¨¡å‹ä¸­é—´ç”Ÿæˆçš„å®‰å…¨æ‹’ç»ç­–ç•¥ã€‚æˆ‘ä»¬å®ç°äº†MOSAï¼Œå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹å…«ç§æ”»å‡»æ–¹æ³•è¿›è¡Œäº†å®‰å…¨æ€§èƒ½æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†MOSAå¯¹é½çš„dLLMåœ¨ç¼–ç ã€æ•°å­¦å’Œä¸€èˆ¬æ¨ç†æ–¹é¢çš„å®ç”¨æ€§ã€‚ç»“æœå……åˆ†è¯æ˜äº†MOSAçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12398v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusion Large Language Modelsï¼ˆdLLMsï¼‰çš„å®‰å…¨æ€§èƒ½åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹å…¶ç‹¬ç‰¹ç”Ÿæˆç‰¹æ€§çš„æ–°å‹å®‰å…¨å¯¹é½æ–¹æ³•â€”â€”Middle-tOken Safety Alignmentï¼ˆMOSAï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå¯¹äºé˜²å¾¡è€…è€Œè¨€ï¼Œå“åº”çš„ä¸­é—´ä»¤ç‰Œå¯¹dLLMè¾“å‡ºçš„æ•´ä½“å®‰å…¨æ€§æ›´ä¸ºå…³é”®ã€‚åŸºäºæ­¤ä¸å¯¹ç§°æ€§ï¼ŒMOSAæ–¹æ³•ç›´æ¥å¯¹æ¨¡å‹çš„ä¸­æœŸç”Ÿæˆè¿›è¡Œå®‰å…¨æ‹’ç»å¯¹é½ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å®ç°ã€‚å®éªŒè¯æ˜ï¼ŒMOSAåœ¨å®‰å…¨æ€§èƒ½æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>dLLMsä½œä¸ºä¸€ç§éè‡ªå›å½’æ¨¡å‹ï¼Œå…·æœ‰ç‹¬ç‰¹çš„è®­ç»ƒå’Œæ¨ç†æ–¹æ³•ï¼Œä½†ç›®å‰å¯¹å…¶å®‰å…¨æ€§çš„ç ”ç©¶ä»ä¸è¶³ã€‚</li>
<li>é˜²å¾¡è€…åœ¨dLLMsçš„å®‰å…¨æ€§ä¸­ï¼Œä¸­é—´ä»¤ç‰Œçš„é‡è§†ç¨‹åº¦é«˜äºåˆå§‹ä»¤ç‰Œã€‚</li>
<li>æ”»å‡»è€…å¯¹dLLMsä¸­é—´ä»¤ç‰Œçš„æ“æ§èƒ½åŠ›æœ‰é™ï¼Œå› ä¸ºdLLMsåœ¨å®é™…ä¸­æœ‰å¼ºçƒˆçš„é¡ºåºç”Ÿæˆå€¾å‘ã€‚</li>
<li>åŸºäºä¸Šè¿°ä¸å¯¹ç§°æ€§ï¼Œæå‡ºäº†Middle-tOken Safety Alignmentï¼ˆMOSAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ç›´æ¥å¯¹æ¨¡å‹çš„ä¸­æœŸç”Ÿæˆè¿›è¡Œå®‰å…¨æ‹’ç»å¯¹é½ã€‚</li>
<li>MOSAæ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œå¯¹æŠ—å…«ç§æ”»å‡»æ–¹æ³•ï¼Œè¡¨ç°å‡ºå“è¶Šçš„å®‰å…¨æ€§èƒ½ã€‚</li>
<li>MOSAæ–¹æ³•åœ¨ç¼–ç¨‹ã€æ•°å­¦å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸­çš„å®ç”¨æ€§å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ded052af1c8c553d15835b8160d5436.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0364225a4fa1378141495d346407dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71e9fe80666be9123890181df165ef03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62bf86cd05d516f94af265c768f9e9dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1711cc37dd4d0ea8ede6621701bfa45c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a4e83a59ef92fd76e2d013b8cb6fef2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f951b3707c28fc830c236aa6c87db956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faaffd080223d0202a75cc21d805e138.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-14e939ae0e1c5761564f4fa7243d1ef3.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  RepreGuard Detecting LLM-Generated Text by Revealing Hidden   Representation Patterns
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d72a890032ba1f8408d8b220e2fcb002.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  Express4D Expressive, Friendly, and Extensible 4D Facial Motion   Generation Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
