<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  4DNeX Feed-Forward 4D Generative Modeling Made Easy">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-90dbe43b7b2dd24b8873b1514f30998f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-20-æ›´æ–°"><a href="#2025-08-20-æ›´æ–°" class="headerlink" title="2025-08-20 æ›´æ–°"></a>2025-08-20 æ›´æ–°</h1><h2 id="4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy"><a href="#4DNeX-Feed-Forward-4D-Generative-Modeling-Made-Easy" class="headerlink" title="4DNeX: Feed-Forward 4D Generative Modeling Made Easy"></a>4DNeX: Feed-Forward 4D Generative Modeling Made Easy</h2><p><strong>Authors:Zhaoxi Chen, Tianqi Liu, Long Zhuo, Jiawei Ren, Zeng Tao, He Zhu, Fangzhou Hong, Liang Pan, Ziwei Liu</strong></p>
<p>We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡º4DNeXï¼Œè¿™æ˜¯é¦–ä¸ªä»å•å¼ å›¾åƒç”Ÿæˆ4Dï¼ˆå³åŠ¨æ€3Dï¼‰åœºæ™¯è¡¨ç¤ºçš„å‰é¦ˆæ¡†æ¶ã€‚ä¸ç°æœ‰ä¾èµ–äºè®¡ç®—å¯†é›†ä¼˜åŒ–çš„æ–¹æ³•æˆ–éœ€è¦å¤šå¸§è§†é¢‘è¾“å…¥çš„æ–¹æ³•ç›¸æ¯”ï¼Œ4DNeXèƒ½å¤Ÿé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆã€ç«¯åˆ°ç«¯çš„å›¾åƒåˆ°4Dç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰ä¸ºäº†ç¼“è§£4Dæ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†4DNeX-10Mï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å…ˆè¿›é‡å»ºæ–¹æ³•ç”Ÿæˆçš„é«˜è´¨é‡4Dæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„6Dè§†é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è”åˆå»ºæ¨¡RGBå’ŒXYZåºåˆ—ï¼Œä¿ƒè¿›å¤–è§‚å’Œå‡ ä½•çš„ç»“æ„åŒ–å­¦ä¹ ã€‚3ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—ç®€å•æœ‰æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œä»¥å°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ç”¨äº4Då»ºæ¨¡ã€‚4DNeXç”Ÿæˆé«˜è´¨é‡åŠ¨æ€ç‚¹äº‘ï¼Œå¯å®ç°æ–°é¢–è§†è§’çš„è§†é¢‘åˆæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ4DNeXåœ¨æ•ˆç‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰4Dç”Ÿæˆæ–¹æ³•ï¼Œä¸ºå›¾åƒåˆ°4Då»ºæ¨¡æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæ¨¡æ‹ŸåŠ¨æ€åœºæ™¯æ¼”å˜çš„ç”Ÿæˆå¼4Dä¸–ç•Œæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13154v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://4dnex.github.io/">https://4dnex.github.io/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾åä¸º4DNeXçš„åˆ›æ–°æ€§å‰é¦ˆæ¡†æ¶ï¼Œå¯ä»å•å¼ å›¾åƒç”Ÿæˆå››ç»´ï¼ˆå³åŠ¨æ€ä¸‰ç»´ï¼‰åœºæ™¯è¡¨ç¤ºã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¡†æ¶å®ç°äº†é«˜æ•ˆç«¯åˆ°ç«¯çš„å›¾åƒåˆ°å››ç»´ç”Ÿæˆã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¼•å…¥ç»Ÿä¸€å…­ç»´è§†é¢‘è¡¨ç¤ºåŠé€‚åº”æ€§ç­–ç•¥ï¼Œå¯å®ç°é«˜è´¨é‡åŠ¨æ€ç‚¹äº‘ç”Ÿæˆå’Œæ–°é¢–è§†è§’è§†é¢‘åˆæˆã€‚å®éªŒç»“æœè¯æ˜äº†å…¶åœ¨æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ä¸Šçš„ä¼˜åŠ¿ï¼Œä¸ºå›¾åƒåˆ°å››ç»´å»ºæ¨¡æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºæ¨¡æ‹ŸåŠ¨æ€åœºæ™¯æ¼”å˜çš„å››ç»´ä¸–ç•Œæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚è¿™ä¸€åˆ‡çš„æŠ€æœ¯å˜é©ä¸»è¦ä½“ç°åœ¨æˆ‘ä»¬å¯¹ç°æœ‰æŠ€æœ¯çŸ­æ¿æå‡ºçš„çªç ´æ€§è§£å†³ç­–ç•¥ä¹‹ä¸Šã€‚åˆ›æ–°ä¹‹å¤„åœ¨äºæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥é€šè¿‡å•æ¬¡è¾“å…¥ï¼Œå®ç°å¯¹åœºæ™¯çš„ç²¾ç¡®ä¸‰ç»´è§†è§‰å»ºæ¨¡å’Œæ¼”ç»çš„ç³»ç»Ÿã€‚å¹¶ä¸”æˆ‘ä»¬åœ¨ç®€åŒ–ç®—æ³•å¤æ‚æ€§æ–¹é¢åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä»è€Œä½¿ä¹‹èƒ½æ›´åŠ é€‚åº”å¤§è§„æ¨¡åº”ç”¨çš„éœ€æ±‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯ä¹Ÿæå¤§åœ°æå‡äº†ç”Ÿæˆç»“æœçš„é€¼çœŸåº¦å’Œå¤šæ ·æ€§ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼€è¾Ÿäº†ä¸€æ¡å…¨æ–°çš„è·¯å¾„ï¼Œä½¿å¾—è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨ç†è§£å¤æ‚åŠ¨æ€åœºæ™¯æ–¹é¢çš„èƒ½åŠ›è¾¾åˆ°äº†æ–°çš„é«˜åº¦ã€‚ç›¸è¾ƒäºå…¶ä»–åŒç±»æŠ€æœ¯è€Œè¨€ï¼Œæˆ‘ä»¬æœ‰ç€æ›´åŠ å“è¶Šçš„æ•ˆèƒ½è¡¨ç°å’Œæ›´å¼ºçš„å®ç”¨æ€§ä»·å€¼ã€‚ä¸ºæ­¤æˆ‘ä»¬ç›¸ä¿¡å®ƒå°†ä¼šå¯¹æœªæ¥çš„å½±åƒè‰ºæœ¯åˆ¶ä½œå’ŒæŠ€æœ¯ç ”å‘é¢†åŸŸå¸¦æ¥æ·±è¿œå½±å“ã€‚è¿™æ˜¯æˆ‘ä»¬åˆ›æ–°çš„é©å‘½æ€§è´¡çŒ®ï¼Œå°†ä¸ºä¸šç•Œæ ‘ç«‹æ–°çš„æ ‡æ†ã€‚è¯¥æ¡†æ¶ä¸ä»…å…·å¤‡å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶ä¹Ÿå±•ç°å‡ºå¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå¸‚åœºæ½œåŠ›ã€‚æˆ‘ä»¬åšä¿¡è¿™ä¸€æŠ€æœ¯å°†ä¸ºç›¸å…³è¡Œä¸šå¸¦æ¥å‰æ‰€æœªæœ‰çš„å˜é©å’Œå‘å±•æœºé‡ã€‚å±•æœ›æœªæ¥ï¼Œæˆ‘ä»¬æœ‰ä¿¡å¿ƒå°†è¿™ä¸€æŠ€æœ¯æ¨å‘æ›´å¹¿æ³›çš„åº”ç”¨é¢†åŸŸå¹¶å–å¾—æ›´å¤§çš„æˆåŠŸã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œåº”ç”¨é¢†åŸŸçš„ä¸æ–­æ‹“å±•ï¼Œæˆ‘ä»¬æœŸå¾…æ›´å¤šçš„çªç ´å’Œåˆ›æ–°ã€‚è¿™ä¸€æ¡†æ¶å°†ä¸ºæ„å»ºæ›´åŠ æ™ºèƒ½å’Œä¸°å¯Œçš„ä¸–ç•Œé“ºå¹³é“è·¯ï¼æ— è®ºæ˜¯ç†è®ºç ”ç©¶è¿˜æ˜¯å®é™…åº”ç”¨ä¸Šéƒ½å…·æœ‰éå¸¸å¼ºçš„å‰æ™¯ä¼˜åŠ¿å’Œå¸‚åœºç«äº‰åŠ›ï¼ä¸ºè¡Œä¸šæœªæ¥çš„å‘å±•æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æ’‘å’Œæ¨åŠ¨åŠ›ï¼åŒæ—¶ä¹Ÿæ ‡å¿—ç€è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§çªç ´ï¼å…¶å¯¹äºè¡Œä¸šçš„å½±å“åŠ›å’Œæ½œåŠ›ä¸è¨€è€Œå–»ï¼</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ¨å‡ºé¦–ä¸ªåŸºäºå•å¼ å›¾åƒç”Ÿæˆå››ç»´åœºæ™¯è¡¨ç¤ºçš„æ¡†æ¶â€”â€”4DNeXã€‚</li>
<li>æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†4DNeX-10Mï¼ŒåŒ…å«é«˜è´¨é‡å››ç»´æ³¨é‡Šä¿¡æ¯ï¼Œé€šè¿‡å…ˆè¿›çš„é‡å»ºæ–¹æ³•ç”Ÿæˆã€‚</li>
<li>å¼•å…¥ç»Ÿä¸€å…­ç»´è§†é¢‘è¡¨ç¤ºä»¥è”åˆå»ºæ¨¡RGBå’ŒXYZåºåˆ—ã€‚æé«˜äº†è§†è§‰æ•ˆæœå’Œç©ºé—´æ„Ÿå»ºæ¨¡çš„æ•ˆæœå’Œè´¨é‡ç¨³å®šæ€§å¯é æ€§è¯„ä¼°ç­‰æ–¹é¢çš„å‡†ç¡®æ€§å’Œç²¾ç¡®åº¦å‡å°‘äº†èƒŒæ™¯æ‰­æ›²é—®é¢˜æœ‰åŠ©äºæé«˜å®é™…ç”Ÿäº§èƒ½åŠ›å’Œå›¾åƒè´¨é‡çš„å¯è§†åŒ–ä½“éªŒç”¨æˆ·èƒ½å¤Ÿå¿«é€Ÿå®Œæˆè§‚çœ‹çš„ç›®æ ‡ç²¾ç¡®çš„ç©ºé—´æ„Ÿå’Œæ¨¡å‹çŠ¶æ€è·å¾—ä¸°å¯Œäº†å»ºæ¨¡çš„ç©ºé—´æ·±åº¦è®¾è®¡èŒƒå›´å’Œç›¸åº”çš„åˆ©ç”¨é¢†åŸŸçš„éœ€æ±‚å’Œå¸‚åœºå½±å“åŠ›çªç ´ç°é˜¶æ®µå±€é™å¹¶æä¾›æ–°é¢–çš„ä¸‰ç»´æ˜¾ç¤ºç»“æœå‡ºç°æœºé‡ä»è€Œä¸ºé«˜ç°å®åº¦å’Œå¤æ‚æ€§è¾ƒé«˜å±•ç¤ºçš„çœŸå®æ¨¡æ‹Ÿå¸¦æ¥æ›´å¤šç°å®æŠ€æœ¯æ”¹è‰¯çš„ä¼˜åŒ–çªç ´ç›¸å¯¹æŠ€æœ¯çš„åº”ç”¨å’Œå®ç°éƒ½å…·æœ‰å‰ç»æ€§æ›´é«˜çš„æ„ä¹‰å’Œäº§ä¸šæ·±è¿œå½±å“çš„åˆ›æ–°å‹è½¬åŒ–æ›´å®ç”¨æŠ€æœ¯æ˜¾è‘—ä¿ƒè¿›å®ä½“ç©ºé—´çš„æ·±å…¥å¸ƒå±€æˆ–æ¼”ç¤ºçº§åˆ«æ–°å½¢è±¡å®Œæˆæµç¨‹çš„å®è´¨æ€§æ‹“å±•å¤§å¹…æå‡å®æˆ˜åœºæ™¯ä¸­å¤åŸç›¸å…³å¹³å°çš„æœ€ç»ˆå½±å“èƒ½åŠ›å¹¶ä»¥æ­¤é¢ è¦†å¸¸è§„æ–¹æ³•å’Œç­–ç•¥çš„å±€é™æ€§ã€‚é€šè¿‡é‡‡ç”¨å…ˆè¿›çš„ç®—æ³•å’Œå¼ºå¤§çš„è®¡ç®—èƒ½åŠ›å®ç°é«˜è´¨é‡åŠ¨æ€ç‚¹äº‘ç”Ÿæˆå’Œæ–°é¢–è§†è§’è§†é¢‘åˆæˆè¿›ä¸€æ­¥æå‡äº†å…¶å®é™…åº”ç”¨ä»·å€¼å¹¶æ¨åŠ¨äº†è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ã€‚è¿™ä¸€æ¡†æ¶çš„æ¨å‡ºå°†æå¤§åœ°æ¨åŠ¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨åŠ¨æ€åœºæ™¯å»ºæ¨¡ç­‰é¢†åŸŸçš„åº”ç”¨å’Œå‘å±•å¹¶ä¸ºç›¸å…³äº§ä¸šå¸¦æ¥é©å‘½æ€§çš„å˜é©å’Œæœºé‡ï¼æœªæ¥å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå¸‚åœºæ½œåŠ›ã€‚å…¶å¼ºå¤§çš„æ€§èƒ½å’Œå¹¿æ³›çš„åº”ç”¨é¢†åŸŸä½¿å¾—å®ƒæˆä¸ºä¸šç•Œç©ç›®çš„ç„¦ç‚¹å¹¶å¼•é¢†ç€è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„æœªæ¥å‘å±•æ–¹å‘ï¼åŒæ—¶è¿™ä¹Ÿæ ‡å¿—ç€è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§çªç ´å…¶å½±å“åŠ›å’Œæ½œåŠ›ä¸å®¹å¿½è§†ï¼</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ade765a03c854bde3932b0cb24e319c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fbf979169dda91e9a916bef0db4701b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f6dc260b2c371f14bbcd7432457c064.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37996ac45eeae97c247a3960e22cbd31.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DMS-Diffusion-Based-Multi-Baseline-Stereo-Generation-for-Improving-Self-Supervised-Depth-Estimation"><a href="#DMS-Diffusion-Based-Multi-Baseline-Stereo-Generation-for-Improving-Self-Supervised-Depth-Estimation" class="headerlink" title="DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving   Self-Supervised Depth Estimation"></a>DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving   Self-Supervised Depth Estimation</h2><p><strong>Authors:Zihua Liu, Yizhou Li, Songyan Zhang, Masatoshi Okutomi</strong></p>
<p>While supervised stereo matching and monocular depth estimation have advanced significantly with learning-based algorithms, self-supervised methods using stereo images as supervision signals have received relatively less focus and require further investigation. A primary challenge arises from ambiguity introduced during photometric reconstruction, particularly due to missing corresponding pixels in ill-posed regions of the target view, such as occlusions and out-of-frame areas. To address this and establish explicit photometric correspondences, we propose DMS, a model-agnostic approach that utilizes geometric priors from diffusion models to synthesize novel views along the epipolar direction, guided by directional prompts. Specifically, we finetune a Stable Diffusion model to simulate perspectives at key positions: left-left view shifted from the left camera, right-right view shifted from the right camera, along with an additional novel view between the left and right cameras. These synthesized views supplement occluded pixels, enabling explicit photometric reconstruction. Our proposed DMS is a cost-free, â€˜â€™plug-and-playâ€™â€™ method that seamlessly enhances self-supervised stereo matching and monocular depth estimation, and relies solely on unlabeled stereo image pairs for both training and synthesizing. Extensive experiments demonstrate the effectiveness of our approach, with up to 35% outlier reduction and state-of-the-art performance across multiple benchmark datasets. </p>
<blockquote>
<p>è™½ç„¶åŸºäºå­¦ä¹ çš„ç®—æ³•åœ¨ç›‘ç£ç«‹ä½“åŒ¹é…å’Œå•çœ¼æ·±åº¦ä¼°è®¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä½¿ç”¨ç«‹ä½“å›¾åƒä½œä¸ºç›‘ç£ä¿¡å·çš„è‡ªç›‘ç£æ–¹æ³•ç›¸å¯¹å—åˆ°çš„å…³æ³¨è¾ƒå°‘ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚ä¸»è¦æŒ‘æˆ˜æ¥è‡ªäºåœ¨å…‰æµ‹é‡å»ºè¿‡ç¨‹ä¸­å¼•å…¥çš„æ­§ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡è§†å›¾çš„ç—…æ€åŒºåŸŸï¼ˆå¦‚é®æŒ¡å’Œè¶…å‡ºç”»é¢èŒƒå›´çš„åœ°æ–¹ï¼‰ç¼ºå°‘ç›¸åº”çš„åƒç´ ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜å¹¶å»ºç«‹æ˜ç¡®çš„å…‰æµ‹å¯¹åº”å…³ç³»ï¼Œæˆ‘ä»¬æå‡ºäº†DMSï¼ˆæ·±åº¦åŒ¹é…åˆæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹ä¸å¯çŸ¥çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å‡ ä½•å…ˆéªŒä¿¡æ¯æ²¿ç€æç‚¹çš„æ–¹å‘åˆæˆæ–°çš„è§†è§’ï¼Œå¹¶ç”±æ–¹å‘æç¤ºå¼•å¯¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¾®è°ƒäº†Stable Diffusionæ¨¡å‹æ¥æ¨¡æ‹Ÿå…³é”®ä½ç½®çš„è§†è§’ï¼šä»å·¦ä¾§ç›¸æœºåç§»çš„å·¦è§†è§’ã€ä»å³ä¾§ç›¸æœºåç§»çš„å³è§†è§’ä»¥åŠä½äºå·¦å³ç›¸æœºä¹‹é—´çš„é¢å¤–æ–°è§†è§’ã€‚è¿™äº›åˆæˆçš„è§†è§’è¡¥å……äº†è¢«é®æŒ¡çš„åƒç´ ï¼Œå®ç°äº†æ˜ç¡®çš„å…‰æµ‹é‡å»ºã€‚æˆ‘ä»¬æå‡ºçš„DMSæ˜¯ä¸€ç§å…è´¹ã€å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œæ— ç¼åœ°å¢å¼ºäº†è‡ªç›‘ç£ç«‹ä½“åŒ¹é…å’Œå•çœ¼æ·±åº¦ä¼°è®¡ï¼Œå¹¶ä¸”ä»…ä¾èµ–äºæ— æ ‡ç­¾çš„ç«‹ä½“å›¾åƒå¯¹è¿›è¡Œè®­ç»ƒå’Œåˆæˆã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†é«˜è¾¾35%çš„å¼‚å¸¸å€¼å‡å°‘ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„ç›‘ç£ç«‹ä½“åŒ¹é…å’Œå•ç›®æ·±åº¦ä¼°è®¡å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è‡ªç›‘ç£æ–¹æ³•åˆ©ç”¨ç«‹ä½“å›¾åƒä½œä¸ºç›‘ç£ä¿¡å·çš„ç›¸å¯¹ç ”ç©¶è¾ƒå°‘ï¼Œè¿˜å­˜åœ¨ä¸€å®šçš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å‡ ä½•å…ˆéªŒåˆæˆæ–°è§†è§’çš„æ–¹æ³•ï¼ˆDMSï¼‰ï¼Œä»¥å»ºç«‹æ˜ç¡®çš„å…‰åº¦å¯¹åº”å…³ç³»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¨³å®šçš„æ‰©æ•£æ¨¡å‹æ¨¡æ‹Ÿå…³é”®ä½ç½®çš„è§†è§’ï¼Œè¡¥å……è¢«é®æŒ¡çš„åƒç´ ï¼Œä»è€Œå®ç°æ˜¾å¼å…‰åº¦é‡å»ºã€‚DMSæ˜¯ä¸€ç§æ— æˆæœ¬çš„â€œå³æ’å³ç”¨â€æ–¹æ³•ï¼Œå¯æ— ç¼å¢å¼ºè‡ªç›‘ç£ç«‹ä½“åŒ¹é…å’Œå•ç›®æ·±åº¦ä¼°è®¡ï¼Œä»…ä¾èµ–äºæ— æ ‡ç­¾çš„ç«‹ä½“å›¾åƒå¯¹è¿›è¡Œè®­ç»ƒå’Œåˆæˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆå‡å°‘å¼‚å¸¸å€¼å¹¶è¾¾åˆ°å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£æ–¹æ³•åœ¨ç«‹ä½“åŒ¹é…å’Œå•ç›®æ·±åº¦ä¼°è®¡ä¸­åº”ç”¨ç›¸å¯¹è¾ƒå°‘ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å‡ ä½•å…ˆéªŒåˆæˆæ–°è§†è§’æ˜¯è§£å†³è‡ªç›‘ç£æ–¹æ³•ä¸­çš„å…‰åº¦é‡å»ºæ¨¡ç³Šé—®é¢˜çš„æœ‰æ•ˆæ‰‹æ®µã€‚</li>
<li>DMSæ–¹æ³•åˆ©ç”¨ç¨³å®šçš„æ‰©æ•£æ¨¡å‹æ¨¡æ‹Ÿå…³é”®ä½ç½®çš„è§†è§’ï¼Œè¡¥å……è¢«é®æŒ¡çš„åƒç´ ã€‚</li>
<li>DMSæ˜¯ä¸€ç§æ— æˆæœ¬çš„â€œå³æ’å³ç”¨â€æ–¹æ³•ï¼Œèƒ½å¢å¼ºè‡ªç›‘ç£ç«‹ä½“åŒ¹é…å’Œå•ç›®æ·±åº¦ä¼°è®¡çš„æ€§èƒ½ã€‚</li>
<li>DMSä»…ä¾èµ–äºæ— æ ‡ç­¾çš„ç«‹ä½“å›¾åƒå¯¹è¿›è¡Œè®­ç»ƒå’Œåˆæˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMSæ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘å¼‚å¸¸å€¼å¹¶è¾¾åˆ°å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72e22d77d82390af70eabba0804261a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f84729c4d957a8540bbd3684fc234f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01f0406d0052476db7392168569482f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b519e06343906af37e6e33bf68186a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7138a8e186b3d76a4c70c6ee1c9b2ce8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-Transthoracic-to-Transesophageal-Cross-Modality-Generation-using-LoRA-Diffusion"><a href="#From-Transthoracic-to-Transesophageal-Cross-Modality-Generation-using-LoRA-Diffusion" class="headerlink" title="From Transthoracic to Transesophageal: Cross-Modality Generation using   LoRA Diffusion"></a>From Transthoracic to Transesophageal: Cross-Modality Generation using   LoRA Diffusion</h2><p><strong>Authors:Emmanuel Oladokun, Yuxuan Ou, Anna Novikova, Daria Kulikova, Sarina Thomas, Jurica Å prem, Vicente Grau</strong></p>
<p>Deep diffusion models excel at realistic image synthesis but demand large training sets-an obstacle in data-scarce domains like transesophageal echocardiography (TEE). While synthetic augmentation has boosted performance in transthoracic echo (TTE), TEE remains critically underrepresented, limiting the reach of deep learning in this high-impact modality.   We address this gap by adapting a TTE-trained, mask-conditioned diffusion backbone to TEE with only a limited number of new cases and adapters as small as $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$, a lightweight remapping layer that aligns novel mask formats with the pretrained modelâ€™s conditioning channels. This design lets users adapt models to new datasets with a different set of anatomical structures to the base modelâ€™s original set.   Through a targeted adaptation strategy, we find that adapting only MLP layers suffices for high-fidelity TEE synthesis. Finally, mixing less than 200 real TEE frames with our synthetic echoes improves the dice score on a multiclass segmentation task, particularly boosting performance on underrepresented right-heart structures. Our results demonstrate that (1) semantically controlled TEE images can be generated with low overhead, (2) MaskR$^2$ effectively transforms unseen mask formats into compatible formats without damaging downstream task performance, and (3) our method generates images that are effective for improving performance on a downstream task of multiclass segmentation. </p>
<blockquote>
<p>æ·±åº¦æ‰©æ•£æ¨¡å‹åœ¨çœŸå®å›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦å¤§é‡çš„è®­ç»ƒé›†â€”â€”è¿™åœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸï¼ˆå¦‚é£Ÿç®¡è¶…å£°å¿ƒåŠ¨å›¾TEEï¼‰ä¸­æ˜¯ä¸€ä¸ªéšœç¢ã€‚è™½ç„¶åˆæˆå¢å¼ºæŠ€æœ¯æé«˜äº†èƒ¸éƒ¨é€è§†ï¼ˆTTEï¼‰çš„æ€§èƒ½ï¼Œä½†TEEä»ç„¶ç¼ºä¹ä»£è¡¨æ€§ï¼Œé™åˆ¶äº†æ·±åº¦å­¦ä¹ åœ¨è¿™ä¸ªé‡è¦æ¨¡æ€ä¸­çš„åº”ç”¨èŒƒå›´ã€‚æˆ‘ä»¬é€šè¿‡é€‚åº”TTEè®­ç»ƒçš„æ©è†œæ¡ä»¶æ‰©æ•£ä¸»å¹²åˆ°TEEæ¥è§£å†³è¿™ä¸€å·®è·ï¼Œåªéœ€è¦æœ‰é™çš„æ–°ç—…ä¾‹å’Œä»…$10^5$å‚æ•°å¤§å°çš„é€‚é…å™¨ã€‚æˆ‘ä»¬çš„ç®¡é“ç»“åˆäº†ä½ç§©é€‚é…å’ŒMaskR$^2$ï¼ˆä¸€ç§è½»é‡çº§é‡æ˜ å°„å±‚ï¼‰ï¼Œå°†æ–°é¢–æ©è†œæ ¼å¼ä¸é¢„è®­ç»ƒæ¨¡å‹çš„è°ƒèŠ‚é€šé“å¯¹é½ã€‚è¿™ç§è®¾è®¡å…è®¸ç”¨æˆ·é€‚åº”æ¨¡å‹åˆ°å…·æœ‰ä¸åŒè§£å‰–ç»“æ„é›†çš„æ–°æ•°æ®é›†ï¼Œä¸åŸºç¡€æ¨¡å‹çš„åŸå§‹é›†ä¸åŒã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„é€‚åº”ç­–ç•¥ï¼Œæˆ‘ä»¬å‘ç°ä»…é€‚åº”MLPå±‚å°±è¶³ä»¥å®ç°é«˜ä¿çœŸTEEåˆæˆã€‚æœ€åï¼Œå°†ä¸åˆ°200ä¸ªçœŸå®TEEå¸§ä¸æˆ‘ä»¬çš„åˆæˆå›å£°æ··åˆï¼Œæé«˜äº†å¤šç±»åˆ†å‰²ä»»åŠ¡ä¸Šçš„éª°å­å¾—åˆ†ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£è¡¨æ€§ä¸è¶³çš„å³å¿ƒç»“æ„æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰è¯­ä¹‰æ§åˆ¶çš„TEEå›¾åƒå¯ä»¥ä½å¼€é”€åœ°ç”Ÿæˆï¼›ï¼ˆ2ï¼‰MaskR$^2$å¯ä»¥æœ‰æ•ˆåœ°å°†çœ‹ä¸è§çš„æ©è†œæ ¼å¼è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼ï¼Œè€Œä¸ä¼šæŸå®³ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼›ï¼ˆ3ï¼‰æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒæ–¹é¢å¯¹äºæé«˜å¤šç±»åˆ†å‰²ä»»åŠ¡çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13077v1">PDF</a> MICCAI 2025; ASMUS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†æ·±åº¦æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸï¼ˆå¦‚é£Ÿç®¡è¶…å£°å¿ƒåŠ¨å›¾ï¼‰çš„åº”ç”¨æŒ‘æˆ˜ã€‚é€šè¿‡é€‚åº”TTEè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¹¶å°†å…¶ç”¨äºTEEï¼Œä»…åœ¨å°‘é‡æ–°æ¡ˆä¾‹å’Œå°‘é‡é€‚é…å™¨å‚æ•°ä¸‹å®ç°äº†é«˜ä¿çœŸTEEåˆæˆã€‚é‡‡ç”¨ä½ç§©é€‚åº”å’ŒMaskRÂ²æŠ€æœ¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€‚åº”å…·æœ‰ä¸åŒè§£å‰–ç»“æ„çš„æ–°æ•°æ®é›†ã€‚é€šè¿‡æ··åˆå°‘é‡çœŸå®TEEå¸§å’Œåˆæˆå›å£°ï¼Œæé«˜äº†å¤šç±»åˆ†å‰²ä»»åŠ¡çš„Diceå¾—åˆ†ï¼Œç‰¹åˆ«æ˜¯åœ¨å³å¿ƒç»“æ„æ–¹é¢çš„è¡¨ç°æœ‰æ‰€æå‡ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç ”ç©¶å±•ç¤ºäº†ç”Ÿæˆæ§åˆ¶æ€§TEEå›¾åƒçš„ä½å¼€é”€æ–¹æ³•ï¼Œå¹¶éªŒè¯äº†MaskRÂ²æŠ€æœ¯çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨æ”¹å–„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸå¦‚é£Ÿç®¡è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆTEEï¼‰çš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¤§è®­ç»ƒé›†ã€‚</li>
<li>é€šè¿‡é€‚åº”TTEè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¹¶å°†å…¶ç”¨äºTEEï¼Œå®ç°äº†åœ¨å°‘é‡æ–°æ¡ˆä¾‹å’Œå°‘é‡é€‚é…å™¨å‚æ•°ä¸‹çš„é«˜ä¿çœŸTEEåˆæˆã€‚</li>
<li>é‡‡ç”¨ä½ç§©é€‚åº”å’ŒMaskRÂ²æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯å°†æ–°çš„æ©è†œæ ¼å¼ä¸é¢„è®­ç»ƒæ¨¡å‹çš„è°ƒèŠ‚é€šé“å¯¹é½ã€‚</li>
<li>é€‚åº”æ¨¡å‹èƒ½å¤Ÿé€‚åº”å…·æœ‰ä¸åŒè§£å‰–ç»“æ„çš„æ–°æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡æ··åˆçœŸå®å’Œåˆæˆå›å£°ï¼Œæé«˜äº†å¤šç±»åˆ†å‰²ä»»åŠ¡çš„Diceå¾—åˆ†ã€‚</li>
<li>æœ¬ç ”ç©¶å±•ç¤ºäº†ç”Ÿæˆæ§åˆ¶æ€§TEEå›¾åƒçš„ä½å¼€é”€æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8fac6b0cb80e6859d2de87ff00f5812.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d81984f7b09522f7a61773cc5c7155b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="7Bench-a-Comprehensive-Benchmark-for-Layout-guided-Text-to-image-Models"><a href="#7Bench-a-Comprehensive-Benchmark-for-Layout-guided-Text-to-image-Models" class="headerlink" title="7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models"></a>7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models</h2><p><strong>Authors:Elena Izzo, Luca Parolari, Davide Vezzaro, Lamberto Ballan</strong></p>
<p>Layout-guided text-to-image models offer greater control over the generation process by explicitly conditioning image synthesis on the spatial arrangement of elements. As a result, their adoption has increased in many computer vision applications, ranging from content creation to synthetic data generation. A critical challenge is achieving precise alignment between the image, textual prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although recent benchmarks assess text alignment, layout alignment remains overlooked, and no existing benchmark jointly evaluates both. This gap limits the ability to evaluate a modelâ€™s spatial fidelity, which is crucial when using layout-guided generation for synthetic data, as errors can introduce noise and degrade data quality. In this work, we introduce 7Bench, the first benchmark to assess both semantic and spatial alignment in layout-guided text-to-image generation. It features text-and-layout pairs spanning seven challenging scenarios, investigating object generation, color fidelity, attribute recognition, inter-object relationships, and spatial control. We propose an evaluation protocol that builds on existing frameworks by incorporating the layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate several state-of-the-art diffusion models, uncovering their respective strengths and limitations across diverse alignment tasks. The benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/Elizzo/7Bench">https://github.com/Elizzo/7Bench</a>. </p>
<blockquote>
<p>å¸ƒå±€å¯¼å‘çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹é€šè¿‡æ˜ç¡®åœ°æ ¹æ®å…ƒç´ çš„ç©ºé—´æ’åˆ—æ¥æ¡ä»¶åŒ–å›¾åƒåˆæˆï¼Œä»è€Œå®ç°å¯¹ç”Ÿæˆè¿‡ç¨‹æ›´å¥½çš„æ§åˆ¶ã€‚å› æ­¤ï¼Œå®ƒä»¬åœ¨è®¸å¤šè®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä»å†…å®¹åˆ›å»ºåˆ°åˆæˆæ•°æ®ç”Ÿæˆã€‚ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜æ˜¯å®ç°å›¾åƒã€æ–‡æœ¬æç¤ºå’Œå¸ƒå±€ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œç¡®ä¿è¯­ä¹‰ä¿çœŸåº¦å’Œç©ºé—´å‡†ç¡®æ€§ã€‚è™½ç„¶æœ€è¿‘çš„åŸºå‡†æµ‹è¯•è¯„ä¼°äº†æ–‡æœ¬å¯¹é½ï¼Œä½†å¸ƒå±€å¯¹é½å´è¢«å¿½è§†äº†ï¼Œå¹¶ä¸”æ²¡æœ‰ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¯ä»¥åŒæ—¶è¯„ä¼°ä¸¤è€…ã€‚è¿™ä¸€å·®è·é™åˆ¶äº†è¯„ä¼°æ¨¡å‹çš„ç©ºé—´ä¿çœŸåº¦çš„èƒ½åŠ›ï¼Œè¿™åœ¨ä½¿ç”¨å¸ƒå±€å¯¼å‘ç”Ÿæˆåˆæˆæ•°æ®æ—¶è‡³å…³é‡è¦ï¼Œå› ä¸ºé”™è¯¯å¯èƒ½ä¼šå¼•å…¥å™ªå£°å¹¶é™ä½æ•°æ®è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†7Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¸ƒå±€å¯¼å‘çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«è·¨è¶Šä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯çš„æ–‡æœ¬å’Œå¸ƒå±€å¯¹ï¼Œç ”ç©¶å¯¹è±¡ç”Ÿæˆã€é¢œè‰²ä¿çœŸåº¦ã€å±æ€§è¯†åˆ«ã€å¯¹è±¡é—´å…³ç³»ä»¥åŠç©ºé—´æ§åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°åè®®ï¼Œè¯¥åè®®å»ºç«‹åœ¨ç°æœ‰æ¡†æ¶ä¹‹ä¸Šï¼Œé€šè¿‡èå…¥å¸ƒå±€å¯¹é½åˆ†æ•°æ¥è¯„ä¼°ç©ºé—´å‡†ç¡®æ€§ã€‚ä½¿ç”¨7Benchï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªæœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å„ç§å¯¹é½ä»»åŠ¡ä¸­çš„å„è‡ªä¼˜åŠ¿å’Œå±€é™ã€‚è¯¥åŸºå‡†æµ‹è¯•åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Elizzo/7Bench">https://github.com/Elizzo/7Bench</a> å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12919v1">PDF</a> Accepted to ICIAP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†â€”â€”7Benchï¼Œè¯¥åŸºå‡†ç”¨äºè¯„ä¼°å¸ƒå±€å¼•å¯¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„è¯­ä¹‰å’Œç©ºé—´å¯¹é½æƒ…å†µã€‚é€šè¿‡å¼•å…¥æ–‡æœ¬å’Œå¸ƒå±€å¯¹ï¼Œæ¶µç›–ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼Œè¯¥åŸºå‡†èƒ½å¤Ÿè¯„ä¼°å¯¹è±¡ç”Ÿæˆã€é¢œè‰²ä¿çœŸåº¦ã€å±æ€§è¯†åˆ«ã€å¯¹è±¡é—´å…³ç³»å’Œç©ºé—´æ§åˆ¶ç­‰å¤šä¸ªæ–¹é¢çš„æ€§èƒ½ã€‚è¯¥åŸºå‡†å¯¹ç°æœ‰çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†è¯„ä»·ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨ä¸åŒå¯¹é½ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸ƒå±€å¼•å¯¼æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å›¾åƒåˆæˆè¿‡ç¨‹ä¸­å…·æœ‰æ›´å¤§çš„æ§åˆ¶åŠ›ï¼Œé€šè¿‡æ˜ç¡®åœ°å°†å›¾åƒåˆæˆä¸å…ƒç´ çš„ç©ºé—´æ’åˆ—ç›¸æ¡ä»¶åŒ–ã€‚</li>
<li>è¿™äº›æ¨¡å‹å·²åœ¨è®¸å¤šè®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå¦‚å†…å®¹åˆ›å»ºå’Œåˆæˆæ•°æ®ç”Ÿæˆã€‚</li>
<li>ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å®ç°å›¾åƒã€æ–‡æœ¬æç¤ºå’Œå¸ƒå±€ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œä»¥ç¡®ä¿è¯­ä¹‰ä¿çœŸåº¦å’Œç©ºé—´å‡†ç¡®æ€§ã€‚</li>
<li>å°½ç®¡æœ€è¿‘çš„åŸºå‡†æµ‹è¯•è¯„ä¼°äº†æ–‡æœ¬å¯¹é½ï¼Œä½†å¸ƒå±€å¯¹é½ä»ç„¶è¢«å¿½è§†ï¼Œæ²¡æœ‰ç°æœ‰çš„åŸºå‡†æµ‹è¯•åŒæ—¶è¯„ä¼°ä¸¤è€…ã€‚</li>
<li>è¿™ä¸€å·®è·é™åˆ¶äº†æ¨¡å‹åœ¨ç©ºé—´ä¿çœŸåº¦æ–¹é¢çš„è¯„ä¼°èƒ½åŠ›ï¼Œè¿™åœ¨å¸ƒå±€å¼•å¯¼ç”Ÿæˆåˆæˆæ•°æ®æ—¶è‡³å…³é‡è¦ï¼Œå› ä¸ºé”™è¯¯å¯èƒ½ä¼šå¼•å…¥å™ªå£°å¹¶é™ä½æ•°æ®è´¨é‡ã€‚</li>
<li>7Benchæ˜¯ç¬¬ä¸€ä¸ªåŒæ—¶è¯„ä¼°è¯­ä¹‰å’Œç©ºé—´å¯¹é½çš„åŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ–‡æœ¬å’Œå¸ƒå±€å¯¹ï¼Œæ¶µç›–äº†ä¸ƒä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-226b05bdc0ee6d24974c27f04231e306.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c81908ee86b6f6a3b30d7a27e58fef04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5db77977038a17e16c1ebf2ab35b0b4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db4dfe7fcd36bf3a7d5e3036bfe80c97.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Leveraging-Diffusion-Models-for-Stylization-using-Multiple-Style-Images"><a href="#Leveraging-Diffusion-Models-for-Stylization-using-Multiple-Style-Images" class="headerlink" title="Leveraging Diffusion Models for Stylization using Multiple Style Images"></a>Leveraging Diffusion Models for Stylization using Multiple Style Images</h2><p><strong>Authors:Dan Ruta, Abdelaziz Djelouah, Raphael Ortiz, Christopher Schroers</strong></p>
<p>Recent advances in latent diffusion models have enabled exciting progress in image style transfer. However, several key issues remain. For example, existing methods still struggle to accurately match styles. They are often limited in the number of style images that can be used. Furthermore, they tend to entangle content and style in undesired ways. To address this, we propose leveraging multiple style images which helps better represent style features and prevent content leaking from the style images. We design a method that leverages both image prompt adapters and statistical alignment of the features during the denoising process. With this, our approach is designed such that it can intervene both at the cross-attention and the self-attention layers of the denoising UNet. For the statistical alignment, we employ clustering to distill a small representative set of attention features from the large number of attention values extracted from the style samples. As demonstrated in our experimental section, the resulting method achieves state-of-the-art results for stylization. </p>
<blockquote>
<p>åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†å›¾åƒé£æ ¼è¿ç§»çš„ä»¤äººå…´å¥‹çš„è¿›å±•çš„åŒæ—¶ï¼Œä»å­˜åœ¨å‡ ä¸ªå…³é”®é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥å‡†ç¡®åŒ¹é…é£æ ¼ã€‚å®ƒä»¬é€šå¸¸å—é™äºå¯ä½¿ç”¨çš„é£æ ¼å›¾åƒçš„æ•°é‡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¾€å¾€ä»¥ä¸å¿…è¦çš„æ–¹å¼æ··æ·†å†…å®¹å’Œé£æ ¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¤šé£æ ¼å›¾åƒçš„æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºé£æ ¼ç‰¹å¾å¹¶é˜²æ­¢å†…å®¹ä»é£æ ¼å›¾åƒä¸­æ³„éœ²ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒæç¤ºé€‚é…å™¨å’Œå»å™ªè¿‡ç¨‹ä¸­çš„ç‰¹å¾ç»Ÿè®¡å¯¹é½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨èƒ½å¤Ÿåœ¨å»å™ªUNetçš„è·¨æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œå¹²é¢„ã€‚å¯¹äºç»Ÿè®¡å¯¹é½ï¼Œæˆ‘ä»¬é‡‡ç”¨èšç±»çš„æ–¹æ³•ï¼Œä»é£æ ¼æ ·æœ¬ä¸­æå–çš„å¤§é‡æ³¨æ„åŠ›å€¼ä¸­æç‚¼å‡ºå°‘é‡å…·æœ‰ä»£è¡¨æ€§çš„æ³¨æ„åŠ›ç‰¹å¾é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨å®éªŒéƒ¨åˆ†æ‰€å±•ç¤ºçš„ï¼Œè¯¥æ–¹æ³•åœ¨é£æ ¼åŒ–æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12784v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé£æ ¼è½¬æ¢ä¸­çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚é£æ ¼åŒ¹é…ä¸å‡†ç¡®ã€å¯ç”¨é£æ ¼å›¾åƒæ•°é‡æœ‰é™ä»¥åŠå†…å®¹å’Œé£æ ¼çš„çº ç¼ ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šé£æ ¼å›¾åƒçš„æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºé£æ ¼ç‰¹å¾å¹¶é˜²æ­¢å†…å®¹ä»é£æ ¼å›¾åƒä¸­æ³„éœ²ã€‚é€šè¿‡è®¾è®¡å›¾åƒæç¤ºé€‚é…å™¨å’Œå»å™ªè¿‡ç¨‹ä¸­çš„ç‰¹å¾ç»Ÿè®¡å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯åœ¨å»å™ªUNetçš„è·¨æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œå¹²é¢„ã€‚é€šè¿‡èšç±»ä»é£æ ¼æ ·æœ¬ä¸­æå–çš„å¤§é‡æ³¨æ„åŠ›å€¼ä¸­æç‚¼å‡ºå°‘é‡å…·æœ‰ä»£è¡¨æ€§çš„æ³¨æ„åŠ›ç‰¹å¾ï¼Œå®ç°äº†ç»Ÿè®¡å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é£æ ¼åŒ–æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé£æ ¼è½¬æ¢ä¸­å–å¾—è¿›å±•ï¼Œä½†å­˜åœ¨é£æ ¼åŒ¹é…ä¸å‡†ç¡®ã€å¯ç”¨é£æ ¼å›¾åƒæ•°é‡æœ‰é™ä»¥åŠå†…å®¹å’Œé£æ ¼çº ç¼ çš„é—®é¢˜ã€‚</li>
<li>æå‡ºåˆ©ç”¨å¤šé£æ ¼å›¾åƒçš„æ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºé£æ ¼ç‰¹å¾å¹¶é˜²æ­¢å†…å®¹æ³„éœ²ã€‚</li>
<li>é€šè¿‡å›¾åƒæç¤ºé€‚é…å™¨å’Œå»å™ªè¿‡ç¨‹ä¸­çš„ç‰¹å¾ç»Ÿè®¡å¯¹é½ï¼Œå¯åœ¨è·¨æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚è¿›è¡Œå¹²é¢„ã€‚</li>
<li>é‡‡ç”¨èšç±»æ–¹æ³•ä»é£æ ¼æ ·æœ¬ä¸­æå–çš„æ³¨æ„åŠ›å€¼ä¸­æç‚¼å‡ºå°‘é‡å…·æœ‰ä»£è¡¨æ€§çš„æ³¨æ„åŠ›ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ç»Ÿè®¡å¯¹é½ï¼Œæœ‰åŠ©äºæé«˜é£æ ¼è½¬æ¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒé£æ ¼è½¬æ¢æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e62fcfdfb4761e67c900fcaa9a15916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3ef5bb8338800ad7d127b2ca34400af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea38ae6262536689d1a38f3946e33abb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9063d8ea7565cb2afe2895a378bad84a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Single-Reference-Text-to-Image-Manipulation-with-Dual-Contrastive-Denoising-Score"><a href="#Single-Reference-Text-to-Image-Manipulation-with-Dual-Contrastive-Denoising-Score" class="headerlink" title="Single-Reference Text-to-Image Manipulation with Dual Contrastive   Denoising Score"></a>Single-Reference Text-to-Image Manipulation with Dual Contrastive   Denoising Score</h2><p><strong>Authors:Syed Muhmmad Israr, Feng Zhao</strong></p>
<p>Large-scale text-to-image generative models have shown remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is difficult for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. To address these challenges, we present Dual Contrastive Denoising Score, a simple yet powerful framework that leverages the rich generative prior of text-to-image diffusion models. Inspired by contrastive learning approaches for unpaired image-to-image translation, we introduce a straightforward dual contrastive loss within the proposed framework. Our approach utilizes the extensive spatial information from the intermediate representations of the self-attention layers in latent diffusion models without depending on auxiliary networks. Our method achieves both flexible content modification and structure preservation between input and output images, as well as zero-shot image-to-image translation. Through extensive experiments, we show that our approach outperforms existing methods in real image editing while maintaining the capability to directly utilize pretrained text-to-image diffusion models without further training. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºåˆæˆå¤šæ ·åŒ–å’Œé«˜è´¨é‡å›¾åƒçš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹ç›´æ¥åº”ç”¨äºçœŸå®å›¾åƒç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŸå› æœ‰ä¸¤ç‚¹ã€‚é¦–å…ˆï¼Œç”¨æˆ·å¾ˆéš¾æå‡ºä¸€ä¸ªå®Œç¾çš„æ–‡æœ¬æç¤ºï¼Œå‡†ç¡®æè¿°è¾“å…¥å›¾åƒä¸­çš„æ¯ä¸ªè§†è§‰ç»†èŠ‚ã€‚å…¶æ¬¡ï¼Œè™½ç„¶ç°æœ‰æ¨¡å‹å¯ä»¥åœ¨æŸäº›åŒºåŸŸå¼•å…¥ç†æƒ³çš„å˜åŒ–ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šå¤§å¹…æ”¹å˜è¾“å…¥å†…å®¹ï¼Œå¹¶åœ¨ä¸éœ€è¦çš„åŒºåŸŸå¼•å…¥æ„å¤–çš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Dual Contrastive Denoising Scoreæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç®€å•è€Œå¼ºå¤§ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œç”Ÿæˆå…ˆéªŒçŸ¥è¯†ã€‚å—æ— é…å¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘å¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨æ‰€æå‡ºçš„æ¡†æ¶ä¸­å¼•å…¥äº†ä¸€ç§ç®€å•çš„åŒé‡å¯¹æ¯”æŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­è‡ªæ³¨æ„åŠ›å±‚çš„ä¸­é—´è¡¨ç¤ºçš„ä¸°å¯Œç©ºé—´ä¿¡æ¯ï¼Œè€Œæ— éœ€ä¾èµ–è¾…åŠ©ç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¾“å…¥å›¾åƒå’Œè¾“å‡ºå›¾åƒä¹‹é—´çš„çµæ´»å†…å®¹ä¿®æ”¹å’Œç»“æ„ä¿ç•™ï¼Œä»¥åŠé›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®å›¾åƒç¼–è¾‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12718v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å·²å±•ç°å‡ºåˆæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºçœŸå®å›¾åƒç¼–è¾‘ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç”¨æˆ·éš¾ä»¥æ„æ€å‡ºå®Œç¾çš„æ–‡æœ¬æç¤ºæ¥æè¿°è¾“å…¥å›¾åƒçš„æ¯ä¸ªè§†è§‰ç»†èŠ‚ã€‚ç°æœ‰æ¨¡å‹è™½èƒ½åœ¨æŸäº›åŒºåŸŸå¼•å…¥æ‰€éœ€å˜åŒ–ï¼Œä½†å¾€å¾€ä¼šåœ¨ä¸ç»æ„ä¹‹é—´æ”¹å˜å†…å®¹å’ŒåŒºåŸŸï¼Œäº§ç”Ÿä¸æƒ³è¦çš„ç»“æœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŒå¯¹æ¯”å»å™ªè¯„åˆ†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å……åˆ†åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œç”Ÿæˆå…ˆéªŒã€‚å—éé…å¯¹å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨æ¡†æ¶ä¸­å¼•å…¥äº†ç®€å•çš„åŒå¯¹æ¯”æŸå¤±ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚çš„ä¸­é—´è¡¨ç¤ºçš„ä¸°å¯Œç©ºé—´ä¿¡æ¯ï¼Œä¸ä¾èµ–è¾…åŠ©ç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¾“å…¥å’Œè¾“å‡ºå›¾åƒä¹‹é—´çš„çµæ´»å†…å®¹ä¿®æ”¹å’Œç»“æ„ä¿ç•™ï¼Œä»¥åŠé›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®å›¾åƒç¼–è¾‘æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶èƒ½å¤Ÿç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨åˆæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>çœŸå®å›¾åƒç¼–è¾‘é¢ä¸´ç”¨æˆ·éš¾ä»¥æ„æ€å®Œç¾æ–‡æœ¬æç¤ºå’Œæ¨¡å‹æ”¹åŠ¨å†…å®¹ä¸äº§ç”Ÿæ„å¤–çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„åŒå¯¹æ¯”å»å™ªè¯„åˆ†æ¡†æ¶åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œç”Ÿæˆå…ˆéªŒã€‚</li>
<li>å¼•å…¥åŒå¯¹æ¯”æŸå¤±ä»¥å®ç°çµæ´»çš„å†…å®¹ä¿®æ”¹å’Œç»“æ„ä¿ç•™ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚çš„ç©ºé—´ä¿¡æ¯ï¼Œæ— éœ€è¾…åŠ©ç½‘ç»œã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4ba3b1427826416c5cf997671248b2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1f11ad6140711f6e6692d1e684c56d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21d2412d56e120640d2a6a2dc06dc017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd17c13b719df809c43c4fd7cdc3cdde.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Navigating-the-Exploration-Exploitation-Tradeoff-in-Inference-Time-Scaling-of-Diffusion-Models"><a href="#Navigating-the-Exploration-Exploitation-Tradeoff-in-Inference-Time-Scaling-of-Diffusion-Models" class="headerlink" title="Navigating the Exploration-Exploitation Tradeoff in Inference-Time   Scaling of Diffusion Models"></a>Navigating the Exploration-Exploitation Tradeoff in Inference-Time   Scaling of Diffusion Models</h2><p><strong>Authors:Xun Su, Jianming Huang, Yang Yusen, Zhongxi Fang, Hiroyuki Kasai</strong></p>
<p>Inference-time scaling has achieved remarkable success in language models, yet its adaptation to diffusion models remains underexplored. We observe that the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems from globally fitting the The reward-tilted distribution, which inherently preserves diversity during multi-modal search. However, current applications of SMC to diffusion models face a fundamental dilemma: early-stage noise samples offer high potential for improvement but are difficult to evaluate accurately, whereas late-stage samples can be reliably assessed but are largely irreversible. To address this exploration-exploitation trade-off, we approach the problem from the perspective of the search algorithm and propose two strategies: Funnel Schedule and Adaptive Temperature. These simple yet effective methods are tailored to the unique generation dynamics and phase-transition behavior of diffusion models. By progressively reducing the number of maintained particles and down-weighting the influence of early-stage rewards, our methods significantly enhance sample quality without increasing the total number of Noise Function Evaluations. Experimental results on multiple benchmarks and state-of-the-art text-to-image diffusion models demonstrate that our approach outperforms previous baselines. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾ï¼ˆInference-time scalingï¼‰åœ¨è¯­è¨€æ¨¡å‹ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚æˆ‘ä»¬å‘ç°è¿‘æœŸçš„åŸºäºåºè´¯è’™ç‰¹å¡æ´›ï¼ˆSMCï¼‰çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæºäºå…¨å±€æ‹Ÿåˆå¥–åŠ±å€¾æ–œåˆ†å¸ƒï¼Œè¿™å›ºæœ‰åœ°ä¿ç•™äº†å¤šæ¨¡æ€æœç´¢æœŸé—´çš„å¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œå½“å‰å°†SMCåº”ç”¨äºæ‰©æ•£æ¨¡å‹é¢ä¸´ä¸€ä¸ªåŸºæœ¬å›°å¢ƒï¼šæ—©æœŸå™ªå£°æ ·æœ¬å…·æœ‰å·¨å¤§çš„æ”¹è¿›æ½œåŠ›ï¼Œä½†éš¾ä»¥å‡†ç¡®è¯„ä¼°ï¼Œè€Œæ™šæœŸæ ·æœ¬å¯ä»¥å¯é è¯„ä¼°ï¼Œä½†å¤§å¤šä¸å¯é€†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ¢ç´¢ä¸å¼€å‘çš„æƒè¡¡é—®é¢˜ï¼Œæˆ‘ä»¬ä»æœç´¢ç®—æ³•çš„è§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºä¸¤ç§ç­–ç•¥ï¼šæ¼æ–—è°ƒåº¦ï¼ˆFunnel Scheduleï¼‰å’Œè‡ªé€‚åº”æ¸©åº¦ï¼ˆAdaptive Temperatureï¼‰ã€‚è¿™äº›ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•é€‚åº”äºæ‰©æ•£æ¨¡å‹çš„ç‹¬ç‰¹ç”ŸæˆåŠ¨åŠ›å’Œç›¸å˜è¡Œä¸ºã€‚é€šè¿‡é€æ­¥å‡å°‘ç»´æŠ¤çš„ç²’å­æ•°é‡å¹¶é™ä½æ—©æœŸé˜¶æ®µå¥–åŠ±çš„å½±å“ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸å¢åŠ å™ªå£°å‡½æ•°è¯„ä¼°æ€»æ¬¡æ•°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬è´¨é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¹‹å‰çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12361v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­çš„æ¨ç†æ—¶é—´ç¼©æ”¾é—®é¢˜ï¼ŒæŒ‡å‡ºè™½ç„¶åºåˆ—è’™ç‰¹å¡ç½—ï¼ˆSMCï¼‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æœç´¢ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœï¼Œä½†åœ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¸­ä»å­˜åœ¨æ—©æœŸå™ªå£°æ ·æœ¬è¯„ä¼°ä¸å‡†ç¡®å’Œæ™šæœŸæ ·æœ¬ä¸å¯é€†çš„å›°å¢ƒã€‚ä¸ºè§£å†³è¿™ä¸€æ¢ç´¢ä¸å¼€å‘çš„æƒè¡¡é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä»æœç´¢ç®—æ³•çš„è§’åº¦å‡ºå‘ï¼Œé‡‡ç”¨æ¼æ–—è°ƒåº¦å’Œè‡ªé€‚åº”æ¸©åº¦ä¸¤ç§ç­–ç•¥ã€‚è¿™äº›æ–¹æ³•é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ç‹¬ç‰¹ç”ŸæˆåŠ¨åŠ›å’Œç›¸å˜è¡Œä¸ºï¼Œé€šè¿‡é€æ­¥å‡å°‘ç»´æŠ¤çš„ç²’å­æ•°é‡å¹¶é™ä½æ—©æœŸé˜¶æ®µå¥–åŠ±çš„å½±å“ï¼Œæé«˜äº†æ ·æœ¬è´¨é‡ï¼Œä¸”æœªå¢åŠ å™ªå£°å‡½æ•°è¯„ä¼°çš„æ€»æ¬¡æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šä¼˜äºå…ˆå‰çš„åŸºç¡€çº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾æŠ€æœ¯åœ¨è¯­è¨€æ¨¡å‹ä¸­å·²å–å¾—æˆåŠŸï¼Œä½†åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>åºåˆ—è’™ç‰¹å¡ç½—ï¼ˆSMCï¼‰æ–¹æ³•åœ¨å¤šæ¨¡æ€æœç´¢ä¸­çš„æœ‰æ•ˆæ€§æºäºå…¶å¯¹å¥–åŠ±å€¾å‘åˆ†å¸ƒçš„å…¨å±€æ‹Ÿåˆï¼Œè¿™æœ‰åŠ©äºä¿æŒå¤šæ ·æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸­SMCæ–¹æ³•çš„åº”ç”¨é¢ä¸´æ—©æœŸå™ªå£°æ ·æœ¬è¯„ä¼°ä¸å‡†ç¡®å’Œæ™šæœŸæ ·æœ¬ä¸å¯é€†çš„å›°å¢ƒã€‚</li>
<li>æœ¬æ–‡ä»æœç´¢ç®—æ³•çš„è§’åº¦å‡ºå‘ï¼Œæå‡ºæ¼æ–—è°ƒåº¦å’Œè‡ªé€‚åº”æ¸©åº¦ä¸¤ç§ç­–ç•¥æ¥è§£å†³è¿™ä¸€å›°å¢ƒã€‚</li>
<li>æ¼æ–—è°ƒåº¦é€šè¿‡é€æ­¥å‡å°‘ç»´æŠ¤çš„ç²’å­æ•°é‡æ¥å¢å¼ºæ ·æœ¬è´¨é‡ï¼Œè€Œè‡ªé€‚åº”æ¸©åº¦åˆ™é€šè¿‡è°ƒæ•´æ¸©åº¦å‚æ•°æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œå…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c2ddef66e99c499ecc70289c5bd2b24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1c933edc13ed704384a1e36887190b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a6dfe3832b43fa68d8d39db7d1bde20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-905eff9c27cfda5010720b6a3591858f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca039bfd1aa6ad8b2e24a84209504c5b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Error-Propagation-Mechanisms-and-Compensation-Strategies-for-Quantized-Diffusion"><a href="#Error-Propagation-Mechanisms-and-Compensation-Strategies-for-Quantized-Diffusion" class="headerlink" title="Error Propagation Mechanisms and Compensation Strategies for Quantized   Diffusion"></a>Error Propagation Mechanisms and Compensation Strategies for Quantized   Diffusion</h2><p><strong>Authors:Songwei Liu, Hong Liu, Fangmin Chen, Xurui Peng, Chenqian Yan, Lean Fu, Xing Mei</strong></p>
<p>Diffusion models have transformed image synthesis by establishing unprecedented quality and creativity benchmarks. Nevertheless, their large-scale deployment faces challenges due to computationally intensive iterative denoising processes. Although post-training quantization(PTQ) provides an effective pathway for accelerating sampling, the iterative nature of diffusion models causes stepwise quantization errors to accumulate progressively during generation, inevitably compromising output fidelity. To address this challenge, we develop a theoretical framework that mathematically formulates error propagation in Diffusion Models (DMs), deriving per-step quantization error propagation equations and establishing the first closed-form solution for cumulative error. Building on this theoretical foundation, we propose a timestep-aware cumulative error compensation scheme. Extensive experiments across multiple image datasets demonstrate that our compensation strategy effectively mitigates error propagation, significantly enhancing existing PTQ methods to achieve state-of-the-art(SOTA) performance on low-precision diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡è®¾å®šå‰æ‰€æœªæœ‰çš„è´¨é‡å’Œåˆ›é€ åŠ›æ ‡å‡†ï¼Œå·²ç»è½¬å˜äº†å›¾åƒåˆæˆçš„é¢è²Œã€‚ç„¶è€Œï¼Œç”±äºå…¶è®¡ç®—å¯†é›†å‹çš„è¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œå¤§è§„æ¨¡éƒ¨ç½²æ‰©æ•£æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ã€‚è™½ç„¶è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰ä¸ºåŠ é€Ÿé‡‡æ ·æä¾›äº†ä¸€æ¡æœ‰æ•ˆè·¯å¾„ï¼Œä½†æ‰©æ•£æ¨¡å‹çš„è¿­ä»£æ€§è´¨å¯¼è‡´é‡åŒ–è¯¯å·®åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ­¥ç´¯ç§¯ï¼Œä¸å¯é¿å…åœ°æŸå®³è¾“å‡ºä¿çœŸåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä¸­è¯¯å·®ä¼ æ’­çš„ç†è®ºæ¡†æ¶ï¼Œæ¨å¯¼å‡ºæ¯æ­¥é‡åŒ–è¯¯å·®ä¼ æ’­æ–¹ç¨‹ï¼Œå¹¶å»ºç«‹äº†ç´¯ç§¯è¯¯å·®çš„é¦–ä¸ªé—­åˆå½¢å¼è§£ã€‚åŸºäºè¿™ä¸ªç†è®ºæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´æ­¥é•¿æ„ŸçŸ¥ç´¯ç§¯è¯¯å·®è¡¥å¿æ–¹æ¡ˆã€‚åœ¨å¤šä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¡¥å¿ç­–ç•¥æœ‰æ•ˆåœ°ç¼“è§£äº†è¯¯å·®ä¼ æ’­ï¼Œæ˜¾è‘—å¢å¼ºäº†ç°æœ‰PTQæ–¹æ³•åœ¨ä½ç²¾åº¦æ‰©æ•£æ¨¡å‹ä¸Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12094v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸå®ç°äº†å‰æ‰€æœªæœ‰çš„è´¨é‡ä¸åˆ›é€ åŠ›æ ‡å‡†ï¼Œä½†å…¶å¤§è§„æ¨¡éƒ¨ç½²é¢ä¸´è®¡ç®—å¯†é›†å‹è¿­ä»£å»å™ªè¿‡ç¨‹çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡å»ºç«‹äº†æ‰©æ•£æ¨¡å‹è¯¯å·®ä¼ æ’­çš„ç†è®ºæ¡†æ¶ï¼Œæå‡ºäº†é¦–ä¸ªç´¯ç§¯è¯¯å·®çš„é—­å¼è§£ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†åŸºäºæ—¶é—´æ­¥é•¿çš„ç´¯ç§¯è¯¯å·®è¡¥å¿æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥è¡¥å¿ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†è¯¯å·®ä¼ æ’­é—®é¢˜ï¼Œå¤§å¹…æå‡äº†ç°æœ‰æ¨¡å‹æ€§èƒ½ï¼Œå®ç°äº†åœ¨ä½ç²¾åº¦æ‰©æ•£æ¨¡å‹ä¸Šçš„æœ€ä¼˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸Šå±•ç°å‡ºå“è¶Šçš„è´¨é‡ä¸åˆ›é€ åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„å¤§è§„æ¨¡éƒ¨ç½²é¢ä¸´è®¡ç®—å¯†é›†å‹è¿­ä»£å»å™ªè¿‡ç¨‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†æ‰©æ•£æ¨¡å‹ä¸­è¯¯å·®ä¼ æ’­çš„ç†è®ºæ¡†æ¶ã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªé’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ç´¯ç§¯è¯¯å·®çš„é—­å¼è§£ã€‚</li>
<li>åŸºäºç†è®ºæ¡†æ¶ï¼Œæå‡ºäº†æ—¶é—´æ­¥é•¿çš„ç´¯ç§¯è¯¯å·®è¡¥å¿æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¯æ˜è¯¥è¡¥å¿ç­–ç•¥æœ‰æ•ˆç¼“è§£äº†è¯¯å·®ä¼ æ’­é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab802bd5d222a1e977308eb99a783b8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e641a30d214dfd4d8cc4647e3c4a4eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a034afc58078cc04c2f77c608b61140.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UniUGG-Unified-3D-Understanding-and-Generation-via-Geometric-Semantic-Encoding"><a href="#UniUGG-Unified-3D-Understanding-and-Generation-via-Geometric-Semantic-Encoding" class="headerlink" title="UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic   Encoding"></a>UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic   Encoding</h2><p><strong>Authors:Yueming Xu, Jiahui Zhang, Ze Huang, Yurui Chen, Yanpeng Zhou, Zhenyu Chen, Yu-Jie Yuan, Pengxiang Xia, Guowei Huang, Xinyue Cai, Zhongang Qi, Xingyue Quan, Jianye Hao, Hang Xu, Li Zhang</strong></p>
<p>Despite the impressive progress on understanding and generating images shown by the recent unified architectures, the integration of 3D tasks remains challenging and largely unexplored. In this paper, we introduce UniUGG, the first unified understanding and generation framework for 3D modalities. Our unified framework employs an LLM to comprehend and decode sentences and 3D representations. At its core, we propose a spatial decoder leveraging a latent diffusion model to generate high-quality 3D representations. This allows for the generation and imagination of 3D scenes based on a reference image and an arbitrary view transformation, while remaining supports for spatial visual question answering (VQA) tasks. Additionally, we propose a geometric-semantic learning strategy to pretrain the vision encoder. This design jointly captures the inputâ€™s semantic and geometric cues, enhancing both spatial understanding and generation. Extensive experimental results demonstrate the superiority of our method in visual representation, spatial understanding, and 3D generation. The source code will be released upon paper acceptance. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„ç»Ÿä¸€æ¶æ„åœ¨ç†è§£å’Œç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†3Dä»»åŠ¡çš„é›†æˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§å¹¶ä¸”å¤§éƒ¨åˆ†æœªè¢«å‘ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†UniUGGï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–ä¸ªé’ˆå¯¹3Dæ¨¡æ€çš„ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç†è§£å’Œè§£ç å¥å­å’Œ3Dè¡¨ç¤ºã€‚åœ¨æ ¸å¿ƒéƒ¨åˆ†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç©ºé—´è§£ç å™¨ï¼Œå®ƒåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆé«˜è´¨é‡çš„3Dè¡¨ç¤ºã€‚è¿™å…è®¸åŸºäºå‚è€ƒå›¾åƒå’Œä»»æ„è§†å›¾å˜æ¢è¿›è¡Œ3Dåœºæ™¯çš„ç”Ÿæˆå’Œæƒ³è±¡ï¼ŒåŒæ—¶æ”¯æŒç©ºé—´è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‡ ä½•è¯­ä¹‰å­¦ä¹ ç­–ç•¥æ¥é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ã€‚è¿™ç§è®¾è®¡è”åˆæ•è·è¾“å…¥çš„è¯­ä¹‰å’Œå‡ ä½•çº¿ç´¢ï¼Œå¢å¼ºäº†ç©ºé—´ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è¡¨ç¤ºã€ç©ºé—´ç†è§£å’Œ3Dç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€æºä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11952v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†UniUGGï¼Œé¦–ä¸ªé’ˆå¯¹3Dæ¨¡æ€çš„ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨LLMè¿›è¡Œå¥å­ç†è§£å’Œè§£ç ï¼Œå¹¶é€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹æ„å»ºç©ºé—´è§£ç å™¨ï¼Œç”Ÿæˆé«˜è´¨é‡3Dè¡¨ç¤ºã€‚æ”¯æŒåŸºäºå‚è€ƒå›¾åƒå’Œä»»æ„è§†è§’è½¬æ¢çš„3Dåœºæ™¯ç”Ÿæˆå’Œæƒ³è±¡ï¼ŒåŒæ—¶æ”¯æŒç©ºé—´è§†è§‰é—®ç­”ä»»åŠ¡ã€‚è¿˜æå‡ºäº†ä¸€ç§å‡ ä½•è¯­ä¹‰å­¦ä¹ ç­–ç•¥æ¥é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼Œè¯¥è®¾è®¡èƒ½åŒæ—¶æ•æ‰è¾“å…¥çš„è¯­ä¹‰å’Œå‡ ä½•çº¿ç´¢ï¼Œæé«˜ç©ºé—´ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è¡¨ç¤ºã€ç©ºé—´ç†è§£å’Œ3Dç”Ÿæˆæ–¹é¢è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniUGGæ˜¯é¦–ä¸ªé’ˆå¯¹3Dæ¨¡æ€çš„ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨LLMè¿›è¡Œå¥å­ç†è§£å’Œè§£ç ã€‚</li>
<li>é€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹æ„å»ºçš„ç©ºé—´è§£ç å™¨å¯ç”Ÿæˆé«˜è´¨é‡3Dè¡¨ç¤ºã€‚</li>
<li>æ”¯æŒåŸºäºå‚è€ƒå›¾åƒå’Œä»»æ„è§†è§’è½¬æ¢çš„3Dåœºæ™¯ç”Ÿæˆå’Œæƒ³è±¡ã€‚</li>
<li>æ”¯æŒç©ºé—´è§†è§‰é—®ç­”ä»»åŠ¡ã€‚</li>
<li>æå‡ºçš„å‡ ä½•è¯­ä¹‰å­¦ä¹ ç­–ç•¥èƒ½åŒæ—¶æ•æ‰è¾“å…¥çš„è¯­ä¹‰å’Œå‡ ä½•çº¿ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-53878180e502265f60101005a4c7463c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90dbe43b7b2dd24b8873b1514f30998f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79841898019097c206e50e4dfef7ecea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d029567079e69d7dff1f3060482df9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6186fc02e27de2321531b1a63c4c1959.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5760e542f18adf2039ece9a45899914.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47368ec6b2d01432d73c85457272403a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation"><a href="#Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation" class="headerlink" title="Object Fidelity Diffusion for Remote Sensing Image Generation"></a>Object Fidelity Diffusion for Remote Sensing Image Generation</h2><p><strong>Authors:Ziqi Ye, Shuran Ma, Jie Yang, Xiaoyi Yang, Ziyang Gong, Xue Yang, Haipeng Wang</strong></p>
<p>High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity images due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a dual-branch diffusion model with diffusion consistency loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively. </p>
<blockquote>
<p>é«˜ç²¾åº¦å¯æ§é¥æ„Ÿå›¾åƒç”Ÿæˆæ—¢æœ‰æ„ä¹‰åˆå…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ç”±äºæ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œå¾€å¾€ä¼šäº§ç”Ÿä½ä¿çœŸå›¾åƒï¼Œè¿™å¯èƒ½ä¼šå½±å“ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†æé«˜é¥æ„Ÿä¸­ç”Ÿæˆç›®æ ‡çš„å‡†ç¡®æ€§å’Œä¿çœŸåº¦ï¼Œæœ¬æ–‡æå‡ºäº†Object Fidelity Diffusionï¼ˆOF-Diffï¼‰ï¼Œæœ‰æ•ˆæé«˜äº†ç”Ÿæˆç›®æ ‡çš„ä¿çœŸåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–æ¬¡åŸºäºé¥æ„Ÿçš„æ‰©æ•£æ¨¡å‹å¸ƒå±€æå–äº†ç›®æ ‡å…ˆéªŒå½¢çŠ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…·æœ‰æ‰©æ•£ä¸€è‡´æ€§æŸå¤±çš„åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨é‡‡æ ·é˜¶æ®µä¸æä¾›çœŸå®å›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸé¥æ„Ÿå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†DDPOæ¥å¾®è°ƒæ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”Ÿæˆçš„é¥æ„Ÿå›¾åƒæ›´åŠ å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸€è‡´ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒOF-Diffåœ¨é¥æ„Ÿé¢†åŸŸçš„å…³é”®è´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤šå½¢æ€å’Œå°ç›®æ ‡ç±»åˆ«çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚ä¾‹å¦‚ï¼Œé£æœºã€èˆ¹åªå’Œè½¦è¾†çš„mAPåˆ†åˆ«æé«˜äº†8.3%ã€7.7%å’Œ4.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10801v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºObject Fidelity Diffusionï¼ˆOF-Diffï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæé«˜é¥æ„Ÿå›¾åƒç”Ÿæˆçš„é«˜ç²¾åº¦å¯æ§æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å¯¹è±¡çš„å…ˆéªŒå½¢çŠ¶ï¼Œå¼•å…¥åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼Œæœ‰æ•ˆæé«˜äº†ç”Ÿæˆå¯¹è±¡çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†DDPOæ¥å¾®è°ƒæ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”Ÿæˆçš„é¥æ„Ÿå›¾åƒæ›´åŠ å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼ŒOF-Diffåœ¨é¥æ„Ÿé¢†åŸŸçš„å…³é”®è´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ€å’Œå°å¯¹è±¡ç±»åˆ«çš„æ€§èƒ½ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OF-Diffæ–¹æ³•æé«˜äº†é¥æ„Ÿå›¾åƒç”Ÿæˆçš„é«˜ç²¾åº¦å¯æ§æ€§ã€‚</li>
<li>é€šè¿‡æå–å¯¹è±¡çš„å…ˆéªŒå½¢çŠ¶ï¼Œæé«˜äº†ç”Ÿæˆå¯¹è±¡çš„è´¨é‡ã€‚</li>
<li>å¼•å…¥äº†åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼Œæœ‰æ•ˆæå‡äº†ç”Ÿæˆé¥æ„Ÿå›¾åƒçš„è´¨é‡ã€‚</li>
<li>DDPOçš„å¼•å…¥ä½¿å¾—ç”Ÿæˆçš„é¥æ„Ÿå›¾åƒæ›´åŠ å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸€è‡´ã€‚</li>
<li>OF-Diffåœ¨é¥æ„Ÿé¢†åŸŸçš„å…³é”®è´¨é‡æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨å¤šæ€å’Œå°å¯¹è±¡ç±»åˆ«çš„æ€§èƒ½ä¸Šï¼ŒOF-Diffæœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8d1dd579fa81d56e3f3ae04cdc5fe20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c46ee0a268f055c2e111cfcdfb3a991.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39cb4609b7d16005c74894c58d59a71e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0905b7432205cad3dcdf0d5ad31a87f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8dfe2f10a56c57a52b2f7f4a389702fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ced7713268a6539886c1f76a4d3bc35f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3473d387efc0effaab78bfe13dae996.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"><a href="#NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale" class="headerlink" title="NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale"></a>NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale</h2><p><strong>Authors: NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu</strong></p>
<p>Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community. </p>
<blockquote>
<p>å½“å‰æµè¡Œçš„ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ï¼Œè¦ä¹ˆä¾èµ–äºè®¡ç®—é‡å¤§ä¸”è€—æ—¶çš„æ‰©æ•£æ¨¡å‹æ¥å¤„ç†è¿ç»­çš„å›¾åƒæ ‡è®°ï¼Œè¦ä¹ˆé‡‡ç”¨çŸ¢é‡é‡åŒ–ï¼ˆVQï¼‰æŠ€æœ¯è·å¾—ç¦»æ•£æ ‡è®°ï¼Œä½†ä¼šäº§ç”Ÿé‡åŒ–æŸå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å€ŸåŠ©NextStep-1æ¨¡å‹æ¨åŠ¨è‡ªå›å½’æ¨¡å¼çš„å‘å±•ã€‚NextStep-1æ˜¯ä¸€ä¸ªå…·æœ‰å·¨å¤§è§„æ¨¡çš„è‡ªå›å½’æ¨¡å‹ï¼Œå…·æœ‰ä¸€ä¸ªå·¨å¤§çš„è¯æ±‡è¡¨å¤§å°ä¸ºåºå¤§çš„æ•°æ®é‡çº§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œé‡‡ç”¨äº†åˆ›æ–°çš„æ–‡æœ¬è¡¨ç¤ºæŠ€æœ¯è®­ç»ƒè¿™ä¸ªæ¨¡å‹ä»¥åŒ¹é…è¿ç»­çš„å›¾åƒæ ‡è®°ã€‚è¿™ä¸ªæ¨¡å‹åŸºäºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡æ¥é¢„æµ‹ä¸‹ä¸€æ¡æ ‡è®°æ¥æ‰§è¡Œä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨èåˆçš„æ–¹æ³•å’Œå·¨å¤§çš„è¯­æ–™åº“å±•ç¤ºæƒŠäººçš„æ•ˆæœï¼Œè¡¨æ˜å®ƒèƒ½å¤Ÿé«˜è´¨é‡ç”Ÿæˆå’Œä¿®å¤å›¾ç‰‡å¹¶é‡å¡‘å¤–è§‚æ¨¡å‹æˆ–æ¨¡å¼ï¼ˆæ ¹æ®å•è¯é…å¯¹æ ‡è®°çš„æ•°æ®é›†ï¼‰ã€‚NextStep-1åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†è‡ªå›å½’æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œåœ¨é«˜ä¿çœŸå›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºåŠ²çš„è¡¨ç°ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬ç»Ÿä¸€æ–¹æ³•çš„å¼ºå¤§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†æ–¹ä¾¿å…¬å¼€ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10711v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/NextStep-1">https://github.com/stepfun-ai/NextStep-1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NextStep-1æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆæ–‡æœ¬ç¦»æ•£ä»¤ç‰Œå’Œå›¾åƒè¿ç»­ä»¤ç‰Œè¿›è¡Œè®­ç»ƒçš„14Bå‚æ•°çš„è‡ªå›å½’æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡å®ç°äº†å…ˆè¿›çš„çŠ¶æ€æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„é«˜ä¿çœŸå›¾åƒåˆæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶ç»Ÿä¸€æ–¹æ³•çš„å¼ºå¤§å’Œå¤šåŠŸèƒ½æ€§ã€‚ç¤¾åŒºå°†å‘å¸ƒä»£ç å’Œæ¨¡å‹ä»¥ä¿ƒè¿›å¼€æ”¾ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NextStep-1æ¨¡å‹ç»“åˆæ–‡æœ¬ç¦»æ•£ä»¤ç‰Œå’Œå›¾åƒè¿ç»­ä»¤ç‰Œè¿›è¡Œè®­ç»ƒã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡ã€‚</li>
<li>NextStep-1åœ¨æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°å…ˆè¿›çš„çŠ¶æ€æ€§èƒ½ã€‚</li>
<li>NextStep-1æ¨¡å‹èƒ½å¤Ÿåˆæˆé«˜ä¿çœŸçš„å›¾åƒã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ç»Ÿä¸€æ–¹æ³•ä½¿NextStep-1å…·æœ‰å¼ºå¤§çš„å¤šåŠŸèƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4088e9301ad3aae25109384a4748e215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-256910c7f4da19f8e289c57cc5cff5cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a75ac11b738c063e4f11f8662c0efe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9064c3eae467753ffdaf42bdc1989e0c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation"><a href="#Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation" class="headerlink" title="Hybrid Generative Fusion for Efficient and Privacy-Preserving Face   Recognition Dataset Generation"></a>Hybrid Generative Fusion for Efficient and Privacy-Preserving Face   Recognition Dataset Generation</h2><p><strong>Authors:Feiran Li, Qianqian Xu, Shilong Bao, Boyu Han, Zhiyong Yang, Qingming Huang</strong></p>
<p>In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/datacv_fr">https://github.com/Ferry-Li/datacv_fr</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å‚åŠ DataCV ICCVæŒ‘æˆ˜èµ›çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„é‡ç‚¹åœ¨äºæ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„äººè„¸æ•°æ®é›†æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚æ„å»ºçš„æ•°æ®åº“å¿…é¡»ä¸å«æœ‰ä¸ä»»ä½•ç°æœ‰å…¬å…±äººè„¸æ•°æ®é›†é‡å çš„èº«ä»½ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹åŸºçº¿HSFaceæ•°æ®é›†è¿›è¡Œå…¨é¢æ¸…ç†ï¼Œé€šè¿‡æ··åˆä¸“å®¶ç­–ç•¥ï¼ˆMoEï¼‰ç»“åˆäººè„¸åµŒå…¥èšç±»å’ŒGPT-4oè¾…åŠ©éªŒè¯ï¼Œè¯†åˆ«å¹¶ç§»é™¤é”™è¯¯æ ‡è®°æˆ–ä¸ä¸€è‡´çš„èº«ä»½ä¿¡æ¯ã€‚æˆ‘ä»¬ä¿ç•™æœ€å¤§çš„èº«ä»½ä¸€è‡´æ€§é›†ç¾¤ï¼Œå¹¶å¯¹æ¯ä¸ªèº«ä»½è¿›è¡Œæ•°æ®å¢å¼ºè‡³å›ºå®šæ•°é‡çš„å›¾åƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨Stable Diffusionå’Œæç¤ºå·¥ç¨‹ç”Ÿæˆåˆæˆèº«ä»½ã€‚ç”±äºæ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œæˆ‘ä»¬æ¯ä¸ªèº«ä»½åªç”Ÿæˆä¸€ä¸ªå‚è€ƒå›¾åƒï¼Œå¹¶ä½¿ç”¨Vec2Faceæœ‰æ•ˆåœ°å°†å…¶æ‰©å±•ï¼Œè¿…é€Ÿç”Ÿæˆ49ä¸ªèº«ä»½ä¸€è‡´çš„å˜ä½“ã€‚è¿™ç§æ··åˆæ–¹æ³•èåˆäº†åŸºäºGANå’ŒåŸºäºæ‰©æ•£çš„æ ·æœ¬ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ„å»ºå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³åˆæˆèº«ä»½ä¹‹é—´çš„é«˜è§†è§‰ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå°†å®ƒä»¬å°½æ—©çº³å…¥è®­ç»ƒè®¡åˆ’ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æ˜“åˆ°éš¾é€æ¸é€‚åº”æ ·æœ¬ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ•°æ®é›†æ¯ä¸ªèº«ä»½åŒ…å«50å¼ å›¾åƒï¼Œæ‰€æœ‰æ–°ç”Ÿæˆçš„èº«ä»½éƒ½ç»è¿‡ä¸»æµäººè„¸æ•°æ®é›†çš„éªŒè¯ï¼Œä»¥ç¡®ä¿æ— èº«ä»½ä¿¡æ¯æ³„éœ²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”èµ›ä¸­è·å¾—äº†ç¬¬ä¸€åï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åœ¨é¢å‘è§„æ¨¡ä¸ºåä¸‡ã€äºŒåä¸‡å’Œç™¾ä¸‡çº§åˆ«çš„è¯†åˆ«æ—¶ï¼Œèƒ½å¤Ÿæå‡æ¨¡å‹æ€§èƒ½ã€‚ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/datacv_fr%E3%80%82">https://github.com/Ferry-Li/datacv_frã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10672v2">PDF</a> This paper has been accpeted to ICCV 2025 DataCV Workshop</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹DataCV ICCVæŒ‘æˆ˜èµ›ï¼Œæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„äººè„¸æ•°æ®é›†ç”¨äºè®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ¸…ç†åŸºå‡†HSFaceæ•°æ®é›†å¹¶é‡‡ç”¨åŸºäºäººè„¸åµŒå…¥èšç±»å’ŒGPT-4éªŒè¯çš„ä¸“å®¶ç­–ç•¥å»é™¤è¯¯æ ‡å’Œä¸ä¸€è‡´çš„èº«ä»½ä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨æ•°æ®å¢å¼ºæ‰©å¤§æ•°æ®è§„æ¨¡ã€‚ä¸ºå¢åŠ æ•°æ®é›†å¤šæ ·æ€§ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨Stable Diffusionç”Ÿæˆåˆæˆèº«ä»½å¹¶åˆ©ç”¨Vec2FaceæŠ€æœ¯å¿«é€Ÿç”Ÿæˆå¤šä¸ªä¸€è‡´æ€§çš„å˜ä½“å›¾åƒã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«æ¯ä¸ªèº«ä»½50å¼ å›¾åƒï¼Œæœ‰æ•ˆåº”å¯¹ç«èµ›ä¸­åˆæˆèº«ä»½çš„è§†è§‰ç›¸ä¼¼æ€§æŒ‘æˆ˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºå…¶æ„å»ºçš„è¯¥æ•°æ®é›†èƒ½åœ¨å¤šç§ä¸åŒè§„æ¨¡ï¼ˆä»ä¸‡è‡³ç™¾ä¸‡çº§èº«ä»½ï¼‰çš„æ•°æ®ä¸Šæé«˜æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸€æ–¹æ³•çš„æ’åä½å±…æŒ‘æˆ˜èµ›é¦–ä½^[è¯·æ³¨æ„ï¼Œè¿™ä¸ªæ€»ç»“çœç•¥äº†éƒ¨åˆ†æŠ€æœ¯ç»†èŠ‚ä»¥ä¿æŒç®€æ´]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿä¸ºè§£å†³DataCV ICCVæŒ‘æˆ˜èµ›è€Œæ„å»ºé«˜è´¨é‡äººè„¸æ•°æ®é›†ï¼Œæ—¨åœ¨è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ··åˆä¸“å®¶ç­–ç•¥æ¸…ç†HSFaceæ•°æ®é›†å¹¶æ¶ˆé™¤è¯¯æ ‡è®°å’Œä¸ä¸€è‡´èº«ä»½ä¿¡æ¯ã€‚</li>
<li>åˆ©ç”¨Stable Diffusionç”Ÿæˆåˆæˆèº«ä»½å¹¶åˆ©ç”¨Vec2FaceæŠ€æœ¯å®ç°é«˜æ•ˆå›¾åƒç”Ÿæˆã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¯ä¸ªèº«ä»½å›ºå®šæ•°é‡çš„å›¾åƒï¼Œç¡®ä¿æ•°æ®è§„æ¨¡ä¸€è‡´ä¸”å¤šæ ·ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥åº”å¯¹åˆæˆèº«ä»½è§†è§‰ç›¸ä¼¼æ€§æŒ‘æˆ˜ï¼Œä½¿æ¨¡å‹ä»ç®€å•æ ·æœ¬é€æ¸è¿‡æ¸¡åˆ°å¤æ‚æ ·æœ¬ã€‚</li>
<li>æœ€ç»ˆæ„å»ºçš„æ•°æ®é›†åœ¨å¤šç§ä¸åŒè§„æ¨¡çš„æ•°æ®ä¸Šå‡èƒ½æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73e60db80f3830c19f78d2d6a1185953.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d4629922931eb2cd3e764596778042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37386e5fb6c9d40cc94977fa6fb5ef09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34a73150dbcb1e05eb1e9609d7bc21eb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models"><a href="#Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Translation of Text Embedding via Delta Vector to Suppress Strongly   Entangled Content in Text-to-Image Diffusion Models"></a>Translation of Text Embedding via Delta Vector to Suppress Strongly   Entangled Content in Text-to-Image Diffusion Models</h2><p><strong>Authors:Eunseo Koh, Seunghoo Hong, Tae-Young Kim, Simon S. Woo, Jae-Pil Heo</strong></p>
<p>Text-to-Image (T2I) diffusion models have made significant progress in generating diverse high-quality images from textual prompts. However, these models still face challenges in suppressing content that is strongly entangled with specific words. For example, when generating an image of â€œCharlie Chaplinâ€, a â€œmustacheâ€ consistently appears even if explicitly instructed not to include it, as the concept of â€œmustacheâ€ is strongly entangled with â€œCharlie Chaplinâ€. To address this issue, we propose a novel approach to directly suppress such entangled content within the text embedding space of diffusion models. Our method introduces a delta vector that modifies the text embedding to weaken the influence of undesired content in the generated image, and we further demonstrate that this delta vector can be easily obtained through a zero-shot approach. Furthermore, we propose a Selective Suppression with Delta Vector (SSDV) method to adapt delta vector into the cross-attention mechanism, enabling more effective suppression of unwanted content in regions where it would otherwise be generated. Additionally, we enabled more precise suppression in personalized T2I models by optimizing delta vector, which previous baselines were unable to achieve. Extensive experimental results demonstrate that our approach significantly outperforms existing methods, both in terms of quantitative and qualitative metrics. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¤šæ ·åŒ–é«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æŠ‘åˆ¶ä¸ç‰¹å®šå•è¯å¼ºçƒˆçº ç¼ çš„å†…å®¹æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆâ€œå“åˆ«æ—â€çš„å›¾åƒæ—¶ï¼Œå³ä½¿æ˜ç¡®æŒ‡ç¤ºä¸è¦åŒ…æ‹¬â€œèƒ¡å­â€ï¼Œä½†â€œèƒ¡å­â€å§‹ç»ˆä¼šå‡ºç°ï¼Œå› ä¸ºâ€œèƒ¡å­â€çš„æ¦‚å¿µä¸â€œå“åˆ«æ—â€ç´§å¯†ç›¸è¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´æ¥åœ¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­æŠ‘åˆ¶è¿™ç§çº ç¼ å†…å®¹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ª delta å‘é‡ï¼Œè¯¥å‘é‡å¯ä»¥ä¿®æ”¹æ–‡æœ¬åµŒå…¥ï¼Œä»¥å‡å¼±ç”Ÿæˆå›¾åƒä¸­ä¸éœ€è¦å†…å®¹çš„å½±å“ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜å¯ä»¥é€šè¿‡é›¶æ ·æœ¬æ–¹æ³•è½»æ¾è·å¾—è¿™ä¸ª delta å‘é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰ Delta å‘é‡çš„é€‰æ‹©æ€§æŠ‘åˆ¶ï¼ˆSSDVï¼‰æ–¹æ³•ï¼Œå°† delta å‘é‡é€‚åº”åˆ°äº¤å‰æ³¨æ„æœºåˆ¶ä¸­ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶é‚£äº›ä¼šåœ¨åŸæœ¬ç”ŸæˆåŒºåŸŸå‡ºç°çš„ä¸æƒ³è¦çš„å†…å®¹ã€‚é€šè¿‡ä¼˜åŒ– delta å‘é‡ï¼Œæˆ‘ä»¬è¿˜åœ¨ä¸ªæ€§åŒ–T2Iæ¨¡å‹ä¸­å®ç°äº†æ›´ç²¾ç¡®çš„æŠ‘åˆ¶ï¼Œè¿™æ˜¯ä»¥å‰åŸºçº¿æ— æ³•è¾¾åˆ°çš„ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— è®ºåœ¨å®šé‡è¿˜æ˜¯å®šæ€§æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10407v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå³éš¾ä»¥æŠ‘åˆ¶ä¸ç‰¹å®šè¯è¯­å¼ºçƒˆçº ç¼ çš„å†…å®¹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­ç›´æ¥æŠ‘åˆ¶çº ç¼ å†…å®¹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ delta å‘é‡æ¥ä¿®æ”¹æ–‡æœ¬åµŒå…¥ï¼Œå‡å¼±ç”Ÿæˆå›¾åƒä¸­ä¸éœ€è¦å†…å®¹çš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡é›¶æ ·æœ¬æ–¹æ³•è½»æ¾è·å–è¯¥ delta å‘é‡çš„å¯èƒ½æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¸¦æœ‰ Delta å‘é‡çš„é€‰æ‹©æ€§æŠ‘åˆ¶ï¼ˆSSDVï¼‰æ–¹æ³•ï¼Œå°†å…¶é€‚åº”äºäº¤å‰æ³¨æ„æœºåˆ¶ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶äº†ä¸éœ€è¦çš„å†…å®¹åœ¨ç”Ÿæˆå›¾åƒä¸­çš„åŒºåŸŸã€‚é€šè¿‡ä¼˜åŒ– delta å‘é‡ï¼Œè¿˜å®ç°äº†ä¸ªæ€§åŒ–T2Iæ¨¡å‹çš„æ›´ç²¾ç¡®æŠ‘åˆ¶ï¼Œè¿™æ˜¯ä»¥å‰åŸºçº¿æ–¹æ³•æ— æ³•å®ç°çš„ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†ä»é¢ä¸´æŠ‘åˆ¶ä¸ç‰¹å®šè¯è¯­çº ç¼ å†…å®¹çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥deltaå‘é‡ä¿®æ”¹æ–‡æœ¬åµŒå…¥ï¼Œå‡å¼±ä¸éœ€è¦å†…å®¹åœ¨ç”Ÿæˆå›¾åƒä¸­çš„å½±å“ã€‚</li>
<li>å±•ç¤ºé€šè¿‡é›¶æ ·æœ¬æ–¹æ³•è·å–deltaå‘é‡çš„å¯èƒ½æ€§ã€‚</li>
<li>æå‡ºSSDVæ–¹æ³•ï¼Œé€‚åº”äºäº¤å‰æ³¨æ„æœºåˆ¶ï¼Œæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶ä¸éœ€è¦çš„å†…å®¹åœ¨å›¾åƒç”Ÿæˆä¸­çš„ç‰¹å®šåŒºåŸŸã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–deltaå‘é‡ï¼Œå®ç°ä¸ªæ€§åŒ–T2Iæ¨¡å‹çš„æ›´ç²¾ç¡®æŠ‘åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¡¨ç°åœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³T2Iæ‰©æ•£æ¨¡å‹ä¸­æŠ‘åˆ¶ç‰¹å®šå†…å®¹çš„é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æ–°é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8161e18e93cc54b4a5c45c627185b61a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74d9ea769c7ea7657083f2ff56f00cfe.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Local-Prompt-Adaptation-for-Style-Consistent-Multi-Object-Generation-in-Diffusion-Models"><a href="#Local-Prompt-Adaptation-for-Style-Consistent-Multi-Object-Generation-in-Diffusion-Models" class="headerlink" title="Local Prompt Adaptation for Style-Consistent Multi-Object Generation in   Diffusion Models"></a>Local Prompt Adaptation for Style-Consistent Multi-Object Generation in   Diffusion Models</h2><p><strong>Authors:Ankit Sanjyal</strong></p>
<p>Diffusion models have become a powerful backbone for text-to-image generation, producing high-quality visuals from natural language prompts. However, when prompts involve multiple objects alongside global or local style instructions, the outputs often drift in style and lose spatial coherence, limiting their reliability for controlled, style-consistent scene generation. We present Local Prompt Adaptation (LPA), a lightweight, training-free method that splits the prompt into content and style tokens, then injects them selectively into the U-Netâ€™s attention layers at chosen timesteps. By conditioning object tokens early and style tokens later in the denoising process, LPA improves both layout control and stylistic uniformity without additional training cost. We conduct extensive ablations across parser settings and injection windows, finding that the best configuration â€“ lpa late only with a 300-650 step window â€“ delivers the strongest balance of prompt alignment and style consistency. On the T2I benchmark, LPA improves CLIP-prompt alignment over vanilla SDXL by +0.41% and over SD1.5 by +0.34%, with no diversity loss. On our custom 50-prompt style-rich benchmark, LPA achieves +0.09% CLIP-prompt and +0.08% CLIP-style gains over baseline. Our method is model-agnostic, easy to integrate, and requires only a single configuration change, making it a practical choice for controllable, style-consistent multi-object generation. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æœ‰åŠ›åç›¾ï¼Œèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æç¤ºä¸­äº§ç”Ÿé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œå½“æç¤ºæ¶‰åŠå¤šä¸ªå¯¹è±¡ä»¥åŠå…¨å±€æˆ–å±€éƒ¨é£æ ¼æŒ‡ä»¤æ—¶ï¼Œè¾“å‡ºå¾€å¾€åœ¨é£æ ¼ä¸Šæ¼‚ç§»ï¼Œå¹¶å¤±å»ç©ºé—´è¿è´¯æ€§ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨å¯æ§ã€é£æ ¼ä¸€è‡´çš„åœºæ™¯ç”Ÿæˆä¸­çš„å¯é æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æœ¬åœ°æç¤ºé€‚åº”ï¼ˆLocal Prompt Adaptationï¼Œç®€ç§°LPAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå®ƒå°†æç¤ºåˆ†ä¸ºå†…å®¹ä»¤ç‰Œå’Œé£æ ¼ä»¤ç‰Œï¼Œç„¶åæœ‰é€‰æ‹©æ€§åœ°å°†å…¶æ³¨å…¥U-Netçš„æ³¨æ„åŠ›å±‚åœ¨é€‰æ‹©çš„æ—¶åºç‚¹ä¸Šã€‚é€šè¿‡æ—©æœŸè®¾å®šå¯¹è±¡ä»¤ç‰Œå¹¶åœ¨å»å™ªè¿‡ç¨‹ä¸­ç¨åè®¾å®šé£æ ¼ä»¤ç‰Œï¼ŒLPAæ”¹è¿›äº†å¸ƒå±€æ§åˆ¶å’Œé£æ ¼ä¸€è‡´æ€§ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„è§£æå™¨è®¾ç½®å’Œæ³¨å…¥çª—å£çš„å‰¥ç¦»å®éªŒï¼Œå‘ç°æœ€ä½³é…ç½®â€”â€”ä»…åœ¨300-650æ­¥çª—å£åæœŸä½¿ç”¨lpaâ€”â€”å®ç°äº†æç¤ºå¯¹é½å’Œé£æ ¼ä¸€è‡´æ€§ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚åœ¨T2IåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLPAé€šè¿‡æ”¹è¿›CLIPæç¤ºå¯¹é½ï¼Œåœ¨vanilla SDXLä¸Šæé«˜äº†+0.41%ï¼Œåœ¨SD1.5ä¸Šæé«˜äº†+0.34%ï¼Œæ²¡æœ‰æŸå¤±å¤šæ ·æ€§ã€‚åœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰50ä¸ªæç¤ºé£æ ¼ä¸°å¯Œçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLPAç›¸å¯¹äºåŸºçº¿åœ¨CLIPæç¤ºå’ŒCLIPé£æ ¼ä¸Šåˆ†åˆ«å®ç°äº†+0.09%å’Œ+0.08%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ¨¡å‹æ— å…³çš„ï¼Œæ˜“äºé›†æˆï¼Œåªéœ€è¦è¿›è¡Œä¸€æ¬¡é…ç½®æ›´æ”¹ï¼Œä½¿å…¶æˆä¸ºå¯æ§ã€é£æ ¼ä¸€è‡´çš„å¤šå¯¹è±¡ç”Ÿæˆçš„å®ç”¨é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20094v2">PDF</a> 10 Pages,10 figures, pre-print</p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬æå‡ºäº†å±€éƒ¨æç¤ºé€‚åº”ï¼ˆLPAï¼‰æ–¹æ³•ï¼Œç”¨äºæ”¹å–„æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ—¶çš„é£æ ¼ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•å°†æç¤ºåˆ†ä¸ºå†…å®¹é£æ ¼ä»¤ç‰Œï¼Œé€‰æ‹©æ€§æ³¨å…¥U-Netæ³¨æ„åŠ›å±‚ï¼Œé€šè¿‡æ—©æœŸå¤„ç†å¯¹è±¡ä»¤ç‰Œå’ŒåæœŸå¤„ç†é£æ ¼ä»¤ç‰Œï¼Œæé«˜äº†å¸ƒå±€æ§åˆ¶å’Œé£æ ¼ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€ä½³é…ç½®ä¸‹çš„LPAåœ¨æç¤ºå¯¹é½å’Œé£æ ¼ä¸€è‡´æ€§æ–¹é¢è¡¨ç°æœ€ä½³ã€‚åœ¨T2IåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLPAæé«˜äº†CLIPæç¤ºå¯¹é½çš„å‡†ç¡®ç‡ï¼Œä¸”ä¸ä¼šæŸå¤±å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ã€æ˜“äºé›†æˆã€ä»…éœ€å•ä¸€é…ç½®æ›´æ”¹ç­‰ç‰¹ç‚¹ï¼Œæ˜¯å¯æ§ã€é£æ ¼ä¸€è‡´çš„å¤šå¯¹è±¡ç”Ÿæˆçš„å®é™…é€‰æ‹©ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æœ‰åŠ›æ”¯æŒï¼Œä½†å¤„ç†æ¶‰åŠå¤šä¸ªå¯¹è±¡å’Œå…¨å±€æˆ–å±€éƒ¨é£æ ¼æŒ‡ä»¤çš„æç¤ºæ—¶ï¼Œè¾“å‡ºå¾€å¾€åœ¨é£æ ¼ä¸Šæ¼‚ç§»ï¼Œå¤±å»ç©ºé—´è¿è´¯æ€§ã€‚</li>
<li>å±€éƒ¨æç¤ºé€‚åº”ï¼ˆLPAï¼‰æ˜¯ä¸€ç§è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡å°†æç¤ºåˆ†ä¸ºå†…å®¹å’Œé£æ ¼ä»¤ç‰Œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LPAé€šè¿‡é€‰æ‹©æ€§åœ°å°†ä»¤ç‰Œæ³¨å…¥U-Netçš„æ³¨æ„åŠ›å±‚ï¼Œåœ¨é™å™ªè¿‡ç¨‹ä¸­æ—©æœŸå¤„ç†å¯¹è±¡ä»¤ç‰Œï¼ŒåæœŸå¤„ç†é£æ ¼ä»¤ç‰Œï¼Œæ”¹å–„äº†å¸ƒå±€æ§åˆ¶å’Œé£æ ¼ä¸€è‡´æ€§ã€‚</li>
<li>æœ€ä½³é…ç½®ä¸‹çš„LPAåœ¨æç¤ºå¯¹é½å’Œé£æ ¼ä¸€è‡´æ€§æ–¹é¢è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLPAæé«˜äº†CLIPæç¤ºå¯¹é½çš„å‡†ç¡®ç‡ï¼Œä¸”ä¸ä¼šæŸå¤±å¤šæ ·æ€§ã€‚</li>
<li>LPAå…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ˜“äºé›†æˆï¼Œåªéœ€å•ä¸€é…ç½®æ›´æ”¹ã€‚</li>
<li>LPAä¸ºå¯æ§ã€é£æ ¼ä¸€è‡´çš„å¤šå¯¹è±¡ç”Ÿæˆæä¾›äº†å®ç”¨çš„é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d362c1b18b0baad3f98638348a6f95fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1701a8c8e98036b8d29b037d84b58bd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e15eb015ba68e9f9ca18202b2cdbbdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec3cb64386371c1ec2541f93b00c1121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3118bc6edc59935c47c950c5041bb8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-855ba259707251ca0c4dcd8f1e312e65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb7e703fbfdd151003a5d0833192ee68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0e34141371f0290a9bef77e9473686c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions"><a href="#Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions" class="headerlink" title="Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions"></a>Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions</h2><p><strong>Authors:Chang-Hwan Son</strong></p>
<p>With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios. </p>
<blockquote>
<p>éšç€æ™ºèƒ½CCTVç³»ç»Ÿåœ¨æˆ·å¤–ç¯å¢ƒä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå¯¹äºé€‚åº”æ¶åŠ£å¤©æ°”æ¡ä»¶çš„é¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚æ¶åŠ£å¤©æ°”ä¼šæ˜¾è‘—é™ä½å›¾åƒè´¨é‡ï¼Œè¿›è€Œå¯¼è‡´è¯†åˆ«ç²¾åº¦ä¸‹é™ã€‚è™½ç„¶åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨è§£å†³å¤©æ°”é€€åŒ–é—®é¢˜çš„æ¨¡å—ï¼Œå…¶æ€§èƒ½ä»ç„¶å—åˆ°é™åˆ¶ã€‚è¿™å¯¼è‡´äº†é¢éƒ¨çº¹ç†å’Œç»“æ„å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„ç›²FIRæ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé€€åŒ–æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚å±€éƒ¨SFFTæ¨¡å—é€šè¿‡ä½¿ä½è´¨é‡ï¼ˆLQï¼‰é¢éƒ¨åŒºåŸŸçš„å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒä¸é«˜è´¨é‡ï¼ˆHQï¼‰å¯¹åº”åŒºåŸŸçš„ç»Ÿè®¡åˆ†å¸ƒå¯¹é½ï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œé¢œè‰²ä¿çœŸåº¦ã€‚ä½œä¸ºè¡¥å……ï¼ŒDAFEæ¨¡å—é€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨è¡¨ç¤ºï¼Œå®ç°åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥ç»Ÿè®¡é¢éƒ¨ç‰¹å¾æå–ï¼Œä½¿æ¢å¤è¿‡ç¨‹èƒ½å¤Ÿé€‚åº”ç”±æ¶åŠ£å¤©æ°”å¼•èµ·çš„é€€åŒ–é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é€€åŒ–æ— å…³SFFTæ¨¡å‹åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œå‡†ç¡®é‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„å…ˆè¿›FIRæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSFFTå’ŒDAFEæ¨¡å—åœ¨å¢å¼ºç»“æ„ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å¾—åˆ°äº†éªŒè¯ï¼Œåœ¨æ¶åŠ£å¤©æ°”æƒ…å†µä¸‹éƒ½èƒ½å®ç°é¢éƒ¨æ¢å¤çš„ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07464v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹æˆ·å¤–ç¯å¢ƒä¸­æ™ºèƒ½ç›‘æ§ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹é¢éƒ¨è¯†åˆ«ç³»ç»Ÿåœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ä¼˜åŒ–éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚æ¶åŠ£å¤©æ°”ä¼šæ˜¾è‘—é™å›¾åƒè´¨é‡ï¼Œè¿›è€Œå½±å“è¯†åˆ«å‡†ç¡®åº¦ã€‚è™½ç„¶åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹å·²æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨åº”å¯¹å¤©æ°”å¼•èµ·çš„é™è´¨çš„æ¨¡å—ï¼Œå…¶æ€§èƒ½ä»ç„¶æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§åŸºäºGANçš„ç›²FIRæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé™è´¨æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚SFFTæ¨¡å—é€šè¿‡å¯¹é½ä½è´¨é‡ï¼ˆLQï¼‰é¢éƒ¨åŒºåŸŸä¸é«˜è´¨é‡ï¼ˆHQï¼‰åŒºåŸŸçš„å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œè‰²å½©ä¿çœŸåº¦ã€‚DAFEæ¨¡å—åˆ™é€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨çš„è¡¨ç¤ºï¼Œä½¿é¢éƒ¨ç‰¹å¾æå–åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹æ›´åŠ ç¨³å¥ï¼Œä»è€Œä½¿æ¢å¤è¿‡ç¨‹é€‚åº”ä¸¥é‡çš„å¤©æ°”å¼•èµ·çš„é™è´¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é™è´¨æ— å…³SFFTæ¨¡å‹åœ¨GANå’Œæ‰©æ•£æ¨¡å‹çš„åŸºç¡€ä¸Šçš„é¢éƒ¨å›¾åƒæ¢å¤æ–¹æ³•ä¸­è¡¨ç°æ›´ä½³ï¼Œå°¤å…¶åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œå‡†ç¡®é‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢ã€‚SFFTå’ŒDAFEæ¨¡å—åœ¨æŒ‘æˆ˜å¤©æ°”æƒ…æ™¯ä¸‹çš„é¢éƒ¨æ¢å¤ä¸­ï¼Œéƒ½ç»è¿‡å®è¯å¢å¼ºäº†ç»“æ„ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¶åŠ£å¤©æ°”å¯¹é¢éƒ¨è¯†åˆ«ç³»ç»Ÿé€ æˆæŒ‘æˆ˜ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šå¤©æ°”æ¡ä»¶çš„ä¼˜åŒ–ã€‚</li>
<li>åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„ç°æœ‰é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹åœ¨åº”å¯¹å¤©æ°”å¼•èµ·çš„å›¾åƒé™è´¨æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„GAN-basedç›²FIRæ¡†æ¶ï¼ŒåŒ…å«å±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé™è´¨æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>SFFTæ¨¡å—é€šè¿‡å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒå¯¹é½å¢å¼ºé¢éƒ¨ç»“æ„å’Œè‰²å½©ä¿çœŸåº¦ã€‚</li>
<li>DAFEæ¨¡å—ä½¿é¢éƒ¨ç‰¹å¾æå–åœ¨æ¶åŠ£å¤©æ°”ä¸‹æ›´åŠ ç¨³å¥ï¼Œé€‚åº”å„ç§å¤©æ°”å¼•èµ·çš„é™è´¨ã€‚</li>
<li>æå‡ºçš„é™è´¨æ— å…³SFFTæ¨¡å‹åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œé‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bcdb5d18af53116b2a34b95248e93c21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91092a9af3896afea3319e34c02c2280.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning"></a>LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware   LoRA Fine-Tuning</h2><p><strong>Authors:Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, Tianfan Xue</strong></p>
<p>Video editing using diffusion models has achieved remarkable results in generating high-quality edits for videos. However, current methods often rely on large-scale pretraining, limiting flexibility for specific edits. First-frame-guided editing provides control over the first frame, but lacks flexibility over subsequent frames. To address this, we propose a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models for flexible video editing. Our key innovation is using a spatiotemporal mask to strategically guide the LoRA fine-tuning process. This teaches the model two distinct skills: first, to interpret the mask as a command to either preserve content from the source video or generate new content in designated regions. Second, for these generated regions, LoRA learns to synthesize either temporally consistent motion inherited from the video or novel appearances guided by user-provided reference frames. This dual-capability LoRA grants users control over the editâ€™s entire temporal evolution, allowing complex transformations like an object rotating or a flower blooming. Experimental results show our method achieves superior video editing performance compared to baseline methods. Project Page: <a target="_blank" rel="noopener" href="https://cjeen.github.io/LoRAEdit">https://cjeen.github.io/LoRAEdit</a> </p>
<blockquote>
<p>ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘ç¼–è¾‘å·²ç»åœ¨ä¸ºé«˜è´¨é‡è§†é¢‘ç”Ÿæˆç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚è™½ç„¶ç¬¬ä¸€å¸§å¼•å¯¼ç¼–è¾‘å¯ä»¥æ§åˆ¶ç¬¬ä¸€å¸§ï¼Œä½†å¯¹äºåç»­å¸§çš„çµæ´»æ€§ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ©è†œçš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€‚åº”é¢„è®­ç»ƒçš„å›¾åˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œç”¨äºçµæ´»è§†é¢‘ç¼–è¾‘ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºä½¿ç”¨æ—¶ç©ºæ©è†œæ¥æˆ˜ç•¥æ€§åœ°å¼•å¯¼LoRAå¾®è°ƒè¿‡ç¨‹ã€‚è¿™æ•™ä¼šäº†æ¨¡å‹ä¸¤ç§æˆªç„¶ä¸åŒçš„æŠ€èƒ½ï¼šé¦–å…ˆï¼Œå°†æ©è†œè§£é‡Šä¸ºæ¥è‡ªæºè§†é¢‘çš„æŒ‡ä»¤ï¼Œè¦ä¹ˆä¿ç•™å†…å®¹ï¼Œè¦ä¹ˆåœ¨æŒ‡å®šåŒºåŸŸç”Ÿæˆæ–°å†…å®¹ã€‚å…¶æ¬¡ï¼Œå¯¹äºè¿™äº›ç”Ÿæˆçš„åŒºåŸŸï¼ŒLoRAå­¦ä¹ åˆæˆä»è§†é¢‘ä¸­ç»§æ‰¿çš„æ—¶é—´è¿è´¯è¿åŠ¨æˆ–æ ¹æ®ç”¨æˆ·æä¾›çš„å‚è€ƒå¸§å¼•å¯¼çš„æ–°å¤–è§‚ã€‚è¿™ç§åŒåŠŸèƒ½çš„LoRAä½¿ç”¨æˆ·èƒ½å¤Ÿæ§åˆ¶æ•´ä¸ªæ—¶é—´è½´çš„ç¼–è¾‘æ¼”å˜ï¼Œä»è€Œå®ç°å¤æ‚çš„è½¬æ¢ï¼Œå¦‚ç‰©ä½“æ—‹è½¬æˆ–èŠ±æœµç»½æ”¾ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•åœ¨è§†é¢‘ç¼–è¾‘æ€§èƒ½ä¸Šæ›´èƒœä¸€ç­¹ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cjeen.github.io/LoRAEdit">https://cjeen.github.io/LoRAEdit</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10082v4">PDF</a> 9 pages</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç¼–è¾‘é¢†åŸŸå·²å–å¾—äº†æ˜¾è‘—æˆæœï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘ã€‚ä½†ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå¯¹äºç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§æœ‰é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ©è†œçš„LoRAï¼ˆä½ç§©é€‚åº”ï¼‰è°ƒä¼˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨äºè‡ªé€‚åº”åœ°è°ƒæ•´é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ¨¡å‹ï¼Œä»¥å®ç°çµæ´»çš„è§†é¢‘ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºä½¿ç”¨æ—¶ç©ºæ©è†œæ¥æŒ‡å¯¼LoRAå¾®è°ƒè¿‡ç¨‹ï¼Œèµ‹äºˆç”¨æˆ·æ§åˆ¶æ•´ä¸ªæ—¶é—´æ¼”åŒ–çš„èƒ½åŠ›ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å‡ºè‰²çš„è§†é¢‘ç¼–è¾‘æ€§èƒ½ã€‚è¯¦æƒ…è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cjeen.github.io/LoRAEdit">https://cjeen.github.io/LoRAEdit</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç¼–è¾‘ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡ç¼–è¾‘ã€‚</li>
<li>å½“å‰è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¾èµ–å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç¼ºä¹ç‰¹å®šç¼–è¾‘çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºåŸºäºæ©è†œçš„LoRAè°ƒä¼˜æ–¹æ³•ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨æ—¶ç©ºæ©è†œæŒ‡å¯¼LoRAå¾®è°ƒè¿‡ç¨‹ï¼Œå®ç°è§†é¢‘ç¼–è¾‘çš„çµæ´»æ§åˆ¶ã€‚</li>
<li>LoRAæŠ€æœ¯å¯ä»¥åˆæˆä¸è§†é¢‘ä¸€è‡´çš„è¿ç»­è¿åŠ¨æˆ–ç”¨æˆ·å‚è€ƒå¸§å¼•å¯¼çš„æ–°å¤–è§‚ã€‚</li>
<li>åŒåŠŸèƒ½LoRAä½¿ç”¨æˆ·èƒ½å¤Ÿæ§åˆ¶æ•´ä¸ªæ—¶é—´æ¼”åŒ–ï¼Œå®ç°å¤æ‚è½¬æ¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a80f6278b8da8625d274b4d1624c177b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b523bea842cbe6864369eb70527919c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afc1b58b28c47fc05816d61943753412.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbd38fa37eb7e2f5814e8a2fceab919c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52615007f314127bbbab7a1f524cf042.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e694cbae35f3db72a7c567e0dbd23157.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="D-CODA-Diffusion-for-Coordinated-Dual-Arm-Data-Augmentation"><a href="#D-CODA-Diffusion-for-Coordinated-Dual-Arm-Data-Augmentation" class="headerlink" title="D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation"></a>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</h2><p><strong>Authors:I-Chun Arthur Liu, Jason Chen, Gaurav Sukhatme, Daniel Seita</strong></p>
<p>Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our project website is at: <a target="_blank" rel="noopener" href="https://dcodaaug.github.io/D-CODA/">https://dcodaaug.github.io/D-CODA/</a>. </p>
<blockquote>
<p>å­¦ä¹ åŒæ‰‹åè°ƒæ“ä½œæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒå…·æœ‰é«˜ç»´åº¦å’Œéœ€è¦ä¸¤åªæ‰‹è‡‚ç´§å¯†åè°ƒçš„ç‰¹ç‚¹ã€‚çœ¼åœ¨æ‰‹ä¸Šçš„æ¨¡ä»¿å­¦ä¹ ä½¿ç”¨æ‰‹è…•å®‰è£…çš„ç›¸æœºï¼Œé€šè¿‡ä¸“æ³¨äºä»»åŠ¡ç›¸å…³è§†è§’æ¥ç®€åŒ–æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œæ”¶é›†å„ç§ç¤ºèŒƒä»ç„¶æˆæœ¬é«˜æ˜‚ï¼Œè¿™æ¿€å‘äº†å¯¹å¯æ‰©å±•æ•°æ®å¢å¼ºçš„éœ€æ±‚ã€‚è™½ç„¶ä»¥å‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†å•è‡‚è®¾ç½®ä¸­çš„è§†è§‰å¢å¼ºï¼Œä½†å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°åŒæ‰‹æ“ä½œéœ€è¦ç”Ÿæˆä¸¤åªæ‰‹è‡‚è§†è§’ä¸€è‡´çš„è§‚å¯Ÿç»“æœï¼Œå¹¶äº§ç”Ÿæ—¢æœ‰æ•ˆåˆå¯è¡Œçš„ç›¸åº”åŠ¨ä½œæ ‡ç­¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºçœ¼åœ¨æ‰‹ä¸Šçš„åŒæ‰‹æ¨¡ä»¿å­¦ä¹ çš„æ•°æ®å¢å¼ºæ–¹æ³•â€”â€”æ‰©æ•£åè°ƒåŒè‡‚æ•°æ®å¢å¼ºï¼ˆD-CODAï¼‰ã€‚D-CODAæ˜¯ä¸€ç§ç¦»çº¿æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹åˆæˆæ–°é¢–ã€è§†è§’ä¸€è‡´çš„æ‰‹è…•ç›¸æœºå›¾åƒï¼ŒåŒæ—¶ç”Ÿæˆå…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ã€‚å®ƒé‡‡ç”¨çº¦æŸä¼˜åŒ–ï¼Œç¡®ä¿å¢å¼ºçŠ¶æ€æ¶‰åŠå¤¹æŒå™¨ä¸ç‰©ä½“çš„æ¥è§¦ç¬¦åˆåŒæ‰‹åè°ƒçš„çº¦æŸã€‚æˆ‘ä»¬åœ¨5ä¸ªæ¨¡æ‹Ÿä»»åŠ¡å’Œ3ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šè¯„ä¼°äº†D-CODAã€‚åœ¨2250æ¬¡æ¨¡æ‹Ÿè¯•éªŒå’Œ300æ¬¡çœŸå®è¯•éªŒçš„ç»“æœè¡¨æ˜ï¼Œå®ƒä¼˜äºåŸºå‡†æ–¹æ³•å’Œæ¶ˆèå®éªŒï¼Œæ˜¾ç¤ºå‡ºåœ¨çœ¼åœ¨æ‰‹ä¸Šçš„åŒæ‰‹æ“ä½œä¸­è¿›è¡Œå¯æ‰©å±•æ•°æ®å¢å¼ºçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://dcodaaug.github.io/D-CODA/%E3%80%82">https://dcodaaug.github.io/D-CODA/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04860v2">PDF</a> Accepted to the Conference on Robot Learning (CoRL) 2025</p>
<p><strong>Summary</strong><br>    çœ¼æ‰‹åè°ƒçš„åŒæ‰‹æ“ä½œå­¦ä¹ é¢ä¸´é«˜ç»´åº¦å’ŒåŒè‡‚ç´§å¯†åè°ƒçš„æŒ‘æˆ˜ã€‚é‡‡ç”¨æ‰‹è…•å®‰è£…çš„ç›¸æœºè¿›è¡Œè§†è§‰æ¨¡ä»¿å­¦ä¹ ç®€åŒ–äº†ä»»åŠ¡ç›¸å…³è§†è§’çš„æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œæ”¶é›†å¤šæ ·çš„æ¼”ç¤ºæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¯æ‰©å±•çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹çœ¼æ‰‹åè°ƒåŒæ‰‹æ“ä½œçš„ç¦»çº¿æ•°æ®å¢å¼ºæ–¹æ³•â€”â€”æ‰©æ•£åè°ƒåŒè‡‚æ•°æ®å¢å¼ºï¼ˆD-CODAï¼‰ã€‚è¯¥æ–¹æ³•è®­ç»ƒæ‰©æ•£æ¨¡å‹åˆæˆæ–°é¢–ã€è§†è§’ä¸€è‡´çš„æ‰‹è…•ç›¸æœºå›¾åƒï¼ŒåŒæ—¶ç”Ÿæˆå…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ã€‚é€šè¿‡çº¦æŸä¼˜åŒ–ç¡®ä¿å¢å¼ºçš„æ¶‰åŠå¤¹æŒç‰©ä½“æ¥è§¦çš„çŠ¶æ€ç¬¦åˆåŒæ‰‹åè°ƒçš„çº¦æŸæ¡ä»¶ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒD-CODAä¼˜äºåŸºå‡†æ–¹æ³•å’ŒåºŸé™¤æ–¹æ¡ˆï¼Œå±•ç°å‡ºåœ¨çœ¼æ‰‹åè°ƒåŒæ‰‹æ“ä½œä¸­çš„å¯æ‰©å±•æ•°æ®å¢å¼ºçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ åŒæ‰‹åŠ¨æ“ä½œå› é«˜ç»´åº¦å’Œç´§å¯†åè°ƒè¦æ±‚è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>è§†è§‰æ¨¡ä»¿å­¦ä¹ ä¸­ï¼Œæ‰‹è…•å®‰è£…çš„ç›¸æœºç®€åŒ–äº†ä»»åŠ¡ç›¸å…³è§†è§’çš„æ„ŸçŸ¥ã€‚</li>
<li>æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¯æ‰©å±•çš„æ•°æ®å¢å¼ºæŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>D-CODAæ–¹æ³•é€šè¿‡è®­ç»ƒæ‰©æ•£æ¨¡å‹åˆæˆæ–°é¢–çš„æ‰‹è…•ç›¸æœºå›¾åƒï¼ŒåŒæ—¶ç”Ÿæˆå…³èŠ‚ç©ºé—´åŠ¨ä½œæ ‡ç­¾ã€‚</li>
<li>D-CODAé‡‡ç”¨çº¦æŸä¼˜åŒ–ç¡®ä¿å¢å¼ºçš„åŠ¨ä½œçŠ¶æ€ç¬¦åˆåŒæ‰‹åè°ƒçš„å®é™…çº¦æŸã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºD-CODAä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>D-CODAå±•ç°å‡ºåœ¨çœ¼æ‰‹åè°ƒåŒæ‰‹æ“ä½œä¸­çš„å¯æ‰©å±•æ•°æ®å¢å¼ºçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a5ed103d5aef3706d10d3f7b2e38954b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-244454e08cb96525a2a117d897f1269f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dc948d16ec145fc4d74e6fa716cc9cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cc8b6d6e0a8bb67539254c6e1ffe4a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8832209fb4e542017da30bb25c42d425.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Ambiguous-Image-Segmentation"><a href="#Diffusion-Based-Ambiguous-Image-Segmentation" class="headerlink" title="Diffusion Based Ambiguous Image Segmentation"></a>Diffusion Based Ambiguous Image Segmentation</h2><p><strong>Authors:Jakob LÃ¸nborg Christensen, Morten Rieger Hannemose, Anders Bjorholm Dahl, Vedrana Andersen Dahl</strong></p>
<p>Medical image segmentation often involves inherent uncertainty due to variations in expert annotations. Capturing this uncertainty is an important goal and previous works have used various generative image models for the purpose of representing the full distribution of plausible expert ground truths. In this work, we explore the design space of diffusion models for generative segmentation, investigating the impact of noise schedules, prediction types, and loss weightings. Notably, we find that making the noise schedule harder with input scaling significantly improves performance. We conclude that x- and v-prediction outperform epsilon-prediction, likely because the diffusion process is in the discrete segmentation domain. Many loss weightings achieve similar performance as long as they give enough weight to the end of the diffusion process. We base our experiments on the LIDC-IDRI lung lesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we introduce a randomly cropped variant of the LIDC-IDRI dataset that is better suited for uncertainty in image segmentation. Our model also achieves SOTA in this harder setting. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ç»å¸¸å› ä¸ºä¸“å®¶æ ‡æ³¨çš„å·®å¼‚è€Œå¸¦æœ‰å›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚æ•æ‰è¿™ç§ä¸ç¡®å®šæ€§æ˜¯ä¸€ä¸ªé‡è¦ç›®æ ‡ï¼Œä¹‹å‰çš„ç ”ç©¶å·²ç»ä½¿ç”¨å„ç§ç”Ÿæˆå›¾åƒæ¨¡å‹æ¥è¡¨ç¤ºä¸“å®¶çœŸå®æ ‡ç­¾çš„å®Œæ•´åˆ†å¸ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”Ÿæˆåˆ†å‰²æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´ï¼Œç ”ç©¶äº†å™ªå£°æ—¶é—´è¡¨ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡çš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é€šè¿‡è¾“å…¥ç¼©æ”¾ä½¿å™ªå£°æ—¶é—´è¡¨æ›´åŠ å›°éš¾å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œxé¢„æµ‹å’Œvé¢„æµ‹ä¼˜äºÎµé¢„æµ‹ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæ‰©æ•£è¿‡ç¨‹å¤„äºç¦»æ•£åˆ†å‰²é¢†åŸŸã€‚åªè¦å¯¹æ‰©æ•£è¿‡ç¨‹çš„ç»“æŸç»™äºˆè¶³å¤Ÿçš„é‡è§†ï¼Œè®¸å¤šæŸå¤±æƒé‡éƒ½èƒ½è¾¾åˆ°ç±»ä¼¼çš„æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒåŸºäºLIDC-IDRIè‚ºç—…å˜æ•°æ®é›†ï¼Œå¹¶è·å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†LIDC-IDRIæ•°æ®é›†çš„éšæœºè£å‰ªç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬æ›´é€‚åˆäºå›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™ä¸ªæ›´å›°éš¾çš„ç¯å¢ƒä¸­åŒæ ·è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05977v2">PDF</a> Accepted at SCIA25</p>
<p><strong>æ‘˜è¦</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸“å®¶æ ‡æ³¨å˜åŒ–å¸¦æ¥çš„å›ºæœ‰ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´ã€‚é€šè¿‡è°ƒæ•´å™ªå£°è°ƒåº¦ã€é¢„æµ‹ç±»å‹å’ŒæŸå¤±æƒé‡ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ è¾“å…¥æ ‡åº¦çš„å™ªå£°è°ƒåº¦èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚åŸºäºLIDC-IDRIè‚ºç—…ç¶æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œx-å’Œv-é¢„æµ‹ä¼˜äºÎµ-é¢„æµ‹ï¼Œå¤šç§æŸå¤±æƒé‡åªè¦è¶³å¤Ÿé‡è§†æ‰©æ•£è¿‡ç¨‹çš„æœ€åé˜¶æ®µéƒ½èƒ½å–å¾—ç±»ä¼¼æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†LIDC-IDRIæ•°æ®é›†çš„éšæœºè£å‰ªç‰ˆæœ¬ï¼Œæ›´é€‚åˆäºå›¾åƒåˆ†å‰²çš„ä¸ç¡®å®šæ€§ç ”ç©¶ã€‚æ¨¡å‹åœ¨è¯¥æ›´å›°éš¾çš„ç¯å¢ƒä¸‹ä¹Ÿè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å­˜åœ¨å› ä¸“å®¶æ ‡æ³¨å˜åŒ–å¯¼è‡´çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚</li>
<li>é€šè¿‡æ‰©æ•£æ¨¡å‹çš„è®¾è®¡ç©ºé—´æ¢ç´¢æ¥è§£å†³è¿™ä¸€ä¸ç¡®å®šæ€§ã€‚</li>
<li>å™ªå£°è°ƒåº¦æ˜¯å½±å“æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œå¢åŠ è¾“å…¥æ ‡åº¦èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>x-å’Œv-é¢„æµ‹åœ¨ç¦»æ•£åˆ†å‰²åŸŸçš„æ‰©æ•£è¿‡ç¨‹ä¸­è¡¨ç°ä¼˜äºÎµ-é¢„æµ‹ã€‚</li>
<li>é€‚å½“çš„æŸå¤±æƒé‡é…ç½®ä¹Ÿèƒ½å–å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>åŸºäºLIDC-IDRIè‚ºç—…ç¶æ•°æ®é›†çš„å®éªŒè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45a8dca888124eeed1232d087ca9d8ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de7ffaa87d20e553c841ce75a2cb731.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4512d984a41c79504eec109e680c6028.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Inverse-Bridge-Matching-Distillation"><a href="#Inverse-Bridge-Matching-Distillation" class="headerlink" title="Inverse Bridge Matching Distillation"></a>Inverse Bridge Matching Distillation</h2><p><strong>Authors:Nikita Gushchin, David Li, Daniil Selikhanovych, Evgeny Burnaev, Dmitry Baranchuk, Alexander Korotin</strong></p>
<p>Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup. We provide the code at <a target="_blank" rel="noopener" href="https://github.com/ngushchin/IBMD">https://github.com/ngushchin/IBMD</a> </p>
<blockquote>
<p>å­¦ä¹ æ‰©æ•£æ¡¥æ¨¡å‹ï¼ˆDiffusion Bridge Modelsï¼Œç®€ç§°DBMï¼‰æ˜¯ä¸€ä»¶ç›¸å¯¹å®¹æ˜“çš„äº‹ï¼Œä½†æƒ³è¦è®©å®ƒä»¬åœ¨é€Ÿåº¦å’Œå®ç”¨æ€§æ–¹é¢è¾¾æ ‡å´æ˜¯ä¸€ç§è‰ºæœ¯ã€‚æ‰©æ•£æ¡¥æ¨¡å‹æ˜¯æœ‰å‰æ™¯çš„æ‰©æ•£æ¨¡å‹æ‰©å±•åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒè½¬æ¢æ–¹é¢å¤§æ˜¾èº«æ‰‹ã€‚ç„¶è€Œï¼Œä¸è®¸å¤šç°ä»£æ‰©æ•£å’ŒæµåŠ¨æ¨¡å‹ä¸€æ ·ï¼ŒDBMä¹Ÿé¢ä¸´ç€æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€†æ¡¥åŒ¹é…å…¬å¼çš„æ–°å‹è’¸é¦æŠ€æœ¯ï¼Œå¹¶æ¨å¯¼å‡ºå®ç”¨çš„ç›®æ ‡æ¥è§£å†³å®é™…åº”ç”¨ä¸­çš„é—®é¢˜ã€‚ä¸ä¹‹å‰å¼€å‘çš„DBMè’¸é¦æŠ€æœ¯ä¸åŒï¼Œæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿè’¸é¦æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶çš„DBMç±»å‹ï¼Œåœ¨ä¸€æ¬¡ç”Ÿæˆå™¨ä¸­è’¸é¦æ¨¡å‹ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨æŸåçš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€JPEGæ¢å¤ã€è‰å›¾åˆ°å›¾åƒå’Œå…¶ä»–ä»»åŠ¡åœ¨å†…çš„å¹¿æ³›è®¾ç½®ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶æ¡¥åŒ¹é…æ–¹æ³•ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„è’¸é¦æŠ€æœ¯èƒ½å¤Ÿå°†DBMçš„æ¨ç†é€Ÿåº¦æé«˜4å€è‡³100å€ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šè®¾ç½®ä¸‹ç”šè‡³æä¾›æ¯”æ‰€ä½¿ç”¨çš„æ•™å¸ˆæ¨¡å‹æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ngushchin/IBMD">https://github.com/ngushchin/IBMD</a>æä¾›äº†ç›¸å…³ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01362v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¡¥æ¨¡å‹ï¼ˆDBMsï¼‰æ˜¯æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè½¬æ¢é¢†åŸŸçš„å»¶ä¼¸åº”ç”¨ï¼Œä½†å…¶å’Œå…¶ä»–ç°ä»£æ‰©æ•£åŠæµæ¨¡å‹å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€†æ¡¥åŒ¹é…å…¬å¼çš„æ–°è’¸é¦æŠ€æœ¯ï¼Œå¹¶æ¨å¯¼å‡ºäº†å®ç”¨çš„ç›®æ ‡å‡½æ•°ã€‚ä¸ä¹‹å‰å¼€å‘çš„DBMè’¸é¦æŠ€æœ¯ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è’¸é¦æ¡ä»¶æ€§å’Œéæ¡ä»¶æ€§çš„DBMsï¼Œä¸€æ­¥ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä¸”åªç”¨æŸåçš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨è¶…åˆ†è¾¨ç‡ã€JPEGæ¢å¤ã€è‰å›¾åˆ°å›¾åƒç­‰ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜æˆ‘ä»¬çš„è’¸é¦æŠ€æœ¯å¯ä»¥å°†DBMçš„æ¨ç†é€Ÿåº¦æé«˜4åˆ°100å€ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šè®¾ç½®ä¸‹ç”šè‡³æä¾›æ¯”æ•™å¸ˆæ¨¡å‹æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¡¥æ¨¡å‹ï¼ˆDBMsï¼‰æ˜¯æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè½¬æ¢é¢†åŸŸçš„æ‰©å±•åº”ç”¨ï¼Œä½†å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé€†æ¡¥åŒ¹é…å…¬å¼çš„æ–°çš„è’¸é¦æŠ€æœ¯æ¥è§£å†³æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥è’¸é¦æ¡ä»¶æ€§å’Œéæ¡ä»¶æ€§çš„DBMsï¼Œä¸€æ­¥ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨æŸåçš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨è¶…åˆ†è¾¨ç‡ã€JPEGæ¢å¤å’Œè‰å›¾åˆ°å›¾åƒç­‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>è’¸é¦æŠ€æœ¯å¯ä»¥å°†DBMçš„æ¨ç†é€Ÿåº¦æé«˜4åˆ°100å€ã€‚</li>
<li>åœ¨ç‰¹å®šè®¾ç½®ä¸‹ï¼Œè’¸é¦æŠ€æœ¯æä¾›çš„ç”Ÿæˆè´¨é‡å¯èƒ½è¶…è¿‡åŸå§‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01362">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-022ba9067cf03656088833c3004ebf90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4f562bd69a70f3d2749a659376bda63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-229f8fb93b814437bba8a1b875af5235.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Style-Ambiguity-Loss-Using-CLIP"><a href="#Style-Ambiguity-Loss-Using-CLIP" class="headerlink" title="Style Ambiguity Loss Using CLIP"></a>Style Ambiguity Loss Using CLIP</h2><p><strong>Authors:James Baker</strong></p>
<p>In this work, we explore using the style ambiguity training objective, originally used to approximate creativity, on a diffusion model. However, this objective requires the use of a pretrained classifier and a labeled dataset. We introduce new forms of style ambiguity loss that do not require training a new classifier or a labeled dataset. Instead of using a classifier, we generate centroids in the CLIP embedding space, and images are classified based on their relative distance to said centroids. We find the centroids via K-means clustering of an unlabeled dataset, as well as using text labels to generate CLIP embeddings, to be used as centroids. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jamesBaker361/clipcreate">https://github.com/jamesBaker361/clipcreate</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢åœ¨æ‰©æ•£æ¨¡å‹ä¸Šä½¿ç”¨æœ€åˆç”¨äºè¿‘ä¼¼åˆ›é€ åŠ›çš„é£æ ¼æ¨¡ç³Šè®­ç»ƒç›®æ ‡ã€‚ç„¶è€Œï¼Œè¿™ä¸€ç›®æ ‡éœ€è¦ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†ç±»å™¨å’Œæœ‰æ ‡ç­¾çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¼•å…¥äº†æ–°çš„é£æ ¼æ¨¡ç³ŠæŸå¤±å½¢å¼ï¼Œä¸éœ€è¦è®­ç»ƒæ–°çš„åˆ†ç±»å™¨æˆ–æœ‰æ ‡ç­¾çš„æ•°æ®é›†ã€‚æˆ‘ä»¬ä¸åœ¨ä½¿ç”¨åˆ†ç±»å™¨ï¼Œè€Œæ˜¯åœ¨CLIPåµŒå…¥ç©ºé—´ä¸­ç”Ÿæˆè´¨å¿ƒï¼Œå¹¶æ ¹æ®å›¾åƒä¸æ‰€è¿°è´¨å¿ƒçš„ç›¸å¯¹è·ç¦»å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®é›†è¿›è¡ŒK-meansèšç±»ä»¥åŠä½¿ç”¨æ–‡æœ¬æ ‡ç­¾ç”ŸæˆCLIPåµŒå…¥æ¥æ‰¾åˆ°è´¨å¿ƒã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jamesBaker361/clipcreate%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jamesBaker361/clipcreateæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02055v3">PDF</a> arXiv admin note: substantial text overlap with arXiv:2407.12009</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡æ¢ç´¢äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­ä½¿ç”¨åŸå…ˆç”¨äºè¿‘ä¼¼åˆ›é€ åŠ›çš„é£æ ¼æ¨¡ç³Šè®­ç»ƒç›®æ ‡ã€‚æ–°æ–¹æ³•æ— éœ€è®­ç»ƒæ–°çš„åˆ†ç±»å™¨æˆ–æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œæ˜¯é€šè¿‡ç”ŸæˆCLIPåµŒå…¥ç©ºé—´ä¸­çš„è´¨å¿ƒæ¥è¿›è¡Œå›¾åƒåˆ†ç±»ï¼Œå›¾åƒçš„åˆ†ç±»åŸºäºå®ƒä»¬ä¸æ‰€è¿°è´¨å¿ƒçš„ç›¸å¯¹è·ç¦»ã€‚é€šè¿‡æ— æ ‡ç­¾æ•°æ®é›†çš„K-meansèšç±»å’Œæ–‡æœ¬æ ‡ç­¾ç”ŸæˆCLIPåµŒå…¥ä½œä¸ºè´¨å¿ƒã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jamesBaker361/clipcreate%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jamesBaker361/clipcreateæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é£æ ¼æ¨¡ç³Šè®­ç»ƒç›®æ ‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨è¢«æ¢ç´¢ã€‚</li>
<li>æ–°çš„é£æ ¼æ¨¡ç³ŠæŸå¤±å½¢å¼è¢«å¼•å…¥ï¼Œæ— éœ€è®­ç»ƒæ–°çš„åˆ†ç±»å™¨æˆ–æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡åœ¨CLIPåµŒå…¥ç©ºé—´ä¸­ç”Ÿæˆè´¨å¿ƒï¼Œè¿›è¡Œå›¾åƒåˆ†ç±»ã€‚</li>
<li>å›¾åƒåˆ†ç±»åŸºäºå…¶ä¸è´¨å¿ƒçš„ç›¸å¯¹è·ç¦»ã€‚</li>
<li>ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®é›†çš„K-meansèšç±»å’Œæ–‡æœ¬æ ‡ç­¾ç”ŸæˆCLIPåµŒå…¥ä½œä¸ºè´¨å¿ƒã€‚</li>
<li>æä¾›çš„ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-110f8b18b4df5cfcd1c19f49001497f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce17464c5195811af24c83b9169a1105.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8755e6c14ca7347788a6d0fdde463dc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b241b9a0a5fc16d09d0f169512d9c42a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0ff877a25991c88c19217a0fbd26950f.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a   Lightweight Auto3DSeg and SegResNet Implementation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-54ea4593628d745597d43f098551c054.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-20  Transfer Learning for Neutrino Scattering Domain Adaptation with GANs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
