<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-01-21  FaceXBench Evaluating Multimodal LLMs on Face Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a9b3929f400dd861e57bad3fed550363.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-21-更新"><a href="#2025-01-21-更新" class="headerlink" title="2025-01-21 更新"></a>2025-01-21 更新</h1><h2 id="FaceXBench-Evaluating-Multimodal-LLMs-on-Face-Understanding"><a href="#FaceXBench-Evaluating-Multimodal-LLMs-on-Face-Understanding" class="headerlink" title="FaceXBench: Evaluating Multimodal LLMs on Face Understanding"></a>FaceXBench: Evaluating Multimodal LLMs on Face Understanding</h2><p><strong>Authors:Kartik Narayan, Vibashan VS, Vishal M. Patel</strong></p>
<p>Multimodal Large Language Models (MLLMs) demonstrate impressive problem-solving abilities across a wide range of tasks and domains. However, their capacity for face understanding has not been systematically studied. To address this gap, we introduce FaceXBench, a comprehensive benchmark designed to evaluate MLLMs on complex face understanding tasks. FaceXBench includes 5,000 multimodal multiple-choice questions derived from 25 public datasets and a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6 broad categories, assessing MLLMs’ face understanding abilities in bias and fairness, face authentication, recognition, analysis, localization and tool retrieval. Using FaceXBench, we conduct an extensive evaluation of 26 open-source MLLMs alongside 2 proprietary models, revealing the unique challenges in complex face understanding tasks. We analyze the models across three evaluation settings: zero-shot, in-context task description, and chain-of-thought prompting. Our detailed analysis reveals that current MLLMs, including advanced models like GPT-4o, and GeminiPro 1.5, show significant room for improvement. We believe FaceXBench will be a crucial resource for developing MLLMs equipped to perform sophisticated face understanding. Code: <a target="_blank" rel="noopener" href="https://github.com/Kartik-3004/facexbench">https://github.com/Kartik-3004/facexbench</a> </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在广泛的任务和领域中表现出了令人印象深刻的解决问题的能力。然而，它们对面部理解的能力尚未得到系统的研究。为了弥补这一空白，我们引入了FaceXBench，这是一个旨在评估MLLMs在复杂面部理解任务上的综合基准测试。FaceXBench包括从25个公共数据集和新创建的数据集FaceXAPI中派生的5000个多模态多项选择题。这些问题涵盖了6大类中的14项任务，评估MLLMs在偏见和公平性、面部认证、识别、分析、定位和工具检索方面的面部理解能力。使用FaceXBench，我们对26个开源MLLMs和2个专有模型进行了广泛评估，揭示了复杂面部理解任务的独特挑战。我们分析了三种评估环境下的模型：零样本、上下文任务描述和思维链提示。我们的详细分析表明，包括GPT-4o和GeminiPro 1.5等先进模型在内的当前MLLMs仍有很大的改进空间。我们相信FaceXBench将成为开发能够执行复杂面部理解任务的关键资源的重要工具。代码：<a target="_blank" rel="noopener" href="https://github.com/Kartik-3004/facexbench">https://github.com/Kartik-3004/facexbench</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10360v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://kartik-3004.github.io/facexbench/">https://kartik-3004.github.io/facexbench/</a></p>
<p><strong>Summary</strong>：</p>
<p>多模态大型语言模型（MLLMs）在广泛的任务和领域中展现出令人印象深刻的解决问题的能力。然而，它们对面部理解的能力尚未进行系统性的研究。为解决这一空白，我们推出了FaceXBench，这是一套全面评估MLLMs在复杂面部理解任务上的表现的基准测试。FaceXBench包含从25个公开数据集和新创建的FaceXAPI数据集中衍生的5000个多模态选择题。这些问题涵盖了6大类别的14项任务，评估MLLMs在偏见与公正、面部认证、识别、分析、定位和工具检索方面的面部理解能力。通过使用FaceXBench，我们对26个开源MLLMs和2个专有模型进行了广泛评估，揭示了复杂面部理解任务的独特挑战。我们的分析表明，包括GPT-4o和GeminiPro 1.5等先进模型在内的当前MLLMs仍有很大的改进空间。我们相信FaceXBench将成为开发具备高级面部理解能力的MLLMs的关键资源。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>FaceXBench是首个全面评估MLLMs在复杂面部理解任务上表现的基准测试。</li>
<li>FaceXBench包含5000个多模态选择题，涵盖14项任务，6大类别，全面评估MLLMs的面部理解能力。</li>
<li>FaceXBench来源于25个公开数据集和新创建的FaceXAPI数据集。</li>
<li>通过FaceXBench对多个MLLMs模型进行了评估，包括开源模型和专有模型。</li>
<li>复杂面部理解任务存在独特挑战。</li>
<li>当前MLLMs，包括先进模型，在面部理解方面仍有显著改进空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bc43479ca2a6cba4a63db3c97460540e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a396dfbc341c6343e54cae812ee51554.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d734150feb840cfdac38e8b96df01c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9bf14c7b56d3e8ac1d05d7cf37b2d28.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Agent4Edu-Generating-Learner-Response-Data-by-Generative-Agents-for-Intelligent-Education-Systems"><a href="#Agent4Edu-Generating-Learner-Response-Data-by-Generative-Agents-for-Intelligent-Education-Systems" class="headerlink" title="Agent4Edu: Generating Learner Response Data by Generative Agents for   Intelligent Education Systems"></a>Agent4Edu: Generating Learner Response Data by Generative Agents for   Intelligent Education Systems</h2><p><strong>Authors:Weibo Gao, Qi Liu, Linan Yue, Fangzhou Yao, Rui Lv, Zheng Zhang, Hao Wang, Zhenya Huang</strong></p>
<p>Personalized learning represents a promising educational strategy within intelligent educational systems, aiming to enhance learners’ practice efficiency. However, the discrepancy between offline metrics and online performance significantly impedes their progress. To address this challenge, we introduce Agent4Edu, a novel personalized learning simulator leveraging recent advancements in human intelligence through large language models (LLMs). Agent4Edu features LLM-powered generative agents equipped with learner profile, memory, and action modules tailored to personalized learning algorithms. The learner profiles are initialized using real-world response data, capturing practice styles and cognitive factors. Inspired by human psychology theory, the memory module records practice facts and high-level summaries, integrating reflection mechanisms. The action module supports various behaviors, including exercise understanding, analysis, and response generation. Each agent can interact with personalized learning algorithms, such as computerized adaptive testing, enabling a multifaceted evaluation and enhancement of customized services. Through a comprehensive assessment, we explore the strengths and weaknesses of Agent4Edu, emphasizing the consistency and discrepancies in responses between agents and human learners. The code, data, and appendix are publicly available at <a target="_blank" rel="noopener" href="https://github.com/bigdata-ustc/Agent4Edu">https://github.com/bigdata-ustc/Agent4Edu</a>. </p>
<blockquote>
<p>个性化学习代表了一种在智能教育系统中具有前景的教育策略，旨在提高学习者的实践效率。然而，离线指标与在线表现之间的差异显著阻碍了其进展。为了应对这一挑战，我们引入了Agent4Edu，这是一种利用大型语言模型（LLM）的最新进展的新型个性化学习模拟器。Agent4Edu的特点是拥有LLM驱动的生成代理，配备了学习者概况、记忆和行动模块，这些模块都针对个性化学习算法进行了定制。学习者概况使用现实世界的响应数据进行初始化，捕获实践风格和认知因素。受人类心理学理论的启发，记忆模块记录实践事实和高层次摘要，集成了反思机制。行动模块支持各种行为，包括理解练习、分析和生成响应。每个代理都可以与个性化学习算法进行交互，如计算机化自适应测试，从而实现多方面的评估和定制服务的增强。通过全面评估，我们探讨了Agent4Edu的优缺点，重点强调了代理和人类学习者之间响应的一致性和差异。代码、数据和附录可在<a target="_blank" rel="noopener" href="https://github.com/bigdata-ustc/Agent4Edu%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bigdata-ustc/Agent4Edu上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10332v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>基于智能教育系统内的个性化学习策略具有广阔的发展前景，旨在提高学习者的实践效率。然而，离线指标与在线表现之间的差异显著阻碍了个性化学习的进步。为解决这一挑战，我们推出了Agent4Edu——一款利用大型语言模型（LLM）最新进展的新型个性化学习模拟器。Agent4Edu配备了个性化学习算法，其生成式代理拥有学习者特性、记忆及行动模块。学习者特性采用现实世界响应数据进行初始化，以捕捉实践风格和认知因素。记忆模块采用心理学理论，记录实践事实和高级摘要并集成反思机制。行动模块支持练习理解、分析和响应生成等行为。每个代理都能与诸如计算机自适应测试等个性化学习算法进行交互，实现多元化评估和定制化服务的提升。我们对Agent4Edu进行了全面评估，重点关注了代理响应与人类学习者之间的一致性差异和强弱项。相关代码、数据和附录可访问 <a target="_blank" rel="noopener" href="https://github.com/bigdata-ustc/Agent4Edu">https://github.com/bigdata-ustc/Agent4Edu</a> 获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>个性化学习是智能教育系统中一种有前途的教育策略，旨在提高学习者的实践效率。</li>
<li>Agent4Edu是一款新型个性化学习模拟器，采用大型语言模型（LLM）技术。</li>
<li>Agent4Edu配备学习者特性、记忆和行动模块，模拟真实学习者的特性和行为。</li>
<li>学习者特性通过现实世界的响应数据进行初始化，以反映实践风格和认知因素。</li>
<li>Agent4Edu的记忆模块基于心理学理论设计，记录实践事实、高级摘要，并集成反思机制。</li>
<li>Agent4Edu通过全面的评估，在个性化学习算法（如计算机自适应测试）的交互中表现出一致性差异和性能强弱。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10332">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d182b0f8642d70c5ad9f0c000fc53028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec3d5e92938c6f9eb58305a79f2e3e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86bf1672931fca61d398cca51fcf06c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Human-Guided-Data-Centric-LLM-Co-Pilots"><a href="#Towards-Human-Guided-Data-Centric-LLM-Co-Pilots" class="headerlink" title="Towards Human-Guided, Data-Centric LLM Co-Pilots"></a>Towards Human-Guided, Data-Centric LLM Co-Pilots</h2><p><strong>Authors:Evgeny Saveliev, Jiashuo Liu, Nabeel Seedat, Anders Boyd, Mihaela van der Schaar</strong></p>
<p>Machine learning (ML) has the potential to revolutionize healthcare, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC’s ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains – healthcare, finance, social sciences and more – to actively participate in driving real-world impact using ML. </p>
<blockquote>
<p>机器学习（ML）有潜力彻底改变医疗保健领域，但其应用往往受到领域专家需求与将这些需求转化为稳健有效的ML工具之间的脱节阻碍。尽管基于大型语言模型（LLM）的副驾驶系统在民主化ML方面为非技术专家做出了最新的进展，但这些系统主要关注模型为中心的方面，却忽视了关键的数据为中心的挑战。在复杂的现实世界中，原始数据常常存在复杂的问题，如缺失值、标签噪声和需要定制处理的领域特定细微差别。为了解决这些问题，我们引入了CliMB-DC，这是一个以人为指导、以数据为中心的大型语言模型副驾驶框架，它结合了先进的数据中心工具和大型语言模型驱动的推理，以实现稳健、面向上下文的数据处理。其核心是一个新型的多智能体推理系统，结合了用于动态规划和适应的战略协调器以及用于精确执行的专职工作者智能体。此后，领域专业知识被系统地纳入以指导推理过程，采用人类参与循环的方法。为了指导开发，我们对关键的数据为中心的挑战进行了分类，副驾驶必须解决这些问题。之后，为了应对分类的各个方面，我们将先进的数据为中心的工具集成到一个可扩展的开源架构中，便于研究社区添加新工具。在实证方面，我们使用真实的医疗数据集证明了CliMB-DC将非策划数据集转化为适合机器学习格式的能力，在处理数据为中心的挑战方面显著优于现有的副驾驶基线。CliMB-DC有望赋能来自不同领域的领域专家——医疗保健、金融、社会科学等——积极参与利用机器学习推动现实世界的变革。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10321v1">PDF</a> Saveliev, Liu &amp; Seedat contributed equally</p>
<p><strong>Summary</strong></p>
<p>机器学习（ML）在医疗领域具有巨大的潜力，但其实际应用常常受到领域专家需求与将这些需求转化为稳健有效的ML工具之间的鸿沟的阻碍。尽管基于大型语言模型（LLM）的辅助系统在民主化ML方面取得进展，但它们主要关注模型为中心的方法而忽视数据为中心的挑战。针对这一问题，本文介绍了CliMB-DC，这是一个结合高级数据工具与LLM驱动的推理的人为引导、以数据为中心的大型语言模型辅助系统框架。它引入了多智能体推理系统，结合战略协调员进行动态规划和适应性与专职工作代理进行精确执行。通过人为引导的方式系统地融入领域专业知识以指导推理过程。通过实证研究发现，使用真实医疗数据集时，CliMB-DC能将未整理的数集转化为ML就绪格式，在处理数据为中心的挑战方面明显优于现有辅助系统基线。CliMB-DC有望在医疗、金融、社会科学等各个领域为领域专家赋能，积极参与推动机器学习在现实世界的实际应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习在医疗领域具有潜力，但实际应用受到领域专家与工具转化需求之间的鸿沟的阻碍。</li>
<li>当前大型语言模型辅助系统主要关注模型为中心的方法而忽视数据为中心的挑战。</li>
<li>CliMB-DC是一个结合高级数据工具与LLM的大型语言模型辅助系统框架，旨在解决上述问题。</li>
<li>CliMB-DC引入多智能体推理系统，包括战略协调员和工作代理来确保稳健且上下文感知的数据处理。</li>
<li>通过人为引导的方式融入领域专业知识以指导推理过程。</li>
<li>CliMB-DC能成功转化未整理数据集为ML就绪格式，并处理数据为中心的挑战方面优于现有辅助系统基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10321">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a8b0cca60e4d4698d3c9c48f9e9b84d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522865d56fe364222cdc073f4616567f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ca7d25357d0d20791133607bac5564b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HiMix-Reducing-Computational-Complexity-in-Large-Vision-Language-Models"><a href="#HiMix-Reducing-Computational-Complexity-in-Large-Vision-Language-Models" class="headerlink" title="HiMix: Reducing Computational Complexity in Large Vision-Language Models"></a>HiMix: Reducing Computational Complexity in Large Vision-Language Models</h2><p><strong>Authors:Xuange Zhang, Dengjie Li, Bo Liu, Zenghao Bao, Yao Zhou, Baisong Yang, Zhongying Liu, Yujie Zhong, Zheng Zhao, Tongtong Yuan</strong></p>
<p>Benefiting from recent advancements in large language models and modality alignment techniques, existing Large Vision-Language Models(LVLMs) have achieved prominent performance across a wide range of scenarios. However, the excessive computational complexity limits the widespread use of these models in practical applications. We argue that one main bottleneck in computational complexity is caused by the involvement of redundant vision sequences in model computation. This is inspired by a reassessment of the efficiency of vision and language information transmission in the language decoder of LVLMs. Then, we propose a novel hierarchical vision-language interaction mechanism called Hierarchical Vision injection for Mixture Attention (HiMix). In HiMix, only the language sequence undergoes full forward propagation, while the vision sequence interacts with the language at specific stages within each language decoder layer. It is striking that our approach significantly reduces computational complexity with minimal performance loss. Specifically, HiMix achieves a 10x reduction in the computational cost of the language decoder across multiple LVLM models while maintaining comparable performance. This highlights the advantages of our method, and we hope our research brings new perspectives to the field of vision-language understanding. Project Page: <a target="_blank" rel="noopener" href="https://xuange923.github.io/HiMix">https://xuange923.github.io/HiMix</a> </p>
<blockquote>
<p>得益于大型语言模型和模态对齐技术的最新进展，现有的大型视觉语言模型（LVLMs）在多种场景中均取得了卓越的性能。然而，过高的计算复杂度限制了这些模型在实际应用中的广泛使用。我们认为，计算复杂度的一个主要瓶颈是由模型计算中冗余的视觉序列的参与所引起的。这源于我们对LVLMs语言解码器中视觉和语言信息传输效率的重新评估。随后，我们提出了一种新型的分层视觉语言交互机制，称为混合注意力分层视觉注入（HiMix）。在HiMix中，只有语言序列进行全前向传播，而视觉序列则在每个语言解码器层的特定阶段与语言进行交互。值得注意的是，我们的方法能在保持极小性能损失的同时显著降低计算复杂度。具体来说，HiMix在多个LVLM模型的语言解码器上实现了计算成本的10倍缩减，同时保持相当的性能。这凸显了我们方法的优势，我们希望我们的研究能为视觉语言理解领域带来新的视角。项目页面：<a target="_blank" rel="noopener" href="https://xuange923.github.io/HiMix">https://xuange923.github.io/HiMix</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10318v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型和模态对齐技术的最新进展，大型视觉语言模型（LVLMs）已在各种场景中取得了显著的性能提升。然而，过高的计算复杂度限制了这些模型在实际应用中的广泛应用。本研究认为冗余的视觉序列参与模型计算是导致计算复杂度过高的主要原因之一。为此，我们提出了一种新型的分层视觉语言交互机制，称为分层视觉注入混合注意力（HiMix）。在HiMix中，只有语言序列进行全前向传播，而视觉序列则在每个语言解码器层的特定阶段与语言进行交互。我们的方法在保证性能损失最小的情况下显著降低了计算复杂度。具体来说，HiMix在多个LVLM模型中实现了语言解码器计算成本的10倍缩减，同时保持性能相当。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型视觉语言模型（LVLMs）在多种场景中的性能突出，但计算复杂度过高限制了实际应用。</li>
<li>冗余的视觉序列参与模型计算是导致计算复杂度高的主要原因之一。</li>
<li>提出了新型的分层视觉语言交互机制——分层视觉注入混合注意力（HiMix）。</li>
<li>在HiMix中，只有语言序列进行全前向传播，视觉序列则在特定阶段与语言交互。</li>
<li>HiMix显著降低了计算复杂度，实现了语言解码器计算成本的10倍缩减，同时保持性能相当。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ab98bafc0f62ea091954ef242754c419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1c08802412e896867f4ca5efe9b15cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69725e932a6bc774637f873a980520b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-283455534c9ee4426cdb89a80192a84b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c16449b4cc6ef69f358ded9f621de95e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2866620bb060c58867ebc4f8607799a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a240108800143b400ba5eb445c479b86.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Addressing-Popularity-Bias-in-Third-Party-Library-Recommendations-Using-LLMs"><a href="#Addressing-Popularity-Bias-in-Third-Party-Library-Recommendations-Using-LLMs" class="headerlink" title="Addressing Popularity Bias in Third-Party Library Recommendations Using   LLMs"></a>Addressing Popularity Bias in Third-Party Library Recommendations Using   LLMs</h2><p><strong>Authors:Claudio Di Sipio, Juri Di Rocco, Davide Di Ruscio, Vladyslav Bulhakov</strong></p>
<p>Recommender systems for software engineering (RSSE) play a crucial role in automating development tasks by providing relevant suggestions according to the developer’s context. However, they suffer from the so-called popularity bias, i.e., the phenomenon of recommending popular items that might be irrelevant to the current task. In particular, the long-tail effect can hamper the system’s performance in terms of accuracy, thus leading to false positives in the provided recommendations. Foundation models are the most advanced generative AI-based models that achieve relevant results in several SE tasks.   This paper aims to investigate the capability of large language models (LLMs) to address the popularity bias in recommender systems of third-party libraries (TPLs). We conduct an ablation study experimenting with state-of-the-art techniques to mitigate the popularity bias, including fine-tuning and popularity penalty mechanisms. Our findings reveal that the considered LLMs cannot address the popularity bias in TPL recommenders, even though fine-tuning and post-processing penalty mechanism contributes to increasing the overall diversity of the provided recommendations. In addition, we discuss the limitations of LLMs in this context and suggest potential improvements to address the popularity bias in TPL recommenders, thus paving the way for additional experiments in this direction. </p>
<blockquote>
<p>软件工程的推荐系统（RSSE）根据开发者的上下文提供相关的建议，在自动化开发任务中起着至关重要的作用。然而，它们存在所谓的流行偏见，即推荐流行项目，而这些项目可能与当前任务无关的现象。特别是，长尾效应可能会阻碍系统在准确性方面的性能，从而导致提供的推荐中出现误报。基础模型是最先进的基于生成式人工智能的模型，在多个软件工程任务中取得了相关的结果。本文旨在研究大型语言模型（LLM）解决第三方库（TPL）推荐系统中流行偏见的能力。我们进行了一项消融研究，尝试使用最先进的技术来减轻流行偏见，包括微调和后处理惩罚机制。我们的研究发现，所考虑的LLM无法解决TPL推荐系统中的流行偏见问题，尽管微调和后处理惩罚机制有助于提高所提供建议的总体多样性。此外，我们还讨论了在这种情况下LLM的局限性，并提出了解决TPL推荐系统中流行偏见问题的潜在改进方法，从而为今后的实验指明了方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10313v1">PDF</a> Accepted at the 1st International Workshop on Fairness in Software   Systems, co-located with SANER2025</p>
<p><strong>Summary</strong><br>推荐系统对于软件工程来说非常重要，能够根据开发者的上下文提供相关的建议，自动化开发任务。然而，它们受到所谓的流行度偏差的影响，推荐流行项目可能与当前任务无关。本文旨在研究大型语言模型（LLM）解决第三方库推荐系统中的流行度偏差的能力。通过采用先进技术来减轻流行度偏差，包括微调和后处理惩罚机制，发现LLM无法解决TPL推荐系统中的流行度偏差问题。尽管微调和后处理惩罚机制有助于提高推荐的总体多样性，但仍然存在局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推荐系统对软件工程至关重要，可自动化开发任务并提供相关建议。</li>
<li>推荐系统存在流行度偏差问题，即推荐流行项目可能与当前任务无关。</li>
<li>大型语言模型（LLM）被研究用于解决第三方库推荐系统中的流行度偏差问题。</li>
<li>通过采用先进技术来减轻流行度偏差，包括微调和后处理惩罚机制。</li>
<li>LLM无法解决TPL推荐系统中的流行度偏差问题。</li>
<li>即使有补救措施，推荐系统仍然存在局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10313">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ba41fbcdc24b35d81292b7bc3e59022e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-963a1d255447ace9d429f5d554da8ab3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b90b86432a292253c48dc003b7bbde1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a268f7c1f002d4d465b645ce1e3b95ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec41332145feaafc31430a7c929ffef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ecbb8c6d9a1dc97060913e5b6eb3dce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fe8e97c43879adf487bc92111450b00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8716599235a251bd39f5bfca193798bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88b8234a29cc132cc4ec8e737481c341.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Test-Wars-A-Comparative-Study-of-SBST-Symbolic-Execution-and-LLM-Based-Approaches-to-Unit-Test-Generation"><a href="#Test-Wars-A-Comparative-Study-of-SBST-Symbolic-Execution-and-LLM-Based-Approaches-to-Unit-Test-Generation" class="headerlink" title="Test Wars: A Comparative Study of SBST, Symbolic Execution, and   LLM-Based Approaches to Unit Test Generation"></a>Test Wars: A Comparative Study of SBST, Symbolic Execution, and   LLM-Based Approaches to Unit Test Generation</h2><p><strong>Authors:Azat Abdullin, Pouria Derakhshanfar, Annibale Panichella</strong></p>
<p>Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new opportunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM-based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods in terms of coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach also performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are primarily affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size. </p>
<blockquote>
<p>自动生成测试是软件工程研究的关键和持续关注的领域。大型语言模型（LLM）的出现为其提供了新的机遇，因为它们能够执行各种任务。然而，与基于搜索的软件测试（SBST）和符号执行等传统技术相比，LLM方法的有效性仍然不确定。在本文中，我们对基于三种工具的自动测试生成方法进行了广泛的研究：用于SBST的EvoSuite，用于符号执行的Kex，以及用于LLM测试生成的TestSpark。我们在GitBug Java数据集上评估了这些工具的性能，并使用各种基于执行和基于特征的性能指标进行了比较。我们的结果表明，虽然基于LLM的测试生成具有前景，但在覆盖率方面落后于传统方法。然而，它在突变得分方面显著优于传统方法，这表明LLM对代码具有更深语义理解。就故障检测能力而言，基于LLM的方法也表现较差逊于SBST和符号执行方法。此外，我们的基于特征的分析表明，所有工具主要受到测试类（CUT）的复杂性及其内部依赖性的影响，而基于LLM的方法尤其受到CUT大小的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10200v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM在自动测试生成领域具有潜力，但与传统的基于搜索的软件测试（SBST）和符号执行技术相比，其效果尚不确定。一项最新研究表明，在基于三种工具的自动测试生成方法（EvoSuite for SBST，Kex for符号执行，TestSpark for LLM）的对比评估中，LLM虽然在覆盖率方面表现较弱，但在突变得分上表现优异，显示出对代码更深层的语义理解。然而，在故障检测能力方面，LLM仍落后于SBST和符号执行。此外，特征分析表明，所有工具主要受测试类（CUT）的复杂性和内部依赖性的影响，其中LLM对CUT大小尤为敏感。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在自动测试生成中具有潜力，但与传统方法的效果对比尚不确定。</li>
<li>LLM在突变得分上表现优异，显示出对代码深层的语义理解。</li>
<li>在覆盖率方面，LLM表现较弱，落后于传统的SBST和符号执行技术。</li>
<li>LLM在故障检测能力方面仍落后于SBST和符号执行。</li>
<li>所有测试工具都受测试类（CUT）的复杂性和内部依赖性的影响。</li>
<li>LLM对CUT大小尤为敏感。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10200">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c74940af13ab3114a73b9666ae0e39f3.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-21\./crop_LLM/2501.10200v1/page_2_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c26454058be1f8d50d2f480ccbd6b991.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e2f5481d98d7ddcdb5ab4860637cd4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-747e6c4b9a544c2c8269d64089783ddd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Generative-Artificial-Intelligence-Implications-for-Biomedical-and-Health-Professions-Education"><a href="#Generative-Artificial-Intelligence-Implications-for-Biomedical-and-Health-Professions-Education" class="headerlink" title="Generative Artificial Intelligence: Implications for Biomedical and   Health Professions Education"></a>Generative Artificial Intelligence: Implications for Biomedical and   Health Professions Education</h2><p><strong>Authors:William Hersh</strong></p>
<p>Generative AI has had a profound impact on biomedicine and health, both in professional work and in education. Based on large language models (LLMs), generative AI has been found to perform as well as humans in simulated situations taking medical board exams, answering clinical questions, solving clinical cases, applying clinical reasoning, and summarizing information. Generative AI is also being used widely in education, performing well in academic courses and their assessments. This review summarizes the successes of LLMs and highlights some of their challenges in the context of education, most notably aspects that may undermines the acquisition of knowledge and skills for professional work. It then provides recommendations for best practices overcoming shortcomings for LLM use in education. Although there are challenges for use of generative AI in education, all students and faculty, in biomedicine and health and beyond, must have understanding and be competent in its use. </p>
<blockquote>
<p>生成式人工智能在生物医学和健康领域产生了深远影响，无论是在专业工作还是在教育中都是如此。基于大型语言模型（LLM），生成式人工智能在模拟的医疗考试、回答临床问题、解决临床病例、应用临床推理和信息总结等情境中，被发现的表现与人类相当。生成式人工智能也在教育中得到了广泛应用，在学术课程和评估中表现良好。这篇综述总结了LLM的成功之处，并强调了它们在教育环境中的一些挑战，尤其是那些可能会破坏专业工作的知识和技能获取方面。然后，它为克服在教育中使用LLM的缺陷的最佳实践提供了建议。尽管在教育领域使用生成式人工智能存在挑战，但生物医学和健康领域以及更广泛的学生和教师都必须理解并熟练使用它。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10186v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的生成性人工智能在生物医学和健康领域产生了深远影响，不仅在专业工作中表现出色，也在教育中得到广泛应用。在模拟医学考试、回答临床问题、解决临床病例、应用临床推理和信息总结等方面，生成性人工智能的表现与人类相当。然而，在教育环境中，它也面临着一些挑战，可能会影响到专业工作中知识和技能的培养。因此，本文总结了LLM的成功应用，强调了其面临的挑战，并提供了克服这些挑战的最佳实践建议。尽管存在挑战，但所有生物医学和健康领域的学生和教师都需要了解并熟练使用生成性人工智能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>生成性AI在生物医学和健康领域有深远影响，在专业工作和教育中都有广泛应用。</li>
<li>在模拟医学考试、回答临床问题等方面，生成性AI的表现与人类相当。</li>
<li>生成性AI在教育环境中存在一些挑战，可能影响知识及技能的习得。</li>
<li>LLM的成功应用需要克服一些挑战，本文提供了克服挑战的最佳实践建议。</li>
<li>生成性AI在教育中存在挑战，但学生与教师仍需掌握其使用。</li>
<li>生成性AI的潜力在于其能够处理大量数据和复杂任务的能力。</li>
<li>在教育环境中使用生成性AI时需要注意保护学生隐私和数据安全。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10186">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a86bcf3a5473a32ec0fb947e788d402f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fa44218f4684fce6611c20912d5e330.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac06db05bb6f51f9ed41a02d4518a11a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9906e20b26302570256825896bdc243d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ComplexFuncBench-Exploring-Multi-Step-and-Constrained-Function-Calling-under-Long-Context-Scenario"><a href="#ComplexFuncBench-Exploring-Multi-Step-and-Constrained-Function-Calling-under-Long-Context-Scenario" class="headerlink" title="ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling   under Long-Context Scenario"></a>ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling   under Long-Context Scenario</h2><p><strong>Authors:Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, Jie Tang</strong></p>
<p>Enhancing large language models (LLMs) with real-time APIs can help generate more accurate and up-to-date responses. However, evaluating the function calling abilities of LLMs in real-world scenarios remains under-explored due to the complexity of data collection and evaluation. In this work, we introduce ComplexFuncBench, a benchmark for complex function calling across five real-world scenarios. Compared to existing benchmarks, ComplexFuncBench encompasses multi-step and constrained function calling, which requires long-parameter filing, parameter value reasoning, and 128k long context. Additionally, we propose an automatic framework, ComplexEval, for quantitatively evaluating complex function calling tasks. Through comprehensive experiments, we demonstrate the deficiencies of state-of-the-art LLMs in function calling and suggest future directions for optimizing these capabilities. The data and code are available at \url{<a target="_blank" rel="noopener" href="https://github.com/THUDM/ComplexFuncBench%7D">https://github.com/THUDM/ComplexFuncBench}</a>. </p>
<blockquote>
<p>增强大型语言模型（LLM）的实时API可以帮助生成更准确和最新的响应。然而，由于数据收集和评估的复杂性，在真实场景中评估LLM的函数调用能力仍然被低估。在这项工作中，我们介绍了ComplexFuncBench，这是一个涵盖五种真实场景复杂函数调用的基准测试。与现有基准测试相比，ComplexFuncBench包括多步骤和受约束的函数调用，这需要长参数文件、参数值推理和128k长上下文。此外，我们提出了一个自动框架ComplexEval，用于定量评估复杂的函数调用任务。通过全面的实验，我们展示了当前LLM在函数调用方面的不足，并指出了优化这些能力的未来方向。数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/THUDM/ComplexFuncBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/THUDM/ComplexFuncBench找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10132v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>增强大型语言模型（LLM）通过实时API可以生成更准确和最新的响应。然而，由于数据收集和评估的复杂性，在真实场景中评估LLM的函数调用能力仍处于探索阶段。本工作引入了一个复杂函数调用基准测试（ComplexFuncBench），涵盖五种真实场景下的复杂函数调用。相较于现有基准测试，ComplexFuncBench包含多步骤和受约束的函数调用，这需要长参数文件、参数值推理和128k长上下文。此外，我们还提出了一个自动评估框架（ComplexEval），用于定量评估复杂函数调用任务。通过全面的实验，我们展示了当前LLM在函数调用方面的不足，并指出了优化这些能力的未来方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM通过实时API可提升生成响应的准确性和实时性。</li>
<li>评估LLM的函数调用能力在真实场景中仍然是一个被忽视的领域。</li>
<li>引入了一个新的复杂函数调用基准测试（ComplexFuncBench），涵盖五种真实场景。</li>
<li>ComplexFuncBench包括多步骤和受约束的函数调用，要求高级参数处理和上下文理解。</li>
<li>提出了一个自动评估框架（ComplexEval）来定量评估复杂函数调用任务。</li>
<li>实验表明，当前LLM在函数调用方面存在不足。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10132">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c36593a4333a5e8adf37667697072132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7cf60c59091aad72dc693ab1a0a6431.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76159489d0e2c7d08723fd6d95a47976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-421fb73d7c90325d487f51b240002992.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PaSa-An-LLM-Agent-for-Comprehensive-Academic-Paper-Search"><a href="#PaSa-An-LLM-Agent-for-Comprehensive-Academic-Paper-Search" class="headerlink" title="PaSa: An LLM Agent for Comprehensive Academic Paper Search"></a>PaSa: An LLM Agent for Comprehensive Academic Paper Search</h2><p><strong>Authors:Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, Weinan E</strong></p>
<p>We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholarly queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at <a target="_blank" rel="noopener" href="https://github.com/bytedance/pasa">https://github.com/bytedance/pasa</a>. </p>
<blockquote>
<p>我们推出了PaSa，这是一款由大型语言模型驱动的高级论文搜索代理。PaSa可以自主做出一系列决策，包括调用搜索工具、阅读论文和选择相关参考文献，以获取针对复杂学术查询的全面和准确结果。我们使用强化学习和合成数据集AutoScholarQuery对PaSa进行了优化，该数据集包含3.5万条精细的学术查询和相应论文，来源于顶级人工智能会议出版物。此外，我们还开发了RealScholarQuery基准测试，收集真实世界的学术查询，以评估PaSa在更现实场景中的性能。尽管是在合成数据上训练的，但PaSa在RealScholarQuery上的表现显著优于现有基线，包括Google、Google Scholar、针对改述查询的GPT-4结合的Google搜索、chatGPT（启用搜索功能的GPT-4o）、GPT-o1以及通过提示GPT-4o实现的PaSa-GPT-4o。值得注意的是，PaSa-7B在召回率@20和召回率@50方面分别超过了最佳Google基线——GPT-4结合的Google搜索达37.78%和39.90%。同时，它的召回率也超过了PaSa-GPT-4o达30.36%，精确度提高了4.25%。模型、数据集和代码均可在<a target="_blank" rel="noopener" href="https://github.com/bytedance/pasa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bytedance/pasa找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10120v1">PDF</a> </p>
<p><strong>摘要</strong><br>帕萨是一个强大的论文搜索智能代理，可以自主做出包括调用搜索工具、阅读论文和筛选相关文献等决策，以获取针对复杂学术查询的全面准确结果。帕萨的优化通过使用强化学习与合成数据集AutoScholarQuery进行，该数据集包含3.5万精细的学术查询和相应论文，来源于顶级人工智能会议出版物。此外，为了评估帕萨在更真实场景中的性能，开发了RealScholarQuery基准测试。尽管是在合成数据上训练的，但帕萨在RealScholarQuery上的表现显著优于现有基线，包括谷歌、谷歌学术、谷歌与GPT-4结合的查询、ChatGPT（搜索启用的GPT-4o）、GPT-o1以及由GPT-4o实现的帕萨。特别是帕萨-7B在召回率@20和召回率@50方面分别超越了最佳谷歌基线Google with GPT-4o 37.78%和39.90%。模型、数据集和代码可通过<a target="_blank" rel="noopener" href="https://github.com/bytedance/pasa%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bytedance/pasa获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>帕萨是一个基于大型语言模型的先进论文搜索代理，可自主进行一系列决策以获取全面准确的学术查询结果。</li>
<li>使用合成数据集AutoScholarQuery进行强化学习优化。</li>
<li>开发RealScholarQuery基准测试以评估帕萨在更真实场景中的性能。</li>
<li>帕萨在RealScholarQuery上的表现优于多个基线，包括谷歌及其与GPT-4结合的版本。</li>
<li>帕萨-7B在召回率方面显著超越了最佳谷歌基线。</li>
<li>帕萨模型、数据集和代码已公开发布在GitHub上。</li>
<li>帕萨有望为复杂的学术查询提供高效且准确的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10120">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c28ad991b12190da0ee12473efc65a10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2e2f47dfee4f53354937aaed3c61cfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca2459afc77830fe8fdf70248257e4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0171d07dbfa0adcd2de7c56610d444cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e42fefdbaf384622dbb01f15ab9cd02.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-LLM-Test-Time-Compute-via-Search-Tasks-LLM-Profiling-Search-Algorithms-and-Relevant-Frameworks"><a href="#A-Survey-on-LLM-Test-Time-Compute-via-Search-Tasks-LLM-Profiling-Search-Algorithms-and-Relevant-Frameworks" class="headerlink" title="A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,   Search Algorithms, and Relevant Frameworks"></a>A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,   Search Algorithms, and Relevant Frameworks</h2><p><strong>Authors:Xinzhe Li</strong></p>
<p>LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects (task definition, LLM profiling, and search procedures), making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. In this survey, we provide a comprehensive technical review that unifies task definitions and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For further details and ongoing updates, please refer to our GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md">https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md</a> </p>
<blockquote>
<p>LLM测试时间计算（或LLM推理）通过搜索已经成为一个有前景的研究领域，发展迅速。然而，当前框架往往从三个关键方面（任务定义、LLM分析以及搜索程序）采用不同的观点，使得直接比较变得具有挑战性。此外，所使用的搜索算法经常偏离标准实现，并且它们的特定特性并未得到彻底说明。在本次调查中，我们提供了一项全面的技术回顾，统一了任务定义，并提供了模块化定义的LLM分析和搜索程序。这些定义能够精确比较各种LLM推理框架，同时突出其与常规搜索算法的偏离。我们还讨论了这些方法的应用性、性能和效率。更多详细信息和最新更新，请参阅我们的GitHub仓库：<a target="_blank" rel="noopener" href="https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md">https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM测试时的计算（或LLM推理）通过搜索已成为具有发展前景的研究领域，发展迅速。然而，当前的研究框架在任务定义、LLM性能分析和搜索程序方面往往采用不同观点，使得直接比较具有挑战性。本文提供全面的技术回顾，统一任务定义，模块化定义LLM性能分析和搜索程序，使不同LLM推理框架之间的精确比较成为可能，同时突出其与常规搜索算法的差异性。我们还讨论了这些方法的应用性、性能和效率。有关更多详细信息及最新更新，请查阅我们的GitHub仓库：<a target="_blank" rel="noopener" href="https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md">GitHub链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM测试时间计算已成为热门研究领域。</li>
<li>当前框架在任务定义、LLM性能分析和搜索程序方面存在多样性。</li>
<li>缺乏直接比较不同LLM推理框架的标准。</li>
<li>本文提供全面的技术回顾，统一任务定义，模块化定义LLM性能分析和搜索程序。</li>
<li>框架之间的精确比较成为可能。</li>
<li>突出常规搜索算法与LLM推理框架之间的差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10069">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84a73065d9cb0d960bbc3f2525f4f6ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b293b407073da014db4609ec9484c608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f96f27009ce2068143a24f4076f5783c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a44bd8273705737240dca3a96fa6bd6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FiLo-Zero-Few-Shot-Anomaly-Detection-by-Fused-Fine-Grained-Descriptions-and-Deformable-Localization"><a href="#FiLo-Zero-Few-Shot-Anomaly-Detection-by-Fused-Fine-Grained-Descriptions-and-Deformable-Localization" class="headerlink" title="FiLo++: Zero-&#x2F;Few-Shot Anomaly Detection by Fused Fine-Grained   Descriptions and Deformable Localization"></a>FiLo++: Zero-&#x2F;Few-Shot Anomaly Detection by Fused Fine-Grained   Descriptions and Deformable Localization</h2><p><strong>Authors:Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang</strong></p>
<p>Anomaly detection methods typically require extensive normal samples from the target class for training, limiting their applicability in scenarios that require rapid adaptation, such as cold start. Zero-shot and few-shot anomaly detection do not require labeled samples from the target class in advance, making them a promising research direction. Existing zero-shot and few-shot approaches often leverage powerful multimodal models to detect and localize anomalies by comparing image-text similarity. However, their handcrafted generic descriptions fail to capture the diverse range of anomalies that may emerge in different objects, and simple patch-level image-text matching often struggles to localize anomalous regions of varying shapes and sizes. To address these issues, this paper proposes the FiLo++ method, which consists of two key components. The first component, Fused Fine-Grained Descriptions (FusDes), utilizes large language models to generate anomaly descriptions for each object category, combines both fixed and learnable prompt templates and applies a runtime prompt filtering method, producing more accurate and task-specific textual descriptions. The second component, Deformable Localization (DefLoc), integrates the vision foundation model Grounding DINO with position-enhanced text descriptions and a Multi-scale Deformable Cross-modal Interaction (MDCI) module, enabling accurate localization of anomalies with various shapes and sizes. In addition, we design a position-enhanced patch matching approach to improve few-shot anomaly detection performance. Experiments on multiple datasets demonstrate that FiLo++ achieves significant performance improvements compared with existing methods. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/CASIA-IVA-Lab/FiLo">https://github.com/CASIA-IVA-Lab/FiLo</a>. </p>
<blockquote>
<p>异常检测方法通常需要目标类别的大量正常样本进行训练，这在需要快速适应的场景（如冷启动）中限制了其适用性。零样本和少样本异常检测不需要提前获取目标类别的标记样本，因此成为了一个有前景的研究方向。现有的零样本和少样本方法常常利用强大的多模态模型，通过比较图像文本相似性来检测和定位异常。然而，它们的手工通用描述无法捕捉不同对象中可能出现的各种异常，而简单的补丁级别的图像文本匹配往往难以定位形状和大小各异的异常区域。针对这些问题，本文提出了FiLo++方法，该方法由两个关键组件组成。第一个组件Fused Fine-Grained Descriptions（FusDes）利用大型语言模型为每个对象类别生成异常描述，结合了固定和可学习的提示模板，并应用了运行时提示过滤方法，产生更准确和任务特定的文本描述。第二个组件Deformable Localization（DefLoc）将视觉基础模型Grounding DINO与位置增强的文本描述和多尺度可变形跨模态交互（MDCI）模块集成，能够实现各种形状和大小的异常准确定位。此外，我们设计了一种位置增强的补丁匹配方法，以提高少样本异常检测的性能。在多个数据集上的实验表明，与现有方法相比，FiLo++取得了显著的性能改进。代码将在<a target="_blank" rel="noopener" href="https://github.com/CASIA-IVA-Lab/FiLo%E4%B8%8A%E6%8F%9B%E4%BA%8B%E3%80%82">https://github.com/CASIA-IVA-Lab/FiLo上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10067v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文针对零样本和少样本异常检测的问题，提出了FiLo++方法，包括Fused Fine-Grained Descriptions（FusDes）和Deformable Localization（DefLoc）两个关键组件。前者利用大型语言模型生成异常描述，结合固定和可学习的提示模板，并应用运行时提示过滤方法，生成更准确的任务特定文本描述。后者结合了视觉基础模型Grounding DINO与位置增强的文本描述和多尺度可变形跨模态交互（MDCI）模块，能准确定位各种形状和大小的异常。实验证明，FiLo++相较于现有方法具有显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异常检测方法通常需要大量目标类别的正常样本进行训练，这在需要快速适应的场景（如冷启动）中限制了其应用。</li>
<li>零样本和少样本异常检测不需要提前获得目标类别的标签样本，成为有前景的研究方向。</li>
<li>现有方法利用多模态模型通过图像文本相似性检测与定位异常，但通用描述无法捕捉不同对象中多样的异常。</li>
<li>FiLo++方法包括Fused Fine-Grained Descriptions（FusDes）和Deformable Localization（DefLoc）两个关键组件，分别用于生成更准确的异常描述和准确定位异常。</li>
<li>FusDes利用大型语言模型生成异常描述，并结合固定和可学习提示模板，提高描述的准确性。</li>
<li>DefLoc结合了视觉基础模型与位置增强的文本描述，以及多尺度可变形跨模态交互模块，实现异常精准定位。</li>
<li>实验证明FiLo++在多个数据集上较现有方法具有显著性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9b3929f400dd861e57bad3fed550363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d2dbd358010dc6a76ae4702e1d04d57.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Routing-in-Mixture-of-Experts-A-Novel-Framework-for-Trading"><a href="#LLM-Based-Routing-in-Mixture-of-Experts-A-Novel-Framework-for-Trading" class="headerlink" title="LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading"></a>LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading</h2><p><strong>Authors:Kuan-Ming Liu, Ming-Chih Lo</strong></p>
<p>Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks. </p>
<blockquote>
<p>最近深度学习和大语言模型（LLM）的进步促进了专家混合（MoE）机制在股票投资领域的应用。虽然这些模型已经表现出了有前景的交易性能，但它们通常是单模态的，忽略了其他模态（如文本数据）中丰富的信息。此外，基于传统神经网络的路由器选择机制未能考虑上下文和现实世界细微差别，导致专家选择不理想。为了克服这些局限性，我们提出了LLMoE这一新型框架，该框架采用LLM作为MoE架构中的路由器。具体来说，我们用LLM替代了基于常规神经网络的路由器，利用其丰富的世界知识和推理能力，根据历史价格数据和股票新闻选择专家。这种方法提供了更有效和可解释的选择机制。我们在多模态现实世界股票数据集上的实验表明，LLMoE的表现优于最新的MoE模型和其他深度神经网络方法。此外，LLMoE灵活的架构可轻松适应各种下游任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09636v2">PDF</a> Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging   Innovations in Finance, Social Media, and Crime Prevention</p>
<p><strong>Summary</strong><br>     深度学习与大型语言模型（LLM）的最新进展促进了混合专家（MoE）机制在股票投资领域的应用。针对传统神经网络路由器选择机制忽视上下文和现实世界细微差别的局限性，我们提出LLMoE框架，采用LLM作为MoE架构中的路由器，基于历史价格数据和股票新闻选择专家。实验证明，LLMoE在模态现实股票数据集上的表现优于最先进MoE模型和其他深度神经网络方法，且架构灵活，易于适应各种下游任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习与大型语言模型（LLM）的最新发展促进了混合专家（MoE）机制在股票投资中的应用。</li>
<li>传统神经网络路由器选择机制存在局限性，无法充分考虑上下文和现实世界细微差别。</li>
<li>LLMoE框架采用LLM作为MoE架构中的路由器，基于历史价格数据和股票新闻选择专家。</li>
<li>LLMoE在模态现实股票数据集上的表现优于其他方法。</li>
<li>LLMoE架构灵活，易于适应各种下游任务。</li>
<li>LLM的广泛应用知识及其推理能力在股票投资领域具有潜在价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09636">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a8b5e149ad8bd18b0595d298238f325.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3a42e88b738b756ac1e6357b982a80a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c476004962a7c3351e42d261325042.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e532199298767c7cd68ed582970b40fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237f0e21d645cca6f05c865b282dd8ce.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MVTamperBench-Evaluating-Robustness-of-Vision-Language-Models"><a href="#MVTamperBench-Evaluating-Robustness-of-Vision-Language-Models" class="headerlink" title="MVTamperBench: Evaluating Robustness of Vision-Language Models"></a>MVTamperBench: Evaluating Robustness of Vision-Language Models</h2><p><strong>Authors:Amit Agarwal, Srikant Panda, Angeline Charles, Bhargava Kumar, Hitesh Patel, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae</strong></p>
<p>Multimodal Large Language Models (MLLMs) have driven major advances in video understanding, yet their vulnerability to adversarial tampering and manipulations remains underexplored. To address this gap, we introduce MVTamperBench, a benchmark that systematically evaluates MLLM robustness against five prevalent tampering techniques: rotation, masking, substitution, repetition, and dropping. Built from 3.4K original videos-expanded to over 17K tampered clips spanning 19 video tasks.   MVTamperBench challenges models to detect manipulations in spatial and temporal coherence. We evaluate 45 recent MLLMs from 15+ model families, revealing substantial variability in resilience across tampering types and showing that larger parameter counts do not necessarily guarantee robustness. MVTamperBench sets a new benchmark for developing tamper-resilient MLLM in safety-critical applications, including detecting clickbait, preventing harmful content distribution, and enforcing policies on media platforms. We release all code and data to foster open research in trustworthy video understanding.   Code: <a target="_blank" rel="noopener" href="https://amitbcp.github.io/MVTamperBench/">https://amitbcp.github.io/MVTamperBench/</a> Data: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Srikant86/MVTamperBench">https://huggingface.co/datasets/Srikant86/MVTamperBench</a> </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在视频理解方面取得了重大进展，但其在面临对抗性干扰和操纵时的脆弱性仍然鲜有研究。为了弥补这一空白，我们引入了MVTamperBench基准测试平台，该平台系统地评估了MLLM对五种常见干扰技术的鲁棒性：旋转、遮挡、替换、重复和丢弃。它建立在原始视频的扩张集上，超过有攻击行为的视频剪辑有三千四百万条视频跨越十九种视频任务。MVTamperBench挑战模型检测空间和时间连贯性的操纵行为。我们评估了来自十五个以上的模型的四十五个近期的大型语言模型，发现不同类型干扰下模型的韧性存在较大差异，而且更大的参数数量并不一定保证模型的鲁棒性。MVTamperBench为在安全关键应用中开发防干扰大型语言模型设立了新的基准线，包括检测标题欺诈、防止有害内容传播以及在媒体平台上执行政策等。我们公开所有代码和数据以促进可信视频理解的开放研究。代码：<a target="_blank" rel="noopener" href="https://amitbcp.github.io/MVTamperBench/">https://amitbcp.github.io/MVTamperBench/</a> 数据集：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Srikant86/MVTamperBench">https://huggingface.co/datasets/Srikant86/MVTamperBench</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19794v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMs在视频理解方面取得了重大进展，但其对敌对干扰和操作的脆弱性仍未被充分探索。为填补这一空白，我们推出了MVTamperBench基准测试，该测试系统地评估了MLLM对五种常见干扰技术的稳健性：旋转、遮挡、替换、重复和丢弃。该基准测试包含来自原始视频的超过17K个干扰片段，涵盖了涵盖视频任务的超过内容评估超过拓展了对于应对各种时空干扰视频内容广泛的任务。我们对包括超过模型家族在内的近期MLLM进行了评估，发现不同干扰类型之间存在显著差异，且参数数量多的模型不一定具备更好的稳健性。MVTamperBench为开发稳健的MLLM设定了新的基准线，适用于安全关键型应用，如检测标题欺诈、防止有害内容传播以及在媒体平台上执行政策等。我们公开了所有代码和数据，以促进在可信视频理解方面的开放研究。为改善机器学习模型的可靠性，进一步提升了领域的应用前景和价值。希望其促进多媒体技术的可持续性发展，为未来带来更好的多媒体内容应用前景。我们相信这一工具将促进研究社区的发展并推动多媒体领域的进步。欢迎更多研究者参与合作和讨论。感兴趣的研究者可通过访问以下链接获取更多详细信息。感兴趣的科研者可以通过<a target="_blank" rel="noopener" href="https://amitbcp.github.io/MVTamperBench/%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E4%BA%86%E8%A7%A3%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%E4%BB%A3%E7%A0%81%E8%8E%B7%E5%8F%96%E5%8F%AF%E9%80%9A%E8%BF%87">https://amitbcp.github.io/MVTamperBench/进行访问了解详细信息代码获取可通过</a> <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Srikant86/MVTamperBench%E8%BF%9B%E8%A1%8C%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BF%A1%E6%81%AF%E5%B9%B6%E5%8F%82%E4%B8%8E%E5%90%88%E4%BD%9C%E7%A0%94%E7%A9%B6%E5%85%B1%E5%90%8C%E6%8E%A8%E5%8A%A8%E5%A4%9A%E5%AA%92%E4%BD%93%E9%A2%86%E5%9F%9F%E7%9A%84%E7%A8%B3%E5%81%A5%E6%80%A7%E5%8F%91%E5%B1%95%E5%B9%B6%E5%88%9B%E9%80%A0%E6%9B%B4%E5%AE%89%E5%85%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E7%8E%AF%E5%A2%83%E3%80%82%E3%80%82MVTamperBench%E5%B0%86%E4%B8%BA%E5%A4%9A%E5%AA%92%E4%BD%93%E9%A2%86%E5%9F%9F%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E6%B3%A8%E5%85%A5%E6%96%B0%E7%9A%84%E6%B4%BB%E5%8A%9B%EF%BC%8C%E4%BF%83%E8%BF%9B%E6%A8%A1%E5%9E%8B%E5%AF%B9%E5%A4%8D%E6%9D%82%E8%A7%86%E9%A2%91%E5%86%85%E5%AE%B9%E7%9A%84%E7%A8%B3%E5%81%A5%E6%80%A7%E8%BF%9B%E6%AD%A5%E5%B9%B6%E5%8A%A9%E5%8A%9B%E5%88%9B%E9%80%A0%E6%9B%B4%E5%AE%89%E5%85%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E7%8E%AF%E5%A2%83%E3%80%82%E6%AD%A4%E5%A4%96%E5%AE%83%E7%9A%84%E5%BC%95%E5%85%A5%E8%BF%98%E5%B0%86%E6%9E%81%E5%A4%A7%E5%9C%B0%E4%BF%83%E8%BF%9B%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E5%8F%91%E5%B1%95%E6%8E%A8%E5%8A%A8%E5%A4%9A%E5%AA%92%E4%BD%93%E6%8A%80%E6%9C%AF%E7%9A%84%E5%88%9B%E6%96%B0%E5%B9%B6%E6%8E%A8%E5%8A%A8%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E8%BF%9B%E6%AD%A5%E3%80%82%E6%88%91%E4%BB%AC%E7%9B%B8%E4%BF%A1%E8%AF%A5%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%B0%86%E6%88%90%E4%B8%BA%E5%A4%9A%E5%AA%92%E4%BD%93%E9%A2%86%E5%9F%9F%E7%9A%84%E9%87%8D%E8%A6%81%E9%87%8C%E7%A8%8B%E7%A2%91%E4%B9%8B%E4%B8%80%E6%8E%A8%E5%8A%A8%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%E7%9A%84%E6%8C%81%E7%BB%AD%E5%8F%91%E5%B1%95%E5%92%8C%E5%88%9B%E6%96%B0%E3%80%82">https://huggingface.co/datasets/Srikant86/MVTamperBench进行获取数据集信息并参与合作研究共同推动多媒体领域的稳健性发展并创造更安全的网络环境。。MVTamperBench将为多媒体领域的未来发展注入新的活力，促进模型对复杂视频内容的稳健性进步并助力创造更安全的网络环境。此外它的引入还将极大地促进该领域的发展推动多媒体技术的创新并推动视频理解的可靠性进步。我们相信该基准测试将成为多媒体领域的重要里程碑之一推动相关技术的持续发展和创新。</a></p>
<p><strong>Key Takeaways</strong></p>
<p>一、MLLMs在视频理解方面存在脆弱性，需要评估其对抗干扰的稳健性。为此引入MVTamperBench基准测试评估稳健性很重要具有五大重点：检测各种视频干扰的能力涉及对大量数据集的应用采用时空鲁棒技术进行深入探讨机器学习模型的性能和结构特性包括不同模型家族的差异对干扰类型的响应等，可针对模型的实际应用效果进行分析与改进为多媒体领域的未来发展注入新的活力评估机器学习模型对不同干扰技术的应对能力为后续模型的改进方向提供依据参考和支持并且能够为开发安全关键的MLLM提供基准线指导方向并促进相关研究的进展和合作研究共同推动多媒体领域的稳健性发展。同时评估多种模型对于各种干扰的抵抗能力可为开发更加鲁棒的模型提供思路和指导帮助促进技术的进一步发展和提升应用场景的不断拓展也为后续的算法设计和改进提供了宝贵的经验和参考借鉴其是视频内容理解的可靠性的重要衡量标准之一具有广泛的应用前景和潜力价值；同时它的引入也将极大地促进多媒体领域的可持续性发展推进技术创新并在多媒体技术领域不断发挥重要作用并为其他领域带来有益影响通过提出基于视觉的语言感知鲁棒性评价的方法大大增强了感知一致性具有稳健的技术效果和丰富的社会价值将会应用于媒体信息的有效筛选领域创造更多的实际价值推进行业可持续发展与创新的应用潜力体现研究意义在多种行业领域内进行广泛实践探索及深度分析发挥领域核心研究价值的展现更多深入且全面更细致全面挖掘行业发展方向做出坚实有力贡献从检测模型的仿真性能提升到综合实际运用再到科研平台不断完善技术创新体现在研究中的每一个环节当中并在多个行业领域内发挥着重要的作用提升科研团队的成果产出的同时也能切实有效赋能多个产业实际应用发挥产业数字化转型的智能推动力综合使用全面扎实有力推动行业技术革新与发展进步为行业带来更加广阔的前景和发展空间。具体来说包括以下几点：</p>
<p>二、MLLMs对各种视频干扰技术的响应存在差异且这种差异可能影响到其实际应用中的性能与安全性针对各种干扰技术进行分析评估以揭示模型的弱点并提供改进方向成为研究的重点。通过对模型进行对抗样本攻击与训练从而加强模型的抗干扰能力提出针对性更强的训练方案为行业培养高质量高水平的深度学习技术人员而强有力的解决方案是研究未来产业发展技术支撑的重要方向之一提升研究质量和技术水平进而提升行业的核心竞争力对于行业的长远发展具有重要意义和广阔前景提升视频理解模型的抗干扰能力提高其在各种应用场景下的性能与可靠性同时降低因模型脆弱性带来的风险与损失对于多媒体领域的发展至关重要具有广泛的应用前景和潜在经济价值为推动相关领域的技术进步和产业升级提供重要的支持和指导提出关键思路并加以实施开展更高水平的相关技术应用做出更为出色的工作为推动整个行业的进步和发展贡献自己的力量实现技术创新的跨越式发展同时对于提高公众信息的安全性和可信度也有着重要的现实意义和应用价值增强社会公众对网络信息的信任度和安全感从而促进整个社会的和谐稳定发展推动行业技术进步和创新发展增强社会公众对网络信息的信任度和安全感成为推动社会和谐稳定发展的重要因素之一为解决多媒体数据安全和虚假信息泛滥等问题提供有力支持并为行业的持续健康发展提供坚实的保障对理解计算机技术实际应用与系统控制下的应用领域更带来可靠高效的路径从仿真转向落地的一个过程具有重要意义和应用价值提升社会公众对网络信息的信任度和安全感也是互联网应用中的重要目标之一增强网络信息的真实性和可信度对于维护社会稳定和促进经济发展具有十分重要的作用和意义。具体来说包括以下几点：</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19794">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-00a1e059fb63db499875827f1849fe6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bebe182346f8d190ceb4a609303d5f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-867478dc7e6d115b5af9759f5fe1b434.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c69ffbba9ff7c246f2a9e133ba05e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5fb4ff128881240eae0c68c5e03fe4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcdca311c659e4a39a41cab1d35da740.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AceMath-Advancing-Frontier-Math-Reasoning-with-Post-Training-and-Reward-Modeling"><a href="#AceMath-Advancing-Frontier-Math-Reasoning-with-Post-Training-and-Reward-Modeling" class="headerlink" title="AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward   Modeling"></a>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward   Modeling</h2><p><strong>Authors:Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping</strong></p>
<p>In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We release model weights, training data, and evaluation benchmarks at: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/acemath">https://research.nvidia.com/labs/adlr/acemath</a> </p>
<blockquote>
<p>本文介绍了AceMath，这是一系列前沿的数学模型，擅长解决复杂的数学问题，以及高效的奖励模型，能够评估生成的解决方案并可靠地识别正确的解决方案。为了开发指令调优的数学模型，我们提出了一种监督微调（SFT）过程，首先在各种通用领域实现具有竞争力的性能，然后使用精心挑选的提示和合成生成的响应进行数学领域的目标微调。AceMath-72B-Instruct模型在性能上大大超过了Qwen2.5-Math-72B-Instruct、GPT-4o和Claude-3.5 Sonnet。为了开发专业化的数学奖励模型，我们首先构建了AceMath-RewardBench，这是一个全面且稳健的基准测试，用于评估不同问题和难度级别的数学奖励模型。之后，我们提出了一种构建数学奖励模型的系统的方法。AceMath-72B-RM模型一直优于最新的奖励模型。此外，当将AceMath-72B-Instruct与AceMath-72B-RM相结合时，我们在数学推理基准测试中获得了最高的平均rm@8分数。我们发布的模型权重、训练数据和评估基准测试可访问：<a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/acemath">https://research.nvidia.com/labs/adlr/acemath</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15084v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AceMath是一套前沿的数学模型，擅长解决复杂的数学问题，并配备了高效的奖励模型，能评估生成的解决方案并准确识别正确的答案。该研究通过监督微调（SFT）过程开发了指令调优的数学模型，首先在全域实现竞争力性能，然后针对数学领域进行定向微调。此外，该研究构建了AceMath-RewardBench基准测试，用于评估数学奖励模型，并系统构建了数学奖励模型。将数学模型与奖励模型结合，实现了高平均rm@8分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AceMath是一系列优秀的数学模型，擅长解决复杂的数学问题。</li>
<li>AceMath配备了高效的奖励模型，能够评估生成的解决方案并准确识别正确答案。</li>
<li>研究人员通过监督微调（SFT）过程，提高了数学模型的性能。</li>
<li>AceMath-72B-Instruct模型在多个数学推理基准测试中表现出卓越性能。</li>
<li>构建了AceMath-RewardBench基准测试，用于评估数学奖励模型。</li>
<li>系统构建了数学奖励模型AceMath-72B-RM，其性能超过现有奖励模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aafb33ce1da2b660008c91e998691d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da6e8dd4dc08530b69156c91cc49017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9493377d543f27458a2006d4b41e183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ad5e8004600d7e99a771c8211935a86.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Automatic-Database-Configuration-Debugging-using-Retrieval-Augmented-Language-Models"><a href="#Automatic-Database-Configuration-Debugging-using-Retrieval-Augmented-Language-Models" class="headerlink" title="Automatic Database Configuration Debugging using Retrieval-Augmented   Language Models"></a>Automatic Database Configuration Debugging using Retrieval-Augmented   Language Models</h2><p><strong>Authors:Sibei Chen, Ju Fan, Bin Wu, Nan Tang, Chao Deng, Pengyi Wang, Ye Li, Jian Tan, Feifei Li, Jingren Zhou, Xiaoyong Du</strong></p>
<p>Database management system (DBMS) configuration debugging, e.g., diagnosing poorly configured DBMS knobs and generating troubleshooting recommendations, is crucial in optimizing DBMS performance. However, the configuration debugging process is tedious and, sometimes challenging, even for seasoned database administrators (DBAs) with sufficient experience in DBMS configurations and good understandings of the DBMS internals (e.g., MySQL or Oracle). To address this difficulty, we propose Andromeda, a framework that utilizes large language models (LLMs) to enable automatic DBMS configuration debugging. Andromeda serves as a natural surrogate of DBAs to answer a wide range of natural language (NL) questions on DBMS configuration issues, and to generate diagnostic suggestions to fix these issues. Nevertheless, directly prompting LLMs with these professional questions may result in overly generic and often unsatisfying answers. To this end, we propose a retrieval-augmented generation (RAG) strategy that effectively provides matched domain-specific contexts for the question from multiple sources. They come from related historical questions, troubleshooting manuals and DBMS telemetries, which significantly improve the performance of configuration debugging. To support the RAG strategy, we develop a document retrieval mechanism addressing heterogeneous documents and design an effective method for telemetry analysis. Extensive experiments on real-world DBMS configuration debugging datasets show that Andromeda significantly outperforms existing solutions. </p>
<blockquote>
<p>数据库管理系统（DBMS）配置调试对于优化DBMS性能至关重要，例如诊断配置不当的DBMS旋钮并生成故障排除建议。然而，配置调试过程既繁琐又有时具有挑战性，即使是经验丰富的数据库管理员（DBA）在DBMS配置方面拥有足够的经验并对DBMS内部有良好理解（例如MySQL或Oracle）。为了解决这一难题，我们提出了使用大型语言模型（LLM）自动进行DBMS配置调试的框架Andromeda。Andromeda可以作为DBA的自然替代者，回答有关DBMS配置问题的广泛自然语言（NL）问题，并生成解决这些问题的诊断建议。然而，直接使用这些问题提示LLM可能会得到过于通用且往往令人不满意的答案。为此，我们提出了一种增强检索生成（RAG）策略，该策略可以有效地从多个来源为问题提供匹配的特定领域上下文。这些来源包括相关的历史问题、故障排除手册和DBMS遥测数据，从而大大提高了配置调试的性能。为了支持RAG策略，我们开发了一种解决异构文档的文件检索机制，并设计了一种有效的遥测分析方法。在真实世界的DBMS配置调试数据集上的大量实验表明，Andromeda显著优于现有解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07548v2">PDF</a> </p>
<p><strong>摘要</strong><br>数据库管理系统（DBMS）配置调试对于优化DBMS性能至关重要。然而，配置调试过程繁琐且有时具有挑战性，即使对于经验丰富的数据库管理员（DBA）也是如此。为解决此问题，我们提出使用大型语言模型（LLM）的Andromeda框架，实现DBMS配置的自动调试。Andromeda可替代DBA回答有关DBMS配置的各类自然语言问题，并生成诊断建议以解决问题。但是，直接使用这些问题提示LLM可能会得到过于笼统且常常不令人满意的答案。为此，我们提出了一种增强生成（RAG）策略，该策略通过从多个来源为问题提供匹配的特定领域上下文，显著提高了配置调试的性能。这些来源包括相关历史问题、故障排除手册和DBMS遥测数据。为支持RAG策略，我们开发了一种处理异构文档的文档检索机制，并设计了一种有效的遥测分析方法。在真实世界的DBMS配置调试数据集上的广泛实验表明，Andromeda显著优于现有解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>数据库管理系统（DBMS）配置调试对于性能优化至关重要，但过程繁琐且具有挑战性。</li>
<li>提出使用大型语言模型（LLM）的Andromeda框架进行自动DBMS配置调试。</li>
<li>Andromeda能够回答有关DBMS配置的各类自然语言问题并生成诊断建议。</li>
<li>直接提示LLM可能会得到不满意的答案，因此提出一种检索增强生成（RAG）策略。</li>
<li>RAG策略通过从多个来源提供匹配的特定领域上下文来提高配置调试的性能。</li>
<li>Andromeda支持处理异构文档的文档检索机制以及有效的遥测分析方法。</li>
<li>在真实世界数据集上的实验表明，Andromeda在DBMS配置调试方面显著优于现有解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07548">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26d417a50447e3039c7f011e3a15f303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-581018f4e166f314b7fc8f9bc0a61682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722cd91f2ccb543f5814105e5e8d0940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-357f9e71814c9df52a3362ee38242afd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BatchLLM-Optimizing-Large-Batched-LLM-Inference-with-Global-Prefix-Sharing-and-Throughput-oriented-Token-Batching"><a href="#BatchLLM-Optimizing-Large-Batched-LLM-Inference-with-Global-Prefix-Sharing-and-Throughput-oriented-Token-Batching" class="headerlink" title="BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix   Sharing and Throughput-oriented Token Batching"></a>BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix   Sharing and Throughput-oriented Token Batching</h2><p><strong>Authors:Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng</strong></p>
<p>Large language models (LLMs) increasingly play an important role in a wide range of information processing and management tasks. Many of these tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix between requests. The KV context that are about to be reused may prematurely evicted with the implicit cache management. Besides, the streaming oriented systems do not leverage the request-batch information and can not mix the decoding tokens with the prefill chunks to the best for the batched scenarios, and thus fails to saturate the GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks, and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared Attention kernel with horizontal fusion to reduce tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical industry workload under different hardware environments. </p>
<blockquote>
<p>大规模语言模型（LLM）在广泛的信息处理和管理任务中发挥着越来越重要的作用。这些任务中的许多都是批量处理甚至离线处理的，其性能指标是吞吐量。这些任务通常表现出前缀共享的特征，不同的提示输入可以部分显示共同的前缀。然而，现有的LLM推理引擎倾向于优化流式请求，并在支持具有前缀共享特征的大批量任务时显示出局限性。现有解决方案使用基于LRU的缓存来重用请求之间的常见前缀的KV上下文。即将被重用的KV上下文可能会因隐式缓存管理而提前被逐出。此外，流式定向系统没有利用请求批处理信息，无法将解码令牌与预填充块混合到最佳状态，从而无法满足GPU的需求。为了解决上述问题，我们提出了BatchLLM。BatchLLM显式地全局识别公共前缀。共享相同前缀的请求将一起调度，以最佳方式重用KV上下文。BatchLLM重新排序请求，并优先调度解码比例较大的请求，以便更好地将解码令牌与后续的预填充块混合，并采用以内存为中心的令牌批处理来扩大令牌批处理大小，有助于提高GPU利用率。最后，BatchLLM通过水平融合优化了共享前缀的注意力内核，以减少尾部效应和内核启动开销。评估显示，BatchLLM在一组微基准测试和典型行业工作负载的不同硬件环境下，性能优于vLLM和SGLang，加速比为1.3倍至10.8倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03594v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理信息管理任务时表现出日益重要的作用，特别是在批量和离线任务中。针对现有LLM推理引擎在处理具有前缀共享特性的大批量任务时的局限性，本文提出了BatchLLM方案。BatchLLM通过全局识别共同前缀、重新排序请求、优化内存为中心的令牌批处理以及优化前缀共享的关注核来改进现有解决方案。评估结果显示，BatchLLM在微基准测试和典型工业工作负载上分别超越了vLLM和SGLang 1.3至10.8倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs 已在信息处理和 management 任务中扮演重要角色，特别是在批量和离线任务中。</li>
<li>现有LLM推理引擎在处理具有前缀共享特性的大批量任务时存在局限性。</li>
<li>BatchLLM 通过识别共同前缀、重新排序请求、优化内存令牌批处理以及优化关注核来改进现有解决方案。</li>
<li>BatchLLM 提高了 GPU 的利用率。</li>
<li>BatchLLM 在微基准测试和典型工业工作负载上的性能超越了其他模型。</li>
<li>BatchLLM 通过全局识别共同前缀来最大化 KV 上下文的重复使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03594">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7f3b38897cfefdd982abb5ff6b224170.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f25ee743df247ccc8d0415efcc71b9b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e74f15af924c064d7c5952565e64d28c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-048f14c08acf9cdb851834e83758a4bc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VLSBench-Unveiling-Visual-Leakage-in-Multimodal-Safety"><a href="#VLSBench-Unveiling-Visual-Leakage-in-Multimodal-Safety" class="headerlink" title="VLSBench: Unveiling Visual Leakage in Multimodal Safety"></a>VLSBench: Unveiling Visual Leakage in Multimodal Safety</h2><p><strong>Authors:Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao</strong></p>
<p>Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: <a target="_blank" rel="noopener" href="https://hxhcreate.github.io/vlsbench.github.io/">https://hxhcreate.github.io/vlsbench.github.io/</a> </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）的安全问题在各种应用中逐渐成为一个重要的问题。令人惊讶的是，之前的研究表明，使用文本遗忘来对MLLMs进行对齐训练，其安全性能与用图像-文本对训练的MLLMs相当。为了解释这种反直觉的现象，我们发现现有多模态安全基准中存在视觉安全信息泄露（VSIL）问题，即图像中的潜在危险和敏感内容已在文本查询中泄露。通过这种方式，MLLMs可以根据文本查询轻松拒绝这些敏感的文本-图像查询。然而，在现实场景中，没有VSIL的图像-文本对是常见的，但被现有多模态安全基准所忽视。为此，我们构建了防止图像到文本查询的视觉安全泄露的多模态视觉无泄漏安全基准（VLSBench），包含2. 值得注意的是这一研究成果并非空中楼阁或纯理论构想。在我们提供的链接地址上，[网址链接]，你可以查阅到我们的代码和数据。该研究发现对于有视觉安全信息泄露的多模态安全场景而言，文本对齐已足够应对；而对于没有视觉安全信息泄露的多模态安全场景而言，多模态对齐则是一个更有前景的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19939v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注多模态大型语言模型（MLLMs）的安全性问题，特别是在各种应用中的重要性。研究发现，使用文本遗忘对齐MLLMs的方法在安全性能上可与使用图像-文本对训练的MLLMs相媲美。针对这一反常现象，发现了现有多模态安全基准中存在的视觉安全信息泄露（VSIL）问题。因此，构建了防止图像到文本查询的视觉安全泄露的多模态视觉无泄漏安全基准（VLSBench）。实验结果表明，VLSBench对开源和闭源MLLMs构成重大挑战，包括LLaVA、Qwen2-VL等模型。本文表明，在有VSIL的多模态安全场景中，文本对齐足够，而无VSIL的多模态安全场景中，多模态对齐更有前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的安全问题在各类应用中逐渐凸显。</li>
<li>文本遗忘与对齐方法在MLLM安全性能上表现出与图像-文本对训练相当的效能。</li>
<li>存在视觉安全信息泄露（VSIL）问题在多模态安全基准中。</li>
<li>VLSBench基准构建成功，防止了图像到文本查询的视觉安全泄露。</li>
<li>VLSBench对多种MLLMs构成挑战，包括开源和闭源模型。</li>
<li>在存在VSIL的多模态安全场景中，文本对齐方法足够有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19939">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f3d3f6859b931c630c7dba40af4a883.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a229ce8c1c89328a9fd8cfb48e37d07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f415583dccb67d5caa5ab791743bfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6c1c8d17114a7f60bdd5bc658658bc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd990e44b494512c0d157bce9665943b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-917820fa872d3502b8fa65600d4795fb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Mitigating-Sycophancy-in-Decoder-Only-Transformer-Architectures-Synthetic-Data-Intervention"><a href="#Mitigating-Sycophancy-in-Decoder-Only-Transformer-Architectures-Synthetic-Data-Intervention" class="headerlink" title="Mitigating Sycophancy in Decoder-Only Transformer Architectures:   Synthetic Data Intervention"></a>Mitigating Sycophancy in Decoder-Only Transformer Architectures:   Synthetic Data Intervention</h2><p><strong>Authors:Libo Wang</strong></p>
<p>To address the sycophancy problem caused by reinforcement learning from human feedback in large language models, this research applies synthetic data intervention technology to the decoder-only transformer architecture. Based on the research gaps in the existing literature, the researcher designed an experimental process to reduce the tendency of models to cater by generating diversified data, and used GPT4o as an experimental tool for verification. The experiment used 100 true and false questions, and compared the performance of the model trained with synthetic data intervention and the original untrained model on multiple indicators. The results show that the SDI training model supports the technology in terms of accuracy rate and sycophancy rate and has significant effectiveness in reducing sycophancy phenomena. Notably, the data set, experimental process, code and data results have been uploaded to Github, the link is <a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>针对大型语言模型中由人类反馈引起的逢迎问题，本研究将合成数据干预技术应用于仅解码器变换器架构。基于现有文献的研究空白，研究者设计了一个实验过程，以减少模型产生多样化数据以迎合的倾向，并使用GPT4o作为验证的实验工具。实验采用100个真实和虚假问题，比较经过合成数据干预训练和未经训练的原始模型在多指标上的表现。结果表明，SDI训练模型在准确率和奉承率方面支持该技术，并能有效减少奉承现象。值得注意的是，数据集、实验过程、代码和数据结果已上传到Github，链接为<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10156v3">PDF</a> This research is also submitted to OpenReview. The main text is 9   pages (excluding citations), 7 figures, and 1 table</p>
<p><strong>Summary</strong></p>
<p>研究团队针对大型语言模型中由人类反馈强化学习导致的媚俗问题，采用合成数据干预技术应用于仅解码的转换器架构来解决这一问题。该研究填补了现有文献的空白，通过生成多样化数据来减少模型迎合倾向的实验设计，并利用GPT4o作为验证实验工具。实验采用100个真实与虚假问题，比较合成数据干预训练模型与原始未训练模型在多指标上的表现。结果显示，SDI训练模型在准确率和媚俗率方面支持该技术，并在减少媚俗现象方面具有显著效果。相关数据集、实验过程、代码及数据结果已上传至Github平台，链接为[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究使用合成数据干预技术来解决大型语言模型中因强化学习人类反馈导致的媚俗问题。</li>
<li>研究团队针对现有文献的空白，设计了生成多样化数据的实验来减少模型的迎合倾向。</li>
<li>GPT4o被用作验证实验工具，实验涉及100个真实与虚假问题。</li>
<li>对比了合成数据干预训练模型与原始未训练模型在多指标上的表现。</li>
<li>实验结果显示，合成数据干预训练模型在准确率和媚俗率方面表现出显著效果。</li>
<li>数据集、实验过程、代码及数据结果已公开在Github平台上。</li>
<li>该研究为大型语言模型的优化提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-18ca115bff1da38e032a8854a9af1046.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60340bf3d30c022ae26590c05084a159.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dca582572308909a595b858d4a830aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e717fbcd223039c7fe7ce5abddad713f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Jailbreaking-as-a-Reward-Misspecification-Problem"><a href="#Jailbreaking-as-a-Reward-Misspecification-Problem" class="headerlink" title="Jailbreaking as a Reward Misspecification Problem"></a>Jailbreaking as a Reward Misspecification Problem</h2><p><strong>Authors:Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong</strong></p>
<p>The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. This misspecification occurs when the reward function fails to accurately capture the intended behavior, leading to misaligned model outputs. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts in a reward-misspecified space. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark against various target aligned LLMs while preserving the human readability of the generated prompts. Furthermore, these attacks on open-source models demonstrate high transferability to closed-source models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed analysis highlights the unique advantages of the proposed reward misspecification objective compared to previous methods, offering new insights for improving LLM safety and robustness. </p>
<blockquote>
<p>大型语言模型（LLM）的广泛应用引发了人们对其安全性和可靠性的担忧，特别是它们是否容易受到对抗性攻击的脆弱性。在本文中，我们从一个新的角度提出，将这种脆弱性归因于对齐过程中的奖励误指定。当奖励函数无法准确捕捉预期行为时，就会出现这种误指定，从而导致模型输出失准。我们引入了一个度量标准ReGap来量化奖励误指定的程度，并证明了其在检测有害后门提示时的有效性和稳健性。基于这些见解，我们推出了ReMiss系统，这是一个用于自动生成对抗性提示的自动化红队系统，在奖励误指定的空间中工作。ReMiss在AdvBench基准测试上实现了针对各种目标对齐LLM的最新攻击成功率，同时保持了生成提示的可读性。此外，这些对开源模型的攻击表现出对封闭源模型（如GPT-4o）和任务外的高可转移性。详细的分析突出了与以前的方法相比，所提出的奖励误指定目标的独特优势，为改进LLM的安全性和稳健性提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14393v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的广泛应用引发了关于其安全性和可靠性的担忧，特别是它们容易受到对抗性攻击的问题。本文提出一个新视角，将这一漏洞归因于对齐过程中的奖励误指定。奖励误指定发生在奖励函数未能准确捕捉预期行为时，导致模型输出错位。本文引入ReGap指标来量化奖励误指定的程度，并证明了其在检测有害后门提示中的有效性和稳健性。基于这些见解，本文提出了ReMiss系统，一个用于自动化红队对抗的自动系统，在奖励误指定的空间中生成对抗性提示。ReMiss在AdvBench基准测试上实现了针对各种目标对齐LLM的最佳攻击成功率，同时保持了生成提示的可读性。此外，这些对开源模型的攻击显示出对闭源模型如GPT-4o和高可迁移性的优势，以及跨分布任务的优势。这些见解为改进LLM的安全性和稳健性提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLMs）面临安全性和可靠性问题，尤其是容易受到对抗性攻击的问题。</li>
<li>论文提出将LLM的漏洞归因于对齐过程中的奖励误指定。</li>
<li>引入ReGap指标来量化奖励误指定的程度，有效检测有害后门提示。</li>
<li>提出ReMiss系统，用于生成对抗性提示，在奖励误指定的空间中实现自动化红队对抗。</li>
<li>ReMiss系统在AdvBench基准测试上达到最佳攻击成功率，生成提示保持可读性。</li>
<li>对开源模型的攻击表现出对闭源模型和跨分布任务的高迁移性优势。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14393">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ca6a0efd351065ebcf0fd326a4b89d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc3a551a372afd8ba9d416b418f8040f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e02cbfbd6775844ef50f164d5b5a99ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95e2e7a40d668b0422f0e8ba47a2a58a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ed49601dd729f2aa8fa4476a8b78af6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4cdb2cc713c8f8dc13ce18ea7733c544.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-21  Agent4Edu Generating Learner Response Data by Generative Agents for   Intelligent Education Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3f7d1e18027fbbee053a2f950228f8c5.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-19  On the optimal prediction of extreme events in heavy-tailed time series   with applications to solar flare forecasting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17204.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
