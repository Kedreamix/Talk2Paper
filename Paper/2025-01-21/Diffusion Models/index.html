<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-21  DiffStereo High-Frequency Aware Diffusion Model for Stereo Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-70485c60bb15224906c1dd79f5cacbd7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-21-更新"><a href="#2025-01-21-更新" class="headerlink" title="2025-01-21 更新"></a>2025-01-21 更新</h1><h2 id="DiffStereo-High-Frequency-Aware-Diffusion-Model-for-Stereo-Image-Restoration"><a href="#DiffStereo-High-Frequency-Aware-Diffusion-Model-for-Stereo-Image-Restoration" class="headerlink" title="DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image   Restoration"></a>DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image   Restoration</h2><p><strong>Authors:Huiyun Cao, Yuan Shi, Bin Xia, Xiaoyu Jin, Wenming Yang</strong></p>
<p>Diffusion models (DMs) have achieved promising performance in image restoration but haven’t been explored for stereo images. The application of DM in stereo image restoration is confronted with a series of challenges. The need to reconstruct two images exacerbates DM’s computational cost. Additionally, existing latent DMs usually focus on semantic information and remove high-frequency details as redundancy during latent compression, which is precisely what matters for image restoration. To address the above problems, we propose a high-frequency aware diffusion model, DiffStereo for stereo image restoration as the first attempt at DM in this domain. Specifically, DiffStereo first learns latent high-frequency representations (LHFR) of HQ images. DM is then trained in the learned space to estimate LHFR for stereo images, which are fused into a transformer-based stereo image restoration network providing beneficial high-frequency information of corresponding HQ images. The resolution of LHFR is kept the same as input images, which preserves the inherent texture from distortion. And the compression in channels alleviates the computational burden of DM. Furthermore, we devise a position encoding scheme when integrating the LHFR into the restoration network, enabling distinctive guidance in different depths of the restoration network. Comprehensive experiments verify that by combining generative DM and transformer, DiffStereo achieves both higher reconstruction accuracy and better perceptual quality on stereo super-resolution, deblurring, and low-light enhancement compared with state-of-the-art methods. </p>
<blockquote>
<p>扩散模型（DM）在图像修复方面表现优异，但在立体图像修复领域尚未得到广泛应用。扩散模型应用于立体图像修复面临一系列挑战。重建两个图像的需求加剧了扩散模型的计算成本。此外，现有的潜在扩散模型通常关注语义信息，并在潜在压缩过程中去除高频细节，而这正是图像修复所重视的。为了解决上述问题，我们首次尝试在立体图像修复领域引入扩散模型，提出一种高频感知扩散模型——DiffStereo。具体来说，DiffStereo首先学习高质量图像（HQ）的潜在高频表示（LHFR）。然后，在所学空间内训练扩散模型以估计立体图像的LHFR，并将其融合到基于变压器的立体图像修复网络中，为对应的高质量图像提供有益的高频信息。LHFR的分辨率保持与输入图像相同，保留了原始纹理无失真。同时通道压缩减轻了扩散模型的计算负担。此外，当将LHFR集成到修复网络时，我们设计了一种位置编码方案，在修复网络的不同深度提供独特的指导。综合实验证明，通过结合生成性扩散模型和变压器，DiffStereo在立体超分辨率、去模糊和低光增强方面实现了更高的重建精度和更好的感知质量，相较于其他先进方法具有优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10325v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为DiffStereo的高频感知扩散模型，用于立体图像恢复。该模型首次尝试将扩散模型应用于该领域，解决了立体图像恢复中面临的挑战。通过学习高质量图像的高频表示，DiffStereo训练扩散模型估计立体图像的高频表示，然后将其融合到基于变换的图像恢复网络中，从而提供高质量图像的有益高频信息。该模型通过保持高频表示的分辨率与输入图像相同，保留了固有的纹理，并通过减少通道压缩减轻了扩散模型的计算负担。实验证明，结合生成扩散模型和变换器，DiffStereo在立体超分辨率、去模糊和低光增强方面实现了较高的重建精度和更好的感知质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像恢复领域取得了显著的进展，但尚未在立体图像恢复中得到应用。</li>
<li>首次提出名为DiffStereo的高频感知扩散模型，应用于立体图像恢复。</li>
<li>DiffStereo学习高质量图像的高频表示并将其应用于估计立体图像的高频信息。</li>
<li>通过将高频表示融合到基于变换的图像恢复网络中，DiffStereo提高了图像的感知质量。</li>
<li>DiffStereo通过保持高频表示的分辨率与输入图像相同，保留了纹理细节，同时降低了计算成本。</li>
<li>结合生成扩散模型和变换器技术，DiffStereo在立体图像恢复的多个任务上取得了显著的成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10325">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-204a185289149b31ce55aa82b46a048d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cb96af87d3c5b0b98d762963f4b34f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cf617c316da025430a7147c85397c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f113edca4fbb809a14110e31bba6a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12f1c11fe9608dac67a278fba267761f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b0b6f7b2ffcb9806af933a93577086e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiffuEraser-A-Diffusion-Model-for-Video-Inpainting"><a href="#DiffuEraser-A-Diffusion-Model-for-Video-Inpainting" class="headerlink" title="DiffuEraser: A Diffusion Model for Video Inpainting"></a>DiffuEraser: A Diffusion Model for Video Inpainting</h2><p><strong>Authors:Xiaowen Li, Haolan Xue, Peiran Ren, Liefeng Bo</strong></p>
<p>Recent video inpainting algorithms integrate flow-based pixel propagation with transformer-based generation to leverage optical flow for restoring textures and objects using information from neighboring frames, while completing masked regions through visual Transformers. However, these approaches often encounter blurring and temporal inconsistencies when dealing with large masks, highlighting the need for models with enhanced generative capabilities. Recently, diffusion models have emerged as a prominent technique in image and video generation due to their impressive performance. In this paper, we introduce DiffuEraser, a video inpainting model based on stable diffusion, designed to fill masked regions with greater details and more coherent structures. We incorporate prior information to provide initialization and weak conditioning,which helps mitigate noisy artifacts and suppress hallucinations. Additionally, to improve temporal consistency during long-sequence inference, we expand the temporal receptive fields of both the prior model and DiffuEraser, and further enhance consistency by leveraging the temporal smoothing property of Video Diffusion Models. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency. </p>
<blockquote>
<p>最近的视频补全算法结合了基于流的像素传播和基于转换器的生成，利用光学流恢复纹理和对象，同时使用来自相邻帧的信息，同时通过视觉转换器完成遮罩区域。然而，这些方法在处理大遮罩时经常遇到模糊和时间不一致的问题，这突显了需要具有增强生成能力的模型。最近，扩散模型因其令人印象深刻的性能而成为图像和视频生成中的一项重要技术。在本文中，我们介绍了基于稳定扩散的DiffuEraser视频补全模型，旨在以更高的细节和更连贯的结构填充遮罩区域。我们结合先验信息提供初始化和弱条件设置，这有助于减少噪声伪影并抑制幻觉。此外，为了提高长序列推理期间的时序一致性，我们扩大了先验模型和DiffuEraser的时间感受野，并借助视频扩散模型的时序平滑属性进一步提高了一致性。实验结果表明，我们提出的方法在内容完整性和时间一致性方面优于现有技术，同时保持可接受的效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10018v1">PDF</a> 11pages, 13figures</p>
<p><strong>Summary</strong></p>
<p>基于稳定扩散的DiffuEraser视频修复模型能够有效填补遮挡区域，细节更丰富且结构更连贯。该模型利用先验信息初始化并提供弱条件，减少噪声伪影并抑制幻觉。同时，通过扩大时间接收域并利用视频扩散模型的时序平滑特性，提高了长序列推理的时空一致性。实验结果证明，该方法在内容完整性和时序一致性方面优于现有技术，同时保持了可接受的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffuEraser结合流传播的像素和基于转换器的生成方法恢复纹理和对象。</li>
<li>当前方法在遇到大遮罩时可能出现模糊和时序不一致的问题。</li>
<li>扩散模型在图像和视频生成方面的突出表现使其成为理想选择。</li>
<li>DiffuEraser利用先验信息进行初始化和弱条件化，减少噪声伪影和抑制幻觉。</li>
<li>通过扩大时间接收域提高时序一致性。</li>
<li>利用视频扩散模型的时序平滑特性进一步增强一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10018">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b340379526b79bd4080d097ad54f09d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-672de16b31c2804b2e9322250c1ef29d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec5b7fa3445f5a6f4eb0ebc4ac2c698f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70485c60bb15224906c1dd79f5cacbd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b24b2709eca619ee64f2177b71130e49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d037c9940ec01be7aba07cffdaa4d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e37c094fd48ce711a763b2b3224280.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Physics-informed-DeepCT-Sinogram-Wavelet-Decomposition-Meets-Masked-Diffusion"><a href="#Physics-informed-DeepCT-Sinogram-Wavelet-Decomposition-Meets-Masked-Diffusion" class="headerlink" title="Physics-informed DeepCT: Sinogram Wavelet Decomposition Meets Masked   Diffusion"></a>Physics-informed DeepCT: Sinogram Wavelet Decomposition Meets Masked   Diffusion</h2><p><strong>Authors:Zekun Zhou, Tan Liu, Bing Yu, Yanru Gong, Liu Shi, Qiegen Liu</strong></p>
<p>Diffusion model shows remarkable potential on sparse-view computed tomography (SVCT) reconstruction. However, when a network is trained on a limited sample space, its generalization capability may be constrained, which degrades performance on unfamiliar data. For image generation tasks, this can lead to issues such as blurry details and inconsistencies between regions. To alleviate this problem, we propose a Sinogram-based Wavelet random decomposition And Random mask diffusion Model (SWARM) for SVCT reconstruction. Specifically, introducing a random mask strategy in the sinogram effectively expands the limited training sample space. This enables the model to learn a broader range of data distributions, enhancing its understanding and generalization of data uncertainty. In addition, applying a random training strategy to the high-frequency components of the sinogram wavelet enhances feature representation and improves the ability to capture details in different frequency bands, thereby improving performance and robustness. Two-stage iterative reconstruction method is adopted to ensure the global consistency of the reconstructed image while refining its details. Experimental results demonstrate that SWARM outperforms competing approaches in both quantitative and qualitative performance across various datasets. </p>
<blockquote>
<p>扩散模型在稀疏视图计算机断层扫描（SVCT）重建中显示出显著潜力。然而，当网络在有限的样本空间上进行训练时，其泛化能力可能会受到限制，导致在陌生数据上的性能下降。对于图像生成任务，这可能会导致细节模糊以及区域间的不一致性等问题。为了缓解这一问题，我们提出了基于辛氏图的小波随机分解和随机掩膜扩散模型（SWARM），用于SVCT重建。具体而言，在辛氏图中引入随机掩膜策略，有效地扩大了有限的训练样本空间。这使模型能够学习更广泛的数据分布，增强了对数据不确定性的理解和泛化。此外，对辛氏图小波的高频成分应用随机训练策略，增强了特征表示，提高了捕获不同频段细节的能力，从而提高了性能和稳健性。采用两阶段迭代重建方法，确保重建图像的整体一致性，同时细化其细节。实验结果表明，SWARM在不同数据集上的定量和定性性能均优于其他方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09935v1">PDF</a> </p>
<p><strong>Summary</strong>：扩散模型在稀疏视角计算机断层扫描（SVCT）重建中显示出显著潜力，但在有限样本空间训练的网络在陌生数据上的表现可能会受到限制。为解决图像生成任务中细节模糊和区域间不一致等问题，我们提出了基于辛氏图小波随机分解和随机掩膜扩散模型（SWARM）的SVCT重建方法。通过辛氏图中引入随机掩膜策略有效扩展了有限的训练样本空间，提高了模型对数据不确定性的理解和泛化能力。此外，对辛氏图小波高频成分采用随机训练策略，增强了特征表示和捕捉不同频段细节的能力，提高了性能和鲁棒性。采用两阶段迭代重建方法，确保重建图像的整体一致性并优化细节。实验结果证明SWARM在多个数据集上定量和定性性能均优于其他方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型在SVCT重建中具有显著潜力。</li>
<li>在有限样本空间训练的网络可能限制了其在陌生数据上的表现。</li>
<li>SWARM模型通过引入随机掩膜策略扩展了训练样本空间。</li>
<li>随机训练策略提高了模型对数据不确定性的理解和泛化能力。</li>
<li>对辛氏图小波高频成分采用随机训练策略，增强了特征表示和捕捉细节的能力。</li>
<li>采用两阶段迭代重建方法确保图像全局一致性并优化细节。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09935">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-20e5a97bfa79dc52917b15637a7639bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da058ff37c45bb0744f02c5a73c46f2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49326a45f585853ac31768aae7ba2fa2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CrossModalityDiffusion-Multi-Modal-Novel-View-Synthesis-with-Unified-Intermediate-Representation"><a href="#CrossModalityDiffusion-Multi-Modal-Novel-View-Synthesis-with-Unified-Intermediate-Representation" class="headerlink" title="CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified   Intermediate Representation"></a>CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified   Intermediate Representation</h2><p><strong>Authors:Alex Berian, Daniel Brignac, JhihYang Wu, Natnael Daba, Abhijit Mahalanobis</strong></p>
<p>Geospatial imaging leverages data from diverse sensing modalities-such as EO, SAR, and LiDAR, ranging from ground-level drones to satellite views. These heterogeneous inputs offer significant opportunities for scene understanding but present challenges in interpreting geometry accurately, particularly in the absence of precise ground truth data. To address this, we propose CrossModalityDiffusion, a modular framework designed to generate images across different modalities and viewpoints without prior knowledge of scene geometry. CrossModalityDiffusion employs modality-specific encoders that take multiple input images and produce geometry-aware feature volumes that encode scene structure relative to their input camera positions. The space where the feature volumes are placed acts as a common ground for unifying input modalities. These feature volumes are overlapped and rendered into feature images from novel perspectives using volumetric rendering techniques. The rendered feature images are used as conditioning inputs for a modality-specific diffusion model, enabling the synthesis of novel images for the desired output modality. In this paper, we show that jointly training different modules ensures consistent geometric understanding across all modalities within the framework. We validate CrossModalityDiffusion’s capabilities on the synthetic ShapeNet cars dataset, demonstrating its effectiveness in generating accurate and consistent novel views across multiple imaging modalities and perspectives. </p>
<blockquote>
<p>地理空间成像利用来自不同感知模式的数据，如EO、SAR和LiDAR，从地面无人机到卫星视图的各种数据。这些异构输入为场景理解提供了重大机会，但在没有精确地面真实数据的情况下，对几何的准确解释却带来了挑战。为了解决这一问题，我们提出了CrossModalityDiffusion，这是一个模块化框架，旨在在不同模态和视角生成图像，而无需事先了解场景几何。CrossModalityDiffusion采用特定模态的编码器，这些编码器接受多个输入图像并产生感知几何特征的体积，这些体积编码了相对于其输入相机位置的场景结构。特征体积所处的空间成为统一输入模态的共同基础。这些特征体积被重叠并使用体积渲染技术从新的视角渲染成特征图像。渲染的特征图像用作特定模态扩散模型的调节输入，从而合成所需输出模态的新图像。在本文中，我们展示了联合训练不同的模块可以确保框架内所有模态的几何理解的一致性。我们在合成的ShapeNet汽车数据集上验证了CrossModalityDiffusion的能力，证明了它在生成多个成像模态和视角的新颖且准确的视图方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09838v1">PDF</a> Accepted in the 2025 WACV workshop GeoCV</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个跨模态扩散模型框架（CrossModalityDiffusion），该框架可在不同模态和视角生成图像，无需对场景几何的先验知识。该框架采用模态特定编码器处理多种输入图像，生成包含场景结构的特征体积，并通过体积渲染技术从新颖视角呈现特征图像。这些特征图像为特定模态的扩散模型提供条件输入，从而合成目标模态的新颖图像。本文验证了在不同模块联合训练下，该框架在多模态和透视场景中的几何一致性理解能力。在ShapeNet汽车数据集上的实验表明，该框架能生成准确且一致的新视角跨模态图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Geospatial成像利用多种传感器模态的数据，如EO、SAR和LiDAR，从地面无人机到卫星视图不等。</li>
<li>跨模态扩散模型框架（CrossModalityDiffusion）设计用于不同模态和视角生成图像，无需对场景几何先验知识。</li>
<li>CrossModalityDiffusion采用模态特定编码器生成包含场景结构的特征体积，并将它们放置在统一的空间中以融合不同模态。</li>
<li>通过体积渲染技术从新颖视角呈现特征图像。</li>
<li>特征图像为特定模态的扩散模型提供条件输入，合成目标模态的新颖图像。</li>
<li>联合训练不同模块确保了跨所有模态的几何一致性理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dab66cc4fcc22dedac17aa202d374f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa4bb217507f8e09611f3864152a2a51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f602960b503c4f95a8f4391b18af317.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d47d370da81fc5d59eb6287cb1e5cccb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6a9921106fe7a2e939f4fd8f02bc27c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Lossy-Compression-with-Pretrained-Diffusion-Models"><a href="#Lossy-Compression-with-Pretrained-Diffusion-Models" class="headerlink" title="Lossy Compression with Pretrained Diffusion Models"></a>Lossy Compression with Pretrained Diffusion Models</h2><p><strong>Authors:Jeremy Vonderfecht, Feng Liu</strong></p>
<p>We apply the DiffC algorithm (Theis et al. 2022) to Stable Diffusion 1.5, 2.1, XL, and Flux-dev, and demonstrate that these pretrained models are remarkably capable lossy image compressors. A principled algorithm for lossy compression using pretrained diffusion models has been understood since at least Ho et al. 2020, but challenges in reverse-channel coding have prevented such algorithms from ever being fully implemented. We introduce simple workarounds that lead to the first complete implementation of DiffC, which is capable of compressing and decompressing images using Stable Diffusion in under 10 seconds. Despite requiring no additional training, our method is competitive with other state-of-the-art generative compression methods at low ultra-low bitrates. </p>
<blockquote>
<p>我们将DiffC算法（Theis等人，2022年）应用于Stable Diffusion 1.5、2.1、XL和Flux-dev，并证明这些预训练模型是出色的有损图像压缩器。至少自Ho等人（2020年）以来，已经了解使用预训练扩散模型进行有损压缩的原理算法，但反向信道编码的挑战使得此类算法从未得到全面实施。我们介绍了一些简单的解决方案，首次实现了完整的DiffC实现，能够在不到10秒内使用Stable Diffusion对图像进行压缩和解压缩。我们的方法无需额外的训练，在低超低比特率下与其他最先进的生成压缩方法具有竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09815v1">PDF</a> </p>
<p><strong>Summary</strong><br>     将DiffC算法应用于Stable Diffusion 1.5、2.1、XL和Flux-dev等预训练模型，证明这些模型具有出色的有损图像压缩能力。虽然早在Ho等人在2020年就已经了解了利用预训练扩散模型进行有损压缩的原理，但由于反向信道编码的挑战，相关算法一直没有完全实现。研究团队提出了简单对策，首次完成了DiffC的完整实现，能在不到10秒内使用Stable Diffusion对图像进行压缩和解压缩。该方法无需额外训练，在低超低比特率下与其他先进的生成压缩方法相比具有竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffC算法成功应用于多种预训练扩散模型，包括Stable Diffusion 1.5、2.1、XL和Flux-dev，展示出色的有损图像压缩能力。</li>
<li>尽管自Ho等人在2020年提出利用预训练扩散模型进行有损压缩的原理以来，相关算法一直面临反向信道编码的挑战，但研究团队成功解决了这一问题。</li>
<li>研究团队首次实现了DiffC的完整版本，能够在短时间内（不到10秒）完成图像的压缩和解压缩过程。</li>
<li>该方法无需对模型进行额外训练，具有实际应用价值。</li>
<li>与其他先进的生成压缩方法相比，该方法在超低比特率下表现出竞争力。</li>
<li>这一进展为利用预训练扩散模型进行图像压缩提供了新的可能性和实际应用场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e6e73f06ece4fda86ec3ee710f26f531.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddf9add97d5adc85c6974954ac0ac396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1930235d1ff3d7e595968f5872637829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a76203b00b637804cee47e1d7fa42a3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NeurOp-Diff-Continuous-Remote-Sensing-Image-Super-Resolution-via-Neural-Operator-Diffusion"><a href="#NeurOp-Diff-Continuous-Remote-Sensing-Image-Super-Resolution-via-Neural-Operator-Diffusion" class="headerlink" title="NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion"></a>NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion</h2><p><strong>Authors:Zihao Xu, Yuzhi Tang, Bowen Xu, Qingquan Li</strong></p>
<p>Most publicly accessible remote sensing data suffer from low resolution, limiting their practical applications. To address this, we propose a diffusion model guided by neural operators for continuous remote sensing image super-resolution (NeurOp-Diff). Neural operators are used to learn resolution representations at arbitrary scales, encoding low-resolution (LR) images into high-dimensional features, which are then used as prior conditions to guide the diffusion model for denoising. This effectively addresses the artifacts and excessive smoothing issues present in existing super-resolution (SR) methods, enabling the generation of high-quality, continuous super-resolution images. Specifically, we adjust the super-resolution scale by a scaling factor s, allowing the model to adapt to different super-resolution magnifications. Furthermore, experiments on multiple datasets demonstrate the effectiveness of NeurOp-Diff. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zerono000/NeurOp-Diff">https://github.com/zerono000/NeurOp-Diff</a>. </p>
<blockquote>
<p>大部分公开可访问的遥感数据存在分辨率低的问题，这限制了它们的实际应用。为解决这一问题，我们提出了一种由神经算子引导的扩散模型，用于连续遥感图像超分辨率（NeurOp-Diff）。神经算子用于学习任意尺度的分辨率表示，将低分辨率（LR）图像编码为高维特征，然后将其作为先验条件，引导扩散模型进行去噪。这有效地解决了现有超分辨率（SR）方法中出现的伪影和过度平滑问题，能够生成高质量、连续的超高分辨率图像。具体来说，我们通过缩放因子s调整超分辨率尺度，使模型能够适应不同的超分辨率放大倍数。此外，多个数据集上的实验证明了NeurOp-Diff的有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/zerono000/NeurOp-Diff%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/zerono000/NeurOp-Diff上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09054v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对现有遥感数据分辨率低的问题，提出了一种基于神经算子和扩散模型的连续遥感图像超分辨率方法（NeurOp-Diff）。该方法利用神经算子学习任意尺度的分辨率表示，将低分辨率图像编码为高维特征，并作为先验条件引导扩散模型进行去噪。这解决了现有超分辨率方法中的伪影和过度平滑问题，能够生成高质量、连续的超级分辨率图像。通过调整超分辨率尺度缩放因子s，该模型可适应不同的超分辨率放大倍数。在多个数据集上的实验验证了NeurOp-Diff的有效性。</p>
<p><strong>要点</strong></p>
<ol>
<li>提出了一种基于神经算子和扩散模型的遥感图像超分辨率方法NeurOp-Diff。</li>
<li>神经算子用于学习任意尺度的分辨率表示，并将低分辨率图像转化为高维特征。</li>
<li>高维特征作为先验条件引导扩散模型进行去噪，解决了伪影和过度平滑问题。</li>
<li>通过调整缩放因子s，模型可适应不同的超分辨率放大需求。</li>
<li>NeurOp-Diff在多个数据集上的实验验证了其有效性。</li>
<li>该模型的代码已公开可用。</li>
<li>该方法能够生成高质量的连续超级分辨率图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09054">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-833ba1113001237d1e27042dedece5b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf8859f49f6285166d1153044e49051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640889ab9d48cde97890705c2044351c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DPCL-Diff-The-Temporal-Knowledge-Graph-Reasoning-Based-on-Graph-Node-Diffusion-Model-with-Dual-Domain-Periodic-Contrastive-Learning"><a href="#DPCL-Diff-The-Temporal-Knowledge-Graph-Reasoning-Based-on-Graph-Node-Diffusion-Model-with-Dual-Domain-Periodic-Contrastive-Learning" class="headerlink" title="DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node   Diffusion Model with Dual-Domain Periodic Contrastive Learning"></a>DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node   Diffusion Model with Dual-Domain Periodic Contrastive Learning</h2><p><strong>Authors:Yukun Cao, Lisheng Wang, Luobin Huang</strong></p>
<p>Temporal knowledge graph (TKG) reasoning that infers future missing facts is an essential and challenging task. Predicting future events typically relies on closely related historical facts, yielding more accurate results for repetitive or periodic events. However, for future events with sparse historical interactions, the effectiveness of this method, which focuses on leveraging high-frequency historical information, diminishes. Recently, the capabilities of diffusion models in image generation have opened new opportunities for TKG reasoning. Therefore, we propose a graph node diffusion model with dual-domain periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff) introduces noise into sparsely related events to simulate new events, generating high-quality data that better conforms to the actual distribution. This generative mechanism significantly enhances the model’s ability to reason about new events. Additionally, the dual-domain periodic contrastive learning (DPCL) maps periodic and non-periodic event entities to Poincar&#39;e and Euclidean spaces, leveraging their characteristics to distinguish similar periodic events effectively. Experimental results on four public datasets demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG models in event prediction, demonstrating our approach’s effectiveness. This study also investigates the combined effectiveness of GNDiff and DPCL in TKG tasks. </p>
<blockquote>
<p>时序知识图谱（TKG）推理，推断未来缺失的事实，是一项重要且具有挑战性的任务。预测未来事件通常依赖于密切相关的历史事实，对于重复性或周期性事件，能带来更准确的结果。然而，对于历史交互稀疏的未来事件，这种方法的有效性会降低，因为它主要侧重于利用高频历史信息。最近，扩散模型在图像生成方面的能力为TKG推理提供了新的机会。因此，我们提出了一种具有双域周期性对比学习（DPCL）的图节点扩散模型（DPCL-Diff）。图节点扩散模型（GNDiff）将噪声引入稀疏相关的事件中，以模拟新事件，生成高质量的数据，更好地符合实际分布。这种生成机制显著提高了模型对新事件的推理能力。此外，双域周期性对比学习（DPCL）将周期性和非周期性事件实体映射到Poincaré和欧几里得空间，利用其特性来有效地区分相似的周期性事件。在四个公共数据集上的实验结果表明，DPCL-Diff在事件预测方面显著优于最先进的TKG模型，证明了我们的方法的有效性。本研究还探讨了GNDiff和DPCL在TKG任务中的组合效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01477v2">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>基于时序知识图谱（TKG）推理预测未来缺失事实是一项重要且具挑战性的任务。现有方法主要依赖历史事实进行未来事件预测，对于重复或周期性事件效果较好，但对于历史交互稀疏的未来事件则表现不足。本文提出了结合图节点扩散模型和双域周期性对比学习（DPCL-Diff）的方法，通过引入噪声模拟新事件，提高模型对新事件的推理能力。同时，DPCL将周期性和非周期性事件实体映射到不同的空间，以区分相似的周期性事件。在四个公开数据集上的实验结果表明，DPCL-Diff在事件预测方面显著优于现有TKG模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时序知识图谱（TKG）推理预测未来缺失事实很重要且具挑战性。</li>
<li>现有方法主要依赖历史事实进行预测，对稀疏历史交互的未来事件效果有限。</li>
<li>提出的图节点扩散模型（GNDiff）通过引入噪声模拟新事件，提高模型对新事件的推理能力。</li>
<li>双域周期性对比学习（DPCL）能有效区分周期性事件和非周期性事件实体。</li>
<li>DPCL-Diff方法结合了GNDiff和DPCL，显著提高了在四个公开数据集上的事件预测性能。</li>
<li>GNDiff和DPCL的联合效果在TKG任务中得到了调查验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01477">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b96902c8bbb7e73d5a199b2d3e51fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3545eb03cdcd5a2171d88b744e71688f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d04e77e60f4898d5b3edde8c967d2819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71265a8141a1db97fec5668fc7ce0269.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42aa5ab06b72e19369f0097a2be98afe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DX2CT-Diffusion-Model-for-3D-CT-Reconstruction-from-Bi-or-Mono-planar-2D-X-ray-s"><a href="#DX2CT-Diffusion-Model-for-3D-CT-Reconstruction-from-Bi-or-Mono-planar-2D-X-ray-s" class="headerlink" title="DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar   2D X-ray(s)"></a>DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar   2D X-ray(s)</h2><p><strong>Authors:Yun Su Jeong, Hye Bin Yoo, Il Yong Chun</strong></p>
<p>Computational tomography (CT) provides high-resolution medical imaging, but it can expose patients to high radiation. X-ray scanners have low radiation exposure, but their resolutions are low. This paper proposes a new conditional diffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes from bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key components: 1) modulating feature maps extracted from two-dimensional (2D) X-ray(s) with 3D positions of CT volume using a new transformer and 2) effectively using the modulated 3D position-aware feature maps as conditions of DX2CT. In particular, the proposed transformer can provide conditions with rich information of a target CT slice to the conditional diffusion model, enabling high-quality CT reconstruction. Our experiments with the bi or mono-planar X-ray(s) benchmark datasets show that proposed DX2CT outperforms several state-of-the-art methods. Our codes and model will be available at: <a target="_blank" rel="noopener" href="https://www.github.com/intyeger/DX2CT">https://www.github.com/intyeger/DX2CT</a>. </p>
<blockquote>
<p>计算机断层扫描（CT）提供高分辨率医学成像，但它会使患者暴露在较高的辐射之下。X射线扫描仪的辐射暴露较低，但其分辨率也较低。本文提出了一种新的条件扩散模型DX2CT，能够从双平面或单平面X射线图像重建三维（3D）CT体积。提出的DX2CT由两个关键部分组成：1）使用新变压器将二维（2D）X射线特征图与CT体积的三维位置进行调制，以及2）有效地利用调制后的三维位置感知特征图作为DX2CT的条件。特别是，所提出的变压器可以为条件扩散模型提供目标CT切片的丰富信息，从而实现高质量的CT重建。我们在双平面或单平面X射线基准数据集上的实验表明，所提出的DX2CT优于几种最新方法。我们的代码和模型将在<a target="_blank" rel="noopener" href="https://www.github.com/intyeger/DX2CT">https://www.github.com/intyeger/DX2CT</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08850v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的条件扩散模型DX2CT，可从二维X-ray图像重建出三维CT体积。该模型由两个关键部分组成：一是使用新变压器调制来自二维X-ray的特征图，并结合CT体积的三维位置；二是有效地利用调制后的三维位置感知特征图作为DX2CT的条件。实验表明，DX2CT在二维X-ray基准数据集上优于其他最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种新型条件扩散模型DX2CT，可从二维X-ray图像重建出高质量的三维CT体积。</li>
<li>DX2CT包含两个核心组件：调制特征图的变压器和用于条件扩散模型的三维位置感知特征图。</li>
<li>变压器能够从二维X-ray图像中提取特征图，并结合CT体积的三维位置进行调制。</li>
<li>调制后的三维位置感知特征图被用作DX2CT的条件，以提高重建质量。</li>
<li>实验表明，DX2CT在二维X-ray图像基准数据集上的表现优于其他最先进的方法。</li>
<li>该模型的代码和将公开在GitHub上供公众使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08850">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-12f602f173596666d0068e8a2c61d1e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbe0ac2a77fab04a7719910d8d7a5373.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec517e7397fa2ae0fd334c3b9a5c4a32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7469f87dce8054a4d1291573224c7ef4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Isolated-Diffusion-Optimizing-Multi-Concept-Text-to-Image-Generation-Training-Freely-with-Isolated-Diffusion-Guidance"><a href="#Isolated-Diffusion-Optimizing-Multi-Concept-Text-to-Image-Generation-Training-Freely-with-Isolated-Diffusion-Guidance" class="headerlink" title="Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation   Training-Freely with Isolated Diffusion Guidance"></a>Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation   Training-Freely with Isolated Diffusion Guidance</h2><p><strong>Authors:Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan</strong></p>
<p>Large-scale text-to-image diffusion models have achieved great success in synthesizing high-quality and diverse images given target text prompts. Despite the revolutionary image generation ability, current state-of-the-art models still struggle to deal with multi-concept generation accurately in many cases. This phenomenon is known as &#96;&#96;concept bleeding” and displays as the unexpected overlapping or merging of various concepts. This paper presents a general approach for text-to-image diffusion models to address the mutual interference between different subjects and their attachments in complex scenes, pursuing better text-image consistency. The core idea is to isolate the synthesizing processes of different concepts. We propose to bind each attachment to corresponding subjects separately with split text prompts. Besides, we introduce a revision method to fix the concept bleeding problem in multi-subject synthesis. We first depend on pre-trained object detection and segmentation models to obtain the layouts of subjects. Then we isolate and resynthesize each subject individually with corresponding text prompts to avoid mutual interference. Overall, we achieve a training-free strategy, named Isolated Diffusion, to optimize multi-concept text-to-image synthesis. It is compatible with the latest Stable Diffusion XL (SDXL) and prior Stable Diffusion (SD) models. We compare our approach with alternative methods using a variety of multi-concept text prompts and demonstrate its effectiveness with clear advantages in text-image consistency and user study. </p>
<blockquote>
<p>大规模文本到图像的扩散模型在给定目标文本提示的情况下，已成功合成高质量和多样化的图像。尽管这些革命性的图像生成能力令人印象深刻，但当前最先进的模型仍然在许多情况下难以准确处理多概念生成。这种现象被称为“概念出血”，表现为各种概念的意外重叠或合并。本文提出了一种通用的文本到图像扩散模型方法，以解决复杂场景中不同主题及其附件之间的相互干扰，以追求更好的文本-图像一致性。核心思想是隔离不同概念的合成过程。我们提出通过拆分文本提示来将每个附件与相应的主题分别绑定。此外，我们引入了一种修正方法来解决多主题合成中的概念出血问题。我们首先依赖于预训练的目标检测和分割模型来获得主题布局。然后，我们分别使用相应的文本提示来隔离并重新合成每个主题，以避免相互干扰。总的来说，我们实现了一种名为“孤立扩散”的非训练策略，以优化多概念文本到图像的合成。它与最新的Stable Diffusion XL（SDXL）和之前的Stable Diffusion（SD）模型兼容。我们通过使用多种多概念文本提示将我们的方法与替代方法进行比较，并在文本-图像一致性和用户研究方面显示出其有效性及明显优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16954v2">PDF</a> Accepted by IEEE Transactions on Visualization and Computer Graphics</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对文本到图像扩散模型的新方法，用于解决复杂场景中不同主题及其附件之间的相互干扰问题，从而提高文本与图像的一致性。通过分离不同概念的合成过程，采用分割文本提示将每个附件绑定到相应的主题上。同时，引入了一种修正方法来解决多主题合成中的概念混淆问题。通过依赖于预训练的目标检测和分割模型来获取主题布局，然后孤立并重新合成每个主题，以提高文本与图像的一致性。总体而言，这是一种名为“孤立扩散”的无训练策略，可优化多概念文本到图像的综合生成，与最新的Stable Diffusion XL和之前的Stable Diffusion模型兼容。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像扩散模型在合成高质量和多样化的图像方面取得了巨大成功，但在多概念生成方面仍存在挑战。</li>
<li>当前模型面临的概念混淆问题被称为“概念出血”，表现为各种概念的意外重叠或合并。</li>
<li>提出了一种新的方法来解决复杂场景中不同主题及其附件之间的干扰问题，以提高文本与图像的一致性。</li>
<li>通过分离不同概念的合成过程并采用分割文本提示来绑定每个附件到相应的主题。</li>
<li>引入了一种修正方法来处理多主题合成中的概念混淆问题，依赖于预训练的目标检测和分割模型来获取主题布局。</li>
<li>提出了一种名为“孤立扩散”的无训练策略，优化了多概念文本到图像的综合生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16954">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-85f7a5d9da68054b841d6e82e946721c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59142d846d4e359688805be6f3e9531f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1588650e8316fc8560462f8d745bdd8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e8ba5c7568a73643d725e4fa9b60a09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-564b5c14c225f074d1158ed624da5632.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Generate-E-commerce-Product-Background-by-Integrating-Category-Commonality-and-Personalized-Style"><a href="#Generate-E-commerce-Product-Background-by-Integrating-Category-Commonality-and-Personalized-Style" class="headerlink" title="Generate E-commerce Product Background by Integrating Category   Commonality and Personalized Style"></a>Generate E-commerce Product Background by Integrating Category   Commonality and Personalized Style</h2><p><strong>Authors:Haohan Wang, Wei Feng, Yaoyu Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Zhangang Lin, Jingping Shao</strong></p>
<p>The state-of-the-art methods for e-commerce product background generation suffer from the inefficiency of designing product-wise prompts when scaling up the production, as well as the ineffectiveness of describing fine-grained styles when customizing personalized backgrounds for some specific brands. To address these obstacles, we integrate the category commonality and personalized style into diffusion models. Concretely, we propose a Category-Wise Generator to enable large-scale background generation with only one model for the first time. A unique identifier in the prompt is assigned to each category, whose attention is located on the background by a mask-guided cross attention layer to learn the category-wise style. Furthermore, for products with specific and fine-grained requirements in layout, elements, etc, a Personality-Wise Generator is devised to learn such personalized style directly from a reference image to resolve textual ambiguities, and is trained in a self-supervised manner for more efficient training data usage. To advance research in this field, the first large-scale e-commerce product background generation dataset BG60k is constructed, which covers more than 60k product images from over 2k categories. Experiments demonstrate that our method could generate high-quality backgrounds for different categories, and maintain the personalized background style of reference images. BG60k will be available at \url{<a target="_blank" rel="noopener" href="https://github.com/Whileherham/BG60k%7D">https://github.com/Whileherham/BG60k}</a>. </p>
<blockquote>
<p>当前电子商务产品背景生成的最先进方法，在扩大生产规模时面临着产品设计提示效率低下的问题，以及在为某些特定品牌定制个性化背景时描述细微风格的不有效性。为了解决这些障碍，我们将类别共性与个性化风格融入扩散模型。具体来说，我们首次提出一种类别生成器，只需一个模型即可实现大规模的背景生成。每个类别在提示中都有一个唯一标识符，通过一个掩膜引导交叉注意力层将注意力集中在背景上，以学习类别风格。此外，对于在布局、元素等方面有特定和细微要求的产品，设计了一个个性化生成器，直接从参考图像学习个性化风格，以解决文本歧义问题，并以自我监督的方式进行训练，以更有效地利用训练数据。为了推动该领域的研究，构建了首个大规模电子商务产品背景生成数据集BG60k，该数据集涵盖了来自2k多个类别的超过6万张产品图像。实验表明，我们的方法能够为不同的类别生成高质量的背景，并保持参考图像的个性化背景风格。BG60k数据集将在\url{<a target="_blank" rel="noopener" href="https://github.com/Whileherham/BG60k%7D%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Whileherham/BG60k}上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.13309v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>在当前的电商产品背景生成方法中，随着产品数量的增长，为每一个产品设计专属提示语是非常低效的；同时为某些特定品牌定制精细风格背景时也存在描述不够精确的问题。为解决这些问题，研究团队结合扩散模型中的类别共性及个性化风格提出了创新的解决方案。首先，他们首次提出了一个类别导向生成器，只需一个模型就能实现大规模的背景生成。每个类别都有一个独特的标识符作为提示语，通过遮罩引导的交叉注意力层将其聚焦在背景上，以学习类别的风格特征。此外，为了满足具有特定细节要求的产品的布局和元素等需求，他们还设计了一个个性化生成器，可以从参考图像中学习个性化风格以解决文本歧义问题，并采用自我监督的方式进行训练以更有效地使用训练数据。为了推动该领域的研究进展，他们还构建了首个大规模的电商产品背景生成数据集BG60k，包含超过6万张来自超过两千个类别的产品图像。实验证明，该方法能够为不同类别的产品生成高质量背景，并保留参考图像的个性化背景风格。该数据集现已可在指定网站获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前电商产品背景生成面临的挑战包括设计产品提示的低效性和描述精细风格的困难性。</li>
<li>研究团队通过结合扩散模型中的类别共性及个性化风格来解决上述问题。</li>
<li>提出类别导向生成器，实现大规模背景生成只需一个模型。每个类别有独特的标识符作为提示语。</li>
<li>设计个性化生成器以满足具有特定细节要求的产品的需求，从参考图像中学习个性化风格并解决文本歧义问题。</li>
<li>采用自我监督训练以提高训练数据使用效率。</li>
<li>研究团队构建了首个大规模的电商产品背景生成数据集BG60k，包含超过6万张来自两千个类别的产品图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.13309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6862b1022331c75bdfb5c92da78d23ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cb1f7179677475ed0a9af8ae9e9aaac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c0a7827a15cbbd0bcf8dc09a9c0ac4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5c926d21c5639b073975b0c8209248c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b36c366ed9a66cb6c2b8d822b2f6fbd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-7e2835baee1b0b1d105266f164f847db.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-21  Region-wise stacking ensembles for estimating brain-age using MRI
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02482c9381dc27eea9ef006577460386.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-21  Surface-SOS Self-Supervised Object Segmentation via Neural Surface   Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
