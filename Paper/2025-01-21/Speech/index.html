<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-21  On Ambisonic Source Separation with Spatially Informed Non-negative   Tensor Factorization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a2c85138a6ee5eab3140ae3ff1788df5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    42 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-21-更新"><a href="#2025-01-21-更新" class="headerlink" title="2025-01-21 更新"></a>2025-01-21 更新</h1><h2 id="On-Ambisonic-Source-Separation-with-Spatially-Informed-Non-negative-Tensor-Factorization"><a href="#On-Ambisonic-Source-Separation-with-Spatially-Informed-Non-negative-Tensor-Factorization" class="headerlink" title="On Ambisonic Source Separation with Spatially Informed Non-negative   Tensor Factorization"></a>On Ambisonic Source Separation with Spatially Informed Non-negative   Tensor Factorization</h2><p><strong>Authors:Mateusz Guzik, Konrad Kowalczyk</strong></p>
<p>This article presents a Non-negative Tensor Factorization based method for sound source separation from Ambisonic microphone signals. The proposed method enables the use of prior knowledge about the Directions-of-Arrival (DOAs) of the sources, incorporated through a constraint on the Spatial Covariance Matrix (SCM) within a Maximum a Posteriori (MAP) framework. Specifically, this article presents a detailed derivation of four algorithms that are based on two types of cost functions, namely the squared Euclidean distance and the Itakura-Saito divergence, which are then combined with two prior probability distributions on the SCM, that is the Wishart and the Inverse Wishart. The experimental evaluation of the baseline Maximum Likelihood (ML) and the proposed MAP methods is primarily based on first-order Ambisonic recordings, using four different source signal datasets, three with musical pieces and one containing speech utterances. We consider under-determined, determined, as well as over-determined scenarios by separating two, four and six sound sources, respectively. Furthermore, we evaluate the proposed algorithms for different spherical harmonic orders and at different reverberation time levels, as well as in non-ideal prior knowledge conditions, for increasingly more corrupted DOAs. Overall, in comparison with beamforming and a state-of-the-art separation technique, as well as the baseline ML methods, the proposed MAP approach offers superior separation performance in a variety of scenarios, as shown by the analysis of the experimental evaluation results, in terms of the standard objective separation measures, such as the SDR, ISR, SIR and SAR. </p>
<blockquote>
<p>本文提出了一种基于非负张量分解的方法，用于从Ambisonic麦克风信号中分离声源。该方法可以利用关于源到达方向（DOA）的先验知识，通过最大后验（MAP）框架中的空间协方差矩阵（SCM）约束将其结合。具体来说，本文详细推导了四种基于两种成本函数的算法，即平方欧几里得距离和Itakura-Saito发散，然后与SCM上的两个先验概率分布即Wishart和Inverse Wishart相结合。基线最大似然（ML）和所提出的MAP方法的实验评估主要基于第一阶Ambisonic录音，使用四个不同的源信号数据集，其中三个是音乐作品，一个是包含语音语句的。我们通过分离两个、四个和六个声源，分别考虑欠定、确定以及过度确定的情况。此外，我们还对不同的球面谐波阶数和不同的混响时间水平以及非理想的先验知识条件下，越来越受干扰的DOA进行了算法评估。总体而言，与波束形成和最新的分离技术以及基线ML方法相比，根据实验评估结果的分析，如在标准客观分离措施（如SDR、ISR、SIR和SAR）方面，所提出的MAP方法在各种场景下均表现出优越的分离性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10305v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种基于非负张量分解的方法，用于从Ambisonic麦克风信号中分离声源。该方法能够利用有关声源到达方向（DOA）的先验知识，通过最大后验（MAP）框架中的空间协方差矩阵（SCM）约束将其融入。文章详细推导了四种算法，这些算法基于两种成本函数，即欧几里得距离的平方和Itakura-Saito散度，并结合SCM上的两个先验概率分布，即Wishart和Inverse Wishart。实验评估了基线最大似然（ML）和提出的MAP方法，主要基于一阶Ambisonic录音，使用四个不同的源信号数据集，其中三个是音乐片段，一个包含语音片段。我们考虑了欠定、确定以及过度定的情况，分别分离两个、四个和六个声源。此外，我们还对不同球面谐波阶数和不同混响时间水平以及非理想先验知识条件下的算法进行了评估，针对越来越受腐蚀的DOA。总体而言，与波束形成法和一种最新的分离技术相比，以及基线ML方法相比，所提出的MAP方法在各种场景下表现出优越的分离性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文章提出了一种基于非负张量分解的方法，用于从Ambisonic麦克风信号中分离声源。</li>
<li>该方法结合了声源的到达方向（DOA）的先验知识，通过最大后验（MAP）框架中的空间协方差矩阵（SCM）约束来实现。</li>
<li>文章详细推导了四种算法，这些算法基于两种不同的成本函数和两种先验概率分布。</li>
<li>实验评估使用了多种源信号数据集，包括音乐片段和语音片段，并考虑了不同数量的声源分离情况。</li>
<li>算法在不同条件下进行了评估，包括不同的球面谐波阶数、混响时间水平以及非理想的先验知识条件。</li>
<li>与其他方法和基线ML方法相比，提出的MAP方法在各种场景下表现出更好的声源分离性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10305">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bee35987358e51e2fea22fb68d8120bd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Rhythm-and-Voice-Conversion-of-Dysarthric-to-Healthy-Speech-for-ASR"><a href="#Unsupervised-Rhythm-and-Voice-Conversion-of-Dysarthric-to-Healthy-Speech-for-ASR" class="headerlink" title="Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech   for ASR"></a>Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech   for ASR</h2><p><strong>Authors:Karl El Hajal, Enno Hermann, Ajinkya Kulkarni, Mathew Magimai. -Doss</strong></p>
<p>Automatic speech recognition (ASR) systems are well known to perform poorly on dysarthric speech. Previous works have addressed this by speaking rate modification to reduce the mismatch with typical speech. Unfortunately, these approaches rely on transcribed speech data to estimate speaking rates and phoneme durations, which might not be available for unseen speakers. Therefore, we combine unsupervised rhythm and voice conversion methods based on self-supervised speech representations to map dysarthric to typical speech. We evaluate the outputs with a large ASR model pre-trained on healthy speech without further fine-tuning and find that the proposed rhythm conversion especially improves performance for speakers of the Torgo corpus with more severe cases of dysarthria. Code and audio samples are available at <a target="_blank" rel="noopener" href="https://idiap.github.io/RnV">https://idiap.github.io/RnV</a> . </p>
<blockquote>
<p>自动语音识别（ASR）系统在构音不全的语音上的表现普遍不佳。以前的研究通过调整语速来减少与正常语音的不匹配来解决这个问题。然而，这些方法依赖于转录语音数据来估计语速和音素持续时间，对于未见过的说话者可能无法获得这些数据。因此，我们结合了基于自监督语音表示的无人监督节奏和声音转换方法，将构音不全的语音映射到正常语音。我们使用在健康语音上预训练的大的ASR模型对输出进行评估，无需进一步微调。我们发现，所提出的节奏转换特别提高了Torgo语料库中构音不全更为严重的说话者的性能。代码和音频样本可在<a target="_blank" rel="noopener" href="https://idiap.github.io/RnV%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://idiap.github.io/RnV上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10256v1">PDF</a> Accepted at ICASSP 2025 Satellite Workshop: Workshop on Speech   Pathology Analysis and DEtection (SPADE)</p>
<p><strong>Summary</strong>：针对患有语言障碍（如口齿不清）的人自动语音识别（ASR）系统表现不佳的问题，研究者提出了一种结合无监督节奏和语音转换方法的方法。该方法基于自我监督的语音表征，可将口齿不清的语音映射为正常语音。在不进行微调的情况下，使用大型ASR模型对输出进行评估发现，对于患有严重语言障碍的Torgo语料库中的说话者，所提出的节奏转换方法尤其能提高性能。相关代码和音频样本可在<a target="_blank" rel="noopener" href="https://idiap.github.io/RnV%E6%89%BE%E5%88%B0%E3%80%82">https://idiap.github.io/RnV找到。</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ASR系统在处理口齿不清的语音时表现不佳。</li>
<li>之前的研究通过调整语速来解决这一问题，但这种方法依赖于转录语音数据来估计语速和音素持续时间，这可能不适用于未见过的说话者。</li>
<li>研究者提出了一种结合无监督节奏和语音转换方法的新方法，基于自我监督的语音表征，将口齿不清的语音转换为正常语音。</li>
<li>该方法在不进行额外微调的情况下，使用大型ASR模型进行评估。</li>
<li>对于患有严重语言障碍的说话者，尤其是Torgo语料库中的说话者，新方法的性能显著提高。</li>
<li>相关的代码和音频样本可以在特定网站找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-376e2e9ab76a8dc5d32b50594bfa3576.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18e5bca1db297404851db6dc31f8986c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4a6a6a3d12f5a9d97bf91d8ef3a948a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f090f8e9a1ac8722ce8deb6ad7e08b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d4f8cc61a1327917106f732e803a1f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Conditional-Latent-Diffusion-Based-Speech-Enhancement-Via-Dual-Context-Learning"><a href="#Conditional-Latent-Diffusion-Based-Speech-Enhancement-Via-Dual-Context-Learning" class="headerlink" title="Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context   Learning"></a>Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context   Learning</h2><p><strong>Authors:Shengkui Zhao, Zexu Pan, Kun Zhou, Yukun Ma, Chong Zhang, Bin Ma</strong></p>
<p>Recently, the application of diffusion probabilistic models has advanced speech enhancement through generative approaches. However, existing diffusion-based methods have focused on the generation process in high-dimensional waveform or spectral domains, leading to increased generation complexity and slower inference speeds. Additionally, these methods have primarily modelled clean speech distributions, with limited exploration of noise distributions, thereby constraining the discriminative capability of diffusion models for speech enhancement. To address these issues, we propose a novel approach that integrates a conditional latent diffusion model (cLDM) with dual-context learning (DCL). Our method utilizes a variational autoencoder (VAE) to compress mel-spectrograms into a low-dimensional latent space. We then apply cLDM to transform the latent representations of both clean speech and background noise into Gaussian noise by the DCL process, and a parameterized model is trained to reverse this process, conditioned on noisy latent representations and text embeddings. By operating in a lower-dimensional space, the latent representations reduce the complexity of the generation process, while the DCL process enhances the model’s ability to handle diverse and unseen noise environments. Our experiments demonstrate the strong performance of the proposed approach compared to existing diffusion-based methods, even with fewer iterative steps, and highlight the superior generalization capability of our models to out-of-domain noise datasets (<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio">https://github.com/modelscope/ClearerVoice-Studio</a>). </p>
<blockquote>
<p>近期，扩散概率模型在生成方法方面推动了语音增强的应用发展。然而，现有的基于扩散的方法主要关注高维波形或频谱域的生成过程，导致生成复杂度增加和推理速度较慢。此外，这些方法主要对干净语音分布进行建模，对噪声分布的探索有限，从而限制了扩散模型在语音增强方面的判别能力。为了解决这些问题，我们提出了一种将条件潜在扩散模型（cLDM）与双重上下文学习（DCL）相结合的新方法。我们的方法利用变分自编码器（VAE）将梅尔频谱图压缩到低维潜在空间。然后，我们应用cLDM通过DCL过程将干净语音和背景噪声的潜在表示转换为高斯噪声，并训练参数化模型以在给定带有噪声的潜在表示和文本嵌入的情况下反转这一过程。通过在低维空间中进行操作，潜在表示降低了生成过程的复杂性，而DCL过程提高了模型处理多样化和未见过的噪声环境的能力。我们的实验表明，与现有的基于扩散的方法相比，所提出的方法表现出强大的性能，即使在较少的迭代步骤中也是如此，并突出了我们模型对域外噪声数据集的出色泛化能力（<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio%EF%BC%89%E3%80%82">https://github.com/modelscope/ClearerVoice-Studio）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10052v1">PDF</a> 5 pages, 1 figure, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种结合条件潜在扩散模型（cLDM）和双语境学习（DCL）的语音增强新方法。该方法利用变分自编码器（VAE）将梅尔频谱图压缩到低维潜在空间，应用cLDM将干净语音和背景噪声的潜在表示转化为高斯噪声，再通过训练反向过程模型恢复语音，以处理带有噪声的潜在表示和文本嵌入。该方法在低维空间操作，简化生成过程，提高模型处理不同和未见噪声环境的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散概率模型在语音增强中的生成方法得到了新的应用。</li>
<li>当前扩散方法主要处理高维波形或谱域生成过程，导致生成复杂度增加和推理速度较慢。</li>
<li>现有方法主要建模干净语音分布，对噪声分布探索有限，限制了扩散模型在语音增强中的判别能力。</li>
<li>提出了一种新的结合条件潜在扩散模型（cLDM）和双语境学习（DCL）的方法。</li>
<li>利用变分自编码器（VAE）将梅尔频谱图压缩到低维潜在空间以降低生成复杂度。</li>
<li>cLDM用于将干净语音和背景噪声的潜在表示转化为高斯噪声。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fc46334b2688cbc865bbac72b819a167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc3bbeb7db10053cbed76c3c2ca67cf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae1408df9f02884686d64eb58249a330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68ea5e36e015d9606c5b286c07ee4898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64b376cc5b33b6b7de28e99bf02eac7a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HiFi-SR-A-Unified-Generative-Transformer-Convolutional-Adversarial-Network-for-High-Fidelity-Speech-Super-Resolution"><a href="#HiFi-SR-A-Unified-Generative-Transformer-Convolutional-Adversarial-Network-for-High-Fidelity-Speech-Super-Resolution" class="headerlink" title="HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial   Network for High-Fidelity Speech Super-Resolution"></a>HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial   Network for High-Fidelity Speech Super-Resolution</h2><p><strong>Authors:Shengkui Zhao, Kun Zhou, Zexu Pan, Yukun Ma, Chong Zhang, Bin Ma</strong></p>
<p>The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, a unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features a unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as a powerful encoder, converting low-resolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into high-resolution waveforms. To enhance high-frequency fidelity, we incorporate a multi-band, multi-scale time-frequency discriminator, along with a multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios (<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio">https://github.com/modelscope/ClearerVoice-Studio</a>). </p>
<blockquote>
<p>近期，生成对抗网络（GANs）的应用已基于如梅尔频谱图等中间表示推动了语音超分辨率（SR）的发展。然而，现有的SR方法通常依赖于独立训练并连接的网络，这可能导致表示不一致和语音质量差，特别是在域外场景中。在这项工作中，我们提出了HiFi-SR，这是一个利用端到端对抗训练实现高保真语音超分辨率的统一网络。我们的模型特点是一个统一的transformer-卷积生成器，旨在无缝处理潜在表示的预测及其转换为时间域波形。Transformer网络作为一个强大的编码器，将低分辨率梅尔频谱图转换为潜在空间表示，而卷积网络将这些表示放大为高分辨率波形。为了提高高频保真度，我们在对抗训练过程中融入了多频带、多尺度的时间-频率鉴别器，以及多尺度梅尔重建损失。HiFi-SR通用性很强，能够将4kHz至32kHz之间的任何输入语音信号放大到48kHz的采样率。实验结果表明，无论是在域内还是域外场景中，HiFi-SR在客观指标和ABX偏好测试上均显著优于现有语音SR方法。（<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio%EF%BC%89">https://github.com/modelscope/ClearerVoice-Studio）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10045v1">PDF</a> 5 pages, 5 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于生成对抗网络（GANs）的高保真语音超分辨率技术（HiFi-SR）。HiFi-SR采用端到端的对抗训练，实现高保真语音超分辨率。模型采用统一的全卷积生成器，处理潜在表示的预测及其转换为时间域波形。通过强大的转换器网络将低分辨率梅尔频谱图转换为潜在空间表示，而卷积网络则将这些表示扩展到高分辨率波形。同时引入多频带、多尺度的时间频率判别器和多尺度梅尔重建损失，以提高高频保真度。HiFi-SR具有通用性，能将输入语音信号的采样率从任意4kHz至32kHz提升到48kHz。实验结果显示，HiFi-SR在客观指标和ABX偏好测试中均显著优于现有语音超分辨率方法，适用于不同领域和跨领域场景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的基于生成对抗网络（GANs）的语音超分辨率技术HiFi-SR。</li>
<li>HiFi-SR利用端到端的对抗训练来实现高保真语音超分辨率。</li>
<li>模型包含统一的全卷积生成器，用于处理潜在表示的预测和转换为时间域波形。</li>
<li>转换器网络将低分辨率梅尔频谱图转换为潜在空间表示。</li>
<li>多频带、多尺度的时间频率判别器和多尺度梅尔重建损失被用于提高高频保真度。</li>
<li>HiFi-SR具有通用性，可将输入语音信号的采样率提升到更高的标准（如从任意范围内的低频到高频）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c1639cc2633350b69256bf329dbd5cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a91e50b0aa06008e76b4dd6747a8ffba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-924ca8ceb5a3211f8737fd245aad0a82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-392be9a96ba566740cedf300d9aa30d0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TalkingEyes-Pluralistic-Speech-Driven-3D-Eye-Gaze-Animation"><a href="#TalkingEyes-Pluralistic-Speech-Driven-3D-Eye-Gaze-Animation" class="headerlink" title="TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation"></a>TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation</h2><p><strong>Authors:Yixiang Zhuang, Chunshan Ma, Yao Cheng, Xuan Cheng, Jing Liao, Juncong Lin</strong></p>
<p>Although significant progress has been made in the field of speech-driven 3D facial animation recently, the speech-driven animation of an indispensable facial component, eye gaze, has been overlooked by recent research. This is primarily due to the weak correlation between speech and eye gaze, as well as the scarcity of audio-gaze data, making it very challenging to generate 3D eye gaze motion from speech alone. In this paper, we propose a novel data-driven method which can generate diverse 3D eye gaze motions in harmony with the speech. To achieve this, we firstly construct an audio-gaze dataset that contains about 14 hours of audio-mesh sequences featuring high-quality eye gaze motion, head motion and facial motion simultaneously. The motion data is acquired by performing lightweight eye gaze fitting and face reconstruction on videos from existing audio-visual datasets. We then tailor a novel speech-to-motion translation framework in which the head motions and eye gaze motions are jointly generated from speech but are modeled in two separate latent spaces. This design stems from the physiological knowledge that the rotation range of eyeballs is less than that of head. Through mapping the speech embedding into the two latent spaces, the difficulty in modeling the weak correlation between speech and non-verbal motion is thus attenuated. Finally, our TalkingEyes, integrated with a speech-driven 3D facial motion generator, can synthesize eye gaze motion, eye blinks, head motion and facial motion collectively from speech. Extensive quantitative and qualitative evaluations demonstrate the superiority of the proposed method in generating diverse and natural 3D eye gaze motions from speech. The project page of this paper is: <a target="_blank" rel="noopener" href="https://lkjkjoiuiu.github.io/TalkingEyes_Home/">https://lkjkjoiuiu.github.io/TalkingEyes_Home/</a> </p>
<blockquote>
<p>尽管最近在语音驱动的三维面部动画领域取得了重大进展，但最近的研究忽略了不可或缺的面部组成部分——眼神的语音驱动动画。这主要是因为语音和眼神之间的弱相关性，以及音频注视数据的稀缺，使得仅从语音生成三维眼神运动非常具有挑战性。在本文中，我们提出了一种新颖的数据驱动方法，可以生成与语音协调的多样化的三维眼神运动。为实现这一点，我们首先构建了一个音频注视数据集，其中包含约14小时的音频网格序列，同时呈现高质量的眼神运动、头部运动和面部运动。运动数据是通过对来自现有视听数据集的视频执行轻量级眼神注视拟合和面部重建而获得的。然后，我们定制了一个新颖的语音到运动翻译框架，其中头部运动和眼神运动是从语音中联合生成的，但在两个单独的潜在空间中建模。这种设计源于生理知识，即眼球的旋转范围小于头部。通过将语音嵌入映射到这两个潜在空间，可以减轻对语音和非语言运动之间弱相关性的建模难度。最后，我们的TalkingEyes结合语音驱动的三维面部运动生成器，可以从语音中综合生成眼神运动、眨眼、头部运动和面部运动。大量的定量和定性评估证明，该方法在通过语音生成多样化和自然的三维眼神运动方面具有优越性。该论文的项目页面为：<a target="_blank" rel="noopener" href="https://lkjkjoiuiu.github.io/TalkingEyes_Home/">https://lkjkjoiuiu.github.io/TalkingEyes_Home/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09921v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期虽然语音驱动3D面部动画领域取得了显著进展，但关于面部重要组成部分——眼动的语音驱动动画研究却被忽视。本文提出一种数据驱动的方法，能生成与语音相协调的多样化的3D眼动。首先构建了一个包含约14小时音频网格序列的视听数据集，同时进行眼动、头部运动和面部运动的高质量采集。然后设计了一个新颖的语音到动作转换框架，头部运动和眼动从语音中产生，但分别在两个独立潜空间建模。最后，通过集成语音驱动的3D面部动画生成器，我们的TalkingEyes能从语音中合成眼动、眨眼、头部和面部运动。广泛定量和定性评估表明，该方法在生成多样且自然的3D眼动方面表现卓越。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>近期研究忽视了语音驱动3D面部动画中的眼动部分。</li>
<li>提出了一个数据驱动的方法，生成多样化的与语音协调的3D眼动。</li>
<li>构建了一个包含高质量视听数据的音频网格数据集。</li>
<li>设计了一个新颖的语音到动作转换框架，将头部和眼动分开建模。</li>
<li>集成语音驱动的3D面部动画生成器，能合成多种动作。</li>
<li>方法在生成自然且多样的3D眼动方面表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09921">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-25c9156a034129cae5a665735f1a17b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6a763936a1a5af284b409c8061fb23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17a018e6b270864e17fecceb5aa20544.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2951e2636e6e71fb628ba5ffea3e13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2c85138a6ee5eab3140ae3ff1788df5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53b823a8dc9e8fb5ec9666427dd47fc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23bbd8cbf3691fe6fe589b818a729464.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="persoDA-Personalized-Data-Augmentation-for-Personalized-ASR"><a href="#persoDA-Personalized-Data-Augmentation-for-Personalized-ASR" class="headerlink" title="persoDA: Personalized Data Augmentation for Personalized ASR"></a>persoDA: Personalized Data Augmentation for Personalized ASR</h2><p><strong>Authors:Pablo Peso Parada, Spyros Fontalis, Md Asif Jalal, Karthikeyan Saravanan, Anastasios Drosou, Mete Ozay, Gil Ho Lee, Jungin Lee, Seokyeong Jung</strong></p>
<p>Data augmentation (DA) is ubiquitously used in training of Automatic Speech Recognition (ASR) models. DA offers increased data variability, robustness and generalization against different acoustic distortions. Recently, personalization of ASR models on mobile devices has been shown to improve Word Error Rate (WER). This paper evaluates data augmentation in this context and proposes persoDA; a DA method driven by user’s data utilized to personalize ASR. persoDA aims to augment training with data specifically tuned towards acoustic characteristics of the end-user, as opposed to standard augmentation based on Multi-Condition Training (MCT) that applies random reverberation and noises. Our evaluation with an ASR conformer-based baseline trained on Librispeech and personalized for VOICES shows that persoDA achieves a 13.9% relative WER reduction over using standard data augmentation (using random noise &amp; reverberation). Furthermore, persoDA shows 16% to 20% faster convergence over MCT. </p>
<blockquote>
<p>数据增强（DA）在自动语音识别（ASR）模型的训练中无处不在。DA提供了增加数据变化、稳健性和对不同声学失真的泛化能力。最近，在移动设备上对ASR模型进行个性化展示，能够改善单词错误率（WER）。本文在此背景下评估数据增强，并提出persoDA；一种利用用户数据进行驱动的DA方法，用于个性化ASR。persoDA旨在使用专门针对最终用户声学特征调整的数据来增强训练，而不是基于多条件训练（MCT）的标准增强，后者应用随机混响和噪声。我们在Librispeech上训练的基于ASR conformer的基线并进行个性化处理用于语音评估，发现与标准数据增强相比（使用随机噪声和混响），persoDA相对减少了WER的百分比为百分之十三点九。此外，相较于MCT来说，persoDA的收敛速度提高了百分之十六到百分之二十。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09113v2">PDF</a> ICASSP’25-Copyright 2025 IEEE. Personal use of this material is   permitted. Permission from IEEE must be obtained for all other uses, in any   current or future media, including reprinting&#x2F;republishing this material for   advertising or promotional purposes, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work in other works</p>
<p><strong>Summary</strong></p>
<p>数据增强（DA）在自动语音识别（ASR）模型的训练中应用广泛。本文评估了个人化ASR模型中数据增强的效果，并提出一种名为persoDA的数据增强方法。该方法利用用户数据驱动，旨在针对最终用户的声学特性进行个性化训练，不同于基于多条件训练（MCT）的标准数据增强方法。实验结果显示，persoDA相对于使用随机噪声和回声的标准数据增强方法，实现了相对词错误率（WER）降低13.9%，并且收敛速度提高了16%至20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据增强在自动语音识别模型训练中起着重要作用，能提高数据多变性和模型的稳健性。</li>
<li>针对移动设备上的语音识别模型进行个性化可以提高词错误率（WER）。</li>
<li>本论文提出一种名为persoDA的数据增强方法，根据用户的声学特性进行个性化训练。</li>
<li>persoDA相对于标准的多条件训练（MCT），实现了更低的词错误率（WER）。</li>
<li>persoDA在收敛速度上较MCT有显著提升。</li>
<li>实验结果显示，persoDA在基于Librispeech的语音识别人体工学模型上取得了良好效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7ef72b10e9a0fdc35a013f9700e536ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af275ab2812330140db0dc8942b5f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-077b60f4d996a62378f6026433dee485.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Joint-Automatic-Speech-Recognition-And-Structure-Learning-For-Better-Speech-Understanding"><a href="#Joint-Automatic-Speech-Recognition-And-Structure-Learning-For-Better-Speech-Understanding" class="headerlink" title="Joint Automatic Speech Recognition And Structure Learning For Better   Speech Understanding"></a>Joint Automatic Speech Recognition And Structure Learning For Better   Speech Understanding</h2><p><strong>Authors:Jiliang Hu, Zuchao Li, Mengjia Shen, Haojun Ai, Sheng Li, Jun Zhang</strong></p>
<p>Spoken language understanding (SLU) is a structure prediction task in the field of speech. Recently, many works on SLU that treat it as a sequence-to-sequence task have achieved great success. However, This method is not suitable for simultaneous speech recognition and understanding. In this paper, we propose a joint speech recognition and structure learning framework (JSRSL), an end-to-end SLU model based on span, which can accurately transcribe speech and extract structured content simultaneously. We conduct experiments on name entity recognition and intent classification using the Chinese dataset AISHELL-NER and the English dataset SLURP. The results show that our proposed method not only outperforms the traditional sequence-to-sequence method in both transcription and extraction capabilities but also achieves state-of-the-art performance on the two datasets. </p>
<blockquote>
<p>语音语言理解（SLU）是语音领域的一种结构预测任务。最近，将SLU视为序列到序列任务的许多工作都取得了巨大的成功。然而，这种方法不适用于同步语音识别和理解。在本文中，我们提出了一种联合语音识别和结构学习框架（JSRSL），这是一种基于范围的端到端SLU模型，可以同时准确转录语音并提取结构内容。我们使用中国AISHELL-NER数据集和英语SLURP数据集进行命名实体识别和意图分类实验。结果表明，我们提出的方法不仅在转录和提取能力上优于传统的序列到序列方法，而且在两个数据集上达到了最新的性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07329v2">PDF</a> 5 pages, 2 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种基于区段的联合语音识别和结构学习框架（JSRSL），可同时准确转录语音并提取结构化内容。实验表明，该方法在中文数据集AISHELL-NER和英文数据集SLURP上的表现优于传统的序列到序列方法，并在两个数据集上均达到了最新的技术水平。它不仅改善了语音识别和理解的同步性，而且提高了转录和提取能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spoken language understanding (SLU)是语音领域的一个结构预测任务。序列到序列的SLU方法在多个研究工作中取得显著成果，但不支持语音的实时识别和语义理解。</li>
<li>为解决上述问题，提出了一种新的联合语音识别和结构学习框架（JSRSL），它结合了实时的语音识别和结构化内容提取。</li>
<li>JSRSL是一个基于区段的端到端模型，能够同时准确转录语音并提取结构化的内容。这为解决语言理解的难题提供了一种有效的途径。</li>
<li>该方法使用了两个数据集进行测试：中文数据集AISHELL-NER和英文数据集SLURP。实验结果证明了其优越性。</li>
<li>JSRSL不仅在转录能力上超越了传统的序列到序列方法，而且在两个数据集上都实现了前沿性能。这说明它能够准确地识别和解释复杂的语音信号和含义丰富的句子结构。 </li>
<li>JSRSL框架对于未来的语言处理任务具有广泛的应用前景，特别是在语音识别、机器翻译等领域。其强大的性能使其成为解决复杂语言任务的有效工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b5e8fce9e18239de17e9e69363ae599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc6bf83485a8dacb7fed11cd74d77555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e64bfe3ab29c3ce689364c4445d56607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-753ec3a1e1c08bf4bdf0db6da29d540e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-948fe4d77343f211364ce3ed1ec74e22.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Uncovering-the-Visual-Contribution-in-Audio-Visual-Speech-Recognition"><a href="#Uncovering-the-Visual-Contribution-in-Audio-Visual-Speech-Recognition" class="headerlink" title="Uncovering the Visual Contribution in Audio-Visual Speech Recognition"></a>Uncovering the Visual Contribution in Audio-Visual Speech Recognition</h2><p><strong>Authors:Zhaofeng Lin, Naomi Harte</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) combines auditory and visual speech cues to enhance the accuracy and robustness of speech recognition systems. Recent advancements in AVSR have improved performance in noisy environments compared to audio-only counterparts. However, the true extent of the visual contribution, and whether AVSR systems fully exploit the available cues in the visual domain, remains unclear. This paper assesses AVSR systems from a different perspective, by considering human speech perception. We use three systems: Auto-AVSR, AVEC and AV-RelScore. We first quantify the visual contribution using effective SNR gains at 0 dB and then investigate the use of visual information in terms of its temporal distribution and word-level informativeness. We show that low WER does not guarantee high SNR gains. Our results suggest that current methods do not fully exploit visual information, and we recommend future research to report effective SNR gains alongside WERs. </p>
<blockquote>
<p>视听语音识别（AVSR）结合了听觉和视觉语音线索，以提高语音识别系统的准确性和稳健性。与仅使用音频的同类系统相比，AVSR的最新进展在嘈杂环境中的性能有所提高。然而，视觉贡献的真正程度，以及AVSR系统是否充分利用视觉领域中的可用线索，仍然不清楚。本文通过考虑人类语音感知，从另一个角度评估AVSR系统。我们使用三种系统：Auto-AVSR、AVEC和AV-RelScore。我们首先使用有效信噪比增益（在0分贝处）量化视觉贡献，然后研究视觉信息的使用与其时间分布和词汇级别信息量有关。我们表明，低词错误率并不保证高信噪比增益。我们的结果表明，当前的方法并没有充分利用视觉信息，我们建议未来的研究在报告词错误率的同时，也要报告有效的信噪比增益。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17129v2">PDF</a> 5 pages, 2 figures. Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     音频视觉语音识别（AVSR）结合听觉和视觉语音线索，提高了语音识别系统的准确性和稳健性。最新研究表明，相较于只使用音频的识别系统，AVSR在噪声环境中的表现有所提升。然而，视觉贡献的真实程度以及AVSR系统是否完全利用视觉领域的可用线索尚不清楚。本文通过借鉴人类语音感知来评估AVSR系统，使用Auto-AVSR、AVEC和AV-RelScore三种系统进行研究。我们量化视觉贡献并研究视觉信息的时序分布和词汇级别的信息量。研究发现，低词错误率并不保证信噪比增益高。我们的结果暗示当前的方法并未完全利用视觉信息，建议未来的研究在报告词错误率的同时也要报告有效的信噪比增益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSR结合了听觉和视觉语音线索，提高了语音识别系统的准确性及稳健性。</li>
<li>相较于音频识别系统，AVSR在噪声环境中的表现有所提升。</li>
<li>视觉贡献的真实程度尚不清楚，需要进一步研究。</li>
<li>通过借鉴人类语音感知来评估AVSR系统。</li>
<li>研究中使用了三种系统：Auto-AVSR、AVEC和AV-RelScore。</li>
<li>视觉信息的贡献被量化，同时研究了其在时序分布和词汇级别的信息量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d66cfe72dd0469b5d565b6c465020b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0060dad4ae9fa14d93223b8518ca324.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a715eef15e77a331d0276aa3ba9a089a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-464af9fc67d376bc83735961f31f27ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84cfb859102f8af4e3ffb64688ae83c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-Redundant-Is-the-Transformer-Stack-in-Speech-Representation-Models"><a href="#How-Redundant-Is-the-Transformer-Stack-in-Speech-Representation-Models" class="headerlink" title="How Redundant Is the Transformer Stack in Speech Representation Models?"></a>How Redundant Is the Transformer Stack in Speech Representation Models?</h2><p><strong>Authors:Teresa Dorszewski, Albert Kjøller Jacobsen, Lenka Tětková, Lars Kai Hansen</strong></p>
<p>Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model’s predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models. </p>
<blockquote>
<p>自监督语音表示模型，尤其是利用转换器架构的模型，在语音识别、说话人识别和情感检测等任务中表现出了卓越的性能。关于转换器模型的研究揭示了层之间的高度冗余和潜在的大量剪枝可能性，我们将在这里基于转换器结构的语音表示模型进行研究。我们采用三种相似性度量方法（余弦相似性、中心化内核对齐和相互最近邻对齐）对语音表示模型中的层相似性进行了详细分析。我们的研究结果表明了一种高相似性的块状结构，这表明了主要的两个处理步骤和显著的层冗余性。我们展示了无需进行二次训练即可有效地修剪基于转换器的语音表示模型，在减少高达40%的转换器层的同时，保持模型预测能力的95%以上。此外，我们还采用了一种知识蒸馏方法，用模仿层替代了整个转换器堆栈，将网络规模减少了95-98%，推理时间减少了高达94%。这种计算负载的实质性下降发生在没有明显的性能损失的情况下，这表明转换器堆栈对于语音表示模型的下游应用几乎是完全冗余的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16302v2">PDF</a> To appear at ICASSP 2025 (excluding appendix)</p>
<p><strong>Summary</strong><br>     基于转换器架构的语音自监督表示模型在多个任务中表现出卓越性能，如语音识别、说话人识别和情感检测。研究发现转换器模型层之间存在高冗余性，通过层相似性分析揭示了其块状结构特点，并展示了显著的处理冗余性。在不进行训练后处理的情况下，通过修剪层实现模型效率提升，减少高达40%的转换器层同时保持模型预测能力的95%以上。此外，采用知识蒸馏法以模仿层代替整个转换器堆栈，降低网络规模高达98%，并可将推理时间缩短高达94%，且在计算负荷显著降低的情况下几乎没有性能损失。这显示出对于语音表示模型的下游应用而言，转换器堆栈几乎完全冗余。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督语音表示模型，特别是基于转换器架构的模型，在多种任务中表现卓越。</li>
<li>转换器模型层之间存在高冗余性。</li>
<li>通过层相似性分析揭示了模型块状结构特点。</li>
<li>通过修剪层可以提升模型效率，显著减少计算负荷并保持较高预测性能。</li>
<li>知识蒸馏法可用于以模仿层代替转换器堆栈的大部分部分。</li>
<li>网络规模可大幅缩减，同时推理时间显著减少。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16302">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e12b4e1a35b89a35f0003416839d3580.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5392713e8cbb7badf3a8eb07f915bb72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8bdaac305066f9911b7791b6ad1f9cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9ef6862ae73cb9cf3944cbaef309a81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2b1a36b59633eefabae5081ed21ebd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90e1f8d2e567b2151f18d9aacfe0efea.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Annealed-Multiple-Choice-Learning-Overcoming-limitations-of-Winner-takes-all-with-annealing"><a href="#Annealed-Multiple-Choice-Learning-Overcoming-limitations-of-Winner-takes-all-with-annealing" class="headerlink" title="Annealed Multiple Choice Learning: Overcoming limitations of   Winner-takes-all with annealing"></a>Annealed Multiple Choice Learning: Overcoming limitations of   Winner-takes-all with annealing</h2><p><strong>Authors:David Perera, Victor Letzelter, Théo Mariotte, Adrien Cortés, Mickael Chen, Slim Essid, Gaël Richard</strong></p>
<p>We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation. </p>
<blockquote>
<p>我们介绍了退火多选项学习（aMCL），它将模拟退火与MCL相结合。MCL是一个学习框架，通过预测一小部分合理的假设来处理模糊任务。这些假设是使用胜者全取（WTA）方案进行训练的，该方案促进了预测的多样性。然而，由于WTA的贪婪性质，该方案可能会任意收敛到次优的局部最小值。我们通过使用退火来克服这一局限性，退火增强了训练过程中假设空间的探索。我们从统计物理学和信息理论中汲取灵感，为模型训练轨迹提供了详细描述。此外，我们通过合成数据集、标准UCI基准测试以及语音分离等方面的广泛实验验证了我们的算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15580v3">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong>：结合模拟退火算法和MCL学习框架，提出退火多重选择学习（aMCL）方法。该方法解决了MCL在处理具有不确定性的任务时可能出现的问题，利用模拟退火提高预测的多样性和质量。通过统计物理学和信息理论，描述了模型训练轨迹。实验验证了在合成数据集、UCI标准基准和语音分离上的有效性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出新的学习算法aMCL，结合了模拟退火和MCL框架。</li>
<li>MCL在处理具有不确定性的任务时，通过预测一系列可能假设来处理歧义性。</li>
<li>使用Winner-takes-all（WTA）方案进行训练预测，但可能导致预测结果收敛于任意局部最小值。</li>
<li>模拟退火用于克服WTA方案的局限性，提高预测空间的探索能力。</li>
<li>利用统计物理学和信息理论为模型训练轨迹提供详细描述。</li>
<li>通过合成数据集、UCI标准基准和语音分离实验验证了算法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3707a9afe03dddabaef2dcc880faed07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6175a7f7deecd00f6a8d37ea69f45dcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71b4362678f86fcf2cf1272b57011442.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Zero-Shot-Chinese-English-Code-Switching-ASR-with-kNN-CTC-and-Gated-Monolingual-Datastores"><a href="#Improving-Zero-Shot-Chinese-English-Code-Switching-ASR-with-kNN-CTC-and-Gated-Monolingual-Datastores" class="headerlink" title="Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and   Gated Monolingual Datastores"></a>Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and   Gated Monolingual Datastores</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Hui Wang, Tian-Hao Zhang, Haoqin Sun, Xuechen Wang, Yong Qin</strong></p>
<p>The kNN-CTC model has proven to be effective for monolingual automatic speech recognition (ASR). However, its direct application to multilingual scenarios like code-switching, presents challenges. Although there is potential for performance improvement, a kNN-CTC model utilizing a single bilingual datastore can inadvertently introduce undesirable noise from the alternative language. To address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR) framework that employs dual monolingual datastores and a gated datastore selection mechanism to reduce noise interference. Our method selects the appropriate datastore for decoding each frame, ensuring the injection of language-specific information into the ASR process. We apply this framework to cutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive experiments demonstrate the remarkable effectiveness of our gated datastore mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR. </p>
<blockquote>
<p>kNN-CTC模型在单语种自动语音识别（ASR）中已被证明是有效的。然而，将其直接应用于多语种场景，如代码切换，还存在挑战。尽管有提高性能的潜力，但使用单一双语数据存储的kNN-CTC模型可能会无意中引入来自另一种语言的不希望有的噪声。为了解决这一问题，我们提出了一种基于kNN-CTC的代码切换ASR（CS-ASR）新框架，该框架采用双单语种数据存储和受控数据存储选择机制来减少噪声干扰。我们的方法选择适当的存储库来解码每一帧，确保语言特定信息的注入到ASR过程中。我们将该框架应用于最新的CTC模型，开发了一个先进的CS-ASR系统。大量实验表明，我们的受控数据存储机制在增强零中文-英文切换CS-ASR的性能方面具有显著效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03814v5">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>总结</strong></p>
<p>kNN-CTC模型在单语言自动语音识别（ASR）中表现良好，但在应用于多语言场景如代码切换时面临挑战。使用单一双语数据存储库的kNN-CTC模型可能会引入来自另一种语言的不必要噪声。为解决这一问题，我们提出了一种基于kNN-CTC的代码切换ASR（CS-ASR）框架，采用双单语言数据存储和门控数据存储选择机制以减少噪声干扰。该方法能选择适合每帧解码的数据存储，确保语言特定信息注入ASR过程。我们将此框架应用于前沿的CTC模型，开发出先进的CS-ASR系统。大量实验证明，我们的门控数据存储机制在提升零中文-英文代码切换ASR的性能方面具有显著效果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>kNN-CTC模型在单语言自动语音识别（ASR）中表现有效。</li>
<li>在多语言场景如代码切换中，直接使用kNN-CTC模型会面临挑战。</li>
<li>使用单一双语数据存储库可能会在kNN-CTC模型中引入来自另一种语言的不必要噪声。</li>
<li>提出了基于kNN-CTC的代码切换ASR（CS-ASR）框架。</li>
<li>CS-ASR框架采用双单语言数据存储和门控数据存储选择机制以减少噪声干扰。</li>
<li>门控数据存储选择机制能根据每帧解码选择合适的数据存储。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bc3d52bdf88a7cac3097c8c8ae0699c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa7f849a75523125316245422a79a9e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20580a63f9dd1037603951a2489b99f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-baf66f84fb75b8a7463289fdd6028b85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b473403b18a5e8c48499c807204fee2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4884c051eef4815391f0c4ef46ee2ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ae9a3243a00ac4246540783930d85e2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cec89486ac3ed8cec58639b45c1f4c48.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-01-21  GaussianAvatar-Editor Photorealistic Animatable Gaussian Head Avatar   Editor
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-71265a8141a1db97fec5668fc7ce0269.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-21  DPCL-Diff The Temporal Knowledge Graph Reasoning Based on Graph Node   Diffusion Model with Dual-Domain Periodic Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
