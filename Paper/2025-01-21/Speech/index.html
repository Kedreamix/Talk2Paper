<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-21  On Ambisonic Source Separation with Spatially Informed Non-negative   Tensor Factorization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a2c85138a6ee5eab3140ae3ff1788df5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-21-æ›´æ–°"><a href="#2025-01-21-æ›´æ–°" class="headerlink" title="2025-01-21 æ›´æ–°"></a>2025-01-21 æ›´æ–°</h1><h2 id="On-Ambisonic-Source-Separation-with-Spatially-Informed-Non-negative-Tensor-Factorization"><a href="#On-Ambisonic-Source-Separation-with-Spatially-Informed-Non-negative-Tensor-Factorization" class="headerlink" title="On Ambisonic Source Separation with Spatially Informed Non-negative   Tensor Factorization"></a>On Ambisonic Source Separation with Spatially Informed Non-negative   Tensor Factorization</h2><p><strong>Authors:Mateusz Guzik, Konrad Kowalczyk</strong></p>
<p>This article presents a Non-negative Tensor Factorization based method for sound source separation from Ambisonic microphone signals. The proposed method enables the use of prior knowledge about the Directions-of-Arrival (DOAs) of the sources, incorporated through a constraint on the Spatial Covariance Matrix (SCM) within a Maximum a Posteriori (MAP) framework. Specifically, this article presents a detailed derivation of four algorithms that are based on two types of cost functions, namely the squared Euclidean distance and the Itakura-Saito divergence, which are then combined with two prior probability distributions on the SCM, that is the Wishart and the Inverse Wishart. The experimental evaluation of the baseline Maximum Likelihood (ML) and the proposed MAP methods is primarily based on first-order Ambisonic recordings, using four different source signal datasets, three with musical pieces and one containing speech utterances. We consider under-determined, determined, as well as over-determined scenarios by separating two, four and six sound sources, respectively. Furthermore, we evaluate the proposed algorithms for different spherical harmonic orders and at different reverberation time levels, as well as in non-ideal prior knowledge conditions, for increasingly more corrupted DOAs. Overall, in comparison with beamforming and a state-of-the-art separation technique, as well as the baseline ML methods, the proposed MAP approach offers superior separation performance in a variety of scenarios, as shown by the analysis of the experimental evaluation results, in terms of the standard objective separation measures, such as the SDR, ISR, SIR and SAR. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéè´Ÿå¼ é‡åˆ†è§£çš„æ–¹æ³•ï¼Œç”¨äºä»Ambisonicéº¦å…‹é£ä¿¡å·ä¸­åˆ†ç¦»å£°æºã€‚è¯¥æ–¹æ³•å¯ä»¥åˆ©ç”¨å…³äºæºåˆ°è¾¾æ–¹å‘ï¼ˆDOAï¼‰çš„å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡æœ€å¤§åéªŒï¼ˆMAPï¼‰æ¡†æ¶ä¸­çš„ç©ºé—´åæ–¹å·®çŸ©é˜µï¼ˆSCMï¼‰çº¦æŸå°†å…¶ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡è¯¦ç»†æ¨å¯¼äº†å››ç§åŸºäºä¸¤ç§æˆæœ¬å‡½æ•°çš„ç®—æ³•ï¼Œå³å¹³æ–¹æ¬§å‡ é‡Œå¾—è·ç¦»å’ŒItakura-Saitoå‘æ•£ï¼Œç„¶åä¸SCMä¸Šçš„ä¸¤ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒå³Wishartå’ŒInverse Wishartç›¸ç»“åˆã€‚åŸºçº¿æœ€å¤§ä¼¼ç„¶ï¼ˆMLï¼‰å’Œæ‰€æå‡ºçš„MAPæ–¹æ³•çš„å®éªŒè¯„ä¼°ä¸»è¦åŸºäºç¬¬ä¸€é˜¶Ambisonicå½•éŸ³ï¼Œä½¿ç”¨å››ä¸ªä¸åŒçš„æºä¿¡å·æ•°æ®é›†ï¼Œå…¶ä¸­ä¸‰ä¸ªæ˜¯éŸ³ä¹ä½œå“ï¼Œä¸€ä¸ªæ˜¯åŒ…å«è¯­éŸ³è¯­å¥çš„ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ç¦»ä¸¤ä¸ªã€å››ä¸ªå’Œå…­ä¸ªå£°æºï¼Œåˆ†åˆ«è€ƒè™‘æ¬ å®šã€ç¡®å®šä»¥åŠè¿‡åº¦ç¡®å®šçš„æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ä¸åŒçš„çƒé¢è°æ³¢é˜¶æ•°å’Œä¸åŒçš„æ··å“æ—¶é—´æ°´å¹³ä»¥åŠéç†æƒ³çš„å…ˆéªŒçŸ¥è¯†æ¡ä»¶ä¸‹ï¼Œè¶Šæ¥è¶Šå—å¹²æ‰°çš„DOAè¿›è¡Œäº†ç®—æ³•è¯„ä¼°ã€‚æ€»ä½“è€Œè¨€ï¼Œä¸æ³¢æŸå½¢æˆå’Œæœ€æ–°çš„åˆ†ç¦»æŠ€æœ¯ä»¥åŠåŸºçº¿MLæ–¹æ³•ç›¸æ¯”ï¼Œæ ¹æ®å®éªŒè¯„ä¼°ç»“æœçš„åˆ†æï¼Œå¦‚åœ¨æ ‡å‡†å®¢è§‚åˆ†ç¦»æªæ–½ï¼ˆå¦‚SDRã€ISRã€SIRå’ŒSARï¼‰æ–¹é¢ï¼Œæ‰€æå‡ºçš„MAPæ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºä¼˜è¶Šçš„åˆ†ç¦»æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10305v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºéè´Ÿå¼ é‡åˆ†è§£çš„æ–¹æ³•ï¼Œç”¨äºä»Ambisonicéº¦å…‹é£ä¿¡å·ä¸­åˆ†ç¦»å£°æºã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æœ‰å…³å£°æºåˆ°è¾¾æ–¹å‘ï¼ˆDOAï¼‰çš„å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡æœ€å¤§åéªŒï¼ˆMAPï¼‰æ¡†æ¶ä¸­çš„ç©ºé—´åæ–¹å·®çŸ©é˜µï¼ˆSCMï¼‰çº¦æŸå°†å…¶èå…¥ã€‚æ–‡ç« è¯¦ç»†æ¨å¯¼äº†å››ç§ç®—æ³•ï¼Œè¿™äº›ç®—æ³•åŸºäºä¸¤ç§æˆæœ¬å‡½æ•°ï¼Œå³æ¬§å‡ é‡Œå¾—è·ç¦»çš„å¹³æ–¹å’ŒItakura-Saitoæ•£åº¦ï¼Œå¹¶ç»“åˆSCMä¸Šçš„ä¸¤ä¸ªå…ˆéªŒæ¦‚ç‡åˆ†å¸ƒï¼Œå³Wishartå’ŒInverse Wishartã€‚å®éªŒè¯„ä¼°äº†åŸºçº¿æœ€å¤§ä¼¼ç„¶ï¼ˆMLï¼‰å’Œæå‡ºçš„MAPæ–¹æ³•ï¼Œä¸»è¦åŸºäºä¸€é˜¶Ambisonicå½•éŸ³ï¼Œä½¿ç”¨å››ä¸ªä¸åŒçš„æºä¿¡å·æ•°æ®é›†ï¼Œå…¶ä¸­ä¸‰ä¸ªæ˜¯éŸ³ä¹ç‰‡æ®µï¼Œä¸€ä¸ªåŒ…å«è¯­éŸ³ç‰‡æ®µã€‚æˆ‘ä»¬è€ƒè™‘äº†æ¬ å®šã€ç¡®å®šä»¥åŠè¿‡åº¦å®šçš„æƒ…å†µï¼Œåˆ†åˆ«åˆ†ç¦»ä¸¤ä¸ªã€å››ä¸ªå’Œå…­ä¸ªå£°æºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ä¸åŒçƒé¢è°æ³¢é˜¶æ•°å’Œä¸åŒæ··å“æ—¶é—´æ°´å¹³ä»¥åŠéç†æƒ³å…ˆéªŒçŸ¥è¯†æ¡ä»¶ä¸‹çš„ç®—æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œé’ˆå¯¹è¶Šæ¥è¶Šå—è…èš€çš„DOAã€‚æ€»ä½“è€Œè¨€ï¼Œä¸æ³¢æŸå½¢æˆæ³•å’Œä¸€ç§æœ€æ–°çš„åˆ†ç¦»æŠ€æœ¯ç›¸æ¯”ï¼Œä»¥åŠåŸºçº¿MLæ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„MAPæ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„åˆ†ç¦»æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºéè´Ÿå¼ é‡åˆ†è§£çš„æ–¹æ³•ï¼Œç”¨äºä»Ambisonicéº¦å…‹é£ä¿¡å·ä¸­åˆ†ç¦»å£°æºã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å£°æºçš„åˆ°è¾¾æ–¹å‘ï¼ˆDOAï¼‰çš„å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡æœ€å¤§åéªŒï¼ˆMAPï¼‰æ¡†æ¶ä¸­çš„ç©ºé—´åæ–¹å·®çŸ©é˜µï¼ˆSCMï¼‰çº¦æŸæ¥å®ç°ã€‚</li>
<li>æ–‡ç« è¯¦ç»†æ¨å¯¼äº†å››ç§ç®—æ³•ï¼Œè¿™äº›ç®—æ³•åŸºäºä¸¤ç§ä¸åŒçš„æˆæœ¬å‡½æ•°å’Œä¸¤ç§å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>å®éªŒè¯„ä¼°ä½¿ç”¨äº†å¤šç§æºä¿¡å·æ•°æ®é›†ï¼ŒåŒ…æ‹¬éŸ³ä¹ç‰‡æ®µå’Œè¯­éŸ³ç‰‡æ®µï¼Œå¹¶è€ƒè™‘äº†ä¸åŒæ•°é‡çš„å£°æºåˆ†ç¦»æƒ…å†µã€‚</li>
<li>ç®—æ³•åœ¨ä¸åŒæ¡ä»¶ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸åŒçš„çƒé¢è°æ³¢é˜¶æ•°ã€æ··å“æ—¶é—´æ°´å¹³ä»¥åŠéç†æƒ³çš„å…ˆéªŒçŸ¥è¯†æ¡ä»¶ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•å’ŒåŸºçº¿MLæ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„MAPæ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„å£°æºåˆ†ç¦»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10305">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bee35987358e51e2fea22fb68d8120bd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Rhythm-and-Voice-Conversion-of-Dysarthric-to-Healthy-Speech-for-ASR"><a href="#Unsupervised-Rhythm-and-Voice-Conversion-of-Dysarthric-to-Healthy-Speech-for-ASR" class="headerlink" title="Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech   for ASR"></a>Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech   for ASR</h2><p><strong>Authors:Karl El Hajal, Enno Hermann, Ajinkya Kulkarni, Mathew Magimai. -Doss</strong></p>
<p>Automatic speech recognition (ASR) systems are well known to perform poorly on dysarthric speech. Previous works have addressed this by speaking rate modification to reduce the mismatch with typical speech. Unfortunately, these approaches rely on transcribed speech data to estimate speaking rates and phoneme durations, which might not be available for unseen speakers. Therefore, we combine unsupervised rhythm and voice conversion methods based on self-supervised speech representations to map dysarthric to typical speech. We evaluate the outputs with a large ASR model pre-trained on healthy speech without further fine-tuning and find that the proposed rhythm conversion especially improves performance for speakers of the Torgo corpus with more severe cases of dysarthria. Code and audio samples are available at <a target="_blank" rel="noopener" href="https://idiap.github.io/RnV">https://idiap.github.io/RnV</a> . </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨æ„éŸ³ä¸å…¨çš„è¯­éŸ³ä¸Šçš„è¡¨ç°æ™®éä¸ä½³ã€‚ä»¥å‰çš„ç ”ç©¶é€šè¿‡è°ƒæ•´è¯­é€Ÿæ¥å‡å°‘ä¸æ­£å¸¸è¯­éŸ³çš„ä¸åŒ¹é…æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºè½¬å½•è¯­éŸ³æ•°æ®æ¥ä¼°è®¡è¯­é€Ÿå’ŒéŸ³ç´ æŒç»­æ—¶é—´ï¼Œå¯¹äºæœªè§è¿‡çš„è¯´è¯è€…å¯èƒ½æ— æ³•è·å¾—è¿™äº›æ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºè‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºçš„æ— äººç›‘ç£èŠ‚å¥å’Œå£°éŸ³è½¬æ¢æ–¹æ³•ï¼Œå°†æ„éŸ³ä¸å…¨çš„è¯­éŸ³æ˜ å°„åˆ°æ­£å¸¸è¯­éŸ³ã€‚æˆ‘ä»¬ä½¿ç”¨åœ¨å¥åº·è¯­éŸ³ä¸Šé¢„è®­ç»ƒçš„å¤§çš„ASRæ¨¡å‹å¯¹è¾“å‡ºè¿›è¡Œè¯„ä¼°ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰€æå‡ºçš„èŠ‚å¥è½¬æ¢ç‰¹åˆ«æé«˜äº†Torgoè¯­æ–™åº“ä¸­æ„éŸ³ä¸å…¨æ›´ä¸ºä¸¥é‡çš„è¯´è¯è€…çš„æ€§èƒ½ã€‚ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://idiap.github.io/RnV%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://idiap.github.io/RnVä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10256v1">PDF</a> Accepted at ICASSP 2025 Satellite Workshop: Workshop on Speech   Pathology Analysis and DEtection (SPADE)</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ‚£æœ‰è¯­è¨€éšœç¢ï¼ˆå¦‚å£é½¿ä¸æ¸…ï¼‰çš„äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç»“åˆæ— ç›‘ç£èŠ‚å¥å’Œè¯­éŸ³è½¬æ¢æ–¹æ³•çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºè‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨å¾ï¼Œå¯å°†å£é½¿ä¸æ¸…çš„è¯­éŸ³æ˜ å°„ä¸ºæ­£å¸¸è¯­éŸ³ã€‚åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤§å‹ASRæ¨¡å‹å¯¹è¾“å‡ºè¿›è¡Œè¯„ä¼°å‘ç°ï¼Œå¯¹äºæ‚£æœ‰ä¸¥é‡è¯­è¨€éšœç¢çš„Torgoè¯­æ–™åº“ä¸­çš„è¯´è¯è€…ï¼Œæ‰€æå‡ºçš„èŠ‚å¥è½¬æ¢æ–¹æ³•å°¤å…¶èƒ½æé«˜æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://idiap.github.io/RnV%E6%89%BE%E5%88%B0%E3%80%82">https://idiap.github.io/RnVæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ASRç³»ç»Ÿåœ¨å¤„ç†å£é½¿ä¸æ¸…çš„è¯­éŸ³æ—¶è¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶é€šè¿‡è°ƒæ•´è¯­é€Ÿæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è¿™ç§æ–¹æ³•ä¾èµ–äºè½¬å½•è¯­éŸ³æ•°æ®æ¥ä¼°è®¡è¯­é€Ÿå’ŒéŸ³ç´ æŒç»­æ—¶é—´ï¼Œè¿™å¯èƒ½ä¸é€‚ç”¨äºæœªè§è¿‡çš„è¯´è¯è€…ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç»“åˆæ— ç›‘ç£èŠ‚å¥å’Œè¯­éŸ³è½¬æ¢æ–¹æ³•çš„æ–°æ–¹æ³•ï¼ŒåŸºäºè‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨å¾ï¼Œå°†å£é½¿ä¸æ¸…çš„è¯­éŸ³è½¬æ¢ä¸ºæ­£å¸¸è¯­éŸ³ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œé¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤§å‹ASRæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å¯¹äºæ‚£æœ‰ä¸¥é‡è¯­è¨€éšœç¢çš„è¯´è¯è€…ï¼Œå°¤å…¶æ˜¯Torgoè¯­æ–™åº“ä¸­çš„è¯´è¯è€…ï¼Œæ–°æ–¹æ³•çš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>ç›¸å…³çš„ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯ä»¥åœ¨ç‰¹å®šç½‘ç«™æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-376e2e9ab76a8dc5d32b50594bfa3576.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18e5bca1db297404851db6dc31f8986c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4a6a6a3d12f5a9d97bf91d8ef3a948a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f090f8e9a1ac8722ce8deb6ad7e08b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d4f8cc61a1327917106f732e803a1f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Conditional-Latent-Diffusion-Based-Speech-Enhancement-Via-Dual-Context-Learning"><a href="#Conditional-Latent-Diffusion-Based-Speech-Enhancement-Via-Dual-Context-Learning" class="headerlink" title="Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context   Learning"></a>Conditional Latent Diffusion-Based Speech Enhancement Via Dual Context   Learning</h2><p><strong>Authors:Shengkui Zhao, Zexu Pan, Kun Zhou, Yukun Ma, Chong Zhang, Bin Ma</strong></p>
<p>Recently, the application of diffusion probabilistic models has advanced speech enhancement through generative approaches. However, existing diffusion-based methods have focused on the generation process in high-dimensional waveform or spectral domains, leading to increased generation complexity and slower inference speeds. Additionally, these methods have primarily modelled clean speech distributions, with limited exploration of noise distributions, thereby constraining the discriminative capability of diffusion models for speech enhancement. To address these issues, we propose a novel approach that integrates a conditional latent diffusion model (cLDM) with dual-context learning (DCL). Our method utilizes a variational autoencoder (VAE) to compress mel-spectrograms into a low-dimensional latent space. We then apply cLDM to transform the latent representations of both clean speech and background noise into Gaussian noise by the DCL process, and a parameterized model is trained to reverse this process, conditioned on noisy latent representations and text embeddings. By operating in a lower-dimensional space, the latent representations reduce the complexity of the generation process, while the DCL process enhances the modelâ€™s ability to handle diverse and unseen noise environments. Our experiments demonstrate the strong performance of the proposed approach compared to existing diffusion-based methods, even with fewer iterative steps, and highlight the superior generalization capability of our models to out-of-domain noise datasets (<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio">https://github.com/modelscope/ClearerVoice-Studio</a>). </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨ç”Ÿæˆæ–¹æ³•æ–¹é¢æ¨åŠ¨äº†è¯­éŸ³å¢å¼ºçš„åº”ç”¨å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¸»è¦å…³æ³¨é«˜ç»´æ³¢å½¢æˆ–é¢‘è°±åŸŸçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯¼è‡´ç”Ÿæˆå¤æ‚åº¦å¢åŠ å’Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¸»è¦å¯¹å¹²å‡€è¯­éŸ³åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¯¹å™ªå£°åˆ†å¸ƒçš„æ¢ç´¢æœ‰é™ï¼Œä»è€Œé™åˆ¶äº†æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„åˆ¤åˆ«èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆcLDMï¼‰ä¸åŒé‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDCLï¼‰ç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†æ¢…å°”é¢‘è°±å›¾å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨cLDMé€šè¿‡DCLè¿‡ç¨‹å°†å¹²å‡€è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºè½¬æ¢ä¸ºé«˜æ–¯å™ªå£°ï¼Œå¹¶è®­ç»ƒå‚æ•°åŒ–æ¨¡å‹ä»¥åœ¨ç»™å®šå¸¦æœ‰å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºå’Œæ–‡æœ¬åµŒå…¥çš„æƒ…å†µä¸‹åè½¬è¿™ä¸€è¿‡ç¨‹ã€‚é€šè¿‡åœ¨ä½ç»´ç©ºé—´ä¸­è¿›è¡Œæ“ä½œï¼Œæ½œåœ¨è¡¨ç¤ºé™ä½äº†ç”Ÿæˆè¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œè€ŒDCLè¿‡ç¨‹æé«˜äº†æ¨¡å‹å¤„ç†å¤šæ ·åŒ–å’Œæœªè§è¿‡çš„å™ªå£°ç¯å¢ƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨è¾ƒå°‘çš„è¿­ä»£æ­¥éª¤ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå¹¶çªå‡ºäº†æˆ‘ä»¬æ¨¡å‹å¯¹åŸŸå¤–å™ªå£°æ•°æ®é›†çš„å‡ºè‰²æ³›åŒ–èƒ½åŠ›ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio%EF%BC%89%E3%80%82">https://github.com/modelscope/ClearerVoice-Studioï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10052v1">PDF</a> 5 pages, 1 figure, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆcLDMï¼‰å’ŒåŒè¯­å¢ƒå­¦ä¹ ï¼ˆDCLï¼‰çš„è¯­éŸ³å¢å¼ºæ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†æ¢…å°”é¢‘è°±å›¾å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œåº”ç”¨cLDMå°†å¹²å‡€è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºè½¬åŒ–ä¸ºé«˜æ–¯å™ªå£°ï¼Œå†é€šè¿‡è®­ç»ƒåå‘è¿‡ç¨‹æ¨¡å‹æ¢å¤è¯­éŸ³ï¼Œä»¥å¤„ç†å¸¦æœ‰å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºå’Œæ–‡æœ¬åµŒå…¥ã€‚è¯¥æ–¹æ³•åœ¨ä½ç»´ç©ºé—´æ“ä½œï¼Œç®€åŒ–ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜æ¨¡å‹å¤„ç†ä¸åŒå’Œæœªè§å™ªå£°ç¯å¢ƒçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­çš„ç”Ÿæˆæ–¹æ³•å¾—åˆ°äº†æ–°çš„åº”ç”¨ã€‚</li>
<li>å½“å‰æ‰©æ•£æ–¹æ³•ä¸»è¦å¤„ç†é«˜ç»´æ³¢å½¢æˆ–è°±åŸŸç”Ÿæˆè¿‡ç¨‹ï¼Œå¯¼è‡´ç”Ÿæˆå¤æ‚åº¦å¢åŠ å’Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å»ºæ¨¡å¹²å‡€è¯­éŸ³åˆ†å¸ƒï¼Œå¯¹å™ªå£°åˆ†å¸ƒæ¢ç´¢æœ‰é™ï¼Œé™åˆ¶äº†æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç»“åˆæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆcLDMï¼‰å’ŒåŒè¯­å¢ƒå­¦ä¹ ï¼ˆDCLï¼‰çš„æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†æ¢…å°”é¢‘è°±å›¾å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ä»¥é™ä½ç”Ÿæˆå¤æ‚åº¦ã€‚</li>
<li>cLDMç”¨äºå°†å¹²å‡€è¯­éŸ³å’ŒèƒŒæ™¯å™ªå£°çš„æ½œåœ¨è¡¨ç¤ºè½¬åŒ–ä¸ºé«˜æ–¯å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc46334b2688cbc865bbac72b819a167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc3bbeb7db10053cbed76c3c2ca67cf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae1408df9f02884686d64eb58249a330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68ea5e36e015d9606c5b286c07ee4898.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64b376cc5b33b6b7de28e99bf02eac7a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HiFi-SR-A-Unified-Generative-Transformer-Convolutional-Adversarial-Network-for-High-Fidelity-Speech-Super-Resolution"><a href="#HiFi-SR-A-Unified-Generative-Transformer-Convolutional-Adversarial-Network-for-High-Fidelity-Speech-Super-Resolution" class="headerlink" title="HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial   Network for High-Fidelity Speech Super-Resolution"></a>HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial   Network for High-Fidelity Speech Super-Resolution</h2><p><strong>Authors:Shengkui Zhao, Kun Zhou, Zexu Pan, Yukun Ma, Chong Zhang, Bin Ma</strong></p>
<p>The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, a unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features a unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as a powerful encoder, converting low-resolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into high-resolution waveforms. To enhance high-frequency fidelity, we incorporate a multi-band, multi-scale time-frequency discriminator, along with a multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios (<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio">https://github.com/modelscope/ClearerVoice-Studio</a>). </p>
<blockquote>
<p>è¿‘æœŸï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åº”ç”¨å·²åŸºäºå¦‚æ¢…å°”é¢‘è°±å›¾ç­‰ä¸­é—´è¡¨ç¤ºæ¨åŠ¨äº†è¯­éŸ³è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰çš„å‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SRæ–¹æ³•é€šå¸¸ä¾èµ–äºç‹¬ç«‹è®­ç»ƒå¹¶è¿æ¥çš„ç½‘ç»œï¼Œè¿™å¯èƒ½å¯¼è‡´è¡¨ç¤ºä¸ä¸€è‡´å’Œè¯­éŸ³è´¨é‡å·®ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–åœºæ™¯ä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HiFi-SRï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ç«¯åˆ°ç«¯å¯¹æŠ—è®­ç»ƒå®ç°é«˜ä¿çœŸè¯­éŸ³è¶…åˆ†è¾¨ç‡çš„ç»Ÿä¸€ç½‘ç»œã€‚æˆ‘ä»¬çš„æ¨¡å‹ç‰¹ç‚¹æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„transformer-å·ç§¯ç”Ÿæˆå™¨ï¼Œæ—¨åœ¨æ— ç¼å¤„ç†æ½œåœ¨è¡¨ç¤ºçš„é¢„æµ‹åŠå…¶è½¬æ¢ä¸ºæ—¶é—´åŸŸæ³¢å½¢ã€‚Transformerç½‘ç»œä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„ç¼–ç å™¨ï¼Œå°†ä½åˆ†è¾¨ç‡æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œè€Œå·ç§¯ç½‘ç»œå°†è¿™äº›è¡¨ç¤ºæ”¾å¤§ä¸ºé«˜åˆ†è¾¨ç‡æ³¢å½¢ã€‚ä¸ºäº†æé«˜é«˜é¢‘ä¿çœŸåº¦ï¼Œæˆ‘ä»¬åœ¨å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥äº†å¤šé¢‘å¸¦ã€å¤šå°ºåº¦çš„æ—¶é—´-é¢‘ç‡é‰´åˆ«å™¨ï¼Œä»¥åŠå¤šå°ºåº¦æ¢…å°”é‡å»ºæŸå¤±ã€‚HiFi-SRé€šç”¨æ€§å¾ˆå¼ºï¼Œèƒ½å¤Ÿå°†4kHzè‡³32kHzä¹‹é—´çš„ä»»ä½•è¾“å…¥è¯­éŸ³ä¿¡å·æ”¾å¤§åˆ°48kHzçš„é‡‡æ ·ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨åŸŸå†…è¿˜æ˜¯åŸŸå¤–åœºæ™¯ä¸­ï¼ŒHiFi-SRåœ¨å®¢è§‚æŒ‡æ ‡å’ŒABXåå¥½æµ‹è¯•ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰è¯­éŸ³SRæ–¹æ³•ã€‚ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/modelscope/ClearerVoice-Studio%EF%BC%89">https://github.com/modelscope/ClearerVoice-Studioï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10045v1">PDF</a> 5 pages, 5 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„é«˜ä¿çœŸè¯­éŸ³è¶…åˆ†è¾¨ç‡æŠ€æœ¯ï¼ˆHiFi-SRï¼‰ã€‚HiFi-SRé‡‡ç”¨ç«¯åˆ°ç«¯çš„å¯¹æŠ—è®­ç»ƒï¼Œå®ç°é«˜ä¿çœŸè¯­éŸ³è¶…åˆ†è¾¨ç‡ã€‚æ¨¡å‹é‡‡ç”¨ç»Ÿä¸€çš„å…¨å·ç§¯ç”Ÿæˆå™¨ï¼Œå¤„ç†æ½œåœ¨è¡¨ç¤ºçš„é¢„æµ‹åŠå…¶è½¬æ¢ä¸ºæ—¶é—´åŸŸæ³¢å½¢ã€‚é€šè¿‡å¼ºå¤§çš„è½¬æ¢å™¨ç½‘ç»œå°†ä½åˆ†è¾¨ç‡æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œè€Œå·ç§¯ç½‘ç»œåˆ™å°†è¿™äº›è¡¨ç¤ºæ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡æ³¢å½¢ã€‚åŒæ—¶å¼•å…¥å¤šé¢‘å¸¦ã€å¤šå°ºåº¦çš„æ—¶é—´é¢‘ç‡åˆ¤åˆ«å™¨å’Œå¤šå°ºåº¦æ¢…å°”é‡å»ºæŸå¤±ï¼Œä»¥æé«˜é«˜é¢‘ä¿çœŸåº¦ã€‚HiFi-SRå…·æœ‰é€šç”¨æ€§ï¼Œèƒ½å°†è¾“å…¥è¯­éŸ³ä¿¡å·çš„é‡‡æ ·ç‡ä»ä»»æ„4kHzè‡³32kHzæå‡åˆ°48kHzã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHiFi-SRåœ¨å®¢è§‚æŒ‡æ ‡å’ŒABXåå¥½æµ‹è¯•ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰è¯­éŸ³è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œé€‚ç”¨äºä¸åŒé¢†åŸŸå’Œè·¨é¢†åŸŸåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„è¯­éŸ³è¶…åˆ†è¾¨ç‡æŠ€æœ¯HiFi-SRã€‚</li>
<li>HiFi-SRåˆ©ç”¨ç«¯åˆ°ç«¯çš„å¯¹æŠ—è®­ç»ƒæ¥å®ç°é«˜ä¿çœŸè¯­éŸ³è¶…åˆ†è¾¨ç‡ã€‚</li>
<li>æ¨¡å‹åŒ…å«ç»Ÿä¸€çš„å…¨å·ç§¯ç”Ÿæˆå™¨ï¼Œç”¨äºå¤„ç†æ½œåœ¨è¡¨ç¤ºçš„é¢„æµ‹å’Œè½¬æ¢ä¸ºæ—¶é—´åŸŸæ³¢å½¢ã€‚</li>
<li>è½¬æ¢å™¨ç½‘ç»œå°†ä½åˆ†è¾¨ç‡æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´è¡¨ç¤ºã€‚</li>
<li>å¤šé¢‘å¸¦ã€å¤šå°ºåº¦çš„æ—¶é—´é¢‘ç‡åˆ¤åˆ«å™¨å’Œå¤šå°ºåº¦æ¢…å°”é‡å»ºæŸå¤±è¢«ç”¨äºæé«˜é«˜é¢‘ä¿çœŸåº¦ã€‚</li>
<li>HiFi-SRå…·æœ‰é€šç”¨æ€§ï¼Œå¯å°†è¾“å…¥è¯­éŸ³ä¿¡å·çš„é‡‡æ ·ç‡æå‡åˆ°æ›´é«˜çš„æ ‡å‡†ï¼ˆå¦‚ä»ä»»æ„èŒƒå›´å†…çš„ä½é¢‘åˆ°é«˜é¢‘ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c1639cc2633350b69256bf329dbd5cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a91e50b0aa06008e76b4dd6747a8ffba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-924ca8ceb5a3211f8737fd245aad0a82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-392be9a96ba566740cedf300d9aa30d0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TalkingEyes-Pluralistic-Speech-Driven-3D-Eye-Gaze-Animation"><a href="#TalkingEyes-Pluralistic-Speech-Driven-3D-Eye-Gaze-Animation" class="headerlink" title="TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation"></a>TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation</h2><p><strong>Authors:Yixiang Zhuang, Chunshan Ma, Yao Cheng, Xuan Cheng, Jing Liao, Juncong Lin</strong></p>
<p>Although significant progress has been made in the field of speech-driven 3D facial animation recently, the speech-driven animation of an indispensable facial component, eye gaze, has been overlooked by recent research. This is primarily due to the weak correlation between speech and eye gaze, as well as the scarcity of audio-gaze data, making it very challenging to generate 3D eye gaze motion from speech alone. In this paper, we propose a novel data-driven method which can generate diverse 3D eye gaze motions in harmony with the speech. To achieve this, we firstly construct an audio-gaze dataset that contains about 14 hours of audio-mesh sequences featuring high-quality eye gaze motion, head motion and facial motion simultaneously. The motion data is acquired by performing lightweight eye gaze fitting and face reconstruction on videos from existing audio-visual datasets. We then tailor a novel speech-to-motion translation framework in which the head motions and eye gaze motions are jointly generated from speech but are modeled in two separate latent spaces. This design stems from the physiological knowledge that the rotation range of eyeballs is less than that of head. Through mapping the speech embedding into the two latent spaces, the difficulty in modeling the weak correlation between speech and non-verbal motion is thus attenuated. Finally, our TalkingEyes, integrated with a speech-driven 3D facial motion generator, can synthesize eye gaze motion, eye blinks, head motion and facial motion collectively from speech. Extensive quantitative and qualitative evaluations demonstrate the superiority of the proposed method in generating diverse and natural 3D eye gaze motions from speech. The project page of this paper is: <a target="_blank" rel="noopener" href="https://lkjkjoiuiu.github.io/TalkingEyes_Home/">https://lkjkjoiuiu.github.io/TalkingEyes_Home/</a> </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å¿½ç•¥äº†ä¸å¯æˆ–ç¼ºçš„é¢éƒ¨ç»„æˆéƒ¨åˆ†â€”â€”çœ¼ç¥çš„è¯­éŸ³é©±åŠ¨åŠ¨ç”»ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºè¯­éŸ³å’Œçœ¼ç¥ä¹‹é—´çš„å¼±ç›¸å…³æ€§ï¼Œä»¥åŠéŸ³é¢‘æ³¨è§†æ•°æ®çš„ç¨€ç¼ºï¼Œä½¿å¾—ä»…ä»è¯­éŸ³ç”Ÿæˆä¸‰ç»´çœ¼ç¥è¿åŠ¨éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆä¸è¯­éŸ³åè°ƒçš„å¤šæ ·åŒ–çš„ä¸‰ç»´çœ¼ç¥è¿åŠ¨ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªéŸ³é¢‘æ³¨è§†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«çº¦14å°æ—¶çš„éŸ³é¢‘ç½‘æ ¼åºåˆ—ï¼ŒåŒæ—¶å‘ˆç°é«˜è´¨é‡çš„çœ¼ç¥è¿åŠ¨ã€å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨ã€‚è¿åŠ¨æ•°æ®æ˜¯é€šè¿‡å¯¹æ¥è‡ªç°æœ‰è§†å¬æ•°æ®é›†çš„è§†é¢‘æ‰§è¡Œè½»é‡çº§çœ¼ç¥æ³¨è§†æ‹Ÿåˆå’Œé¢éƒ¨é‡å»ºè€Œè·å¾—çš„ã€‚ç„¶åï¼Œæˆ‘ä»¬å®šåˆ¶äº†ä¸€ä¸ªæ–°é¢–çš„è¯­éŸ³åˆ°è¿åŠ¨ç¿»è¯‘æ¡†æ¶ï¼Œå…¶ä¸­å¤´éƒ¨è¿åŠ¨å’Œçœ¼ç¥è¿åŠ¨æ˜¯ä»è¯­éŸ³ä¸­è”åˆç”Ÿæˆçš„ï¼Œä½†åœ¨ä¸¤ä¸ªå•ç‹¬çš„æ½œåœ¨ç©ºé—´ä¸­å»ºæ¨¡ã€‚è¿™ç§è®¾è®¡æºäºç”Ÿç†çŸ¥è¯†ï¼Œå³çœ¼çƒçš„æ—‹è½¬èŒƒå›´å°äºå¤´éƒ¨ã€‚é€šè¿‡å°†è¯­éŸ³åµŒå…¥æ˜ å°„åˆ°è¿™ä¸¤ä¸ªæ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥å‡è½»å¯¹è¯­éŸ³å’Œéè¯­è¨€è¿åŠ¨ä¹‹é—´å¼±ç›¸å…³æ€§çš„å»ºæ¨¡éš¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬çš„TalkingEyesç»“åˆè¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨è¿åŠ¨ç”Ÿæˆå™¨ï¼Œå¯ä»¥ä»è¯­éŸ³ä¸­ç»¼åˆç”Ÿæˆçœ¼ç¥è¿åŠ¨ã€çœ¨çœ¼ã€å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é€šè¿‡è¯­éŸ³ç”Ÿæˆå¤šæ ·åŒ–å’Œè‡ªç„¶çš„ä¸‰ç»´çœ¼ç¥è¿åŠ¨æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚è¯¥è®ºæ–‡çš„é¡¹ç›®é¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://lkjkjoiuiu.github.io/TalkingEyes_Home/">https://lkjkjoiuiu.github.io/TalkingEyes_Home/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09921v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸè™½ç„¶è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…³äºé¢éƒ¨é‡è¦ç»„æˆéƒ¨åˆ†â€”â€”çœ¼åŠ¨çš„è¯­éŸ³é©±åŠ¨åŠ¨ç”»ç ”ç©¶å´è¢«å¿½è§†ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œèƒ½ç”Ÿæˆä¸è¯­éŸ³ç›¸åè°ƒçš„å¤šæ ·åŒ–çš„3Dçœ¼åŠ¨ã€‚é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦14å°æ—¶éŸ³é¢‘ç½‘æ ¼åºåˆ—çš„è§†å¬æ•°æ®é›†ï¼ŒåŒæ—¶è¿›è¡Œçœ¼åŠ¨ã€å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨çš„é«˜è´¨é‡é‡‡é›†ã€‚ç„¶åè®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„è¯­éŸ³åˆ°åŠ¨ä½œè½¬æ¢æ¡†æ¶ï¼Œå¤´éƒ¨è¿åŠ¨å’Œçœ¼åŠ¨ä»è¯­éŸ³ä¸­äº§ç”Ÿï¼Œä½†åˆ†åˆ«åœ¨ä¸¤ä¸ªç‹¬ç«‹æ½œç©ºé—´å»ºæ¨¡ã€‚æœ€åï¼Œé€šè¿‡é›†æˆè¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬çš„TalkingEyesèƒ½ä»è¯­éŸ³ä¸­åˆæˆçœ¼åŠ¨ã€çœ¨çœ¼ã€å¤´éƒ¨å’Œé¢éƒ¨è¿åŠ¨ã€‚å¹¿æ³›å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¤šæ ·ä¸”è‡ªç„¶çš„3Dçœ¼åŠ¨æ–¹é¢è¡¨ç°å“è¶Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¿‘æœŸç ”ç©¶å¿½è§†äº†è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„çœ¼åŠ¨éƒ¨åˆ†ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„ä¸è¯­éŸ³åè°ƒçš„3Dçœ¼åŠ¨ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«é«˜è´¨é‡è§†å¬æ•°æ®çš„éŸ³é¢‘ç½‘æ ¼æ•°æ®é›†ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„è¯­éŸ³åˆ°åŠ¨ä½œè½¬æ¢æ¡†æ¶ï¼Œå°†å¤´éƒ¨å’Œçœ¼åŠ¨åˆ†å¼€å»ºæ¨¡ã€‚</li>
<li>é›†æˆè¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ç”Ÿæˆå™¨ï¼Œèƒ½åˆæˆå¤šç§åŠ¨ä½œã€‚</li>
<li>æ–¹æ³•åœ¨ç”Ÿæˆè‡ªç„¶ä¸”å¤šæ ·çš„3Dçœ¼åŠ¨æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25c9156a034129cae5a665735f1a17b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6a763936a1a5af284b409c8061fb23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17a018e6b270864e17fecceb5aa20544.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2951e2636e6e71fb628ba5ffea3e13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2c85138a6ee5eab3140ae3ff1788df5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53b823a8dc9e8fb5ec9666427dd47fc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23bbd8cbf3691fe6fe589b818a729464.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="persoDA-Personalized-Data-Augmentation-for-Personalized-ASR"><a href="#persoDA-Personalized-Data-Augmentation-for-Personalized-ASR" class="headerlink" title="persoDA: Personalized Data Augmentation for Personalized ASR"></a>persoDA: Personalized Data Augmentation for Personalized ASR</h2><p><strong>Authors:Pablo Peso Parada, Spyros Fontalis, Md Asif Jalal, Karthikeyan Saravanan, Anastasios Drosou, Mete Ozay, Gil Ho Lee, Jungin Lee, Seokyeong Jung</strong></p>
<p>Data augmentation (DA) is ubiquitously used in training of Automatic Speech Recognition (ASR) models. DA offers increased data variability, robustness and generalization against different acoustic distortions. Recently, personalization of ASR models on mobile devices has been shown to improve Word Error Rate (WER). This paper evaluates data augmentation in this context and proposes persoDA; a DA method driven by userâ€™s data utilized to personalize ASR. persoDA aims to augment training with data specifically tuned towards acoustic characteristics of the end-user, as opposed to standard augmentation based on Multi-Condition Training (MCT) that applies random reverberation and noises. Our evaluation with an ASR conformer-based baseline trained on Librispeech and personalized for VOICES shows that persoDA achieves a 13.9% relative WER reduction over using standard data augmentation (using random noise &amp; reverberation). Furthermore, persoDA shows 16% to 20% faster convergence over MCT. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºï¼ˆDAï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„è®­ç»ƒä¸­æ— å¤„ä¸åœ¨ã€‚DAæä¾›äº†å¢åŠ æ•°æ®å˜åŒ–ã€ç¨³å¥æ€§å’Œå¯¹ä¸åŒå£°å­¦å¤±çœŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå¯¹ASRæ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å±•ç¤ºï¼Œèƒ½å¤Ÿæ”¹å–„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚æœ¬æ–‡åœ¨æ­¤èƒŒæ™¯ä¸‹è¯„ä¼°æ•°æ®å¢å¼ºï¼Œå¹¶æå‡ºpersoDAï¼›ä¸€ç§åˆ©ç”¨ç”¨æˆ·æ•°æ®è¿›è¡Œé©±åŠ¨çš„DAæ–¹æ³•ï¼Œç”¨äºä¸ªæ€§åŒ–ASRã€‚persoDAæ—¨åœ¨ä½¿ç”¨ä¸“é—¨é’ˆå¯¹æœ€ç»ˆç”¨æˆ·å£°å­¦ç‰¹å¾è°ƒæ•´çš„æ•°æ®æ¥å¢å¼ºè®­ç»ƒï¼Œè€Œä¸æ˜¯åŸºäºå¤šæ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰çš„æ ‡å‡†å¢å¼ºï¼Œåè€…åº”ç”¨éšæœºæ··å“å’Œå™ªå£°ã€‚æˆ‘ä»¬åœ¨Librispeechä¸Šè®­ç»ƒçš„åŸºäºASR conformerçš„åŸºçº¿å¹¶è¿›è¡Œä¸ªæ€§åŒ–å¤„ç†ç”¨äºè¯­éŸ³è¯„ä¼°ï¼Œå‘ç°ä¸æ ‡å‡†æ•°æ®å¢å¼ºç›¸æ¯”ï¼ˆä½¿ç”¨éšæœºå™ªå£°å’Œæ··å“ï¼‰ï¼ŒpersoDAç›¸å¯¹å‡å°‘äº†WERçš„ç™¾åˆ†æ¯”ä¸ºç™¾åˆ†ä¹‹åä¸‰ç‚¹ä¹ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºMCTæ¥è¯´ï¼ŒpersoDAçš„æ”¶æ•›é€Ÿåº¦æé«˜äº†ç™¾åˆ†ä¹‹åå…­åˆ°ç™¾åˆ†ä¹‹äºŒåã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09113v2">PDF</a> ICASSPâ€™25-Copyright 2025 IEEE. Personal use of this material is   permitted. Permission from IEEE must be obtained for all other uses, in any   current or future media, including reprinting&#x2F;republishing this material for   advertising or promotional purposes, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work in other works</p>
<p><strong>Summary</strong></p>
<p>æ•°æ®å¢å¼ºï¼ˆDAï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„è®­ç»ƒä¸­åº”ç”¨å¹¿æ³›ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸ªäººåŒ–ASRæ¨¡å‹ä¸­æ•°æ®å¢å¼ºçš„æ•ˆæœï¼Œå¹¶æå‡ºä¸€ç§åä¸ºpersoDAçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç”¨æˆ·æ•°æ®é©±åŠ¨ï¼Œæ—¨åœ¨é’ˆå¯¹æœ€ç»ˆç”¨æˆ·çš„å£°å­¦ç‰¹æ€§è¿›è¡Œä¸ªæ€§åŒ–è®­ç»ƒï¼Œä¸åŒäºåŸºäºå¤šæ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰çš„æ ‡å‡†æ•°æ®å¢å¼ºæ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒpersoDAç›¸å¯¹äºä½¿ç”¨éšæœºå™ªå£°å’Œå›å£°çš„æ ‡å‡†æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå®ç°äº†ç›¸å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½13.9%ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿåº¦æé«˜äº†16%è‡³20%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹è®­ç»ƒä¸­èµ·ç€é‡è¦ä½œç”¨ï¼Œèƒ½æé«˜æ•°æ®å¤šå˜æ€§å’Œæ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>é’ˆå¯¹ç§»åŠ¨è®¾å¤‡ä¸Šçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–å¯ä»¥æé«˜è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>æœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºpersoDAçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæ ¹æ®ç”¨æˆ·çš„å£°å­¦ç‰¹æ€§è¿›è¡Œä¸ªæ€§åŒ–è®­ç»ƒã€‚</li>
<li>persoDAç›¸å¯¹äºæ ‡å‡†çš„å¤šæ¡ä»¶è®­ç»ƒï¼ˆMCTï¼‰ï¼Œå®ç°äº†æ›´ä½çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>persoDAåœ¨æ”¶æ•›é€Ÿåº¦ä¸Šè¾ƒMCTæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒpersoDAåœ¨åŸºäºLibrispeechçš„è¯­éŸ³è¯†åˆ«äººä½“å·¥å­¦æ¨¡å‹ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ef72b10e9a0fdc35a013f9700e536ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8af275ab2812330140db0dc8942b5f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-077b60f4d996a62378f6026433dee485.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Joint-Automatic-Speech-Recognition-And-Structure-Learning-For-Better-Speech-Understanding"><a href="#Joint-Automatic-Speech-Recognition-And-Structure-Learning-For-Better-Speech-Understanding" class="headerlink" title="Joint Automatic Speech Recognition And Structure Learning For Better   Speech Understanding"></a>Joint Automatic Speech Recognition And Structure Learning For Better   Speech Understanding</h2><p><strong>Authors:Jiliang Hu, Zuchao Li, Mengjia Shen, Haojun Ai, Sheng Li, Jun Zhang</strong></p>
<p>Spoken language understanding (SLU) is a structure prediction task in the field of speech. Recently, many works on SLU that treat it as a sequence-to-sequence task have achieved great success. However, This method is not suitable for simultaneous speech recognition and understanding. In this paper, we propose a joint speech recognition and structure learning framework (JSRSL), an end-to-end SLU model based on span, which can accurately transcribe speech and extract structured content simultaneously. We conduct experiments on name entity recognition and intent classification using the Chinese dataset AISHELL-NER and the English dataset SLURP. The results show that our proposed method not only outperforms the traditional sequence-to-sequence method in both transcription and extraction capabilities but also achieves state-of-the-art performance on the two datasets. </p>
<blockquote>
<p>è¯­éŸ³è¯­è¨€ç†è§£ï¼ˆSLUï¼‰æ˜¯è¯­éŸ³é¢†åŸŸçš„ä¸€ç§ç»“æ„é¢„æµ‹ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå°†SLUè§†ä¸ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„è®¸å¤šå·¥ä½œéƒ½å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸é€‚ç”¨äºåŒæ­¥è¯­éŸ³è¯†åˆ«å’Œç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆè¯­éŸ³è¯†åˆ«å’Œç»“æ„å­¦ä¹ æ¡†æ¶ï¼ˆJSRSLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºèŒƒå›´çš„ç«¯åˆ°ç«¯SLUæ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶å‡†ç¡®è½¬å½•è¯­éŸ³å¹¶æå–ç»“æ„å†…å®¹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸­å›½AISHELL-NERæ•°æ®é›†å’Œè‹±è¯­SLURPæ•°æ®é›†è¿›è¡Œå‘½åå®ä½“è¯†åˆ«å’Œæ„å›¾åˆ†ç±»å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ä»…åœ¨è½¬å½•å’Œæå–èƒ½åŠ›ä¸Šä¼˜äºä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07329v2">PDF</a> 5 pages, 2 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºåŒºæ®µçš„è”åˆè¯­éŸ³è¯†åˆ«å’Œç»“æ„å­¦ä¹ æ¡†æ¶ï¼ˆJSRSLï¼‰ï¼Œå¯åŒæ—¶å‡†ç¡®è½¬å½•è¯­éŸ³å¹¶æå–ç»“æ„åŒ–å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸­æ–‡æ•°æ®é›†AISHELL-NERå’Œè‹±æ–‡æ•°æ®é›†SLURPä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—æ–¹æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚å®ƒä¸ä»…æ”¹å–„äº†è¯­éŸ³è¯†åˆ«å’Œç†è§£çš„åŒæ­¥æ€§ï¼Œè€Œä¸”æé«˜äº†è½¬å½•å’Œæå–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spoken language understanding (SLU)æ˜¯è¯­éŸ³é¢†åŸŸçš„ä¸€ä¸ªç»“æ„é¢„æµ‹ä»»åŠ¡ã€‚åºåˆ—åˆ°åºåˆ—çš„SLUæ–¹æ³•åœ¨å¤šä¸ªç ”ç©¶å·¥ä½œä¸­å–å¾—æ˜¾è‘—æˆæœï¼Œä½†ä¸æ”¯æŒè¯­éŸ³çš„å®æ—¶è¯†åˆ«å’Œè¯­ä¹‰ç†è§£ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è”åˆè¯­éŸ³è¯†åˆ«å’Œç»“æ„å­¦ä¹ æ¡†æ¶ï¼ˆJSRSLï¼‰ï¼Œå®ƒç»“åˆäº†å®æ—¶çš„è¯­éŸ³è¯†åˆ«å’Œç»“æ„åŒ–å†…å®¹æå–ã€‚</li>
<li>JSRSLæ˜¯ä¸€ä¸ªåŸºäºåŒºæ®µçš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å‡†ç¡®è½¬å½•è¯­éŸ³å¹¶æå–ç»“æ„åŒ–çš„å†…å®¹ã€‚è¿™ä¸ºè§£å†³è¯­è¨€ç†è§£çš„éš¾é¢˜æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„é€”å¾„ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼šä¸­æ–‡æ•°æ®é›†AISHELL-NERå’Œè‹±æ–‡æ•°æ®é›†SLURPã€‚å®éªŒç»“æœè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>JSRSLä¸ä»…åœ¨è½¬å½•èƒ½åŠ›ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½å®ç°äº†å‰æ²¿æ€§èƒ½ã€‚è¿™è¯´æ˜å®ƒèƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«å’Œè§£é‡Šå¤æ‚çš„è¯­éŸ³ä¿¡å·å’Œå«ä¹‰ä¸°å¯Œçš„å¥å­ç»“æ„ã€‚ </li>
<li>JSRSLæ¡†æ¶å¯¹äºæœªæ¥çš„è¯­è¨€å¤„ç†ä»»åŠ¡å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸã€‚å…¶å¼ºå¤§çš„æ€§èƒ½ä½¿å…¶æˆä¸ºè§£å†³å¤æ‚è¯­è¨€ä»»åŠ¡çš„æœ‰æ•ˆå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b5e8fce9e18239de17e9e69363ae599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc6bf83485a8dacb7fed11cd74d77555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e64bfe3ab29c3ce689364c4445d56607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-753ec3a1e1c08bf4bdf0db6da29d540e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-948fe4d77343f211364ce3ed1ec74e22.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Uncovering-the-Visual-Contribution-in-Audio-Visual-Speech-Recognition"><a href="#Uncovering-the-Visual-Contribution-in-Audio-Visual-Speech-Recognition" class="headerlink" title="Uncovering the Visual Contribution in Audio-Visual Speech Recognition"></a>Uncovering the Visual Contribution in Audio-Visual Speech Recognition</h2><p><strong>Authors:Zhaofeng Lin, Naomi Harte</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) combines auditory and visual speech cues to enhance the accuracy and robustness of speech recognition systems. Recent advancements in AVSR have improved performance in noisy environments compared to audio-only counterparts. However, the true extent of the visual contribution, and whether AVSR systems fully exploit the available cues in the visual domain, remains unclear. This paper assesses AVSR systems from a different perspective, by considering human speech perception. We use three systems: Auto-AVSR, AVEC and AV-RelScore. We first quantify the visual contribution using effective SNR gains at 0 dB and then investigate the use of visual information in terms of its temporal distribution and word-level informativeness. We show that low WER does not guarantee high SNR gains. Our results suggest that current methods do not fully exploit visual information, and we recommend future research to report effective SNR gains alongside WERs. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†å¬è§‰å’Œè§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ä¸ä»…ä½¿ç”¨éŸ³é¢‘çš„åŒç±»ç³»ç»Ÿç›¸æ¯”ï¼ŒAVSRçš„æœ€æ–°è¿›å±•åœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚ç„¶è€Œï¼Œè§†è§‰è´¡çŒ®çš„çœŸæ­£ç¨‹åº¦ï¼Œä»¥åŠAVSRç³»ç»Ÿæ˜¯å¦å……åˆ†åˆ©ç”¨è§†è§‰é¢†åŸŸä¸­çš„å¯ç”¨çº¿ç´¢ï¼Œä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡é€šè¿‡è€ƒè™‘äººç±»è¯­éŸ³æ„ŸçŸ¥ï¼Œä»å¦ä¸€ä¸ªè§’åº¦è¯„ä¼°AVSRç³»ç»Ÿã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§ç³»ç»Ÿï¼šAuto-AVSRã€AVECå’ŒAV-RelScoreã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æœ‰æ•ˆä¿¡å™ªæ¯”å¢ç›Šï¼ˆåœ¨0åˆ†è´å¤„ï¼‰é‡åŒ–è§†è§‰è´¡çŒ®ï¼Œç„¶åç ”ç©¶è§†è§‰ä¿¡æ¯çš„ä½¿ç”¨ä¸å…¶æ—¶é—´åˆ†å¸ƒå’Œè¯æ±‡çº§åˆ«ä¿¡æ¯é‡æœ‰å…³ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½è¯é”™è¯¯ç‡å¹¶ä¸ä¿è¯é«˜ä¿¡å™ªæ¯”å¢ç›Šã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„æ–¹æ³•å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬å»ºè®®æœªæ¥çš„ç ”ç©¶åœ¨æŠ¥å‘Šè¯é”™è¯¯ç‡çš„åŒæ—¶ï¼Œä¹Ÿè¦æŠ¥å‘Šæœ‰æ•ˆçš„ä¿¡å™ªæ¯”å¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17129v2">PDF</a> 5 pages, 2 figures. Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆå¬è§‰å’Œè§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œæé«˜äº†è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œç›¸è¾ƒäºåªä½¿ç”¨éŸ³é¢‘çš„è¯†åˆ«ç³»ç»Ÿï¼ŒAVSRåœ¨å™ªå£°ç¯å¢ƒä¸­çš„è¡¨ç°æœ‰æ‰€æå‡ã€‚ç„¶è€Œï¼Œè§†è§‰è´¡çŒ®çš„çœŸå®ç¨‹åº¦ä»¥åŠAVSRç³»ç»Ÿæ˜¯å¦å®Œå…¨åˆ©ç”¨è§†è§‰é¢†åŸŸçš„å¯ç”¨çº¿ç´¢å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡é€šè¿‡å€Ÿé‰´äººç±»è¯­éŸ³æ„ŸçŸ¥æ¥è¯„ä¼°AVSRç³»ç»Ÿï¼Œä½¿ç”¨Auto-AVSRã€AVECå’ŒAV-RelScoreä¸‰ç§ç³»ç»Ÿè¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬é‡åŒ–è§†è§‰è´¡çŒ®å¹¶ç ”ç©¶è§†è§‰ä¿¡æ¯çš„æ—¶åºåˆ†å¸ƒå’Œè¯æ±‡çº§åˆ«çš„ä¿¡æ¯é‡ã€‚ç ”ç©¶å‘ç°ï¼Œä½è¯é”™è¯¯ç‡å¹¶ä¸ä¿è¯ä¿¡å™ªæ¯”å¢ç›Šé«˜ã€‚æˆ‘ä»¬çš„ç»“æœæš—ç¤ºå½“å‰çš„æ–¹æ³•å¹¶æœªå®Œå…¨åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œå»ºè®®æœªæ¥çš„ç ”ç©¶åœ¨æŠ¥å‘Šè¯é”™è¯¯ç‡çš„åŒæ—¶ä¹Ÿè¦æŠ¥å‘Šæœ‰æ•ˆçš„ä¿¡å™ªæ¯”å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSRç»“åˆäº†å¬è§‰å’Œè§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œæé«˜äº†è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§åŠç¨³å¥æ€§ã€‚</li>
<li>ç›¸è¾ƒäºéŸ³é¢‘è¯†åˆ«ç³»ç»Ÿï¼ŒAVSRåœ¨å™ªå£°ç¯å¢ƒä¸­çš„è¡¨ç°æœ‰æ‰€æå‡ã€‚</li>
<li>è§†è§‰è´¡çŒ®çš„çœŸå®ç¨‹åº¦å°šä¸æ¸…æ¥šï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å€Ÿé‰´äººç±»è¯­éŸ³æ„ŸçŸ¥æ¥è¯„ä¼°AVSRç³»ç»Ÿã€‚</li>
<li>ç ”ç©¶ä¸­ä½¿ç”¨äº†ä¸‰ç§ç³»ç»Ÿï¼šAuto-AVSRã€AVECå’ŒAV-RelScoreã€‚</li>
<li>è§†è§‰ä¿¡æ¯çš„è´¡çŒ®è¢«é‡åŒ–ï¼ŒåŒæ—¶ç ”ç©¶äº†å…¶åœ¨æ—¶åºåˆ†å¸ƒå’Œè¯æ±‡çº§åˆ«çš„ä¿¡æ¯é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d66cfe72dd0469b5d565b6c465020b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0060dad4ae9fa14d93223b8518ca324.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a715eef15e77a331d0276aa3ba9a089a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-464af9fc67d376bc83735961f31f27ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84cfb859102f8af4e3ffb64688ae83c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-Redundant-Is-the-Transformer-Stack-in-Speech-Representation-Models"><a href="#How-Redundant-Is-the-Transformer-Stack-in-Speech-Representation-Models" class="headerlink" title="How Redundant Is the Transformer Stack in Speech Representation Models?"></a>How Redundant Is the Transformer Stack in Speech Representation Models?</h2><p><strong>Authors:Teresa Dorszewski, Albert KjÃ¸ller Jacobsen, Lenka TÄ›tkovÃ¡, Lars Kai Hansen</strong></p>
<p>Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the modelâ€™s predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models. </p>
<blockquote>
<p>è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åˆ©ç”¨è½¬æ¢å™¨æ¶æ„çš„æ¨¡å‹ï¼Œåœ¨è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººè¯†åˆ«å’Œæƒ…æ„Ÿæ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å…³äºè½¬æ¢å™¨æ¨¡å‹çš„ç ”ç©¶æ­ç¤ºäº†å±‚ä¹‹é—´çš„é«˜åº¦å†—ä½™å’Œæ½œåœ¨çš„å¤§é‡å‰ªæå¯èƒ½æ€§ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡ŒåŸºäºè½¬æ¢å™¨ç»“æ„çš„è¯­éŸ³è¡¨ç¤ºæ¨¡å‹è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰ç§ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•ï¼ˆä½™å¼¦ç›¸ä¼¼æ€§ã€ä¸­å¿ƒåŒ–å†…æ ¸å¯¹é½å’Œç›¸äº’æœ€è¿‘é‚»å¯¹é½ï¼‰å¯¹è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ä¸­çš„å±‚ç›¸ä¼¼æ€§è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†ä¸€ç§é«˜ç›¸ä¼¼æ€§çš„å—çŠ¶ç»“æ„ï¼Œè¿™è¡¨æ˜äº†ä¸»è¦çš„ä¸¤ä¸ªå¤„ç†æ­¥éª¤å’Œæ˜¾è‘—çš„å±‚å†—ä½™æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ— éœ€è¿›è¡ŒäºŒæ¬¡è®­ç»ƒå³å¯æœ‰æ•ˆåœ°ä¿®å‰ªåŸºäºè½¬æ¢å™¨çš„è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œåœ¨å‡å°‘é«˜è¾¾40%çš„è½¬æ¢å™¨å±‚çš„åŒæ—¶ï¼Œä¿æŒæ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„95%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œç”¨æ¨¡ä»¿å±‚æ›¿ä»£äº†æ•´ä¸ªè½¬æ¢å™¨å †æ ˆï¼Œå°†ç½‘ç»œè§„æ¨¡å‡å°‘äº†95-98%ï¼Œæ¨ç†æ—¶é—´å‡å°‘äº†é«˜è¾¾94%ã€‚è¿™ç§è®¡ç®—è´Ÿè½½çš„å®è´¨æ€§ä¸‹é™å‘ç”Ÿåœ¨æ²¡æœ‰æ˜æ˜¾çš„æ€§èƒ½æŸå¤±çš„æƒ…å†µä¸‹ï¼Œè¿™è¡¨æ˜è½¬æ¢å™¨å †æ ˆå¯¹äºè¯­éŸ³è¡¨ç¤ºæ¨¡å‹çš„ä¸‹æ¸¸åº”ç”¨å‡ ä¹æ˜¯å®Œå…¨å†—ä½™çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16302v2">PDF</a> To appear at ICASSP 2025 (excluding appendix)</p>
<p><strong>Summary</strong><br>     åŸºäºè½¬æ¢å™¨æ¶æ„çš„è¯­éŸ³è‡ªç›‘ç£è¡¨ç¤ºæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººè¯†åˆ«å’Œæƒ…æ„Ÿæ£€æµ‹ã€‚ç ”ç©¶å‘ç°è½¬æ¢å™¨æ¨¡å‹å±‚ä¹‹é—´å­˜åœ¨é«˜å†—ä½™æ€§ï¼Œé€šè¿‡å±‚ç›¸ä¼¼æ€§åˆ†ææ­ç¤ºäº†å…¶å—çŠ¶ç»“æ„ç‰¹ç‚¹ï¼Œå¹¶å±•ç¤ºäº†æ˜¾è‘—çš„å¤„ç†å†—ä½™æ€§ã€‚åœ¨ä¸è¿›è¡Œè®­ç»ƒåå¤„ç†çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¿®å‰ªå±‚å®ç°æ¨¡å‹æ•ˆç‡æå‡ï¼Œå‡å°‘é«˜è¾¾40%çš„è½¬æ¢å™¨å±‚åŒæ—¶ä¿æŒæ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„95%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œé‡‡ç”¨çŸ¥è¯†è’¸é¦æ³•ä»¥æ¨¡ä»¿å±‚ä»£æ›¿æ•´ä¸ªè½¬æ¢å™¨å †æ ˆï¼Œé™ä½ç½‘ç»œè§„æ¨¡é«˜è¾¾98%ï¼Œå¹¶å¯å°†æ¨ç†æ—¶é—´ç¼©çŸ­é«˜è¾¾94%ï¼Œä¸”åœ¨è®¡ç®—è´Ÿè·æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹å‡ ä¹æ²¡æœ‰æ€§èƒ½æŸå¤±ã€‚è¿™æ˜¾ç¤ºå‡ºå¯¹äºè¯­éŸ³è¡¨ç¤ºæ¨¡å‹çš„ä¸‹æ¸¸åº”ç”¨è€Œè¨€ï¼Œè½¬æ¢å™¨å †æ ˆå‡ ä¹å®Œå…¨å†—ä½™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºè½¬æ¢å™¨æ¶æ„çš„æ¨¡å‹ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>è½¬æ¢å™¨æ¨¡å‹å±‚ä¹‹é—´å­˜åœ¨é«˜å†—ä½™æ€§ã€‚</li>
<li>é€šè¿‡å±‚ç›¸ä¼¼æ€§åˆ†ææ­ç¤ºäº†æ¨¡å‹å—çŠ¶ç»“æ„ç‰¹ç‚¹ã€‚</li>
<li>é€šè¿‡ä¿®å‰ªå±‚å¯ä»¥æå‡æ¨¡å‹æ•ˆç‡ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—è´Ÿè·å¹¶ä¿æŒè¾ƒé«˜é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ³•å¯ç”¨äºä»¥æ¨¡ä»¿å±‚ä»£æ›¿è½¬æ¢å™¨å †æ ˆçš„å¤§éƒ¨åˆ†éƒ¨åˆ†ã€‚</li>
<li>ç½‘ç»œè§„æ¨¡å¯å¤§å¹…ç¼©å‡ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´æ˜¾è‘—å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e12b4e1a35b89a35f0003416839d3580.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5392713e8cbb7badf3a8eb07f915bb72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8bdaac305066f9911b7791b6ad1f9cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9ef6862ae73cb9cf3944cbaef309a81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2b1a36b59633eefabae5081ed21ebd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90e1f8d2e567b2151f18d9aacfe0efea.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Annealed-Multiple-Choice-Learning-Overcoming-limitations-of-Winner-takes-all-with-annealing"><a href="#Annealed-Multiple-Choice-Learning-Overcoming-limitations-of-Winner-takes-all-with-annealing" class="headerlink" title="Annealed Multiple Choice Learning: Overcoming limitations of   Winner-takes-all with annealing"></a>Annealed Multiple Choice Learning: Overcoming limitations of   Winner-takes-all with annealing</h2><p><strong>Authors:David Perera, Victor Letzelter, ThÃ©o Mariotte, Adrien CortÃ©s, Mickael Chen, Slim Essid, GaÃ«l Richard</strong></p>
<p>We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†é€€ç«å¤šé€‰é¡¹å­¦ä¹ ï¼ˆaMCLï¼‰ï¼Œå®ƒå°†æ¨¡æ‹Ÿé€€ç«ä¸MCLç›¸ç»“åˆã€‚MCLæ˜¯ä¸€ä¸ªå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é¢„æµ‹ä¸€å°éƒ¨åˆ†åˆç†çš„å‡è®¾æ¥å¤„ç†æ¨¡ç³Šä»»åŠ¡ã€‚è¿™äº›å‡è®¾æ˜¯ä½¿ç”¨èƒœè€…å…¨å–ï¼ˆWTAï¼‰æ–¹æ¡ˆè¿›è¡Œè®­ç»ƒçš„ï¼Œè¯¥æ–¹æ¡ˆä¿ƒè¿›äº†é¢„æµ‹çš„å¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œç”±äºWTAçš„è´ªå©ªæ€§è´¨ï¼Œè¯¥æ–¹æ¡ˆå¯èƒ½ä¼šä»»æ„æ”¶æ•›åˆ°æ¬¡ä¼˜çš„å±€éƒ¨æœ€å°å€¼ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é€€ç«æ¥å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œé€€ç«å¢å¼ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­å‡è®¾ç©ºé—´çš„æ¢ç´¢ã€‚æˆ‘ä»¬ä»ç»Ÿè®¡ç‰©ç†å­¦å’Œä¿¡æ¯ç†è®ºä¸­æ±²å–çµæ„Ÿï¼Œä¸ºæ¨¡å‹è®­ç»ƒè½¨è¿¹æä¾›äº†è¯¦ç»†æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åˆæˆæ•°æ®é›†ã€æ ‡å‡†UCIåŸºå‡†æµ‹è¯•ä»¥åŠè¯­éŸ³åˆ†ç¦»ç­‰æ–¹é¢çš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15580v3">PDF</a> NeurIPS 2024</p>
<p><strong>Summary</strong>ï¼šç»“åˆæ¨¡æ‹Ÿé€€ç«ç®—æ³•å’ŒMCLå­¦ä¹ æ¡†æ¶ï¼Œæå‡ºé€€ç«å¤šé‡é€‰æ‹©å­¦ä¹ ï¼ˆaMCLï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†MCLåœ¨å¤„ç†å…·æœ‰ä¸ç¡®å®šæ€§çš„ä»»åŠ¡æ—¶å¯èƒ½å‡ºç°çš„é—®é¢˜ï¼Œåˆ©ç”¨æ¨¡æ‹Ÿé€€ç«æé«˜é¢„æµ‹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚é€šè¿‡ç»Ÿè®¡ç‰©ç†å­¦å’Œä¿¡æ¯ç†è®ºï¼Œæè¿°äº†æ¨¡å‹è®­ç»ƒè½¨è¿¹ã€‚å®éªŒéªŒè¯äº†åœ¨åˆæˆæ•°æ®é›†ã€UCIæ ‡å‡†åŸºå‡†å’Œè¯­éŸ³åˆ†ç¦»ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºæ–°çš„å­¦ä¹ ç®—æ³•aMCLï¼Œç»“åˆäº†æ¨¡æ‹Ÿé€€ç«å’ŒMCLæ¡†æ¶ã€‚</li>
<li>MCLåœ¨å¤„ç†å…·æœ‰ä¸ç¡®å®šæ€§çš„ä»»åŠ¡æ—¶ï¼Œé€šè¿‡é¢„æµ‹ä¸€ç³»åˆ—å¯èƒ½å‡è®¾æ¥å¤„ç†æ­§ä¹‰æ€§ã€‚</li>
<li>ä½¿ç”¨Winner-takes-allï¼ˆWTAï¼‰æ–¹æ¡ˆè¿›è¡Œè®­ç»ƒé¢„æµ‹ï¼Œä½†å¯èƒ½å¯¼è‡´é¢„æµ‹ç»“æœæ”¶æ•›äºä»»æ„å±€éƒ¨æœ€å°å€¼ã€‚</li>
<li>æ¨¡æ‹Ÿé€€ç«ç”¨äºå…‹æœWTAæ–¹æ¡ˆçš„å±€é™æ€§ï¼Œæé«˜é¢„æµ‹ç©ºé—´çš„æ¢ç´¢èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ç»Ÿè®¡ç‰©ç†å­¦å’Œä¿¡æ¯ç†è®ºä¸ºæ¨¡å‹è®­ç»ƒè½¨è¿¹æä¾›è¯¦ç»†æè¿°ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®é›†ã€UCIæ ‡å‡†åŸºå‡†å’Œè¯­éŸ³åˆ†ç¦»å®éªŒéªŒè¯äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3707a9afe03dddabaef2dcc880faed07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6175a7f7deecd00f6a8d37ea69f45dcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71b4362678f86fcf2cf1272b57011442.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Zero-Shot-Chinese-English-Code-Switching-ASR-with-kNN-CTC-and-Gated-Monolingual-Datastores"><a href="#Improving-Zero-Shot-Chinese-English-Code-Switching-ASR-with-kNN-CTC-and-Gated-Monolingual-Datastores" class="headerlink" title="Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and   Gated Monolingual Datastores"></a>Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and   Gated Monolingual Datastores</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Hui Wang, Tian-Hao Zhang, Haoqin Sun, Xuechen Wang, Yong Qin</strong></p>
<p>The kNN-CTC model has proven to be effective for monolingual automatic speech recognition (ASR). However, its direct application to multilingual scenarios like code-switching, presents challenges. Although there is potential for performance improvement, a kNN-CTC model utilizing a single bilingual datastore can inadvertently introduce undesirable noise from the alternative language. To address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR) framework that employs dual monolingual datastores and a gated datastore selection mechanism to reduce noise interference. Our method selects the appropriate datastore for decoding each frame, ensuring the injection of language-specific information into the ASR process. We apply this framework to cutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive experiments demonstrate the remarkable effectiveness of our gated datastore mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR. </p>
<blockquote>
<p>kNN-CTCæ¨¡å‹åœ¨å•è¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºå¤šè¯­ç§åœºæ™¯ï¼Œå¦‚ä»£ç åˆ‡æ¢ï¼Œè¿˜å­˜åœ¨æŒ‘æˆ˜ã€‚å°½ç®¡æœ‰æé«˜æ€§èƒ½çš„æ½œåŠ›ï¼Œä½†ä½¿ç”¨å•ä¸€åŒè¯­æ•°æ®å­˜å‚¨çš„kNN-CTCæ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­å¼•å…¥æ¥è‡ªå¦ä¸€ç§è¯­è¨€çš„ä¸å¸Œæœ›æœ‰çš„å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºkNN-CTCçš„ä»£ç åˆ‡æ¢ASRï¼ˆCS-ASRï¼‰æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŒå•è¯­ç§æ•°æ®å­˜å‚¨å’Œå—æ§æ•°æ®å­˜å‚¨é€‰æ‹©æœºåˆ¶æ¥å‡å°‘å™ªå£°å¹²æ‰°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‰æ‹©é€‚å½“çš„å­˜å‚¨åº“æ¥è§£ç æ¯ä¸€å¸§ï¼Œç¡®ä¿è¯­è¨€ç‰¹å®šä¿¡æ¯çš„æ³¨å…¥åˆ°ASRè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬å°†è¯¥æ¡†æ¶åº”ç”¨äºæœ€æ–°çš„CTCæ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ä¸ªå…ˆè¿›çš„CS-ASRç³»ç»Ÿã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å—æ§æ•°æ®å­˜å‚¨æœºåˆ¶åœ¨å¢å¼ºé›¶ä¸­æ–‡-è‹±æ–‡åˆ‡æ¢CS-ASRçš„æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03814v5">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>kNN-CTCæ¨¡å‹åœ¨å•è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åº”ç”¨äºå¤šè¯­è¨€åœºæ™¯å¦‚ä»£ç åˆ‡æ¢æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä½¿ç”¨å•ä¸€åŒè¯­æ•°æ®å­˜å‚¨åº“çš„kNN-CTCæ¨¡å‹å¯èƒ½ä¼šå¼•å…¥æ¥è‡ªå¦ä¸€ç§è¯­è¨€çš„ä¸å¿…è¦å™ªå£°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºkNN-CTCçš„ä»£ç åˆ‡æ¢ASRï¼ˆCS-ASRï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨åŒå•è¯­è¨€æ•°æ®å­˜å‚¨å’Œé—¨æ§æ•°æ®å­˜å‚¨é€‰æ‹©æœºåˆ¶ä»¥å‡å°‘å™ªå£°å¹²æ‰°ã€‚è¯¥æ–¹æ³•èƒ½é€‰æ‹©é€‚åˆæ¯å¸§è§£ç çš„æ•°æ®å­˜å‚¨ï¼Œç¡®ä¿è¯­è¨€ç‰¹å®šä¿¡æ¯æ³¨å…¥ASRè¿‡ç¨‹ã€‚æˆ‘ä»¬å°†æ­¤æ¡†æ¶åº”ç”¨äºå‰æ²¿çš„CTCæ¨¡å‹ï¼Œå¼€å‘å‡ºå…ˆè¿›çš„CS-ASRç³»ç»Ÿã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„é—¨æ§æ•°æ®å­˜å‚¨æœºåˆ¶åœ¨æå‡é›¶ä¸­æ–‡-è‹±æ–‡ä»£ç åˆ‡æ¢ASRçš„æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>kNN-CTCæ¨¡å‹åœ¨å•è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­è¡¨ç°æœ‰æ•ˆã€‚</li>
<li>åœ¨å¤šè¯­è¨€åœºæ™¯å¦‚ä»£ç åˆ‡æ¢ä¸­ï¼Œç›´æ¥ä½¿ç”¨kNN-CTCæ¨¡å‹ä¼šé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨å•ä¸€åŒè¯­æ•°æ®å­˜å‚¨åº“å¯èƒ½ä¼šåœ¨kNN-CTCæ¨¡å‹ä¸­å¼•å…¥æ¥è‡ªå¦ä¸€ç§è¯­è¨€çš„ä¸å¿…è¦å™ªå£°ã€‚</li>
<li>æå‡ºäº†åŸºäºkNN-CTCçš„ä»£ç åˆ‡æ¢ASRï¼ˆCS-ASRï¼‰æ¡†æ¶ã€‚</li>
<li>CS-ASRæ¡†æ¶é‡‡ç”¨åŒå•è¯­è¨€æ•°æ®å­˜å‚¨å’Œé—¨æ§æ•°æ®å­˜å‚¨é€‰æ‹©æœºåˆ¶ä»¥å‡å°‘å™ªå£°å¹²æ‰°ã€‚</li>
<li>é—¨æ§æ•°æ®å­˜å‚¨é€‰æ‹©æœºåˆ¶èƒ½æ ¹æ®æ¯å¸§è§£ç é€‰æ‹©åˆé€‚çš„æ•°æ®å­˜å‚¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc3d52bdf88a7cac3097c8c8ae0699c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa7f849a75523125316245422a79a9e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20580a63f9dd1037603951a2489b99f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-baf66f84fb75b8a7463289fdd6028b85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b473403b18a5e8c48499c807204fee2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4884c051eef4815391f0c4ef46ee2ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ae9a3243a00ac4246540783930d85e2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-21/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cec89486ac3ed8cec58639b45c1f4c48.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-21  GaussianAvatar-Editor Photorealistic Animatable Gaussian Head Avatar   Editor
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-21/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-71265a8141a1db97fec5668fc7ce0269.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-21  DPCL-Diff The Temporal Knowledge Graph Reasoning Based on Graph Node   Diffusion Model with Dual-Domain Periodic Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
