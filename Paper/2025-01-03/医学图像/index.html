<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-03  When SAM2 Meets Video Shadow and Mirror Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b1c0c3f99332e35912e94921a2277486.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-03-æ›´æ–°"><a href="#2025-01-03-æ›´æ–°" class="headerlink" title="2025-01-03 æ›´æ–°"></a>2025-01-03 æ›´æ–°</h1><h2 id="When-SAM2-Meets-Video-Shadow-and-Mirror-Detection"><a href="#When-SAM2-Meets-Video-Shadow-and-Mirror-Detection" class="headerlink" title="When SAM2 Meets Video Shadow and Mirror Detection"></a>When SAM2 Meets Video Shadow and Mirror Detection</h2><p><strong>Authors:Leiping Jie</strong></p>
<p>As the successor to the Segment Anything Model (SAM), the Segment Anything Model 2 (SAM2) not only improves performance in image segmentation but also extends its capabilities to video segmentation. However, its effectiveness in segmenting rare objects that seldom appear in videos remains underexplored. In this study, we evaluate SAM2 on three distinct video segmentation tasks: Video Shadow Detection (VSD) and Video Mirror Detection (VMD). Specifically, we use ground truth point or mask prompts to initialize the first frame and then predict corresponding masks for subsequent frames. Experimental results show that SAM2â€™s performance on these tasks is suboptimal, especially when point prompts are used, both quantitatively and qualitatively. Code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/LeipingJie/SAM2Video%7D">https://github.com/LeipingJie/SAM2Video}</a> </p>
<blockquote>
<p>ä½œä¸ºSegment Anything Modelï¼ˆSAMï¼‰çš„ç»§ä»»è€…ï¼ŒSegment Anything Model 2ï¼ˆSAM2ï¼‰ä¸ä»…åœ¨å›¾åƒåˆ†å‰²æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œè¿˜èƒ½å°†èƒ½åŠ›æ‰©å±•è‡³è§†é¢‘åˆ†å‰²ã€‚ç„¶è€Œï¼Œå®ƒåœ¨åˆ†å‰²è§†é¢‘ä¸­æå°‘å‡ºç°çš„ç½•è§ç‰©ä½“æ–¹é¢çš„æ•ˆæœå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„è§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šè¯„ä¼°äº†SAM2çš„è¡¨ç°ï¼šè§†é¢‘é˜´å½±æ£€æµ‹ï¼ˆVSDï¼‰å’Œè§†é¢‘é•œåƒæ£€æµ‹ï¼ˆVMDï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨çœŸå®ç‚¹æˆ–é®ç½©æç¤ºæ¥åˆå§‹åŒ–ç¬¬ä¸€å¸§ï¼Œç„¶åé¢„æµ‹åç»­å¸§çš„ç›¸åº”é®ç½©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAM2åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ç‚¹æç¤ºæ—¶ï¼Œæ— è®ºåœ¨æ•°é‡ä¸Šå’Œè´¨é‡ä¸Šéƒ½æ˜¯å¦‚æ­¤ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š\url{<a target="_blank" rel="noopener" href="https://github.com/LeipingJie/SAM2Video%7D">https://github.com/LeipingJie/SAM2Video}</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19293v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸçš„Segment Anything Model 2ï¼ˆSAM2ï¼‰æ¨¡å‹åœ¨è§†é¢‘åˆ†å‰²æ–¹é¢æœ‰æ‰€çªç ´ï¼Œä½†åœ¨ç½•è§ç‰©ä½“çš„è§†é¢‘åˆ†å‰²ä¸Šè¡¨ç°æœ‰å¾…æé«˜ã€‚æœ¬ç ”ç©¶å¯¹SAM2åœ¨è§†é¢‘é˜´å½±æ£€æµ‹å’Œè§†é¢‘é•œåƒæ£€æµ‹ç­‰ä¸‰ä¸ªä¸åŒè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚ä½¿ç”¨çœŸå®æ ‡è®°ç‚¹æˆ–é®ç½©æç¤ºåˆå§‹åŒ–ç¬¬ä¸€å¸§ï¼Œç„¶åé¢„æµ‹åç»­å¸§çš„ç›¸åº”é®ç½©ã€‚ä»£ç å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/LeipingJie/SAM2Video%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LeipingJie/SAM2Videoè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2ä½œä¸ºSegment Anything Modelï¼ˆSAMï¼‰çš„ç»§ä»»è€…ï¼Œä¸ä»…åœ¨å›¾åƒåˆ†å‰²æ–¹é¢æå‡äº†æ€§èƒ½ï¼Œè¿˜æ‰©å±•äº†è§†é¢‘åˆ†å‰²çš„èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹SAM2åœ¨è§†é¢‘é˜´å½±æ£€æµ‹ï¼ˆVSDï¼‰å’Œè§†é¢‘é•œåƒæ£€æµ‹ï¼ˆVMDï¼‰ç­‰è§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨çœŸå®æ ‡è®°ç‚¹æˆ–é®ç½©æç¤ºæ¥åˆå§‹åŒ–ç¬¬ä¸€å¸§ï¼Œå¹¶é¢„æµ‹åç»­å¸§çš„ç›¸åº”é®ç½©ï¼Œæ˜¯SAM2åœ¨å¤„ç†è§†é¢‘åˆ†å‰²ä»»åŠ¡æ—¶çš„ä¸€ç§é‡è¦æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAM2åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ç‚¹æç¤ºæ—¶ï¼Œå®šé‡å’Œå®šæ€§è¡¨ç°å‡ä¸ä½³ã€‚</li>
<li>SAM2åœ¨ç½•è§ç‰©ä½“çš„è§†é¢‘åˆ†å‰²ä¸Šçš„æ€§èƒ½è¿˜éœ€è¿›ä¸€æ­¥æ¢ç´¢å’Œæå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db1f2a4f36d1530b9f58c84abd54dcc3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c34ee2fad0dd7726e2eea2aa3ef9082e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e760699127a8487877a0e3802c905ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-767172e45ac26d80e63b14b0aaf7ee8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6e7853b1288a25bf542ec5ca9af8271.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80f4f5ed6d5250e5ee92490c80ba1959.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mask-Factory-Towards-High-quality-Synthetic-Data-Generation-for-Dichotomous-Image-Segmentation"><a href="#Mask-Factory-Towards-High-quality-Synthetic-Data-Generation-for-Dichotomous-Image-Segmentation" class="headerlink" title="Mask Factory: Towards High-quality Synthetic Data Generation for   Dichotomous Image Segmentation"></a>Mask Factory: Towards High-quality Synthetic Data Generation for   Dichotomous Image Segmentation</h2><p><strong>Authors:Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan</strong></p>
<p>Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at \url{<a target="_blank" rel="noopener" href="https://qian-hao-tian.github.io/MaskFactory/%7D">https://qian-hao-tian.github.io/MaskFactory/}</a>. </p>
<blockquote>
<p>äºŒå€¼å›¾åƒåˆ†å‰²ï¼ˆDISï¼‰ä»»åŠ¡éœ€è¦é«˜åº¦ç²¾ç¡®çš„æ ‡æ³¨ï¼Œè€Œä¼ ç»Ÿçš„æ•°æ®é›†åˆ›å»ºæ–¹æ³•åŠ³åŠ¨å¼ºåº¦å¤§ã€æˆæœ¬é«˜ï¼Œå¹¶éœ€è¦å¹¿æ³›çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚è™½ç„¶ä½¿ç”¨åˆæˆæ•°æ®å¯¹äºDISæ˜¯ä¸€ä¸ªè§£å†³è¿™äº›æŒ‘æˆ˜çš„å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å½“å‰çš„ç”Ÿæˆæ¨¡å‹å’ŒæŠ€æœ¯ä»ç„¶é¢ä¸´åœºæ™¯åå·®ã€å™ªå£°å¼•èµ·çš„é”™è¯¯ä»¥åŠè®­ç»ƒæ ·æœ¬å˜åŒ–æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•â€”â€”<strong>ourmodel</strong>ï¼Œå®ƒä¸ºç”Ÿæˆå¤šæ ·ä¸”ç²¾ç¡®çš„æ•°æ®é›†æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—å‡å°‘äº†å‡†å¤‡æ—¶é—´å’Œæˆæœ¬ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§é€šç”¨çš„æ©è†œç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åˆšæ€§å’Œéåˆšæ€§ç¼–è¾‘æŠ€æœ¯æ¥ç”Ÿæˆé«˜è´¨é‡åˆæˆæ©è†œã€‚å…·ä½“æ¥è¯´ï¼Œåˆšæ€§ç¼–è¾‘åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼Œåœ¨æ— æ ·æœ¬æ¡ä»¶ä¸‹å®ç°ç²¾ç¡®çš„è§†ç‚¹è½¬æ¢ï¼Œè€Œéåˆšæ€§ç¼–è¾‘åˆ™é‡‡ç”¨å¯¹æŠ—æ€§è®­ç»ƒå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤æ‚ä¸”æ‹“æ‰‘ä¸€è‡´çš„ä¿®æ”¹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šæ¡ä»¶æ§åˆ¶ç”Ÿæˆæ–¹æ³•ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒå’Œç²¾ç¡®åˆ†å‰²æ©è†œå¯¹ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„DIS5Kæ•°æ®é›†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://qian-hao-tian.github.io/MaskFactory/]%E8%AE%BF%E9%97%AE%E3%80%82">https://qian-hao-tian.github.io/MaskFactory/]è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹å›¾åƒåˆ†å‰²æ•°æ®é›†ç”Ÿæˆæ–¹æ³•â€”â€”æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆourmodelï¼‰ï¼Œè§£å†³äº†ä¼ ç»Ÿåˆ›å»ºæ•°æ®é›†æ–¹å¼æ‰€é¢ä¸´çš„åŠ³åŠ¨å¼ºåº¦å¤§ã€æˆæœ¬é«˜æ˜‚å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†éœ€æ±‚å¼ºç­‰é—®é¢˜ã€‚é€šè¿‡ç»“åˆåˆšæ€§å’Œéåˆšæ€§ç¼–è¾‘æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡åˆæˆæ©è†œï¼Œå¹¶åˆ©ç”¨å¤šæ¡ä»¶æ§åˆ¶ç”Ÿæˆæ–¹æ³•ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒå’Œç²¾ç¡®åˆ†å‰²æ©è†œã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„DIS5Kæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è´¨é‡å’Œæ•ˆç‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²ä»»åŠ¡éœ€è¦å¤§é‡ç²¾ç¡®æ ‡æ³¨çš„æ•°æ®é›†ï¼Œä¼ ç»Ÿåˆ›å»ºæ–¹æ³•æˆæœ¬é«˜ä¸”éœ€è¦å¤§é‡é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>å½“å‰ç”Ÿæˆæ¨¡å‹é¢ä¸´åœºæ™¯åå·®ã€å™ªå£°å¼•èµ·çš„é”™è¯¯å’Œè®­ç»ƒæ ·æœ¬å˜åŒ–æœ‰é™ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å›¾åƒåˆ†å‰²æ•°æ®é›†ç”Ÿæˆæ–¹æ³•â€”â€”æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆourmodelï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ç»“åˆåˆšæ€§å’Œéåˆšæ€§ç¼–è¾‘æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡åˆæˆæ©è†œã€‚</li>
<li>åˆ©ç”¨å¤šæ¡ä»¶æ§åˆ¶ç”Ÿæˆæ–¹æ³•ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒå’Œç²¾ç¡®åˆ†å‰²æ©è†œã€‚</li>
<li>åœ¨å¹¿æ³›ä½¿ç”¨çš„DIS5Kæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6a7509bb5250ab84bc290c1a33c97b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c0c3f99332e35912e94921a2277486.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2287665b2b547322249f1e85e0a3c5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474ed34159b63c807511e70c8ee71da4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HELPNet-Hierarchical-Perturbations-Consistency-and-Entropy-guided-Ensemble-for-Scribble-Supervised-Medical-Image-Segmentation"><a href="#HELPNet-Hierarchical-Perturbations-Consistency-and-Entropy-guided-Ensemble-for-Scribble-Supervised-Medical-Image-Segmentation" class="headerlink" title="HELPNet: Hierarchical Perturbations Consistency and Entropy-guided   Ensemble for Scribble Supervised Medical Image Segmentation"></a>HELPNet: Hierarchical Perturbations Consistency and Entropy-guided   Ensemble for Scribble Supervised Medical Image Segmentation</h2><p><strong>Authors:Xiao Zhang, Shaoxuan Wu, Peilin Zhang, Zhuo Jin, Xiaosong Xiong, Qirong Bu, Jingkun Chen, Jun Feng</strong></p>
<p>Creating fully annotated labels for medical image segmentation is prohibitively time-intensive and costly, emphasizing the necessity for innovative approaches that minimize reliance on detailed annotations. Scribble annotations offer a more cost-effective alternative, significantly reducing the expenses associated with full annotations. However, scribble annotations offer limited and imprecise information, failing to capture the detailed structural and boundary characteristics necessary for accurate organ delineation. To address these challenges, we propose HELPNet, a novel scribble-based weakly supervised segmentation framework, designed to bridge the gap between annotation efficiency and segmentation performance. HELPNet integrates three modules. The Hierarchical perturbations consistency (HPC) module enhances feature learning by employing density-controlled jigsaw perturbations across global, local, and focal views, enabling robust modeling of multi-scale structural representations. Building on this, the Entropy-guided pseudo-label (EGPL) module evaluates the confidence of segmentation predictions using entropy, generating high-quality pseudo-labels. Finally, the structural prior refinement (SPR) module incorporates connectivity and bounded priors to enhance the precision and reliability and pseudo-labels. Experimental results on three public datasets ACDC, MSCMRseg, and CHAOS show that HELPNet significantly outperforms state-of-the-art methods for scribble-based weakly supervised segmentation and achieves performance comparable to fully supervised methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/IPMI-NWU/HELPNet">https://github.com/IPMI-NWU/HELPNet</a>. </p>
<blockquote>
<p>ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²åˆ›å»ºå®Œå…¨æ³¨é‡Šçš„æ ‡ç­¾æ˜¯éå¸¸è€—æ—¶å’Œæ˜‚è´µçš„ï¼Œè¿™å¼ºè°ƒäº†å¯¹åˆ›æ–°æ–¹æ³•çš„å¿…è¦æ€§ï¼Œè¿™äº›æ–¹æ³•éœ€è¦å°½é‡å‡å°‘å¯¹è¯¦ç»†æ³¨é‡Šçš„ä¾èµ–ã€‚æ¶‚é¸¦æ³¨é‡Šæä¾›äº†ä¸€ç§æ›´å…·æˆæœ¬æ•ˆç›Šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ˜¾è‘—é™ä½äº†ä¸å®Œæ•´æ³¨é‡Šç›¸å…³çš„è´¹ç”¨ã€‚ç„¶è€Œï¼Œæ¶‚é¸¦æ³¨é‡Šæä¾›çš„ä¿¡æ¯æœ‰é™ä¸”ä¸å‡†ç¡®ï¼Œæ— æ³•æ•æ‰åˆ°ç”¨äºå‡†ç¡®å™¨å®˜æç»˜æ‰€éœ€çš„è¯¦ç»†ç»“æ„å’Œè¾¹ç•Œç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HELPNetï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¶‚é¸¦çš„å¼±ç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥è¡¥æ³¨é‡Šæ•ˆç‡å’Œåˆ†å‰²æ€§èƒ½ä¹‹é—´çš„å·®è·ã€‚HELPNeté›†æˆäº†ä¸‰ä¸ªæ¨¡å—ã€‚åˆ†å±‚æ‰°åŠ¨ä¸€è‡´æ€§ï¼ˆHPCï¼‰æ¨¡å—é€šè¿‡é‡‡ç”¨å…¨å±€ã€å±€éƒ¨å’Œç„¦ç‚¹è§†å›¾ä¸Šçš„å¯†åº¦æ§åˆ¶æ‹¼å›¾æ‰°åŠ¨æ¥å¢å¼ºç‰¹å¾å­¦ä¹ ï¼Œä»è€Œå®ç°å¯¹å¤šå°ºåº¦ç»“æ„è¡¨ç¤ºçš„ç¨³å¥å»ºæ¨¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç†µå¼•å¯¼ä¼ªæ ‡ç­¾ï¼ˆEGPLï¼‰æ¨¡å—ä½¿ç”¨ç†µè¯„ä¼°åˆ†å‰²é¢„æµ‹çš„ä¿¡å¿ƒï¼Œç”Ÿæˆé«˜è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚æœ€åï¼Œç»“æ„å…ˆéªŒç»†åŒ–ï¼ˆSPRï¼‰æ¨¡å—ç»“åˆäº†è¿æ¥å’Œè¾¹ç•Œå…ˆéªŒçŸ¥è¯†ï¼Œæé«˜äº†ä¼ªæ ‡ç­¾çš„ç²¾ç¡®æ€§å’Œå¯é æ€§ã€‚åœ¨ACDCã€MSCMRsegå’ŒCHAOSä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHELPNetåœ¨åŸºäºæ¶‚é¸¦çš„å¼±ç›‘ç£åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šå¯ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸åª²ç¾ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IPMI-NWU/HELPNet">https://github.com/IPMI-NWU/HELPNet</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18738v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åˆ›æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ ‡æ³¨æ–¹æ³•HELPNetè¢«æå‡ºï¼Œæ—¨åœ¨é€šè¿‡å±‚æ¬¡åŒ–æ‰°åŠ¨ä¸€è‡´æ€§æ¨¡å—ã€ç†µå¼•å¯¼ä¼ªæ ‡ç­¾æ¨¡å—å’Œç»“æ„å…ˆéªŒç»†åŒ–æ¨¡å—ï¼Œç¼©å°æ¶‚é¸¦æ³¨é‡Šä¸åˆ†å‰²æ€§èƒ½ä¹‹é—´çš„å·®è·ï¼Œæé«˜æ ‡æ³¨æ•ˆç‡å¹¶å…¼é¡¾åˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHELPNetåœ¨æ¶‚é¸¦æ³¨é‡Šçš„å¼±ç›‘ç£åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²çš„å®Œå…¨æ³¨é‡Šæ ‡ç­¾åˆ›å»ºæ˜¯è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„ï¼Œéœ€è¦åˆ›æ–°æ–¹æ³•æ¥å‡å°‘ä¾èµ–è¯¦ç»†çš„æ³¨é‡Šã€‚</li>
<li>æ¶‚é¸¦æ³¨é‡Šä½œä¸ºä¸€ç§æ›´ç»æµçš„æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†å…¨æ³¨é‡Šçš„è´¹ç”¨ï¼Œä½†æä¾›çš„ä¿¡æ¯æœ‰é™ä¸”ä¸ç²¾ç¡®ã€‚</li>
<li>HELPNetæ˜¯ä¸€ä¸ªåŸºäºæ¶‚é¸¦çš„å¼±ç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ³¨é‡Šæ•ˆç‡å’Œåˆ†å‰²æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚</li>
<li>HELPNetåŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šå±‚æ¬¡åŒ–æ‰°åŠ¨ä¸€è‡´æ€§æ¨¡å—ã€ç†µå¼•å¯¼ä¼ªæ ‡ç­¾æ¨¡å—å’Œç»“æ„å…ˆéªŒç»†åŒ–æ¨¡å—ã€‚</li>
<li>å±‚æ¬¡åŒ–æ‰°åŠ¨ä¸€è‡´æ€§æ¨¡å—é€šè¿‡å¯†åº¦æ§åˆ¶çš„æ‹¼å›¾æ‰°åŠ¨å¢å¼ºç‰¹å¾å­¦ä¹ ï¼Œå¹¶å»ºæ¨¡å¤šå°ºåº¦ç»“æ„è¡¨ç¤ºã€‚</li>
<li>ç†µå¼•å¯¼ä¼ªæ ‡ç­¾æ¨¡å—åˆ©ç”¨ç†µè¯„ä¼°åˆ†å‰²é¢„æµ‹çš„ç½®ä¿¡åº¦ï¼Œç”Ÿæˆé«˜è´¨é‡ä¼ªæ ‡ç­¾ã€‚</li>
<li>ç»“æ„å…ˆéªŒç»†åŒ–æ¨¡å—ç»“åˆè¿é€šæ€§å’Œè¾¹ç•Œå…ˆéªŒæé«˜ä¼ªæ ‡ç­¾çš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aabd324032c2355834bcd8c20404719d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e340d7695df5e57fb7048273a14fc509.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466f64f96a8db18fd36298036e356394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71eb62c039b376ed333769fd670a75c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bba76b5d678ed2d3083f1f1a5cb69d0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Edge-AI-for-Agriculture-Lightweight-Vision-Models-for-Disease-Detection-in-Resource-Limited-Settings"><a href="#Edge-AI-for-Agriculture-Lightweight-Vision-Models-for-Disease-Detection-in-Resource-Limited-Settings" class="headerlink" title="Edge-AI for Agriculture: Lightweight Vision Models for Disease Detection   in Resource-Limited Settings"></a>Edge-AI for Agriculture: Lightweight Vision Models for Disease Detection   in Resource-Limited Settings</h2><p><strong>Authors:Harsh Joshi</strong></p>
<p>This research paper presents the development of a lightweight and efficient computer vision pipeline aimed at assisting farmers in detecting orange diseases using minimal resources. The proposed system integrates advanced object detection, classification, and segmentation models, optimized for deployment on edge devices, ensuring functionality in resource-limited environments. The study evaluates the performance of various state-of-the-art models, focusing on their accuracy, computational efficiency, and generalization capabilities. Notable findings include the Vision Transformer achieving 96 accuracy in orange species classification and the lightweight YOLOv8-S model demonstrating exceptional object detection performance with minimal computational overhead. The research highlights the potential of modern deep learning architectures to address critical agricultural challenges, emphasizing the importance of model complexity versus practical utility. Future work will explore expanding datasets, model compression techniques, and federated learning to enhance the applicability of these systems in diverse agricultural contexts, ultimately contributing to more sustainable farming practices. </p>
<blockquote>
<p>è¿™ç¯‡ç ”ç©¶è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆç‡çš„è®¡ç®—æœºè§†è§‰æµç¨‹çš„å¼€å‘æƒ…å†µï¼Œè¯¥æµç¨‹æ—¨åœ¨åˆ©ç”¨æœ€å°‘çš„èµ„æºå¸®åŠ©å†œæ°‘æ£€æµ‹æŸ‘æ©˜ç–¾ç—…ã€‚æ‰€æå‡ºçš„ç³»ç»Ÿé›†æˆäº†å…ˆè¿›çš„å¯¹è±¡æ£€æµ‹ã€åˆ†ç±»å’Œåˆ†å‰²æ¨¡å‹ï¼Œé’ˆå¯¹è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½æ­£å¸¸è¿è¡Œã€‚è¯¥ç ”ç©¶è¯„ä¼°äº†å„ç§æœ€æ–°æ¨¡å‹çš„è¡¨ç°ï¼Œä¸»è¦å…³æ³¨å…¶å‡†ç¡®æ€§ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVision Transformeråœ¨æŸ‘æ©˜å“ç§åˆ†ç±»æ–¹é¢è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ï¼Œè€Œè½»é‡çº§çš„YOLOv8-Sæ¨¡å‹åœ¨å¯¹è±¡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè®¡ç®—å¼€é”€æå°ã€‚è¯¥ç ”ç©¶çªå‡ºäº†ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„è§£å†³é‡è¦å†œä¸šæŒ‘æˆ˜çš„æ½œåŠ›ï¼Œå¼ºè°ƒæ¨¡å‹å¤æ‚åº¦ä¸å®é™…æ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚æœªæ¥çš„å·¥ä½œå°†æ¢ç´¢æ‰©å¤§æ•°æ®é›†ã€æ¨¡å‹å‹ç¼©æŠ€æœ¯å’Œè”é‚¦å­¦ä¹ ï¼Œä»¥æé«˜è¿™äº›ç³»ç»Ÿåœ¨å„ç§å†œä¸šç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ï¼Œæœ€ç»ˆä¸ºæ›´å¯æŒç»­çš„å†œä¸šå®è·µåšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18635v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§è½»é‡çº§ã€é«˜æ•ˆç‡çš„è®¡ç®—æœºè§†è§‰ç®¡é“ï¼Œæ—¨åœ¨åˆ©ç”¨æœ‰é™çš„èµ„æºååŠ©å†œæ°‘æ£€æµ‹æŸ‘æ©˜ç–¾ç—…ã€‚è¯¥ç³»ç»Ÿæ•´åˆäº†å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹ã€åˆ†ç±»å’Œåˆ†å‰²æ¨¡å‹ï¼Œä¼˜åŒ–åå¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œç¡®ä¿åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½æ­£å¸¸è¿è¡Œã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ï¼Œé‡ç‚¹å…³æ³¨å…¶å‡†ç¡®æ€§ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒVision Transformeråœ¨æŸ‘æ©˜å“ç§åˆ†ç±»æ–¹é¢çš„å‡†ç¡®ç‡è¾¾åˆ°äº†96%ï¼Œè€Œè½»é‡çº§çš„YOLOv8-Sæ¨¡å‹åœ¨ç‰©ä½“æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè®¡ç®—å¼€é”€æå°ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„åœ¨è§£å†³å†œä¸šæŒ‘æˆ˜ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†æ¨¡å‹å¤æ‚åº¦ä¸å®é™…æ•ˆç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚æœªæ¥çš„å·¥ä½œå°†æ¢ç´¢æ‰©å¤§æ•°æ®é›†ã€æ¨¡å‹å‹ç¼©æŠ€æœ¯å’Œè”é‚¦å­¦ä¹ ï¼Œä»¥æé«˜è¿™äº›ç³»ç»Ÿåœ¨å¤šç§å†œä¸šç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ï¼Œä¸ºæ›´å¯æŒç»­çš„å†œä¸šå®è·µåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼€å‘äº†ä¸€ç§è½»é‡çº§ã€é«˜æ•ˆç‡çš„è®¡ç®—æœºè§†è§‰ç®¡é“ï¼Œç”¨äºååŠ©å†œæ°‘æ£€æµ‹æŸ‘æ©˜ç–¾ç—…ã€‚</li>
<li>ç³»ç»Ÿæ•´åˆäº†å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹ã€åˆ†ç±»å’Œåˆ†å‰²æ¨¡å‹ï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Vision Transformeråœ¨æŸ‘æ©˜å“ç§åˆ†ç±»ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†96%ã€‚</li>
<li>YOLOv8-Sæ¨¡å‹åœ¨ç‰©ä½“æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸”è®¡ç®—å¼€é”€å°ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç°ä»£æ·±åº¦å­¦ä¹ æ¶æ„åœ¨å†œä¸šæŒ‘æˆ˜ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43163421dc81ee1920620bd9e8f2a0f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d79d8ee50b35e6bf1e1f592cca9f32d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f36f08413e14056652abe75cf31396a9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis"><a href="#VisionGRU-A-Linear-Complexity-RNN-Model-for-Efficient-Image-Analysis" class="headerlink" title="VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis"></a>VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis</h2><p><strong>Authors:Shicheng Yin, Kaixuan Yin, Weixing Chen, Enbo Huang, Yang Liu</strong></p>
<p>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two dominant models for image analysis. While CNNs excel at extracting multi-scale features and ViTs effectively capture global dependencies, both suffer from high computational costs, particularly when processing high-resolution images. Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have attracted attention due to their efficiency. However, their performance in image classification tasks remains limited. To address these challenges, this paper introduces VisionGRU, a novel RNN-based architecture designed for efficient image classification. VisionGRU leverages a simplified Gated Recurrent Unit (minGRU) to process large-scale image features with linear complexity. It divides images into smaller patches and progressively reduces the sequence length while increasing the channel depth, thus facilitating multi-scale feature extraction. A hierarchical 2DGRU module with bidirectional scanning captures both local and global contexts, improving long-range dependency modeling, particularly for tasks like semantic segmentation. Experimental results on the ImageNet and ADE20K datasets demonstrate that VisionGRU outperforms ViTs, significantly reducing memory usage and computational costs, especially for high-resolution images. These findings underscore the potential of RNN-based approaches for developing efficient and scalable computer vision solutions. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a>. </p>
<blockquote>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ˜¯å›¾åƒåˆ†æä¸­çš„ä¸¤ç§ä¸»å¯¼æ¨¡å‹ã€‚è™½ç„¶CNNæ“…é•¿æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œè€ŒViTèƒ½æœ‰æ•ˆæ•æ‰å…¨å±€ä¾èµ–æ€§ï¼Œä½†ä¸¤è€…éƒ½å­˜åœ¨è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚è¿‘å¹´æ¥ï¼Œç”±äºæ•ˆç‡è¾ƒé«˜ï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆå›¾åƒåˆ†ç±»çš„æ–°å‹RNNæ¶æ„â€”â€”VisionGRUã€‚VisionGRUåˆ©ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ã€‚å®ƒå°†å›¾åƒåˆ†æˆè¾ƒå°çš„æ–‘å—ï¼Œé€æ­¥å‡å°‘åºåˆ—é•¿åº¦ï¼ŒåŒæ—¶å¢åŠ é€šé“æ·±åº¦ï¼Œä»è€Œä¾¿äºå¤šå°ºåº¦ç‰¹å¾æå–ã€‚å…·æœ‰åŒå‘æ‰«æçš„åˆ†å±‚2DGRUæ¨¡å—å¯ä»¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›äº†é•¿è·ç¦»ä¾èµ–å»ºæ¨¡ï¼Œå°¤å…¶é€‚ç”¨äºè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ã€‚åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionGRUä¼˜äºViTsï¼Œæå¤§åœ°å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ï¼Œå°¤å…¶å¯¹äºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åŸºäºRNNçš„æ–¹æ³•åœ¨å¼€å‘é«˜æ•ˆä¸”å¯æ‰©å±•çš„è®¡ç®—æœºè§†è§‰è§£å†³æ–¹æ¡ˆæ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU%E4%B8%8A%E6%8F%AD%E6%9B%B4%E3%80%82">https://github.com/YangLiu9208/VisionGRUä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18178v2">PDF</a> Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/YangLiu9208/VisionGRU">https://github.com/YangLiu9208/VisionGRU</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºRNNçš„æ–°å‹å›¾åƒåˆ†ç±»æ¶æ„â€”â€”VisionGRUï¼Œç”¨äºè§£å†³CNNå’ŒViTåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚VisionGRUåˆ©ç”¨ç®€åŒ–çš„é—¨æ§å¾ªç¯å•å…ƒï¼ˆminGRUï¼‰ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ï¼Œé€šè¿‡åˆ†å‰²å›¾åƒæˆå°å—å¹¶æ¸è¿›å¼å‡å°‘åºåˆ—é•¿åº¦åŒæ—¶å¢åŠ é€šé“æ·±åº¦ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾æå–ã€‚å…¶å±‚æ¬¡åŒ–çš„2DGRUæ¨¡å—ç»“åˆåŒå‘æ‰«æï¼Œèƒ½æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå°¤å…¶å¯¹äºè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œåœ¨é•¿æœŸä¾èµ–å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨ImageNetå’ŒADE20Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionGRUåœ¨é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæ€§èƒ½ä¼˜äºViTï¼Œå°¤å…¶æ˜¯å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisionGRUæ˜¯ä¸€ç§æ–°å‹çš„RNNæ¶æ„ï¼Œæ—¨åœ¨è§£å†³å›¾åƒåˆ†ç±»ä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>VisionGRUåˆ©ç”¨ç®€åŒ–çš„Gated Recurrent Unitï¼ˆminGRUï¼‰å¤„ç†å¤§è§„æ¨¡å›¾åƒç‰¹å¾ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚</li>
<li>é€šè¿‡å°†å›¾åƒåˆ†å‰²æˆå°å—ï¼ŒVisionGRUå®ç°äº†å¤šå°ºåº¦ç‰¹å¾æå–ã€‚</li>
<li>VisionGRUé‡‡ç”¨å±‚æ¬¡åŒ–çš„2DGRUæ¨¡å—ï¼Œç»“åˆåŒå‘æ‰«æï¼Œä»¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>VisionGRUåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ã€‚</li>
<li>ä¸ViTç›¸æ¯”ï¼ŒVisionGRUåœ¨é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a85fe118fa527a73edc6f2d8766f4732.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd437ed5c29515688f16cc8c0903e437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73664fb72c773b1bf490d6a20e930639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0f8b7955c0062d89f3422d901b00d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fecfa3a65b425b6a4fd26d38b4a8f5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ReXTrust-A-Model-for-Fine-Grained-Hallucination-Detection-in-AI-Generated-Radiology-Reports"><a href="#ReXTrust-A-Model-for-Fine-Grained-Hallucination-Detection-in-AI-Generated-Radiology-Reports" class="headerlink" title="ReXTrust: A Model for Fine-Grained Hallucination Detection in   AI-Generated Radiology Reports"></a>ReXTrust: A Model for Fine-Grained Hallucination Detection in   AI-Generated Radiology Reports</h2><p><strong>Authors:Romain Hardy, Sung Eun Kim, Pranav Rajpurkar</strong></p>
<p>The increasing adoption of AI-generated radiology reports necessitates robust methods for detecting hallucinationsâ€“false or unfounded statements that could impact patient care. We present ReXTrust, a novel framework for fine-grained hallucination detection in AI-generated radiology reports. Our approach leverages sequences of hidden states from large vision-language models to produce finding-level hallucination risk scores. We evaluate ReXTrust on a subset of the MIMIC-CXR dataset and demonstrate superior performance compared to existing approaches, achieving an AUROC of 0.8751 across all findings and 0.8963 on clinically significant findings. Our results show that white-box approaches leveraging model hidden states can provide reliable hallucination detection for medical AI systems, potentially improving the safety and reliability of automated radiology reporting. </p>
<blockquote>
<p>éšç€AIç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šè¶Šæ¥è¶Šå¤šåœ°è¢«é‡‡çº³ï¼Œéœ€è¦å¯é çš„æ£€æµ‹å¹»è±¡ï¼ˆå¯èƒ½å¯¹æ‚£è€…æŠ¤ç†äº§ç”Ÿå½±å“çš„ä¸çœŸå®æˆ–æ²¡æœ‰æ ¹æ®çš„é™ˆè¿°ï¼‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ReXTrustï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç²¾ç»†ç²’åº¦å¹»è±¡æ£€æµ‹çš„æ–°å‹æ¡†æ¶ï¼Œé€‚ç”¨äºAIç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€åºåˆ—æ¥ç”Ÿæˆå‘ç°çº§åˆ«çš„å¹»è±¡é£é™©åˆ†æ•°ã€‚æˆ‘ä»¬åœ¨MIMIC-CXRæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¯„ä¼°äº†ReXTrustï¼Œå¹¶å±•ç¤ºäº†ç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„å“è¶Šæ€§èƒ½ï¼Œåœ¨æ‰€æœ‰å‘ç°ä¸­çš„AUROCè¾¾åˆ°0.8751ï¼Œåœ¨å…·æœ‰ä¸´åºŠæ„ä¹‰ä¸Šçš„å‘ç°ä¸­è¾¾åˆ°0.8963ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æ¨¡å‹éšè—çŠ¶æ€çš„white-boxæ–¹æ³•å¯ä»¥ä¸ºåŒ»ç–—AIç³»ç»Ÿæä¾›å¯é çš„å¹»è±¡æ£€æµ‹ï¼Œä»è€Œæé«˜è‡ªåŠ¨æ”¾å°„æŠ¥å‘Šçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15264v2">PDF</a> Accepted to AIMedHealth 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½ç”Ÿæˆçš„åŒ»å­¦å½±åƒæŠ¥å‘Šä¸­çš„å‡æˆ–æœªç»è¯å®é™ˆè¿°å¯¹æ‚£è€…æŠ¤ç†å¯èƒ½äº§ç”Ÿçš„å½±å“ï¼Œéœ€è¦å¼ºå¤§çš„æ–¹æ³•æ¥æ£€æµ‹è¿™äº›é™ˆè¿°çš„å¯é æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ReXTrustï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç²¾ç»†ç²’åº¦å¹»è§‰æ£€æµ‹çš„æ–°é¢–æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€åºåˆ—æ¥ç”Ÿæˆå‘ç°çº§åˆ«çš„å¹»è§‰é£é™©åˆ†æ•°ã€‚åœ¨MIMIC-CXRæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¯„ä¼°ReXTrustï¼Œæ˜¾ç¤ºå‡ºæ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„æ€§èƒ½ï¼Œæ‰€æœ‰å‘ç°è¾¾åˆ°AUROCçš„0.8751ï¼Œä¸´åºŠé‡è¦å‘ç°è¾¾åˆ°AUROCçš„0.8963ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨æ¨¡å‹éšè—çŠ¶æ€çš„ç™½è‰²ç›’å­æ–¹æ³•å¯ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›å¯é çš„å¹»è§‰æ£€æµ‹ï¼Œä»è€Œå¯èƒ½æé«˜è‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒæŠ¥å‘Šçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„åŒ»å­¦å½±åƒæŠ¥å‘Šä¸­å­˜åœ¨å‡æˆ–æœªç»è¯å®é™ˆè¿°çš„é£é™©ã€‚</li>
<li>ReXTrustæ˜¯ä¸€ç§ç”¨äºç²¾ç»†ç²’åº¦å¹»è§‰æ£€æµ‹çš„æ¡†æ¶ã€‚</li>
<li>ReXTruståˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€åºåˆ—æ¥ç”Ÿæˆå‘ç°çº§åˆ«çš„å¹»è§‰é£é™©åˆ†æ•°ã€‚</li>
<li>åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šè¯„ä¼°ReXTrustï¼Œæ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ReXTruståœ¨æ‰€æœ‰å‘ç°ä¸Šçš„AUROCä¸º0.8751ï¼Œåœ¨å…·æœ‰ä¸´åºŠæ„ä¹‰å‘ç°ä¸Šä¸º0.8963ã€‚</li>
<li>åˆ©ç”¨æ¨¡å‹éšè—çŠ¶æ€çš„ç™½è‰²ç›’å­æ–¹æ³•å¯ä»¥å¯é åœ°æ£€æµ‹åŒ»å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„å¹»è§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9b0e4f3515b1a165a2aa1ab59f2771a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be73a10682d39586ee1d6615b7995a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1c06daa1f89dc28994cef2192684bcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c5cade60fa8f93b9ef70c56d993f49a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Artifact2Artifact-Self-incentive-artifact-removal-for-photoacoustic-imaging-without-any-data"><a href="#Zero-Shot-Artifact2Artifact-Self-incentive-artifact-removal-for-photoacoustic-imaging-without-any-data" class="headerlink" title="Zero-Shot Artifact2Artifact: Self-incentive artifact removal for   photoacoustic imaging without any data"></a>Zero-Shot Artifact2Artifact: Self-incentive artifact removal for   photoacoustic imaging without any data</h2><p><strong>Authors:Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li</strong></p>
<p>Photoacoustic imaging (PAI) uniquely combines optical contrast with the penetration depth of ultrasound, making it critical for clinical applications. However, the quality of 3D PAI is often degraded due to reconstruction artifacts caused by the sparse and angle-limited configuration of detector arrays. Existing iterative or deep learning-based methods are either time-consuming or require large training datasets, significantly limiting their practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a zero-shot self-supervised artifact removal method based on a super-lightweight network, which leverages the fact that reconstruction artifacts are sensitive to irregularities caused by data loss. By introducing random perturbations to the acquired PA data, it spontaneously generates subset data, which in turn stimulates the network to learn the artifact patterns in the reconstruction results, thus enabling zero-shot artifact removal. This approach requires neither training data nor prior knowledge of the artifacts, and is capable of artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or slice images in 3D PAI acquired with arbitrarily sparse or angle-limited detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in both simulation study and $ in\ vivo $ animal experiments. Results demonstrate that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing zero-shot methods, and for the $ in\ vivo $ rat liver, ZS-A2A improves CNR from 17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in the following GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/JaegerCQ/ZS-A2A">https://github.com/JaegerCQ/ZS-A2A</a>. </p>
<blockquote>
<p>å…‰å£°æˆåƒï¼ˆPAIï¼‰ç‹¬ç‰¹åœ°ç»“åˆäº†å…‰å­¦å¯¹æ¯”åº¦å’Œè¶…å£°çš„ç©¿é€æ·±åº¦ï¼Œä½¿å…¶æˆä¸ºä¸´åºŠåº”ç”¨çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºæ¢æµ‹å™¨é˜µåˆ—é…ç½®ç¨€ç–ä¸”è§’åº¦å—é™å¯¼è‡´çš„é‡å»ºä¼ªå½±ï¼Œä¸‰ç»´PAIçš„è´¨é‡å¾€å¾€ä¼šä¸‹é™ã€‚ç°æœ‰çš„è¿­ä»£æˆ–åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è¦ä¹ˆè€—æ—¶è¿‡é•¿ï¼Œè¦ä¹ˆéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæå¤§åœ°é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¶…è½»é‡çº§ç½‘ç»œçš„é›¶æ ·æœ¬è‡ªç›‘ç£ä¼ªå½±å»é™¤æ–¹æ³•â€”â€”Zero-Shot Artifact2Artifactï¼ˆZS-A2Aï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é‡å»ºä¼ªå½±å¯¹æ•°æ®ä¸¢å¤±å¼•èµ·çš„ä¸è§„åˆ™æ€§æ•æ„Ÿçš„æœºåˆ¶ã€‚é€šè¿‡å¯¹é‡‡é›†çš„PAæ•°æ®è¿›è¡Œéšæœºæ‰°åŠ¨ï¼Œå®ƒè‡ªå‘åœ°ç”Ÿæˆå­é›†æ•°æ®ï¼Œä»è€Œåˆºæ¿€ç½‘ç»œå­¦ä¹ é‡å»ºç»“æœä¸­çš„ä¼ªå½±æ¨¡å¼ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬ä¼ªå½±å»é™¤ã€‚è¿™ç§æ–¹æ³•æ—¢ä¸éœ€è¦è®­ç»ƒæ•°æ®ï¼Œä¹Ÿä¸éœ€è¦äº‹å…ˆäº†è§£ä¼ªå½±ä¿¡æ¯ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¯¹ä¸‰ç»´PAIè¿›è¡Œä¼ªå½±å»é™¤ã€‚å¯¹äºä½¿ç”¨ä»»æ„ç¨€ç–æˆ–è§’åº¦å—é™çš„æ¢æµ‹å™¨é˜µåˆ—è·å¾—çš„ä¸‰ç»´PAIçš„æœ€å¤§æŒ¯å¹…æŠ•å½±å›¾åƒæˆ–åˆ‡ç‰‡å›¾åƒï¼ŒZS-A2Aé‡‡ç”¨è‡ªæˆ‘æ¿€åŠ±ç­–ç•¥å®Œæˆä¼ªå½±å»é™¤ï¼Œæé«˜äº†ä¿¡å™ªæ¯”ï¼ˆCNRï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ä»¿çœŸç ”ç©¶å’Œä½“å†…åŠ¨ç‰©å®éªŒéªŒè¯äº†ZS-A2Açš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„é›¶æ ·æœ¬æ–¹æ³•ç›¸æ¯”ï¼ŒZS-A2Aè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å¯¹äºä½“å†…å¤§é¼ è‚è„å®éªŒï¼ŒZS-A2Aåœ¨çŸ­çŸ­8ç§’å†…å°†CNRä»17.48æé«˜åˆ°43.46ã€‚ZS-A2Açš„é¡¹ç›®å°†åœ¨ä»¥ä¸‹GitHubä»“åº“ä¸­æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/JaegerCQ/ZS-A2A%E3%80%82">https://github.com/JaegerCQ/ZS-A2Aã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14873v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé›¶æ ·æœ¬è‡ªç›‘ç£çš„å»é™¤å…‰å£°æˆåƒï¼ˆPAIï¼‰é‡å»ºä¼ªå½±çš„æ–¹æ³•â€”â€”Zero-Shot Artifact2Artifactï¼ˆZS-A2Aï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¶…è½»é‡çº§ç½‘ç»œï¼Œé€šè¿‡å¼•å…¥éšæœºæ‰°åŠ¨åˆ°è·å–çš„PAæ•°æ®ï¼Œè‡ªå‘åœ°ç”Ÿæˆå­é›†æ•°æ®ï¼Œä»è€Œå­¦ä¹ é‡å»ºç»“æœä¸­çš„ä¼ªå½±æ¨¡å¼ï¼Œå®ç°é›¶æ ·æœ¬ä¼ªå½±å»é™¤ã€‚è¯¥æ–¹æ³•æ—¢ä¸éœ€è¦è®­ç»ƒæ•°æ®ï¼Œä¹Ÿä¸éœ€è¦äº‹å…ˆäº†è§£ä¼ªå½±ä¿¡æ¯ï¼Œå¹¶èƒ½æœ‰æ•ˆå»é™¤3D PAIä¸­çš„ä¼ªå½±ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒZS-A2Aåœ¨æ¨¡æ‹Ÿç ”ç©¶å’Œä½“å†…åŠ¨ç‰©å®éªŒä¸­å‡å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å£°æˆåƒï¼ˆPAIï¼‰ç»“åˆäº†å…‰å­¦å¯¹æ¯”åº¦å’Œè¶…å£°æ³¢çš„ç©¿é€æ·±åº¦ï¼Œå¯¹äºä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>3D PAIçš„è´¨é‡é€šå¸¸ç”±äºæ¢æµ‹å™¨é˜µåˆ—çš„ç¨€ç–å’Œè§’åº¦é™åˆ¶é…ç½®è€Œå—åˆ°å½±å“ï¼Œå¯¼è‡´é‡å»ºä¼ªå½±ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ï¼Œå¦‚è¿­ä»£æˆ–æ·±åº¦å­¦ä¹ ï¼Œå­˜åœ¨è€—æ—¶é•¿æˆ–éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>ZS-A2Aæ–¹æ³•åŸºäºé›¶æ ·æœ¬è‡ªç›‘ç£å­¦ä¹ ï¼Œåˆ©ç”¨è¶…è½»é‡çº§ç½‘ç»œå»é™¤é‡å»ºä¼ªå½±ã€‚</li>
<li>ZS-A2Aé€šè¿‡å¼•å…¥éšæœºæ‰°åŠ¨åˆ°è·å–çš„PAæ•°æ®ï¼Œè‡ªå‘ç”Ÿæˆå­é›†æ•°æ®ï¼Œåˆºæ¿€ç½‘ç»œå­¦ä¹ ä¼ªå½±æ¨¡å¼ã€‚</li>
<li>ZS-A2Aæ—¢ä¸éœ€è¦è®­ç»ƒæ•°æ®ï¼Œä¹Ÿä¸éœ€è¦å…³äºä¼ªå½±çš„å…ˆéªŒçŸ¥è¯†ï¼Œèƒ½æœ‰æ•ˆå»é™¤3D PAIä¸­çš„ä¼ªå½±ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒZS-A2Aåœ¨æ¨¡æ‹Ÿç ”ç©¶å’Œä½“å†…åŠ¨ç‰©å®éªŒä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—æé«˜å¯¹æ¯”å™ªå£°æ¯”ï¼ˆCNRï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc0772f20553e5462d42f9d0f2d8d8e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5eee0138563284fbbd307d6848075a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e017b8c95854cdc96643ff51c4ff1d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d52f5b2ca18b7459ae2965f19c5e5f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1799846d27bdeb0d877b0f4f2fd60e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-accc0edd854bbece275d448ef8dc8680.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the modelâ€™s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>ç°æœ‰çš„å°æ ·åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆFSMISï¼‰æ— æ³•è§£å†³åŒ»å­¦æˆåƒä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼šç”±äºä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„é¢†åŸŸåç§»ï¼Œé™åˆ¶äº†å…¶åœ¨å½“å‰FSMISä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè·¨é¢†åŸŸå°æ ·åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ–°ç›®æ ‡é¢†åŸŸæ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯ã€‚æˆ‘ä»¬å—åˆ°ä¸åŒé¢†åŸŸé—´é¢‘ç‡åŸŸç›¸ä¼¼æ€§ç‰¹å¾çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç”±äºå™¨å®˜å’Œç—…å˜çš„ä¸åŒå¤–è§‚å¯¼è‡´çš„åŸŸå†…æ–¹å·®ï¼ˆintra-domain varianceï¼‰å’Œç”±ä¸åŒåŒ»å­¦æˆåƒæŠ€æœ¯å¯¼è‡´çš„åŸŸé—´æ–¹å·®ï¼ˆinter-domain varianceï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªMSFæ¨¡å—ï¼Œä»¥æ•´åˆç”±FAMæ¨¡å—åˆ†ç¦»çš„ä¸åŒçš„é¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥å‡è½»è·¨åŸŸæ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚ç»“åˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬çš„FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°æ ·è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v4">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰çš„è·¨åŸŸå°æ ·åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡FAMæ¨¡å—å’ŒMSFæ¨¡å—çš„ååŒä½œç”¨ï¼Œæœ‰æ•ˆè§£å†³äº†ç”±äºä¸åŒæˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸæ¼‚ç§»é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹åœ¨æœ‰é™æ ‡ç­¾æ•°æ®ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹é¢ä¸´è·¨åŸŸé—®é¢˜ï¼Œå³ä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„åŸŸåç§»é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è·¨åŸŸå°æ ·åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿåœ¨æœ‰é™æ ‡ç­¾æ•°æ®ä¸‹é€‚åº”æ›´å¹¿æ³›åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯çš„é€šç”¨æ¨¡å‹ã€‚</li>
<li>FAMNetæ¨¡å‹ç”±é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆã€‚</li>
<li>FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šç”±äºå™¨å®˜å’Œç—…å˜çš„ä¸åŒå¤–è§‚å¼•èµ·çš„åŸŸå†…æ–¹å·®ä»¥åŠç”±äºä¸åŒåŒ»å­¦æˆåƒæŠ€æœ¯å¼•èµ·çš„åŸŸé—´æ–¹å·®ã€‚</li>
<li>MSFæ¨¡å—æ—¨åœ¨æ•´åˆç”±FAMæ¨¡å—åˆ†ç¦»çš„ä¸åŒçš„é¢‘ç‡ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å‡è½»åŸŸé—´æ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚</li>
<li>FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°æ ·è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa98f3e63dd1dfa34a0fe8c49c6c6931.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0796f64fbec939deef49b781663f0ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-def190b9c5343be34695a04abbf24490.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="KACQ-DCNN-Uncertainty-Aware-Interpretable-Kolmogorov-Arnold-Classical-Quantum-Dual-Channel-Neural-Network-for-Heart-Disease-Detection"><a href="#KACQ-DCNN-Uncertainty-Aware-Interpretable-Kolmogorov-Arnold-Classical-Quantum-Dual-Channel-Neural-Network-for-Heart-Disease-Detection" class="headerlink" title="KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold   Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection"></a>KACQ-DCNN: Uncertainty-Aware Interpretable Kolmogorov-Arnold   Classical-Quantum Dual-Channel Neural Network for Heart Disease Detection</h2><p><strong>Authors:Md Abrar Jahin, Md. Akmol Masud, M. F. Mridha, Zeyar Aung, Nilanjan Dey</strong></p>
<p>Heart failure is a leading cause of global mortality, necessitating improved diagnostic strategies. Classical machine learning models struggle with challenges such as high-dimensional data, class imbalances, poor feature representations, and lack of interpretability. While quantum machine learning holds promise, current hybrid models have not fully exploited quantum advantages. In this paper, we propose the Kolmogorov-Arnold Classical-Quantum Dual-Channel Neural Network (KACQ-DCNN), a novel hybrid architecture that replaces traditional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs), enabling learnable univariate activation functions. Our KACQ-DCNN 4-qubit, 1-layer model outperforms 37 benchmark models, including 16 classical and 12 quantum neural networks, achieving an accuracy of 92.03%, with macro-average precision, recall, and F1 scores of 92.00%. It also achieved a ROC-AUC of 94.77%, surpassing other models by significant margins, as validated by paired t-tests with a significance threshold of 0.0056 (after Bonferroni correction). Ablation studies highlight the synergistic effect of classical-quantum integration, improving performance by about 2% over MLP variants. Additionally, LIME and SHAP explainability techniques enhance feature interpretability, while conformal prediction provides robust uncertainty quantification. Our results demonstrate that KACQ-DCNN improves cardiovascular diagnostics by combining high accuracy with interpretability and uncertainty quantification. </p>
<blockquote>
<p>å¿ƒåŠ›è¡°ç«­æ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œéœ€è¦æ”¹è¿›è¯Šæ–­ç­–ç•¥ã€‚ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹é¢ä¸´é«˜ç»´æ•°æ®ã€ç±»åˆ«ä¸å¹³è¡¡ã€ç‰¹å¾è¡¨ç¤ºä¸ä½³å’Œç¼ºä¹å¯è§£é‡Šæ€§ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚è™½ç„¶é‡å­æœºå™¨å­¦ä¹ å¾ˆæœ‰å‰æ™¯ï¼Œä½†å½“å‰çš„æ··åˆæ¨¡å‹è¿˜æ²¡æœ‰å®Œå…¨å‘æŒ¥é‡å­ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Kolmogorov-Arnoldç»å…¸-é‡å­åŒé€šé“ç¥ç»ç½‘ç»œï¼ˆKACQ-DCNNï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰æ›¿ä»£äº†ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä½¿å­¦ä¹ å•å˜é‡æ¿€æ´»å‡½æ•°æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„KACQ-DCNNæ˜¯ä¸€ä¸ª4é‡å­ä½ã€1å±‚çš„æ¨¡å‹ï¼Œè¡¨ç°ä¼˜äº37ç§åŸºå‡†æ¨¡å‹ï¼ŒåŒ…æ‹¬16ç§ç»å…¸æ¨¡å‹å’Œ12ç§é‡å­ç¥ç»ç½‘ç»œï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†92.03%ï¼Œå®è§‚å¹³å‡ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°å‡ä¸º92.00%ã€‚å…¶ROC-AUCè¾¾åˆ°äº†94.77%ï¼Œè¶…è¶Šå…¶ä»–æ¨¡å‹ä¸€å¤§æˆªï¼Œè¿™ä¸€ç»“æœé€šè¿‡é…å¯¹tæ£€éªŒå¾—åˆ°éªŒè¯ï¼Œæ˜¾è‘—æ€§é˜ˆå€¼ä¸º0.0056ï¼ˆç»Bonferroniæ ¡æ­£åï¼‰ã€‚æ¶ˆèç ”ç©¶çªå‡ºäº†ç»å…¸ä¸é‡å­é›†æˆçš„ååŒä½œç”¨ï¼Œä¸å¤šå±‚æ„ŸçŸ¥æœºå˜ç§ç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†çº¦2%ã€‚æ­¤å¤–ï¼ŒLIMEå’ŒSHAPçš„å¯è§£é‡Šæ€§æŠ€æœ¯æé«˜äº†ç‰¹å¾çš„å¯è§£é‡Šæ€§ï¼Œè€Œé¡ºåº”æ€§é¢„æµ‹æä¾›äº†ç¨³å¥çš„ä¸ç¡®å®šæ€§é‡åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜ï¼ŒKACQ-DCNNé€šè¿‡ç»“åˆé«˜å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæ”¹è¿›äº†å¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07446v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ··åˆç¥ç»ç½‘ç»œæ¶æ„â€”â€”Kolmogorov-Arnoldç»å…¸é‡å­åŒé€šé“ç¥ç»ç½‘ç»œï¼ˆKACQ-DCNNï¼‰ï¼Œç”¨äºå¿ƒè„è¡°ç«­çš„è¯Šæ–­ã€‚è¯¥æ¶æ„é‡‡ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ›¿ä»£ä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå®ç°å¯å­¦ä¹ çš„å•å˜é‡æ¿€æ´»å‡½æ•°ã€‚KACQ-DCNNæ¨¡å‹åœ¨å¿ƒè„è¡°ç«­è¯Šæ–­ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç›¸è¾ƒäº37ç§åŸºå‡†æ¨¡å‹ï¼Œå…¶å‡†ç¡®ç‡é«˜è¾¾92.03%ï¼ŒåŒæ—¶å…·æœ‰è¾ƒé«˜çš„å®å¹³å‡ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1å¾—åˆ†ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>KACQ-DCNNæ˜¯ä¸€ç§æ–°å‹çš„æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†ç»å…¸æœºå™¨å­¦ä¹ å’Œé‡å­æœºå™¨å­¦ä¹ çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ¶æ„é‡‡ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†é«˜ç»´æ•°æ®ã€ç±»ä¸å¹³è¡¡å’Œç‰¹å¾è¡¨ç¤ºä¸è‰¯ç­‰é—®é¢˜ã€‚</li>
<li>KACQ-DCNNåœ¨å¿ƒè„è¡°ç«­è¯Šæ–­æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œè¾¾åˆ°92.03%ï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„å®å¹³å‡ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1å¾—åˆ†ã€‚</li>
<li>æ¨¡å‹é€šè¿‡LIMEå’ŒSHAPè§£é‡Šæ€§æŠ€æœ¯æé«˜äº†ç‰¹å¾çš„å¯è§£é‡Šæ€§ï¼Œä½¿å¾—è¯Šæ–­ç»“æœæ›´æ˜“äºç†è§£ã€‚</li>
<li>é€šè¿‡é…å¯¹tæ£€éªŒéªŒè¯äº†æ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨æ˜¾è‘—æ€§æ°´å¹³ä¸‹çš„ç¨³å®šæ€§ã€‚</li>
<li>æ¶ˆèç ”ç©¶æ˜¾ç¤ºäº†ç»å…¸ä¸é‡å­æ•´åˆçš„ååŒä½œç”¨ï¼Œç›¸è¾ƒäºå¤šå±‚æ„ŸçŸ¥å™¨å˜ä½“ï¼Œæ€§èƒ½æé«˜äº†çº¦2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc27974d65bcf13fd39e9f7ec3bc7153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce3d066e5f7e349d214ebede2ef46ae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b14a0b8a01764e72c42a7b63bf60f94.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Feasibility-Study-of-a-Diffusion-Based-Model-for-Cross-Modal-Generation-of-Knee-MRI-from-X-ray-Integrating-Radiographic-Feature-Information"><a href="#Feasibility-Study-of-a-Diffusion-Based-Model-for-Cross-Modal-Generation-of-Knee-MRI-from-X-ray-Integrating-Radiographic-Feature-Information" class="headerlink" title="Feasibility Study of a Diffusion-Based Model for Cross-Modal Generation   of Knee MRI from X-ray: Integrating Radiographic Feature Information"></a>Feasibility Study of a Diffusion-Based Model for Cross-Modal Generation   of Knee MRI from X-ray: Integrating Radiographic Feature Information</h2><p><strong>Authors:Zhe Wang, Yung Hsin Chen, Aladine Chetouani, Fabian Bauer, Yuhua Ru, Fang Chen, Liping Zhang, Rachid Jennane, Mohamed Jarraya</strong></p>
<p>Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, often diagnosed using X-rays due to its cost-effectiveness. While Magnetic Resonance Imaging (MRI) provides superior soft tissue visualization and serves as a valuable supplementary diagnostic tool, its high cost and limited accessibility significantly restrict its widespread use. To explore the feasibility of bridging this imaging gap, we conducted a feasibility study leveraging a diffusion-based model that uses an X-ray image as conditional input, alongside target depth and additional patient-specific feature information, to generate corresponding MRI sequences. Our findings demonstrate that the MRI volumes generated by our approach is visually closer to real MRI scans. Moreover, increasing inference steps enhances the continuity and smoothness of the synthesized MRI sequences. Through ablation studies, we further validate that integrating supplementary patient-specific information, beyond what X-rays alone can provide, enhances the accuracy and clinical relevance of the generated MRI, which underscores the potential of leveraging external patient-specific information to improve the MRI generation. This study is available at <a target="_blank" rel="noopener" href="https://zwang78.github.io/">https://zwang78.github.io/</a>. </p>
<blockquote>
<p>è†å…³èŠ‚éª¨å…³èŠ‚ç‚ï¼ˆKOAï¼‰æ˜¯ä¸€ç§å¸¸è§çš„éª¨éª¼è‚Œè‚‰ç–¾ç—…ï¼Œç”±äºå…¶æˆæœ¬æ•ˆç›Šé«˜ï¼Œé€šå¸¸é€šè¿‡Xå°„çº¿è¿›è¡Œè¯Šæ–­ã€‚è™½ç„¶ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æä¾›äº†å‡ºè‰²çš„è½¯ç»„ç»‡å¯è§†åŒ–æ•ˆæœï¼Œå¹¶ä½œä¸ºæœ‰ä»·å€¼çš„è¾…åŠ©è¯Šæ–­å·¥å…·ï¼Œä½†å…¶é«˜æ˜‚çš„æˆæœ¬å’Œæœ‰é™çš„å¯åŠæ€§æå¤§åœ°é™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†æ¢ç´¢ç¼©å°è¿™ä¸€æˆåƒå·®è·çš„å¯è¡Œæ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¯è¡Œæ€§ç ”ç©¶ï¼Œåˆ©ç”¨åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œä»¥Xå°„çº¿å›¾åƒä½œä¸ºæ¡ä»¶è¾“å…¥ï¼ŒåŒæ—¶ä»¥ç›®æ ‡æ·±åº¦å’Œé¢å¤–çš„æ‚£è€…ç‰¹å®šç‰¹å¾ä¿¡æ¯ï¼Œç”Ÿæˆç›¸åº”çš„MRIåºåˆ—ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„MRIä½“ç§¯åœ¨è§†è§‰ä¸Šæ›´æ¥è¿‘çœŸå®çš„MRIæ‰«æç»“æœã€‚è€Œä¸”ï¼Œå¢åŠ æ¨ç†æ­¥éª¤å¯ä»¥æé«˜åˆæˆMRIåºåˆ—çš„è¿ç»­æ€§å’Œå¹³æ»‘åº¦ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯äº†é™¤äº†Xå°„çº¿æ‰€æä¾›çš„ä¹‹å¤–ï¼Œæ•´åˆé¢å¤–çš„æ‚£è€…ç‰¹å®šä¿¡æ¯å¯ä»¥æé«˜ç”Ÿæˆçš„MRIçš„å‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ï¼Œè¿™çªæ˜¾äº†åˆ©ç”¨å¤–éƒ¨æ‚£è€…ç‰¹å®šä¿¡æ¯æ”¹å–„MRIç”Ÿæˆçš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://zwang78.github.io/">https://zwang78.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06997v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è†å…³èŠ‚éª¨å…³èŠ‚ç‚ï¼ˆKOAï¼‰çš„å½±åƒè¯Šæ–­é—®é¢˜ã€‚ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥Xå…‰å›¾åƒä¸ºæ¡ä»¶è¾“å…¥ï¼Œç»“åˆç›®æ ‡æ·±åº¦å’Œæ‚£è€…ç‰¹å®šç‰¹å¾ä¿¡æ¯ï¼Œç”Ÿæˆç›¸åº”çš„MRIåºåˆ—ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„MRIä½“ç§¯åœ¨è§†è§‰ä¸Šæ›´æ¥è¿‘çœŸå®MRIæ‰«æç»“æœï¼Œå¢åŠ æ¨ç†æ­¥éª¤å¯æé«˜åˆæˆMRIåºåˆ—çš„è¿ç»­æ€§å’Œå¹³æ»‘åº¦ã€‚é€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†ç»“åˆæ‚£è€…ç‰¹å®šä¿¡æ¯èƒ½æé«˜ç”Ÿæˆçš„MRIå‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è†å…³èŠ‚éª¨å…³èŠ‚ç‚ï¼ˆKOAï¼‰æ˜¯å¸¸è§çš„è‚Œè‚‰éª¨éª¼ç–¾ç—…ï¼Œé€šå¸¸ä½¿ç”¨Xå…‰è¿›è¡Œæˆæœ¬æ•ˆç›Šè¯Šæ–­ã€‚</li>
<li>æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰èƒ½æä¾›ä¼˜è´¨çš„è½¯ç»„ç»‡å¯è§†åŒ–æ•ˆæœï¼Œæ˜¯å®è´µçš„è¾…åŠ©è¯Šæ–­å·¥å…·ï¼Œä½†å…¶é«˜æ˜‚æˆæœ¬å’Œæœ‰é™çš„å¯è®¿é—®æ€§é™åˆ¶äº†å¹¿æ³›ä½¿ç”¨ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥Xå…‰å›¾åƒä¸ºæ¡ä»¶è¾“å…¥ï¼Œç”Ÿæˆç›¸åº”çš„MRIåºåˆ—ï¼Œä»¥ç¼©å°å½±åƒè¯Šæ–­çš„å·®è·ã€‚</li>
<li>ç”ŸæˆMRIä½“ç§¯åœ¨è§†è§‰ä¸Šæ›´æ¥è¿‘çœŸå®MRIæ‰«æç»“æœã€‚</li>
<li>å¢åŠ æ¨ç†æ­¥éª¤å¯æé«˜åˆæˆMRIåºåˆ—çš„è¿ç»­æ€§å’Œå¹³æ»‘åº¦ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜ç»“åˆæ‚£è€…ç‰¹å®šä¿¡æ¯èƒ½æé«˜ç”Ÿæˆçš„MRIå‡†ç¡®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20d7e25bc56e61e713cd1f0ee1ee564f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fe2b3d42a0aadd83867156d3cdc38f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e08ae58c1c2d646e784971dbc10aba3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5b3ec89669ff36c29dfd03d92066f42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5af2e6ec1d575781744aa04939956d63.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01e720ea437cda965f5e4bec593f73ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08d8bc1a65e2ec491d52346002d09f20.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Multiscale-Latent-Diffusion-Model-for-Enhanced-Feature-Extraction-from-Medical-Images"><a href="#Multiscale-Latent-Diffusion-Model-for-Enhanced-Feature-Extraction-from-Medical-Images" class="headerlink" title="Multiscale Latent Diffusion Model for Enhanced Feature Extraction from   Medical Images"></a>Multiscale Latent Diffusion Model for Enhanced Feature Extraction from   Medical Images</h2><p><strong>Authors:Rabeya Tus Sadia, Jie Zhang, Jin Chen</strong></p>
<p>Various imaging modalities are used in patient diagnosis, each offering unique advantages and valuable insights into anatomy and pathology. Computed Tomography (CT) is crucial in diagnostics, providing high-resolution images for precise internal organ visualization. CTâ€™s ability to detect subtle tissue variations is vital for diagnosing diseases like lung cancer, enabling early detection and accurate tumor assessment. However, variations in CT scanner models and acquisition protocols introduce significant variability in the extracted radiomic features, even when imaging the same patient. This variability poses considerable challenges for downstream research and clinical analysis, which depend on consistent and reliable feature extraction. Current methods for medical image feature extraction, often based on supervised learning approaches, including GAN-based models, face limitations in generalizing across different imaging environments. In response to these challenges, we propose LTDiff++, a multiscale latent diffusion model designed to enhance feature extraction in medical imaging. The model addresses variability by standardizing non-uniform distributions in the latent space, improving feature consistency. LTDiff++ utilizes a UNet++ encoder-decoder architecture coupled with a conditional Denoising Diffusion Probabilistic Model (DDPM) at the latent bottleneck to achieve robust feature extraction and standardization. Extensive empirical evaluations on both patient and phantom CT datasets demonstrate significant improvements in image standardization, with higher Concordance Correlation Coefficients (CCC) across multiple radiomic feature categories. Through these advancements, LTDiff++ represents a promising solution for overcoming the inherent variability in medical imaging data, offering improved reliability and accuracy in feature extraction processes. </p>
<blockquote>
<p>åœ¨æ‚£è€…è¯Šæ–­ä¸­ï¼Œå„ç§æˆåƒæ¨¡å¼è¢«å¹¿æ³›åº”ç”¨ï¼Œæ¯ç§æ¨¡å¼éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå¯¹äºè§£å‰–å­¦å’Œç—…ç†å­¦æœ‰ä»·å€¼çš„è§è§£ã€‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œå®ƒæä¾›é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œç”¨äºç²¾ç¡®çš„å†…éƒ¨å™¨å®˜å¯è§†åŒ–ã€‚CTæ£€æµ‹ç»†å¾®ç»„ç»‡å˜åŒ–çš„èƒ½åŠ›å¯¹äºè¯Šæ–­è‚ºç™Œç­‰ç–¾ç—…è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿå®ç°æ—©æœŸæ£€æµ‹å’Œå‡†ç¡®çš„è‚¿ç˜¤è¯„ä¼°ã€‚ç„¶è€Œï¼ŒCTæ‰«æä»ªå‹å·å’Œé‡‡é›†åè®®çš„å·®å¼‚ä¼šåœ¨æå–æ”¾å°„å­¦ç‰¹å¾æ—¶å¼•å…¥é‡å¤§å˜åŒ–ï¼Œå³ä½¿åœ¨ä¸ºåŒä¸€æ‚£è€…æˆåƒæ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å˜åŒ–ç»™ä¸‹æ¸¸ç ”ç©¶å’Œä¸´åºŠåˆ†æå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™äº›åˆ†æä¾èµ–äºä¸€è‡´å’Œå¯é çš„ç‰¹å¾æå–ã€‚å½“å‰åŸºäºåŒ»å­¦å›¾åƒç‰¹å¾æå–çš„æ–¹æ³•ï¼Œé€šå¸¸åŸºäºæœ‰ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºGANçš„æ¨¡å‹ï¼Œåœ¨æ¨å¹¿è‡³ä¸åŒæˆåƒç¯å¢ƒæ—¶é¢ä¸´å±€é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LTDiff++ï¼Œè¿™æ˜¯ä¸€ç§å¤šå°ºåº¦æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºåŒ»å­¦æˆåƒä¸­çš„ç‰¹å¾æå–ã€‚è¯¥æ¨¡å‹é€šè¿‡æ ‡å‡†åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„éå‡åŒ€åˆ†å¸ƒæ¥è§£å†³å˜å¼‚æ€§é—®é¢˜ï¼Œæé«˜ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚LTDiff++é‡‡ç”¨UNet++ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç»“åˆæ½œåœ¨ç“¶é¢ˆå¤„çš„æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œå®ç°ç¨³å¥çš„ç‰¹å¾æå–å’Œæ ‡å‡†åŒ–ã€‚åœ¨æ‚£è€…å’Œå¹»å½±CTæ•°æ®é›†ä¸Šçš„å¤§é‡å®è¯è¯„ä¼°è¡¨æ˜ï¼Œå›¾åƒæ ‡å‡†åŒ–çš„æ”¹è¿›éå¸¸æ˜¾è‘—ï¼Œå¤šä¸ªæ”¾å°„å­¦ç‰¹å¾ç±»åˆ«çš„ç¬¦åˆåº¦ç›¸å…³ç³»æ•°ï¼ˆCCCï¼‰æœ‰æ‰€æé«˜ã€‚é€šè¿‡è¿™äº›è¿›æ­¥ï¼ŒLTDiff++æ˜¯å…‹æœåŒ»å­¦æˆåƒæ•°æ®å›ºæœ‰å¯å˜æ€§çš„ä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç‰¹å¾æå–è¿‡ç¨‹æä¾›äº†æ”¹è¿›çš„å¯ä¿¡åº¦å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04000v3">PDF</a> version_3</p>
<p><strong>Summary</strong>ï¼š<br>åŒ»å­¦æˆåƒä¸­å¤šç§æ¨¡æ€çš„åº”ç”¨å¯¹äºæ‚£è€…è¯Šæ–­è‡³å…³é‡è¦ï¼Œå„æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰åœ¨è¯Šæ–­ä¸­å°¤ä¸ºé‡è¦ï¼Œèƒ½ç”Ÿæˆé«˜è§£æåº¦å›¾åƒï¼Œç²¾ç¡®å±•ç¤ºå†…éƒ¨å™¨å®˜ï¼Œå¯¹äºå¦‚è‚ºç™Œç­‰ç–¾ç—…çš„è¯Šæ–­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œä¸åŒCTæ‰«æä»ªå‹å·å’Œé‡‡é›†åè®®å¯¼è‡´çš„æ”¾å°„ç»„å­¦ç‰¹å¾æå–å·®å¼‚ï¼Œä¸ºä¸‹æ¸¸ç ”ç©¶å’Œä¸´åºŠåˆ†æå¸¦æ¥æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºLTDiff++è¿™ä¸€å¤šå°ºåº¦æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ ‡å‡†åŒ–æ½œåœ¨ç©ºé—´çš„éå‡åŒ€åˆ†å¸ƒæ¥æå‡åŒ»å­¦æˆåƒçš„ç‰¹å¾æå–ã€‚æ¨¡å‹åˆ©ç”¨UNet++ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç»“åˆæ½œåœ¨ç“¶é¢ˆå¤„çš„æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œå®ç°ç¨³å¥çš„ç‰¹å¾æå–å’Œæ ‡å‡†åŒ–ã€‚åœ¨æ‚£è€…å’Œå¹»å½±CTæ•°æ®é›†ä¸Šçš„å¤§é‡å®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œå›¾åƒæ ‡å‡†åŒ–æœ‰æ˜æ˜¾æ”¹å–„ï¼Œå„ç±»æ”¾å°„ç»„å­¦ç‰¹å¾çš„ç¬¦åˆåº¦å…³è”ç³»æ•°ï¼ˆCCCï¼‰æ›´é«˜ã€‚å› æ­¤ï¼ŒLTDiff++æ˜¯è§£å†³åŒ»å­¦æˆåƒæ•°æ®å›ºæœ‰å˜å¼‚æ€§çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå¯æå‡ç‰¹å¾æå–çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»å­¦æˆåƒä¸­å¤šç§æˆåƒæ¨¡æ€çš„åº”ç”¨ä¸ºæ‚£è€…è¯Šæ–­æä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚</li>
<li>CTæ‰«æåœ¨è¯Šæ–­ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿæ—©æœŸæ£€æµ‹å¹¶å‡†ç¡®è¯„ä¼°è‚¿ç˜¤ã€‚</li>
<li>ä¸åŒCTæ‰«æä»ªå’Œé‡‡é›†åè®®å¯¼è‡´ç‰¹å¾æå–çš„æ˜¾è‘—å˜åŒ–ï¼Œä¸ºä¸‹æ¸¸ç ”ç©¶å’Œä¸´åºŠåˆ†æå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>LTDiff++æ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡æ ‡å‡†åŒ–æ½œåœ¨ç©ºé—´çš„éå‡åŒ€åˆ†å¸ƒæ¥æå‡åŒ»å­¦æˆåƒçš„ç‰¹å¾æå–ã€‚</li>
<li>LTDiff++åˆ©ç”¨UNet++å’Œæ¡ä»¶DDPMæ¥å®ç°ç¨³å¥çš„ç‰¹å¾æå–å’Œæ ‡å‡†åŒ–ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒLTDiff++åœ¨å›¾åƒæ ‡å‡†åŒ–æ–¹é¢æœ‰æ˜æ˜¾æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42fba0ca2d0d87acb68cb50cf245107c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6927fb523b7e67b714836e5fdf28fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c956697bd9db9ba503eb4abaf7e018b6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="INSIGHTBUDDY-AI-Medication-Extraction-and-Entity-Linking-using-Large-Language-Models-and-Ensemble-Learning"><a href="#INSIGHTBUDDY-AI-Medication-Extraction-and-Entity-Linking-using-Large-Language-Models-and-Ensemble-Learning" class="headerlink" title="INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large   Language Models and Ensemble Learning"></a>INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large   Language Models and Ensemble Learning</h2><p><strong>Authors:Pablo Romero, Lifeng Han, Goran Nenadic</strong></p>
<p>Medication Extraction and Mining play an important role in healthcare NLP research due to its practical applications in hospital settings, such as their mapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.). In this work, we investigate state-of-the-art LLMs in text mining tasks on medications and their related attributes such as dosage, route, strength, and adverse effects. In addition, we explore different ensemble learning methods (\textsc{Stack-Ensemble} and \textsc{Voting-Ensemble}) to augment the model performances from individual LLMs. Our ensemble learning result demonstrated better performances than individually fine-tuned base models BERT, RoBERTa, RoBERTa-L, BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and PubMedBERT across general and specific domains. Finally, we build up an entity linking function to map extracted medical terminologies into the SNOMED-CT codes and the British National Formulary (BNF) codes, which are further mapped to the Dictionary of Medicines and Devices (dm+d), and ICD. Our modelâ€™s toolkit and desktop applications are publicly available (at \url{<a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/ensemble-NER%7D">https://github.com/HECTA-UoM/ensemble-NER}</a>). </p>
<blockquote>
<p>è¯ç‰©æå–å’ŒæŒ–æ˜åœ¨åŒ»ç–—ä¿å¥NLPç ”ç©¶ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œå› ä¸ºå®ƒåœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ï¼Œä¾‹å¦‚å°†å…¶æ˜ å°„åˆ°æ ‡å‡†ä¸´åºŠçŸ¥è¯†åº“ï¼ˆå¦‚SNOMED-CTã€BNFç­‰ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æœ€å…ˆè¿›çš„æ–‡æœ¬æŒ–æ˜ä»»åŠ¡ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé’ˆå¯¹è¯ç‰©åŠå…¶ç›¸å…³å±æ€§ï¼ˆå¦‚å‰‚é‡ã€ç»™è¯é€”å¾„ã€å¼ºåº¦å’Œä¸è‰¯ååº”ï¼‰è¿›è¡Œç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸åŒçš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼ˆStack-Ensembleå’ŒVoting-Ensembleï¼‰ï¼Œä»¥æé«˜å•ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„é›†æˆå­¦ä¹ ç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸€èˆ¬å’Œç‰¹å®šé¢†åŸŸä¸Šï¼Œå…¶æ€§èƒ½å‡ä¼˜äºç»è¿‡å•ç‹¬å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚BERTã€RoBERTaã€RoBERTa-Lã€BioBERTã€BioClinicalBERTã€BioMedRoBERTaã€ClinicalBERTå’ŒPubMedBERTã€‚æœ€åï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå®ä½“é“¾æ¥åŠŸèƒ½ï¼Œå°†æå–çš„åŒ»ç–—æœ¯è¯­æ˜ å°„åˆ°SNOMED-CTä»£ç å’Œè‹±å›½å›½å®¶é…æ–¹é›†ï¼ˆBNFï¼‰ä»£ç ï¼Œå¹¶è¿›ä¸€æ­¥æ˜ å°„åˆ°è¯å“å’Œè®¾å¤‡è¯å…¸ï¼ˆdm+dï¼‰å’ŒICDã€‚æˆ‘ä»¬çš„æ¨¡å‹å·¥å…·åŒ…å’Œæ¡Œé¢åº”ç”¨ç¨‹åºå·²å…¬å¼€å¯ç”¨ï¼ˆç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/HECTA-UoM/ensemble-NER%EF%BC%89%E3%80%82">https://github.com/HECTA-UoM/ensemble-NERï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19467v2">PDF</a> ongoing work, 24 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŒ»è¯æå–å’ŒæŒ–æ˜åœ¨åŒ»ç–—è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬ä½¿ç”¨æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯ç‰©æ–‡æœ¬æŒ–æ˜ä»»åŠ¡ï¼Œæ¢ç´¢ä¸åŒçš„é›†æˆå­¦ä¹ æ–¹æ³•æ¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶æ„å»ºä¸€ä¸ªå®ä½“é“¾æ¥åŠŸèƒ½ï¼Œå°†æå–çš„åŒ»ç–—æœ¯è¯­æ˜ å°„åˆ°SNOMED-CTä»£ç å’Œè‹±å›½å›½å®¶å¤„æ–¹é›†ï¼ˆBNFï¼‰ä»£ç ç­‰æ ‡å‡†ä¸´åºŠçŸ¥è¯†åº“ä¸­ã€‚æ¨¡å‹çš„å·¥å…·åŒ…å’Œæ¡Œé¢åº”ç”¨ç¨‹åºå·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»è¯æå–å’ŒæŒ–æ˜åœ¨åŒ»ç–—è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ€§ï¼Œå°¤å…¶åœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚</li>
<li>ä½¿ç”¨æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè¯ç‰©æ–‡æœ¬æŒ–æ˜ä»»åŠ¡ã€‚</li>
<li>æ¢ç´¢äº†ä¸åŒçš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚Stack-Ensembleå’ŒVoting-Ensembleï¼‰æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é›†æˆå­¦ä¹ æ¨¡å‹åœ¨ä¸€èˆ¬å’Œç‰¹å®šé¢†åŸŸä¸Šçš„è¡¨ç°å‡ä¼˜äºå•ä¸ªç²¾ç»†è°ƒæ•´çš„åŸºå‡†æ¨¡å‹ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå®ä½“é“¾æ¥åŠŸèƒ½ï¼Œå°†æå–çš„åŒ»ç–—æœ¯è¯­æ˜ å°„åˆ°SNOMED-CTä»£ç ç­‰æ ‡å‡†ä¸´åºŠçŸ¥è¯†åº“ä¸­ã€‚</li>
<li>æ¨¡å‹å…¬å¼€å¯ç”¨ï¼Œå¹¶æä¾›äº†æ¡Œé¢åº”ç”¨ç¨‹åºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6307dfbdc53430c3eedf1d1fb16ca1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a2eca9d72f34f7dca5e76166da52e7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec3bbe7a5031da3ac94755d0a3405ce5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Exploring-Fine-Grained-Image-Text-Alignment-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Exploring-Fine-Grained-Image-Text-Alignment-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Exploring Fine-Grained Image-Text Alignment for Referring Remote Sensing   Image Segmentation"></a>Exploring Fine-Grained Image-Text Alignment for Referring Remote Sensing   Image Segmentation</h2><p><strong>Authors:Sen Lei, Xinyu Xiao, Tianlin Zhang, Heng-Chao Li, Zhenwei Shi, Qing Zhu</strong></p>
<p>Given a language expression, referring remote sensing image segmentation (RRSIS) aims to identify ground objects and assign pixel-wise labels within the imagery. The one of key challenges for this task is to capture discriminative multi-modal features via text-image alignment. However, the existing RRSIS methods use one vanilla and coarse alignment, where the language expression is directly extracted to be fused with the visual features. In this paper, we argue that a &#96;&#96;fine-grained image-text alignmentâ€™â€™ can improve the extraction of multi-modal information. To this point, we propose a new referring remote sensing image segmentation method to fully exploit the visual and linguistic representations. Specifically, the original referring expression is regarded as context text, which is further decoupled into the ground object and spatial position texts. The proposed fine-grained image-text alignment module (FIAM) would simultaneously leverage the features of the input image and the corresponding texts, obtaining better discriminative multi-modal representation. Meanwhile, to handle the various scales of ground objects in remote sensing, we introduce a Text-aware Multi-scale Enhancement Module (TMEM) to adaptively perform cross-scale fusion and intersections. We evaluate the effectiveness of the proposed method on two public referring remote sensing datasets including RefSegRS and RRSIS-D, and our method obtains superior performance over several state-of-the-art methods. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Shaosifan/FIANet">https://github.com/Shaosifan/FIANet</a>. </p>
<blockquote>
<p>å¯¹äºç»™å®šçš„è¯­è¨€è¡¨è¾¾å¼ï¼Œé¥æ„Ÿå›¾åƒåˆ†å‰²å¼•ç”¨ï¼ˆRRSISï¼‰æ—¨åœ¨è¯†åˆ«åœ°é¢ç›®æ ‡å¹¶ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…æ ‡ç­¾ã€‚æ­¤ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯é€šè¿‡æ–‡æœ¬å›¾åƒå¯¹é½æ¥æ•è·åˆ¤åˆ«å¼å¤šæ¨¡æ€ç‰¹å¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RRSISæ–¹æ³•ä½¿ç”¨ä¸€ç§æ™®é€šçš„ç²—å¯¹é½æ–¹å¼ï¼Œå…¶ä¸­ç›´æ¥æå–è¯­è¨€è¡¨è¾¾å¼ä¸è§†è§‰ç‰¹å¾è¿›è¡Œèåˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºâ€œç²¾ç»†çš„å›¾åƒæ–‡æœ¬å¯¹é½â€å¯ä»¥æ”¹å–„å¤šæ¨¡æ€ä¿¡æ¯çš„æå–ã€‚é’ˆå¯¹è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é¥æ„Ÿå›¾åƒåˆ†å‰²å¼•ç”¨æ–¹æ³•ï¼Œä»¥å……åˆ†åˆ©ç”¨è§†è§‰å’Œè¯­è¨€å­¦è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒåŸå§‹å¼•ç”¨è¡¨è¾¾å¼è¢«è§†ä¸ºä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œå¹¶è¿›ä¸€æ­¥è§£è€¦ä¸ºåœ°é¢ç›®æ ‡å’Œç©ºé—´ä½ç½®æ–‡æœ¬ã€‚æ‰€æå‡ºçš„ç²¾ç»†å›¾åƒæ–‡æœ¬å¯¹é½æ¨¡å—ï¼ˆFIAMï¼‰å°†åŒæ—¶åˆ©ç”¨è¾“å…¥å›¾åƒå’Œç›¸åº”æ–‡æœ¬çš„ç‰¹å¾ï¼Œä»¥è·å¾—æ›´å¥½çš„åˆ¤åˆ«å¼å¤šæ¨¡æ€è¡¨ç¤ºã€‚åŒæ—¶ï¼Œä¸ºäº†å¤„ç†é¥æ„Ÿä¸­åœ°é¢ç›®æ ‡çš„å„ç§å°ºåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–‡æœ¬æ„ŸçŸ¥å¤šå°ºåº¦å¢å¼ºæ¨¡å—ï¼ˆTMEMï¼‰æ¥æ‰§è¡Œè‡ªé€‚åº”è·¨å°ºåº¦èåˆå’Œäº¤å‰ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€çš„é¥æ„Ÿå¼•ç”¨æ•°æ®é›†RefSegRSå’ŒRRSIS-Dä¸Šè¯„ä¼°äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸å‡ ç§æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è·å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Shaosifan/FIANet%E3%80%82">https://github.com/Shaosifan/FIANetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13637v2">PDF</a> Accepted by IEEE TGRS</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„æŒ‡ä»£åˆ†å‰²ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨è¾ƒä¸ºç²—ç³™çš„æ–‡æœ¬-å›¾åƒå¯¹é½æ–¹å¼ï¼Œéš¾ä»¥æœ‰æ•ˆæå–å¤šæ¨¡æ€ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç²¾ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡åŒæ—¶åˆ©ç”¨å›¾åƒå’Œå¯¹åº”æ–‡æœ¬çš„ç‰¹å¾ï¼Œè·å¾—æ›´å¥½çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†é¥æ„Ÿä¸­åœ°é¢ç›®æ ‡çš„å„ç§å°ºåº¦ï¼Œå¼•å…¥äº†æ–‡æœ¬æ„ŸçŸ¥å¤šå°ºåº¦å¢å¼ºæ¨¡å—ï¼Œå®ç°è·¨å°ºåº¦èåˆå’Œäº¤é›†ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„æŒ‡ä»£åˆ†å‰²ä»»åŠ¡æ—¨åœ¨é€šè¿‡è¯­è¨€è¡¨è¾¾å¼è¯†åˆ«åœ°é¢ç›®æ ‡å¹¶ä¸ºå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ åˆ†é…æ ‡ç­¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨è¾ƒç²—çš„æ–‡æœ¬-å›¾åƒå¯¹é½æ–¹å¼ï¼Œéš¾ä»¥æå–å¤šæ¨¡æ€ç‰¹å¾ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç²¾ç»†ç²’åº¦å›¾åƒ-æ–‡æœ¬å¯¹é½æ–¹æ³•ï¼Œæ”¹è¿›äº†å¤šæ¨¡æ€ä¿¡æ¯çš„æå–ã€‚</li>
<li>å¼•å…¥äº†æ–‡æœ¬æ„ŸçŸ¥å¤šå°ºåº¦å¢å¼ºæ¨¡å—ï¼Œå¤„ç†é¥æ„Ÿä¸­åœ°é¢ç›®æ ‡çš„å„ç§å°ºåº¦ã€‚</li>
<li>æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>æå‡ºçš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-234bda8300addacb23950adae6bede37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e643455dd8fc7830030733c934569934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9012bfcbb32d791fc2d86134724d5320.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-807d86b803196ec2cbfd8491744a3012.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c9fa899998bbc37c8ab9b5ed7c83068.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TAVP-Task-Adaptive-Visual-Prompt-for-Cross-domain-Few-shot-Segmentation"><a href="#TAVP-Task-Adaptive-Visual-Prompt-for-Cross-domain-Few-shot-Segmentation" class="headerlink" title="TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation"></a>TAVP: Task-Adaptive Visual Prompt for Cross-domain Few-shot Segmentation</h2><p><strong>Authors:Jiaqi Yang, Yaning Zhang, Jingxi Hu, Xiangjian He, Linlin Shen, Guoping Qiu</strong></p>
<p>While large visual models (LVM) demonstrated significant potential in image understanding, due to the application of large-scale pre-training, the Segment Anything Model (SAM) has also achieved great success in the field of image segmentation, supporting flexible interactive cues and strong learning capabilities. However, SAMâ€™s performance often falls short in cross-domain and few-shot applications. Previous work has performed poorly in transferring prior knowledge from base models to new applications. To tackle this issue, we propose a task-adaptive auto-visual prompt framework, a new paradigm for Cross-dominan Few-shot segmentation (CD-FSS). First, a Multi-level Feature Fusion (MFF) was used for integrated feature extraction as prior knowledge. Besides, we incorporate a Class Domain Task-Adaptive Auto-Prompt (CDTAP) module to enable class-domain agnostic feature extraction and generate high-quality, learnable visual prompts. This significant advancement uses a unique generative approach to prompts alongside a comprehensive model structure and specialized prototype computation. While ensuring that the prior knowledge of SAM is not discarded, the new branch disentangles category and domain information through prototypes, guiding it in adapting the CD-FSS. Comprehensive experiments across four cross-domain datasets demonstrate that our model outperforms the state-of-the-art CD-FSS approach, achieving an average accuracy improvement of 1.3% in the 1-shot setting and 11.76% in the 5-shot setting. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œè¿™æ˜¯ç”±äºå¤§è§„æ¨¡é¢„è®­ç»ƒçš„åº”ç”¨ï¼Œä½†Segment Anything Modelï¼ˆSAMï¼‰ä¹Ÿåœ¨å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œæ”¯æŒçµæ´»çš„äº¤äº’å¼çº¿ç´¢å’Œå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSAMåœ¨è·¨åŸŸå’Œå°‘é‡åº”ç”¨ä¸­çš„è¡¨ç°å¾€å¾€ä¸å°½å¦‚äººæ„ã€‚å…ˆå‰çš„å·¥ä½œåœ¨å°†åŸºç¡€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†è½¬ç§»åˆ°æ–°åº”ç”¨ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨è§†è§‰æç¤ºæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„Cross-dominan Few-shot segmentationï¼ˆCD-FSSï¼‰èŒƒå¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šå±‚æ¬¡ç‰¹å¾èåˆï¼ˆMFFï¼‰è¿›è¡Œé›†æˆç‰¹å¾æå–ä½œä¸ºå…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†Class Domain Task-Adaptive Auto-Promptï¼ˆCDTAPï¼‰æ¨¡å—ï¼Œä»¥å®ç°ç±»åŸŸæ— å…³çš„ç‰¹å¾æå–ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡ã€å¯å­¦ä¹ çš„è§†è§‰æç¤ºã€‚è¿™ä¸€é‡å¤§è¿›å±•é‡‡ç”¨äº†ä¸€ç§ç‹¬ç‰¹çš„ç”Ÿæˆæç¤ºæ–¹æ³•ï¼Œè¾…ä»¥å…¨é¢çš„æ¨¡å‹ç»“æ„å’Œä¸“é—¨çš„åŸå‹è®¡ç®—ã€‚åœ¨ç¡®ä¿SAMçš„å…ˆéªŒçŸ¥è¯†ä¸è¢«ä¸¢å¼ƒçš„åŒæ—¶ï¼Œæ–°åˆ†æ”¯é€šè¿‡åŸå‹è§£å¼€ç±»åˆ«å’ŒåŸŸä¿¡æ¯ï¼Œä»è€Œå¼•å¯¼å…¶é€‚åº”CD-FSSã€‚åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„CD-FSSæ–¹æ³•ï¼Œåœ¨1æ¬¡æ‹æ‘„çš„æƒ…å†µä¸‹å¹³å‡å‡†ç¡®ç‡æé«˜äº†1.3%ï¼Œåœ¨5æ¬¡æ‹æ‘„çš„æƒ…å†µä¸‹å¹³å‡å‡†ç¡®ç‡æé«˜äº†11.76%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05393v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰æ¨¡å‹åœ¨å›¾åƒç†è§£æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼ŒSegment Anything Model (SAM) åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸä¹Ÿå–å¾—äº†å¾ˆå¤§æˆåŠŸï¼Œå…·æœ‰çµæ´»çš„äº¤äº’å¼æç¤ºå’Œå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚ä½†åœ¨è·¨åŸŸå’Œå°‘é‡æ ·æœ¬åº”ç”¨ä¸­ï¼ŒSAMçš„è¡¨ç°å¾€å¾€ä¸å°½å¦‚äººæ„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨è§†è§‰æç¤ºæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è·¨åŸŸå°‘é‡åˆ†å‰²ï¼ˆCD-FSSï¼‰èŒƒå¼ã€‚æˆ‘ä»¬é€šè¿‡å¤šçº§åˆ«ç‰¹å¾èåˆï¼ˆMFFï¼‰è¿›è¡Œå…ˆéªŒçŸ¥è¯†èåˆï¼Œå¹¶ç»“åˆç±»åŸŸä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨æç¤ºï¼ˆCDTAPï¼‰æ¨¡å—ï¼Œå®ç°ç±»åŸŸæ— å…³çš„ç‰¹å¾æå–ï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¯å­¦ä¹ çš„è§†è§‰æç¤ºã€‚è¯¥æ¨¡å‹ç»“æ„å…¨é¢ï¼Œä½¿ç”¨ç‹¬ç‰¹çš„ç”Ÿæˆå¼æç¤ºæ–¹æ³•ï¼Œå¹¶é€šè¿‡åŸå‹è®¡ç®—è¿›è¡Œå¼•å¯¼ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°çš„CD-FSSæ–¹æ³•ï¼Œåœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹å¹³å‡å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†1.3%å’Œ11.76%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰åœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>Segment Anything Model (SAM) åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—å¾ˆå¤§æˆåŠŸï¼Œä½†å­˜åœ¨è·¨åŸŸå’Œå°‘é‡æ ·æœ¬åº”ç”¨ä¸­çš„æ€§èƒ½çŸ­æ¿ã€‚</li>
<li>æå‡ºäº†ä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨è§†è§‰æç¤ºæ¡†æ¶ï¼Œä¸€ç§é’ˆå¯¹è·¨åŸŸå°‘é‡åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„æ–°èŒƒå¼ã€‚</li>
<li>ä½¿ç”¨å¤šçº§åˆ«ç‰¹å¾èåˆï¼ˆMFFï¼‰è¿›è¡Œå…ˆéªŒçŸ¥è¯†èåˆã€‚</li>
<li>ç»“åˆç±»åŸŸä»»åŠ¡è‡ªé€‚åº”è‡ªåŠ¨æç¤ºï¼ˆCDTAPï¼‰æ¨¡å—ï¼Œå®ç°ç±»åŸŸæ— å…³çš„ç‰¹å¾æå–å’Œé«˜è´¨é‡è§†è§‰æç¤ºç”Ÿæˆã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç‹¬ç‰¹çš„ç”Ÿæˆå¼æç¤ºæ–¹æ³•å’Œå…¨é¢çš„æ¨¡å‹ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4541d57fc7cef23263f894fdb4ab19c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b5094632539bd64ce887a7ddf2dfae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1a58eb1d51be19d717063965e48a6d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c92f6aeb75f67bae2eec0ead5ca1128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b07a91a16c6948fedbdf391cc9cd89fa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Gradient-Alignment-Improves-Test-Time-Adaptation-for-Medical-Image-Segmentation"><a href="#Gradient-Alignment-Improves-Test-Time-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="Gradient Alignment Improves Test-Time Adaptation for Medical Image   Segmentation"></a>Gradient Alignment Improves Test-Time Adaptation for Medical Image   Segmentation</h2><p><strong>Authors:Ziyang Chen, Yiwen Ye, Yongsheng Pan, Yong Xia</strong></p>
<p>Although recent years have witnessed significant advancements in medical image segmentation, the pervasive issue of domain shift among medical images from diverse centres hinders the effective deployment of pre-trained models. Many Test-time Adaptation (TTA) methods have been proposed to address this issue by fine-tuning pre-trained models with test data during inference. These methods, however, often suffer from less-satisfactory optimization due to suboptimal optimization direction (dictated by the gradient) and fixed step-size (predicated on the learning rate). In this paper, we propose the Gradient alignment-based Test-time adaptation (GraTa) method to improve both the gradient direction and learning rate in the optimization procedure. Unlike conventional TTA methods, which primarily optimize the pseudo gradient derived from a self-supervised objective, our method incorporates an auxiliary gradient with the pseudo one to facilitate gradient alignment. Such gradient alignment enables the model to excavate the similarities between different gradients and correct the gradient direction to approximate the empirical gradient related to the current segmentation task. Additionally, we design a dynamic learning rate based on the cosine similarity between the pseudo and auxiliary gradients, thereby empowering the adaptive fine-tuning of pre-trained models on diverse test data. Extensive experiments establish the effectiveness of the proposed gradient alignment and dynamic learning rate and substantiate the superiority of our GraTa method over other state-of-the-art TTA methods on a benchmark medical image segmentation task. The code and weights of pre-trained source models are available at <a target="_blank" rel="noopener" href="https://github.com/Chen-Ziyang/GraTa">https://github.com/Chen-Ziyang/GraTa</a>. </p>
<blockquote>
<p>å°½ç®¡è¿‘å¹´æ¥åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ¥è‡ªä¸åŒä¸­å¿ƒçš„åŒ»å­¦å›¾åƒå­˜åœ¨çš„é¢†åŸŸåç§»é—®é¢˜ä»ç„¶é˜»ç¢ç€é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆéƒ¨ç½²ã€‚è®¸å¤šæµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•å·²è¢«æå‡ºï¼Œé€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„æµ‹è¯•æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç”±äºä¼˜åŒ–æ–¹å‘ä¸ä½³ï¼ˆç”±æ¢¯åº¦å†³å®šï¼‰å’Œå›ºå®šæ­¥é•¿ï¼ˆä»¥å­¦ä¹ ç‡é¢„æµ‹ï¼‰è€Œå¯¼è‡´ä¼˜åŒ–æ•ˆæœä¸å°½äººæ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¢¯åº¦å¯¹é½çš„æµ‹è¯•æ—¶é€‚åº”ï¼ˆGraTaï¼‰æ–¹æ³•ï¼Œä»¥æ”¹è¿›ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ–¹å‘å’Œå­¦ä¹ ç‡ã€‚ä¸åŒäºä¸»è¦ä¼˜åŒ–è‡ªç›‘ç£ç›®æ ‡æ´¾ç”Ÿå‡ºçš„ä¼ªæ¢¯åº¦çš„ä¼ ç»ŸTTAæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¾…åŠ©æ¢¯åº¦å’Œä¼ªæ¢¯åº¦ï¼Œä»¥å®ç°æ¢¯åº¦å¯¹é½ã€‚è¿™ç§æ¢¯åº¦å¯¹é½ä½¿æ¨¡å‹èƒ½å¤ŸæŒ–æ˜ä¸åŒæ¢¯åº¦ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶çº æ­£æ¢¯åº¦æ–¹å‘ä»¥è¿‘ä¼¼å½“å‰åˆ†å‰²ä»»åŠ¡çš„å®è¯æ¢¯åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§è®¾è®¡äº†ä¸€ç§åŠ¨æ€å­¦ä¹ ç‡ï¼Œä»è€Œå®ç°åœ¨å¤šæ ·æµ‹è¯•æ•°æ®ä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹çš„è‡ªé€‚åº”å¾®è°ƒã€‚å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„æ¢¯åº¦å¯¹é½å’ŒåŠ¨æ€å­¦ä¹ ç‡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯å®æˆ‘ä»¬çš„GraTaæ–¹æ³•åœ¨åŸºå‡†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„TTAæ–¹æ³•ã€‚é¢„è®­ç»ƒæ¨¡å‹çš„ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chen-Ziyang/GraTa%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Chen-Ziyang/GraTaä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07343v4">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¢¯åº¦å¯¹é½çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆGraTaï¼‰æ–¹æ³•ï¼Œæ”¹è¿›äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ–¹å‘å’Œå­¦ä¹ ç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦å®ç°æ¢¯åº¦å¯¹é½ï¼Œèƒ½æŒ–æ˜ä¸åŒæ¢¯åº¦é—´çš„ç›¸ä¼¼æ€§å¹¶æ ¡æ­£æ¢¯åº¦æ–¹å‘ä»¥é€¼è¿‘å½“å‰åˆ†å‰²ä»»åŠ¡çš„å®è¯æ¢¯åº¦ã€‚åŒæ—¶ï¼ŒåŸºäºä½™å¼¦ç›¸ä¼¼åº¦è®¾è®¡åŠ¨æ€å­¦ä¹ ç‡ï¼Œä½¿é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šæ ·æµ‹è¯•æ•°æ®ä¸Šå®ç°è‡ªé€‚åº”å¾®è°ƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨åŸºå‡†åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­ï¼Œä¸åŒæ¥æºçš„å›¾åƒå­˜åœ¨çš„é¢†åŸŸåç§»é—®é¢˜å½±å“äº†é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆéƒ¨ç½²ã€‚</li>
<li>æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•é€šè¿‡æµ‹è¯•æ•°æ®åœ¨æ¨ç†æ—¶å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ç°æœ‰TTAæ–¹æ³•ç”±äºæ¢¯åº¦æ–¹å‘å’Œå­¦ä¹ ç‡çš„ä¸è¶³ï¼Œä¼˜åŒ–æ•ˆæœå¸¸ä¸ç†æƒ³ã€‚</li>
<li>GraTaæ–¹æ³•ç»“åˆä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦å®ç°æ¢¯åº¦å¯¹é½ï¼Œä¼˜åŒ–æ¢¯åº¦æ–¹å‘ã€‚</li>
<li>åŠ¨æ€å­¦ä¹ ç‡è®¾è®¡åŸºäºä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå®ç°è‡ªé€‚åº”å¾®è°ƒã€‚</li>
<li>GraTaæ–¹æ³•åœ¨åŸºå‡†åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºå…¶ä»–å…ˆè¿›TTAæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f71537c9572b5d34d7211854776d75c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c709fb976679c941a37c07b17deb29e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9250577a125ca45e8b05f56b81e9777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dffd18fc7d80600e9752037706ba2e4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Fast-TILs-â€“-A-Pipeline-for-Efficient-TILs-Estimation-in-Non-Small-Cell-Lung-Cancer"><a href="#Fast-TILs-â€“-A-Pipeline-for-Efficient-TILs-Estimation-in-Non-Small-Cell-Lung-Cancer" class="headerlink" title="Fast TILs â€“ A Pipeline for Efficient TILs Estimation in Non-Small Cell   Lung Cancer"></a>Fast TILs â€“ A Pipeline for Efficient TILs Estimation in Non-Small Cell   Lung Cancer</h2><p><strong>Authors:Nikita Shvetsov, Anders Sildnes, Masoud Tafavvoghi, Lill-Tove Rasmussen Busund, Stig Dalen, Kajsa MÃ¸llersen, Lars Ailo Bongo, Thomas K. Kilvaer</strong></p>
<p>Addressing the critical need for accurate prognostic biomarkers in cancer treatment, quantifying tumor-infiltrating lymphocytes (TILs) in non-small cell lung cancer (NSCLC) presents considerable challenges. Manual TIL quantification in whole slide images (WSIs) is laborious and subject to variability, potentially undermining patient outcomes. Our study introduces an automated pipeline that utilizes semi-stochastic patch sampling, patch classification to retain prognostically relevant patches, and cell quantification using the HoVer-Net model to streamline the TIL evaluation process. This pipeline efficiently excludes approximately 70% of areas not relevant for prognosis and requires only 5% of the remaining patches to maintain prognostic accuracy (c-index &#x3D; 0.65). The computational efficiency achieved does not sacrifice prognostic accuracy, as demonstrated by the TILs scoreâ€™s strong association with patient survival, which outperforms traditional CD8 IHC scoring methods. While the pipeline demonstrates potential for enhancing NSCLC prognostication and personalization of treatment, comprehensive clinical validation is still required. Future research should focus on verifying its broader clinical utility and investigating additional biomarkers to improve NSCLC prognosis. </p>
<blockquote>
<p>é’ˆå¯¹ç™Œç—‡æ²»ç–—ä¸­å‡†ç¡®é¢„åç”Ÿç‰©æ ‡å¿—ç‰©çš„è¿«åˆ‡éœ€æ±‚ï¼Œéå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ä¸­è‚¿ç˜¤æµ¸æ¶¦æ·‹å·´ç»†èƒï¼ˆTILsï¼‰çš„é‡åŒ–é¢ä¸´ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚åœ¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­è¿›è¡Œæ‰‹åŠ¨TILé‡åŒ–æ—¢è´¹æ—¶åˆå­˜åœ¨å¯å˜å› ç´ ï¼Œå¯èƒ½ä¼šå¯¹æ‚£è€…é¢„åç»“æœäº§ç”Ÿæ½œåœ¨å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–æµç¨‹ï¼Œè¯¥æµç¨‹é‡‡ç”¨åŠéšæœºè¡¥ä¸é‡‡æ ·ï¼Œé€šè¿‡è¡¥ä¸åˆ†ç±»ä¿ç•™ä¸é¢„åç›¸å…³çš„è¡¥ä¸ï¼Œå¹¶ä½¿ç”¨HoVer-Netæ¨¡å‹è¿›è¡Œç»†èƒé‡åŒ–ï¼Œä»¥ç®€åŒ–TILè¯„ä¼°è¿‡ç¨‹ã€‚è¯¥æµç¨‹æœ‰æ•ˆåœ°æ’é™¤äº†å¤§çº¦70%ä¸é¢„åæ— å…³çš„åŒºåŸŸï¼Œå¹¶ä»…ä¿ç•™çº¦å‰©ä½™åŒºåŸŸçš„5%ï¼Œä»¥ä¿æŒé¢„åå‡†ç¡®æ€§ï¼ˆcæŒ‡æ•°&#x3D; 0.65ï¼‰ã€‚æ‰€å®ç°çš„è®¡ç®—æ•ˆç‡å¹¶æ²¡æœ‰ç‰ºç‰²é¢„åå‡†ç¡®æ€§ï¼Œå¦‚TILè¯„åˆ†ä¸æ‚£è€…ç”Ÿå­˜çš„å¼ºçƒˆç›¸å…³æ€§æ‰€ç¤ºï¼Œå…¶ä¼˜äºä¼ ç»Ÿçš„CD8å…ç–«ç»„ç»‡åŒ–å­¦è¯„åˆ†æ–¹æ³•ã€‚è™½ç„¶è¯¥æµç¨‹åœ¨å¢å¼ºNSCLCé¢„åå’Œæ²»ç–—ä¸ªæ€§åŒ–æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»éœ€è¦è¿›è¡Œå…¨é¢çš„ä¸´åºŠéªŒè¯ã€‚æœªæ¥çš„ç ”ç©¶åº”ä¾§é‡äºéªŒè¯å…¶åœ¨ä¸´åºŠä¸Šçš„æ›´å¹¿æ³›åº”ç”¨ï¼Œå¹¶ç ”ç©¶å…¶ä»–ç”Ÿç‰©æ ‡å¿—ç‰©ä»¥æé«˜NSCLCçš„é¢„åæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.02913v2">PDF</a> 25 pages, 10 figures, 7 appendix pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ä¸­è‚¿ç˜¤æµ¸æ¶¦æ·‹å·´ç»†èƒï¼ˆTILsï¼‰çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ã€‚è¯¥æµç¨‹é‡‡ç”¨åŠéšæœºè¡¥ä¸é‡‡æ ·ã€è¡¥ä¸åˆ†ç±»ä¿ç•™é¢„åç›¸å…³è¡¥ä¸ä»¥åŠä½¿ç”¨HoVer-Netæ¨¡å‹è¿›è¡Œç»†èƒè®¡æ•°ï¼Œæé«˜äº†TILè¯„ä¼°çš„æ•ˆç‡ï¼Œåœ¨ä¿è¯é¢„åå‡†ç¡®æ€§çš„åŒæ—¶å¤§å¤§å‡å°‘äº†è¯„ä¼°çš„å·¥ä½œé‡ã€‚æ­¤æµç¨‹å±•ç¤ºçš„TILsè¯„åˆ†ä¸æ‚£è€…ç”Ÿå­˜æœ‰å¼ºå…³è”ï¼Œå¹¶ä¼˜äºä¼ ç»Ÿçš„CD8å…ç–«ç»„ç»‡åŒ–å­¦è¯„åˆ†æ–¹æ³•ã€‚å°½ç®¡è¯¥æµç¨‹åœ¨éå°ç»†èƒè‚ºç™Œé¢„åå’Œä¸ªæ€§åŒ–æ²»ç–—æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»éœ€å…¨é¢çš„ä¸´åºŠéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¿ç˜¤æµ¸æ¶¦æ·‹å·´ç»†èƒï¼ˆTILsï¼‰åœ¨éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ä¸­çš„é‡åŒ–æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæ‰‹åŠ¨é‡åŒ–åŠ³åŠ¨å¼ºåº¦å¤§ä¸”ç»“æœä¸ç¨³å®šã€‚</li>
<li>å¼•å…¥çš„è‡ªåŠ¨åŒ–æµç¨‹åˆ©ç”¨åŠéšæœºè¡¥ä¸é‡‡æ ·å’ŒHoVer-Netæ¨¡å‹æé«˜äº†æ•ˆç‡ï¼Œä»…éœ€è¦å¾ˆå°‘ä¸€éƒ¨åˆ†åŒºåŸŸå°±èƒ½ç»´æŒå‡†ç¡®çš„é¢„åè¯„ä¼°ã€‚</li>
<li>è¯¥æµç¨‹åœ¨ä¿è¯è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå¹¶æœªç‰ºç‰²é¢„åå‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ‚£è€…ç”Ÿå­˜å…³è”ã€‚</li>
<li>ä¸ä¼ ç»ŸCD8 IHCè¯„åˆ†æ–¹æ³•ç›¸æ¯”ï¼Œæ­¤æµç¨‹å±•ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ­¤æµç¨‹åœ¨NSCLCçš„é¢„åå’Œä¸ªæ€§åŒ–æ²»ç–—æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»éœ€å…¨é¢çš„ä¸´åºŠéªŒè¯ã€‚</li>
<li>æœªæ¥ç ”ç©¶åº”ä¾§é‡äºéªŒè¯è¯¥æµç¨‹åœ¨ä¸´åºŠä¸Šçš„å¹¿æ³›åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.02913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c9abe7074ee6c8a9c2f3df9dd9f5596.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af66a8e627e0f5bf7c11ebb422b46e13.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Nanoscale-cuticle-mass-density-variations-influenced-by-pigmentation-in-butterfly-wing-scales"><a href="#Nanoscale-cuticle-mass-density-variations-influenced-by-pigmentation-in-butterfly-wing-scales" class="headerlink" title="Nanoscale cuticle mass density variations influenced by pigmentation in   butterfly wing scales"></a>Nanoscale cuticle mass density variations influenced by pigmentation in   butterfly wing scales</h2><p><strong>Authors:Deepan Balakrishnan, Anupama Prakash, Benedikt J. Daurer, CÃ©dric Finet, Ying Chen Lim, Zhou Shen, Pierre Thibault, AntÃ³nia Monteiro, N. Duane Loh</strong></p>
<p>How pigment distribution influences the cuticle density within a microscopic butterfly wing scale, and how both impact final reflected color remains unknown. We used ptychographic X-ray computed tomography to quantitatively determine, at nanoscale resolutions, the three-dimensional mass density of scales with pigmentation differences. By comparing cuticle densities between pairs of scales with pigmentation differences, we determine that the density of the lower lamina is inversely correlated with pigmentation. In the upper lamina structure, low pigment levels also correlate with sheet-like chitin structures as opposed to rod-like structures. Within each scale, we determine that the lower lamina in all scales has the highest density and distinct layers within the lower lamina help explain reflected color. We hypothesize that pigments, in addition to absorbing specific wavelengths, can affect cuticle polymerization, density, and refractive index, thereby impacting reflected wavelengths that produce colors. </p>
<blockquote>
<p>è‰²ç´ åˆ†å¸ƒå¦‚ä½•å½±å“å¾®è§‚è´è¶ç¿…è†€é³ç‰‡å†…çš„è§’è´¨å±‚å¯†åº¦ï¼Œä»¥åŠä¸¤è€…å¦‚ä½•å…±åŒå½±å“æœ€ç»ˆåå°„çš„é¢œè‰²ä»æ˜¯æœªçŸ¥çš„ã€‚æˆ‘ä»¬ä½¿ç”¨Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææœ¯å®šé‡æµ‹å®šå…·æœ‰ä¸åŒè‰²ç´ æ²‰ç€çš„é³ç‰‡çš„ä¸‰ç»´è´¨é‡å¯†åº¦ï¼Œè¾¾åˆ°çº³ç±³çº§åˆ†è¾¨ç‡ã€‚é€šè¿‡æ¯”è¾ƒå…·æœ‰ä¸åŒè‰²ç´ æ²‰ç€çš„é³ç‰‡ä¹‹é—´çš„è§’è´¨å±‚å¯†åº¦ï¼Œæˆ‘ä»¬å‘ç°ä¸‹å±‚é³ç‰‡çš„å¯†åº¦ä¸è‰²ç´ æ²‰ç€å‘ˆè´Ÿç›¸å…³ã€‚åœ¨ä¸Šå±‚ç»“æ„ä¸­ï¼Œä½è‰²ç´ æ°´å¹³ä¸ç‰‡çŠ¶å‡ ä¸è´¨ç»“æ„ç›¸å…³ï¼Œè€Œä¸æ˜¯æ†çŠ¶ç»“æ„ã€‚åœ¨æ¯ä¸ªé³ç‰‡å†…éƒ¨ï¼Œæˆ‘ä»¬ç¡®å®šæ‰€æœ‰é³ç‰‡çš„ä¸‹å±‚éƒ½å…·æœ‰æœ€é«˜çš„å¯†åº¦ï¼Œä¸‹å±‚å†…çš„ä¸åŒå±‚æœ‰åŠ©äºè§£é‡Šåå°„é¢œè‰²ã€‚æˆ‘ä»¬å‡è®¾é™¤äº†å¸æ”¶ç‰¹å®šæ³¢é•¿å¤–ï¼Œè‰²ç´ è¿˜å¯ä»¥å½±å“è§’è´¨å±‚çš„èšåˆã€å¯†åº¦å’ŒæŠ˜å°„ç‡ï¼Œä»è€Œå½±å“åå°„æ³¢é•¿å¹¶äº§ç”Ÿé¢œè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.16628v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææŠ€æœ¯ï¼Œå®šé‡ç ”ç©¶äº†å…·æœ‰ä¸åŒè‰²ç´ åˆ†å¸ƒçš„è´è¶ç¿…è†€é³ç‰‡çš„ä¸‰ç»´è´¨é‡å¯†åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œé³ç‰‡ä¸‹å±‚çš„å¯†åº¦ä¸è‰²ç´ å«é‡å‘ˆè´Ÿç›¸å…³ï¼Œè€Œåœ¨ä¸Šå±‚ç»“æ„ä¸­ï¼Œä½è‰²ç´ æ°´å¹³ä¸ç‰‡çŠ¶å‡ ä¸è´¨ç»“æ„ç›¸å…³ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªé³ç‰‡ä¸­ä¸‹å±‚å…·æœ‰æœ€é«˜çš„å¯†åº¦ï¼Œå¹¶ä¸”ä¸åŒå±‚ä¹‹é—´çš„ç»“æ„æœ‰åŠ©äºè§£é‡Šåå°„çš„é¢œè‰²ã€‚å› æ­¤ï¼Œæœ¬æ–‡å‡è®¾è‰²ç´ é™¤äº†å¸æ”¶ç‰¹å®šæ³¢é•¿å¤–ï¼Œè¿˜å¯èƒ½å½±å“è§’è´¨å±‚èšåˆã€å¯†åº¦å’ŒæŠ˜å°„ç‡ï¼Œä»è€Œå½±å“åå°„æ³¢é•¿äº§ç”Ÿçš„é¢œè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«ææŠ€æœ¯å®šé‡ç ”ç©¶è´è¶ç¿…è†€é³ç‰‡çš„ä¸‰ç»´è´¨é‡å¯†åº¦ã€‚</li>
<li>å‘ç°é³ç‰‡ä¸‹å±‚çš„å¯†åº¦ä¸è‰²ç´ å«é‡å‘ˆè´Ÿç›¸å…³ã€‚</li>
<li>åœ¨ä¸Šå±‚ç»“æ„ä¸­ï¼Œä½è‰²ç´ æ°´å¹³ä¸ç‰‡çŠ¶å‡ ä¸è´¨ç»“æ„æœ‰å…³ã€‚</li>
<li>æ¯ä¸ªé³ç‰‡ä¸­ä¸‹å±‚å…·æœ‰æœ€é«˜çš„å¯†åº¦ã€‚</li>
<li>ä¸åŒå±‚ä¹‹é—´çš„ç»“æ„æœ‰åŠ©äºè§£é‡Šåå°„çš„é¢œè‰²ã€‚</li>
<li>è‰²ç´ å¯èƒ½å½±å“è§’è´¨å±‚çš„èšåˆã€å¯†åº¦å’ŒæŠ˜å°„ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.16628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e601f633bb67f379bf1e5efd29ab735.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b5615297355166f8ca440a1e03b2419e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-04  A Survey on Large Language Model Acceleration based on KV Cache   Management
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-687cbd5444566be4dfc9e746744597b4.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-03  AdaDiff Adaptive Step Selection for Fast Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
