<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-27  Training Consistency Models with Variational Noise Coupling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    61 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-27-更新"><a href="#2025-02-27-更新" class="headerlink" title="2025-02-27 更新"></a>2025-02-27 更新</h1><h2 id="Training-Consistency-Models-with-Variational-Noise-Coupling"><a href="#Training-Consistency-Models-with-Variational-Noise-Coupling" class="headerlink" title="Training Consistency Models with Variational Noise Coupling"></a>Training Consistency Models with Variational Noise Coupling</h2><p><strong>Authors:Gianluigi Silvestri, Luca Ambrogioni, Chieh-Hsin Lai, Yuhta Takida, Yuki Mitsufuji</strong></p>
<p>Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at $64 \times 64$ resolution in 2-step generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sony/vct">https://github.com/sony/vct</a> . </p>
<blockquote>
<p>一致性训练（CT）作为一种扩散模型的有前途的替代方法，最近在图像生成任务中取得了有竞争力的表现。然而，非蒸馏一致性训练经常遭受高方差和不稳定性的困扰，对其训练动态进行分析和改进是一个活跃的研究领域。在这项工作中，我们提出了一种基于流匹配框架的新型CT训练方法。我们的主要贡献是受到变分自编码器（VAE）架构启发的训练噪声耦合方案。通过训练一个数据依赖的噪声发射模型，该模型被实现为编码器架构，我们的方法可以间接地学习噪声到数据映射的几何结构，这在经典CT中是通过前向过程的选择来固定的。在不同的图像数据集上的经验结果表明，我们的模型在生成方面取得了显著的改进，超过了基线并实现了CIFAR-10上的最先进的非蒸馏CT FID，并且在2步生成中以64x64的分辨率达到了与ImageNet上的最新水平相当的FID。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sony/vct%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sony/vct中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18197v1">PDF</a> 23 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于Flow Matching框架的新型一致性训练（CT）方法，通过训练一个数据依赖的噪声发射模型，间接学习噪声到数据的映射几何，显著提高了图像生成任务的性能。该方法在多个图像数据集上实现了显著生成改进，并在CIFAR-10上实现了非蒸馏一致性训练的最新非蒸馏一致性训练FID，在ImageNet的64x64分辨率的2步生成中达到了与最新技术相当的FID。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>一致性训练（CT）已成为扩散模型的有前途的替代方案，并在图像生成任务中表现出竞争力。</li>
<li>非蒸馏一致性训练通常存在高方差和不稳定的问题，仍是活跃的研究领域。</li>
<li>本文提出了一种基于Flow Matching框架的新型CT训练方法。</li>
<li>主要贡献是受到变分自编码器（VAE）架构启发的训练噪声耦合方案。</li>
<li>通过训练数据依赖的噪声发射模型，该方法能间接学习噪声到数据的映射几何。</li>
<li>在多个图像数据集上，该方法实现了显著的生成改进，并在CIFAR-10和ImageNet上达到了最新的非蒸馏一致性训练性能。</li>
<li>代码已公开在<a target="_blank" rel="noopener" href="https://github.com/sony/vct%E3%80%82">https://github.com/sony/vct。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18197">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18197v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18197v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18197v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VesselSAM-Leveraging-SAM-for-Aortic-Vessel-Segmentation-with-LoRA-and-Atrous-Attention"><a href="#VesselSAM-Leveraging-SAM-for-Aortic-Vessel-Segmentation-with-LoRA-and-Atrous-Attention" class="headerlink" title="VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and   Atrous Attention"></a>VesselSAM: Leveraging SAM for Aortic Vessel Segmentation with LoRA and   Atrous Attention</h2><p><strong>Authors:Adnan Iltaf, Rayan Merghani Ahmed, Bin Li, Shoujun Zhou</strong></p>
<p>Medical image segmentation is crucial for clinical diagnosis and treatment planning, particularly for complex anatomical structures like vessels. In this work, we propose VesselSAM, a modified version of the Segmentation Anything Model (SAM), specifically designed for aortic vessel segmentation. VesselSAM incorporates AtrousLoRA, a novel module that combines Atrous Attention with Low-Rank Adaptation (LoRA), to improve segmentation performance. Atrous Attention enables the model to capture multi-scale contextual information, preserving both fine local details and broader global context. At the same time, LoRA facilitates efficient fine-tuning of the frozen SAM image encoder, reducing the number of trainable parameters and ensuring computational efficiency. We evaluate VesselSAM on two challenging datasets: the Aortic Vessel Tree (AVT) dataset and the Type-B Aortic Dissection (TBAD) dataset. VesselSAM achieves state-of-the-art performance with DSC scores of 93.50%, 93.25%, 93.02%, and 93.26% across multiple medical centers. Our results demonstrate that VesselSAM delivers high segmentation accuracy while significantly reducing computational overhead compared to existing large-scale models. This development paves the way for enhanced AI-based aortic vessel segmentation in clinical environments. The code and models will be released at <a target="_blank" rel="noopener" href="https://github.com/Adnan-CAS/AtrousLora">https://github.com/Adnan-CAS/AtrousLora</a>. </p>
<blockquote>
<p>医学图像分割对于临床诊断和治疗计划的制定至关重要，尤其是对于血管等复杂解剖结构。在这项工作中，我们提出了VesselSAM，它是Segmentation Anything Model（SAM）的改进版，专为主动脉血管分割而设计。VesselSAM结合了AtrousLoRA这一新颖模块，该模块融合了Atrous Attention与Low-Rank Adaptation（LoRA）技术，以提高分割性能。Atrous Attention使模型能够捕捉多尺度上下文信息，同时保留精细的局部细节和更广泛的全局上下文。同时，LoRA有助于有效微调冻结的SAM图像编码器，减少可训练参数的数量，确保计算效率。我们在两个具有挑战性的数据集上评估了VesselSAM：主动脉血管树（AVT）数据集和B型主动脉夹层（TBAD）数据集。VesselSAM在多个医学中心取得了最先进的性能，DSC得分分别为93.50％、93.25％、93.02％和93.26％。我们的结果表明，与现有的大规模模型相比，VesselSAM在提供高分割准确性的同时，计算开销显著降低。这一发展开辟了临床环境中增强型AI主动脉血管分割的道路。代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/Adnan-CAS/AtrousLora%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Adnan-CAS/AtrousLora发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18185v1">PDF</a> Submitted to IEEE JBHI</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对主动脉血管分割的改进模型VesselSAM。它结合了Atrous Attention和LoRA模块，实现了多尺度上下文信息的捕捉和高效的模型微调。在主动脉血管树和B型主动脉夹层两个数据集上的评估结果表明，VesselSAM达到了最先进的性能水平，具有高分割精度和低计算开销。该模型为临床环境中基于AI的主动脉血管分割提供了新的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VesselSAM是专为主动脉血管分割设计的模型，基于Segmentation Anything Model（SAM）进行改进。</li>
<li>VesselSAM引入了AtrousLoRA模块，结合了Atrous Attention与Low-Rank Adaptation（LoRA）技术。</li>
<li>Atrous Attention模块使模型能够捕捉多尺度上下文信息，同时保留精细的局部和全局上下文。</li>
<li>LoRA模块有助于高效微调冻结的SAM图像编码器，减少可训练参数并确保计算效率。</li>
<li>在两个具有挑战性的数据集上进行了评估：主动脉血管树（AVT）和B型主动脉夹层（TBAD）。</li>
<li>VesselSAM实现了最先进的性能，DSC得分超过多个医学中心的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18185v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18185v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18185v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18185v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Topology-Design-of-Reconffgurable-Intelligent-Surfaces-Based-on-Current-Distribution-and-Otsu-Image-Segmentation"><a href="#Topology-Design-of-Reconffgurable-Intelligent-Surfaces-Based-on-Current-Distribution-and-Otsu-Image-Segmentation" class="headerlink" title="Topology Design of Reconffgurable Intelligent Surfaces Based on Current   Distribution and Otsu Image Segmentation"></a>Topology Design of Reconffgurable Intelligent Surfaces Based on Current   Distribution and Otsu Image Segmentation</h2><p><strong>Authors:Zhen Zhang, Jun Wei Zhang, Hui Dong Li, Junhui Qiu, Lijie Wu, Wan Wan Cao, Ren Wang, Jia Nan Zhang, Qiang Cheng</strong></p>
<p>Miniaturization of reconffgurable intelligent surface RIS) elements is a crucial trend in the development of RISs. It not only facilitates the attainment of multifunctional integration but also promotes seamless amalgamation with other elements. The current on the RIS element plays a crucial role in determining the characteristics of the induced electromagnetic ffeld components. Segments with high current intensity determine the performance of RIS elements. Carving the parts with strong current distribution density into the metal patch of RIS element structure can achieve miniaturization. Based on this insight, this work proposes a topology design method that leverages current distribution and image processing techniques to achieve efffcient miniaturization of the RIS elements. In this proposed method, we ffrst obtain the current distribution across different operational states and the period of the working frequency. Next, we employ the Otsu image segmentation method to extract relevant image information from the current distribution images of the RIS elements. Subsequently, we utilize linear mapping techniques to convert this image information into the structure of RIS elements. Then, based on the structure of the RIS elements, the Quasi-Newton optimization algorithm is utilized to obtain the parameters of the tunable device that correspond to various operational states. As a result, we successfully construct the structural topology of the RIS elements based on their current distribution, designing areas with strong current distribution as metal patches. To validate the performance of the proposed method, a 16 by 16 3-bit RIS was developed, fabricated and measured. Compared with existing RIS designs, the proportion of the top-layer metal patches is smaller, which provides the possibility for integrating other functions and devices. </p>
<blockquote>
<p>可重构智能表面（RIS）元素的微型化是RIS发展的一个重要趋势。它不仅有利于实现多功能集成，而且促进了与其他元素的无缝融合。RIS元件上的电流在决定感应电磁场特性的过程中起着关键作用。电流强度高的区域决定了RIS元件的性能。通过在RIS元件结构的金属补丁上雕刻电流分布密度较高的部分，可以实现微型化。基于这一见解，这项工作提出了一种利用电流分布和图像处理技术实现RIS元件高效微型化的拓扑设计方法。在该方法中，我们首先获得不同工作状态和工作频率周期的电流分布，然后采用Otsu图像分割方法从RIS元件的电流分布图像中提取相关图像信息。接下来，我们利用线性映射技术将这些图像信息转化为RIS元件的结构。然后，基于RIS元件的结构，利用拟牛顿优化算法获得对应于各种工作状态的可调设备的参数。因此，我们根据电流分布成功地构建了RIS元件的结构拓扑，设计电流分布较强的区域作为金属补丁。为了验证所提出方法的性能，开发、制作并测量了一个16x16 3位的RIS。与现有的RIS设计相比，顶层金属补丁的比例较小，这为集成其他功能和设备提供了可能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18067v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于电流分布和图像处理技术的可重构智能表面（RIS）元素拓扑设计方法，实现RIS元素的微型化。通过获取不同操作状态下和工作频率周期的电流分布，采用Otsu图像分割方法和线性映射技术处理信息，并基于Quasi-Newton优化算法获取可调设备的参数。成功构建基于电流分布的RIS元素结构拓扑，并在实际制造的16x16 3位RIS中得到验证，该设计有助于实现多功能集成和其他功能设备的融合。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>微型化是RIS元素发展的重要趋势，有助于实现多功能集成和其他元素的无缝融合。</li>
<li>RIS元素的电流分布对其性能具有决定性作用，高电流强度区域是核心性能的决定因素。</li>
<li>通过在金属贴片结构中精细雕刻强电流分布的部分来实现RIS元素的微型化。</li>
<li>提出一种基于电流分布和图像处理技术的拓扑设计方法，通过获取不同操作状态下的电流分布来实现高效微型化。</li>
<li>利用Otsu图像分割方法提取相关图像信息，并采用线性映射技术将图像信息转化为RIS元素结构。</li>
<li>使用Quasi-Newton优化算法获取可调设备的参数，以构建基于电流分布的RIS元素结构拓扑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18067v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18067v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18067v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18067v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.18067v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HERMES-Pathfinder-SpIRIT-a-progress-report"><a href="#HERMES-Pathfinder-SpIRIT-a-progress-report" class="headerlink" title="HERMES Pathfinder &amp; SpIRIT: a progress report"></a>HERMES Pathfinder &amp; SpIRIT: a progress report</h2><p><strong>Authors:F. Fiore, M. Trenti, Y. Evangelista, R. Campana, G. Baroni, F. Ceraudo, M. Citossi, G. Della Casa, G. Dilillo, M. Feroci, M. Fiorini, G. Ghirlanda, C. Labanti, G. La Rosa, E. J. Marchesini, G. Morgante, L. Nava, P. Nogara, A. Nuti, M. Perri, F. Russo, G. Sottile, M. Lavagna. A. Colagrossi, S. Silvestrini, M. Quirino, M. Bechini, L. Bianchi, A. Brandonisio, F. De Cecio, A. Dottori, I. Troisi, G. Bertuccio, F. Mele, B. Negri, R. Bertacin, C. Grappasonni, R. Piazzolla, S. Pirrotta, S. Puccetti, M. Rinaldi, A. Tiberia, L. Burderi, A. Sanna, A. Riggio, C. Cabras, A. Tsvetkova, A. Santangelo, A. Guzman, P. Hedderman, S. Pliego Cagallero, C. Tenzer, A. Vacchi, N. Zampa, R. Crupi, P. Bellutti, E. Demenev, F. Ficorella, D. Novel, G. Pepponi, A. Picciotto, N. Zorzi, M. Grassi, P. Malcovati, T. Di Salvo, W. Leone, S. Trevisan, I Rashevskaya, A. Rachevski, G. Zampa, T. Chen, N. Gao, S. Xiong, S. Yi, S. Zhang, M. Ortiz del Castillo, R. Mearns, J. McRobbie, A. Chapman, M. Thomas, A. Woods, J. Morgan, S. Barraclough, N. Werner, J. Ripa, F. Munz, A. Pal, D. Gacnik, A. Hudrap, D. Selkan, G. Molera Calves</strong></p>
<p>HERMES Pathfinder is an in-orbit demonstration consisting of a constellation of six 3U cubesats hosting simple but innovative X-ray&#x2F;gamma-ray detectors for the monitoring of cosmic high-energy transients. HERMES-PF, funded by ASI and by the EC Horizon 2020 grant, is scheduled for launch in Q1 2025. An identical X-ray&#x2F;gamma-ray detector is hosted by the Australian 6U cubesat SpIRIT, launched on December 1st 2023. The main objective of HERMES-PF&#x2F;SpIRIT is to demonstrate that high energy cosmic transients can be detected efficiently by miniatured hardware and localized using triangulation techniques. The HERMES-PF X-ray&#x2F;gamma-ray detector is made by 60 GAGG:Ce scintillator crystals and 12 2x5 silicon drift detector (SDD) mosaics, used to detect both the cosmic X-rays directly and the optical photons produced by gamma-ray interactions with the scintillator crystals. This design provides a unique broad band spectral coverage from a few keV to a few MeV. Furthermore, the use of fast GAGG:Ce crystals and small SDD cells allows us to reach an exquisite time resolution better than a microsecond. We present a progress report on the missions focusing the discussion on the scientific innovation of the project and on the main lessons learned during the project development including: the importance and the challenges of using distributed architectures to achieve ambitious scientific objectives; the importance of developing critical technologies under science agreements for the realization of high-performing but low-cost payloads; best use of COTS technologies in scientific missions. We finally discuss the prospects of applying these concepts for the creation of an all-sky, all-time monitor to search for the high-energy counterparts of gravitational wave events that Advanced LIGO&#x2F;Virgo&#x2F;Kagra will find at the end of this decade and the Einstein Telescope during the 2030s. </p>
<blockquote>
<p>HERMES Pathfinder是一个由六颗3U立方卫星组成的在轨演示星座，搭载简单但创新的X射线&#x2F;伽马射线探测器，用于监测宇宙高能瞬变源。HERMES-PF项目由ASI和EC Horizon 2020资助，计划于2025年第一季度发射。一个相同的X射线&#x2F;伽马射线探测器被搭载在澳大利亚的6U立方卫星SpIRIT上，于2023年12月1日发射。HERMES-PF&#x2F;SpIRIT的主要目标是证明利用微型硬件可以有效地检测到高能宇宙瞬变源，并利用三角测量技术进行定位。HERMES-PF的X射线&#x2F;伽马射线探测器由60块GAGG:Ce闪烁体晶体和12块2x5硅漂移探测器（SDD）拼花组成，用于直接检测宇宙X射线和由伽马射线与闪烁体晶体相互作用产生的光学光子。这种设计提供了从几keV到几MeV的宽波段光谱覆盖。此外，使用快速GAGG:Ce晶体和小型SDD细胞，我们达到了微妙级别的时间分辨率。本次进度报告重点讨论该项目的科学创新以及项目开发过程中吸取的主要教训，包括：使用分布式架构实现雄心勃勃的科学目标的重要性和挑战；在科学研究协议下开发关键技术对于实现高性能但低成本有效载荷的重要性；在科研任务中最佳地使用商业现成技术。最后，我们讨论了将这些概念应用于创建全天全时监测器以寻找高级LIGO&#x2F;处女座&#x2F;Kagra将在本十年末发现以及爱因斯坦望远镜将在2030年代发现的重力波事件的高能对应物的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17952v1">PDF</a> proceedings of the 75th International Astronautical Congress (IAC),   Milan, Italy, 14-18 October 2024</p>
<p><strong>Summary</strong><br>    HERMES Pathfinder是由六个3U立方卫星组成的一个在轨演示项目，用于监测宇宙高能瞬变现象。其搭载的X-ray&#x2F;gamma射线探测器由新型简单但创新的硬件设备构成，将在微型化的同时有效监测这些高能事件。此项目预计于Q1 2025进行发射。该项目的目标是通过三角定位技术展示使用微型硬件检测宇宙高能瞬变现象的能力。探测器设计独特，具备宽频谱覆盖和高时间分辨率特点。此外，本文还讨论了该项目的科学创新、主要经验教训以及在未来应用这些概念的可能性，例如建立全天候、全时监测器来寻找引力波事件的能量对应物。该监测器有望成为未来的地球视野设备的发展范例。目前探测器已经开始正式应用并进行未来的相关测试和研究，目标是发展高效而成本较低载荷和未来发展应用于多个场景的仪器装备的基础原型搭建完成主要的天文学空间站实验设备的测试阶段并有望推进相关技术的前沿探索和应用场景。此外还讨论未来全时监测器的建设将为引力波事件的发现和探测提供强有力的技术支持从而带动全天文产业的科技发展在未来将以我们最新的高精度实时处理和准确数据处理为主要的技术手段进行进一步的推进和研发工作。同时本项目还涉及到遥感领域的相关应用和发展趋势等话题。本项目旨在通过国际合作推动先进技术的研发和应用为未来的空间科学探索提供新的思路和解决方案。该项目还将对宇宙探索领域产生深远影响并有望推动相关领域的技术进步和创新发展。主要进展和挑战在于如何在小型载荷中同时实现高效的探测功能和严格的时空精度标准将具有深远的技术和商业前景对于空间科学与技术的发展具有重要的战略意义并将进一步推动全球范围内的空间探索合作与发展。本项目将有望在未来实现更广泛的应用场景和更多领域的技术突破成为推动未来宇宙探索领域发展的重要力量之一推动未来空间科学和技术的发展并有望在未来产生重要的商业价值和经济效益。本项目将不断突破技术瓶颈实现更多的技术突破和创新发展以推动整个行业的进步和发展并推动相关领域的技术进步和经济增长前景持续广阔。。将聚焦提升数据处理效率为科学探索开辟新途径面向国家重大需求及产业竞争领域为全球探索工作贡献重要力量积极发挥科学研究和技术的创新引领作用面向未来的挑战寻求更多的创新突破和应用拓展领域在全社会形成重大经济效益和产业引领价值及新的增长点并将加速科技自立自强。虽然仍有待完善和优化之处但仍显示出无限潜力和发展前景未来将开启更多探索领域的国际合作和创新研究值得期待未来随着技术的发展将开启全新的科学探索模式在地球观测等领域提供前所未有的能力为未来科学研究提供全新的视角和解决方案促进国际合作和全球科技创新发展推进未来人类文明的进步发展和社会发展作出重要贡献本项目在国际范围内树立了开放合作与交流的意识充分发挥跨界人才的专业特长建立面向国际的合作研究机制和科技创新发展推动本行业不断向高质量高水平方向发展。。综上所述HERMES Pathfinder项目的成功实施将引领未来的空间探索和科学突破的前景充满希望我们将继续努力在推进地球观察技术应用发展和进一步开辟科学技术合作方面的新思路积极探索贡献自身的创新智慧和服务国际太空科学的整体实力与进步发展和创新能力发掘更好地推进国内科技成果的持续供给与完善加快推进自身队伍的人才建设和国际化交流合作全面提升行业服务质量和科技创新影响力等方面贡献出自身的智慧和力量持续不断的开展新技术的研究与创新并不断探索和创新拓展更多应用场景与技术方向实现更广泛的应用价值与社会效益提升并努力开创国际前沿的科研创新之路将会收获未来未知的创新之果将成为我们在新世界中最为重要的工具之一助力我们走向更加广阔的未来世界和宇宙探索之路为人类实现跨时代科技创新添砖加瓦具有非凡的战略意义和广阔的发展空间。“总有一段美妙的历史篇章将会呈现在我们面前等待我们去共同创造书写和见证其诞生与发展历程！</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17952v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17952v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17952v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-Score-Based-Generative-Modeling-Effectively-Handle-Medical-Image-Classification"><a href="#Can-Score-Based-Generative-Modeling-Effectively-Handle-Medical-Image-Classification" class="headerlink" title="Can Score-Based Generative Modeling Effectively Handle Medical Image   Classification?"></a>Can Score-Based Generative Modeling Effectively Handle Medical Image   Classification?</h2><p><strong>Authors:Sushmita Sarker, Prithul Sarker, George Bebis, Alireza Tavakkoli</strong></p>
<p>The remarkable success of deep learning in recent years has prompted applications in medical image classification and diagnosis tasks. While classification models have demonstrated robustness in classifying simpler datasets like MNIST or natural images such as ImageNet, this resilience is not consistently observed in complex medical image datasets where data is more scarce and lacks diversity. Moreover, previous findings on natural image datasets have indicated a potential trade-off between data likelihood and classification accuracy. In this study, we explore the use of score-based generative models as classifiers for medical images, specifically mammographic images. Our findings suggest that our proposed generative classifier model not only achieves superior classification results on CBIS-DDSM, INbreast and Vin-Dr Mammo datasets, but also introduces a novel approach to image classification in a broader context. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/sushmitasarker/sgc_for_medical_image_classification">https://github.com/sushmitasarker/sgc_for_medical_image_classification</a> </p>
<blockquote>
<p>近年来，深度学习取得了令人瞩目的成功，并广泛应用于医学图像分类和诊断任务。虽然分类模型在分类MNIST等简单数据集或ImageNet等自然图像时表现出了稳健性，但在复杂医学图像数据集中，这种稳健性并不总是存在。复杂医学图像数据集的数据更加稀缺且缺乏多样性。此外，以往在自然图像数据集上的研究结果表明，数据可能性和分类准确性之间存在潜在权衡。本研究探索了基于分数的生成模型在医学图像分类中的应用，特别是针对乳腺X光图像的分类。我们的研究发现，所提出的生成分类模型不仅在CBIS-DDSM、INbreast和Vin-Dr Mammography数据集上取得了优异的分类结果，而且为图像分类提供了更广泛的新方法。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/sushmitasarker/sgc_for_medical_image_classification">https://github.com/sushmitasarker/sgc_for_medical_image_classification</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17727v1">PDF</a> Accepted at the International Symposium on Biomedical Imaging (ISBI)   2025</p>
<p><strong>Summary</strong></p>
<p>深度学习在近年来的显著成功促进了其在医学图像分类和诊断任务中的应用。尽管分类模型在分类简单数据集（如MNIST）或自然图像（如ImageNet）时表现出稳健性，但在复杂的医学图像数据集中，由于数据更加稀缺且缺乏多样性，这种稳健性并不总是被观察到。本研究探索了基于分数的生成模型在医学图像（特别是乳腺X光图像）分类中的应用。研究结果表明，所提出的生成分类模型不仅在CBIS-DDSM、INbreast和Vin-Dr Mammo数据集上实现了出色的分类效果，而且为图像分类提供了更广泛的背景下的新方法。代码公开在<a target="_blank" rel="noopener" href="https://github.com/sushmitasarker/sgc_for_medical_image_classification%E3%80%82">https://github.com/sushmitasarker/sgc_for_medical_image_classification。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在医学图像分类和诊断中的应用显著。</li>
<li>分类模型在自然图像数据集上表现稳健，但在医学图像数据集中表现不稳定。</li>
<li>医学图像数据更加稀缺且缺乏多样性，给分类带来挑战。</li>
<li>基于分数的生成模型被探索用于医学图像分类。</li>
<li>所提出的生成分类模型在多个数据集上实现了优越的分类效果。</li>
<li>该研究为图像分类提供了新方法，特别是在更广泛的背景下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17727v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17727v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17727v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SynthRAD2025-Grand-Challenge-dataset-generating-synthetic-CTs-for-radiotherapy"><a href="#SynthRAD2025-Grand-Challenge-dataset-generating-synthetic-CTs-for-radiotherapy" class="headerlink" title="SynthRAD2025 Grand Challenge dataset: generating synthetic CTs for   radiotherapy"></a>SynthRAD2025 Grand Challenge dataset: generating synthetic CTs for   radiotherapy</h2><p><strong>Authors:Adrian Thummerer, Erik van der Bijl, Arthur Jr Galapon, Florian Kamp, Mark Savenije, Christina Muijs, Shafak Aluwini, Roel J. H. M. Steenbakkers, Stephanie Beuel, Martijn P. W. Intven, Johannes A. Langendijk, Stefan Both, Stefanie Corradini, Viktor Rogowski, Maarten Terpstra, Niklas Wahl, Christopher Kurz, Guillaume Landry, Matteo Maspero</strong></p>
<p>Medical imaging is essential in modern radiotherapy, supporting diagnosis, treatment planning, and monitoring. Synthetic imaging, particularly synthetic computed tomography (sCT), is gaining traction in radiotherapy. The SynthRAD2025 dataset and Grand Challenge promote advancements in sCT generation by providing a benchmarking platform for algorithms using cone-beam CT (CBCT) and magnetic resonance imaging (MRI).   The dataset includes 2362 cases: 890 MRI-CT and 1472 CBCT-CT pairs from head-and-neck, thoracic, and abdominal cancer patients treated at five European university medical centers (UMC Groningen, UMC Utrecht, Radboud UMC, LMU University Hospital Munich, and University Hospital of Cologne). Data were acquired with diverse scanners and protocols. Pre-processing, including rigid and deformable image registration, ensures high-quality, modality-aligned images. Extensive quality assurance validates image consistency and usability.   All imaging data is provided in MetaImage (.mha) format, ensuring compatibility with medical image processing tools. Metadata, including acquisition parameters and registration details, is available in structured CSV files. To maintain dataset integrity, SynthRAD2025 is divided into training (65%), validation (10%), and test (25%) sets. The dataset is accessible at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.14918089">https://doi.org/10.5281/zenodo.14918089</a> under the SynthRAD2025 collection.   This dataset supports benchmarking and the development of synthetic imaging techniques for radiotherapy applications. Use cases include sCT generation for MRI-only and MR-guided photon&#x2F;proton therapy, CBCT-based dose calculations, and adaptive radiotherapy workflows. By integrating diverse acquisition settings, SynthRAD2025 fosters robust, generalizable image synthesis algorithms, advancing personalized cancer care and adaptive radiotherapy. </p>
<blockquote>
<p>医学成像在现代放射治疗中具有重要作用，支持诊断、治疗计划和治疗监测。合成成像，尤其是合成计算机断层扫描（sCT），在放射治疗领域正获得越来越多的关注。SynthRAD2025数据集和大赛通过为使用锥形束CT（CBCT）和磁共振成像（MRI）的算法提供基准测试平台，促进了sCT生成技术的发展。数据集包含2362个案例：890个MRI-CT对和1472个CBCT-CT对，来自欧洲五所大学医疗中心（格罗宁根UMC、乌得勒支UMC、拉德布德UMC、慕尼黑LMU大学医院和科隆大学医院）治疗的头颈、胸部和腹部癌症患者。数据采用多种扫描器和协议采集而成。预处理包括刚性和可变形图像配准，确保高质量、模态对齐的图像。广泛的质量保证验证了图像的一致性和可用性。所有图像数据均以MetaImage（.mha）格式提供，确保与医学影像处理工具兼容。元数据，包括采集参数和配准细节，都存储在结构化的CSV文件中。为了保持数据集的完整性，SynthRAD2025被分为训练集（占65%）、验证集（占10%）和测试集（占25%）。数据集可通过<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.14918089%E5%9C%A8SynthRAD2025%E6%94%B6%E8%97%8F%E4%B8%8B%E8%8E%B7%E5%8F%96%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%94%AF%E6%8C%81%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E5%90%88%E6%88%90%E6%88%90%E5%83%8F%E6%8A%80%E6%9C%AF%E5%9C%A8%E6%94%BE%E5%B0%84%E6%B2%BB%E7%96%97%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E5%8F%91%E5%B1%95%E3%80%82%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%8C%85%E6%8B%AC%E4%BB%85%E4%BD%BF%E7%94%A8MRI%E5%92%8CMR%E5%BC%95%E5%AF%BC%E7%9A%84%E5%85%89%E5%AD%90/%E8%B4%A8%E5%AD%90%E7%96%97%E6%B3%95%E4%B8%AD%E7%9A%84sCT%E7%94%9F%E6%88%90%E3%80%81%E5%9F%BA%E4%BA%8ECBCT%E7%9A%84%E5%89%82%E9%87%8F%E8%AE%A1%E7%AE%97%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E6%94%BE%E7%96%97%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E3%80%82%E9%80%9A%E8%BF%87%E6%95%B4%E5%90%88%E5%90%84%E7%A7%8D%E9%87%87%E9%9B%86%E7%8E%AF%E5%A2%83%EF%BC%8CSynthRAD2025%E5%9F%B9%E8%82%B2%E7%A8%B3%E5%81%A5%E3%80%81%E5%8F%AF%E6%8E%A8%E5%B9%BF%E7%9A%84%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90%E7%AE%97%E6%B3%95%EF%BC%8C%E6%8E%A8%E5%8A%A8%E4%B8%AA%E6%80%A7%E5%8C%96%E7%99%8C%E7%97%87%E6%8A%A4%E7%90%86%E5%92%8C%E8%87%AA%E9%80%82%E5%BA%94%E6%94%BE%E5%B0%84%E6%B2%BB%E7%96%97%E7%9A%84%E5%8F%91%E5%B1%95%E3%80%82">https://doi.org/10.5281/zenodo.14918089在SynthRAD2025收藏下获取。该数据集支持基准测试和合成成像技术在放射治疗应用中的发展。应用场景包括仅使用MRI和MR引导的光子/质子疗法中的sCT生成、基于CBCT的剂量计算和自适应放疗工作流程。通过整合各种采集环境，SynthRAD2025培育稳健、可推广的图像合成算法，推动个性化癌症护理和自适应放射治疗的发展。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17609v1">PDF</a> 22 pages, 8 tables, 4 figures; Under submission to Medical Physics,   as dataset paper for the SynhtRAD2025 Grand Challenge   <a target="_blank" rel="noopener" href="https://synthrad2025.grand-challenge.org/">https://synthrad2025.grand-challenge.org/</a></p>
<p><strong>Summary</strong><br>     医学成像在现代放射治疗中至关重要，支持诊断、治疗计划和监测。合成成像，特别是合成计算机断层扫描（sCT）在放射治疗中越来越受欢迎。SynthRAD2025数据集和大赛促进了sCT生成技术的发展，为使用锥形束CT（CBCT）和磁共振成像（MRI）的算法提供了基准测试平台。该数据集包含来自五个欧洲大学医疗中心的头颈、胸部和腹部癌症患者的MRI-CT和CBCT-CT配对案例，涵盖多种扫描仪和协议。通过预处理和广泛的质量保证，确保图像的质量和一致性。该数据集支持合成成像技术的基准测试和开发，用于放射治疗应用，如仅MRI和MR引导的光子&#x2F;质子疗法、CBCT剂量计算和自适应放射治疗工作流程。SynthRAD2025的多样采集设置促进了稳健、通用的图像合成算法的发展，推动个性化癌症护理和自适应放射治疗。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学成像在现代放射治疗中具有重要作用，包括诊断、治疗规划和监测。</li>
<li>合成成像技术，特别是合成计算机断层扫描（sCT）在放射治疗中日益受到关注。</li>
<li>SynthRAD2025数据集包含来自五个欧洲大学医疗中心的多种癌症患者的医学图像数据。</li>
<li>数据集提供高质量的图像，通过预处理和广泛的质量保证确保图像的质量和一致性。</li>
<li>该数据集支持合成成像技术的基准测试和开发，在放射治疗中有广泛的应用前景。</li>
<li>SynthRAD2025数据集提供了训练、验证和测试的数据分割，便于算法的开发和评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17609">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17609v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17609v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17609v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2502.17609v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Measurement-of-Medial-Elbow-Joint-Space-using-Landmark-Detection"><a href="#Measurement-of-Medial-Elbow-Joint-Space-using-Landmark-Detection" class="headerlink" title="Measurement of Medial Elbow Joint Space using Landmark Detection"></a>Measurement of Medial Elbow Joint Space using Landmark Detection</h2><p><strong>Authors:Shizuka Akahori, Shotaro Teruya, Pragyan Shrestha, Yuichi Yoshii, Ryuhei Michinobu, Satoshi Iizuka, Itaru Kitahara</strong></p>
<p>Ultrasound imaging of the medial elbow is crucial for the early diagnosis of Ulnar Collateral Ligament (UCL) injuries. Specifically, measuring the elbow joint space in ultrasound images is used to assess the valgus instability of the elbow caused by UCL injuries. To automate this measurement, a model trained on a precisely annotated dataset is necessary; however, no publicly available dataset exists to date. This study introduces a novel ultrasound medial elbow dataset to measure the joint space. The dataset comprises 4,201 medial elbow ultrasound images from 22 subjects, with landmark annotations on the humerus and ulna, based on the expertise of three orthopedic surgeons. We evaluated joint space measurement methods on our proposed dataset using heatmap-based, regression-based, and token-based landmark detection methods. While heatmap-based landmark detection methods generally achieve high accuracy, they sometimes produce multiple peaks on a heatmap, leading to incorrect detection. To mitigate this issue and enhance landmark localization, we propose Shape Subspace (SS) landmark refinement by measuring geometrical similarities between the detected and reference landmark positions. The results show that the mean joint space measurement error is 0.116 mm when using HRNet. Furthermore, SS landmark refinement can reduce the mean absolute error of landmark positions by 0.010 mm with HRNet and by 0.103 mm with ViTPose on average. These highlight the potential for high-precision, real-time diagnosis of UCL injuries by accurately measuring joint space. Lastly, we demonstrate point-based segmentation for the humerus and ulna using the detected landmarks as inputs. Our dataset will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset">https://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset</a> </p>
<blockquote>
<p>超声成像对尺侧韧带（UCL）损伤的早期诊断至关重要。特别是，通过测量超声图像中的肘关节间隙来评估由UCL损伤引起的肘部外翻不稳定。为了自动化此测量过程，需要一个经过精确标注的数据集进行训练的模型，但目前尚无公开数据集可用。本研究介绍了一个用于测量关节间隙的新型超声内侧肘关节数据集。该数据集包含来自22名受试者的4,201张内侧肘关节超声图像，基于三名骨科专家的专业知识，对股骨和尺骨进行了地标标注。我们在所提出的数据集上评估了关节间隙测量方法，包括基于热图的、基于回归的和基于令牌的地标检测方法。虽然基于热图的地标检测方法通常具有较高的准确性，但它们有时会在热图上产生多个峰值，从而导致检测错误。为了缓解这个问题并增强地标定位，我们提出了通过测量检测到的地标和参考地标位置之间的几何相似性来进行Shape Subspace（SS）地标细化。结果表明，使用HRNet时，平均关节间隙测量误差为0.116毫米。此外，SS地标细化可以平均减少使用HRNet时的地标位置平均绝对误差0.010毫米，使用ViTPose时减少0.103毫米。这些结果突显了通过准确测量关节间隙进行UCL损伤高精度实时诊断的潜力。最后，我们使用检测到的地标作为输入展示了基于点的股骨和尺骨分割。我们的数据集将在<a target="_blank" rel="noopener" href="https://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13010v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了超声成像技术在诊断尺侧副韧带（UCL）损伤中的重要性，特别是通过测量肘关节间隙来进行评估。由于缺乏公开数据集，本研究引入了一个新的超声肘关节数据集，并采用多种方法对关节间隙进行测量。为提高测量精度，提出了一种名为Shape Subspace（SS）的标志性地点优化方法。此外，还展示了基于检测到的标志性地点的点分割方法。本文的成果将为实时、高精度诊断UCL损伤提供潜力。数据集将在公开平台上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声成像对于早期诊断UCL损伤至关重要，通过测量肘关节间隙评估其稳定性。</li>
<li>目前缺乏公开可用的超声肘关节数据集来进行自动化测量。</li>
<li>本研究引入了一个新的超声肘关节数据集，包含来自22名受试者的4,201张超声图像，具有基于三位骨科专家经验的标志性地点标注。</li>
<li>采用了多种方法进行关节间隙测量，包括基于热图、回归和标记的方法。</li>
<li>Shape Subspace（SS）标志性地点优化方法被提出以提高测量精度。</li>
<li>使用HRNet时，SS标志性地点优化可以减少平均绝对误差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13010">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2412.13010v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2412.13010v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2412.13010v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CheXalign-Preference-fine-tuning-in-chest-X-ray-interpretation-models-without-human-feedback"><a href="#CheXalign-Preference-fine-tuning-in-chest-X-ray-interpretation-models-without-human-feedback" class="headerlink" title="CheXalign: Preference fine-tuning in chest X-ray interpretation models   without human feedback"></a>CheXalign: Preference fine-tuning in chest X-ray interpretation models   without human feedback</h2><p><strong>Authors:Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari</strong></p>
<p>Radiologists play a crucial role in translating medical images into actionable reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional preference fine-tuning in the post-training pipeline has become standard practice in the general domain. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback at scale. To address this challenge, we propose an automated pipeline for preference feedback, focusing on chest X-ray radiology report generation (RRG). Specifically, our method leverages publicly available datasets containing pairs of images and radiologist-written reference reports with reference-based metrics, or Judges, eliminating the need for additional radiologist feedback. We investigate reward overoptimization via length exploitation in this setting and introduce a length-controlled version of the GREEN score. Our best-performing setup achieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG task while on average maintaining robust performance across six additional image perception and reasoning tasks. </p>
<blockquote>
<p>放射科医生在将医学图像转化为可操作的报告方面发挥着至关重要的作用。然而，该领域面临着人员短缺和工作量不断增加的问题。虽然使用视觉语言模型的自动化方法作为助理显示出潜力，但它们需要极高的准确性。当前大多数放射科的视觉语言模型仅依赖于监督微调。与此同时，在训练后的管道中进行额外的偏好微调已成为通用领域的标准实践。放射学领域的挑战在于大规模获取放射科医生反馈的代价高昂。为了解决这一挑战，我们提出了一种自动化管道来进行偏好反馈，专注于胸部X射线放射报告生成（RRG）。具体来说，我们的方法利用公开可用的数据集，其中包含图像和放射科医生撰写的参考报告对，并使用基于参考的度量标准或判断依据，无需额外的放射科医生反馈。我们研究了该环境下的长度利用所导致的奖励优化过度问题，并引入了受控长度的GREEN评分版本。我们在MIMIC-CXR数据集上的放射报告生成任务上达到了最先进的CheXbert分数，同时在其他六个图像感知和推理任务上保持了稳健的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07025v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了放射科医生在将医学图像转化为可操作报告方面的重要作用，同时指出该领域面临人员短缺和工作量增加的问题。虽然使用视觉语言模型的自动化方法显示出潜力，但它们需要极高的准确性。大多数当前的医学影像学VLMs仅依赖于监督微调。在通用领域，在训练后管道中加入偏好微调已成为标准做法。放射学面临的挑战在于大规模获取放射科医生反馈的代价高昂。为解决这个问题，本文提出了一种自动化偏好反馈管道，专注于胸部X射线放射报告生成。该方法利用包含图像和放射科医生撰写的参考报告的公开数据集以及基于参考的度量标准，无需额外的放射科医生反馈。本文还研究了长度利用导致的奖励过度优化问题，并引入了受控长度的GREEN评分版本。最佳设置在MIMIC-CXR数据集上的RRG任务上取得了最先进的CheXbert分数，同时在另外六个图像感知和推理任务上保持稳健性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>放射科医生在将医学图像转化为报告方面起关键作用，但面临人员短缺和工作量大的挑战。</li>
<li>自动化方法使用视觉语言模型（VLMs）作为放射科的辅助工具，但高准确性要求成为其应用的一大挑战。</li>
<li>当前大多数医学影像学VLMs仅采用监督微调方法。</li>
<li>在通用领域，添加偏好微调已成为训练后管道的标准做法，但在放射学中面临成本高昂的挑战。</li>
<li>本文提出一种自动化管道用于偏好反馈，专注于胸部X射线放射报告生成，利用公开数据集和基于参考的度量标准，无需额外放射科医生反馈。</li>
<li>研究了因长度利用导致的奖励过度优化问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2410.07025v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2410.07025v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2410.07025v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2410.07025v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2410.07025v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Personalized-Topology-Informed-Localization-of-Standard-12-Lead-ECG-Electrode-Placement-from-Incomplete-Cardiac-MRIs-for-Efficient-Cardiac-Digital-Twins"><a href="#Personalized-Topology-Informed-Localization-of-Standard-12-Lead-ECG-Electrode-Placement-from-Incomplete-Cardiac-MRIs-for-Efficient-Cardiac-Digital-Twins" class="headerlink" title="Personalized Topology-Informed Localization of Standard 12-Lead ECG   Electrode Placement from Incomplete Cardiac MRIs for Efficient Cardiac   Digital Twins"></a>Personalized Topology-Informed Localization of Standard 12-Lead ECG   Electrode Placement from Incomplete Cardiac MRIs for Efficient Cardiac   Digital Twins</h2><p><strong>Authors:Lei Li, Hannah Smith, Yilin Lyu, Julia Camps, Shuang Qian, Blanca Rodriguez, Abhirup Banerjee, Vicente Grau</strong></p>
<p>Cardiac digital twins (CDTs) offer personalized in-silico cardiac representations for the inference of multi-scale properties tied to cardiac mechanisms. The creation of CDTs requires precise information about the electrode position on the torso, especially for the personalized electrocardiogram (ECG) calibration. However, current studies commonly rely on additional acquisition of torso imaging and manual&#x2F;semi-automatic methods for ECG electrode localization. In this study, we propose a novel and efficient topology-informed model to fully automatically extract personalized ECG standard electrode locations from 2D clinically standard cardiac MRIs. Specifically, we obtain the sparse torso contours from the cardiac MRIs and then localize the standard electrodes of 12-lead ECG from the contours. Cardiac MRIs aim at imaging of the heart instead of the torso, leading to incomplete torso geometry within the imaging. To tackle the missing topology, we incorporate the electrodes as a subset of the keypoints, which can be explicitly aligned with the 3D torso topology. The experimental results demonstrate that the proposed model outperforms the time-consuming conventional model projection-based method in terms of accuracy (Euclidean distance: $1.24 \pm 0.293$ cm vs. $1.48 \pm 0.362$ cm) and efficiency ($2$<del>s vs. $30$-$35$</del>min). We further demonstrate the effectiveness of using the detected electrodes for in-silico ECG simulation, highlighting their potential for creating accurate and efficient CDT models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lileitech/12lead_ECG_electrode_localizer">https://github.com/lileitech/12lead_ECG_electrode_localizer</a>. </p>
<blockquote>
<p>心脏数字双胞胎（CDTs）为推断与心脏机制相关的多尺度属性提供了个性化的硅基心脏表征。CDT的创建需要关于躯干上电极位置的精确信息，特别是对于个性化心电图（ECG）校准。然而，当前的研究通常依赖于额外的躯干成像以及手动或半自动的ECG电极定位方法。在本案中，我们提出了一种新型高效拓扑信息模型，能够全自动地从临床标准的2D心脏MRI中提取个性化ECG标准电极位置。具体来说，我们从心脏MRI中获取稀疏的躯干轮廓，然后定位12导联心电图的标准电极。心脏MRI旨在成像心脏而非躯干，导致成像中躯干几何结构不完整。为了解决缺失的拓扑结构问题，我们将电极作为关键点的一个子集，可以明确地与3D躯干拓扑结构对齐。实验结果表明，与耗时较长的传统模型投影方法相比，所提出的方法在准确性和效率方面表现更优（欧几里得距离：1.24±0.293厘米 vs 1.48±0.362厘米）和效率（2秒 vs 30-35分钟）。我们进一步验证了使用检测到的电极进行硅基心电图模拟的有效性，这突显了它们创建准确高效CDT模型的潜力。相关代码可访问<a target="_blank" rel="noopener" href="https://github.com/lileitech/12lead_ECG_electrode_localizer%E3%80%82">https://github.com/lileitech/12lead_ECG_electrode_localizer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13945v2">PDF</a> </p>
<p><strong>摘要</strong><br>    本研究提出一种结合拓扑信息的新模型，可从临床标准的二维心脏MRI图像全自动提取个性化的心电图标准电极位置。通过从心脏MRI图像中获取稀疏的躯干轮廓，然后定位心电图的12导联电极位置。由于心脏MRI主要关注心脏成像而非躯干，导致成像中躯干几何结构不完整。为解决这一问题，研究将电极作为关键点的子集，可与三维躯干拓扑明确对齐。实验结果表明，新模型在准确性和效率上均优于基于模型投影的传统耗时方法。此外，研究还展示了使用检测到的电极进行虚拟心电图模拟的有效性，突显其在创建准确高效的Cardiac数字双胞胎模型方面的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Cardiac数字双胞胎（CDTs）能够基于多尺度属性对心脏机制进行推理，创建个性化的虚拟心脏表示。</li>
<li>当前研究在心电图电极定位上依赖额外的躯干成像和手动&#x2F;半自动方法。</li>
<li>提出一种新型拓扑信息模型，能全自动从临床标准的二维心脏MRI图像提取个性化心电图标准电极位置。</li>
<li>通过结合稀疏的躯干轮廓和12导联心电图电极的定位，解决因心脏MRI主要关注心脏成像导致的躯干几何结构不完整问题。</li>
<li>新模型在准确性和效率上优于传统的模型投影方法。</li>
<li>检测到的心电图电极用于有效的虚拟心电图模拟，突显其在创建Cardic数字双胞胎模型方面的潜力。</li>
<li>相关代码已公开，可供进一步研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13945">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2408.13945v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2408.13945v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2408.13945v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2408.13945v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2408.13945v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2408.13945v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Argus-Benchmarking-and-Enhancing-Vision-Language-Models-for-3D-Radiology-Report-Generation"><a href="#Argus-Benchmarking-and-Enhancing-Vision-Language-Models-for-3D-Radiology-Report-Generation" class="headerlink" title="Argus: Benchmarking and Enhancing Vision-Language Models for 3D   Radiology Report Generation"></a>Argus: Benchmarking and Enhancing Vision-Language Models for 3D   Radiology Report Generation</h2><p><strong>Authors:Che Liu, Zhongwei Wan, Yuqi Wang, Hui Shen, Haozhe Wang, Kangyu Zheng, Mi Zhang, Rossella Arcucci</strong></p>
<p>Automatic radiology report generation holds significant potential to streamline the labor-intensive process of report writing by radiologists, particularly for 3D radiographs such as CT scans. While CT scans are critical for clinical diagnostics, they remain less explored compared to 2D radiographs. To date, there has been no comprehensive benchmark for 3D radiograph report generation (3DRRG), nor sufficient investigation into the optimal training strategies for Vision Language Models (VLMs) in this context, particularly with respect to vision encoder choices, visual token compression, and model scaling. In this work, we make three key contributions. We curate <strong>CT-3DRRG</strong>, the largest <strong>publicly</strong> available 3D CT-report dataset, establishing a robust and diverse benchmark for evaluating VLM performance on 3DRRG. Furthermore, we propose a comprehensive training recipe for building high-performing VLMs for 3DRRG, exploring key factors such as vision encoder pretraining strategies, visual token compression, and the impact of data &amp; model scale. Guided by these findings, we introduce <strong>Argus</strong>, a state-of-the-art family of VLMs that achieve superior performance across different model sizes and input 3D medical image resolutions, efficiently processing high-resolution 3D images up to $512 \times 512 \times 256$[^1]. </p>
<blockquote>
<p>自动放射学报告生成在简化放射科医生报告写作这一劳动密集型流程方面具有巨大潜力，特别是在对三维射影像（如CT扫描）上尤为如此。虽然CT扫描在临床诊断中至关重要，但与二维放射影像相比，它们的研究仍然较少。迄今为止，尚无针对三维放射影像报告生成（3DRRG）的综合基准测试，对于该背景下的视觉语言模型（VLM）的最佳训练策略的研究也还不够充分，尤其是在视觉编码器选择、视觉令牌压缩和模型缩放方面。在这项工作中，我们做出了三项关键贡献。我们整理推出了最大的公开可用三维CT报告数据集CT-3DRRG，为评估VLM在3DRRG上的性能建立了稳健且多样化的基准测试。此外，我们为构建高性能的用于3DRRG的VLM提出了一种全面的训练方案，探讨了视觉编码器预训练策略、视觉令牌压缩以及数据和模型规模的影响等关键因素。根据这些发现，我们推出了最先进的VLM家族产品Argus，在不同模型尺寸和输入的三维医学影像分辨率上均表现出卓越性能，能够高效处理高达512×512×256分辨率的三维高解析度图像[^1]。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07146v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动放射学报告生成在简化放射科医生报告写作流程方面的潜力，特别是在三维放射影像（如CT扫描）方面。文章提出了CT-3DRRG数据集，为评估VLM在三维放射影像报告生成中的性能提供了稳健且多样化的基准。同时，文章还提出了一种构建高性能VLM的综合训练策略，并介绍了Argus系列模型，该模型在不同模型尺寸和输入的三维医学图像分辨率上实现卓越性能，能够高效处理高达$512 \times 512 \times 256$的高分辨率三维图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动放射学报告生成具有简化放射科医生报告写作流程的潜力，特别是在三维放射影像如CT扫描方面。</li>
<li>提出了CT-3DRRG数据集，为评估VLM在三维放射影像报告生成中的性能提供了基准。</li>
<li>探讨了构建高性能VLM的综合训练策略，包括视觉编码器预训练策略、视觉符号压缩和数据与模型规模的影响。</li>
<li>介绍了Argus系列模型，实现了在不同模型尺寸和输入的三维医学图像分辨率上的卓越性能。</li>
<li>Argus模型能够高效处理高至$512 \times 512 \times 256$分辨率的三维图像。</li>
<li>文章填补了关于三维放射影像报告生成的基准和VLM训练策略研究的空白。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07146">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2406.07146v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Exploring-Quasi-Global-Solutions-to-Compound-Lens-Based-Computational-Imaging-Systems"><a href="#Exploring-Quasi-Global-Solutions-to-Compound-Lens-Based-Computational-Imaging-Systems" class="headerlink" title="Exploring Quasi-Global Solutions to Compound Lens Based Computational   Imaging Systems"></a>Exploring Quasi-Global Solutions to Compound Lens Based Computational   Imaging Systems</h2><p><strong>Authors:Yao Gao, Qi Jiang, Shaohua Gao, Lei Sun, Kailun Yang, Kaiwei Wang</strong></p>
<p>Recently, joint design approaches that simultaneously optimize optical systems and downstream algorithms through data-driven learning have demonstrated superior performance over traditional separate design approaches. However, current joint design approaches heavily rely on the manual identification of initial lenses, posing challenges and limitations, particularly for compound lens systems with multiple potential starting points. In this work, we present Quasi-Global Search Optics (QGSO) to automatically design compound lens based computational imaging systems through two parts: (i) Fused Optimization Method for Automatic Optical Design (OptiFusion), which searches for diverse initial optical systems under certain design specifications; and (ii) Efficient Physic-aware Joint Optimization (EPJO), which conducts parallel joint optimization of initial optical systems and image reconstruction networks with the consideration of physical constraints, culminating in the selection of the optimal solution in all search results. Extensive experimental results illustrate that QGSO serves as a transformative end-to-end lens design paradigm for superior global search ability, which automatically provides compound lens based computational imaging systems with higher imaging quality compared to existing paradigms. The source code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/LiGpy/QGSO">https://github.com/LiGpy/QGSO</a>. </p>
<blockquote>
<p>最近，通过数据驱动学习同时优化光学系统和下游算法的联合设计方法，已显示出其相较于传统分离设计方法的卓越性能。然而，当前的联合设计方法严重依赖于初始镜头的手动识别，这带来了挑战和局限性，特别是对于具有多个潜在起始点的复合镜头系统。在这项工作中，我们提出了准全局搜索光学（QGSO）方法，通过以下两个部分自动设计基于复合镜头的计算成像系统：（i）自动光学设计的融合优化方法（OptiFusion），它会在特定的设计规格下搜索多样化的初始光学系统；（ii）高效的物理感知联合优化（EPJO），它在考虑物理约束的同时，对初始光学系统和图像重建网络进行并行联合优化，最终从所有搜索结果中选择最佳解决方案。大量的实验结果证明，QGSO作为一种变革性的端到端镜头设计范式，具有卓越的全局搜索能力，能够自动为基于复合镜头的计算成像系统提供更高的成像质量，相较于现有的方法有着显著的优势。源代码将在<a target="_blank" rel="noopener" href="https://github.com/LiGpy/QGSO%E4%B8%8A%E5%BC%BA%E5%85%AC%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/LiGpy/QGSO上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19201v2">PDF</a> Accepted to IEEE Transactions on Computational Imaging (TCI). The   source code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/LiGpy/QGSO">https://github.com/LiGpy/QGSO</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Quasi-Global Search Optics（QGSO）的联合设计方法，该方法可自动设计复合透镜计算成像系统，通过搜索多种初始光学系统并进行优化，实现全局最优解，从而提高成像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前联合设计方法通过数据驱动学习同时优化光学系统和下游算法，表现出卓越性能。</li>
<li>传统单独设计方法存在挑战和局限性，特别是在具有多个潜在起点的复合透镜系统中。</li>
<li>QGSO通过两部分实现自动设计复合透镜计算成像系统：Fused Optimization Method for Automatic Optical Design（OptiFusion）和Efficient Physic-aware Joint Optimization（EPJO）。</li>
<li>OptiFusion在特定设计规格下搜索各种初始光学系统。</li>
<li>EPJO考虑物理约束，对初始光学系统和图像重建网络进行并行联合优化，从所有搜索结果中选择最佳解决方案。</li>
<li>QGSO具有卓越的全局搜索能力，能自动提供具有较高成像质量的复合透镜计算成像系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.19201">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2404.19201v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2404.19201v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2404.19201v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="T3D-Advancing-3D-Medical-Vision-Language-Pre-training-by-Learning-Multi-View-Visual-Consistency"><a href="#T3D-Advancing-3D-Medical-Vision-Language-Pre-training-by-Learning-Multi-View-Visual-Consistency" class="headerlink" title="T3D: Advancing 3D Medical Vision-Language Pre-training by Learning   Multi-View Visual Consistency"></a>T3D: Advancing 3D Medical Vision-Language Pre-training by Learning   Multi-View Visual Consistency</h2><p><strong>Authors:Che Liu, Cheng Ouyang, Yinda Chen, Cesar César Quilodrán-Casas, Lei Ma, Jie Fu, Yike Guo, Anand Shah, Wenjia Bai, Rossella Arcucci</strong></p>
<p>While 3D visual self-supervised learning (vSSL) shows promising results in capturing visual representations, it overlooks the clinical knowledge from radiology reports. Meanwhile, 3D medical vision-language pre-training (MedVLP) remains underexplored due to the lack of a large-scale, publicly available 3D medical image-report dataset. To bridge this gap, we introduce <strong>CT-3DVLP</strong>, the first and largest <strong>public</strong> 3D volume-report dataset, establishing a comprehensive benchmark for 3D MedVLP research. Meanwhile, we propose the <strong>T3D</strong> framework, which enhances 3D MedVLP beyond naive CLIP-style alignment that directly pairs volumes with reports but neglects local visual representations. Instead, we introduce <strong>Text-informed Multi-view Alignment (TMA)</strong>, a novel approach that clusters volumetric data while enforcing consistency across different views of the same volume-report pair. TMA integrates textual features into fine-grained visual representations, ensuring contextual coherence across views. We evaluate T3D across multiple downstream tasks in both unimodal and cross-modal settings, including zero-shot and fine-tuned classification, cross-modal retrieval, report generation, and semantic segmentation. Our results show that T3D consistently outperforms existing vSSL and multimodal methods, demonstrating superior zero-shot and fine-tuning capabilities and setting a new benchmark for 3D medical image understanding. </p>
<blockquote>
<p>虽然三维视觉自监督学习（vSSL）在捕捉视觉表示方面显示出有前景的结果，但它忽略了来自放射学报告的医学知识。同时，由于缺乏大规模、公开的3D医学图像报告数据集，3D医学视觉语言预训练（MedVLP）仍被较少探索。为了弥补这一空白，我们引入了<strong>CT-3DVLP</strong>，这是首个最大的<strong>公开</strong>3D体积报告数据集，为3D MedVLP研究建立了综合基准测试。同时，我们提出了<strong>T3D</strong>框架，它超越了简单的CLIP风格对齐方式，直接配对体积与报告，但忽略了局部视觉表示。相反，我们引入了<strong>文本信息多视图对齐（TMA）</strong>，这是一种新方法，它聚类体积数据，同时强制执行同一体积报告对的不同视图之间的一致性。TMA将文本特征集成到精细的视觉表示中，确保跨视图的上下文连贯性。我们在多种下游任务中评估了T3D，包括单模态和跨模态设置下的零样本和微调分类、跨模态检索、报告生成和语义分割。我们的结果表明，T3D在现有vSSL和多模态方法上表现更优越，展现出出色的零样本和微调能力，并为三维医学图像理解设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01529v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了CT-3DVLP数据集以及基于此数据集的T3D框架的研究与应用。CT-3DVLP是首个最大的公开3D医学图像报告数据集，为3D MedVLP研究提供了全面的基准测试平台。同时，提出的T3D框架通过引入文本信息多视角对齐（TMA）技术，优化了单纯的CLIP风格对齐方法，确保不同视角之间的视觉表征具有上下文连贯性。在多下游任务评估中，T3D展现出出色的零样本和微调能力，为3D医学图像理解设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了CT-3DVLP数据集作为首个公开的3D医学图像报告数据集的重要性。</li>
<li>提出了T3D框架来优化现有的视觉自监督学习和多模态学习方法。</li>
<li>TMA技术能够整合文本特征到精细的视觉表征中，确保不同视角之间的上下文连贯性。</li>
<li>T3D框架在多下游任务中展现出卓越性能，包括零样本和微调分类、跨模态检索、报告生成和语义分割等任务。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01529">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2312.01529v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2312.01529v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2312.01529v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2312.01529v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SCALES-Boost-Binary-Neural-Network-for-Image-Super-Resolution-with-Efficient-Scalings"><a href="#SCALES-Boost-Binary-Neural-Network-for-Image-Super-Resolution-with-Efficient-Scalings" class="headerlink" title="SCALES: Boost Binary Neural Network for Image Super-Resolution with   Efficient Scalings"></a>SCALES: Boost Binary Neural Network for Image Super-Resolution with   Efficient Scalings</h2><p><strong>Authors:Renjie Wei, Zechun Liu, Yuchen Fan, Runsheng Wang, Ru Huang, Meng Li</strong></p>
<p>Deep neural networks for image super-resolution (SR) have demonstrated superior performance. However, the large memory and computation consumption hinders their deployment on resource-constrained devices. Binary neural networks (BNNs), which quantize the floating point weights and activations to 1-bit can significantly reduce the cost. Although BNNs for image classification have made great progress these days, existing BNNs for SR still suffer from a large performance gap between the FP SR networks. To this end, we observe the activation distribution in SR networks and find much larger pixel-to-pixel, channel-to-channel, layer-to-layer, and image-to-image variation in the activation distribution than image classification networks. However, existing BNNs for SR fail to capture these variations that contain rich information for image reconstruction, leading to inferior performance. To address this problem, we propose SCALES, a binarization method for SR networks that consists of the layer-wise scaling factor, the spatial re-scaling method, and the channel-wise re-scaling method, capturing the layer-wise, pixel-wise, and channel-wise variations efficiently in an input-dependent manner. We evaluate our method across different network architectures and datasets. For CNN-based SR networks, our binarization method SCALES outperforms the prior art method by 0.2dB with fewer parameters and operations. With SCALES, we achieve the first accurate binary Transformer-based SR network, improving PSNR by more than 1dB compared to the baseline method. </p>
<blockquote>
<p>深度神经网络在图像超分辨率（SR）方面的表现已经展现出卓越的性能。然而，其大量的内存和计算消耗阻碍了其在资源受限设备上的部署。二进制神经网络（BNNs）可以将浮点权重和激活值量化为1位，从而显著降低成本。尽管最近用于图像分类的BNNs取得了很大进展，但现有的用于SR的BNNs与FP SR网络之间仍存在较大的性能差距。为此，我们观察SR网络中的激活分布，发现与图像分类网络相比，激活分布在像素到像素、通道到通道、层到层和图像到图像之间的变化更大。然而，现有的用于SR的BNNs无法捕获这些变化，这些变化包含丰富的图像重建信息，从而导致性能下降。为了解决这一问题，我们提出了SCALES，这是一种针对SR网络的二值化方法，它包括逐层缩放因子、空间重新缩放方法和通道重新缩放方法，以输入依赖的方式高效捕获逐层、像素级和通道级的变异。我们在不同的网络架构和数据集上评估了我们的方法。对于基于CNN的SR网络，我们的二值化方法SCALES在较少参数和运算的情况下，比现有技术方法高出0.2dB。借助SCALES，我们实现了首个准确的基于二进制Transformer的SR网络，与基准方法相比，PSNR提高了超过1dB。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.12270v2">PDF</a> Accpeted by DATE 2025</p>
<p><strong>Summary</strong><br>     深度神经网络在图像超分辨率（SR）方面的性能卓越，但内存和计算消耗大，难以在资源受限的设备上部署。二进制神经网络（BNNs）可以将浮点权重和激活量化到1位，显著降低成本。尽管BNNs在图像分类方面取得了很大进展，但现有的SR-BNNs与浮点SR网络之间仍存在较大性能差距。为解决此问题，我们观察到SR网络中激活分布的更大变化，并发现现有SR-BNNs无法捕获这些变化，导致性能不佳。为此，我们提出了针对SR网络的二进制化方法—— SCALES，包括逐层缩放因子、空间重新缩放方法和通道重新缩放方法，以输入相关的方式高效捕获逐层、像素级和通道级的差异。我们的方法在不同的网络架构和数据集上进行了评估，对于基于CNN的SR网络，我们的二进制化方法优于现有技术0.2dB，并且参数和运算更少。借助 SCALES，我们实现了首个准确的二进制Transformer-based SR网络，与基线方法相比提高了超过1dB的PSNR。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络在图像超分辨率方面的性能出色，但资源消耗大。</li>
<li>二进制神经网络（BNNs）能够显著降低深度神经网络的内存和计算成本。</li>
<li>在SR网络中，激活分布的变化比图像分类网络更大。</li>
<li>现有SR-BNNs无法捕获这些变化丰富的信息用于图像重建，导致性能不佳。</li>
<li>提出了一种新的针对SR网络的二进制化方法—— SCALES。</li>
<li>SCALES方法包括逐层缩放因子、空间重新缩放方法和通道重新缩放方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2303.12270">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_医学图像/2303.12270v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/Interactive/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Interactive/2409.05860v2/page_1_0.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-02-27  Nonlinear Gravitational Radiation Reaction Failed Tail, Memories &   Squares
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_0_0.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-27  LDGen Enhancing Text-to-Image Synthesis via Large Language Model-Driven   Language Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">12809.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
