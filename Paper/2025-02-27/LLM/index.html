<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-27  LLM-Based Design Pattern Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f77f7318064117728c2f612e77128504.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-27-æ›´æ–°"><a href="#2025-02-27-æ›´æ–°" class="headerlink" title="2025-02-27 æ›´æ–°"></a>2025-02-27 æ›´æ–°</h1><h2 id="LLM-Based-Design-Pattern-Detection"><a href="#LLM-Based-Design-Pattern-Detection" class="headerlink" title="LLM-Based Design Pattern Detection"></a>LLM-Based Design Pattern Detection</h2><p><strong>Authors:Christian Schindler, Andreas Rausch</strong></p>
<p>Detecting design pattern instances in unfamiliar codebases remains a challenging yet essential task for improving software quality and maintainability. Traditional static analysis tools often struggle with the complexity, variability, and lack of explicit annotations that characterize real-world pattern implementations. In this paper, we present a novel approach leveraging Large Language Models to automatically identify design pattern instances across diverse codebases. Our method focuses on recognizing the roles classes play within the pattern instances. By providing clearer insights into software structure and intent, this research aims to support developers, improve comprehension, and streamline tasks such as refactoring, maintenance, and adherence to best practices. </p>
<blockquote>
<p>åœ¨é™Œç”Ÿçš„ä»£ç åº“ä¸­æ£€æµ‹è®¾è®¡æ¨¡å¼å®ä¾‹ä»ç„¶æ˜¯æé«˜è½¯ä»¶è´¨é‡å’Œå¯ç»´æŠ¤æ€§çš„é‡è¦ä»»åŠ¡ï¼Œä¸”å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„é™æ€åˆ†æå·¥å…·å¾€å¾€éš¾ä»¥åº”å¯¹ç°å®ä¸–ç•Œæ¨¡å¼å®ç°æ‰€ç‰¹æœ‰çš„å¤æ‚æ€§ã€å¤šå˜æ€§å’Œç¼ºä¹æ˜ç¡®æ³¨é‡Šçš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨è¯†åˆ«ä¸åŒä»£ç åº“ä¸­çš„è®¾è®¡æ¨¡å¼å®ä¾‹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºè¯†åˆ«æ¨¡å¼å®ä¾‹ä¸­ç±»æ‰€æ‰®æ¼”çš„è§’è‰²ã€‚é€šè¿‡æ›´æ¸…æ™°åœ°æ­ç¤ºè½¯ä»¶ç»“æ„å’Œæ„å›¾ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ”¯æŒå¼€å‘äººå‘˜ã€æé«˜ç†è§£åŠ›å¹¶ç®€åŒ–ä»»åŠ¡ï¼Œå¦‚é‡æ„ã€ç»´æŠ¤å’Œéµå¾ªæœ€ä½³å®è·µç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18458v1">PDF</a> Submitted Version, that was accepted at PATTERNS 2025</p>
<p><strong>Summary</strong><br>åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­ï¼Œæ£€æµ‹æœªçŸ¥ä»£ç åº“ä¸­çš„è®¾è®¡æ¨¡å¼å®ä¾‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜ä½†è‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·å¸¸å› çœŸå®ä¸–ç•Œæ¨¡å¼å®ç°çš„å¤æ‚æ€§ã€å¤šå˜æ€§å’Œç¼ºä¹æ˜ç¡®æ³¨é‡Šè€Œæ„Ÿåˆ°å›°æ‰°ã€‚æœ¬ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯†åˆ«å¤šç§ä»£ç åº“ä¸­è®¾è®¡æ¨¡å¼å®ä¾‹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¾§é‡äºè¯†åˆ«æ¨¡å¼å®ä¾‹ä¸­ç±»çš„è§’è‰²ï¼Œæ—¨åœ¨ä¸ºå¼€å‘è€…æä¾›æ›´æ¸…æ™°çš„è½¯ä»¶ç»“æ„å’Œæ„å›¾æ´å¯Ÿï¼Œä»¥æ”¯æŒå¼€å‘ã€æé«˜ç†è§£ï¼Œå¹¶ç®€åŒ–å¦‚é‡æ„ã€ç»´æŠ¤å’Œéµå¾ªæœ€ä½³å®è·µç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ£€æµ‹è®¾è®¡æ¨¡å¼ä¸­å®ä¾‹åœ¨æœªçŸ¥ä»£ç åº“ä¸­æ˜¯ä¸€ä¸ªé‡è¦ä¸”å…·æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·åœ¨é¢å¯¹çœŸå®ä¸–ç•Œæ¨¡å¼å®ç°æ—¶ï¼Œå¸¸å¸¸å—åˆ°å¤æ‚æ€§ã€å¤šå˜æ€§å’Œæ³¨é‡Šä¸è¶³çš„å½±å“ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥è‡ªåŠ¨è¯†åˆ«å¤šç§ä»£ç åº“ä¸­çš„è®¾è®¡æ¨¡å¼å®ä¾‹ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•ä¾§é‡äºè¯†åˆ«æ¨¡å¼å®ä¾‹ä¸­ç±»çš„è§’è‰²ã€‚</li>
<li>è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡æä¾›æ›´æ¸…æ™°çš„è½¯ä»¶ç»“æ„å’Œæ„å›¾æ´å¯Ÿæ¥æ”¯æŒå¼€å‘è€…ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæ”¹è¿›è½¯ä»¶è´¨é‡å’Œå¯ç»´æŠ¤æ€§ï¼Œå¹¶ç®€åŒ–å¦‚é‡æ„ã€ç»´æŠ¤å’Œéµå¾ªæœ€ä½³å®è·µç­‰ä»»åŠ¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55a56d415929b4cb9bb3065393724b8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88c2f8a219bd626a5c058a52b54238a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bf32c28c5f286feefa50fc6b7fd3f89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5e7139f212c2e69f305ea0ef0481e01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcf09aa50ac387eac5971055ff2ea72a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a6b3cd7a48cf6a4b60cbc5aae91316.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SWE-RL-Advancing-LLM-Reasoning-via-Reinforcement-Learning-on-Open-Software-Evolution"><a href="#SWE-RL-Advancing-LLM-Reasoning-via-Reinforcement-Learning-on-Open-Software-Evolution" class="headerlink" title="SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open   Software Evolution"></a>SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open   Software Evolution</h2><p><strong>Authors:Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I. Wang</strong></p>
<p>The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developerâ€™s reasoning processes and solutions by learning from extensive open-source software evolution data â€“ the record of a softwareâ€™s entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified â€“ a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (&lt;100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data. </p>
<blockquote>
<p>æœ€è¿‘çš„DeepSeek-R1ç‰ˆæœ¬å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶DeepSeek-R1å’Œå…¶ä»–åç»­å·¥ä½œä¸»è¦å…³æ³¨å°†RLåº”ç”¨äºç«èµ›ç¼–ç å’Œæ•°å­¦é—®é¢˜ï¼Œä½†æœ¬æ–‡ä»‹ç»äº†SWE-RLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†RLä¸ºåŸºç¡€çš„LLMæ¨ç†æ‰©å±•åˆ°ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹çš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨åŸºäºè½»é‡çº§è§„åˆ™çš„å¥–åŠ±ï¼ˆä¾‹å¦‚ï¼ŒçœŸå®ç­”æ¡ˆå’ŒLLMç”Ÿæˆè§£å†³æ–¹æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ï¼‰ï¼ŒSWE-RLèƒ½å¤Ÿè®©LLMé€šè¿‡å­¦ä¹ å¤§é‡çš„å¼€æºè½¯ä»¶è¿›åŒ–æ•°æ®ï¼Œæ¥è‡ªä¸»æ¢å¤å¼€å‘è€…çš„æ¨ç†è¿‡ç¨‹è§£å†³æ–¹æ¡ˆâ€”â€”è½¯ä»¶æ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„è®°å½•ï¼ŒåŒ…æ‹¬å…¶ä»£ç å¿«ç…§ã€ä»£ç æ›´æ”¹å’Œäº‹ä»¶å¦‚é—®é¢˜å’Œæ‹‰å–è¯·æ±‚ã€‚æˆ‘ä»¬çš„æ¨ç†æ¨¡å‹æ˜¯å»ºç«‹åœ¨Llama 3ä¹‹ä¸Šçš„ï¼Œåä¸ºLlama3-SWE-RL-70Bï¼Œåœ¨SWE-bench Verifiedï¼ˆä¸€ä¸ªç»è¿‡äººå·¥éªŒè¯çš„GitHubå®é™…é—®é¢˜é›†åˆï¼‰ä¸Šè¾¾åˆ°äº†41.0%çš„è§£å†³ç‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä¸­ç­‰è§„æ¨¡ï¼ˆ&lt;100Bï¼‰LLMçš„æœ€ä½³æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³å¯ä¸é¢†å…ˆçš„ä¸“æœ‰LLMå¦‚GPT-4oç›¸åª²ç¾ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡åªåœ¨è½¯ä»¶è¿›åŒ–æ•°æ®ä¸Šæ‰§è¡ŒRLï¼ŒLlama3-SWE-RLè¿˜å±•ç°å‡ºäº†é€šç”¨çš„æ¨ç†æŠ€èƒ½ã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨äº”ä¸ªè·¨åŸŸä»»åŠ¡ä¸­å–å¾—äº†æ›´å¥½çš„ç»“æœï¼Œå³åŠŸèƒ½ç¼–ç ã€åº“çš„ä½¿ç”¨ã€ä»£ç æ¨ç†ã€æ•°å­¦å’Œä¸€èˆ¬è¯­è¨€ç†è§£ï¼Œè€ŒåŸºäºç›‘ç£å¾®è°ƒçš„æ–¹æ³•ç”šè‡³ä¼šå¯¼è‡´å¹³å‡æ€§èƒ½ä¸‹é™ã€‚æ€»çš„æ¥è¯´ï¼ŒSWE-RLä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨å¤§é‡è½¯ä»¶å·¥ç¨‹æ•°æ®ä¸Šæé«˜LLMçš„æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†ä¸€ä¸ªæ–°æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1åŠåç»­å·¥ä½œçš„æˆæœå‡¸æ˜¾äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è€Œæœ¬æ–‡ä»‹ç»çš„SWE-RLæ˜¯é¦–ä¸ªå°†RLåº”ç”¨äºçœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹çš„LLMæ¨ç†æ–¹æ³•ã€‚é€šè¿‡åŸºäºè½»é‡çº§è§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼ŒSWE-RLä½¿LLMsèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ å¼€å‘è€…æ¨ç†è¿‡ç¨‹åŠè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä»å¤§é‡å¼€æºè½¯ä»¶è¿›åŒ–æ•°æ®ä¸­å­¦ä¹ ã€‚Llama3-SWE-RLæ¨¡å‹åœ¨SWE-bench Verifiedä¸Šçš„è§£å†³ç‡é«˜è¾¾41.0%ï¼Œå±•ç°äº†å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜è¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨éé¢†åŸŸä»»åŠ¡ä¸­ä¹Ÿæœ‰ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>SWE-RLæ˜¯é¦–ä¸ªå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºçœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹çš„LLMæ¨ç†æ–¹æ³•ã€‚</li>
<li>SWE-RLä½¿ç”¨åŸºäºè½»é‡çº§è§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿LLMsèƒ½å¤Ÿå­¦ä¹ å¼€å‘è€…æ¨ç†è¿‡ç¨‹åŠè§£å†³æ–¹æ¡ˆã€‚</li>
<li>Llama3-SWE-RLæ¨¡å‹åœ¨SWE-bench Verifiedä¸Šçš„è§£å†³ç‡è¾¾åˆ°41.0%ï¼Œè¡¨ç°å“è¶Šã€‚</li>
<li>Llama3-SWE-RLæ¨¡å‹å±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œåœ¨éé¢†åŸŸä»»åŠ¡ä¸­ä¹Ÿæœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨å¤§é‡è½¯ä»¶å·¥ç¨‹æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72c5d2f4f5f18807964e28761b373c05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-201bd357539b45c6c33228c1cd922117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e6270e8a3ec843350ae21dfcdd11cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7083a5257f5994c9f162a80b72276fc0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GLEAN-Generalized-Category-Discovery-with-Diverse-and-Quality-Enhanced-LLM-Feedback"><a href="#GLEAN-Generalized-Category-Discovery-with-Diverse-and-Quality-Enhanced-LLM-Feedback" class="headerlink" title="GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced   LLM Feedback"></a>GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced   LLM Feedback</h2><p><strong>Authors:Henry Peng Zou, Siffi Singh, Yi Nian, Jianfeng He, Jason Cai, Saab Mansour, Hang Su</strong></p>
<p>Generalized Category Discovery (GCD) is a practical and challenging open-world task that aims to recognize both known and novel categories in unlabeled data using limited labeled data from known categories. Due to the lack of supervision, previous GCD methods face significant challenges, such as difficulty in rectifying errors for confusing instances, and inability to effectively uncover and leverage the semantic meanings of discovered clusters. Therefore, additional annotations are usually required for real-world applicability. However, human annotation is extremely costly and inefficient. To address these issues, we propose GLEAN, a unified framework for generalized category discovery that actively learns from diverse and quality-enhanced LLM feedback. Our approach leverages three different types of LLM feedback to: (1) improve instance-level contrastive features, (2) generate category descriptions, and (3) align uncertain instances with LLM-selected category descriptions. Extensive experiments demonstrate the superior performance of \MethodName over state-of-the-art models across diverse datasets, metrics, and supervision settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/amazon-science/Glean">https://github.com/amazon-science/Glean</a>. </p>
<blockquote>
<p>å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆGCDï¼‰æ˜¯ä¸€é¡¹å®ç”¨ä¸”å…·æŒ‘æˆ˜æ€§çš„å¼€æ”¾ä¸–ç•Œä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿ç”¨æ¥è‡ªå·²çŸ¥ç±»åˆ«çš„æœ‰é™æ ‡è®°æ•°æ®æ¥è¯†åˆ«æ— æ ‡è®°æ•°æ®ä¸­çš„å·²çŸ¥å’Œæ–°å‹ç±»åˆ«ã€‚ç”±äºç¼ºä¹ç›‘ç£ï¼Œä»¥å‰çš„GCDæ–¹æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¾‹å¦‚çº æ­£é”™è¯¯å®ä¾‹çš„å›°éš¾ï¼Œä»¥åŠæ— æ³•æœ‰æ•ˆå‘ç°å’Œåˆ©ç”¨å‘ç°é›†ç¾¤çš„è¯­ä¹‰å«ä¹‰ã€‚å› æ­¤ï¼Œé€šå¸¸éœ€è¦å¯¹ç°å®ä¸–ç•Œçš„åº”ç”¨è¿›è¡Œé¢å¤–çš„æ³¨é‡Šã€‚ç„¶è€Œï¼Œäººå·¥æ³¨é‡Šæˆæœ¬æé«˜ä¸”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GLEANï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¹¿ä¹‰ç±»åˆ«å‘ç°çš„ç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿä¸»åŠ¨ä»å¤šæ ·åŒ–å’Œè´¨é‡æé«˜çš„LLMåé¦ˆä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸‰ç§ä¸åŒç±»å‹çš„LLMåé¦ˆæ¥æ”¹å–„ï¼šï¼ˆ1ï¼‰å®ä¾‹çº§å¯¹æ¯”ç‰¹å¾ï¼Œï¼ˆ2ï¼‰ç”Ÿæˆç±»åˆ«æè¿°ï¼Œä»¥åŠï¼ˆ3ï¼‰å°†ä¸ç¡®å®šçš„å®ä¾‹ä¸LLMé€‰æ‹©çš„ç±»åˆ«æè¿°å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ\MethodNameåœ¨å¤šç§æ•°æ®é›†ã€æŒ‡æ ‡å’Œç›‘ç£è®¾ç½®ä¸Šçš„æ€§èƒ½å‡ä¼˜äºæœ€æ–°æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/amazon-science/Glean%E3%80%82">https://github.com/amazon-science/Gleanã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GCDä»»åŠ¡æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„å·²çŸ¥ç±»åˆ«æ ‡ç­¾æ•°æ®ï¼Œåœ¨æœªè¢«æ ‡ç­¾çš„æ•°æ®ä¸­è¯†åˆ«å·²çŸ¥å’Œæ–°å‹ç±»åˆ«ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜å¦‚çº é”™å›°éš¾å’Œå‘ç°ä¸åˆ©ç”¨è¯­ä¹‰å«ä¹‰çš„éš¾é¢˜ï¼Œæå‡ºäº†GLEANæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åé¦ˆæ¥ä¸»åŠ¨å­¦ä¹ ï¼Œæ”¹å–„å®ä¾‹çº§åˆ«çš„å¯¹æ¯”ç‰¹å¾ã€ç”Ÿæˆç±»åˆ«æè¿°å¹¶ä¸ä¸ç¡®å®šå®ä¾‹å¯¹é½ã€‚å®éªŒè¯æ˜å…¶åœ¨ä¸åŒæ•°æ®é›†ã€æŒ‡æ ‡å’Œç›‘ç®¡ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GCDæ˜¯ä¸€ä¸ªå®ç”¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾ä¸–ç•Œä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«å·²çŸ¥å’Œæ–°å‹ç±»åˆ«ã€‚</li>
<li>ç°æœ‰GCDæ–¹æ³•é¢ä¸´çº é”™å›°éš¾å’Œå‘ç°è¯­ä¹‰å«ä¹‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†GLEANæ¡†æ¶ï¼Œåˆ©ç”¨LLMåé¦ˆè¿›è¡Œä¸»åŠ¨å­¦ä¹ ã€‚</li>
<li>GLEANé€šè¿‡ä¸‰ç§ä¸åŒç±»å‹çš„LLMåé¦ˆæ¥æ”¹å–„å®ä¾‹çº§åˆ«çš„å¯¹æ¯”ç‰¹å¾ã€ç”Ÿæˆç±»åˆ«æè¿°å¹¶ä¸ä¸ç¡®å®šå®ä¾‹å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜GLEANåœ¨å¤šç§æ•°æ®é›†ã€æŒ‡æ ‡å’Œç›‘ç®¡ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>GLEANä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e2ebf907991aed9393040906f2e7a7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74330cae3ebeee9552ed049a80419541.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-605cb9af59bcf419549009b717469ad5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2bbc5e50879bb7ef641fe7eef6e7e4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9987938380842524d8820bfbc9a62b3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-952bcb2d121716880db41df992e4da8d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OmniAlign-V-Towards-Enhanced-Alignment-of-MLLMs-with-Human-Preference"><a href="#OmniAlign-V-Towards-Enhanced-Alignment-of-MLLMs-with-Human-Preference" class="headerlink" title="OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference"></a>OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference</h2><p><strong>Authors:Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosong Cao, Weiyun Wang, Jiaqi Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, Haodong Duan, Hua Yang, Kai Chen</strong></p>
<p>Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMsâ€™ alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMsâ€™ alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at <a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/OmniAlign-V">https://github.com/PhoenixZ810/OmniAlign-V</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸»è¦é›†ä¸­åœ¨å¢å¼ºåŸºç¡€èƒ½åŠ›ä¸Šï¼Œä½†åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Vï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«20ä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬çš„ç»¼åˆæ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å›¾åƒã€å¤æ‚çš„é—®é¢˜å’Œä¸åŒçš„å“åº”æ ¼å¼ï¼Œæ—¨åœ¨æé«˜MLLMä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„äººç±»æ³¨é‡ŠåŸºå‡†ï¼Œç”¨äºè¯„ä¼°MLLMä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€šè¿‡OmniAlign-Vå¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜äººç±»åå¥½å¯¹é½ç¨‹åº¦ï¼ŒåŒæ—¶åœ¨æ ‡å‡†VQAåŸºå‡†æµ‹è¯•ä¸­ä¿æŒæˆ–æé«˜æ€§èƒ½ï¼Œä¿æŒå…¶åŸºç¡€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/OmniAlign-V">https://github.com/PhoenixZ810/OmniAlign-V</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼€æºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨æé«˜åŸºç¡€èƒ½åŠ›ä¸Šï¼Œåœ¨ç¬¦åˆäººç±»åå¥½æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†OmniAlign-Væ•°æ®é›†ï¼ŒåŒ…å«äºŒåä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ï¼Œæ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§ç­”æ¡ˆæ ¼å¼ï¼Œæ—¨åœ¨æé«˜MLLMsä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†MM-AlignBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±äººç±»æ ‡æ³¨çš„ä¸“é—¨è¯„ä¼°MLLMsä¸äººç±»ä»·å€¼è§‚ä¸€è‡´æ€§çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œèƒ½æ˜¾è‘—æé«˜ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡åœ¨æ ‡å‡†è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œä¿ç•™äº†å…¶åŸºç¡€èƒ½åŠ›ã€‚ç›¸å…³æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/OmniAlign-V%E3%80%82">https://github.com/PhoenixZ810/OmniAlign-Vã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniAlign-Væ•°æ®é›†åŒ…å«äºŒåä¸‡é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤šæ ·åŒ–å›¾åƒã€å¤æ‚é—®é¢˜å’Œå¤šç§ç­”æ¡ˆæ ¼å¼ï¼Œæä¾›å…¨é¢çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>MM-AlignBenchæ˜¯ä¸€ä¸ªä¸“é—¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚ä¸€è‡´æ€§çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒæˆ–ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•å¯ä»¥æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨æé«˜ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒæˆ–æå‡æ¨¡å‹åœ¨æ ‡å‡†è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ç›¸å…³æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºåç»­ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2419f3d12c2d3135529eaa10a409f109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-639b80b54fd4d5d62075121ae97981e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83395193c4ba76e2bb54b32621414183.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1653be6a9f2796fa56686acb2f8a38d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a925ce646e5540336ec220019245b2c8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MindMem-Multimodal-for-Predicting-Advertisement-Memorability-Using-LLMs-and-Deep-Learning"><a href="#MindMem-Multimodal-for-Predicting-Advertisement-Memorability-Using-LLMs-and-Deep-Learning" class="headerlink" title="MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs   and Deep Learning"></a>MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs   and Deep Learning</h2><p><strong>Authors:Sepehr Asgarian, Qayam Jetha, Jouhyun Jeon</strong></p>
<p>In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearmanâ€™s correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market. </p>
<blockquote>
<p>åœ¨å¹¿å‘Šç«äº‰æ¿€çƒˆçš„èƒŒæ™¯ä¸‹ï¼ŒæˆåŠŸå…³é”®åœ¨äºæœ‰æ•ˆé©¾é©­å¹¶åˆ©ç”¨æ¶ˆè´¹è€…ã€å¹¿å‘Šå•†å’Œå¹¿å‘Šå¹³å°ä¹‹é—´çš„å¤æ‚äº’åŠ¨ã€‚è¿™äº›å¤šæ–¹é¢çš„äº’åŠ¨ä¿ƒä½¿å¹¿å‘Šå•†ä¼˜åŒ–ç­–ç•¥ï¼Œå¯¹æ¶ˆè´¹è€…è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€æé«˜å“ç‰Œè®¤çŸ¥åº¦ï¼Œå¹¶é‡èº«å®šåˆ¶å¹¿å‘Šå†…å®¹ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MindMemï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºå¹¿å‘Šè®°å¿†åŠ›çš„å¤šæ¨¡æ€é¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰æ•°æ®ï¼ŒMindMemè¾¾åˆ°äº†å‰æ²¿çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨LAMBDAæ•°æ®é›†ä¸Šçš„æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°è¾¾åˆ°0.631ï¼Œåœ¨Memento10Kæ•°æ®é›†ä¸Šè¾¾åˆ°0.731ï¼ŒæŒç»­è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æç¡®å®šäº†å½±å“å¹¿å‘Šè®°å¿†åŠ›çš„å…³é”®å› ç´ ï¼Œå¦‚è§†é¢‘èŠ‚å¥ã€åœºæ™¯å¤æ‚æ€§å’Œæƒ…æ„Ÿå…±é¸£ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†MindMem-ReAdï¼ˆMindMemé©±åŠ¨å†ç”Ÿå¹¿å‘Šï¼‰ï¼Œé‡‡ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡æ‹Ÿæ¥ä¼˜åŒ–å¹¿å‘Šå†…å®¹å’Œæ”¾ç½®ï¼Œä½¿å¾—å¹¿å‘Šè®°å¿†åŠ›æé«˜äº†é«˜è¾¾74.12%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†äººå·¥æ™ºèƒ½åœ¨å¹¿å‘Šä¸­çš„å˜é©æ½œåŠ›ï¼Œä¸ºå¹¿å‘Šå•†æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥æ¨åŠ¨å‚ä¸æ„Ÿã€æé«˜ç«äº‰åŠ›ï¼Œå¹¶åœ¨å¿«é€Ÿå˜åŒ–çš„å¸‚åœºä¸­æœ€å¤§é™åº¦åœ°å‘æŒ¥ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18371v1">PDF</a> 7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML:   Markets, Incentives, and Generative AI Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¹¿å‘Šç«äº‰æ¿€çƒˆçš„å¸‚åœºç¯å¢ƒä¸­ï¼Œé€šè¿‡æ„å»ºå¤šæ¨¡æ€é¢„æµ‹æ¨¡å‹MindMemæé«˜å¹¿å‘Šè®°å¿†åº¦çš„æŠ€æœ¯ã€‚è¯¥æ¨¡å‹èåˆäº†æ–‡æœ¬ã€è§†è§‰å’Œå¬è§‰æ•°æ®ï¼Œé€šè¿‡ä¼˜åŒ–å¹¿å‘Šå†…å®¹å’ŒæŠ•æ”¾ç­–ç•¥ï¼Œå®ç°å¹¿å‘Šè®°å¿†åº¦çš„æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMindMemåœ¨LAMBDAå’ŒMemento10Kæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åˆ†æç¡®å®šäº†è§†é¢‘èŠ‚å¥ã€åœºæ™¯å¤æ‚æ€§å’Œæƒ…æ„Ÿå…±é¸£ç­‰å…³é”®å½±å“å¹¿å‘Šè®°å¿†åº¦çš„å› ç´ ã€‚æ­¤å¤–ï¼Œæ¨å‡ºçš„MindMem-ReAdé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä¼˜åŒ–å¹¿å‘Šå†…å®¹å’ŒæŠ•æ”¾ï¼Œæé«˜äº†å¹¿å‘Šçš„è®°å¿†åº¦ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†äººå·¥æ™ºèƒ½åœ¨å¹¿å‘Šé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MindMemæ¨¡å‹é€šè¿‡èåˆå¤šæ¨¡æ€æ•°æ®æå‡å¹¿å‘Šè®°å¿†åº¦ã€‚</li>
<li>åœ¨LAMBDAå’ŒMemento10Kæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åˆ†æç¡®å®šäº†è§†é¢‘èŠ‚å¥ã€åœºæ™¯å¤æ‚æ€§å’Œæƒ…æ„Ÿå…±é¸£ç­‰å½±å“å¹¿å‘Šè®°å¿†åº¦çš„å…³é”®å› ç´ ã€‚</li>
<li>MindMem-ReAdé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿä¼˜åŒ–å¹¿å‘Šå†…å®¹å’ŒæŠ•æ”¾ã€‚</li>
<li>MindMem-ReAdå®ç°äº†å¹¿å‘Šè®°å¿†åº¦çš„æ˜¾è‘—æå‡ï¼Œè¾¾åˆ°æœ€é«˜æå‡74.12%ã€‚</li>
<li>è¿™äº›æŠ€æœ¯æˆæœå±•ç¤ºäº†äººå·¥æ™ºèƒ½åœ¨å¹¿å‘Šé¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e50875c5484ed7f4d0b4bdab2d7729a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de859067494b34de9176d9ccdbcbe4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc88cdeb32ab652176a6d46ef9d51f7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72d8e78986c5d248fcd0a11f4bcf834f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d92cea00a1cb4cff490d1fec90c7aef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb85f222e8d56f973e1f1554708b7a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20862ea94f350e7f6aa7381b0b1f0a7c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BRIDO-Bringing-Democratic-Order-to-Abstractive-Summarization"><a href="#BRIDO-Bringing-Democratic-Order-to-Abstractive-Summarization" class="headerlink" title="BRIDO: Bringing Democratic Order to Abstractive Summarization"></a>BRIDO: Bringing Democratic Order to Abstractive Summarization</h2><p><strong>Authors:Junhyun Lee, Harshith Goka, Hyeonmok Ko</strong></p>
<p>Hallucination refers to the inaccurate, irrelevant, and inconsistent text generated from large language models (LLMs). While the LLMs have shown great promise in a variety of tasks, the issue of hallucination still remains a major challenge for many practical uses. In this paper, we tackle the issue of hallucination in abstract text summarization by mitigating exposure bias. Existing models targeted for exposure bias mitigation, namely BRIO, aim for better summarization quality in the ROUGE score. We propose a model that uses a similar exposure bias mitigation strategy but with a goal that is aligned with less hallucination. We conjecture that among a group of candidate outputs, ones with hallucinations will comprise the minority of the whole group. That is, candidates with less similarity with others will have a higher chance of containing hallucinated content. Our method uses this aspect and utilizes contrastive learning, incentivizing candidates with high inter-candidate ROUGE scores. We performed experiments on the XSum and CNN&#x2F;DM summarization datasets, and our method showed 6.25% and 3.82% improvement, respectively, on the consistency G-Eval score over BRIO. </p>
<blockquote>
<p>å¹»è§‰æŒ‡çš„æ˜¯ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ä¸å‡†ç¡®ã€ä¸ç›¸å…³å’Œä¸ä¸€è‡´çš„æ–‡æœ¬ã€‚è™½ç„¶LLMåœ¨å„ç§ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å¹»è§‰é—®é¢˜ä»ç„¶æ˜¯è®¸å¤šå®é™…åº”ç”¨ä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å‡è½»æš´éœ²åè§æ¥è§£å†³æŠ½è±¡æ–‡æœ¬æ‘˜è¦ä¸­çš„å¹»è§‰é—®é¢˜ã€‚é’ˆå¯¹æš´éœ²åè§ç¼“è§£çš„ç°æœ‰æ¨¡å‹ï¼Œå³BRIOï¼Œæ—¨åœ¨æé«˜ROUGEåˆ†æ•°çš„æ‘˜è¦è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨ç±»ä¼¼æš´éœ²åè§ç¼“è§£ç­–ç•¥çš„æ¨¡å‹ï¼Œä½†å…¶ç›®æ ‡æ˜¯ä¸å‡å°‘å¹»è§‰ç›¸ä¸€è‡´ã€‚æˆ‘ä»¬çŒœæƒ³åœ¨ä¸€ç»„å€™é€‰è¾“å‡ºä¸­ï¼Œå¸¦æœ‰å¹»è§‰çš„å°†æ˜¯æ•´ä¸ªå°ç»„ä¸­çš„å°‘æ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸å…¶ä»–å€™é€‰è€…ç›¸ä¼¼åº¦è¾ƒä½çš„å€™é€‰è€…æ›´æœ‰å¯èƒ½åŒ…å«è™šæ„å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¿™ä¸€ç‰¹ç‚¹ï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæ¿€åŠ±å…·æœ‰è¾ƒé«˜å€™é€‰è€…é—´ROUGEåˆ†æ•°çš„å€™é€‰è€…ã€‚æˆ‘ä»¬åœ¨XSumå’ŒCNN&#x2F;DMæ‘˜è¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€è‡´æ€§G-Evalåˆ†æ•°ä¸Šç›¸å¯¹äºBRIOåˆ†åˆ«æé«˜äº†6.25%å’Œ3.82%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18342v1">PDF</a> 13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»æƒ³é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šè¿‡å‡å°‘æš´éœ²åè§æ¥è§£å†³æŠ½è±¡æ–‡æœ¬æ‘˜è¦ä¸­å¹»æƒ³é—®é¢˜çš„æ–¹æ³•ã€‚æ–°æ–¹æ³•ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæ¿€åŠ±é«˜ROUGEå¾—åˆ†çš„å€™é€‰æ‘˜è¦ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†å…¶æé«˜äº†ä¸€è‡´æ€§G-Evalåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è¯¦ç»†é˜è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»æƒ³é—®é¢˜åŠå…¶å¯¹äºå®é™…åº”ç”¨çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>ç°å­˜çš„æ¨¡å‹å¦‚BRIOæ—¨åœ¨é€šè¿‡å‡è½»æš´éœ²åè§æ¥æé«˜æ‘˜è¦è´¨é‡ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹é‡‡ç”¨ç±»ä¼¼çš„ç­–ç•¥ï¼Œä½†ç›®æ ‡æ›´ä¾§é‡äºå‡å°‘å¹»æƒ³ã€‚</li>
<li>ä½œè€…å‡è®¾åœ¨å€™é€‰è¾“å‡ºä¸­ï¼Œå«æœ‰å¹»æƒ³çš„æ–‡æœ¬å°†å å°‘æ•°ã€‚ä¸å…¶ä»–å€™é€‰æ‘˜è¦ç›¸ä¼¼åº¦è¾ƒä½çš„æ–‡æœ¬æ›´å¯èƒ½åŒ…å«å¹»æƒ³å†…å®¹ã€‚</li>
<li>æ–°çš„æ–¹æ³•åˆ©ç”¨è¿™ä¸€è§‚ç‚¹å¹¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæ¿€åŠ±é«˜ROUGEå¾—åˆ†çš„å€™é€‰æ‘˜è¦ã€‚</li>
<li>åœ¨XSumå’ŒCNN&#x2F;DMæ‘˜è¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨ä¸€è‡´æ€§G-Evalåˆ†æ•°ä¸Šç›¸å¯¹äºBRIOæœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f30a24b21765977cbc69129d49a870f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecc959041027dda0ee945d4f4b34aa2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3e7d54fd24dff505ed42771b02dfcc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e93174274cc4c89393177f0b127cc185.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models"><a href="#From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models" class="headerlink" title="From System 1 to System 2: A Survey of Reasoning Large Language Models"></a>From System 1 to System 2: A Survey of Reasoning Large Language Models</h2><p><strong>Authors:Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu</strong></p>
<p>Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAIâ€™s o1&#x2F;o3 and DeepSeekâ€™s R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{<a target="_blank" rel="noopener" href="https://github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub">https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub</a> Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field. </p>
<blockquote>
<p>å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½éœ€è¦å®Œå–„ä»å¿«é€Ÿç›´è§‰ç³»ç»Ÿ1åˆ°è¾ƒæ…¢ã€æ›´æ…é‡çš„ç³»ç»Ÿ2æ¨ç†çš„è¿‡æ¸¡ã€‚ç³»ç»Ÿ1æ“…é•¿å¿«é€Ÿå¯å‘å¼å†³ç­–ï¼Œè€Œç³»ç»Ÿ2åˆ™ä¾èµ–äºé€»è¾‘æ¨ç†ä»¥åšå‡ºæ›´å‡†ç¡®çš„åˆ¤æ–­å’Œå‡å°‘åè§ã€‚åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢æœ‰æ‰€æ¬ ç¼ºï¼Œå› ä¸ºå®ƒä»¬å°šæœªå®Œå…¨æ¥å—ç³»ç»Ÿ2æ€ç»´æ‰€ç‰¹æœ‰çš„é€æ­¥åˆ†ææ­¥éª¤ã€‚æœ€è¿‘ï¼ŒåƒOpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1ç­‰æ¨ç†LLMåœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†ä¸“å®¶çº§çš„æ€§èƒ½ï¼Œå®ƒä»¬æ¨¡ä»¿äº†ç³»ç»Ÿ2çš„æ·±æ€ç†Ÿè™‘æ¨ç†ï¼Œå±•ç¤ºäº†äººç±»èˆ¬çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™ç¯‡ç»¼è¿°é¦–å…ˆç®€è¦æ¦‚è¿°äº†åŸºç¡€LLMå’Œç³»ç»Ÿ2æŠ€æœ¯çš„æ—©æœŸå‘å±•è¿›å±•ï¼Œæ¢è®¨äº†å®ƒä»¬çš„ç»“åˆå¦‚ä½•ä¸ºæ¨ç†LLMé“ºå¹³é“è·¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•æ„å»ºæ¨ç†LLMï¼Œåˆ†æå…¶ç‰¹ç‚¹ï¼Œä»‹ç»å®ç°é«˜çº§æ¨ç†çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä»¥åŠå„ç±»æ¨ç†LLMçš„æ¼”å˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¦‚è¿°äº†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ·±å…¥æ¯”è¾ƒäº†ä»£è¡¨æ€§æ¨ç†LLMçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¨è¿›æ¨ç†LLMçš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¹¶é€šè¿‡å®æ—¶GitHubä»“åº“æ¥è·Ÿè¸ªæœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡ç»¼è¿°èƒ½ä½œä¸ºå®è´µèµ„æºï¼Œæ¿€å‘è¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„åˆ›æ–°å¹¶æ¨åŠ¨å…¶è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17419v2">PDF</a> Slow-thinking, Large Language Models, Human-like Reasoning, Decision   Making in AI, AGI</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å®ç°äººç±»æ°´å¹³æ™ºèƒ½éœ€è¦å®Œå–„çš„ä»å¿«é€Ÿç›´è§‰ç³»ç»Ÿä¸€ï¼ˆSystem 1ï¼‰åˆ°è¾ƒæ…¢ä½†æ›´å®¡æ…çš„ç³»ç»ŸäºŒï¼ˆSystem 2ï¼‰æ¨ç†çš„è½¬å˜ã€‚åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œä½†ç¼ºä¹å¤æ‚æ¨ç†çš„æ·±åº¦ï¼Œå°šæœªå…¨é¢é‡‡ç”¨çœŸæ­£çš„ç³»ç»ŸäºŒé€æ­¥åˆ†æç‰¹æ€§ã€‚æœ€è¿‘ï¼Œå¦‚OpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1ç­‰æ¨ç†LLMå·²åœ¨æ•°å­¦å’Œç¼–ç ç­‰é¢†åŸŸå±•ç°å‡ºä¸“å®¶çº§æ€§èƒ½ï¼Œæ¨¡æ‹Ÿç³»ç»ŸäºŒçš„å®¡æ…æ¨ç†å¹¶å±•ç¤ºäººç±»èˆ¬çš„è®¤çŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡æ¦‚è¿°äº†åŸºç¡€LLMå’Œç³»ç»ŸäºŒæŠ€æœ¯çš„æ—©æœŸå‘å±•ï¼Œæ¢è®¨äº†å®ƒä»¬å¦‚ä½•ç»“åˆä¸ºæ¨ç†LLMé“ºå¹³é“è·¯ï¼Œå¹¶è®¨è®ºäº†å¦‚ä½•æ„å»ºæ¨ç†LLMï¼Œåˆ†æå…¶ç‰¹ç‚¹ã€æ ¸å¿ƒæ–¹æ³•å’Œå„ç§æ¨ç†LLMçš„æ¼”å˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†å¯¹æ¨ç†åŸºå‡†æµ‹è¯•çš„æ¦‚è¿°ï¼Œæ·±å…¥æ¯”è¾ƒäº†ä»£è¡¨æ€§æ¨ç†LLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ç°äººç±»æ°´å¹³æ™ºèƒ½éœ€è¦å®Œå–„ä»å¿«é€Ÿç›´è§‰å†³ç­–åˆ°è¾ƒæ…¢ä½†æ›´å®¡æ…çš„ç³»ç»ŸäºŒæ¨ç†çš„è½¬å˜ã€‚</li>
<li>åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½æ“…é•¿å¿«é€Ÿå†³ç­–ï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢å­˜åœ¨æ·±åº¦ä¸è¶³ã€‚</li>
<li>æ¨ç†LLMå¦‚OpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1å·²åœ¨ç‰¹å®šé¢†åŸŸå±•ç°å‡ºä¸“å®¶çº§æ€§èƒ½ï¼Œæ¨¡æ‹Ÿç³»ç»ŸäºŒçš„å®¡æ…æ¨ç†ã€‚</li>
<li>æ¨ç†LLMçš„æ„å»ºæ¶‰åŠå¯¹æ¨¡å‹ç‰¹æ€§çš„åˆ†æã€æ ¸å¿ƒæ–¹æ³•çš„é‡‡ç”¨ä»¥åŠä¸åŒæ¨ç†LLMçš„æ¼”å˜ã€‚</li>
<li>æ¨ç†åŸºå‡†æµ‹è¯•æä¾›äº†è¯„ä¼°LLMæ€§èƒ½çš„æ ‡å‡†ï¼Œæ·±å…¥æ¯”è¾ƒäº†ä»£è¡¨æ€§æ¨ç†LLMä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>ç»“åˆåŸºç¡€LLMå’Œç³»ç»ŸäºŒæŠ€æœ¯ä¸ºæ¨ç†LLMçš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-910fca5899b2f6202effa76cd13e2189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbb3d1cc08dc9913ec8a49f6d08f8be0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf37d8e00c34d7d4f34a141ea26291c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e277db38961f6e3017eb812bafcfa3e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4386b526a8f491f0885903a09e234e7a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level â€œnovelty.â€ Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. </p>
<blockquote>
<p>æ•°æ®çš„å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤šç§æ•°æ®å¤šæ ·æ€§çš„æ„è¯†æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä»¥æ„å»ºé«˜è´¨é‡æ•°æ®é›†å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„åŸºæœ¬é—®é¢˜ä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æ•°æ®å·¥ç¨‹çš„æ˜ç¡®æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¤§é‡å¾®è°ƒå®éªŒè¯„ä¼°äº†ç°æœ‰1 1ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§åº¦é‡åº”é€‚å½“è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ·æœ¬çº§åˆ«â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumå‡†ç¡®æ•æ‰äº†å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´ªå©ªçš„ã€é¢å‘å¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v2">PDF</a> 15 pages. The related codes and resources will be released later.   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong></p>
<p>æ•°æ®å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§åŸºäºå¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©æ–¹æ³•ä»¥æ„å»ºé«˜è´¨é‡æ•°æ®é›†å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¦‚ä½•ç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„åŸºç¡€é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æ•°æ®å·¥ç¨‹å®è·µçš„æ˜ç¡®æŒ‡å¯¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†ç°æœ‰çš„11ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¤§é‡çš„å¾®è°ƒå®éªŒè¯„ä¼°å®ƒä»¬ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§æµ‹é‡åº”é€‚å½“è€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumå‡†ç¡®æ•æ‰å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´ªå©ªçš„ã€ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¤šæ ·æ€§å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤šç§æ•°æ®é€‰æ‹©æ–¹æ³•ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰çš„æ•°æ®å¤šæ ·æ€§å®šä¹‰å’Œæµ‹é‡æ–¹æ³•ä»ç„¶å¤„äºæœªå……åˆ†æ¢ç´¢é˜¶æ®µï¼Œç¼ºä¹æ˜ç¡®çš„æŒ‡å¯¼åŸåˆ™ç”¨äºæ•°æ®å·¥ç¨‹å®è·µã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°äº†å¤šç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°åº”ç»“åˆè€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚å’Œæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒæ¥å®šä¹‰å¯é çš„å¤šæ ·æ€§æµ‹é‡æ ‡å‡†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ ·æ€§æŒ‡æ ‡NovelSumï¼ŒåŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€ï¼Œèƒ½å¤Ÿå‡†ç¡®æ•æ‰æ•°æ®å¤šæ ·æ€§å˜åŒ–å¹¶ä¸æ¨¡å‹æ€§èƒ½é«˜åº¦ç›¸å…³ã€‚</li>
<li>NovelSumåœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨ç°å‡ºå…¶æœ‰æ•ˆæ€§ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39bd68b06f8cf5b4b6d780976b944e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e13d1819d42fc8227602d59168d40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73708b42657d862618f4d0f17d41eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef13d395163cfa16cb12d2c25d19dd20.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking"><a href="#Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking" class="headerlink" title="Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking"></a>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking</h2><p><strong>Authors:Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</strong></p>
<p>Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2%, and outperforms Transformer&#x2F;Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å‚æ•°çº¦æŸä¸‹é¢ä¸´ç€å›ºæœ‰çš„æ€§èƒ½ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„å…³é”®ä»¤ç‰Œæ—¶ã€‚ç»éªŒåˆ†æè¡¨æ˜ï¼ŒæŒ‘æˆ˜ä»¤ç‰Œä¼šåœ¨å„å±‚ä¹‹é—´å¼•å‘çªç„¶çš„æ¢¯åº¦å³°å€¼ï¼Œæš´éœ²äº†æ ‡å‡†Transformerä¸­çš„æ¶æ„åº”åŠ›ç‚¹ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†å†…æ€è€ƒTransformerï¼ˆITTï¼‰ï¼Œå®ƒé‡æ–°æƒ³è±¡å±‚è®¡ç®—ä¸ºéšå¼æ€è€ƒæ­¥éª¤ã€‚ITTé€šè¿‡è‡ªé€‚åº”ä»¤ç‰Œè·¯ç”±åŠ¨æ€åˆ†é…è®¡ç®—ï¼Œé€šè¿‡æ®‹å·®æ€è€ƒè¿æ¥è¿­ä»£ä¼˜åŒ–è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨æ€è€ƒæ­¥éª¤ç¼–ç æ¥åŒºåˆ†æ¨ç†é˜¶æ®µã€‚ITTèƒ½å¤Ÿåœ¨ä¸å¢åŠ å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå¯¹å…³é”®ä»¤ç‰Œè¿›è¡Œæ›´æ·±å…¥çš„å¤„ç†ã€‚åœ¨1.6äº¿è‡³è¿‘4äº¿çš„å‚æ•°æ¨¡å‹è¯„ä¼°ä¸­ï¼ŒITTå®ç°äº†ä»…ä½¿ç”¨è¿‘ä¸€åŠå‚æ•°ï¼ˆå³è¿‘äº¿å‚æ•°ï¼‰å³å¯è¾¾åˆ°è¿‘ç™¾åˆ†ä¹‹ä¹åå…­çš„æ€§èƒ½è¡¨ç°ï¼Œå‡å°‘äº†ç™¾åˆ†ä¹‹å››åä¸‰çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨åä¸€é¡¹åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºTransformer&#x2F;Loopå˜ä½“ã€‚é€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„å¼¹æ€§è®¡ç®—åˆ†é…ï¼ŒITTé€šè¿‡éšå¼æ€è€ƒè·¯å¾„çš„æ¶æ„ä¼˜åŒ–æ¥å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13842v2">PDF</a> 15 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„å…³é”®ä»¤ç‰Œæ—¶é¢ä¸´æ€§èƒ½ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºInner Thinking Transformerï¼ˆITTï¼‰çš„æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡é‡æ–°æƒ³è±¡å±‚è®¡ç®—ä½œä¸ºéšå¼æ€è€ƒæ­¥éª¤æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ITTé€šè¿‡åŠ¨æ€è®¡ç®—åˆ†é…ã€æ®‹å·®æ€ç»´è¿æ¥ä»¥åŠæ€ç»´æ­¥éª¤ç¼–ç ç­‰æŠ€æœ¯å®ç°æ€§èƒ½æå‡ã€‚åœ¨å‚æ•°é™åˆ¶ä¸‹ï¼ŒITTèƒ½å¤Ÿå®ç°å¯¹å…³é”®ä»¤ç‰Œçš„æ·±åº¦å¤„ç†è€Œä¸å¢åŠ å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒITTåœ¨å‚æ•°å‡å°‘çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ï¼Œå¹¶ä¼˜åŒ–äº†éšå¼æ€è€ƒè·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†çš„å…³é”®ä»¤ç‰Œæ—¶å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>ITTé€šè¿‡é‡æ–°è®¾è®¡Transformeræ¶æ„æ¥æ”¹è¿›æ€§èƒ½ï¼Œå°†å±‚è®¡ç®—è§†ä¸ºéšå¼æ€è€ƒæ­¥éª¤ã€‚</li>
<li>ITTé€šè¿‡åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€æ®‹å·®æ€ç»´è¿æ¥å’Œæ€ç»´æ­¥éª¤ç¼–ç ç­‰æŠ€æœ¯å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>ITTèƒ½å¤Ÿåœ¨å‚æ•°é™åˆ¶ä¸‹å®ç°å¯¹å…³é”®ä»¤ç‰Œçš„æ·±åº¦å¤„ç†ã€‚</li>
<li>ITTåœ¨å‚æ•°å‡å°‘çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ï¼Œå®ç°äº†96.5%çš„æ€§èƒ½æå‡ã€‚</li>
<li>ITTèƒ½å¤Ÿå‡å°‘è®­ç»ƒæ•°æ®ä½¿ç”¨é‡ï¼Œé™ä½äº†43.2%çš„è®­ç»ƒæ•°æ®éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bcf485a95d0b50b09c9a5fe774131b52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a5bfe87abe44f5efe73053d519d088e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a55452d6bdf7e6f2a7a123878c59a2ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e40220fd414dace1b0c8e0be8cb71bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f77f7318064117728c2f612e77128504.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mol-LLaMA-Towards-General-Understanding-of-Molecules-in-Large-Molecular-Language-Model"><a href="#Mol-LLaMA-Towards-General-Understanding-of-Molecules-in-Large-Molecular-Language-Model" class="headerlink" title="Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular   Language Model"></a>Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular   Language Model</h2><p><strong>Authors:Dongki Kim, Wonbin Lee, Sung Ju Hwang</strong></p>
<p>Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to usersâ€™ queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis. Our project page is at <a target="_blank" rel="noopener" href="https://mol-llama.github.io/">https://mol-llama.github.io/</a>. </p>
<blockquote>
<p>ç†è§£åˆ†å­æ˜¯ç†è§£ç”Ÿç‰©ä½“å’Œæ¨åŠ¨è¯ç‰©å‘ç°è¿›å±•çš„å…³é”®ï¼Œè¿™éœ€è¦è·¨è¶ŠåŒ–å­¦å’Œç”Ÿç‰©å­¦çš„è·¨å­¦ç§‘çŸ¥è¯†ã€‚å°½ç®¡å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹åœ¨è§£é‡Šåˆ†å­ç»“æ„æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬çš„æŒ‡ä»¤æ•°æ®é›†ä»…é™äºä»»åŠ¡å¯¼å‘æ•°æ®é›†çš„å…·ä½“çŸ¥è¯†ï¼Œå¹¶æ²¡æœ‰å®Œå…¨è¦†ç›–åˆ†å­çš„åŸºæœ¬ç‰¹å¾ï¼Œé˜»ç¢äº†å®ƒä»¬ä½œä¸ºé€šç”¨åˆ†å­åŠ©ç†çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Mol-LLaMAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ŒæŒæ¡ä»¥åˆ†å­ä¸ºä¸­å¿ƒçš„ä¸€èˆ¬çŸ¥è¯†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…³é”®æ•°æ®ç±»å‹ï¼Œæ¶µç›–åˆ†å­çš„åŸºæœ¬ç‰¹å¾ï¼Œèå…¥æ¥è‡ªåˆ†å­ç»“æ„çš„åŸºæœ¬çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜å¯¹åˆ†å­ç‰¹å¾çš„ç†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿæ•´åˆæ¥è‡ªä¸åŒåˆ†å­ç¼–ç å™¨çš„è¡¥å……ä¿¡æ¯ï¼Œåˆ©ç”¨ä¸åŒåˆ†å­è¡¨å¾çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMol-LLaMAèƒ½å¤Ÿç†è§£åˆ†å­çš„ä¸€èˆ¬ç‰¹å¾ï¼Œå¹¶å¯¹ç”¨æˆ·çš„æŸ¥è¯¢ç”Ÿæˆç›¸å…³å“åº”ï¼Œæä¾›è¯¦ç»†è§£é‡Šï¼Œè¿™è¡¨æ˜å…¶ä½œä¸ºé€šç”¨åˆ†å­åˆ†æåŠ©ç†çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://mol-llama.github.io/%E3%80%82">https://mol-llama.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13449v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mol-llama.github.io/">https://mol-llama.github.io/</a></p>
<p><strong>Summary</strong><br>åˆ†å­æ˜¯ç”Ÿç‰©ä½“çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯è¯ç‰©ç ”å‘çš„åŸºç¡€ã€‚ç›®å‰çš„å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹è™½ç„¶æˆåŠŸåº”ç”¨äºè§£è¯»åˆ†å­ç»“æ„ï¼Œä½†ç”±äºè®­ç»ƒæ•°æ®é›†ä»…é™äºç‰¹å®šä»»åŠ¡å¯¼å‘çš„æ•°æ®é›†ï¼Œæœªèƒ½å…¨é¢è¦†ç›–åˆ†å­çš„åŸºæœ¬ç‰¹æ€§ï¼Œé™åˆ¶äº†å…¶ä½œä¸ºé€šç”¨åˆ†å­åŠ©æ‰‹çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºMol-LLaMAæ¨¡å‹ï¼Œé€šè¿‡å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒèšç„¦äºåˆ†å­çš„ä¸€èˆ¬çŸ¥è¯†ã€‚æˆ‘ä»¬è®¾è®¡æ¶µç›–åˆ†å­åŸºæœ¬ç‰¹å¾çš„å…³é”®æ•°æ®ç±»å‹ï¼ŒåŒæ—¶å¼•å…¥æ¨¡å—æ•´åˆä¸åŒåˆ†å­ç¼–ç å™¨çš„è¡¥å……ä¿¡æ¯ï¼Œå……åˆ†åˆ©ç”¨å„ç§åˆ†å­è¡¨å¾çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMol-LLaMAèƒ½å¤Ÿç†è§£åˆ†å­çš„åŸºæœ¬ç‰¹å¾ï¼Œå¹¶é’ˆå¯¹ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆè¯¦ç»†è§£é‡Šï¼Œæ˜¾ç¤ºå‡ºä½œä¸ºé€šç”¨åˆ†å­åˆ†æåŠ©æ‰‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£åˆ†å­å¯¹äº†è§£ç”Ÿç‰©ä½“å’Œè¯ç‰©ç ”å‘çš„é‡è¦æ€§ã€‚</li>
<li>å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹åœ¨è§£è¯»åˆ†å­ç»“æ„æ–¹é¢å·²å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>å½“å‰æ¨¡å‹å—é™äºç‰¹å®šä»»åŠ¡å¯¼å‘çš„æ•°æ®é›†ï¼Œæœªèƒ½å…¨é¢è¦†ç›–åˆ†å­çš„åŸºæœ¬ç‰¹æ€§ã€‚</li>
<li>Mol-LLaMAæ¨¡å‹é€šè¿‡å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒï¼Œæ—¨åœ¨ç†è§£åˆ†å­çš„é€šç”¨çŸ¥è¯†ã€‚</li>
<li>Mol-LLaMAè®¾è®¡æ¶µç›–åˆ†å­åŸºæœ¬ç‰¹å¾çš„å…³é”®æ•°æ®ç±»å‹ã€‚</li>
<li>æ¨¡å‹å¼•å…¥æ•´åˆä¸åŒåˆ†å­ç¼–ç å™¨ä¿¡æ¯çš„æ¨¡å—ï¼Œä»¥æé«˜å¯¹åˆ†å­ç‰¹å¾çš„ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e50b8bd8a14b296fcc1abf77e73306a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e81a21080db2396b1155e2ac4b3c4222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63ff8ac10d2f16354718bc78b4a91289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d90d6d92b402f38f490ed33c20c784c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis"><a href="#Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis" class="headerlink" title="Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis"></a>Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis</h2><p><strong>Authors:Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Ning Wang, Zhenhong Long, Jiaojiao Zhao, Zipeng Wang, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian</strong></p>
<p>DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, and their corresponding 4-bit quantized models on the enhanced A-Eval benchmark, A-Eval-2.0. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmarks and periodically update this paper to provide more comprehensive and accurate evaluation results. Please refer to the latest version of the paper for the most recent results and conclusions. </p>
<blockquote>
<p>DeepSeek-R1ä»¥å…¶ä½è®­ç»ƒæˆæœ¬å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›è€Œè‘—ç§°ï¼Œå·²åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»å®é™…åº”ç”¨çš„è§’åº¦è¿›è¡Œçš„è¯¦ç»†è¯„ä¼°ä»ç„¶ç¼ºä¹ï¼Œè¿™ä½¿å¾—ç”¨æˆ·éš¾ä»¥é’ˆå¯¹å…¶ç‰¹å®šéœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹DeepSeek-V3ã€DeepSeek-R1ã€DeepSeek-R1-Distill-Qwenç³»åˆ—ã€DeepSeek-R ä¿®æ™ºæ…§ä¹‹è¿½æ±‚ä»¥åŠå¯¹åº”å…·å¤‡â€œé£æ³‰è’¸é¦â€ï¼ˆLlamaç³»åˆ—ï¼‰çš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ï¼ˆA-Eval-2.0ï¼‰ä¸Šå¯¹å…¶è¿›è¡Œäº†4ä½é‡åŒ–æ¨¡å‹çš„æµ‹è¯•ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä¸å…¶è’¸é¦å¯¹åº”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ†æäº†æ¨ç†å¢å¼ºå¦‚ä½•å½±å“ä¸åŒå®é™…ä»»åŠ¡çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶æ¨ç†å¢å¼ºæ¨¡å‹é€šå¸¸åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¹¶éåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä»»åŠ¡ä¸æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½æå‡å·®å¼‚æ˜¾è‘—ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¸®åŠ©ç”¨æˆ·é€‰æ‹©æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡æ€§èƒ½å±‚æ¬¡åˆ†ç±»å’Œç›´è§‚çš„çº¿å½¢å›¾æ¥è¡¡é‡DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚å…·ä½“çš„ç¤ºä¾‹æä¾›äº†åˆ‡å®å¯è¡Œçš„è§è§£ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©å’Œéƒ¨ç½²æœ€å…·æˆæœ¬æ•ˆç›Šçš„DeepSeekæ¨¡å‹ï¼Œç¡®ä¿åœ¨å®é™…åº”ç”¨ä¸­å®ç°æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬åŠªåŠ›å»ºç«‹å…¨é¢ã€å®¢è§‚ã€æƒå¨çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œä½†æµ‹è¯•æ ·æœ¬çš„é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒçš„ç‰¹å¾ä»¥åŠè¯„ä¼°æ ‡å‡†çš„è®¾å®šä¸å¯é¿å…åœ°ä¼šå¯¹è¯„ä¼°ç»“æœå¼•å…¥ä¸€å®šçš„åè§ã€‚æˆ‘ä»¬å°†ä¸æ–­ä¼˜åŒ–è¯„ä¼°åŸºå‡†æµ‹è¯•å¹¶å®šæœŸæ›´æ–°æœ¬æ–‡å†…å®¹ï¼Œä»¥æä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚æœ‰å…³æœ€æ–°çš„æˆæœå’Œç»“è®ºï¼Œè¯·å‚è€ƒæœ€æ–°ç‰ˆæœ¬çš„è®ºæ–‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11164v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¶è¾ƒä½çš„è®­ç»ƒæˆæœ¬å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼ŒDeepSeek-R1å·²åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯è¡¨ç°æ°´å¹³ã€‚ç„¶è€Œï¼Œå¯¹äºå…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„è¯¦ç»†è¯„ä¼°ä»æ˜¾ä¸è¶³ï¼Œä½¿å¾—ç”¨æˆ·éš¾ä»¥ä¸ºå…¶ç‰¹å®šéœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é’ˆå¯¹DeepSeek-V3ç³»åˆ—ã€DeepSeek-R1ç³»åˆ—åŠå…¶è’¸é¦ç‰ˆæœ¬ï¼ˆåŒ…æ‹¬DeepSeek-R1-Distill-Qwenå’ŒDeepSeek-R1-Distill-Llamaç³»åˆ—ï¼‰ï¼Œä»¥åŠå®ƒä»¬çš„ç›¸åº”4ä½é‡åŒ–æ¨¡å‹ï¼Œåœ¨å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ï¼ˆA-Eval-2.0ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚é€šè¿‡æ¯”è¾ƒåŸå§‹æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸å…¶è’¸é¦å¯¹åº”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ†æäº†æ¨ç†å¢å¼ºå¦‚ä½•å½±å“ä¸åŒå®é™…ä»»åŠ¡çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶æ¨ç†å¢å¼ºæ¨¡å‹é€šå¸¸å¼ºå¤§ï¼Œä½†å¹¶ä¸æ™®éé€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼Œæ€§èƒ½æå‡åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­å·®å¼‚æ˜¾è‘—ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¸®åŠ©ç”¨æˆ·è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œæˆ‘ä»¬é€šè¿‡æ€§èƒ½åˆ†çº§å’Œç›´è§‚å›¾è¡¨é‡åŒ–äº†DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚é€šè¿‡å…·ä½“ç¤ºä¾‹ï¼Œæˆ‘ä»¬æä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©å’Œéƒ¨ç½²æœ€å…·æˆæœ¬æ•ˆç›Šçš„DeepSeekæ¨¡å‹ï¼Œç¡®ä¿åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®ç°æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬åŠªåŠ›å»ºç«‹å…¨é¢ã€å®¢è§‚ã€æƒå¨çš„è¯„ä¼°åŸºå‡†ï¼Œä½†æµ‹è¯•æ ·æœ¬çš„é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒç‰¹å¾å’Œè¯„ä¼°æ ‡å‡†çš„è®¾å®šå¯èƒ½ä¼šç»™è¯„ä¼°ç»“æœå¸¦æ¥ä¸€å®šçš„åè§ã€‚æˆ‘ä»¬å°†æŒç»­ä¼˜åŒ–è¯„ä¼°åŸºå‡†å¹¶å®šæœŸæ›´æ–°æœ¬è®ºæ–‡ï¼Œä»¥æä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚æœ‰å…³æœ€æ–°ç»“æœå’Œç»“è®ºï¼Œè¯·å‚è€ƒè®ºæ–‡æœ€æ–°ç‰ˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å¤šç§DeepSeekæ¨¡å‹ï¼ˆåŒ…æ‹¬åŸå§‹æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å’Œè’¸é¦æ¨¡å‹ï¼‰åœ¨å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å‘ç°æ¨ç†å¢å¼ºæ¨¡å‹åœ¨å¤šæ ·åŒ–å®é™…ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå¹¶éæ‰€æœ‰ä»»åŠ¡éƒ½è¡¨ç°å‡ºæ™®éä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡æ€§èƒ½åˆ†çº§å’Œç›´è§‚å›¾è¡¨é‡åŒ–DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œä¸ºç”¨æˆ·æä¾›æ¨¡å‹é€‰æ‹©çš„æŒ‡å¯¼ã€‚</li>
<li>æä¾›å…·ä½“ç¤ºä¾‹å’Œå¯æ“ä½œæ€§çš„è§è§£ï¼Œå¸®åŠ©ç”¨æˆ·æ ¹æ®æ€§èƒ½å’Œèµ„æºæ•ˆç‡é€‰æ‹©æœ€åˆé€‚çš„DeepSeekæ¨¡å‹ã€‚</li>
<li>æŒ‡å‡ºè¯„ä¼°è¿‡ç¨‹ä¸­å­˜åœ¨çš„æ½œåœ¨åè§ï¼Œå¹¶å¼ºè°ƒæŒç»­ä¼˜åŒ–è¯„ä¼°åŸºå‡†çš„é‡è¦æ€§ã€‚</li>
<li>å¼ºè°ƒå®šæœŸæ›´æ–°è®ºæ–‡ä»¥æä¾›æœ€æ–°è¯„ä¼°ç»“æœçš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-968d4720f2c22de097eca9cc8836c673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b19085bb5b8545585e97993ec51bc97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b4984e02e3f12f3b43e574d41cb5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c6e6ac92ff1fe2cda5abfe1b99bfe14.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TractoGPT-A-GPT-architecture-for-White-Matter-Segmentation"><a href="#TractoGPT-A-GPT-architecture-for-White-Matter-Segmentation" class="headerlink" title="TractoGPT: A GPT architecture for White Matter Segmentation"></a>TractoGPT: A GPT architecture for White Matter Segmentation</h2><p><strong>Authors:Anoushkrit Goel, Simroop Singh, Ankita Joshi, Ranjeet Ranjan Jha, Chirag Ahuja, Aditya Nigam, Arnav Bhavsar</strong></p>
<p>White matter bundle segmentation is crucial for studying brain structural connectivity, neurosurgical planning, and neurological disorders. White Matter Segmentation remains challenging due to structural similarity in streamlines, subject variability, symmetry in 2 hemispheres, etc. To address these challenges, we propose TractoGPT, a GPT-based architecture trained on streamline, cluster, and fusion data representations separately. TractoGPT is a fully-automatic method that generalizes across datasets and retains shape information of the white matter bundles. Experiments also show that TractoGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores. We use TractoInferno and 105HCP datasets and validate generalization across dataset. </p>
<blockquote>
<p>ç™½è´¨æŸåˆ†å‰²å¯¹äºç ”ç©¶è„‘ç»“æ„è¿æ¥ã€ç¥ç»å¤–ç§‘è§„åˆ’å’Œç¥ç»ç–¾ç—…è‡³å…³é‡è¦ã€‚ç”±äºæµçº¿ç»“æ„ç›¸ä¼¼æ€§ã€ä¸ªä½“å·®å¼‚ã€ä¸¤ä¸ªåŠçƒå¯¹ç§°æ€§ç­‰å› ç´ ï¼Œç™½è´¨åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TractoGPTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºGPTçš„æ¶æ„ï¼Œåˆ†åˆ«è®­ç»ƒæµçº¿ã€é›†ç¾¤å’Œèåˆæ•°æ®è¡¨ç¤ºã€‚TractoGPTæ˜¯ä¸€ç§å…¨è‡ªåŠ¨æ–¹æ³•ï¼Œå¯è·¨æ•°æ®é›†æ¨å¹¿å¹¶ä¿ç•™ç™½è´¨æŸçš„å½¢çŠ¶ä¿¡æ¯ã€‚å®éªŒè¿˜è¡¨æ˜ï¼ŒTractoGPTçš„å¹³å‡DICEã€é‡å å’Œè¿‡åº¦é‡å åˆ†æ•°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨TractoInfernoå’Œ105HCPæ•°æ®é›†éªŒè¯è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15464v2">PDF</a> Accepted as a conference paper at 23rd IEEE International Symposium   on Biomedical Imaging 2025. IEEE holds the copyright for this publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç™½è´¨æŸåˆ†å‰²åœ¨ç ”ç©¶è„‘ç»“æ„è¿æ¥æ€§ã€ç¥ç»æ‰‹æœ¯è§„åˆ’å’Œç¥ç»ç–¾ç—…æ–¹é¢çš„é‡è¦æ€§ã€‚é’ˆå¯¹ç™½è´¨åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºGPTçš„æ¶æ„TractoGPTï¼Œè¯¥æ¶æ„åˆ†åˆ«è®­ç»ƒæµçº¿ã€é›†ç¾¤å’Œèåˆæ•°æ®è¡¨ç¤ºã€‚TractoGPTæ˜¯ä¸€ç§å…¨è‡ªåŠ¨æ–¹æ³•ï¼Œå¯è·¨æ•°æ®é›†è¿›è¡Œæ¨å¹¿å¹¶ä¿ç•™ç™½è´¨æŸçš„å½¢çŠ¶ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒTractoGPTåœ¨å¹³å‡DICEã€é‡å å’Œè¦†ç›–å¾—åˆ†æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚ä½¿ç”¨TractoInfernoå’Œ105HCPæ•°æ®é›†éªŒè¯äº†è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™½è´¨æŸåˆ†å‰²åœ¨ç ”ç©¶è„‘ç»“æ„è¿æ¥æ€§ã€ç¥ç»æ‰‹æœ¯è§„åˆ’å’Œç¥ç»ç–¾ç—…æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰ç™½è´¨åˆ†å‰²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æµçº¿ç»“æ„ç›¸ä¼¼æ€§ã€ä¸»é¢˜å·®å¼‚ã€ä¸¤ä¾§å¤§è„‘å¯¹ç§°æ€§ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºTractoGPTçš„GPT-basedæ¶æ„ï¼Œç”¨äºè§£å†³ç™½è´¨åˆ†å‰²çš„æŒ‘æˆ˜ã€‚</li>
<li>TractoGPTé€šè¿‡åˆ†åˆ«è®­ç»ƒæµçº¿ã€é›†ç¾¤å’Œèåˆæ•°æ®è¡¨ç¤ºæ¥å®ç°å…¨è‡ªåŠ¨åˆ†å‰²ã€‚</li>
<li>TractoGPTå¯è·¨æ•°æ®é›†è¿›è¡Œæ¨å¹¿ï¼Œå¹¶ä¿ç•™ç™½è´¨æŸçš„å½¢çŠ¶ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTractoGPTåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39043502ce05245fa3ac6c6b3ab68936.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-732e19bfc71ed16289e2b8b4696b3781.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7aad7cc5474fabec43c2d9547de5c3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32dc17a176d911de87c9ed0b83f9bed2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf4d70788fe5a85eb05230a916692d3c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Does-Table-Source-Matter-Benchmarking-and-Improving-Multimodal-Scientific-Table-Understanding-and-Reasoning"><a href="#Does-Table-Source-Matter-Benchmarking-and-Improving-Multimodal-Scientific-Table-Understanding-and-Reasoning" class="headerlink" title="Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning"></a>Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning</h2><p><strong>Authors:Bohao Yang, Yingji Zhang, Dong Liu, AndrÃ© Freitas, Chenghua Lin</strong></p>
<p>Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences. While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities. We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions. Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities. Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity. Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/MMSci_Table">https://github.com/Bernard-Yang/MMSci_Table</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å…·å¤‡äº†å…ˆè¿›çš„è¡¨æ ¼ç†è§£èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¾èµ–äºå°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰èƒ½å¤Ÿå®ç°ç›´æ¥è§†è§‰å¤„ç†ï¼Œä½†ç”±äºå›ºå®šè¾“å…¥å›¾åƒåˆ†è¾¨ç‡å’Œæ•°å€¼æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œå®ƒä»¬åœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼æ—¶é¢ä¸´å±€é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰åŠ¨æ€è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„å¤šæ¨¡æ€ç§‘å­¦è¡¨æ ¼ç†è§£ä¸æ¨ç†çš„ç»¼åˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰MMSci-Preï¼Œä¸€ä¸ªåŒ…å«52Kç§‘å­¦è¡¨æ ¼ç»“æ„è¯†åˆ«æ ·æœ¬çš„ç‰¹å®šé¢†åŸŸè¡¨æ ¼ç»“æ„å­¦ä¹ æ•°æ®é›†ï¼Œï¼ˆ2ï¼‰MMSci-Insï¼Œä¸€ä¸ªåŒ…å«12Kæ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ä¸ªåŸºäºè¡¨æ ¼çš„ä»»åŠ¡ï¼Œï¼ˆ3ï¼‰MMSci-Evalï¼Œä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ•°å€¼æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«3114ä¸ªæµ‹è¯•æ ·æœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç‰¹å®šé¢†åŸŸæ–¹æ³•ä½¿ç”¨52Kç§‘å­¦è¡¨æ ¼å›¾åƒç›¸æ¯”ä½¿ç”¨15ä¸‡å¼ é€šç”¨é¢†åŸŸè¡¨æ ¼å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†æ•°æ®è´¨é‡ä¼˜äºæ•°é‡çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€LLMæ¨¡å‹å…·æœ‰åŠ¨æ€è¾“å…¥åˆ†è¾¨ç‡ï¼Œåœ¨é€šç”¨è¡¨æ ¼ç†è§£å’Œæ•°å€¼æ¨ç†èƒ½åŠ›æ–¹é¢å‡å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨ç‹¬ç«‹æ•°æ®é›†ä¸Šå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/MMSci_Table%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Bernard-Yang/MMSci_Tableä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13042v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¡¨æ ¼ç†è§£æ–¹é¢æœ‰æ‰€è¿›æ­¥ï¼Œä½†ä»éœ€å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬åºåˆ—ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è™½å¯å®ç°ç›´æ¥è§†è§‰å¤„ç†ï¼Œä½†å› å›ºå®šè¾“å…¥å›¾åƒåˆ†è¾¨ç‡å’Œæ•°å€¼æ¨ç†èƒ½åŠ›ä¸è¶³ï¼Œåœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼æ—¶å­˜åœ¨å±€é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«åŠ¨æ€è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„å¤šæ¨¡æ€ç§‘å­¦è¡¨æ ¼ç†è§£ä¸æ¨ç†çš„ç»¼åˆæ¡†æ¶ï¼Œç”±MMSci-Preï¼ˆ5.2ä¸‡ç§‘å­¦è¡¨æ ¼ç»“æ„è¯†åˆ«æ ·æœ¬çš„é¢†åŸŸç‰¹å®šè¡¨æ ¼ç»“æ„å­¦ä¹ æ•°æ®é›†ï¼‰ã€MMSci-Insï¼ˆ1.2ä¸‡æ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–ä¸‰ä¸ªè¡¨æ ¼ä»»åŠ¡ï¼‰å’ŒMMSci-Evalï¼ˆä¸“é—¨ç”¨äºè¯„ä¼°æ•°å€¼æ¨ç†èƒ½åŠ›çš„3114ä¸ªæµ‹è¯•æ ·æœ¬çš„åŸºå‡†æµ‹è¯•ï¼‰ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ„æˆã€‚å®éªŒè¡¨æ˜ï¼Œä¸15ä¸‡é€šç”¨åŸŸè¡¨æ ¼ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„é¢†åŸŸç‰¹å®šæ–¹æ³•åœ¨å¤„ç†5.2ä¸‡ç§‘å­¦è¡¨æ ¼å›¾åƒæ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œçªæ˜¾äº†æ•°æ®è´¨é‡è€Œéæ•°é‡çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºçš„å…·æœ‰åŠ¨æ€è¾“å…¥åˆ†è¾¨ç‡çš„åŸºäºè¡¨æ ¼çš„MLLMåœ¨é€šç”¨è¡¨æ ¼ç†è§£å’Œæ•°å€¼æ¨ç†èƒ½åŠ›ä¸Šå‡æœ‰æ˜¾è‘—æé«˜ï¼Œå¯¹æœªå…¬å¼€æ•°æ®é›†å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸLLMåœ¨è¡¨æ ¼ç†è§£æ–¹é¢å–å¾—è¿›æ­¥ï¼Œä½†ä»éœ€è½¬æ¢è¡¨æ ¼ä¸ºæ–‡æœ¬åºåˆ—ã€‚</li>
<li>MLLMè™½æ”¯æŒç›´æ¥è§†è§‰å¤„ç†ï¼Œä½†åœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼æ—¶å­˜åœ¨å›ºå®šè¾“å…¥å›¾åƒåˆ†è¾¨ç‡å’Œæ•°å€¼æ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼ŒåŒ…å«åŠ¨æ€è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„å¤šæ¨¡æ€ç§‘å­¦è¡¨æ ¼ç†è§£ä¸æ¨ç†ã€‚</li>
<li>ç»¼åˆæ¡†æ¶ç”±ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ„æˆï¼šMMSci-Preã€MMSci-Inså’ŒMMSci-Evalã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œé¢†åŸŸç‰¹å®šæ–¹æ³•åœ¨å¤„ç†ç§‘å­¦è¡¨æ ¼å›¾åƒæ—¶è¡¨ç°ä¼˜å¼‚ï¼Œå¼ºè°ƒæ•°æ®è´¨é‡çš„é‡è¦æ€§ã€‚</li>
<li>åŸºäºè¡¨æ ¼çš„MLLMå…·æœ‰åŠ¨æ€è¾“å…¥åˆ†è¾¨ç‡ï¼Œåœ¨é€šç”¨è¡¨æ ¼ç†è§£å’Œæ•°å€¼æ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f787aa6d9ffe525fed063602c5a759e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae25ebe38e86698b28773c2c4b27b78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fc2b5699f5854009b6072e35aa84c4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-890c8d785bcd4f9a64317321dce29c1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b66c9d0e074938268b5f55e4e28b9e1d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering"><a href="#Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering" class="headerlink" title="Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering"></a>Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering</h2><p><strong>Authors:Santhosh Thottingal</strong></p>
<p>This paper introduces an approach to question answering over knowledge bases like Wikipedia and Wikidata by performing â€œquestion-to-questionâ€ matching and retrieval from a dense vector embedding store. Instead of embedding document content, we generate a comprehensive set of questions for each logical content unit using an instruction-tuned LLM. These questions are vector-embedded and stored, mapping to the corresponding content. Vector embedding of user queries are then matched against this question vector store. The highest similarity score leads to direct retrieval of the associated article content, eliminating the need for answer generation. Our method achieves high cosine similarity ( &gt; 0.9 ) for relevant question pairs, enabling highly precise retrieval. This approach offers several advantages including computational efficiency, rapid response times, and increased scalability. We demonstrate its effectiveness on Wikipedia and Wikidata, including multimedia content through structured fact retrieval from Wikidata, opening up new pathways for multimodal question answering. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åœ¨çŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰ä¸Šè¿›è¡Œé—®ç­”çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰§è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€çš„åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨åº“ä¸­è¿›è¡Œæ£€ç´¢æ¥å®ç°ã€‚æˆ‘ä»¬ä¸ä¸ºæ–‡æ¡£å†…å®¹ç”ŸæˆåµŒå…¥ï¼Œè€Œæ˜¯ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸ªé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç»„å…¨é¢çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜è¢«åµŒå…¥å‘é‡å¹¶å­˜å‚¨ï¼Œæ˜ å°„åˆ°ç›¸åº”çš„å†…å®¹ã€‚ç„¶åï¼Œå°†ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸æ­¤é—®é¢˜å‘é‡å­˜å‚¨åº“è¿›è¡ŒåŒ¹é…ã€‚æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†å°†ç›´æ¥å¯¼è‡´ç›´æ¥æ£€ç´¢ç›¸å…³çš„æ–‡ç« å†…å®¹ï¼Œæ— éœ€ç”Ÿæˆç­”æ¡ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç›¸å…³é—®é¢˜å¯¹çš„é«˜ä½™å¼¦ç›¸ä¼¼æ€§ï¼ˆ&gt; 0.9ï¼‰ï¼Œå¯å®ç°é«˜åº¦ç²¾ç¡®çš„æ£€ç´¢ã€‚è¿™ç§æ–¹æ³•æä¾›äº†å‡ ä¸ªä¼˜ç‚¹ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºã€‚æˆ‘ä»¬åœ¨Wikipediaå’ŒWikidataä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é€šè¿‡ä»Wikidataè¿›è¡Œç»“æ„åŒ–äº‹å®æ£€ç´¢æ¥åŒ…å«å¤šåª’ä½“å†…å®¹ï¼Œä¸ºå¤šåª’ä½“é—®ç­”æ‰“å¼€äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11301v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡æ‰§è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€åŒ¹é…å’Œä»å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨ä¸­è¿›è¡Œæ£€ç´¢ï¼Œå®ç°åœ¨çŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰ä¸Šè¿›è¡Œé—®ç­”çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä¸ºé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç»„å…¨é¢çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‘é‡åµŒå…¥å’Œå­˜å‚¨ã€‚ç„¶åï¼Œå°†ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸é—®é¢˜å‘é‡å­˜å‚¨è¿›è¡ŒåŒ¹é…ã€‚æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†ç›´æ¥å¯¼è‡´ç›´æ¥æ£€ç´¢ç›¸å…³æ–‡ç« å†…å®¹ï¼Œæ— éœ€ç”Ÿæˆç­”æ¡ˆã€‚æ­¤æ–¹æ³•åœ¨Wikipediaå’ŒWikidataä¸Šå–å¾—äº†è¾ƒé«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ&gt; 0.9ï¼‰ï¼ŒåŒ…æ‹¬é€šè¿‡ä»Wikidataæ£€ç´¢ç»“æ„åŒ–äº‹å®æ¥æ£€ç´¢å¤šåª’ä½“å†…å®¹ï¼Œä¸ºå¤šåª’ä½“é—®ç­”å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºç­‰ä¼˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é—®ç­”åŒ¹é…å’Œæ£€ç´¢æ–¹æ³•ï¼Œé€šè¿‡å¯†é›†å‘é‡åµŒå…¥å­˜å‚¨è¿›è¡Œâ€œé—®é¢˜åˆ°é—®é¢˜â€åŒ¹é…ã€‚</li>
<li>è¯¥æ–¹æ³•é’ˆå¯¹çŸ¥è¯†åº“ï¼ˆå¦‚Wikipediaå’ŒWikidataï¼‰ä¸­çš„å†…å®¹è¿›è¡Œæ“ä½œã€‚</li>
<li>æ–¹æ³•ä¸åµŒå…¥æ–‡æ¡£å†…å®¹ï¼Œè€Œæ˜¯ä¸ºé€»è¾‘å†…å®¹å•å…ƒç”Ÿæˆä¸€ç³»åˆ—é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå‘é‡åµŒå…¥å’Œå­˜å‚¨ã€‚</li>
<li>é€šè¿‡åŒ¹é…ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åµŒå…¥ä¸é—®é¢˜å‘é‡å­˜å‚¨ï¼Œå®ç°ç²¾ç¡®æ£€ç´¢ç›¸å…³æ–‡ç« å†…å®¹ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é«˜ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ&gt; 0.9ï¼‰ï¼Œæé«˜äº†æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æ­¤æ–¹æ³•å…·æœ‰è®¡ç®—æ•ˆç‡é«˜ã€å“åº”é€Ÿåº¦å¿«å’Œå¯æ‰©å±•æ€§å¼ºç­‰ä¼˜ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea77df0c06b7e28df20aff752d5e1e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ef7522c412876487ecaffe5967303a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dca9d1367e4496207d775c52be059c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a8f5a326170a70d3e732f5fd775f6e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b4c66cc698d7229edd2d296d4c782b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications â€“ such as author-publication history, author affiliation, research topics, and citation counts â€“ we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>ç§‘ç ”å›¢é˜Ÿå†…éƒ¨çš„åŠ¨æ€å¯¹äºå†³å®šç ”ç©¶äº§å‡ºçš„æ€§è´¨å’Œå½±å“åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å¯¹è´¡çŒ®çš„å…¨é¢æƒ…å¢ƒåˆ†æã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç§‘ç ”å›¢é˜Ÿä¸­çš„ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»çš„å˜é©æ€§æ–¹æ³•ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„èšç±»æ–¹æ³•ï¼Œå®ƒæä¾›äº†æ›´ä¸ºç²¾ç»†çš„åˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡åˆ©ç”¨å¼€æºå’Œä¸“æœ‰LLMï¼ˆå¦‚GPT-4ã€Llama3 70Bã€Llama2 70Bå’ŒMistral 7x8Bï¼‰æ¥è¡¥å……å’Œå¢å¼ºè¿™äº›æ–¹æ³•ï¼Œç”¨äºè§’è‰²åˆ†ç±»ã€‚é€šè¿‡å°‘é‡æç¤ºï¼Œæˆ‘ä»¬å¯¹ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯æ˜GPT-4åœ¨å¤šä¸ªç±»åˆ«ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„XGBoostå’ŒBERTç­‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬å»ºç«‹ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„é¢„æµ‹æ¨¡å‹ï¼Œä½¿ç”¨10ä¸ªç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨OpenAlexæ•°æ®åº“è¡ç”Ÿçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®åº“æä¾›äº†æœ‰å…³å­¦æœ¯å‡ºç‰ˆç‰©ï¼ˆå¦‚ä½œè€…å‡ºç‰ˆå†å²ã€ä½œè€…å…³è”ä¿¡æ¯ã€ç ”ç©¶ä¸»é¢˜å’Œå¼•ç”¨æ¬¡æ•°ç­‰ï¼‰çš„è¯¦ç»†å…ƒæ•°æ®ï¼Œæˆ‘ä»¬è·å¾—äº†0.76çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†ä½œè€…è§’è‰²åˆ†ç±»çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v3">PDF</a> Accepted by Quantitative Science Studies (QSS)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç§‘å­¦å›¢é˜ŸåŠ¨åŠ›å­¦å¯¹ç ”ç©¶äº§å‡ºçš„æ€§è´¨å’Œå½±å“åŠ›çš„é‡è¦æ€§ã€‚ç°æœ‰çš„åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å…¨é¢çš„è´¡çŒ®åˆ†æï¼Œå› æ­¤æå‡ºäº†åˆ©ç”¨å…ˆè¿›çš„LLMæŠ€æœ¯é‡æ–°å®šä¹‰å’Œè¡¥å……ç°æœ‰çš„ç ”ç©¶æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨å°‘é‡æç¤ºè¿›è¡Œä½œè€…è§’è‰²çš„åˆ†ç±»ã€‚é€šè¿‡GPT-4ç­‰æ¨¡å‹ï¼Œç ”ç©¶è¯æ˜GPT-4åœ¨å¤šç±»åˆ«ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹å’Œä¼ ç»Ÿæ–¹æ³•å¦‚XGBoostå’ŒBERTã€‚åˆ©ç”¨æ¥è‡ªOpenAlexæ•°æ®åº“çš„è¯¦ç»†å…ƒæ•°æ®æ„å»ºçš„é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹ä¹Ÿå®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ï¼Œè¡¨æ˜å¯¹ä½œè€…è§’è‰²çš„ç¨³å¥åˆ†ç±»ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™ä¸€æŠ€æœ¯æå‡äº†ç§‘ç ”é¢†åŸŸçš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜ç ”ç©¶çš„æ•ˆç‡å’Œè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç§‘å­¦å›¢é˜ŸåŠ¨åŠ›å­¦åœ¨ç ”ç©¶äº§å‡ºçš„å½±å“ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ç°æœ‰ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å…¨é¢çš„è´¡çŒ®åˆ†æã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„LLMæŠ€æœ¯ï¼ˆå¦‚GPT-4ç­‰ï¼‰è¿›è¡Œä½œè€…è§’è‰²åˆ†ç±»æä¾›æ›´ç²¾ç»†çš„åˆ†æã€‚</li>
<li>GPT-4åœ¨å¤šä¸ªç±»åˆ«ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹å’Œä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚XGBoostå’ŒBERTï¼‰ã€‚</li>
<li>åˆ©ç”¨OpenAlexæ•°æ®åº“çš„è¯¦ç»†å…ƒæ•°æ®æ„å»ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48d7104f8458fc3d0d86e98ccf4b453c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7004172d9fb6699b9a6091ca0d081d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5617eb243b81c147dba300cef9fd59cf.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning"><a href="#AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning" class="headerlink" title="AgentRefine: Enhancing Agent Generalization through Refinement Tuning"></a>AgentRefine: Enhancing Agent Generalization through Refinement Tuning</h2><p><strong>Authors:Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu</strong></p>
<p>Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†å·²ç»è¯æ˜äº†å®ƒä»¬æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»ã€‚ç„¶è€Œï¼Œå¼€æºLLMå’Œå•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼‰ä¹‹é—´ä»å­˜åœ¨å¾ˆå¤§å·®è·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºé€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œç°æœ‰çš„ä»£ç†è®­ç»ƒè¯­æ–™åº“åœ¨å†…éƒ¨è¯„ä¼°é›†ä¸Šçš„è¡¨ç°ä»¤äººæ»¡æ„ï¼Œä½†éš¾ä»¥æ¨å¹¿åˆ°å¤–éƒ¨é›†ã€‚è¿™äº›ä»£ç†è°ƒæ•´å·¥ä½œé¢ä¸´ä¸¥é‡çš„æ ¼å¼é”™è¯¯ï¼Œå¹¶ç»å¸¸é•¿æ—¶é—´é™·å…¥åŒæ ·çš„é”™è¯¯ã€‚æˆ‘ä»¬åˆ†æè®¤ä¸ºï¼Œæ³›åŒ–èƒ½åŠ›å·®çš„æ ¹æºåœ¨äºå¯¹å‡ ç§æ‰‹åŠ¨ä»£ç†ç¯å¢ƒçš„è¿‡åº¦é€‚åº”ä»¥åŠå¯¹æ–°æƒ…å†µçš„é€‚åº”ä¸è¶³ã€‚ä»–ä»¬éš¾ä»¥æ‰§è¡Œé”™è¯¯çš„è¡ŒåŠ¨æ­¥éª¤ï¼Œæ— æ³•ä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œåªæ˜¯è®°å¿†ç°æœ‰çš„è§‚å¯Ÿ-è¡ŒåŠ¨å…³ç³»ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„AgentRefineæ¡†æ¶ç”¨äºä»£ç†è°ƒæ•´ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯è®©æ¨¡å‹å­¦ä¼šé€šè¿‡è§‚å¯Ÿè½¨è¿¹æ¥çº æ­£è‡ªå·±çš„é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»£ç†åˆæˆæ¡†æ¶ï¼Œä»¥æ¶µç›–å„ç§ç¯å¢ƒå’Œä»»åŠ¡ï¼Œå¹¶æç¤ºå¼ºå¤§çš„LLMæ ¹æ®ç¯å¢ƒåé¦ˆç»†åŒ–å…¶é”™è¯¯è¡ŒåŠ¨ã€‚AgentRefineåœ¨å¤šç§ä»£ç†ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä»£ç†è°ƒæ•´å·¥ä½œã€‚å®ƒè¿˜å…·æœ‰æ›´å¥½çš„æŠ—æ‰°åŠ¨æ€§ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„æƒ³æ³•ã€‚æˆ‘ä»¬çš„å‘ç°å»ºç«‹äº†ä»£ç†æ³›åŒ–å’Œè‡ªæˆ‘å®Œå–„ä¹‹é—´çš„å…³è”ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01702v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ‰§è¡Œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€æºLLMä¸å•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼‰ä¹‹é—´ä»å­˜åœ¨å·¨å¤§å·®è·ã€‚æœ¬æ–‡ä¸“æ³¨äºé€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„ä»£ç†è®­ç»ƒè¯­æ–™åº“åœ¨å†…éƒ¨è¯„ä¼°é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤–éƒ¨è¯„ä¼°é›†ä¸Šæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä»£ç†è°ƒæ•´å·¥ä½œé¢ä¸´ä¸¥é‡çš„æ ¼å¼é”™è¯¯ï¼Œå¹¶ç»å¸¸é•¿æ—¶é—´é™·å…¥ç›¸åŒçš„é”™è¯¯ä¸­ã€‚æˆ‘ä»¬åˆ†æè®¤ä¸ºï¼Œå…¶æ³›åŒ–èƒ½åŠ›å¼±æºäºå¯¹å¤šç§æ‰‹åŠ¨ä»£ç†ç¯å¢ƒçš„è¿‡åº¦é€‚åº”ä»¥åŠå¯¹æ–°æƒ…å†µçš„é€‚åº”èƒ½åŠ›ä¸è¶³ã€‚ä»–ä»¬éš¾ä»¥é‡‡å–æ­£ç¡®çš„è¡ŒåŠ¨æ­¥éª¤ï¼Œæ— æ³•ä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œåªæ˜¯è®°å¿†ç°æœ‰çš„è§‚å¯Ÿä¸è¡ŒåŠ¨ä¹‹é—´çš„å…³ç³»ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹çš„AgentRefineæ¡†æ¶æ¥è¿›è¡Œä»£ç†è°ƒæ•´ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è½¨è¿¹ä¸­çš„è§‚å¯Ÿä½¿æ¨¡å‹å­¦ä¼šçº æ­£è‡ªå·±çš„é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«å„ç§ç¯å¢ƒå’Œä»»åŠ¡çš„ä»£ç†åˆæˆæ¡†æ¶ï¼Œå¹¶æç¤ºå¼ºå¤§çš„LLMæ ¹æ®ç¯å¢ƒåé¦ˆæ¥å¾®è°ƒå…¶é”™è¯¯è¡ŒåŠ¨ã€‚AgentRefineåœ¨å¤šç§ä»£ç†ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä»£ç†è°ƒæ•´å·¥ä½œï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„é¢å¯¹å¹²æ‰°çš„ç¨³å¥æ€§ä»¥åŠæ¨ç†è¿‡ç¨‹ä¸­çš„å¤šæ ·åŒ–æ€ç»´ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å»ºç«‹äº†ä»£ç†æ³›åŒ–ä¸è‡ªæˆ‘å®Œå–„ä¹‹é—´çš„å…³è”ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä¸å•†ä¸šæ¨¡å‹å¦‚GPTç³»åˆ—ç›¸æ¯”ä»å­˜åœ¨å·®è·ã€‚</li>
<li>ç°æœ‰LLMä»£ç†è®­ç»ƒåœ¨å†…éƒ¨è¯„ä¼°ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤–éƒ¨è¯„ä¼°ä¸Šæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</li>
<li>LLMä»£ç†è°ƒæ•´é¢ä¸´æ ¼å¼é”™è¯¯å’Œéš¾ä»¥çº æ­£é•¿æœŸé”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>LLMæ³›åŒ–èƒ½åŠ›å¼±æºäºå¯¹å¤šä¸ªæ‰‹åŠ¨ç¯å¢ƒçš„è¿‡åº¦é€‚åº”å’Œå¯¹æ–°æƒ…å†µçš„é€‚åº”èƒ½åŠ›ä¸è¶³ã€‚</li>
<li>LLMéš¾ä»¥ä»ç»éªŒä¸­å­¦ä¹ ï¼Œä»…ä¾èµ–è®°å¿†ç°æœ‰çš„è§‚å¯Ÿä¸è¡ŒåŠ¨å…³ç³»ã€‚</li>
<li>AgentRefineæ¡†æ¶é€šè¿‡ä½¿æ¨¡å‹å­¦ä¼šè‡ªæˆ‘çº æ­£é”™è¯¯æ¥æé«˜LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2255ed0a5d7d3c56a172d243ce39bcc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e790c1bbb63d57fa516cb7978a1323bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-435710b79c861a4ca2ec450f7d8973a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4de5db3d55f928025855b461beb43b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-750d49d44f60a9a8b4ff9f1ed0375f83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-776bf9f9537ebf89bd5d7871d8d30a1d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Domain-Specific-Translation-with-Open-Source-Large-Language-Models-Resource-Oriented-Analysis"><a href="#Domain-Specific-Translation-with-Open-Source-Large-Language-Models-Resource-Oriented-Analysis" class="headerlink" title="Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis"></a>Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis</h2><p><strong>Authors:Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem</strong></p>
<p>In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language pairs with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in specialized translation quality compared to multilingual encoder-decoder MT models such as NLLB-200. In three out of four language directions in our study, NLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in medical translation. While fine-tuning LLMs such as Mistral and Llama improves their performance at medical translation, these models still fall short compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve higher-quality domain-specific translation, especially in medium-resource and low-resource settings. As larger LLMs outperform their 8B variants, this also encourages pre-training domain-specific medium-sized LMs to improve quality and efficiency in specialized translation tasks. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å¼€æºçš„è‡ªåŠ¨å›å½’è§£ç å™¨ä¸“ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä»»åŠ¡å¯¼å‘å‹æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ç¿»è¯‘æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒé‡ç‚¹å…³æ³¨åŒ»ç–—é¢†åŸŸï¼Œå¹¶æ¶µç›–äº†å››ç§è¯­è¨€å¯¹ï¼Œè¿™äº›è¯­è¨€å¯¹çš„èµ„æºå¯ç”¨æ€§å„ä¸ç›¸åŒï¼šè‹±è¯­åˆ°æ³•è¯­ã€è‹±è¯­åˆ°è‘¡è„ç‰™è¯­ã€è‹±è¯­åˆ°æ–¯ç“¦å¸Œé‡Œè¯­å’Œæ–¯ç“¦å¸Œé‡Œè¯­åˆ°è‹±è¯­ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›æ­¥ï¼Œä½†LLMåœ¨ä¸“é—¨ç¿»è¯‘çš„è´¨é‡æ–¹é¢ä¸è¯¸å¦‚NLLB-200ç­‰å¤šè¯­è¨€ç¼–ç å™¨-è§£ç å™¨MTæ¨¡å‹ä¹‹é—´ä»å­˜åœ¨æ˜æ˜¾å·®è·ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œå››ä¸ªè¯­è¨€æ–¹å‘ä¸­æœ‰ä¸‰ä¸ªæ–¹å‘çš„NLLB-200 3.3Båœ¨åŒ»ç–—ç¿»è¯‘æ–¹é¢çš„è¡¨ç°ä¼˜äºæ‰€æœ‰è§„æ¨¡ä¸º8Bå‚æ•°çš„LLMã€‚è™½ç„¶å¾®è°ƒè¯¸å¦‚Mistralå’ŒLlamaç­‰LLMå¯ä»¥æ”¹å–„å…¶åœ¨åŒ»ç–—ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶æ— æ³•ä¸ç»è¿‡å¾®è°ƒè¿‡çš„NLLB-200 3.3Bæ¨¡å‹ç›¸æŠ—è¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸­ç­‰èµ„æºå’Œä½èµ„æºç¯å¢ƒä¸­ï¼Œå®ç°é«˜è´¨é‡çš„ä¸“ä¸šé¢†åŸŸç¿»è¯‘ä»éœ€è¦ä¸“é—¨çš„æœºå™¨ç¿»è¯‘æ¨¡å‹ã€‚éšç€æ›´å¤§çš„LLMåœ¨è¡¨ç°ä¸Šè¶…è¶Šäº†å…¶è§„æ¨¡ä¸º8Bçš„å˜ä½“ï¼Œè¿™ä¹Ÿé¼“åŠ±åœ¨ä¸“é—¨çš„ç¿»è¯‘ä»»åŠ¡ä¸­é¢„å…ˆè®­ç»ƒä¸­ç­‰è§„æ¨¡çš„ä¸“ä¸šé¢†åŸŸLMä»¥æé«˜è´¨é‡å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¯¹æ¯”äº†å¼€æºçš„è‡ªåŠ¨å›å½’è§£ç å™¨ä¸“ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä»»åŠ¡å¯¼å‘å‹æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„ç¿»è¯‘æ€§èƒ½ã€‚å®éªŒæ¶‰åŠå››ç§è¯­è¨€å¯¹ï¼ŒåŒ…æ‹¬è‹±è¯­å¯¹æ³•è¯­ã€è‹±è¯­å¯¹è‘¡è„ç‰™è¯­ã€è‹±è¯­å¯¹æ–¯ç“¦å¸Œé‡Œè¯­å’Œæ–¯ç“¦å¸Œé‡Œè¯­å¯¹è‹±è¯­ã€‚å°½ç®¡LLMæœ€è¿‘æœ‰è¿›å±•ï¼Œä½†åœ¨ä¸“ä¸šç¿»è¯‘è´¨é‡æ–¹é¢ä¸å¤šè¯­è¨€ç¼–ç å™¨-è§£ç å™¨MTæ¨¡å‹ï¼ˆå¦‚NLLB-200ï¼‰ç›¸æ¯”ä»å­˜åœ¨æ˜æ˜¾å·®è·ã€‚åœ¨æœ¬æ–‡ç ”ç©¶çš„å››ä¸ªè¯­è¨€æ–¹å‘ä¸­æœ‰ä¸‰ä¸ªæ–¹å‘ï¼ŒNLLB-200 3.3Båœ¨åŒ»å­¦ç¿»è¯‘ä¸­è¡¨ç°ä¼˜äºæ‰€æœ‰8Bå‚æ•°èŒƒå›´å†…çš„LLMã€‚è™½ç„¶å¾®è°ƒLLMï¼ˆå¦‚Mistralå’ŒLlamaï¼‰å¯ä»¥æé«˜åŒ»å­¦ç¿»è¯‘æ€§èƒ½ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶æ— æ³•ä¸ç»è¿‡å¾®è°ƒä¼˜åŒ–çš„NLLB-200æ¨¡å‹åŒ¹æ•Œã€‚æœ¬æ–‡å‘ç°ï¼Œå®ç°é«˜è´¨é‡çš„ä¸“é¡¹ç¿»è¯‘ä»éœ€é‡‡ç”¨ä¸“ä¸šåŒ–çš„æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ç­‰èµ„æºå’Œä½èµ„æºç¯å¢ƒä¸­å°¤ä¸ºå¦‚æ­¤ã€‚æ›´å¤§çš„LLMè¡¨ç°ä¼˜äºå…¶8Bå˜ä½“ï¼Œè¿™ä¹Ÿé¼“åŠ±åœ¨ä¸“é¡¹ç¿»è¯‘ä»»åŠ¡ä¸­é¢„è®­ç»ƒä¸­ç­‰è§„æ¨¡çš„ä¸“ä¸šé¢†åŸŸLMä»¥æé«˜è´¨é‡å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨åŒ»å­¦é¢†åŸŸçš„ç¿»è¯‘æ€§èƒ½å¯¹æ¯”ä¸­ï¼ŒLLMä¸ä»»åŠ¡å¯¼å‘å‹æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼ˆå¦‚NLLB-200ï¼‰å­˜åœ¨å·®è·ã€‚</li>
<li>NLLB-200 3.3Båœ¨å››ä¸ªè¯­è¨€æ–¹å‘ä¸­çš„ä¸‰ä¸ªæ–¹å‘ä¸Šçš„åŒ»å­¦ç¿»è¯‘è¡¨ç°ä¼˜äºç‰¹å®šå¤§å°èŒƒå›´å†…çš„LLMã€‚</li>
<li>å°½ç®¡LLMé€šè¿‡å¾®è°ƒå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†ä»æ— æ³•è¶…è¶Šç»è¿‡ä¼˜åŒ–çš„NLLB-200æ¨¡å‹ã€‚</li>
<li>åœ¨ä¸­ç­‰èµ„æºå’Œä½èµ„æºç¯å¢ƒä¸‹ï¼Œä¸“ä¸šåŒ–çš„æœºå™¨ç¿»è¯‘æ¨¡å‹æ˜¯å®ç°é«˜è´¨é‡ä¸“é¡¹ç¿»è¯‘çš„å…³é”®ã€‚</li>
<li>æ›´å¤§çš„LLMè¡¨ç°ä¼˜äºå…¶è¾ƒå°å˜ä½“ï¼Œè¡¨æ˜é¢„è®­ç»ƒä¸­ç­‰è§„æ¨¡çš„ä¸“ä¸šé¢†åŸŸLMæœ‰åŠ©äºæé«˜ä¸“é¡¹ç¿»è¯‘çš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>LLMåœ¨ç‰¹å®šé¢†åŸŸçš„ç¿»è¯‘ä¸­ä»éœ€è¿›ä¸€æ­¥çš„ä¼˜åŒ–å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cdfc3ddcf84a6b615785392c5f693e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee21cbbd0f6197dc4be9fb131c46bb39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0c07a7b58a473b67225fe907e3199f4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Examining-the-Self-Improvement-Capabilities-of-Large-Language-Models"><a href="#Mind-the-Gap-Examining-the-Self-Improvement-Capabilities-of-Large-Language-Models" class="headerlink" title="Mind the Gap: Examining the Self-Improvement Capabilities of Large   Language Models"></a>Mind the Gap: Examining the Self-Improvement Capabilities of Large   Language Models</h2><p><strong>Authors:Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, Udaya Ghai</strong></p>
<p>Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the generation-verification gap. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement â€“ a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries. </p>
<blockquote>
<p>è‡ªæˆ‘æå‡æ˜¯å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„è®­ç»ƒã€åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ¨ç†ä¸­çš„ä¸€ç§æœºåˆ¶ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸€ä¸ªæ¨¡å‹éªŒè¯å…¶è‡ªèº«è¾“å‡ºçš„æ¡†æ¶ï¼ŒåŸºäºè¿™ç§éªŒè¯æ¥è¿‡æ»¤æˆ–é‡æ–°åŠ æƒæ•°æ®ï¼Œå¹¶æç‚¼è¿‡æ»¤åçš„æ•°æ®ã€‚å°½ç®¡æœ‰è®¸å¤šç»éªŒæ€§æˆåŠŸï¼Œä½†å¯¹å…¶çš„åŸºæœ¬ç†è§£ä»ç„¶ç¼ºä¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹LLMçš„è‡ªæˆ‘æå‡è¿›è¡Œäº†å…¨é¢ã€æ¨¡å—åŒ–å’Œå¯æ§çš„ç ”ç©¶ã€‚æˆ‘ä»¬ä¸ºè‡ªæˆ‘æå‡æä¾›äº†æ•°å­¦å…¬å¼ï¼Œè¿™ä¸»è¦ç”±æˆ‘ä»¬å½¢å¼åŒ–ä¸ºç”ŸæˆéªŒè¯å·®è·çš„é‡æ¥æ§åˆ¶ã€‚é€šè¿‡å¯¹å„ç§æ¨¡å‹å®¶æ—å’Œä»»åŠ¡è¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å‘ç°è‡ªæˆ‘æå‡çš„è§„æ¨¡åŒ–ç°è±¡â€”â€”ç”ŸæˆéªŒè¯å·®è·çš„æŸç§å˜ä½“éšæ¨¡å‹é¢„è®­ç»ƒæµ®ç‚¹è¿ç®—æ¬¡æ•°è€Œå•è°ƒå˜åŒ–ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†ä½•æ—¶å¯ä»¥å®ç°è‡ªæˆ‘æå‡ï¼Œè¿­ä»£è‡ªæˆ‘æå‡çš„ç¨‹åºä»¥åŠæé«˜å…¶æ€§èƒ½çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†å…·æœ‰å®é™…æ„ä¹‰çš„LLMè‡ªæˆ‘æå‡çš„ç†è§£ï¼Œè€Œä¸”è¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æ‰“å¼€äº†å¯¹å…¶èƒ½åŠ›å’Œè¾¹ç•Œçš„ä¼—å¤šé€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02674v2">PDF</a> ICLR 2025; 41 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªæˆ‘æ”¹è¿›æœºåˆ¶ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ¨ç†ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªæ¨¡å‹éªŒè¯è‡ªèº«è¾“å‡ºã€åŸºäºéªŒè¯è¿‡æ»¤æˆ–é‡æ–°åŠ æƒæ•°æ®ï¼Œå¹¶è’¸é¦è¿‡æ»¤æ•°æ®çš„æ¡†æ¶ã€‚å°½ç®¡æœ‰ä¸€äº›ç»éªŒæ€§æˆåŠŸï¼Œä½†å¯¹å…¶åŸºæœ¬åŸç†çš„ç†è§£ä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹LLMè‡ªæˆ‘æ”¹è¿›è¿›è¡Œäº†å…¨é¢ã€æ¨¡å—åŒ–å’Œå—æ§çš„ç ”ç©¶ï¼Œæä¾›äº†è‡ªæˆ‘æ”¹è¿›çš„æ•°å­¦å…¬å¼ï¼Œä¸»è¦ç”±æˆ‘ä»¬å½¢å¼åŒ–ä¸ºç”Ÿæˆ-éªŒè¯å·®è·çš„é‡æ¥æ§åˆ¶ã€‚é€šè¿‡å®éªŒä¸å„ç§æ¨¡å‹å’Œä»»åŠ¡ï¼Œæˆ‘ä»¬å‘ç°äº†è‡ªæˆ‘æ”¹è¿›çš„è§„æ¨¡ç°è±¡â€”â€”ç”Ÿæˆ-éªŒè¯å·®è·çš„æŸç§å˜ä½“éšæ¨¡å‹é¢„è®­ç»ƒæµ®ç‚¹è¿ç®—æ¬¡æ•°è€Œå•è°ƒå˜åŒ–ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è‡ªæˆ‘æ”¹è¿›çš„å¯èƒ½æ€§ã€è¿­ä»£è‡ªæˆ‘æ”¹è¿›ç¨‹åºä»¥åŠæé«˜å…¶æ€§èƒ½çš„æ–¹æ³•ã€‚æœ¬æ–‡çš„ç ”ç©¶ç»“æœä¸ä»…æœ‰åŠ©äºç†è§£LLMè‡ªæˆ‘æ”¹è¿›çš„å®é™…æ„ä¹‰ï¼Œè€Œä¸”ä¸ºæœªæ¥ç ”ç©¶å…¶èƒ½åŠ›å’Œè¾¹ç•Œå¼€è¾Ÿäº†è®¸å¤šé€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè‡ªæˆ‘æ”¹è¿›æ¶‰åŠé¢„è®­ç»ƒã€åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ¨ç†ã€‚</li>
<li>æ¨¡å‹é€šè¿‡éªŒè¯è‡ªèº«è¾“å‡ºæ¥å®ç°è‡ªæˆ‘æ”¹è¿›ï¼Œå¹¶åŸºäºæ­¤è¿‡æ»¤æˆ–é‡æ–°åŠ æƒæ•°æ®ã€‚</li>
<li>æ–‡ç« æå‡ºäº†è‡ªæˆ‘æ”¹è¿›çš„æ•°å­¦å…¬å¼ï¼Œå—ç”Ÿæˆ-éªŒè¯å·®è·æ§åˆ¶ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œç”Ÿæˆ-éªŒè¯å·®è·çš„æŸç§å˜ä½“éšæ¨¡å‹é¢„è®­ç»ƒæµ®ç‚¹è¿ç®—æ¬¡æ•°å•è°ƒå˜åŒ–ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†è‡ªæˆ‘æ”¹è¿›çš„å¯èƒ½æ€§åŠæé«˜å…¶æ€§èƒ½çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶æœ‰åŠ©äºç†è§£LLMè‡ªæˆ‘æ”¹è¿›çš„å®é™…æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0d938cd2d0b8c387c013f682862463d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b567513b2d46cc6cc21aa06979f2c21f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fee8ed4858005ef90234f25126eb1c20.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Defense-Against-Prompt-Injection-Attack-by-Leveraging-Attack-Techniques"><a href="#Defense-Against-Prompt-Injection-Attack-by-Leveraging-Attack-Techniques" class="headerlink" title="Defense Against Prompt Injection Attack by Leveraging Attack Techniques"></a>Defense Against Prompt Injection Attack by Leveraging Attack Techniques</h2><p><strong>Authors:Yulin Chen, Haoran Li, Zihao Zheng, Yangqiu Song, Dekai Wu, Bryan Hooi</strong></p>
<p>With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attackerâ€™s instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMsâ€™ instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: Could these attack techniques be utilized for defensive purposes? In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing training-free defense approaches, achieving state-of-the-art results. </p>
<blockquote>
<p>éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä¸ºMicrosoft Copilotç­‰LLMé›†æˆåº”ç”¨æä¾›äº†åŠ¨åŠ›ã€‚ç„¶è€Œï¼Œéšç€LLMçš„ä¸æ–­å‘å±•ï¼Œæ–°çš„æ¼æ´ï¼Œå°¤å…¶æ˜¯æç¤ºæ³¨å…¥æ”»å‡»ä¹Ÿéšä¹‹å‡ºç°ã€‚è¿™äº›æ”»å‡»è¯±å¯¼LLMåç¦»åŸå§‹è¾“å…¥æŒ‡ä»¤ï¼Œæ‰§è¡Œæ³¨å…¥åœ¨æ•°æ®å†…å®¹ï¼ˆå¦‚æ£€ç´¢ç»“æœï¼‰ä¸­çš„æ”»å‡»è€…æŒ‡ä»¤ã€‚æœ€è¿‘çš„æ”»å‡»æ–¹æ³•åˆ©ç”¨LLMéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ä»¥åŠå®ƒä»¬æ— æ³•åŒºåˆ†æ³¨å…¥åœ¨æ•°æ®å†…å®¹ä¸­çš„æŒ‡ä»¤ï¼Œå®ç°äº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚åœ¨æ¯”è¾ƒæ”»å‡»å’Œé˜²å¾¡æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬å…·æœ‰ç›¸ä¼¼çš„è®¾è®¡ç›®æ ‡ï¼Œå³å¼•å¯¼æ¨¡å‹å¿½ç•¥ä¸éœ€è¦çš„æŒ‡ä»¤ï¼Œè½¬è€Œæ‰§è¡Œæ‰€éœ€çš„æŒ‡ä»¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç›´è§‚çš„é—®é¢˜ï¼šè¿™äº›æ”»å‡»æŠ€æœ¯èƒ½å¦ç”¨äºé˜²å¾¡ç›®çš„ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é‡å¤æ”»å‡»è¿‡ç¨‹ä½†ä½¿ç”¨åŸå§‹è¾“å…¥æŒ‡ä»¤è€Œä¸æ˜¯æ³¨å…¥çš„æŒ‡ä»¤ï¼Œåè½¬äº†æç¤ºæ³¨å…¥æ–¹æ³•çš„æ„å›¾ï¼ŒåŸºäºä»¥å‰æ— éœ€è®­ç»ƒçš„æ”»å‡»æ–¹æ³•å¼€å‘äº†æ–°å‹é˜²å¾¡æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é˜²å¾¡æŠ€æœ¯ä¼˜äºç°æœ‰çš„æ— éœ€è®­ç»ƒçš„é˜²å¾¡æ–¹æ³•ï¼Œå–å¾—äº†æœ€æ–°çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00459v3">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>éšç€æŠ€æœ¯çš„å‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œæ¨åŠ¨äº†å¦‚Microsoft Copilotç­‰LLMé›†æˆåº”ç”¨çš„å‡ºç°ã€‚ç„¶è€Œï¼Œæ–°çš„æ¼æ´ä¹Ÿä¸æ–­æ¶Œç°ï¼Œç‰¹åˆ«æ˜¯æç¤ºæ³¨å…¥æ”»å‡»ã€‚è¿™äº›æ”»å‡»ä½¿LLMåç¦»åŸå§‹è¾“å…¥æŒ‡ä»¤å¹¶æ‰§è¡Œæ”»å‡»è€…åœ¨æ•°æ®å†…å®¹ä¸­æ³¨å…¥çš„æŒ‡ä»¤ã€‚æœ¬æ–‡åˆ©ç”¨LLMéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›åŠå…¶æ— æ³•åŒºåˆ†æ•°æ®å†…å®¹ä¸­æ³¨å…¥æŒ‡ä»¤çš„ç¼ºé™·è¿›è¡Œæ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ”»å‡»æŠ€æœ¯è½¬åŒ–ä¸ºé˜²å¾¡æŠ€æœ¯çš„åˆ›æ–°æ–¹æ³•ï¼ŒåŸºäºä»¥å¾€æ— è®­ç»ƒæ”»å‡»æŠ€æœ¯ï¼Œé€šè¿‡é‡å¤æ”»å‡»è¿‡ç¨‹ä½†ä½¿ç”¨åŸå§‹è¾“å…¥æŒ‡ä»¤è€Œéæ³¨å…¥æŒ‡ä»¤è¿›è¡Œé˜²å¾¡ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§é˜²å¾¡æŠ€æœ¯ä¼˜äºç°æœ‰çš„æ— è®­ç»ƒé˜²å¾¡æ–¹æ³•ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä¹Ÿå­˜åœ¨æç¤ºæ³¨å…¥æ”»å‡»çš„æ–°æ¼æ´ã€‚</li>
<li>è¿™äº›æ”»å‡»ä½¿LLMsåç¦»åŸå§‹æŒ‡ä»¤å¹¶æ‰§è¡Œæ”»å‡»è€…æ³¨å…¥çš„æŒ‡ä»¤ã€‚</li>
<li>æ”»å‡»æ–¹æ³•å’Œé˜²å¾¡æ–¹æ³•çš„è®¾è®¡ç›®æ ‡ç›¸ä¼¼ï¼Œæ—¨åœ¨å¼•å¯¼æ¨¡å‹æ‰§è¡Œæƒ³è¦çš„æŒ‡ä»¤è€Œéä¸æƒ³è¦çš„æŒ‡ä»¤ã€‚</li>
<li>è®ºæ–‡å°†æ”»å‡»æŠ€æœ¯çš„æ„å›¾é¢ å€’ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºæ— è®­ç»ƒæ”»å‡»æ–¹æ³•çš„é˜²å¾¡æ–¹æ³•ã€‚</li>
<li>è¯¥é˜²å¾¡æ–¹æ³•é€šè¿‡é‡å¤æ”»å‡»è¿‡ç¨‹ä½†ä½¿ç”¨åŸå§‹è¾“å…¥æŒ‡ä»¤è¿›è¡Œé˜²å¾¡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¿™ç§é˜²å¾¡æŠ€æœ¯æ¯”ç°æœ‰æ— è®­ç»ƒé˜²å¾¡æ–¹æ³•æ›´æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4744ecb8c8f9c32416bd462247479616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100f9eee4f83bef4c932574e8d7372bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ae0392a85dab5147db4c409aedfedf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d582f34ebbd78790eb894909b621fe43.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Gnothi-Seauton-Empowering-Faithful-Self-Interpretability-in-Black-Box-Transformers"><a href="#Gnothi-Seauton-Empowering-Faithful-Self-Interpretability-in-Black-Box-Transformers" class="headerlink" title="Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box   Transformers"></a>Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box   Transformers</h2><p><strong>Authors:Shaobo Wang, Hongxuan Tang, Mingyang Wang, Hongrui Zhang, Xuyang Liu, Weiya Li, Xuming Hu, Linfeng Zhang</strong></p>
<p>The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, AutoGnothi, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. AutoGnothi enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that AutoGnothi offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability. </p>
<blockquote>
<p>å…³äºå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰çš„æ ¸å¿ƒäº‰è®ºåœ¨äºè‡ªè§£é‡Šæ¨¡å‹ä¸äº‹åè§£é‡Šé»‘ç®±æ¨¡å‹çš„å¯¹æ¯”ã€‚è‡ªè§£é‡Šæ¨¡å‹ï¼ˆå¦‚åŸºäºæ¦‚å¿µçš„ç½‘ç»œï¼‰é€šè¿‡è¿æ¥å†³ç­–ä¸äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥æä¾›è§è§£ï¼Œä½†å®ƒä»¬å¾€å¾€é¢ä¸´æ€§èƒ½å’Œå¯æ‰©å±•æ€§çš„æŒ‘æˆ˜ã€‚ç›¸åï¼Œäº‹åæ–¹æ³•ï¼ˆå¦‚æ²™æ™®åˆ©å€¼ï¼‰è™½ç„¶åœ¨ç†è®ºä¸Šå¾ˆç¨³å¥ï¼Œä½†åœ¨è®¡ç®—ä¸Šå´éå¸¸æ˜‚è´µä¸”èµ„æºå¯†é›†ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸¤ç§ç ”ç©¶æ€è·¯ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ä¸¤è€…çš„ä¼˜åŠ¿ç»“åˆèµ·æ¥çš„æ–°æ–¹æ³•ï¼Œä¸ºé»‘ç®±æ¨¡å‹æä¾›ç†è®ºä¿è¯çš„è‡ªè§£é‡Šæ€§ï¼ŒåŒæ—¶ä¸å¦¥åé¢„æµ‹ç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„ç®¡é“AutoGnothiï¼Œå®ƒå°†ä¸€ä¸ªå°å‹è¾…åŠ©ç½‘ç»œé›†æˆåˆ°é»‘ç®±æ¨¡å‹ä¸­ï¼Œå…è®¸å®ƒç”Ÿæˆæ²™æ™®åˆ©å€¼è§£é‡Šï¼Œè€Œä¸ä¼šæ”¹å˜åŸå§‹ç½‘ç»œå‚æ•°ã€‚è¿™ç§ä¾§é¢è°ƒæ•´çš„æ–¹æ³•å¤§å¤§é™ä½äº†å†…å­˜ã€è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼Œå…¶ä¸­å…¨å¾®è°ƒæ˜¯æœ€ä¼˜åŸºçº¿ã€‚AutoGnothiä½¿é»‘ç®±æ¨¡å‹èƒ½å¤Ÿä»¥æœ€å°çš„å¼€é”€é¢„æµ‹å’Œè§£é‡Šå…¶é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAutoGnothiå¯¹è§†è§‰å’Œè¯­è¨€ä»»åŠ¡éƒ½æä¾›äº†å‡†ç¡®çš„è§£é‡Šï¼Œåœ¨å¯æ¯”çš„è¯ é‡Šæ€§ä¸‹å…·æœ‰å‡ºè‰²çš„è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21815v2">PDF</a> Accepted by ICLR 2025, 29 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å›´ç»•å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­çš„è‡ªè§£é‡Šæ¨¡å‹ä¸é»‘ç®±æ¨¡å‹çš„åæœŸè§£é‡Šå±•å¼€è¾©è®ºã€‚è‡ªè§£é‡Šæ¨¡å‹å¦‚åŸºäºæ¦‚å¿µçš„ç½‘ç»œé€šè¿‡è¿æ¥å†³ç­–ä¸äººç±»å¯ç†è§£çš„æ¦‚å¿µæä¾›æ´å¯ŸåŠ›ï¼Œä½†åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ç›¸åï¼Œå¦‚æ²™æ™®åˆ©å€¼ç­‰çš„åæœŸæ–¹æ³•è™½ç„¶ç†è®ºç¨³å¥ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ä¸”èµ„æºå¯†é›†ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸¤ç±»ç ”ç©¶çš„å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œä¸ºé»‘ç®±æ¨¡å‹æä¾›ç†è®ºä¿è¯çš„è‡ªè§£é‡Šæ€§ï¼ŒåŒæ—¶ä¸æŸå®³é¢„æµ‹ç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„ç®¡é“AutoGnothiï¼Œå®ƒå°†ä¸€ä¸ªå°å‹ä¾§ç½‘ç»œé›†æˆåˆ°é»‘ç®±æ¨¡å‹ä¸­ï¼Œèƒ½å¤Ÿç”Ÿæˆæ²™æ™®åˆ©å€¼è§£é‡Šï¼Œè€Œæ— éœ€æ›´æ”¹åŸå§‹ç½‘ç»œå‚æ•°ã€‚è¿™ç§ä¾§è°ƒèŠ‚æ–¹æ³•æ˜¾è‘—é™ä½äº†å†…å­˜ã€è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œä¼˜äºä¼ ç»Ÿçš„å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼ˆä»¥å…¨å¾®è°ƒä½œä¸ºæœ€ä½³åŸºçº¿ï¼‰ã€‚AutoGnothiä½¿é»‘ç®±æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å’Œè§£é‡Šå…¶é¢„æµ‹ç»“æœï¼Œå‡ ä¹æ— éœ€é¢å¤–å¼€é”€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAutoGnothiå¯ä¸ºè§†è§‰å’Œè¯­è¨€ä»»åŠ¡æä¾›ç²¾ç¡®çš„è§£é‡Šï¼Œåœ¨è®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªè§£é‡Šæ¨¡å‹ä¸é»‘ç®±æ¨¡å‹çš„åæœŸè§£é‡Šåœ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰é¢†åŸŸä¸­å½¢æˆæ ¸å¿ƒè¾©è®ºã€‚</li>
<li>è‡ªè§£é‡Šæ¨¡å‹å¦‚æ¦‚å¿µç½‘ç»œèƒ½å¤Ÿæä¾›å†³ç­–ä¸äººç±»ç†è§£ä¹‹é—´çš„å…³è”ï¼Œä½†é¢ä¸´æ€§èƒ½ä¸å¯æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>åæœŸæ–¹æ³•å¦‚æ²™æ™®åˆ©å€¼è™½ç„¶ç†è®ºç¨³å¥ä½†è®¡ç®—æˆæœ¬é«˜ä¸”èµ„æºå¯†é›†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•AutoGnothiï¼Œç»“åˆäº†è‡ªè§£é‡Šæ¨¡å‹ä¸é»‘ç®±æ¨¡å‹çš„ä¼˜ç‚¹ã€‚</li>
<li>AutoGnothié€šè¿‡å¼•å…¥å°å‹ä¾§ç½‘ç»œå®ç°äº†å¯¹é»‘ç®±æ¨¡å‹çš„ç†è®ºä¿è¯è‡ªè§£é‡Šæ€§ã€‚</li>
<li>ä¾§è°ƒèŠ‚æ–¹æ³•é™ä½äº†å†…å­˜ã€è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œä¼˜äºä¼ ç»Ÿå‚æ•°é«˜æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25f2575238b00a70d58c982d6d89ce67.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e008774a72aab9124eede42d257f7e8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5b3c06f8d9fd0afef74e727936816c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9305c0aa208544a7fe26118cfe9539f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f74eb918c52c8a7f23f40c93529baa7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-08e2895a318dd12f07ce9551d82799a5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-27  AgentRM Enhancing Agent Generalization with Reward Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-719fc22c81500005b260d458a23807bd.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-26  BiPO Bidirectional Partial Occlusion Network for Text-to-Motion   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
