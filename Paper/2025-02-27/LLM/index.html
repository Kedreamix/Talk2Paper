<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-27  LLM-Based Design Pattern Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f77f7318064117728c2f612e77128504.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-27-更新"><a href="#2025-02-27-更新" class="headerlink" title="2025-02-27 更新"></a>2025-02-27 更新</h1><h2 id="LLM-Based-Design-Pattern-Detection"><a href="#LLM-Based-Design-Pattern-Detection" class="headerlink" title="LLM-Based Design Pattern Detection"></a>LLM-Based Design Pattern Detection</h2><p><strong>Authors:Christian Schindler, Andreas Rausch</strong></p>
<p>Detecting design pattern instances in unfamiliar codebases remains a challenging yet essential task for improving software quality and maintainability. Traditional static analysis tools often struggle with the complexity, variability, and lack of explicit annotations that characterize real-world pattern implementations. In this paper, we present a novel approach leveraging Large Language Models to automatically identify design pattern instances across diverse codebases. Our method focuses on recognizing the roles classes play within the pattern instances. By providing clearer insights into software structure and intent, this research aims to support developers, improve comprehension, and streamline tasks such as refactoring, maintenance, and adherence to best practices. </p>
<blockquote>
<p>在陌生的代码库中检测设计模式实例仍然是提高软件质量和可维护性的重要任务，且具有挑战性。传统的静态分析工具往往难以应对现实世界模式实现所特有的复杂性、多变性和缺乏明确注释的问题。在本文中，我们提出了一种利用大型语言模型自动识别不同代码库中的设计模式实例的新方法。我们的方法侧重于识别模式实例中类所扮演的角色。通过更清晰地揭示软件结构和意图，本研究旨在支持开发人员、提高理解力并简化任务，如重构、维护和遵循最佳实践等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18458v1">PDF</a> Submitted Version, that was accepted at PATTERNS 2025</p>
<p><strong>Summary</strong><br>在现代软件开发中，检测未知代码库中的设计模式实例是一项具有挑战但至关重要的任务。传统静态分析工具常因真实世界模式实现的复杂性、多变性和缺乏明确注释而感到困扰。本研究利用大型语言模型提出了一种自动识别多种代码库中设计模式实例的新方法。该方法侧重于识别模式实例中类的角色，旨在为开发者提供更清晰的软件结构和意图洞察，以支持开发、提高理解，并简化如重构、维护和遵循最佳实践等任务。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>检测设计模式中实例在未知代码库中是一个重要且具挑战的任务。</li>
<li>传统静态分析工具在面对真实世界模式实现时，常常受到复杂性、多变性和注释不足的影响。</li>
<li>利用大型语言模型（LLM）可以自动识别多种代码库中的设计模式实例。</li>
<li>所提出的方法侧重于识别模式实例中类的角色。</li>
<li>该研究旨在通过提供更清晰的软件结构和意图洞察来支持开发者。</li>
<li>该方法有助于改进软件质量和可维护性，并简化如重构、维护和遵循最佳实践等任务。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-55a56d415929b4cb9bb3065393724b8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88c2f8a219bd626a5c058a52b54238a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bf32c28c5f286feefa50fc6b7fd3f89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5e7139f212c2e69f305ea0ef0481e01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcf09aa50ac387eac5971055ff2ea72a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a6b3cd7a48cf6a4b60cbc5aae91316.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SWE-RL-Advancing-LLM-Reasoning-via-Reinforcement-Learning-on-Open-Software-Evolution"><a href="#SWE-RL-Advancing-LLM-Reasoning-via-Reinforcement-Learning-on-Open-Software-Evolution" class="headerlink" title="SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open   Software Evolution"></a>SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open   Software Evolution</h2><p><strong>Authors:Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I. Wang</strong></p>
<p>The recent DeepSeek-R1 release has demonstrated the immense potential of reinforcement learning (RL) in enhancing the general reasoning capabilities of large language models (LLMs). While DeepSeek-R1 and other follow-up work primarily focus on applying RL to competitive coding and math problems, this paper introduces SWE-RL, the first approach to scale RL-based LLM reasoning for real-world software engineering. Leveraging a lightweight rule-based reward (e.g., the similarity score between ground-truth and LLM-generated solutions), SWE-RL enables LLMs to autonomously recover a developer’s reasoning processes and solutions by learning from extensive open-source software evolution data – the record of a software’s entire lifecycle, including its code snapshots, code changes, and events such as issues and pull requests. Trained on top of Llama 3, our resulting reasoning model, Llama3-SWE-RL-70B, achieves a 41.0% solve rate on SWE-bench Verified – a human-verified collection of real-world GitHub issues. To our knowledge, this is the best performance reported for medium-sized (&lt;100B) LLMs to date, even comparable to leading proprietary LLMs like GPT-4o. Surprisingly, despite performing RL solely on software evolution data, Llama3-SWE-RL has even emerged with generalized reasoning skills. For example, it shows improved results on five out-of-domain tasks, namely, function coding, library use, code reasoning, mathematics, and general language understanding, whereas a supervised-finetuning baseline even leads to performance degradation on average. Overall, SWE-RL opens up a new direction to improve the reasoning capabilities of LLMs through reinforcement learning on massive software engineering data. </p>
<blockquote>
<p>最近的DeepSeek-R1版本展示了强化学习（RL）在提升大型语言模型（LLM）通用推理能力方面的巨大潜力。虽然DeepSeek-R1和其他后续工作主要关注将RL应用于竞赛编码和数学问题，但本文介绍了SWE-RL，这是第一个将RL为基础的LLM推理扩展到现实世界软件工程的方法。通过利用基于轻量级规则的奖励（例如，真实答案和LLM生成解决方案之间的相似度得分），SWE-RL能够让LLM通过学习大量的开源软件进化数据，来自主恢复开发者的推理过程解决方案——软件整个生命周期的记录，包括其代码快照、代码更改和事件如问题和拉取请求。我们的推理模型是建立在Llama 3之上的，名为Llama3-SWE-RL-70B，在SWE-bench Verified（一个经过人工验证的GitHub实际问题集合）上达到了41.0%的解决率。据我们所知，这是迄今为止中等规模（&lt;100B）LLM的最佳性能表现，甚至可与领先的专有LLM如GPT-4o相媲美。令人惊讶的是，尽管只在软件进化数据上执行RL，Llama3-SWE-RL还展现出了通用的推理技能。例如，它在五个跨域任务中取得了更好的结果，即功能编码、库的使用、代码推理、数学和一般语言理解，而基于监督微调的方法甚至会导致平均性能下降。总的来说，SWE-RL为通过强化学习在大量软件工程数据上提高LLM的推理能力开辟了一个新方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18449v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1及后续工作的成果凸显了强化学习（RL）在提升大型语言模型（LLM）通用推理能力方面的巨大潜力。而本文介绍的SWE-RL是首个将RL应用于真实世界软件工程的LLM推理方法。通过基于轻量级规则的奖励机制，SWE-RL使LLMs能够自主学习开发者推理过程及解决方案，并从大量开源软件进化数据中学习。Llama3-SWE-RL模型在SWE-bench Verified上的解决率高达41.0%，展现了卓越性能。此外，该模型还表现出良好的通用推理能力，在非领域任务中也有优异表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1展示了强化学习在提升大型语言模型通用推理能力方面的潜力。</li>
<li>SWE-RL是首个将强化学习应用于真实世界软件工程的LLM推理方法。</li>
<li>SWE-RL使用基于轻量级规则的奖励机制，使LLMs能够学习开发者推理过程及解决方案。</li>
<li>Llama3-SWE-RL模型在SWE-bench Verified上的解决率达到41.0%，表现卓越。</li>
<li>Llama3-SWE-RL模型展现出良好的通用推理能力，在非领域任务中也有优异表现。</li>
<li>该模型通过强化学习在大量软件工程数据上进行训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-72c5d2f4f5f18807964e28761b373c05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-201bd357539b45c6c33228c1cd922117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e6270e8a3ec843350ae21dfcdd11cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7083a5257f5994c9f162a80b72276fc0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GLEAN-Generalized-Category-Discovery-with-Diverse-and-Quality-Enhanced-LLM-Feedback"><a href="#GLEAN-Generalized-Category-Discovery-with-Diverse-and-Quality-Enhanced-LLM-Feedback" class="headerlink" title="GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced   LLM Feedback"></a>GLEAN: Generalized Category Discovery with Diverse and Quality-Enhanced   LLM Feedback</h2><p><strong>Authors:Henry Peng Zou, Siffi Singh, Yi Nian, Jianfeng He, Jason Cai, Saab Mansour, Hang Su</strong></p>
<p>Generalized Category Discovery (GCD) is a practical and challenging open-world task that aims to recognize both known and novel categories in unlabeled data using limited labeled data from known categories. Due to the lack of supervision, previous GCD methods face significant challenges, such as difficulty in rectifying errors for confusing instances, and inability to effectively uncover and leverage the semantic meanings of discovered clusters. Therefore, additional annotations are usually required for real-world applicability. However, human annotation is extremely costly and inefficient. To address these issues, we propose GLEAN, a unified framework for generalized category discovery that actively learns from diverse and quality-enhanced LLM feedback. Our approach leverages three different types of LLM feedback to: (1) improve instance-level contrastive features, (2) generate category descriptions, and (3) align uncertain instances with LLM-selected category descriptions. Extensive experiments demonstrate the superior performance of \MethodName over state-of-the-art models across diverse datasets, metrics, and supervision settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/amazon-science/Glean">https://github.com/amazon-science/Glean</a>. </p>
<blockquote>
<p>广义类别发现（GCD）是一项实用且具挑战性的开放世界任务，旨在使用来自已知类别的有限标记数据来识别无标记数据中的已知和新型类别。由于缺乏监督，以前的GCD方法面临重大挑战，例如纠正错误实例的困难，以及无法有效发现和利用发现集群的语义含义。因此，通常需要对现实世界的应用进行额外的注释。然而，人工注释成本极高且效率低下。为了解决这些问题，我们提出了GLEAN，这是一个用于广义类别发现的统一框架，能够主动从多样化和质量提高的LLM反馈中学习。我们的方法利用三种不同类型的LLM反馈来改善：（1）实例级对比特征，（2）生成类别描述，以及（3）将不确定的实例与LLM选择的类别描述对齐。大量实验表明，\MethodName在多种数据集、指标和监督设置上的性能均优于最新模型。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/amazon-science/Glean%E3%80%82">https://github.com/amazon-science/Glean。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18414v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GCD任务旨在利用有限的已知类别标签数据，在未被标签的数据中识别已知和新型类别。为应对挑战如纠错困难和发现与利用语义含义的难题，提出了GLEAN框架，利用大型语言模型（LLM）反馈来主动学习，改善实例级别的对比特征、生成类别描述并与不确定实例对齐。实验证明其在不同数据集、指标和监管环境下的性能优于现有模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GCD是一个实用且具有挑战性的开放世界任务，旨在识别已知和新型类别。</li>
<li>现有GCD方法面临纠错困难和发现语义含义的挑战。</li>
<li>为解决这些问题，提出了GLEAN框架，利用LLM反馈进行主动学习。</li>
<li>GLEAN通过三种不同类型的LLM反馈来改善实例级别的对比特征、生成类别描述并与不确定实例对齐。</li>
<li>实验证明GLEAN在多种数据集、指标和监管环境下的性能优于现有模型。</li>
<li>GLEAN代码已公开在GitHub上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e2ebf907991aed9393040906f2e7a7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74330cae3ebeee9552ed049a80419541.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-605cb9af59bcf419549009b717469ad5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2bbc5e50879bb7ef641fe7eef6e7e4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9987938380842524d8820bfbc9a62b3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-952bcb2d121716880db41df992e4da8d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OmniAlign-V-Towards-Enhanced-Alignment-of-MLLMs-with-Human-Preference"><a href="#OmniAlign-V-Towards-Enhanced-Alignment-of-MLLMs-with-Human-Preference" class="headerlink" title="OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference"></a>OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference</h2><p><strong>Authors:Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosong Cao, Weiyun Wang, Jiaqi Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, Haodong Duan, Hua Yang, Kai Chen</strong></p>
<p>Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs’ alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs’ alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at <a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/OmniAlign-V">https://github.com/PhoenixZ810/OmniAlign-V</a>. </p>
<blockquote>
<p>近期开源多模态大语言模型（MLLMs）的进步主要集中在增强基础能力上，但在人类偏好对齐方面存在显著差距。本文介绍了OmniAlign-V，这是一个包含20万高质量训练样本的综合数据集，具有多样化的图像、复杂的问题和不同的响应格式，旨在提高MLLM与人类偏好的对齐程度。我们还推出了MM-AlignBench，这是一个专门设计的人类注释基准，用于评估MLLM与人类价值观的对齐程度。实验结果表明，使用监督微调（SFT）或直接偏好优化（DPO）通过OmniAlign-V对MLLM进行微调，可以显著提高人类偏好对齐程度，同时在标准VQA基准测试中保持或提高性能，保持其基础能力。我们的数据集、基准测试、代码和检查点已发布在<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/OmniAlign-V">https://github.com/PhoenixZ810/OmniAlign-V</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期开源多模态大型语言模型（MLLMs）的进展主要集中在提高基础能力上，在符合人类偏好方面存在显著差距。本文介绍了OmniAlign-V数据集，包含二十万高质量训练样本，涵盖多样化图像、复杂问题和多种答案格式，旨在提高MLLMs与人类偏好的一致性。此外，还推出了MM-AlignBench，这是一个由人类标注的专门评估MLLMs与人类价值观一致性的基准测试。实验结果显示，使用有监督微调（SFT）或直接偏好优化（DPO）对MLLMs进行微调，能显著提高与人类偏好的一致性，同时保持或提升在标准视觉问答基准测试上的性能，保留了其基础能力。相关数据集、基准测试、代码和检查点已发布在：<a target="_blank" rel="noopener" href="https://github.com/PhoenixZ810/OmniAlign-V%E3%80%82">https://github.com/PhoenixZ810/OmniAlign-V。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniAlign-V数据集包含二十万高质量训练样本，旨在提高多模态大型语言模型与人类偏好的一致性。</li>
<li>数据集涵盖多样化图像、复杂问题和多种答案格式，提供全面的训练样本。</li>
<li>MM-AlignBench是一个专门评估多模态大型语言模型与人类价值观一致性的基准测试。</li>
<li>使用有监督微调或直接偏好优化方法可以提高多模态大型语言模型与人类偏好的一致性。</li>
<li>在提高一致性的同时，能够保持或提升模型在标准视觉问答基准测试上的性能。</li>
<li>该研究的相关数据集、基准测试、代码和检查点已公开发布，便于后续研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18411">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2419f3d12c2d3135529eaa10a409f109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-639b80b54fd4d5d62075121ae97981e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83395193c4ba76e2bb54b32621414183.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1653be6a9f2796fa56686acb2f8a38d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a925ce646e5540336ec220019245b2c8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MindMem-Multimodal-for-Predicting-Advertisement-Memorability-Using-LLMs-and-Deep-Learning"><a href="#MindMem-Multimodal-for-Predicting-Advertisement-Memorability-Using-LLMs-and-Deep-Learning" class="headerlink" title="MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs   and Deep Learning"></a>MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs   and Deep Learning</h2><p><strong>Authors:Sepehr Asgarian, Qayam Jetha, Jouhyun Jeon</strong></p>
<p>In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearman’s correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market. </p>
<blockquote>
<p>在广告竞争激烈的背景下，成功关键在于有效驾驭并利用消费者、广告商和广告平台之间的复杂互动。这些多方面的互动促使广告商优化策略，对消费者行为进行建模、提高品牌认知度，并量身定制广告内容。针对这些挑战，我们推出了MindMem，这是一款用于广告记忆力的多模态预测模型。通过整合文本、视觉和听觉数据，MindMem达到了前沿的性能表现，在LAMBDA数据集上的斯皮尔曼相关系数达到0.631，在Memento10K数据集上达到0.731，持续超越现有方法。此外，我们的分析确定了影响广告记忆力的关键因素，如视频节奏、场景复杂性和情感共鸣。在此基础上，我们推出了MindMem-ReAd（MindMem驱动再生广告），采用基于大型语言模型的模拟来优化广告内容和放置，使得广告记忆力提高了高达74.12%。我们的研究结果突出了人工智能在广告中的变革潜力，为广告商提供了一个强大的工具，可以推动参与感、提高竞争力，并在快速变化的市场中最大限度地发挥作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18371v1">PDF</a> 7 pages, 5 figures, 4 Tables, AAAI 2025 Economics of Modern ML:   Markets, Incentives, and Generative AI Workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在广告竞争激烈的市场环境中，通过构建多模态预测模型MindMem提高广告记忆度的技术。该模型融合了文本、视觉和听觉数据，通过优化广告内容和投放策略，实现广告记忆度的显著提升。研究表明，MindMem在LAMBDA和Memento10K数据集上的表现均优于现有方法，同时分析确定了视频节奏、场景复杂性和情感共鸣等关键影响广告记忆度的因素。此外，推出的MindMem-ReAd通过大型语言模型模拟优化广告内容和投放，提高了广告的记忆度。这些发现展示了人工智能在广告领域的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MindMem模型通过融合多模态数据提升广告记忆度。</li>
<li>在LAMBDA和Memento10K数据集上的表现超越现有方法，表现出较高的预测准确性。</li>
<li>分析确定了视频节奏、场景复杂性和情感共鸣等影响广告记忆度的关键因素。</li>
<li>MindMem-ReAd通过大型语言模型模拟优化广告内容和投放。</li>
<li>MindMem-ReAd实现了广告记忆度的显著提升，达到最高提升74.12%。</li>
<li>这些技术成果展示了人工智能在广告领域的巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e50875c5484ed7f4d0b4bdab2d7729a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de859067494b34de9176d9ccdbcbe4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc88cdeb32ab652176a6d46ef9d51f7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72d8e78986c5d248fcd0a11f4bcf834f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d92cea00a1cb4cff490d1fec90c7aef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb85f222e8d56f973e1f1554708b7a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20862ea94f350e7f6aa7381b0b1f0a7c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BRIDO-Bringing-Democratic-Order-to-Abstractive-Summarization"><a href="#BRIDO-Bringing-Democratic-Order-to-Abstractive-Summarization" class="headerlink" title="BRIDO: Bringing Democratic Order to Abstractive Summarization"></a>BRIDO: Bringing Democratic Order to Abstractive Summarization</h2><p><strong>Authors:Junhyun Lee, Harshith Goka, Hyeonmok Ko</strong></p>
<p>Hallucination refers to the inaccurate, irrelevant, and inconsistent text generated from large language models (LLMs). While the LLMs have shown great promise in a variety of tasks, the issue of hallucination still remains a major challenge for many practical uses. In this paper, we tackle the issue of hallucination in abstract text summarization by mitigating exposure bias. Existing models targeted for exposure bias mitigation, namely BRIO, aim for better summarization quality in the ROUGE score. We propose a model that uses a similar exposure bias mitigation strategy but with a goal that is aligned with less hallucination. We conjecture that among a group of candidate outputs, ones with hallucinations will comprise the minority of the whole group. That is, candidates with less similarity with others will have a higher chance of containing hallucinated content. Our method uses this aspect and utilizes contrastive learning, incentivizing candidates with high inter-candidate ROUGE scores. We performed experiments on the XSum and CNN&#x2F;DM summarization datasets, and our method showed 6.25% and 3.82% improvement, respectively, on the consistency G-Eval score over BRIO. </p>
<blockquote>
<p>幻觉指的是由大型语言模型（LLM）生成的不准确、不相关和不一致的文本。虽然LLM在各种任务中显示出巨大的潜力，但幻觉问题仍然是许多实际应用中的主要挑战。在本文中，我们通过减轻暴露偏见来解决抽象文本摘要中的幻觉问题。针对暴露偏见缓解的现有模型，即BRIO，旨在提高ROUGE分数的摘要质量。我们提出了一种使用类似暴露偏见缓解策略的模型，但其目标是与减少幻觉相一致。我们猜想在一组候选输出中，带有幻觉的将是整个小组中的少数。也就是说，与其他候选者相似度较低的候选者更有可能包含虚构内容。我们的方法利用这一特点，采用对比学习，激励具有较高候选者间ROUGE分数的候选者。我们在XSum和CNN&#x2F;DM摘要数据集上进行了实验，我们的方法在一致性G-Eval分数上相对于BRIO分别提高了6.25%和3.82%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18342v1">PDF</a> 13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready</p>
<p><strong>Summary</strong><br>文本主要探讨了大型语言模型（LLM）中的幻想问题，并提出了一种通过减少暴露偏见来解决抽象文本摘要中幻想问题的方法。新方法使用对比学习，激励高ROUGE得分的候选摘要，并通过实验证明了其提高了一致性G-Eval分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本详细阐述了大型语言模型（LLM）中的幻想问题及其对于实际应用的主要挑战。</li>
<li>现存的模型如BRIO旨在通过减轻暴露偏见来提高摘要质量。</li>
<li>提出的模型采用类似的策略，但目标更侧重于减少幻想。</li>
<li>作者假设在候选输出中，含有幻想的文本将占少数。与其他候选摘要相似度较低的文本更可能包含幻想内容。</li>
<li>新的方法利用这一观点并采用对比学习，激励高ROUGE得分的候选摘要。</li>
<li>在XSum和CNN&#x2F;DM摘要数据集上的实验表明，新方法在一致性G-Eval分数上相对于BRIO有所提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7f30a24b21765977cbc69129d49a870f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecc959041027dda0ee945d4f4b34aa2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3e7d54fd24dff505ed42771b02dfcc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e93174274cc4c89393177f0b127cc185.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models"><a href="#From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models" class="headerlink" title="From System 1 to System 2: A Survey of Reasoning Large Language Models"></a>From System 1 to System 2: A Survey of Reasoning Large Language Models</h2><p><strong>Authors:Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu</strong></p>
<p>Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI’s o1&#x2F;o3 and DeepSeek’s R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{<a target="_blank" rel="noopener" href="https://github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub">https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub</a> Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field. </p>
<blockquote>
<p>实现人类水平的智能需要完善从快速直觉系统1到较慢、更慎重的系统2推理的过渡。系统1擅长快速启发式决策，而系统2则依赖于逻辑推理以做出更准确的判断和减少偏见。基础大型语言模型（LLM）擅长快速决策，但在复杂推理方面有所欠缺，因为它们尚未完全接受系统2思维所特有的逐步分析步骤。最近，像OpenAI的o1&#x2F;o3和DeepSeek的R1等推理LLM在数学和编程等领域表现出了专家级的性能，它们模仿了系统2的深思熟虑推理，展示了人类般的认知能力。这篇综述首先简要概述了基础LLM和系统2技术的早期发展进展，探讨了它们的结合如何为推理LLM铺平道路。接下来，我们将讨论如何构建推理LLM，分析其特点，介绍实现高级推理的核心方法，以及各类推理LLM的演变。此外，我们还概述了推理基准测试，深入比较了代表性推理LLM的性能。最后，我们探讨了推进推理LLM的有前途的方向，并通过实时GitHub仓库来跟踪最新进展。我们希望这篇综述能作为宝贵资源，激发这一快速演变领域的创新并推动其进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17419v2">PDF</a> Slow-thinking, Large Language Models, Human-like Reasoning, Decision   Making in AI, AGI</p>
<p><strong>Summary</strong></p>
<p>该文探讨了实现人类水平智能需要完善的从快速直觉系统一（System 1）到较慢但更审慎的系统二（System 2）推理的转变。基础大型语言模型（LLMs）擅长快速决策，但缺乏复杂推理的深度，尚未全面采用真正的系统二逐步分析特性。最近，如OpenAI的o1&#x2F;o3和DeepSeek的R1等推理LLM已在数学和编码等领域展现出专家级性能，模拟系统二的审慎推理并展示人类般的认知能力。本文概述了基础LLM和系统二技术的早期发展，探讨了它们如何结合为推理LLM铺平道路，并讨论了如何构建推理LLM，分析其特点、核心方法和各种推理LLM的演变。此外，本文还提供了对推理基准测试的概述，深入比较了代表性推理LLM的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>实现人类水平智能需要完善从快速直觉决策到较慢但更审慎的系统二推理的转变。</li>
<li>基础大型语言模型（LLMs）虽擅长快速决策，但在复杂推理方面存在深度不足。</li>
<li>推理LLM如OpenAI的o1&#x2F;o3和DeepSeek的R1已在特定领域展现出专家级性能，模拟系统二的审慎推理。</li>
<li>推理LLM的构建涉及对模型特性的分析、核心方法的采用以及不同推理LLM的演变。</li>
<li>推理基准测试提供了评估LLM性能的标准，深入比较了代表性推理LLM之间的性能差异。</li>
<li>结合基础LLM和系统二技术为推理LLM的发展铺平了道路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-910fca5899b2f6202effa76cd13e2189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbb3d1cc08dc9913ec8a49f6d08f8be0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf37d8e00c34d7d4f34a141ea26291c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e277db38961f6e3017eb812bafcfa3e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4386b526a8f491f0885903a09e234e7a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level “novelty.” Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. </p>
<blockquote>
<p>数据的多样性对于大语言模型的指令调整至关重要。现有研究已经探索了多种数据多样性的意识数据选择方法，以构建高质量数据集并增强模型性能。然而，关于精确定义和测量数据多样性的基本问题仍然缺乏足够的探索，这限制了数据工程的明确指导。为了解决这一问题，我们通过大量微调实验评估了现有1 1种多样性测量方法与模型性能的相关性。我们的结果表明，可靠的多样性度量应适当考虑样本之间的差异以及样本空间中的信息分布。在此基础上，我们提出了基于样本级别“新颖性”的NovelSum新多样性指标。在模拟和真实数据上的实验表明，NovelSum准确捕捉了多样性变化，与指令调整模型性能的相关性达到0.97，突显其在指导数据工程实践中的价值。以NovelSum为优化目标，我们进一步开发了一种贪婪的、面向多样性的数据选择策略，其性能优于现有方法，验证了我们的指标的有效性和实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v2">PDF</a> 15 pages. The related codes and resources will be released later.   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong></p>
<p>数据多样性对于大语言模型的指令调整至关重要。现有研究已经探索了各种基于多样性的数据选择方法以构建高质量数据集并增强模型性能。然而，如何精确定义和测量数据多样性的基础问题尚未得到充分探索，这限制了数据工程实践的明确指导。为解决这一问题，我们系统地分析了现有的11种多样性测量方法，并通过大量的微调实验评估它们与模型性能的相关性。结果表明，可靠的多样性测量应适当考虑样本间的差异以及样本空间中的信息分布。基于此，我们提出了基于样本级“新颖性”的NovelSum新多样性指标。在模拟数据和真实数据上的实验表明，NovelSum准确捕捉多样性变化，与指令调整模型性能的相关性达到0.97，突显其在指导数据工程实践中的价值。以NovelSum为优化目标，我们进一步开发了一种贪婪的、以多样性为导向的数据选择策略，优于现有方法，验证了我们的指标的有效性和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据多样性对大型语言模型的指令调整至关重要。</li>
<li>现有研究已经探索了多种数据选择方法以提升模型性能。</li>
<li>现有的数据多样性定义和测量方法仍然处于未充分探索阶段，缺乏明确的指导原则用于数据工程实践。</li>
<li>通过实验评估了多种多样性测量方法的有效性，发现应结合考虑样本间的差异和样本空间中的信息分布来定义可靠的多样性测量标准。</li>
<li>提出了一种新的多样性指标NovelSum，基于样本级“新颖性”，能够准确捕捉数据多样性变化并与模型性能高度相关。</li>
<li>NovelSum在模拟数据和真实数据上的实验表现出其有效性，与指令调整模型性能的相关性达到0.97。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-39bd68b06f8cf5b4b6d780976b944e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e13d1819d42fc8227602d59168d40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73708b42657d862618f4d0f17d41eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef13d395163cfa16cb12d2c25d19dd20.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking"><a href="#Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking" class="headerlink" title="Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking"></a>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking</h2><p><strong>Authors:Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</strong></p>
<p>Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2%, and outperforms Transformer&#x2F;Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways. </p>
<blockquote>
<p>大型语言模型（LLM）在参数约束下面临着固有的性能瓶颈，特别是在处理需要复杂推理的关键令牌时。经验分析表明，挑战令牌会在各层之间引发突然的梯度峰值，暴露了标准Transformer中的架构应力点。基于这一见解，我们提出了内思考Transformer（ITT），它重新想象层计算为隐式思考步骤。ITT通过自适应令牌路由动态分配计算，通过残差思考连接迭代优化表示，并使用思考步骤编码来区分推理阶段。ITT能够在不增加参数的情况下，对关键令牌进行更深入的处理。在1.6亿至近4亿的参数模型评估中，ITT实现了仅使用近一半参数（即近亿参数）即可达到近百分之九十六的性能表现，减少了百分之四十三的训练数据，并在十一项基准测试中优于Transformer&#x2F;Loop变体。通过推理过程中的弹性计算分配，ITT通过隐式思考路径的架构优化来平衡性能和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13842v2">PDF</a> 15 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理需要复杂推理的关键令牌时面临性能瓶颈。本文提出一种名为Inner Thinking Transformer（ITT）的新型架构，它通过重新想象层计算作为隐式思考步骤来解决这一问题。ITT通过动态计算分配、残差思维连接以及思维步骤编码等技术实现性能提升。在参数限制下，ITT能够实现对关键令牌的深度处理而不增加参数。实验表明，ITT在参数减少的情况下实现了高性能，并优化了隐式思考路径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在处理复杂推理的关键令牌时存在性能瓶颈。</li>
<li>ITT通过重新设计Transformer架构来改进性能，将层计算视为隐式思考步骤。</li>
<li>ITT通过动态分配计算资源、残差思维连接和思维步骤编码等技术实现性能提升。</li>
<li>ITT能够在参数限制下实现对关键令牌的深度处理。</li>
<li>ITT在参数减少的情况下实现了高性能，实现了96.5%的性能提升。</li>
<li>ITT能够减少训练数据使用量，降低了43.2%的训练数据需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bcf485a95d0b50b09c9a5fe774131b52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a5bfe87abe44f5efe73053d519d088e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a55452d6bdf7e6f2a7a123878c59a2ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e40220fd414dace1b0c8e0be8cb71bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f77f7318064117728c2f612e77128504.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mol-LLaMA-Towards-General-Understanding-of-Molecules-in-Large-Molecular-Language-Model"><a href="#Mol-LLaMA-Towards-General-Understanding-of-Molecules-in-Large-Molecular-Language-Model" class="headerlink" title="Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular   Language Model"></a>Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular   Language Model</h2><p><strong>Authors:Dongki Kim, Wonbin Lee, Sung Ju Hwang</strong></p>
<p>Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to users’ queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis. Our project page is at <a target="_blank" rel="noopener" href="https://mol-llama.github.io/">https://mol-llama.github.io/</a>. </p>
<blockquote>
<p>理解分子是理解生物体和推动药物发现进展的关键，这需要跨越化学和生物学的跨学科知识。尽管大型分子语言模型在解释分子结构方面取得了显著的成功，但它们的指令数据集仅限于任务导向数据集的具体知识，并没有完全覆盖分子的基本特征，阻碍了它们作为通用分子助理的能力。为了解决这一问题，我们提出了Mol-LLaMA，这是一个大型分子语言模型，通过多模式指令调整，掌握以分子为中心的一般知识。为此，我们设计了关键数据类型，涵盖分子的基本特征，融入来自分子结构的基本知识。此外，为了提高对分子特征的理解，我们引入了一个模块，该模块能够整合来自不同分子编码器的补充信息，利用不同分子表征的独特优势。我们的实验结果表明，Mol-LLaMA能够理解分子的一般特征，并对用户的查询生成相关响应，提供详细解释，这表明其作为通用分子分析助理的潜力。我们的项目页面是<a target="_blank" rel="noopener" href="https://mol-llama.github.io/%E3%80%82">https://mol-llama.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13449v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mol-llama.github.io/">https://mol-llama.github.io/</a></p>
<p><strong>Summary</strong><br>分子是生物体的关键组成部分，也是药物研发的基础。目前的大型分子语言模型虽然成功应用于解读分子结构，但由于训练数据集仅限于特定任务导向的数据集，未能全面覆盖分子的基本特性，限制了其作为通用分子助手的能力。为解决这一问题，我们推出Mol-LLaMA模型，通过多模态指令微调聚焦于分子的一般知识。我们设计涵盖分子基本特征的关键数据类型，同时引入模块整合不同分子编码器的补充信息，充分利用各种分子表征的独特优势。实验结果显示，Mol-LLaMA能够理解分子的基本特征，并针对用户查询生成详细解释，显示出作为通用分子分析助手的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>理解分子对了解生物体和药物研发的重要性。</li>
<li>大型分子语言模型在解读分子结构方面已取得了显著成功。</li>
<li>当前模型受限于特定任务导向的数据集，未能全面覆盖分子的基本特性。</li>
<li>Mol-LLaMA模型通过多模态指令微调，旨在理解分子的通用知识。</li>
<li>Mol-LLaMA设计涵盖分子基本特征的关键数据类型。</li>
<li>模型引入整合不同分子编码器信息的模块，以提高对分子特征的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e50b8bd8a14b296fcc1abf77e73306a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e81a21080db2396b1155e2ac4b3c4222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63ff8ac10d2f16354718bc78b4a91289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d90d6d92b402f38f490ed33c20c784c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis"><a href="#Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis" class="headerlink" title="Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis"></a>Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis</h2><p><strong>Authors:Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Ning Wang, Zhenhong Long, Jiaojiao Zhao, Zipeng Wang, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian</strong></p>
<p>DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we evaluate the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, and their corresponding 4-bit quantized models on the enhanced A-Eval benchmark, A-Eval-2.0. By comparing original instruction-tuned models with their distilled counterparts, we analyze how reasoning enhancements impact performance across diverse practical tasks. Our results show that reasoning-enhanced models, while generally powerful, do not universally outperform across all tasks, with performance gains varying significantly across tasks and models. To further assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications and intuitive line charts. Specific examples provide actionable insights to help users select and deploy the most cost-effective DeepSeek models, ensuring optimal performance and resource efficiency in real-world applications. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmarks and periodically update this paper to provide more comprehensive and accurate evaluation results. Please refer to the latest version of the paper for the most recent results and conclusions. </p>
<blockquote>
<p>DeepSeek-R1以其低训练成本和出色的推理能力而著称，已在各种基准测试中达到了最先进的性能。然而，从实际应用的角度进行的详细评估仍然缺乏，这使得用户难以针对其特定需求选择最合适的DeepSeek模型。为了弥补这一空白，我们对DeepSeek-V3、DeepSeek-R1、DeepSeek-R1-Distill-Qwen系列、DeepSeek-R 修智慧之追求以及对应具备“飞泉蒸馏”（Llama系列）的模型进行了评估，并在增强的A-Eval基准测试（A-Eval-2.0）上对其进行了4位量化模型的测试。通过比较原始指令调优模型与其蒸馏对应模型，我们分析了推理增强如何影响不同实际任务的性能。结果表明，虽然推理增强模型通常功能强大，但并非在所有任务上都表现出最佳性能，任务与模型之间的性能提升差异显著。为了进一步帮助用户选择模型，我们通过性能层次分类和直观的线形图来衡量DeepSeek模型的能力边界。具体的示例提供了切实可行的见解，帮助用户选择和部署最具成本效益的DeepSeek模型，确保在实际应用中实现最佳性能和资源效率。值得注意的是，尽管我们努力建立全面、客观、权威的评估基准测试，但测试样本的选择、数据分布的特征以及评估标准的设定不可避免地会对评估结果引入一定的偏见。我们将不断优化评估基准测试并定期更新本文内容，以提供更全面和准确的评估结果。有关最新的成果和结论，请参考最新版本的论文。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11164v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于其较低的训练成本和出色的推理能力，DeepSeek-R1已在各种基准测试中达到最新技术表现水平。然而，对于其在现实世界应用中的详细评估仍显不足，使得用户难以为其特定需求选择最合适的DeepSeek模型。为解决这一空白，我们针对DeepSeek-V3系列、DeepSeek-R1系列及其蒸馏版本（包括DeepSeek-R1-Distill-Qwen和DeepSeek-R1-Distill-Llama系列），以及它们的相应4位量化模型，在增强的A-Eval基准测试（A-Eval-2.0）上进行了评估。通过比较原始指令微调模型与其蒸馏对应模型，我们分析了推理增强如何影响不同实际任务的性能。结果显示，虽然推理增强模型通常强大，但并不普遍适用于所有任务，性能提升在不同任务和模型中差异显著。为了进一步帮助用户进行模型选择，我们通过性能分级和直观图表量化了DeepSeek模型的能力边界。通过具体示例，我们提供了可操作性的见解，帮助用户选择和部署最具成本效益的DeepSeek模型，确保在真实世界应用中实现最佳性能和资源效率。值得注意的是，尽管我们努力建立全面、客观、权威的评估基准，但测试样本的选择、数据分布特征和评估标准的设定可能会给评估结果带来一定的偏见。我们将持续优化评估基准并定期更新本论文，以提供更全面和准确的评估结果。有关最新结果和结论，请参考论文最新版本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比多种DeepSeek模型（包括原始指令微调模型和蒸馏模型）在增强的A-Eval基准测试上的表现。</li>
<li>发现推理增强模型在多样化实际任务中的性能差异显著，并非所有任务都表现出普遍优势。</li>
<li>通过性能分级和直观图表量化DeepSeek模型的能力边界，为用户提供模型选择的指导。</li>
<li>提供具体示例和可操作性的见解，帮助用户根据性能和资源效率选择最合适的DeepSeek模型。</li>
<li>指出评估过程中存在的潜在偏见，并强调持续优化评估基准的重要性。</li>
<li>强调定期更新论文以提供最新评估结果的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-968d4720f2c22de097eca9cc8836c673.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b19085bb5b8545585e97993ec51bc97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b4984e02e3f12f3b43e574d41cb5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c6e6ac92ff1fe2cda5abfe1b99bfe14.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TractoGPT-A-GPT-architecture-for-White-Matter-Segmentation"><a href="#TractoGPT-A-GPT-architecture-for-White-Matter-Segmentation" class="headerlink" title="TractoGPT: A GPT architecture for White Matter Segmentation"></a>TractoGPT: A GPT architecture for White Matter Segmentation</h2><p><strong>Authors:Anoushkrit Goel, Simroop Singh, Ankita Joshi, Ranjeet Ranjan Jha, Chirag Ahuja, Aditya Nigam, Arnav Bhavsar</strong></p>
<p>White matter bundle segmentation is crucial for studying brain structural connectivity, neurosurgical planning, and neurological disorders. White Matter Segmentation remains challenging due to structural similarity in streamlines, subject variability, symmetry in 2 hemispheres, etc. To address these challenges, we propose TractoGPT, a GPT-based architecture trained on streamline, cluster, and fusion data representations separately. TractoGPT is a fully-automatic method that generalizes across datasets and retains shape information of the white matter bundles. Experiments also show that TractoGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores. We use TractoInferno and 105HCP datasets and validate generalization across dataset. </p>
<blockquote>
<p>白质束分割对于研究脑结构连接、神经外科规划和神经疾病至关重要。由于流线结构相似性、个体差异、两个半球对称性等因素，白质分割仍然具有挑战性。为了解决这些挑战，我们提出了TractoGPT，这是一种基于GPT的架构，分别训练流线、集群和融合数据表示。TractoGPT是一种全自动方法，可跨数据集推广并保留白质束的形状信息。实验还表明，TractoGPT的平均DICE、重叠和过度重叠分数优于最先进的方法。我们使用TractoInferno和105HCP数据集验证跨数据集的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15464v2">PDF</a> Accepted as a conference paper at 23rd IEEE International Symposium   on Biomedical Imaging 2025. IEEE holds the copyright for this publication</p>
<p><strong>Summary</strong></p>
<p>本文介绍了白质束分割在研究脑结构连接性、神经手术规划和神经疾病方面的重要性。针对白质分割的挑战，提出了一种基于GPT的架构TractoGPT，该架构分别训练流线、集群和融合数据表示。TractoGPT是一种全自动方法，可跨数据集进行推广并保留白质束的形状信息。实验表明，TractoGPT在平均DICE、重叠和覆盖得分方面优于现有先进技术。使用TractoInferno和105HCP数据集验证了跨数据集的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>白质束分割在研究脑结构连接性、神经手术规划和神经疾病方面具有重要意义。</li>
<li>当前白质分割面临诸多挑战，如流线结构相似性、主题差异、两侧大脑对称性等问题。</li>
<li>提出了一种名为TractoGPT的GPT-based架构，用于解决白质分割的挑战。</li>
<li>TractoGPT通过分别训练流线、集群和融合数据表示来实现全自动分割。</li>
<li>TractoGPT可跨数据集进行推广，并保留白质束的形状信息。</li>
<li>实验结果显示，TractoGPT在多个评估指标上优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-39043502ce05245fa3ac6c6b3ab68936.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-732e19bfc71ed16289e2b8b4696b3781.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7aad7cc5474fabec43c2d9547de5c3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32dc17a176d911de87c9ed0b83f9bed2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf4d70788fe5a85eb05230a916692d3c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Does-Table-Source-Matter-Benchmarking-and-Improving-Multimodal-Scientific-Table-Understanding-and-Reasoning"><a href="#Does-Table-Source-Matter-Benchmarking-and-Improving-Multimodal-Scientific-Table-Understanding-and-Reasoning" class="headerlink" title="Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning"></a>Does Table Source Matter? Benchmarking and Improving Multimodal   Scientific Table Understanding and Reasoning</h2><p><strong>Authors:Bohao Yang, Yingji Zhang, Dong Liu, André Freitas, Chenghua Lin</strong></p>
<p>Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences. While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities. We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions. Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities. Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity. Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/MMSci_Table">https://github.com/Bernard-Yang/MMSci_Table</a>. </p>
<blockquote>
<p>最近的大型语言模型（LLM）已经具备了先进的表格理解能力，但它们依赖于将表格转换为文本序列。虽然多模态大型语言模型（MLLM）能够实现直接视觉处理，但由于固定输入图像分辨率和数值推理能力不足，它们在处理科学表格时面临局限。我们提出了一个具有动态输入图像分辨率的多模态科学表格理解与推理的综合框架。我们的框架包括三个关键组成部分：（1）MMSci-Pre，一个包含52K科学表格结构识别样本的特定领域表格结构学习数据集，（2）MMSci-Ins，一个包含12K样本的指令调整数据集，涵盖三个基于表格的任务，（3）MMSci-Eval，一个专门设计用于评估数值推理能力的基准测试，包含3114个测试样本。大量实验表明，我们的特定领域方法使用52K科学表格图像相比使用15万张通用领域表格实现了卓越的性能，这凸显了数据质量优于数量的重要性。我们提出的多模态LLM模型具有动态输入分辨率，在通用表格理解和数值推理能力方面均实现了显著改进，并在独立数据集上具有强大的泛化能力。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/Bernard-Yang/MMSci_Table%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Bernard-Yang/MMSci_Table上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13042v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期的大型语言模型（LLM）在表格理解方面有所进步，但仍需将表格转换为文本序列。多模态大型语言模型（MLLM）虽可实现直接视觉处理，但因固定输入图像分辨率和数值推理能力不足，在处理科学表格时存在局限。我们提出了一个包含动态输入图像分辨率的多模态科学表格理解与推理的综合框架，由MMSci-Pre（5.2万科学表格结构识别样本的领域特定表格结构学习数据集）、MMSci-Ins（1.2万样本的指令调整数据集，涵盖三个表格任务）和MMSci-Eval（专门用于评估数值推理能力的3114个测试样本的基准测试）三个关键组成部分构成。实验表明，与15万通用域表格相比，我们的领域特定方法在处理5.2万科学表格图像时表现出卓越性能，突显了数据质量而非数量的重要性。我们提出的具有动态输入分辨率的基于表格的MLLM在通用表格理解和数值推理能力上均有显著提高，对未公开数据集具有很强的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期LLM在表格理解方面取得进步，但仍需转换表格为文本序列。</li>
<li>MLLM虽支持直接视觉处理，但在处理科学表格时存在固定输入图像分辨率和数值推理能力方面的局限。</li>
<li>提出一个综合框架，包含动态输入图像分辨率的多模态科学表格理解与推理。</li>
<li>综合框架由三个关键组成部分构成：MMSci-Pre、MMSci-Ins和MMSci-Eval。</li>
<li>实验显示，领域特定方法在处理科学表格图像时表现优异，强调数据质量的重要性。</li>
<li>基于表格的MLLM具有动态输入分辨率，在通用表格理解和数值推理能力上显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f787aa6d9ffe525fed063602c5a759e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae25ebe38e86698b28773c2c4b27b78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fc2b5699f5854009b6072e35aa84c4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-890c8d785bcd4f9a64317321dce29c1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b66c9d0e074938268b5f55e4e28b9e1d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering"><a href="#Question-to-Question-Retrieval-for-Hallucination-Free-Knowledge-Access-An-Approach-for-Wikipedia-and-Wikidata-Question-Answering" class="headerlink" title="Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering"></a>Question-to-Question Retrieval for Hallucination-Free Knowledge Access:   An Approach for Wikipedia and Wikidata Question Answering</h2><p><strong>Authors:Santhosh Thottingal</strong></p>
<p>This paper introduces an approach to question answering over knowledge bases like Wikipedia and Wikidata by performing “question-to-question” matching and retrieval from a dense vector embedding store. Instead of embedding document content, we generate a comprehensive set of questions for each logical content unit using an instruction-tuned LLM. These questions are vector-embedded and stored, mapping to the corresponding content. Vector embedding of user queries are then matched against this question vector store. The highest similarity score leads to direct retrieval of the associated article content, eliminating the need for answer generation. Our method achieves high cosine similarity ( &gt; 0.9 ) for relevant question pairs, enabling highly precise retrieval. This approach offers several advantages including computational efficiency, rapid response times, and increased scalability. We demonstrate its effectiveness on Wikipedia and Wikidata, including multimedia content through structured fact retrieval from Wikidata, opening up new pathways for multimodal question answering. </p>
<blockquote>
<p>本文介绍了一种在知识库（如Wikipedia和Wikidata）上进行问答的方法，通过执行“问题到问题”的匹配和从密集向量嵌入存储库中进行检索来实现。我们不为文档内容生成嵌入，而是使用指令调整的大型语言模型（LLM）为每个逻辑内容单元生成一组全面的问题。这些问题被嵌入向量并存储，映射到相应的内容。然后，将用户查询的向量嵌入与此问题向量存储库进行匹配。最高相似度得分将直接导致直接检索相关的文章内容，无需生成答案。我们的方法实现了相关问题对的高余弦相似性（&gt; 0.9），可实现高度精确的检索。这种方法提供了几个优点，包括计算效率高、响应速度快和可扩展性强。我们在Wikipedia和Wikidata上展示了其有效性，包括通过从Wikidata进行结构化事实检索来包含多媒体内容，为多媒体问答打开了新的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11301v3">PDF</a> </p>
<p><strong>Summary</strong>：<br>本文介绍了一种通过执行“问题到问题”匹配和从密集向量嵌入存储中进行检索，实现在知识库（如Wikipedia和Wikidata）上进行问答的方法。该方法不嵌入文档内容，而是为逻辑内容单元生成一组全面的问题，并使用指令微调的大型语言模型进行向量嵌入和存储。然后，将用户查询的向量嵌入与问题向量存储进行匹配。最高相似度得分直接导致直接检索相关文章内容，无需生成答案。此方法在Wikipedia和Wikidata上取得了较高的余弦相似度（&gt; 0.9），包括通过从Wikidata检索结构化事实来检索多媒体内容，为多媒体问答开辟了新途径，具有计算效率高、响应速度快和可扩展性强等优点。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>引入了一种新的问答匹配和检索方法，通过密集向量嵌入存储进行“问题到问题”匹配。</li>
<li>该方法针对知识库（如Wikipedia和Wikidata）中的内容进行操作。</li>
<li>方法不嵌入文档内容，而是为逻辑内容单元生成一系列问题，并使用指令微调的大型语言模型进行向量嵌入和存储。</li>
<li>通过匹配用户查询的向量嵌入与问题向量存储，实现精确检索相关文章内容。</li>
<li>该方法实现了高余弦相似度（&gt; 0.9），提高了检索的准确性和效率。</li>
<li>此方法具有计算效率高、响应速度快和可扩展性强等优点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ea77df0c06b7e28df20aff752d5e1e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ef7522c412876487ecaffe5967303a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dca9d1367e4496207d775c52be059c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a8f5a326170a70d3e732f5fd775f6e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b4c66cc698d7229edd2d296d4c782b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications – such as author-publication history, author affiliation, research topics, and citation counts – we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>科研团队内部的动态对于决定研究产出的性质和影响力至关重要。然而，现有的基于自我报告和聚类的作者角色分类方法缺乏对贡献的全面情境分析。因此，我们提出了一种利用先进的大型语言模型（LLM）对科研团队中的作者角色进行分类的变革性方法，相比传统的聚类方法，它提供了更为精细的分析。具体来说，我们旨在通过利用开源和专有LLM（如GPT-4、Llama3 70B、Llama2 70B和Mistral 7x8B）来补充和增强这些方法，用于角色分类。通过少量提示，我们对作者角色进行分类，并证明GPT-4在多个类别中的表现优于其他模型，超越了传统的XGBoost和BERT等方法。我们的方法还包括建立一个基于深度学习的预测模型，使用10个特征进行训练。该模型在OpenAlex数据库衍生的数据集上进行训练，该数据库提供了有关学术出版物（如作者出版历史、作者关联信息、研究主题和引用次数等）的详细元数据，我们获得了0.76的F1分数，证明了作者角色分类的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v3">PDF</a> Accepted by Quantitative Science Studies (QSS)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了科学团队动力学对研究产出的性质和影响力的重要性。现有的基于自我报告和聚类的作者角色分类方法缺乏全面的贡献分析，因此提出了利用先进的LLM技术重新定义和补充现有的研究方法，并利用少量提示进行作者角色的分类。通过GPT-4等模型，研究证明GPT-4在多类别中表现优于其他模型和传统方法如XGBoost和BERT。利用来自OpenAlex数据库的详细元数据构建的预测深度学习模型也实现了较高的F1分数，表明对作者角色的稳健分类。总体而言，这一技术提升了科研领域的作者角色分类方法，有助于提高研究的效率和质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>科学团队动力学在研究产出的影响中具有重要作用。</li>
<li>基于自我报告和聚类的现有作者角色分类方法缺乏全面的贡献分析。</li>
<li>利用先进的LLM技术（如GPT-4等）进行作者角色分类提供更精细的分析。</li>
<li>GPT-4在多个类别中的表现优于其他模型和传统方法（如XGBoost和BERT）。</li>
<li>利用OpenAlex数据库的详细元数据构建的深度学习模型实现了较高的F1分数。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48d7104f8458fc3d0d86e98ccf4b453c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7004172d9fb6699b9a6091ca0d081d94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5617eb243b81c147dba300cef9fd59cf.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning"><a href="#AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning" class="headerlink" title="AgentRefine: Enhancing Agent Generalization through Refinement Tuning"></a>AgentRefine: Enhancing Agent Generalization through Refinement Tuning</h2><p><strong>Authors:Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu</strong></p>
<p>Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理已经证明了它们执行复杂任务的能力，类似于人类。然而，开源LLM和商业模型（如GPT系列）之间仍存在很大差距。在本文中，我们专注于通过指令微调提高LLM的代理泛化能力。我们首先观察到，现有的代理训练语料库在内部评估集上的表现令人满意，但难以推广到外部集。这些代理调整工作面临严重的格式错误，并经常长时间陷入同样的错误。我们分析认为，泛化能力差的根源在于对几种手动代理环境的过度适应以及对新情况的适应不足。他们难以执行错误的行动步骤，无法从经验中学习，而只是记忆现有的观察-行动关系。受此启发，我们提出了一种新型的AgentRefine框架用于代理调整。核心思想是让模型学会通过观察轨迹来纠正自己的错误。具体来说，我们提出了一个代理合成框架，以涵盖各种环境和任务，并提示强大的LLM根据环境反馈细化其错误行动。AgentRefine在多种代理任务上的泛化能力方面显著优于最新的代理调整工作。它还具有更好的抗扰动性，并在推理过程中可以产生多样化的想法。我们的发现建立了代理泛化和自我完善之间的关联，为未来研究提供了新的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01702v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的代理人在复杂任务中展现出与人类相似的执行能力。然而，开源LLM与商业模型（如GPT系列）之间仍存在巨大差距。本文专注于通过指令微调提高LLM的代理泛化能力。我们发现现有的代理训练语料库在内部评估集上表现良好，但在外部评估集上泛化能力较差。代理调整工作面临严重的格式错误，并经常长时间陷入相同的错误中。我们分析认为，其泛化能力弱源于对多种手动代理环境的过度适应以及对新情况的适应能力不足。他们难以采取正确的行动步骤，无法从经验中学习，而只是记忆现有的观察与行动之间的关系。基于这些见解，我们提出了新型的AgentRefine框架来进行代理调整。其核心思想是通过轨迹中的观察使模型学会纠正自己的错误。具体来说，我们提出了一个包含各种环境和任务的代理合成框架，并提示强大的LLM根据环境反馈来微调其错误行动。AgentRefine在多种代理任务上的泛化能力显著优于最新的代理调整工作，并且具有更好的面对干扰的稳健性以及推理过程中的多样化思维生成能力。我们的研究建立了代理泛化与自我完善之间的关联，为未来研究提供了新的范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在复杂任务中表现出强大的能力，但与商业模型如GPT系列相比仍存在差距。</li>
<li>现有LLM代理训练在内部评估上表现良好，但在外部评估上泛化能力较差。</li>
<li>LLM代理调整面临格式错误和难以纠正长期错误的问题。</li>
<li>LLM泛化能力弱源于对多个手动环境的过度适应和对新情况的适应能力不足。</li>
<li>LLM难以从经验中学习，仅依赖记忆现有的观察与行动关系。</li>
<li>AgentRefine框架通过使模型学会自我纠正错误来提高LLM的代理泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2255ed0a5d7d3c56a172d243ce39bcc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e790c1bbb63d57fa516cb7978a1323bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-435710b79c861a4ca2ec450f7d8973a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4de5db3d55f928025855b461beb43b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-750d49d44f60a9a8b4ff9f1ed0375f83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-776bf9f9537ebf89bd5d7871d8d30a1d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Domain-Specific-Translation-with-Open-Source-Large-Language-Models-Resource-Oriented-Analysis"><a href="#Domain-Specific-Translation-with-Open-Source-Large-Language-Models-Resource-Oriented-Analysis" class="headerlink" title="Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis"></a>Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis</h2><p><strong>Authors:Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem</strong></p>
<p>In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language pairs with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs exhibit a clear gap in specialized translation quality compared to multilingual encoder-decoder MT models such as NLLB-200. In three out of four language directions in our study, NLLB-200 3.3B outperforms all LLMs in the size range of 8B parameters in medical translation. While fine-tuning LLMs such as Mistral and Llama improves their performance at medical translation, these models still fall short compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve higher-quality domain-specific translation, especially in medium-resource and low-resource settings. As larger LLMs outperform their 8B variants, this also encourages pre-training domain-specific medium-sized LMs to improve quality and efficiency in specialized translation tasks. </p>
<blockquote>
<p>在这项工作中，我们比较了开源的自动回归解码器专用的大型语言模型（LLM）和任务导向型机器翻译（MT）模型在特定领域的翻译性能。我们的实验重点关注医疗领域，并涵盖了四种语言对，这些语言对的资源可用性各不相同：英语到法语、英语到葡萄牙语、英语到斯瓦希里语和斯瓦希里语到英语。尽管最近有进步，但LLM在专门翻译的质量方面与诸如NLLB-200等多语言编码器-解码器MT模型之间仍存在明显差距。在我们的研究中，四个语言方向中有三个方向的NLLB-200 3.3B在医疗翻译方面的表现优于所有规模为8B参数的LLM。虽然微调诸如Mistral和Llama等LLM可以改善其在医疗翻译方面的性能，但这些模型仍然无法与经过微调过的NLLB-200 3.3B模型相抗衡。我们的研究结果表明，在中等资源和低资源环境中，实现高质量的专业领域翻译仍需要专门的机器翻译模型。随着更大的LLM在表现上超越了其规模为8B的变体，这也鼓励在专门的翻译任务中预先训练中等规模的专业领域LM以提高质量和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文对比了开源的自动回归解码器专用大型语言模型（LLM）和任务导向型机器翻译（MT）模型在医学领域的翻译性能。实验涉及四种语言对，包括英语对法语、英语对葡萄牙语、英语对斯瓦希里语和斯瓦希里语对英语。尽管LLM最近有进展，但在专业翻译质量方面与多语言编码器-解码器MT模型（如NLLB-200）相比仍存在明显差距。在本文研究的四个语言方向中有三个方向，NLLB-200 3.3B在医学翻译中表现优于所有8B参数范围内的LLM。虽然微调LLM（如Mistral和Llama）可以提高医学翻译性能，但这些模型仍然无法与经过微调优化的NLLB-200模型匹敌。本文发现，实现高质量的专项翻译仍需采用专业化的机器翻译模型，特别是在中等资源和低资源环境中尤为如此。更大的LLM表现优于其8B变体，这也鼓励在专项翻译任务中预训练中等规模的专业领域LM以提高质量和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在医学领域的翻译性能对比中，LLM与任务导向型机器翻译模型（如NLLB-200）存在差距。</li>
<li>NLLB-200 3.3B在四个语言方向中的三个方向上的医学翻译表现优于特定大小范围内的LLM。</li>
<li>尽管LLM通过微调可以提高性能，但仍无法超越经过优化的NLLB-200模型。</li>
<li>在中等资源和低资源环境下，专业化的机器翻译模型是实现高质量专项翻译的关键。</li>
<li>更大的LLM表现优于其较小变体，表明预训练中等规模的专业领域LM有助于提高专项翻译的质量和效率。</li>
<li>LLM在特定领域的翻译中仍需进一步的优化和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cdfc3ddcf84a6b615785392c5f693e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee21cbbd0f6197dc4be9fb131c46bb39.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0c07a7b58a473b67225fe907e3199f4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Examining-the-Self-Improvement-Capabilities-of-Large-Language-Models"><a href="#Mind-the-Gap-Examining-the-Self-Improvement-Capabilities-of-Large-Language-Models" class="headerlink" title="Mind the Gap: Examining the Self-Improvement Capabilities of Large   Language Models"></a>Mind the Gap: Examining the Self-Improvement Capabilities of Large   Language Models</h2><p><strong>Authors:Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, Udaya Ghai</strong></p>
<p>Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the generation-verification gap. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement – a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries. </p>
<blockquote>
<p>自我提升是大规模语言模型（LLM）预训练、后训练和测试时间推理中的一种机制。我们探索了一个模型验证其自身输出的框架，基于这种验证来过滤或重新加权数据，并提炼过滤后的数据。尽管有许多经验性成功，但对其的基本理解仍然缺乏。在这项工作中，我们对LLM的自我提升进行了全面、模块化和可控的研究。我们为自我提升提供了数学公式，这主要由我们形式化为生成验证差距的量来控制。通过对各种模型家族和任务进行实验，我们发现自我提升的规模化现象——生成验证差距的某种变体随模型预训练浮点运算次数而单调变化。我们还研究了何时可以实现自我提升，迭代自我提升的程序以及提高其性能的方法。我们的研究不仅推动了具有实际意义的LLM自我提升的理解，而且还为未来的研究打开了对其能力和边界的众多途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02674v2">PDF</a> ICLR 2025; 41 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLM）的自我改进机制，包括预训练、后训练和测试时间推理。文章介绍了一个模型验证自身输出、基于验证过滤或重新加权数据，并蒸馏过滤数据的框架。尽管有一些经验性成功，但对其基本原理的理解仍然缺乏。本文首次对LLM自我改进进行了全面、模块化和受控的研究，提供了自我改进的数学公式，主要由我们形式化为生成-验证差距的量来控制。通过实验与各种模型和任务，我们发现了自我改进的规模现象——生成-验证差距的某种变体随模型预训练浮点运算次数而单调变化。我们还探讨了自我改进的可能性、迭代自我改进程序以及提高其性能的方法。本文的研究结果不仅有助于理解LLM自我改进的实际意义，而且为未来研究其能力和边界开辟了许多途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM自我改进涉及预训练、后训练和测试时间推理。</li>
<li>模型通过验证自身输出来实现自我改进，并基于此过滤或重新加权数据。</li>
<li>文章提出了自我改进的数学公式，受生成-验证差距控制。</li>
<li>实验显示，生成-验证差距的某种变体随模型预训练浮点运算次数单调变化。</li>
<li>文章探讨了自我改进的可能性及提高其性能的方法。</li>
<li>本文研究有助于理解LLM自我改进的实际意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02674">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0d938cd2d0b8c387c013f682862463d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b567513b2d46cc6cc21aa06979f2c21f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fee8ed4858005ef90234f25126eb1c20.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Defense-Against-Prompt-Injection-Attack-by-Leveraging-Attack-Techniques"><a href="#Defense-Against-Prompt-Injection-Attack-by-Leveraging-Attack-Techniques" class="headerlink" title="Defense Against Prompt Injection Attack by Leveraging Attack Techniques"></a>Defense Against Prompt Injection Attack by Leveraging Attack Techniques</h2><p><strong>Authors:Yulin Chen, Haoran Li, Zihao Zheng, Yangqiu Song, Dekai Wu, Bryan Hooi</strong></p>
<p>With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attacker’s instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMs’ instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: Could these attack techniques be utilized for defensive purposes? In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing training-free defense approaches, achieving state-of-the-art results. </p>
<blockquote>
<p>随着技术的进步，大型语言模型（LLM）在各种自然语言处理（NLP）任务中取得了显著的成绩，为Microsoft Copilot等LLM集成应用提供了动力。然而，随着LLM的不断发展，新的漏洞，尤其是提示注入攻击也随之出现。这些攻击诱导LLM偏离原始输入指令，执行注入在数据内容（如检索结果）中的攻击者指令。最近的攻击方法利用LLM遵循指令的能力以及它们无法区分注入在数据内容中的指令，实现了较高的攻击成功率（ASR）。在比较攻击和防御方法时，我们发现它们具有相似的设计目标，即引导模型忽略不需要的指令，转而执行所需的指令。因此，我们提出了一个直观的问题：这些攻击技术能否用于防御目的？在本文中，我们通过重复攻击过程但使用原始输入指令而不是注入的指令，反转了提示注入方法的意图，基于以前无需训练的攻击方法开发了新型防御方法。我们的综合实验表明，我们的防御技术优于现有的无需训练的防御方法，取得了最新的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00459v3">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>随着技术的发展，大型语言模型（LLM）在各种自然语言处理（NLP）任务中取得了显著的成绩，推动了如Microsoft Copilot等LLM集成应用的出现。然而，新的漏洞也不断涌现，特别是提示注入攻击。这些攻击使LLM偏离原始输入指令并执行攻击者在数据内容中注入的指令。本文利用LLM遵循指令的能力及其无法区分数据内容中注入指令的缺陷进行攻击。本文提出了一种将攻击技术转化为防御技术的创新方法，基于以往无训练攻击技术，通过重复攻击过程但使用原始输入指令而非注入指令进行防御。实验证明，这种防御技术优于现有的无训练防御方法，取得了最新结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在自然语言处理任务中表现出卓越性能，但也存在提示注入攻击的新漏洞。</li>
<li>这些攻击使LLMs偏离原始指令并执行攻击者注入的指令。</li>
<li>攻击方法和防御方法的设计目标相似，旨在引导模型执行想要的指令而非不想要的指令。</li>
<li>论文将攻击技术的意图颠倒，开发了一种基于无训练攻击方法的防御方法。</li>
<li>该防御方法通过重复攻击过程但使用原始输入指令进行防御。</li>
<li>实验证明，这种防御技术比现有无训练防御方法更有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4744ecb8c8f9c32416bd462247479616.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100f9eee4f83bef4c932574e8d7372bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ae0392a85dab5147db4c409aedfedf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d582f34ebbd78790eb894909b621fe43.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Gnothi-Seauton-Empowering-Faithful-Self-Interpretability-in-Black-Box-Transformers"><a href="#Gnothi-Seauton-Empowering-Faithful-Self-Interpretability-in-Black-Box-Transformers" class="headerlink" title="Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box   Transformers"></a>Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box   Transformers</h2><p><strong>Authors:Shaobo Wang, Hongxuan Tang, Mingyang Wang, Hongrui Zhang, Xuyang Liu, Weiya Li, Xuming Hu, Linfeng Zhang</strong></p>
<p>The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, AutoGnothi, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. AutoGnothi enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that AutoGnothi offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability. </p>
<blockquote>
<p>关于可解释人工智能（XAI）的核心争论在于自解释模型与事后解释黑箱模型的对比。自解释模型（如基于概念的网络）通过连接决策与人类可理解的概念来提供见解，但它们往往面临性能和可扩展性的挑战。相反，事后方法（如沙普利值）虽然在理论上很稳健，但在计算上却非常昂贵且资源密集。为了弥合这两种研究思路之间的鸿沟，我们提出了一种将两者的优势结合起来的新方法，为黑箱模型提供理论保证的自解释性，同时不妥协预测精度。具体来说，我们引入了一种参数高效的管道AutoGnothi，它将一个小型辅助网络集成到黑箱模型中，允许它生成沙普利值解释，而不会改变原始网络参数。这种侧面调整的方法大大降低了内存、训练和推理成本，超越了传统的参数高效方法，其中全微调是最优基线。AutoGnothi使黑箱模型能够以最小的开销预测和解释其预测。大量实验表明，AutoGnothi对视觉和语言任务都提供了准确的解释，在可比的诠释性下具有出色的计算效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21815v2">PDF</a> Accepted by ICLR 2025, 29 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>本文围绕可解释人工智能（XAI）中的自解释模型与黑箱模型的后期解释展开辩论。自解释模型如基于概念的网络通过连接决策与人类可理解的概念提供洞察力，但在性能和可扩展性方面常常遇到困难。相反，如沙普利值等的后期方法虽然理论稳健，但计算成本高且资源密集。为了弥合这两类研究的差距，本文提出了一种新方法，结合两者的优势，为黑箱模型提供理论保证的自解释性，同时不损害预测精度。具体来说，本文引入了一种参数高效的管道AutoGnothi，它将一个小型侧网络集成到黑箱模型中，能够生成沙普利值解释，而无需更改原始网络参数。这种侧调节方法显著降低了内存、训练和推理成本，优于传统的参数高效方法（以全微调作为最佳基线）。AutoGnothi使黑箱模型能够预测和解释其预测结果，几乎无需额外开销。大量实验表明，AutoGnothi可为视觉和语言任务提供精确的解释，在计算效率方面表现出色，同时保持了相当的可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自解释模型与黑箱模型的后期解释在可解释人工智能（XAI）领域中形成核心辩论。</li>
<li>自解释模型如概念网络能够提供决策与人类理解之间的关联，但面临性能与可扩展性问题。</li>
<li>后期方法如沙普利值虽然理论稳健但计算成本高且资源密集。</li>
<li>提出了一种新方法AutoGnothi，结合了自解释模型与黑箱模型的优点。</li>
<li>AutoGnothi通过引入小型侧网络实现了对黑箱模型的理论保证自解释性。</li>
<li>侧调节方法降低了内存、训练和推理成本，优于传统参数高效方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-25f2575238b00a70d58c982d6d89ce67.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e008774a72aab9124eede42d257f7e8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5b3c06f8d9fd0afef74e727936816c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9305c0aa208544a7fe26118cfe9539f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f74eb918c52c8a7f23f40c93529baa7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-08e2895a318dd12f07ce9551d82799a5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-27  AgentRM Enhancing Agent Generalization with Reward Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-26/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-719fc22c81500005b260d458a23807bd.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-02-26  BiPO Bidirectional Partial Occlusion Network for Text-to-Motion   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
