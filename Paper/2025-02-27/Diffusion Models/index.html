<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-27  LDGen Enhancing Text-to-Image Synthesis via Large Language Model-Driven   Language Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    60 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-27-æ›´æ–°"><a href="#2025-02-27-æ›´æ–°" class="headerlink" title="2025-02-27 æ›´æ–°"></a>2025-02-27 æ›´æ–°</h1><h2 id="LDGen-Enhancing-Text-to-Image-Synthesis-via-Large-Language-Model-Driven-Language-Representation"><a href="#LDGen-Enhancing-Text-to-Image-Synthesis-via-Large-Language-Model-Driven-Language-Representation" class="headerlink" title="LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven   Language Representation"></a>LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven   Language Representation</h2><p><strong>Authors:Pengzhi Li, Pengfei Yu, Zide Liu, Wei He, Xuhao Pan, Xudong Rao, Tao Wei, Wei Chen</strong></p>
<p>In this paper, we introduce LDGen, a novel method for integrating large language models (LLMs) into existing text-to-image diffusion models while minimizing computational demands. Traditional text encoders, such as CLIP and T5, exhibit limitations in multilingual processing, hindering image generation across diverse languages. We address these challenges by leveraging the advanced capabilities of LLMs. Our approach employs a language representation strategy that applies hierarchical caption optimization and human instruction techniques to derive precise semantic information,. Subsequently, we incorporate a lightweight adapter and a cross-modal refiner to facilitate efficient feature alignment and interaction between LLMs and image features. LDGen reduces training time and enables zero-shot multilingual image generation. Experimental results indicate that our method surpasses baseline models in both prompt adherence and image aesthetic quality, while seamlessly supporting multiple languages. Project page: <a target="_blank" rel="noopener" href="https://zrealli.github.io/LDGen">https://zrealli.github.io/LDGen</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†LDGenï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆåˆ°ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒæ—¶æœ€å°åŒ–è®¡ç®—éœ€æ±‚çš„æ–°æ–¹æ³•ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå¦‚CLIPå’ŒT5ï¼Œåœ¨å¤šè¯­è¨€å¤„ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œé˜»ç¢äº†ä¸åŒè¯­è¨€çš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨LLMçš„å…ˆè¿›åŠŸèƒ½æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§è¯­è¨€è¡¨ç¤ºç­–ç•¥ï¼Œé€šè¿‡åˆ†å±‚æ ‡é¢˜ä¼˜åŒ–å’Œäººå·¥æŒ‡ä»¤æŠ€æœ¯æ¥æ¨å¯¼ç²¾ç¡®è¯­ä¹‰ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§é€‚é…å™¨å’Œè·¨æ¨¡æ€ç²¾ç‚¼å™¨ï¼Œä»¥ä¿ƒè¿›LLMå’Œå›¾åƒç‰¹å¾ä¹‹é—´çš„æœ‰æ•ˆç‰¹å¾å¯¹é½å’Œäº¤äº’ã€‚LDGenç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ï¼Œå®ç°äº†é›¶å¯åŠ¨å¤šè¯­è¨€å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æç¤ºéµå¾ªå’Œå›¾åƒç¾å­¦è´¨é‡æ–¹é¢éƒ½è¶…è¿‡äº†åŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶æ— ç¼æ”¯æŒå¤šç§è¯­è¨€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zrealli.github.io/LDGen%E3%80%82">https://zrealli.github.io/LDGenã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LDGenï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆåˆ°ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼ŒåŒæ—¶å°½é‡å‡å°‘è®¡ç®—éœ€æ±‚çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚CLIPå’ŒT5ï¼‰åœ¨å¤šè¯­è¨€å¤„ç†æ–¹é¢çš„å±€é™æ€§ï¼Œé˜»ç¢äº†è·¨å¤šç§è¯­è¨€çš„å›¾åƒç”Ÿæˆé—®é¢˜ï¼Œæˆ‘ä»¬å€ŸåŠ©LLMsçš„å…ˆè¿›åŠŸèƒ½æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚LDGené‡‡ç”¨ä¸€ç§è¯­è¨€è¡¨ç¤ºç­–ç•¥ï¼Œé€šè¿‡å±‚æ¬¡åŒ–çš„æ ‡é¢˜ä¼˜åŒ–å’Œäººç±»æŒ‡ä»¤æŠ€æœ¯æ¥æå–ç²¾ç¡®è¯­ä¹‰ä¿¡æ¯ã€‚éšåï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„é€‚é…å™¨å’Œè·¨æ¨¡æ€ç²¾ç‚¼å™¨ï¼Œä»¥ä¿ƒè¿›LLMså’Œå›¾åƒç‰¹å¾ä¹‹é—´çš„æœ‰æ•ˆç‰¹å¾å¯¹é½å’Œäº¤äº’ã€‚LDGenç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬å¤šè¯­è¨€å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æç¤ºéµå¾ªå’Œå›¾åƒç¾å­¦è´¨é‡æ–¹é¢è¶…è¶Šäº†åŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶æ”¯æŒå¤šç§è¯­è¨€æ— ç¼å¯¹æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDGenæ˜¯ä¸€ç§é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆ°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡å¹¶æ‰©å¤§è¯­è¨€æ”¯æŒèŒƒå›´ã€‚</li>
<li>ä¼ ç»Ÿæ–‡æœ¬ç¼–ç å™¨åœ¨å¤šè¯­è¨€å¤„ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œè€ŒLDGenåˆ©ç”¨LLMsçš„å…ˆè¿›åŠŸèƒ½æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LDGené‡‡ç”¨ç‰¹å®šçš„è¯­è¨€è¡¨ç¤ºç­–ç•¥ï¼Œç»“åˆå±‚æ¬¡åŒ–çš„æ ‡é¢˜ä¼˜åŒ–å’Œäººç±»æŒ‡ä»¤æŠ€æœ¯ï¼Œä»¥æå–ç²¾ç¡®çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„é€‚é…å™¨å’Œè·¨æ¨¡æ€ç²¾ç‚¼å™¨ï¼Œä¿ƒè¿›LLMså’Œå›¾åƒç‰¹å¾ä¹‹é—´çš„æœ‰æ•ˆäº¤äº’å’Œç‰¹å¾å¯¹é½ã€‚</li>
<li>LDGenèƒ½å¤Ÿå‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå¹¶æ”¯æŒé›¶æ ·æœ¬å¤šè¯­è¨€å›¾åƒç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLDGenåœ¨æç¤ºéµå¾ªå’Œå›¾åƒç¾å­¦è´¨é‡æ–¹é¢è¶…è¶Šäº†åŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18302v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18302v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18302v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18302v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18302v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Consistent-Novel-Views-via-3D-Epipolar-Attention-without-Re-Training"><a href="#Synthesizing-Consistent-Novel-Views-via-3D-Epipolar-Attention-without-Re-Training" class="headerlink" title="Synthesizing Consistent Novel Views via 3D Epipolar Attention without   Re-Training"></a>Synthesizing Consistent Novel Views via 3D Epipolar Attention without   Re-Training</h2><p><strong>Authors:Botao Ye, Sifei Liu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang</strong></p>
<p>Large diffusion models demonstrate remarkable zero-shot capabilities in novel view synthesis from a single image. However, these models often face challenges in maintaining consistency across novel and reference views. A crucial factor leading to this issue is the limited utilization of contextual information from reference views. Specifically, when there is an overlap in the viewing frustum between two views, it is essential to ensure that the corresponding regions maintain consistency in both geometry and appearance. This observation leads to a simple yet effective approach, where we propose to use epipolar geometry to locate and retrieve overlapping information from the input view. This information is then incorporated into the generation of target views, eliminating the need for training or fine-tuning, as the process requires no learnable parameters. Furthermore, to enhance the overall consistency of generated views, we extend the utilization of epipolar attention to a multi-view setting, allowing retrieval of overlapping information from the input view and other target views. Qualitative and quantitative experimental results demonstrate the effectiveness of our method in significantly improving the consistency of synthesized views without the need for any fine-tuning. Moreover, This enhancement also boosts the performance of downstream applications such as 3D reconstruction. The code is available at <a target="_blank" rel="noopener" href="https://github.com/botaoye/ConsisSyn">https://github.com/botaoye/ConsisSyn</a>. </p>
<blockquote>
<p>å¤§å‹æ‰©æ•£æ¨¡å‹åœ¨å•å›¾åƒåˆæˆæ–°è§†è§’ä¸­å±•ç°å‡ºæ˜¾è‘—çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç»´æŒæ–°è§†è§’å’Œå‚è€ƒè§†è§’ä¹‹é—´çš„ä¸€è‡´æ€§æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚å¯¼è‡´è¿™ä¸€é—®é¢˜çš„å…³é”®å› ç´ æ˜¯å¯¹å‚è€ƒè§†è§’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ©ç”¨æœ‰é™ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ä¸¤ä¸ªè§†è§’ä¹‹é—´çš„è§†å›¾æˆªæ–­å¤„æœ‰é‡å æ—¶ï¼Œç¡®ä¿ç›¸åº”åŒºåŸŸåœ¨å‡ ä½•å’Œå¤–è§‚ä¸Šéƒ½ä¿æŒä¸€è‡´è‡³å…³é‡è¦ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨æå‡ ä½•æ¥å®šä½å’Œæ£€ç´¢è¾“å…¥è§†å›¾ä¸­çš„é‡å ä¿¡æ¯ã€‚ç„¶åå°†è¿™äº›ä¿¡æ¯èå…¥ç›®æ ‡è§†å›¾çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ— éœ€è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒï¼Œå› ä¸ºè¯¥è¿‡ç¨‹ä¸éœ€è¦å¯å­¦ä¹ çš„å‚æ•°ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºç”Ÿæˆè§†å›¾çš„æ•´ä½“ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å°†ææ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨æ‰©å±•åˆ°å¤šè§†è§’ç¯å¢ƒï¼Œå…è®¸ä»è¾“å…¥è§†å›¾å’Œå…¶ä»–ç›®æ ‡è§†å›¾ä¸­æ£€ç´¢é‡å ä¿¡æ¯ã€‚å®šæ€§å’Œå®šé‡å®éªŒç»“æœéƒ½è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ˜¾è‘—æé«˜åˆæˆè§†å›¾ä¸€è‡´æ€§çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”æ— éœ€è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œè¿™ç§æ”¹è¿›è¿˜æå‡äº†ä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚3Dé‡å»ºï¼‰çš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/botaoye/ConsisSyn%E3%80%82">https://github.com/botaoye/ConsisSynã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18219v1">PDF</a> 3DV 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ‰©æ•£æ¨¡å‹åœ¨å•å›¾åƒåˆæˆæ–°è§†è§’ä¸­å…·æœ‰å‡ºè‰²çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†åœ¨ä¿æŒæ–°é¢–è§‚ç‚¹å’Œå‚è€ƒè§‚ç‚¹çš„ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å…¶å…³é”®é—®é¢˜æ˜¯æœªèƒ½å……åˆ†åˆ©ç”¨å‚è€ƒè§†è§’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å½“ä¸¤ä¸ªè§†è§’çš„è§‚å¯Ÿè§†é‡æœ‰é‡å æ—¶ï¼Œç¡®ä¿ç›¸åº”åŒºåŸŸçš„å‡ ä½•å’Œå¤–è§‚ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æå‡ ä½•å®šä½å¹¶æ£€ç´¢è¾“å…¥è§†è§’çš„é‡å ä¿¡æ¯ï¼Œå°†å…¶èå…¥ç›®æ ‡è§†è§’çš„ç”Ÿæˆä¸­ï¼Œæ— éœ€è®­ç»ƒæˆ–å¾®è°ƒã€‚æ­¤å¤–ï¼Œè¿˜å°†ææ³¨æ„åŠ›æ‰©å±•åˆ°å¤šè§†è§’ç¯å¢ƒï¼Œæé«˜ç”Ÿæˆè§†è§’çš„æ•´ä½“ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æé«˜åˆæˆè§†è§’çš„ä¸€è‡´æ€§åŒæ—¶ï¼Œä¸éœ€è¦ä»»ä½•å¾®è°ƒã€‚æ­¤å¤–ï¼Œæ­¤æ”¹è¿›è¿˜å¢å¼ºäº†ä¸‹æ¸¸åº”ç”¨å¦‚3Dé‡å»ºçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ‰©æ•£æ¨¡å‹åœ¨å•å›¾åƒåˆæˆæ–°è§†è§’å±•ç°å‡ºè‰²çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é¢ä¸´ä¿æŒæ–°é¢–å’Œå‚è€ƒè§‚ç‚¹ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æŒ‘æˆ˜çš„å…³é”®é—®é¢˜åœ¨äºæœªèƒ½å……åˆ†åˆ©ç”¨å‚è€ƒè§†è§’çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åˆ©ç”¨æå‡ ä½•å®šä½å¹¶æ£€ç´¢è¾“å…¥è§†è§’çš„é‡å ä¿¡æ¯ï¼Œæé«˜ç”Ÿæˆç›®æ ‡è§†è§’çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒæˆ–å¾®è°ƒã€‚</li>
<li>ææ³¨æ„åŠ›æœºåˆ¶æ‰©å±•åˆ°å¤šè§†è§’ç¯å¢ƒï¼Œè¿›ä¸€æ­¥æé«˜ç”Ÿæˆè§†è§’çš„æ•´ä½“ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—æé«˜åˆæˆè§†è§’çš„ä¸€è‡´æ€§ï¼Œå¹¶å¢å¼ºä¸‹æ¸¸åº”ç”¨å¦‚3Dé‡å»ºçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18219v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18219v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18219v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18219v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Training-Consistency-Models-with-Variational-Noise-Coupling"><a href="#Training-Consistency-Models-with-Variational-Noise-Coupling" class="headerlink" title="Training Consistency Models with Variational Noise Coupling"></a>Training Consistency Models with Variational Noise Coupling</h2><p><strong>Authors:Gianluigi Silvestri, Luca Ambrogioni, Chieh-Hsin Lai, Yuhta Takida, Yuki Mitsufuji</strong></p>
<p>Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at $64 \times 64$ resolution in 2-step generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sony/vct">https://github.com/sony/vct</a> . </p>
<blockquote>
<p>ä¸€è‡´æ€§è®­ç»ƒï¼ˆCTï¼‰ä½œä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„åˆ†æ­¥å»ºæ¨¡æ›¿ä»£æ–¹æ¡ˆæœ€è¿‘å´­éœ²å¤´è§’ï¼Œå®ƒåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéè’¸é¦ä¸€è‡´æ€§è®­ç»ƒå¸¸å¸¸é¢ä¸´é«˜æ–¹å·®å’Œä¸ç¨³å®šçš„é—®é¢˜ï¼Œå› æ­¤åˆ†æå’Œæé«˜å…¶è®­ç»ƒåŠ¨æ€æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…æ¡†æ¶çš„æ–°å‹CTè®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯å€Ÿé‰´å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„çš„å¯å‘ï¼Œè®¾è®¡äº†ä¸€ç§è®­ç»ƒåçš„å™ªå£°è€¦åˆæ–¹æ¡ˆã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªä½œä¸ºç¼–ç å™¨æ¶æ„å®ç°çš„æ•°æ®ä¾èµ–å™ªå£°å‘å°„æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é—´æ¥å­¦ä¹ å™ªå£°åˆ°æ•°æ®æ˜ å°„çš„å‡ ä½•ç»“æ„ï¼Œè¿™åœ¨ç»å…¸CTä¸­æ˜¯é€šè¿‡å‰å‘è¿‡ç¨‹çš„é€‰æ‹©å›ºå®šçš„ã€‚åœ¨ä¸åŒçš„å›¾åƒæ•°æ®é›†ä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆæ–¹é¢æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸”åœ¨CIFAR-10ä¸Šè¶…è¶Šäº†åŸºå‡†æµ‹è¯•å¹¶å®ç°äº†æœ€å…ˆè¿›çš„éè’¸é¦CT FIDï¼Œåœ¨ImageNetçš„64x64åˆ†è¾¨ç‡çš„2æ­¥ç”Ÿæˆä¸­è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„FIDã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/vct%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sony/vctæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18197v1">PDF</a> 23 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…æ¡†æ¶çš„æ–°å‹ä¸€è‡´æ€§è®­ç»ƒï¼ˆCTï¼‰æ–¹æ³•ï¼Œä¸»è¦è´¡çŒ®åœ¨äºå—å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„å¯å‘çš„è®­ç»ƒå™ªå£°è€¦åˆæ–¹æ¡ˆã€‚é€šè¿‡è®­ç»ƒæ•°æ®ä¾èµ–çš„å™ªå£°å‘å°„æ¨¡å‹ï¼Œå®ç°ä¸ºç¼–ç å™¨æ¶æ„ï¼Œè¯¥æ–¹æ³•å¯é—´æ¥å­¦ä¹ å™ªå£°åˆ°æ•°æ®çš„æ˜ å°„å‡ ä½•ï¼Œè¿™æ˜¯ç»å…¸CTä¸­å‰å‘è¿‡ç¨‹çš„é€‰æ‹©æ‰€å›ºå®šçš„ã€‚åœ¨å¤šç§å›¾åƒæ•°æ®é›†ä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆæ–¹é¢æœ‰æ˜¾è‘—æ”¹å–„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†åŸºçº¿å¹¶å®ç°éè’¸é¦CTçš„CIFAR-10ä¸Šçš„æœ€æ–°æŠ€æœ¯ï¼Œåœ¨2æ­¥ç”Ÿæˆä¸­ä»¥64Ã—64åˆ†è¾¨ç‡åœ¨ImageNetä¸Šè¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„FIDã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä¸€è‡´æ€§è®­ç»ƒï¼ˆCTï¼‰ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨å›¾ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>éè’¸é¦ä¸€è‡´æ€§è®­ç»ƒå­˜åœ¨é«˜æ–¹å·®å’Œä¸ç¨³å®šæ€§çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…æ¡†æ¶çš„æ–°å‹CTè®­ç»ƒæ–¹æ³•ã€‚</li>
<li>å¼•å…¥å—å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¶æ„å¯å‘çš„è®­ç»ƒå™ªå£°è€¦åˆæ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡è®­ç»ƒæ•°æ®ä¾èµ–çš„å™ªå£°å‘å°„æ¨¡å‹ï¼Œé—´æ¥å­¦ä¹ å™ªå£°åˆ°æ•°æ®çš„æ˜ å°„å‡ ä½•ã€‚</li>
<li>åœ¨CIFAR-10æ•°æ®é›†ä¸Šå®ç°äº†éè’¸é¦CTçš„é¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨ImageNetçš„2æ­¥ç”Ÿæˆä¸­è¾¾åˆ°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„FIDã€‚</li>
<li>å¯ç”¨ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/vct%E3%80%82">https://github.com/sony/vctã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18197v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18197v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18197v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PromptMID-Modal-Invariant-Descriptors-Based-on-Diffusion-and-Vision-Foundation-Models-for-Optical-SAR-Image-Matching"><a href="#PromptMID-Modal-Invariant-Descriptors-Based-on-Diffusion-and-Vision-Foundation-Models-for-Optical-SAR-Image-Matching" class="headerlink" title="PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision   Foundation Models for Optical-SAR Image Matching"></a>PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision   Foundation Models for Optical-SAR Image Matching</h2><p><strong>Authors:Han Nie, Bin Luo, Jun Liu, Zhitao Fu, Huan Zhou, Shuo Zhang, Weixing Liu</strong></p>
<p>The ideal goal of image matching is to achieve stable and efficient performance in unseen domains. However, many existing learning-based optical-SAR image matching methods, despite their effectiveness in specific scenarios, exhibit limited generalization and struggle to adapt to practical applications. Repeatedly training or fine-tuning matching models to address domain differences is not only not elegant enough but also introduces additional computational overhead and data production costs. In recent years, general foundation models have shown great potential for enhancing generalization. However, the disparity in visual domains between natural and remote sensing images poses challenges for their direct application. Therefore, effectively leveraging foundation models to improve the generalization of optical-SAR image matching remains challenge. To address the above challenges, we propose PromptMID, a novel approach that constructs modality-invariant descriptors using text prompts based on land use classification as priors information for optical and SAR image matching. PromptMID extracts multi-scale modality-invariant features by leveraging pre-trained diffusion models and visual foundation models (VFMs), while specially designed feature aggregation modules effectively fuse features across different granularities. Extensive experiments on optical-SAR image datasets from four diverse regions demonstrate that PromptMID outperforms state-of-the-art matching methods, achieving superior results in both seen and unseen domains and exhibiting strong cross-domain generalization capabilities. The source code will be made publicly available <a target="_blank" rel="noopener" href="https://github.com/HanNieWHU/PromptMID">https://github.com/HanNieWHU/PromptMID</a>. </p>
<blockquote>
<p>å›¾åƒåŒ¹é…çš„ç†æƒ³ç›®æ ‡æ˜¯åœ¨æœªè§é¢†åŸŸå®ç°ç¨³å®šä¸”é«˜æ•ˆçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡è®¸å¤šç°æœ‰çš„åŸºäºå­¦ä¹ çš„å…‰å­¦-SARå›¾åƒåŒ¹é…æ–¹æ³•åœ¨ç‰¹å®šåœºæ™¯ä¸­å…·æœ‰æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é€‚åº”å®é™…åº”ç”¨ã€‚é’ˆå¯¹é¢†åŸŸå·®å¼‚é‡å¤è®­ç»ƒæˆ–å¾®è°ƒåŒ¹é…æ¨¡å‹ï¼Œä¸ä»…ä¸å¤Ÿä¼˜é›…ï¼Œè€Œä¸”å¼•å…¥äº†é¢å¤–çš„è®¡ç®—å¼€é”€å’Œæ•°æ®ç”Ÿäº§æˆæœ¬ã€‚è¿‘å¹´æ¥ï¼Œé€šç”¨åŸºç¡€æ¨¡å‹åœ¨æé«˜æ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè‡ªç„¶å›¾åƒå’Œé¥æ„Ÿå›¾åƒè§†è§‰é¢†åŸŸä¹‹é—´çš„å·®å¼‚ä¸ºå…¶ç›´æ¥åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæœ‰æ•ˆåˆ©ç”¨åŸºç¡€æ¨¡å‹æ¥æé«˜å…‰å­¦-SARå›¾åƒåŒ¹é…çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚é’ˆå¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PromptMIDï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåœŸåœ°ç”¨é€”åˆ†ç±»çš„æ–‡æœ¬æç¤ºæ„å»ºæ¨¡æ€ä¸å˜æè¿°ç¬¦çš„æ–°æ–¹æ³•ï¼Œä½œä¸ºå…‰å­¦å’ŒSARå›¾åƒåŒ¹é…çš„å…ˆéªŒä¿¡æ¯ã€‚PromptMIDé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰æå–å¤šå°ºåº¦æ¨¡æ€ä¸å˜ç‰¹å¾ï¼ŒåŒæ—¶ä¸“é—¨è®¾è®¡çš„ç‰¹å¾èšåˆæ¨¡å—æœ‰æ•ˆåœ°èåˆäº†ä¸åŒç²’åº¦çš„ç‰¹å¾ã€‚åœ¨æ¥è‡ªå››ä¸ªä¸åŒåœ°åŒºçš„å…‰å­¦-SARå›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPromptMIDä¼˜äºæœ€å…ˆè¿›çš„åŒ¹é…æ–¹æ³•ï¼Œåœ¨å·²çŸ¥å’ŒæœªçŸ¥é¢†åŸŸéƒ½å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å°†å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/HannieWHU/PromptMID%E3%80%82">https://github.com/HanNieWHU/PromptMIDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18104v1">PDF</a> 15 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>å…‰å­¦ä¸SARå›¾åƒåŒ¹é…çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•åœ¨æœªè§é¢†åŸŸå®ç°ç¨³å®šä¸”é«˜æ•ˆçš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•åœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é€‚åº”å®é™…åº”ç”¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ–‡æœ¬æç¤ºæ„å»ºæ¨¡æ€ä¸å˜æè¿°ç¬¦çš„æ–°æ–¹æ³•PromptMIDï¼Œåˆ©ç”¨åœŸåœ°ç”¨é€”åˆ†ç±»ä½œä¸ºå…ˆéªŒä¿¡æ¯ï¼Œç”¨äºå…‰å­¦å’ŒSARå›¾åƒåŒ¹é…ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹æå–å¤šå°ºåº¦æ¨¡æ€ä¸å˜ç‰¹å¾ï¼Œå¹¶è®¾è®¡ç‰¹å¾èšåˆæ¨¡å—å®ç°ä¸åŒç²’åº¦ç‰¹å¾çš„èåˆã€‚åœ¨å››ä¸ªä¸åŒåŒºåŸŸçš„å…‰å­¦-SARå›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPromptMIDåœ¨å¯è§å’Œæœªè§é¢†åŸŸå‡ä¼˜äºå½“å‰å…ˆè¿›çš„åŒ¹é…æ–¹æ³•ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåŒ¹é…çš„ç†æƒ³ç›®æ ‡æ˜¯å®ç°æœªè§é¢†åŸŸçš„ç¨³å®šé«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>ç°æœ‰å…‰å­¦-SARå›¾åƒåŒ¹é…æ–¹æ³•æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥é€‚åº”å®é™…åº”ç”¨ã€‚</li>
<li>PromptMIDæ˜¯ä¸€ç§åŸºäºæ–‡æœ¬æç¤ºçš„æ¨¡æ€ä¸å˜æè¿°ç¬¦æ„å»ºæ–°æ–¹æ³•ã€‚</li>
<li>PromptMIDåˆ©ç”¨åœŸåœ°ç”¨é€”åˆ†ç±»ä½œä¸ºå…ˆéªŒä¿¡æ¯ï¼Œç”¨äºå…‰å­¦å’ŒSARå›¾åƒåŒ¹é…ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹æå–å¤šå°ºåº¦æ¨¡æ€ä¸å˜ç‰¹å¾ã€‚</li>
<li>ç‰¹å¾èšåˆæ¨¡å—å®ç°ä¸åŒç²’åº¦ç‰¹å¾çš„èåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18104v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18104v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18104v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18104v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.18104v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3D-Anatomical-Structure-guided-Deep-Learning-for-Accurate-Diffusion-Microstructure-Imaging"><a href="#3D-Anatomical-Structure-guided-Deep-Learning-for-Accurate-Diffusion-Microstructure-Imaging" class="headerlink" title="3D Anatomical Structure-guided Deep Learning for Accurate Diffusion   Microstructure Imaging"></a>3D Anatomical Structure-guided Deep Learning for Accurate Diffusion   Microstructure Imaging</h2><p><strong>Authors:Xinrui Ma, Jian Cheng, Wenxin Fan, Ruoyou Wu, Yongquan Ye, Shanshan Wang</strong></p>
<p>Diffusion magnetic resonance imaging (dMRI) is a crucial non-invasive technique for exploring the microstructure of the living human brain. Traditional hand-crafted and model-based tissue microstructure reconstruction methods often require extensive diffusion gradient sampling, which can be time-consuming and limits the clinical applicability of tissue microstructure information. Recent advances in deep learning have shown promise in microstructure estimation; however, accurately estimating tissue microstructure from clinically feasible dMRI scans remains challenging without appropriate constraints. This paper introduces a novel framework that achieves high-fidelity and rapid diffusion microstructure imaging by simultaneously leveraging anatomical information from macro-level priors and mutual information across parameters. This approach enhances time efficiency while maintaining accuracy in microstructure estimation. Experimental results demonstrate that our method outperforms four state-of-the-art techniques, achieving a peak signal-to-noise ratio (PSNR) of 30.51$\pm$0.58 and a structural similarity index measure (SSIM) of 0.97$\pm$0.004 in estimating parametric maps of multiple diffusion models. Notably, our method achieves a 15$\times$ acceleration compared to the dense sampling approach, which typically utilizes 270 diffusion gradients. </p>
<blockquote>
<p>æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æ˜¯æ¢ç´¢æ´»äººè„‘å¾®è§‚ç»“æ„çš„é‡è¦æ— åˆ›æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„æ‰‹å·¥å’ŒåŸºäºæ¨¡å‹çš„å¾®è§‚ç»“æ„é‡å»ºæ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ‰©æ•£æ¢¯åº¦é‡‡æ ·ï¼Œè¿™æ—¢è€—æ—¶ä¸”é™åˆ¶äº†å¾®è§‚ç»“æ„ä¿¡æ¯åœ¨ä¸´åºŠä¸Šçš„åº”ç”¨ã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•åœ¨å¾®è§‚ç»“æ„ä¼°è®¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œç„¶è€Œï¼Œä»ä¸´åºŠä¸Šå¯è¡Œçš„dMRIæ‰«æä¸­å‡†ç¡®ä¼°è®¡å¾®è§‚ç»“æ„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé™¤éé‡‡ç”¨é€‚å½“çš„çº¦æŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡åŒæ—¶åˆ©ç”¨å®è§‚å…ˆéªŒè§£å‰–ä¿¡æ¯å’Œå‚æ•°é—´çš„äº’ä¿¡æ¯ï¼Œå®ç°é«˜ä¿çœŸå’Œå¿«é€Ÿçš„æ‰©æ•£å¾®è§‚ç»“æ„æˆåƒã€‚è¿™ç§æ–¹æ³•åœ¨æé«˜æ—¶é—´æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†å¾®è§‚ç»“æ„ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼°è®¡å¤šä¸ªæ‰©æ•£æ¨¡å‹çš„å‚æ•°æ˜ å°„æ–¹é¢ä¼˜äºå››ç§æœ€æ–°æŠ€æœ¯ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰è¾¾åˆ°30.51Â±0.58ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ä¸º0.97Â±0.004ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¯†é›†é‡‡æ ·æ–¹æ³•ç›¸æ¯”å®ç°äº†15å€çš„åŠ é€Ÿï¼Œå¯†é›†é‡‡æ ·æ–¹æ³•é€šå¸¸ä½¿ç”¨270ä¸ªæ‰©æ•£æ¢¯åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17933v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ‰©æ•£å¾®è§‚ç»“æ„æˆåƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å®è§‚çº§åˆ«çš„å…ˆéªŒè§£å‰–ä¿¡æ¯å’Œå‚æ•°é—´çš„äº’ä¿¡æ¯ï¼Œå®ç°äº†é«˜ä¿çœŸå’Œå¿«é€Ÿçš„æ‰©æ•£å¾®è§‚ç»“æ„æˆåƒã€‚æ­¤æ–¹æ³•åœ¨æé«˜æ—¶é—´æ•ˆç‡çš„åŒæ—¶ä¿æŒäº†å¾®è§‚ç»“æ„ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œå¹¶è¶…è¶Šäº†å››ç§æœ€æ–°çš„æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æ˜¯æ¢ç´¢æ´»ä½“äººè„‘å¾®è§‚ç»“æ„çš„é‡è¦éä¾µå…¥æŠ€æœ¯ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰‹åŠ¨åˆ¶ä½œå’ŒåŸºäºæ¨¡å‹çš„ç»„ç»‡å¾®è§‚ç»“æ„é‡å»ºæ–¹æ³•éœ€è¦å¹¿æ³›çš„æ‰©æ•£æ¢¯åº¦é‡‡æ ·ï¼Œè¿™æ—¢è€—æ—¶åˆé™åˆ¶äº†ç»„ç»‡å¾®è§‚ç»“æ„ä¿¡æ¯åœ¨ä¸´åºŠä¸Šçš„é€‚ç”¨æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å¾®è§‚ç»“æ„ä¼°è®¡æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†è¦ä»ä¸´åºŠä¸Šå¯è¡Œçš„dMRIæ‰«æä¸­å‡†ç¡®ä¼°è®¡ç»„ç»‡å¾®è§‚ç»“æ„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ‰©æ•£å¾®è§‚ç»“æ„æˆåƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å®è§‚çº§åˆ«çš„å…ˆéªŒè§£å‰–ä¿¡æ¯å’Œå‚æ•°é—´çš„äº’ä¿¡æ¯ï¼Œå®ç°äº†å¿«é€Ÿè€Œé«˜ä¿çœŸçš„å¾®è§‚ç»“æ„æˆåƒã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†å››ç§æœ€æ–°çš„æŠ€æœ¯ï¼Œåœ¨ä¼°è®¡å¤šä¸ªæ‰©æ•£æ¨¡å‹çš„å‚æ•°æ˜ å°„æ—¶ï¼Œè¾¾åˆ°äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰30.51Â±0.58å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰0.97Â±0.004ã€‚</li>
<li>æ­¤æ–¹æ³•å®ç°äº†ä¸ä¼ ç»Ÿå¯†é›†é‡‡æ ·æ–¹æ³•ç›¸æ¯”çš„15å€åŠ é€Ÿï¼Œåè€…é€šå¸¸ä½¿ç”¨270ä¸ªæ‰©æ•£æ¢¯åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17933v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17933v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17933v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HRR-Hierarchical-Retrospection-Refinement-for-Generated-Image-Detection"><a href="#HRR-Hierarchical-Retrospection-Refinement-for-Generated-Image-Detection" class="headerlink" title="HRR: Hierarchical Retrospection Refinement for Generated Image Detection"></a>HRR: Hierarchical Retrospection Refinement for Generated Image Detection</h2><p><strong>Authors:Peipei Yuan, Zijing Xie, Shuo Ye, Hong Chen, Yulong Wang</strong></p>
<p>Generative artificial intelligence holds significant potential for abuse, and generative image detection has become a key focus of research. However, existing methods primarily focused on detecting a specific generative model and emphasizing the localization of synthetic regions, while neglecting the interference caused by image size and style on model learning. Our goal is to reach a fundamental conclusion: Is the image real or generated? To this end, we propose a diffusion model-based generative image detection framework termed Hierarchical Retrospection Refinement~(HRR). It designs a multi-scale style retrospection module that encourages the model to generate detailed and realistic multi-scale representations, while alleviating the learning biases introduced by dataset styles and generative models. Additionally, based on the principle of correntropy sparse additive machine, a feature refinement module is designed to reduce the impact of redundant features on learning and capture the intrinsic structure and patterns of the data, thereby improving the modelâ€™s generalization ability. Extensive experiments demonstrate the HRR framework consistently delivers significant performance improvements, outperforming state-of-the-art methods in generated image detection task. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å­˜åœ¨è¢«æ»¥ç”¨çš„å·¨å¤§æ½œåŠ›ï¼Œç”Ÿæˆå›¾åƒæ£€æµ‹å·²æˆä¸ºç ”ç©¶çš„å…³é”®ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ£€æµ‹ç‰¹å®šçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å¼ºè°ƒåˆæˆåŒºåŸŸçš„å®šä½ï¼Œè€Œå¿½ç•¥äº†å›¾åƒå¤§å°å’Œé£æ ¼å¯¹æ¨¡å‹å­¦ä¹ é€ æˆçš„å¹²æ‰°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¾—å‡ºä¸€ä¸ªåŸºæœ¬ç»“è®ºï¼šå›¾åƒæ˜¯çœŸå®è¿˜æ˜¯ç”Ÿæˆçš„ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå›¾åƒæ£€æµ‹æ¡†æ¶ï¼Œç§°ä¸ºåˆ†å±‚å›é¡¾ç»†åŒ–ï¼ˆHRRï¼‰ã€‚å®ƒè®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦é£æ ¼å›é¡¾æ¨¡å—ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆè¯¦ç»†å’Œé€¼çœŸçš„å¤šå°ºåº¦è¡¨ç¤ºï¼ŒåŒæ—¶å‡è½»æ•°æ®é›†é£æ ¼å’Œç”Ÿæˆæ¨¡å‹å¼•å…¥çš„å­¦ä¹ åè§ã€‚æ­¤å¤–ï¼ŒåŸºäºcorrentropyç¨€ç–åŠ æ³•æœºå™¨çš„åŸç†ï¼Œè®¾è®¡äº†ä¸€ä¸ªç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œä»¥å‡å°‘å†—ä½™ç‰¹å¾å¯¹å­¦ä¹ çš„å½±å“ï¼Œæ•æ‰æ•°æ®çš„å†…åœ¨ç»“æ„å’Œæ¨¡å¼ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHRRæ¡†æ¶åœ¨ç”Ÿæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå§‹ç»ˆè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17862v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ½œåœ¨æ»¥ç”¨é£é™©ï¼Œæå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå›¾åƒæ£€æµ‹æ¡†æ¶â€”â€”Hierarchical Retrospection Refinement (HRR)ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦é£æ ¼å›é¡¾æ¨¡å—ï¼Œå¯ä»¥ç¼“è§£æ•°æ®é›†é£æ ¼å’Œç”Ÿæˆæ¨¡å‹å¼•å…¥çš„å­¦ä¹ åè§ï¼Œå¹¶åŸºäºç†µç†è®ºè®¾è®¡äº†ä¸€ä¸ªç‰¹å¾ç²¾ç‚¼æ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒHRRæ¡†æ¶åœ¨ç”Ÿæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å­˜åœ¨æ»¥ç”¨é£é™©ï¼Œç”Ÿæˆå›¾åƒæ£€æµ‹æˆä¸ºç ”ç©¶é‡ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ£€æµ‹ç‰¹å®šç”Ÿæˆæ¨¡å‹å¹¶å¼ºè°ƒåˆæˆåŒºåŸŸçš„å®šä½ï¼Œå¿½è§†äº†å›¾åƒå¤§å°å’Œé£æ ¼å¯¹æ¨¡å‹å­¦ä¹ çš„å½±å“ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå›¾åƒæ£€æµ‹æ¡†æ¶â€”â€”Hierarchical Retrospection Refinement (HRR)ã€‚</li>
<li>HRRæ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦é£æ ¼å›é¡¾æ¨¡å—ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†ä¸”é€¼çœŸçš„å¤šå°ºåº¦è¡¨ç¤ºã€‚</li>
<li>HRRæ¡†æ¶åŸºäºç†µç†è®ºè®¾è®¡äº†ä¸€ä¸ªç‰¹å¾ç²¾ç‚¼æ¨¡å—ï¼Œæ—¨åœ¨å‡å°‘å†—ä½™ç‰¹å¾å¯¹å­¦ä¹ çš„å¹²æ‰°ï¼Œå¹¶æ•æ‰æ•°æ®çš„å†…åœ¨ç»“æ„å’Œæ¨¡å¼ã€‚</li>
<li>HRRæ¡†æ¶é€šè¿‡æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ç”Ÿæˆå›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17862v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17862v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17862v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="On-the-Vulnerability-of-Concept-Erasure-in-Diffusion-Models"><a href="#On-the-Vulnerability-of-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="On the Vulnerability of Concept Erasure in Diffusion Models"></a>On the Vulnerability of Concept Erasure in Diffusion Models</h2><p><strong>Authors:Lucas Beerens, Alex D. Richardson, Kaicheng Zhang, Dongdong Chen</strong></p>
<p>The proliferation of text-to-image diffusion models has raised significant privacy and security concerns, particularly regarding the generation of copyrighted or harmful images. To address these issues, research on machine unlearning has developed various concept erasure methods, which aim to remove the effect of unwanted data through post-hoc training. However, we show these erasure techniques are vulnerable, where images of supposedly erased concepts can still be generated using adversarially crafted prompts. We introduce RECORD, a coordinate-descent-based algorithm that discovers prompts capable of eliciting the generation of erased content. We demonstrate that RECORD significantly beats the attack success rate of current state-of-the-art attack methods. Furthermore, our findings reveal that models subjected to concept erasure are more susceptible to adversarial attacks than previously anticipated, highlighting the urgency for more robust unlearning approaches. We open source all our code at <a target="_blank" rel="noopener" href="https://github.com/LucasBeerens/RECORD">https://github.com/LucasBeerens/RECORD</a> </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ™®åŠå¼•å‘äº†å…³äºéšç§å’Œå®‰å…¨æ€§çš„é‡å¤§æ‹…å¿§ï¼Œå°¤å…¶æ˜¯å…³äºç”Ÿæˆç‰ˆæƒæˆ–æœ‰å®³å›¾åƒçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœºå™¨é—å¿˜çš„ç ”ç©¶å·²ç»å¼€å‘å‡ºäº†å„ç§æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ—¨åœ¨é€šè¿‡äº‹åè®­ç»ƒæ¥æ¶ˆé™¤ä¸æƒ³è¦æ•°æ®çš„å½±å“ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å±•ç¤ºè¿™äº›æ¶ˆé™¤æŠ€æœ¯æ˜¯æœ‰ç¼ºé™·çš„ï¼Œå…¶ä¸­è¢«å‡å®šå·²æ¶ˆé™¤çš„æ¦‚å¿µçš„å›¾åƒä»ç„¶å¯ä»¥é€šè¿‡å¯¹æŠ—æ€§åˆ¶ä½œçš„æç¤ºæ¥ç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†RECORDï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåæ ‡ä¸‹é™ç®—æ³•ï¼Œèƒ½å¤Ÿå‘ç°èƒ½å¤Ÿå¼•å‘å·²åˆ é™¤å†…å®¹ç”Ÿæˆçš„æç¤ºã€‚æˆ‘ä»¬è¯æ˜RECORDæ˜¾è‘—æé«˜äº†å½“å‰æœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»å—æ¦‚å¿µæ¶ˆé™¤å¤„ç†çš„æ¨¡å‹æ¯”é¢„æœŸæ›´å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥çš„é—å¿˜æ–¹æ³•çš„ç´§è¿«æ€§ã€‚æˆ‘ä»¬æ‰€æœ‰çš„ä»£ç å‡å·²å¼€æºï¼š<a target="_blank" rel="noopener" href="https://github.com/LucasBeerens/RECORD">https://github.com/LucasBeerens/RECORD</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17537v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ™®åŠå¼•å‘äº†å…³äºç”Ÿæˆç‰ˆæƒæˆ–æœ‰å®³å›¾åƒçš„é‡è¦éšç§å’Œå®‰å…¨æ‹…å¿§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœºå™¨é—å¿˜ç ”ç©¶å¼€å‘äº†å„ç§æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡äº‹åè®­ç»ƒæ¶ˆé™¤ä¸æƒ³è¦æ•°æ®çš„å½±å“ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜è¿™äº›æ¶ˆé™¤æŠ€æœ¯å­˜åœ¨æ¼æ´ï¼Œé€šè¿‡ç‰¹å®šå¯¹æŠ—æ€§æ„å»ºçš„æç¤ºä»å¯ç”Ÿæˆçœ‹ä¼¼å·²åˆ é™¤çš„æ¦‚å¿µçš„å›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†RECORDç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåæ ‡ä¸‹é™çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå‘ç°èƒ½å¤Ÿæ¿€å‘å·²åˆ é™¤å†…å®¹ç”Ÿæˆçš„æç¤ºã€‚æˆ‘ä»¬è¯æ˜RECORDæ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜å‘ç°ï¼Œç»å†æ¦‚å¿µæ¶ˆé™¤çš„æ¨¡å‹æ¯”ä»¥å¾€é¢„æœŸæ›´å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œè¿™çªæ˜¾äº†å¯¹æ›´ç¨³å¥çš„é—å¿˜æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/LucasBeerens/RECORD%E3%80%82">https://github.com/LucasBeerens/RECORDã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ™®åŠå¼•å‘éšç§å’Œå®‰å…¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…³äºç”Ÿæˆç‰ˆæƒæˆ–æœ‰å®³å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºæ¦‚å¿µæ¶ˆé™¤æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡äº‹åè®­ç»ƒæ¶ˆé™¤ä¸æƒ³è¦çš„æ•°æ®å½±å“ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•å­˜åœ¨æ¼æ´ï¼Œå¯¹æŠ—æ€§æ„å»ºçš„æç¤ºå¯ä»¥ç”Ÿæˆçœ‹ä¼¼å·²åˆ é™¤çš„æ¦‚å¿µçš„å›¾åƒã€‚</li>
<li>å¼•å…¥RECORDç®—æ³•ï¼Œèƒ½æœ‰æ•ˆå‘ç°èƒ½å¤Ÿæ¿€å‘å·²åˆ é™¤å†…å®¹ç”Ÿæˆçš„æç¤ºï¼Œæ”»å‡»æˆåŠŸç‡é«˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºï¼Œç»å†æ¦‚å¿µæ¶ˆé™¤çš„æ¨¡å‹æ›´å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚</li>
<li>éœ€è¦æ›´ç¨³å¥çš„é—å¿˜æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17537v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17537v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17537v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17537v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17537v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DICEPTION-A-Generalist-Diffusion-Model-for-Visual-Perceptual-Tasks"><a href="#DICEPTION-A-Generalist-Diffusion-Model-for-Visual-Perceptual-Tasks" class="headerlink" title="DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks"></a>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</h2><p><strong>Authors:Canyu Zhao, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, Chunhua Shen</strong></p>
<p>Our primary goal here is to create a good, generalist perception model that can tackle multiple tasks, within limits on computational resources and training data. To achieve this, we resort to text-to-image diffusion models pre-trained on billions of images. Our exhaustive evaluation metrics demonstrate that DICEPTION effectively tackles multiple perception tasks, achieving performance on par with state-of-the-art models. We achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs. 1B pixel-level annotated images). Inspired by Wang et al., DICEPTION formulates the outputs of various perception tasks using color encoding; and we show that the strategy of assigning random colors to different instances is highly effective in both entity segmentation and semantic segmentation. Unifying various perception tasks as conditional image generation enables us to fully leverage pre-trained text-to-image models. Thus, DICEPTION can be efficiently trained at a cost of orders of magnitude lower, compared to conventional models that were trained from scratch. When adapting our model to other tasks, it only requires fine-tuning on as few as 50 images and 1% of its parameters. DICEPTION provides valuable insights and a more promising solution for visual generalist models. Homepage: <a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Diception">https://aim-uofa.github.io/Diception</a>, Huggingface Demo: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Canyu/Diception-Demo">https://huggingface.co/spaces/Canyu/Diception-Demo</a>. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯åœ¨æœ‰é™çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®æ¡ä»¶ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†å¤šé¡¹ä»»åŠ¡çš„è‰¯å¥½é€šç”¨æ„ŸçŸ¥æ¨¡å‹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ•°åäº¿å¼ å›¾åƒè¿›è¡Œé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°æŒ‡æ ‡è¡¨æ˜ï¼ŒDICEPTIONæœ‰æ•ˆåœ°è§£å†³äº†å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›æ¨¡å‹çš„è¡¨ç°ç›¸å½“ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é‡ä»…ä¸ºå…¶0.06%ï¼ˆä¾‹å¦‚ï¼Œ60ä¸‡å¼ å¯¹æ¯”åäº¿åƒç´ çº§æ³¨é‡Šå›¾åƒï¼‰ï¼Œä¾¿è¾¾åˆ°äº†ä¸SAM-vit-hç›¸å½“çš„æ•ˆæœã€‚DICEPTIONå—åˆ°Wangç­‰äººçš„å¯å‘ï¼Œé‡‡ç”¨é¢œè‰²ç¼–ç æ¥åˆ¶å®šå„ç§æ„ŸçŸ¥ä»»åŠ¡çš„è¾“å‡ºï¼›æˆ‘ä»¬è¯æ˜ï¼Œä¸ºä¸åŒå®ä¾‹åˆ†é…éšæœºé¢œè‰²çš„ç­–ç•¥åœ¨å®ä½“åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ä¸­éƒ½æä¸ºæœ‰æ•ˆã€‚å°†å„ç§æ„ŸçŸ¥ä»»åŠ¡ç»Ÿä¸€ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚å› æ­¤ï¼Œä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„å¸¸è§„æ¨¡å‹ç›¸æ¯”ï¼ŒDICEPTIONçš„è®­ç»ƒæˆæœ¬å¤§å¤§é™ä½ã€‚å½“å°†æˆ‘ä»¬çš„æ¨¡å‹é€‚åº”äºå…¶ä»–ä»»åŠ¡æ—¶ï¼Œå®ƒåªéœ€è¦åœ¨å°‘é‡å›¾åƒï¼ˆè‡³å¤š50å¼ ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥åŠä½¿ç”¨å…¶å‚æ•°çš„1%ã€‚DICEPTIONä¸ºè§†è§‰é€šç”¨æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæ›´æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ä¸»é¡µï¼š<a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Diception">https://aim-uofa.github.io/Diception</a>ï¼ŒHuggingfaceæ¼”ç¤ºï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Canyu/Diception-Demo">https://huggingface.co/spaces/Canyu/Diception-Demo</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17157v2">PDF</a> 29 pages, 20 figures. Homepage: <a target="_blank" rel="noopener" href="https://aim-uofa.github.io/Diception">https://aim-uofa.github.io/Diception</a>,   Huggingface Demo: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Canyu/Diception-Demo">https://huggingface.co/spaces/Canyu/Diception-Demo</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ—¨åœ¨åˆ›å»ºè‰¯å¥½çš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åœ¨è®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®å—é™çš„æƒ…å†µä¸‹å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚é€šè¿‡å€ŸåŠ©åœ¨æ•°åäº¿å›¾åƒä¸Šé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ŒDICEPTIONèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡ï¼Œæ€§èƒ½è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚å…¶ç­–ç•¥æ˜¯é€šè¿‡å¯¹ä¸åŒå®ä¾‹åˆ†é…éšæœºé¢œè‰²ï¼Œåœ¨å®ä½“åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éƒ½è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ã€‚é€šè¿‡å°†å„ç§æ„ŸçŸ¥ä»»åŠ¡ç»Ÿä¸€ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆï¼ŒDICEPTIONèƒ½å¤Ÿå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè®­ç»ƒæˆæœ¬å¤§å¤§é™ä½ï¼Œç›¸æ¯”ä»å¤´å¼€å§‹è®­ç»ƒçš„ä¼ ç»Ÿæ¨¡å‹æ•ˆç‡æ›´é«˜ã€‚è¯¥æ¨¡å‹åœ¨é€‚åº”å…¶ä»–ä»»åŠ¡æ—¶ï¼Œä»…éœ€å°‘é‡å›¾åƒå’Œ1%çš„å‚æ•°è¿›è¡Œå¾®è°ƒã€‚DICEPTIONä¸ºè§†è§‰é€šç”¨æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæ›´æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DICEPTIONæ¨¡å‹æ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿå¤„ç†å¤šä¸ªä»»åŠ¡çš„é€šç”¨æ„ŸçŸ¥æ¨¡å‹ï¼Œå—é™äºè®¡ç®—èµ„æºå’Œè®­ç»ƒæ•°æ®ã€‚</li>
<li>è¯¥æ¨¡å‹å€ŸåŠ©é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¾¾åˆ°å¤„ç†å¤šä¸ªæ„ŸçŸ¥ä»»åŠ¡çš„ç›®æ ‡ã€‚</li>
<li>DICEPTIONé€šè¿‡å¯¹ä¸åŒå®ä¾‹åˆ†é…éšæœºé¢œè‰²ï¼Œå®ç°äº†åœ¨å®ä½“åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ä¸­çš„é«˜æ•ˆè¡¨ç°ã€‚</li>
<li>é€šè¿‡å°†æ„ŸçŸ¥ä»»åŠ¡ç»Ÿä¸€ä¸ºæ¡ä»¶å›¾åƒç”Ÿæˆï¼ŒDICEPTIONèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬-å›¾åƒæ¨¡å‹ï¼Œé™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>DICEPTIONåœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œä»…éœ€å°‘é‡æ•°æ®å’Œå‚æ•°å¾®è°ƒã€‚</li>
<li>DICEPTIONçš„æ€§èƒ½ä¸ä¸šç•Œé¢†å…ˆæ°´å¹³ç›¸å½“ï¼Œä½¿ç”¨çš„æ•°æ®é‡ä»…ä¸ºSAM-vit-hæ¨¡å‹çš„0.06%ã€‚</li>
<li>DICEPTIONä¸ºè§†è§‰é€šç”¨æ¨¡å‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆå’Œæœ‰ä»·å€¼çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17157v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.17157v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ConsistentDreamer-View-Consistent-Meshes-Through-Balanced-Multi-View-Gaussian-Optimization"><a href="#ConsistentDreamer-View-Consistent-Meshes-Through-Balanced-Multi-View-Gaussian-Optimization" class="headerlink" title="ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View   Gaussian Optimization"></a>ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View   Gaussian Optimization</h2><p><strong>Authors:Onat Åahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu</strong></p>
<p>Recent advances in diffusion models have significantly improved 3D generation, enabling the use of assets generated from an image for embodied AI simulations. However, the one-to-many nature of the image-to-3D problem limits their use due to inconsistent content and quality across views. Previous models optimize a 3D model by sampling views from a view-conditioned diffusion prior, but diffusion models cannot guarantee view consistency. Instead, we present ConsistentDreamer, where we first generate a set of fixed multi-view prior images and sample random views between them with another diffusion model through a score distillation sampling (SDS) loss. Thereby, we limit the discrepancies between the views guided by the SDS loss and ensure a consistent rough shape. In each iteration, we also use our generated multi-view prior images for fine-detail reconstruction. To balance between the rough shape and the fine-detail optimizations, we introduce dynamic task-dependent weights based on homoscedastic uncertainty, updated automatically in each iteration. Additionally, we employ opacity, depth distortion, and normal alignment losses to refine the surface for mesh extraction. Our method ensures better view consistency and visual quality compared to the state-of-the-art. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•æå¤§åœ°æé«˜äº†3Dç”Ÿæˆçš„å›¾åƒè´¨é‡ï¼Œä½¿å¾—å¯ä»¥ä½¿ç”¨ä»è¿™äº›å›¾åƒç”Ÿæˆçš„èµ„äº§è¿›è¡Œå®ä½“AIæ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œç”±äºå›¾åƒåˆ°3Dé—®é¢˜çš„å¤šå¯¹ä¸€ç‰¹æ€§ï¼Œå¯¼è‡´ä»ä¸åŒè§†è§’çœ‹å†…å®¹å¹¶ä¸ä¸€è‡´å’Œè´¨é‡ä¸ç¨³å®šï¼Œä»è€Œé™åˆ¶äº†å…¶ä½¿ç”¨ã€‚ä¹‹å‰çš„æ¨¡å‹é€šè¿‡ä»è§†è§’è°ƒèŠ‚çš„æ‰©æ•£å…ˆéªŒä¸­é‡‡æ ·è§†è§’ä¼˜åŒ–3Dæ¨¡å‹ï¼Œä½†æ‰©æ•£æ¨¡å‹æ— æ³•ä¿è¯è§†è§’çš„ä¸€è‡´æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ConsistentDreamerï¼Œé¦–å…ˆç”Ÿæˆä¸€ç»„å›ºå®šçš„å¤šè§†è§’å…ˆéªŒå›¾åƒï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ä½¿ç”¨å¦ä¸€ä¸ªæ‰©æ•£æ¨¡å‹è¿›è¡Œéšæœºè§†è§’é‡‡æ ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡SDSæŸå¤±æŒ‡å¯¼çš„è§†å›¾é™åˆ¶å·®å¼‚ï¼Œå¹¶ç¡®ä¿ä¸€è‡´çš„ç²—ç•¥å½¢çŠ¶ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ç”Ÿæˆçš„å¤šè§†è§’å…ˆéªŒå›¾åƒè¿›è¡Œç²¾ç»†ç»†èŠ‚é‡å»ºã€‚ä¸ºäº†åœ¨ç²—ç•¥å½¢çŠ¶å’Œç²¾ç»†ç»†èŠ‚ä¼˜åŒ–ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¼‚æ–¹å·®ä¸ç¡®å®šæ€§çš„åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­è‡ªåŠ¨æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±æ¥ä¼˜åŒ–è¡¨é¢ä»¥è¿›è¡Œç½‘æ ¼æå–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§’ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢ç›¸æ¯”å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ç¡®ä¿äº†æ›´å¥½çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09278v3">PDF</a> Manuscript accepted by Pattern Recognition Letters. Project Page:   <a target="_blank" rel="noopener" href="https://onatsahin.github.io/ConsistentDreamer/">https://onatsahin.github.io/ConsistentDreamer/</a></p>
<p><strong>Summary</strong><br>     æœ€æ–°æ‰©æ•£æ¨¡å‹æ”¹è¿›æ¨åŠ¨äº†3Dç”Ÿæˆçš„è¿›æ­¥ï¼Œèƒ½åˆ©ç”¨å›¾åƒç”Ÿæˆèµ„äº§åº”ç”¨äºAIä»¿çœŸã€‚ç„¶è€Œï¼Œå›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šé—®é¢˜é€ æˆå†…å®¹åœ¨ä¸åŒè§†è§’ä¸‹å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚æœ¬æ–‡æå‡ºConsistentDreamerï¼Œé€šè¿‡ç”Ÿæˆå›ºå®šå¤šè§†è§’å…ˆéªŒå›¾åƒå¹¶åœ¨æ­¤ä¹‹é—´é‡‡æ ·éšæœºè§†è§’ï¼Œç»“åˆè¯„åˆ†è’¸é¦é‡‡æ ·æŸå¤±å®ç°è§†è§’ä¸€è‡´æ€§ã€‚é€šè¿‡åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡å¹³è¡¡ç²—ç³™å½¢çŠ¶ä¸ç»†èŠ‚ä¼˜åŒ–ï¼Œå¹¶é‡‡ç”¨ä¸é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±ç»†åŒ–è¡¨é¢è¿›è¡Œç½‘æ ¼æå–ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”æ›´å¥½çš„è§†è§’ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†3Dç”Ÿæˆçš„è¿›æ­¥ï¼Œä½¿å¾—å›¾åƒç”Ÿæˆçš„èµ„äº§å¯ç”¨äºAIä»¿çœŸã€‚</li>
<li>å›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šé—®é¢˜å¯¼è‡´å†…å®¹åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ConsistentDreameré€šè¿‡ç”Ÿæˆå›ºå®šå¤šè§†è§’å…ˆéªŒå›¾åƒï¼Œç»“åˆè¯„åˆ†è’¸é¦é‡‡æ ·æŸå¤±å®ç°è§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡ç”¨äºå¹³è¡¡ç²—ç³™å½¢çŠ¶å’Œç»†èŠ‚ä¼˜åŒ–çš„å¹³è¡¡ã€‚</li>
<li>ä¸é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±ç”¨äºç»†åŒ–è¡¨é¢è¿›è¡Œç½‘æ ¼æå–ã€‚</li>
<li>è¯¥æ–¹æ³•ç¡®ä¿äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”æ›´å¥½çš„è§†è§’ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2502.09278v3/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SG-I2V-Self-Guided-Trajectory-Control-in-Image-to-Video-Generation"><a href="#SG-I2V-Self-Guided-Trajectory-Control-in-Image-to-Video-Generation" class="headerlink" title="SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation"></a>SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</h2><p><strong>Authors:Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell</strong></p>
<p>Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guided$\unicode{x2013}$offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while significantly narrowing down the performance gap with supervised models in terms of visual quality and motion fidelity. Additional details and video results are available on our project page: <a target="_blank" rel="noopener" href="https://kmcode1.github.io/Projects/SG-I2V">https://kmcode1.github.io/Projects/SG-I2V</a> </p>
<blockquote>
<p>å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆæ–¹æ³•å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ã€é€¼çœŸçš„è´¨é‡ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿæˆçš„è§†é¢‘ä¸­è°ƒæ•´ç‰¹å®šå…ƒç´ ï¼Œå¦‚ç‰©ä½“è¿åŠ¨æˆ–ç›¸æœºç§»åŠ¨ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªç¹ççš„è¯•é”™è¿‡ç¨‹ï¼Œä¾‹å¦‚ï¼Œæ¶‰åŠä½¿ç”¨ä¸åŒçš„éšæœºç§å­é‡æ–°ç”Ÿæˆè§†é¢‘ã€‚æœ€è¿‘çš„æŠ€æœ¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥éµå¾ªæ¡ä»¶ä¿¡å·æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–ç‚¹è½¨è¿¹ã€‚ç„¶è€Œï¼Œè¿™ç§å¾®è°ƒè¿‡ç¨‹å¯èƒ½è®¡ç®—é‡å¤§ï¼Œå¹¶ä¸”éœ€è¦å¸¦æœ‰æ³¨é‡Šå¯¹è±¡è¿åŠ¨çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¯èƒ½éš¾ä»¥è·å¾—ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SG-I2Vï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ§çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé‡‡ç”¨è‡ªæˆ‘å¼•å¯¼çš„æ–¹å¼ï¼Œä»…ä¾é é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„çŸ¥è¯†å®ç°é›¶æ ·æœ¬æ§åˆ¶ï¼Œæ— éœ€å¾®è°ƒæˆ–å¤–éƒ¨çŸ¥è¯†ã€‚æˆ‘ä»¬çš„é›¶æ ·æœ¬æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè¿åŠ¨ä¿çœŸåº¦æ–¹é¢è¶…è¶Šäº†æ— ç›‘ç£åŸºçº¿ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†ä¸ç›‘ç£æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚æ›´å¤šç»†èŠ‚å’Œè§†é¢‘ç»“æœå¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://kmcode1.github.io/Projects/SG-I2V">https://kmcode1.github.io/Projects/SG-I2V</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04989v3">PDF</a> ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://kmcode1.github.io/Projects/SG-I2V/">https://kmcode1.github.io/Projects/SG-I2V/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSG-I2Vçš„è‡ªå¼•å¯¼å¯æ§å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚å®ƒä¾èµ–äºé¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒæˆ–å¤–éƒ¨çŸ¥è¯†ï¼Œå³å¯å®ç°é›¶æ ·æœ¬æ§åˆ¶ã€‚è¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè¿åŠ¨ä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†æ— ç›‘ç£åŸºå‡†ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†ä¸ç›‘ç£æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²å…·å¤‡ä»¤äººå°è±¡æ·±åˆ»çš„é€¼çœŸè´¨é‡ã€‚</li>
<li>è°ƒæ•´ç”Ÿæˆè§†é¢‘ä¸­çš„ç‰¹å®šå…ƒç´ ï¼ˆå¦‚ç‰©ä½“è¿åŠ¨æˆ–ç›¸æœºç§»åŠ¨ï¼‰é€šå¸¸æ˜¯ä¸€ä¸ªç¹ççš„è¯•é”™è¿‡ç¨‹ã€‚</li>
<li>æœ€è¿‘çš„æŠ€æœ¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥éµå¾ªæ¡ä»¶ä¿¡å·ï¼ˆå¦‚è¾¹ç•Œæ¡†æˆ–ç‚¹è½¨è¿¹ï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>å¾®è°ƒè¿‡ç¨‹è®¡ç®—é‡å¤§ï¼Œéœ€è¦å¸¦æœ‰æ ‡æ³¨ç‰©ä½“è¿åŠ¨çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†éš¾ä»¥è·å¾—ã€‚</li>
<li>å¼•å…¥SG-I2Væ¡†æ¶ï¼Œå®ç°å¯æ§å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼Œå…·å¤‡è‡ªå¼•å¯¼ç‰¹æ€§ã€‚</li>
<li>SG-I2Vä¾èµ–é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒæˆ–å¤–éƒ¨çŸ¥è¯†ï¼Œå®ç°é›¶æ ·æœ¬æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2411.04989v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2411.04989v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2411.04989v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Both-Ears-Wide-Open-Towards-Language-Driven-Spatial-Audio-Generation"><a href="#Both-Ears-Wide-Open-Towards-Language-Driven-Spatial-Audio-Generation" class="headerlink" title="Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation"></a>Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation</h2><p><strong>Authors:Peiwen Sun, Sitong Cheng, Xiangtai Li, Zhen Ye, Huadai Liu, Honggang Zhang, Wei Xue, Yike Guo</strong></p>
<p>Recently, diffusion models have achieved great success in mono-channel audio generation. However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions. Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models. To the best of our knowledge, this work represents the first attempt to address these issues. We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources. Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation. Existing audio generation models tend to generate rather random and indistinct spatial audio. To provide accurate guidance for Latent Diffusion Models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance. By leveraging spatial guidance, our model not only achieves the objective of generating immersive and controllable spatial audio from text but also extends to other modalities as the pioneer attempt. Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods. The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å•å£°é“éŸ³é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå½“æ¶‰åŠåˆ°ç«‹ä½“å£°éŸ³é¢‘ç”Ÿæˆæ—¶ï¼Œå£°éŸ³åœºæ™¯é€šå¸¸åŒ…å«å¤šä¸ªç‰©ä½“å’Œæ–¹å‘ï¼Œæƒ…å†µå¤æ‚ã€‚ç”±äºæ•°æ®æˆæœ¬é«˜æ˜‚å’Œç”Ÿæˆæ¨¡å‹ä¸ç¨³å®šï¼Œåˆ©ç”¨ç©ºé—´ä¸Šä¸‹æ–‡æ§åˆ¶ç«‹ä½“å£°éŸ³é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡å°è¯•è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€åŸºäºæ¨¡æ‹Ÿã€GPTè¾…åŠ©çš„æ•°æ®é›†BEWO-1Mï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„å£°éŸ³åœºæ™¯å’Œæè¿°ï¼Œç”šè‡³åŒ…æ‹¬åŠ¨æ€å’Œå¤šé‡æ¥æºçš„å†…å®¹ã€‚é™¤äº†æ–‡æœ¬æ¨¡æ€å¤–ï¼Œæˆ‘ä»¬è¿˜è·å–äº†ä¸€ç³»åˆ—å›¾åƒï¼Œå¹¶é€šè¿‡æ£€ç´¢åˆç†é…å¯¹ç«‹ä½“å£°éŸ³é¢‘ï¼Œä»¥æ¨åŠ¨å¤šæ¨¡æ€ç”Ÿæˆã€‚ç°æœ‰çš„éŸ³é¢‘ç”Ÿæˆæ¨¡å‹å€¾å‘äºç”Ÿæˆéšæœºä¸”æ¨¡ç³Šçš„ç©ºé—´éŸ³é¢‘ã€‚ä¸ºäº†ä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹æä¾›å‡†ç¡®çš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialSonicæ¨¡å‹ï¼Œåˆ©ç”¨ç©ºé—´æ„ŸçŸ¥ç¼–ç å™¨å’Œæ–¹ä½çŠ¶æ€çŸ©é˜µæ¥æä¾›åˆç†çš„ç©ºé—´æŒ‡å¯¼ã€‚é€šè¿‡åˆ©ç”¨ç©ºé—´æŒ‡å¯¼ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…å®ç°äº†ä»æ–‡æœ¬ç”Ÿæˆæ²‰æµ¸å¼ä¸”å¯æ§çš„ç©ºé—´éŸ³é¢‘çš„ç›®æ ‡ï¼Œè€Œä¸”è¿˜æ‰©å±•åˆ°å…¶ä»–æ¨¡æ€ï¼Œä½œä¸ºé¦–æ¬¡å°è¯•ã€‚æœ€åï¼Œåœ¨å…¬å¹³çš„è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬å¯¹æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œäº†ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ï¼Œä»¥æ¯”è¾ƒæˆ‘ä»¬çš„æ–¹æ³•ä¸æµè¡Œæ–¹æ³•ã€‚ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼Œå°¤å…¶èƒ½å¤Ÿç”Ÿæˆéµå¾ªç‰©ç†è§„åˆ™çš„ç«‹ä½“å£°éŸ³é¢‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10676v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å•å£°é“éŸ³é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨ç«‹ä½“å£°éŸ³é¢‘ç”Ÿæˆæ–¹é¢ï¼Œç”±äºæ•°æ®æˆæœ¬é«˜æ˜‚å’Œç”Ÿæˆæ¨¡å‹ä¸ç¨³å®šï¼Œæ§åˆ¶å…·æœ‰å¤šä¸ªå¯¹è±¡å’Œæ–¹å‘çš„å¤æ‚åœºæ™¯çš„å£°éŸ³ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€ä»¿çœŸã€GPTè¾…åŠ©çš„BEWO-1Mæ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸°å¯Œçš„å£°éŸ³åœºæ™¯å’Œæè¿°ï¼Œè¿˜åŒ…æ‹¬åŠ¨æ€å’Œå¤šä¸ªæ¥æºã€‚é™¤äº†æ–‡æœ¬æ¨¡å¼å¤–ï¼Œè¿˜é€šè¿‡æ£€ç´¢è·å¾—äº†ä¸€ç»„å›¾åƒå’Œåˆç†é…å¯¹çš„ç«‹ä½“å£°éŸ³é¢‘ï¼Œä»¥æ¨åŠ¨å¤šæ¨¡å¼ç”Ÿæˆã€‚ç°æœ‰éŸ³é¢‘ç”Ÿæˆæ¨¡å‹å€¾å‘äºç”Ÿæˆéšæœºå’Œæ¨¡ç³Šçš„ç©ºé—´éŸ³é¢‘ã€‚ä¸ºäº†ä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹æä¾›å‡†ç¡®çš„æŒ‡å¯¼ï¼Œå¼•å…¥äº†SpatialSonicæ¨¡å‹ï¼Œåˆ©ç”¨ç©ºé—´æ„ŸçŸ¥ç¼–ç å™¨å’Œæ–¹ä½çŠ¶æ€çŸ©é˜µæ¥æ­ç¤ºåˆç†çš„ç©ºé—´æŒ‡å¯¼ã€‚é€šè¿‡åˆ©ç”¨ç©ºé—´æŒ‡å¯¼ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…å®ç°äº†ä»æ–‡æœ¬ç”Ÿæˆæ²‰æµ¸å¼å¯æ§ç©ºé—´éŸ³é¢‘çš„ç›®æ ‡ï¼Œè¿˜ä½œä¸ºé¦–æ¬¡å°è¯•æ‰©å±•åˆ°å…¶ä»–æ¨¡å¼ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œäº†ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆï¼Œèƒ½å¤Ÿç”Ÿæˆç¬¦åˆç‰©ç†è§„åˆ™çš„ç«‹ä½“å£°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç«‹ä½“å£°éŸ³é¢‘ç”Ÿæˆä¸Šé­é‡æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ•°æ®æˆæœ¬é«˜æ˜‚å’Œç”Ÿæˆæ¨¡å‹ä¸ç¨³å®šã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†å¤§è§„æ¨¡ã€ä»¿çœŸã€GPTè¾…åŠ©çš„BEWO-1Mæ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„å£°éŸ³åœºæ™¯å’Œæè¿°ï¼ŒåŒ…å«åŠ¨æ€å’Œå¤šä¸ªæ¥æºã€‚</li>
<li>é™¤äº†æ–‡æœ¬æ¨¡å¼å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡æ£€ç´¢ç»“åˆå›¾åƒå’Œç«‹ä½“å£°éŸ³é¢‘ï¼Œæ¨åŠ¨å¤šæ¨¡å¼ç”Ÿæˆã€‚</li>
<li>ç°æœ‰éŸ³é¢‘ç”Ÿæˆæ¨¡å‹å€¾å‘äºç”Ÿæˆéšæœºå’Œæ¨¡ç³Šçš„ç©ºé—´éŸ³é¢‘ã€‚</li>
<li>å¼•å…¥äº†SpatialSonicæ¨¡å‹ï¼Œåˆ©ç”¨ç©ºé—´æ„ŸçŸ¥ç¼–ç å™¨å’Œæ–¹ä½çŠ¶æ€çŸ©é˜µï¼Œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹æä¾›å‡†ç¡®çš„ç©ºé—´æŒ‡å¯¼ã€‚</li>
<li>æ¨¡å‹èƒ½ç”Ÿæˆæ²‰æµ¸å¼å¯æ§ç©ºé—´éŸ³é¢‘ï¼Œå¹¶ä»æ–‡æœ¬æ‰©å±•åˆ°å…¶ä»–æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2410.10676v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2410.10676v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2410.10676v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2410.10676v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2410.10676v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Training-free-Camera-Control-for-Video-Generation"><a href="#Training-free-Camera-Control-for-Video-Generation" class="headerlink" title="Training-free Camera Control for Video Generation"></a>Training-free Camera Control for Video Generation</h2><p><strong>Authors:Chen Hou, Zhibo Chen</strong></p>
<p>We propose a training-free and robust solution to offer camera movement control for off-the-shelf video diffusion models. Unlike previous work, our method does not require any supervised finetuning on camera-annotated datasets or self-supervised training via data augmentation. Instead, it can be plug-and-play with most pretrained video diffusion models and generate camera-controllable videos with a single image or text prompt as input. The inspiration for our work comes from the layout prior that intermediate latents encode for the generated results, thus rearranging noisy pixels in them will cause the output content to relocate as well. As camera moving could also be seen as a type of pixel rearrangement caused by perspective change, videos can be reorganized following specific camera motion if their noisy latents change accordingly. Building on this, we propose CamTrol, which enables robust camera control for video diffusion models. It is achieved by a two-stage process. First, we model image layout rearrangement through explicit camera movement in 3D point cloud space. Second, we generate videos with camera motion by leveraging the layout prior of noisy latents formed by a series of rearranged images. Extensive experiments have demonstrated its superior performance in both video generation and camera motion alignment compared with other finetuned methods. Furthermore, we show the capability of CamTrol to generalize to various base models, as well as its impressive applications in scalable motion control, dealing with complicated trajectories and unsupervised 3D video generation. Videos available at <a target="_blank" rel="noopener" href="https://lifedecoder.github.io/CamTrol/">https://lifedecoder.github.io/CamTrol/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç°æˆçš„è§†é¢‘æ‰©æ•£æ¨¡å‹æä¾›æ‘„åƒæœºè¿åŠ¨æ§åˆ¶ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦åœ¨æ‘„åƒæœºæ³¨é‡Šæ•°æ®é›†ä¸Šè¿›è¡Œä»»ä½•æœ‰ç›‘ç£çš„å¾®è°ƒæˆ–ä½¿ç”¨æ•°æ®å¢å¼ºçš„è‡ªæˆ‘ç›‘ç£è®­ç»ƒã€‚ç›¸åï¼Œå®ƒå¯ä»¥ä¸å¤§å¤šæ•°é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹å³æ’å³ç”¨ï¼Œå¹¶ä½¿ç”¨å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥æ¥ç”Ÿæˆæ‘„åƒæœºå¯æ§çš„è§†é¢‘ã€‚æˆ‘ä»¬å·¥ä½œçš„çµæ„Ÿæ¥è‡ªäºå¸ƒå±€ä¼˜å…ˆçš„åŸåˆ™ï¼Œå³ä¸­é—´æ½œåœ¨ç¼–ç ç”Ÿæˆçš„ç»“æœï¼Œå› æ­¤é‡æ–°æ’åˆ—å…¶ä¸­çš„å™ªå£°åƒç´ å°†å¯¼è‡´è¾“å‡ºå†…å®¹ä¹Ÿå‘ç”Ÿç§»åŠ¨ã€‚ç”±äºæ‘„åƒæœºç§»åŠ¨ä¹Ÿå¯ä»¥è¢«è§†ä¸ºç”±è§†è§’å˜åŒ–å¼•èµ·çš„åƒç´ é‡æ–°æ’åˆ—çš„ä¸€ç§å½¢å¼ï¼Œå› æ­¤å¦‚æœå®ƒä»¬çš„å™ªå£°æ½œåœ¨ç›¸åº”æ”¹å˜ï¼Œè§†é¢‘å°±å¯ä»¥æ ¹æ®ç‰¹å®šçš„æ‘„åƒæœºè¿åŠ¨è¿›è¡Œé‡ç»„ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†CamTrolï¼Œå®ƒä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹å®ç°äº†ç¨³å¥çš„æ‘„åƒæœºæ§åˆ¶ã€‚è¿™æ˜¯é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹å®ç°çš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡3Dç‚¹äº‘ç©ºé—´ä¸­çš„æ˜¾å¼æ‘„åƒæœºè¿åŠ¨å¯¹å›¾åƒå¸ƒå±€è¿›è¡Œé‡æ–°æ’åˆ—è¿›è¡Œå»ºæ¨¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”±ä¸€ç³»åˆ—é‡æ–°æ’åˆ—çš„å›¾åƒå½¢æˆçš„å™ªå£°å¸ƒå±€çš„å…ˆéªŒçŸ¥è¯†æ¥ç”Ÿæˆå…·æœ‰æ‘„åƒæœºè¿åŠ¨çš„è§†é¢‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨è§†é¢‘ç”Ÿæˆå’Œæ‘„åƒæœºè¿åŠ¨å¯¹é½æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†CamTrolåœ¨ä¸åŒåŸºç¡€æ¨¡å‹ä¸Šçš„é€šç”¨èƒ½åŠ›ä»¥åŠå…¶åœ¨å¯æ‰©å±•è¿åŠ¨æ§åˆ¶ã€å¤„ç†å¤æ‚è½¨è¿¹å’Œæ— ç›‘ç£3Dè§†é¢‘ç”Ÿæˆæ–¹é¢çš„ä»¤äººå°è±¡æ·±åˆ»çš„åº”ç”¨ã€‚è§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://lifedecoder.github.io/CamTrol/%E8%A7%88%E8%AF%9D%E3%80%82">https://lifedecoder.github.io/CamTrol/è§‚çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10126v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºç°æˆçš„è§†é¢‘æ‰©æ•£æ¨¡å‹æä¾›æ‘„åƒå¤´æ§åˆ¶åŠŸèƒ½ã€‚ä¸åŒäºä»¥å¾€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€å¯¹æ‘„åƒå¤´æ³¨é‡Šæ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒæˆ–é€šè¿‡æ•°æ®å¢å¼ºè¿›è¡Œè‡ªç›‘ç£è®­ç»ƒã€‚ç›¸åï¼Œå®ƒå¯ä»¥ä¸å¤§å¤šæ•°é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå³æ’å³ç”¨ï¼Œå¹¶ä½¿ç”¨å•å¼ å›¾åƒæˆ–æ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥æ¥ç”Ÿæˆå¯æ§æ‘„åƒå¤´è§†é¢‘ã€‚é€šè¿‡å¸ƒå±€å…ˆéªŒï¼Œæˆ‘ä»¬æå‡ºCamTrolï¼Œå®ç°å¯¹è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç¨³å¥æ‘„åƒå¤´æ§åˆ¶ã€‚å®ƒé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹å®ç°ï¼šé¦–å…ˆï¼Œåœ¨3Dç‚¹äº‘ç©ºé—´ä¸­é€šè¿‡æ˜ç¡®çš„æ‘„åƒå¤´ç§»åŠ¨æ¥å»ºæ¨¡å›¾åƒå¸ƒå±€è°ƒæ•´ï¼›å…¶æ¬¡ï¼Œåˆ©ç”¨ä¸€ç³»åˆ—é‡æ–°æ’åˆ—çš„å›¾åƒä¸­çš„å¸ƒå±€å…ˆéªŒï¼Œé€šè¿‡æ‘„åƒå¤´è¿åŠ¨ç”Ÿæˆè§†é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCamTrolåœ¨è§†é¢‘ç”Ÿæˆå’Œæ‘„åƒå¤´è¿åŠ¨å¯¹é½æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶èƒ½å¹¿æ³›åº”ç”¨äºå„ç§åŸºç¡€æ¨¡å‹ã€å¤æ‚çš„è½¨è¿¹ä»¥åŠæ— ç›‘ç£çš„3Dè§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ‘„åƒå¤´æ§åˆ¶è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ–¹æ³•å³æ’å³ç”¨ï¼Œå…¼å®¹å¤§å¤šæ•°é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨å¸ƒå±€å…ˆéªŒå®ç°æ‘„åƒå¤´æ§åˆ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹ç”Ÿæˆå¯æ§æ‘„åƒå¤´è§†é¢‘ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µå»ºæ¨¡å›¾åƒå¸ƒå±€åœ¨3Dç‚¹äº‘ç©ºé—´ä¸­çš„è°ƒæ•´ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¸ƒå±€å…ˆéªŒç”Ÿæˆå…·æœ‰ç‰¹å®šæ‘„åƒå¤´è¿åŠ¨çš„è§†é¢‘ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºCamTrolåœ¨è§†é¢‘ç”Ÿæˆå’Œæ‘„åƒå¤´è¿åŠ¨å¯¹é½ä¸Šæ€§èƒ½å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2406.10126v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2406.10126v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2406.10126v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2406.10126v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="You-Only-Sample-Once-Taming-One-Step-Text-to-Image-Synthesis-by-Self-Cooperative-Diffusion-GANs"><a href="#You-Only-Sample-Once-Taming-One-Step-Text-to-Image-Synthesis-by-Self-Cooperative-Diffusion-GANs" class="headerlink" title="You Only Sample Once: Taming One-Step Text-to-Image Synthesis by   Self-Cooperative Diffusion GANs"></a>You Only Sample Once: Taming One-Step Text-to-Image Synthesis by   Self-Cooperative Diffusion GANs</h2><p><strong>Authors:Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, Jing Tang</strong></p>
<p>Recently, some works have tried to combine diffusion and Generative Adversarial Networks (GANs) to alleviate the computational cost of the iterative denoising inference in Diffusion Models (DMs). However, existing works in this line suffer from either training instability and mode collapse or subpar one-step generation learning efficiency. To address these issues, we introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. Specifically, we smooth the adversarial divergence by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we extend our YOSO to one-step text-to-image generation based on pre-trained models by several effective training techniques (i.e., latent perceptual loss and latent discriminator for efficient training along with the latent DMs; the informative prior initialization (IPI), and the quick adaption stage for fixing the flawed noise scheduler). Experimental results show that YOSO achieves the state-of-the-art one-step generation performance even with Low-Rank Adaptation (LoRA) fine-tuning. In particular, we show that the YOSO-PixArt-$\alpha$ can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without extra explicit training, requiring only ~10 A800 days for fine-tuning. Our code is provided at <a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/YOSO">https://github.com/Luo-Yihong/YOSO</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä¸€äº›ç ”ç©¶å°è¯•å°†æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼ˆDMsï¼‰ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç›¸ç»“åˆï¼Œä»¥å‡è½»è¿­ä»£å»å™ªæ¨æ–­çš„è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œåœ¨è¿™ä¸€æ–¹å‘ä¸Šå­˜åœ¨è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒæˆ–å•æ­¥ç”Ÿæˆå­¦ä¹ æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†YOSOï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå¿«é€Ÿã€å¯æ‰©å±•ã€é«˜ä¿çœŸä¸€æ­¥å›¾åƒåˆæˆè€Œè®¾è®¡çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰é«˜åº¦çš„è®­ç»ƒç¨³å®šæ€§å’Œæ¨¡å¼è¦†ç›–æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å»å™ªç”Ÿæˆå™¨æœ¬èº«å¹³æ»‘å¯¹æŠ—å‘æ•£ï¼Œè¿›è¡Œè‡ªåä½œå­¦ä¹ ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½œä¸ºä»å¤´å¼€å§‹è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å‡ ç§æœ‰æ•ˆçš„è®­ç»ƒæŠ€æœ¯å°†æˆ‘ä»¬çš„YOSOæ‰©å±•åˆ°åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆä¾‹å¦‚ï¼Œæ½œåœ¨æ„ŸçŸ¥æŸå¤±å’Œæ½œåœ¨é‰´åˆ«å™¨ç”¨äºæœ‰æ•ˆè®­ç»ƒä»¥åŠæ½œåœ¨DMsï¼›ä¿¡æ¯å…ˆéªŒåˆå§‹åŒ–ï¼ˆIPIï¼‰å’Œå¿«é€Ÿé€‚åº”é˜¶æ®µç”¨äºä¿®å¤æœ‰ç¼ºé™·çš„å™ªå£°è°ƒåº¦å™¨ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒYOSOä¹Ÿå®ç°äº†æœ€å…ˆè¿›çš„å•æ­¥ç”Ÿæˆæ€§èƒ½ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†YOSO-PixArt-Î±åœ¨512åˆ†è¾¨ç‡ä¸Šç»è¿‡ä¸€æ­¥è®­ç»ƒçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶èƒ½åœ¨æ— éœ€é¢å¤–æ˜ç¡®è®­ç»ƒçš„æƒ…å†µä¸‹é€‚åº”1024åˆ†è¾¨ç‡ï¼Œå¾®è°ƒä»…éœ€çº¦10ä¸ªA800å¤©ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Luo-Yihong/YOSO%E3%80%82">https://github.com/Luo-Yihong/YOSOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12931v6">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæœ‰ç ”ç©¶å·¥ä½œå°è¯•å°†æ‰©æ•£ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç»“åˆï¼Œä»¥é™ä½æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¿­ä»£å»å™ªæ¨æ–­çš„è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒæˆ–å•æ¬¡ç”Ÿæˆå­¦ä¹ æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºYOSOï¼Œè¿™æ˜¯ä¸€ç§ä¸ºå¿«é€Ÿã€å¯ä¼¸ç¼©ã€é«˜ä¿çœŸä¸€æ­¥å›¾åƒåˆæˆè®¾è®¡çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰é«˜åº¦çš„è®­ç»ƒç¨³å®šæ€§å’Œæ¨¡å¼è¦†ç›–ã€‚é€šè¿‡å»å™ªç”Ÿæˆå™¨æœ¬èº«å¹³æ»‘å¯¹æŠ—å‘æ•£ï¼Œå®ç°è‡ªæˆ‘åä½œå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½œä¸ºä»å¤´å¼€å§‹è®­ç»ƒçš„ä¸€æ­¥ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰ç«äº‰æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‡ ç§æœ‰æ•ˆçš„è®­ç»ƒæŠ€æœ¯ï¼Œæˆ‘ä»¬å°†YOSOæ‰©å±•åˆ°åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒYOSOä¹Ÿå®ç°äº†æœ€æ–°çš„ä¸€æ­¥ç”Ÿæˆæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†YOSO-PixArt-Î±åœ¨512åˆ†è¾¨ç‡ä¸Šçš„ä¸€æ­¥è®­ç»ƒç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ï¼Œå¹¶é€‚åº”åˆ°1024åˆ†è¾¨ç‡è€Œæ— éœ€é¢å¤–çš„æ˜ç¡®è®­ç»ƒï¼Œå¾®è°ƒåªéœ€çº¦10ä¸ªA800å¤©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ç»“åˆæ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒåŠå•æ¬¡ç”Ÿæˆæ•ˆç‡é—®é¢˜ã€‚</li>
<li>YOSOæ¨¡å‹è¢«è®¾è®¡ç”¨äºå¿«é€Ÿã€é«˜ä¿çœŸçš„ä¸€æ­¥å›¾åƒåˆæˆï¼Œå…·å¤‡é«˜è®­ç»ƒç¨³å®šæ€§å’Œæ¨¡å¼è¦†ç›–ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘åä½œå­¦ä¹ å’Œæœ‰æ•ˆçš„è®­ç»ƒæŠ€æœ¯ï¼ŒYOSOæ¨¡å‹æé«˜äº†æ€§èƒ½ã€‚</li>
<li>YOSOæ¨¡å‹å¯ä»¥åœ¨ä¸åŒçš„åˆ†è¾¨ç‡ä¸‹é€‚åº”ï¼Œå¹¶å±•ç¤ºäº†åœ¨512åˆ†è¾¨ç‡ä¸Šçš„ä¸€æ­¥ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>YOSOæ¨¡å‹é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒå®ç°äº†å…ˆè¿›çš„ä¸€æ­¥ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹çš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2403.12931v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2403.12931v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2403.12931v6/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Continuous-Diffusion-for-Mixed-Type-Tabular-Data"><a href="#Continuous-Diffusion-for-Mixed-Type-Tabular-Data" class="headerlink" title="Continuous Diffusion for Mixed-Type Tabular Data"></a>Continuous Diffusion for Mixed-Type Tabular Data</h2><p><strong>Authors:Markus Mueller, Kathrin Gruber, Dennis Fok</strong></p>
<p>Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes.To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at <a target="_blank" rel="noopener" href="https://github.com/muellermarkus/cdtd">https://github.com/muellermarkus/cdtd</a>. </p>
<blockquote>
<p>åŸºäºåˆ†æ•°ç”Ÿæˆæ¨¡å‹ï¼Œé€šå¸¸è¢«ç§°ä¸ºæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒæ•°æ®æ–¹é¢å·²ç»è¯æ˜æ˜¯æˆåŠŸçš„ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„åº”ç”¨ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CDTDï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„è¿ç»­æ‰©æ•£æ¨¡å‹ã€‚CDTDåŸºäºåˆ†æ•°åŒ¹é…å’Œåˆ†æ•°æ’å€¼çš„ç»„åˆï¼Œä»¥å¼ºåˆ¶è¿ç»­ç±»åˆ«ç‰¹å¾çš„ç»Ÿä¸€è¿ç»­å™ªå£°åˆ†å¸ƒã€‚æˆ‘ä»¬æ˜ç¡®æ‰¿è®¤é€šè¿‡ä¾èµ–æ¨¡å‹ç‰¹å®šçš„æŸå¤±æ ¡å‡†å’Œåˆå§‹åŒ–æ–¹æ¡ˆæ¥ç»Ÿä¸€ä¸åŒç±»å‹æ•°æ®çš„å¿…è¦æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„é«˜å¼‚è´¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”ç‰¹å¾æˆ–ç±»å‹ç‰¹å®šçš„å™ªå£°æ—¶é—´è¡¨ã€‚è¿™äº›ç¡®ä¿äº†è·¨ç‰¹å¾çš„ç”Ÿæˆæ€§èƒ½å¹³è¡¡ï¼Œå¹¶ä¼˜åŒ–äº†æ¨¡å‹å®¹é‡åœ¨ç‰¹å¾å’Œæ‰©æ•£æ—¶é—´ä¸Šçš„åˆ†é…ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTDå§‹ç»ˆä¼˜äºæœ€æ–°çš„åŸºå‡†æ¨¡å‹ï¼Œèƒ½å¤Ÿå‡ºè‰²åœ°æ•è·ç‰¹å¾ç›¸å…³æ€§ï¼Œå¹¶ä¸”å™ªå£°æ—¶é—´å®‰æ’ä¸­çš„å¼‚è´¨æ€§æé«˜äº†æ ·æœ¬è´¨é‡ã€‚å¤åˆ¶ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/muellermarkus/cdtd%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/muellermarkus/cdtdæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10431v4">PDF</a> published in ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨æ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„åº”ç”¨ä¸Šä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºCDTDï¼Œä¸€ç§é€‚ç”¨äºæ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„è¿ç»­æ‰©æ•£æ¨¡å‹ã€‚CDTDé€šè¿‡åˆ†æ•°åŒ¹é…å’Œåˆ†æ•°æ’å€¼çš„æ–°å‹ç»“åˆï¼Œä¸ºè¿ç»­å’Œåˆ†ç±»ç‰¹å¾å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€çš„è¿ç»­å™ªå£°åˆ†å¸ƒã€‚é€šè¿‡ä¾èµ–æ¨¡å‹ç‰¹å®šæŸå¤±æ ¡å‡†å’Œåˆå§‹åŒ–æ–¹æ¡ˆï¼Œæˆ‘ä»¬æ˜ç¡®è®¤è¯†åˆ°ç»Ÿä¸€ä¸åŒæ•°æ®ç±»å‹çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„é«˜å¼‚è´¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”ç‰¹å¾æˆ–ç±»å‹ç‰¹å®šçš„å™ªå£°æ—¶é—´è¡¨ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCDTDæŒç»­è¶…è¶Šç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œç‰¹å¾ç›¸å…³æ€§æ•æ‰æä½³ï¼Œä¸”å™ªå£°æ—¶é—´è¡¨è®¾è®¡çš„å¼‚è´¨æ€§æå‡äº†æ ·æœ¬è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ··åˆç±»å‹è¡¨æ ¼æ•°æ®æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>CDTDæ˜¯ä¸€ç§é’ˆå¯¹æ··åˆç±»å‹è¡¨æ ¼æ•°æ®çš„è¿ç»­æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼ºåˆ¶æ‰§è¡Œç»Ÿä¸€çš„å™ªå£°åˆ†å¸ƒæ¥å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®ã€‚</li>
<li>CDTDé‡‡ç”¨æ–°å‹ç»“åˆæ–¹å¼â€”â€”åˆ†æ•°åŒ¹é…å’Œåˆ†æ•°æ’å€¼ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä¾èµ–ç‰¹å®šçš„æŸå¤±æ ¡å‡†å’Œåˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥ç»Ÿä¸€ä¸åŒæ•°æ®ç±»å‹ã€‚</li>
<li>ä¸ºåº”å¯¹æ··åˆç±»å‹è¡¨æ ¼æ•°æ®ä¸­çš„é«˜å¼‚è´¨æ€§ï¼ŒCDTDå¼•å…¥äº†è‡ªé€‚åº”ç‰¹å¾æˆ–ç±»å‹ç‰¹å®šçš„å™ªå£°æ—¶é—´è¡¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCDTDåœ¨ç”Ÿæˆæ··åˆç±»å‹è¡¨æ ¼æ•°æ®æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.10431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2312.10431v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2312.10431v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2312.10431v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Language-Guided-Diffusion-Model-for-Visual-Grounding"><a href="#Language-Guided-Diffusion-Model-for-Visual-Grounding" class="headerlink" title="Language-Guided Diffusion Model for Visual Grounding"></a>Language-Guided Diffusion Model for Visual Grounding</h2><p><strong>Authors:Sijia Chen, Baochun Li</strong></p>
<p>Visual grounding (VG) tasks involve explicit cross-modal alignment, as semantically corresponding image regions are to be located for the language phrases provided. Existing approaches complete such visual-text reasoning in a single-step manner. Their performance causes high demands on large-scale anchors and over-designed multi-modal fusion modules based on human priors, leading to complicated frameworks that may be difficult to train and overfit to specific scenarios. Even worse, such once-for-all reasoning mechanisms are incapable of refining boxes continuously to enhance query-region matching. In contrast, in this paper, we formulate an iterative reasoning process by denoising diffusion modeling. Specifically, we propose a language-guided diffusion framework for visual grounding, LG-DVG, which trains the model to progressively reason queried object boxes by denoising a set of noisy boxes with the language guide. To achieve this, LG-DVG gradually perturbs query-aligned ground truth boxes to noisy ones and reverses this process step by step, conditional on query semantics. Extensive experiments for our proposed framework on five widely used datasets validate the superior performance of solving visual grounding, a cross-modal alignment task, in a generative way. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/iQua/vgbase/tree/main/examples/DiffusionVG">https://github.com/iQua/vgbase/tree/main/examples/DiffusionVG</a>. </p>
<blockquote>
<p>è§†è§‰å®šä½ï¼ˆVGï¼‰ä»»åŠ¡æ¶‰åŠæ˜ç¡®çš„è·¨æ¨¡æ€å¯¹é½ï¼Œéœ€è¦ä¸ºæä¾›çš„è¯­è¨€çŸ­è¯­å®šä½è¯­ä¹‰å¯¹åº”çš„å›¾åƒåŒºåŸŸã€‚ç°æœ‰æ–¹æ³•ä»¥å•æ­¥æ–¹å¼å®Œæˆè¿™ç§è§†è§‰æ–‡æœ¬æ¨ç†ã€‚å®ƒä»¬çš„æ€§èƒ½å¯¹å¤§è§„æ¨¡é”šç‚¹å’ŒåŸºäºäººç±»å…ˆéªŒçš„è¿‡åº¦è®¾è®¡çš„å¤šæ¨¡æ€èåˆæ¨¡å—æå‡ºäº†é«˜è¦æ±‚ï¼Œå¯¼è‡´æ¡†æ¶å¤æ‚ï¼Œå¯èƒ½éš¾ä»¥è®­ç»ƒå’Œè¿‡åº¦é€‚åº”ç‰¹å®šåœºæ™¯ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè¿™ç§ä¸€æ¬¡æ€§çš„æ¨ç†æœºåˆ¶æ— æ³•è¿ç»­è°ƒæ•´æ¡†ä»¥æ”¹è¿›æŸ¥è¯¢åŒºåŸŸåŒ¹é…ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ¬æ–‡é‡‡ç”¨é™å™ªæ‰©æ•£å»ºæ¨¡æ¥åˆ¶å®šè¿­ä»£æ¨ç†è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè§†è§‰å®šä½çš„è¯­è¨€å¼•å¯¼æ‰©æ•£æ¡†æ¶LG-DVGï¼Œè¯¥æ¡†æ¶è®­ç»ƒæ¨¡å‹é€šè¿‡è¯­è¨€æŒ‡å—é€æ­¥æ¨ç†æŸ¥è¯¢å¯¹è±¡æ¡†ï¼Œé€šè¿‡å¯¹ä¸€ç»„å™ªå£°æ¡†è¿›è¡Œé™å™ªã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒLG-DVGå°†æŸ¥è¯¢å¯¹é½çš„åœ°é¢çœŸå®æ¡†é€æ¸æ‰°åŠ¨ä¸ºå™ªå£°æ¡†ï¼Œå¹¶æ ¹æ®æŸ¥è¯¢è¯­ä¹‰é€æ­¥åè½¬è¿™ä¸€è¿‡ç¨‹ã€‚åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬æå‡ºçš„æ¡†æ¶è¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†ä»¥ç”Ÿæˆæ–¹å¼è§£å†³è·¨æ¨¡æ€å¯¹é½ä»»åŠ¡â€”â€”è§†è§‰å®šä½ä»»åŠ¡çš„ä¼˜è¶Šæ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/iQua/vgbase/tree/main/examples/DiffusionVG%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/iQua/vgbase/tree/main/examples/DiffusionVGå¤„è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09599v3">PDF</a> 20 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå»å™ªæ‰©æ•£å»ºæ¨¡çš„è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼Œç”¨äºè§†è§‰å®šä½ä»»åŠ¡ã€‚é€šè¿‡è¯­è¨€å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼ˆLG-DVGï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿé€æ­¥æ¨ç†æŸ¥è¯¢å¯¹è±¡æ¡†ï¼Œé€šè¿‡å»å™ªä¸€ç»„å™ªå£°æ¡†å’Œè¯­è¨€å¼•å¯¼æ¥å®ç°ã€‚è¯¥æ–¹æ³•åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶åœ¨è§£å†³è·¨æ¨¡æ€å¯¹é½ä»»åŠ¡â€”â€”è§†è§‰å®šä½ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å®šä½ï¼ˆVGï¼‰ä»»åŠ¡æ¶‰åŠæ˜ç¡®çš„è·¨æ¨¡æ€å¯¹é½ï¼Œéœ€è¦ä¸ºæä¾›çš„è¯­è¨€çŸ­è¯­å®šä½è¯­ä¹‰ä¸Šå¯¹åº”çš„å›¾åƒåŒºåŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨ä¸€æ¬¡å®Œæˆçš„æ–¹å¼å¤„ç†è§†è§‰æ–‡æœ¬æ¨ç†ï¼Œå¯¹å¤§è§„æ¨¡é”šç‚¹å’Œè¿‡åº¦è®¾è®¡çš„å¤šæ¨¡æ€èåˆæ¨¡å—æœ‰è¾ƒé«˜è¦æ±‚ï¼Œå¯¼è‡´æ¡†æ¶å¤æ‚ï¼Œå¯èƒ½éš¾ä»¥è®­ç»ƒå’Œé€‚åº”ç‰¹å®šåœºæ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•è¿ç»­ä¼˜åŒ–æ¡†ä»¥æ”¹è¿›æŸ¥è¯¢åŒºåŸŸåŒ¹é…ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºå»å™ªæ‰©æ•£å»ºæ¨¡çš„è¿­ä»£æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼ˆLG-DVGï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LG-DVGé€šè¿‡é€æ­¥å°†æŸ¥è¯¢å¯¹é½çš„ ground truth æ¡†æ‰°åŠ¨ä¸ºå™ªå£°æ¡†ï¼Œå¹¶åŸºäºæŸ¥è¯¢è¯­ä¹‰é€æ­¥åè½¬è¿™ä¸€è¿‡ç¨‹ï¼Œå®ç°é€æ­¥æ¨ç†ã€‚</li>
<li>åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†LG-DVGåœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2308.09599v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2308.09599v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2308.09599v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_Diffusion Models/2308.09599v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-27/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_åŒ»å­¦å›¾åƒ/2406.07146v3/page_2_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-27  Training Consistency Models with Variational Noise Coupling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-27/NeRF/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-27\./crop_NeRF/2403.12931v6/page_4_0.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-27  HEROS-GAN Honed-Energy Regularized and Optimal Supervised GAN for   Enhancing Accuracy and Range of Low-Cost Accelerometers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
