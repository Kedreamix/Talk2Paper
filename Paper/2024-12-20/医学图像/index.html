<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-20  CAD-Recode Reverse Engineering CAD Code from Point Clouds">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e80b19e2c7dc8e34c906e815957f9ebe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-20-æ›´æ–°"><a href="#2024-12-20-æ›´æ–°" class="headerlink" title="2024-12-20 æ›´æ–°"></a>2024-12-20 æ›´æ–°</h1><h2 id="CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds"><a href="#CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds" class="headerlink" title="CAD-Recode: Reverse Engineering CAD Code from Point Clouds"></a>CAD-Recode: Reverse Engineering CAD Code from Point Clouds</h2><p><strong>Authors:Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</strong></p>
<p>Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained solely on a proposed synthetic dataset of one million diverse CAD sequences. CAD-Recode significantly outperforms existing methods across three datasets while requiring fewer input points. Notably, it achieves 10 times lower mean Chamfer distance than state-of-the-art methods on DeepCAD and Fusion360 datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡ä¾æ¬¡ç»˜åˆ¶å‚æ•°è‰å›¾å¹¶åº”ç”¨CADæ“ä½œæ¥è·å¾—çš„ä¸‰ç»´æ¨¡å‹ã€‚3D CADé€†å‘å·¥ç¨‹çš„é—®é¢˜åœ¨äºä»ç‚¹äº‘ç­‰3Dè¡¨ç¤ºé‡å»ºè‰å›¾å’ŒCADæ“ä½œåºåˆ—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªå±‚æ¬¡çš„å…¨æ–°è´¡çŒ®æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼šCADåºåˆ—è¡¨ç¤ºã€ç½‘ç»œè®¾è®¡å’Œæ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†CADè‰å›¾æŒ¤å‹åºåˆ—è¡¨ç¤ºä¸ºPythonä»£ç ã€‚æ‰€æå‡ºçš„CAD-Recodeå°†ç‚¹äº‘è½¬æ¢ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œæ—¶å¯é‡å»ºCADæ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹Pythonä»£ç çš„æš´éœ²ï¼Œåˆ©ç”¨ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„LLMä½œä¸ºCAD-Recodeçš„è§£ç å™¨ï¼Œå¹¶ä¸è½»é‡çº§çš„ç‚¹äº‘æŠ•å½±ä»ªç›¸ç»“åˆã€‚CAD-Recodeä»…åœ¨ä¸€ä¸ªåŒ…å«ä¸€ç™¾ä¸‡å¤šä¸ªCADåºåˆ—çš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­ï¼ŒCAD-Recodeæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ‰€éœ€çš„è¾“å…¥ç‚¹æ•°æ›´å°‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šå®ç°äº†æ¯”æœ€æ–°æŠ€æœ¯ä½10å€çš„å¹³å‡Chamferè·ç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„CAD Pythonä»£ç è¾“å‡ºå¯ä»¥é€šè¿‡å¸‚é¢ä¸Šçš„LLMè¿›è¡Œè§£é‡Šï¼Œä»è€Œèƒ½å¤Ÿæœ‰ç‚¹äº‘è¿›è¡ŒCADç¼–è¾‘å’Œç‰¹å®šçš„é—®ç­”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14042v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†è®¡ç®—æœºä¸‰ç»´CADæ¨¡å‹çš„é€†å‘å·¥ç¨‹é—®é¢˜ï¼Œé€šè¿‡ä¸‰ä¸ªå±‚æ¬¡çš„è´¡çŒ®ï¼šCADåºåˆ—è¡¨ç¤ºã€ç½‘ç»œè®¾è®¡å’Œæ•°æ®é›†ã€‚æå‡ºä¸€ç§åä¸ºCAD-Recodeçš„æ–¹æ³•ï¼Œå°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œåå¯é‡å»ºCADæ¨¡å‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„å°‘é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£ç CAD-Recodeå¹¶ä¸è½»é‡çº§ç‚¹äº‘æŠ•å½±ä»ªç»“åˆã€‚ä»…åœ¨æå‡ºçš„ç™¾ä¸‡çº§åˆæˆCADåºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒCAD-Recodeåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”è¾“å…¥ç‚¹æ›´å°‘ã€‚æ­¤å¤–ï¼Œå®ƒçš„è¾“å‡ºæ˜¯æ˜“äºç†è§£çš„CAD Pythonä»£ç ï¼Œä½¿ç‚¹äº‘èƒ½å¤Ÿè¿›è¡ŒCADç¼–è¾‘å’Œé—®ç­”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CAD-Recodeè§£å†³äº†è®¡ç®—æœºä¸‰ç»´CADæ¨¡å‹çš„é€†å‘å·¥ç¨‹é—®é¢˜ã€‚</li>
<li>CAD-Recodeå°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç è¿›è¡Œé‡å»ºCADæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£ç CAD-Recodeã€‚</li>
<li>CAD-Recodeåœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CAD-Recodeè¾“å…¥ç‚¹éœ€æ±‚æ›´å°‘ï¼Œä¸”å®ç°äº†æ›´ä½çš„Chamferè·ç¦»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e25d7eb0cc8854f180ad0774b6b068b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22b5fe8fec241094bc912f8d9be7a1de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25b7780bc0f0fc4b40cf19519278cc37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b82201bed22ed54cd867c9af5861d923.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e03a5ce6205dac33a10bf58a58af4c83.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HC-LLM-Historical-Constrained-Large-Language-Models-for-Radiology-Report-Generation"><a href="#HC-LLM-Historical-Constrained-Large-Language-Models-for-Radiology-Report-Generation" class="headerlink" title="HC-LLM: Historical-Constrained Large Language Models for Radiology   Report Generation"></a>HC-LLM: Historical-Constrained Large Language Models for Radiology   Report Generation</h2><p><strong>Authors:Tengfei Liu, Jiapu Wang, Yongli Hu, Mingjie Li, Junfei Yi, Xiaojun Chang, Junbin Gao, Baocai Yin</strong></p>
<p>Radiology report generation (RRG) models typically focus on individual exams, often overlooking the integration of historical visual or textual data, which is crucial for patient follow-ups. Traditional methods usually struggle with long sequence dependencies when incorporating historical information, but large language models (LLMs) excel at in-context learning, making them well-suited for analyzing longitudinal medical data. In light of this, we propose a novel Historical-Constrained Large Language Models (HC-LLM) framework for RRG, empowering LLMs with longitudinal report generation capabilities by constraining the consistency and differences between longitudinal images and their corresponding reports. Specifically, our approach extracts both time-shared and time-specific features from longitudinal chest X-rays and diagnostic reports to capture disease progression. Then, we ensure consistent representation by applying intra-modality similarity constraints and aligning various features across modalities with multimodal contrastive and structural constraints. These combined constraints effectively guide the LLMs in generating diagnostic reports that accurately reflect the progression of the disease, achieving state-of-the-art results on the Longitudinal-MIMIC dataset. Notably, our approach performs well even without historical data during testing and can be easily adapted to other multimodal large models, enhancing its versatility. </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰æ¨¡å‹é€šå¸¸å…³æ³¨ä¸ªåˆ«æ£€æŸ¥ï¼Œå¾€å¾€å¿½ç•¥äº†å†å²è§†è§‰æˆ–æ–‡æœ¬æ•°æ®çš„æ•´åˆï¼Œè¿™å¯¹äºæ‚£è€…éšè®¿è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨æ•´åˆå†å²ä¿¡æ¯æ—¶é€šå¸¸éš¾ä»¥å¤„ç†é•¿åºåˆ—ä¾èµ–é—®é¢˜ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œéå¸¸é€‚åˆåˆ†æçºµå‘åŒ»ç–—æ•°æ®ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å†å²çº¦æŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆHC-LLMï¼‰æ¡†æ¶ï¼Œç”¨äºRRGï¼Œé€šè¿‡çº¦æŸçºµå‘å›¾åƒä¸å…¶ç›¸åº”æŠ¥å‘Šä¹‹é—´çš„ä¸€è‡´æ€§å’Œå·®å¼‚ï¼Œå¢å¼ºLLMçš„çºµå‘æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»çºµå‘çš„èƒ¸éƒ¨Xå°„çº¿å’Œè¯Šæ–­æŠ¥å‘Šä¸­æå–æ—¶é—´å…±äº«å’Œæ—¶é—´ç‰¹å®šçš„ç‰¹å¾ï¼Œä»¥æ•æ‰ç–¾ç—…çš„è¿›å±•ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨æ¨¡æ€å†…ç›¸ä¼¼æ€§çº¦æŸå¹¶è·¨æ¨¡æ€å¯¹é½å„ç§ç‰¹å¾ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¯¹æ¯”å’Œç»“æ„çº¦æŸæ¥ç¡®ä¿ä¸€è‡´è¡¨ç¤ºã€‚è¿™äº›ç»„åˆçº¦æŸæœ‰æ•ˆåœ°æŒ‡å¯¼äº†LLMç”Ÿæˆèƒ½å‡†ç¡®åæ˜ ç–¾ç—…è¿›å±•çš„è¯Šæ–­æŠ¥å‘Šï¼Œåœ¨Longitudinal-MIMICæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³ä½¿åœ¨æµ‹è¯•æ—¶æ²¡æœ‰å†å²æ•°æ®ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾é€‚åº”å…¶ä»–å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼Œå¢å¼ºäº†å…¶é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11070v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå†å²çº¦æŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆHC-LLMï¼‰æ¡†æ¶ï¼Œç”¨äºæ”¾å°„æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»çºµå‘çš„èƒ¸éƒ¨Xå°„çº¿å’Œè¯Šæ–­æŠ¥å‘Šä¸­æå–æ—¶é—´å…±äº«å’Œæ—¶é—´ç‰¹å®šçš„ç‰¹å¾ï¼Œä»¥æ•æ‰ç–¾ç—…çš„è¿›å±•ï¼Œå¹¶é€šè¿‡å¤šç§çº¦æŸç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚æ­¤æ¡†æ¶åœ¨Longitudinal-MIMICæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RRGæ¨¡å‹é€šå¸¸å…³æ³¨ä¸ªä½“æ£€æŸ¥ï¼Œä½†å¿½ç•¥å†å²è§†è§‰æˆ–æ–‡æœ¬æ•°æ®çš„æ•´åˆï¼Œè¿™å¯¹æ‚£è€…éšè®¿è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨æ•´åˆå†å²ä¿¡æ¯æ—¶é¢ä¸´é•¿æœŸåºåˆ—ä¾èµ–é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œé€‚åˆåˆ†æçºµå‘åŒ»ç–—æ•°æ®ã€‚</li>
<li>HC-LLMæ¡†æ¶ç»“åˆäº†çºµå‘æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡çº¦æŸçºµå‘å›¾åƒä¸ç›¸åº”æŠ¥å‘Šä¹‹é—´çš„ä¸€è‡´æ€§å’Œå·®å¼‚æ¥å®ç°ã€‚</li>
<li>HC-LLMä»çºµå‘èƒ¸éƒ¨Xå°„çº¿å’Œè¯Šæ–­æŠ¥å‘Šä¸­æå–æ—¶é—´å…±äº«å’Œæ—¶é—´ç‰¹å®šçš„ç‰¹å¾ï¼Œä»¥æ•æ‰ç–¾ç—…è¿›å±•ã€‚</li>
<li>é€šè¿‡åº”ç”¨å†…éƒ¨æ¨¡æ€ç›¸ä¼¼æ€§çº¦æŸå’Œè·¨æ¨¡æ€å¤šæ¨¡æ€å¯¹æ¯”å’Œç»“æ„çº¦æŸï¼Œç¡®ä¿ä¸€è‡´æ€§çš„è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76945c661d4768fe52147d84157cda90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-941213c37206f2bb35d8a457cc83c0ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-195c6aaeb95f53bd3d97b31c78f0ec36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6e074f2606ffa82837ef17e9d821f7f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mask-Enhanced-Deeply-Supervised-Prostate-Cancer-Detection-on-B-mode-Micro-Ultrasound"><a href="#Mask-Enhanced-Deeply-Supervised-Prostate-Cancer-Detection-on-B-mode-Micro-Ultrasound" class="headerlink" title="Mask Enhanced Deeply Supervised Prostate Cancer Detection on B-mode   Micro-Ultrasound"></a>Mask Enhanced Deeply Supervised Prostate Cancer Detection on B-mode   Micro-Ultrasound</h2><p><strong>Authors:Lichun Zhang, Steve Ran Zhou, Moon Hyung Choi, Jeong Hoon Lee, Shengtian Sang, Adam Kinnaird, Wayne G. Brisbane, Giovanni Lughezzani, Davide Maffei, Vittorio Fasulo, Patrick Albers, Sulaiman Vesal, Wei Shao, Ahmed N. El Kaffas, Richard E. Fan, Geoffrey A. Sonn, Mirabela Rusu</strong></p>
<p>Prostate cancer is a leading cause of cancer-related deaths among men. The recent development of high frequency, micro-ultrasound imaging offers improved resolution compared to conventional ultrasound and potentially a better ability to differentiate clinically significant cancer from normal tissue. However, the features of prostate cancer remain subtle, with ambiguous borders with normal tissue and large variations in appearance, making it challenging for both machine learning and humans to localize it on micro-ultrasound images.   We propose a novel Mask Enhanced Deeply-supervised Micro-US network, termed MedMusNet, to automatically and more accurately segment prostate cancer to be used as potential targets for biopsy procedures. MedMusNet leverages predicted masks of prostate cancer to enforce the learned features layer-wisely within the network, reducing the influence of noise and improving overall consistency across frames.   MedMusNet successfully detected 76% of clinically significant cancer with a Dice Similarity Coefficient of 0.365, significantly outperforming the baseline Swin-M2F in specificity and accuracy (Wilcoxon test, Bonferroni correction, p-value&lt;0.05). While the lesion-level and patient-level analyses showed improved performance compared to human experts and different baseline, the improvements did not reach statistical significance, likely on account of the small cohort.   We have presented a novel approach to automatically detect and segment clinically significant prostate cancer on B-mode micro-ultrasound images. Our MedMusNet model outperformed other models, surpassing even human experts. These preliminary results suggest the potential for aiding urologists in prostate cancer diagnosis via biopsy and treatment decision-making. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œæ˜¯ç”·æ€§ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚æœ€è¿‘å¼€å‘çš„é«˜é¢‘å¾®è¶…å£°æˆåƒä¸å¸¸è§„è¶…å£°ç›¸æ¯”å…·æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡ï¼Œå¹¶å¯èƒ½å…·æœ‰æ›´å¥½çš„èƒ½åŠ›æ¥åŒºåˆ†ä¸´åºŠé‡è¦çš„ç™Œç—‡å’Œæ­£å¸¸ç»„ç»‡ã€‚ç„¶è€Œï¼Œå‰åˆ—è…ºç™Œçš„ç‰¹å¾ä»ç„¶å¾ˆå¾®å¦™ï¼Œä¸æ­£å¸¸ç»„ç»‡çš„è¾¹ç•Œæ¨¡ç³Šï¼Œå¤–è§‚å˜åŒ–å¾ˆå¤§ï¼Œæ— è®ºæ˜¯æœºå™¨å­¦ä¹ è¿˜æ˜¯äººç±»éƒ½å¾ˆéš¾åœ¨å¾®è¶…å£°å›¾åƒä¸Šå®šä½å®ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ©è†œå¢å¼ºæ·±åº¦ç›‘ç£å¾®è¶…å£°ç½‘ç»œï¼Œç§°ä¸ºMedMusNetï¼Œå¯ä»¥è‡ªåŠ¨æ›´å‡†ç¡®åœ°åˆ†å‰²å‰åˆ—è…ºç™Œï¼Œç”¨ä½œæ´»æ£€ç¨‹åºçš„æ½œåœ¨ç›®æ ‡ã€‚MedMusNetåˆ©ç”¨é¢„æµ‹çš„å‰åˆ—è…ºç™Œæ©è†œåœ¨ç½‘ç»œå†…é€å±‚å¼ºåˆ¶å­¦ä¹ ç‰¹å¾ï¼Œå‡å°‘å™ªå£°çš„å½±å“ï¼Œæé«˜å¸§é—´æ•´ä½“ä¸€è‡´æ€§ã€‚MedMusNetæˆåŠŸæ£€æµ‹åˆ°äº†76%çš„ä¸´åºŠé‡è¦ç™Œç—‡ï¼ŒDiceç›¸ä¼¼ç³»æ•°ä¸º0.365ï¼Œåœ¨ç‰¹å¼‚æ€§å’Œå‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿Swin-M2Fï¼ˆWilcoxonæ£€éªŒï¼ŒBonferroniæ ¡æ­£ï¼Œpå€¼&lt;0.05ï¼‰ã€‚è™½ç„¶ç—…ç¶æ°´å¹³å’Œæ‚£è€…æ°´å¹³åˆ†ææ˜¾ç¤ºä¸äººç±»ä¸“å®¶å’Œä¸åŒåŸºçº¿ç›¸æ¯”æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œä½†æé«˜å¹¶æœªè¾¾åˆ°ç»Ÿè®¡å­¦æ„ä¹‰ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºé˜Ÿåˆ—äººæ•°è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨Bæ¨¡å¼å¾®è¶…å£°å›¾åƒä¸Šè‡ªåŠ¨æ£€æµ‹å’Œåˆ†å‰²ä¸´åºŠé‡è¦å‰åˆ—è…ºç™Œçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„MedMusNetæ¨¡å‹ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†äººç±»ä¸“å®¶ã€‚è¿™äº›åˆæ­¥ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æœ‰æ½œåŠ›è¾…åŠ©æ³Œå°¿ç§‘åŒ»ç”Ÿè¿›è¡Œå‰åˆ—è…ºç™Œè¯Šæ–­ã€æ´»æ£€å’Œæ²»ç–—å†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMedMusNetçš„æ–°é¢–çš„å‰åˆ—è…ºç™Œè‡ªåŠ¨æ£€æµ‹ä¸åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨Bæ¨¡å¼å¾®è¶…å£°å›¾åƒä¸Šè¡¨ç°ä¼˜å¼‚ã€‚é€šè¿‡åˆ©ç”¨é¢„æµ‹çš„ç™Œç—‡æ©è†œï¼Œåœ¨ç½‘ç»œå†…é€å±‚å¼ºåŒ–å­¦ä¹ ç‰¹å¾ï¼Œå‡å°‘å™ªå£°å½±å“ï¼Œæé«˜å¸§é—´ä¸€è‡´æ€§ã€‚ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹Swin-M2Fï¼ŒMedMusNetåœ¨ç‰¹å¼‚æ€§åŠå‡†ç¡®åº¦ä¸Šæœ‰æ‰€æå‡ï¼Œå°¤å…¶æ˜¯åœ¨ä¸´åºŠæ˜¾è‘—ç™Œç—‡æ£€æµ‹æ–¹é¢è¡¨ç°çªå‡ºã€‚è™½ç„¶ç›¸å¯¹äºäººç±»ä¸“å®¶çš„æ€§èƒ½æå‡å°šæœªè¾¾åˆ°ç»Ÿè®¡å­¦æ„ä¹‰ï¼Œä½†åˆæ­¥ç»“æœä»æ˜¾ç¤ºå‡ºå…¶åœ¨è¾…åŠ©æ³Œå°¿ç§‘åŒ»ç”Ÿè¿›è¡Œå‰åˆ—è…ºç™Œè¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜é¢‘å¾®è¶…å£°æˆåƒæŠ€æœ¯ç›¸è¾ƒäºä¼ ç»Ÿè¶…å£°å…·æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡ï¼Œèƒ½æ›´å¥½åœ°åŒºåˆ†ä¸´åºŠæ˜¾è‘—ç™Œç—‡ä¸æ­£å¸¸ç»„ç»‡ã€‚</li>
<li>å‰åˆ—è…ºç™Œçš„ç‰¹å¾å¾®å¦™ï¼Œä¸æ­£å¸¸ç»„ç»‡è¾¹ç•Œæ¨¡ç³Šä¸”å¤–è§‚å˜åŒ–å¤§ï¼Œä½¿å¾—æœºå™¨å­¦ä¹ å’Œäººç±»éƒ½éš¾ä»¥å®šä½ã€‚</li>
<li>MedMusNetæ¨¡å‹åˆ©ç”¨é¢„æµ‹çš„ç™Œç—‡æ©è†œï¼Œå¼ºåŒ–å­¦ä¹ ç‰¹å¾å¹¶å‡å°‘å™ªå£°å½±å“ï¼Œæé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>MedMusNetæˆåŠŸæ£€æµ‹å‡º76%çš„ä¸´åºŠæ˜¾è‘—ç™Œç—‡ï¼ŒDiceç›¸ä¼¼ç³»æ•°ä¸º0.365ã€‚ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹Swin-M2Fï¼Œå…¶åœ¨ç‰¹å¼‚æ€§å’Œå‡†ç¡®åº¦ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>è™½ç„¶ç›¸å¯¹äºäººç±»ä¸“å®¶çš„æ€§èƒ½æå‡æœªè¾¾ç»Ÿè®¡å­¦æ„ä¹‰ï¼Œä½†MedMusNetçš„åˆæ­¥ç»“æœä»æ˜¾ç¤ºå‡ºå…¶åœ¨è¾…åŠ©å‰åˆ—è…ºç™Œè¯Šæ–­æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>MedMusNetæ¨¡å‹åœ¨Bæ¨¡å¼å¾®è¶…å£°å›¾åƒä¸Šçš„è‡ªåŠ¨æ£€æµ‹ä¸åˆ†å‰²æŠ€æœ¯æœ‰åŠ©äºåŒ»ç”Ÿè¿›è¡Œå‰åˆ—è…ºç™Œçš„è¯Šæ–­ã€æ´»æ£€å’Œæ²»ç–—å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff51196382b9a3e124464d027ae146e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09d5d5cd5d1364be9d833dfc368ffa5a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Biological-and-Radiological-Dictionary-of-Radiomics-Features-Addressing-Understandable-AI-Issues-in-Personalized-Prostate-Cancer-Dictionary-Version-PM1-0"><a href="#Biological-and-Radiological-Dictionary-of-Radiomics-Features-Addressing-Understandable-AI-Issues-in-Personalized-Prostate-Cancer-Dictionary-Version-PM1-0" class="headerlink" title="Biological and Radiological Dictionary of Radiomics Features: Addressing   Understandable AI Issues in Personalized Prostate Cancer; Dictionary Version   PM1.0"></a>Biological and Radiological Dictionary of Radiomics Features: Addressing   Understandable AI Issues in Personalized Prostate Cancer; Dictionary Version   PM1.0</h2><p><strong>Authors:Mohammad R. Salmanpour, Sajad Amiri, Sara Gharibi, Ahmad Shariftabrizi, Yixi Xu, William B Weeks, Arman Rahmim, Ilker Hacihaliloglu</strong></p>
<p>We investigate the connection between visual semantic features defined in PI-RADS and associated risk factors, moving beyond abnormal imaging findings, establishing a shared framework between medical and AI professionals by creating a standardized dictionary of biological&#x2F;radiological RFs. Subsequently, 6 interpretable and seven complex classifiers, linked with nine interpretable feature selection algorithms (FSA) applied to risk factors, were extracted from segmented lesions in T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC) multiparametric-prostate MRI sequences to predict the UCLA scores. We then utilized the created dictionary to interpret the best-predictive models. Combining T2WI, DWI, and ADC with FSAs including ANOVA F-test, Correlation Coefficient, and Fisher Score, and utilizing logistic regression, identified key features: The 90th percentile from T2WI, which captures hypo-intensity related to prostate cancer risk; Variance from T2WI, indicating lesion heterogeneity; shape metrics including Least Axis Length and Surface Area to Volume ratio from ADC, describing lesion shape and compactness; and Run Entropy from ADC, reflecting texture consistency. This approach achieved the highest average accuracy of 0.78, significantly outperforming single-sequence methods (p-value&lt;0.05). The developed dictionary for Prostate-MRI (PM1.0) serves as a common language, fosters collaboration between clinical professionals and AI developers to advance trustworthy AI solutions that support reliable&#x2F;interpretable clinical decisions. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†PI-RADSä¸­å®šä¹‰çš„è§†è§‰è¯­ä¹‰ç‰¹å¾å’Œç›¸å…³é£é™©å› ç´ ä¹‹é—´çš„è”ç³»ï¼Œä¸ä»…å…³æ³¨å¼‚å¸¸æˆåƒç»“æœï¼Œè¿˜é€šè¿‡åˆ›å»ºç”Ÿç‰©å­¦&#x2F;æ”¾å°„å­¦RFçš„æ ‡å‡†å­—å…¸ï¼Œåœ¨åŒ»ç–—å’Œäººå·¥æ™ºèƒ½ä¸“ä¸šäººå£«ä¹‹é—´å»ºç«‹å…±äº«æ¡†æ¶ã€‚éšåï¼Œæˆ‘ä»¬ä»T2åŠ æƒæˆåƒï¼ˆT2WIï¼‰ã€æ‰©æ•£åŠ æƒæˆåƒï¼ˆDWIï¼‰å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰å¤šå‚æ•°å‰åˆ—è…ºMRIåºåˆ—ä¸­åˆ†å‰²çš„ç—…ç¶ä¸­æå–äº†6ä¸ªå¯è§£é‡Šå’Œä¸ƒä¸ªå¤æ‚åˆ†ç±»å™¨ï¼Œè¿™äº›åˆ†ç±»å™¨ä¸åº”ç”¨äºé£é™©å› ç´ çš„ä¹ä¸ªå¯è§£é‡Šç‰¹å¾é€‰æ‹©ç®—æ³•ï¼ˆFSAï¼‰ç›¸å…³è”ï¼Œä»¥é¢„æµ‹UCLAè¯„åˆ†ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ›å»ºçš„å­—å…¸æ¥è§£é‡Šæœ€ä½³é¢„æµ‹æ¨¡å‹ã€‚ç»“åˆT2WIã€DWIå’ŒADCä¸FSAï¼ŒåŒ…æ‹¬ANOVA Fæ£€éªŒã€ç›¸å…³ç³»æ•°å’Œè´¹èˆå°”å¾—åˆ†ï¼Œå¹¶åˆ©ç”¨é€»è¾‘å›å½’ï¼Œç¡®å®šäº†å…³é”®ç‰¹å¾ï¼šT2WIçš„90thç™¾åˆ†ä½æ•°ï¼Œæ•æ‰ä¸å‰åˆ—è…ºç™Œé£é™©ç›¸å…³çš„ä½å¼ºåº¦ï¼›è¡¨ç¤ºç—…å˜å¼‚è´¨æ€§çš„T2WIæ–¹å·®ï¼›æ¥è‡ªADCçš„å½¢çŠ¶åº¦é‡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æœ€å°è½´é•¿ã€è¡¨é¢ç§¯ä¸ä½“ç§¯æ¯”ï¼Œæè¿°ç—…å˜çš„å½¢çŠ¶å’Œç´§å‡‘åº¦ï¼›ä»¥åŠåæ˜ çº¹ç†ä¸€è‡´æ€§çš„ADCçš„Run Entropyã€‚è¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®ç‡0.78ï¼Œæ˜¾è‘—ä¼˜äºå•åºåˆ—æ–¹æ³•ï¼ˆpå€¼&lt;0.05ï¼‰ã€‚æ‰€å¼€å‘çš„å‰åˆ—è…ºMRIå­—å…¸ï¼ˆPM1.0ï¼‰ä½œä¸ºä¸€ç§é€šç”¨è¯­è¨€ï¼Œä¿ƒè¿›äº†ä¸´åºŠä¸“ä¸šäººå£«å’Œäººå·¥æ™ºèƒ½å¼€å‘è€…ä¹‹é—´çš„åˆä½œï¼Œæ¨åŠ¨äº†å¯ä¿¡èµ–çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„å‘å±•ï¼Œæ”¯æŒå¯é &#x2F;å¯è§£é‡Šçš„ä¸´åºŠå†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10967v2">PDF</a> 24 pages, 3 Figures, 2 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†PI-RADSå®šä¹‰çš„è§†è§‰è¯­ä¹‰ç‰¹å¾ä¸ç›¸å…³é£é™©å› ç´ ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–è¯å…¸æ¥æè¿°ç”Ÿç‰©å­¦&#x2F;æ”¾å°„å­¦ä¸­çš„é£é™©å› ç´ ã€‚åˆ©ç”¨è¯¥è¯å…¸å¹¶ç»“åˆç‰¹å¾é€‰æ‹©ç®—æ³•ï¼ˆFSAï¼‰ï¼Œå¯¹T2åŠ æƒæˆåƒï¼ˆT2WIï¼‰ã€æ‰©æ•£åŠ æƒæˆåƒï¼ˆDWIï¼‰å’Œè¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰çš„å¤šå‚æ•°å‰åˆ—è…ºMRIåºåˆ—è¿›è¡Œç‰¹å¾æå–å’Œæ¨¡å‹é¢„æµ‹ï¼Œå®ç°å¯¹UCLAè¯„åˆ†çš„é¢„æµ‹ã€‚è¯¥æ–¹æ³•æˆåŠŸæ„å»ºäº†å¹³å‡å‡†ç¡®ç‡ä¸º0.78çš„é¢„æµ‹æ¨¡å‹ï¼Œä¸”æ˜¾è‘—ä¼˜äºå•åºåˆ—æ–¹æ³•ã€‚æ‰€å¼€å‘çš„PM1.0è¯å…¸ä½œä¸ºä¸´åºŠä¸“ä¸šäººå£«ä¸äººå·¥æ™ºèƒ½å¼€å‘è€…ä¹‹é—´çš„å…±åŒè¯­è¨€ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¯é ä¸”å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„å‘å±•ï¼Œä»¥æ”¯æŒå¯ä¿¡çš„ä¸´åºŠå†³ç­–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†PI-RADSè§†è§‰è¯­ä¹‰ç‰¹å¾ä¸é£é™©å› ç´ çš„å…³è”ï¼Œå¹¶è¶…è¶Šäº†å¼‚å¸¸æˆåƒå‘ç°ï¼Œå»ºç«‹äº†åŒ»å­¦å’ŒAIä¸“ä¸šäººå£«ä¹‹é—´çš„å…±äº«æ¡†æ¶ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªæ ‡å‡†åŒ–è¯å…¸æ¥æè¿°ç”Ÿç‰©å­¦&#x2F;æ”¾å°„å­¦ä¸­çš„é£é™©å› ç´ ï¼ˆPM1.0ï¼‰ã€‚</li>
<li>ä½¿ç”¨äº†å¤šç§ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼ˆFSAï¼‰æ¥ä»å¤šå‚æ•°å‰åˆ—è…ºMRIåºåˆ—ä¸­æå–å…³é”®ç‰¹å¾ã€‚</li>
<li>ç»“åˆT2WIã€DWIå’ŒADCæ•°æ®é¢„æµ‹UCLAè¯„åˆ†ï¼Œæ„å»ºäº†å¹³å‡å‡†ç¡®ç‡ä¸º0.78çš„é¢„æµ‹æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æˆåŠŸè¯†åˆ«äº†ä¸å‰åˆ—è…ºç™Œé£é™©ç›¸å…³çš„å…³é”®ç‰¹å¾ï¼Œå¦‚T2WIçš„90thç™¾åˆ†ä½æ•°çš„ä½å¼ºåº¦ã€T2WIçš„æ–¹å·®è¡¨ç¤ºçš„ç—…å˜å¼‚è´¨æ€§ç­‰ã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¾è‘—ä¼˜äºå•åºåˆ—æ–¹æ³•ï¼ˆpå€¼&lt;0.05ï¼‰ã€‚</li>
<li>PM1.0è¯å…¸ä¸ºä¸´åºŠä¸“ä¸šäººå£«å’Œäººå·¥æ™ºèƒ½å¼€å‘è€…æä¾›äº†å…±åŒè¯­è¨€ï¼Œä¿ƒè¿›äº†å¯é ä¸”å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1d630dca6df23dc0664931eaaf5cb77a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d203d73718fe998fd4e29755a5a5b61f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb83164b14c9a6fe07209142d9a16658.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="BATseg-Boundary-aware-Multiclass-Spinal-Cord-Tumor-Segmentation-on-3D-MRI-Scans"><a href="#BATseg-Boundary-aware-Multiclass-Spinal-Cord-Tumor-Segmentation-on-3D-MRI-Scans" class="headerlink" title="BATseg: Boundary-aware Multiclass Spinal Cord Tumor Segmentation on 3D   MRI Scans"></a>BATseg: Boundary-aware Multiclass Spinal Cord Tumor Segmentation on 3D   MRI Scans</h2><p><strong>Authors:Hongkang Song, Zihui Zhang, Yanpeng Zhou, Jie Hu, Zishuo Wang, Hou Him Chan, Chon Lok Lei, Chen Xu, Yu Xin, Bo Yang</strong></p>
<p>Spinal cord tumors significantly contribute to neurological morbidity and mortality. Precise morphometric quantification, encompassing the size, location, and type of such tumors, holds promise for optimizing treatment planning strategies. Although recent methods have demonstrated excellent performance in medical image segmentation, they primarily focus on discerning shapes with relatively large morphology such as brain tumors, ignoring the challenging problem of identifying spinal cord tumors which tend to have tiny sizes, diverse locations, and shapes. To tackle this hard problem of multiclass spinal cord tumor segmentation, we propose a new method, called BATseg, to learn a tumor surface distance field by applying our new multiclass boundary-aware loss function. To verify the effectiveness of our approach, we also introduce the first and large-scale spinal cord tumor dataset. It comprises gadolinium-enhanced T1-weighted 3D MRI scans from 653 patients and contains the four most common spinal cord tumor types: astrocytomas, ependymomas, hemangioblastomas, and spinal meningiomas. Extensive experiments on our dataset and another public kidney tumor segmentation dataset show that our proposed method achieves superior performance for multiclass tumor segmentation. </p>
<blockquote>
<p>è„Šé«“è‚¿ç˜¤å¯¹ç¥ç»ç³»ç»Ÿçš„å‘ç—…ç‡å’Œæ­»äº¡ç‡æœ‰æ˜¾è‘—å½±å“ã€‚ç²¾ç¡®çš„å½¢æ€è®¡é‡é‡åŒ–ï¼ŒåŒ…æ‹¬è‚¿ç˜¤çš„å¤§å°ã€ä½ç½®å’Œç±»å‹ï¼Œæœ‰æœ›ä¼˜åŒ–æ²»ç–—è§„åˆ’ç­–ç•¥ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨è¾¨åˆ«å½¢æ€ç›¸å¯¹è¾ƒå¤§çš„è‚¿ç˜¤ï¼Œå¦‚è„‘è‚¿ç˜¤ï¼Œè€Œå¿½ç•¥äº†è¯†åˆ«è„Šé«“è‚¿ç˜¤çš„éš¾é¢˜ï¼Œè¿™äº›è‚¿ç˜¤å¾€å¾€å°ºå¯¸å°ã€ä½ç½®å¤šæ ·ã€å½¢æ€å„å¼‚ã€‚ä¸ºäº†è§£å†³å¤šç±»è„Šé«“è‚¿ç˜¤åˆ†å‰²è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºBATsegï¼Œé€šè¿‡åº”ç”¨æˆ‘ä»¬æ–°çš„å¤šç±»è¾¹ç•Œæ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥å­¦ä¹ è‚¿ç˜¤è¡¨é¢è·ç¦»åœºã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„è„Šé«“è‚¿ç˜¤æ•°æ®é›†ã€‚å®ƒåŒ…å«653ä¾‹æ‚£è€…çš„é’†å¢å¼ºT1åŠ æƒ3D MRIæ‰«æï¼ŒåŒ…å«å››ç§æœ€å¸¸è§çš„è„Šé«“è‚¿ç˜¤ç±»å‹ï¼šæ˜Ÿå½¢ç»†èƒç˜¤ã€å®¤ç®¡è†œç˜¤ã€è¡€ç®¡æ¯ç»†èƒç˜¤å’Œè„Šé«“è„‘è†œç˜¤ã€‚åœ¨æˆ‘ä»¬æ•°æ®é›†å’Œå¦ä¸€ä¸ªå…¬å…±è‚¾è„è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šç±»è‚¿ç˜¤åˆ†å‰²æ–¹é¢å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06507v1">PDF</a> ECCV 2024 Workshop on BioImage Computing. Code and data are available   at: <a target="_blank" rel="noopener" href="https://github.com/vLAR-group/BATseg">https://github.com/vLAR-group/BATseg</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç²¾ç¡®å½¢æ€å­¦æµ‹é‡çš„æ–°æ–¹æ³•ï¼Œå¯¹äºä¼˜åŒ–è„Šé«“è‚¿ç˜¤æ²»ç–—ç­–ç•¥å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥è§£å†³å¤šç±»è„Šé«“è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºBATsegçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åº”ç”¨æ–°çš„å¤šç±»è¾¹ç•Œæ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥å­¦ä¹ è‚¿ç˜¤è¡¨é¢è·ç¦»åœºã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡è„Šé«“è‚¿ç˜¤æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª653ä½æ‚£è€…çš„é’†å¢å¼ºT1åŠ æƒä¸‰ç»´MRIæ‰«ææ•°æ®ï¼Œå¹¶è¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç±»è‚¿ç˜¤åˆ†å‰²ä¸Šå…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬æœ€å¸¸è§çš„å››ç§è„Šé«“è‚¿ç˜¤ç±»å‹ï¼šç¥ç»èƒ¶è´¨ç˜¤ã€ç¥ç»çº¤ç»´ç˜¤ç—…ã€å†…çš®æ ·ç»†èƒå›Šæ ·ç»†èƒç˜¤å’Œè„Šé«“è„Šè†œç˜¤ã€‚æ­¤é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯æ›´ç²¾å‡†åœ°æ²»ç–—è„Šé«“è‚¿ç˜¤æ‚£è€…ï¼Œç¼“è§£ä»–ä»¬ç¥ç»å—æŸç¨‹åº¦ä¸¥é‡çš„ç°è±¡ï¼Œç”šè‡³æ˜¯é˜²æ²»è¿›ä¸€æ­¥åŠ å‰§æ‚£è€…çš„ç¥ç»é—®é¢˜ä»¥æé«˜ç”Ÿæ´»è´¨é‡ç­‰ç°å®ç”Ÿå­˜éœ€æ±‚å‹åŠ›çš„æƒ…å†µå‡ºç°ã€‚æ‰€ç ”å‘çš„è¯¥æ•°æ®åº“å°†å¯¹ç²¾å‡†æ²»ç–—å…·æœ‰æ¨åŠ¨ä½œç”¨ã€‚ç»è¿‡è¯•éªŒè¯æ˜å…¶æ–¹æ³•å¯¹äºåˆ†å‰²å°ºå¯¸å°ã€ä½ç½®å¤šæ ·ä¸”å½¢æ€å„å¼‚çš„è„Šé«“è‚¿ç˜¤æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚æ­¤æ–¹æ³•æœ‰åŠ©äºæå‡æ²»ç–—ç²¾å‡†åº¦ä¸é¢„åæ•ˆæœã€‚è¯¥æ•°æ®é›†ä¹Ÿå¯ç”¨äºæœªæ¥å¯¹å¤šç±»è‚¿ç˜¤åˆ†å‰²ç®—æ³•è¿›è¡Œæ›´æ·±å…¥çš„è¯„ä¼°å’Œä¼˜åŒ–ã€‚è¯¥ç ”ç©¶çš„æˆæœå¯¹äºæé«˜åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯çš„å®é™…åº”ç”¨ä»·å€¼å’Œæ¨è¿›ç›¸å…³é¢†åŸŸçš„ç§‘ç ”è¿›å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç®€è€Œè¨€ä¹‹ï¼Œç ”ç©¶ä¸ºè„Šé«“è‚¿ç˜¤çš„æ²»ç–—æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b244e34fcd36b86bd4d9fbdeb74eca6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cc8ca320303215405b67b8e94d4e2f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff7b190ea1359cc57b13ad2c26a71963.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c9239e151346fba9b87126e9743c1df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8db4b16e606bb5fad6dce1271b0a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94768a91decf1892695baba634396906.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT"><a href="#Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT" class="headerlink" title="Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT"></a>Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT</h2><p><strong>Authors:Zi Li, Ying Chen, Zeli Chen, Yanzhou Su, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Yunhai Bai, Zhinlin Zheng, Le Lu, Yirui Wang, Jia Ge, Xianghua Ye, Senxiang Yan, Dakai Jin</strong></p>
<p>In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians typically delineate the gross tumor volume (GTV) using non-contrast planning computed tomography to ensure accurate radiation dose delivery. However, the low contrast between tumors and adjacent normal tissues necessitates that radiation oncologists manually delineate the tumors, often relying on diagnostic MRI for guidance. % In this study, we propose a novel approach to directly segment NPC gross tumors on non-contrast planning CT images, circumventing potential registration errors when aligning MRI or MRI-derived tumor masks to planning CT. To address the low contrast issues between tumors and adjacent normal structures in planning CT, we introduce a 3D Semantic Asymmetry Tumor segmentation (SATs) method. Specifically, we posit that a healthy nasopharyngeal region is characteristically bilaterally symmetric, whereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then, we propose a Siamese contrastive learning segmentation framework that minimizes the voxel-wise distance between original and flipped areas without tumor and encourages a larger distance between original and flipped areas with tumor. Thus, our approach enhances the sensitivity of features to semantic asymmetries. % Extensive experiments demonstrate that the proposed SATs achieves the leading NPC GTV segmentation performance in both internal and external testing, \emph{e.g.}, with at least 2% absolute Dice score improvement and 12% average distance error reduction when compared to other state-of-the-art methods in the external testing. </p>
<blockquote>
<p>åœ¨é¼»å’½ç™Œï¼ˆNPCï¼‰çš„æ”¾å°„æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œä¸´åºŠåŒ»ç”Ÿé€šå¸¸ä½¿ç”¨éå¯¹æ¯”è®¡åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ¥ç²¾ç¡®æç»˜å¤§ä½“è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVï¼‰ï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„è¾å°„å‰‚é‡ä¼ é€’ã€‚ç„¶è€Œï¼Œè‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¿™ä½¿å¾—æ”¾å°„è‚¿ç˜¤å­¦å®¶å¿…é¡»æ‰‹åŠ¨æç»˜è‚¿ç˜¤ï¼Œé€šå¸¸ä¾èµ–è¯Šæ–­æ€§ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œå¼•å¯¼ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨éå¯¹æ¯”è®¡åˆ’CTå›¾åƒä¸Šç›´æ¥åˆ†å‰²é¼»å’½ç™Œå¤§ä½“è‚¿ç˜¤çš„æ–°æ–¹æ³•ï¼Œä»è€Œé¿å…äº†å°†MRIæˆ–MRIè¡ç”Ÿçš„è‚¿ç˜¤æ©è†œä¸è®¡åˆ’CTè¿›è¡Œå¯¹é½æ—¶å¯èƒ½å‡ºç°çš„æ½œåœ¨æ³¨å†Œè¯¯å·®ã€‚ä¸ºäº†è§£å†³è§„åˆ’CTä¸­è‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»“æ„ä¹‹é—´å¯¹æ¯”åº¦ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†3Dè¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¤ä¸ºå¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰å…¸å‹çš„åŒä¾§å¯¹ç§°æ€§ï¼Œè€Œé¼»å’½ç™Œçš„å‡ºç°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ€å°åŒ–åŸå§‹åŒºåŸŸå’Œç¿»è½¬åŒºåŸŸï¼ˆæ— è‚¿ç˜¤ï¼‰ä¹‹é—´çš„ä½“ç´ è·ç¦»ï¼ŒåŒæ—¶é¼“åŠ±åŸå§‹åŒºåŸŸå’Œç¿»è½¬åŒºåŸŸï¼ˆæœ‰è‚¿ç˜¤ï¼‰ä¹‹é—´çš„è·ç¦»å¢å¤§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯¹è¯­ä¹‰ä¸å¯¹ç§°æ€§çš„ç‰¹å¾æ•æ„Ÿæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºSATsåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„NPC GTVåˆ†å‰²æ€§èƒ½ï¼Œä¾‹å¦‚ä¸å¤–éƒ¨æµ‹è¯•ä¸­çš„å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè‡³å°‘æé«˜äº†2ï¼…çš„ç»å¯¹Diceå¾—åˆ†å’Œé™ä½äº†12ï¼…çš„å¹³å‡è·ç¦»è¯¯å·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18290v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œç›´æ¥åœ¨éå¯¹æ¯”å‰‚è§„åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸Šåˆ†å‰²é¼»å’½ç™Œï¼ˆNPCï¼‰çš„è‚¿ç˜¤ä½“ç§¯ï¼Œé¿å…äº†MRIæˆ–MRIè¡ç”Ÿçš„è‚¿ç˜¤æ©è†œä¸è§„åˆ’CTå¯¹é½æ—¶å¯èƒ½å‡ºç°çš„æ³¨å†Œè¯¯å·®ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰å…¸å‹çš„åŒä¾§å¯¹ç§°æ€§ï¼Œè€Œé¼»å’½ç™Œçš„å‡ºç°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§è¿™ä¸€ç‰¹æ€§ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºè¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰çš„3Dåˆ†å‰²æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒSATsåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„é¼»å’½ç™ŒGTVåˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„é¼»å’½ç™Œè‚¿ç˜¤ä½“ç§¯åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥åº”ç”¨äºéå¯¹æ¯”å‰‚è§„åˆ’CTå›¾åƒã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨å¥åº·é¼»å’½åŒºåŸŸçš„åŒä¾§å¯¹ç§°æ€§ä»¥åŠé¼»å’½ç™Œå¯¹æ­¤å¯¹ç§°æ€§çš„ç ´åæ¥è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>ç ”ç©¶ä¸­å¼•å…¥äº†åä¸ºè¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰çš„3Dåˆ†å‰²æ–¹æ³•ï¼Œä»¥å¤„ç†è‚¿ç˜¤ä¸ç›¸é‚»æ­£å¸¸ç»„ç»‡ä¹‹é—´å¯¹æ¯”åº¦ä½çš„é—®é¢˜ã€‚</li>
<li>SATsæ–¹æ³•é€šè¿‡æœ€å°åŒ–åŸå§‹å’Œç¿»è½¬åŒºåŸŸï¼ˆæ— è‚¿ç˜¤ï¼‰ä¹‹é—´çš„ä½“ç´ è·ç¦»ï¼Œå¹¶é¼“åŠ±åŸå§‹å’Œç¿»è½¬åŒºåŸŸï¼ˆæœ‰è‚¿ç˜¤ï¼‰ä¹‹é—´ä¿æŒè¾ƒå¤§çš„è·ç¦»ï¼Œä»è€Œæé«˜ç‰¹å¾å¯¹è¯­ä¹‰ä¸å¯¹ç§°çš„æ•æ„Ÿæ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSATsæ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œåœ¨å¤–éƒ¨æµ‹è¯•ä¸­å®ç°äº†è‡³å°‘2%çš„ç»å¯¹Diceåˆ†æ•°æé«˜å’Œ12%çš„å¹³å‡è·ç¦»è¯¯å·®é™ä½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc0692acb8c793eb4912c8a7d3d5c1b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca48d1045381b4b186e49609dc6f60b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bec9d8edcad32f1564b13152bb8cdfd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-ViCTORIA-project-description-of-a-multi-frequency-radio-survey-of-the-Virgo-galaxy-cluster"><a href="#The-ViCTORIA-project-description-of-a-multi-frequency-radio-survey-of-the-Virgo-galaxy-cluster" class="headerlink" title="The ViCTORIA project: description of a multi-frequency radio survey of   the Virgo galaxy cluster"></a>The ViCTORIA project: description of a multi-frequency radio survey of   the Virgo galaxy cluster</h2><p><strong>Authors:F. de Gasperin, H. W. Edler, A. Boselli, P. Serra, M. Fossati, V. Heesen, A. Merloni, M. Murgia, T. H. Reiprich, A. Spasic, N. Zabel</strong></p>
<p>The Virgo cluster is the closest richest nearby galaxy cluster. It is in the formation process, with a number of sub-clusters undergoing merging and interactions. Although a great laboratory to study galaxy evolution and cluster formation, its large apparent size and the severe dynamic range limitations due to the presence of the bright radio source Virgo A (M 87) reduced the ability of past wide-area radio surveys to image the region with high sensitivity and fidelity. In this paper we describe the â€œVirgo Cluster multi-Telescope Observations in Radio of Interacting galaxies and AGNâ€ (ViCTORIA) project. The survey and its data reduction strategy are designed to mitigate the challenges of this field and deliver: images from 42 MHz to 1.7 GHz frequencies of the Virgo cluster, about 60 times deeper than existing data, in full polarisation, and including a blind HI survey that aims at mapping seven times more galaxies than previous experiments and without selection biases. Data have been collected with the Low-Frequency Array (LOFAR) and with MeerKAT in L-band, including polarisation and enough frequency resolution to conduct local HI studies. At the distance of Virgo, current radio instruments have the resolution to probe scales of ~500 pc and the sensitivity to study dwarf galaxies, the most fragile systems given their shallow gravitational potential wells, making Virgo a unique laboratory to study galaxy evolution and AGN feedback in a rich environment. In this work, we present some preliminary results, including high resolution images of the radio emission surrounding M 87, that show that the lobes are filled with filamentary structures. The combination of the presented radio surveys with state-of-the-art optical, UV, X-ray surveys will massively increase the scientific output from the studies of the Virgo cluster, making the ViCTORIA Projectâ€™s legacy value outstanding. </p>
<blockquote>
<p>å¤„å¥³åº§æ˜Ÿç³»å›¢æ˜¯é™„è¿‘æœ€ä¸°å¯Œä¸”è·ç¦»æœ€è¿‘çš„å¤©ä½“æ˜Ÿç³»å›¢ä¹‹ä¸€ã€‚å®ƒæ­£å¤„äºå½¢æˆè¿‡ç¨‹ä¸­ï¼Œå¤šä¸ªå­æ˜Ÿç³»å›¢æ­£åœ¨åˆå¹¶å’Œäº’åŠ¨ã€‚å°½ç®¡å®ƒæ˜¯ä¸€ä¸ªç ”ç©¶æ˜Ÿç³»æ¼”åŒ–å’Œæ˜Ÿç³»å›¢å½¢æˆçš„é‡è¦å®éªŒå®¤ï¼Œä½†ç”±äºå…¶å·¨å¤§çš„è¡¨é¢å¤§å°ä»¥åŠæ˜äº®æ— çº¿ç”µæºå¤„å¥³åº§Aï¼ˆM 87ï¼‰äº§ç”Ÿçš„åŠ¨æ€èŒƒå›´ä¸¥é‡é™åˆ¶å› ç´ ï¼Œè¿‡å»çš„å¤§å‹åŒºåŸŸæ— çº¿ç”µè°ƒæŸ¥å¾ˆéš¾ä»¥é«˜åº¦æ•æ„Ÿæ€§å’Œä¿çœŸåº¦æ¥å‘ˆç°è¯¥åŒºåŸŸã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†â€œå¤„å¥³åº§æ˜Ÿç³»å›¢äº¤äº’æ˜Ÿç³»å’Œæ´»è·ƒæ˜Ÿç³»æ ¸çš„æ— çº¿ç”µå¤šæœ›è¿œé•œè§‚æµ‹â€ï¼ˆViCTORIAï¼‰é¡¹ç›®ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥åŠå…¶æ•°æ®ç¼©å‡ç­–ç•¥æ—¨åœ¨ç¼“è§£è¯¥é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå¹¶å¸¦æ¥ä»¥ä¸‹æˆæœï¼šä»42å…†èµ«åˆ°1.7åƒå…†èµ«é¢‘ç‡çš„å¤„å¥³åº§æ˜Ÿå›¢çš„å›¾åƒï¼Œå…¶æ·±åº¦å¤§çº¦æ˜¯ç°æœ‰æ•°æ®çš„60å€ï¼Œä»¥å…¨æåŒ–å½¢å¼å‘ˆç°ï¼Œå¹¶åŒ…æ‹¬ä¸€ä¸ªæ—¨åœ¨æ˜ å°„æ¯”å…ˆå‰å®éªŒæ›´å¤šçš„æ˜Ÿç³»çš„ç›²HIè°ƒæŸ¥ï¼Œç›®æ ‡æ˜¯æ²¡æœ‰åè§åœ°æ˜ å°„å‡ºæ¯”å…ˆå‰å®éªŒæ›´å¤šçš„ä¸ƒå€æ˜Ÿç³»ã€‚æˆ‘ä»¬ä½¿ç”¨ä½é¢‘é˜µåˆ—ï¼ˆLOFARï¼‰å’ŒMeerKATåœ¨Læ³¢æ®µæ”¶é›†æ•°æ®ï¼ŒåŒ…æ‹¬æåŒ–ä»¥åŠè¶³å¤Ÿçš„é¢‘ç‡åˆ†è¾¨ç‡æ¥è¿›è¡Œå±€éƒ¨HIç ”ç©¶ã€‚åœ¨å¤„å¥³åº§çš„èŒƒå›´å†…ï¼Œå½“å‰çš„æ— çº¿ç”µä»ªå™¨å…·æœ‰æ¢æµ‹çº¦500ä¸ªå¤©æ–‡å•ä½çš„å°ºåº¦çš„åˆ†è¾¨ç‡ï¼Œå¹¶å…·æœ‰ç ”ç©¶çŸ®æ˜Ÿç³»ç­‰çš„çµæ•åº¦ï¼Œè¿™äº›çŸ®æ˜Ÿç³»ç”±äºå…¶è¾ƒæµ…çš„å¼•åŠ›åŠ¿äº•è€Œæˆä¸ºæœ€è„†å¼±çš„ç³»ç»Ÿä¹‹ä¸€ï¼Œè¿™ä½¿å¾—å¤„å¥³åº§æˆä¸ºä¸€ä¸ªç‹¬ç‰¹çš„å®éªŒå®¤æ¥ç ”ç©¶ä¸°å¯Œçš„ç¯å¢ƒä¸­æ˜Ÿç³»æ¼”åŒ–ä»¥åŠæ´»åŠ¨æ˜Ÿç³»æ ¸åé¦ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›åˆæ­¥ç»“æœï¼ŒåŒ…æ‹¬å›´ç»•M 87çš„é«˜åˆ†è¾¨ç‡å›¾åƒæ˜¾ç¤ºå…¶ç“£ç”±çº¤ç»´çŠ¶ç»“æ„å¡«å……ã€‚ç»“åˆæœ€æ–°çš„å…‰å­¦ã€ç´«å¤–çº¿å’ŒXå°„çº¿è°ƒæŸ¥ä¸æœ¬æ¬¡å‘ˆç°çš„æ— çº¿ç”µè°ƒæŸ¥å°†å¤§å¤§æé«˜ä»å¤„å¥³åº§æ˜Ÿå›¢çš„ç ”ç©¶ä¸­è·å¾—çš„ç§‘å­¦äº§å‡ºï¼Œä»è€Œçªæ˜¾å‡ºViCTORIAé¡¹ç›®çš„æ°å‡ºä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18204v2">PDF</a> Accepted for publication in A&amp;A</p>
<p><strong>Summary</strong><br>    Virgoæ˜Ÿç³»å›¢æ˜¯é™„è¿‘æœ€ä¸°å¯Œçš„æ˜Ÿç³»å›¢ï¼Œæ­£å¤„äºå½¢æˆè¿‡ç¨‹ä¸­ï¼ŒåŒ…å«è®¸å¤šæ­£åœ¨åˆå¹¶å’Œäº¤äº’çš„å­æ˜Ÿç³»å›¢ã€‚ç”±äºVirgo Aï¼ˆM 87ï¼‰çš„å½±å“ï¼Œè¿‡å»çš„å¤§èŒƒå›´æ— çº¿ç”µè°ƒæŸ¥éš¾ä»¥åœ¨æ­¤åŒºåŸŸè¿›è¡Œé«˜çµæ•åº¦å’Œä¿çœŸåº¦çš„æˆåƒã€‚æœ¬æ–‡ä»‹ç»äº†â€œVirgoæ˜Ÿç³»å›¢å¤šæœ›è¿œé•œè§‚æµ‹äº¤äº’æ˜Ÿç³»å’Œæ´»è·ƒæ˜Ÿç³»æ ¸çš„æ— çº¿ç”µâ€ï¼ˆViCTORIAï¼‰é¡¹ç›®ï¼Œæ—¨åœ¨å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œå¹¶æä¾›ä»42 MHzåˆ°1.7 GHzé¢‘ç‡çš„å¤„å¥³åº§æ˜Ÿç³»å›¢å›¾åƒï¼Œæ·±åº¦çº¦ä¸ºç°æœ‰æ•°æ®çš„60å€ï¼ŒåŒ…æ‹¬å…¨æåŒ–ä»¥åŠæ—¨åœ¨æ¯”å…ˆå‰å®éªŒå¤šæ˜ å°„ä¸ƒå€æ˜Ÿç³»çš„ç›²HIè°ƒæŸ¥ã€‚åˆæ­¥ç»“æœæ˜¾ç¤ºM 87å‘¨å›´çš„æ— çº¿ç”µå‘å°„é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ˜¾ç¤ºç»†ä¸çŠ¶ç»“æ„ã€‚ç»“åˆæœ€æ–°çš„å…‰å­¦ã€ç´«å¤–çº¿å’ŒXå°„çº¿è°ƒæŸ¥ï¼ŒViCTORIAé¡¹ç›®å°†å¤§å¤§æé«˜å¤„å¥³åº§æ˜Ÿç³»å›¢çš„ç ”ç©¶äº§å‡ºï¼Œå…·æœ‰æ°å‡ºçš„é—äº§ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Virgoæ˜Ÿç³»å›¢æ˜¯é™„è¿‘æœ€ä¸°å¯Œçš„æ˜Ÿç³»å›¢ï¼Œæ­£åœ¨å½¢æˆè¿‡ç¨‹ä¸­ï¼ŒåŒ…å«å¤šä¸ªå­æ˜Ÿç³»å›¢é—´çš„åˆå¹¶å’Œäº¤äº’ä½œç”¨ã€‚</li>
<li>ç”±äºVirgo Açš„å½±å“ï¼Œè¿‡å»å¯¹Virgoæ˜Ÿç³»å›¢è¿›è¡Œå¤§èŒƒå›´æ— çº¿ç”µè°ƒæŸ¥å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ViCTORIAé¡¹ç›®æ—¨åœ¨å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæä¾›å‰æ‰€æœªæœ‰çš„æ·±åº¦å’Œé«˜åˆ†è¾¨ç‡çš„æ— çº¿ç”µå›¾åƒã€‚</li>
<li>ViCTORIAé¡¹ç›®é¦–æ¬¡è¿›è¡Œäº†åŒ…æ‹¬å…¨æåŒ–çš„æ— çº¿ç”µè°ƒæŸ¥ï¼Œæ¶µç›–ä»42 MHzåˆ°1.7 GHzçš„é¢‘ç‡èŒƒå›´ã€‚</li>
<li>é¡¹ç›®åŒ…æ‹¬ç›²HIè°ƒæŸ¥ï¼Œæ—¨åœ¨æ˜ å°„æ›´å¤šæ˜Ÿç³»ï¼Œå‡å°‘é€‰æ‹©åè§ã€‚</li>
<li>ä½¿ç”¨æœ€æ–°æŠ€æœ¯è·å¾—çš„åˆæ­¥ç»“æœæ˜¾ç¤ºM 87å‘¨å›´çš„æ— çº¿ç”µå‘å°„é«˜åˆ†è¾¨ç‡å›¾åƒå…·æœ‰ç»†ä¸çŠ¶ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8149a5c9942baf94ce283de0d598069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a1403df0f3267287b44e6199688d8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c10693ddb5ac21ad67212098e6f414e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e6b7a9e8eef36b47498d7212b29194c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9afc889d3df3a75af2a5e1121489d6e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4399be23837f97747b1d8e3693c744c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Panning-for-gold-with-the-Neil-Gehrels-Swift-Observatory-an-optimal-strategy-for-finding-the-counterparts-to-gravitational-wave-events"><a href="#Panning-for-gold-with-the-Neil-Gehrels-Swift-Observatory-an-optimal-strategy-for-finding-the-counterparts-to-gravitational-wave-events" class="headerlink" title="Panning for gold with the Neil Gehrels Swift Observatory: an optimal   strategy for finding the counterparts to gravitational wave events"></a>Panning for gold with the Neil Gehrels Swift Observatory: an optimal   strategy for finding the counterparts to gravitational wave events</h2><p><strong>Authors:R. A. J. Eyles-Ferris, P. A. Evans, A. A. Breeveld, S. B. Cenko, S. Dichiara, J. A. Kennea, N. J. Klingler, N. P. M. Kuin, F. E. Marshall, S. R. Oates, M. J. Page, S. Ronchini, M. H. Siegel, A. Tohuvavohu, S. Campana, V. Dâ€™Elia, J. P. Osborne, K. L. Page, M. De Pasquale, E. Troja</strong></p>
<p>The LIGO, Virgo and KAGRA gravitational wave observatories are currently undertaking their O4 observing run offering the opportunity to discover new electromagnetic counterparts to gravitational wave events. We examine the capability of the Neil Gehrels Swift Observatory (Swift) to respond to these triggers, primarily binary neutron star mergers, with both the UV&#x2F;Optical Telescope (UVOT) and the X-ray Telescope (XRT). We simulate Swiftâ€™s response to a trigger under different strategies using model skymaps, convolving these with the 2MPZ catalogue to produce an ordered list of observing fields, deriving the time taken for Swift to reach the correct field and simulating the instrumental responses to modelled kilonovae and short gamma-ray burst afterglows. We find that UVOT using the $u$ filter with an exposure time of order 120 s is optimal for most follow-up observations and that we are likely to detect counterparts in $\sim6$% of all binary neutron star triggers detectable by LVK in O4. We find that the gravitational wave 90% error area and measured distance to the trigger allow us to select optimal triggers to follow-up. Focussing on sources less than 300 Mpc away or 500 Mpc if the error area is less than a few hundred square degrees, distances greater than previously assumed, offer the best opportunity for discovery by Swift with $\sim5 - 30$% of triggers having detection probabilities $\geq 0.5$. At even greater distances, we can further optimise our follow-up by adopting a longer 250 s or 500 s exposure time. </p>
<blockquote>
<p>LIGOã€Virgoå’ŒKAGRAå¼•åŠ›æ³¢è§‚æµ‹ç«™ç›®å‰æ­£åœ¨å¼€å±•O4è§‚æµ‹è¿è¡Œï¼Œæœ‰æœºä¼šå‘ç°å¼•åŠ›æ³¢äº‹ä»¶çš„æ–°ç”µç£å¯¹åº”ä½“ã€‚æˆ‘ä»¬ç ”ç©¶äº†å°¼å°”Â·ç›–å°”å°”æ–¯Â·æ–¯å¨å¤«ç‰¹è§‚æµ‹ç«™ï¼ˆSwiftï¼‰å¯¹è¿™äº›è§¦å‘äº‹ä»¶çš„å“åº”èƒ½åŠ›ï¼Œä¸»è¦æ˜¯åŒä¸­å­æ˜Ÿåˆå¹¶äº‹ä»¶ï¼ŒåŒ…æ‹¬ç´«å¤–çº¿&#x2F;å…‰å­¦æœ›è¿œé•œï¼ˆUVOTï¼‰å’ŒXå°„çº¿æœ›è¿œé•œï¼ˆXRTï¼‰ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸åŒç­–ç•¥æ¨¡æ‹ŸSwiftå¯¹è§¦å‘çš„å“åº”ï¼Œä½¿ç”¨æ¨¡å‹æ˜Ÿå›¾å¹¶ç»“åˆ2MPZç›®å½•ç”Ÿæˆæœ‰åºè§‚æµ‹åœºåˆ—è¡¨ï¼Œæ¨ç®—Swiftåˆ°è¾¾æ­£ç¡®è§‚æµ‹åœºçš„æ—¶é—´ï¼Œå¹¶æ¨¡æ‹Ÿä»ªå™¨å¯¹æ¨¡æ‹Ÿçš„åƒæ–°æ˜Ÿå’ŒçŸ­ä¼½é©¬å°„çº¿æš´ä½™è¾‰çš„å“åº”ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨uæ»¤é•œæ›å…‰æ—¶é—´çº¦ä¸º120ç§’çš„UVOTæœ€é€‚åˆå¤§å¤šæ•°åç»­è§‚æµ‹ï¼Œæˆ‘ä»¬å¯èƒ½æ£€æµ‹åˆ°LVKåœ¨O4ä¸­å¯æ£€æµ‹åˆ°çš„æ‰€æœ‰åŒä¸­å­æ˜Ÿè§¦å‘äº‹ä»¶çš„çº¦6%ã€‚æˆ‘ä»¬å‘ç°å¼•åŠ›æ³¢çš„90%è¯¯å·®åŒºåŸŸå’Œåˆ°è§¦å‘çš„æµ‹é‡è·ç¦»æœ‰åŠ©äºæˆ‘ä»¬é€‰æ‹©æœ€ä½³è§¦å‘äº‹ä»¶è¿›è¡Œåç»­è§‚æµ‹ã€‚å¯¹äºè·ç¦»å°äº300Mpcæˆ–å¦‚æœè¯¯å·®é¢ç§¯å°äºå‡ ç™¾å¹³æ–¹åº¦åˆ™è·ç¦»ä¸º500Mpcçš„æºï¼Œæ¯”ä»¥å¾€å‡è®¾çš„æ›´è¿œçš„è·ç¦»æä¾›äº†Swiftå‘ç°çš„æ–°æœºä¼šï¼Œçº¦æœ‰5-30%çš„è§¦å‘äº‹ä»¶æ£€æµ‹æ¦‚ç‡â‰¥0.5ã€‚åœ¨æ›´å¤§çš„è·ç¦»ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é‡‡ç”¨æ›´é•¿çš„250ç§’æˆ–500ç§’æ›å…‰æ—¶é—´æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æˆ‘ä»¬çš„åç»­è§‚æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05072v2">PDF</a> 16 pages, 13 figures. Final version accepted by MNRAS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LIGOã€Virgoå’ŒKAGRAå¼•åŠ›æ³¢è§‚æµ‹ç«™æ­£åœ¨è¿›è¡ŒO4è§‚æµ‹è¿è¡Œï¼Œå¯»æ‰¾ä¸å¼•åŠ›æ³¢äº‹ä»¶ç›¸å¯¹åº”çš„æ–°ç”µç£å¯¹åº”ç‰©ã€‚æœ¬æ–‡ç ”ç©¶äº†å°¼å°”Â·ç›–å°”å°”æ–¯Â·æ–¯å¨å¤«ç‰¹å¤©æ–‡å°ï¼ˆSwiftï¼‰å¯¹è¿™äº›è§¦å‘äº‹ä»¶ï¼ˆä¸»è¦æ˜¯åŒä¸­å­æ˜Ÿåˆå¹¶ï¼‰çš„å“åº”èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç´«å¤–&#x2F;å…‰å­¦æœ›è¿œé•œï¼ˆUVOTï¼‰å’ŒXå°„çº¿æœ›è¿œé•œï¼ˆXRTï¼‰ã€‚é€šè¿‡æ¨¡æ‹Ÿæ–¯å¨å¤«ç‰¹å¯¹ä¸åŒç­–ç•¥çš„å“åº”ï¼Œç»“åˆæ¨¡å‹æ˜Ÿå›¾ä¸2MPZç›®å½•ï¼Œç”Ÿæˆè§‚æµ‹åœºæ¸…å•ï¼Œå¹¶æ¨ç®—æ–¯å¨å¤«ç‰¹åˆ°è¾¾æ­£ç¡®è§‚æµ‹åœºçš„æ—¶é—´ã€‚åŒæ—¶æ¨¡æ‹Ÿä»ªå™¨å¯¹æ¨¡æ‹Ÿçš„åƒæ–°æ˜Ÿå’ŒçŸ­ä¼½é©¬å°„çº¿æš´ä½™è¾‰çš„å“åº”ã€‚ç ”ç©¶å‘ç°ï¼ŒUVOTä½¿ç”¨uæ»¤é•œï¼Œæ›å…‰æ—¶é—´çº¦120ç§’ï¼Œæœ€é€‚åˆå¤§å¤šæ•°åç»­è§‚æµ‹ï¼Œåœ¨O4æœŸé—´ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæ£€æµ‹åˆ°LVKå¯æ£€æµ‹åˆ°çš„åŒä¸­å­æ˜Ÿè§¦å‘äº‹ä»¶çš„çº¦6%ã€‚æˆ‘ä»¬å‘ç°å¼•åŠ›æ³¢çš„90%è¯¯å·®åŒºåŸŸå’Œè§¦å‘æºçš„è·ç¦»æœ‰åŠ©äºæˆ‘ä»¬é€‰æ‹©æœ€ä½³è§¦å‘äº‹ä»¶è¿›è¡Œåç»­è§‚æµ‹ã€‚èšç„¦è·ç¦»å°äº300Mpcæˆ–è¯¯å·®é¢ç§¯å°äºå‡ ç™¾å¹³æ–¹åº¦æ—¶è·ç¦»å¤§äºä»¥å‰å‡è®¾çš„æ¥æºï¼Œæä¾›æœ€ä½³çš„å‘ç°æœºä¼šã€‚æ–¯å¨å¤«ç‰¹æœ‰çº¦5-30%çš„è§¦å‘äº‹ä»¶æ£€æµ‹æ¦‚ç‡å¤§äºæˆ–ç­‰äº0.5ã€‚å³ä½¿åœ¨æ›´å¤§çš„è·ç¦»ä¸Šï¼Œé€šè¿‡é‡‡ç”¨æ›´é•¿çš„250ç§’æˆ–500ç§’æ›å…‰æ—¶é—´ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–åç»­è§‚æµ‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LIGOã€Virgoå’ŒKAGRAå¼•åŠ›æ³¢è§‚æµ‹ç«™æ­£åœ¨è¿›è¡ŒO4è§‚æµ‹è¿è¡Œï¼Œç›®æ ‡æ˜¯å‘ç°æ–°çš„ç”µç£å¯¹åº”ä½“ã€‚</li>
<li>ç ”ç©¶äº†Swiftå¤©æ–‡å°å¯¹å¼•åŠ›æ³¢è§¦å‘äº‹ä»¶çš„å“åº”èƒ½åŠ›ï¼ŒåŒ…æ‹¬UVOTå’ŒXRTçš„å“åº”ã€‚</li>
<li>UVOTä½¿ç”¨uæ»¤é•œï¼Œæ›å…‰æ—¶é—´çº¦120ç§’ï¼Œæœ€é€‚åˆè¿›è¡Œåç»­è§‚æµ‹ã€‚</li>
<li>åœ¨O4æœŸé—´ï¼Œé¢„è®¡èƒ½æ£€æµ‹åˆ°çº¦6%çš„LVKå¯æ£€æµ‹åˆ°çš„åŒä¸­å­æ˜Ÿè§¦å‘äº‹ä»¶ã€‚</li>
<li>å¼•åŠ›æ³¢çš„90%è¯¯å·®åŒºåŸŸå’Œè§¦å‘æºçš„è·ç¦»æ˜¯é€‰æ‹©æœ€ä½³è§¦å‘äº‹ä»¶è¿›è¡Œåç»­è§‚æµ‹çš„å…³é”®å› ç´ ã€‚</li>
<li>æœ€ä½³å‘ç°æœºä¼šå‡ºç°åœ¨è·ç¦»å°äº300Mpcæˆ–ç‰¹å®šè¯¯å·®åŒºåŸŸçš„æ¥æºï¼Œå¹¶è€ƒè™‘æ›´é•¿çš„æ›å…‰æ—¶é—´ä»¥æé«˜æ£€æµ‹æ¦‚ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5bc4a7d0a8cd017b32a20ffd838bfb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a4b44397040ca4f8004ddf58de89e5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b059f615812be3bd0bf7a467ff96cee9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90c4771e04eb1f33bc28f25110819e23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41b466f7f2147a4b3df3ee9fecda216.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Day-Night-Adaptation-An-Innovative-Source-free-Adaptation-Framework-for-Medical-Image-Segmentation"><a href="#Day-Night-Adaptation-An-Innovative-Source-free-Adaptation-Framework-for-Medical-Image-Segmentation" class="headerlink" title="Day-Night Adaptation: An Innovative Source-free Adaptation Framework for   Medical Image Segmentation"></a>Day-Night Adaptation: An Innovative Source-free Adaptation Framework for   Medical Image Segmentation</h2><p><strong>Authors:Ziyang Chen, Yiwen Ye, Yongsheng Pan, Jingfeng Zhang, Yanning Zhang, Yong Xia</strong></p>
<p>Distribution shifts widely exist in medical images acquired from different medical centres, hindering the deployment of semantic segmentation models trained on one centre (source domain) to another (target domain). While unsupervised domain adaptation has shown significant promise in mitigating these shifts, it poses privacy risks due to sharing data between centres. To facilitate adaptation while preserving data privacy, source-free domain adaptation (SFDA) and test-time adaptation (TTA) have emerged as effective paradigms, relying solely on target domain data. However, SFDA requires a pre-collected target domain dataset before deployment. TTA insufficiently exploit the potential value of test data, as it processes the test data only once. Considering that most medical centres operate during the day and remain inactive at night in clinical practice, we propose a novel adaptation framework called Day-Night Adaptation (DyNA) with above insights, which performs adaptation through day-night cycles without requiring access to source data. During the day, a low-frequency prompt is trained to adapt the frozen model to each test sample. We construct a memory bank for prompt initialization and develop a warm-up mechanism to enhance prompt training. During the night, we reuse test data collected from the day and introduce a global student model to bridge the knowledge between teacher and student models, facilitating model fine-tuning while ensuring training stability. Extensive experiments demonstrate that our DyNA outperforms existing TTA and SFDA methods on two benchmark medical image segmentation tasks. Code will be available after the paper is published. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåœ¨ä¸åŒåŒ»å­¦ä¸­å¿ƒé‡‡é›†æ—¶å­˜åœ¨å¹¿æ³›çš„åˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œè¿™é˜»ç¢äº†åœ¨ä¸€ä¸ªä¸­å¿ƒï¼ˆæºåŸŸï¼‰è®­ç»ƒçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹åœ¨å¦ä¸€ä¸ªä¸­å¿ƒï¼ˆç›®æ ‡åŸŸï¼‰çš„åº”ç”¨éƒ¨ç½²ã€‚å°½ç®¡æ— ç›‘ç£åŸŸè‡ªé€‚åº”åœ¨ç¼“è§£è¿™äº›è½¬ç§»æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºéœ€è¦åœ¨ä¸­å¿ƒä¹‹é—´å…±äº«æ•°æ®ï¼Œå› æ­¤å­˜åœ¨éšç§é£é™©ã€‚ä¸ºäº†åœ¨é€‚åº”è¿‡ç¨‹ä¸­ä¿æŠ¤æ•°æ®éšç§ï¼Œå‡ºç°äº†æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰å’Œæµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰è¿™ä¸¤ç§æœ‰æ•ˆçš„èŒƒå¼ï¼Œå®ƒä»¬ä»…ä¾èµ–äºç›®æ ‡åŸŸæ•°æ®ã€‚ç„¶è€Œï¼ŒSFDAéœ€è¦åœ¨éƒ¨ç½²å‰é¢„å…ˆæ”¶é›†ç›®æ ‡åŸŸæ•°æ®é›†ã€‚è€ŒTTAæœªèƒ½å……åˆ†åˆ©ç”¨æµ‹è¯•æ•°æ®çš„æ½œåœ¨ä»·å€¼ï¼Œå› ä¸ºå®ƒä»…å¤„ç†ä¸€æ¬¡æµ‹è¯•æ•°æ®ã€‚è€ƒè™‘åˆ°å¤§å¤šæ•°åŒ»å­¦ä¸­å¿ƒçš„æ—¥å¸¸è¿è¥æƒ…å†µï¼Œå³åœ¨ç™½å¤©è¿è¡Œè€Œåœ¨å¤œé—´ä¸´åºŠå®è·µä¸­å¤„äºéæ´»è·ƒçŠ¶æ€ï¼Œæˆ‘ä»¬ç»“åˆä¸Šè¿°è§è§£æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæ˜¼å¤œé€‚åº”â€ï¼ˆDyNAï¼‰çš„æ–°å‹è‡ªé€‚åº”æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¼å¤œå¾ªç¯è¿›è¡Œè‡ªé€‚åº”ï¼Œæ— éœ€è®¿é—®æºæ•°æ®ã€‚åœ¨ç™½å¤©ï¼Œæˆ‘ä»¬è®­ç»ƒä½é¢‘æç¤ºæ¥é€‚åº”æ¯ä¸ªæµ‹è¯•æ ·æœ¬çš„å†»ç»“æ¨¡å‹ã€‚æˆ‘ä»¬æ„å»ºäº†ç”¨äºæç¤ºåˆå§‹åŒ–çš„å†…å­˜é“¶è¡Œï¼Œå¹¶å¼€å‘äº†ä¸€ç§é¢„çƒ­æœºåˆ¶ä»¥å¢å¼ºæç¤ºè®­ç»ƒã€‚åœ¨å¤œé—´ï¼Œæˆ‘ä»¬é‡å¤ä½¿ç”¨ç™½å¤©æ”¶é›†çš„æµ‹è¯•æ•°æ®ï¼Œå¹¶å¼•å…¥å…¨å±€å­¦ç”Ÿæ¨¡å‹æ¥æ¡¥æ¥æ•™å¸ˆå’Œå­¦å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„çŸ¥è¯†ï¼Œä»è€Œå¸®åŠ©è¿›è¡Œæ¨¡å‹å¾®è°ƒå¹¶ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DyNAåœ¨ä¸¤é¡¹åŸºå‡†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„TTAå’ŒSFDAæ–¹æ³•ã€‚ä»£ç å°†åœ¨è®ºæ–‡å‘è¡¨åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13472v2">PDF</a> 16 pages, 4 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨åŒ»å­¦å›¾åƒåœ¨ä¸åŒåŒ»å­¦ä¸­å¿ƒé—´çš„åˆ†å¸ƒå·®å¼‚å¯¼è‡´çš„é¢†åŸŸé€‚åº”æ€§é—®é¢˜ã€‚é’ˆå¯¹æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•å­˜åœ¨çš„æ•°æ®å…±äº«éšç§é—®é¢˜ï¼Œæå‡ºäº†æºæ— å…³é¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰å’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰ä¸¤å¤§æœ‰æ•ˆæ¨¡å¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç»“åˆåŒ»å­¦ä¸­å¿ƒçš„æ—¥å¸¸è¿è¥ç‰¹ç‚¹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é¢†åŸŸé€‚åº”æ¡†æ¶â€”â€”Day-Night Adaptationï¼ˆDyNAï¼‰ï¼Œå®ƒé€šè¿‡æ—¥å¤œå‘¨æœŸè¿›è¡Œæ¨¡å‹é€‚åº”ï¼Œæ— éœ€è®¿é—®æºæ•°æ®ã€‚è¯¥æ¡†æ¶åœ¨æ—¥é—´é‡‡ç”¨ä½é¢‘æç¤ºå¯¹å†»ç»“æ¨¡å‹è¿›è¡Œé€‚åº”è®­ç»ƒï¼Œå¹¶å»ºç«‹æç¤ºåˆå§‹åŒ–è®°å¿†åº“å’Œé¢„çƒ­æœºåˆ¶æ¥å¢å¼ºæç¤ºè®­ç»ƒæ•ˆæœã€‚å¤œé—´åˆ™åˆ©ç”¨æ—¥é—´æ”¶é›†çš„æµ‹è¯•æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¹¶å¼•å…¥å…¨å±€å­¦ç”Ÿæ¨¡å‹ä»¥æ¡¥æ¥æ•™å¸ˆå’Œå­¦ç”Ÿçš„çŸ¥è¯†ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒDyNAåœ¨ä¸¤é¡¹åŸºå‡†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„TTAå’ŒSFDAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåœ¨ä¸åŒåŒ»å­¦ä¸­å¿ƒé—´å­˜åœ¨åˆ†å¸ƒå·®å¼‚ï¼Œå½±å“æ¨¡å‹åº”ç”¨ã€‚</li>
<li>æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•è™½æœ‰åŠ©äºç¼“è§£åˆ†å¸ƒå·®å¼‚ï¼Œä½†å­˜åœ¨æ•°æ®å…±äº«éšç§é—®é¢˜ã€‚</li>
<li>æºæ— å…³é¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰å’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆTTAï¼‰æ˜¯ä¸¤ç§æœ‰æ•ˆä¿æŠ¤éšç§çš„é€‚åº”æ¨¡å¼ã€‚</li>
<li>æå‡ºçš„Day-Night Adaptationï¼ˆDyNAï¼‰æ¡†æ¶ç»“åˆåŒ»å­¦ä¸­å¿ƒçš„æ—¥å¸¸è¿è¥ç‰¹ç‚¹ï¼Œé€šè¿‡æ—¥å¤œå‘¨æœŸè¿›è¡Œæ¨¡å‹é€‚åº”ï¼Œæ— éœ€è®¿é—®æºæ•°æ®ã€‚</li>
<li>DyNAé‡‡ç”¨ä½é¢‘æç¤ºè¿›è¡Œæ—¥é—´æ¨¡å‹é€‚åº”è®­ç»ƒï¼Œå¹¶å»ºç«‹æç¤ºåˆå§‹åŒ–è®°å¿†åº“å’Œé¢„çƒ­æœºåˆ¶ã€‚</li>
<li>å¤œé—´åˆ©ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¼•å…¥å…¨å±€å­¦ç”Ÿæ¨¡å‹ä»¥ç¡®ä¿è®­ç»ƒç¨³å®šæ€§å’ŒçŸ¥è¯†æ¡¥æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebae9d6db2660fd2a06144342c1f3efa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43c19a48fe1669dca4a3edb5c4c851b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c503e552d27cbc8d848e10c16eaee8bb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Invertible-ResNets-for-Inverse-Imaging-Problems-Competitive-Performance-with-Provable-Regularization-Properties"><a href="#Invertible-ResNets-for-Inverse-Imaging-Problems-Competitive-Performance-with-Provable-Regularization-Properties" class="headerlink" title="Invertible ResNets for Inverse Imaging Problems: Competitive Performance   with Provable Regularization Properties"></a>Invertible ResNets for Inverse Imaging Problems: Competitive Performance   with Provable Regularization Properties</h2><p><strong>Authors:Clemens Arndt, Judith Nickel</strong></p>
<p>Learning-based methods have demonstrated remarkable performance in solving inverse problems, particularly in image reconstruction tasks. Despite their success, these approaches often lack theoretical guarantees, which are crucial in sensitive applications such as medical imaging. Recent works by Arndt et al (2023 Inverse Problems 39 125018, 2024 Inverse Problems 40 045021) addressed this gap by analyzing a data-driven reconstruction method based on invertible residual networks (iResNets). They revealed that, under reasonable assumptions, this approach constitutes a convergent regularization scheme. However, the performance of the reconstruction method was only validated on academic toy problems and small-scale iResNet architectures. In this work, we address this gap by evaluating the performance of iResNets on two real-world imaging tasks: a linear blurring operator and a nonlinear diffusion operator. To do so, we extend some of the theoretical results from Arndt et al to encompass nonlinear inverse problems and offer insights for the design of large-scale performant iResNet architectures. Through numerical experiments, we compare the performance of our iResNet models against state-of-the-art neural networks, confirming their efficacy. Additionally, we numerically investigate the theoretical guarantees of this approach and demonstrate how the invertibility of the network enables a deeper analysis of the learned forward operator and its learned regularization. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨è§£å†³åé—®é¢˜æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹ç†è®ºä¿è¯ï¼Œè¿™åœ¨åŒ»ç–—æˆåƒç­‰æ•æ„Ÿåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚Arndtç­‰äººè¿‘æœŸçš„å·¥ä½œï¼ˆ2023å¹´é€†å‘é—®é¢˜39 125018ï¼›2024å¹´é€†å‘é—®é¢˜40 045021ï¼‰é€šè¿‡åˆ†æåŸºäºå¯é€†æ®‹å·®ç½‘ç»œï¼ˆiResNetsï¼‰çš„æ•°æ®é©±åŠ¨é‡å»ºæ–¹æ³•ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚ä»–ä»¬è¯æ˜ï¼Œåœ¨åˆç†çš„å‡è®¾ä¸‹ï¼Œè¿™ç§æ–¹æ³•æ„æˆäº†ä¸€ç§æ”¶æ•›çš„æ­£åˆ™åŒ–æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œé‡å»ºæ–¹æ³•çš„æ€§èƒ½ä»…åœ¨å­¦æœ¯ç©å…·é—®é¢˜å’Œå°å‹iResNetæ¶æ„ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°iResNetsåœ¨ç°å®ä¸–ç•Œæˆåƒä»»åŠ¡ï¼ˆçº¿æ€§æ¨¡ç³Šç®—å­å’Œéçº¿æ€§æ‰©æ•£ç®—å­ï¼‰ä¸Šçš„æ€§èƒ½æ¥è§£å†³è¿™ä¸€å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ‰©å±•äº†Arndtç­‰äººçš„ä¸€äº›ç†è®ºç»“æœï¼Œä»¥æ¶µç›–éçº¿æ€§åé—®é¢˜ï¼Œå¹¶ä¸ºè®¾è®¡å¤§è§„æ¨¡é«˜æ€§èƒ½iResNetæ¶æ„æä¾›äº†è§è§£ã€‚é€šè¿‡æ•°å€¼å®éªŒï¼Œæˆ‘ä»¬å°†iResNetæ¨¡å‹çš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„ç¥ç»ç½‘ç»œè¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»æ•°å€¼ä¸Šç ”ç©¶äº†è¯¥æ–¹æ³•çš„ç†è®ºä¿è¯ï¼Œå¹¶å±•ç¤ºäº†ç½‘ç»œçš„å¯é€†æ€§å¦‚ä½•ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´æ·±å…¥åœ°åˆ†æå­¦ä¹ åˆ°çš„æ­£å‘ç®—å­å’Œå…¶å­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13482v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨è§£å†³åé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ç¼ºä¹ç†è®ºä¿è¯ï¼Œè¿™åœ¨åŒ»ç–—æˆåƒç­‰æ•æ„Ÿåº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ†æäº†åŸºäºå¯é€†æ®‹å·®ç½‘ç»œï¼ˆiResNetsï¼‰çš„æ•°æ®é©±åŠ¨é‡å»ºæ–¹æ³•ï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨åˆç†å‡è®¾ä¸‹æ„æˆæ”¶æ•›æ­£åˆ™åŒ–æ–¹æ¡ˆçš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œè¯¥é‡å»ºæ–¹æ³•çš„æ€§èƒ½ä»…åœ¨å­¦æœ¯ç©å…·é—®é¢˜å’Œå°è§„æ¨¡iResNetæ¶æ„ä¸Šå¾—åˆ°éªŒè¯ã€‚æœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°iResNetsåœ¨çœŸå®ä¸–ç•Œæˆåƒä»»åŠ¡ï¼ˆçº¿æ€§æ¨¡ç³Šç®—å­å’Œéçº¿æ€§æ‰©æ•£ç®—å­ï¼‰ä¸Šçš„æ€§èƒ½æ¥å¼¥è¡¥è¿™ä¸€ä¸è¶³ã€‚ç ”ç©¶æ‰©å±•äº†ä¹‹å‰çš„ç†è®ºç»“æœï¼Œæ¶µç›–äº†éçº¿æ€§åé—®é¢˜ï¼Œå¹¶ä¸ºè®¾è®¡é«˜æ€§èƒ½çš„å¤§è§„æ¨¡iResNetæ¶æ„æä¾›äº†è§è§£ã€‚æ•°å€¼å®éªŒè¯å®iResNetæ¨¡å‹ä¸æœ€æ–°ç¥ç»ç½‘ç»œç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶å¯¹ç½‘ç»œçš„å¯é€†æ€§å¦‚ä½•ä¿ƒè¿›å¯¹æ‰€å­¦å‰å‘ç®—å­å’Œæ­£åˆ™åŒ–çš„æ·±å…¥åˆ†æè¿›è¡Œäº†æ•°å€¼æ¢è®¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ æ–¹æ³•åœ¨å›¾åƒé‡å»ºä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>å¯¹äºæ•æ„Ÿåº”ç”¨å¦‚åŒ»ç–—æˆåƒï¼Œç†è®ºä¿è¯è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºå¯é€†æ®‹å·®ç½‘ç»œï¼ˆiResNetsï¼‰çš„æ•°æ®é©±åŠ¨é‡å»ºæ–¹æ³•æ„æˆæ”¶æ•›æ­£åˆ™åŒ–æ–¹æ¡ˆã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶ä»…åœ¨å­¦æœ¯ç©å…·é—®é¢˜å’Œå°è§„æ¨¡æ¶æ„ä¸ŠéªŒè¯äº†iResNetsçš„æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†iResNetsåœ¨çœŸå®ä¸–ç•Œæˆåƒä»»åŠ¡ï¼ˆå¦‚çº¿æ€§æ¨¡ç³Šå’Œéçº¿æ€§æ‰©æ•£ï¼‰ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ‰©å±•äº†ç†è®ºç»“æœä»¥æ¶µç›–éçº¿æ€§åé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a2348daa4d9a60257d27b09b7edc85a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MGH-Radiology-Llama-A-Llama-3-70B-Model-for-Radiology"><a href="#MGH-Radiology-Llama-A-Llama-3-70B-Model-for-Radiology" class="headerlink" title="MGH Radiology Llama: A Llama 3 70B Model for Radiology"></a>MGH Radiology Llama: A Llama 3 70B Model for Radiology</h2><p><strong>Authors:Yucheng Shi, Peng Shu, Zhengliang Liu, Zihao Wu, Quanzheng Li, Tianming Liu, Ninghao Liu, Xiang Li</strong></p>
<p>In recent years, the field of radiology has increasingly harnessed the power of artificial intelligence (AI) to enhance diagnostic accuracy, streamline workflows, and improve patient care. Large language models (LLMs) have emerged as particularly promising tools, offering significant potential in assisting radiologists with report generation, clinical decision support, and patient communication. This paper presents an advanced radiology-focused large language model: MGH Radiology Llama. It is developed using the Llama 3 70B model, building upon previous domain-specific models like Radiology-GPT and Radiology-Llama2. Leveraging a unique and comprehensive dataset from Massachusetts General Hospital, comprising over 6.5 million de-identified medical reports across various imaging modalities, the model demonstrates significant improvements in generating accurate and clinically relevant radiology impressions given the corresponding findings. Our evaluation, incorporating both traditional metrics and a GPT-4-based assessment, highlights the enhanced performance of this work over general-purpose LLMs. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ”¾å°„å­¦é¢†åŸŸè¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„åŠ›é‡ï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€ä¼˜åŒ–å·¥ä½œæµç¨‹å’Œæ”¹å–„æ‚£è€…æŠ¤ç†ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºç‰¹åˆ«æœ‰å‰æ™¯çš„å·¥å…·ï¼Œåœ¨è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿç”ŸæˆæŠ¥å‘Šã€ä¸´åºŠå†³ç­–æ”¯æŒå’Œæ‚£è€…æ²Ÿé€šæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾ä¸“æ³¨äºæ”¾å°„å­¦çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼šMGHæ”¾å°„å­¦æ‹‰é©¬æ¨¡å‹ã€‚å®ƒæ˜¯ä»¥æ‹‰é©¬3 70Bæ¨¡å‹ä¸ºåŸºç¡€å¼€å‘çš„ï¼Œå»ºç«‹åœ¨ä¹‹å‰çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ä¸Šï¼Œå¦‚æ”¾å°„å­¦-GPTå’Œæ”¾å°„å­¦æ‹‰é©¬2ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ¥è‡ªéº»çœæ€»åŒ»é™¢ï¼ˆMassachusetts General Hospitalï¼‰çš„ç‹¬ç‰¹ä¸”å…¨é¢çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡650ä¸‡ä»½å»æ ‡è¯†åŒ–çš„åŒ»å­¦æŠ¥å‘Šï¼Œæ¶µç›–å„ç§æˆåƒæ¨¡å¼ï¼Œæ ¹æ®ç›¸åº”çš„æ£€æŸ¥ç»“æœç”Ÿæˆå‡†ç¡®ä¸”ä¸´åºŠç›¸å…³çš„æ”¾å°„å­¦å°è±¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“åˆäº†ä¼ ç»ŸæŒ‡æ ‡å’ŒåŸºäºGPT-4çš„è¯„ä¼°ï¼Œçªå‡ºäº†è¿™é¡¹å·¥ä½œç›¸è¾ƒäºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11848v2">PDF</a> 11 pages, 3 figures, 1 table</p>
<p><strong>Summary</strong><br>åŒ»å­¦é¢†åŸŸè¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€ä¼˜åŒ–å·¥ä½œæµç¨‹å’Œæ”¹å–„æ‚£è€…æŠ¤ç†ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾å…ˆè¿›çš„é’ˆå¯¹æ”¾å°„ç§‘çš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼šMGHæ”¾å°„ç§‘æ‹‰ç›ï¼ˆLlamaï¼‰ã€‚å®ƒåˆ©ç”¨é©¬è¨è¯¸å¡å·ç»¼åˆåŒ»é™¢çš„å¤§è§„æ¨¡è„±å¯†åŒ»ç–—æŠ¥å‘Šæ•°æ®é›†ï¼Œåœ¨å„ç§æˆåƒæ¨¡å¼ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œèƒ½ç”Ÿæˆå‡†ç¡®ä¸”ä¸´åºŠç›¸å…³çš„æ”¾å°„å­¦å°è±¡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦é¢†åŸŸæ—¥ç›Šä¾èµ–äººå·¥æ™ºèƒ½æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€ä¼˜åŒ–å·¥ä½œæµç¨‹å’Œæ”¹å–„æ‚£è€…æŠ¤ç†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ”¾å°„ç§‘é¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œå¯è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡ŒæŠ¥å‘Šç”Ÿæˆã€ä¸´åºŠå†³ç­–æ”¯æŒå’Œæ‚£è€…æ²Ÿé€šã€‚</li>
<li>MGHæ”¾å°„ç§‘æ‹‰ç›æ¨¡å‹æ˜¯åŸºäºLlama 3 70Bæ¨¡å‹å¼€å‘çš„å…ˆè¿›æ”¾å°„ç§‘å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨é©¬è¨è¯¸å¡å·ç»¼åˆåŒ»é™¢çš„ç»¼åˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡650ä¸‡ä»½è„±å¯†åŒ»ç–—æŠ¥å‘Šã€‚</li>
<li>æ¨¡å‹èƒ½ç”Ÿæˆå‡†ç¡®ä¸”ä¸´åºŠç›¸å…³çš„æ”¾å°„å­¦å°è±¡ï¼Œåœ¨å¤šç§æˆåƒæ¨¡å¼ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯„ä¼°ç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2ad20d213407866c76b9f4ef483a904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3f48f5e093a4f49f02a9ca3b776b6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2651f57afb983d98fe1adfde36efc33.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Gradient-Alignment-Improves-Test-Time-Adaptation-for-Medical-Image-Segmentation"><a href="#Gradient-Alignment-Improves-Test-Time-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="Gradient Alignment Improves Test-Time Adaptation for Medical Image   Segmentation"></a>Gradient Alignment Improves Test-Time Adaptation for Medical Image   Segmentation</h2><p><strong>Authors:Ziyang Chen, Yiwen Ye, Yongsheng Pan, Yong Xia</strong></p>
<p>Although recent years have witnessed significant advancements in medical image segmentation, the pervasive issue of domain shift among medical images from diverse centres hinders the effective deployment of pre-trained models. Many Test-time Adaptation (TTA) methods have been proposed to address this issue by fine-tuning pre-trained models with test data during inference. These methods, however, often suffer from less-satisfactory optimization due to suboptimal optimization direction (dictated by the gradient) and fixed step-size (predicated on the learning rate). In this paper, we propose the Gradient alignment-based Test-time adaptation (GraTa) method to improve both the gradient direction and learning rate in the optimization procedure. Unlike conventional TTA methods, which primarily optimize the pseudo gradient derived from a self-supervised objective, our method incorporates an auxiliary gradient with the pseudo one to facilitate gradient alignment. Such gradient alignment enables the model to excavate the similarities between different gradients and correct the gradient direction to approximate the empirical gradient related to the current segmentation task. Additionally, we design a dynamic learning rate based on the cosine similarity between the pseudo and auxiliary gradients, thereby empowering the adaptive fine-tuning of pre-trained models on diverse test data. Extensive experiments establish the effectiveness of the proposed gradient alignment and dynamic learning rate and substantiate the superiority of our GraTa method over other state-of-the-art TTA methods on a benchmark medical image segmentation task. The code and weights of pre-trained source models are available at <a target="_blank" rel="noopener" href="https://github.com/Chen-Ziyang/GraTa">https://github.com/Chen-Ziyang/GraTa</a>. </p>
<blockquote>
<p>å°½ç®¡è¿‘å¹´æ¥åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æ¥è‡ªä¸åŒä¸­å¿ƒçš„åŒ»å­¦å›¾åƒå­˜åœ¨çš„é¢†åŸŸåç§»é—®é¢˜ä»ç„¶é˜»ç¢ç€é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆéƒ¨ç½²ã€‚è®¸å¤šæµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•å·²è¢«æå‡ºï¼Œé€šè¿‡æ¨ç†è¿‡ç¨‹ä¸­çš„æµ‹è¯•æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ç”±äºä¼˜åŒ–æ–¹å‘ä¸ä½³ï¼ˆç”±æ¢¯åº¦å†³å®šï¼‰å’Œå›ºå®šæ­¥é•¿ï¼ˆåŸºäºå­¦ä¹ ç‡ï¼‰è€Œä¼˜åŒ–æ•ˆæœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¢¯åº¦å¯¹é½çš„æµ‹è¯•æ—¶é€‚åº”ï¼ˆGraTaï¼‰æ–¹æ³•ï¼Œä»¥æ”¹è¿›ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ–¹å‘å’Œå­¦ä¹ ç‡ã€‚ä¸åŒäºä¸»è¦ä¼˜åŒ–è‡ªç›‘ç£ç›®æ ‡æ‰€æ´¾ç”Ÿå‡ºçš„ä¼ªæ¢¯åº¦çš„ä¼ ç»ŸTTAæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¾…åŠ©æ¢¯åº¦å’Œä¼ªæ¢¯åº¦ï¼Œä»¥å®ç°æ¢¯åº¦å¯¹é½ã€‚è¿™ç§æ¢¯åº¦å¯¹é½ä½¿æ¨¡å‹èƒ½å¤ŸæŒ–æ˜ä¸åŒæ¢¯åº¦ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶çº æ­£æ¢¯åº¦æ–¹å‘ä»¥é€¼è¿‘ä¸å½“å‰åˆ†å‰²ä»»åŠ¡ç›¸å…³çš„ç»éªŒæ¢¯åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§è®¾è®¡äº†ä¸€ç§åŠ¨æ€å­¦ä¹ ç‡ï¼Œä»è€Œå®ç°å¯¹ä¸åŒæµ‹è¯•æ•°æ®çš„é¢„è®­ç»ƒæ¨¡å‹çš„è‡ªé€‚åº”å¾®è°ƒã€‚å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„æ¢¯åº¦å¯¹é½å’ŒåŠ¨æ€å­¦ä¹ ç‡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯å®æˆ‘ä»¬çš„GraTaæ–¹æ³•åœ¨åŸºå‡†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„TTAæ–¹æ³•ã€‚é¢„è®­ç»ƒæ¨¡å‹çš„ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chen-Ziyang/GraTa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Chen-Ziyang/GraTaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07343v3">PDF</a> 9 pages, 3 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸä¸­çš„åŸŸè¿ç§»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦å¯¹é½çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆGraTaï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦æ¥å®ç°æ¢¯åº¦å¯¹é½ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„åŠ¨æ€å­¦ä¹ ç‡ï¼Œä»¥è‡ªé€‚åº”å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„TTAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå­˜åœ¨åŸŸè¿ç§»é—®é¢˜ï¼Œå½±å“é¢„è®­ç»ƒæ¨¡å‹çš„æœ‰æ•ˆéƒ¨ç½²ã€‚</li>
<li>æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•æ—¨åœ¨é€šè¿‡æµ‹è¯•æ•°æ®åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ç°æœ‰TTAæ–¹æ³•å¸¸å¸¸å› ä¼˜åŒ–æ–¹å‘ä¸ä½³å’Œå›ºå®šå­¦ä¹ ç‡è€Œä¼˜åŒ–æ•ˆæœä¸ä½³ã€‚</li>
<li>GraTaæ–¹æ³•ç»“åˆä¼ªæ¢¯åº¦å’Œè¾…åŠ©æ¢¯åº¦å®ç°æ¢¯åº¦å¯¹é½ï¼Œä¼˜åŒ–æ¢¯åº¦æ–¹å‘å’Œå­¦ä¹ ç‡ã€‚</li>
<li>GraTaæ–¹æ³•é€šè¿‡æŒ–æ˜ä¸åŒæ¢¯åº¦ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ ¡æ­£æ¢¯åº¦æ–¹å‘ä»¥é€¼è¿‘å½“å‰åˆ†å‰²ä»»åŠ¡çš„å®è¯æ¢¯åº¦ã€‚</li>
<li>GraTaæ–¹æ³•è®¾è®¡äº†ä¸€ç§åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„åŠ¨æ€å­¦ä¹ ç‡ï¼Œå®ç°è‡ªé€‚åº”å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1437880ab243f5c2203623d4186c92a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66585d65f45b0a424ab506a21c8272c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2844a49c7f034c3abe0063a8b0cf3fe2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff17b93d0cb6d0511f9e7bc9e89f35a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac55ee628ebc02b7160d45e75adbd28b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd382156255b7de8954ae706570c7ea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc87bf15b8f4d802685797e858e08003.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Synthesis-and-characterization-of-the-novel-breathing-pyrochlore-compound-Ba3Tm2Zn5O11"><a href="#Synthesis-and-characterization-of-the-novel-breathing-pyrochlore-compound-Ba3Tm2Zn5O11" class="headerlink" title="Synthesis and characterization of the novel breathing pyrochlore   compound Ba3Tm2Zn5O11"></a>Synthesis and characterization of the novel breathing pyrochlore   compound Ba3Tm2Zn5O11</h2><p><strong>Authors:Lalit Yadav, Rabindranath Bag, Ramesh Dhakal, Stephen M. Winter, Jeffrey G. Rau, Sachith E. Dissanayake, Alexander I. Kolesnikov, Andrey A. Podlesnyak, Craig M. Brown, Nicholas P. Butch, David Graf, Michel J. P. Gingras, Sara Haravifard</strong></p>
<p>In this study, a novel material from the rare-earth based breathing pyrochlore family, Ba3Tm2Zn5O11, was successfully synthesized. Powder x-ray diffraction and high-resolution powder neutron diffraction confirmed phase purity and the F-43m breathing pyrochlore crystal structure, while thermogravimetric analysis revealed incongruent melting behavior compared to its counterpart, Ba3Yb2Zn5O11. High-quality single crystals of Ba3Tm2Zn5O11 were grown using the traveling solvent floating zone technique and assessed using Laue x-ray diffraction and single crystal x-ray diffraction. Thermodynamic characterization indicated paramagnetic behavior down to 0.05 K, and inelastic neutron scattering measurements identified distinct dispersionless crystal electric field energy bands, with the fitted crystal electric field model predicting a single-ion singlet ground state and an energy gap of ~9 meV separating it from the first excited (singlet) state. Additional low-energy excitation studies on single crystals revealed dispersionless bands at 0.8 and 1 meV. Computed phonon dispersions from first-principles calculations ruled out phonons as the origin of these modes, further illustrating the puzzling and unique properties of Ba3Tm2Zn5O11. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼ŒæˆåŠŸåˆæˆäº†ä¸€ç§æ¥è‡ªç¨€åœŸåŸºå‘¼å¸å‹çƒ§ç»¿çŸ³å®¶æ—çš„æ–°å‹ææ–™Ba3Tm2Zn5O11ã€‚é€šè¿‡ç²‰æœ«Xå°„çº¿è¡å°„å’Œé«˜åˆ†è¾¨ç‡ç²‰æœ«ä¸­å­è¡å°„è¯å®äº†å…¶ç›¸çº¯åº¦å’ŒF-43må‘¼å¸å‹çƒ§ç»¿çŸ³æ™¶ä½“ç»“æ„ã€‚çƒ­é‡åˆ†ææ˜¾ç¤ºå…¶ç†”èè¡Œä¸ºä¸å¦ä¸€ç§ææ–™Ba3Yb2Zn5O11ä¸ä¸€è‡´ã€‚ä½¿ç”¨æµåŠ¨æº¶å‰‚æµ®åŒºæŠ€æœ¯ç”Ÿé•¿äº†Ba3Tm2Zn5O11çš„é«˜è´¨é‡å•æ™¶ï¼Œå¹¶ä½¿ç”¨åŠ³å„Xå°„çº¿è¡å°„å’Œå•æ™¶Xå°„çº¿è¡å°„è¿›è¡Œäº†è¯„ä¼°ã€‚çƒ­åŠ›å­¦è¡¨å¾è¡¨æ˜ï¼Œå…¶è¡¨ç°è‡³0.05Kä»ä¸ºé¡ºç£æ€§è¡Œä¸ºã€‚éå¼¹æ€§ä¸­å­æ•£å°„æµ‹é‡ç¡®å®šäº†æ— æ•£å°„æ™¶ä½“ç”µåœºèƒ½å¸¦çš„ç‰¹å¾ï¼Œæ‹Ÿåˆçš„æ™¶ä½“ç”µåœºæ¨¡å‹é¢„æµ‹äº†å•ç¦»å­åŸºæ€ä¸ºå•é‡æ€ï¼Œä¸ç¬¬ä¸€æ¿€å‘å•é‡æ€ä¹‹é—´å­˜åœ¨çº¦9meVçš„èƒ½éš™ã€‚å¯¹å•æ™¶è¿›è¡Œçš„é¢å¤–ä½èƒ½æ¿€å‘ç ”ç©¶è¡¨æ˜ï¼Œåœ¨0.8å’Œ1meVå¤„å­˜åœ¨æ— æ•£å°„å¸¦ã€‚åŸºäºç¬¬ä¸€åŸç†è®¡ç®—å¾—å‡ºçš„å£°å­æ•£å°„æ’é™¤äº†å£°å­æ˜¯è¿™äº›æ¨¡å¼çš„èµ·æºï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº†Ba3Tm2Zn5O11ç‹¬ç‰¹ä¸”ä»¤äººå›°æƒ‘çš„ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00222v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ­¤ç ”ç©¶æˆåŠŸåˆæˆäº†ä¸€ç§æ¥è‡ªç¨€åœŸåŸºå‘¼å¸é‡‘é”°çŸ³å®¶æ—çš„æ–°ææ–™Ba3Tm2Zn5O11ã€‚é€šè¿‡ç²‰æœ«Xå°„çº¿è¡å°„å’Œé«˜åˆ†è¾¨ç‡ç²‰æœ«ä¸­å­è¡å°„ç¡®è®¤å…¶ç›¸çº¯åº¦å’ŒF-43må‘¼å¸é‡‘é”°çŸ³æ™¶ä½“ç»“æ„ã€‚çƒ­é‡åˆ†ææ˜¾ç¤ºå…¶ä¸Ba3Yb2Zn5O11ç›¸æ¯”å…·æœ‰ä¸ç›¸èçš„ç†”åŒ–è¡Œä¸ºã€‚åˆ©ç”¨æµ®åŠ¨åŒºç†”æ³•ç”Ÿé•¿é«˜è´¨é‡å•æ™¶ï¼Œå¹¶é€šè¿‡åŠ³å„Xå°„çº¿è¡å°„å’Œå•æ™¶Xå°„çº¿è¡å°„è¿›è¡Œè¯„ä¼°ã€‚çƒ­åŠ›å­¦è¡¨å¾æ˜¾ç¤ºè‡³0.05Kè¡¨ç°å‡ºé¡ºç£æ€§è¡Œä¸ºï¼Œéå¼¹æ€§ä¸­å­æ•£å°„æµ‹é‡è¯†åˆ«å‡ºæ— å‘æ•£æ™¶ä½“ç”µåœºèƒ½å¸¦ï¼Œæ‹Ÿåˆæ™¶ä½“ç”µåœºæ¨¡å‹é¢„æµ‹å•ç¦»å­åŸºæ€å•é‡æ€å’Œçº¦9meVçš„èƒ½éš™ä¸ç¬¬ä¸€æ¿€å‘ï¼ˆå•é‡æ€ï¼‰çŠ¶æ€åˆ†ç¦»ã€‚å•æ™¶çš„ä½èƒ½æ¿€å‘ç ”ç©¶æ˜¾ç¤ºå‡ºåœ¨0.8å’Œ1meVæ—¶çš„æ— å‘æ•£å¸¦ã€‚ç¬¬ä¸€æ€§åŸç†è®¡ç®—å¾—å‡ºçš„å£°å­åˆ†æ•£æ’é™¤äº†å£°å­æ˜¯è¿™äº›æ¨¡å¼çš„èµ·æºï¼Œè¿›ä¸€æ­¥å±•ç¤ºäº†Ba3Tm2Zn5O11ç‹¬ç‰¹ä¸”ç¥ç§˜çš„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æˆåŠŸåˆæˆæ–°å‹ææ–™Ba3Tm2Zn5O11ï¼Œå±äºç¨€åœŸåŸºå‘¼å¸é‡‘é”°çŸ³å®¶æ—ã€‚</li>
<li>é€šè¿‡å¤šç§è¡å°„æŠ€æœ¯ç¡®è®¤äº†å…¶ç›¸çº¯åº¦å’Œæ™¶ä½“ç»“æ„ã€‚</li>
<li>çƒ­é‡åˆ†ææ˜¾ç¤ºè¯¥ææ–™ä¸å¯¹æ¯”ææ–™Ba3Yb2Zn5O11çš„ç†”åŒ–è¡Œä¸ºä¸åŒã€‚</li>
<li>åˆ©ç”¨æ—…è¡Œæº¶å‰‚æµ®åŠ¨åŒºç†”æŠ€æœ¯ç”Ÿé•¿äº†é«˜è´¨é‡å•æ™¶ã€‚</li>
<li>ææ–™è¡¨ç°å‡ºé¡ºç£æ€§è¡Œä¸ºï¼Œä¸”éå¼¹æ€§ä¸­å­æ•£å°„æµ‹é‡æ­ç¤ºäº†ç‰¹å®šçš„æ™¶ä½“ç”µåœºç‰¹å¾ã€‚</li>
<li>å•æ™¶çš„ä½èƒ½æ¿€å‘ç ”ç©¶æ˜¾ç¤ºå­˜åœ¨æ— å‘æ•£å¸¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-03b979aa912f8897281e1c3e4619de4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a2e109bf7e654d05cfc16943880c6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecc0a9ee68f98bc80c4fbd570d67ca6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-619e518cdae67b5552ccc1ea3502a45c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5f9ae38ed96659efe6601bbbec1ca4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad1726dbc7d96b0b371575cae4774429.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Are-Vision-xLSTM-Embedded-UNet-More-Reliable-in-Medical-3D-Image-Segmentation"><a href="#Are-Vision-xLSTM-Embedded-UNet-More-Reliable-in-Medical-3D-Image-Segmentation" class="headerlink" title="Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image   Segmentation?"></a>Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image   Segmentation?</h2><p><strong>Authors:Pallabi Dutta, Soham Bose, Swalpa Kumar Roy, Sushmita Mitra</strong></p>
<p>The development of efficient segmentation strategies for medical images has evolved from its initial dependence on Convolutional Neural Networks (CNNs) to the current investigation of hybrid models that combine CNNs with Vision Transformers. There is an increasing focus on creating architectures that are both high-performance and computationally efficient, able to be deployed on remote systems with limited resources. Although transformers can capture global dependencies in the input space, they face challenges from the corresponding high computational and storage expenses involved. This paper investigates the integration of CNNs with Vision Extended Long Short-Term Memory (Vision-xLSTM)s by introducing the novel {\it \textbf{U-VixLSTM}}.   The Vision-xLSTM blocks capture temporal and global relationships within the patches, as extracted from the CNN feature maps. The convolutional feature reconstruction path upsamples the output volume from the Vision-xLSTM blocks, to produce the segmentation output. Our primary objective is to propose that Vision-xLSTM forms an appropriate backbone for medical image segmentation, offering excellent performance with reduced computational costs. The U-VixLSTM exhibits superior performance, compared to the state-of-the-art networks in the publicly available Synapse, ISIC and ACDC datasets. Code provided: <a target="_blank" rel="noopener" href="https://github.com/duttapallabi2907/U-VixLSTM">https://github.com/duttapallabi2907/U-VixLSTM</a> </p>
<blockquote>
<p>åŒ»å­¦å½±åƒçš„é«˜æ•ˆåˆ†å‰²ç­–ç•¥å‘å±•ä»æœ€åˆä¾èµ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åˆ°å½“å‰å¯¹ç»“åˆCNNä¸è§†è§‰å˜å‹å™¨çš„æ··åˆæ¨¡å‹çš„æ¢ç´¢ã€‚ç›®å‰ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨åˆ›å»ºé«˜æ€§èƒ½ä¸”è®¡ç®—æ•ˆç‡é«˜çš„æ¶æ„ï¼Œèƒ½å¤Ÿéƒ¨ç½²åœ¨èµ„æºæœ‰é™çš„è¿œç¨‹ç³»ç»Ÿä¸Šã€‚è™½ç„¶å˜å‹å™¨å¯ä»¥æ•è·è¾“å…¥ç©ºé—´ä¸­çš„å…¨å±€ä¾èµ–æ€§ï¼Œä½†å®ƒä»¬ä¹Ÿé¢ä¸´ç€ç›¸åº”çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å°†CNNä¸è§†è§‰æ‰©å±•é•¿çŸ­æ—¶è®°å¿†ï¼ˆVision-xLSTMï¼‰ç½‘ç»œç›¸ç»“åˆï¼Œå¼•å…¥æ–°å‹U-VixLSTMæ¨¡å‹è¿›è¡Œç ”ç©¶ã€‚Vision-xLSTMå—æ•è·ä»CNNç‰¹å¾å›¾ä¸­æå–çš„è¡¥ä¸å†…çš„ä¸´æ—¶å’Œå…¨å±€å…³ç³»ã€‚å·ç§¯ç‰¹å¾é‡å»ºè·¯å¾„å¯¹Vision-xLSTMå—çš„è¾“å‡ºä½“ç§¯è¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥äº§ç”Ÿåˆ†å‰²è¾“å‡ºã€‚æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æå‡ºVision-xLSTMå¯ä½œä¸ºåŒ»å­¦å½±åƒåˆ†å‰²çš„åˆé€‚éª¨å¹²ç½‘ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æä¾›å‡ºè‰²çš„æ€§èƒ½ã€‚ä¸å…¬å¼€å¯ç”¨çš„Synapseã€ISICå’ŒACDCæ•°æ®é›†ä¸­æœ€å…ˆè¿›çš„ç½‘ç»œç›¸æ¯”ï¼ŒU-VixLSTMè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/duttapallabi2907/U-VixLSTM">https://github.com/duttapallabi2907/U-VixLSTM</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16993v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„ä¼ ç»ŸåŒ»å­¦å½±åƒåˆ†å‰²ç­–ç•¥æ­£é€æ¸èåˆè§†çº¿è½¬æ¨¡å‹ï¼Œä»¥é€‚åº”æ›´åŠ é«˜æ•ˆä¸”å…·æœ‰é«˜æ€§èƒ½è®¡ç®—è¦æ±‚çš„åŒ»ç–—å›¾åƒå¤„ç†éœ€æ±‚ã€‚è¯¥è®ºæ–‡åˆ›æ–°æ€§æå‡ºäº†ç»“åˆCNNå’Œæ‰©å±•çš„è§†åŸŸé•¿çŸ­æœŸè®°å¿†ï¼ˆVision-xLSTMï¼‰æ¨¡å‹çš„U-VixLSTMç½‘ç»œæ¶æ„ã€‚è¿™ä¸€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å›¾åƒå—ä¸­çš„æ—¶åºå’Œå…¨å±€å…³ç³»ï¼Œå¹¶åœ¨å…¬å¼€æ•°æ®é›†Synapseã€ISICå’ŒACDCä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰ä¸»æµç½‘ç»œã€‚å…¶è®¡ç®—æˆæœ¬è¾ƒä½ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„è¿œç¨‹ç³»ç»Ÿéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†å‰²ç­–ç•¥æ­£ä»ä¾èµ–å·ç§¯ç¥ç»ç½‘ç»œå‘æ··åˆæ¨¡å‹è½¬å˜ï¼Œç»“åˆCNNä¸è§†çº¿è½¬æ¨¡å‹ã€‚</li>
<li>U-VixLSTMç»“åˆäº†CNNå’ŒVision-xLSTMæ¨¡å‹ï¼Œå½¢æˆæ–°é¢–æ¶æ„ã€‚</li>
<li>Vision-xLSTMå—èƒ½æ•æ‰å›¾åƒä¸­çš„æ—¶åºå’Œå…¨å±€å…³ç³»ã€‚</li>
<li>U-VixLSTMåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰ä¸»æµç½‘ç»œã€‚</li>
<li>U-VixLSTMè®¡ç®—æˆæœ¬ä½ï¼Œé€‚åˆéƒ¨ç½²åœ¨èµ„æºå—é™çš„è¿œç¨‹ç³»ç»Ÿä¸Šã€‚</li>
<li>è¯¥è®ºæ–‡æä¾›äº†U-VixLSTMç½‘ç»œçš„ä»£ç å®ç°ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåŒ»ç–—å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ä¸ªæ–°çš„é«˜æ•ˆç­–ç•¥æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a11adcbaf127b0688144a3da05db2568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3c78305500484fd253166d8c03a7c24.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Wills-Aligner-Multi-Subject-Collaborative-Brain-Visual-Decoding"><a href="#Wills-Aligner-Multi-Subject-Collaborative-Brain-Visual-Decoding" class="headerlink" title="Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding"></a>Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding</h2><p><strong>Authors:Guangyin Bao, Qi Zhang, Zixuan Gong, Jialei Zhou, Wei Fan, Kun Yi, Usman Naseem, Liang Hu, Duoqian Miao</strong></p>
<p>Decoding visual information from human brain activity has seen remarkable advancements in recent research. However, the diversity in cortical parcellation and fMRI patterns across individuals has prompted the development of deep learning models tailored to each subject. The personalization limits the broader applicability of brain visual decoding in real-world scenarios. To address this issue, we introduce Wills Aligner, a novel approach designed to achieve multi-subject collaborative brain visual decoding. Wills Aligner begins by aligning the fMRI data from different subjects at the anatomical level. It then employs delicate mixture-of-brain-expert adapters and a meta-learning strategy to account for individual fMRI pattern differences. Additionally, Wills Aligner leverages the semantic relation of visual stimuli to guide the learning of inter-subject commonality, enabling visual decoding for each subject to draw insights from other subjectsâ€™ data. We rigorously evaluate our Wills Aligner across various visual decoding tasks, including classification, cross-modal retrieval, and image reconstruction. The experimental results demonstrate that Wills Aligner achieves promising performance. </p>
<blockquote>
<p>ä»äººç±»å¤§è„‘æ´»åŠ¨ä¸­è§£ç è§†è§‰ä¿¡æ¯çš„ç ”ç©¶åœ¨è¿‘æ®µæ—¶é—´å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä¸åŒä¸ªä½“é—´çš®è´¨åˆ†åŒºå’ŒåŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ¨¡å¼çš„å¤šæ ·æ€§ä¿ƒä½¿äº†é’ˆå¯¹æ¯ä¸ªä¸ªä½“é‡èº«å®šåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ªäººåŒ–é™åˆ¶äº†è„‘è§†è§‰è§£ç åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ›´å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Wills Alignerï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å®ç°å¤šä¸»ä½“åä½œè„‘è§†è§‰è§£ç çš„æ–°å‹æ–¹æ³•ã€‚Wills Aligneré¦–å…ˆé€šè¿‡è§£å‰–å±‚é¢å°†ä¸åŒä¸»ä½“çš„fMRIæ•°æ®è¿›è¡Œå¯¹é½ã€‚ç„¶åï¼Œå®ƒé‡‡ç”¨ç²¾ç»†çš„æ··åˆè„‘ä¸“å®¶é€‚é…å™¨å’Œå…ƒå­¦ä¹ ç­–ç•¥æ¥åº”å¯¹ä¸ªä½“fMRIæ¨¡å¼å·®å¼‚ã€‚æ­¤å¤–ï¼ŒWills Alignerè¿˜åˆ©ç”¨è§†è§‰åˆºæ¿€ä¹‹é—´çš„è¯­ä¹‰å…³ç³»æ¥æŒ‡å¯¼è·¨ä¸»ä½“å…±åŒæ€§çš„å­¦ä¹ ï¼Œä½¿æ¯ä¸ªä¸»ä½“çš„è§†è§‰è§£ç éƒ½èƒ½ä»å…¶ä»–ä¸»ä½“çš„æ•°æ®ä¸­è·å–æ´è§ã€‚æˆ‘ä»¬åœ¨å„ç§è§†è§‰è§£ç ä»»åŠ¡ä¸­å¯¹Wills Alignerè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢å’Œå›¾åƒé‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWills Alignerè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13282v2">PDF</a> AAAI 2025, 16 pages</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè§£ç é¢†åŸŸçš„æ–°è¿›å±•é€šè¿‡å¼•å…¥ä¸ªæ€§åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåº”å¯¹äº†è„‘éƒ¨çš®è´¨ç»†åˆ†å’ŒåŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒæ¨¡å¼åœ¨ä¸ªä½“é—´çš„å·®å¼‚æ€§é—®é¢˜ã€‚ä¸ºæ‰©å¤§åº”ç”¨èŒƒå›´ï¼Œæˆ‘ä»¬æå‡ºäº†Wills Aligneræ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è§£å‰–å­¦å±‚çº§å¯¹é½ä¸åŒä¸ªä½“çš„fMRIæ•°æ®ï¼Œåˆ©ç”¨ç²¾ç»†çš„æ··åˆè„‘ä¸“å®¶é€‚é…å™¨å’Œå…ƒå­¦ä¹ ç­–ç•¥æ¥è§£å†³ä¸ªä½“å·®å¼‚é—®é¢˜ã€‚æ­¤å¤–ï¼ŒWills Alignerè¿˜åˆ©ç”¨è§†è§‰åˆºæ¿€çš„è¯­ä¹‰å…³ç³»æ¥æŒ‡å¯¼è·¨ä¸»ä½“å…±åŒæ€§çš„å­¦ä¹ ï¼Œä½¿æ¯ä¸ªä¸»ä½“çš„è§†è§‰è§£ç éƒ½èƒ½ä»å…¶ä»–ä¸»ä½“çš„æ•°æ®ä¸­è·ç›Šã€‚ç»è¿‡åˆ†ç±»ã€è·¨æ¨¡æ€æ£€ç´¢å’Œå›¾åƒé‡å»ºç­‰å¤šç§è§†è§‰è§£ç ä»»åŠ¡çš„ä¸¥æ ¼è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜Wills Alignerè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè§£ç é¢†åŸŸé¢ä¸´ä¸ªä½“å·®å¼‚å¯¼è‡´çš„æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è§£ç è¿‡ç¨‹ä¸­éœ€è¦æ ¹æ®ä¸ªä½“è¿›è¡Œå®šåˆ¶ã€‚</li>
<li>Wills Aligneré€šè¿‡è§£å‰–å­¦å±‚çº§å¯¹é½fMRIæ•°æ®æ¥è§£å†³ä¸ªä½“å·®å¼‚é—®é¢˜ã€‚</li>
<li>Wills Alignerä½¿ç”¨æ··åˆè„‘ä¸“å®¶é€‚é…å™¨å’Œå…ƒå­¦ä¹ ç­–ç•¥æ¥å¤„ç†ä¸ªä½“å·®å¼‚ã€‚</li>
<li>Wills Aligneråˆ©ç”¨è§†è§‰åˆºæ¿€çš„è¯­ä¹‰å…³ç³»æŒ‡å¯¼è·¨ä¸»ä½“å…±åŒæ€§å­¦ä¹ ã€‚</li>
<li>Wills Aligneråœ¨å„ç§è§†è§‰è§£ç ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be8c5792507ed0121ba4e942623f8c84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dcbd9fac8a62636ed5b17cf818ceacf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-637a2dadb2d0cff92486c664f0d7009b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7463e9d69c99402f26e2d9b8daca7999.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7c96a90b97231c1881758bbcac4ed36.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MindTuner-Cross-Subject-Visual-Decoding-with-Visual-Fingerprint-and-Semantic-Correction"><a href="#MindTuner-Cross-Subject-Visual-Decoding-with-Visual-Fingerprint-and-Semantic-Correction" class="headerlink" title="MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and   Semantic Correction"></a>MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and   Semantic Correction</h2><p><strong>Authors:Zixuan Gong, Qi Zhang, Guangyin Bao, Lei Zhu, Ke Liu, Liang Hu, Duoqian Miao</strong></p>
<p>Decoding natural visual scenes from brain activity has flourished, with extensive research in single-subject tasks and, however, less in cross-subject tasks. Reconstructing high-quality images in cross-subject tasks is a challenging problem due to profound individual differences between subjects and the scarcity of data annotation. In this work, we proposed MindTuner for cross-subject visual decoding, which achieves high-quality and rich semantic reconstructions using only 1 hour of fMRI training data benefiting from the phenomena of visual fingerprint in the human visual system and a novel fMRI-to-text alignment paradigm. Firstly, we pre-train a multi-subject model among 7 subjects and fine-tune it with scarce data on new subjects, where LoRAs with Skip-LoRAs are utilized to learn the visual fingerprint. Then, we take the image modality as the intermediate pivot modality to achieve fMRI-to-text alignment, which achieves impressive fMRI-to-text retrieval performance and corrects fMRI-to-image reconstruction with fine-tuned semantics. The results of both qualitative and quantitative analyses demonstrate that MindTuner surpasses state-of-the-art cross-subject visual decoding models on the Natural Scenes Dataset (NSD), whether using training data of 1 hour or 40 hours. </p>
<blockquote>
<p>è§£ç è‡ªç„¶è§†è§‰åœºæ™¯çš„ç ”ç©¶å·²ç»ä»å•ä¸€ä»»åŠ¡çš„ç ”ç©¶æ‰©å±•åˆ°äº†å¤šä¸ªä»»åŠ¡çš„ç ”ç©¶ï¼Œè€Œåœ¨è·¨ä»»åŠ¡ç ”ç©¶ä¸­ä»æœ‰è®¸å¤šå·¥ä½œè¦åšã€‚åœ¨è·¨ä»»åŠ¡ä¸­é‡æ„é«˜è´¨é‡å›¾åƒæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºä¸åŒä¸ªä½“ä¹‹é—´å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œå¹¶ä¸”æ•°æ®æ ‡æ³¨ç¨€ç¼ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè·¨ä»»åŠ¡è§†è§‰è§£ç çš„MindTunerï¼Œå®ƒä»…ä½¿ç”¨1å°æ—¶çš„fMRIè®­ç»ƒæ•°æ®å°±èƒ½å®ç°é«˜è´¨é‡å’Œä¸°å¯Œçš„è¯­ä¹‰é‡æ„ï¼Œè¿™å¾—ç›Šäºäººç±»è§†è§‰ç³»ç»Ÿä¸­çš„è§†è§‰æŒ‡çº¹ç°è±¡ä»¥åŠä¸€ç§æ–°çš„fMRIåˆ°æ–‡æœ¬çš„å¯¹é½èŒƒå¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨7ä¸ªä¸»ä½“ä¹‹é—´é¢„è®­ç»ƒäº†ä¸€ä¸ªå¤šä¸»ä½“æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç¨€ç¼ºæ•°æ®å¯¹æ–°ä¸»ä½“è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­ä½¿ç”¨LoRAså’ŒSkip-LoRAsæ¥å­¦ä¹ è§†è§‰æŒ‡çº¹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»¥å›¾åƒæ¨¡å¼ä½œä¸ºä¸­é—´ä¸­å¿ƒæ¨¡å¼æ¥å®ç°fMRIåˆ°æ–‡æœ¬çš„å¯¹é½ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„fMRIåˆ°æ–‡æœ¬çš„æ£€ç´¢æ€§èƒ½ï¼Œå¹¶é€šè¿‡å¾®è°ƒè¯­ä¹‰æ ¡æ­£äº†fMRIåˆ°å›¾åƒçš„é‡å»ºã€‚å®šæ€§å’Œå®šé‡åˆ†æçš„ç»“æœå‡è¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ä½¿ç”¨1å°æ—¶æˆ–40å°æ—¶çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒMindTuneråœ¨è‡ªç„¶åœºæ™¯æ•°æ®é›†ï¼ˆNSDï¼‰ä¸Šçš„è·¨ä»»åŠ¡è§†è§‰è§£ç æ€§èƒ½å‡è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12630v2">PDF</a> AAAI 2025, 14 pages</p>
<p><strong>Summary</strong><br>è§£ç è‡ªç„¶è§†è§‰åœºæ™¯ç ”ç©¶æ´»è·ƒäºé’ˆå¯¹å•ä¸ªä¸»ä½“çš„ä»»åŠ¡ä¸Šï¼Œè·¨ä¸»ä½“ä»»åŠ¡ä¸Šçš„ç ”ç©¶è¾ƒå°‘ã€‚åˆ©ç”¨è·¨ä¸»ä½“ä»»åŠ¡ä¸­çš„è„‘ç¥ç»å½±åƒæ•°æ®è¿›è¡Œé«˜è´¨é‡å›¾åƒé‡å»ºæå…·æŒ‘æˆ˜æ€§ï¼ŒåŸå› åœ¨äºä¸åŒä¸ªä½“é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ä¸”æ•°æ®æ ‡æ³¨ç¨€ç¼ºã€‚æœ¬ç ”ç©¶æå‡ºMindTunerç”¨äºè·¨ä¸»ä½“è§†è§‰è§£ç ï¼Œä»…ä½¿ç”¨ä¸€å°æ—¶çš„fMRIè®­ç»ƒæ•°æ®ä¾¿å®ç°äº†é«˜è´¨é‡ä¸”ä¸°å¯Œçš„è¯­ä¹‰é‡å»ºï¼Œå¾—ç›Šäºäººç±»è§†è§‰ç³»ç»Ÿä¸­çš„è§†è§‰æŒ‡çº¹ç°è±¡ä»¥åŠæ–°é¢–çš„fMRI-æ–‡æœ¬å¯¹é½èŒƒå¼ã€‚é€šè¿‡é¢„è®­ç»ƒå¤šä¸»ä½“æ¨¡å‹å¹¶åœ¨æ–°ä¸»ä½“ä¸Šå¾®è°ƒï¼Œåˆ©ç”¨LoRAsä¸Skip-LoRAså­¦ä¹ è§†è§‰æŒ‡çº¹ã€‚æ­¤å¤–ï¼Œä»¥å›¾åƒæ¨¡æ€ä½œä¸ºä¸­é—´æ¢çº½æ¨¡æ€å®ç°fMRI-æ–‡æœ¬å¯¹é½ï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„fMRI-æ–‡æœ¬æ£€ç´¢æ€§èƒ½ï¼Œå¹¶ä¿®æ­£äº†å…·æœ‰ç²¾ç»†è¯­ä¹‰çš„fMRI-å›¾åƒé‡å»ºã€‚å®šæ€§ä¸å®šé‡åˆ†æç»“æœæ˜¾ç¤ºï¼Œæ— è®ºæ˜¯åœ¨ä½¿ç”¨ä¸€å°æ—¶è¿˜æ˜¯å››åå°æ—¶çš„è®­ç»ƒæ•°æ®æƒ…å†µä¸‹ï¼ŒMindTuneråœ¨å¤©ç„¶åœºæ™¯æ•°æ®é›†ä¸Šçš„è·¨ä¸»ä½“è§†è§‰è§£ç è¡¨ç°å‡è¶…è¶Šç°æœ‰é¡¶å°–æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å…³æ³¨è·¨ä¸»ä½“ä»»åŠ¡ä¸­è„‘ç¥ç»å½±åƒæ•°æ®çš„é«˜è´¨é‡å›¾åƒé‡å»ºæŒ‘æˆ˜ã€‚</li>
<li>MindTuneræ–¹æ³•ä»…ä½¿ç”¨ä¸€å°æ—¶çš„fMRIè®­ç»ƒæ•°æ®å®ç°é«˜è´¨é‡å’Œä¸°å¯Œçš„è¯­ä¹‰é‡å»ºã€‚</li>
<li>åˆ©ç”¨è§†è§‰æŒ‡çº¹ç°è±¡å’Œæ–°é¢–çš„fMRI-æ–‡æœ¬å¯¹é½èŒƒå¼ä¿ƒè¿›è§£ç ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒå¤šä¸»ä½“æ¨¡å‹å¹¶åœ¨æ–°ä¸»ä½“ä¸Šå¾®è°ƒï¼Œä½¿ç”¨LoRAsä¸Skip-LoRAså­¦ä¹ è§†è§‰æŒ‡çº¹ã€‚</li>
<li>ä»¥å›¾åƒæ¨¡æ€ä½œä¸ºä¸­é—´æ¢çº½å®ç°fMRI-æ–‡æœ¬å¯¹é½ï¼Œæé«˜fMRI-æ–‡æœ¬æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>MindTuneråœ¨å¤©ç„¶åœºæ™¯æ•°æ®é›†ä¸Šçš„è·¨ä¸»ä½“è§†è§‰è§£ç è¡¨ç°è¶…è¶Šç°æœ‰é¡¶å°–æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf7d00e1236d98d2a9b6b9076bc4b7b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d8e64d0374c064ee97248d9dfd9496.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a07033c9a39fb745f3c66ca07addf18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ce319a152db7f218e8161e9da38b807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-769f6d5b8ef6300bb94ac50995f74b04.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-a-Comprehensive-Efficient-and-Promptable-Anatomic-Structure-Segmentation-Model-using-3D-Whole-body-CT-Scans"><a href="#Towards-a-Comprehensive-Efficient-and-Promptable-Anatomic-Structure-Segmentation-Model-using-3D-Whole-body-CT-Scans" class="headerlink" title="Towards a Comprehensive, Efficient and Promptable Anatomic Structure   Segmentation Model using 3D Whole-body CT Scans"></a>Towards a Comprehensive, Efficient and Promptable Anatomic Structure   Segmentation Model using 3D Whole-body CT Scans</h2><p><strong>Authors:Heng Guo, Jianfeng Zhang, Jiaxing Huang, Tony C. W. Mok, Dazhou Guo, Ke Yan, Le Lu, Dakai Jin, Minfeng Xu</strong></p>
<p>Segment anything model (SAM) demonstrates strong generalization ability on natural image segmentation. However, its direct adaptation in medical image segmentation tasks shows significant performance drops. It also requires an excessive number of prompt points to obtain a reasonable accuracy. Although quite a few studies explore adapting SAM into medical image volumes, the efficiency of 2D adaptation methods is unsatisfactory and 3D adaptation methods are only capable of segmenting specific organs&#x2F;tumors. In this work, we propose a comprehensive and scalable 3D SAM model for whole-body CT segmentation, named CT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation model using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively, ensuring the modelâ€™s accurate responses to higher-dimensional spatial prompts is crucial, and 3D patch-wise training is required due to GPU memory constraints. Therefore, we propose two key technical developments: 1) a progressively and spatially aligned prompt encoding method to effectively encode click prompts in local 3D space; and 2) a cross-patch prompt scheme to capture more 3D spatial context, which is beneficial for reducing the editing workloads when interactively prompting on large organs. CT-SAM3D is trained using a curated dataset of 1204 CT scans containing 107 whole-body anatomies and extensively validated using five datasets, achieving significantly better results against all previous SAM-derived models. Code, data, and our 3D interactive segmentation tool with quasi-real-time responses are available at <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/ct-sam3d">https://github.com/alibaba-damo-academy/ct-sam3d</a>. </p>
<blockquote>
<p>åˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ç›´æ¥åº”ç”¨SAMä¼šä½¿å…¶æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚å®ƒè¿˜éœ€è¦å¤§é‡çš„æç¤ºç‚¹æ‰èƒ½è·å¾—åˆç†çš„ç²¾åº¦ã€‚å°½ç®¡æœ‰ä¸€äº›ç ”ç©¶å°è¯•å°†SAMé€‚åº”åˆ°åŒ»å­¦å›¾åƒä½“ç§¯ä¸­ï¼Œä½†2Dé€‚åº”æ–¹æ³•æ•ˆç‡ä¸é«˜ï¼Œè€Œ3Dé€‚åº”æ–¹æ³•åªèƒ½å¯¹ç‰¹å®šå™¨å®˜&#x2F;è‚¿ç˜¤è¿›è¡Œåˆ†å‰²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢ä¸”å¯æ‰©å±•çš„3D SAMæ¨¡å‹ï¼Œç”¨äºå…¨èº«CTåˆ†å‰²ï¼Œåä¸ºCT-SAM3Dã€‚æˆ‘ä»¬å¹¶ä¸æ˜¯å»é€‚åº”SAMï¼Œè€Œæ˜¯æå‡ºä½¿ç”¨ï¼ˆè¿‘ä¹ï¼‰å…¨æ ‡è®°CTæ•°æ®é›†çš„ä¸€ä¸ª3Då¯æç¤ºåˆ†å‰²æ¨¡å‹ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒCT-SAM3Dï¼Œç¡®ä¿æ¨¡å‹å¯¹æ›´é«˜ç»´åº¦ç©ºé—´æç¤ºçš„å‡†ç¡®å“åº”è‡³å…³é‡è¦ï¼Œå¹¶ä¸”ç”±äºGPUå†…å­˜é™åˆ¶ï¼Œéœ€è¦è¿›è¡Œ3Dè¡¥ä¸å¼è®­ç»ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯å‘å±•ï¼š1ï¼‰ä¸€ç§æ¸è¿›ä¸”ç©ºé—´å¯¹é½çš„æç¤ºç¼–ç æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åœ¨å±€éƒ¨3Dç©ºé—´ä¸­ç¼–ç ç‚¹å‡»æç¤ºï¼›2ï¼‰ä¸€ç§è·¨è¡¥ä¸æç¤ºæ–¹æ¡ˆï¼Œä»¥æ•è·æ›´å¤šçš„3Dç©ºé—´ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºåœ¨å¤§å‹å™¨å®˜ä¸Šè¿›è¡Œäº¤äº’å¼æç¤ºæ—¶å‡å°‘ç¼–è¾‘å·¥ä½œé‡æ˜¯æœ‰ç›Šçš„ã€‚CT-SAM3Dæ˜¯ä½¿ç”¨åŒ…å«107ä¸ªå…¨èº«è§£å‰–ç»“æ„çš„1204ä¸ªCTæ‰«æçš„ç²¾é€‰æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„ï¼Œå¹¶åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œä¸æ‰€æœ‰å…ˆå‰çš„SAMè¡ç”Ÿæ¨¡å‹ç›¸æ¯”å–å¾—äº†æ˜¾è‘—æ›´å¥½çš„ç»“æœã€‚ä»£ç ã€æ•°æ®ä»¥åŠæˆ‘ä»¬çš„å…·æœ‰å‡†å®æ—¶å“åº”çš„3Däº¤äº’å¼åˆ†å‰²å·¥å…·å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/ct-sam3d%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alibaba-damo-academy/ct-sam3dæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.15063v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç”¨äºå…¨èº«CTå›¾åƒåˆ†å‰²çš„ç»¼åˆã€å¯æ‰©å±•çš„3D SAMæ¨¡å‹â€”â€”CT-SAM3Dã€‚ç›¸è¾ƒäºSAMå’Œå…¶ä»–æ¨¡å‹ï¼ŒCT-SAM3Då±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½å’Œé€‚ç”¨æ€§ã€‚è¯¥ç ”ç©¶åœ¨æœ‰æ•ˆè®­ç»ƒæ¨¡å‹çš„åŒæ—¶ï¼Œæå‡ºäº†ä¸¤ç§å…³é”®æŠ€æœ¯æ”¹è¿›ï¼Œä»¥å®ç°æ›´é«˜çš„å“åº”å‡†ç¡®æ€§å¹¶é™ä½å†…å­˜é™åˆ¶å¸¦æ¥çš„å¹²æ‰°ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬ç©ºé—´å¯¹é½æç¤ºç¼–ç æŠ€æœ¯å’Œè·¨åŒºå—æç¤ºæ–¹æ¡ˆï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„æ„ŸçŸ¥èƒ½åŠ›å¹¶å‡å°‘ç¼–è¾‘å·¥ä½œé‡ã€‚æ­¤å¤–ï¼ŒCT-SAM3Dåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…ˆå‰çš„SAMè¡ç”Ÿæ¨¡å‹ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SAMåœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ç›´æ¥å°†SAMåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²éœ€è¦å¤§é‡æç¤ºç‚¹æ‰èƒ½è·å¾—åˆç†çš„å‡†ç¡®åº¦ã€‚</li>
<li>è™½ç„¶å·²æœ‰ç ”ç©¶å°è¯•å°†SAMé€‚åº”äºåŒ»å­¦å›¾åƒä½“ç§¯ï¼Œä½†æ•ˆç‡ä¸è¶³ä¸”å¯¹æŸäº›å™¨å®˜&#x2F;è‚¿ç˜¤çš„é€‚åº”æ€§æœ‰é™ã€‚</li>
<li>æå‡ºä¸€ç§å…¨é¢çš„ä¸‰ç»´CTå›¾åƒåˆ†å‰²æ¨¡å‹CT-SAM3Dï¼Œé€‚ç”¨äºå…¨èº«CTå›¾åƒåˆ†å‰²ã€‚</li>
<li>CT-SAM3Dé€šè¿‡ä¸¤ç§å…³é”®æŠ€æœ¯æ”¹è¿›æ¥ç¡®ä¿å¯¹æ›´é«˜ç»´åº¦ç©ºé—´æç¤ºçš„å‡†ç¡®å“åº”å¹¶å…‹æœGPUå†…å­˜é™åˆ¶ï¼šç©ºé—´å¯¹é½æç¤ºç¼–ç æ–¹æ³•å’Œè·¨åŒºå—æç¤ºæ–¹æ¡ˆã€‚</li>
<li>CT-SAM3Dåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ‰€æœ‰å…ˆå‰çš„SAMè¡ç”Ÿæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.15063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e80b19e2c7dc8e34c906e815957f9ebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a50a24fc3046c7c4f7888f6b8bd7d4d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d107ddcbe75fbc6a20c051fdd0aca724.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e109bd1882f922c823f6b8739ce431e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Signal-Reconstruction-from-Samples-at-Unknown-Locations-with-Application-to-2D-Unknown-View-Tomography"><a href="#Signal-Reconstruction-from-Samples-at-Unknown-Locations-with-Application-to-2D-Unknown-View-Tomography" class="headerlink" title="Signal Reconstruction from Samples at Unknown Locations with Application   to 2D Unknown View Tomography"></a>Signal Reconstruction from Samples at Unknown Locations with Application   to 2D Unknown View Tomography</h2><p><strong>Authors:Sheel Shah, Kaishva Shah, Karthik S. Gurumoorthy, Ajit Rajwade</strong></p>
<p>It is well known that a band-limited signal can be reconstructed from its uniformly spaced samples if the sampling rate is sufficiently high. More recently, it has been proved that one can reconstruct a 1D band-limited signal even if the exact sample locations are unknown, but given a uniform distribution of the sample locations and their ordering in 1D. In this work, we extend the analytical error bounds in such scenarios for quasi-bandlimited (QBL) signals, and for the case of arbitrary but known sampling distributions. We also prove that such reconstruction methods are resilient to a certain proportion of errors in the specification of the sample location ordering. We then express the problem of tomographic reconstruction of 2D images from 1D Radon projections under unknown angles (2D UVT) with known angle distribution, as a special case for reconstruction of QBL signals from samples at unknown locations with known distribution. Building upon our theoretical background, we present asymptotic bounds for 2D QBL image reconstruction from 1D Radon projections in the unknown angles setting, and present an extensive set of simulations to verify these bounds in varied parameter regimes. To the best of our knowledge, this is the first piece of work to perform such an analysis for 2D UVT and explicitly relate it to advances in sampling theory, even though the associated reconstruction algorithms have been known for a long time. </p>
<blockquote>
<p>ä¼—æ‰€å‘¨çŸ¥ï¼Œå¦‚æœé‡‡æ ·ç‡è¶³å¤Ÿé«˜ï¼Œå¸¦é™ä¿¡å·å¯ä»¥ä»å…¶å‡åŒ€é—´éš”çš„æ ·æœ¬ä¸­é‡å»ºã€‚æœ€è¿‘ï¼Œå·²ç»è¯æ˜å³ä½¿ä¸çŸ¥é“ç¡®åˆ‡çš„æ ·æœ¬ä½ç½®ï¼Œä½†åªè¦ç»™å®šæ ·æœ¬ä½ç½®çš„ä¸€ç»´å‡åŒ€åˆ†å¸ƒå’Œå®ƒä»¬çš„æ’åºï¼Œä¹Ÿå¯ä»¥é‡å»ºä¸€ç»´å¸¦é™ä¿¡å·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ‰©å±•äº†æ­¤ç±»åœºæ™¯ä¸‹å‡†å¸¦é™ï¼ˆQBLï¼‰ä¿¡å·çš„åˆ†æè¯¯å·®ç•Œé™ï¼Œä»¥åŠä»»æ„ä½†å·²çŸ¥çš„é‡‡æ ·åˆ†å¸ƒçš„æƒ…å†µã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†è¿™ç§é‡å»ºæ–¹æ³•å¯¹æŒ‡å®šæ ·æœ¬ä½ç½®æ’åºä¸­çš„ä¸€å®šæ¯”ä¾‹çš„è¯¯å·®å…·æœ‰éŸ§æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†äºŒç»´å›¾åƒä»æœªçŸ¥è§’åº¦çš„ä¸€ç»´ Radon æŠ•å½±ï¼ˆ2D UVTï¼‰çš„å±‚ææˆåƒé‡å»ºé—®é¢˜è¡¨è¾¾ä¸ºä»æœªçŸ¥ä½ç½®çš„æ ·æœ¬é‡å»ºQBLä¿¡å·çš„ç‰¹æ®Šæ¡ˆä¾‹ï¼Œå·²çŸ¥å…¶åˆ†å¸ƒã€‚åŸºäºæˆ‘ä»¬çš„ç†è®ºèƒŒæ™¯ï¼Œæˆ‘ä»¬ä¸ºæœªçŸ¥è§’åº¦è®¾ç½®ä¸‹ä»äºŒç»´QBLå›¾åƒçš„ä¸€ç»´RadonæŠ•å½±é‡å»ºæä¾›äº†æ¸è¿‘ç•Œé™ï¼Œå¹¶é€šè¿‡å¤§é‡çš„æ¨¡æ‹ŸéªŒè¯äº†åœ¨å„ç§å‚æ•°ç¯å¢ƒä¸‹çš„è¿™äº›ç•Œé™ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°½ç®¡ç›¸å…³çš„é‡å»ºç®—æ³•å·²ç»å­˜åœ¨å¾ˆé•¿æ—¶é—´äº†ï¼Œä½†è¿™æ˜¯é¦–æ¬¡è¿›è¡Œæ­¤ç±»åˆ†æå…³äºäºŒç»´UVTçš„å·¥ä½œå¹¶å°†å…¶ä¸é‡‡æ ·ç†è®ºçš„è¿›å±•æ˜ç¡®è”ç³»èµ·æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.06376v2">PDF</a> This is a preprint of a paper accepted to Signal Processing   (Elsevier)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬è®¨è®ºäº†ä»å‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬ä¸­é‡å»ºä¸€ç»´å¸¦é™ä¿¡å·çš„æ–¹æ³•ï¼Œå¹¶æ‰©å±•åˆ°å‡†å¸¦é™ä¿¡å·å’Œä»»æ„ä½†å·²çŸ¥çš„é‡‡æ ·åˆ†å¸ƒçš„æƒ…å†µã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ­¤ç±»é‡å»ºæ–¹æ³•å¯¹æ ·æœ¬ä½ç½®æ’åºçš„ç‰¹å®šé”™è¯¯æ¯”ä¾‹å…·æœ‰å¼¹æ€§ã€‚æ­¤å¤–ï¼Œå°†äºŒç»´å›¾åƒä»æœªçŸ¥è§’åº¦çš„ä¸€ç»´RadonæŠ•å½±é‡å»ºé—®é¢˜è¡¨è¿°ä¸ºä»å…·æœ‰å·²çŸ¥åˆ†å¸ƒçš„æœªçŸ¥æ ·æœ¬ä½ç½®é‡å»ºQBLä¿¡å·çš„ç‰¹ä¾‹ã€‚åŸºäºç†è®ºèƒŒæ™¯ï¼Œä¸ºäºŒç»´QBLå›¾åƒä»æœªçŸ¥è§’åº¦çš„ä¸€ç»´RadonæŠ•å½±é‡å»ºæä¾›äº†æ¸è¿‘è¾¹ç•Œï¼Œå¹¶é€šè¿‡å¤§é‡æ¨¡æ‹ŸéªŒè¯äº†è¿™äº›è¾¹ç•Œåœ¨ä¸åŒå‚æ•°æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚è¿™æ˜¯é¦–æ¬¡å°†äºŒç»´æœªçŸ¥è§’åº¦å˜æ¢ä¸é‡‡æ ·ç†è®ºè¿›å±•ç›¸ç»“åˆè¿›è¡Œåˆ†æï¼Œå¹¶æ˜ç¡®å…³è”èµ·æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸¦é™ä¿¡å·å¯ä»¥ä»å…¶å‡åŒ€é—´éš”çš„æ ·æœ¬ä¸­é‡å»ºå‡ºæ¥ï¼Œå‰ææ˜¯é‡‡æ ·ç‡è¶³å¤Ÿé«˜ã€‚</li>
<li>ä¸€ç»´å¸¦é™ä¿¡å·å¯ä»¥ä»æœªçŸ¥ç¡®åˆ‡æ ·æœ¬ä½ç½®ä¸­é‡å»ºï¼Œåªè¦ç»™å®šæ ·æœ¬ä½ç½®çš„å‡åŒ€åˆ†å¸ƒå’Œå®ƒä»¬åœ¨ä¸€ç»´ä¸­çš„æ’åºã€‚</li>
<li>å¯¹äºå‡†å¸¦é™ä¿¡å·å’Œä»»æ„ä½†å·²çŸ¥çš„é‡‡æ ·åˆ†å¸ƒï¼Œç ”ç©¶æ‰©å±•äº†åˆ†æè¯¯å·®è¾¹ç•Œã€‚</li>
<li>æ­¤ç±»é‡å»ºæ–¹æ³•å¯¹æ ·æœ¬ä½ç½®æ’åºçš„ç‰¹å®šé”™è¯¯æ¯”ä¾‹å…·æœ‰é²æ£’æ€§ã€‚</li>
<li>äºŒç»´å›¾åƒçš„RadonæŠ•å½±é‡å»ºé—®é¢˜å¯è§†ä¸ºä»å…·æœ‰å·²çŸ¥åˆ†å¸ƒçš„æœªçŸ¥æ ·æœ¬ä½ç½®é‡å»ºQBLä¿¡å·çš„ç‰¹ä¾‹ã€‚</li>
<li>ç ”ç©¶ä¸ºäºŒç»´QBLå›¾åƒä»æœªçŸ¥è§’åº¦çš„ä¸€ç»´RadonæŠ•å½±é‡å»ºæä¾›äº†æ¸è¿‘è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2304.06376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1dc0f301e9352af27eee220171a811cf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c533e46ae19cc805c73a90ab515107a2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-21  OpenEMMA Open-Source Multimodal Model for End-to-End Autonomous Driving
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-20/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-031d76647d4c20b50a4c274cdad6a4cb.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-20  Idea23D Collaborative LMM Agents Enable 3D Model Generation from   Interleaved Multimodal Inputs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15230.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
