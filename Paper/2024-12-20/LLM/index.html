<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-20  Advanced Reasoning and Transformation Engine for Multi-Step Insight   Synthesis in Data Analytics with Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b786e0f3eb4ea4d32cddfaf0be9601a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-20-æ›´æ–°"><a href="#2024-12-20-æ›´æ–°" class="headerlink" title="2024-12-20 æ›´æ–°"></a>2024-12-20 æ›´æ–°</h1><h2 id="Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics-with-Large-Language-Models"><a href="#Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics-with-Large-Language-Models" class="headerlink" title="Advanced Reasoning and Transformation Engine for Multi-Step Insight   Synthesis in Data Analytics with Large Language Models"></a>Advanced Reasoning and Transformation Engine for Multi-Step Insight   Synthesis in Data Analytics with Large Language Models</h2><p><strong>Authors:Atin Sakkeer Hussain</strong></p>
<p>This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç”¨äºæ•°æ®è§£æä¸­çš„å¤šæ­¥éª¤è§è§£åˆæˆçš„å…ˆè¿›æ¨ç†ä¸è½¬æ¢å¼•æ“ï¼ˆARTEMIS-DAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚ARTEMIS-DAé›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šPlannerï¼Œå®ƒå°†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºç»“æ„åŒ–ã€é¡ºåºæŒ‡ä»¤ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è½¬æ¢ã€é¢„æµ‹å»ºæ¨¡å’Œå¯è§†åŒ–ï¼›Coderï¼Œå®ƒåŠ¨æ€ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ä»¥æ‰§è¡Œè¿™äº›æŒ‡ä»¤ï¼›ä»¥åŠGrapherï¼Œå®ƒè§£é‡Šç”Ÿæˆçš„å¯è§†åŒ–ä»¥å¾—å‡ºå¯æ“ä½œçš„è§è§£ã€‚é€šè¿‡åè°ƒè¿™äº›ç»„ä»¶ä¹‹é—´çš„åä½œï¼ŒARTEMIS-DAæœ‰æ•ˆåœ°ç®¡ç†æ¶‰åŠé«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨ä¸åŒæ•°æ®æ¨¡å¼çš„ç»¼åˆçš„å¤æ‚åˆ†æå·¥ä½œæµç¨‹ã€‚è¯¥æ¡†æ¶åœ¨WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¯æ˜äº†å…¶å¤„ç†å¤æ‚åˆ†æä»»åŠ¡çš„ç²¾ç¡®æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡å°†LLMçš„æ¨ç†èƒ½åŠ›ä¸è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œä»¥åŠè§†è§‰åˆ†æç›¸ç»“åˆï¼ŒARTEMIS-DAä¸ºå¤šæ­¥éª¤è§è§£åˆæˆæä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†æ•°æ®åˆ†æä¸­çš„ä¸€ç³»åˆ—æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14146v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>é«˜çº§æ¨ç†ä¸è½¬æ¢å¼•æ“ï¼ˆARTEMIS-DAï¼‰åœ¨æ•°æ®åˆ†æä¸­çš„åº”ç”¨ï¼Œå±•ç¤ºäº†å¤šæ­¥æ´å¯Ÿåˆæˆçš„ä¸€ç§æ–°é¢–æ¡†æ¶ã€‚å®ƒç»“åˆäº†è§„åˆ’å™¨ã€ç¼–è¯‘å™¨å’Œå›¾å½¢å™¨ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå®ç°äº†å¯¹å¤æ‚å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡çš„ç²¾ç¡®è§£å†³ã€‚é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œè‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œä»¥åŠè§†è§‰åˆ†æï¼ŒARTEMIS-DAå®ç°è·¨å¤šç§æ•°æ®æ¨¡å¼çš„é«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œåˆæˆï¼Œæä¾›ç¨³å¥å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆä»¥è¿›è¡Œå¤šæ­¥éª¤çš„è§è§£åˆæˆï¼Œè§£å†³äº†ä¸€ç³»åˆ—æ•°æ®åˆ†ææŒ‘æˆ˜ã€‚æ­¤æ¡†æ¶å®ç°äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½æ°´å¹³ï¼Œæ˜¾ç¤ºå‡ºè§£å†³å¤æ‚åˆ†æä»»åŠ¡çš„ç²¾ç¡®æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ARTEMIS-DAæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤šä¸ªç»„ä»¶æ¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡ã€‚å®ƒåŒ…æ‹¬äº†ç”¨äºæ‹†åˆ†å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„è§„åˆ’å™¨ï¼Œä»¥äº§ç”Ÿç»“æ„åŒ–å’Œé¡ºåºåŒ–çš„æŒ‡ä»¤é›†ã€‚</li>
<li>ARTEMIS-DAçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€æ˜¯ç¼–è¯‘å™¨ï¼Œå®ƒèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç æ¥å®ç°è¿™äº›æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜é…å¤‡äº†å›¾å½¢å™¨ï¼Œèƒ½å¤Ÿè§£é‡Šç”Ÿæˆçš„å¯è§†åŒ–ç»“æœä»¥è·å–å¯æ“ä½œçš„è§è§£ã€‚</li>
<li>ARTEMIS-DAæˆåŠŸåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä¸è‡ªåŠ¨åŒ–çš„ä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠè§†è§‰åˆ†æç»“åˆåœ¨äº†ä¸€èµ·ã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿå¤„ç†é«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨å„ç§æ•°æ®æ¨¡å¼çš„åˆæˆã€‚</li>
<li>è¯¥æ¡†æ¶çš„æ€§èƒ½è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ°´å¹³ï¼Œå¹¶åœ¨WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œè¯æ˜äº†å®ƒèƒ½å¤Ÿç²¾ç¡®ä¸”çµæ´»åœ°è§£å†³å¤æ‚åˆ†æä»»åŠ¡ã€‚</li>
<li>ARTEMIS-DAæä¾›äº†ä¸€ç§ç¨³å¥å’Œå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè¿›è¡Œå¤šæ­¥éª¤çš„è§è§£åˆæˆï¼Œå¯¹äºæ•°æ®åˆ†æé¢†åŸŸçš„å¹¿æ³›æŒ‘æˆ˜å…·æœ‰å¾ˆå¼ºçš„åº”å¯¹èƒ½åŠ›ã€‚è¿™ä¸€è§£å†³æ–¹æ¡ˆå°†æœ‰åŠ©äºæ”¹è¿›æ•°æ®å¤„ç†å’Œåˆ†æçš„æ•ˆç‡åŠå‡†ç¡®æ€§ã€‚</li>
<li>ARTEMIS-DAæ¡†æ¶å¼ºè°ƒäº†æ•°æ®é¢„å¤„ç†ã€è½¬æ¢ã€é¢„æµ‹å»ºæ¨¡å’Œå¯è§†åŒ–çš„é‡è¦æ€§ï¼Œè¿™äº›éƒ½æ˜¯æ•°æ®åˆ†æè¿‡ç¨‹ä¸­çš„å…³é”®æ­¥éª¤ã€‚é€šè¿‡ä¼˜åŒ–è¿™äº›æ­¥éª¤ï¼Œè¯¥æ¡†æ¶æé«˜äº†æ•°æ®åˆ†æçš„æ•´ä½“æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0bc63d2670fe3f338bb4d74dc20fad46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-152cb53ea94c8c505412d75c9c95d10d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddf9f19888890e2735f18e5a265d2623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a630d566ddcb61473a42d304525567e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45be89566a46e7d0ce7e392c3c6e63ec.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GLIDER-Grading-LLM-Interactions-and-Decisions-using-Explainable-Ranking"><a href="#GLIDER-Grading-LLM-Interactions-and-Decisions-using-Explainable-Ranking" class="headerlink" title="GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking"></a>GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking</h2><p><strong>Authors:Darshan Deshpande, Selvan Sunitha Ravi, Sky CH-Wang, Bartosz Mielczarek, Anand Kannappan, Rebecca Qian</strong></p>
<p>The LLM-as-judge paradigm is increasingly being adopted for automated evaluation of model outputs. While LLM judges have shown promise on constrained evaluation tasks, closed source LLMs display critical shortcomings when deployed in real world applications due to challenges of fine grained metrics and explainability, while task specific evaluation models lack cross-domain generalization. We introduce GLIDER, a powerful 3B evaluator LLM that can score any text input and associated context on arbitrary user defined criteria. GLIDER shows higher Pearsonâ€™s correlation than GPT-4o on FLASK and greatly outperforms prior evaluation models, achieving comparable performance to LLMs 17x its size. GLIDER supports fine-grained scoring, multilingual reasoning, span highlighting and was trained on 685 domains and 183 criteria. Extensive qualitative analysis shows that GLIDER scores are highly correlated with human judgments, with 91.3% human agreement. We have open-sourced GLIDER to facilitate future research. </p>
<blockquote>
<p>LLMä½œä¸ºåˆ¤æ–­ç³»ç»Ÿçš„æ¨¡å¼åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹è¾“å‡ºæ–¹é¢è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚å°½ç®¡LLMä½œä¸ºè¯„å§”åœ¨å—çº¦æŸçš„è¯„ä¼°ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éƒ¨ç½²å°é—­å¼LLMæ—¶ï¼Œç”±äºå…¶é¢ä¸´ç²¾ç»†åº¦æŒ‡æ ‡å’Œè§£é‡Šæ€§çš„æŒ‘æˆ˜ï¼Œæš´éœ²å‡ºå…³é”®ç¼ºé™·ã€‚è€Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è¯„ä¼°æ¨¡å‹ç¼ºä¹è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºGLIDERï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ‹¥æœ‰ç”¨æˆ·è‡ªå®šä¹‰æ ‡å‡†è¯„åˆ†çš„è¯„ä»·å‹LLMç³»ç»Ÿï¼Œèƒ½å¤Ÿç»™ä»»ä½•æ–‡æœ¬è¾“å…¥å’Œç›¸åº”çš„ä¸Šä¸‹æ–‡æ‰“åˆ†ã€‚GLIDERæ˜¾ç¤ºå‡ºé«˜äºGPT-4oçš„çš®å°”é€Šç›¸å…³ç³»æ•°ã€‚ä¸ç°æœ‰è¯„ä¼°æ¨¡å‹ç›¸æ¯”ï¼ŒGLIDERåœ¨å¤§å°ºå¯¸ä¸Šæœ‰ä¼˜è¶Šçš„è¡¨ç°ã€‚åŒæ—¶å®ƒèƒ½å¤Ÿè¿›è¡Œç²¾ç»†æ‰“åˆ†ã€å¤šè¯­è¨€æ¨ç†å’Œé«˜äº®è·¨åº¦åˆ†æã€‚å®ƒçš„è®­ç»ƒæ¶‰åŠåˆ°å¯¹å„å¤§é¢†åŸŸã€å¤šè¾¾685ä¸ªé¢†åŸŸçš„æ”¯æŒï¼Œæ¶µç›–çš„è¯„ä¼°æ ‡å‡†è¾¾åˆ°æƒŠäººçš„é«˜è¾¾çš„è·¨åº¦é«˜äº®æ–‡æœ¬å’Œä¸‰å¤§ç±»åˆ«183ä¸ªè¯„åˆ¤æ ‡å‡†ï¼æ·±åº¦å®šæ€§åˆ†æè¡¨æ˜ï¼ŒGLIDERè¯„åˆ†ä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œå…¶å‡†ç¡®æ€§é«˜è¾¾91.3%ã€‚ä¸ºäº†è®©æ›´å¤šçš„äººè¿›è¡Œç ”ç©¶å’Œå‘å±•æŠ€æœ¯æˆ‘ä»¬å·²ç»å¼€æºGLIDERæ¨¡å‹ä¾›å¤§å®¶å­¦ä¹ å‚è€ƒå’Œä½¿ç”¨äº¤æµåˆ†äº«ç ”ç©¶æˆæœä»¥æ­¤è¿›ä¸€æ­¥ä¿ƒè¿›æ¨¡å‹çš„æ›´æ–°å’Œå®Œå–„è´¡çŒ®æ™ºæ…§å’ŒåŠ›é‡è®©æŠ€æœ¯å‘å±•æ›´æœ‰æ¸©åº¦å’Œåˆ›é€ åŠ›å¹¶é‡Šæ”¾æ½œåŠ›æ›´å¥½çš„èµ‹èƒ½ç¤¾ä¼šç”Ÿäº§ç”Ÿæ´»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14140v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å¼åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹è¾“å‡ºæ–¹é¢è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç²¾ç»†ç²’åº¦æŒ‡æ ‡å’Œè§£é‡Šæ€§æŒ‘æˆ˜ï¼Œå°é—­æºLLMsæ˜¾ç¤ºå‡ºå…³é”®ç¼ºé™·ã€‚æœ¬æ–‡ä»‹ç»GLIDERæ¨¡å‹ï¼Œä¸€ç§å¼ºå¤§çš„ç”¨äºæ–‡æœ¬å’Œä¸Šä¸‹æ–‡è¯„ä»·çš„LLMï¼Œèƒ½åŸºäºç”¨æˆ·å®šä¹‰çš„ä»»æ„æ ‡å‡†å¯¹æ–‡æœ¬è¿›è¡Œè¯„åˆ†ã€‚GLIDERæ€§èƒ½å“è¶Šï¼Œå¦‚åœ¨FLASKä¸Šçš„Pearsonç›¸å…³æ€§é«˜äºGPT-4oï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰è¯„ä¼°æ¨¡å‹ï¼Œå¹¶ä¸”ç›¸å¯¹äºæŸäº›æ¨¡å‹å®ç°è·¨åŸŸæ³›åŒ–ã€‚GLIDERæ”¯æŒç²¾ç»†è¯„åˆ†ã€å¤šè¯­è¨€æ¨ç†ã€è·¨åº¦é«˜äº®ç­‰åŠŸèƒ½ï¼Œç»è¿‡åœ¨685ä¸ªé¢†åŸŸå’Œ183ä¸ªæ ‡å‡†ä¸Šçš„è®­ç»ƒåæ€§èƒ½å¼ºå¤§ã€‚å®šæ€§åˆ†ææ˜¾ç¤ºï¼ŒGLIDERå¾—åˆ†ä¸äººç±»åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œäººç±»åŒæ„åº¦è¾¾91.3%ã€‚æœ¬ç ”ç©¶å…¬å¼€äº†GLIDERæ¨¡å‹ä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä½œä¸ºè¯„åˆ¤è€…çš„æ¨¡å¼åœ¨è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹è¾“å‡ºæ–¹é¢è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚</li>
<li>å°é—­æºLLMsåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­é¢ä¸´ç²¾ç»†ç²’åº¦æŒ‡æ ‡å’Œè§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>GLIDERæ˜¯ä¸€ç§å¼ºå¤§çš„æ–‡æœ¬è¯„ä»·LLMï¼Œå¯åŸºäºç”¨æˆ·å®šä¹‰çš„ä»»æ„æ ‡å‡†å¯¹æ–‡æœ¬è¿›è¡Œè¯„åˆ†ã€‚</li>
<li>GLIDERåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰çš„è¯„ä¼°æ¨¡å‹ï¼Œå¹¶å®ç°äº†è·¨åŸŸæ³›åŒ–ã€‚</li>
<li>GLIDERæ”¯æŒç²¾ç»†è¯„åˆ†ã€å¤šè¯­è¨€æ¨ç†ã€è·¨åº¦é«˜äº®ç­‰åŠŸèƒ½ã€‚</li>
<li>GLIDERç»è¿‡åœ¨å¤šä¸ªé¢†åŸŸå’Œæ ‡å‡†ä¸Šçš„è®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15325c5425854490c51252bfb1775da0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a1e32c9f3a23768772852d54f9b705a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Future-Research-Avenues-for-Artificial-Intelligence-in-Digital-Gaming-An-Exploratory-Report"><a href="#Future-Research-Avenues-for-Artificial-Intelligence-in-Digital-Gaming-An-Exploratory-Report" class="headerlink" title="Future Research Avenues for Artificial Intelligence in Digital Gaming:   An Exploratory Report"></a>Future Research Avenues for Artificial Intelligence in Digital Gaming:   An Exploratory Report</h2><p><strong>Authors:Markus Dablander</strong></p>
<p>Video games are a natural and synergistic application domain for artificial intelligence (AI) systems, offering both the potential to enhance player experience and immersion, as well as providing valuable benchmarks and virtual environments to advance AI technologies in general. This report presents a high-level overview of five promising research pathways for applying state-of-the-art AI methods, particularly deep learning, to digital gaming within the context of the current research landscape. The objective of this work is to outline a curated, non-exhaustive list of encouraging research directions at the intersection of AI and video games that may serve to inspire more rigorous and comprehensive research efforts in the future. We discuss (i) investigating large language models as core engines for game agent modelling, (ii) using neural cellular automata for procedural game content generation, (iii) accelerating computationally expensive in-game simulations via deep surrogate modelling, (iv) leveraging self-supervised learning to obtain useful video game state embeddings, and (v) training generative models of interactive worlds using unlabelled video data. We also briefly address current technical challenges associated with the integration of advanced deep learning systems into video game development, and indicate key areas where further progress is likely to be beneficial. </p>
<blockquote>
<p>ç”µå­æ¸¸æˆæ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿçš„è‡ªç„¶ååŒåº”ç”¨é¢†åŸŸï¼Œåœ¨æé«˜ç©å®¶ä½“éªŒå’Œæ²‰æµ¸æ„Ÿçš„åŒæ—¶ï¼Œä¹Ÿä¸ºæ¨è¿›AIæŠ€æœ¯æœ¬èº«æä¾›äº†å®è´µçš„åŸºå‡†æµ‹è¯•å’Œè™šæ‹Ÿç¯å¢ƒã€‚æœ¬æŠ¥å‘Šä»å½“å‰ç ”ç©¶ç°çŠ¶å‡ºå‘ï¼Œä»‹ç»äº†å°†æœ€å‰æ²¿çš„äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œå°¤å…¶æ˜¯æ·±åº¦å­¦ä¹ åº”ç”¨äºæ•°å­—æ¸¸æˆæ—¶çš„äº”ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘çš„é«˜å±‚æ¬¡æ¦‚è¿°ã€‚è¿™é¡¹å·¥ä½œçš„ç›®çš„æ˜¯æ¦‚è¿°äººå·¥æ™ºèƒ½å’Œç”µå­æ¸¸æˆäº¤ç•Œå¤„é¼“èˆäººå¿ƒçš„ç ”ç©¶æ–¹å‘çš„éè¯¦å°½åˆ—è¡¨ï¼Œå¯èƒ½åœ¨æœªæ¥æ¿€å‘æ›´ä¸ºä¸¥æ ¼å’Œå…¨é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬è®¨è®ºäº†ï¼ˆiï¼‰ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¸¸æˆä»£ç†å»ºæ¨¡çš„æ ¸å¿ƒå¼•æ“ï¼Œï¼ˆiiï¼‰ä½¿ç”¨ç¥ç»ç½‘ç»œç»†èƒè‡ªåŠ¨æœºè¿›è¡Œæ¸¸æˆå†…å®¹çš„ç¨‹åºåŒ–ç”Ÿæˆï¼Œï¼ˆiiiï¼‰é€šè¿‡æ·±åº¦ä»£ç†å»ºæ¨¡åŠ é€Ÿæ¸¸æˆä¸­è®¡ç®—æ˜‚è´µçš„æ¨¡æ‹Ÿï¼Œï¼ˆivï¼‰åˆ©ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ è·å¾—æœ‰ç”¨çš„ç”µå­æ¸¸æˆçŠ¶æ€åµŒå…¥ï¼Œä»¥åŠï¼ˆvï¼‰ä½¿ç”¨æœªæ ‡è®°çš„è§†é¢‘æ•°æ®è®­ç»ƒäº¤äº’å¼ä¸–ç•Œçš„ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ç®€è¦ä»‹ç»äº†å°†å…ˆè¿›çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ•´åˆåˆ°æ¸¸æˆå¼€å‘ä¸­é¢ä¸´çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†è¿›ä¸€æ­¥å–å¾—è¿›å±•çš„å…³é”®é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14085v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç”µå­æ¸¸æˆæ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿåº”ç”¨çš„è‡ªç„¶å’ŒååŒé¢†åŸŸï¼Œèƒ½æå‡ç©å®¶ä½“éªŒå’Œæ²‰æµ¸æ„Ÿï¼ŒåŒæ—¶ä¸ºæ¨è¿›äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•æä¾›å®è´µçš„åŸºå‡†æµ‹è¯•å’Œè™šæ‹Ÿç¯å¢ƒã€‚æŠ¥å‘Šæ¦‚è¿°äº†å°†æœ€æ–°çš„äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œå°¤å…¶æ˜¯æ·±åº¦å­¦ä¹ ï¼Œåº”ç”¨äºæ•°å­—æ¸¸æˆçš„äº”ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚æ—¨åœ¨æå‡ºä¸€ä»½éè¯¦å°½çš„ã€é¼“èˆäººå¿ƒçš„ç ”ç©¶è·¯å¾„æ¸…å•ï¼Œä¸ºæœªæ¥åœ¨äººå·¥æ™ºèƒ½ä¸ç”µå­æ¸¸æˆäº¤å‰é¢†åŸŸè¿›è¡Œæ›´ä¸¥æ ¼å’Œå…¨é¢çš„ç ”ç©¶æä¾›çµæ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘æ¸¸æˆé¢†åŸŸæ˜¯äººå·¥æ™ºèƒ½åº”ç”¨çš„è‡ªç„¶å’ŒååŒé¢†åŸŸï¼Œæœ‰åŠ©äºæ¨è¿›AIæŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>æŠ¥å‘Šä»‹ç»äº†äº”ä¸ªåœ¨äººå·¥æ™ºèƒ½å’Œç”µå­æ¸¸æˆäº¤å‰é¢†åŸŸæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>ç ”ç©¶äººå‘˜æ­£åœ¨æ¢ç´¢å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¸¸æˆä»£ç†å»ºæ¨¡çš„æ ¸å¿ƒå¼•æ“ã€‚</li>
<li>ç¥ç»ç»†èƒè‡ªåŠ¨æœºè¢«ç”¨äºç¨‹åºåŒ–æ¸¸æˆå†…å®¹ç”Ÿæˆã€‚</li>
<li>æ·±åº¦æ›¿èº«å»ºæ¨¡è¢«ç”¨æ¥åŠ é€Ÿæ¸¸æˆä¸­è®¡ç®—é‡å¤§çš„æ¨¡æ‹Ÿã€‚</li>
<li>è‡ªæˆ‘ç›‘ç£å­¦ä¹ è¢«ç”¨æ¥è·å–æœ‰ç”¨çš„è§†é¢‘æ¸¸æˆçŠ¶æ€åµŒå…¥ã€‚</li>
<li>ä½¿ç”¨æ— æ ‡ç­¾è§†é¢‘æ•°æ®è®­ç»ƒäº¤äº’ä¸–ç•Œçš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-422f7d88488cd98db5c08eb4a752ee1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663d0ac5160695abe70932f1afb28e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb7b844ff71a2ccf70e3602ccf2fd69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-595ae1ff4a0eef49829faf8b926491c6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rango-Adaptive-Retrieval-Augmented-Proving-for-Automated-Software-Verification"><a href="#Rango-Adaptive-Retrieval-Augmented-Proving-for-Automated-Software-Verification" class="headerlink" title="Rango: Adaptive Retrieval-Augmented Proving for Automated Software   Verification"></a>Rango: Adaptive Retrieval-Augmented Proving for Automated Software   Verification</h2><p><strong>Authors:Kyle Thompson, Nuno Saavedra, Pedro Carrott, Kevin Fisher, Alex Sanchez-Stern, Yuriy Brun, JoÃ£o F. Ferreira, Sorin Lerner, Emily First</strong></p>
<p>Formal verification using proof assistants, such as Coq, enables the creation of high-quality software. However, the verification process requires significant expertise and manual effort to write proofs. Recent work has explored automating proof synthesis using machine learning and large language models (LLMs). This work has shown that identifying relevant premises, such as lemmas and definitions, can aid synthesis. We present Rango, a fully automated proof synthesis tool for Coq that automatically identifies relevant premises and also similar proofs from the current project and uses them during synthesis. Rango uses retrieval augmentation at every step of the proof to automatically determine which proofs and premises to include in the context of its fine-tuned LLM. In this way, Rango adapts to the project and to the evolving state of the proof. We create a new dataset, CoqStoq, of 2,226 open-source Coq projects and 196,929 theorems from GitHub, which includes both training data and a curated evaluation benchmark of well-maintained projects. On this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is 29% more theorems than the prior state-of-the-art tool Tactician. Our evaluation also shows that Rango adding relevant proofs to its context leads to a 47% increase in the number of theorems proven. </p>
<blockquote>
<p>ä½¿ç”¨Coqç­‰è¯æ˜åŠ©æ‰‹è¿›è¡Œå½¢å¼åŒ–éªŒè¯æœ‰åŠ©äºåˆ›å»ºé«˜è´¨é‡çš„è½¯ä»¶ã€‚ç„¶è€Œï¼ŒéªŒè¯è¿‡ç¨‹éœ€è¦å¤§é‡çš„ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨ç¼–å†™è¯æ˜çš„åŠªåŠ›ã€‚æœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åˆæˆè¯æ˜çš„æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯†åˆ«ç›¸å…³çš„å‰æï¼Œå¦‚å¼•ç†å’Œå®šä¹‰ï¼Œå¯ä»¥è¾…åŠ©åˆæˆã€‚æˆ‘ä»¬æå‡ºäº†Rangoï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„Coqè¯æ˜åˆæˆå·¥å…·ï¼Œå®ƒä¼šè‡ªåŠ¨è¯†åˆ«ç›¸å…³çš„å‰æå’Œå½“å‰é¡¹ç›®ä¸­çš„ç±»ä¼¼è¯æ˜ï¼Œå¹¶åœ¨åˆæˆè¿‡ç¨‹ä¸­ä½¿ç”¨å®ƒä»¬ã€‚Rangoåœ¨è¯æ˜çš„æ¯ä¸ªæ­¥éª¤ä¸­éƒ½ä½¿ç”¨æ£€ç´¢å¢å¼ºæ¥è‡ªåŠ¨ç¡®å®šè¦åŒ…å«åœ¨å…¶å¾®è°ƒLLMä¸Šä¸‹æ–‡ä¸­çš„è¯æ˜å’Œå‰æã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRangoé€‚åº”äºé¡¹ç›®å’Œä¸æ–­å˜åŒ–çš„è¯æ˜çŠ¶æ€ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CoqStoqï¼Œå®ƒåŒ…æ‹¬æ¥è‡ªGitHubçš„2226ä¸ªå¼€æºCoqé¡¹ç›®å’Œ196929ä¸ªå®šç†ï¼Œå…¶ä¸­åŒ…æ‹¬è®­ç»ƒæ•°æ®å’Œç²¾é€‰çš„ç»´æŠ¤è‰¯å¥½çš„é¡¹ç›®è¯„ä¼°åŸºå‡†ã€‚åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒRangoåˆæˆäº†32.0%çš„å®šç†çš„è¯æ˜ï¼Œæ¯”ç°æœ‰æœ€å…ˆè¿›çš„å·¥å…·Tacticianå¤šè¯æ˜äº†29%çš„å®šç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿˜è¡¨æ˜ï¼ŒRangoå°†å…¶ä¸Šä¸‹æ–‡ä¸­çš„ç›¸å…³è¯æ˜å¢åŠ å¯¼è‡´è¯æ˜äº†47%çš„å®šç†æ•°é‡å¢åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14063v1">PDF</a> In Proceedings of the 47th International Conference on Software   Engineering (ICSE), Ottawa, ON, Canada, April 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å½¢å¼åŒ–éªŒè¯ä½¿ç”¨Coqç­‰è¯æ˜åŠ©æ‰‹å¯ä»¥åˆ›å»ºé«˜è´¨é‡çš„è½¯ä»¶ï¼Œä½†éªŒè¯è¿‡ç¨‹éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œå¤§é‡æ‰‹åŠ¨ç¼–å†™è¯æ˜çš„å·¥ä½œã€‚æœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨åˆæˆè¯æ˜çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†Rangoï¼Œä¸€ä¸ªå…¨è‡ªåŠ¨çš„Coqè¯æ˜åˆæˆå·¥å…·ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨è¯†åˆ«ç›¸å…³çš„å‰æå’Œç±»ä¼¼è¯æ˜ï¼Œå¹¶åœ¨åˆæˆè¿‡ç¨‹ä¸­ä½¿ç”¨å®ƒä»¬ã€‚Rangoåœ¨è¯æ˜çš„æ¯ä¸ªæ­¥éª¤ä¸­éƒ½ä½¿ç”¨æ£€ç´¢å¢å¼ºæ¥è‡ªåŠ¨ç¡®å®šåŒ…å«å“ªäº›è¯æ˜å’Œå‰æï¼Œä»¥é€‚åº”é¡¹ç›®å’Œè¯æ˜çš„ä¸æ–­æ¼”å˜çŠ¶æ€ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†CoqStoqï¼ŒåŒ…å«æ¥è‡ªGitHubçš„2226ä¸ªå¼€æºCoqé¡¹ç›®å’Œ196929ä¸ªå®šç†ï¼Œå…¶ä¸­åŒ…æ‹¬è®­ç»ƒæ•°æ®å’Œç²¾é€‰çš„è¯„ä¼°åŸºå‡†æµ‹è¯•é¡¹ç›®ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRangoèƒ½å¤Ÿåˆæˆ32.0%çš„å®šç†è¯æ˜ï¼Œæ¯”ç°æœ‰æœ€å…ˆè¿›çš„å·¥å…·Tacticianå¤šè¯æ˜äº†29%çš„å®šç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿˜æ˜¾ç¤ºï¼ŒRangoå°†å…¶ç›¸å…³è¯æ˜æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œå¯¼è‡´è¯æ˜çš„å®šç†æ•°é‡å¢åŠ äº†47%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½¢å¼åŒ–éªŒè¯ä½¿ç”¨è¯æ˜åŠ©æ‰‹å¦‚Coqå¯¹äºåˆ›å»ºé«˜è´¨é‡è½¯ä»¶è‡³å…³é‡è¦ã€‚</li>
<li>è‡ªåŠ¨åŒ–è¯æ˜åˆæˆæ­£åœ¨å—åˆ°å…³æ³¨ï¼Œå¹¶å·²ç»å¼€å§‹ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>Rangoæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„Coqè¯æ˜åˆæˆå·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«ç›¸å…³çš„å‰æå’Œç±»ä¼¼è¯æ˜ã€‚</li>
<li>Rangoåœ¨è¯æ˜çš„æ¯ä¸ªæ­¥éª¤éƒ½ä½¿ç”¨æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”é¡¹ç›®å’Œè¯æ˜çš„ä¸æ–­æ¼”å˜çŠ¶æ€ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªåä¸ºCoqStoqçš„æ–°æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªGitHubçš„å¼€æºCoqé¡¹ç›®å’Œå®šç†ã€‚</li>
<li>Rangoåœ¨åŸºå‡†æµ‹è¯•ä¸­æˆåŠŸè¯æ˜äº†æ¯”ç°æœ‰å·¥å…·æ›´å¤šçš„å®šç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2ab9b8f0681a7c7da9861dbc65e082b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3415cc8f7740133a2354712cbb95dcc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-179eeede4918a64749c32a71c41b81a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d3bb92e3ae941be7e0b0fc55fc72710.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1647b9998dee011019007d394a9d964.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Review-of-Multimodal-Explainable-Artificial-Intelligence-Past-Present-and-Future"><a href="#A-Review-of-Multimodal-Explainable-Artificial-Intelligence-Past-Present-and-Future" class="headerlink" title="A Review of Multimodal Explainable Artificial Intelligence: Past,   Present and Future"></a>A Review of Multimodal Explainable Artificial Intelligence: Past,   Present and Future</h2><p><strong>Authors:Shilin Sun, Wenbin An, Feng Tian, Fang Nan, Qidong Liu, Jun Liu, Nazaraf Shah, Ping Chen</strong></p>
<p>Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the â€œblack-boxâ€ nature of AI models. To address these concerns, eXplainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal eXplainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions. A project related to this review has been created at <a target="_blank" rel="noopener" href="https://github.com/ShilinSun/mxai_review">https://github.com/ShilinSun/mxai_review</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é€šè¿‡è®¡ç®—èƒ½åŠ›çš„è¿›æ­¥å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„å¢é•¿è€Œè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›æ­¥ä¹Ÿå¢åŠ äº†å¯¹è§£é‡ŠAIæ¨¡å‹â€œé»‘ç®±â€æ€§è´¨çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œä»¥é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ä¸ºé‡ç‚¹çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰çš„å‡ºç°ï¼Œå¢å¼ºäº†äººç±»å¯¹AIå†³ç­–è¿‡ç¨‹çš„äº†è§£å’Œä¿¡ä»»ã€‚åœ¨å¤šæ¨¡æ€æ•°æ®èåˆå’Œå¤æ‚æ¨ç†åœºæ™¯ä¸­ï¼Œå¤šæ¨¡æ€å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆMXAIï¼‰çš„æå‡ºèåˆäº†å¤šç§æ¨¡æ€æ¥è¿›è¡Œé¢„æµ‹å’Œè§£é‡Šä»»åŠ¡ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè™½ç„¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„çªç ´ï¼Œä½†å…¶å¤æ‚æ€§è¿›ä¸€æ­¥åŠ å‰§äº†MXAIçš„é—®é¢˜ã€‚ä¸ºäº†æ·±å…¥äº†è§£MXAIæ–¹æ³•çš„å‘å±•ï¼Œå¹¶ä¸ºæ„å»ºæ›´é€æ˜ã€å…¬å¹³å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿæä¾›å…³é”®æŒ‡å¯¼ï¼Œæˆ‘ä»¬ä»å†å²çš„è§’åº¦å›é¡¾äº†MXAIæ–¹æ³•ï¼Œå¹¶æ ¹æ®å››ä¸ªæ—¶ä»£è¿›è¡Œåˆ†ç±»ï¼šä¼ ç»Ÿæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€åˆ¤åˆ«åŸºç¡€æ¨¡å‹å’Œç”ŸæˆLLMã€‚æˆ‘ä»¬è¿˜å›é¡¾äº†MXAIç ”ç©¶ä¸­ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ï¼Œæœ€åè®¨è®ºäº†æœªæ¥çš„æŒ‘æˆ˜å’Œæ–¹å‘ã€‚æœ‰å…³æ­¤å®¡æŸ¥çš„é¡¹ç›®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShilinSun/mxai_review%E5%88%9B%E5%BB%BA%E3%80%82">https://github.com/ShilinSun/mxai_reviewåˆ›å»ºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14056v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•è¿…é€Ÿï¼Œä¼´éšè®¡ç®—èƒ½åŠ›çš„è¿›æ­¥å’Œæµ·é‡æ•°æ®çš„å¢é•¿ï¼Œå¸¦æ¥äº†è§£é‡Šæ€§æŒ‘æˆ˜ï¼ˆâ€œé»‘ç®±â€æ€§è´¨ï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå‡ºç°äº†è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ï¼Œæ³¨é‡é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ï¼Œæå‡äººç±»å¯¹AIå†³ç­–è¿‡ç¨‹çš„ç†è§£å’Œä¿¡ä»»ã€‚åœ¨å¤šæ¨¡æ€æ•°æ®èåˆå’Œå¤æ‚åœºæ™¯æ¨ç†ä¸­ï¼Œå¤šæ¨¡æ€è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆMXAIï¼‰æ•´åˆå¤šç§æ¨¡æ€è¿›è¡Œé¢„æµ‹å’Œè§£é‡Šä»»åŠ¡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥çªç ´ï¼Œä½†å…¶å¤æ‚æ€§è¿›ä¸€æ­¥åŠ å‰§äº†MXAIçš„é—®é¢˜ã€‚æœ¬æ–‡å›é¡¾äº†MXAIæ–¹æ³•çš„å†å²å‘å±•ï¼Œå°†å…¶åˆ†ä¸ºä¼ ç»Ÿæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€åˆ¤åˆ«åŸºç¡€æ¨¡å‹å’Œç”Ÿæˆå‹LLMå››ä¸ªæ—¶ä»£ï¼Œå¹¶è¯„è¿°äº†MXAIç ”ç©¶ä¸­çš„è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ï¼Œæœ€åè®¨è®ºäº†æœªæ¥çš„æŒ‘æˆ˜å’Œå‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•ä¼´éšç€è§£é‡Šæ€§çš„æŒ‘æˆ˜ï¼Œå³â€œé»‘ç®±â€æ€§è´¨ã€‚</li>
<li>è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ³¨é‡é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ï¼Œæ—¨åœ¨æå‡äººç±»å¯¹AIå†³ç­–è¿‡ç¨‹çš„äº†è§£å’Œä¿¡ä»»ã€‚</li>
<li>å¤šæ¨¡æ€è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆMXAIï¼‰åœ¨å¤šæ¨¡æ€æ•°æ®èåˆå’Œå¤æ‚åœºæ™¯æ¨ç†ä¸­æ•´åˆå¤šç§æ¨¡æ€è¿›è¡Œé¢„æµ‹å’Œè§£é‡Šã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥çªç ´ï¼Œä½†å¢åŠ äº†AIçš„å¤æ‚æ€§ï¼Œå¯¹MXAIæå‡ºæ–°çš„æŒ‘æˆ˜ã€‚</li>
<li>MXAIæ–¹æ³•çš„å†å²å‘å±•å¯åˆ†ä¸ºä¼ ç»Ÿæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€åˆ¤åˆ«åŸºç¡€æ¨¡å‹å’Œç”Ÿæˆå‹LLMå››ä¸ªæ—¶ä»£ã€‚</li>
<li>åœ¨MXAIç ”ç©¶ä¸­ï¼Œè¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†çš„é€‰æ‹©è‡³å…³é‡è¦ã€‚</li>
<li>æœªæ¥çš„MXAIå‘å±•ä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿›ä¸€æ­¥æé«˜é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€åº”å¯¹å¤æ‚æ•°æ®çš„æŒ‘æˆ˜ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2c50c8f8fa399b9eb55df65c59cee315.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b1b0fbf9ac368f7cf2f58d04290a7a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2373beec2c6ece8a8a0a307608639dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c94eb54126a053a495f1cbddbb07dee8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b786e0f3eb4ea4d32cddfaf0be9601a9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds"><a href="#CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds" class="headerlink" title="CAD-Recode: Reverse Engineering CAD Code from Point Clouds"></a>CAD-Recode: Reverse Engineering CAD Code from Point Clouds</h2><p><strong>Authors:Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</strong></p>
<p>Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained solely on a proposed synthetic dataset of one million diverse CAD sequences. CAD-Recode significantly outperforms existing methods across three datasets while requiring fewer input points. Notably, it achieves 10 times lower mean Chamfer distance than state-of-the-art methods on DeepCAD and Fusion360 datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡ä¾æ¬¡ç»˜åˆ¶å‚æ•°åŒ–è‰å›¾å¹¶åº”ç”¨CADæ“ä½œæ¥è·å¾—ä¸‰ç»´æ¨¡å‹ã€‚3D CADé€†å‘å·¥ç¨‹çš„é—®é¢˜åœ¨äºä»ç‚¹äº‘ç­‰3Dè¡¨ç¤ºä¸­é‡å»ºè‰å›¾å’ŒCADæ“ä½œåºåˆ—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ–¹é¢çš„æ–°è´¡çŒ®æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼šCADåºåˆ—è¡¨ç¤ºã€ç½‘ç»œè®¾è®¡å’Œæ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†CADè‰å›¾æŒ¤å‹åºåˆ—è¡¨ç¤ºä¸ºPythonä»£ç ã€‚æ‰€æå‡ºçš„CAD-Recodeå°†ç‚¹äº‘è½¬æ¢ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œè¯¥ä»£ç å³å¯é‡å»ºCADæ¨¡å‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹Pythonä»£ç çš„æš´éœ²ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªè¾ƒå°çš„LLMä½œä¸ºCAD-Recodeçš„è§£ç å™¨ï¼Œå¹¶å°†å…¶ä¸ä¸€ä¸ªè½»é‡çº§çš„ç‚¹äº‘æŠ•å½±ä»ªç›¸ç»“åˆã€‚CAD-Recodeä»…åœ¨ä¸€ç™¾ä¸‡ä¸ªå¤šæ ·åŒ–çš„åˆæˆCADåºåˆ—æ‰€æ„æˆçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼ŒCAD-Recodeæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”éœ€è¦çš„è¾“å…¥ç‚¹æ•°æ›´å°‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šå®ç°äº†æ¯”æœ€æ–°æŠ€æœ¯ä½10å€çš„å¹³å‡Chamferè·ç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„CAD Pythonä»£ç è¾“å‡ºå¯ä»¥è¢«å¸‚é¢ä¸Šçš„LLMè§£é‡Šï¼Œä»è€Œèƒ½å¤Ÿæœ‰ç‚¹äº‘è¿›è¡ŒCADç¼–è¾‘å’Œç‰¹å®šçš„CADé—®é¢˜å›ç­”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14042v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„é€†å‘å·¥ç¨‹é—®é¢˜ï¼Œé€šè¿‡ä¸‰ä¸ªå±‚é¢çš„åˆ›æ–°è´¡çŒ®å®ç°äº†ä»ç‚¹äº‘åˆ°CADè®¾è®¡åºåˆ—çš„è½¬æ¢ã€‚æå‡ºå°†CADè‰å›¾æŒ¤å‹åºåˆ—è¡¨ç¤ºä¸ºPythonä»£ç ï¼Œå¹¶å¼€å‘äº†CAD-Recodeç³»ç»Ÿï¼Œèƒ½å°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œåé‡å»ºCADæ¨¡å‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹Pythonä»£ç çš„æš´éœ²ï¼Œç»“åˆè½»é‡çº§ç‚¹äº‘æŠ•å½±ä»ªï¼Œå®ç°äº†åœ¨å°‘é‡æ•°æ®ä¸‹çš„è®­ç»ƒã€‚CAD-Recodeåœ¨åˆæˆæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒä½çš„Chamferè·ç¦»ã€‚æ­¤å¤–ï¼Œè¾“å‡ºçš„CAD Pythonä»£ç å…·æœ‰å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿè¢«å³ç”¨çš„LLMç”¨äºCADç¼–è¾‘å’Œå›ç­”æœ‰å…³ç‚¹äº‘çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADæ¨¡å‹çš„é€†å‘å·¥ç¨‹é—®é¢˜æ¶‰åŠä»3Dè¡¨ç¤ºï¼ˆå¦‚ç‚¹äº‘ï¼‰é‡å»ºè‰å›¾å’Œè®¾è®¡åºåˆ—ã€‚</li>
<li>CAD-Recodeç³»ç»Ÿå°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç ï¼Œé‡ç°CADæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†Pythonä»£ç ï¼Œç»“åˆè½»é‡çº§ç‚¹äº‘æŠ•å½±ä»ªå®ç°é«˜æ•ˆè§£ç ã€‚</li>
<li>CAD-Recodeåœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CAD-Recodeåœ¨DeepCADå’ŒFusion360æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒä½çš„Chamferè·ç¦»ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>è¾“å‡ºçš„CAD Pythonä»£ç å…·æœ‰å¯è§£é‡Šæ€§ï¼Œä¾¿äºäººç±»ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e25d7eb0cc8854f180ad0774b6b068b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22b5fe8fec241094bc912f8d9be7a1de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25b7780bc0f0fc4b40cf19519278cc37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b82201bed22ed54cd867c9af5861d923.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e03a5ce6205dac33a10bf58a58af4c83.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Hansel-Output-Length-Controlling-Framework-for-Large-Language-Models"><a href="#Hansel-Output-Length-Controlling-Framework-for-Large-Language-Models" class="headerlink" title="Hansel: Output Length Controlling Framework for Large Language Models"></a>Hansel: Output Length Controlling Framework for Large Language Models</h2><p><strong>Authors:Seoha Song, Junhyun Lee, Hyeonmok Ko</strong></p>
<p>Despite the great success of large language models (LLMs), efficiently controlling the length of the output sequence still remains a challenge. In this paper, we propose Hansel, an efficient framework for length control in LLMs without affecting its generation ability. Hansel utilizes periodically outputted hidden special tokens to keep track of the remaining target length of the output sequence. Together with techniques to avoid abrupt termination of the output, this seemingly simple method proved to be efficient and versatile, while not harming the coherency and fluency of the generated text. The framework can be applied to any pre-trained LLMs during the finetuning stage of the model, regardless of its original positional encoding method. We demonstrate this by finetuning four different LLMs with Hansel and show that the mean absolute error of the output sequence decreases significantly in every model and dataset compared to the prompt-based length control finetuning. Moreover, the framework showed a substantially improved ability to extrapolate to target lengths unseen during finetuning, such as long dialog responses or extremely short summaries. This indicates that the model learns the general means of length control, rather than learning to match output lengths to those seen during training. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†æœ‰æ•ˆåœ°æ§åˆ¶è¾“å‡ºåºåˆ—çš„é•¿åº¦ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Hanselï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨LLMä¸­è¿›è¡Œé•¿åº¦æ§åˆ¶çš„é«˜æ•ˆæ¡†æ¶ï¼Œä¸ä¼šå½±å“å…¶ç”Ÿæˆèƒ½åŠ›ã€‚Hanselåˆ©ç”¨å®šæœŸè¾“å‡ºçš„éšè—ç‰¹æ®Šæ ‡è®°æ¥è·Ÿè¸ªè¾“å‡ºåºåˆ—çš„å‰©ä½™ç›®æ ‡é•¿åº¦ã€‚ç»“åˆé¿å…è¾“å‡ºçªç„¶ç»ˆæ­¢çš„æŠ€æœ¯ï¼Œè¿™ç§çœ‹ä¼¼ç®€å•çš„æ–¹æ³•è¢«è¯æ˜æ˜¯é«˜æ•ˆå’Œé€šç”¨çš„ï¼ŒåŒæ—¶ä¸ä¼šæŸå®³ç”Ÿæˆæ–‡æœ¬çš„ä¸€è‡´æ€§å’Œæµç•…æ€§ã€‚è¯¥æ¡†æ¶å¯åº”ç”¨äºæ¨¡å‹å¾®è°ƒé˜¶æ®µçš„ä»»ä½•é¢„è®­ç»ƒLLMï¼Œè€Œæ— éœ€è€ƒè™‘å…¶åŸå§‹çš„ä½ç½®ç¼–ç æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨Hanselå¯¹å››ç§ä¸åŒçš„LLMè¿›è¡Œå¾®è°ƒæ¥è¯æ˜è¿™ä¸€ç‚¹ï¼Œå¹¶è¡¨æ˜ä¸åŸºäºæç¤ºçš„é•¿åº¦æ§åˆ¶å¾®è°ƒç›¸æ¯”ï¼Œåœ¨æ¯ä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šï¼Œè¾“å‡ºåºåˆ—çš„å¹³å‡ç»å¯¹è¯¯å·®éƒ½ä¼šæ˜¾è‘—é™ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨å¾®è°ƒæœŸé—´æœªè§çš„ç›®æ ‡é•¿åº¦ä¸Šè¡¨ç°å‡ºäº†æå¤§çš„å¤–æ¨èƒ½åŠ›ï¼Œä¾‹å¦‚é•¿å¯¹è¯å“åº”æˆ–æå…¶ç®€çŸ­çš„æ‘˜è¦ã€‚è¿™è¡¨æ˜æ¨¡å‹å­¦ä¹ çš„æ˜¯ä¸€èˆ¬çš„é•¿åº¦æ§åˆ¶æ–¹æ³•ï¼Œè€Œä¸æ˜¯å­¦ä¹ å°†è¾“å‡ºé•¿åº¦ä¸è®­ç»ƒæœŸé—´æ‰€è§åˆ°çš„ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14033v1">PDF</a> 13 pages, 6 figures; accepted to AAAI-25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHanselçš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æœ‰æ•ˆæ§åˆ¶è¾“å‡ºåºåˆ—çš„é•¿åº¦ï¼ŒåŒæ—¶ä¸æŸå®³æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚Hanselé€šè¿‡å®šæœŸè¾“å‡ºç‰¹æ®Šçš„éšè—æ ‡è®°æ¥è·Ÿè¸ªå‰©ä½™çš„ç›®æ ‡é•¿åº¦ï¼Œå¹¶é¿å…è¾“å‡ºåºåˆ—çš„çªç„¶ç»ˆæ­¢ã€‚è¯¥æ–¹æ³•æ—¢é«˜æ•ˆåˆé€šç”¨ï¼ŒåŒæ—¶ä¸å½±å“ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§å’Œæµç•…æ€§ã€‚è¯¥æ¡†æ¶å¯åœ¨æ¨¡å‹å¾®è°ƒé˜¶æ®µåº”ç”¨äºä»»ä½•é¢„è®­ç»ƒLLMï¼Œæ— è®ºå…¶åŸå§‹ä½ç½®ç¼–ç æ–¹æ³•å¦‚ä½•ã€‚å®éªŒè¯æ˜ï¼ŒHanselèƒ½æ˜¾è‘—æé«˜è¾“å‡ºåºåˆ—çš„å‡†ç¡®åº¦ï¼Œå¹¶å±•ç°å‡ºå¯¹æœªè§è¿‡çš„ç›®æ ‡é•¿åº¦çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Hanselæ¡†æ¶å®ç°äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¯¹è¾“å‡ºåºåˆ—é•¿åº¦çš„æœ‰æ•ˆæ§åˆ¶ã€‚</li>
<li>Hanselåˆ©ç”¨å®šæœŸè¾“å‡ºçš„éšè—ç‰¹æ®Šæ ‡è®°æ¥è·Ÿè¸ªç›®æ ‡é•¿åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¢ä¸ä¼šæŸå®³æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¹Ÿä¸ä¼šå½±å“è¾“å‡ºæ–‡æœ¬çš„è¿è´¯æ€§å’Œæµç•…æ€§ã€‚</li>
<li>Hanselæ¡†æ¶å¯åœ¨æ¨¡å‹å¾®è°ƒé˜¶æ®µåº”ç”¨äºæ‰€æœ‰é¢„è®­ç»ƒçš„LLMã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒHanselèƒ½æ˜¾è‘—é™ä½è¾“å‡ºåºåˆ—çš„å¹³å‡ç»å¯¹è¯¯å·®ã€‚</li>
<li>Hanselèƒ½æé«˜æ¨¡å‹å¯¹æœªè§è¿‡çš„ç›®æ ‡é•¿åº¦çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bf8353e6f5198abe2570a971b490a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d3e67d989824fd7527b849c309671fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55b75426997b87190422ee22a3eea19e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35b28f955436475ebb37be4915d58104.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-169d4718a84cf1c196468e80820e2d2a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Cognition-Chain-for-Explainable-Psychological-Stress-Detection-on-Social-Media"><a href="#Cognition-Chain-for-Explainable-Psychological-Stress-Detection-on-Social-Media" class="headerlink" title="Cognition Chain for Explainable Psychological Stress Detection on Social   Media"></a>Cognition Chain for Explainable Psychological Stress Detection on Social   Media</h2><p><strong>Authors:Xin Wang, Boyan Gao, Yi Dai, Lei Cao, Liang Zhao, Yibo Yang, David Clifton</strong></p>
<p>Stress is a pervasive global health issue that can lead to severe mental health problems. Early detection offers timely intervention and prevention of stress-related disorders. The current early detection models perform â€œblack boxâ€ inference suffering from limited explainability and trust which blocks the real-world clinical application. Thanks to the generative properties introduced by the Large Language Models (LLMs), the decision and the prediction from such models are semi-interpretable through the corresponding description. However, the existing LLMs are mostly trained for general purposes without the guidance of psychological cognitive theory. To this end, we first highlight the importance of prior theory with the observation of performance boosted by the chain-of-thoughts tailored for stress detection. This method termed Cognition Chain explicates the generation of stress through a step-by-step cognitive perspective based on cognitive appraisal theory with a progress pipeline: Stimulus $\rightarrow$ Evaluation $\rightarrow$ Reaction $\rightarrow$ Stress State, guiding LLMs to provide comprehensive reasoning explanations. We further study the benefits brought by the proposed Cognition Chain format by utilising it as a synthetic dataset generation template for LLMs instruction-tuning and introduce CogInstruct, an instruction-tuning dataset for stress detection. This dataset is developed using a three-stage self-reflective annotation pipeline that enables LLMs to autonomously generate and refine instructional data. By instruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable stress detection model. Evaluations demonstrate that CogLLM achieves outstanding performance while enhancing explainability. Our work contributes a novel approach by integrating cognitive theories into LLM reasoning processes, offering a promising direction for future explainable AI research. </p>
<blockquote>
<p>å‹åŠ›æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„å…¨çƒå¥åº·é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡çš„å¿ƒç†å¥åº·é—®é¢˜ã€‚æ—©æœŸå‘ç°å¯ä»¥ä¸ºå¹²é¢„å’Œé¢„é˜²ä¸å‹åŠ›ç›¸å…³çš„ç–¾ç—…æä¾›åŠæ—¶çš„æœºä¼šã€‚å½“å‰æ—©æœŸæ£€æµ‹æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹å­˜åœ¨â€œé»‘ç®±â€é—®é¢˜ï¼Œç¼ºä¹è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ï¼Œé˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠåº”ç”¨ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå±æ€§ï¼Œè¿™äº›æ¨¡å‹çš„å†³ç­–å’Œé¢„æµ‹å¯ä»¥é€šè¿‡ç›¸åº”çš„æè¿°è¿›è¡ŒåŠè§£é‡Šã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMå¤§å¤šæ˜¯ä¸ºäº†é€šç”¨ç›®çš„è€Œè®­ç»ƒçš„ï¼Œæ²¡æœ‰å¿ƒç†è®¤çŸ¥ç†è®ºçš„æŒ‡å¯¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè¦å¼ºè°ƒå…ˆéªŒç†è®ºçš„é‡è¦æ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°é€šè¿‡é’ˆå¯¹å‹åŠ›æ£€æµ‹çš„æ€ç»´é“¾è€Œæé«˜çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºè®¤çŸ¥é“¾ï¼Œå®ƒé€šè¿‡åŸºäºè®¤çŸ¥è¯„ä¼°ç†è®ºçš„é€æ­¥è®¤çŸ¥è§†è§’æ¥è§£é‡Šå‹åŠ›çš„äº§ç”Ÿï¼Œå…¶è¿›å±•ç®¡é“åŒ…æ‹¬ï¼šåˆºæ¿€â†’è¯„ä¼°â†’ååº”â†’å‹åŠ›çŠ¶æ€ï¼ŒæŒ‡å¯¼LLMæä¾›å…¨é¢çš„æ¨ç†è§£é‡Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†æ‰€æå‡ºçš„è®¤çŸ¥é“¾æ ¼å¼å¸¦æ¥çš„å¥½å¤„ï¼Œé€šè¿‡å°†å…¶ç”¨ä½œLLMæŒ‡ä»¤è°ƒæ•´çš„åˆæˆæ•°æ®é›†ç”Ÿæˆæ¨¡æ¿ï¼Œå¹¶å¼•å…¥äº†ç”¨äºå‹åŠ›æ£€æµ‹çš„CogInstructæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é‡‡ç”¨ä¸‰é˜¶æ®µè‡ªæˆ‘åæ€æ³¨é‡Šç®¡é“å¼€å‘ï¼Œä½¿LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå’Œç»†åŒ–æŒ‡ä»¤æ•°æ®ã€‚é€šè¿‡ç”¨CogInstructæŒ‡ä»¤è°ƒæ•´Llama3ï¼Œæˆ‘ä»¬å¼€å‘äº†å¯è§£é‡Šçš„å‹åŠ›æ£€æµ‹æ¨¡å‹CogLLMã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCogLLMåœ¨æå‡è§£é‡Šæ€§çš„åŒæ—¶å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å°†è®¤çŸ¥ç†è®ºæ•´åˆåˆ°LLMæ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸ºæœªæ¥çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14009v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å‹åŠ›æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¥åº·é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡çš„å¿ƒç†å¥åº·é—®é¢˜ã€‚æ—©æœŸæ£€æµ‹å¯åŠæ—¶å¹²é¢„å’Œé¢„é˜²ä¸å‹åŠ›ç›¸å…³çš„ç–¾ç—…ã€‚å½“å‰æ—©æœŸæ£€æµ‹æ¨¡å‹å­˜åœ¨è§£é‡Šæ€§å’Œä¿¡ä»»åº¦æœ‰é™çš„â€œé»‘ç®±â€æ¨ç†é—®é¢˜ï¼Œé˜»ç¢äº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸´åºŠä¸­çš„åº”ç”¨ã€‚å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå±æ€§ï¼Œå†³ç­–å’Œé¢„æµ‹å¯é€šè¿‡ç›¸åº”æè¿°å®ç°åŠè§£é‡Šã€‚ç„¶è€Œï¼Œç°æœ‰LLMå¤§å¤šç”¨äºé€šç”¨ç›®çš„è®­ç»ƒï¼Œç¼ºä¹å¿ƒç†å­¦è®¤çŸ¥ç†è®ºçš„æŒ‡å¯¼ã€‚æœ¬ç ”ç©¶å¼ºè°ƒå…ˆéªŒç†è®ºçš„é‡è¦æ€§ï¼Œå¹¶è§‚å¯Ÿåˆ°é€šè¿‡é’ˆå¯¹å‹åŠ›æ£€æµ‹çš„æ€ç»´é“¾ï¼ˆCognition Chainï¼‰æå‡æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŸºäºè®¤çŸ¥è¯„ä¼°ç†è®ºçš„é€æ­¥è®¤çŸ¥è§†è§’ï¼Œé€šè¿‡åˆºæ¿€â†’è¯„ä¼°â†’ååº”â†’å‹åŠ›çŠ¶æ€çš„è¿‡ç¨‹ç®¡é“æ¥è§£é‡Šå‹åŠ›çš„äº§ç”Ÿã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†è®¤çŸ¥é“¾æ ¼å¼å¸¦æ¥çš„å¥½å¤„ï¼Œé€šè¿‡å°†å…¶ç”¨ä½œLLMæŒ‡ä»¤è°ƒæ•´çš„åˆæˆæ•°æ®é›†æ¨¡æ¿ï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹å‹åŠ›æ£€æµ‹çš„æŒ‡ä»¤æ•°æ®é›†CogInstructã€‚è¯¥æ•°æ®é›†é‡‡ç”¨ä¸‰é˜¶æ®µè‡ªæˆ‘åæ€æ³¨é‡Šç®¡é“å¼€å‘ï¼Œä½¿LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆå’Œç»†åŒ–æŒ‡ä»¤æ•°æ®ã€‚é€šè¿‡ç”¨CogInstructæŒ‡ä»¤è°ƒæ•´Llama3ï¼Œæˆ‘ä»¬å¼€å‘äº†å¯è§£é‡Šçš„å‹åŠ›æ£€æµ‹æ¨¡å‹CogLLMã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCogLLMåœ¨å–å¾—å“è¶Šæ€§èƒ½çš„åŒæ—¶æé«˜äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æ•´åˆè®¤çŸ¥ç†è®ºåˆ°LLMçš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä¸ºæœªæ¥çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‹åŠ›æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¥åº·é—®é¢˜ï¼Œæ—©æœŸæ£€æµ‹å¯¹åŠæ—¶å¹²é¢„å’Œé¢„é˜²å‹åŠ›ç›¸å…³ç–¾ç—…è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ—©æœŸæ£€æµ‹æ¨¡å‹å­˜åœ¨â€œé»‘ç®±â€æ¨ç†é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸´åºŠä¸­çš„åº”ç”¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå±æ€§å¯ä»¥æé«˜æ¨¡å‹çš„åŠè§£é‡Šæ€§ã€‚</li>
<li>ç°æœ‰LLMå¤§å¤šç¼ºä¹å¿ƒç†å­¦è®¤çŸ¥ç†è®ºçš„æŒ‡å¯¼ã€‚</li>
<li>æå‡ºäº†è®¤çŸ¥é“¾ï¼ˆCognition Chainï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè®¤çŸ¥è¯„ä¼°ç†è®ºæ¥è§£é‡Šå‹åŠ›äº§ç”Ÿçš„æ­¥éª¤ã€‚</li>
<li>è®¤çŸ¥é“¾æ–¹æ³•é€šè¿‡ä½œä¸ºLLMæŒ‡ä»¤è°ƒæ•´çš„åˆæˆæ•°æ®é›†æ¨¡æ¿å¸¦æ¥å¥½å¤„ï¼Œå¹¶å¼•å…¥äº†CogInstructæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤è°ƒæ•´LLMå¼€å‘çš„CogLLMæ¨¡å‹åœ¨å‹åŠ›æ£€æµ‹ä¸­å–å¾—äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f931397ac8cfb5d2685cc2b060122e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14536ac56aeb53d5e3ccf72097fd891a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6c4335c3071e4faa7ff4702138314ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0882ce0ea70f1eaad405c9b8d2a3a5bb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="InstructSeg-Unifying-Instructed-Visual-Segmentation-with-Multi-modal-Large-Language-Models"><a href="#InstructSeg-Unifying-Instructed-Visual-Segmentation-with-Multi-modal-Large-Language-Models" class="headerlink" title="InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal   Large Language Models"></a>InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal   Large Language Models</h2><p><strong>Authors:Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, Yujiu Yang</strong></p>
<p>Boosted by Multi-modal Large Language Models (MLLMs), text-guided universal segmentation models for the image and video domains have made rapid progress recently. However, these methods are often developed separately for specific domains, overlooking the similarities in task settings and solutions across these two areas. In this paper, we define the union of referring segmentation and reasoning segmentation at both the image and video levels as Instructed Visual Segmentation (IVS). Correspondingly, we propose InstructSeg, an end-to-end segmentation pipeline equipped with MLLMs for IVS. Specifically, we employ an object-aware video perceiver to extract temporal and object information from reference frames, facilitating comprehensive video understanding. Additionally, we introduce vision-guided multi-granularity text fusion to better integrate global and detailed text information with fine-grained visual guidance. By leveraging multi-task and end-to-end training, InstructSeg demonstrates superior performance across diverse image and video segmentation tasks, surpassing both segmentation specialists and MLLM-based methods with a single model. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/congvvc/InstructSeg">https://github.com/congvvc/InstructSeg</a>. </p>
<blockquote>
<p>å¾—ç›Šäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨åŠ¨ï¼Œé¢å‘å›¾åƒå’Œè§†é¢‘é¢†åŸŸçš„æ–‡æœ¬å¼•å¯¼é€šç”¨åˆ†å‰²æ¨¡å‹æœ€è¿‘å–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå¼€å‘ï¼Œå¿½ç•¥äº†è¿™ä¸¤ä¸ªé¢†åŸŸåœ¨ä»»åŠ¡è®¾ç½®å’Œè§£å†³æ–¹æ¡ˆä¸Šçš„ç›¸ä¼¼æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å›¾åƒå’Œè§†é¢‘çº§åˆ«çš„å¼•ç”¨åˆ†å‰²å’Œæ¨ç†åˆ†å‰²çš„è”åˆå®šä¹‰ä¸ºæŒ‡ä»¤æ€§è§†è§‰åˆ†å‰²ï¼ˆIVSï¼‰ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†InstructSegï¼Œè¿™æ˜¯ä¸€ä¸ªé…å¤‡MLLMsçš„ç«¯åˆ°ç«¯åˆ†å‰²ç®¡é“ï¼Œç”¨äºIVSã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§å¯¹è±¡æ„ŸçŸ¥è§†é¢‘æ„ŸçŸ¥å™¨ï¼Œä»å‚è€ƒå¸§ä¸­æå–æ—¶é—´å’Œå¯¹è±¡ä¿¡æ¯ï¼Œä¿ƒè¿›å…¨é¢çš„è§†é¢‘ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰å¼•å¯¼çš„å¤šç²’åº¦æ–‡æœ¬èåˆï¼Œä»¥æ›´å¥½åœ°å°†å…¨å±€å’Œè¯¦ç»†æ–‡æœ¬ä¿¡æ¯ä¸ç²¾ç»†çš„è§†è§‰å¼•å¯¼ç›¸ç»“åˆã€‚é€šè¿‡åˆ©ç”¨å¤šä»»åŠ¡å’Œç«¯åˆ°ç«¯è®­ç»ƒï¼ŒInstructSegåœ¨å¤šç§å›¾åƒå’Œè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½¿ç”¨å•ä¸€æ¨¡å‹è¶…è¶Šäº†åˆ†å‰²ä¸“å®¶åŸºäºMLLMçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/congvvc,COM,%E8%BF%99%E4%B8%AA%E7%BF%BB%E8%AF%91%E4%BB%85%E7%94%A8%E4%BA%8E%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E4%BA%A4%E6%B5%81%E4%B9%8B%E7%94%A8%E3%80%82">https://github.com/congvvc,COM,è¿™ä¸ªç¿»è¯‘ä»…ç”¨äºä¸ªäººå­¦ä¹ äº¤æµä¹‹ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14006v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–‡æœ¬å¼•å¯¼å›¾åƒå’Œè§†é¢‘é¢†åŸŸé€šç”¨åˆ†å‰²æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹å›¾åƒå’Œè§†é¢‘é¢†åŸŸçš„åˆ†å‰²ä»»åŠ¡ï¼Œè¯¥æ–‡æå‡ºäº†ç»Ÿä¸€æ¡†æ¶ï¼Œå³æŒ‡ä»¤è§†è§‰åˆ†å‰²ï¼ˆIVSï¼‰ï¼Œå¹¶ç›¸åº”æå‡ºäº†é…å¤‡MLLMsçš„ç«¯åˆ°ç«¯åˆ†å‰²ç®¡é“InstructSegã€‚InstructSegé‡‡ç”¨å¯¹è±¡æ„ŸçŸ¥è§†é¢‘æ„ŸçŸ¥å™¨æå–å‚è€ƒå¸§çš„æ—¶ç©ºå’Œå¯¹è±¡ä¿¡æ¯ï¼Œä¿ƒè¿›å…¨é¢è§†é¢‘ç†è§£ã€‚é€šè¿‡å¼•å…¥è§†è§‰å¼•å¯¼çš„å¤šç²’åº¦æ–‡æœ¬èåˆï¼Œæ›´å¥½åœ°å°†å…¨å±€å’Œè¯¦ç»†æ–‡æœ¬ä¿¡æ¯ä¸ç²¾ç»†çš„è§†è§‰æŒ‡å¯¼ç›¸ç»“åˆã€‚é€šè¿‡å¤šä»»åŠ¡å’Œç«¯åˆ°ç«¯è®­ç»ƒï¼ŒInstructSegåœ¨å¤šç§å›¾åƒå’Œè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†åˆ†å‰²ä¸“å®¶å’ŒåŸºäºMLLMçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨åŠ¨äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒå’Œè§†é¢‘åˆ†å‰²æ¨¡å‹çš„å¿«é€Ÿå‘å±•ã€‚</li>
<li>ç›®å‰çš„ç ”ç©¶å¾€å¾€é’ˆå¯¹ç‰¹å®šé¢†åŸŸå¼€å‘æ¨¡å‹ï¼Œå¿½ç•¥äº†å›¾åƒå’Œè§†é¢‘é¢†åŸŸåœ¨ä»»åŠ¡è®¾ç½®å’Œè§£å†³æ–¹æ¡ˆä¸Šçš„ç›¸ä¼¼æ€§ã€‚</li>
<li>æŒ‡ä»¤è§†è§‰åˆ†å‰²ï¼ˆIVSï¼‰æ¡†æ¶èåˆäº†å›¾åƒå’Œè§†é¢‘å±‚é¢çš„æŒ‡ä»£åˆ†å‰²å’Œæ¨ç†åˆ†å‰²ã€‚</li>
<li>InstructSegæ˜¯ä¸€ä¸ªé…å¤‡MLLMsçš„ç«¯åˆ°ç«¯åˆ†å‰²ç®¡é“ï¼Œç”¨äºå®ç°IVSã€‚</li>
<li>InstructSegé‡‡ç”¨å¯¹è±¡æ„ŸçŸ¥è§†é¢‘æ„ŸçŸ¥å™¨ä»¥æå–å‚è€ƒå¸§çš„æ—¶ç©ºå’Œå¯¹è±¡ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†è§†è§‰å¼•å¯¼çš„å¤šç²’åº¦æ–‡æœ¬èåˆï¼Œä»¥æ›´å¥½åœ°é›†æˆå…¨å±€å’Œè¯¦ç»†æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>InstructSegé€šè¿‡å¤šä»»åŠ¡å’Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œåœ¨å¤šç§å›¾åƒå’Œè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a3cc86861b9db2910c5a87a905687b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7e387b9f38d05879070868de2702cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7882fbf075ede52984f4570e194c1718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e10e34ee3d9c15b099914cbb9d915fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02f6f6d8946e3aa7aa8d4d63100bad4e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Few-shot-Steerable-Alignment-Adapting-Rewards-and-LLM-Policies-with-Neural-Processes"><a href="#Few-shot-Steerable-Alignment-Adapting-Rewards-and-LLM-Policies-with-Neural-Processes" class="headerlink" title="Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with   Neural Processes"></a>Few-shot Steerable Alignment: Adapting Rewards and LLM Policies with   Neural Processes</h2><p><strong>Authors:Katarzyna Kobalczyk, Claudio Fanconi, Hao Sun, Mihaela van der Schaar</strong></p>
<p>As large language models (LLMs) become increasingly embedded in everyday applications, ensuring their alignment with the diverse preferences of individual users has become a critical challenge. Currently deployed approaches typically assume homogeneous user objectives and rely on single-objective fine-tuning. However, human preferences are inherently heterogeneous, influenced by various unobservable factors, leading to conflicting signals in preference data. Existing solutions addressing this diversity often require costly datasets labelled for specific objectives and involve training multiple reward models or LLM policies, which is computationally expensive and impractical. In this work, we present a novel framework for few-shot steerable alignment, where usersâ€™ underlying preferences are inferred from a small sample of their choices. To achieve this, we extend the Bradley-Terry-Luce model to handle heterogeneous preferences with unobserved variability factors and propose its practical implementation for reward modelling and LLM fine-tuning. Thanks to our proposed approach of functional parameter-space conditioning, LLMs trained with our framework can be adapted to individual preferences at inference time, generating outputs over a continuum of behavioural modes. We empirically validate the effectiveness of methods, demonstrating their ability to capture and align with diverse human preferences in a data-efficient manner. Our code is made available at: <a target="_blank" rel="noopener" href="https://github.com/kasia-kobalczyk/few-shot-steerable-alignment">https://github.com/kasia-kobalczyk/few-shot-steerable-alignment</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„åµŒå…¥ç¨‹åº¦ä¸æ–­æé«˜ï¼Œç¡®ä¿å®ƒä»¬ä¸ä¸ªåˆ«ç”¨æˆ·çš„å¤šæ ·åŒ–åå¥½ä¿æŒä¸€è‡´å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚å½“å‰éƒ¨ç½²çš„æ–¹æ³•é€šå¸¸å‡è®¾ç”¨æˆ·ç›®æ ‡æ˜¯ä¸€è‡´çš„ï¼Œå¹¶ä¾èµ–äºå•ä¸€ç›®æ ‡çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œäººç±»åå¥½æœ¬è´¨ä¸Šæ˜¯å¼‚è´¨æ€§çš„ï¼Œå—åˆ°å„ç§ä¸å¯è§‚å¯Ÿå› ç´ çš„å½±å“ï¼Œå¯¼è‡´åå¥½æ•°æ®ä¸­çš„ä¿¡å·ç›¸äº’å†²çªã€‚ç°æœ‰è§£å†³è¿™ç§å¤šæ ·æ€§çš„æ–¹æ³•é€šå¸¸éœ€è¦ä¸ºç‰¹å®šç›®æ ‡æ ‡è®°çš„æ˜‚è´µæ•°æ®é›†ï¼Œå¹¶æ¶‰åŠè®­ç»ƒå¤šä¸ªå¥–åŠ±æ¨¡å‹æˆ–LLMç­–ç•¥ï¼Œè¿™åœ¨è®¡ç®—ä¸Šå¾ˆæ˜‚è´µä¸”ä¸åˆ‡å®é™…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å°‘æ ·æœ¬å¯æ§åˆ¶å¯¹é½æ¡†æ¶ï¼Œç”¨äºä»ç”¨æˆ·é€‰æ‹©çš„å°æ ·æœ¬ä¸­æ¨æ–­å…¶æ½œåœ¨åå¥½ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ‰©å±•äº†Bradley-Terry-Luceæ¨¡å‹ï¼Œä»¥å¤„ç†å…·æœ‰æœªè§‚æµ‹å˜å¼‚å› ç´ çš„å¼‚è´¨åå¥½ï¼Œå¹¶æå‡ºäº†å…¶å®ç”¨äºå¥–åŠ±å»ºæ¨¡å’ŒLLMè°ƒå‚çš„å®é™…å®æ–½ã€‚ç”±äºæˆ‘ä»¬æå‡ºçš„åŠŸèƒ½å‚æ•°ç©ºé—´è°ƒèŠ‚æ–¹æ³•ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶è®­ç»ƒçš„LLMå¯ä»¥åœ¨æ¨ç†æ—¶é€‚åº”ä¸ªäººåå¥½ï¼Œåœ¨è¿ç»­çš„è¡Œä¸ºæ¨¡å¼ä¸Šç”Ÿæˆè¾“å‡ºã€‚æˆ‘ä»¬å®è¯éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å®ƒä»¬åœ¨æ•°æ®é«˜æ•ˆçš„æ–¹å¼ä¸‹æ•æ‰å’Œä¸äººç±»å¤šæ ·åŒ–åå¥½ä¿æŒä¸€è‡´çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/kasia-kobalczyk/few-shot-steerable-alignment%E3%80%82">https://github.com/kasia-kobalczyk/few-shot-steerable-alignmentã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„æ™®åŠå¸¦æ¥äº†å¦‚ä½•æ»¡è¶³ä¸åŒç”¨æˆ·çš„å¤šæ ·åŒ–åå¥½è¿™ä¸€æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾ç”¨æˆ·ç›®æ ‡ä¸€è‡´ï¼Œä¾èµ–å•ä¸€ç›®æ ‡çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œäººç±»åå¥½å…·æœ‰å¤©ç”Ÿçš„å¤šæ ·æ€§ï¼Œå—åˆ°å„ç§ä¸å¯è§‚å¯Ÿå› ç´ çš„å½±å“ï¼Œå¯¼è‡´åå¥½æ•°æ®ä¸­çš„ä¿¡å·å†²çªã€‚ä¸ºè§£å†³è¿™ç§å¤šæ ·æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å°‘æ•°æ ·æœ¬å¯æ“æ§å¯¹é½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»ç”¨æˆ·é€‰æ‹©çš„å°‘é‡æ ·æœ¬ä¸­æ¨æ–­å…¶åå¥½ã€‚æˆ‘ä»¬æ‰©å±•äº†Bradley-Terry-Luceæ¨¡å‹ä»¥å¤„ç†å…·æœ‰ä¸å¯è§‚å¯Ÿçš„å˜å¼‚å› ç´ çš„å¼‚è´¨åå¥½ï¼Œå¹¶ä¸ºå…¶åœ¨å¥–åŠ±å»ºæ¨¡å’ŒLLMå¾®è°ƒä¸­çš„å®é™…åº”ç”¨æå‡ºäº†åˆ‡å®å¯è¡Œçš„å®æ–½æ–¹æ³•ã€‚é€šè¿‡åŠŸèƒ½å‚æ•°ç©ºé—´æ¡ä»¶çš„æ–¹æ³•ï¼Œç”¨æˆ‘ä»¬çš„æ¡†æ¶è®­ç»ƒçš„LLMå¯ä»¥åœ¨æ¨ç†é˜¶æ®µé€‚åº”ä¸ªäººåå¥½ï¼Œç”Ÿæˆä¸€ç³»åˆ—è¡Œä¸ºæ¨¡å¼çš„è¾“å‡ºã€‚æˆ‘ä»¬å®è¯éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°æ®é«˜æ•ˆçš„æ–¹å¼ä¸‹æ•æ‰å’Œæ»¡è¶³äººç±»å¤šæ ·æ€§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­éœ€è¦æ»¡è¶³ä¸åŒç”¨æˆ·çš„å¤šæ ·åŒ–åå¥½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸å‡è®¾ç”¨æˆ·ç›®æ ‡ä¸€è‡´ï¼Œä½†äººç±»åå¥½å…·æœ‰å¤©ç”Ÿçš„å¤šæ ·æ€§ã€‚</li>
<li>äººç±»åå¥½çš„å¤šæ ·æ€§å—åˆ°å„ç§ä¸å¯è§‚å¯Ÿå› ç´ çš„å½±å“ï¼Œå¯¼è‡´åå¥½æ•°æ®ä¸­çš„ä¿¡å·å†²çªã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å°‘æ•°æ ·æœ¬å¯æ“æ§å¯¹é½æ¡†æ¶ï¼Œå¯ä»¥ä»ç”¨æˆ·é€‰æ‹©çš„å°‘é‡æ ·æœ¬ä¸­æ¨æ–­å…¶åå¥½ã€‚</li>
<li>æ‰©å±•äº†Bradley-Terry-Luceæ¨¡å‹ä»¥å¤„ç†å…·æœ‰ä¸å¯è§‚å¯Ÿçš„å˜å¼‚å› ç´ çš„å¼‚è´¨åå¥½ã€‚</li>
<li>é€šè¿‡åŠŸèƒ½å‚æ•°ç©ºé—´æ¡ä»¶çš„æ–¹æ³•ï¼ŒLLMå¯ä»¥åœ¨æ¨ç†é˜¶æ®µé€‚åº”ä¸ªäººåå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a6fe174039e4aaa57f5c8cf8d7806c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58e109d601c98b7d17841e2c7f519242.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-954a0c9d8bac53512f36968a758ded8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fc96e12e3406a3e03c5374ebf33d2b0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Pipeline-Analysis-for-Developing-Instruct-LLMs-in-Low-Resource-Languages-A-Case-Study-on-Basque"><a href="#Pipeline-Analysis-for-Developing-Instruct-LLMs-in-Low-Resource-Languages-A-Case-Study-on-Basque" class="headerlink" title="Pipeline Analysis for Developing Instruct LLMs in Low-Resource   Languages: A Case Study on Basque"></a>Pipeline Analysis for Developing Instruct LLMs in Low-Resource   Languages: A Case Study on Basque</h2><p><strong>Authors:Ander Corral, Ixak Sarasua, Xabier Saralegi</strong></p>
<p>Large language models (LLMs) are typically optimized for resource-rich languages like English, exacerbating the gap between high-resource and underrepresented languages. This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences. Our findings demonstrate that continual pre-training with a high-quality Basque corpus of around 600 million words improves natural language understanding (NLU) of the foundational model by over 12 points. Moreover, instruction tuning and human preference alignment using automatically translated datasets proved highly effective, resulting in a 24-point improvement in instruction-following performance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct, establish a new state-of-the-art for Basque in the sub-10B parameter category. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é’ˆå¯¹èµ„æºä¸°å¯Œå¦‚è‹±è¯­çš„è¯­è¨€è¿›è¡Œä¼˜åŒ–ï¼Œè¿™åŠ å‰§äº†èµ„æºä¸°å¯Œçš„è¯­è¨€å’Œä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ä¹‹é—´çš„å·®è·ã€‚æœ¬ç ”ç©¶è¯¦ç»†åˆ†æäº†å¼€å‘èƒ½å¤Ÿåœ¨ä½èµ„æºè¯­è¨€ï¼ˆç‰¹åˆ«æ˜¯å·´æ–¯å…‹è¯­ï¼‰éµå¾ªæŒ‡ä»¤çš„æ¨¡å‹çš„ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®é˜¶æ®µï¼šé¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨å¤§çº¦6äº¿å•è¯çš„é«˜è´¨é‡å·´æ–¯å…‹è¯­è¯­æ–™åº“è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œæé«˜äº†åŸºç¡€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰èƒ½åŠ›è¶…è¿‡12ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è‡ªåŠ¨ç¿»è¯‘æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´å’Œäººç±»åå¥½å¯¹é½è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼ŒæŒ‡ä»¤éµå¾ªæ€§èƒ½æé«˜äº†24ä¸ªç‚¹ã€‚æ‰€å¾—æ¨¡å‹Llama-eus-8Bå’ŒLlama-eus-8B-instructåœ¨å°äº10Bå‚æ•°çš„ç±»åˆ«ä¸­ä¸ºå·´æ–¯å…‹è¯­åˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13922v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‹±è¯­ç­‰èµ„æºä¸°å¯Œå‹è¯­è¨€çš„ä¼˜åŒ–ä¸ŠæŠ•å…¥è¾ƒå¤šï¼ŒåŠ å‰§äº†é«˜èµ„æºè¯­è¨€å’Œä»£è¡¨æ€§ä¸è¶³çš„è¯­è¨€ä¹‹é—´çš„å·®è·ã€‚æœ¬ç ”ç©¶è¯¦ç»†åˆ†æäº†åœ¨ä½èµ„æºè¯­è¨€å·´æ–¯å…‹è¯­ä¸Šå¼€å‘éµå¾ªæŒ‡ä»¤æ¨¡å‹çš„ä¸‰é¡¹å…³é”®ç­–ç•¥ï¼šé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒä»¥åŠä¸äººç±»åå¥½å¯¹é½ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨é«˜è´¨é‡å·´æ–¯å…‹è¯­è¯­æ–™åº“çš„æŒç»­é¢„è®­ç»ƒï¼Œæå‡äº†åŸºç¡€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›è¶…è¿‡12ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡è‡ªåŠ¨ç¿»è¯‘æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒä¸äººç±»åå¥½å¯¹é½ï¼Œæ˜¾è‘—æé«˜äº†æŒ‡ä»¤éµå¾ªæ€§èƒ½ï¼Œæ”¹å–„äº†24ä¸ªç™¾åˆ†ç‚¹ã€‚æœ€ç»ˆå»ºç«‹çš„æ¨¡å‹Llama-eus-8Bå’ŒLlama-eus-8B-instructåœ¨å‚æ•°ç±»åˆ«ä½äº10Bçš„æƒ…å†µä¸‹ä¸ºå·´æ–¯å…‹è¯­æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‹±è¯­ç­‰èµ„æºä¸°å¯Œå‹è¯­è¨€çš„ä¼˜åŒ–ä¸ŠæŠ•å…¥è¾ƒå¤šï¼Œéœ€è¦å…³æ³¨ä½èµ„æºè¯­è¨€çš„æ¨¡å‹å‘å±•ã€‚</li>
<li>åœ¨å·´æ–¯å…‹è¯­ä¸Šå¼€å‘éµå¾ªæŒ‡ä»¤æ¨¡å‹çš„å…³é”®ç­–ç•¥åŒ…æ‹¬é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒä»¥åŠä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>æŒç»­é¢„è®­ç»ƒå¯ä»¥æå‡åŸºç¡€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒå’Œä½¿ç”¨è‡ªåŠ¨ç¿»è¯‘æ•°æ®é›†èƒ½å¤Ÿæé«˜æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªæ€§èƒ½ã€‚</li>
<li>æœ€ç»ˆå»ºç«‹çš„æ¨¡å‹Llama-eus-8Bå’ŒLlama-eus-8B-instructåœ¨å·´æ–¯å…‹è¯­é¢†åŸŸæ ‘ç«‹äº†æ–°çš„æ ‡å‡†ã€‚</li>
<li>ç ”ç©¶æˆæœæœ‰åŠ©äºç¼©å°é«˜èµ„æºè¯­è¨€å’Œä½èµ„æºè¯­è¨€ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a824f0c3fab6377f411be74da74e2f0c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8efc1c216785eeb7be31d3ebdc70f15.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models"><a href="#Typhoon-2-A-Family-of-Open-Text-and-Multimodal-Thai-Large-Language-Models" class="headerlink" title="Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models"></a>Typhoon 2: A Family of Open Text and Multimodal Thai Large Language   Models</h2><p><strong>Authors:Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai</strong></p>
<p>This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ various post-training techniques to enhance Thai language performance while preserving the base modelsâ€™ original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs simultaneously. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Typhoon 2ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹æ³°è¯­ä¼˜åŒ–çš„æ–‡æœ¬å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚Typhoon2-Textå»ºç«‹åœ¨æœ€æ–°å¼€æ”¾æ¨¡å‹ï¼ˆå¦‚Llama 3å’ŒQwen2ï¼‰çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹è‹±è¯­å’Œæ³°è¯­æ•°æ®çš„æ··åˆè¿›è¡ŒæŒç»­é¢„è®­ç»ƒã€‚æˆ‘ä»¬é‡‡ç”¨å„ç§åè®­ç»ƒæŠ€æœ¯ï¼Œä»¥æé«˜æ³°è¯­æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åŸå§‹åŠŸèƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ç³»åˆ—ä¸åŒå¤§å°çš„æ–‡æœ¬æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»1äº¿åˆ°70äº¿ï¼Œæ—¢æœ‰åŸºç¡€æ¨¡å‹ä¹Ÿæœ‰æŒ‡ä»¤è°ƒä¼˜çš„å˜ä½“å¯ä¾›é€‰æ‹©ã€‚Typhoon2-Visionåœ¨ä¿ç•™ä¸€èˆ¬è§†è§‰åŠŸèƒ½ï¼ˆå¦‚å›¾åƒå­—å¹•ï¼‰çš„åŒæ—¶ï¼Œæé«˜äº†å¯¹æ³°è¯­æ–‡æ¡£çš„ç†è§£èƒ½åŠ›ã€‚Typhoon2-Audioå¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€è¯­éŸ³å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶åŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13702v1">PDF</a> technical report, 55 pages</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†Typhoon 2ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬é’ˆå¯¹æ³°è¯­çš„æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚Typhoon2-TextåŸºäºæœ€å‰æ²¿çš„å¼€æ”¾æ¨¡å‹ï¼Œå¦‚Llama 3å’ŒQwen2ï¼Œè¿›è¡ŒæŒç»­çš„é¢„è®­ç»ƒï¼Œå¹¶åº”ç”¨å¤šç§åè®­ç»ƒæŠ€æœ¯æé«˜æ³°è¯­æ€§èƒ½ã€‚Typhoon2-Visionæ”¹è¿›äº†æ³°è¯­æ–‡æ¡£ç†è§£ï¼ŒåŒæ—¶ä¿ç•™äº†é€šç”¨çš„è§†è§‰åŠŸèƒ½ã€‚Typhoon2-Audioå¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†éŸ³é¢‘ã€è¯­éŸ³å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶åŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Typhoon 2ç³»åˆ—æ¨¡å‹é’ˆå¯¹æ³°è¯­è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘æ¨¡å‹ã€‚</li>
<li>Typhoon2-TextåŸºäºå‰æ²¿å¼€æ”¾æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶æå‡æ³°è¯­æ€§èƒ½ã€‚</li>
<li>Typhoon2-Visionæ”¹è¿›äº†æ³°è¯­æ–‡æ¡£ç†è§£ï¼ŒåŒæ—¶ä¿ç•™é€šç”¨è§†è§‰åŠŸèƒ½ã€‚</li>
<li>Typhoon2-Audioå…·å¤‡ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³èƒ½åŠ›ï¼Œèƒ½å¤„ç†å¤šç§è¾“å…¥å¹¶ç”Ÿæˆç›¸åº”è¾“å‡ºã€‚</li>
<li>æ¨¡å‹ç³»åˆ—åŒ…å«ä¸åŒå¤§å°çš„æ–‡æœ¬æ¨¡å‹ï¼Œä»1äº¿åˆ°70äº¿å‚æ•°ï¼Œæœ‰åŸºç¡€å‹å’ŒæŒ‡ä»¤ä¼˜åŒ–å‹ä¸¤ç§ã€‚</li>
<li>æ¨¡å‹åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µé‡‡ç”¨å¤šç§æŠ€æœ¯æ¥æå‡æ€§èƒ½å¹¶ä¼˜åŒ–é’ˆå¯¹æ³°è¯­çš„å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bc776d60c3f5267b207b3f0dcadc1ef.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="G-VEval-A-Versatile-Metric-for-Evaluating-Image-and-Video-Captions-Using-GPT-4o"><a href="#G-VEval-A-Versatile-Metric-for-Evaluating-Image-and-Video-Captions-Using-GPT-4o" class="headerlink" title="G-VEval: A Versatile Metric for Evaluating Image and Video Captions   Using GPT-4o"></a>G-VEval: A Versatile Metric for Evaluating Image and Video Captions   Using GPT-4o</h2><p><strong>Authors:Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung</strong></p>
<p>Evaluation metric of visual captioning is important yet not thoroughly explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are limited in zero-shot scenarios. Advanced Language Model-based metrics also struggle with aligning to nuanced human preferences. To address these issues, we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and supports three modes: reference-free, reference-only, and combined, accommodating both video and image inputs. We also propose MSVD-Eval, a new dataset for video captioning evaluation, to establish a more transparent and consistent framework for both human experts and evaluation metrics. It is designed to address the lack of clear criteria in existing datasets by introducing distinct dimensions of Accuracy, Completeness, Conciseness, and Relevance (ACCR). Extensive results show that G-VEval outperforms existing methods in correlation with human annotations, as measured by Kendall tau-b and Kendall tau-c. This provides a flexible solution for diverse captioning tasks and suggests a straightforward yet effective approach for large language models to understand video content, paving the way for advancements in automated captioning. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/ztangaj/gveval">https://github.com/ztangaj/gveval</a> </p>
<blockquote>
<p>è§†è§‰æè¿°çš„è¯„ä»·æŒ‡æ ‡åœ¨è¯„ä¼°ä¸­éå¸¸é‡è¦ä½†å°šæœªè¢«å®Œå…¨æ¢ç´¢ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡å¦‚BLEUã€METEORã€CIDErå’ŒROUGEå¾€å¾€å¿½ç•¥äº†è¯­ä¹‰æ·±åº¦ï¼Œè€Œè®­ç»ƒå¥½çš„è¯„ä¼°æŒ‡æ ‡å¦‚CLIP-Scoreã€PAC-Så’ŒPolosåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸­å—åˆ°é™åˆ¶ã€‚å…ˆè¿›çš„åŸºäºè¯­è¨€æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ä¹Ÿéš¾ä»¥ä¸äººç±»å¾®å¦™çš„åå¥½ä¿æŒä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†G-VEvalè¿™ä¸€æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒçµæ„Ÿæ¥æºäºG-Evalå¹¶ç”±æ–°å‹GPT-4oæä¾›æ”¯æŒã€‚G-VEvalä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é“¾å¼æ€ç»´æ¨ç†ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼ï¼šæ— å‚è€ƒã€ä»…æœ‰å‚è€ƒå’Œç»„åˆæ¨¡å¼ï¼Œå¯ä»¥å¤„ç†è§†é¢‘å’Œå›¾åƒè¾“å…¥ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†MSVD-Evalè¿™ä¸€æ–°çš„è§†é¢‘æè¿°è¯„ä¼°æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºäººå·¥ä¸“å®¶å’Œè¯„ä»·æŒ‡æ ‡å»ºç«‹ä¸€ä¸ªæ›´åŠ é€æ˜å’Œä¸€è‡´çš„æ¡†æ¶ã€‚å®ƒæ—¨åœ¨é€šè¿‡å¼•å…¥å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ç®€æ´æ€§å’Œç›¸å…³æ€§ï¼ˆACCRï¼‰çš„ä¸åŒç»´åº¦æ¥è§£å†³ç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹æ˜ç¡®æ ‡å‡†çš„é—®é¢˜ã€‚å¤§é‡ç»“æœæ˜¾ç¤ºï¼ŒG-VEvalä¸äººå·¥æ³¨é‡Šçš„ç›¸å…³æ€§é«˜äºç°æœ‰æ–¹æ³•ï¼Œè¿™ä¸€è¡¨ç°æ˜¯é€šè¿‡è‚¯å¾·å°”tau-bå’Œè‚¯å¾·å°”tau-cæ¥è¡¡é‡çš„ã€‚è¿™ä¸ºå¤šæ ·åŒ–çš„æè¿°ä»»åŠ¡æä¾›äº†çµæ´»è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç†è§£è§†é¢‘å†…å®¹æä¾›äº†ç›´æ¥æœ‰æ•ˆçš„é€”å¾„ï¼Œä¸ºè‡ªåŠ¨æè¿°çš„è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ztangaj/gveval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ztangaj/gvevalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13647v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨è§†è§‰æè¿°çš„è¯„ä»·æŒ‡æ ‡é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»ŸæŒ‡æ ‡å¦‚BLEUã€METEORç­‰ç¼ºä¹è¯­ä¹‰æ·±åº¦ï¼Œè€Œè®­ç»ƒæŒ‡æ ‡å¦‚CLIP-Scoreç­‰åˆ™åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹æœ‰é™åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºæ–°å‹æŒ‡æ ‡G-VEvalï¼Œå®ƒç»“åˆäº†G-Evalçš„çµæ„Ÿå’ŒGPT-4oçš„èƒ½åŠ›ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼å¹¶å¯¹è§†é¢‘å’Œå›¾åƒè¾“å…¥éƒ½æœ‰è‰¯å¥½çš„é€‚åº”æ€§ã€‚åŒæ—¶ï¼Œä¸ºå»ºç«‹æ›´é€æ˜å’Œä¸€è‡´çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨å‡ºMSVD-Evalæ•°æ®é›†ï¼Œè®¾è®¡ç”¨æ¥è§£å†³ç°æœ‰æ•°æ®é›†çš„æ¨¡ç³Šæ ‡å‡†é—®é¢˜ã€‚å®éªŒç»“æœè¯å®G-VEvalä¸äººç±»æ³¨é‡Šçš„å…³è”åº¦é«˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æè¿°çš„è¯„ä»·æŒ‡æ ‡å­˜åœ¨è¯­ä¹‰æ·±åº¦ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä»·æŒ‡æ ‡å¦‚BLEUã€METEORç­‰åœ¨è¯­ä¹‰æ·±åº¦ä¸Šæœ‰æ‰€æ¬ ç¼ºã€‚</li>
<li>è®­ç»ƒæŒ‡æ ‡å¦‚CLIP-Scoreåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹æœ‰å±€é™æ€§ã€‚</li>
<li>æå‡ºæ–°å‹è¯„ä»·æŒ‡æ ‡G-VEvalï¼Œç»“åˆG-Evalçµæ„Ÿå’ŒGPT-4oèƒ½åŠ›ã€‚</li>
<li>G-VEvalæ”¯æŒä¸‰ç§æ¨¡å¼ï¼Œé€‚åº”è§†é¢‘å’Œå›¾åƒè¾“å…¥ã€‚</li>
<li>ä¸ºå»ºç«‹æ›´é€æ˜çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¨å‡ºMSVD-Evalæ•°æ®é›†ã€‚</li>
<li>MSVD-Evalæ•°æ®é›†æ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†çš„æ¨¡ç³Šæ ‡å‡†é—®é¢˜ï¼ŒåŒ…æ‹¬å››ä¸ªç»´åº¦ï¼šå‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€ç®€æ´æ€§å’Œç›¸å…³æ€§ï¼ˆACCRï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ffb1dfc2705d4a93bee47310522e0ca1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7fc10da5df30a520bc348796e7a85b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08675a629168b4a5d332646c96b905ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaf27a3173cd36f7b70161734d64e0da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76b360cb4464ac3fe41d54d24040e5bd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Combining-Aggregated-Attention-and-Transformer-Architecture-for-Accurate-and-Efficient-Performance-of-Spiking-Neural-Networks"><a href="#Combining-Aggregated-Attention-and-Transformer-Architecture-for-Accurate-and-Efficient-Performance-of-Spiking-Neural-Networks" class="headerlink" title="Combining Aggregated Attention and Transformer Architecture for Accurate   and Efficient Performance of Spiking Neural Networks"></a>Combining Aggregated Attention and Transformer Architecture for Accurate   and Efficient Performance of Spiking Neural Networks</h2><p><strong>Authors:Hangming Zhang, Alexander Sboev, Roman Rybka, Qiang Yu</strong></p>
<p>Spiking Neural Networks have attracted significant attention in recent years due to their distinctive low-power characteristics. Meanwhile, Transformer models, known for their powerful self-attention mechanisms and parallel processing capabilities, have demonstrated exceptional performance across various domains, including natural language processing and computer vision. Despite the significant advantages of both SNNs and Transformers, directly combining the low-power benefits of SNNs with the high performance of Transformers remains challenging. Specifically, while the sparse computing mode of SNNs contributes to reduced energy consumption, traditional attention mechanisms depend on dense matrix computations and complex softmax operations. This reliance poses significant challenges for effective execution in low-power scenarios. Given the tremendous success of Transformers in deep learning, it is a necessary step to explore the integration of SNNs and Transformers to harness the strengths of both. In this paper, we propose a novel model architecture, Spike Aggregation Transformer (SAFormer), that integrates the low-power characteristics of SNNs with the high-performance advantages of Transformer models. The core contribution of SAFormer lies in the design of the Spike Aggregated Self-Attention (SASA) mechanism, which significantly simplifies the computation process by calculating attention weights using only the spike matrices query and key, thereby effectively reducing energy consumption. Additionally, we introduce a Depthwise Convolution Module (DWC) to enhance the feature extraction capabilities, further improving overall accuracy. We evaluated and demonstrated that SAFormer outperforms state-of-the-art SNNs in both accuracy and energy consumption, highlighting its significant advantages in low-power and high-performance computing. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œå› å…¶ç‹¬ç‰¹çš„ä½åŠŸè€—ç‰¹æ€§è¿‘å¹´æ¥å¤‡å—å…³æ³¨ã€‚åŒæ—¶ï¼Œä»¥å¼ºå¤§çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå¹¶è¡Œå¤„ç†èƒ½åŠ›è€Œé—»åçš„Transformeræ¨¡å‹ï¼Œåœ¨åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰åœ¨å†…çš„å„ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡SNNså’ŒTransformeréƒ½æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä½†ç›´æ¥å°†SNNsçš„ä½åŠŸè€—ä¼˜åŠ¿ä¸Transformerçš„é«˜æ€§èƒ½ç›¸ç»“åˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å…·ä½“è€Œè¨€ï¼Œè™½ç„¶SNNsçš„ç¨€ç–è®¡ç®—æ¨¡å¼æœ‰åŠ©äºå‡å°‘èƒ½æºæ¶ˆè€—ï¼Œä½†ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ä¾èµ–äºå¯†é›†çŸ©é˜µè®¡ç®—å’Œå¤æ‚çš„softmaxè¿ç®—ã€‚è¿™ç§ä¾èµ–åœ¨ä½åŠŸè€—åœºæ™¯ä¸­æœ‰æ•ˆæ‰§è¡Œå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚é‰´äºTransformeråœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å·¨å¤§æˆåŠŸï¼Œæ¢ç´¢å°†SNNså’ŒTransformerç›¸ç»“åˆä»¥åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿æ˜¯å¿…è¦çš„æ­¥éª¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼Œå³Spike Aggregation Transformerï¼ˆSAFormerï¼‰ï¼Œå®ƒå°†SNNsçš„ä½åŠŸè€—ç‰¹æ€§ä¸Transformeræ¨¡å‹çš„é«˜æ€§èƒ½ä¼˜åŠ¿ç›¸ç»“åˆã€‚SAFormerçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºè®¾è®¡äº†Spike Aggregated Self-Attentionï¼ˆSASAï¼‰æœºåˆ¶ï¼Œå®ƒé€šè¿‡ä»…ä½¿ç”¨æŸ¥è¯¢å’Œå…³é”®è„‰å†²çŸ©é˜µæ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œç®€åŒ–äº†è®¡ç®—è¿‡ç¨‹ï¼Œå¹¶æœ‰æ•ˆåœ°é™ä½äº†èƒ½è€—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Depthwise Convolution Moduleï¼ˆDWCï¼‰ä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æé«˜æ•´ä½“å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¯„ä¼°å’Œè¯æ˜äº†SAFormeråœ¨å‡†ç¡®æ€§å’Œèƒ½è€—æ–¹é¢å‡ä¼˜äºæœ€æ–°çš„SNNsï¼Œå‡¸æ˜¾äº†å…¶åœ¨ä½åŠŸè€—å’Œé«˜æ€§èƒ½è®¡ç®—ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13553v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸï¼Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å› å…¶ä½åŠŸè€—ç‰¹æ€§å—åˆ°å¹¿æ³›å…³æ³¨ã€‚è€Œå˜å‹å™¨æ¨¡å‹åˆ™ä»¥å…¶å¼ºå¤§çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å°½ç®¡SNNså’Œå˜å‹å™¨å„æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†å°†SNNsçš„ä½åŠŸè€—ä¼˜åŠ¿ä¸å˜å‹å™¨çš„é«˜æ€§èƒ½ç›¸ç»“åˆä»å…·æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨SNNsçš„ç¨€ç–è®¡ç®—æ¨¡å¼æœ‰åŠ©äºé™ä½èƒ½è€—ï¼Œè€Œä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶å´ä¾èµ–äºå¯†é›†çŸ©é˜µè®¡ç®—å’Œå¤æ‚çš„softmaxè¿ç®—ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ¶æ„â€”â€”Spike Aggregation Transformerï¼ˆSAFormerï¼‰ï¼Œç»“åˆäº†SNNsçš„ä½åŠŸè€—å’Œå˜å‹å™¨æ¨¡å‹çš„é«˜æ€§èƒ½ä¼˜åŠ¿ã€‚SAFormerçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºè®¾è®¡äº†Spike Aggregated Self-Attentionï¼ˆSASAï¼‰æœºåˆ¶ï¼Œé€šè¿‡ä»…ä½¿ç”¨è„‰å†²çŸ©é˜µæŸ¥è¯¢å’Œé”®æ¥è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œä»è€Œæœ‰æ•ˆå‡å°‘èƒ½è€—ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Depthwise Convolution Moduleï¼ˆDWCï¼‰ä»¥å¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œè¿›ä¸€æ­¥æé«˜æ•´ä½“å‡†ç¡®æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSAFormeråœ¨å‡†ç¡®æ€§å’Œèƒ½è€—æ–¹é¢å‡ä¼˜äºæœ€æ–°SNNsï¼Œçªæ˜¾å…¶åœ¨ä½åŠŸè€—å’Œé«˜æ€§èƒ½è®¡ç®—ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å› ä½åŠŸè€—ç‰¹æ€§å—åˆ°å…³æ³¨ï¼Œè€Œå˜å‹å™¨æ¨¡å‹åœ¨é«˜æ€§èƒ½è®¡ç®—ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç›´æ¥ç»“åˆSNNså’Œå˜å‹å™¨çš„ä¼˜åŠ¿å…·æœ‰æŒ‘æˆ˜ï¼Œå› ä¸ºä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚</li>
<li>SAFormeræ¨¡å‹ç»“åˆäº†SNNså’Œå˜å‹å™¨ï¼Œé€šè¿‡Spike Aggregated Self-Attentionï¼ˆSASAï¼‰æœºåˆ¶ç®€åŒ–è®¡ç®—è¿‡ç¨‹ï¼Œé™ä½èƒ½è€—ã€‚</li>
<li>SAFormerå¼•å…¥çš„Depthwise Convolution Moduleï¼ˆDWCï¼‰å¢å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ï¼Œæé«˜äº†æ•´ä½“å‡†ç¡®æ€§ã€‚</li>
<li>SAFormeråœ¨å‡†ç¡®æ€§å’Œèƒ½è€—æ–¹é¢å‡ä¼˜äºç°æœ‰SNNsã€‚</li>
<li>SAFormerå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«é€‚ç”¨äºä½åŠŸè€—å’Œé«˜æ€§èƒ½è®¡ç®—åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-03f2050e91924d74735d837347cebd4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e47f3d12925a6a251eef1f48a7a109ea.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-Secret-Recipe-A-Guide-For-Supervised-Fine-Tuning-Small-LLMs"><a href="#Unveiling-the-Secret-Recipe-A-Guide-For-Supervised-Fine-Tuning-Small-LLMs" class="headerlink" title="Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small   LLMs"></a>Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small   LLMs</h2><p><strong>Authors:Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wang, Krishnateja Killamsetty, Shivchander Sudalairaj, Wenlong Zhao, Seungwook Han, Abhishek Bhandwaldar, Guangxuan Xu, Kai Xu, Ligong Han, Luke Inglis, Akash Srivastava</strong></p>
<p>The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. Key insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, enabling early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observed no significant difference in performance between phased and stacked training strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets and models, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive environment for LLM research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·é€ æˆäº†ä¸€ä¸ªæ˜¾è‘—çš„ä¸å¹³è¡¡ï¼šå·¥ä¸šç ”ç©¶å®éªŒå®¤å‡­å€Ÿä»–ä»¬çš„è®¡ç®—èµ„æºã€ä¸“å®¶å›¢é˜Ÿå’Œå…ˆè¿›çš„åŸºç¡€è®¾æ–½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¾®è°ƒLLMï¼Œè€Œä¸ªäººå¼€å‘è€…å’Œå°å‹ç»„ç»‡åˆ™ç”±äºèµ„æºæœ‰é™è€Œé¢ä¸´éšœç¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡ä¸€é¡¹å…³äºä½¿ç”¨æ¶µç›–å„ç§çŸ¥è¯†é¢†åŸŸå’ŒæŠ€èƒ½çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å¯¹LLMè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒçš„ç»¼åˆç ”ç©¶æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä¸“æ³¨äºå°å‹LLMï¼ˆ3Båˆ°7Bå‚æ•°ï¼‰ï¼Œä»¥æé«˜å…¶æˆæœ¬å’Œå¯è®¿é—®æ€§ã€‚æˆ‘ä»¬æ¢ç´¢äº†å››ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹çš„å„ç§è®­ç»ƒé…ç½®å’Œç­–ç•¥ã€‚æˆ‘ä»¬è¯¦ç»†è®°å½•äº†è¿™äº›é…ç½®ï¼Œå¹¶æ­ç¤ºäº†æŒ‘æˆ˜ä¸€äº›å¸¸è§çš„è®­ç»ƒå®è·µçš„å‘ç°ï¼ŒåŒ…æ‹¬æ¥è‡ªTULUçš„è¶…å‚æ•°æ¨èå’ŒOrcaæ¨èçš„åˆ†é˜¶æ®µè®­ç»ƒã€‚æˆ‘ä»¬å·¥ä½œçš„å…³é”®è§è§£åŒ…æ‹¬ï¼šï¼ˆiï¼‰è¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°ä¸è¾ƒä½çš„å­¦ä¹ ç‡ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨MMLUã€MTBenchå’ŒOpen LLM Leaderboardç­‰åŸºå‡†æµ‹è¯•ä¸Šæé«˜æ¨¡å‹æ€§èƒ½ï¼›ï¼ˆiiï¼‰æ—©æœŸè®­ç»ƒåŠ¨æ€ï¼Œå¦‚è¾ƒä½çš„æ¢¯åº¦èŒƒæ•°å’Œè¾ƒé«˜çš„æŸå¤±å€¼ï¼Œæ˜¯æ›´å¥½çš„æœ€ç»ˆæ¨¡å‹æ€§èƒ½çš„å¼ºçƒˆæŒ‡æ ‡ï¼Œèƒ½å¤Ÿæå‰ç»ˆæ­¢æ¬¡ä¼˜è¿è¡Œå¹¶å®ç°é‡å¤§çš„è®¡ç®—èŠ‚çœï¼›ï¼ˆiiiï¼‰é€šè¿‡å¯¹é¢„çƒ­æ­¥éª¤å’Œå­¦ä¹ ç‡è°ƒåº¦ç­‰è¶…å‚æ•°çš„å…¨é¢æ¢ç´¢ï¼Œæˆ‘ä»¬ä¸ºå®è·µè€…æä¾›äº†æŒ‡å¯¼ï¼Œå¹¶å‘ç°æŸäº›ç®€åŒ–å¹¶ä¸ä¼šæŸå®³æ€§èƒ½ï¼›ï¼ˆivï¼‰æˆ‘ä»¬è§‚å¯Ÿåˆ°åˆ†é˜¶æ®µå’Œå †å è®­ç»ƒç­–ç•¥ä¹‹é—´çš„æ€§èƒ½æ²¡æœ‰æ˜¾è‘—å·®å¼‚ï¼Œä½†å †å è®­ç»ƒæ›´ç®€å•ä¸”æ ·æœ¬æ•ˆç‡æ›´é«˜ã€‚è¿™äº›å‘ç°ç¨³å¥åœ°é€‚ç”¨äºå„ç§æ•°æ®é›†å’Œæ¨¡å‹ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½ä¸ºå®è·µè€…å¾®è°ƒå°å‹LLMæä¾›æŒ‡å—ï¼Œå¹¶ä¿ƒè¿›LLMç ”ç©¶çš„æ›´åŠ åŒ…å®¹çš„ç¯å¢ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13337v1">PDF</a> 33 pages, 19 figures. Appendix included in submission. Submitted to   ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·å¯¼è‡´èµ„æºå·®è·é€æ¸åŠ å¤§ï¼Œå·¥ä¸šç ”ç©¶å®éªŒå®¤å› æ‹¥æœ‰è®¡ç®—èµ„æºã€ä¸“ä¸šå›¢é˜Ÿå’Œå…ˆè¿›åŸºç¡€è®¾æ–½èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¾®è°ƒLLMï¼Œè€Œä¸ªäººå¼€å‘è€…å’Œå°å‹ä¼ä¸šåˆ™é¢ä¸´èµ„æºé™åˆ¶çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å…¨é¢ç ”ç©¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å¯¹LLMçš„ç›‘ç£å¾®è°ƒæ¥ç¼©å°è¿™ä¸€å·®è·ï¼Œæ¶µç›–ä¸åŒçŸ¥è¯†é¢†åŸŸå’ŒæŠ€èƒ½ã€‚ç ”ç©¶é‡ç‚¹ä¸ºå°å‹LLMï¼ˆå‚æ•°èŒƒå›´ä»3Båˆ°7Bï¼‰ï¼Œä»¥æé«˜å…¶æˆæœ¬æ•ˆç›Šå’Œå¯åŠæ€§ã€‚åœ¨å››ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹ä¸Šæ¢ç´¢äº†ä¸åŒçš„è®­ç»ƒé…ç½®å’Œç­–ç•¥ã€‚è¯¦ç»†è®°å½•äº†è¿™äº›é…ç½®ï¼Œæ­ç¤ºäº†æŒ‘æˆ˜å¸¸è§è®­ç»ƒå®è·µçš„å‘ç°ï¼ŒåŒ…æ‹¬æ¥è‡ªTULUçš„è¶…å‚æ•°æ¨èå’ŒOrcaçš„é˜¶æ®µæ€§è®­ç»ƒå»ºè®®ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£åŒ…æ‹¬ï¼š</p>
<ul>
<li>æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä¸è¾ƒä½çš„å­¦ä¹ ç‡ç›¸ç»“åˆï¼Œåœ¨MMLUã€MTBenchå’ŒOpen LLM Leaderboardç­‰åŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ—©æœŸè®­ç»ƒåŠ¨æ€ï¼Œå¦‚è¾ƒä½çš„æ¢¯åº¦èŒƒæ•°å’Œè¾ƒé«˜çš„æŸå¤±å€¼ï¼Œæ˜¯æœ€ç»ˆæ¨¡å‹æ€§èƒ½çš„è‰¯å¥½æŒ‡æ ‡ï¼Œå¯æå‰ç»ˆæ­¢æ¬¡ä¼˜è¿è¡Œå¹¶å®ç°æ˜¾è‘—çš„è®¡ç®—èŠ‚çœã€‚</li>
<li>é€šè¿‡å…¨é¢æ¢ç´¢é¢„çƒ­æ­¥éª¤å’Œå­¦ä¹ ç‡è°ƒåº¦ç­‰è¶…å‚æ•°ï¼Œä¸ºå®è·µè€…æä¾›æŒ‡å¯¼ï¼Œå¹¶å‘ç°æŸäº›ç®€åŒ–å¹¶ä¸ä¼šå½±å“æ€§èƒ½ã€‚</li>
<li>æˆ‘ä»¬è§‚å¯Ÿåˆ°åˆ†é˜¶æ®µå’Œå †å è®­ç»ƒç­–ç•¥åœ¨æ€§èƒ½ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ï¼Œä½†å †å è®­ç»ƒæ›´ç®€å•ä¸”æ ·æœ¬æ•ˆç‡æ›´é«˜ã€‚è¿™äº›å‘ç°å¯¹ä¸åŒçš„æ•°æ®é›†å’Œæ¨¡å‹å…·æœ‰ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½ä¸ºå¾®è°ƒLLMçš„å®è·µè€…æä¾›æŒ‡å¯¼ï¼Œå¹¶æ¨åŠ¨LLMç ”ç©¶æ›´åŠ åŒ…å®¹çš„ç¯å¢ƒçš„å½¢æˆã€‚</li>
</ul>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä¸è¾ƒä½çš„å­¦ä¹ ç‡èƒ½æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ—©æœŸè®­ç»ƒåŠ¨æ€æ˜¯é¢„æµ‹æœ€ç»ˆæ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæŒ‡æ ‡ã€‚</li>
<li>å…¨é¢çš„è¶…å‚æ•°æ¢ç´¢ä¸ºå®è·µè€…æä¾›äº†æŒ‡å¯¼ã€‚</li>
<li>åˆ†é˜¶æ®µå’Œå †å è®­ç»ƒç­–ç•¥åœ¨æ€§èƒ½ä¸Šç›¸ä¼¼ï¼Œä½†å †å è®­ç»ƒæ›´ç®€æ´é«˜æ•ˆã€‚</li>
<li>ç ”ç©¶ç»“æœåœ¨ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹ä¸Šå…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶æœ‰åŠ©äºç¼©å°èµ„æºå·®è·ï¼Œä¸ºå¾®è°ƒLLMçš„å®è·µè€…æä¾›æŒ‡å¯¼ã€‚</li>
<li>ç ”ç©¶ä¿ƒè¿›äº†LLMç ”ç©¶çš„æ›´åŒ…å®¹ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6989afb323788ba7546da7be71ac3c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08c81105c71c141a69008022b3605c57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a1d8de8a69f867178d936d18960fc7f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis"><a href="#M2SE-A-Multistage-Multitask-Instruction-Tuning-Strategy-for-Unified-Sentiment-and-Emotion-Analysis" class="headerlink" title="M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis"></a>M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified   Sentiment and Emotion Analysis</h2><p><strong>Authors:Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang</strong></p>
<p>Sentiment analysis and emotion recognition are crucial for applications such as human-computer interaction and depression detection. Traditional unimodal methods often fail to capture the complexity of emotional expressions due to conflicting signals from different modalities. Current Multimodal Large Language Models (MLLMs) also face challenges in detecting subtle facial expressions and addressing a wide range of emotion-related tasks. To tackle these issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy for general-purpose MLLMs. It employs a combined approach to train models on tasks such as multimodal sentiment analysis, emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction. We also introduce the Emotion Multitask dataset (EMT), a custom dataset that supports these five tasks. Our model, Emotion Universe (EmoVerse), is built on a basic MLLM framework without modifications, yet it achieves substantial improvements across these tasks when trained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-of-the-art results in sentiment and emotion tasks. These results highlight the effectiveness of M2SE in enhancing multimodal emotion perception. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE">https://github.com/xiaoyaoxinyi/M2SE</a>. </p>
<blockquote>
<p>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨äººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ç”±äºæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡å·ç›¸äº’å†²çªï¼Œå¾€å¾€æ— æ³•æ•æ‰æƒ…ç»ªè¡¨è¾¾çš„å¤æ‚æ€§ã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ£€æµ‹ç»†å¾®é¢éƒ¨è¡¨æƒ…å’Œåº”å¯¹å„ç§æƒ…ç»ªç›¸å…³ä»»åŠ¡æ—¶ä¹Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M2SEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé€šç”¨MLLMsçš„å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿå’Œæƒ…ç»ªæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ã€‚å®ƒé‡‡ç”¨ç»„åˆæ–¹æ³•ï¼Œåœ¨è¯¸å¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ã€æƒ…ç»ªæ¨ç†æ¨æ–­å’Œæƒ…ç»ªå› æœå…³ç³»å¯¹æå–ç­‰ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†æƒ…æ„Ÿå¤šä»»åŠ¡æ•°æ®é›†ï¼ˆEMTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒè¿™äº”ä¸ªä»»åŠ¡è‡ªå®šä¹‰æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€œæƒ…æ„Ÿå®‡å®™â€ï¼ˆEmoVerseï¼‰å»ºç«‹åœ¨åŸºæœ¬çš„MLLMæ¡†æ¶ä¸Šï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œä½†å½“ä½¿ç”¨M2SEç­–ç•¥è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒåœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEmoVerseä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æƒ…æ„Ÿå’Œæƒ…ç»ªä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚è¿™äº›ç»“æœçªå‡ºäº†M2SEåœ¨æé«˜å¤šæ¨¡æ€æƒ…æ„Ÿæ„ŸçŸ¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaoyaoxinyi/M2SE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaoyaoxinyi/M2SEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08049v2">PDF</a> </p>
<p><strong>Summary</strong><br>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«åœ¨äººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰é¢†åŸŸæœ‰ç€é‡è¦åº”ç”¨ã€‚ä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ç”±äºä¸åŒæ¨¡æ€çš„ä¿¡å·å†²çªï¼Œå¸¸å¸¸æ— æ³•æ•æ‰æƒ…æ„Ÿè¡¨è¾¾çš„å¤æ‚æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºM2SEç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿä¸æƒ…ç»ªæŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼Œç”¨äºé€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚å®ƒé‡‡ç”¨ç»„åˆçš„æ–¹æ³•è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨å®Œæˆè¯¸å¦‚å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ç­‰äº”ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥Emotion Multitaskæ•°æ®é›†ï¼ˆEMTï¼‰æ¥æ”¯æŒè¿™äº”ä¸ªä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨M2SEç­–ç•¥çš„æ¨¡å‹åœ¨æƒ…æ„Ÿä¸æƒ…ç»ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå±•ç°äº†è¯¥ç­–ç•¥åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿæ„ŸçŸ¥ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿåˆ†æå’Œæƒ…ç»ªè¯†åˆ«å¯¹äºäººæœºäº¤äº’å’ŒæŠ‘éƒç—‡æ£€æµ‹ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•éš¾ä»¥æ•æ‰æƒ…æ„Ÿè¡¨è¾¾çš„å¤æ‚æ€§ï¼Œå› ä¸ºä¸åŒæ¨¡æ€çš„ä¿¡å·ä¼šäº§ç”Ÿå†²çªã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†M2SEç­–ç•¥ï¼Œä¸€ç§ç”¨äºé€šç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¤šé˜¶æ®µå¤šä»»åŠ¡æƒ…æ„Ÿä¸æƒ…ç»ªæŒ‡ä»¤å¾®è°ƒç­–ç•¥ã€‚</li>
<li>M2SEç­–ç•¥ç»“åˆäº†å¤šç§ä»»åŠ¡è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªè¯†åˆ«ç­‰äº”ä¸ªä»»åŠ¡ã€‚</li>
<li>å¼•å…¥çš„Emotion Multitaskæ•°æ®é›†ï¼ˆEMTï¼‰æ”¯æŒè¿™äº”ä¸ªä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨M2SEç­–ç•¥çš„æ¨¡å‹åœ¨æƒ…æ„Ÿä¸æƒ…ç»ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f4a87a463e046134764dd3bf88a94e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb6d4b0eee7bfa7df2ee20d21c2a870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-656eb8b6e5a7ad3dbe81dc98ecefc19e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dabbfc40d3170439bedcf456165cbfc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-013e121e722acb5e7a7aaa3f10382a8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aed6d3fa0691b18c7f1a199e283b4c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b8821a0398494275c6700434720d2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b269adea7064e2e0754c6ec8a7ec527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f71e0c2c8814932cd099c9803a9b4499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ff114cd346d3c4faa47458f6182966a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Transformers-Can-Navigate-Mazes-With-Multi-Step-Prediction"><a href="#Transformers-Can-Navigate-Mazes-With-Multi-Step-Prediction" class="headerlink" title="Transformers Can Navigate Mazes With Multi-Step Prediction"></a>Transformers Can Navigate Mazes With Multi-Step Prediction</h2><p><strong>Authors:Niklas Nolte, Ouail Kitouni, Adina Williams, Mike Rabbat, Mark Ibrahim</strong></p>
<p>Despite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning. This limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation. The standard next single token prediction objective, however, offers no explicit mechanism to predict multiple steps ahead - or revisit the path taken so far. Consequently, in this work we study whether explicitly predicting multiple steps ahead (and backwards) can improve transformersâ€™ maze navigation. We train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction and MLM-U, an objective explicitly predicting multiple steps ahead and backwards. We find that MLM-U considerably improves transformersâ€™ ability to navigate mazes compared to standard next token prediction across maze types and complexities. We also find MLM-U training is 4x more sample efficient and converges 2x faster in terms of GPU training hours relative to next token training. Finally, for more complex mazes we find MLM-U benefits from scaling to larger transformers. Remarkably, we find transformers trained with MLM-U outperform larger transformers trained with next token prediction using additional supervision from A* search traces. We hope these findings underscore the promise of learning objectives to advance transformersâ€™ capacity for long-term planning. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/maze_navigation_MLMU">https://github.com/facebookresearch/maze_navigation_MLMU</a> </p>
<blockquote>
<p>å°½ç®¡å®ƒä»¬åœ¨è¯­è¨€å»ºæ¨¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†ç»è¿‡è®­ç»ƒçš„ç”¨äºé¢„æµ‹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªæ ‡è®°çš„è½¬æ¢å™¨åœ¨è¿›è¡Œé•¿æœŸè§„åˆ’æ—¶å´é‡åˆ°äº†å›°éš¾ã€‚è¿™ç§å±€é™æ€§åœ¨éœ€è¦é¢„è§æœªæ¥ä»¥è§„åˆ’å¤šä¸ªæ­¥éª¤çš„ä»»åŠ¡ï¼ˆå¦‚è¿·å®«å¯¼èˆªï¼‰ä¸­è¡¨ç°å¾—å°¤ä¸ºæ˜æ˜¾ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„ä¸‹ä¸€ä¸ªå•ä¸€æ ‡è®°é¢„æµ‹ç›®æ ‡å¹¶æ²¡æœ‰æä¾›æ˜ç¡®çš„æœºåˆ¶æ¥é¢„æµ‹æœªæ¥çš„å¤šä¸ªæ­¥éª¤ï¼Œä¹Ÿæ— æ³•é‡æ–°è®¿é—®è¿„ä»Šä¸ºæ­¢æ‰€èµ°çš„è·¯å¾„ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ˜ç¡®é¢„æµ‹æœªæ¥ï¼ˆå’Œå›æº¯ï¼‰å¤šä¸ªæ­¥éª¤æ˜¯å¦èƒ½å¤Ÿæ”¹å–„è½¬æ¢å™¨çš„è¿·å®«å¯¼èˆªèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ç›¸åŒè®¾ç½®ä¸‹ä»å¤´å¼€å§‹è®­ç»ƒå‚æ•°åŒ¹é…çš„è½¬æ¢å™¨ï¼Œä»¥åœ¨å¤šç§ç±»å‹å’Œå¤§å°çš„è¿·å®«ä¸­è¿›è¡Œå¯¼èˆªï¼Œä½¿ç”¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’ŒMLMUï¼ˆä¸€ç§æ˜ç¡®é¢„æµ‹æœªæ¥å’Œå›æº¯å¤šä¸ªæ­¥éª¤çš„ç›®æ ‡ï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›¸æ¯”ï¼ŒMLMUåœ¨è¿·å®«å¯¼èˆªæ–¹é¢å¤§å¤§æé«˜äº†è½¬æ¢å™¨çš„èƒ½åŠ›ï¼Œå¹¶ä¸”è¿™ç§æ”¹è¿›åœ¨å„ç§ç±»å‹å’Œå¤æ‚åº¦çš„è¿·å®«ä¸­éƒ½å­˜åœ¨ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç›¸å¯¹äºä¸‹ä¸€ä¸ªæ ‡è®°è®­ç»ƒï¼ŒMLMUè®­ç»ƒæ ·æœ¬æ•ˆç‡æé«˜äº†4å€ï¼Œåœ¨GPUè®­ç»ƒå°æ—¶æ•°æ–¹é¢æ”¶æ•›é€Ÿåº¦æé«˜äº†ä¸¤å€ã€‚å¯¹äºæ›´å¤æ‚çš„è¿·å®«ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨MLMUçš„æ›´å¤§è½¬æ¢å™¨å—ç›Šäºè§„æ¨¡æ‰©å¤§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨MLMUè®­ç»ƒçš„è½¬æ¢å™¨åœ¨é™„åŠ çš„A*æœç´¢è½¨è¿¹ç›‘ç£ä¸‹è¶…è¿‡äº†æ›´å¤§è½¬æ¢å™¨ä½¿ç”¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„è®­ç»ƒè¡¨ç°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å‘ç°èƒ½å¤Ÿçªæ˜¾å­¦ä¹ ç›®æ ‡åœ¨æ¨åŠ¨è½¬æ¢å™¨é•¿æœŸè§„åˆ’èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/maze_navigation_MLMU">https://github.com/facebookresearch/maze_navigation_MLMU</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05117v2">PDF</a> 20 pages, 15 figures</p>
<p><strong>Summary</strong>ï¼šé€šè¿‡æ˜ç¡®é¢„æµ‹æœªæ¥å¤šä¸ªæ­¥éª¤ï¼ˆåŠåå‘é¢„æµ‹ï¼‰çš„ç›®æ ‡ï¼Œæ”¹è¿›äº†å˜å‹å™¨åœ¨è¿·å®«å¯¼èˆªä¸­çš„è¡¨ç°ã€‚ä¸æ ‡å‡†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›¸æ¯”ï¼ŒMLMUç›®æ ‡æ˜¾è‘—æé«˜å˜å‹å™¨åœ¨ä¸åŒç±»å‹å’Œå¤æ‚è¿·å®«ä¸­çš„å¯¼èˆªèƒ½åŠ›ï¼Œæ›´æœ‰æ•ˆç‡å¹¶åŠ é€Ÿæ”¶æ•›ã€‚å¯¹äºæ›´å¤æ‚çš„è¿·å®«ï¼Œæ›´å¤§è§„æ¨¡çš„å˜å‹å™¨å—ç›ŠäºMLMUè®­ç»ƒã€‚è¯¥ç ”ç©¶å‘ç°MLMUç›®æ ‡åœ¨æ¨è¿›å˜å‹å™¨çš„é•¿æœŸè§„åˆ’èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å˜å‹å™¨åœ¨è‡ªç„¶è¯­è¨€å»ºæ¨¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨éœ€è¦é•¿æœŸè§„åˆ’çš„ä»»åŠ¡ä¸­è¡¨ç°å—é™ï¼Œå¦‚è¿·å®«å¯¼èˆªã€‚</li>
<li>æ ‡å‡†ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡æ²¡æœ‰æ˜ç¡®çš„æœºåˆ¶æ¥é¢„æµ‹æœªæ¥çš„å¤šä¸ªæ­¥éª¤æˆ–é‡æ–°è®¿é—®å·²èµ°è¿‡çš„è·¯å¾„ã€‚</li>
<li>é€šè¿‡æ˜ç¡®é¢„æµ‹æœªæ¥å’Œè¿‡å»çš„å¤šä¸ªæ­¥éª¤ï¼ˆMLMUç›®æ ‡ï¼‰ï¼Œå¯ä»¥æ”¹å–„å˜å‹å™¨åœ¨è¿·å®«å¯¼èˆªä¸­çš„è¡¨ç°ã€‚</li>
<li>MLMUç›®æ ‡æ˜¾è‘—æé«˜å˜å‹å™¨åœ¨ä¸åŒç±»å‹å’Œå¤æ‚è¿·å®«ä¸­çš„å¯¼èˆªèƒ½åŠ›ã€‚</li>
<li>MLMUè®­ç»ƒç›¸å¯¹äºä¸‹ä¸€ä¸ªæ ‡è®°è®­ç»ƒæ›´åŠ é«˜æ•ˆï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚</li>
<li>å¯¹äºæ›´å¤æ‚çš„è¿·å®«ï¼Œæ›´å¤§è§„æ¨¡çš„å˜å‹å™¨å—ç›ŠäºMLMUè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d58f836bf1bfab5ccdf1292af0ecf1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05cdfc69b7438b304137694d3ceca207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-362c2274d5776e0798e56d80ae8ecf9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58049a7ac3341e9f868d10e384203960.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Examining-Multimodal-Gender-and-Content-Bias-in-ChatGPT-4o"><a href="#Examining-Multimodal-Gender-and-Content-Bias-in-ChatGPT-4o" class="headerlink" title="Examining Multimodal Gender and Content Bias in ChatGPT-4o"></a>Examining Multimodal Gender and Content Bias in ChatGPT-4o</h2><p><strong>Authors:Roberto Balestri</strong></p>
<p>This study investigates ChatGPT-4oâ€™s multimodal content generation, highlighting significant disparities in its treatment of sexual content and nudity versus violent and drug-related themes. Detailed analysis reveals that ChatGPT-4o consistently censors sexual content and nudity, while showing leniency towards violence and drug use. Moreover, a pronounced gender bias emerges, with female-specific content facing stricter regulation compared to male-specific content. This disparity likely stems from media scrutiny and public backlash over past AI controversies, prompting tech companies to impose stringent guidelines on sensitive issues to protect their reputations. Our findings emphasize the urgent need for AI systems to uphold genuine ethical standards and accountability, transcending mere political correctness. This research contributes to the understanding of biases in AI-driven language and multimodal models, calling for more balanced and ethical content moderation practices. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ChatGPT-4oçš„å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨å¤„ç†æ€§å†…å®¹å’Œè£¸éœ²åœºæ™¯ä¸æš´åŠ›åŠè¯ç‰©ç›¸å…³ä¸»é¢˜æ—¶å­˜åœ¨çš„æ˜¾è‘—å·®å¼‚ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼ŒChatGPT-4oå¯¹æ€§å†…å®¹å’Œè£¸éœ²åœºæ™¯è¿›è¡ŒæŒç»­å®¡æŸ¥ï¼Œè€Œå¯¹æš´åŠ›å’Œè¯ç‰©ä½¿ç”¨çš„å†…å®¹åˆ™è¡¨ç°å‡ºå®½å®¹ã€‚æ­¤å¤–ï¼Œè¿˜å‡ºç°äº†æ˜æ˜¾çš„æ€§åˆ«åè§ï¼Œå¥³æ€§ç‰¹å®šå†…å®¹é¢ä¸´çš„ç›‘ç®¡æ¯”ç”·æ€§ç‰¹å®šå†…å®¹æ›´ä¸ºä¸¥æ ¼ã€‚è¿™ç§å·®å¼‚å¾ˆå¯èƒ½æºäºè¿‡å»äººå·¥æ™ºèƒ½äº‰è®®ä¸­çš„åª’ä½“å®¡æŸ¥å’Œå…¬ä¼—åå¯¹ï¼Œä¿ƒä½¿ç§‘æŠ€å…¬å¸å¯¹æ•æ„Ÿé—®é¢˜å®æ–½ä¸¥æ ¼å‡†åˆ™ä»¥ä¿æŠ¤å…¶å£°èª‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäººå·¥æ™ºèƒ½ç³»ç»Ÿè¿«åˆ‡éœ€è¦åšæŒçœŸæ­£çš„é“å¾·æ ‡å‡†å’Œé—®è´£åˆ¶ï¼Œè¶…è¶Šå•çº¯çš„æ”¿æ²»æ­£ç¡®æ€§ã€‚æœ¬ç ”ç©¶ä¸ºç†è§£äººå·¥æ™ºèƒ½é©±åŠ¨è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹ä¸­çš„åè§åšå‡ºäº†è´¡çŒ®ï¼Œå‘¼åé‡‡ç”¨æ›´å¹³è¡¡ã€æ›´é“å¾·çš„å†…å®¹ç®¡ç†å®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19140v1">PDF</a> 17 pages, 4 figures, 3 tables. Conference: â€œ14th International   Conference on Artificial Intelligence, Soft Computing and Applications (AIAA   2024), London, 23-24 November 2024â€ It will be published in the proceedings   â€œDavid C. Wyld et al. (Eds): IoTE, CNDC, DSA, AIAA, NLPTA, DPPR - 2024â€</p>
<p><strong>Summary</strong><br>ChatGPT-4oåœ¨å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¹æ€§å†…å®¹å’Œè£¸éœ²åœºæ™¯è¿›è¡Œä¸¥æ ¼å®¡æŸ¥ï¼Œè€Œå¯¹æš´åŠ›å’Œæ¯’å“ç›¸å…³å†…å®¹è¾ƒä¸ºå®½å®¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°æ˜æ˜¾çš„æ€§åˆ«åè§ï¼Œå¥³æ€§ç›¸å…³å†…å®¹å—åˆ°çš„ç›‘ç®¡æ›´ä¸ºä¸¥æ ¼ã€‚è¿™å¯èƒ½ä¸åª’ä½“å¯¹è¿‡å»äººå·¥æ™ºèƒ½äº‰è®®çš„å…³æ³¨å’Œå…¬ä¼—çš„åå‡»æœ‰å…³ï¼Œä¿ƒä½¿æŠ€æœ¯å…¬å¸é‡‡ç”¨ä¸¥æ ¼çš„æ•æ„Ÿæ€§æŒ‡å—ä»¥ä¿æŠ¤å…¶å£°èª‰ã€‚è¯¥ç ”ç©¶å‘¼åAIç³»ç»Ÿä¿æŒçœŸæ­£çš„é“å¾·æ ‡å‡†å’Œè´£ä»»å¿ƒï¼Œè€Œä¸ä»…ä»…æ˜¯è¿½æ±‚æ”¿æ²»æ­£ç¡®æ€§ã€‚è¿™ä¸€å‘ç°æœ‰åŠ©äºäº†è§£äººå·¥æ™ºèƒ½é©±åŠ¨è¯­è¨€å’Œå¤šåª’ä½“æ¨¡å‹ä¸­çš„åè§ï¼Œå‘¼åé‡‡ç”¨æ›´åŠ å¹³è¡¡å’Œé“å¾·çš„å†…å®¹å®¡æ ¸å®è·µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChatGPT-4oåœ¨å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆä¸­å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¯¹æ€§å†…å®¹å’Œè£¸éœ²åœºæ™¯çš„è¡¨è¿°å¤„ç†å¾—æ›´åŠ ä¸¥æ ¼ã€‚</li>
<li>æš´åŠ›å’Œæ¯’å“ç›¸å…³å†…å®¹åœ¨ChatGPT-4oçš„å¤„ç†ä¸­æ˜¾å¾—è¾ƒä¸ºå®½å®¹ã€‚</li>
<li>ç ”ç©¶å‘ç°ChatGPT-4oåœ¨å¤„ç†å†…å®¹æ—¶å¯¹æ€§åˆ«å­˜åœ¨åè§ï¼Œå¥³æ€§ç›¸å…³å†…å®¹å—åˆ°çš„ç›‘ç®¡æ›´ä¸ºä¸¥æ ¼ã€‚</li>
<li>è¿™ç§å·®å¼‚å¯èƒ½æºäºåª’ä½“å¯¹è¿‡å»äººå·¥æ™ºèƒ½äº‰è®®çš„å…³æ³¨åŠå…¬ä¼—åå“ã€‚</li>
<li>æŠ€æœ¯å…¬å¸å› è¿™äº›äº‰è®®è€Œé‡‡å–ä¸¥æ ¼çš„æ•æ„Ÿæ€§æŒ‡å—ä»¥ä¿æŠ¤å…¶å£°èª‰ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒAIç³»ç»Ÿéœ€è¦è¶…è¶Šæ”¿æ²»æ­£ç¡®æ€§ï¼Œç»´æŒçœŸæ­£çš„é“å¾·æ ‡å‡†å’Œè´£ä»»å¿ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a38145daa3458c50b5fe3b20045a7f79.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Using-Large-Language-Models-for-Expert-Prior-Elicitation-in-Predictive-Modelling"><a href="#Using-Large-Language-Models-for-Expert-Prior-Elicitation-in-Predictive-Modelling" class="headerlink" title="Using Large Language Models for Expert Prior Elicitation in Predictive   Modelling"></a>Using Large Language Models for Expert Prior Elicitation in Predictive   Modelling</h2><p><strong>Authors:Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi</strong></p>
<p>Large language models (LLMs), trained on diverse data effectively acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency hinder their direct application for specialised tasks. In fields such as clinical research, acquiring expert annotations or prior knowledge about predictive models is often costly and time-consuming. This study proposes the use of LLMs to elicit expert prior distributions for predictive models. This approach also provides an alternative to in-context learning, where language models are tasked with making predictions directly. In this work, we compare LLM-elicited and uninformative priors, evaluate whether LLMs truthfully generate parameter distributions, and propose a model selection strategy for in-context learning and prior elicitation. Our findings show that LLM-elicited prior parameter distributions significantly reduce predictive error compared to uninformative priors in low-data settings. Applied to clinical problems, this translates to fewer required biological samples, lowering cost and resources. Prior elicitation also consistently outperforms and proves more reliable than in-context learning at a lower cost, making it a preferred alternative in our setting. We demonstrate the utility of this method across various use cases, including clinical applications. For infection prediction, using LLM-elicited priors reduced the number of required labels to achieve the same accuracy as an uninformative prior by 55%, 200 days earlier in the study. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åœ¨å¤šæ ·åŒ–æ•°æ®ä¸Šçš„è®­ç»ƒï¼Œæœ‰æ•ˆè·å–äº†è·¨å„ç§é¢†åŸŸçš„å¤§é‡ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®¡ç®—å¤æ‚æ€§ã€æˆæœ¬å’Œç¼ºä¹é€æ˜åº¦é˜»ç¢äº†å®ƒä»¬åœ¨ç‰¹æ®Šä»»åŠ¡ä¸Šçš„ç›´æ¥åº”ç”¨ã€‚åœ¨ä¸´åºŠç ”ç©¶é¢†åŸŸï¼Œè·å–ä¸“å®¶æ³¨é‡Šæˆ–å…³äºé¢„æµ‹æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å¾€å¾€æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚æœ¬ç ”ç©¶æå‡ºä½¿ç”¨LLMæ¥æ¿€å‘é¢„æµ‹æ¨¡å‹çš„ä¸“å®¶å…ˆéªŒåˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•è¿˜ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†æ›¿ä»£æ–¹æ¡ˆï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹è¢«ç›´æ¥ç”¨äºè¿›è¡Œé¢„æµ‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†LLMæ¿€å‘çš„å’Œæ— ä¿¡æ¯çš„å…ˆéªŒï¼Œè¯„ä¼°äº†LLMæ˜¯å¦çœŸå®åœ°ç”Ÿæˆå‚æ•°åˆ†å¸ƒï¼Œå¹¶æå‡ºäº†ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå…ˆéªŒæ¿€å‘çš„æ¨¡å‹é€‰æ‹©ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸æ— ä¿¡æ¯å…ˆéªŒç›¸æ¯”ï¼ŒLLMæ¿€å‘çš„å…ˆéªŒå‚æ•°åˆ†å¸ƒåœ¨ä½æ•°æ®è®¾ç½®ä¸­æ˜¾è‘—é™ä½äº†é¢„æµ‹è¯¯å·®ã€‚åº”ç”¨äºä¸´åºŠé—®é¢˜ï¼Œè¿™æ„å‘³ç€å‡å°‘äº†æ‰€éœ€çš„ç”Ÿç‰©æ ·æœ¬æ•°é‡ï¼Œé™ä½äº†æˆæœ¬å¹¶èŠ‚çœäº†èµ„æºã€‚å…ˆéªŒæ¿€å‘ä¹Ÿå§‹ç»ˆä¼˜äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶ä¸”åœ¨æˆæœ¬æ›´ä½çš„æƒ…å†µä¸‹è¡¨ç°å‡ºæ›´é«˜çš„å¯é æ€§ï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­æˆä¸ºé¦–é€‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨å„ç§ç”¨ä¾‹ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ï¼ŒåŒ…æ‹¬ä¸´åºŠåº”ç”¨ç¨‹åºã€‚åœ¨æ„ŸæŸ“é¢„æµ‹æ–¹é¢ï¼Œä½¿ç”¨LLMæ¿€å‘çš„å…ˆéªŒå°†å®ç°ä¸æ— ä¿¡æ¯å…ˆéªŒç›¸åŒå‡†ç¡®åº¦çš„æ‰€éœ€æ ‡ç­¾æ•°é‡å‡å°‘äº†55%ï¼Œå¹¶ä¸”åœ¨ç ”ç©¶ä¸­çš„æå‰äº†200å¤©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17284v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»è¿‡å¤šæ ·åŒ–æ•°æ®è®­ç»ƒåï¼Œèƒ½å¤Ÿè·¨åŸŸè·å–å¹¿æ³›çš„ä¿¡æ¯ã€‚ä½†å…¶è®¡ç®—å¤æ‚æ€§ã€æˆæœ¬å’Œä¸é€æ˜æ€§é™åˆ¶äº†å…¶åœ¨ä¸“é¡¹ä»»åŠ¡ä¸­çš„ç›´æ¥åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨LLMæ¥æ¿€å‘é¢„æµ‹æ¨¡å‹çš„ä¸“å®¶å…ˆéªŒåˆ†å¸ƒï¼Œä¸ºè¯­å¢ƒå­¦ä¹ æä¾›æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æ¯”è¾ƒäº†LLMæ¿€å‘çš„å’Œæ— ä¿¡æ¯çš„å…ˆéªŒï¼Œè¯„ä¼°äº†LLMæ˜¯å¦çœŸå®åœ°ç”Ÿæˆå‚æ•°åˆ†å¸ƒï¼Œå¹¶ä¸ºè¯­å¢ƒå­¦ä¹ å’Œå…ˆéªŒæ¿€å‘æå‡ºäº†æ¨¡å‹é€‰æ‹©ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMæ¿€å‘çš„å…ˆéªŒå‚æ•°åˆ†å¸ƒåœ¨ä½æ•°æ®ç¯å¢ƒä¸­æ˜¾è‘—å‡å°‘äº†é¢„æµ‹è¯¯å·®ã€‚åœ¨ä¸´åºŠåŒ»å­¦é—®é¢˜ä¸­ï¼Œè¿™æ„å‘³ç€å‡å°‘äº†æ‰€éœ€çš„ç”Ÿç‰©æ ·æœ¬æ•°é‡ï¼Œé™ä½äº†æˆæœ¬å’Œèµ„æºæ¶ˆè€—ã€‚ä¸è¯­å¢ƒå­¦ä¹ ç›¸æ¯”ï¼Œå…ˆéªŒæ¿€å‘è¡¨ç°å‡ºäº†ä¸€è‡´æ€§å’Œå¯é æ€§ï¼Œå¹¶é™ä½äº†æˆæœ¬ï¼Œä½¿å…¶æˆä¸ºé¦–é€‰æ–¹æ³•ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å„ç§ç”¨ä¾‹ä¸­çš„å®ç”¨æ€§ï¼ŒåŒ…æ‹¬ä¸´åºŠåº”ç”¨ç¨‹åºã€‚å¯¹äºæ„ŸæŸ“é¢„æµ‹ï¼Œä½¿ç”¨LLMæ¿€å‘çš„å…ˆéªŒå¯å°†è¾¾åˆ°ç›¸åŒå‡†ç¡®åº¦çš„æ ‡ç­¾æ•°é‡å‡å°‘55%ï¼Œå¹¶åœ¨ç ”ç©¶æ—©æœŸæå‰äº†200å¤©å®ç°è¿™ä¸€ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsç»è¿‡å¤šæ ·åŒ–æ•°æ®è®­ç»ƒåèƒ½å¤Ÿè·¨åŸŸè·å–å¹¿æ³›ä¿¡æ¯ã€‚</li>
<li>LLMsçš„è®¡ç®—å¤æ‚æ€§ã€æˆæœ¬å’Œä¸é€æ˜æ€§é™åˆ¶äº†å…¶åœ¨ä¸“é¡¹ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>åˆ©ç”¨LLMæ¿€å‘é¢„æµ‹æ¨¡å‹çš„ä¸“å®¶å…ˆéªŒåˆ†å¸ƒå¯ä¸ºè¯­å¢ƒå­¦ä¹ æä¾›æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>LLMæ¿€å‘çš„å…ˆéªŒå‚æ•°åˆ†å¸ƒåœ¨ä½æ•°æ®ç¯å¢ƒä¸­æ˜¾è‘—å‡å°‘é¢„æµ‹è¯¯å·®ã€‚</li>
<li>åœ¨ä¸´åºŠåŒ»å­¦é—®é¢˜ä¸­ï¼Œä½¿ç”¨LLMæ¿€å‘çš„å…ˆéªŒå¯å‡å°‘æ‰€éœ€çš„ç”Ÿç‰©æ ·æœ¬æ•°é‡ï¼Œé™ä½æˆæœ¬å’Œèµ„æºæ¶ˆè€—ã€‚</li>
<li>ä¸è¯­å¢ƒå­¦ä¹ ç›¸æ¯”ï¼Œå…ˆéªŒæ¿€å‘å±•ç°å‡ºæ›´é«˜çš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f52b642e0e28f9bb35a666166db5007a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-200b5d518078ce64d6a186a04a794832.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eb16d8481265b11f92f6f43aec4a1be4.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-20  ROMAS A Role-Based Multi-Agent System for Database monitoring and   Planning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-19/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5291e9ff07cac3f6ca260ffb69ba9609.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-19  Light-T2M A Lightweight and Fast Model for Text-to-motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16663.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
