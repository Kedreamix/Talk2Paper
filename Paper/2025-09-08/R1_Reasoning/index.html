<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  SimpleTIR End-to-End Reinforcement Learning for Multi-Turn   Tool-Integrated Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01321v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning"><a href="#SimpleTIR-End-to-End-Reinforcement-Learning-for-Multi-Turn-Tool-Integrated-Reasoning" class="headerlink" title="SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn   Tool-Integrated Reasoning"></a>SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn   Tool-Integrated Reasoning</h2><p><strong>Authors:Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, Bo An</strong></p>
<p>Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸å¤–éƒ¨å·¥å…·è¿›è¡Œäº¤äº’ï¼Œå³æ‰€è°“çš„å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰èŒƒå¼ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†TIRæ‰©å±•åˆ°å¤šè½®åœºæ™¯å¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒå¸¸å¸¸ä¼šå—åˆ°è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½å´©æºƒçš„é˜»ç¢ã€‚æˆ‘ä»¬ç¡®å®šè¿™ç§ä¸ç¨³å®šä¸»è¦ç”±å¤–éƒ¨å·¥å…·åé¦ˆçš„åˆ†å¸ƒæ¼‚ç§»å¼•èµ·ï¼Œå¯¼è‡´ç”Ÿæˆä½æ¦‚ç‡ä»¤ç‰Œã€‚è¿™ä¸ªé—®é¢˜åœ¨å¤šè½®ä¸­æ„ˆæ¼”æ„ˆçƒˆï¼Œå¯¼è‡´æ¢¯åº¦èŒƒæ•°ç¾éš¾æ€§çˆ†ç‚¸ï¼Œä»è€Œç ´åè®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SimpleTIRï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„ç®—æ³•ï¼Œå¯ä»¥ç¨³å®šå¤šè½®TIRè®­ç»ƒã€‚å®ƒçš„æ ¸å¿ƒç­–ç•¥æ˜¯è¯†åˆ«å’Œè¿‡æ»¤å‡ºåŒ…å«ç©ºè½¬çš„è½¨è¿¹ï¼Œå³é‚£äº›æ—¢æ²¡æœ‰äº§ç”Ÿä»£ç å—ä¹Ÿæ²¡æœ‰äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆçš„è½®æ¬¡ã€‚é€šè¿‡ä»ç­–ç•¥æ›´æ–°ä¸­åˆ é™¤è¿™äº›æœ‰é—®é¢˜çš„è½¨è¿¹ï¼ŒSimpleTIRæœ‰æ•ˆåœ°é˜»æ­¢äº†æœ‰å®³çš„é«˜å¹…åº¦æ¢¯åº¦ï¼Œä»è€Œç¨³å®šäº†å­¦ä¹ åŠ¨æ€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSimpleTIRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»Qwen2.5-7BåŸºç¡€æ¨¡å‹å¼€å§‹ï¼ŒSimpleTIRå°†AIME24å¾—åˆ†ä»ä»…æ–‡æœ¬çš„åŸºçº¿22.1æé«˜åˆ°50.5ã€‚æ­¤å¤–ï¼Œé€šè¿‡é¿å…ç›‘ç£å¾®è°ƒï¼ˆsupervised fine-tuningï¼‰çš„çº¦æŸï¼ŒSimpleTIRé¼“åŠ±æ¨¡å‹å‘ç°å¤šæ ·åŒ–å’Œå¤æ‚çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’Œäº¤å‰éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02479v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰èŒƒå¼ä¸å¤–éƒ¨å·¥å…·äº¤äº’ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†TIRæ‰©å±•åˆ°å¤šè½®åœºæ™¯æ—¶ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¸¸é¢ä¸´è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚æœ¬æ–‡æŒ‡å‡ºè¿™ç§ä¸ç¨³å®šä¸»è¦ç”±å¤–éƒ¨å·¥å…·åé¦ˆçš„åˆ†å¸ƒæ¼‚ç§»å¯¼è‡´ç”Ÿæˆä½æ¦‚ç‡æ ‡è®°å¼•èµ·ã€‚è¯¥é—®é¢˜åœ¨å¤šè½®äº¤äº’ä¸­åŠ å‰§ï¼Œå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼Œç ´åè®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºSimpleTIRç®—æ³•ï¼Œä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ç¨³å®šå¤šè½®TIRè®­ç»ƒã€‚å…¶æ ¸å¿ƒç­–ç•¥æ˜¯è¯†åˆ«å’Œè¿‡æ»¤æ‰åŒ…å«ç©ºæ´å›åˆçš„è½¨è¿¹ï¼Œå³æ—¢ä¸äº§ç”Ÿä»£ç å—ä¹Ÿä¸äº§ç”Ÿæœ€ç»ˆç­”æ¡ˆçš„å›åˆã€‚é€šè¿‡ä»ç­–ç•¥æ›´æ–°ä¸­ç§»é™¤è¿™äº›æœ‰é—®é¢˜çš„è½¨è¿¹ï¼ŒSimpleTIRæœ‰æ•ˆåœ°é˜»æ­¢äº†æœ‰å®³çš„é«˜å¹…åº¦æ¢¯åº¦ï¼Œä»è€Œç¨³å®šå­¦ä¹ åŠ¨æ€ã€‚å®éªŒè¡¨æ˜ï¼ŒSimpleTIRåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä»Qwen2.5-7BåŸºç¡€æ¨¡å‹å¼€å§‹ï¼Œå°†AIME24å¾—åˆ†ä»ä»…æ–‡æœ¬çš„22.1æé«˜åˆ°50.5ã€‚æ­¤å¤–ï¼ŒSimpleTIRé¿å…äº†ç›‘ç£ç²¾ç»†è°ƒæ•´çš„é™åˆ¶ï¼Œé¼“åŠ±æ¨¡å‹å‘ç°å¤šæ ·åŒ–å’Œå¤æ‚çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚è‡ªæˆ‘æ ¡æ­£å’Œäº¤å‰éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä¸å¤–éƒ¨å·¥å…·äº¤äº’ï¼Œæé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šè½®åœºæ™¯ä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡ŒTIRé¢ä¸´è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸‹é™çš„æŒ‘æˆ˜ã€‚</li>
<li>è®­ç»ƒä¸ç¨³å®šçš„ä¸»è¦åŸå› æ˜¯å¤–éƒ¨å·¥å…·åé¦ˆçš„åˆ†å¸ƒæ¼‚ç§»å¯¼è‡´ç”Ÿæˆä½æ¦‚ç‡æ ‡è®°ã€‚</li>
<li>SimpleTIRç®—æ³•é€šè¿‡è¯†åˆ«å’Œè¿‡æ»¤æ‰åŒ…å«ç©ºæ´å›åˆçš„è½¨è¿¹æ¥ç¨³å®šå¤šè½®TIRè®­ç»ƒã€‚</li>
<li>SimpleTIRå®ç°äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>SimpleTIRå°†AIME24å¾—åˆ†ä»ä»…æ–‡æœ¬çš„åŸºå‡†æé«˜è‡³æ›´é«˜æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02479v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02479v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02479v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02479v2/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Unifi3D-A-Study-on-3D-Representations-for-Generation-and-Reconstruction-in-a-Common-Framework"><a href="#Unifi3D-A-Study-on-3D-Representations-for-Generation-and-Reconstruction-in-a-Common-Framework" class="headerlink" title="Unifi3D: A Study on 3D Representations for Generation and Reconstruction   in a Common Framework"></a>Unifi3D: A Study on 3D Representations for Generation and Reconstruction   in a Common Framework</h2><p><strong>Authors:Nina Wiedemann, Sainan Liu, Quentin Leboutet, Katelyn Gao, Benjamin Ummenhofer, Michael Paulitsch, Kai Yuan</strong></p>
<p>Following rapid advancements in text and image generation, research has increasingly shifted towards 3D generation. Unlike the well-established pixel-based representation in images, 3D representations remain diverse and fragmented, encompassing a wide variety of approaches such as voxel grids, neural radiance fields, signed distance functions, point clouds, or octrees, each offering distinct advantages and limitations. In this work, we present a unified evaluation framework designed to assess the performance of 3D representations in reconstruction and generation. We compare these representations based on multiple criteria: quality, computational efficiency, and generalization performance. Beyond standard model benchmarking, our experiments aim to derive best practices over all steps involved in the 3D generation pipeline, including preprocessing, mesh reconstruction, compression with autoencoders, and generation. Our findings highlight that reconstruction errors significantly impact overall performance, underscoring the need to evaluate generation and reconstruction jointly. We provide insights that can inform the selection of suitable 3D models for various applications, facilitating the development of more robust and application-specific solutions in 3D generation. The code for our framework is available at <a target="_blank" rel="noopener" href="https://github.com/isl-org/unifi3d">https://github.com/isl-org/unifi3d</a>. </p>
<blockquote>
<p>éšç€æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆçš„å¿«é€Ÿå‘å±•ï¼Œç ”ç©¶è¶Šæ¥è¶Šå¤šåœ°è½¬å‘3Dç”Ÿæˆã€‚ä¸å›¾åƒä¸­å·²å»ºç«‹åŸºäºåƒç´ çš„è¡¨ç¤ºä¸åŒï¼Œ3Dè¡¨ç¤ºä»ç„¶å¤šæ ·ä¸”åˆ†æ•£ï¼Œæ¶µç›–äº†å¤šç§æ–¹æ³•ï¼Œå¦‚ä½“ç´ ç½‘æ ¼ã€ç¥ç»è¾å°„åœºã€æœ‰ç¬¦å·è·ç¦»å‡½æ•°ã€ç‚¹äº‘æˆ–å…«å‰æ ‘ç­‰ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°3Dè¡¨ç¤ºåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬åŸºäºå¤šä¸ªæ ‡å‡†å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œæ¯”è¾ƒï¼šè´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ã€‚é™¤äº†æ ‡å‡†æ¨¡å‹åŸºå‡†æµ‹è¯•ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒæ—¨åœ¨å¾—å‡ºæ¶‰åŠæ•´ä¸ª3Dç”Ÿæˆç®¡é“çš„æ‰€æœ‰æ­¥éª¤çš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€ç½‘æ ¼é‡å»ºã€ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨çš„å‹ç¼©å’Œç”Ÿæˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡å»ºè¯¯å·®å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼Œå¼ºè°ƒéœ€è¦è”åˆè¯„ä¼°ç”Ÿæˆå’Œé‡å»ºã€‚æˆ‘ä»¬æä¾›äº†è§è§£ï¼Œå¯ä»¥ä¸ºå„ç§åº”ç”¨é€‰æ‹©åˆé€‚çš„ä¸‰ç»´æ¨¡å‹æä¾›ä¿¡æ¯ï¼Œä¿ƒè¿›å¼€å‘æ›´ç¨³å¥ä¸”é’ˆå¯¹ç‰¹å®šåº”ç”¨çš„3Dç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/isl-org/unifi3d">https://github.com/isl-org/unifi3d</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02474v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ–‡æœ¬å’Œå›¾åƒç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä¹‹åï¼Œç ”ç©¶é€æ¸è½¬å‘3Dç”ŸæˆæŠ€æœ¯ã€‚ä¸åŒäºå›¾åƒä¸­å·²æˆç†Ÿçš„åƒç´ åŸºç¡€è¡¨ç¤ºï¼Œ3Dè¡¨ç¤ºä»ç„¶å¤šæ ·ä¸”åˆ†æ•£ï¼ŒåŒ…æ‹¬ä½“ç´ ç½‘æ ¼ã€ç¥ç»è¾å°„åœºã€æœ‰ç¬¦å·è·ç¦»å‡½æ•°ã€ç‚¹äº‘å’Œå…«å‰æ ‘ç­‰å¤šç§æ–¹æ³•ï¼Œå„æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œå±€é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°3Dè¡¨ç¤ºåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬åŸºäºè´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ç­‰å¤šä¸ªæ ‡å‡†å¯¹è¿™äº›è¡¨ç¤ºè¿›è¡Œäº†æ¯”è¾ƒã€‚é™¤äº†æ ‡å‡†æ¨¡å‹åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜æ—¨åœ¨æ¨å¯¼æ¶‰åŠæ•´ä¸ª3Dç”Ÿæˆç®¡é“çš„æ‰€æœ‰æ­¥éª¤çš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€ç½‘æ ¼é‡å»ºã€è‡ªåŠ¨ç¼–ç å™¨çš„å‹ç¼©å’Œç”Ÿæˆã€‚æˆ‘ä»¬å‘ç°é‡å»ºé”™è¯¯å¯¹æ•´ä½“æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ï¼Œå¼ºè°ƒéœ€è¦è”åˆè¯„ä¼°å’Œç”Ÿæˆå’Œé‡å»ºã€‚æˆ‘ä»¬çš„è§è§£å¯ä»¥ä¸ºå„ç§åº”ç”¨é€‰æ‹©åˆé€‚çš„ä¸‰ç»´æ¨¡å‹æä¾›ä¾æ®ï¼Œä¿ƒè¿›ä¸‰ç»´ç”ŸæˆæŠ€æœ¯åœ¨ç¨³å¥æ€§å’Œåº”ç”¨ç‰¹å®šè§£å†³æ–¹æ¡ˆæ–¹é¢çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å·²é€æ¸ä»æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆè½¬å‘3Dç”ŸæˆæŠ€æœ¯ã€‚</li>
<li>3Dè¡¨ç¤ºæ–¹æ³•å¤šæ ·ï¼ŒåŒ…æ‹¬ä½“ç´ ç½‘æ ¼ã€ç¥ç»è¾å°„åœºç­‰ï¼Œå„æœ‰ä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°3Dè¡¨ç¤ºåœ¨é‡å»ºå’Œç”Ÿæˆä¸­çš„æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°åŸºäºè´¨é‡ã€è®¡ç®—æ•ˆç‡å’Œæ³›åŒ–æ€§èƒ½ç­‰å¤šä¸ªæ ‡å‡†ã€‚</li>
<li>å®éªŒç›®çš„åŒ…æ‹¬æ¨å¯¼3Dç”Ÿæˆç®¡é“çš„æœ€ä½³å®è·µã€‚</li>
<li>é‡å»ºé”™è¯¯å¯¹3Dè¡¨ç¤ºçš„æ•´ä½“æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02474v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02474v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02474v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DCPO-Dynamic-Clipping-Policy-Optimization"><a href="#DCPO-Dynamic-Clipping-Policy-Optimization" class="headerlink" title="DCPO: Dynamic Clipping Policy Optimization"></a>DCPO: Dynamic Clipping Policy Optimization</h2><p><strong>Authors:Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, Rihui Xin</strong></p>
<p>Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7&#x2F;31.6) and GRPO (36.7&#x2F;32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3&#x2F;19.0), surpassing GRPO (13.3&#x2F;10.5) and DAPO (20.0&#x2F;15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPOâ€™s effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ç»å¸¸é¢ä¸´é›¶æ¢¯åº¦çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜ä¸»è¦æ˜¯ç”±äºå›ºå®šå‰ªè¾‘è¾¹ç•Œçš„ä»¤ç‰Œçº§åˆ«æ¦‚ç‡æ¯”ç‡ä»¥åŠæ ‡å‡†åŒ–ç›¸åŒå¥–åŠ±æ‰€å¯¼è‡´çš„ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¢¯åº¦æ›´æ–°æ— æ•ˆä»¥åŠç”Ÿæˆçš„å“åº”åˆ©ç”¨ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å‰ªè¾‘ç­–ç•¥ä¼˜åŒ–ï¼ˆDCPOï¼‰ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§åŠ¨æ€å‰ªè¾‘ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥åŸºäºä»¤ç‰Œç‰¹å®šå…ˆéªŒæ¦‚ç‡è‡ªé€‚åº”åœ°è°ƒæ•´å‰ªè¾‘è¾¹ç•Œï¼Œä»¥å¢å¼ºä»¤ç‰Œçº§åˆ«çš„æ¢ç´¢ï¼Œä»¥åŠä¸€ç§å¹³æ»‘ä¼˜åŠ¿æ ‡å‡†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥åœ¨ç´¯ç§¯çš„è®­ç»ƒæ­¥éª¤ä¸­æ ‡å‡†åŒ–å¥–åŠ±ï¼Œä»¥æé«˜ç”Ÿæˆå“åº”çš„å“åº”çº§åˆ«æœ‰æ•ˆåˆ©ç”¨ã€‚DCPOåœ¨åŸºäºå››ç§ä¸åŒæ¨¡å‹çš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME24åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOåœ¨è´ªå©ªè§£ç ä¸‹å®ç°äº†Avg@1çš„46.7ï¼Œåœ¨32æ¬¡é‡‡æ ·ä¸‹å®ç°äº†Avg@32çš„38.8ï¼Œè¶…è¶Šäº†DAPOï¼ˆ36.7&#x2F;31.6ï¼‰å’ŒGRPOï¼ˆ36.7&#x2F;32.1ï¼‰åœ¨Qwen2.5-Math-7Bæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚åœ¨åŸºäºQwen2.5-14Bçš„AIME25åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDCPOçš„æ€§èƒ½è¾¾åˆ°äº†ï¼ˆ23.3&#x2F;19.0ï¼‰ï¼Œè¶…è¶Šäº†GRPOï¼ˆ13.3&#x2F;10.5ï¼‰å’ŒDAPOï¼ˆ20.0&#x2F;15.3ï¼‰ã€‚æ­¤å¤–ï¼ŒDCPOåœ¨å››ä¸ªæ¨¡å‹ä¸­çš„éé›¶ä¼˜åŠ¿å¹³å‡æé«˜äº†28%ï¼Œè®­ç»ƒæ•ˆç‡æ˜¯DAPOçš„ä¸¤å€ï¼Œå¹¶ä¸”ä¸GRPOå’ŒDAPOç›¸æ¯”ï¼Œä»¤ç‰Œå‰ªè¾‘æ¯”ä¾‹é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†DCPOåœ¨åˆ©ç”¨ç”Ÿæˆæ•°æ®æ–¹é¢æ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02333v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¦‚GRPOå¸¸å¸¸é­é‡é›¶æ¢¯åº¦é—®é¢˜ã€‚è¯¥é—®é¢˜ä¸»è¦æºäºå›ºå®šå‰ªè¾‘è¾¹ç•Œçš„ä»¤ç‰Œçº§æ¦‚ç‡æ¯”ç‡ä»¥åŠç›¸åŒå¥–åŠ±çš„æ ‡å‡†åŒ–ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¢¯åº¦æ›´æ–°æ— æ•ˆå’Œç”Ÿæˆå“åº”çš„åˆ©ç”¨ç‡ä½ä¸‹ã€‚æœ¬ç ”ç©¶æå‡ºåŠ¨æ€å‰ªè¾‘ç­–ç•¥ä¼˜åŒ–ï¼ˆDCPOï¼‰ï¼Œå¼•å…¥åŠ¨æ€å‰ªè¾‘ç­–ç•¥ï¼Œæ ¹æ®ä»¤ç‰Œç‰¹å®šå…ˆéªŒæ¦‚ç‡è‡ªé€‚åº”è°ƒæ•´å‰ªè¾‘è¾¹ç•Œï¼Œä»¥æé«˜ä»¤ç‰Œçº§æ¢ç´¢ï¼Œä»¥åŠå¹³æ»‘ä¼˜åŠ¿æ ‡å‡†åŒ–æŠ€æœ¯ï¼Œåœ¨ç´¯ç§¯è®­ç»ƒæ­¥éª¤ä¸­æ ‡å‡†åŒ–å¥–åŠ±ï¼Œä»¥æé«˜å“åº”çº§çš„æœ‰æ•ˆåˆ©ç”¨ç‡ã€‚DCPOåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŸºäºå››ç§ä¸åŒæ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME24åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOåœ¨è´ªå©ªè§£ç ä¸‹è¾¾åˆ°46.7çš„Avg@1ï¼Œåœ¨32æ¬¡é‡‡æ ·ä¸‹è¾¾åˆ°38.8çš„Avg@32ï¼Œè¶…è¶Šäº†DAPOï¼ˆ36.7&#x2F;31.6ï¼‰å’ŒGRPOï¼ˆ36.7&#x2F;32.1ï¼‰åœ¨Qwen2.5-Math-7Bæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚åœ¨åŸºäºQwen2.5-14Bçš„AIME25åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDCPOè¾¾åˆ°ï¼ˆ23.3&#x2F;19.0ï¼‰çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†GRPOï¼ˆ13.3&#x2F;10.5ï¼‰å’ŒDAPOï¼ˆ20.0&#x2F;15.3ï¼‰ã€‚æ­¤å¤–ï¼ŒDCPOåœ¨å››ä¸ªæ¨¡å‹ä¸­çš„éé›¶ä¼˜åŠ¿å¹³å‡æé«˜äº†28%ï¼Œè®­ç»ƒæ•ˆç‡æ˜¯DAPOçš„ä¸¤å€ï¼Œå¹¶ä¸”æ˜¾è‘—é™ä½äº†ä»¤ç‰Œå‰ªè¾‘æ¯”ç‡ï¼Œä¸GRPOå’ŒDAPOç›¸æ¯”é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœçªæ˜¾äº†DCPOåœ¨åˆ©ç”¨ç”Ÿæˆæ•°æ®æ–¹é¢æ›´æœ‰æ•ˆåœ°è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLVRæ¡†æ¶å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚GRPOé¢ä¸´é›¶æ¢¯åº¦é—®é¢˜ï¼Œä¸»è¦æºäºå›ºå®šçš„å‰ªè¾‘è¾¹ç•Œå’Œå¥–åŠ±æ ‡å‡†åŒ–ã€‚</li>
<li>DCPOé€šè¿‡åŠ¨æ€è°ƒæ•´å‰ªè¾‘ç­–ç•¥å’Œå¼•å…¥å¹³æ»‘ä¼˜åŠ¿æ ‡å‡†åŒ–æŠ€æœ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DCPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨AIME24å’ŒAIME25ä¸Šçš„è¡¨ç°çªå‡ºã€‚</li>
<li>DCPOç›¸æ¯”å…¶ä»–æ–¹æ³•åœ¨éé›¶ä¼˜åŠ¿å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>DCPOæ˜¾è‘—é™ä½äº†ä»¤ç‰Œå‰ªè¾‘æ¯”ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02333v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02333v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02333v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System"><a href="#Baichuan-M2-Scaling-Medical-Capability-with-Large-Verifier-System" class="headerlink" title="Baichuan-M2: Scaling Medical Capability with Large Verifier System"></a>Baichuan-M2: Scaling Medical Capability with Large Verifier System</h2><p><strong>Authors:Baichuan-M2 Team,  :, Chengfeng Dou, Chong Liu, Fan Yang, Fei Li, Jiyuan Jia, Mingyang Chen, Qiang Ju, Shuai Wang, Shunya Dang, Tianpeng Li, Xiangrong Zeng, Yijie Zhou, Chenzheng Zhu, Da Pan, Fei Deng, Guangwei Ai, Guosheng Dong, Hongda Zhang, Jinyang Tai, Jixiang Hong, Kai Lu, Linzhuang Sun, Peidong Guo, Qian Ma, Rihui Xin, Shihui Yang, Shusen Zhang, Yichuan Mo, Zheng Liang, Zhishou Zhang, Hengfu Cui, Zuyi Zhu, Xiaochuan Wang</strong></p>
<p>As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯¹è¯å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸æ–­è¿›æ­¥ï¼Œå®ƒä»¬åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å®é™…åº”ç”¨å·²æˆä¸ºé‡è¦çš„ç ”ç©¶ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œåœ¨è¯¸å¦‚USMLEç­‰é™æ€åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½çš„åŒ»ç–—LLMï¼Œåœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠå†³ç­–åˆ¶å®šä¸­çš„å®ç”¨æ€§å´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¿™ç§å·®å¼‚çš„äº§ç”Ÿæ˜¯å› ä¸ºä¼ ç»Ÿè€ƒè¯•æ— æ³•æ•æ‰åˆ°åŒ»ç–—å’¨è¯¢çš„åŠ¨æ€äº¤äº’æ€§è´¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹åŠ¨æ€éªŒè¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è¶…è¶Šäº†é™æ€ç­”æ¡ˆéªŒè¯å™¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜ä¿çœŸåº¦çš„äº¤äº’å¼å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šä½¿ç”¨å»æ ‡è¯†åŒ»ç–—è®°å½•åˆ›å»ºç°å®ä¸´åºŠç¯å¢ƒçš„ç—…äººæ¨¡æ‹Ÿå™¨ï¼Œä»¥åŠåŠ¨æ€ç”Ÿæˆå¤šç»´åº¦è¯„ä»·æŒ‡æ ‡çš„ä¸´åºŠè§„åˆ™ç”Ÿæˆå™¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ç™½å·M2ï¼Œè¿™æ˜¯ä¸€ä¸ª32Bå‚æ•°çš„åŒ»ç–—å¢å¼ºæ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œå¹¶æ”¹è¿›äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ã€‚åœ¨HealthBenchä¸Šè¯„ä¼°ï¼Œç™½å·M2è¶…è¶Šäº†æ‰€æœ‰å…¶ä»–å¼€æºæ¨¡å‹å’Œæœ€å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„HealthBench HardåŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ†è¶…è¿‡32åˆ†ï¼ˆä¹‹å‰åªæœ‰GPT-5æ‰èƒ½è¾¾åˆ°ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œå¥å£®çš„åŠ¨æ€éªŒè¯ç³»ç»Ÿå¯¹äºå°†LLMèƒ½åŠ›ä¸å®é™…ä¸´åºŠåº”ç”¨ç›¸ç»“åˆè‡³å…³é‡è¦ï¼Œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½éƒ¨ç½²çš„æ€§èƒ½å‚æ•°æƒè¡¡å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02208v1">PDF</a> Baichuan-M2 Technical Report</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼šè¯å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¸æ–­è¿›æ­¥ï¼Œå®ƒä»¬åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å®é™…åº”ç”¨å·²æˆä¸ºå…³é”®ç ”ç©¶ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œå­˜åœ¨æ˜¾è‘—çš„å·®è·åœ¨äºLLMåœ¨é™æ€åŸºå‡†æµ‹è¯•ï¼ˆå¦‚USMLEï¼‰ä¸Šçš„è¡¨ç°ä¸åœ¨å®é™…ä¸´åºŠå†³ç­–ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°å‹åŠ¨æ€éªŒè¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬æ‚£è€…æ¨¡æ‹Ÿå™¨å’Œä¸´åºŠè§„åˆ™ç”Ÿæˆå™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜ä¿çœŸåº¦çš„äº¤äº’å¼å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå›¢é˜Ÿå¼€å‘äº†å‚æ•°ä¸º32Bçš„åŒ»ç–—æ¨ç†å¢å¼ºæ¨¡å‹Baichuan-M2ï¼Œè¯¥æ¨¡å‹åœ¨HealthBenchä¸Šçš„è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰å¼€æºæ¨¡å‹ä»¥åŠå¤§å¤šæ•°å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†ä¹‹å‰åªæœ‰GPT-5æ‰èƒ½è¶…è¶Šçš„å¥åº·é•¿æ¤…ç¡¬æ ‡å‡†åˆ†æ•°ä»¥ä¸Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå»ºç«‹å¯é çš„åŠ¨æ€éªŒè¯ç³»ç»Ÿå¯¹äºå°†LLMèƒ½åŠ›ä¸å®é™…ä¸´åºŠåº”ç”¨ç›¸ç»“åˆè‡³å…³é‡è¦ï¼Œä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½éƒ¨ç½²çš„æ€§èƒ½å‚æ•°æƒè¡¡å»ºç«‹äº†æ–°çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„å®é™…åº”ç”¨ä¸é™æ€åŸºå‡†æµ‹è¯•ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>ä¼ ç»Ÿè€ƒè¯•æ— æ³•æ•æ‰åŒ»ç–—å’¨è¯¢çš„åŠ¨æ€ã€äº¤äº’æ€§è´¨ã€‚</li>
<li>æ–°å‹åŠ¨æ€éªŒè¯æ¡†æ¶åŒ…æ‹¬æ‚£è€…æ¨¡æ‹Ÿå™¨å’Œä¸´åºŠè§„åˆ™ç”Ÿæˆå™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>åŠ¨æ€éªŒè¯æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜ä¿çœŸåº¦çš„äº¤äº’å¼å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿã€‚</li>
<li>Baichuan-M2æ¨¡å‹åœ¨HealthBenchä¸Šçš„è¡¨ç°è¶…è¶Šäº†å…¶ä»–æ¨¡å‹å’Œä¹‹å‰çš„æ ‡å‡†ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå»ºç«‹å¯é çš„åŠ¨æ€éªŒè¯ç³»ç»Ÿå¯¹äºLLMåœ¨ä¸´åºŠåº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02208v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02208v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02208v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02208v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Understanding-Space-Is-Rocket-Science-â€“-Only-Top-Reasoning-Models-Can-Solve-Spatial-Understanding-Tasks"><a href="#Understanding-Space-Is-Rocket-Science-â€“-Only-Top-Reasoning-Models-Can-Solve-Spatial-Understanding-Tasks" class="headerlink" title="Understanding Space Is Rocket Science â€“ Only Top Reasoning Models Can   Solve Spatial Understanding Tasks"></a>Understanding Space Is Rocket Science â€“ Only Top Reasoning Models Can   Solve Spatial Understanding Tasks</h2><p><strong>Authors:Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. Oâ€™Connor, Anthony Ventresque</strong></p>
<p>We propose RocketScience, an open-source contrastive VLM benchmark that tests for spatial relation understanding. It is comprised of entirely new real-world image-text pairs covering mostly relative spatial understanding and the order of objects. The benchmark is designed to be very easy for humans and hard for the current generation of VLMs, and this is empirically verified. Our results show a striking lack of spatial relation understanding in open source and frontier commercial VLMs and a surprisingly high performance of reasoning models. Additionally, we perform a disentanglement analysis to separate the contributions of object localization and spatial reasoning in chain-of-thought-based models and find that the performance on the benchmark is bottlenecked by spatial reasoning and not object localization capabilities. We release the dataset with a CC-BY-4.0 license and make the evaluation code available at: <a target="_blank" rel="noopener" href="https://github.com/nilshoehing/rocketscience">https://github.com/nilshoehing/rocketscience</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†RocketScienceï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å¯¹æ¯”å¼VLMåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹è¯•å¯¹ç©ºé—´å…³ç³»çš„ç†è§£ã€‚å®ƒç”±å…¨æ–°çš„ç°å®ä¸–ç•Œå›¾åƒæ–‡æœ¬å¯¹ç»„æˆï¼Œä¸»è¦æ¶µç›–ç›¸å¯¹ç©ºé—´ç†è§£å’Œç‰©ä½“é¡ºåºã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•å¯¹äººç±»æ¥è¯´å¾ˆå®¹æ˜“ï¼Œå¯¹å½“å‰çš„VLMæ¥è¯´å¾ˆéš¾ï¼Œè¿™å·²ç»å¾—åˆ°äº†å®è¯éªŒè¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å¼€æºå’Œå‰æ²¿çš„å•†ä¸šVLMä¸­ï¼Œå¯¹ç©ºé—´å…³ç³»çš„ç†è§£å­˜åœ¨æ˜æ˜¾çš„ç¼ºä¹ï¼Œè€Œæ¨ç†æ¨¡å‹çš„è¡¨ç°å´å‡ºäººæ„æ–™åœ°å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ‹†è§£åˆ†æï¼Œä»¥åŒºåˆ†åŸºäºæ€ç»´é“¾çš„æ¨¡å‹ä¸­å¯¹è±¡å®šä½å’Œç©ºé—´æ¨ç†çš„è´¡çŒ®ï¼Œå‘ç°è¯¥åŸºå‡†æµ‹è¯•çš„è¡¨ç°å—åˆ°ç©ºé—´æ¨ç†çš„åˆ¶çº¦ï¼Œè€Œä¸æ˜¯å¯¹è±¡å®šä½èƒ½åŠ›ã€‚æˆ‘ä»¬ä»¥CC-BY-4. 0è®¸å¯è¯å‘å¸ƒæ•°æ®é›†ï¼Œå¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/nilshoehing/rocketscience">https://github.com/nilshoehing/rocketscience</a>ä¸Šæä¾›è¯„ä¼°ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02175v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ä¸ªåä¸ºRocketScienceçš„å¼€æºå¯¹æ¯”è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹æµ‹è¯•ç©ºé—´å…³ç³»ç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å…¨æ–°çš„ç°å®å›¾åƒæ–‡æœ¬å¯¹ï¼Œä¸»è¦æ¶µç›–ç›¸å¯¹ç©ºé—´ç†è§£å’Œç‰©ä½“é¡ºåºã€‚äººç±»å®¹æ˜“å®Œæˆè€Œå½“å‰ä¸€ä»£è§†è§‰è¯­è¨€æ¨¡å‹éš¾ä»¥å®Œæˆï¼Œç»éªŒè¯ç¡®æ˜¯å¦‚æ­¤ã€‚ç»“æœæ˜¾ç¤ºå¼€æºå’Œå‰æ²¿å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å…³ç³»ç†è§£ä¸Šçš„æ˜¾è‘—ç¼ºå¤±ï¼Œä»¥åŠæ¨ç†æ¨¡å‹å‡ºäººæ„æ–™çš„è‰¯å¥½è¡¨ç°ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†ç¦»å¯¹è±¡å®šä½å’Œç©ºé—´æ¨ç†çš„è´¡çŒ®ï¼Œå‘ç°è¯¥åŸºå‡†æµ‹è¯•çš„ç“¶é¢ˆåœ¨äºç©ºé—´æ¨ç†èƒ½åŠ›è€Œä¸æ˜¯å¯¹è±¡å®šä½èƒ½åŠ›ã€‚å·²å‘å¸ƒæ•°æ®é›†å¹¶é™„å¸¦CC-BY-4.0è®¸å¯è¯ï¼Œè¯„ä¼°ä»£ç å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/nilshoehing/rocketscience">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RocketScienceæ˜¯ä¸€ä¸ªæµ‹è¯•è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´å…³ç³»ç†è§£èƒ½åŠ›çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ç°å®å›¾åƒæ–‡æœ¬å¯¹ï¼Œç€é‡äºç›¸å¯¹ç©ºé—´ç†è§£å’Œç‰©ä½“é¡ºåºã€‚</li>
<li>äººç±»å®¹æ˜“å®Œæˆæ­¤åŸºå‡†æµ‹è¯•ï¼Œè€Œå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹éš¾ä»¥å®Œæˆã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºå¼€æºå’Œå•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å…³ç³»ç†è§£ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚</li>
<li>æ¨ç†æ¨¡å‹è¡¨ç°æ„å¤–è‰¯å¥½ã€‚</li>
<li>åŸºå‡†æµ‹è¯•çš„ç“¶é¢ˆåœ¨äºç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œè€Œéå¯¹è±¡å®šä½èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02175v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02175v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02175v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02175v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Omnidirectional-Spatial-Modeling-from-Correlated-Panoramas"><a href="#Omnidirectional-Spatial-Modeling-from-Correlated-Panoramas" class="headerlink" title="Omnidirectional Spatial Modeling from Correlated Panoramas"></a>Omnidirectional Spatial Modeling from Correlated Panoramas</h2><p><strong>Authors:Xinshen Zhang, Tongxi Fu, Xu Zheng</strong></p>
<p>Omnidirectional scene understanding is vital for various downstream applications, such as embodied AI, autonomous driving, and immersive environments, yet remains challenging due to geometric distortion and complex spatial relations in 360{\deg} imagery. Existing omnidirectional methods achieve scene understanding within a single frame while neglecting cross-frame correlated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the \textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas visual question answering in the holistic 360{\deg} scenes. CFpano consists of over 2700 images together with over 8000 question-answer pairs, and the question types include both multiple choice and open-ended VQA. Building upon our CFpano, we further present \methodname, a multi-modal large language model (MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of tailored reward functions for robust and consistent reasoning with cross-frame correlated panoramas. Benchmark experiments with existing MLLMs are conducted with our CFpano. The experimental results demonstrate that \methodname achieves state-of-the-art performance across both multiple-choice and open-ended VQA tasks, outperforming strong baselines on all major reasoning categories (\textbf{+5.37%} in overall performance). Our analyses validate the effectiveness of GRPO and establish a new benchmark for panoramic scene understanding. </p>
<blockquote>
<p>å…¨æ™¯åœºæ™¯ç†è§£å¯¹äºå„ç§ä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚åµŒå…¥å¼äººå·¥æ™ºèƒ½ã€è‡ªåŠ¨é©¾é©¶å’Œæ²‰æµ¸å¼ç¯å¢ƒï¼‰è‡³å…³é‡è¦ï¼Œç„¶è€Œç”±äº360åº¦å›¾åƒä¸­çš„å‡ ä½•å¤±çœŸå’Œå¤æ‚çš„ç©ºé—´å…³ç³»ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„å…¨æ™¯æ–¹æ³•åœ¨åŒä¸€å¸§å†…å®ç°åœºæ™¯ç†è§£ï¼Œä½†å¿½ç•¥äº†è·¨å¸§ç›¸å…³å…¨æ™¯å›¾ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>CFpano</strong>ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹å…¨æ™¯åœºæ™¯ä¸­è·¨å¸§ç›¸å…³å…¨æ™¯å›¾çš„è§†è§‰é—®ç­”çš„åŸºå‡†æ•°æ®é›†ã€‚<strong>CFpano</strong>åŒ…å«è¶…è¿‡2700å¼ å›¾åƒå’Œè¶…è¿‡8000ä¸ªé—®ç­”å¯¹ï¼Œé—®é¢˜ç±»å‹åŒ…æ‹¬å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ã€‚åŸºäºæˆ‘ä»¬çš„CFpanoï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†\methodnameï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œä¸€ç³»åˆ—å®šåˆ¶çš„å¥–åŠ±å‡½æ•°è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°è·¨å¸§ç›¸å…³å…¨æ™¯å›¾çš„ç¨³å¥å’Œä¸€è‡´æ¨ç†ã€‚æˆ‘ä»¬åœ¨CFpanoåŸºå‡†æ•°æ®é›†ä¸Šä¸ç°æœ‰çš„MLLMè¿›è¡Œäº†å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ\methodnameåœ¨å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸­éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨æ‰€æœ‰ä¸»è¦æ¨ç†ç±»åˆ«ä¸Šéƒ½è¶…è¿‡äº†å¼ºå¤§çš„åŸºå‡†çº¿ï¼ˆæ€»ä½“æ€§èƒ½æé«˜\textbf{5.37%}ï¼‰ã€‚æˆ‘ä»¬çš„åˆ†æéªŒè¯äº†GRPOçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºå…¨æ™¯åœºæ™¯ç†è§£å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02164v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨æ™¯åœºæ™¯ç†è§£çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å‡ ä½•å¤±çœŸå’Œå¤æ‚çš„ç©ºé—´å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æ•°æ®é›†CFpanoï¼Œä¸“é—¨ç”¨äºå…¨æ™¯åœºæ™¯ä¸­çš„è·¨å¸§ç›¸å…³å…¨æ™¯è§†è§‰é—®ç­”ä»»åŠ¡ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡2700å¼ å›¾åƒå’Œè¶…è¿‡8000ä¸ªé—®ç­”å¯¹ï¼ŒåŒ…æ‹¬å¤šç§ç±»å‹çš„é—®é¢˜ã€‚åŸºäºCFpanoæ•°æ®é›†ï¼Œè¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå®šåˆ¶å¥–åŠ±å‡½æ•°è¿›è¡Œå¾®è°ƒï¼Œå®ç°è·¨å¸§ç›¸å…³å…¨æ™¯çš„ç¨³å¥å’Œä¸€è‡´æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜è§£ç­”ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œåœ¨ä¸»è¦æ¨ç†ç±»åˆ«ä¸Šä¼˜äºå¼ºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨æ™¯åœºæ™¯ç†è§£å¯¹äºå¤šç§ä¸‹æ¸¸åº”ç”¨å¦‚äººå·¥æ™ºèƒ½å®ä½“ã€è‡ªåŠ¨é©¾é©¶å’Œæ²‰æµ¸å¼ç¯å¢ƒè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•å¸§å†…çš„å…¨æ™¯åœºæ™¯ç†è§£ï¼Œå¿½ç•¥äº†è·¨å¸§ç›¸å…³å…¨æ™¯çš„å·®è·ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„åŸºå‡†æ•°æ®é›†CFpanoï¼Œä¸“é—¨ç”¨äºè·¨å¸§ç›¸å…³å…¨æ™¯è§†è§‰é—®ç­”ä»»åŠ¡ã€‚</li>
<li>CFpanoåŒ…å«å¤§é‡å›¾åƒå’Œé—®ç­”å¯¹ï¼Œé—®é¢˜ç±»å‹åŒ…æ‹¬å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œé‡‡ç”¨GRPOå’Œå®šåˆ¶å¥–åŠ±å‡½æ•°è¿›è¡Œè·¨å¸§ç›¸å…³å…¨æ™¯çš„ç¨³å¥æ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å…¨æ™¯åœºæ™¯ç†è§£ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºç°æœ‰å¼ºåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02164v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02164v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02164v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02164v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02164v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02164v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DeepSeek-performs-better-than-other-Large-Language-Models-in-Dental-Cases"><a href="#DeepSeek-performs-better-than-other-Large-Language-Models-in-Dental-Cases" class="headerlink" title="DeepSeek performs better than other Large Language Models in Dental   Cases"></a>DeepSeek performs better than other Large Language Models in Dental   Cases</h2><p><strong>Authors:Hexian Zhang, Xinyu Yan, Yanqi Yang, Lijian Jin, Ping Yang, Junwen Wang</strong></p>
<p>Large language models (LLMs) hold transformative potential in healthcare, yet their capacity to interpret longitudinal patient narratives remains inadequately explored. Dentistry, with its rich repository of structured clinical data, presents a unique opportunity to rigorously assess LLMsâ€™ reasoning abilities. While several commercial LLMs already exist, DeepSeek, a model that gained significant attention earlier this year, has also joined the competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini 2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal dental case vignettes through open-ended clinical tasks. Using 34 standardized longitudinal periodontal cases (comprising 258 question-answer pairs), we assessed model performance via automated metrics and blinded evaluations by licensed dentists. DeepSeek emerged as the top performer, demonstrating superior faithfulness (median score &#x3D; 0.528 vs. 0.367-0.457) and higher expert ratings (median &#x3D; 4.5&#x2F;5 vs. 4.0&#x2F;5), without significantly compromising readability. Our study positions DeepSeek as the leading LLM for case analysis, endorses its integration as an adjunct tool in both medical education and research, and highlights its potential as a domain-specific agent. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å˜é©æ€§æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬è§£é‡Šçºµå‘æ‚£è€…å™è¿°çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç‰™ç§‘æ‹¥æœ‰ä¸°å¯Œçš„ç»“æ„åŒ–ä¸´åºŠæ•°æ®ä»“åº“ï¼Œä¸ºä¸¥æ ¼è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†ç‹¬ç‰¹æœºä¼šã€‚è™½ç„¶å·²æœ‰ä¸€äº›å•†ä¸šLLMå­˜åœ¨ï¼Œä½†ä»Šå¹´æ—©äº›æ—¶å€™å¼•èµ·å¹¿æ³›å…³æ³¨çš„DeepSeekä¹ŸåŠ å…¥äº†ç«äº‰ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å››ç§æœ€å…ˆè¿›çš„LLMï¼ˆGPT-4oã€Gemini 2.0 Flashã€Copilotå’ŒDeepSeek V3ï¼‰åœ¨åˆ†æçºµå‘ç‰™ç§‘ç—…ä¾‹æ‘˜è¦çš„èƒ½åŠ›ï¼Œé€šè¿‡å¼€æ”¾å¼ä¸´åºŠä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨34ä¸ªæ ‡å‡†åŒ–çš„çºµå‘ç‰™å‘¨ç—…ä¾‹ï¼ˆåŒ…å«258ä¸ªé—®ç­”å¯¹ï¼‰ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œç›²è¯„ç‰™åŒ»è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚DeepSeekè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œåœ¨å¿ å®åº¦æ–¹é¢å°¤ä¸ºå‡ºè‰²ï¼ˆä¸­ä½æ•°å¾—åˆ†&#x3D;0.528 vs. 0.367-0.457ï¼‰ï¼Œå¹¶è·å¾—æ›´é«˜çš„ä¸“å®¶è¯„åˆ†ï¼ˆä¸­ä½æ•°&#x3D; 4.5&#x2F;5 vs. 4.0&#x2F;5ï¼‰ï¼ŒåŒæ—¶ä¸æŸå®³å¯è¯»æ€§ã€‚æœ¬ç ”ç©¶å°†DeepSeekå®šä½ä¸ºæ¡ˆä¾‹åˆ†æçš„é¢†å…ˆLLMï¼Œæ”¯æŒå°†å…¶é›†æˆåˆ°åŒ»å­¦æ•™è‚²å’Œç ”ç©¶ä¸­ä½œä¸ºè¾…åŠ©å·¥å…·ï¼Œå¹¶çªå‡ºäº†å…¶ä½œä¸ºé¢†åŸŸç‰¹å®šä»£ç†çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02036v1">PDF</a> Abstract word count: 171; Total word count: 3130; Total number of   tables: 2; Total number of figures: 3; Number of references: 32</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸå…·æœ‰å˜é©æ€§æ½œåŠ›ï¼Œä½†åœ¨è§£è¯»çºµå‘æ‚£è€…å™è¿°æ–¹é¢å°šå¾…å……åˆ†æ¢ç´¢ã€‚ç‰™ç§‘ä¸°å¯Œçš„ç»“æ„åŒ–ä¸´åºŠæ•°æ®ä¸ºä¸¥æ ¼è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›æä¾›äº†ç‹¬ç‰¹æœºä¼šã€‚åœ¨è¯„ä¼°äº†å››ç§æœ€å…ˆè¿›çš„LLMsåï¼ŒDeepSeekè¡¨ç°å‡ºå“è¶Šçš„åˆ†æçºµå‘ç‰™ç§‘æ¡ˆä¾‹çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹æ€§èƒ½çš„è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œç‰™åŒ»çš„ç›²è¯„ä¸­å‡è¡¨ç°æœ€ä½³ã€‚å®ƒè¢«è§†ä¸ºé¢†å…ˆçš„LLMæ¡ˆä¾‹åˆ†æå·¥å…·ï¼Œå¯æ•´åˆåˆ°åŒ»å­¦æ•™è‚²å’Œç ”ç©¶ä¸­ä½œä¸ºè¾…åŠ©å·¥å…·ï¼Œå¹¶å±•ç°å‡ºä½œä¸ºç‰¹å®šé¢†åŸŸä»£ç†çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„æ½œåŠ›å°šæœªå®Œå…¨å‘æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£æçºµå‘æ‚£è€…å™è¿°æ–¹é¢ã€‚</li>
<li>ç‰™ç§‘æ•°æ®çš„ç»“æ„åŒ–ç‰¹ç‚¹ä½¿å…¶æˆä¸ºè¯„ä¼°LLMsæ¨ç†èƒ½åŠ›çš„ç†æƒ³åœºæ‰€ã€‚</li>
<li>è¯„ä¼°äº†å››ç§å…ˆè¿›çš„LLMsï¼ˆGPT-4oã€Gemini 2.0 Flashã€Copilotå’ŒDeepSeek V3ï¼‰åœ¨ç‰™ç§‘æ¡ˆä¾‹ä¸Šçš„è¡¨ç°ã€‚</li>
<li>DeepSeekåœ¨åˆ†æçºµå‘ç‰™ç§‘æ¡ˆä¾‹æ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>DeepSeekåœ¨è‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œç‰™åŒ»ç›²è¯„ä¸­çš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>DeepSeekè¢«è§†ä¸ºé¢†å…ˆçš„LLMæ¡ˆä¾‹åˆ†æå·¥å…·ï¼Œé€‚åˆä½œä¸ºåŒ»å­¦æ•™è‚²å’Œç ”ç©¶çš„è¾…åŠ©å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.02036v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AutoDrive-R-2-Incentivizing-Reasoning-and-Self-Reflection-Capacity-for-VLA-Model-in-Autonomous-Driving"><a href="#AutoDrive-R-2-Incentivizing-Reasoning-and-Self-Reflection-Capacity-for-VLA-Model-in-Autonomous-Driving" class="headerlink" title="AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity   for VLA Model in Autonomous Driving"></a>AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity   for VLA Model in Autonomous Driving</h2><p><strong>Authors:Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengxuan Qian, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng Zhang, Shuo Li</strong></p>
<p>Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡æ•´åˆå¤šæ¨¡å¼æ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›ï¼Œæœ€è¿‘å±•ç¤ºäº†å˜é©æ½œåŠ›ã€‚ç„¶è€Œï¼Œå†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§å’Œè¿è´¯æ€§ä»¥åŠåŠ¨ä½œåºåˆ—çš„åˆç†æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AutoDrive-R$^2$ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„VLAæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰å¤„ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ¨ç†å’Œè‡ªåæ€èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åˆ›æ–°æ€§çš„CoTæ•°æ®é›†ï¼Œåä¸ºnuScenesR$^2$-6Kï¼Œç”¨äºç›‘ç£å¾®è°ƒï¼Œå®ƒé€šè¿‡å››æ­¥é€»è¾‘é“¾æœ‰æ•ˆåœ°åœ¨è¾“å…¥ä¿¡æ¯å’Œè¾“å‡ºè½¨è¿¹ä¹‹é—´å»ºç«‹è®¤çŸ¥æ¡¥æ¢ï¼Œå¹¶è¿›è¡Œè‡ªæˆ‘åæ€ä»¥è¿›è¡ŒéªŒè¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†åœ¨RLé˜¶æ®µæœ€å¤§åŒ–æ¨ç†å’Œè‡ªåæ€ï¼Œæˆ‘ä»¬åœ¨åŸºäºç‰©ç†çš„å¥–åŠ±æ¡†æ¶å†…è¿›ä¸€æ­¥é‡‡ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´å¯¹é½ã€è½¦è¾†åŠ¨æ€å’Œæ—¶é—´å¹³æ»‘æ ‡å‡†ï¼Œä»¥ç¡®ä¿å¯é çš„ç°å®è½¨è¿¹è§„åˆ’ã€‚åœ¨nuSceneså’ŒWaymoæ•°æ®é›†çš„å¹¿æ³›è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01944v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ï¼ŒVision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹é€šè¿‡æ•´åˆå¤šæ¨¡å¼æ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›ï¼Œå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§å’Œè¿è´¯æ€§ä»¥åŠè¡ŒåŠ¨åºåˆ—çš„åˆç†æ€§ä»å­˜åœ¨è¾ƒå¤§çš„æ¢ç´¢ç©ºé—´ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AutoDrive-R$^2$è¿™ä¸€æ–°å‹VLAæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰å¤„ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ¨ç†å’Œåæ€èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ›æ–°æ€§åœ°æå‡ºäº†nuScenesR$^2$-6Kæ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒï¼Œæœ‰æ•ˆåœ°åœ¨è¾“å…¥ä¿¡æ¯å’Œè¾“å‡ºè½¨è¿¹ä¹‹é—´å»ºç«‹è®¤çŸ¥æ¡¥æ¢ã€‚åŒæ—¶ï¼Œåœ¨RLé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•ï¼Œåœ¨ç‰©ç†å¥–åŠ±æ¡†æ¶å†…èå…¥ç©ºé—´å¯¹é½ã€è½¦è¾†åŠ¨æ€å’Œæ—¶åºå¹³æ»‘æ ‡å‡†ï¼Œç¡®ä¿å¯é çš„è½¨è¿¹è§„åˆ’ã€‚åœ¨nuSceneså’ŒWaymoæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„å“è¶Šæ€§èƒ½å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡å¼æ„ŸçŸ¥ä¸å†³ç­–èƒ½åŠ›æ¨åŠ¨è‡ªä¸»é©¾é©¶çš„å‘å±•ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­ï¼Œå†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§å’Œè¿è´¯æ€§ä»¥åŠè¡ŒåŠ¨åºåˆ—çš„åˆç†æ€§æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†AutoDrive-R$^2$æ¡†æ¶ï¼Œç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰å¤„ç†å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå¢å¼ºäº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ¨ç†å’Œåæ€èƒ½åŠ›ã€‚</li>
<li>åˆ›æ–°æ€§åœ°æ¨å‡ºnuScenesR$^2$-6Kæ•°æ®é›†ç”¨äºç›‘ç£å¾®è°ƒï¼Œæå‡è®¤çŸ¥æ¡¥æ¢çš„å»ºç«‹ã€‚</li>
<li>åœ¨RLé˜¶æ®µé‡‡ç”¨GRPOç®—æ³•ï¼Œç»“åˆç‰©ç†å¥–åŠ±æ¡†æ¶ï¼Œç¡®ä¿å¯é çš„è½¨è¿¹è§„åˆ’ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨nuSceneså’ŒWaymoæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å†³ç­–è¿‡ç¨‹æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01944v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01944v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Speech-Recognition"><a href="#Group-Relative-Policy-Optimization-for-Speech-Recognition" class="headerlink" title="Group Relative Policy Optimization for Speech Recognition"></a>Group Relative Policy Optimization for Speech Recognition</h2><p><strong>Authors:Prashanth Gurunath Shivakumar, Yile Gu, Ankur Gandhe, Ivan Bulyko</strong></p>
<p>Speech Recognition has seen a dramatic shift towards adopting Large Language Models (LLMs). This shift is partly driven by good scalability properties demonstrated by LLMs, ability to leverage large amounts of labelled, unlabelled speech and text data, streaming capabilities with auto-regressive framework and multi-tasking with instruction following characteristics of LLMs. However, simple next-token prediction objective, typically employed with LLMs, have certain limitations in performance and challenges with hallucinations. In this paper, we propose application of Group Relative Policy Optimization (GRPO) to enable reinforcement learning from human feedback for automatic speech recognition (ASR). We design simple rule based reward functions to guide the policy updates. We demonstrate significant improvements in word error rate (upto 18.4% relative), reduction in hallucinations, increased robustness on out-of-domain datasets and effectiveness in domain adaptation. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«åœ¨é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å‘ç”Ÿäº†å·¨å¤§è½¬å˜ã€‚è¿™ä¸€è½¬å˜éƒ¨åˆ†æ˜¯ç”±äºLLMæ‰€å±•ç°çš„è‰¯å¥½å¯æ‰©å±•æ€§ã€èƒ½å¤Ÿåˆ©ç”¨å¤§é‡æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„è¯­éŸ³å’Œæ–‡æœ¬æ•°æ®ã€å…·æœ‰è‡ªå›å½’æ¡†æ¶çš„æµå¼å¤„ç†èƒ½åŠ›ä»¥åŠå…·æœ‰æŒ‡ä»¤éµå¾ªç‰¹æ€§çš„å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›æ‰€é©±åŠ¨çš„ã€‚ç„¶è€Œï¼Œé€šå¸¸ä¸LLMä¸€èµ·ä½¿ç”¨çš„ç®€å•ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡åœ¨æ€§èƒ½å’Œå¹»è§‰æ–¹é¢å…·æœ‰ä¸€å®šçš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å°†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åº”ç”¨äºè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œä»¥ä»äººç±»åé¦ˆä¸­è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è®¾è®¡äº†åŸºäºç®€å•è§„åˆ™çš„å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼ç­–ç•¥æ›´æ–°ã€‚æˆ‘ä»¬åœ¨è¯é”™è¯¯ç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ˆæœ€å¤šç›¸å¯¹é™ä½18.4%ï¼‰ã€å¹»è§‰å‡å°‘ã€å¯¹åŸŸå¤–æ•°æ®é›†çš„é²æ£’æ€§å¢å¼ºä»¥åŠåŸŸé€‚åº”çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01939v1">PDF</a> Accepted for ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„åº”ç”¨å·²å¼•èµ·æ˜¾è‘—è½¬å˜ã€‚é€šè¿‡åˆ©ç”¨å¤§é‡çš„æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„è¯­éŸ³å’Œæ–‡æœ¬æ•°æ®ã€æµå¼å¤„ç†çš„è‡ªåŠ¨å›å½’æ¡†æ¶ä»¥åŠå¤šä»»åŠ¡æŒ‡ä»¤éµå¾ªç‰¹æ€§ï¼ŒLLMså±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚ç„¶è€Œï¼Œç®€å•é‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡ä¹Ÿå­˜åœ¨æ€§èƒ½ä¸Šçš„å±€é™å’ŒæŒ‘æˆ˜ï¼Œå¦‚å¹»è§†é—®é¢˜ã€‚æœ¬æ–‡æå‡ºåº”ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•ï¼Œé€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥å®ç°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚è®¾è®¡åŸºäºç®€å•è§„åˆ™çš„å¥–åŠ±åŠŸèƒ½æ¥å¼•å¯¼ç­–ç•¥æ›´æ–°ã€‚åœ¨è¯é”™è¯¯ç‡æ–¹é¢å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ˆç›¸å¯¹é™ä½18.4%ï¼‰ã€å¹»è§†å‡å°‘ã€å¯¹åŸŸå¤–æ•°æ®é›†çš„é²æ£’æ€§å¢å¼ºä»¥åŠåŸŸé€‚åº”çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³è¯†åˆ«ä¸­å±•ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¾—ç›Šäºå…¶åˆ©ç”¨å¤§é‡è¯­éŸ³å’Œæ–‡æœ¬æ•°æ®çš„èƒ½åŠ›ã€æµå¼å¤„ç†çš„è‡ªåŠ¨å›å½’æ¡†æ¶ä»¥åŠå¤šä»»åŠ¡æŒ‡ä»¤éµå¾ªç‰¹æ€§ã€‚</li>
<li>ç®€å•é‡‡ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ç›®æ ‡åœ¨è¯­éŸ³è¯†åˆ«ä¸­å­˜åœ¨æ€§èƒ½å±€é™å’ŒæŒ‘æˆ˜ï¼Œå¦‚æ€§èƒ½ä¸è¶³å’Œå¹»è§†é—®é¢˜ã€‚</li>
<li>Group Relative Policy Optimization (GRPO)è¢«åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œä»¥é€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>é€šè¿‡è®¾è®¡åŸºäºç®€å•è§„åˆ™çš„å¥–åŠ±åŠŸèƒ½ï¼ŒGRPOèƒ½å¤Ÿæ˜¾è‘—æ”¹è¿›è¯é”™è¯¯ç‡ï¼Œç›¸å¯¹é™ä½è¾¾18.4%ã€‚</li>
<li>GRPOåœ¨å‡å°‘å¹»è§†ã€å¢å¼ºå¯¹åŸŸå¤–æ•°æ®é›†çš„é²æ£’æ€§ä»¥åŠæé«˜åŸŸé€‚åº”æœ‰æ•ˆæ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ–¹æ³•çš„åº”ç”¨ä¸ºè¯­éŸ³è¯†åˆ«é¢†åŸŸå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œæœ‰æœ›è§£å†³å½“å‰çš„ä¸€äº›æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01939v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01939v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01939v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01939v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01939v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Kwai-Keye-VL-1-5-Technical-Report"><a href="#Kwai-Keye-VL-1-5-Technical-Report" class="headerlink" title="Kwai Keye-VL 1.5 Technical Report"></a>Kwai Keye-VL 1.5 Technical Report</h2><p><strong>Authors:Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang</strong></p>
<p>In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the modelâ€™s context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰©å±•äº†å…¶å¤šæ¨¡æ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè§†é¢‘çš„åŠ¨æ€å’Œä¿¡æ¯å¯†é›†ç‰¹æ€§ï¼Œè§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è§†é¢‘å†…å®¹æ—¶ï¼Œåœ¨ç©ºé—´åˆ†è¾¨ç‡å’Œæ—¶é—´è¦†ç›–ä¹‹é—´éš¾ä»¥å–èˆã€‚æˆ‘ä»¬æ¨å‡ºäº†Keye-VL-1.5ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°è§£å†³äº†è§†é¢‘ç†è§£ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¿«æ…¢è§†é¢‘ç¼–ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®å¸§é—´ç›¸ä¼¼æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œä»¥é«˜åˆ†è¾¨ç‡å¤„ç†å…·æœ‰é‡å¤§è§†è§‰å˜åŒ–çš„å…³é”®å¸§ï¼ˆæ…¢è·¯å¾„ï¼‰ï¼Œå¹¶ä»¥è¾ƒä½åˆ†è¾¨ç‡å¤„ç†ç›¸å¯¹é™æ€å¸§ï¼Œå¢åŠ æ—¶é—´è¦†ç›–ï¼ˆå¿«è·¯å¾„ï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å®æ–½äº†åˆ†é˜¶æ®µçš„å››é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç³»ç»Ÿåœ°æ‰©å±•äº†æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»8Kæ‰©å±•åˆ°128Kä»¤ç‰Œï¼Œèƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘å’Œæ›´å¤æ‚çš„è§†è§‰å†…å®¹ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„åè®­ç»ƒç®¡é“ï¼Œä¸“æ³¨äºæ¨ç†å¢å¼ºå’Œäººç±»åå¥½å¯¹é½ï¼ŒåŒ…æ‹¬ä¸€ä¸ª5æ­¥çš„æ€ç»´é“¾æ•°æ®æ„å»ºè¿‡ç¨‹ã€åŸºäºè¿­ä»£GSPOçš„å¼ºåŒ–å­¦ä¹ ä»¥åŠé’ˆå¯¹å›°éš¾æƒ…å†µçš„æ¸è¿›æç¤ºæç¤ºå’Œæ ¡å‡†è®­ç»ƒã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå¹¿æ³›è¯„ä¼°å’Œå†…éƒ¨äººç±»ä¸¥æ ¼è¯„ä¼°ä¸­ï¼ŒKeye-VL-1.5ç›¸è¾ƒäºç°æœ‰æ¨¡å‹è¡¨ç°å‡ºäº†æ˜¾è‘—æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01563v2">PDF</a> Github page: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Keye/Keye">https://github.com/Kwai-Keye/Keye</a></p>
<p><strong>Summary</strong><br>è§†é¢‘ç†è§£ä»æ˜¯å› è§†é¢‘çš„åŠ¨æ€å’Œä¿¡æ¯å¯†é›†ç‰¹æ€§è€Œå…·æœ‰æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è§†é¢‘å†…å®¹æ—¶ï¼Œåœ¨åˆ†è¾¨ç‡å’Œæ—¶é—´è¦†ç›–ä¹‹é—´æƒè¡¡å›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºKeye-VL-1.5ï¼Œé€šè¿‡ä¸‰ç§åˆ›æ–°è§£å†³è§†é¢‘ç†è§£çš„æ ¹æœ¬æŒ‘æˆ˜ï¼šå¼•å…¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºçš„å¿«æ…¢è§†é¢‘ç¼–ç ç­–ç•¥ï¼›å®æ–½åˆ†é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿æ¨¡å‹å¤„ç†æ›´é•¿çš„è§†é¢‘å’Œæ›´å¤æ‚çš„è§†è§‰å†…å®¹ï¼›å¼€å‘ä¸“æ³¨äºæ¨ç†å¢å¼ºå’Œäººç±»åå¥½å¯¹é½çš„åæœŸè®­ç»ƒç®¡é“ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•å’Œå†…éƒ¨äººç±»è¯„ä¼°ä¸­ï¼ŒKeye-VL-1.5è¾ƒç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå°¤å…¶åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Language Models (LLMs) å·²æ‰©å±•åˆ°å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä½†è§†é¢‘ç†è§£ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è§†é¢‘æ—¶åœ¨ç©ºé—´åˆ†è¾¨ç‡å’Œæ—¶é—´è¦†ç›–ä¹‹é—´å­˜åœ¨æƒè¡¡å›°éš¾ã€‚</li>
<li>Keye-VL-1.5 é€šè¿‡å¿«æ…¢è§†é¢‘ç¼–ç ç­–ç•¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>Keye-VL-1.5 å®æ–½åˆ†é˜¶æ®µé¢„è®­ç»ƒæ–¹æ³•ï¼Œä½¿æ¨¡å‹èƒ½å¤„ç†æ›´é•¿çš„è§†é¢‘å’Œå¤æ‚çš„è§†è§‰å†…å®¹ã€‚</li>
<li>Keye-VL-1.5 å¼€å‘åæœŸè®­ç»ƒç®¡é“ï¼Œä¸“æ³¨äºæ¨ç†å¢å¼ºå’Œäººç±»åå¥½å¯¹é½ã€‚</li>
<li>Keye-VL-1.5 åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ï¼ŒåŒæ—¶åœ¨é€šç”¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ä¿æŒç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01563v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01563v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01563v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-High-Data-Efficiency-in-Reinforcement-Learning-with-Verifiable-Reward"><a href="#Towards-High-Data-Efficiency-in-Reinforcement-Learning-with-Verifiable-Reward" class="headerlink" title="Towards High Data Efficiency in Reinforcement Learning with Verifiable   Reward"></a>Towards High Data Efficiency in Reinforcement Learning with Verifiable   Reward</h2><p><strong>Authors:Xinyu Tang, Zhenduo Zhang, Yurou Liu, Wayne Xin Zhao, Zujie Wen, Zhiqiang Zhang, Jun Zhou</strong></p>
<p>Recent advances in large reasoning models have leveraged reinforcement learning with verifiable rewards (RLVR) to improve reasoning capabilities. However, scaling these methods typically requires extensive rollout computation and large datasets, leading to high training costs and low data efficiency. To mitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization pipeline that combines optimized strategies for both offline and online data selection. In the offline phase, we curate a high-quality subset of training samples based on diversity, influence, and appropriate difficulty. During online RLVR training, we introduce a sample-level explorability metric to dynamically filter samples with low exploration potential, thereby reducing substantial rollout computational costs. Furthermore, we incorporate a replay mechanism for under-explored samples to ensure adequate training, which enhances the modelâ€™s final convergence performance. Experiments across five reasoning benchmarks show that DEPO consistently outperforms existing methods in both offline and online data selection scenarios. Notably, using only 20% of the training data, our approach achieves a 1.85 times speed-up on AIME24 and a 1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹è¿›å±•åˆ©ç”¨äº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ‰©å±•è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ»šåŠ¨è®¡ç®—å’Œå¤§å‹æ•°æ®é›†ï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜å’Œæ•°æ®æ•ˆç‡ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DEPOï¼Œä¸€ä¸ªæ•°æ®é«˜æ•ˆç­–ç•¥ä¼˜åŒ–ç®¡é“ï¼Œç»“åˆäº†ç¦»çº¿æ•°æ®å’Œåœ¨çº¿æ•°æ®é€‰æ‹©çš„ä¼˜åŒ–ç­–ç•¥ã€‚åœ¨ç¦»çº¿é˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®å¤šæ ·æ€§ã€å½±å“åŠ›å’Œé€‚å½“çš„éš¾åº¦é€‰æ‹©åŸ¹è®­æ ·æœ¬çš„é«˜è´¨é‡å­é›†ã€‚åœ¨åœ¨çº¿RLVRè®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ ·æœ¬çº§åˆ«çš„æ¢ç´¢æ€§æŒ‡æ ‡ï¼Œä»¥åŠ¨æ€è¿‡æ»¤å…·æœ‰ä½æ¢ç´¢æ½œåŠ›çš„æ ·æœ¬ï¼Œä»è€Œå¤§å¤§å‡å°‘æ»šåŠ¨è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºæ¢ç´¢ä¸è¶³çš„æ ·æœ¬åŠ å…¥äº†é‡æ’­æœºåˆ¶ï¼Œä»¥ç¡®ä¿è¶³å¤Ÿçš„è®­ç»ƒï¼Œè¿™æé«˜äº†æ¨¡å‹çš„æœ€ç»ˆæ”¶æ•›æ€§èƒ½ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOåœ¨ç¦»çº¿æ•°æ®é€‰æ‹©å’Œåœ¨çº¿æ•°æ®é€‰æ‹©åœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AIME24ä¸Šå®ç°äº†ç›¸å¯¹äºåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„GRPOåŠ é€Ÿ1.85å€çš„é€Ÿåº¦æå‡ï¼Œå¹¶åœ¨AIME25ä¸Šå®ç°äº†åŠ é€Ÿ1.66å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01321v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹æ¨ç†é¢†åŸŸçš„æœ€æ–°è¿›å±•é€šè¿‡é‡‡ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ‰©å±•æ—¶é€šå¸¸éœ€è¦å¤§é‡çš„æ»šåŠ¨è®¡ç®—å’Œå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜æ˜‚ä¸”æ•°æ®æ•ˆç‡ä½ä¸‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DEPOè¿™ä¸€æ•°æ®é«˜æ•ˆç­–ç•¥ä¼˜åŒ–ç®¡é“ï¼Œç»“åˆäº†ç¦»çº¿ä¸åœ¨çº¿æ•°æ®é€‰æ‹©çš„ä¼˜åŒ–ç­–ç•¥ã€‚ç¦»çº¿é˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®å¤šæ ·æ€§ã€å½±å“åŠ›å’Œé€‚å½“éš¾åº¦é€‰æ‹©é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬å­é›†ã€‚åœ¨çº¿RLVRè®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å¼•å…¥æ ·æœ¬çº§æ¢ç´¢æ€§æŒ‡æ ‡ï¼ŒåŠ¨æ€è¿‡æ»¤ä½æ¢ç´¢æ½œåŠ›çš„æ ·æœ¬ï¼Œå¤§å¤§é™ä½äº†æ»šåŠ¨è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çº³å…¥äº†ä¸€ç§å›æ”¾æœºåˆ¶ï¼Œç”¨äºç¡®ä¿å¯¹æœªå……åˆ†æ¢ç´¢çš„æ ·æœ¬è¿›è¡Œå……è¶³è®­ç»ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„æœ€ç»ˆæ”¶æ•›æ€§èƒ½ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDEPOåœ¨ç¦»çº¿ä¸åœ¨çº¿æ•°æ®é€‰æ‹©åœºæ™¯ä¸­å‡è¾ƒç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ç§€ã€‚ä»…ä½¿ç”¨20%çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AIME24ä¸Šçš„é€Ÿåº¦æé«˜äº†1.85å€ï¼Œåœ¨AIME25ä¸Šçš„é€Ÿåº¦æé«˜äº†1.66å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°è¿›å±•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡å¤§æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œæ•°æ®æ•ˆç‡ä½çš„é—®é¢˜ã€‚</li>
<li>DEPOæ˜¯ä¸€ä¸ªæ•°æ®é«˜æ•ˆç­–ç•¥ä¼˜åŒ–ç®¡é“ï¼Œç»“åˆäº†ç¦»çº¿ä¸åœ¨çº¿æ•°æ®é€‰æ‹©çš„ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>ç¦»çº¿é˜¶æ®µé€‰æ‹©é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬å­é›†æ˜¯åŸºäºå¤šæ ·æ€§ã€å½±å“åŠ›å’Œé€‚å½“éš¾åº¦ã€‚</li>
<li>åœ¨çº¿è®­ç»ƒæœŸé—´å¼•å…¥æ ·æœ¬çº§æ¢ç´¢æ€§æŒ‡æ ‡ä»¥åŠ¨æ€è¿‡æ»¤ä½æ½œåŠ›æ ·æœ¬å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>DEPOå¼•å…¥å›æ”¾æœºåˆ¶ç¡®ä¿å¯¹æœªå……åˆ†æ¢ç´¢çš„æ ·æœ¬è¿›è¡Œå……è¶³è®­ç»ƒï¼Œæé«˜æ¨¡å‹æ”¶æ•›æ€§èƒ½ã€‚</li>
<li>åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDEPOè¾ƒç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01321v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01321v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Large-Language-Model-for-Knowledge-Graph-Completion-via-Structure-Aware-Alignment-Tuning"><a href="#Enhancing-Large-Language-Model-for-Knowledge-Graph-Completion-via-Structure-Aware-Alignment-Tuning" class="headerlink" title="Enhancing Large Language Model for Knowledge Graph Completion via   Structure-Aware Alignment-Tuning"></a>Enhancing Large Language Model for Knowledge Graph Completion via   Structure-Aware Alignment-Tuning</h2><p><strong>Authors:Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan</strong></p>
<p>Knowledge graph completion (KGC) aims to infer new knowledge and make predictions from knowledge graphs. Recently, large language models (LLMs) have exhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily focus on designing task-specific instructions, achieving promising advancements. However, there are still two critical challenges. First, existing methods often ignore the inconsistent representation spaces between natural language and graph structures. Second, most approaches design separate instructions for different KGC tasks, leading to duplicate works and time-consuming processes. To address these challenges, we propose SAT, a novel framework that enhances LLMs for KGC via structure-aware alignment-tuning. Specifically, we first introduce hierarchical knowledge alignment to align graph embeddings with the natural language space through multi-task contrastive learning. Then, we propose structural instruction tuning to guide LLMs in performing structure-aware reasoning over KGs, using a unified graph instruction combined with a lightweight knowledge adapter. Experimental results on two KGC tasks across four benchmark datasets demonstrate that SAT significantly outperforms state-of-the-art methods, especially in the link prediction task with improvements ranging from 8.7% to 29.8%. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨ä»çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†å¹¶è¿›è¡Œé¢„æµ‹ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºäº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚LLMå¢å¼ºçš„KGCæ–¹æ³•ä¸»è¦ä¾§é‡äºè®¾è®¡ç‰¹å®šä»»åŠ¡æŒ‡ä»¤ï¼Œå¹¶å–å¾—äº†æœ‰å¸Œæœ›çš„è¿›å±•ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è‡ªç„¶è¯­è¨€ä¸å›¾å½¢ç»“æ„ä¹‹é—´ä¸ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ã€‚å…¶æ¬¡ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¸ºä¸åŒçš„KGCä»»åŠ¡è®¾è®¡å•ç‹¬çš„æŒ‡ä»¤ï¼Œå¯¼è‡´å·¥ä½œé‡å¤å’Œè€—æ—¶è¿‡ç¨‹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SATï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½è°ƒæ•´å¢å¼ºLLMçš„æ–°å‹KGCæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥åˆ†å±‚çŸ¥è¯†å¯¹é½ï¼Œé€šè¿‡å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å°†å›¾åµŒå…¥ä¸è‡ªç„¶è¯­è¨€ç©ºé—´å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºç»“æ„æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼LLMåœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œç»“æ„æ„ŸçŸ¥æ¨ç†ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å›¾å½¢æŒ‡ä»¤å’Œè½»é‡çº§çŸ¥è¯†é€‚é…å™¨ã€‚åœ¨ä¸¤ä¸ªKGCä»»åŠ¡ã€å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSATæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­çš„æ”¹è¿›èŒƒå›´ä»8.7%åˆ°29.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01166v1">PDF</a> EMNLP 2025, Main, Long Paper</p>
<p><strong>Summary</strong><br>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ—¨åœ¨ä»çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†å¹¶è¿›è¡Œé¢„æµ‹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼ŒLLMå¢å¼ºçš„KGCæ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡ç‰¹å®šä»»åŠ¡æŒ‡ä»¤ï¼Œå¹¶å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¸€æ˜¯å¿½è§†è‡ªç„¶è¯­è¨€ä¸å›¾å½¢ç»“æ„ä¹‹é—´ä¸ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ï¼›äºŒæ˜¯ä¸ºä¸åŒçš„KGCä»»åŠ¡è®¾è®¡å•ç‹¬çš„æŒ‡ä»¤ï¼Œå¯¼è‡´é‡å¤å·¥ä½œå’Œè€—æ—¶è¿‡ç¨‹ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºSATæ¡†æ¶ï¼Œé€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½è°ƒæ•´å¢å¼ºLLMsåœ¨KGCä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSATåœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ä¸¤ä¸ªKGCä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå°¤å…¶åœ¨é“¾æ¥é¢„æµ‹ä»»åŠ¡ä¸­çš„æ”¹è¿›èŒƒå›´ä»8.7%åˆ°29.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰çš„ç›®æ ‡æ˜¯ä»çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­æ–°çŸ¥è¯†å’Œè¿›è¡Œé¢„æµ‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨KGCä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LLMå¢å¼ºçš„KGCæ–¹æ³•é€šè¿‡è®¾è®¡ç‰¹å®šä»»åŠ¡æŒ‡ä»¤å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥è‡ªç„¶è¯­è¨€ä¸å›¾å½¢ç»“æ„ä¹‹é—´ä¸ä¸€è‡´çš„è¡¨ç¤ºç©ºé—´ã€‚</li>
<li>å¤§å¤šæ•°æ–¹æ³•ä¸ºä¸åŒçš„KGCä»»åŠ¡è®¾è®¡å•ç‹¬æŒ‡ä»¤ï¼Œå¯¼è‡´é‡å¤å·¥ä½œå’Œè€—æ—¶ã€‚</li>
<li>SATæ¡†æ¶é€šè¿‡ç»“æ„æ„ŸçŸ¥å¯¹é½è°ƒæ•´å¢å¼ºLLMsåœ¨KGCä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01166v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01166v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01166v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01166v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01166v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01166v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Self-Exploring-Language-Models-for-Explainable-Link-Forecasting-on-Temporal-Graphs-via-Reinforcement-Learning"><a href="#Self-Exploring-Language-Models-for-Explainable-Link-Forecasting-on-Temporal-Graphs-via-Reinforcement-Learning" class="headerlink" title="Self-Exploring Language Models for Explainable Link Forecasting on   Temporal Graphs via Reinforcement Learning"></a>Self-Exploring Language Models for Explainable Link Forecasting on   Temporal Graphs via Reinforcement Learning</h2><p><strong>Authors:Zifeng Ding, Shenyang Huang, Zeyu Cao, Emma Kondrup, Zachary Yang, Xingyue Huang, Yuan Sui, Zhangdie Yuan, Yuqicheng Zhu, Xianglong Hu, Yuan He, Farimah Poursafaei, Michael Bronstein, Andreas Vlachos</strong></p>
<p>Forecasting future links is a central task in temporal graph (TG) reasoning, requiring models to leverage historical interactions to predict upcoming ones. Traditional neural approaches, such as temporal graph neural networks, achieve strong performance but lack explainability and cannot be applied to unseen graphs without retraining. Recent studies have begun to explore using large language models (LLMs) for graph reasoning, but most of them are constrained to static graphs or small synthetic TGs and lack the evaluation of the quality of reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs. ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning strategies from graph structure and to produce explanations that directly justify their predictions. To enable evaluation on LLM-generated reasoning traces, we propose a new evaluation protocol combining ranking metrics with an LLM-as-a-Judge system that assesses both the quality of reasoning and the impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning Qwen3-4B under our framework, show that it outperforms much larger frontier LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality explanations confirmed by both the LLM judge and human evaluation. </p>
<blockquote>
<p>é¢„æµ‹æœªæ¥é“¾æ¥æ˜¯æ—¶åºå›¾ï¼ˆTGï¼‰æ¨ç†ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹åˆ©ç”¨å†å²äº¤äº’æ¥é¢„æµ‹å³å°†åˆ°æ¥çš„äº¤äº’ã€‚ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œå¦‚æ—¶åºå›¾ç¥ç»ç½‘ç»œï¼Œè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œæ— æ³•åœ¨æœªé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹åº”ç”¨äºæœªè§è¿‡çš„å›¾ã€‚æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå›¾æ¨ç†ï¼Œä½†å¤§å¤šæ•°éƒ½å±€é™äºé™æ€å›¾æˆ–å°åˆæˆTGï¼Œå¹¶ä¸”ç¼ºä¹å¯¹LLMç”Ÿæˆçš„æ¨ç†è½¨è¿¹è´¨é‡çš„è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ—¶åºå›¾çš„æ¨ç†å¢å¼ºå­¦ä¹ ï¼ˆReaL-TGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨ç°å®ä¸–ç•Œçš„TGä¸Šæ‰§è¡Œå¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ã€‚ReaL-TGä½¿ç”¨ç»“æœå¯¼å‘çš„å¥–åŠ±æ¥é¼“åŠ±æ¨¡å‹ä»å›¾å½¢ç»“æ„ä¸­è‡ªæˆ‘æ¢ç´¢æ¨ç†ç­–ç•¥ï¼Œå¹¶äº§ç”Ÿç›´æ¥è¯æ˜å…¶é¢„æµ‹çš„è§£é‡Šã€‚ä¸ºäº†èƒ½å¤Ÿå¯¹LLMç”Ÿæˆçš„æ¨ç†è½¨è¿¹è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®ï¼Œç»“åˆæ’åæŒ‡æ ‡å’Œä½œä¸ºæ³•å®˜çš„LLMç³»ç»Ÿï¼Œè¯„ä¼°æ¨ç†è´¨é‡ä»¥åŠè™šæ„å†…å®¹çš„å½±å“ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸‹å¾®è°ƒQwen3-4Bè·å¾—çš„ReaL-TG-4Bå®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨æ’åæŒ‡æ ‡ä¸Šè¶…è¶Šäº†åŒ…æ‹¬GPT-5 miniåœ¨å†…çš„å‰æ²¿æ›´å¤§å‹LLMï¼ŒåŒæ—¶äº§ç”Ÿäº†ç”±LLMæ³•å®˜å’Œäººç±»è¯„ä¼°ç¡®è®¤çš„é«˜è´¨é‡è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ—¶åºå›¾ï¼ˆTemporal Graphï¼ŒTGï¼‰ä¸­çš„é“¾æ¥é¢„æµ‹ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯è§£é‡Šæ€§å­¦ä¹ æ–¹æ³•â€”â€”Reasoning-Enhanced Learning for Temporal Graphsï¼ˆReaL-TGï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ‰§è¡Œå¯è§£é‡Šçš„é“¾æ¥é¢„æµ‹ï¼Œåˆ©ç”¨ç»“æœå¯¼å‘çš„å¥–åŠ±æœºåˆ¶é¼“åŠ±æ¨¡å‹è‡ªæˆ‘æ¢ç´¢æ¨ç†ç­–ç•¥ï¼ŒåŒæ—¶ç”Ÿæˆè§£é‡Šé¢„æµ‹çš„ç›´æ¥è¯æ®ã€‚ä¸ºè¯„ä¼°LLMç”Ÿæˆçš„æ¨ç†è½¨è¿¹è´¨é‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®ï¼Œç»“åˆæ’åæŒ‡æ ‡å’ŒLLMä½œä¸ºè¯„å§”çš„ç³»ç»Ÿæ¥è¯„ä¼°æ¨ç†è´¨é‡å’Œå¹»è§‰çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡ReaL-TGæ¡†æ¶å¾®è°ƒå¾—åˆ°çš„ReaL-TG-4Bæ¨¡å‹åœ¨æ’åæŒ‡æ ‡ä¸Šä¼˜äºå‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-5 miniç­‰ï¼ŒåŒæ—¶äº§ç”Ÿçš„è§£é‡Šå¾—åˆ°äº†LLMè¯„å§”å’Œäººç±»è¯„ä¼°è€…çš„ç¡®è®¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶åºå›¾é“¾æ¥é¢„æµ‹æ˜¯æ—¶åºå›¾æ¨ç†çš„æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€è¦åˆ©ç”¨å†å²äº¤äº’é¢„æµ‹æœªæ¥äº¤äº’ã€‚</li>
<li>ä¼ ç»Ÿç¥ç»ç½‘ç»œæ–¹æ³•å¦‚æ—¶åºå›¾ç¥ç»ç½‘ç»œè™½ç„¶æ€§èƒ½å¼ºå¤§ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œæ— æ³•åº”ç”¨äºæœªè§è¿‡çš„å›¾è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å›¾æ¨ç†ä¸­çš„åº”ç”¨å¼€å§‹å—åˆ°å…³æ³¨ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»…é™äºé™æ€å›¾æˆ–å°åˆæˆæ—¶åºå›¾ï¼Œç¼ºä¹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹è´¨é‡è¯„ä¼°ã€‚</li>
<li>ReaL-TGæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¾®è°ƒLLMsï¼Œä»¥æ‰§è¡Œå…·æœ‰å¯è§£é‡Šæ€§çš„æ—¶åºå›¾é“¾æ¥é¢„æµ‹ã€‚</li>
<li>ReaL-TGä½¿ç”¨ç»“æœå¯¼å‘çš„å¥–åŠ±æœºåˆ¶é¼“åŠ±æ¨¡å‹è‡ªæˆ‘æ¢ç´¢æ¨ç†ç­–ç•¥ï¼Œå¹¶ç”Ÿæˆè§£é‡Šé¢„æµ‹çš„ç›´æ¥è¯æ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åè®®æ¥è¯„ä¼°LLMç”Ÿæˆçš„æ¨ç†è½¨è¿¹è´¨é‡ï¼Œç»“åˆæ’åæŒ‡æ ‡å’ŒLLMä½œä¸ºè¯„å§”çš„ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00975v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00975v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00975v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RPRO-Ranked-Preference-Reinforcement-Optimization-for-Enhancing-Medical-QA-and-Diagnostic-Reasoning"><a href="#RPRO-Ranked-Preference-Reinforcement-Optimization-for-Enhancing-Medical-QA-and-Diagnostic-Reasoning" class="headerlink" title="RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical   QA and Diagnostic Reasoning"></a>RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical   QA and Diagnostic Reasoning</h2><p><strong>Authors:Chia-Hsuan Hsu, Jun-En Ding, Hsin-Ling Hsu, Feng Liu, Fang-Ming Hung</strong></p>
<p>Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that uniquely combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO differentiates itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley-Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA and MedQA-USMLE show consistent improvements over strong baselines. Remarkably, our 1.1B parameter model outperforms much larger 7B-13B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement offers a scalable and effective approach to building more reliable, clinically grounded medical LLMs. </p>
<blockquote>
<p>åŒ»ç–—é—®ç­”éœ€è¦èåˆé¢†åŸŸçŸ¥è¯†å’Œé€»è¾‘æ¨ç†çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ç”Ÿæˆçš„æ¨ç†é“¾ç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ’ååå¥½å¼ºåŒ–ä¼˜åŒ–ï¼ˆRPROï¼‰è¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œå…¶ç‹¬ç‰¹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ä¼˜åŒ–ï¼Œä»¥æå‡ä¸´åºŠæ€ç»´é“¾ï¼ˆCoTï¼‰æ€§èƒ½ã€‚RPROé€šè¿‡é‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œä½¿è¾“å‡ºä¸æ—¢å®šä¸´åºŠå·¥ä½œæµç¨‹ä¿æŒä¸€è‡´ï¼ŒåŒæ—¶è‡ªåŠ¨è¯†åˆ«å’Œçº æ­£ä½è´¨é‡æ¨ç†é“¾ï¼Œä»è€Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸åŒºåˆ«ã€‚ä¸åŒäºä¼ ç»Ÿçš„é…å¯¹åå¥½æ–¹æ³•ï¼ŒRPROå¼•å…¥äº†åŸºäºBradley-Terryæ¨¡å‹çš„ç»„æ’åä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†KLæ•£åº¦æ­£åˆ™åŒ–ä»¥å®ç°ç¨³å®šè®­ç»ƒã€‚åœ¨PubMedQAå’ŒMedQA-USMLEä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼ŒRPROå…·æœ‰æŒç»­ä¸€è‡´çš„æ”¹è¿›æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬1.1Bå‚æ•°æ¨¡å‹çš„è¡¨ç°ä¼˜äº7B-13Bçš„å¤§å‹æ¨¡å‹ï¼ŒåŒ…æ‹¬åŒ»ç–—ä¸“ä¸šå˜ä½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåå¥½ä¼˜åŒ–ä¸è´¨é‡é©±åŠ¨ç»†åŒ–ç›¸ç»“åˆï¼Œä¸ºæ„å»ºæ›´å¯é ã€ä»¥ä¸´åºŠä¸ºåŸºç¡€çš„åŒ»ç–—LLMæä¾›äº†ä¸€ç§å¯æ‰©å±•å’Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00974v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>åŒ»å­¦é—®ç­”éœ€è¦èåˆé¢†åŸŸçŸ¥è¯†å’Œé€»è¾‘æ¨ç†çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº§ç”Ÿçš„æ¨ç†é“¾å¾€å¾€ç¼ºä¹äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRanked Preference Reinforcement Optimizationï¼ˆRPROï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ä¼˜åŒ–ï¼Œæå‡ä¸´åºŠæ€ç»´é“¾ï¼ˆCoTï¼‰çš„è¡¨ç°ã€‚RPROé‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œä½¿è¾“å‡ºç¬¦åˆä¸´åºŠå·¥ä½œæµç¨‹ï¼Œå¹¶è‡ªåŠ¨è¯†åˆ«å’Œçº æ­£ä½è´¨é‡æ¨ç†é“¾ã€‚ä¸ä¼ ç»Ÿçš„ä¸€å¯¹ä¸€åå¥½æ–¹æ³•ä¸åŒï¼ŒRPROå¼•å…¥åŸºäºBradley-Terryæ¨¡å‹çš„ç¾¤ç»„æ’åä¼˜åŒ–ï¼Œå¹¶ç»“åˆKLæ•£åº¦æ­£åˆ™åŒ–è¿›è¡Œç¨³å®šè®­ç»ƒã€‚åœ¨PubMedQAå’ŒMedQA-USMLEä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿ï¼ŒRPROæœ‰æŒç»­æ€§çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‚æ•°è§„æ¨¡ä¸º1.1Bçš„æ¨¡å‹è¡¨ç°ä¼˜äº7B-13Bçš„å¤§å‹æ¨¡å‹ï¼ŒåŒ…æ‹¬åŒ»å­¦ä¸“ç”¨å˜ä½“ã€‚è¿™è¡¨æ˜ç»“åˆåå¥½ä¼˜åŒ–ä¸è´¨é‡é©±åŠ¨ç»†åŒ–æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„é€”å¾„ï¼Œç”¨äºæ„å»ºæ›´å¯é ã€ä¸´åºŠåŸºç¡€æ‰å®çš„åŒ»å­¦LLMsã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦é—®ç­”éœ€è¦é›†æˆé¢†åŸŸçŸ¥è¯†å’Œé€»è¾‘æ¨ç†çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿçš„æ¨ç†é“¾åœ¨äº‹å®å‡†ç¡®æ€§å’Œä¸´åºŠå¯é æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>RPROæ¡†æ¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åå¥½é©±åŠ¨æ¨ç†ä¼˜åŒ–ï¼Œæå‡ä¸´åºŠæ€ç»´é“¾çš„è¡¨ç°ã€‚</li>
<li>RPROé‡‡ç”¨ä»»åŠ¡é€‚åº”æ€§æ¨ç†æ¨¡æ¿å’Œæ¦‚ç‡è¯„ä¼°æœºåˆ¶ï¼Œä½¿è¾“å‡ºæ›´ç¬¦åˆä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
<li>RPROèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å’Œçº æ­£ä½è´¨é‡çš„æ¨ç†é“¾ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRPROå¼•å…¥ç¾¤ç»„æ’åä¼˜åŒ–å’ŒKLæ•£åº¦æ­£åˆ™åŒ–è¿›è¡Œç¨³å®šè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00974v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00974v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CoreThink-A-Symbolic-Reasoning-Layer-to-reason-over-Long-Horizon-Tasks-with-LLMs"><a href="#CoreThink-A-Symbolic-Reasoning-Layer-to-reason-over-Long-Horizon-Tasks-with-LLMs" class="headerlink" title="CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks   with LLMs"></a>CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks   with LLMs</h2><p><strong>Authors:Jay Vaghasiya, Omkar Ghugarkar, Vishvesh Bhat, Vipul Dholaria, Julian McAuley</strong></p>
<p>We introduce CoreThink, a state-of-the-art Reasoning Layer built upon a novel reasoning method called General Symbolics. This approach diverges from reasoning paradigms such as test-time scaling, Supervised Fine-Tuning (SFT), and Reinforcement Learning with Verifiable Rewards (RLVR). CoreThink General Symbolic Reasoner (GSR) is specifically structured around three key use cases: tool-calling, code generation, and planning, demonstrating exemplary performance across a total of seven benchmarks in their respective areas. Notably, we are achieving SOTA scores of 66.66% on Livecodebench v6, 89% on Instruction-Following Evals, and 24.4% on ARC-AGI-2. We also present an agentic coding IDE, developed using the principles of General Symbolics, which achieves a state-of-the-art accuracy of 62.3% on SWE-Bench Lite. We are able to achieve these improvements without any fine-tuning or training costs. Our Reasoning Layer is designed to provide a pure performance uplift, ensuring that a modelâ€™s accuracy on reasoning tasks is never negatively impacted. We argue that incumbent methods will eventually lead to diminishing returns in LLM performance, necessitating the development of new reasoning techniques. This technical report details our approach at a high level and the availability of the CoreThink models for reasoning-intensive use cases. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CoreThinkï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæœ€æ–°æ¨ç†æ–¹æ³•â€”â€”é€šç”¨ç¬¦å·ï¼ˆGeneral Symbolicsï¼‰çš„å…ˆè¿›æ¨ç†å±‚ã€‚æ­¤æ–¹æ³•ä¸åŒäºæµ‹è¯•æ—¶ç¼©æ”¾ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ç­‰æ¨ç†èŒƒå¼ã€‚CoreThinké€šç”¨ç¬¦å·æ¨ç†å™¨ï¼ˆGSRï¼‰ç‰¹åˆ«å›´ç»•ä¸‰ç§å…³é”®ç”¨ä¾‹æ„å»ºï¼šå·¥å…·è°ƒç”¨ã€ä»£ç ç”Ÿæˆå’Œè§„åˆ’ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å„è‡ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨Livecodebench v6ä¸Šè¾¾åˆ°äº†66.66%çš„SOTAåˆ†æ•°ï¼Œåœ¨æŒ‡ä»¤è·Ÿéšè¯„ä¼°ä¸Šè¾¾åˆ°äº†89%ï¼Œåœ¨ARC-AGI-2ä¸Šè¾¾åˆ°äº†24.4%ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªä½¿ç”¨é€šç”¨ç¬¦å·åŸåˆ™å¼€å‘çš„ä»£ç†ç¼–ç IDEï¼Œåœ¨SWE-Bench Liteä¸Šè¾¾åˆ°äº†62.3%çš„æœ€æ–°å‡†ç¡®æ€§ã€‚æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸äº§ç”Ÿä»»ä½•å¾®è°ƒæˆ–åŸ¹è®­æˆæœ¬çš„æƒ…å†µä¸‹å®ç°è¿™äº›æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¨ç†å±‚æ—¨åœ¨æä¾›çº¯ç²¹çš„æ€§èƒ½æå‡ï¼Œç¡®ä¿æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ä¸ä¼šå—åˆ°è´Ÿé¢å½±å“ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç°æœ‰æ–¹æ³•æœ€ç»ˆå°†å¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ”¶ç›Šé€’å‡ï¼Œå› æ­¤éœ€è¦å¼€å‘æ–°çš„æ¨ç†æŠ€æœ¯ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šä»é«˜å±‚æ¬¡ä¸Šè¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬çš„æ–¹æ³•ä»¥åŠCoreThinkæ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹ç”¨ä¾‹ä¸­çš„å¯ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00971v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CoreThinkæ˜¯ä¸€æ¬¾åŸºäºå…¨æ–°é€šç”¨ç¬¦å·æ¨ç†æ–¹æ³•å»ºç«‹çš„å…ˆè¿›æ¨ç†å±‚ã€‚å®ƒä¸æµ‹è¯•æ—¶ç¼©æ”¾ã€ç›‘ç£å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ç­‰æ¨ç†æ¨¡å¼ä¸åŒã€‚CoreThinké€šç”¨ç¬¦å·æ¨ç†å™¨ï¼ˆGSRï¼‰é’ˆå¯¹å·¥å…·è°ƒç”¨ã€ä»£ç ç”Ÿæˆå’Œè§„åˆ’ç­‰ä¸‰ä¸ªå…³é”®åº”ç”¨åœºæ™¯è¿›è¡Œä¸“é—¨è®¾è®¡ï¼Œåœ¨å„è‡ªçš„é¢†åŸŸé‡Œå®ç°äº†ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å“è¶Šè¡¨ç°ï¼Œä¾‹å¦‚åœ¨Livecodebench v6ä¸Šè¾¾åˆ°66.66%çš„é¡¶å°–åˆ†æ•°ã€‚æˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾é‡‡ç”¨é€šç”¨ç¬¦å·å­¦åŸç†çš„ç¼–ç¨‹IDEï¼Œåœ¨SWE-Bench Liteä¸Šè¾¾åˆ°äº†62.3%çš„é¡¶å°–å‡†ç¡®ç‡ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨ç†å±‚æ—¨åœ¨æä¾›çº¯ç²¹çš„æ€§èƒ½æå‡ï¼Œç¡®ä¿æ¨¡å‹çš„æ¨ç†ä»»åŠ¡ç²¾åº¦ä¸å—è´Ÿé¢å½±å“ã€‚æˆ‘ä»¬é¢„æµ‹ç°æœ‰çš„æ–¹æ³•æœ€ç»ˆä¼šå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ”¶ç›Šé€’å‡ï¼Œå› æ­¤éœ€è¦å¼€å‘æ–°çš„æ¨ç†æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoreThinkæ˜¯ä¸€ä¸ªåŸºäºå…¨æ–°é€šç”¨ç¬¦å·æ¨ç†æ–¹æ³•çš„å…ˆè¿›æ¨ç†å±‚ã€‚</li>
<li>CoreThinkå®ç°äº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å“è¶Šè¡¨ç°ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨ã€ä»£ç ç”Ÿæˆå’Œè§„åˆ’ç­‰é¢†åŸŸã€‚</li>
<li>CoreThinkçš„æ¨ç†å±‚æ—¨åœ¨åœ¨ä¸æŸå®³æ¨¡å‹æ¨ç†ä»»åŠ¡ç²¾åº¦çš„å‰æä¸‹ï¼Œæä¾›çº¯ç²¹çš„æ€§èƒ½æå‡ã€‚</li>
<li>CoreThinkæ¨å‡ºäº†åŸºäºé€šç”¨ç¬¦å·å­¦åŸç†çš„ç¼–ç¨‹IDEï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
<li>ç°æœ‰æ¨ç†æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ”¶ç›Šé€’å‡ï¼Œå› æ­¤éœ€è¦æ–°çš„æ¨ç†æŠ€æœ¯ã€‚</li>
<li>CoreThinkæ¨¡å‹é€‚ç”¨äºéœ€è¦å¤§é‡æ¨ç†çš„ä½¿ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00971v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00971v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00971v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00971v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00971v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00971v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Probe-Rewrite-Evaluate-A-Workflow-for-Reliable-Benchmarks-and-Quantifying-Evaluation-Awareness"><a href="#Probe-Rewrite-Evaluate-A-Workflow-for-Reliable-Benchmarks-and-Quantifying-Evaluation-Awareness" class="headerlink" title="Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and   Quantifying Evaluation Awareness"></a>Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and   Quantifying Evaluation Awareness</h2><p><strong>Authors:Lang Xiong, Nishant Bhargava, Wesley Chang, Jianhang Hong, Haihao Liu, Kevin Zhu</strong></p>
<p>Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as â€œevaluation awareness.â€ This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a modelâ€™s true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from â€œtest-likeâ€ to â€œdeploy-likeâ€ and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten â€œdeploy-likeâ€ prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ„ŸçŸ¥åˆ°ä»ç°å®ä¸–ç•Œéƒ¨ç½²ç¯å¢ƒåˆ°å—æ§è¯„ä¼°ç¯å¢ƒçš„å˜æ›´æ—¶ï¼Œé€šå¸¸ä¼šè¡¨ç°å‡ºæ˜¾è‘—çš„è¡Œä¸ºå˜åŒ–ï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºâ€œè¯„ä¼°æ„è¯†â€ã€‚è¿™ç§å·®å¼‚å¯¹äººå·¥æ™ºèƒ½å¯¹é½æå‡ºäº†å…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºåŸºå‡†æ€§èƒ½å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®å®‰å…¨æ€§å’Œè¯šå®åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ“ä½œæç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¥ç³»ç»Ÿåœ°é‡åŒ–è¿™äº›è¡Œä¸ºå˜åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹æç¤ºè¿›è¡Œä»â€œæµ‹è¯•å‹â€åˆ°â€œéƒ¨ç½²å‹â€çš„æŒç»­é‡è¡¨åˆ†ï¼Œå¹¶åˆ©ç”¨LLMé‡å†™ç­–ç•¥å°†è¿™äº›æç¤ºè½¬å‘æ›´è‡ªç„¶ã€æ›´ç¬¦åˆéƒ¨ç½²é£æ ¼çš„ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ä»»åŠ¡ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œåœ¨è§’è‰²æ‰®æ¼”æ•°æ®é›†ä¸Šè¿›è¡Œé‡å†™åï¼Œæ¢é’ˆå¹³å‡å¾—åˆ†æé«˜äº†30%ã€‚åœ¨åŸå§‹å’Œé‡å†™åçš„æç¤ºä¸Šè¯„ä¼°ä¸€ç³»åˆ—æœ€æ–°æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°é‡å†™çš„â€œéƒ¨ç½²å‹â€æç¤ºå¼•å‘äº†è¡Œä¸ºå’Œæ€åº¦çš„æ˜¾è‘—ä¸”ä¸€è‡´çš„å˜åŒ–ã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯šå®å›åº”çš„å¹³å‡å¢åŠ äº†5.26%ï¼Œè€Œæ¬ºéª—æ€§å›åº”çš„å¹³å‡ä¸‹é™äº†12.40%ã€‚æ­¤å¤–ï¼Œæ‹’ç»ç‡å¹³å‡å¢åŠ äº†6.38%ï¼Œè¡¨æ˜å®‰å…¨åˆè§„æ€§æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯„ä¼°æ„è¯†æ˜¯ä¸€ä¸ªå¯é‡åŒ–çš„ã€å¯æ“æ§çš„å› ç´ ï¼Œç›´æ¥å½±å“LLMçš„è¡Œä¸ºï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ„ŸçŸ¥çš„æµ‹è¯•ç¯å¢ƒä¸­æ›´å®¹æ˜“äº§ç”Ÿä¸å®‰å…¨æˆ–æ¬ºéª—æ€§çš„è¾“å‡ºã€‚è¿™å¼ºè°ƒäº†åœ¨å®é™…éƒ¨ç½²å‰ï¼Œéœ€è¦æ›´ç°å®çš„è¯„ä¼°æ¡†æ¶æ¥å‡†ç¡®è¡¡é‡æ¨¡å‹å¯¹é½çš„çœŸå®éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00591v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ„ŸçŸ¥åˆ°ä»ç°å®ä¸–ç•Œéƒ¨ç½²ç¯å¢ƒåˆ°å—æ§è¯„ä¼°ç¯å¢ƒçš„è½¬å˜æ—¶ï¼Œä¼šè¡¨ç°å‡ºæ˜¾è‘—çš„è¡Œä¸ºå˜åŒ–ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œè¯„ä¼°æ„è¯†â€ã€‚è¿™ç§è¡Œä¸ºå·®å¼‚ç»™AIå¯¹é½å¸¦æ¥äº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå› ä¸ºåŸºå‡†æµ‹è¯•æ€§èƒ½å¯èƒ½æ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çš„çœŸå®å®‰å…¨æ€§å’Œè¯šå®åº¦ã€‚æœ¬ç ”ç©¶é€šè¿‡æ“çºµæç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡æ¥ç³»ç»Ÿåœ°é‡åŒ–è¿™äº›è¡Œä¸ºå˜åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨çº¿æ€§æ¢é’ˆå¯¹æç¤ºè¿›è¡Œä»â€œæµ‹è¯•å‹â€åˆ°â€œéƒ¨ç½²å‹â€çš„è¿ç»­è¯„åˆ†ï¼Œå¹¶åˆ©ç”¨LLMé‡å†™ç­–ç•¥æ¥è½¬å˜è¿™äº›æç¤ºï¼Œä½¿å…¶æ›´åŠ è‡ªç„¶ã€è´´è¿‘éƒ¨ç½²ç¯å¢ƒï¼ŒåŒæ—¶ä¿ç•™åŸå§‹ä»»åŠ¡ã€‚é€šè¿‡æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æˆ˜ç•¥è§’è‰²æ‰®æ¼”æ•°æ®é›†ä¸Šé‡å†™æç¤ºåï¼Œæ¢é’ˆå¹³å‡å¾—åˆ†æé«˜äº†30%ã€‚è¯„ä¼°ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¿™äº›åŸå§‹å’Œé‡å†™æç¤ºä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å‘ç°é‡å†™çš„â€œéƒ¨ç½²å‹â€æç¤ºå¼•å‘äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„è¡Œä¸ºå˜åŒ–ã€‚æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯šå®å›åº”çš„å¹³å‡å¢åŠ äº†5.26%ï¼Œç›¸åº”çš„æ¬ºéª—å›åº”å¹³å‡å‡å°‘äº†12.4%ã€‚æ­¤å¤–ï¼Œæ‹’ç»ç‡å¹³å‡å¢åŠ äº†6.38%ï¼Œè¡¨æ˜å®‰å…¨åˆè§„æ€§æœ‰æ‰€æé«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯„ä¼°æ„è¯†æ˜¯ä¸€ä¸ªå¯é‡åŒ–ä¸”å¯æ“æ§çš„å› ç´ ï¼Œç›´æ¥å½±å“LLMçš„è¡Œä¸ºï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ„ŸçŸ¥çš„æµ‹è¯•ç¯å¢ƒä¸­æ›´å®¹æ˜“äº§ç”Ÿä¸å®‰å…¨æˆ–æ¬ºéª—æ€§çš„è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°ç¯å¢ƒå’Œå®é™…éƒ¨ç½²ç¯å¢ƒä¸­çš„è¡Œä¸ºå­˜åœ¨å·®å¼‚ï¼Œç§°ä¸ºâ€œè¯„ä¼°æ„è¯†â€ã€‚</li>
<li>è¿™ç§è¡Œä¸ºå·®å¼‚å¯¹AIå¯¹é½æ„æˆæŒ‘æˆ˜ï¼Œå› ä¸ºåŸºå‡†æµ‹è¯•æ€§èƒ½å¯èƒ½æ— æ³•åæ˜ æ¨¡å‹çš„çœŸå®å®‰å…¨æ€§å’Œè¯šå®åº¦ã€‚</li>
<li>é€šè¿‡æ“çºµæç¤ºçš„æ„ŸçŸ¥ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥ç³»ç»Ÿåœ°é‡åŒ–LLMçš„è¡Œä¸ºå˜åŒ–ã€‚</li>
<li>å¼•å…¥çº¿æ€§æ¢é’ˆæ–¹æ³•è¯„åˆ†æç¤ºï¼Œå¹¶å‘ç°é‡å†™æç¤ºä»¥æ›´è´´è¿‘éƒ¨ç½²ç¯å¢ƒå¯ä»¥æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>é‡å†™åçš„æç¤ºå¯¼è‡´æ¨¡å‹è¡Œä¸ºæ˜¾è‘—ä¸”ä¸€è‡´åœ°æ”¹å˜ï¼Œè¡¨ç°ä¸ºè¯šå®å›åº”å¢åŠ ï¼Œæ¬ºéª—å›åº”å‡å°‘ï¼Œå®‰å…¨åˆè§„æ€§æé«˜ã€‚</li>
<li>è¯„ä¼°æ„è¯†æ˜¯ä¸€ä¸ªå¯é‡åŒ–ä¸”å¯æ“æ§çš„å› ç´ ï¼Œç›´æ¥å½±å“LLMçš„è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00591v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00591v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00591v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00591v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00591v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00591v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ERank-Fusing-Supervised-Fine-Tuning-and-Reinforcement-Learning-for-Effective-and-Efficient-Text-Reranking"><a href="#ERank-Fusing-Supervised-Fine-Tuning-and-Reinforcement-Learning-for-Effective-and-Efficient-Text-Reranking" class="headerlink" title="ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for   Effective and Efficient Text Reranking"></a>ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for   Effective and Efficient Text Reranking</h2><p><strong>Authors:Yuzheng Cai, Yanzhao Zhang, Dingkun Long, Mingxin Li, Pengjun Xie, Weiguo Zheng</strong></p>
<p>Text reranking models are a crucial component in modern systems like Retrieval-Augmented Generation, tasked with selecting the most relevant documents prior to generation. However, current Large Language Models (LLMs) powered rerankers often face a fundamental trade-off. On one hand, Supervised Fine-Tuning based pointwise methods that frame relevance as a binary classification task lack the necessary scoring discrimination, particularly for those built on reasoning LLMs. On the other hand, approaches designed for complex reasoning often employ powerful yet inefficient listwise formulations, rendering them impractical for low latency applications. To resolve this dilemma, we introduce ERank, a highly effective and efficient pointwise reranker built from a reasoning LLM that excels across diverse relevance scenarios. We propose a novel two-stage training pipeline that begins with Supervised Fine-Tuning (SFT). In this stage, we move beyond binary labels and train the model generatively to output fine grained integer scores, which significantly enhances relevance discrimination. The model is then further refined using Reinforcement Learning (RL) with a novel, listwise derived reward. This technique instills global ranking awareness into the efficient pointwise architecture. We evaluate the ERank reranker on the BRIGHT, FollowIR, TREC DL, and BEIR benchmarks, demonstrating superior effectiveness and robustness compared to existing approaches. On the reasoning-intensive BRIGHT benchmark, our ERank-4B achieves an nDCG@10 of 38.7, while a larger 32B variant reaches a state of the art nDCG@10 of 40.2. </p>
<blockquote>
<p>æ–‡æœ¬é‡æ’æ¨¡å‹æ˜¯ç°ä»£æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œè´Ÿè´£åœ¨ç”Ÿæˆå‰é€‰æ‹©æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„é‡æ–°æ’åå™¨å¸¸å¸¸é¢ä¸´ä¸€ä¸ªåŸºæœ¬çš„æƒè¡¡ã€‚ä¸€æ–¹é¢ï¼ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰çš„ç‚¹æ€æ–¹æ³•å°†ç›¸å…³æ€§ä½œä¸ºäºŒåˆ†ç±»ä»»åŠ¡æ¥å¤„ç†ï¼Œç¼ºä¹å¿…è¦çš„è¯„åˆ†åˆ¤åˆ«åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å¦ä¸€æ–¹é¢ï¼Œé’ˆå¯¹å¤æ‚æ¨ç†çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å¼ºå¤§ä½†ä½æ•ˆçš„åˆ—è¡¨å¼å…¬å¼ï¼Œä½¿å¾—å®ƒä»¬ä¸é€‚ç”¨äºä½å»¶è¿Ÿåº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å›°å¢ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ERankï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”é«˜æ•ˆçš„ç‚¹æ€é‡æ–°æ’åå™¨ï¼Œç”±æ“…é•¿å¤„ç†å„ç§ç›¸å…³æ€§åœºæ™¯çš„å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºè€Œæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé¦–å…ˆä»ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¼€å§‹ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è¶…è¶Šäº†äºŒå…ƒæ ‡ç­¾çš„é™åˆ¶ï¼Œä»¥ç”Ÿæˆæ–¹å¼è®­ç»ƒæ¨¡å‹ä»¥è¾“å‡ºç²¾ç»†ç²’åº¦çš„æ•´æ•°åˆ†æ•°ï¼Œè¿™æ˜¾è‘—æé«˜äº†ç›¸å…³æ€§çš„åˆ¤åˆ«åŠ›ã€‚ç„¶åè¿›ä¸€æ­¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œç»†åŒ–è°ƒæ•´æ¨¡å‹å‚æ•°ä¸è¡Œä¸ºç‰¹å¾é€‰å–è®¾ç½®å†³ç­–æ¨¡å¼æˆ–å‡†åˆ™ä¿®æ­£è¯„åˆ†å‡½æ•°ï¼Œå¹¶ä½¿ç”¨ä¸€ç§æ–°å‹çš„åˆ—è¡¨å¼æ´¾ç”Ÿå¥–åŠ±ã€‚è¿™ç§æŠ€æœ¯å°†å…¨å±€æ’åæ„è¯†çŒè¾“åˆ°é«˜æ•ˆçš„ç‚¹æ€æ¶æ„ä¸­ã€‚æˆ‘ä»¬åœ¨BRIGHTã€FollowIRã€TREC DLå’ŒBEIRåŸºå‡†æµ‹è¯•é›†ä¸Šå¯¹ERanké‡æ–°æ’åå™¨è¿›è¡Œäº†è¯„ä¼°è¯æ˜äº†å®ƒåœ¨å…ˆè¿›çš„æ–¹æ³•å’Œä»»åŠ¡ç›®æ ‡ä¸Šæ‰€è¡¨ç°å¾—æ›´å‡ºè‰²å¼ºå¤§ä¸”å…·æœ‰é²æ£’æ€§å¯¹äºéœ€è¦å¼ºçƒˆé€»è¾‘æ¨ç†èƒ½åŠ›çš„brightåŸºå‡†æµ‹è¯•æ•°æ®é›†æˆ‘ä»¬çš„ERank-4Bæ¨¡å‹å®ç°äº†nDCG@10ä¸º38.7çš„å¾—åˆ†è€Œæ›´å¤§çš„32Bå˜ä½“åˆ™è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„nDCG@10ä¸º40.2çš„å¾—åˆ†è¡¨ç°å“è¶Šåœ¨é‡æ¨ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ä¼˜åŠ¿æ˜æ˜¾é¢†å…ˆäºä¸šç•Œå…¶ä»–æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00520v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°ä»£ç³»ç»Ÿä¸­é‡è¦ç»„ä»¶â€”â€”æ–‡æœ¬é‡æ’åºæ¨¡å‹ï¼ˆERankï¼‰çš„è®¾è®¡ä¸å®ç°ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡æ’åºä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼ŒERankæ¨¡å‹é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆä¸”é«˜æ•ˆçš„ç‚¹æ€é‡æ’åºæ–¹æ³•ï¼Œå¹¶åº”ç”¨äºå¤šç§ç›¸å…³æ€§åœºæ™¯ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹ä¸ä»…æå‡äº†åˆ¤åˆ«ç›¸å…³æ€§èƒ½åŠ›ï¼Œä¹Ÿå®ç°äº†å…¨å±€æ’åæ„è¯†ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒERankè¡¨ç°å‡ºäº†å“è¶Šçš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ERankæ˜¯ä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬é‡æ’åºä»»åŠ¡çš„é«˜æ•ˆä¸”æœ‰æ•ˆçš„æ¨¡å‹ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡æ’åºä»»åŠ¡ä¸­é¢ä¸´æƒè¡¡ï¼šç‚¹æ€æ–¹æ³•ç¼ºä¹è¯„åˆ†åˆ¤åˆ«åŠ›ï¼Œè€Œå¤æ‚çš„æ¨ç†æ–¹æ³•æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>ERanké€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šé¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒç”Ÿæˆç²¾ç»†ç²’åº¦æ•´æ•°å¾—åˆ†ï¼Œç„¶åé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜å…¨å±€æ’åæ„è¯†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00520v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00520v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00520v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00520v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00520v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.00520v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Igniting-Creative-Writing-in-Small-Language-Models-LLM-as-a-Judge-versus-Multi-Agent-Refined-Rewards"><a href="#Igniting-Creative-Writing-in-Small-Language-Models-LLM-as-a-Judge-versus-Multi-Agent-Refined-Rewards" class="headerlink" title="Igniting Creative Writing in Small Language Models: LLM-as-a-Judge   versus Multi-Agent Refined Rewards"></a>Igniting Creative Writing in Small Language Models: LLM-as-a-Judge   versus Multi-Agent Refined Rewards</h2><p><strong>Authors:Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models">https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºä»¤äººç©ç›®çš„åˆ›é€ æ€§å†™ä½œèƒ½åŠ›ï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚é˜»ç¢äº†å…¶å¹¿æ³›åº”ç”¨ã€‚å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å½“å‰çš„æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨åˆ›æ–°æ€§æ–¹é¢é‡åˆ°å›°éš¾ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡åœ¨ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»AIåé¦ˆï¼ˆRLAIFï¼‰æ¡†æ¶å†…æ¢ç´¢äº†ä¸¤ç§ç‹¬ç‰¹çš„AIé©±åŠ¨å¥–åŠ±ç­–ç•¥ï¼Œä»¥æ¿€å‘ä¸€ä¸ª7Bå‚æ•°SLMçš„åˆ›é€ æ€§å†™ä½œï¼Œç‰¹åˆ«æ˜¯ç”¨äºç”Ÿæˆä¸­æ–‡é—®å€™è¯­ã€‚ç¬¬ä¸€ç§ç­–ç•¥é‡‡ç”¨ä¸€ä¸ªRMï¼Œè¯¥RMç»è¿‡ä¸€ä¸ªæ–°å‹çš„å¤šæ™ºèƒ½ä½“æ‹’ç»é‡‡æ ·æ¡†æ¶è®­ç»ƒçš„é«˜è´¨é‡åå¥½æ•°æ®ï¼Œè¯¥æ¡†æ¶ä¸“ä¸ºåˆ›é€ æ€§ä»»åŠ¡è®¾è®¡ã€‚ç¬¬äºŒç§æ›´æ–°é¢–çš„ç­–ç•¥åˆ©ç”¨äº†ä¸€ä¸ªåŸåˆ™æŒ‡å¯¼çš„LLM-as-a-Judgeï¼Œå…¶å¥–åŠ±å‡½æ•°é€šè¿‡ä¸€ç§å¯¹æŠ—æ€§è®­ç»ƒæ–¹æ¡ˆå’Œä¸€ä¸ªåå°„æœºåˆ¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç›´æ¥æä¾›å¥–åŠ±ä¿¡å·ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè™½ç„¶ä¸¤ç§æ–¹æ³•éƒ½æ˜¾è‘—æé«˜äº†åˆ›æ„è¾“å‡ºçš„è´¨é‡ï¼Œä½†åŸåˆ™æŒ‡å¯¼çš„LLM-as-a-Judgeæ˜æ˜¾äº§ç”Ÿäº†æ›´é«˜è´¨é‡çš„ç”Ÿæˆç»“æœã€‚æ­¤å¤–ï¼Œå®ƒåœ¨è®­ç»ƒæ•ˆç‡å’Œå‡å°‘å¯¹äººç±»æ³¨é‡Šæ•°æ®çš„ä¾èµ–æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºåˆ›é€ æ€§SLMæä¾›äº†å¯ä¼¸ç¼©å’Œæœ‰æ•ˆçš„é€”å¾„ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¸äººç±»åˆ¤æ–­çš„é«˜åº¦ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models%E3%80%82">https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Modelsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21476v1">PDF</a> EMNLP 2025 Main</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒŠäººçš„åˆ›æ„å†™ä½œèƒ½åŠ›ï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚é™åˆ¶äº†å¹¿æ³›åº”ç”¨ã€‚å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç¼ºä¹æ–°é¢–æ€§ï¼Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åˆ™æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æ¢ç´¢äº†RLAIFæ¡†æ¶ä¸‹ä¸¤ç§ç‹¬ç‰¹çš„AIé©±åŠ¨å¥–åŠ±ç­–ç•¥ï¼Œä»¥æ¿€å‘ä¸€ä¸ª7Bå‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ„å†™ä½œï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆä¸­æ–‡é—®å€™è¯­ã€‚ç¬¬ä¸€ä¸ªç­–ç•¥ä½¿ç”¨RMè®­ç»ƒï¼ŒåŸºäºæ–°å‹å¤šæ™ºèƒ½ä½“æ‹’ç»é‡‡æ ·æ¡†æ¶ç²¾é€‰çš„é«˜è´¨é‡åå¥½æ•°æ®ï¼Œé’ˆå¯¹åˆ›æ„ä»»åŠ¡è®¾è®¡ã€‚ç¬¬äºŒä¸ªæ›´åˆ›æ–°çš„ç­–ç•¥æ˜¯é‡‡ç”¨åŸåˆ™æŒ‡å¯¼çš„LLM-as-a-Judgeï¼Œå…¶å¥–åŠ±åŠŸèƒ½é€šè¿‡å…·æœ‰åæ€æœºåˆ¶çš„å¯¹æŠ—è®­ç»ƒæ–¹æ¡ˆä¼˜åŒ–ï¼Œç›´æ¥æä¾›å¥–åŠ±ä¿¡å·ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½æ˜¾è‘—æé«˜åˆ›æ„è¾“å‡ºæ°´å¹³ï¼Œè€ŒåŸåˆ™æŒ‡å¯¼çš„LLM-as-a-Judgeåœ¨ç”Ÿæˆè´¨é‡ä¸Šæ›´èƒœä¸€ç­¹ï¼ŒåŒæ—¶åœ¨è®­ç»ƒæ•ˆç‡å’Œå‡å°‘ä¾èµ–äººç±»æ ‡æ³¨æ•°æ®æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºå°å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ„å‘å±•æä¾›äº†æ›´å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é€”å¾„ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨<a target="_blank" rel="noopener" href="https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Modelså…¬å¼€å¯ç”¨ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°åˆ›æ„å†™ä½œèƒ½åŠ›ï¼Œä½†è®¡ç®—éœ€æ±‚å¤§ï¼Œåº”ç”¨å—é™ã€‚</li>
<li>å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†ä¸¤ç§AIé©±åŠ¨å¥–åŠ±ç­–ç•¥ï¼Œç”¨äºæ¿€å‘å°å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ„å†™ä½œï¼Œç‰¹åˆ«æ˜¯ä¸­æ–‡é—®å€™è¯­ç”Ÿæˆã€‚</li>
<li>ç¬¬ä¸€ç§ç­–ç•¥ä½¿ç”¨RMè®­ç»ƒï¼ŒåŸºäºæ–°å‹å¤šæ™ºèƒ½ä½“æ‹’ç»é‡‡æ ·æ¡†æ¶ç²¾é€‰é«˜è´¨é‡åå¥½æ•°æ®ã€‚</li>
<li>ç¬¬äºŒç§ç­–ç•¥é‡‡ç”¨åŸåˆ™æŒ‡å¯¼çš„LLM-as-a-Judgeï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒä¸åæ€æœºåˆ¶ä¼˜åŒ–å¥–åŠ±åŠŸèƒ½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºä¸¤ç§æ–¹æ³•æé«˜åˆ›æ„è¾“å‡ºï¼ŒåŸåˆ™æŒ‡å¯¼çš„LLM-as-a-Judgeåœ¨ç”Ÿæˆè´¨é‡ã€è®­ç»ƒæ•ˆç‡å’Œæ•°æ®ä¾èµ–æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21476v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21476v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21476v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21476v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Challenges-and-Applications-of-Large-Language-Models-A-Comparison-of-GPT-and-DeepSeek-family-of-models"><a href="#Challenges-and-Applications-of-Large-Language-Models-A-Comparison-of-GPT-and-DeepSeek-family-of-models" class="headerlink" title="Challenges and Applications of Large Language Models: A Comparison of   GPT and DeepSeek family of models"></a>Challenges and Applications of Large Language Models: A Comparison of   GPT and DeepSeek family of models</h2><p><strong>Authors:Shubham Sharma, Sneha Tuli, Narendra Badam</strong></p>
<p>Large Language Models (LLMs) are transforming AI across industries, but their development and deployment remain complex. This survey reviews 16 key challenges in building and using LLMs and examines how these challenges are addressed by two state-of-the-art models with unique approaches: OpenAIâ€™s closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a large open source Mixture-of-Experts model. Through this comparison, we showcase the trade-offs between closed source models (robust safety, fine-tuned reliability) and open source models (efficiency, adaptability). We also explore LLM applications across different domains (from chatbots and coding tools to healthcare and education), highlighting which model attributes are best suited for each use case. This article aims to guide AI researchers, developers, and decision-makers in understanding current LLM capabilities, limitations, and best practices. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å„è¡Œä¸šæ¨åŠ¨äººå·¥æ™ºèƒ½çš„å˜é©ï¼Œä½†å…¶å¼€å‘å’Œéƒ¨ç½²ä»ç„¶å¤æ‚ã€‚è¿™ç¯‡ç»¼è¿°æ–‡ç« å›é¡¾äº†åœ¨æ„å»ºå’Œä½¿ç”¨LLMè¿‡ç¨‹ä¸­æ‰€é¢ä¸´çš„16é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†è¿™ä¸¤ä¸ªæœ€å‰æ²¿æ¨¡å‹æ˜¯å¦‚ä½•åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„ï¼šOpenAIçš„é—­æºGPT-4oï¼ˆ2024å¹´5æœˆæ›´æ–°ï¼‰å’ŒDeepSeek-V3-0324ï¼ˆå¤§å‹å¼€æºä¸“å®¶æ··åˆæ¨¡å‹ï¼Œ2025å¹´3æœˆï¼‰ã€‚é€šè¿‡å¯¹æ¯”åˆ†æï¼Œæœ¬æ–‡å±•ç¤ºäº†é—­æºæ¨¡å‹ï¼ˆç¨³å¥çš„å®‰å…¨æ€§å’Œç²¾ç»†è°ƒæ•´åçš„å¯é æ€§ï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆæ•ˆç‡å’Œé€‚åº”æ€§ï¼‰ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†LLMåœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨ï¼ˆä»èŠå¤©æœºå™¨äººå’Œç¼–ç å·¥å…·åˆ°åŒ»ç–—ä¿å¥å’Œæ•™è‚²ï¼‰ï¼Œå¼ºè°ƒæ¯ä¸ªç”¨ä¾‹æœ€é€‚åˆçš„æ¨¡å‹å±æ€§ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºAIç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå†³ç­–è€…æä¾›æŒ‡å¯¼ï¼Œäº†è§£å½“å‰LLMçš„èƒ½åŠ›ã€å±€é™æ€§å’Œæœ€ä½³å®è·µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21377v1">PDF</a> 18 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨ä¸ºå„è¡Œä¸šå¸¦æ¥äººå·¥æ™ºèƒ½çš„å˜é©ï¼Œä½†å…¶å¼€å‘å’Œéƒ¨ç½²ä»ç„¶é¢ä¸´å¤æ‚æ€§ã€‚æœ¬æ–‡å›é¡¾äº†æ„å»ºå’Œä½¿ç”¨LLMsæ‰€é¢ä¸´çš„16é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ä¸¤ç§å‰æ²¿æ¨¡å‹åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼šOpenAIçš„é—­æºGPT-4oï¼ˆ2024å¹´äº”æœˆæ›´æ–°ç‰ˆï¼‰å’Œå¤§å‹å¼€æºä¸“å®¶æ··åˆæ¨¡å‹DeepSeek-V3-0324ï¼ˆå…¬å¼€æºä»£ç ï¼‰ï¼ˆå°†äºæœªæ¥é‡‡ç”¨æ›´æœ‰æ•ˆç‡ä»¥åŠæ›´å…·é€‚åº”æ€§çš„è·¯çº¿å‘å±•ï¼‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« æ¢è®¨äº†ä¸åŒé¢†åŸŸçš„åº”ç”¨æ¡ˆä¾‹ï¼Œä¾‹å¦‚èŠå¤©æœºå™¨äººã€ç¼–ç å·¥å…·ã€åŒ»ç–—ä¿å¥å’Œæ•™è‚²ç­‰ï¼Œå¹¶å¼ºè°ƒæ¯ä¸ªç”¨ä¾‹æœ€é€‚åˆçš„æ¨¡å‹å±æ€§ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºAIç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå†³ç­–è€…æä¾›å…³äºå½“å‰LLMçš„èƒ½åŠ›ã€å±€é™æ€§å’Œæœ€ä½³å®è·µçš„ç†è§£ã€‚å¸Œæœ›ä¸ºè¯»è€…æä¾›æ›´å¤šä¿¡æ¯å’Œå‚è€ƒå»ºè®®ã€‚éšç€æŠ€æœ¯è¿›ä¸€æ­¥å‘å±•ï¼Œè¯­è¨€æ¨¡å‹æœªæ¥ä¹Ÿå°†è¶Šæ¥è¶Šæ™ºèƒ½åŒ–ã€‚ç ”ç©¶è¯­è¨€å’Œæœºå™¨è¯­è¨€çš„æ·±åº¦èåˆèƒ½å¤Ÿå¢å¼ºäººä¸è®¡ç®—æœºçš„æ™ºèƒ½äº’åŠ¨æ°´å¹³ï¼Œè¿™å¯¹äºè¡Œä¸šçš„å‘å±•å’Œåˆ›æ–°éå¸¸æœ‰ç›Šã€‚ä¹Ÿå¿…å°†æˆä¸ºå¼•é¢†æ™ºèƒ½äº§ä¸šå‘å±•çš„æœªæ¥é©±åŠ¨åŠ›ã€‚ç»¼ä¸Šæ–‡ç« è¯¦ç»†æ€»ç»“äº†LLMå‘å±•ç°çŠ¶å’ŒæŒ‘æˆ˜è¿›è¡Œäº†ä¸€äº›è¯¦å°½æ¢è®¨å’Œæ€è€ƒæä¾›äº†æ·±å…¥äº†è§£å½“ä¸‹æœ€æ–°çš„è§‚ç‚¹æ€è·¯ä¾æ®åŠå‘å±•æ„ä¹‰ä»‹ç»å’Œæ€è€ƒé‡è¦æ€§é—®é¢˜å¹¶åœ¨æ–‡ç« å†…å®¹ä¸­è·å¾—è‰¯å¥½çš„ç†è§£å’Œè®¤çŸ¥å¯¹äººå·¥æ™ºèƒ½é¢†åŸŸå‘å±•èµ·åˆ°äº†å¾ˆå¥½çš„å¯ç¤ºä½œç”¨ã€‚è¿™ç¯‡æ–‡ç« åœ¨AIé¢†åŸŸæ€èµ·äº†ä¸€æ¬¡è¾ƒå¤§çš„çŸ¥è¯†å…±äº«é£æ³¢å› å…¶ä½œä¸ºç›¸å¯¹ç»¼åˆçš„å¤§å°ºåº¦å‘ˆç°ä»ä¸åŒè§†è§’ç»†è‡´å®¡è§†ä¸å‘ˆç°çš„å½¢å¼å¯¹è¡Œä¸šå‘å±•æœ‰å¾ˆå¥½çš„æŒ‡å¼•æ„ä¹‰å±•ç¤ºäº†å…³é”®æ–¹é¢çš„æ€»ç»“å’Œå…¨å±€çš„è®¤çŸ¥ä½“éªŒæ›´å¥½åœ°å®ç°ç°æœ‰èƒŒæ™¯ä¸‹ä¸šå†…äº¤æµå’Œäº’è¡¥ä»è€Œæå‡äººå·¥æ™ºèƒ½æŠ€æœ¯å®é™…ä½¿ç”¨çš„ç²¾åº¦å’Œä½¿ç”¨ä¾¿æ·åº¦ä¸Šæä¾›äº†ä¸€ä¸ªç ”ç©¶çƒ­ç‚¹æˆ–è€…è§‚å¯Ÿåˆ‡å…¥ç‚¹æå…·æ—¶ä»£æ€§åŠå…¶æŒ‡å¯¼æ€§ä½œç”¨ä¸”å…·æœ‰å¾ˆå¼ºçš„åˆ›æ–°æ€§ä»¤äººæ·±æ€èƒ½å¤Ÿæ¿€èµ·äººä»¬å¯¹è¡Œä¸šæ·±åº¦æ¢è®¨å’Œå…±åŒå­¦ä¹ çš„çƒ­æ½®èƒ½å¤Ÿå¼•èµ·è¡Œä¸šå†…äººå£«çš„å…±é¸£å’Œè®¤åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªè¡Œä¸šä¸­æ¨åŠ¨äº†AIçš„å‘å±•ï¼Œä½†å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>æ–‡ç« å¯¹æ¯”åˆ†æäº†å‰æ²¿çš„ä¸¤ç§è¯­è¨€æ¨¡å‹GPT-4oå’ŒDeepSeek-V3-0324ï¼Œçªæ˜¾é—­æºå’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„ä¼˜åŠ£æƒè¡¡ã€‚</li>
<li>LLMåœ¨ä¸åŒé¢†åŸŸï¼ˆå¦‚èŠå¤©æœºå™¨äººã€ç¼–ç¨‹å·¥å…·ç­‰ï¼‰çš„åº”ç”¨å¾—åˆ°æ¢è®¨ï¼Œå¼ºè°ƒæ¨¡å‹å±æ€§ä¸ç”¨ä¾‹çš„åŒ¹é…æ€§ã€‚</li>
<li>æ–‡ç« æ—¨åœ¨å¸®åŠ©AIç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜å’Œå†³ç­–è€…äº†è§£LLMçš„èƒ½åŠ›ã€å±€é™æ€§å’Œæœ€ä½³å®è·µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21377v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21377v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21377v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21377v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21377v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Improving-Aviation-Safety-Analysis-Automated-HFACS-Classification-Using-Reinforcement-Learning-with-Group-Relative-Policy-Optimization"><a href="#Improving-Aviation-Safety-Analysis-Automated-HFACS-Classification-Using-Reinforcement-Learning-with-Group-Relative-Policy-Optimization" class="headerlink" title="Improving Aviation Safety Analysis: Automated HFACS Classification Using   Reinforcement Learning with Group Relative Policy Optimization"></a>Improving Aviation Safety Analysis: Automated HFACS Classification Using   Reinforcement Learning with Group Relative Policy Optimization</h2><p><strong>Authors:Arash Ahmadi, Sarah Sharif, Yaser Banad</strong></p>
<p>Analyzing the human factors behind aviation accidents is crucial for preventing future incidents, yet traditional methods using the Human Factors Analysis and Classification System (HFACS) are limited by scalability and consistency. To address this, we introduce an automated HFACS classification framework for aviation safety analysis that utilizes Reinforcement Learning with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B language model. Our approach incorporates a multi-component reward system tailored for aviation safety analysis and integrates synthetic data generation to overcome class imbalance in accident datasets. The resulting GRPO-optimized model achieved noticeable performance gains, including a 350% increase in exact match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy of 0.8800. Significantly, our specialized model outperforms state-of-the-art LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key metrics. This research also proposes exact match accuracy in multi-label HFACS classification problem as a new benchmarking methodology to evaluate the advanced reasoning capabilities of language models. Ultimately, our work validates that smaller, domain-optimized models can provide a computationally efficient and better solution for critical safety analysis. This approach makes powerful, low-latency deployment on resource-constrained edge devices feasible. </p>
<blockquote>
<p>åˆ†æèˆªç©ºäº‹æ•…ä¸­çš„äººä¸ºå› ç´ å¯¹äºé¢„é˜²æœªæ¥äº‹æ•…è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨äººä¸ºå› ç´ åˆ†æå’Œåˆ†ç±»ç³»ç»Ÿï¼ˆHFACSï¼‰çš„ä¼ ç»Ÿæ–¹æ³•å—åˆ°å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”¨äºèˆªç©ºå®‰å…¨åˆ†æçš„è‡ªåŠ¨åŒ–HFACSåˆ†ç±»æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¸¦æœ‰ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ æ¥å¾®è°ƒLlama-3.1 8Bè¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨é’ˆå¯¹èˆªç©ºå®‰å…¨åˆ†æé‡èº«å®šåˆ¶çš„å¤šç»„ä»¶å¥–åŠ±ç³»ç»Ÿï¼Œå¹¶æ•´åˆåˆæˆæ•°æ®ç”Ÿæˆï¼Œä»¥å…‹æœäº‹æ•…æ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ç»è¿‡GRPOä¼˜åŒ–çš„æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶ä¸­ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡æé«˜äº†350%ï¼ˆä»0.0400æé«˜åˆ°0.1800ï¼‰ï¼Œéƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡æé«˜åˆ°0.8800ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ä¸“ä¸šæ¨¡å‹åœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5-miniå’ŒGemini-2.5-fiashã€‚æœ¬ç ”ç©¶è¿˜æå‡ºå°†å¤šæ ‡ç­¾HFACSåˆ†ç±»é—®é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä½œä¸ºæ–°çš„åŸºå‡†è¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„å…ˆè¿›æ¨ç†èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å·¥ä½œéªŒè¯äº†å°å‹ã€åŸŸä¼˜åŒ–çš„æ¨¡å‹å¯ä»¥ä¸ºç”¨æˆ·æä¾›è®¡ç®—é«˜æ•ˆä¸”æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå…³é”®å®‰å…¨åˆ†æã€‚è¿™ç§æ–¹æ³•ä½¿å¾—åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¼ºå¤§ã€ä½å»¶è¿Ÿçš„æ¨¡å‹å˜å¾—å¯è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21201v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç»“åˆç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•ï¼Œå¯¹èˆªç©ºå®‰å…¨åˆ†æä¸­çš„äººä¸ºå› ç´ è¿›è¡Œåˆ†ç±»ã€‚è¯¥ç ”ç©¶ä½¿ç”¨Llama-3.1 8Bè¯­è¨€æ¨¡å‹ï¼Œå¹¶ç»“åˆå¤šæˆåˆ†å¥–åŠ±ç³»ç»Ÿå’Œåˆæˆæ•°æ®ç”Ÿæˆï¼Œè§£å†³æ•°æ®é›†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚æ–°æ¨¡å‹åœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä¸Šæé«˜äº†350%ï¼Œéƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡ä¹Ÿæœ‰æ‰€æé«˜ï¼Œä¸”åœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†å°†å¤šæ ‡ç­¾HFACSåˆ†ç±»é—®é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä½œä¸ºæ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ€ç»ˆéªŒè¯äº†å°å‹ã€ä¼˜åŒ–çš„æ¨¡å‹åœ¨å…³é”®å®‰å…¨åˆ†ææ–¹é¢å¯æä¾›é«˜æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç”¨äºèˆªç©ºå®‰å…¨åˆ†æä¸­çš„HFACSåˆ†ç±»ã€‚</li>
<li>é‡‡ç”¨Llama-3.1 8Bè¯­è¨€æ¨¡å‹ï¼Œç»“åˆå¤šæˆåˆ†å¥–åŠ±ç³»ç»Ÿæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åˆæˆæ•°æ®ç”Ÿæˆè§£å†³æ•°æ®é›†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>æ¨¡å‹åœ¨ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä¸Šå®ç°æ˜¾è‘—å¢é•¿ï¼Œè¾¾åˆ°0.1800ï¼Œä¼˜äºå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æå‡ºå°†å¤šæ ‡ç­¾HFACSåˆ†ç±»é—®é¢˜çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä½œä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚</li>
<li>å°å‹ã€ä¼˜åŒ–çš„æ¨¡å‹åœ¨å…³é”®å®‰å…¨åˆ†ææ–¹é¢è¡¨ç°å‡ºé«˜æ•ˆæ€§èƒ½ï¼Œé€‚åˆèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21201v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21201v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21201v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21201v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21201v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2508.21201v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2502.11419v2/page_5_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  GPT-FT An Efficient Automated Feature Transformation Using GPT for   Sequence Reconstruction and Performance Enhancement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-07/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-07\./crop_Text-to-Motion/2508.20604v1/page_1_0.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Embracing Aleatoric Uncertainty Generating Diverse 3D Human Motion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
