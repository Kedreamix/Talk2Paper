<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Box-Level Class-Balanced Sampling for Active Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2407.15199v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="Box-Level-Class-Balanced-Sampling-for-Active-Object-Detection"><a href="#Box-Level-Class-Balanced-Sampling-for-Active-Object-Detection" class="headerlink" title="Box-Level Class-Balanced Sampling for Active Object Detection"></a>Box-Level Class-Balanced Sampling for Active Object Detection</h2><p><strong>Authors:Jingyi Liao, Xun Xu, Chuan-Sheng Foo, Lile Cai</strong></p>
<p>Training deep object detectors demands expensive bounding box annotation. Active learning (AL) is a promising technique to alleviate the annotation burden. Performing AL at box-level for object detection, i.e., selecting the most informative boxes to label and supplementing the sparsely-labelled image with pseudo labels, has been shown to be more cost-effective than selecting and labelling the entire image. In box-level AL for object detection, we observe that models at early stage can only perform well on majority classes, making the pseudo labels severely class-imbalanced. We propose a class-balanced sampling strategy to select more objects from minority classes for labelling, so as to make the final training data, \ie, ground truth labels obtained by AL and pseudo labels, more class-balanced to train a better model. We also propose a task-aware soft pseudo labelling strategy to increase the accuracy of pseudo labels. We evaluate our method on public benchmarking datasets and show that our method achieves state-of-the-art performance. </p>
<blockquote>
<p>è®­ç»ƒæ·±åº¦ç›®æ ‡æ£€æµ‹å™¨éœ€è¦å¤§é‡çš„è¾¹ç•Œæ¡†æ ‡æ³¨ã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰æ˜¯ä¸€ç§æœ‰æœ›å‡è½»æ ‡æ³¨è´Ÿæ‹…çš„æŠ€æœ¯ã€‚å¯¹äºç›®æ ‡æ£€æµ‹è¿›è¡Œæ¡†çº§ä¸»åŠ¨å­¦ä¹ ï¼Œå³é€‰æ‹©æœ€å…·æœ‰ä¿¡æ¯é‡çš„æ¡†è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶é€šè¿‡ä¼ªæ ‡ç­¾å¯¹ç¨€ç–æ ‡æ³¨çš„å›¾åƒè¿›è¡Œè¡¥å……ï¼Œå·²è¢«è¯æ˜æ¯”é€‰æ‹©å’Œæ ‡æ³¨æ•´ä¸ªå›¾åƒæ›´å…·æˆæœ¬æ•ˆç›Šã€‚åœ¨ç›®æ ‡æ£€æµ‹çš„æ¡†çº§ä¸»åŠ¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ—©æœŸçš„æ¨¡å‹åªèƒ½åœ¨å¤šæ•°ç±»ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¯¼è‡´ä¼ªæ ‡ç­¾å‡ºç°ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»åˆ«å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©æ›´å¤šçš„å°‘æ•°ç±»åˆ«å¯¹è±¡è¿›è¡Œæ ‡æ³¨ï¼Œä»¥ä½¿æœ€ç»ˆçš„è®­ç»ƒæ•°æ®ï¼ˆå³é€šè¿‡ä¸»åŠ¨å­¦ä¹ è·å¾—çš„çœŸå®æ ‡ç­¾å’Œä¼ªæ ‡ç­¾ï¼‰åœ¨ç±»åˆ«ä¸Šæ›´åŠ å¹³è¡¡ï¼Œä»è€Œè®­ç»ƒå‡ºæ›´å¥½çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„ŸçŸ¥çš„è½¯ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œä»¥æé«˜ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17849v1">PDF</a> Accepted to ICIP2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºä¸»åŠ¨å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹ä¸­çš„ç›’çº§é‡‡æ ·ç­–ç•¥ã€‚æ–‡ç« æŒ‡å‡ºï¼Œåœ¨æ—©æœŸçš„æ¨¡å‹è®­ç»ƒä¸­ï¼Œæ¨¡å‹å¯¹ä¸»è¦ç±»åˆ«çš„è¡¨ç°è¾ƒå¥½ï¼Œå¯¼è‡´ä¼ªæ ‡ç­¾å‡ºç°ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ç±»å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œæ—¨åœ¨é€‰æ‹©æ›´å¤šçš„å°‘æ•°ç±»åˆ«ç›®æ ‡è¿›è¡Œæ ‡æ³¨ï¼Œä½¿å¾—æœ€ç»ˆçš„è®­ç»ƒæ•°æ®æ›´ä¸ºå¹³è¡¡ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä»»åŠ¡æ„ŸçŸ¥çš„è½¯ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œä»¥æé«˜ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹éœ€è¦æ˜‚è´µçš„è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰å¯å‡è½»æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>åœ¨ç›®æ ‡æ£€æµ‹çš„ç›’çº§ä¸»åŠ¨å­¦ä¹ ä¸­ï¼Œæ—©æœŸæ¨¡å‹å¯¹ä¸»è¦ç±»åˆ«çš„è¡¨ç°è¾ƒå¥½ï¼Œå¯¼è‡´ä¼ªæ ‡ç­¾å‡ºç°ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>æå‡ºç±»å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œé€‰æ‹©æ›´å¤šçš„å°‘æ•°ç±»åˆ«ç›®æ ‡è¿›è¡Œæ ‡æ³¨ï¼Œä½¿è®­ç»ƒæ•°æ®æ›´ä¸ºå¹³è¡¡ã€‚</li>
<li>æå‡ºä»»åŠ¡æ„ŸçŸ¥çš„è½¯ä¼ªæ ‡ç­¾ç­–ç•¥ï¼Œæé«˜ä¼ªæ ‡ç­¾çš„å‡†ç¡®æ€§ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17849v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17849v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17849v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17849v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17849v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Spatial-Temporal-Human-Object-Interaction-Detection"><a href="#Spatial-Temporal-Human-Object-Interaction-Detection" class="headerlink" title="Spatial-Temporal Human-Object Interaction Detection"></a>Spatial-Temporal Human-Object Interaction Detection</h2><p><strong>Authors:Xu Sun, Yunqing He, Tongwei Ren, Gangshan Wu</strong></p>
<p>In this paper, we propose a new instance-level human-object interaction detection task on videos called ST-HOID, which aims to distinguish fine-grained human-object interactions (HOIs) and the trajectories of subjects and objects. It is motivated by the fact that HOI is crucial for human-centric video content understanding. To solve ST-HOID, we propose a novel method consisting of an object trajectory detection module and an interaction reasoning module. Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct extensive experiments to evaluate the effectiveness of our method. The experimental results demonstrate that our method outperforms the baselines generated by the state-of-the-art methods of image human-object interaction detection, video visual relation detection and video human-object interaction recognition. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘å®ä¾‹çº§äºº-ç‰©äº¤äº’æ£€æµ‹ä»»åŠ¡ï¼Œç§°ä¸ºST-HOIDã€‚è¯¥ä»»åŠ¡æ—¨åœ¨åŒºåˆ†ç²¾ç»†çš„äºº-ç‰©äº¤äº’ï¼ˆHOIï¼‰ä»¥åŠä¸»ä½“å’Œç‰©ä½“çš„è½¨è¿¹ã€‚å…¶åŠ¨æœºåœ¨äºï¼Œäºº-ç‰©äº¤äº’å¯¹äºä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘å†…å®¹ç†è§£è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³ST-HOIDï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰©ä½“è½¨è¿¹æ£€æµ‹æ¨¡å—å’Œäº¤äº’æ¨ç†æ¨¡å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ç”¨äºST-HOIDè¯„ä¼°çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†VidOR-HOIDï¼Œå…¶ä¸­åŒ…å«10831ä¸ªæ—¶ç©ºäºº-ç‰©äº¤äº’å®ä¾‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒæ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒäºº-ç‰©äº¤äº’æ£€æµ‹ã€è§†é¢‘è§†è§‰å…³ç³»æ£€æµ‹å’Œè§†é¢‘äºº-ç‰©äº¤äº’è¯†åˆ«ç­‰ç°æœ‰æŠ€æœ¯çš„åŸºç¡€ä¸Šå–å¾—äº†ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘å®ä¾‹çº§äººæœºäº¤äº’æ£€æµ‹ä»»åŠ¡â€”â€”ST-HOIDï¼Œæ—¨åœ¨ç²¾ç»†åŒºåˆ†è§†é¢‘ä¸­çš„äººæœºäº¤äº’ï¼ˆHOIï¼‰ä»¥åŠä¸»ä½“å’Œå¯¹è±¡çš„è½¨è¿¹ã€‚ä¸ºè§£å†³ST-HOIDï¼Œæå‡ºäº†ä¸€ç§ç”±ç›®æ ‡è½¨è¿¹æ£€æµ‹æ¨¡å—å’Œäº¤äº’æ¨ç†æ¨¡å—ç»„æˆçš„æ–°æ–¹æ³•ã€‚åŒæ—¶æ„å»ºäº†é¦–ä¸ªç”¨äºST-HOIDè¯„ä¼°çš„VidOR-HOIDæ•°æ®é›†ï¼ŒåŒ…å«10,831ä¸ªæ—¶ç©ºäººæœºäº¤äº’å®ä¾‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå›¾åƒäººæœºäº¤äº’æ£€æµ‹ã€è§†é¢‘è§†è§‰å…³ç³»æ£€æµ‹å’Œè§†é¢‘äººæœºäº¤äº’è¯†åˆ«çš„æœ€æ–°åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘å®ä¾‹çº§äººæœºäº¤äº’æ£€æµ‹ä»»åŠ¡â€”â€”ST-HOIDã€‚</li>
<li>ST-HOIDæ—¨åœ¨ç²¾ç»†åŒºåˆ†è§†é¢‘ä¸­çš„äººæœºäº¤äº’ä»¥åŠä¸»ä½“å’Œå¯¹è±¡çš„è½¨è¿¹ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç”±ç›®æ ‡è½¨è¿¹æ£€æµ‹æ¨¡å—å’Œäº¤äº’æ¨ç†æ¨¡å—ç»„æˆçš„æ–°æ–¹æ³•æ¥è§£å†³ST-HOIDä»»åŠ¡ã€‚</li>
<li>æ„å»ºäº†é¦–ä¸ªç”¨äºST-HOIDè¯„ä¼°çš„VidOR-HOIDæ•°æ®é›†ã€‚</li>
<li>VidOR-HOIDæ•°æ®é›†åŒ…å«å¤§é‡çš„æ—¶ç©ºäººæœºäº¤äº’å®ä¾‹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17270v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17270v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17270v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17270v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17270v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17270v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Contrastive-Prompt-Clustering-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Contrastive Prompt Clustering for Weakly Supervised Semantic   Segmentation"></a>Contrastive Prompt Clustering for Weakly Supervised Semantic   Segmentation</h2><p><strong>Authors:Wangyu Wu, Zhenhong Chen, Xiaowen Ma, Wenqiao Zhang, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained attention for its cost-effectiveness. Most existing methods emphasize inter-class separation, often neglecting the shared semantics among related categories and lacking fine-grained discrimination. To address this, we propose Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large Language Models (LLMs) to derive category clusters that encode intrinsic inter-class relationships, and further introduces a class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation. This hierarchical design leverages clusters as coarse-grained semantic priors while preserving fine-grained boundaries, thereby reducing confusion among visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014 demonstrate that CPC surpasses existing state-of-the-art methods in WSSS. </p>
<blockquote>
<p>åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾è¿›è¡Œå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å› å…¶æˆæœ¬æ•ˆç›Šè€Œå¤‡å—å…³æ³¨ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å¼ºè°ƒç±»é—´åˆ†ç¦»ï¼Œå¾€å¾€å¿½ç•¥äº†ç›¸å…³ç±»åˆ«ä¹‹é—´çš„å…±äº«è¯­ä¹‰ï¼Œä¸”ç¼ºä¹ç²¾ç»†çš„ç²’åº¦é‰´åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹æ¯”æç¤ºèšç±»ï¼ˆCPCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„WSSSæ¡†æ¶ã€‚CPCåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ¨å¯¼ç¼–ç ç±»é—´å†…åœ¨å…³ç³»çš„ç±»åˆ«èšç±»ï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§ç±»æ„ŸçŸ¥çš„è¡¥ä¸çº§å¯¹æ¯”æŸå¤±ï¼Œä»¥åŠ å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åˆ†ç¦»ã€‚è¿™ç§å±‚æ¬¡åŒ–çš„è®¾è®¡åˆ©ç”¨èšç±»ä½œä¸ºç²—ç²’åº¦çš„è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†çš„è¾¹ç•Œï¼Œä»è€Œå‡å°‘è§†è§‰ä¸Šç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCPCè¶…è¶Šäº†ç°æœ‰çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17009v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºå¯¹æ¯”æç¤ºèšç±»ï¼ˆCPCï¼‰çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨å¯¼ç±»åˆ«èšç±»ï¼Œå¹¶å¼•å…¥ç±»æ„ŸçŸ¥è¡¥ä¸çº§å¯¹æ¯”æŸå¤±ï¼Œä»¥åŠ å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åˆ†ç¦»ã€‚CPCçš„å±‚æ¬¡è®¾è®¡åˆ©ç”¨èšç±»ä½œä¸ºç²—ç²’åº¦è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†è¾¹ç•Œï¼Œå‡å°‘è§†è§‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„æ··æ·†ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCPCåœ¨WSSSé¢†åŸŸè¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰é—®é¢˜å—åˆ°å…³æ³¨ï¼Œå› å…¶åœ¨ä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾çš„æƒ…å†µä¸‹å…·æœ‰æˆæœ¬æ•ˆç›Šã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†ç±»åˆ«é—´çš„å…±äº«è¯­ä¹‰ï¼Œç¼ºä¹ç²¾ç»†ç²’åº¦é‰´åˆ«ã€‚</li>
<li>æå‡ºæ–°çš„WSSSæ¡†æ¶â€”â€”å¯¹æ¯”æç¤ºèšç±»ï¼ˆCPCï¼‰ã€‚</li>
<li>CPCåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨å¯¼ç±»åˆ«èšç±»ï¼Œç¼–ç å†…åœ¨ç±»åˆ«é—´å…³ç³»ã€‚</li>
<li>CPCå¼•å…¥ç±»æ„ŸçŸ¥è¡¥ä¸çº§å¯¹æ¯”æŸå¤±ï¼Œä»¥åŠ å¼ºç±»å†…ä¸€è‡´æ€§å’Œç±»é—´åˆ†ç¦»ã€‚</li>
<li>CPCçš„å±‚æ¬¡è®¾è®¡åˆ©ç”¨èšç±»ä½œä¸ºç²—ç²’åº¦è¯­ä¹‰å…ˆéªŒï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17009v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2508.17009v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DriveIndia-An-Object-Detection-Dataset-for-Diverse-Indian-Traffic-Scenes"><a href="#DriveIndia-An-Object-Detection-Dataset-for-Diverse-Indian-Traffic-Scenes" class="headerlink" title="DriveIndia: An Object Detection Dataset for Diverse Indian Traffic   Scenes"></a>DriveIndia: An Object Detection Dataset for Diverse Indian Traffic   Scenes</h2><p><strong>Authors:Rishav Kumar, D. Santhosh Reddy, P. Rajalakshmi</strong></p>
<p>We introduce DriveIndia, a large-scale object detection dataset purpose-built to capture the complexity and unpredictability of Indian traffic environments. The dataset contains 66,986 high-resolution images annotated in YOLO format across 24 traffic-relevant object categories, encompassing diverse conditions such as varied weather (fog, rain), illumination changes, heterogeneous road infrastructure, and dense, mixed traffic patterns and collected over 120+ hours and covering 3,400+ kilometers across urban, rural, and highway routes. DriveIndia offers a comprehensive benchmark for real-world autonomous driving challenges. We provide baseline results using state-of-the-art YOLO family models, with the top-performing variant achieving a mAP50 of 78.7%. Designed to support research in robust, generalizable object detection under uncertain road conditions, DriveIndia will be publicly available via the TiHAN-IIT Hyderabad dataset repository <a target="_blank" rel="noopener" href="https://tihan.iith.ac.in/TiAND.html">https://tihan.iith.ac.in/TiAND.html</a> (Terrestrial Datasets -&gt; Camera Dataset). </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DriveIndiaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæ•æ‰å°åº¦äº¤é€šç¯å¢ƒçš„å¤æ‚æ€§å’Œä¸å¯é¢„æµ‹æ€§è€Œæ„å»ºçš„å¤§è§„æ¨¡ç‰©ä½“æ£€æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«66,986å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œé‡‡ç”¨YOLOæ ¼å¼è¿›è¡Œæ ‡æ³¨ï¼Œæ¶µç›–24ä¸ªä¸äº¤é€šç›¸å…³çš„ç‰©ä½“ç±»åˆ«ï¼Œæ¶‰åŠå¤šç§æ¡ä»¶ï¼Œå¦‚å¤šå˜çš„å¤©æ°”ï¼ˆé›¾ã€é›¨ï¼‰ã€å…‰ç…§å˜åŒ–ã€é“è·¯åŸºç¡€è®¾æ–½çš„å¤šæ ·æ€§ä»¥åŠå¯†é›†ã€æ··åˆçš„äº¤é€šæ¨¡å¼ï¼Œå¹¶åœ¨è¶…è¿‡120å°æ—¶çš„æ—¶é—´å†…ï¼Œåœ¨åŸå¸‚ã€ä¹¡æ‘å’Œé«˜é€Ÿå…¬è·¯è·¯çº¿ç­‰è¶…è¿‡3,400å…¬é‡Œçš„è·ç¦»å†…æ”¶é›†ã€‚DriveIndiaä¸ºç°å®ä¸–ç•Œä¸­çš„è‡ªåŠ¨é©¾é©¶æŒ‘æˆ˜æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„YOLOå®¶æ—æ¨¡å‹æä¾›åŸºçº¿ç»“æœï¼Œè¡¨ç°æœ€ä½³çš„å˜ä½“è¾¾åˆ°50%çš„mAPï¼ˆå¹³å‡å‡†ç¡®ç‡ï¼‰ã€‚è®¾è®¡ç”¨äºæ”¯æŒä¸ç¡®å®šé“è·¯æ¡ä»¶ä¸‹çš„ç¨³å¥ã€é€šç”¨ç‰©ä½“æ£€æµ‹ç ”ç©¶ï¼ŒDriveIndiaå°†é€šè¿‡TiHAN-å°åº¦ç†å·¥å­¦é™¢æµ·å¾—æ‹‰å·´æ•°æ®é›†å­˜å‚¨åº“å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://tihan.iith.ac.in/TiAND.html">https://tihan.iith.ac.in/TiAND.html</a>ï¼ˆé™†åœ°æ•°æ®é›†-&gt;ç›¸æœºæ•°æ®é›†ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19912v4">PDF</a> Accepted at ITSC 2025 Conference. Updated the Table 2 of Benchmark   Results</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä»‹ç»äº†ä¸€ä¸ªåä¸ºDriveIndiaçš„å¤§å‹ç‰©ä½“æ£€æµ‹æ•°æ®é›†ï¼Œä¸“ä¸ºæ•æ‰å°åº¦äº¤é€šç¯å¢ƒçš„å¤æ‚æ€§å’Œä¸å¯é¢„æµ‹æ€§è€Œè®¾è®¡ã€‚åŒ…å«6.6ä¸‡ä½™å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ ‡æ³¨äº†24ä¸ªä¸äº¤é€šç›¸å…³çš„å¯¹è±¡ç±»åˆ«ï¼Œæ¶µç›–å¤šç§æ¡ä»¶å¦‚ä¸åŒå¤©æ°”ã€å…‰ç…§å˜åŒ–ã€é“è·¯åŸºç¡€è®¾æ–½å¤šæ ·ä»¥åŠå¯†é›†æ··åˆçš„äº¤é€šæ¨¡å¼ã€‚è¯¥æ•°æ®é›†æä¾›YOLOæ ¼å¼çš„æ•°æ®åŸºå‡†æµ‹è¯•æˆç»©æŠ¥å‘Šã€‚æ­¤æ•°æ®é›†æä¾›äº†YOLOå®¶æ—æ¨¡å‹æœ€é«˜è¡¨ç°æ€§èƒ½çš„æ¨¡å‹ç‰ˆæœ¬å®ç°äº†ä¸€ä¸ªçº¦ä¸º78.7%çš„mAP50æŒ‡æ ‡ï¼Œå¯æ»¡è¶³ç ”ç©¶å¤æ‚æ¡ä»¶ä¸‹çš„é²æ£’é€šç”¨ç‰©ä½“æ£€æµ‹çš„éœ€è¦ã€‚å…¶æ”¯æŒåº”å¯¹ä¸ç¡®å®šè·¯å†µçš„ç ”ç©¶å’Œä¸€èˆ¬çš„ç‰©ä½“æ£€æµ‹éœ€æ±‚ï¼Œå¯å…¬å¼€è®¿é—®çš„æ•°æ®åº“é“¾æ¥æ˜¯ï¼š<a target="_blank" rel="noopener" href="https://tihan.iith.ac.in/TiAND.html%E3%80%82%E8%BF%99%E6%98%AF%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9A%84%E4%B8%80%E5%A4%A7%E7%AA%81%E7%A0%B4%EF%BC%8C%E5%80%BC%E5%BE%97%E5%B9%BF%E6%B3%9B%E5%85%B3%E6%B3%A8%E5%92%8C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%E3%80%82%E9%A9%B1%E5%8A%A8%E5%8D%B0%E5%BA%A6%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%BB%BA%E7%AB%8B%E6%98%AF%E4%B8%80%E4%B8%AA%E5%80%BC%E5%BE%97%E5%80%9F%E9%89%B4%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%8C%E4%B8%BA%E6%88%91%E4%BB%AC%E5%B1%95%E7%A4%BA%E4%BA%86%E5%9C%A8%E5%A4%9A%E6%A0%B7%E5%8C%96%E7%8E%B0%E5%AE%9E%E7%8E%AF%E5%A2%83%E4%B8%AD%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E8%87%AA%E5%8A%A8%E8%A1%8C%E9%A9%B6%E6%96%B9%E6%A1%88%E7%9A%84%E5%85%B3%E9%94%AE%E6%96%B9%E6%B3%95%E5%92%8C%E6%8A%80%E6%9C%AF%E5%8F%AF%E8%83%BD%E4%B9%8B%E4%B8%80%E3%80%82%E5%85%B6%E4%B8%BB%E8%A6%81%E7%9B%AE%E7%9A%84%E5%B0%B1%E6%98%AF%E8%BE%85%E5%8A%A9%E5%85%AC%E4%BC%97%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91%E5%AF%B9%E4%BA%8E%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E7%9A%84%E9%9C%80%E6%B1%82%E5%8F%98%E5%8C%96%E8%B5%B7%E5%88%B0%E4%BA%86%E5%85%B3%E9%94%AE%E7%9A%84%E5%BC%95%E5%AF%BC%E4%BD%9C%E7%94%A8%E3%80%82%E8%AE%A9%E6%88%91%E4%BB%AC%E5%AF%B9%E6%AD%A4%E5%BA%94%E7%94%A8%E6%9B%B4%E4%B8%BA%E6%B7%B1%E5%85%A5%E7%9A%84%E6%8C%96%E6%8E%98%E5%92%8C%E6%94%B9%E8%BF%9B%E5%85%B6%E5%8A%9F%E8%83%BD%E6%9D%A5%E6%BB%A1%E8%B6%B3%E7%94%A8%E6%88%B7%E7%9A%84%E9%9C%80%E6%B1%82%E5%92%8C%E5%B8%82%E5%9C%BA%E7%8E%AF%E5%A2%83%E7%9A%84%E5%A4%9A%E5%8F%98%E6%80%A7%E9%97%AE%E9%A2%98%E4%BB%A5%E4%BD%93%E7%8E%B0%E5%85%B6%E5%8A%9F%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7%E4%B8%8D%E6%96%AD%E6%8F%90%E5%8D%87%E7%9A%84%E7%8B%AC%E7%89%B9%E4%BD%9C%E7%94%A8%E5%92%8C%E5%BA%94%E7%94%A8%E4%BB%B7%E5%80%BC%E7%9A%84%E6%8E%A8%E5%B9%BF%E5%92%8C%E5%AE%8C%E5%96%84%E7%8E%B0%E6%9C%89%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BA%A7%E5%93%81%E4%BB%A5%E8%BE%BE%E5%88%B0%E6%99%BA%E8%83%BD%E5%8C%96%E7%9A%84%E7%8E%B0%E5%AE%9E%E7%94%9F%E6%B4%BB%E6%89%80%E6%8B%A5%E6%9C%89%E4%BA%A7%E5%93%81%E6%9C%8D%E5%8A%A1%E4%BB%B7%E5%80%BC%E7%9A%84%E7%9B%AE%E6%A0%87%E3%80%82%E8%BF%99%E6%98%AF%E4%B8%80%E9%A1%B9%E5%BC%80%E5%88%9B%E6%80%A7%E7%9A%84%E5%B7%A5%E4%BD%9C%EF%BC%8C%E5%AF%B9%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%8A%80%E6%9C%AF%E7%9A%84%E5%8F%91%E5%B1%95%E5%85%B7%E6%9C%89%E9%87%8D%E8%A6%81%E5%BD%B1%E5%93%8D%E3%80%82%E5%AE%83%E6%9C%89%E5%8A%A9%E4%BA%8E%E6%8E%A8%E5%8A%A8%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%8A%80%E6%9C%AF%E5%9C%A8%E5%A4%8D%E6%9D%82%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E8%90%BD%E5%9C%B0%E5%BA%94%E7%94%A8%EF%BC%8C%E5%AF%B9%E4%BA%8E%E6%8F%90%E9%AB%98%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%E5%92%8C%E5%AE%89%E5%85%A8%E6%80%A7%E5%85%B7%E6%9C%89%E9%87%8D%E8%A6%81%E6%84%8F%E4%B9%89%E3%80%82">https://tihan.iith.ac.in/TiAND.htmlã€‚è¿™æ˜¯å®ç°è‡ªåŠ¨é©¾é©¶çš„ä¸€å¤§çªç ´ï¼Œå€¼å¾—å¹¿æ³›å…³æ³¨å’Œæ·±å…¥æ¢ç´¢ã€‚é©±åŠ¨å°åº¦é¡¹ç›®çš„å»ºç«‹æ˜¯ä¸€ä¸ªå€¼å¾—å€Ÿé‰´çš„ä¾‹å­ï¼Œä¸ºæˆ‘ä»¬å±•ç¤ºäº†åœ¨å¤šæ ·åŒ–ç°å®ç¯å¢ƒä¸­å®ç°é«˜æ€§èƒ½è‡ªåŠ¨è¡Œé©¶æ–¹æ¡ˆçš„å…³é”®æ–¹æ³•å’ŒæŠ€æœ¯å¯èƒ½ä¹‹ä¸€ã€‚å…¶ä¸»è¦ç›®çš„å°±æ˜¯è¾…åŠ©å…¬ä¼—æ·±å…¥ç†è§£ç‰©è”ç½‘å¯¹äºå®é™…åº”ç”¨çš„éœ€æ±‚å˜åŒ–èµ·åˆ°äº†å…³é”®çš„å¼•å¯¼ä½œç”¨ã€‚è®©æˆ‘ä»¬å¯¹æ­¤åº”ç”¨æ›´ä¸ºæ·±å…¥çš„æŒ–æ˜å’Œæ”¹è¿›å…¶åŠŸèƒ½æ¥æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚å’Œå¸‚åœºç¯å¢ƒçš„å¤šå˜æ€§é—®é¢˜ä»¥ä½“ç°å…¶åŠŸèƒ½ç¨³å®šæ€§ä¸æ–­æå‡çš„ç‹¬ç‰¹ä½œç”¨å’Œåº”ç”¨ä»·å€¼çš„æ¨å¹¿å’Œå®Œå–„ç°æœ‰çš„ç›¸å…³äº§å“ä»¥è¾¾åˆ°æ™ºèƒ½åŒ–çš„ç°å®ç”Ÿæ´»æ‰€æ‹¥æœ‰äº§å“æœåŠ¡ä»·å€¼çš„ç›®æ ‡ã€‚è¿™æ˜¯ä¸€é¡¹å¼€åˆ›æ€§çš„å·¥ä½œï¼Œå¯¹è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦å½±å“ã€‚å®ƒæœ‰åŠ©äºæ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è½åœ°åº”ç”¨ï¼Œå¯¹äºæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é²æ£’æ€§å’Œå®‰å…¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>DriveIndiaæ˜¯ä¸€ä¸ªé’ˆå¯¹å°åº¦äº¤é€šç¯å¢ƒçš„å¤§å‹ç‰©ä½“æ£€æµ‹æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡6ä¸‡å¼ é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ ‡æ³¨äº†å¤šç§äº¤é€šç›¸å…³å¯¹è±¡ç±»åˆ«ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤šç§æ¡ä»¶ï¼ŒåŒ…æ‹¬ä¸åŒå¤©æ°”ã€å…‰ç…§å˜åŒ–ã€é“è·¯åŸºç¡€è®¾æ–½å¤šæ ·æ€§å’Œå¯†é›†æ··åˆçš„äº¤é€šæ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.19912v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.19912v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.19912v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.19912v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.19912v4/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.19912v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Transferring-Styles-for-Reduced-Texture-Bias-and-Improved-Robustness-in-Semantic-Segmentation-Networks"><a href="#Transferring-Styles-for-Reduced-Texture-Bias-and-Improved-Robustness-in-Semantic-Segmentation-Networks" class="headerlink" title="Transferring Styles for Reduced Texture Bias and Improved Robustness in   Semantic Segmentation Networks"></a>Transferring Styles for Reduced Texture Bias and Improved Robustness in   Semantic Segmentation Networks</h2><p><strong>Authors:Ben Hamscher, Edgar Heinert, Annika MÃ¼tze, Kira Maag, Matthias Rottmann</strong></p>
<p>Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„å½¢çŠ¶å’Œçº¹ç†åè§ï¼Œè¿™äº›åè§ä¼šå½±å“å…¶æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸å¸¸è§„DNNè®­ç»ƒç›¸æ¯”ï¼Œä½¿ç”¨é£æ ¼åŒ–å›¾åƒè¿›è¡Œè®­ç»ƒå‡å°‘äº†å›¾åƒåˆ†ç±»ä¸­çš„çº¹ç†åè§ï¼Œå¹¶æé«˜äº†å¯¹å›¾åƒè…èš€çš„ç¨³å¥æ€§ã€‚ä¸ºäº†æ¨è¿›è¿™ä¸€ç ”ç©¶é¢†åŸŸï¼Œæˆ‘ä»¬ç ”ç©¶äº†é£æ ¼è½¬æ¢æ˜¯å¦ä¹Ÿèƒ½åœ¨è¯­ä¹‰åˆ†å‰²ä¸­äº§ç”Ÿè¿™ä¸¤ç§æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨äººå·¥å›¾åƒåŒºåŸŸä¹‹é—´æ‰§è¡Œé£æ ¼è½¬æ¢ã€‚è¿™äº›éšæœºåŒºåŸŸç”±é€‰å®šæ•°é‡çš„Voronoiç»†èƒç»„æˆã€‚ç„¶åå°†ç”Ÿæˆçš„é£æ ¼è½¬æ¢æ•°æ®ç”¨äºè®­ç»ƒè¯­ä¹‰åˆ†å‰²DNNï¼Œç›®çš„æ˜¯å‡å°‘å…¶å¯¹çº¹ç†çº¿ç´¢çš„ä¾èµ–ï¼ŒåŒæ—¶å¢å¼ºå…¶å¯¹åŸºäºå½¢çŠ¶ç‰¹å¾çš„ä¾èµ–ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œäº‹å®è¯æ˜ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²ä¸­ï¼Œé£æ ¼è½¬æ¢å¢å¼ºå‡å°‘äº†çº¹ç†åè§ï¼Œå¹¶å¤§å¤§æé«˜äº†å¯¹å¸¸è§å›¾åƒè…èš€ä»¥åŠå¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™äº›è§‚å¯Ÿç»“æœåœ¨åŸå¸‚æ™¯è§‚æ•°æ®é›†å’ŒPASCAL Contextæ•°æ®é›†ä¸Šçš„å·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¶æ„ä¸­éƒ½å¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾ç¤ºäº†æ‰€æå‡ºæ–¹æ³•çš„æ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10239v2">PDF</a> accepted at ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„å½¢çŠ¶å’Œçº¹ç†åè§å½±å“å…¶æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸å¸¸è§„DNNè®­ç»ƒç›¸æ¯”ï¼Œä½¿ç”¨é£æ ¼åŒ–å›¾åƒè¿›è¡Œè®­ç»ƒå‡å°‘äº†å›¾åƒåˆ†ç±»ä¸­çš„çº¹ç†åè§ï¼Œå¹¶æé«˜äº†å¯¹å›¾åƒè…èš€çš„ç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†é£æ ¼è½¬ç§»æ˜¯å¦ä¹Ÿèƒ½åœ¨è¯­ä¹‰åˆ†å‰²ä¸­äº§ç”Ÿè¿™ä¸¤ç§æ•ˆæœã€‚é€šè¿‡åœ¨ä¸åŒçš„äººå·¥å›¾åƒåŒºåŸŸè¿›è¡Œé£æ ¼è½¬ç§»ï¼Œå¹¶ä½¿ç”¨é£æ ¼è½¬ç§»åçš„æ•°æ®è®­ç»ƒè¯­ä¹‰åˆ†å‰²DNNï¼Œæ—¨åœ¨å‡å°‘å…¶å¯¹çº¹ç†çº¿ç´¢çš„ä¾èµ–ï¼ŒåŒæ—¶æé«˜å…¶å¯¹åŸºäºå½¢çŠ¶çš„ç‰¹å¾çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²ä¸­ï¼Œé£æ ¼è½¬ç§»å¢å¼ºå‡å°‘äº†çº¹ç†åè§ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å¯¹å¸¸è§å›¾åƒè…èš€ä»¥åŠå¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™äº›è§‚å¯Ÿç»“æœåœ¨åŸå¸‚æ™¯è§‚æ•°æ®é›†å’ŒPASCAL Contextæ•°æ®é›†ä¸Šçš„å·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¶æ„ä¸­éƒ½å¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾ç¤ºäº†è¯¥æ–¹æ³•çš„æ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DNNåœ¨å›¾åƒåˆ†ç±»ä¸­å­˜åœ¨å½¢çŠ¶å’Œçº¹ç†åè§ï¼Œå½±å“æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</li>
<li>ä¸å¸¸è§„DNNè®­ç»ƒç›¸æ¯”ï¼Œä½¿ç”¨é£æ ¼åŒ–å›¾åƒè®­ç»ƒå¯æé«˜å›¾åƒåˆ†ç±»çš„ç¨³å¥æ€§ã€‚</li>
<li>é£æ ¼è½¬ç§»è¢«åº”ç”¨äºè¯­ä¹‰åˆ†å‰²ï¼Œä»¥å‡å°‘å¯¹çº¹ç†çº¿ç´¢çš„ä¾èµ–å¹¶æé«˜å¯¹å½¢çŠ¶ç‰¹å¾çš„ä¾èµ–ã€‚</li>
<li>é£æ ¼è½¬ç§»å¢å¼ºåœ¨è¯­ä¹‰åˆ†å‰²ä¸­å‡å°‘äº†çº¹ç†åè§ã€‚</li>
<li>é£æ ¼è½¬ç§»å¢å¼ºæé«˜äº†è¯­ä¹‰åˆ†å‰²æ¨¡å‹å¯¹å¸¸è§å›¾åƒè…èš€å’Œå¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœåœ¨åŸå¸‚æ™¯è§‚æ•°æ®é›†å’ŒPASCAL Contextæ•°æ®é›†ä¸Šçš„å·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¶æ„ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.10239v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.10239v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.10239v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.10239v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NOCTIS-Novel-Object-Cyclic-Threshold-based-Instance-Segmentation"><a href="#NOCTIS-Novel-Object-Cyclic-Threshold-based-Instance-Segmentation" class="headerlink" title="NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation"></a>NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</h2><p><strong>Authors:Max Gandyra, Alessandro Santonicola, Michael Beetz</strong></p>
<p>Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed for all kinds of novel objects without (re-) training has proven to be a difficult task. To handle this, we present a new training-free framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models: Grounded-SAM 2 for object proposals with precise bounding boxes and corresponding segmentation masks; and DINOv2 for robust class and patch embeddings, due to its zero-shot capabilities. Internally, the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings with a new cyclic thresholding (CT) mechanism that mitigates unstable matches caused by repetitive textures or visually similar patterns. Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by object selection bias; (ii) the usage of the average confidence of the proposals bounding box and mask as a scoring component; and (iii) an RGB-only pipeline that performs even better than RGB-D ones. We empirically show that NOCTIS, without further training&#x2F;fine tuning, attains state-of-the-art results regarding the mean AP score, w.r.t. the best RGB and RGB-D methods on the seven core datasets of the BOP 2023 challenge for the â€œModel-based 2D segmentation of unseen objectsâ€ task. </p>
<blockquote>
<p>åœ¨RGBå›¾åƒä¸­å¯¹æ–°å‹ç‰©ä½“å®ä¾‹è¿›è¡Œå®ä¾‹åˆ†å‰²ï¼Œç»™å®šæ¯ä¸ªç‰©ä½“çš„ç¤ºä¾‹å›¾åƒï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªçŸ¥åé—®é¢˜ã€‚è®¾è®¡ä¸€ç§è¶³å¤Ÿé€šç”¨çš„æ¨¡å‹ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æ–°å‹ç‰©ä½“è€Œæ— éœ€ï¼ˆé‡æ–°ï¼‰è®­ç»ƒï¼Œå·²è¢«è¯æ˜æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºï¼šåŸºäºå¾ªç¯é˜ˆå€¼çš„æ–°å‹ç‰©ä½“å®ä¾‹åˆ†å‰²ï¼ˆNOCTISï¼‰ã€‚NOCTISé›†æˆäº†ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼šç”¨äºç”Ÿæˆç²¾ç¡®è¾¹ç•Œæ¡†å’Œç›¸åº”åˆ†å‰²æ©ç çš„Grounded-SAM 2ï¼›ä»¥åŠå…·æœ‰é›¶æ ·æœ¬èƒ½åŠ›çš„DINOv2ï¼Œç”¨äºç”Ÿæˆç¨³å¥çš„ç±»åˆ«å’Œè¡¥ä¸åµŒå…¥ã€‚åœ¨å†…éƒ¨ï¼Œé€šè¿‡åŸºäºç±»åˆ«åµŒå…¥çš„ç›¸ä¼¼æ€§ä»¥åŠä¸è¡¥ä¸åµŒå…¥çš„å¹³å‡æœ€å¤§ç›¸ä¼¼æ€§çš„å¯¹è±¡åŒ¹é…åˆ†æ•°æ¥å®ç°ææ¡ˆå¯¹è±¡åŒ¹é…ï¼Œé‡‡ç”¨æ–°çš„å¾ªç¯é˜ˆå€¼ï¼ˆCTï¼‰æœºåˆ¶æ¥ç¼“è§£ç”±é‡å¤çº¹ç†æˆ–è§†è§‰ç›¸ä¼¼æ¨¡å¼å¼•èµ·çš„ä¸ç¨³å®šåŒ¹é…ã€‚é™¤äº†CTï¼ŒNOCTISè¿˜å¼•å…¥äº†ï¼šï¼ˆiï¼‰ä¸€ç§ä¸å—å¯¹è±¡é€‰æ‹©åè§å½±å“çš„å¤–è§‚åˆ†æ•°ï¼›ï¼ˆiiï¼‰ä½¿ç”¨ææ¡ˆè¾¹ç•Œæ¡†å’Œæ©ç çš„å¹³å‡ç½®ä¿¡åº¦ä½œä¸ºè¯„åˆ†ç»„ä»¶ï¼›ï¼ˆiiiï¼‰ä»…ä½¿ç”¨RGBçš„ç®¡é“ï¼Œå…¶æ€§èƒ½ç”šè‡³ä¼˜äºRGB-Dç®¡é“ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼ŒNOCTISæ— éœ€è¿›ä¸€æ­¥çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œå³å¯åœ¨BOP 2023æŒ‘æˆ˜çš„ä¸ƒä¸ªæ ¸å¿ƒæ•°æ®é›†ä¸Šï¼Œé’ˆå¯¹â€œæœªè§ç‰©ä½“çš„åŸºäºæ¨¡å‹çš„äºŒç»´åˆ†å‰²â€ä»»åŠ¡ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„å¹³å‡å‡†ç¡®ç‡ï¼ˆmean AP scoreï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01463v2">PDF</a> 10 pages, 3 figures, 5 tables, ICLR 2026 preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹ç‰©ä½“å®ä¾‹åˆ†å‰²æ¡†æ¶â€”â€”NOCTISã€‚å®ƒé€šè¿‡é›†æˆGrounded-SAM 2å’ŒDINOv2ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ©ç”¨å¾ªç¯é˜ˆå€¼æœºåˆ¶å®ç°å¯¹è±¡ææ¡ˆåŒ¹é…ï¼Œå¼•å…¥å¤–è§‚è¯„åˆ†å¹¶ä¼˜åŒ–è¯„åˆ†ç»„ä»¶ï¼Œä»è€Œåœ¨ä»…ä½¿ç”¨RGBå›¾åƒçš„æƒ…å†µä¸‹è¾¾åˆ°ç”šè‡³è¶…è¶Šäº†RGB-Dæ–¹æ³•çš„æ•ˆæœã€‚åœ¨BOP 2023æŒ‘æˆ˜çš„ä¸ƒä¸ªæ ¸å¿ƒæ•°æ®é›†ä¸Šï¼ŒNOCTISåœ¨â€œæœªè§ç‰©ä½“çš„æ¨¡å‹åŸºç¡€äºŒç»´åˆ†å‰²â€ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¹³å‡ç²¾åº¦ï¼ˆmean AP scoreï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NOCTISæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹ç‰©ä½“å®ä¾‹åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>é›†æˆGrounded-SAM 2å’ŒDINOv2ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹å®ç°ç²¾å‡†çš„å¯¹è±¡ææ¡ˆåŒ¹é…ã€‚</li>
<li>é€šè¿‡å¾ªç¯é˜ˆå€¼æœºåˆ¶ï¼ˆCTï¼‰å¤„ç†å› é‡å¤çº¹ç†æˆ–ç›¸ä¼¼æ¨¡å¼å¯¼è‡´çš„ä¸ç¨³å®šåŒ¹é…é—®é¢˜ã€‚</li>
<li>å¼•å…¥å¤–è§‚è¯„åˆ†ï¼Œä¸å—å¯¹è±¡é€‰æ‹©åè§çš„å½±å“ã€‚</li>
<li>ä½¿ç”¨ææ¡ˆçš„è¾¹ç•Œæ¡†å’Œé®ç½©çš„å¹³å‡ç½®ä¿¡åº¦ä½œä¸ºè¯„åˆ†ç»„ä»¶ã€‚</li>
<li>NOCTISåœ¨ä»…ä½¿ç”¨RGBå›¾åƒçš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶ŠRGB-Dæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.01463v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.01463v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2507.01463v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Egocentric-Human-Object-Interaction-Detection-A-New-Benchmark-and-Method"><a href="#Egocentric-Human-Object-Interaction-Detection-A-New-Benchmark-and-Method" class="headerlink" title="Egocentric Human-Object Interaction Detection: A New Benchmark and   Method"></a>Egocentric Human-Object Interaction Detection: A New Benchmark and   Method</h2><p><strong>Authors:Kunyuan Deng, Yi Wang, Lap-Pui Chau</strong></p>
<p>Egocentric human-object interaction (Ego-HOI) detection is crucial for intelligent agents to understand and assist human activities from a first-person perspective. However, progress has been hindered by the lack of benchmarks and methods tailored to egocentric challenges such as severe hand-object occlusion. In this paper, we introduce the real-world Ego-HOI detection task and the accompanying Ego-HOIBench, a new dataset with over 27K egocentric images and explicit, fine-grained hand-verb-object triplet annotations across 123 categories. Ego-HOIBench covers diverse daily scenarios, object types, and both single- and two-hand interactions, offering a comprehensive testbed for Ego-HOI research. Benchmarking existing third-person HOI detectors on Ego-HOIBench reveals significant performance gaps, highlighting the need for egocentric-specific solutions. To this end, we propose Hand Geometry and Interactivity Refinement (HGIR), a lightweight, plug-and-play scheme that leverages hand pose and geometric cues to enhance interaction representations. Specifically, HGIR explicitly extracts global hand geometric features from the estimated hand pose proposals, and further refines interaction features through pose-interaction attention, enabling the model to focus on subtle hand-object relationship differences even under severe occlusion. HGIR significantly improves Ego-HOI detection performance across multiple baselines, achieving new state-of-the-art results on Ego-HOIBench. Our dataset and method establish a solid foundation for future research in egocentric vision and human-object interaction understanding. Project page: <a target="_blank" rel="noopener" href="https://dengkunyuan.github.io/EgoHOIBench/">https://dengkunyuan.github.io/EgoHOIBench/</a> </p>
<blockquote>
<p>ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¸ç‰©ä½“äº¤äº’ï¼ˆEgo-HOIï¼‰æ£€æµ‹å¯¹äºæ™ºèƒ½ä¸»ä½“ä»ç¬¬ä¸€äººç§°è§†è§’ç†è§£å’Œè¾…åŠ©äººç±»æ´»åŠ¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒæŒ‘æˆ˜çš„åŸºå‡†å’Œæ–¹æ³•ï¼Œå¦‚æ‰‹éƒ¨ä¸ç‰©ä½“çš„ä¸¥é‡é®æŒ¡ç­‰ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é˜»ç¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç°å®ä¸–ç•Œä¸­çš„Ego-HOIæ£€æµ‹ä»»åŠ¡ä»¥åŠé…å¥—ä½¿ç”¨çš„Ego-HOIBenchæ•°æ®é›†ã€‚Ego-HOIBenchåŒ…å«è¶…è¿‡2.7ä¸‡å¼ ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾ç‰‡ï¼ŒåŒ…å«æ‰‹ã€åŠ¨è¯å’Œå¯¹è±¡ä¸‰æ–¹é¢çš„è¯¦ç»†æ ‡æ³¨ä¿¡æ¯ï¼Œæ¶‰åŠæ—¥å¸¸åœºæ™¯å’Œå¤šç§ç±»å‹çš„ç‰©ä½“åŠäº¤äº’åœºæ™¯ã€‚å®ƒä¸ºEgo-HOIç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æµ‹è¯•å¹³å°ã€‚å¯¹ç°æœ‰ç¬¬ä¸‰äººç§°HOIæ£€æµ‹å™¨åœ¨Ego-HOIBenchä¸Šçš„åŸºå‡†æµ‹è¯•è¡¨æ˜å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œè¿™çªæ˜¾äº†å¯¹ç‰¹å®šäºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒè§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰‹å‡ ä½•å’Œäº¤äº’ç»†åŒ–ï¼ˆHGIRï¼‰ç½‘ç»œæ¡†æ¶æ˜¯ä¸€ç§è½»é‡çº§ä¸”å³æ’å³ç”¨çš„æ–¹æ¡ˆï¼Œåˆ©ç”¨æ‰‹åŠ¿å’Œå‡ ä½•çº¿ç´¢å¢å¼ºäº¤äº’è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒHGIRä»ä¼°è®¡çš„æ‰‹éƒ¨å§¿æ€ææ¡ˆä¸­æ˜ç¡®æå–å…¨å±€æ‰‹éƒ¨å‡ ä½•ç‰¹å¾ï¼Œå¹¶é€šè¿‡å§¿æ€äº¤äº’æ³¨æ„åŠ›è¿›ä¸€æ­¥ç»†åŒ–äº¤äº’ç‰¹å¾ï¼Œä½¿æ¨¡å‹å³ä½¿åœ¨ä¸¥é‡é®æŒ¡ä¸‹ä¹Ÿèƒ½å…³æ³¨æ‰‹éƒ¨ä¸ç‰©ä½“ä¹‹é—´å¾®å¦™çš„å…³ç³»å·®å¼‚ã€‚HGIRæ˜¾è‘—æé«˜äº†å¤šä¸ªåŸºçº¿ä»»åŠ¡çš„Ego-HOIæ£€æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨Ego-HOIBenchä¸Šå–å¾—äº†æœ€æ–°çš„æœ€ä½³ç»“æœã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œæ–¹æ³•ä¸ºæœªæ¥çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰å’Œäººä¸ç‰©ä½“äº¤äº’ç†è§£ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://dengkunyuan.github.io/EgoHOIBench/">https://dengkunyuan.github.io/EgoHOIBench/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14189v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘ç¬¬ä¸€äººç§°è§†è§’çš„ä»¥äººä¸ºä¸­å¿ƒçš„äººä¸ç‰©ä½“äº¤äº’ï¼ˆEgo-HOIï¼‰æ£€æµ‹ä»»åŠ¡çš„é‡è¦æ€§ï¼Œå¹¶é’ˆå¯¹ç¼ºä¹é’ˆå¯¹ç¬¬ä¸€äººç§°è§†è§’çš„æŒ‘æˆ˜æ€§æ•°æ®é›†çš„é—®é¢˜ï¼Œæ¨å‡ºäº†æ–°çš„æ•°æ®é›†Ego-HOIBenchã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡27Kå¼ ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾åƒå’Œè¯¦ç»†çš„ç²¾ç»†æ ‡æ³¨çš„æ‰‹-åŠ¨è¯-ç‰©ä½“ä¸‰å…ƒç»„æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹Ego-HOIæ£€æµ‹ä»»åŠ¡çš„æ–°æ–¹æ³•Hand Geometry and Interactivity Refinementï¼ˆHGIRï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰‹åŠ¿å’Œå‡ ä½•çº¿ç´¢æ¥å¢å¼ºäº¤äº’è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜Ego-HOIæ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»‹ç»äº†é¢å‘ç¬¬ä¸€äººç§°è§†è§’çš„ä»¥äººä¸ºä¸­å¿ƒçš„äººä¸ç‰©ä½“äº¤äº’ï¼ˆEgo-HOIï¼‰æ£€æµ‹ä»»åŠ¡çš„é‡è¦æ€§ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹ç¬¬ä¸€äººç§°è§†è§’æŒ‘æˆ˜çš„åŸºå‡†æ•°æ®é›†æ˜¯ç ”ç©¶çš„ç“¶é¢ˆã€‚</li>
<li>æ¨å‡ºäº†æ–°çš„æ•°æ®é›†Ego-HOIBenchï¼ŒåŒ…å«è¶…è¿‡27Kå¼ ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å›¾åƒå’Œè¯¦ç»†çš„ç²¾ç»†æ ‡æ³¨ã€‚</li>
<li>ç°æœ‰ç¬¬ä¸‰äººç§°HOIæ£€æµ‹å™¨åœ¨Ego-HOIBenchä¸Šçš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œéœ€è¦é’ˆå¯¹ç¬¬ä¸€äººç§°è§†è§’çš„ç‰¹å®šè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºäº†Hand Geometry and Interactivity Refinementï¼ˆHGIRï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ‰‹åŠ¿å’Œå‡ ä½•çº¿ç´¢å¢å¼ºäº¤äº’è¡¨ç¤ºã€‚</li>
<li>HGIRé€šè¿‡æå–æ‰‹éƒ¨å…¨å±€å‡ ä½•ç‰¹å¾å’Œå§¿åŠ¿äº¤äº’æ³¨æ„åŠ›ï¼Œå³ä½¿åœ¨ä¸¥é‡é®æŒ¡ä¸‹ä¹Ÿèƒ½å…³æ³¨æ‰‹éƒ¨ä¸ç‰©ä½“çš„å¾®å¦™å…³ç³»å·®å¼‚ã€‚</li>
<li>HGIRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†Ego-HOIæ£€æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨Ego-HOIBenchä¸Šå–å¾—äº†æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.14189v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.14189v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.14189v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.14189v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.14189v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CarboFormer-A-Lightweight-Semantic-Segmentation-Architecture-for-Efficient-Carbon-Dioxide-Detection-Using-Optical-Gas-Imaging"><a href="#CarboFormer-A-Lightweight-Semantic-Segmentation-Architecture-for-Efficient-Carbon-Dioxide-Detection-Using-Optical-Gas-Imaging" class="headerlink" title="CarboFormer: A Lightweight Semantic Segmentation Architecture for   Efficient Carbon Dioxide Detection Using Optical Gas Imaging"></a>CarboFormer: A Lightweight Semantic Segmentation Architecture for   Efficient Carbon Dioxide Detection Using Optical Gas Imaging</h2><p><strong>Authors:Taminul Islam, Toqi Tahamid Sarker, Mohamed G Embaby, Khaled R Ahmed, Amer AbuGhazaleh</strong></p>
<p>Carbon dioxide (CO$_2$) emissions are critical indicators of both environmental impact and various industrial processes, including livestock management. We introduce CarboFormer, a lightweight semantic segmentation framework for Optical Gas Imaging (OGI), designed to detect and quantify CO$_2$ emissions across diverse applications. Our approach integrates an optimized encoder-decoder architecture with specialized multi-scale feature fusion and auxiliary supervision strategies to effectively model both local details and global relationships in gas plume imagery while achieving competitive accuracy with minimal computational overhead for resource-constrained environments. We contribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR) dataset, which simulates gas leaks with systematically varied flow rates (10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions from dairy cow rumen fluid in vitro experiments. Extensive evaluations demonstrate that CarboFormer achieves competitive performance with 84.88% mIoU on CCR and 92.98% mIoU on RTA, while maintaining computational efficiency with only 5.07M parameters and operating at 84.68 FPS. The model shows particular effectiveness in challenging low-flow scenarios and significantly outperforms other lightweight methods like SegFormer-B0 (83.36% mIoU on CCR) and SegNeXt (82.55% mIoU on CCR), making it suitable for real-time monitoring on resource-constrained platforms such as programmable drones. Our work advances both environmental sensing and precision livestock management by providing robust and efficient tools for CO$_2$ emission analysis. </p>
<blockquote>
<p>äºŒæ°§åŒ–ç¢³ï¼ˆCO2ï¼‰æ’æ”¾æ˜¯ç¯å¢ƒå½±å“å’Œå„ç§å·¥ä¸šè¿‡ç¨‹ï¼ˆåŒ…æ‹¬ç‰²ç•œç®¡ç†ï¼‰çš„å…³é”®æŒ‡æ ‡ã€‚æˆ‘ä»¬ä»‹ç»äº†CarboFormerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…‰å­¦æ°”ä½“æˆåƒï¼ˆOGIï¼‰çš„è½»é‡çº§è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹å¹¶é‡åŒ–å„ç§åº”ç”¨ä¸­CO2çš„æ’æ”¾é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¼˜åŒ–çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå…·æœ‰å¤šå°ºåº¦ç‰¹å¾èåˆå’Œè¾…åŠ©ç›‘ç£ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹æ°”ä½“å–·å°„å›¾åƒä¸­çš„å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®ç°å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§å’Œæœ€å°çš„è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬è´¡çŒ®äº†ä¸¤ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼šï¼ˆ1ï¼‰æ§åˆ¶äºŒæ°§åŒ–ç¢³æ’æ”¾ï¼ˆCCRï¼‰æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿç³»ç»Ÿå˜åŒ–çš„æ°”æµç‡ï¼ˆ10-100 SCCMï¼‰ä¸‹çš„æ°”ä½“æ³„æ¼ï¼›ï¼ˆ2ï¼‰å®æ—¶å®‰ç§‘å§†ï¼ˆRTAï¼‰æ•°æ®é›†ï¼Œä¸“æ³¨äºä½“å¤–å®éªŒä¸­å¥¶ç‰›ç˜¤èƒƒæµä½“çš„æ’æ”¾ã€‚å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒCarboFormeråœ¨CCRä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¾¾åˆ°84.88%çš„mIoUï¼Œåœ¨RTAä¸Šè¾¾åˆ°92.98%çš„mIoUï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä»…æœ‰5.07Må‚æ•°ï¼Œè¿è¡Œé€Ÿåº¦ä¸ºæ¯ç§’84.68å¸§ã€‚è¯¥æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½æµé‡åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºå…¶ä»–è½»é‡çº§æ–¹æ³•ï¼Œå¦‚SegFormer-B0ï¼ˆCCRä¸Šä¸º83.36%çš„mIoUï¼‰å’ŒSegNeXtï¼ˆCCRä¸Šä¸º82.55%çš„mIoUï¼‰ã€‚å› æ­¤ï¼Œå®ƒé€‚åˆåœ¨èµ„æºå—é™çš„å¹³å°ï¼ˆå¦‚å¯ç¼–ç¨‹æ— äººæœºï¼‰ä¸Šè¿›è¡Œå®æ—¶ç›‘è§†ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æä¾›ç”¨äºCO2æ’æ”¾åˆ†æçš„ç¨³å¥å’Œé«˜æ•ˆå·¥å…·ï¼Œæ¨åŠ¨äº†ç¯å¢ƒæ„ŸçŸ¥å’Œç²¾ç¡®ç‰²ç•œç®¡ç†çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05360v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CarboFormerï¼Œä¸€ä¸ªä¸ºå…‰å­¦æ°”ä½“æˆåƒï¼ˆOGIï¼‰è®¾è®¡çš„è½»é‡çº§è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å¹¶é‡åŒ–COâ‚‚æ’æ”¾ï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¼˜åŒ–åçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆå’Œè¾…åŠ©ç›‘ç£ç­–ç•¥ï¼Œæœ‰æ•ˆå»ºæ¨¡æ°”ä½“äº‘å›¢å½±åƒä¸­çš„å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€å…³ç³»ï¼ŒåŒæ—¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å®ç°å…·æœ‰ç«äº‰åŠ›çš„ç²¾åº¦å’Œè¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚æ–‡ç« è´¡çŒ®äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼šæ§åˆ¶äºŒæ°§åŒ–ç¢³é‡Šæ”¾ï¼ˆCCRï¼‰æ•°æ®é›†å’Œå®æ—¶å®‰ç§‘å§†ï¼ˆRTAï¼‰æ•°æ®é›†ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒCarboFormeråœ¨CCRä¸Šå®ç°äº†84.88%çš„mIoUï¼Œåœ¨RTAä¸Šå®ç°äº†92.98%çš„mIoUï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä»…æœ‰5.07Må‚æ•°ï¼Œè¿è¡Œé€Ÿåº¦ä¸º84.68 FPSã€‚è¯¥æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§çš„ä½æµé‡åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–è½»é‡çº§æ–¹æ³•ï¼Œé€‚åˆåœ¨èµ„æºå—é™çš„å¹³å°ï¼ˆå¦‚å¯ç¼–ç¨‹æ— äººæœºï¼‰ä¸Šè¿›è¡Œå®æ—¶ç›‘æµ‹ã€‚æœ¬æ–‡çš„å·¥ä½œä¸ºäºŒæ°§åŒ–ç¢³æ’æ”¾åˆ†æå’Œç²¾å‡†ç•œç‰§ä¸šç®¡ç†æä¾›äº†ç¨³å¥é«˜æ•ˆçš„å·¥å…·ï¼Œæ¨åŠ¨äº†ç¯å¢ƒæ„ŸçŸ¥å’Œç²¾å‡†ç•œç‰§ä¸šçš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CarboFormeræ˜¯ä¸€ä¸ªé’ˆå¯¹å…‰å­¦æ°”ä½“æˆåƒçš„è½»é‡çº§è¯­ä¹‰åˆ†å‰²æ¡†æ¶ï¼Œç”¨äºæ£€æµ‹å’Œé‡åŒ–COâ‚‚æ’æ”¾ã€‚</li>
<li>æ¡†æ¶æ•´åˆäº†ä¼˜åŒ–åçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶ç»“åˆå¤šå°ºåº¦ç‰¹å¾èåˆå’Œè¾…åŠ©ç›‘ç£ç­–ç•¥ã€‚</li>
<li>è´¡çŒ®äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼šCCRå’ŒRTAï¼Œåˆ†åˆ«æ¨¡æ‹Ÿç³»ç»Ÿå˜åŒ–æµç‡å’Œå¥¶ç‰›ä½“å†…å‘é…µå®éªŒä¸­çš„æ°”ä½“æ’æ”¾ã€‚</li>
<li>CarboFormeråœ¨CCRå’ŒRTAæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†84.88%å’Œ92.98%çš„mIoUï¼Œè¡¨ç°å‡ºè‰²ã€‚</li>
<li>æ¨¡å‹åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å…·æœ‰è®¡ç®—æ•ˆç‡å’Œå®æ—¶æ€§èƒ½ï¼Œé€‚åˆåœ¨å¯ç¼–ç¨‹æ— äººæœºç­‰å¹³å°ä¸Šè¿›è¡Œå®æ—¶ç›‘æµ‹ã€‚</li>
<li>æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§çš„ä½æµé‡åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–è½»é‡çº§æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.05360v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.05360v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.05360v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2506.05360v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Advancing-Marine-Research-UWSAM-Framework-and-UIIS10K-Dataset-for-Precise-Underwater-Instance-Segmentation"><a href="#Advancing-Marine-Research-UWSAM-Framework-and-UIIS10K-Dataset-for-Precise-Underwater-Instance-Segmentation" class="headerlink" title="Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for   Precise Underwater Instance Segmentation"></a>Advancing Marine Research: UWSAM Framework and UIIS10K Dataset for   Precise Underwater Instance Segmentation</h2><p><strong>Authors:Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Chongyi Li, Laurence T. Yang, Weidong Zhang, Sam Kwong</strong></p>
<p>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K">https://github.com/LiamLian0727/UIIS10K</a>. </p>
<blockquote>
<p>éšç€å¤§è§„æ¨¡å»ºæ¨¡çš„æœ€æ–°çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å„ç§è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´æ€§èƒ½å±€é™ï¼Œè€Œå®ƒä»¬è¾ƒé«˜çš„è®¡ç®—è¦æ±‚è¿›ä¸€æ­¥é˜»ç¢äº†åœ¨æ°´ä¸‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼Œå…¶ä¸­åŒ…æ‹¬10,048å¼ å…·æœ‰åƒç´ çº§æ³¨é‡Šçš„10ç±»å›¾åƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸“ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„UWSAMæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œå¹¶å°†å…¶è¿ç”¨åˆ°è¾ƒå°çš„ViT-Smallå›¾åƒç¼–ç å™¨ä¸Šï¼Œä»¥å®ç°æœ‰æ•ˆçš„è§†è§‰è¡¨å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯çš„æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œå®ƒä¼šè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œä¸æ˜¯æ˜¾å¼æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ä½œä¸ºæç¤ºï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œä»¥å®ç°é«˜æ•ˆçš„åˆ†å‰²ã€‚ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹éå¸¸æœ‰æ•ˆï¼Œåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†å¯¹æœ€å…ˆè¿›æ–¹æ³•æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/LiamLian0727/UIIS10Kè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15581v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤§è§„æ¨¡å»ºæ¨¡çš„çªç ´ï¼ŒSegment Anything Modelï¼ˆSAMï¼‰åœ¨å¤šç§è§†è§‰åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ°´ä¸‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ŒSAMåŠå…¶å˜ä½“åœ¨ç«¯åˆ°ç«¯çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§è§„æ¨¡çš„æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼Œå¹¶ä»‹ç»äº†ä¸ºæ°´ä¸‹å®ä¾‹è‡ªåŠ¨å‡†ç¡®åˆ†å‰²è€Œè®¾è®¡çš„UWSAMæ¨¡å‹ã€‚UWSAMé€šè¿‡åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»SAMçš„ViT-Hugeå›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œç”¨äºæ›´æœ‰æ•ˆçš„è§†è§‰è¡¨å¾å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜ä¸ºUWSAMè®¾è®¡äº†ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œè€Œéæ˜ç¡®æä¾›å‰æ™¯ç‚¹æˆ–æ¡†ï¼Œä»è€Œä½¿ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®å®šä½æ°´ä¸‹å®ä¾‹ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SAMåœ¨è§†è§‰åº”ç”¨ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨æ°´ä¸‹å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å› ç¼ºä¹æ°´ä¸‹é¢†åŸŸä¸“ä¸šçŸ¥è¯†è€Œå—é™ã€‚</li>
<li>æå‡ºäº†å¤§è§„æ¨¡æ°´ä¸‹å®ä¾‹åˆ†å‰²æ•°æ®é›†UIIS10Kï¼ŒåŒ…å«10,048å¼ å…·æœ‰åƒç´ çº§æ³¨é‡Šçš„å›¾åƒï¼Œæ¶‰åŠ10ä¸ªç±»åˆ«ã€‚</li>
<li>ä»‹ç»äº†ä¸ºæ°´ä¸‹å®ä¾‹åˆ†å‰²è€Œè®¾è®¡çš„UWSAMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯æé«˜æ€§èƒ½ã€‚</li>
<li>UWSAMä½¿ç”¨åŸºäºMask GATçš„æ°´ä¸‹çŸ¥è¯†è’¸é¦ï¼ˆMG-UKDï¼‰æ–¹æ³•ï¼Œä»å¤§å‹å›¾åƒç¼–ç å™¨æç‚¼çŸ¥è¯†ï¼Œç”¨äºæ›´æœ‰æ•ˆçš„è§†è§‰è¡¨å¾å­¦ä¹ ã€‚</li>
<li>å¼•å…¥äº†ç«¯åˆ°ç«¯æ°´ä¸‹æç¤ºç”Ÿæˆå™¨ï¼ˆEUPGï¼‰ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ°´ä¸‹æç¤ºï¼Œæé«˜ç½‘ç»œå®šä½æ°´ä¸‹å®ä¾‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>UWSAMåœ¨å¤šä¸ªæ°´ä¸‹å®ä¾‹æ•°æ®é›†ä¸Šå®ç°äº†å¯¹æœ€æ–°æŠ€æœ¯çš„æ˜¾è‘—æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.15581v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.15581v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.15581v3/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.15581v3/page_3_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.15581v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.15581v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection"><a href="#MonoCoP-Chain-of-Prediction-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"></a>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</h2><p><strong>Authors:Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</strong></p>
<p>Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets. </p>
<blockquote>
<p>å‡†ç¡®é¢„æµ‹3Då±æ€§å¯¹äºå•ç›®3Då¯¹è±¡æ£€æµ‹ï¼ˆMono3Dï¼‰è‡³å…³é‡è¦ï¼Œæ·±åº¦ä¼°è®¡åˆ™æ˜¯æœ€å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå°†2Då›¾åƒæ˜ å°„åˆ°3Dç©ºé—´å­˜åœ¨å›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤šç§æ·±åº¦çº¿ç´¢ï¼ˆä¾‹å¦‚ä¼°è®¡æ·±åº¦ä¸ç¡®å®šæ€§ã€å»ºæ¨¡æ·±åº¦è¯¯å·®ï¼‰æ¥æé«˜æ·±åº¦å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å‡†ç¡®çš„æ·±åº¦é¢„æµ‹éœ€è¦ä¾èµ–äºå…¶ä»–3Då±æ€§ï¼Œå› ä¸ºè¿™äº›å±æ€§é€šè¿‡3Dåˆ°2Dçš„æŠ•å½±å›ºæœ‰åœ°ç›¸äº’å…³è”ï¼Œè¿™æœ€ç»ˆé™åˆ¶äº†æ€»ä½“å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æœ¬æ–‡å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¯å‘ï¼Œæå‡ºäº†MonoCoPæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é¢„æµ‹é“¾ï¼ˆCoPï¼‰æ¥æŒ‰é¡ºåºå’Œæœ‰æ¡ä»¶åœ°é¢„æµ‹å±æ€§ï¼Œä¸»è¦é€šè¿‡ä¸‰ä¸ªå…³é”®è®¾è®¡å®ç°ã€‚é¦–å…ˆï¼Œå®ƒä¸ºæ¯ä¸ª3Då±æ€§é‡‡ç”¨è½»é‡çº§çš„AttributeNetï¼ˆANï¼‰æ¥å­¦ä¹ ç‰¹å®šäºå±æ€§çš„ç‰¹å¾ã€‚æ¥ä¸‹æ¥ï¼ŒMonoCoPæ„å»ºäº†ä¸€ä¸ªæ˜ç¡®çš„é“¾æ¡æ¥ä¼ æ’­ä»ä¸€ä¸ªå±æ€§å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ°ä¸‹ä¸€ä¸ªå±æ€§ã€‚æœ€åï¼ŒMonoCoPä½¿ç”¨æ®‹å·®è¿æ¥æ¥æ²¿é“¾æ¡èšåˆæ¯ä¸ªå±æ€§çš„ç‰¹å¾ï¼Œç¡®ä¿åç»­å±æ€§é¢„æµ‹ä¾èµ–äºæ‰€æœ‰å…ˆå‰å¤„ç†çš„å±æ€§ï¼ŒåŒæ—¶ä¸ä¼šå¿˜è®°æ—©æœŸå±æ€§çš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MonoCoPåœ¨KITTIæ’è¡Œæ¦œä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”æ— éœ€é¢å¤–æ•°æ®ï¼Œåœ¨Waymoå’ŒnuScenesæ­£é¢æ•°æ®é›†ä¸Šè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04594v5">PDF</a> I plan to re-format and re-write this paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºChain-of-Predictionï¼ˆCoPï¼‰çš„å•çœ¼ä¸‰ç»´ç‰©ä½“æ£€æµ‹ï¼ˆMono3Dï¼‰æ–¹æ³•ã€‚å®ƒé€šè¿‡å¯¹å¤šä¸ªä¸‰ç»´å±æ€§è¿›è¡Œåºåˆ—é¢„æµ‹å’Œæ¡ä»¶é¢„æµ‹ï¼Œæé«˜äº†æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ„å»ºæ˜¾å¼é“¾ä¼ æ’­ç‰¹å®šç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ®‹å·®è¿æ¥ç¡®ä¿åç»­å±æ€§é¢„æµ‹åŸºäºæ‰€æœ‰å…ˆå‰å¤„ç†è¿‡çš„å±æ€§ï¼Œæé«˜äº†æ•´ä½“å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚åœ¨KITTIã€Waymoå’ŒnuScenesæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MonoCoPæ–¹æ³•åˆ©ç”¨Chain-of-Predictionï¼ˆCoPï¼‰è¿›è¡Œä¸‰ç»´å±æ€§é¢„æµ‹ï¼Œä»¥æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºæ˜¾å¼é“¾ä¼ æ’­ç‰¹å®šç‰¹å¾ï¼Œæ¯ä¸ªä¸‰ç»´å±æ€§éƒ½æœ‰å¯¹åº”çš„AttributeNetï¼ˆANï¼‰æ¥å­¦ä¹ å±æ€§ç‰¹å®šç‰¹å¾ã€‚</li>
<li>MonoCoPé€šè¿‡æ®‹å·®è¿æ¥ç¡®ä¿åç»­å±æ€§é¢„æµ‹åŸºäºæ‰€æœ‰å…ˆå‰å¤„ç†è¿‡çš„å±æ€§ï¼Œä»è€Œæé«˜æ•´ä½“å‡†ç¡®æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMonoCoPåœ¨KITTIã€Waymoå’ŒnuScenesæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä¾èµ–é¢å¤–æ•°æ®ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>MonoCoPå¼ºè°ƒäº†ä¸‰ç»´å±æ€§ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œé€šè¿‡æ¡ä»¶é¢„æµ‹æé«˜äº†æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.04594v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.04594v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.04594v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.04594v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2505.04594v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Transferable-Mask-Transformer-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation"><a href="#Transferable-Mask-Transformer-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation" class="headerlink" title="Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation"></a>Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation</h2><p><strong>Authors:Jianhua Liu, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Ruizhe Zhao, Guan Wang, Yang Li</strong></p>
<p>Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTsâ€™ attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMTâ€™s superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available. </p>
<blockquote>
<p>å…³äºVision Transformersï¼ˆViTsï¼‰çš„æœ€æ–°è¿›å±•å·²ç»åœ¨è¯­ä¹‰åˆ†å‰²é¢†åŸŸè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œå½“å°†é¢„è®­ç»ƒçš„ViTsé€‚åº”åˆ°æ–°ç›®æ ‡åŸŸæ—¶ï¼Œç”±äºåˆ†å¸ƒè½¬ç§»ï¼Œå¾€å¾€ä¼šå‡ºç°æ€§èƒ½ä¸Šçš„æ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´å…¨å±€æ³¨æ„åŠ›ä¸ä½³ã€‚ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯æ•°æ®é©±åŠ¨çš„ï¼Œå½“æºåŸŸå’Œç›®æ ‡åŸŸåœ¨çº¹ç†ã€å°ºåº¦æˆ–å¯¹è±¡å…±ç°æ¨¡å¼ä¸Šå­˜åœ¨å·®å¼‚æ—¶ï¼Œå®ƒä»¬å¯èƒ½æ— æ³•æœ‰æ•ˆåœ°å…³æ³¨å…³é”®å¯¹è±¡ã€‚è™½ç„¶å…¨å±€å’Œè¡¥ä¸çº§åˆ«çš„åŸŸé€‚åº”æ–¹æ³•æä¾›äº†éƒ¨åˆ†è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºä¸åŒå›¾åƒåŒºåŸŸåœ¨è½¬ç§»èƒ½åŠ›ä¸Šçš„ç©ºé—´å¼‚è´¨æ€§ï¼ŒåŠ¨æ€å½¢çŠ¶åŒºåŸŸçš„åŒºåŸŸçº§é€‚åº”è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†Transferable Mask Transformerï¼ˆTMTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯­ä¹‰åˆ†å‰²çš„æ–°å‹åŒºåŸŸçº§é€‚åº”æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç©ºé—´è½¬ç§»èƒ½åŠ›åˆ†ææ¥å¯¹é½è·¨åŸŸè¡¨ç¤ºã€‚TMTç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰åŸºäºè‡ªé€‚åº”èšç±»çš„è½¬ç§»èƒ½åŠ›ä¼°è®¡å™¨ï¼ˆACTEï¼‰ï¼Œå®ƒåŠ¨æ€åœ°å°†å›¾åƒåˆ†å‰²æˆç»“æ„å’Œè¯­ä¹‰ä¸Šè¿è´¯çš„åŒºåŸŸï¼Œä»¥è¿›è¡Œå±€éƒ¨åŒ–çš„è½¬ç§»èƒ½åŠ›è¯„ä¼°ï¼›ï¼ˆ2ï¼‰å¯è½¬ç§»çš„æ©æ¨¡æ³¨æ„åŠ›ï¼ˆTMAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åŒºåŸŸç‰¹å®šçš„è½¬ç§»èƒ½åŠ›å›¾é›†æˆåˆ°ViTsçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¼˜å…ˆé€‚åº”ä½è½¬ç§»èƒ½åŠ›å’Œé«˜è¯­ä¹‰ä¸ç¡®å®šæ€§çš„åŒºåŸŸã€‚åœ¨20ä¸ªè·¨åŸŸå¯¹çš„ç»¼åˆè¯„ä¼°ä¸­ï¼ŒTMTçš„è¡¨ç°å“è¶Šï¼Œä¸ç®€å•çš„å¾®è°ƒç›¸æ¯”ï¼Œå¹³å‡æé«˜äº†2%çš„MIoUï¼Œä¸æœ€æ–°çš„åŸºçº¿ç›¸æ¯”å¢åŠ äº†1.28%ã€‚æºä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05774v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„æ–°å‹åŒºåŸŸçº§è‡ªé€‚åº”æ¡†æ¶â€”â€”Transferable Mask Transformerï¼ˆTMTï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡ç©ºé—´å¯è½¬ç§»æ€§åˆ†æå®ç°å¯¹è·¨åŸŸè¡¨ç¤ºçš„åŒ¹é…ã€‚TMTåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šè‡ªé€‚åº”èšç±»è½¬ç§»è¯„ä¼°å™¨ï¼ˆACTEï¼‰å’Œå¯è½¬ç§»æ©è†œæ³¨æ„åŠ›æ¨¡å—ï¼ˆTMAï¼‰ã€‚ACTEåŠ¨æ€åœ°å°†å›¾åƒåˆ†å‰²ä¸ºç»“æ„åŒ–å’Œè¯­ä¹‰è¿è´¯çš„åŒºåŸŸï¼Œè¿›è¡Œå±€éƒ¨è½¬ç§»è¯„ä¼°ï¼›è€ŒTMAæ¨¡å—åˆ™å°†åŒºåŸŸç‰¹å®šçš„è½¬ç§»èƒ½åŠ›å›¾èå…¥ViTçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¼˜å…ˆé€‚åº”ä½è½¬ç§»èƒ½åŠ›å’Œé«˜è¯­ä¹‰ä¸ç¡®å®šæ€§çš„åŒºåŸŸã€‚é€šè¿‡å¹¿æ³›çš„è·¨åŸŸå¯¹æ¯”å®éªŒéªŒè¯ï¼ŒTMTç›¸è¾ƒäºåŸºå‡†æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViTs)åœ¨è¯­ä¹‰åˆ†å‰²ä¸Šå–å¾—äº†æ–°çš„çªç ´ï¼Œä½†åœ¨ç›®æ ‡åŸŸé€‚åº”æ—¶é¢ä¸´æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨è·¨åŸŸæƒ…å†µä¸‹å¯èƒ½æ— æ³•æœ‰æ•ˆå…³æ³¨å…³é”®å¯¹è±¡ï¼Œç‰¹åˆ«æ˜¯å½“æºåŸŸå’Œç›®æ ‡åŸŸåœ¨çº¹ç†ã€å°ºåº¦æˆ–å¯¹è±¡å…±ç°æ¨¡å¼ä¸Šå­˜åœ¨å·®å¼‚æ—¶ã€‚</li>
<li>å½“å‰çš„å…¨å±€å’Œè¡¥ä¸çº§åŸŸé€‚åº”æ–¹æ³•è™½ç„¶æä¾›äº†éƒ¨åˆ†è§£å†³æ–¹æ¡ˆï¼Œä½†åŒºåŸŸçº§é€‚åº”æ›´ä¸ºé‡è¦ã€‚</li>
<li>Transferable Mask Transformerï¼ˆTMTï¼‰æ˜¯ä¸€ä¸ªæ–°å‹åŒºåŸŸçº§é€‚åº”æ¡†æ¶ï¼Œé€šè¿‡ç©ºé—´å¯è½¬ç§»æ€§åˆ†æå®ç°è·¨åŸŸè¡¨ç¤ºçš„åŒ¹é…ã€‚</li>
<li>TMTåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šACTEå’ŒTMAï¼Œåˆ†åˆ«ç”¨äºåŠ¨æ€åŒºåŸŸåˆ†å‰²å’Œèå…¥ViTçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2504.05774v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2504.05774v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2504.05774v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2504.05774v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exponentially-Weighted-Instance-Aware-Repeat-Factor-Sampling-for-Long-Tailed-Object-Detection-Model-Training-in-Unmanned-Aerial-Vehicles-Surveillance-Scenarios"><a href="#Exponentially-Weighted-Instance-Aware-Repeat-Factor-Sampling-for-Long-Tailed-Object-Detection-Model-Training-in-Unmanned-Aerial-Vehicles-Surveillance-Scenarios" class="headerlink" title="Exponentially Weighted Instance-Aware Repeat Factor Sampling for   Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles   Surveillance Scenarios"></a>Exponentially Weighted Instance-Aware Repeat Factor Sampling for   Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles   Surveillance Scenarios</h2><p><strong>Authors:Taufiq Ahmed, Abhishek Kumar, Constantino Ãlvarez Casado, Anlan Zhang, Tuomo HÃ¤nninen, Lauri Loven, Miguel Bordallo LÃ³pez, Sasu Tarkoma</strong></p>
<p>Object detection models often struggle with class imbalance, where rare categories appear significantly less frequently than common ones. Existing sampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and Instance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting sample frequencies based on image and instance counts. However, these methods are based on linear adjustments, which limit their effectiveness in long-tailed distributions. This work introduces Exponentially Weighted Instance-Aware Repeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential scaling to better differentiate between rare and frequent classes. E-IRFS adjusts sampling probabilities using an exponential function applied to the geometric mean of image and instance frequencies, ensuring a more adaptive rebalancing strategy. We evaluate E-IRFS on a dataset derived from the Fireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11 object detection models to identify fire, smoke, people and lakes in emergency scenarios. The results show that E-IRFS improves detection performance by 22% over the baseline and outperforms RFS and IRFS, particularly for rare categories. The analysis also highlights that E-IRFS has a stronger effect on lightweight models with limited capacity, as these models rely more on data sampling strategies to address class imbalance. The findings demonstrate that E-IRFS improves rare object detection in resource-constrained environments, making it a suitable solution for real-time applications such as UAV-based emergency monitoring. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/futurians/E-IRFS">https://github.com/futurians/E-IRFS</a>. </p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹æ¨¡å‹ç»å¸¸é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œå…¶ä¸­ç¨€æœ‰ç±»åˆ«çš„å‡ºç°é¢‘ç‡æ˜¾è‘—ä½æ–¼å¸¸è§ç±»åˆ«ã€‚ç°æœ‰çš„åŸºäºé‡‡æ ·çš„å†å¹³è¡¡ç­–ç•¥ï¼Œå¦‚é‡å¤å› å­é‡‡æ ·ï¼ˆRFSï¼‰å’Œå®ä¾‹æ„ŸçŸ¥é‡å¤å› å­é‡‡æ ·ï¼ˆIRFSï¼‰ï¼Œé€šè¿‡æ ¹æ®å›¾åƒå’Œå®ä¾‹è®¡æ•°è°ƒæ•´æ ·æœ¬é¢‘ç‡æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åŸºäºçº¿æ€§è°ƒæ•´ï¼Œåœ¨é•¿å°¾åˆ†å¸ƒä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†åŠ æƒå®ä¾‹æ„ŸçŸ¥é‡å¤å› å­é‡‡æ ·ï¼ˆE-IRFSï¼‰ï¼Œå®ƒæ˜¯IRFSçš„æ‰©å±•ï¼Œåº”ç”¨æŒ‡æ•°ç¼©æ”¾æ¥æ›´å¥½åœ°åŒºåˆ†ç¨€æœ‰ç±»åˆ«å’Œå¸¸è§ç±»åˆ«ã€‚E-IRFSé€šè¿‡ä½¿ç”¨åº”ç”¨äºå›¾åƒå’Œå®ä¾‹é¢‘ç‡å‡ ä½•å‡å€¼çš„æŒ‡æ•°å‡½æ•°æ¥è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œç¡®ä¿æ›´è‡ªé€‚åº”çš„å†å¹³è¡¡ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä»Fireman-UAV-RGBTæ•°æ®é›†æ´¾ç”Ÿçš„æ•°æ®é›†å’Œå¦å¤–å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°äº†E-IRFSï¼Œä½¿ç”¨YOLOv11å¯¹è±¡æ£€æµ‹æ¨¡å‹æ¥è¯†åˆ«ç´§æ€¥åœºæ™¯ä¸­çš„ç«ç¾ã€çƒŸé›¾ã€äººå‘˜å’Œæ¹–æ³Šã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼ŒE-IRFSæé«˜äº†22%çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºRFSå’ŒIRFSï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€æœ‰ç±»åˆ«æ–¹é¢ã€‚åˆ†æè¿˜å¼ºè°ƒï¼ŒE-IRFSå¯¹å®¹é‡æœ‰é™çš„è½»é‡çº§æ¨¡å‹å½±å“æ›´å¤§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ›´ä¾èµ–æ•°æ®é‡‡æ ·ç­–ç•¥æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒE-IRFSåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æé«˜äº†ç¨€æœ‰å¯¹è±¡æ£€æµ‹èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºé€‚ç”¨äºå®æ—¶åº”ç”¨ï¼ˆå¦‚åŸºäºæ— äººæœºçš„ç´§æ€¥ç›‘æµ‹ï¼‰çš„åˆé€‚è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/futurians/E-IRFS%E3%80%82">https://github.com/futurians/E-IRFSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21893v2">PDF</a> 7 pages, 2 figures, 9 tables, 6 formulas, conference paper, code   available</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹ç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­å¸¸è§çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·ç­–ç•¥â€”â€”åŸºäºæŒ‡æ•°æƒé‡çš„å®ä¾‹æ„ŸçŸ¥é‡å¤å› å­é‡‡æ ·ï¼ˆE-IRFSï¼‰ã€‚E-IRFSå¯¹å›¾åƒå’Œå®ä¾‹é¢‘ç‡çš„å‡ ä½•å‡å€¼åº”ç”¨æŒ‡æ•°å‡½æ•°æ¥è°ƒæ•´é‡‡æ ·æ¦‚ç‡ï¼Œæ›´æœ‰æ•ˆåœ°å¹³è¡¡äº†ç¨€æœ‰å’Œå¸¸è§ç±»åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åº”æ€¥åœºæ™¯ä¸‹çš„æ•°æ®é›†ä¸Šï¼ŒE-IRFSèƒ½æé«˜ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç¨€æœ‰ç±»åˆ«æ—¶è¡¨ç°æ›´ä½³ã€‚æ­¤å¤–ï¼Œå¯¹äºå®¹é‡æœ‰é™çš„è½»é‡çº§æ¨¡å‹ï¼ŒE-IRFSçš„å½±å“æ›´ä¸ºæ˜¾è‘—ã€‚å®ƒä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„ç¨€æœ‰ç›®æ ‡æ£€æµ‹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€‚åˆç”¨äºæ— äººæœºåº”æ€¥ç›‘æµ‹ç­‰å®æ—¶åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç›®æ ‡æ£€æµ‹æ¨¡å‹é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç¨€æœ‰ç±»åˆ«å‡ºç°é¢‘ç‡è¿œä½äºå¸¸è§ç±»åˆ«ã€‚</li>
<li>ç°æœ‰é‡‡æ ·ç­–ç•¥å¦‚RFSå’ŒIRFSé€šè¿‡è°ƒæ•´æ ·æœ¬é¢‘ç‡æ¥å¹³è¡¡æ­¤ç±»é—®é¢˜ï¼Œä½†æ•ˆæœæœ‰é™ã€‚</li>
<li>å¼•å…¥E-IRFSç­–ç•¥ï¼Œé€šè¿‡å¯¹å›¾åƒå’Œå®ä¾‹é¢‘ç‡çš„å‡ ä½•å‡å€¼åº”ç”¨æŒ‡æ•°å‡½æ•°æ¥è°ƒæ•´é‡‡æ ·æ¦‚ç‡ã€‚</li>
<li>E-IRFSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶èƒ½æœ‰æ•ˆæé«˜ç›®æ ‡æ£€æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç¨€æœ‰ç±»åˆ«æ–¹é¢ã€‚</li>
<li>E-IRFSå¯¹è½»é‡çº§æ¨¡å‹çš„å½±å“æ›´ä¸ºæ˜¾è‘—ï¼Œè¿™äº›æ¨¡å‹æ›´ä¾èµ–äºæ•°æ®é‡‡æ ·ç­–ç•¥æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>E-IRFSé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒä¸­çš„ç¨€æœ‰ç›®æ ‡æ£€æµ‹ï¼Œä¸ºå®æ—¶åº”ç”¨å¦‚æ— äººæœºåº”æ€¥ç›‘æµ‹æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2503.21893v2/page_5_2.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-Agent-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation Agent for Weakly Supervised Semantic Segmentation"></a>Image Augmentation Agent for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Xianglin Qiu, Siqi Song, Zhenhong Chen, Xiaowei Huang, Fei Ma, Jimin Xiao</strong></p>
<p>Weakly-supervised semantic segmentation (WSSS) has achieved remarkable progress using only image-level labels. However, most existing WSSS methods focus on designing new network structures and loss functions to generate more accurate dense labels, overlooking the limitations imposed by fixed datasets, which can constrain performance improvements. We argue that more diverse trainable images provides WSSS richer information and help model understand more comprehensive semantic pattern. Therefore in this paper, we introduce a novel approach called Image Augmentation Agent (IAA) which shows that it is possible to enhance WSSS from data generation perspective. IAA mainly design an augmentation agent that leverages large language models (LLMs) and diffusion models to automatically generate additional images for WSSS. In practice, to address the instability in prompt generation by LLMs, we develop a prompt self-refinement mechanism. It allow LLMs to re-evaluate the rationality of generated prompts to produce more coherent prompts. Additionally, we insert an online filter into diffusion generation process to dynamically ensure the quality and balance of generated images. Experimental results show that our method significantly surpasses state-of-the-art WSSS approaches on the PASCAL VOC 2012 and MS COCO 2014 datasets. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„WSSSæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°æ¥ç”Ÿæˆæ›´å‡†ç¡®çš„å¯†é›†æ ‡ç­¾ï¼Œè€Œå¿½è§†äº†å›ºå®šæ•°æ®é›†å¸¦æ¥çš„é™åˆ¶ï¼Œè¿™äº›é™åˆ¶å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½çš„æå‡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæä¾›æ›´å¤šå¯è®­ç»ƒå›¾åƒå¯ä»¥ä¸ºWSSSæä¾›æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶å¸®åŠ©æ¨¡å‹ç†è§£æ›´å…¨é¢çš„è¯­ä¹‰æ¨¡å¼ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºImage Augmentation Agentï¼ˆIAAï¼‰çš„æ–°æ–¹æ³•ï¼Œè¡¨æ˜ä»æ•°æ®ç”Ÿæˆçš„è§’åº¦å¢å¼ºWSSSæ˜¯å¯èƒ½çš„ã€‚IAAä¸»è¦è®¾è®¡äº†ä¸€ä¸ªå¢å¼ºä»£ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ä¸ºWSSSç”Ÿæˆé¢å¤–çš„å›¾åƒã€‚åœ¨å®è·µä¸­ï¼Œä¸ºäº†è§£å†³LLMç”Ÿæˆæç¤ºçš„ä¸ç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æç¤ºè‡ªæˆ‘å®Œå–„æœºåˆ¶ã€‚å®ƒå…è®¸LLMé‡æ–°è¯„ä¼°ç”Ÿæˆçš„æç¤ºçš„åˆç†æ€§ï¼Œä»¥äº§ç”Ÿæ›´è¿è´¯çš„æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥äº†ä¸€ä¸ªåœ¨çº¿è¿‡æ»¤å™¨ï¼Œä»¥åŠ¨æ€ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„WSSSæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20439v4">PDF</a> Accepted at Neurocomputing 2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºImage Augmentation Agentï¼ˆIAAï¼‰çš„æ–¹æ³•ï¼Œä»æ•°æ®ç”Ÿæˆçš„è§’åº¦æå‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢å¤–çš„å›¾åƒï¼Œå¹¶è®¾è®¡äº†æç¤ºè‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶å’Œåœ¨çº¿è¿‡æ»¤å™¨ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡æ€§ã€‚åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„WSSSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSSæ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡æ–°çš„ç½‘ç»œç»“æ„å’ŒæŸå¤±å‡½æ•°ä»¥ç”Ÿæˆæ›´å‡†ç¡®çš„å¯†é›†æ ‡ç­¾ï¼Œä½†å›ºå®šæ•°æ®é›†çš„å±€é™æ€§é™åˆ¶äº†æ€§èƒ½çš„æå‡ã€‚</li>
<li>æœ¬æ–‡ä»æ•°æ®ç”Ÿæˆçš„è§’åº¦æå‡ºä¸€ç§åä¸ºIAAçš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜WSSSçš„æ€§èƒ½ã€‚</li>
<li>IAAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé¢å¤–çš„å›¾åƒï¼Œä¸ºWSSSæä¾›æ›´å¤šå…ƒåŒ–çš„ä¿¡æ¯ã€‚</li>
<li>ä¸ºäº†è§£å†³LLMsåœ¨æç¤ºç”Ÿæˆä¸­çš„ä¸ç¨³å®šé—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§æç¤ºè‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶ï¼Œä½¿LLMsèƒ½å¤Ÿé‡æ–°è¯„ä¼°ç”Ÿæˆçš„æç¤ºçš„åˆç†æ€§ï¼Œäº§ç”Ÿæ›´è¿è´¯çš„æç¤ºã€‚</li>
<li>åœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥åœ¨çº¿è¿‡æ»¤å™¨ï¼Œä»¥åŠ¨æ€ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¹³è¡¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIAAæ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„WSSSæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2412.20439v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2412.20439v4/page_2_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-with-Denoising-Diffusion-for-3D-Object-Detection"><a href="#V2X-R-Cooperative-LiDAR-4D-Radar-Fusion-with-Denoising-Diffusion-for-3D-Object-Detection" class="headerlink" title="V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D   Object Detection"></a>V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D   Object Detection</h2><p><strong>Authors:Xun Huang, Jinlong Wang, Qiming Xia, Siheng Chen, Bisheng Yang, Xin Li, Cheng Wang, Chenglu Wen</strong></p>
<p>Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weather-robust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%&#x2F;6.70% in foggy&#x2F;snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R">https://github.com/ylwhxht/V2X-R</a>. </p>
<blockquote>
<p>å½“å‰çš„è½¦å¯¹å¤–ç•Œï¼ˆV2Xï¼‰ç³»ç»Ÿå·²ç»é€šè¿‡æ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®æ˜¾è‘—å¢å¼ºäº†3Då¯¹è±¡æ£€æµ‹åŠŸèƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ä¼šå‡ºç°æ€§èƒ½ä¸‹é™ã€‚å¤©æ°”ç¨³å®šçš„4Dé›·è¾¾æä¾›äº†å¤šæ™®å‹’å’Œé¢å¤–çš„å‡ ä½•ä¿¡æ¯ï¼Œä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†å¯èƒ½æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†V2X-Rï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆäº†æ¿€å…‰é›·è¾¾ã€ç›¸æœºå’Œ4Dé›·è¾¾çš„æ¨¡æ‹ŸV2Xæ•°æ®é›†ã€‚V2X-RåŒ…å«12,079ä¸ªåœºæ™¯ï¼Œå…¶ä¸­åŒ…æ‹¬æ¿€å…‰é›·è¾¾å’Œ4Dé›·è¾¾ç‚¹äº‘37,727å¸§ï¼Œå›¾åƒ150,908å¼ ï¼Œä»¥åŠæ ‡æ³¨çš„3Dè½¦è¾†è¾¹ç•Œæ¡†170,859ä¸ªã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Då¯¹è±¡æ£€æµ‹çš„æ–°å‹åˆä½œå¼æ¿€å…‰é›·è¾¾-4Dé›·è¾¾èåˆç®¡é“ï¼Œå¹¶é‡‡ç”¨äº†å¤šç§èåˆç­–ç•¥æ¥å®ç°å®ƒã€‚ä¸ºäº†å®ç°å¤©æ°”ç¨³å®šçš„æ£€æµ‹ï¼Œæˆ‘ä»¬è¿˜åœ¨èåˆç®¡é“ä¸­æå‡ºäº†å¤šæ¨¡å¼å»å™ªæ‰©æ•£ï¼ˆMDDï¼‰æ¨¡å—ã€‚MDDåˆ©ç”¨å¤©æ°”ç¨³å®šçš„4Dé›·è¾¾ç‰¹å¾ä½œä¸ºæ¡ä»¶ï¼Œæç¤ºæ‰©æ•£æ¨¡å‹å¯¹å˜ˆæ‚çš„æ¿€å…‰é›·è¾¾ç‰¹å¾è¿›è¡Œå»å™ªã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¿€å…‰é›·è¾¾-4Dé›·è¾¾èåˆç®¡é“åœ¨V2X-Ræ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„MDDæ¨¡å—è¿›ä¸€æ­¥æ”¹è¿›äº†åŸºæœ¬èåˆæ¨¡å‹åœ¨é›¾å¤©å’Œé›ªå¤©çš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†5.73%å’Œ6.70%ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ­£å¸¸æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ylwhxht/V2X-R%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/ylwhxht/V2X-Rå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08402v5">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å½“å‰è½¦è¾†åˆ°ä¸‡ç‰©ï¼ˆV2Xï¼‰ç³»ç»Ÿå·²ç»é€šè¿‡æ¿€å…‰é›·è¾¾å’Œæ‘„åƒå¤´æ•°æ®æ˜¾è‘—æé«˜äº†ä¸‰ç»´ç‰©ä½“æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ä¼šå‡ºç°æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†V2X-Ræ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†èåˆäº†æ¿€å…‰é›·è¾¾ã€æ‘„åƒå¤´å’Œå››ç»´é›·è¾¾æ•°æ®ã€‚V2X-RåŒ…å«äº†ä¸°å¯Œçš„æ•°æ®åœºæ™¯å’Œæ ‡æ³¨æ¡†ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†ä¸€ç§æ–°å‹çš„é›·è¾¾ä¸æ¿€å…‰é›·è¾¾èåˆç®¡é“ï¼Œå®ç°äº†ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å—ï¼ˆMDDï¼‰ï¼Œä»¥æé«˜æ¶åŠ£å¤©æ°”ä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨V2X-Ræ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é›¾å¤©å’Œé›ªå¤©æ¡ä»¶ä¸‹è¿›ä¸€æ­¥æé«˜äº†åŸºæœ¬èåˆæ¨¡å‹çš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨å…¬å¼€å¹³å°ä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰V2Xç³»ç»Ÿåœ¨æ¶åŠ£å¤©æ°”ä¸‹ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„V2X-Ræ•°æ®é›†ï¼Œèåˆäº†æ¿€å…‰é›·è¾¾ã€æ‘„åƒå¤´å’Œå››ç»´é›·è¾¾æ•°æ®ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é›·è¾¾ä¸æ¿€å…‰é›·è¾¾èåˆç®¡é“ï¼Œå®ç°äº†ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å»å™ªæ‰©æ•£æ¨¡å—ï¼ˆMDDï¼‰ï¼Œæé«˜æ¶åŠ£å¤©æ°”ä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨V2X-Ræ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨é›¾å¤©å’Œé›ªå¤©æ¡ä»¶ä¸‹ï¼ŒMDDæ¨¡å—èƒ½è¿›ä¸€æ­¥æé«˜åŸºæœ¬èåˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.08402v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.08402v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.08402v5/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.08402v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.08402v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2411.08402v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance"><a href="#Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance" class="headerlink" title="Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance"></a>Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance</h2><p><strong>Authors:Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen</strong></p>
<p>Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using Class-Prompt Appending and Visual Prior Blending to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a class balancing algorithm to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance</a>. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤§é‡æ ‡æ³¨å·¥ä½œã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œæ¶‰åŠæ—‹è½¬ã€ç¿»è½¬ç­‰ç®€å•å˜æ¢ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–°å›¾åƒï¼Œä½†å¾€å¾€ç¼ºä¹å…³é”®è¯­ä¹‰ç»´åº¦çš„å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆæ¨¡å‹ä½œä¸ºæ•°æ®å¢å¼ºçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆå·²ç»å‡ºç°ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºæ•°æ®ã€‚å¯æ§ç”Ÿæˆæ¨¡å‹é€šè¿‡ä½¿ç”¨åŸå§‹å›¾åƒçš„æç¤ºå’Œè§†è§‰å‚è€ƒï¼Œä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåˆ›å»ºæœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒå…·æœ‰éš¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨ç±»æç¤ºè¿½åŠ å’Œè§†è§‰å…ˆéªŒæ··åˆçš„æœ‰æ•ˆæç¤ºç”Ÿæˆï¼Œä»¥æé«˜å¯¹çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨ï¼Œä½¿ç®¡é“èƒ½å¤Ÿåœ¨ä¿æŒåˆ†å‰²æ ‡è®°ç±»ç»“æ„çš„åŒæ—¶ï¼Œç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§ç±»å¹³è¡¡ç®—æ³•ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶è·å¾—å¹³è¡¡çš„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidanceä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06002v5">PDF</a> Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313</p>
<p><strong>Summary</strong><br>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ ‡æ³¨ä»»åŠ¡å¦‚è¯­ä¹‰åˆ†å‰²è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ç®€å•ï¼Œç¼ºä¹å¤šæ ·æ€§ã€‚ç”Ÿæˆæ¨¡å‹é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒè¿›è¡Œæ•°æ®å¢å¼ºã€‚å¯æ§ç”Ÿæˆæ¨¡å‹é€šè¿‡æç¤ºå’ŒåŸå§‹å›¾åƒè§†è§‰å‚è€ƒä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œä½†ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡å¼•å…¥ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼Œé€šè¿‡é«˜æ•ˆæç¤ºç”Ÿæˆå’Œè§†è§‰å…ˆéªŒèåˆï¼Œåœ¨çœŸå®å›¾åƒä¸­å¢å¼ºå¯¹æ ‡è®°ç±»çš„æ³¨æ„åŠ›ï¼Œç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒï¼ŒåŒæ—¶ä¿æŒåˆ†å‰²æ ‡è®°ç±»çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œå®ç°ç±»åˆ«å¹³è¡¡ç®—æ³•ï¼Œç¡®ä¿åˆæˆå’ŒåŸå§‹å›¾åƒåˆå¹¶æ—¶çš„è®­ç»ƒæ•°æ®é›†å¹³è¡¡ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†ç®¡é“ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨åƒç´ çº§æ ‡æ³¨ä»»åŠ¡ä¸­éå¸¸é‡è¦ï¼Œå°¤å…¶æ˜¯è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•ç®€å•ä½†ç¼ºä¹å¤šæ ·æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹å¯ä»¥æœ‰æ•ˆè§£å†³æ•°æ®å¢å¼ºé—®é¢˜ï¼Œå°¤å…¶æ˜¯å¯æ§ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>å¯æ§ç”Ÿæˆæ¨¡å‹ä½¿ç”¨æç¤ºå’ŒåŸå§‹å›¾åƒè§†è§‰å‚è€ƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„æ•°æ®å¢å¼ºã€‚</li>
<li>ç”Ÿæˆåˆæˆå›¾åƒæ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å‡†ç¡®åæ˜ åŸå§‹å›¾åƒçš„å†…å®¹å’Œç»“æ„ã€‚</li>
<li>å¼•å…¥ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼ŒåŒ…æ‹¬é«˜æ•ˆæç¤ºç”Ÿæˆå’Œè§†è§‰å…ˆéªŒèåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.06002v5/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.06002v5/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.06002v5/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.06002v5/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.06002v5/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2409.06002v5/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ReCLIP-Learn-to-Rectify-the-Bias-of-CLIP-for-Unsupervised-Semantic-Segmentation"><a href="#ReCLIP-Learn-to-Rectify-the-Bias-of-CLIP-for-Unsupervised-Semantic-Segmentation" class="headerlink" title="ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic   Segmentation"></a>ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic   Segmentation</h2><p><strong>Authors:Jingyun Wang, Guoliang Kang</strong></p>
<p>Recent works utilize CLIP to perform the challenging unsupervised semantic segmentation task where only images without annotations are available. However, we observe that when adopting CLIP to such a pixel-level understanding task, unexpected bias (including class-preference bias and space-preference bias) occurs. Previous works donâ€™t explicitly model the bias, which largely constrains the segmentation performance. In this paper, we propose to explicitly model and rectify the bias existing in CLIP to facilitate the unsupervised semantic segmentation task. Specifically, we design a learnable â€œReferenceâ€ prompt to encode class-preference bias and a projection of the positional embedding in the vision transformer to encode space-preference bias respectively. To avoid interference, two kinds of biases are firstly independently encoded into different features, i.e., the Reference feature and the positional feature. Via a matrix multiplication between the Reference feature and the positional feature, a bias logit map is generated to explicitly represent two kinds of biases. Then we rectify the logits of CLIP via a simple element-wise subtraction. To make the rectified results smoother and more contextual, we design a mask decoder which takes the feature of CLIP and the rectified logits as input and outputs a rectified segmentation mask with the help of Gumbel-Softmax operation. A contrastive loss based on the masked visual features and the text features of different classes is imposed, which makes the bias modeling and rectification process meaningful and effective. Extensive experiments on various benchmarks including PASCAL VOC, PASCAL Context, ADE20K, Cityscapes, and COCO Stuff demonstrate that our method performs favorably against previous state-of-the-arts. The implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/dogehhh/ReCLIP">https://github.com/dogehhh/ReCLIP</a>. </p>
<blockquote>
<p>è¿‘æœŸå·¥ä½œåˆ©ç”¨CLIPæ¥æ‰§è¡Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ä»…æä¾›æ²¡æœ‰æ³¨é‡Šçš„å›¾åƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å°†CLIPåº”ç”¨äºè¿™ç§åƒç´ çº§çš„ç†è§£ä»»åŠ¡æ—¶ï¼Œä¼šå‡ºç°æ„æƒ³ä¸åˆ°çš„åè§ï¼ˆåŒ…æ‹¬ç±»åˆ«åå¥½åè§å’Œç©ºé—´åå¥½åè§ï¼‰ã€‚ä»¥å‰çš„å·¥ä½œæ²¡æœ‰æ˜¾å¼åœ°å¯¹åè§è¿›è¡Œå»ºæ¨¡ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†åˆ†å‰²æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¯¹CLIPä¸­å­˜åœ¨çš„åè§è¿›è¡Œæ˜¾å¼å»ºæ¨¡å’Œçº æ­£ï¼Œä»¥ä¿ƒè¿›æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„â€œå‚è€ƒâ€æç¤ºæ¥ç¼–ç ç±»åˆ«åå¥½åè§ï¼Œå¹¶åˆ©ç”¨è§†è§‰å˜å‹å™¨ä¸­çš„ä½ç½®åµŒå…¥çš„æŠ•å½±æ¥ç¼–ç ç©ºé—´åå¥½åè§ã€‚ä¸ºäº†é¿å…å¹²æ‰°ï¼Œä¸¤ç§åè§é¦–å…ˆè¢«ç‹¬ç«‹ç¼–ç ä¸ºä¸åŒçš„ç‰¹å¾ï¼Œå³å‚è€ƒç‰¹å¾å’Œä½ç½®ç‰¹å¾ã€‚é€šè¿‡å‚è€ƒç‰¹å¾ä¸ä½ç½®ç‰¹å¾ä¹‹é—´çš„çŸ©é˜µä¹˜æ³•ï¼Œç”Ÿæˆä¸€ä¸ªåè§é€»è¾‘å›¾ï¼Œæ˜¾å¼è¡¨ç¤ºä¸¤ç§åè§ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ç®€å•çš„å…ƒç´ çº§å‡æ³•æ¥çº æ­£CLIPçš„é€»è¾‘å€¼ã€‚ä¸ºäº†ä½¿æ ¡æ­£ç»“æœæ›´å¹³æ»‘ã€æ›´å…·ä¸Šä¸‹æ–‡æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ©è†œè§£ç å™¨ï¼Œå®ƒæ¥å—CLIPçš„ç‰¹å¾å’Œæ ¡æ­£åçš„é€»è¾‘å€¼ä½œä¸ºè¾“å…¥ï¼Œå€ŸåŠ©Gumbel-Softmaxæ“ä½œè¾“å‡ºä¸€ä¸ªæ ¡æ­£åçš„åˆ†å‰²æ©è†œã€‚åŸºäºæ©ç è§†è§‰ç‰¹å¾å’Œä¸åŒç±»åˆ«æ–‡æœ¬ç‰¹å¾çš„å¯¹æ¯”æŸå¤±ï¼Œä½¿å¾—åè§å»ºæ¨¡å’Œçº æ­£è¿‡ç¨‹æœ‰æ„ä¹‰ä¸”æœ‰æ•ˆã€‚åœ¨åŒ…æ‹¬PASCAL VOCã€PASCAL Contextã€ADE20Kã€Cityscapeså’ŒCOCO Stuffç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¹‹å‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚å®ç°è¯¦æƒ…å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/dogehhh/ReCLIP%E3%80%82">https://github.com/dogehhh/ReCLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06747v3">PDF</a> Extended version of our CVPR 24 paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPçš„æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œå­˜åœ¨åŒ…æ‹¬ç±»åˆ«åå¥½åè§å’Œç©ºé—´åå¥½åè§åœ¨å†…çš„æ„å¤–åè§ã€‚æœ¬æ–‡æå‡ºæ˜¾å¼å»ºæ¨¡å¹¶çº æ­£CLIPä¸­å­˜åœ¨çš„åè§ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚é€šè¿‡è®¾è®¡å¯å­¦ä¹ çš„â€œå‚è€ƒâ€æç¤ºæ¥ç¼–ç ç±»åˆ«åå¥½åè§ï¼Œå¹¶é€šè¿‡è§†è§‰å˜å‹å™¨ä¸­çš„ä½ç½®åµŒå…¥æŠ•å½±æ¥ç¼–ç ç©ºé—´åå¥½åè§ã€‚ç‹¬ç«‹ç¼–ç ä¸¤ç§åè§ï¼Œç”Ÿæˆåå·®é€»è¾‘å›¾ä»¥æ˜¾å¼è¡¨ç¤ºä¸¤ç§åè§ï¼Œå¹¶é€šè¿‡ç®€å•å…ƒç´ å‡æ³•çº æ­£CLIPçš„é€»è¾‘ã€‚è®¾è®¡äº†ä¸€ä¸ªæ©è†œè§£ç å™¨ï¼Œå°†CLIPçš„ç‰¹å¾å’Œçº æ­£åçš„é€»è¾‘å›¾ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºçº æ­£åçš„åˆ†å‰²æ©è†œã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPè¢«ç”¨äºæ‰§è¡Œæ— ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>æ‰§è¡Œåƒç´ çº§åˆ«ç†è§£ä»»åŠ¡æ—¶å­˜åœ¨æ„å¤–åè§ã€‚è¿™äº›åè§åŒ…æ‹¬ç±»åˆ«åå¥½åè§å’Œç©ºé—´åå¥½åè§ã€‚</li>
<li>ä¹‹å‰çš„å·¥ä½œæ²¡æœ‰æ˜¾å¼å»ºæ¨¡è¿™ç§åè§ï¼Œè¿™é™åˆ¶äº†åˆ†å‰²æ€§èƒ½ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºå¯¹CLIPä¸­çš„åè§è¿›è¡Œå»ºæ¨¡å’Œçº æ­£çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è®¾è®¡å¯å­¦ä¹ çš„â€œå‚è€ƒâ€æç¤ºå’Œä½ç½®åµŒå…¥æŠ•å½±æ¥åˆ†åˆ«ç¼–ç ç±»åˆ«åå¥½åè§å’Œç©ºé—´åå¥½åè§ã€‚</li>
<li>é€šè¿‡çŸ©é˜µä¹˜æ³•ç”Ÿæˆåå·®é€»è¾‘å›¾æ¥æ˜¾å¼è¡¨ç¤ºè¿™ä¸¤ç§åè§ï¼Œå¹¶é€šè¿‡å…ƒç´ å‡æ³•çº æ­£CLIPçš„é€»è¾‘ã€‚</li>
<li>è®¾è®¡æ©è†œè§£ç å™¨è¾“å‡ºçº æ­£åçš„åˆ†å‰²æ©è†œï¼Œé‡‡ç”¨Gumbel-Softmaxæ“ä½œå¸®åŠ©ç”Ÿæˆç»“æœã€‚</li>
<li>é€šè¿‡åŸºäºå¯¹æ¯”æŸå¤±çš„æ©ç›–è§†è§‰ç‰¹å¾å’Œä¸åŒç±»åˆ«æ–‡æœ¬ç‰¹å¾çš„å¯¹æ¯”æŸå¤±æ–¹æ³•æ¥å®ç°åå·®å»ºæ¨¡å’Œçº æ­£è¿‡ç¨‹çš„æ„ä¹‰å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06747">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2408.06747v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2408.06747v3/page_2_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multiple-Object-Detection-and-Tracking-in-Panoramic-Videos-for-Cycling-Safety-Analysis"><a href="#Multiple-Object-Detection-and-Tracking-in-Panoramic-Videos-for-Cycling-Safety-Analysis" class="headerlink" title="Multiple Object Detection and Tracking in Panoramic Videos for Cycling   Safety Analysis"></a>Multiple Object Detection and Tracking in Panoramic Videos for Cycling   Safety Analysis</h2><p><strong>Authors:Jingwei Guo, Yitai Cheng, Meihui Wang, Ilya Ilyankou, Natchapon Jongwiriyanurak, Xiaowei Gao, Nicola Christie, James Haworth</strong></p>
<p>Cyclists face a disproportionate risk of injury, yet conventional crash records are too limited to reconstruct the circumstances of incidents or to diagnose risk at the finer spatial and temporal detail needed for targeted interventions. Recently, naturalistic studies have gained traction as a way to capture the complex behavioural and infrastructural factors that contribute to crashes. These approaches typically involve the collection and analysis of video data. A video promising format is panoramic video, which can record 360-degree views around a rider. However, its use is limited by severe distortions, large numbers of small objects and boundary continuity. This study addresses these challenges by proposing a novel three-step framework: (1) enhancing object detection accuracy on panoramic imagery by segmenting and projecting the original 360-degree images into four perspective sub-images, thus reducing distortion; (2) modifying multi-object tracking models to incorporate boundary continuity and object category information for improved tracking consistency; and (3) validating the proposed approach through a real-world application focused on detecting overtaking manoeuvres by vehicles around cyclists. The methodology is evaluated using panoramic videos recorded by cyclists on Londonâ€™s roadways under diverse conditions. Experimental results demonstrate notable improvements over baseline methods, achieving higher average precision across varying image resolutions. Moreover, the enhanced tracking approach yields a 3.0% increase in multi-object tracking accuracy and a 4.6% improvement in identification F-score. The overtaking detection task achieves a high F-score of 0.81, illustrating the practical effectiveness of the proposed method in real-world cycling safety scenarios. The code is available on GitHub (<a target="_blank" rel="noopener" href="https://github.com/SpaceTimeLab/360_object_tracking">https://github.com/SpaceTimeLab/360_object_tracking</a>) to ensure reproducibility. </p>
<blockquote>
<p>éª‘è‡ªè¡Œè½¦çš„äººé¢ä¸´ç€ä¸æˆæ¯”ä¾‹çš„ä¼¤å®³é£é™©ï¼Œç„¶è€Œï¼Œä¼ ç»Ÿçš„ç¢°æ’è®°å½•è¿‡äºå±€é™ï¼Œæ— æ³•é‡å»ºäº‹ä»¶æƒ…å†µæˆ–é’ˆå¯¹éœ€è¦è¿›è¡Œç²¾ç»†æ—¶ç©ºç»†èŠ‚çš„é£é™©è¯Šæ–­ä»¥å®æ–½æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½ã€‚æœ€è¿‘ï¼Œè‡ªç„¶ä¸»ä¹‰ç ”ç©¶ä½œä¸ºä¸€ç§æ•æ‰å¯¼è‡´ç¢°æ’çš„å¤æ‚è¡Œä¸ºå’Œç¯å¢ƒå› ç´ çš„æ–¹æ³•å¼€å§‹å—åˆ°å…³æ³¨ã€‚è¿™äº›æ–¹æ³•é€šå¸¸æ¶‰åŠè§†é¢‘æ•°æ®çš„æ”¶é›†å’Œåˆ†æã€‚ä¸€ç§æœ‰å‰æ™¯çš„è§†é¢‘æ ¼å¼æ˜¯å…¨æ™¯è§†é¢‘ï¼Œå®ƒå¯ä»¥è®°å½•éª‘è¡Œè€…å‘¨å›´çš„360åº¦è§†å›¾ã€‚ç„¶è€Œï¼Œå…¶ä½¿ç”¨å—åˆ°ä¸¥é‡å¤±çœŸã€å¤§é‡å°ç‰©ä½“å’Œè¾¹ç•Œè¿ç»­æ€§é—®é¢˜çš„é™åˆ¶ã€‚æœ¬ç ”ç©¶é€šè¿‡æå‡ºä¸€ä¸ªæ–°å‹çš„ä¸‰æ­¥æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰é€šè¿‡å°†åŸå§‹360åº¦å›¾åƒåˆ†å‰²å¹¶æŠ•å½±åˆ°å››ä¸ªé€è§†å­å›¾åƒä¸­ï¼Œæé«˜å…¨æ™¯å›¾åƒçš„ç›®æ ‡æ£€æµ‹ç²¾åº¦ï¼Œä»è€Œå‡å°‘å¤±çœŸï¼›ï¼ˆ2ï¼‰ä¿®æ”¹å¤šç›®æ ‡è·Ÿè¸ªæ¨¡å‹ï¼Œä»¥çº³å…¥è¾¹ç•Œè¿ç»­æ€§å’Œç›®æ ‡ç±»åˆ«ä¿¡æ¯ï¼Œä»¥æé«˜è·Ÿè¸ªçš„ä¸€è‡´æ€§ï¼›ï¼ˆ3ï¼‰é€šè¿‡å…³æ³¨æ£€æµ‹è‡ªè¡Œè½¦å‘¨å›´è½¦è¾†è¶…è½¦åŠ¨ä½œçš„ç°å®åº”ç”¨æ¥éªŒè¯æ‰€æå‡ºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨éª‘è¡Œè€…åœ¨ä¼¦æ•¦é“è·¯ä¸Šåœ¨å„ç§æ¡ä»¶ä¸‹å½•åˆ¶çš„å…¨æ™¯è§†é¢‘è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§å›¾åƒåˆ†è¾¨ç‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹³å‡ç²¾åº¦æ›´é«˜ã€‚æ­¤å¤–ï¼Œæ”¹è¿›åçš„è·Ÿè¸ªæ–¹æ³•ä½¿å¤šç›®æ ‡è·Ÿè¸ªç²¾åº¦æé«˜äº†3.0%ï¼Œè¯†åˆ«Fåˆ†æ•°æé«˜äº†4.6%ã€‚è¶…è½¦æ£€æµ‹ä»»åŠ¡å–å¾—äº†é«˜è¾¾0.81çš„Fåˆ†æ•°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç°å®éª‘è¡Œå®‰å…¨åœºæ™¯ä¸­çš„å®é™…æœ‰æ•ˆæ€§ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/SpaceTimeLab/360_object_tracking%EF%BC%89%EF%BC%8C%E4%BB%A5%E7%A1%AE%E4%BF%9D%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E3%80%82">https://github.com/SpaceTimeLab/360_object_trackingï¼‰ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15199v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¾ªç¯éª‘è¡Œè€…åœ¨äº‹æ•…ä¸­é¢ä¸´ä¸æˆæ¯”ä¾‹çš„ä¼¤å®³é£é™©ï¼Œä¼ ç»Ÿçš„äº‹æ•…è®°å½•æ–¹æ³•è¿‡äºå±€é™ï¼Œæ— æ³•è¯¦ç»†é‡å»ºäº‹æ•…æƒ…å†µæˆ–é’ˆå¯¹æ›´ç²¾ç»†çš„ç©ºé—´å’Œæ—¶é—´ç»†èŠ‚è¿›è¡Œé£é™©è¯„ä¼°ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰æ­¥æ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºå…¨æ™¯å½±åƒçš„ç›®æ ‡æ£€æµ‹ç²¾åº¦ã€ä¿®æ”¹å¤šç›®æ ‡è·Ÿè¸ªæ¨¡å‹ä»¥èå…¥è¾¹ç•Œè¿ç»­æ€§å’Œç›®æ ‡ç±»åˆ«ä¿¡æ¯ï¼Œå¹¶éªŒè¯åœ¨æ£€æµ‹éª‘è¡Œè€…å‘¨å›´è½¦è¾†è¶…è½¦åŠ¨ä½œæ–¹é¢çš„å®ç”¨æ€§ã€‚è¯¥ç ”ç©¶æ–¹æ³•åœ¨ä¼¦æ•¦é“è·¯éª‘è¡Œè€…çš„å…¨æ™¯è§†é¢‘ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç ”ç©¶æˆæœçš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¾ªç¯éª‘è¡Œè€…é¢ä¸´çš„äº‹æ•…ä¼¤å®³é£é™©ä¸æˆæ¯”ä¾‹ï¼Œä¼ ç»Ÿçš„äº‹æ•…è®°å½•æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è‡ªç„¶ä¸»ä¹‰ç ”ç©¶æ˜¯äº†è§£äº‹æ•…å¤æ‚å› ç´ çš„æœ‰æ•ˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡è§†é¢‘æ•°æ®çš„æ”¶é›†å’Œåˆ†æã€‚</li>
<li>å…¨æ™¯è§†é¢‘åœ¨è®°å½•éª‘è¡Œè€…å‘¨å›´360åº¦æƒ…å†µæ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å­˜åœ¨ä¸¥é‡çš„å¤±çœŸã€å°ç›®æ ‡æ•°é‡è¿‡å¤šå’Œè¾¹ç•Œè¿ç»­æ€§é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰æ­¥æ¡†æ¶ï¼ŒåŒ…æ‹¬å¢å¼ºå…¨æ™¯å½±åƒçš„ç›®æ ‡æ£€æµ‹ç²¾åº¦ã€æ”¹è¿›å¤šç›®æ ‡è·Ÿè¸ªæ¨¡å‹ï¼Œå¹¶éªŒè¯å…¶åœ¨å®é™…æ£€æµ‹è¶…è½¦åŠ¨ä½œæ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>ç ”ç©¶åœ¨ä¼¦æ•¦é“è·¯éª‘è¡Œè€…çš„å…¨æ™¯è§†é¢‘ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬å¹³å‡ç²¾åº¦ã€å¤šç›®æ ‡è·Ÿè¸ªå‡†ç¡®æ€§å’Œè¯†åˆ«Fåˆ†æ•°çš„æé«˜ã€‚</li>
<li>è¿‡é“æ£€æµ‹ä»»åŠ¡çš„é«˜Fåˆ†æ•°è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å®é™…éª‘è¡Œå®‰å…¨åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2407.15199v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2407.15199v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2407.15199v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2407.15199v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Repurposing-SAM-for-User-Defined-Semantics-Aware-Segmentation"><a href="#Repurposing-SAM-for-User-Defined-Semantics-Aware-Segmentation" class="headerlink" title="Repurposing SAM for User-Defined Semantics Aware Segmentation"></a>Repurposing SAM for User-Defined Semantics Aware Segmentation</h2><p><strong>Authors:Rohit Kundu, Sudipta Paul, Arindam Dutta, Amit K. Roy-Chowdhury</strong></p>
<p>The Segment Anything Model (SAM) excels at generating precise object masks from input prompts but lacks semantic awareness, failing to associate its generated masks with specific object categories. To address this limitation, we propose U-SAM, a novel framework that imbibes semantic awareness into SAM, enabling it to generate targeted masks for user-specified object categories. Given only object class names as input from the user, U-SAM provides pixel-level semantic annotations for images without requiring any labeled&#x2F;unlabeled samples from the test data distribution. Our approach leverages synthetically generated or web crawled images to accumulate semantic information about the desired object classes. We then learn a mapping function between SAMâ€™s mask embeddings and object class labels, effectively enhancing SAM with granularity-specific semantic recognition capabilities. As a result, users can obtain meaningful and targeted segmentation masks for specific objects they request, rather than generic and unlabeled masks. We evaluate U-SAM on PASCAL VOC 2012 and MSCOCO-80, achieving significant mIoU improvements of +17.95% and +5.20%, respectively, over state-of-the-art methods. By transforming SAM into a semantically aware segmentation model, U-SAM offers a practical and flexible solution for pixel-level annotation across diverse and unseen domains in a resource-constrained environment. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰åœ¨æ ¹æ®è¾“å…¥æç¤ºç”Ÿæˆç²¾ç¡®å¯¹è±¡æ©è†œæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹è¯­ä¹‰æ„è¯†ï¼Œæ— æ³•å°†å…¶ç”Ÿæˆçš„æ©è†œä¸ç‰¹å®šå¯¹è±¡ç±»åˆ«ç›¸å…³è”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†U-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªå°†è¯­ä¹‰æ„è¯†èå…¥SAMçš„æ–°å‹æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿé’ˆå¯¹ç”¨æˆ·æŒ‡å®šçš„å¯¹è±¡ç±»åˆ«ç”Ÿæˆå®šå‘æ©è†œã€‚U-SAMä»…æ¥æ”¶ç”¨æˆ·æä¾›çš„å¯¹è±¡ç±»åˆ«åç§°ä½œä¸ºè¾“å…¥ï¼Œä¸ºå›¾åƒæä¾›åƒç´ çº§è¯­ä¹‰æ³¨é‡Šï¼Œè€Œæ— éœ€æµ‹è¯•æ•°æ®åˆ†å¸ƒä¸­çš„ä»»ä½•æœ‰æ ‡ç­¾æˆ–æ— æ ‡ç­¾æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨åˆæˆç”Ÿæˆæˆ–ç½‘ç»œçˆ¬è™«å›¾åƒæ¥ç§¯ç´¯æœ‰å…³æ‰€éœ€å¯¹è±¡ç±»åˆ«çš„è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬å­¦ä¹ SAMçš„æ©è†œåµŒå…¥å’Œå¯¹è±¡ç±»åˆ«æ ‡ç­¾ä¹‹é—´çš„æ˜ å°„å‡½æ•°ï¼Œæœ‰æ•ˆåœ°å¢å¼ºSAMçš„ç²’åº¦ç‰¹å®šè¯­ä¹‰è¯†åˆ«èƒ½åŠ›ã€‚å› æ­¤ï¼Œç”¨æˆ·å¯ä»¥è·å–ä»–ä»¬è¯·æ±‚çš„å…·ä½“å¯¹è±¡çš„æœ‰æ„ä¹‰å’Œå®šå‘åˆ†å‰²æ©è†œï¼Œè€Œä¸æ˜¯é€šç”¨å’Œæ— æ ‡ç­¾çš„æ©è†œã€‚æˆ‘ä»¬åœ¨PASCAL VOC 2012å’ŒMSCOCO-80ä¸Šè¯„ä¼°äº†U-SAMï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒmIoUåˆ†åˆ«æé«˜äº†+17.95%å’Œ+5.20%ã€‚é€šè¿‡å°†SAMè½¬å˜ä¸ºè¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²æ¨¡å‹ï¼ŒU-SAMä¸ºèµ„æºå—é™ç¯å¢ƒä¸­çš„è·¨ä¸åŒå’Œæœªè§é¢†åŸŸçš„åƒç´ çº§æ³¨é‡Šæä¾›äº†å®ç”¨ä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02420v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>U-SAMæ¡†æ¶è§£å†³äº†SAMæ¨¡å‹åœ¨ç”Ÿæˆç²¾ç¡®å¯¹è±¡æ©è†œæ—¶ç¼ºä¹è¯­ä¹‰æ„è¯†çš„é—®é¢˜ã€‚é€šè¿‡è¾“å…¥ç”¨æˆ·æŒ‡å®šçš„å¯¹è±¡ç±»åˆ«åç§°ï¼ŒU-SAMä¸ºå›¾åƒæä¾›åƒç´ çº§è¯­ä¹‰æ³¨é‡Šï¼Œæ— éœ€æµ‹è¯•æ•°æ®åˆ†å¸ƒçš„ä»»ä½•æ ‡è®°&#x2F;æœªæ ‡è®°æ ·æœ¬ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åˆæˆå›¾åƒæˆ–ç½‘ç»œçˆ¬è™«ç´¯ç§¯æ‰€éœ€å¯¹è±¡ç±»åˆ«çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå­¦ä¹ SAMçš„æ©è†œåµŒå…¥å’Œå¯¹è±¡ç±»åˆ«æ ‡ç­¾ä¹‹é—´çš„æ˜ å°„å‡½æ•°ï¼Œä»è€Œæé«˜SAMå¯¹ç‰¹å®šç²’åº¦è¯­ä¹‰çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒU-SAMåœ¨PASCAL VOC 2012å’ŒMSCOCO-80ä¸Šåˆ†åˆ«å®ç°äº†æ˜¾è‘—çš„mIoUæ”¹è¿›+17.95%å’Œ+5.2%ã€‚U-SAMå°†SAMè½¬åŒ–ä¸ºå…·æœ‰è¯­ä¹‰æ„è¯†çš„åˆ†å‰²æ¨¡å‹ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­è·¨ä¸åŒå’Œæœªè§é¢†åŸŸçš„åƒç´ çº§æ³¨é‡Šæä¾›äº†å®ç”¨ä¸”çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-SAMè§£å†³äº†SAMæ¨¡å‹ç¼ºä¹è¯­ä¹‰æ„è¯†çš„é—®é¢˜ã€‚</li>
<li>U-SAMå¯ä»¥é€šè¿‡è¾“å…¥ç”¨æˆ·æŒ‡å®šçš„å¯¹è±¡ç±»åˆ«åç§°ï¼Œç”Ÿæˆé’ˆå¯¹ç‰¹å®šå¯¹è±¡çš„åƒç´ çº§è¯­ä¹‰æ³¨é‡Šã€‚</li>
<li>U-SAMåˆ©ç”¨åˆæˆå›¾åƒæˆ–ç½‘ç»œçˆ¬è™«æ•°æ®æ¥ç´¯ç§¯è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>U-SAMé€šè¿‡æ˜ å°„å‡½æ•°æé«˜äº†å¯¹ç‰¹å®šç²’åº¦è¯­ä¹‰çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>U-SAMåœ¨PASCAL VOC 2012å’ŒMSCOCO-80æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„mIoUæ”¹è¿›ã€‚</li>
<li>U-SAMå°†SAMè½¬åŒ–ä¸ºå…·æœ‰è¯­ä¹‰æ„è¯†çš„åˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.02420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2312.02420v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2312.02420v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_æ£€æµ‹_åˆ†å‰²_è·Ÿè¸ª/2312.02420v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/Speech/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Speech/2503.16718v2/page_0_0.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Towards Inclusive Communication A Unified LLM-Based Framework for Sign   Language, Lip Movements, and Audio Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_2_0.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  FedMVP Federated Multimodal Visual Prompt Tuning for Vision-Language   Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
