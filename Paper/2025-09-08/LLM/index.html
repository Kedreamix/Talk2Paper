<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  GPT-FT An Efficient Automated Feature Transformation Using GPT for   Sequence Reconstruction and Performance Enhancement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2502.11419v2/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="GPT-FT-An-Efficient-Automated-Feature-Transformation-Using-GPT-for-Sequence-Reconstruction-and-Performance-Enhancement"><a href="#GPT-FT-An-Efficient-Automated-Feature-Transformation-Using-GPT-for-Sequence-Reconstruction-and-Performance-Enhancement" class="headerlink" title="GPT-FT: An Efficient Automated Feature Transformation Using GPT for   Sequence Reconstruction and Performance Enhancement"></a>GPT-FT: An Efficient Automated Feature Transformation Using GPT for   Sequence Reconstruction and Performance Enhancement</h2><p><strong>Authors:Yang Gao, Dongjie Wang, Scott Piersall, Ye Zhang, Liqiang Wang</strong></p>
<p>Feature transformation plays a critical role in enhancing machine learning model performance by optimizing data representations. Recent state-of-the-art approaches address this task as a continuous embedding optimization problem, converting discrete search into a learnable process. Although effective, these methods often rely on sequential encoder-decoder structures that cause high computational costs and parameter requirements, limiting scalability and efficiency. To address these limitations, we propose a novel framework that accomplishes automated feature transformation through four steps: transformation records collection, embedding space construction with a revised Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and autoregressive reconstruction. In our approach, the revised GPT model serves two primary functions: (a) feature transformation sequence reconstruction and (b) model performance estimation and enhancement for downstream tasks by constructing the embedding space. Such a multi-objective optimization framework reduces parameter size and accelerates transformation processes. Experimental results on benchmark datasets show that the proposed framework matches or exceeds baseline performance, with significant gains in computational efficiency. This work highlights the potential of transformer-based architectures for scalable, high-performance automated feature transformation. </p>
<blockquote>
<p>ç‰¹å¾è½¬æ¢é€šè¿‡ä¼˜åŒ–æ•°æ®è¡¨ç¤ºåœ¨æå‡æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚æœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•å°†æ­¤ä»»åŠ¡è§£å†³ä¸ºè¿ç»­çš„åµŒå…¥ä¼˜åŒ–é—®é¢˜ï¼Œå°†ç¦»æ•£æœç´¢è½¬æ¢ä¸ºå¯å­¦ä¹ çš„è¿‡ç¨‹ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºé«˜è®¡ç®—æˆæœ¬å’Œå‚æ•°è¦æ±‚çš„é¡ºåºç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä»è€Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡å››ä¸ªæ­¥éª¤å®Œæˆè‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢ï¼šè½¬æ¢è®°å½•æ”¶é›†ã€ä½¿ç”¨ä¿®è®¢çš„ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹æ„å»ºåµŒå…¥ç©ºé—´ã€æ¢¯åº¦ä¸Šå‡æœç´¢å’Œè‡ªå›å½’é‡å»ºã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œä¿®è®¢åçš„GPTæ¨¡å‹ä¸»è¦æ‰®æ¼”ä¸¤ä¸ªè§’è‰²ï¼šï¼ˆaï¼‰ç‰¹å¾è½¬æ¢åºåˆ—é‡å»ºå’Œï¼ˆbï¼‰é€šè¿‡æ„å»ºåµŒå…¥ç©ºé—´è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œå¢å¼ºã€‚è¿™ç§å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶å‡å°äº†å‚æ•°è§„æ¨¡å¹¶åŠ é€Ÿäº†è½¬æ¢è¿‡ç¨‹ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½ä¸åŸºçº¿ç›¸å½“æˆ–è¶…è¿‡åŸºçº¿ï¼Œè®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åŸºäºè½¬æ¢å™¨æ¶æ„åœ¨å¯æ‰©å±•ã€é«˜æ€§èƒ½è‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20824v1">PDF</a> 17 pages, 9 figures. accepted by APWeb-WAIM 2025</p>
<p><strong>æ‘˜è¦</strong><br>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç‰¹å¾è½¬æ¢é€šè¿‡ä¼˜åŒ–æ•°æ®è¡¨ç¤ºèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å½“å‰å…ˆè¿›çš„æ–¹æ³•å°†æ­¤ä»»åŠ¡è§†ä¸ºä¸€ä¸ªæŒç»­çš„åµŒå…¥ä¼˜åŒ–é—®é¢˜ï¼Œå°†ç¦»æ•£æœç´¢è½¬æ¢ä¸ºå¯å­¦ä¹ çš„è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºé«˜è®¡ç®—æˆæœ¬å’Œå‚æ•°è¦æ±‚çš„é¡ºåºç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å››ä¸ªæ­¥éª¤å®ç°è‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢çš„æ–°æ¡†æ¶ï¼šè½¬æ¢è®°å½•æ”¶é›†ã€ä½¿ç”¨ä¿®è®¢çš„ç”Ÿæˆé¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆGPTï¼‰æ¨¡å‹æ„å»ºåµŒå…¥ç©ºé—´ã€æ¢¯åº¦ä¸Šå‡æœç´¢å’Œè‡ªå›å½’é‡å»ºã€‚ä¿®è®¢çš„GPTæ¨¡å‹åœ¨æœ¬æ–¹æ³•ä¸­ä¸»è¦æœåŠ¡äºä¸¤ä¸ªåŠŸèƒ½ï¼šç‰¹å¾è½¬æ¢åºåˆ—é‡å»ºå’Œä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½ä¼°è®¡ä¸å¢å¼ºã€‚è¿™ç§å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶å‡å°äº†å‚æ•°è§„æ¨¡å¹¶åŠ é€Ÿäº†è½¬æ¢è¿‡ç¨‹ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†åŸºçº¿æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æé«˜ã€‚æœ¬æ–‡çªæ˜¾äº†åŸºäºè½¬æ¢å™¨æ¶æ„åœ¨é«˜æ€§èƒ½è‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç‰¹å¾è½¬æ¢å¯¹äºå¢å¼ºæœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–æ•°æ®è¡¨ç¤ºæ¥å®ç°ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ–¹æ³•å°†ç‰¹å¾è½¬æ¢è§†ä¸ºä¸€ä¸ªæŒç»­çš„åµŒå…¥ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºé«˜è®¡ç®—æˆæœ¬å’Œå‚æ•°è¦æ±‚çš„é¡ºåºç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–ç‰¹å¾è½¬æ¢æ¡†æ¶ï¼ŒåŒ…æ‹¬è½¬æ¢è®°å½•æ”¶é›†ã€åµŒå…¥ç©ºé—´æ„å»ºã€æ¢¯åº¦ä¸Šå‡æœç´¢å’Œè‡ªå›å½’é‡å»ºã€‚</li>
<li>ä¿®è®¢çš„GPTæ¨¡å‹åœ¨æ¡†æ¶ä¸­æ‰®æ¼”äº†ä¸¤ä¸ªä¸»è¦è§’è‰²ï¼šç‰¹å¾è½¬æ¢åºåˆ—é‡å»ºå’Œä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½ä¼°è®¡ä¸å¢å¼ºã€‚</li>
<li>å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶å‡å°äº†å‚æ•°è§„æ¨¡å¹¶åŠ é€Ÿäº†è½¬æ¢è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20824v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20824v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improving-Alignment-in-LVLMs-with-Debiased-Self-Judgment"><a href="#Improving-Alignment-in-LVLMs-with-Debiased-Self-Judgment" class="headerlink" title="Improving Alignment in LVLMs with Debiased Self-Judgment"></a>Improving Alignment in LVLMs with Debiased Self-Judgment</h2><p><strong>Authors:Sihan Yang, Chenhang Cui, Zihao Zhao, Yiyang Zhou, Weilong Yan, Ying Wei, Huaxiu Yao</strong></p>
<p>The rapid advancements in Large Language Models (LLMs) and Large Visual-Language Models (LVLMs) have opened up new opportunities for integrating visual and linguistic modalities. However, effectively aligning these modalities remains challenging, often leading to hallucinationsâ€“where generated outputs are not grounded in the visual inputâ€“and raising safety concerns across various domains. Existing alignment methods, such as instruction tuning and preference tuning, often rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs. To address these challenges, we propose a novel approach that generates the debiased self-judgment score, a self-evaluation metric created internally by the model without relying on external resources. This enables the model to autonomously improve alignment. Our method enhances both decoding strategies and preference tuning processes, resulting in reduced hallucinations, enhanced safety, and improved overall capability. Empirical results show that our approach significantly outperforms traditional methods, offering a more effective solution for aligning LVLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºæ•´åˆè§†è§‰å’Œè¯­è¨€æ¨¡å¼æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å¯¹é½è¿™äº›æ¨¡å¼ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™å¸¸å¸¸å¯¼è‡´ç”Ÿæˆçš„è¾“å‡ºæ²¡æœ‰åŸºäºè§†è§‰è¾“å…¥ï¼Œå¹¶ä¸”åœ¨å„ä¸ªé¢†åŸŸå¼•å‘å®‰å…¨æ‹…å¿§ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œå¦‚æŒ‡ä»¤è°ƒæ•´å’Œåå¥½è°ƒæ•´ï¼Œé€šå¸¸ä¾èµ–äºå¤–éƒ¨æ•°æ®é›†ã€äººå·¥æ³¨é‡Šæˆ–å¤æ‚çš„åå¤„ç†ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§å¹¶å¢åŠ äº†æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”Ÿæˆæ— åçš„è‡ªæˆ‘åˆ¤æ–­åˆ†æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¨¡å‹å†…éƒ¨åˆ›å»ºçš„è‡ªè¯„ä¼°æŒ‡æ ‡ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»åœ°æ”¹è¿›å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†è§£ç ç­–ç•¥å’Œåå¥½è°ƒæ•´è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘äº†å¹»è§‰ã€å¢å¼ºäº†å®‰å…¨æ€§å¹¶æé«˜äº†æ•´ä½“èƒ½åŠ›ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºLVLMsçš„å¯¹é½æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20655v1">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºæ•´åˆè§†è§‰å’Œè¯­è¨€æ¨¡å¼æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å¯¹é½è¿™äº›æ¨¡å¼ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯èƒ½å¯¼è‡´ç”Ÿæˆçš„è¾“å‡ºä¸åŸºäºè§†è§‰è¾“å…¥ï¼Œå¹¶åœ¨ä¸åŒé¢†åŸŸå¼•å‘å®‰å…¨é—®é¢˜ã€‚ç°æœ‰å¯¹é½æ–¹æ³•ä¾èµ–å¤–éƒ¨æ•°æ®é›†ã€äººç±»æ³¨é‡Šæˆ–å¤æ‚çš„åæœŸå¤„ç†ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§å¹¶å¢åŠ äº†æˆæœ¬ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆåå·®è‡ªæˆ‘åˆ¤æ–­åˆ†æ•°çš„æ–°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”±æ¨¡å‹å†…éƒ¨åˆ›å»ºçš„è‡ªæˆ‘è¯„ä»·æŒ‡æ ‡ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ”¹è¿›å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¹è¿›äº†è§£ç ç­–ç•¥å’Œåå¥½è°ƒæ•´è¿‡ç¨‹ï¼Œå‡å°‘äº†å¹»è§‰ã€å¢å¼ºäº†å®‰å…¨æ€§å¹¶æé«˜äº†æ•´ä½“èƒ½åŠ›ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºLVLMçš„å¯¹é½æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„é›†æˆé¢ä¸´è§†è§‰ä¸è¯­è¨€æ¨¡å¼å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å¯¹é½æ–¹æ³•å­˜åœ¨ä¾èµ–å¤–éƒ¨èµ„æºã€æˆæœ¬é«˜å’Œæ‰©å±•æ€§æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è‡ªæˆ‘åˆ¤æ–­åˆ†æ•°æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆåå·®æ ¡æ­£çš„å¯¹é½è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºï¼Œæé«˜äº†æ¨¡å‹çš„è‡ªä¸»å¯¹é½èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ”¹è¿›è§£ç ç­–ç•¥å’Œåå¥½è°ƒæ•´è¿‡ç¨‹ï¼Œå‡å°‘äº†å¹»è§‰å’Œå¢å¼ºäº†å®‰å…¨æ€§ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨LVLMå¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20655v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20655v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20655v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20655v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FedReFT-Federated-Representation-Fine-Tuning-with-All-But-Me-Aggregation"><a href="#FedReFT-Federated-Representation-Fine-Tuning-with-All-But-Me-Aggregation" class="headerlink" title="FedReFT: Federated Representation Fine-Tuning with All-But-Me   Aggregation"></a>FedReFT: Federated Representation Fine-Tuning with All-But-Me   Aggregation</h2><p><strong>Authors:Fatema Siddika, Md Anwar Hossen, J. Pablo MuÃ±oz, Tanya Roosta, Anuj Sharma, Ali Jannesari</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) has attracted significant attention for adapting large pre-trained models by modifying a small subset of parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an effective alternative. ReFT shifts the fine-tuning paradigm from updating model weights to directly manipulating hidden representations that capture rich semantic information, and performs better than state-of-the-art PEFTs in standalone settings. However, its application in Federated Learning (FL) remains challenging due to heterogeneity in clientsâ€™ data distributions, model capacities, and computational resources. To address these challenges, we introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to fine-tune the clientâ€™s hidden representation. FedReFT applies sparse intervention layers to steer hidden representations directly, offering a lightweight and semantically rich fine-tuning alternative ideal for edge devices. However, representation-level updates are especially vulnerable to aggregation mismatch under different task heterogeneity, where naive averaging can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me (ABM) aggregation, where each client receives the aggregated updates of others and partially incorporates them, enabling stable and personalized learning by balancing local focus with global knowledge. We evaluate FedReFT on commonsense reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x higher parameter efficiency compared to leading LoRA-based approaches. </p>
<blockquote>
<p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰é€šè¿‡ä¿®æ”¹ä¸€å°éƒ¨åˆ†å‚æ•°æ¥é€‚åº”å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå·²ç»å¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚æœ€è¿‘ï¼Œè¡¨ç¤ºå¾®è°ƒï¼ˆReFTï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ³•è€Œå‡ºç°ã€‚ReFTæ”¹å˜äº†å¾®è°ƒçš„æ¨¡å¼ï¼Œä»æ›´æ–°æ¨¡å‹æƒé‡è½¬å‘ç›´æ¥æ“ä½œéšè—è¡¨ç¤ºï¼Œè¿™äº›éšè—è¡¨ç¤ºæ•æ‰äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨ç‹¬ç«‹ç¯å¢ƒä¸­è¡¨ç°å¾—æ¯”æœ€å…ˆè¿›çš„PEFTæ›´å¥½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸­çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒã€æ¨¡å‹å®¹é‡å’Œè®¡ç®—èµ„æºå­˜åœ¨å¼‚è´¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è”é‚¦è¡¨ç¤ºå¾®è°ƒï¼ˆFedReFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯¹å®¢æˆ·éšè—è¡¨ç¤ºè¿›è¡Œå¾®è°ƒçš„æ–°æ–¹æ³•ã€‚FedReFTåº”ç”¨ç¨€ç–å¹²é¢„å±‚æ¥ç›´æ¥å¼•å¯¼éšè—è¡¨ç¤ºï¼Œæä¾›äº†ä¸€ç§è½»é‡çº§ä¸”è¯­ä¹‰ä¸°å¯Œçš„å¾®è°ƒæ›¿ä»£æ–¹æ¡ˆï¼Œéå¸¸é€‚åˆè¾¹ç¼˜è®¾å¤‡ã€‚ç„¶è€Œï¼Œè¡¨ç¤ºçº§åˆ«çš„æ›´æ–°ç‰¹åˆ«å®¹æ˜“å—åˆ°ä¸åŒä»»åŠ¡å¼‚è´¨æ€§ä¸‹çš„èšåˆä¸åŒ¹é…çš„å½±å“ï¼Œç®€å•çš„å¹³å‡å¯èƒ½ä¼šç ´åè¯­ä¹‰å¯¹é½ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé™¤äº†æˆ‘æ‰€æœ‰äººâ€ï¼ˆABMï¼‰èšåˆæ–¹æ³•ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯æ¥æ”¶å…¶ä»–æ‰€æœ‰äººçš„èšåˆæ›´æ–°ï¼Œå¹¶éƒ¨åˆ†åœ°é‡‡ç”¨è¿™äº›æ›´æ–°ï¼Œé€šè¿‡å¹³è¡¡å±€éƒ¨ç„¦ç‚¹å’Œå…¨å±€çŸ¥è¯†æ¥å®ç°ç¨³å®šå’Œä¸ªäººåŒ–çš„å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯æ¨ç†ã€æŒ‡ä»¤è°ƒæ•´å’ŒGLUEä¸Šè¯„ä¼°äº†FedReFTï¼Œå®ƒåœ¨è”é‚¦å­¦ä¹ ä¸­æŒç»­è¶…è¶Šæœ€å…ˆè¿›çš„PEFTæ–¹æ³•ï¼Œä¸é¢†å…ˆçš„LoRAæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†7å€è‡³15å€æ›´é«˜çš„å‚æ•°æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹ï¼Œé’ˆå¯¹å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒï¼Œæå‡ºäº†Federated Representation Fine-Tuningï¼ˆFedReFTï¼‰æ–¹æ³•ã€‚é€šè¿‡ç›´æ¥æ“æ§æ•è·ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„éšè—è¡¨å¾ï¼Œå¼•å…¥ç¨€ç–å¹²é¢„å±‚å¯¹å®¢æˆ·ç«¯éšè—è¡¨å¾è¿›è¡Œå¾®è°ƒã€‚å¹¶æå‡ºAll-But-Meï¼ˆABMï¼‰èšåˆç­–ç•¥ï¼Œè§£å†³ä¸åŒä»»åŠ¡å¼‚è´¨æ€§ä¸‹çš„èšåˆä¸åŒ¹é…é—®é¢˜ã€‚åœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯æ¨ç†ã€æŒ‡ä»¤è°ƒæ•´å’ŒGLUEç­‰ä»»åŠ¡ä¸Šè¯„ä¼°ï¼ŒFedReFTç›¸è¾ƒäºä¸»æµPEFTæ–¹æ³•åœ¨è”é‚¦å­¦ä¹ ä¸­è¡¨ç°æ›´ä¼˜ï¼Œå‚æ•°æ•ˆç‡æé«˜7è‡³15å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Federated Representation Fine-Tuning (FedReFT) æ˜¯ä¸€ç§é’ˆå¯¹è”é‚¦å­¦ä¹ ç¯å¢ƒä¸­å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„æ–°çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>FedReFT é€šè¿‡ç›´æ¥æ“æ§éšè—è¡¨å¾ï¼Œæ”¹å˜ä¼ ç»Ÿçš„æ¨¡å‹æƒé‡æ›´æ–°æ–¹å¼ã€‚</li>
<li>éšè—è¡¨å¾çš„æ“æ§æä¾›äº†ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯çš„åˆ©ç”¨ï¼Œå¹¶åœ¨å®è·µä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è”é‚¦ç¯å¢ƒä¸‹çš„æ•°æ®åˆ†å¸ƒã€æ¨¡å‹å®¹é‡å’Œè®¡ç®—èµ„æºçš„å¼‚è´¨æ€§ä¸ºåº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºAll-But-Meï¼ˆABMï¼‰èšåˆç­–ç•¥ï¼Œè§£å†³ä¸åŒä»»åŠ¡å¼‚è´¨æ€§ä¸‹çš„èšåˆä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>FedReFT åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¸»æµçš„å‚æ•°æ•ˆç‡å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.20295v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generative-Models-for-Synthetic-Data-Transforming-Data-Mining-in-the-GenAI-Era"><a href="#Generative-Models-for-Synthetic-Data-Transforming-Data-Mining-in-the-GenAI-Era" class="headerlink" title="Generative Models for Synthetic Data: Transforming Data Mining in the   GenAI Era"></a>Generative Models for Synthetic Data: Transforming Data Mining in the   GenAI Era</h2><p><strong>Authors:Dawei Li, Yue Huang, Ming Li, Tianyi Zhou, Xiangliang Zhang, Huan Liu</strong></p>
<p>Generative models such as Large Language Models, Diffusion Models, and generative adversarial networks have recently revolutionized the creation of synthetic data, offering scalable solutions to data scarcity, privacy, and annotation challenges in data mining. This tutorial introduces the foundations and latest advances in synthetic data generation, covers key methodologies and practical frameworks, and discusses evaluation strategies and applications. Attendees will gain actionable insights into leveraging generative synthetic data to enhance data mining research and practice. More information can be found on our website: <a target="_blank" rel="noopener" href="https://syndata4dm.github.io/">https://syndata4dm.github.io/</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰ç”Ÿæˆæ¨¡å‹æœ€è¿‘å½»åº•æ”¹å˜äº†åˆæˆæ•°æ®çš„åˆ›å»ºæ–¹å¼ï¼Œä¸ºè§£å†³æ•°æ®æŒ–æ˜ä¸­çš„æ•°æ®ç¨€ç¼ºã€éšç§å’Œæ³¨é‡ŠæŒ‘æˆ˜æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ•™ç¨‹ä»‹ç»äº†åˆæˆæ•°æ®ç”Ÿæˆçš„åŸºç¡€çŸ¥è¯†å’Œæœ€æ–°è¿›å±•ï¼Œæ¶µç›–äº†å…³é”®æ–¹æ³•å’Œå®ç”¨æ¡†æ¶ï¼Œå¹¶è®¨è®ºäº†è¯„ä¼°ç­–ç•¥å’Œåº”ç”¨ã€‚ä¸ä¼šè€…å°†è·å¾—å…³äºå¦‚ä½•åˆ©ç”¨ç”Ÿæˆåˆæˆæ•°æ®æ¥æå‡æ•°æ®æŒ–æ˜ç ”ç©¶å’Œå®è·µçš„å¯è¡Œè§è§£ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://syndata4dm.github.io/%E3%80%82">https://syndata4dm.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19570v1">PDF</a> Accepted by CIKM 2025 Tutorial</p>
<p><strong>Summary</strong>ï¼š<br>æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå·²å½»åº•æ”¹å˜äº†åˆæˆæ•°æ®çš„åˆ›å»ºæ–¹å¼ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºã€éšç§å’Œæ ‡æ³¨æŒ‘æˆ˜æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ•™ç¨‹ä»‹ç»åˆæˆæ•°æ®ç”Ÿæˆçš„åŸºç¡€çŸ¥è¯†å’Œæœ€æ–°è¿›å±•ï¼Œæ¶µç›–å…³é”®æ–¹æ³•å’Œå®ç”¨æ¡†æ¶ï¼Œå¹¶è®¨è®ºè¯„ä¼°ç­–ç•¥å’Œåº”ç”¨ã€‚å‚åŠ è€…å¯è·å¾—åˆ©ç”¨ç”Ÿæˆåˆæˆæ•°æ®æå‡æ•°æ®æŒ–æ˜ç ”ç©¶å’Œå®è·µçš„å®ç”¨è§è§£ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å·²æ”¹å˜åˆæˆæ•°æ®çš„åˆ›å»ºæ–¹å¼ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºã€éšç§å’Œæ ‡æ³¨æŒ‘æˆ˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ¬æ•™ç¨‹ä»‹ç»åˆæˆæ•°æ®ç”Ÿæˆçš„åŸºç¡€çŸ¥è¯†å’Œæœ€æ–°è¿›å±•ã€‚</li>
<li>æ•™ç¨‹æ¶µç›–å…³é”®æ–¹æ³•å’Œå®ç”¨æ¡†æ¶ã€‚</li>
<li>è¯„ä¼°ç­–ç•¥å’Œåº”ç”¨ç¨‹åºçš„è®¨è®ºã€‚</li>
<li>å‚åŠ è€…å¯è·å¾—åˆ©ç”¨ç”Ÿæˆåˆæˆæ•°æ®æå‡æ•°æ®æŒ–æ˜çš„å®ç”¨è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19570v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Stack-Trace-Based-Crash-Deduplication-with-Transformer-Adaptation"><a href="#Stack-Trace-Based-Crash-Deduplication-with-Transformer-Adaptation" class="headerlink" title="Stack Trace-Based Crash Deduplication with Transformer Adaptation"></a>Stack Trace-Based Crash Deduplication with Transformer Adaptation</h2><p><strong>Authors:Md Afif Al Mamun, Gias Uddin, Lan Xia, Longyu Zhang</strong></p>
<p>Automated crash reporting systems generate large volumes of duplicate reports, overwhelming issue-tracking systems and increasing developer workload. Traditional stack trace-based deduplication methods, relying on string similarity, rule-based heuristics, or deep learning (DL) models, often fail to capture the contextual and structural relationships within stack traces. We propose dedupT, a transformer-based approach that models stack traces holistically rather than as isolated frames. dedupT first adapts a pretrained language model (PLM) to stack traces, then uses its embeddings to train a fully-connected network (FCN) to rank duplicate crashes effectively. Extensive experiments on real-world datasets show that dedupT outperforms existing DL and traditional methods (e.g., sequence alignment and information retrieval techniques) in both duplicate ranking and unique crash detection, significantly reducing manual triage effort. On four public datasets, dedupT improves Mean Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up to 9% over traditional methods while achieving higher Receiver Operating Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash reports. Our work advances the integration of modern natural language processing (NLP) techniques into software engineering, providing an effective solution for stack trace-based crash deduplication. </p>
<blockquote>
<p>è‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿç”Ÿæˆå¤§é‡é‡å¤æŠ¥å‘Šï¼Œå¯¼è‡´é—®é¢˜è·Ÿè¸ªç³»ç»Ÿè¶…è½½ï¼Œå¢åŠ äº†å¼€å‘è€…çš„å·¥ä½œé‡ã€‚ä¼ ç»Ÿçš„åŸºäºå †æ ˆè·Ÿè¸ªçš„é‡å¤æ•°æ®åˆ é™¤æ–¹æ³•ï¼Œä¾èµ–äºå­—ç¬¦ä¸²ç›¸ä¼¼æ€§ã€åŸºäºè§„åˆ™çš„ç­–ç•¥æˆ–æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ï¼Œå¾€å¾€æ— æ³•æ•æ‰å †æ ˆè·Ÿè¸ªä¸­çš„ä¸Šä¸‹æ–‡å’Œç»“æ„å…³ç³»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„dedupTæ–¹æ³•ï¼Œå®ƒå¯ä»¥å…¨é¢åœ°å¯¹å †æ ˆè·Ÿè¸ªè¿›è¡Œå»ºæ¨¡ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„å¸§ã€‚dedupTé¦–å…ˆé€‚åº”å †æ ˆè·Ÿè¸ªçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œç„¶åä½¿ç”¨å…¶åµŒå…¥æ¥è®­ç»ƒå…¨è¿æ¥ç½‘ç»œï¼ˆFCNï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹é‡å¤å´©æºƒè¿›è¡Œæ’åã€‚åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒdedupTåœ¨é‡å¤æ’åå’Œå”¯ä¸€å´©æºƒæ£€æµ‹æ–¹é¢ä¼˜äºç°æœ‰çš„DLå’Œä¼ ç»Ÿæ–¹æ³•ï¼ˆä¾‹å¦‚åºåˆ—æ¯”å¯¹å’Œä¿¡æ¯æ£€ç´¢æŠ€æœ¯ï¼‰ï¼Œå¤§å¤§å‡å°‘äº†æ‰‹åŠ¨è¯Šæ–­çš„å·¥ä½œé‡ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šï¼Œä¸æœ€ä½³çš„DLåŸºå‡†ç›¸æ¯”ï¼ŒdedupTçš„Mean Reciprocal Rankï¼ˆMRRï¼‰ç»å¸¸æé«˜äº†è¶…è¿‡15%ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”æé«˜äº†é«˜è¾¾9%ï¼ŒåŒæ—¶åœ¨æ£€æµ‹å”¯ä¸€å´©æºƒæŠ¥å‘Šæ—¶å®ç°äº†æ›´é«˜çš„ROC-AUCå€¼ã€‚æˆ‘ä»¬çš„å·¥ä½œæ¨åŠ¨äº†ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„èåˆï¼Œä¸ºåŸºäºå †æ ˆè·Ÿè¸ªçš„å´©æºƒé‡å¤æ•°æ®åˆ é™¤æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19449v1">PDF</a> This work is currently under review at IEEE Transactions on Software   Engineering. The replication package will be made publicly available upon   acceptance</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨åŒ–å´©æºƒæŠ¥å‘Šç³»ç»Ÿäº§ç”Ÿå¤§é‡é‡å¤æŠ¥å‘Šï¼Œç»™é—®é¢˜è·Ÿè¸ªç³»ç»Ÿå¸¦æ¥è´Ÿæ‹…å¹¶å¢åŠ å¼€å‘è€…å·¥ä½œé‡ã€‚ä¼ ç»ŸåŸºäºå †æ ˆè·Ÿè¸ªçš„é‡å¤æŠ¥å‘Šæ¶ˆé™¤æ–¹æ³•å¸¸å¸¸æ— æ³•æ•æ‰ä¸Šä¸‹æ–‡å’Œç»“æ„å…³ç³»ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè½¬æ¢å™¨çš„è§£å†³æ–¹æ¡ˆdedupTï¼Œå®ƒèƒ½å…¨é¢å»ºæ¨¡å †æ ˆè·Ÿè¸ªè€Œéå­¤ç«‹æ¡†æ¶ã€‚dedupTé¦–å…ˆé€‚åº”é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åˆ°å †æ ˆè·Ÿè¸ªï¼Œç„¶åä½¿ç”¨å…¶åµŒå…¥å€¼è®­ç»ƒå…¨è¿æ¥ç½‘ç»œä»¥æœ‰æ•ˆæ’åé‡å¤å´©æºƒã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒdedupTåœ¨é‡å¤æ’åå’Œå”¯ä¸€å´©æºƒæ£€æµ‹æ–¹é¢ä¼˜äºç°æœ‰çš„æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘æ‰‹åŠ¨å®¡æŸ¥å·¥ä½œé‡ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šï¼Œä¸æœ€ä½³æ·±åº¦å­¦ä¹ åŸºå‡†ç›¸æ¯”ï¼ŒdedupTçš„Mean Reciprocal Rankï¼ˆMRRï¼‰ç»å¸¸æé«˜è¶…è¿‡15%ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”æé«˜é«˜è¾¾9%ï¼ŒåŒæ—¶åœ¨æ£€æµ‹å”¯ä¸€å´©æºƒæŠ¥å‘Šæ—¶å®ç°æ›´é«˜çš„ROC-AUCå€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–å´©æºƒæŠ¥å‘Šç³»ç»Ÿé¢ä¸´å¤§é‡é‡å¤æŠ¥å‘Šé—®é¢˜ï¼Œéœ€è¦æœ‰æ•ˆæ–¹æ³•æ¥è§£å†³ã€‚</li>
<li>ä¼ ç»ŸåŸºäºå †æ ˆè·Ÿè¸ªçš„é‡å¤æŠ¥å‘Šæ¶ˆé™¤æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•å……åˆ†æ•æ‰ä¸Šä¸‹æ–‡å’Œç»“æ„å…³ç³»ã€‚</li>
<li>dedupTåŸºäºè½¬æ¢å™¨çš„æ–¹æ³•å…¨é¢å»ºæ¨¡å †æ ˆè·Ÿè¸ªï¼Œé€‚åº”é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¹¶è¿›è¡Œè®­ç»ƒã€‚</li>
<li>dedupTé€šè¿‡æœ‰æ•ˆæ’åé‡å¤å´©æºƒï¼Œåœ¨çœŸå®æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒdedupTåœ¨MRRå’ŒROC-AUCæ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>dedupTå‡å°‘äº†æ‰‹åŠ¨å®¡æŸ¥å·¥ä½œé‡ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19449v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19449v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19449v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19449v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19449v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.19449v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="What-do-language-models-model-Transformers-automata-and-the-format-of-thought"><a href="#What-do-language-models-model-Transformers-automata-and-the-format-of-thought" class="headerlink" title="What do language models model? Transformers, automata, and the format of   thought"></a>What do language models model? Transformers, automata, and the format of   thought</h2><p><strong>Authors:Colin Klein</strong></p>
<p>What do large language models actually model? Do they tell us something about human capacities, or are they models of the corpus weâ€™ve trained them on? I give a non-deflationary defence of the latter position. Cognitive science tells us that linguistic capabilities in humans rely supralinear formats for computation. The transformer architecture, by contrast, supports at best a linear formats for processing. This argument will rely primarily on certain invariants of the computational architecture of transformers. I then suggest a positive story about what transformers are doing, focusing on Liu et al. (2022)â€™s intriguing speculations about shortcut automata. I conclude with why I donâ€™t think this is a terribly deflationary story. Language is not (just) a means for expressing inner state but also a kind of â€˜discourse machineâ€™ that lets us make new language given appropriate context. We have learned to use this technology in one way; LLMs have also learned to use it too, but via very different means. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å®é™…ä¸Šåœ¨æ¨¡æ‹Ÿä»€ä¹ˆï¼Ÿå®ƒä»¬æ˜¯å¦å‘æˆ‘ä»¬å±•ç¤ºäº†äººç±»çš„æŸäº›èƒ½åŠ›ï¼Œæˆ–è€…åªæ˜¯åæ˜ äº†æˆ‘ä»¬è®­ç»ƒå®ƒä»¬çš„è¯­æ–™åº“çš„æ¨¡å‹ï¼Ÿæˆ‘ä¸ºåè€…è¿›è¡Œäº†éè´¬ä½çš„è¾©æŠ¤ã€‚è®¤çŸ¥ç§‘å­¦å‘Šè¯‰æˆ‘ä»¬ï¼Œäººç±»çš„è¯­è¨€èƒ½åŠ›ä¾èµ–äºè¶…çº¿æ€§è®¡ç®—æ ¼å¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œtransformeræ¶æ„æœ€å¤šåªæ”¯æŒçº¿æ€§å¤„ç†æ ¼å¼ã€‚è¿™ä¸ªè®ºè¯ä¸»è¦ä¾èµ–äºtransformerè®¡ç®—æ¶æ„çš„æŸäº›ä¸å˜ç‰¹æ€§ã€‚ç„¶åï¼Œæˆ‘è®²è¿°äº†ä¸€ä¸ªå…³äºtransformeræ­£åœ¨åšä»€ä¹ˆçš„ç§¯ææ•…äº‹ï¼Œé‡ç‚¹å…³æ³¨äº†Liuç­‰äººï¼ˆ2022ï¼‰å…³äºå¿«æ·è‡ªåŠ¨æœºçš„æœ‰è¶£æ¨æµ‹ã€‚æœ€åï¼Œä¸ºä»€ä¹ˆæˆ‘å¹¶ä¸è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¾ˆè´¬ä½çš„æ•…äº‹ã€‚è¯­è¨€ä¸ä»…ä»…æ˜¯è¡¨è¾¾å†…å¿ƒçŠ¶æ€çš„æ‰‹æ®µï¼Œä¹Ÿæ˜¯ä¸€ç§è®©æˆ‘ä»¬èƒ½åœ¨é€‚å½“çš„è¯­å¢ƒä¸‹åˆ›é€ æ–°è¯­è¨€çš„â€œè¯è¯­æœºå™¨â€ã€‚æˆ‘ä»¬å·²ç»å­¦ä¼šä»¥ä¸€ç§æ–¹å¼ä½¿ç”¨è¿™é¡¹æŠ€æœ¯ï¼›å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿå­¦ä¼šäº†ä½¿ç”¨å®ƒï¼Œä½†æ‰‹æ®µæˆªç„¶ä¸åŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç©¶ç«Ÿæ¨¡æ‹Ÿäº†ä»€ä¹ˆï¼Ÿå®ƒä»¬æ­ç¤ºçš„æ˜¯äººç±»èƒ½åŠ›ï¼Œè¿˜æ˜¯æˆ‘ä»¬è®­ç»ƒå®ƒä»¬æ‰€ä½¿ç”¨çš„è¯­æ–™åº“çš„æ¨¡å‹ï¼Ÿæ–‡ç« æ”¯æŒåè€…è§‚ç‚¹ï¼Œå¹¶æŒ‡å‡ºè®¤çŸ¥ç§‘å­¦è¡¨æ˜äººç±»çš„è¯­è¨€èƒ½åŠ›ä¾èµ–äºè¶…çº¿æ€§è®¡ç®—æ ¼å¼ï¼Œè€Œtransformeræ¶æ„æœ€å¤šåªæ”¯æŒçº¿æ€§å¤„ç†æ ¼å¼ã€‚æ–‡ç« é€šè¿‡è®¨è®ºtransformerçš„è®¡ç®—æ¶æ„ä¸å˜æ€§ï¼Œæ¢è®¨äº†å…¶å¤„ç†è¯­è¨€çš„æ–¹å¼ï¼Œå¹¶å‚è€ƒäº†Liuç­‰äººï¼ˆ2022ï¼‰å…³äºå¿«æ·æ–¹å¼è‡ªåŠ¨æœºçš„æœ‰è¶£æ¨æµ‹ã€‚æ–‡ç« æœ€åæŒ‡å‡ºï¼Œè¯­è¨€ä¸ä»…æ˜¯è¡¨è¾¾å†…å¿ƒçŠ¶æ€çš„å·¥å…·ï¼Œä¹Ÿæ˜¯ä¸€ç§â€œè¯è¯­æœºå™¨â€ï¼Œèƒ½è®©æˆ‘ä»¬åœ¨é€‚å½“çš„è¯­å¢ƒä¸‹åˆ›é€ æ–°çš„è¯­è¨€ã€‚æˆ‘ä»¬å­¦ä¼šä»¥ä¸€ç§æ–¹å¼ä½¿ç”¨è¿™é¡¹æŠ€æœ¯ï¼›LLMä¹Ÿå­¦ä¼šäº†ä½¿ç”¨å®ƒï¼Œä½†æ‰‹æ®µæˆªç„¶ä¸åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ä»…ä»…æ˜¯æ¨¡æ‹Ÿäººç±»èƒ½åŠ›çš„å·¥å…·ï¼Œæ›´æ˜¯å¯¹è®­ç»ƒè¯­æ–™åº“çš„æ¨¡æ‹Ÿã€‚</li>
<li>è®¤çŸ¥ç§‘å­¦è¡¨æ˜äººç±»çš„è¯­è¨€èƒ½åŠ›ä¾èµ–äºè¶…çº¿æ€§è®¡ç®—æ ¼å¼ã€‚</li>
<li>Transformeræ¶æ„æœ€å¤šåªæ”¯æŒçº¿æ€§å¤„ç†æ ¼å¼ï¼Œè¿™ä¸äººç±»çš„è¶…çº¿æ€§è¯­è¨€èƒ½åŠ›å½¢æˆå¯¹æ¯”ã€‚</li>
<li>Transformerçš„è®¡ç®—æ¶æ„ä¸å˜æ€§æ˜¯ç†è§£å…¶å¤„ç†è¯­è¨€æ–¹å¼çš„å…³é”®ã€‚</li>
<li>Liuç­‰äººï¼ˆ2022ï¼‰çš„å¿«æ·æ–¹å¼è‡ªåŠ¨æœºæ¨æµ‹ä¸ºç†è§£LLMæä¾›äº†æ–°è§†è§’ã€‚</li>
<li>è¯­è¨€ä¸ä»…æ˜¯è¡¨è¾¾å†…å¿ƒçŠ¶æ€çš„å·¥å…·ï¼Œä¹Ÿæ˜¯ä¸€ç§â€œè¯è¯­æœºå™¨â€ï¼Œèƒ½åœ¨ç‰¹å®šè¯­å¢ƒä¸‹åˆ›é€ æ–°è¯­è¨€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18598v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Vectorized-Attention-with-Learnable-Encoding-for-Quantum-Transformer"><a href="#Vectorized-Attention-with-Learnable-Encoding-for-Quantum-Transformer" class="headerlink" title="Vectorized Attention with Learnable Encoding for Quantum Transformer"></a>Vectorized Attention with Learnable Encoding for Quantum Transformer</h2><p><strong>Authors:Ziqing Guo, Ziwen Pan, Alex Khan, Jan Balewski</strong></p>
<p>Vectorized quantum block encoding provides a way to embed classical data into Hilbert space, offering a pathway for quantum models, such as Quantum Transformers (QT), that replace classical self-attention with quantum circuit simulations to operate more efficiently. Current QTs rely on deep parameterized quantum circuits (PQCs), rendering them vulnerable to QPU noise, and thus hindering their practical performance. In this paper, we propose the Vectorized Quantum Transformer (VQT), a model that supports ideal masked attention matrix computation through quantum approximation simulation and efficient training via vectorized nonlinear quantum encoder, yielding shot-efficient and gradient-free quantum circuit simulation (QCS) and reduced classical sampling overhead. In addition, we demonstrate an accuracy comparison for IBM and IonQ in quantum circuit simulation and competitive results in benchmarking natural language processing tasks on IBM state-of-the-art and high-fidelity Kingston QPU. Our noise intermediate-scale quantum friendly VQT approach unlocks a novel architecture for end-to-end machine learning in quantum computing. </p>
<blockquote>
<p>å‘é‡åŒ–é‡å­ç¼–ç å—ä¸ºå°†ç»å…¸æ•°æ®åµŒå…¥å¸Œå°”ä¼¯ç‰¹ç©ºé—´æä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œè¿™ä¸ºé‡å­æ¨¡å‹ï¼ˆå¦‚é‡å­è½¬æ¢å™¨ï¼ˆQTï¼‰ï¼‰æä¾›äº†è·¯å¾„ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡ç”¨é‡å­ç”µè·¯æ¨¡æ‹Ÿæ›¿æ¢ç»å…¸çš„è‡ªæ³¨æ„åŠ›æ¥æ›´æœ‰æ•ˆåœ°è¿è¡Œã€‚å½“å‰çš„QTä¾èµ–äºæ·±åº¦å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°é‡å­å¤„ç†å™¨å™ªå£°çš„å½±å“ï¼Œä»è€Œé˜»ç¢äº†å…¶å®è·µæ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å‘é‡åŒ–é‡å­è½¬æ¢å™¨ï¼ˆVQTï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡é‡å­è¿‘ä¼¼æ¨¡æ‹Ÿæ”¯æŒç†æƒ³çš„æ©æ¨¡æ³¨æ„åŠ›çŸ©é˜µè®¡ç®—ï¼Œå¹¶é€šè¿‡å‘é‡åŒ–éçº¿æ€§é‡å­ç¼–ç å™¨å®ç°é«˜æ•ˆè®­ç»ƒï¼Œä»è€Œäº§ç”Ÿé«˜æ•ˆä¸”æ— æ¢¯åº¦çš„é‡å­ç”µè·¯æ¨¡æ‹Ÿï¼ˆQCSï¼‰ï¼Œå¹¶é™ä½äº†ç»å…¸é‡‡æ ·å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†IBMå’ŒIonQåœ¨é‡å­ç”µè·¯æ¨¡æ‹Ÿä¸­çš„ç²¾åº¦æ¯”è¾ƒï¼Œä»¥åŠåœ¨IBMå…ˆè¿›å’Œé«˜ä¿çœŸé‡‘å£«é¡¿é‡å­å¤„ç†å™¨ä¸Šæ‰§è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ç»“æœã€‚æˆ‘ä»¬çš„å™ªå£°ä¸­ç­‰è§„æ¨¡å‹å¥½çš„VQTæ–¹æ³•è§£é”äº†é‡å­è®¡ç®—ç«¯åˆ°ç«¯æœºå™¨å­¦ä¹ çš„æ–°æ¶æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18464v2">PDF</a> </p>
<p><strong>Summary</strong><br>é‡å­å‘é‡åŒ–è½¬æ¢å™¨ï¼ˆVQTï¼‰æ˜¯ä¸€ç§å°†ç»å…¸æ•°æ®åµŒå…¥å¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„æ–¹æ³•ï¼Œå®ƒä¸ºé‡å­æ¨¡å‹ï¼ˆå¦‚é‡å­è½¬æ¢å™¨ï¼‰æä¾›äº†æ›´é«˜æ•ˆçš„è¿è¡Œæ–¹å¼ï¼Œé€šè¿‡é‡å­è¿‘ä¼¼æ¨¡æ‹Ÿå®ç°ç†æƒ³çš„æ©ç æ³¨æ„åŠ›çŸ©é˜µè®¡ç®—ï¼Œå¹¶é€šè¿‡å‘é‡åŒ–çš„éçº¿æ€§é‡å­ç¼–ç å™¨å®ç°é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„VQTæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹IBMå’ŒIonQçš„é‡å­ç”µè·¯æ¨¡æ‹Ÿè¿›è¡Œäº†å‡†ç¡®æ€§æ¯”è¾ƒï¼Œå¹¶åœ¨IBMå°–ç«¯é«˜ä¿çœŸåº¦Kingstoné‡å­å¤„ç†å™¨ä¸Šè¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚è¯¥å™ªéŸ³ä¸­é—´å°ºåº¦é‡å­å‹å¥½çš„VQTæ–¹æ³•ä¸ºç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ é‡å­è®¡ç®—å¼€å¯äº†ä¸€ç§æ–°çš„æ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å­å‘é‡åŒ–ç¼–ç æ˜¯ä¸€ç§å°†ç»å…¸æ•°æ®åµŒå…¥Hilbertç©ºé—´çš„æŠ€æœ¯ã€‚</li>
<li>é‡å­è½¬æ¢å™¨ï¼ˆQTï¼‰é€šè¿‡åˆ©ç”¨é‡å­ç”µè·¯æ¨¡æ‹Ÿæ›¿ä»£ç»å…¸è‡ªæ³¨æ„åŠ›ï¼Œä»¥å®ç°æ›´é«˜æ•ˆè¿è¡Œã€‚</li>
<li>ç°æœ‰QTä¾èµ–æ·±åº¦å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ï¼Œæ˜“å—é‡å­å¤„ç†å™¨å™ªå£°å½±å“ã€‚</li>
<li>æå‡ºå‘é‡åŒ–é‡å­è½¬æ¢å™¨ï¼ˆVQTï¼‰æ¨¡å‹ï¼Œé€šè¿‡é‡å­è¿‘ä¼¼æ¨¡æ‹Ÿå®ç°æ©ç æ³¨æ„åŠ›çŸ©é˜µè®¡ç®—ã€‚</li>
<li>VQTåˆ©ç”¨å‘é‡åŒ–çš„éçº¿æ€§é‡å­ç¼–ç å™¨å®ç°é«˜æ•ˆè®­ç»ƒã€æ— æ¢¯åº¦é‡å­ç”µè·¯æ¨¡æ‹Ÿå’Œå‡å°‘ç»å…¸é‡‡æ ·å¼€é”€ã€‚</li>
<li>VQTæ–¹æ³•åœ¨IBMå’ŒIonQçš„é‡å­ç”µè·¯æ¨¡æ‹Ÿä¸­æ˜¾ç¤ºå‡ºå‡†ç¡®æ€§ï¼Œå¹¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.18464v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Nemotron-Nano-9B-v2ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚Nemotron-Nano-9B-v2å»ºç«‹åœ¨Nemotron-Hæ¶æ„çš„åŸºç¡€ä¸Šï¼Œå°†Transformeræ¶æ„ä¸­å¤§éƒ¨åˆ†çš„è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œä»¥å®ç°åœ¨ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€è€ƒè½¨è¿¹æ—¶çš„æ›´å¿«æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆåœ¨20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šä½¿ç”¨FP8è®­ç»ƒé…æ–¹é¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰æ¥åˆ›å»ºNemotron-Nano-9B-v2ã€‚åœ¨å¯¹Nemotron-Nano-12B-v2-Baseè¿›è¡Œå¯¹é½åï¼Œæˆ‘ä»¬é‡‡ç”¨Minitronç­–ç•¥æ¥å‹ç¼©å’Œè’¸é¦æ¨¡å‹ï¼Œæ—¨åœ¨èƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A10G GPUï¼ˆå…·æœ‰22GiBå†…å­˜ï¼Œbfloat16ç²¾åº¦ï¼‰ä¸Šè¿›è¡Œæœ€å¤šè¾¾128kä»¤ç‰Œçš„æ¨ç†ã€‚ä¸ç°æœ‰çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen3-8Bï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºï¼ŒNemotron-Nano-9B-v2å®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨å¦‚8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œçš„æ¨ç†è®¾ç½®ä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚æˆ‘ä»¬å°†Nemotron-Nano-9B-v2ã€Nemotron-Nano12B-v2-Baseä»¥åŠNemotron-Nano-9B-v2çš„åŸºç‚¹å’Œå¤šæ•°é¢„è®­ç»ƒå’Œåç»­è®­ç»ƒæ•°æ®é›†ä¸€èµ·åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Nemotron-Nano-9B-v2æ˜¯æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶åœ¨ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡åŸºäºNemotron-Hæ¶æ„ï¼Œç”¨Mamba-2å±‚æ›¿æ¢Transformeræ¶æ„ä¸­çš„å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»¥å®ç°ç”Ÿæˆé•¿æ€è€ƒè½¨è¿¹æ—¶çš„æ¨ç†é€Ÿåº¦æå‡ã€‚æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒ12äº¿å‚æ•°çš„æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰åœ¨Hugging Faceä¸Šå‘å¸ƒï¼Œä¹‹ååœ¨å•ä¸ªNVIDIA A10G GPUä¸Šè¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼Œå®ç°åœ¨æ¨ç†è®¾ç½®ä¸­çš„é«˜ååé‡å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nemotron-Nano-9B-v2æ˜¯ä¸€ä¸ªæ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†çš„ååé‡å¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åŸºäºNemotron-Hæ¶æ„ï¼Œç”¨Mamba-2å±‚æ›¿æ¢è‡ªæ³¨æ„åŠ›å±‚ä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°çš„æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¨¡å‹å¯ä»¥åœ¨å•ä¸ªNVIDIA A10G GPUä¸Šè¿›è¡Œæ¨ç†ï¼Œå…·æœ‰é«˜è¾¾6å€çš„é«˜ååé‡ã€‚</li>
<li>ä¸ç°æœ‰çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ç›¸æ¯”ï¼ˆä¾‹å¦‚Qwen3-8Bï¼‰ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹åŠå…¶ç›¸å…³æ•°æ®é›†å·²åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14444v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14444v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14444v4/page_2_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Letâ€™s-Use-ChatGPT-To-Write-Our-Paper-Benchmarking-LLMs-To-Write-the-Introduction-of-a-Research-Paper"><a href="#Letâ€™s-Use-ChatGPT-To-Write-Our-Paper-Benchmarking-LLMs-To-Write-the-Introduction-of-a-Research-Paper" class="headerlink" title="Letâ€™s Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the   Introduction of a Research Paper"></a>Letâ€™s Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the   Introduction of a Research Paper</h2><p><strong>Authors:Krishna Garg, Firoz Shaik, Sambaran Bandyopadhyay, Cornelia Caragea</strong></p>
<p>As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMsâ€™ ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverickâ€™s superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets. </p>
<blockquote>
<p>éšç€è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è€…é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå†™ä½œåŠ©æ‰‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶è®ºæ–‡å¼•è¨€ä»ç„¶æ˜¯ä¸€ä¸ªæ—¢å…·æŒ‘æˆ˜æ€§åˆè‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ç§‘å­¦å¼•è¨€ç”Ÿæˆï¼ˆSciIGï¼‰è¿™ä¸€ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä»æ ‡é¢˜ã€æ‘˜è¦å’Œç›¸å…³æ–‡çŒ®ä¸­äº§ç”Ÿè¿è´¯å¼•è¨€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªNAACL 2025å’ŒICLR 2025è®ºæ–‡çš„æ–°æ•°æ®é›†ï¼Œè¯„ä¼°äº†äº”ç§æœ€æ–°æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ï¼ˆDeepSeek-v3ã€Gemma-3-12Bã€LLaMA 4-Maverickã€MistralAI Small 3.1ï¼‰å’Œé—­æºGPT-4oç³»ç»Ÿï¼Œæ¶‰åŠå¤šä¸ªç»´åº¦ï¼šè¯æ±‡é‡å ã€è¯­ä¹‰ç›¸ä¼¼æ€§ã€å†…å®¹è¦†ç›–ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ã€å¼•ç”¨æ­£ç¡®æ€§å’Œå™è¿°è´¨é‡ã€‚æˆ‘ä»¬çš„ç»¼åˆæ¡†æ¶ç»“åˆäº†è‡ªåŠ¨åº¦é‡æŒ‡æ ‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„è¯„ä»·ã€‚ç»“æœè¡¨æ˜ï¼ŒLLaMA-4 Maverickåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¿ å®æ€§æ–¹é¢ã€‚æ­¤å¤–ï¼Œä¸‰æç¤ºæ³•å§‹ç»ˆä¼˜äºå°‘æç¤ºæ³•ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æœ‰æ•ˆçš„ç ”ç©¶å†™ä½œåŠ©æ‰‹æä¾›äº†å®é™…è§è§£ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„å­¦æœ¯å†™ä½œè®¾å®šäº†ç°å®çš„æœŸæœ›ã€‚ä¸ºäº†ä¿ƒè¿›é‡ç°æ€§å’Œæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ‰€æœ‰ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14273v2">PDF</a> 20 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>LLMsä½œä¸ºå†™ä½œåŠ©æ‰‹åœ¨å­¦æœ¯ç•Œæ—¥ç›Šæ™®åŠï¼Œç”Ÿæˆé«˜è´¨é‡ç ”ç©¶è®ºæ–‡å¼•è¨€æ—¢å…·æŒ‘æˆ˜æ€§åˆè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†Scientific Introduction Generationï¼ˆSciIGï¼‰ä»»åŠ¡ï¼Œè¯„ä¼°LLMsä»æ ‡é¢˜ã€æ‘˜è¦å’Œç›¸å…³å·¥ä½œç”Ÿæˆè¿è´¯å¼•è¨€çš„èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨NAACL 2025å’ŒICLR 2025è®ºæ–‡æ•°æ®é›†ï¼Œè¯„ä¼°äº†äº”ä¸ªæœ€æ–°æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ï¼ˆDeepSeek-v3ã€Gemma-3-12Bã€LLaMA 4-Maverickã€MistralAI Small 3.1ï¼‰å’Œé—­æºGPT-4oç³»ç»Ÿï¼Œæ¶µç›–å¤šä¸ªç»´åº¦ã€‚ç»“æœæ˜¾ç¤ºLLaMA-4 Maverickåœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¿ å®æ€§æ–¹é¢ã€‚æ­¤å¤–ï¼Œä¸‰æç¤ºæç¤ºæ³•ä¸€è´¯ä¼˜äºå°‘æç¤ºæ³•ã€‚ç ”ç©¶æä¾›å®ç”¨è§è§£ï¼Œæœ‰åŠ©äºå¼€å‘æœ‰æ•ˆçš„ç ”ç©¶å†™ä½œåŠ©æ‰‹ï¼Œå¹¶ä¸ºLLMè¾…åŠ©å­¦æœ¯å†™ä½œè®¾å®šç°å®æœŸæœ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨ä½œå†™ä½œåŠ©æ‰‹ï¼Œç”Ÿæˆé«˜è´¨é‡ç ”ç©¶è®ºæ–‡å¼•è¨€å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºScientific Introduction Generation (SciIG)ä»»åŠ¡ï¼Œè¯„ä¼°LLMsåœ¨ç”Ÿæˆå¼•è¨€æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨NAACL 2025å’ŒICLR 2025è®ºæ–‡æ•°æ®é›†è¿›è¡Œå®è¯ç ”ç©¶ã€‚</li>
<li>è¯„ä¼°äº†å¤šä¸ªLLMsæ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹ã€‚</li>
<li>è¯„ä¼°ç»´åº¦åŒ…æ‹¬è¯æ±‡é‡å ã€è¯­ä¹‰ç›¸ä¼¼æ€§ã€å†…å®¹è¦†ç›–ã€å¿ å®æ€§ã€ä¸€è‡´æ€§ã€å¼•ç”¨æ­£ç¡®æ€§å’Œå™äº‹è´¨é‡ã€‚</li>
<li>LLaMA-4 Maverickåœ¨å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14273v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14273v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14273v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.14273v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Region-Level-Context-Aware-Multimodal-Understanding"><a href="#Region-Level-Context-Aware-Multimodal-Understanding" class="headerlink" title="Region-Level Context-Aware Multimodal Understanding"></a>Region-Level Context-Aware Multimodal Understanding</h2><p><strong>Authors:Hongliang Wei, Xianqi Zhang, Xingtao Wang, Xiaopeng Fan, Debin Zhao</strong></p>
<p>Despite significant progress, existing research on Multimodal Large Language Models (MLLMs) mainly focuses on general visual understanding, overlooking the ability to integrate textual context associated with objects for a more context-aware multimodal understanding â€“ an ability we refer to as Region-level Context-aware Multimodal Understanding (RCMU). To address this limitation, we first formulate the RCMU task, which requires models to respond to user instructions by integrating both image content and textual information of regions or objects. To equip MLLMs with RCMU capabilities, we propose Region-level Context-aware Visual Instruction Tuning (RCVIT), which incorporates object information into the model input and enables the model to utilize bounding box coordinates to effectively associate objectsâ€™ visual content with their textual information. To address the lack of datasets, we introduce the RCMU dataset, a large-scale visual instruction tuning dataset that covers multiple RCMU tasks. We also propose RC&amp;P-Bench, a comprehensive benchmark that can evaluate the performance of MLLMs in RCMU and multimodal personalized understanding tasks. Additionally, we propose a reference-free evaluation metric to perform a comprehensive and fine-grained evaluation of the region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental results indicate that RC-Qwen2-VL models not only achieve outstanding performance on multiple RCMU tasks but also demonstrate successful applications in multimodal RAG and personalized conversation. Our data, model and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/hongliang-wei/RC-MLLM">https://github.com/hongliang-wei/RC-MLLM</a> </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…³äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸€èˆ¬çš„è§†è§‰ç†è§£ä¸Šï¼Œå¿½ç•¥äº†æ•´åˆä¸å¯¹è±¡ç›¸å…³çš„æ–‡æœ¬ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œä»¥å®ç°æ›´å…·ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šæ¨¡æ€ç†è§£â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºåŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ï¼ˆRCMUï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¶å®šäº†RCMUä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹é€šè¿‡æ•´åˆå›¾åƒå†…å®¹å’ŒåŒºåŸŸæˆ–å¯¹è±¡çš„æ–‡æœ¬ä¿¡æ¯æ¥å“åº”ç”¨æˆ·æŒ‡ä»¤ã€‚ä¸ºäº†èµ‹äºˆMLLMsä»¥RCMUèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆRCVITï¼‰ï¼Œå®ƒå°†å¯¹è±¡ä¿¡æ¯èå…¥æ¨¡å‹è¾“å…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¾¹ç•Œæ¡†åæ ‡æœ‰æ•ˆåœ°å°†å¯¹è±¡çš„è§†è§‰å†…å®¹ä¸æ–‡æœ¬ä¿¡æ¯å…³è”èµ·æ¥ã€‚ä¸ºäº†è§£å†³æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†RCMUæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šä¸ªRCMUä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†RC&amp;P-BenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒå¯ä»¥è¯„ä¼°MLLMsåœ¨RCMUå’Œå¤šæ¨¡æ€ä¸ªæ€§åŒ–ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒæè¿°è¿›è¡Œæ›´å…¨é¢å’Œç²¾ç»†çš„è¯„ä¼°ã€‚é€šè¿‡åœ¨Qwen2-VLæ¨¡å‹ä¸Šåº”ç”¨RCVITå’ŒRCMUæ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†RC-Qwen2-VLæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRC-Qwen2-VLæ¨¡å‹ä¸ä»…åœ¨å¤šä¸ªRCMUä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¤šæ¨¡æ€RAGå’Œä¸ªæ€§åŒ–å¯¹è¯ä¸­ä¹Ÿè¡¨ç°å‡ºæˆåŠŸçš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ•°æ®ã€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hongliang-wei/RC-MLLM%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/hongliang-wei/RC-MLLMä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12263v2">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç ”ç©¶ç°çŠ¶åŠå…¶å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ï¼ˆRCMUï¼‰æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæ–‡ç« æå‡ºäº†åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆRCVITï¼‰çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†RCMUæ•°æ®é›†å’ŒRC&amp;P-BenchåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡RCVITæ–¹æ³•å¯¹Qwen2-VLæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ŒæˆåŠŸå¼€å‘å‡ºRC-Qwen2-VLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹RCMUä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶æˆåŠŸåº”ç”¨äºå¤šæ¨¡æ€RAGå’Œä¸ªæ€§åŒ–å¯¹è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsè™½ç„¶å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸»è¦å…³æ³¨ä¸€èˆ¬è§†è§‰ç†è§£ï¼Œå¿½ç•¥äº†ä¸å¯¹è±¡ç›¸å…³çš„æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æ•´åˆèƒ½åŠ›ï¼Œå³åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ¨¡æ€ç†è§£ï¼ˆRCMUï¼‰ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆRCVITï¼‰æ–¹æ³•ï¼Œå°†å¯¹è±¡ä¿¡æ¯çº³å…¥æ¨¡å‹è¾“å…¥ï¼Œå¹¶ä½¿ç”¨è¾¹ç•Œæ¡†åæ ‡å…³è”å¯¹è±¡çš„è§†è§‰å†…å®¹ä¸æ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥RCMUæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªRCMUä»»åŠ¡ã€‚</li>
<li>æå‡ºRC&amp;P-BenchåŸºå‡†æµ‹è¯•ï¼Œå¯è¯„ä¼°MLLMsåœ¨RCMUå’Œå¤šæ¨¡æ€ä¸ªæ€§åŒ–ç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ— å‚è€ƒè¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹åŒºåŸŸçº§ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒæè¿°è¿›è¡Œæ›´å…¨é¢ã€ç²¾ç»†çš„è¯„ä¼°ã€‚</li>
<li>é€šè¿‡RCVITæ–¹æ³•å¯¹Qwen2-VLæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ŒæˆåŠŸå¼€å‘å‡ºRC-Qwen2-VLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹RCMUä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>RC-Qwen2-VLæ¨¡å‹æˆåŠŸåº”ç”¨äºå¤šæ¨¡æ€RAGå’Œä¸ªæ€§åŒ–å¯¹è¯ï¼Œå±•ç¤ºäº†å…¶å®ç”¨æ€§å’Œæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.12263v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.12263v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.12263v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.12263v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.12263v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.12263v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction"><a href="#Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction" class="headerlink" title="Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction"></a>Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Khaled Shaban, Hozaifa Kassab</strong></p>
<p>Quranic Question Answering presents unique challenges due to the linguistic complexity of Classical Arabic and the semantic richness of religious texts. In this paper, we propose a novel two-stage framework that addresses both passage retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned Arabic language models to achieve superior ranking performance. For answer extraction, we employ instruction-tuned large language models with few-shot prompting to overcome the limitations of fine-tuning on small datasets. Our approach achieves state-of-the-art results on the Quran QA 2023 Shared Task, with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of 0.669 for extraction, substantially outperforming previous methods. These results demonstrate that combining model ensembling and instruction-tuned language models effectively addresses the challenges of low-resource question answering in specialized domains. </p>
<blockquote>
<p>ç”±äºå¤å…¸é˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬ä¸°å¯Œçš„è¯­ä¹‰å†…æ¶µï¼Œä¼Šæ–¯å…°é—®ç­”æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒæ—¶è§£å†³äº†æ®µè½æ£€ç´¢å’Œç­”æ¡ˆæå–ã€‚å¯¹äºæ®µè½æ£€ç´¢ï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒé˜¿æ‹‰ä¼¯è¯­è¨€æ¨¡å‹æ¥å®ç°å“è¶Šçš„æ’åæ€§èƒ½ã€‚å¯¹äºç­”æ¡ˆæå–ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å‡ æ¬¡æç¤ºæ¥å…‹æœåœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼Šæ–¯å…°é—®ç­”2023å…±äº«ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œæ£€ç´¢çš„MAP@10ä¸º0.3128ï¼ŒMRR@10ä¸º0.5763ï¼Œæå–çš„pAP@10ä¸º0.669ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»“åˆæ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ç‰¹å®šé¢†åŸŸä½èµ„æºé—®ç­”ä¸­çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06971v2">PDF</a> 8 pages , 4 figures , Accepted in Aiccsa 2025 ,   <a target="_blank" rel="noopener" href="https://conferences.sigappfr.org/aiccsa2025/">https://conferences.sigappfr.org/aiccsa2025/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºè§£å†³å› å¤å…¸é˜¿æ‹‰ä¼¯è¯­çš„å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬ä¸°å¯Œçš„è¯­ä¹‰å†…æ¶µæ‰€å¸¦æ¥çš„å¤å…°ç»é—®ç­”æŒ‘æˆ˜ã€‚é€šè¿‡å¾®è°ƒé˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹è¿›è¡Œæ®µè½æ£€ç´¢ï¼Œå¹¶é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆæå–ï¼Œè¯¥æ–¹æ³•åœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚é€šè¿‡æ¨¡å‹ç»„åˆï¼Œå–å¾—äº†ä¼˜äºä»¥å¾€æ–¹æ³•çš„ä¼˜å¼‚æ•ˆæœã€‚æ­¤ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹ç»„åˆå’ŒæŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹æœ‰æ•ˆåœ°è§£å†³äº†ä½èµ„æºä¸“ä¸šé¢†åŸŸçš„é—®ç­”æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤å…°ç»é—®ç­”é¢ä¸´å¤å…¸é˜¿æ‹‰ä¼¯è¯­çš„å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬ä¸°å¯Œè¯­ä¹‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼ŒåŒ…æ‹¬æ®µè½æ£€ç´¢å’Œç­”æ¡ˆæå–ã€‚</li>
<li>ä½¿ç”¨å¾®è°ƒé˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹è¿›è¡Œæ®µè½æ£€ç´¢ï¼Œè¾¾åˆ°ä¼˜è´¨æ’åæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå°‘æ ·æœ¬æç¤ºæ¥å…‹æœåœ¨å°å‹æ•°æ®é›†ä¸Šç²¾ç»†è°ƒæ•´çš„å±€é™æ€§ã€‚</li>
<li>åœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼ŒåŒ…æ‹¬MAP@10å’ŒMRR@10çš„æ£€ç´¢æŒ‡æ ‡ä»¥åŠpAP@10çš„æå–æŒ‡æ ‡ã€‚</li>
<li>æ­¤æ–¹æ³•æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.06971v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.06971v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.06971v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.06971v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.06971v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech"><a href="#Parallel-GPT-Harmonizing-the-Independence-and-Interdependence-of-Acoustic-and-Semantic-Information-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech"></a>Parallel GPT: Harmonizing the Independence and Interdependence of   Acoustic and Semantic Information for Zero-Shot Text-to-Speech</h2><p><strong>Authors:Jingyuan Xing, Zhipeng Li, Jialong Mai, Xiaofen Xing, Xiangmin Xu</strong></p>
<p>Advances in speech representation and large language models have enhanced zero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS models face challenges in capturing the complex correlations between acoustic and semantic features, resulting in a lack of expressiveness and similarity. The primary reason lies in the complex relationship between semantic and acoustic features, which manifests independent and interdependent aspects.This paper introduces a TTS framework that combines both autoregressive (AR) and non-autoregressive (NAR) modules to harmonize the independence and interdependence of acoustic and semantic information. The AR model leverages the proposed Parallel Tokenizer to synthesize the top semantic and acoustic tokens simultaneously. In contrast, considering the interdependence, the Coupled NAR model predicts detailed tokens based on the general AR modelâ€™s output. Parallel GPT, built on this architecture, is designed to improve zero-shot text-to-speech synthesis through its parallel structure. Experiments on English and Chinese datasets demonstrate that the proposed model significantly outperforms the quality and efficiency of the synthesis of existing zero-shot TTS models. Speech demos are available at <a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/">https://t1235-ch.github.io/pgpt/</a>. </p>
<blockquote>
<p>è¯­éŸ³è¡¨ç¤ºå’Œå¤§è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æé«˜äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹åœ¨æ•æ‰å£°å­¦ç‰¹å¾å’Œè¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³è”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ç¼ºä¹è¡¨è¾¾æ€§å’Œç›¸ä¼¼æ€§ã€‚ä¸»è¦åŸå› åœ¨äºè¯­ä¹‰ç‰¹å¾å’Œå£°å­¦ç‰¹å¾ä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œè¡¨ç°ä¸ºç‹¬ç«‹å’Œç›¸äº’ä¾èµ–çš„æ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„TTSæ¡†æ¶ï¼Œä»¥åè°ƒå£°å­¦ä¿¡æ¯å’Œè¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ã€‚ARæ¨¡å‹åˆ©ç”¨æå‡ºçš„å¹¶è¡Œåˆ†è¯å™¨åŒæ—¶åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ã€‚ç›¸åï¼Œè€ƒè™‘åˆ°ç›¸äº’ä¾èµ–æ€§ï¼Œè€¦åˆçš„NARæ¨¡å‹åŸºäºé€šç”¨ARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†çš„æ ‡è®°ã€‚åŸºäºæ­¤æ¶æ„æ„å»ºçš„å¹¶è¡ŒGPTæ—¨åœ¨é€šè¿‡å…¶å¹¶è¡Œç»“æ„æ”¹è¿›é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆã€‚åœ¨è‹±è¯­å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨åˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚è¯­éŸ³æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://t1-ch.github.io/pgpt/">https://t1-ch.github.io/pgpt/</a>ä¸ŠæŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04141v2">PDF</a> Submitted to IEEE&#x2F;ACM Transactions on Audio, Speech, and Language   Processing (TASLP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“åˆè‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å—çš„æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¡†æ¶ã€‚è¿™ä¸€æ¡†æ¶æ—¨åœ¨åè°ƒè¯­éŸ³å’Œè¯­ä¹‰ä¿¡æ¯çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä¾èµ–æ€§ï¼Œé€šè¿‡åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ä»¥åŠåŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†æ ‡è®°æ¥æé«˜é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³çš„åˆæˆè´¨é‡å’Œæ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ä»‹ç»äº†æ–°çš„TTSæ¡†æ¶å¦‚ä½•é€šè¿‡ä½¿ç”¨ARå’ŒNARæ¨¡å—æ¥è§£å†³é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ä¸­è¯­ä¹‰å’Œå£°éŸ³ç‰¹å¾ä¹‹é—´å¤æ‚å…³ç³»çš„æŒ‘æˆ˜ã€‚</li>
<li>ARæ¨¡å‹åˆ©ç”¨å¹¶è¡Œæ ‡è®°å™¨åŒæ—¶åˆæˆé¡¶çº§è¯­ä¹‰å’Œå£°éŸ³æ ‡è®°ï¼Œå¼ºè°ƒä¸¤è€…çš„åŒæ­¥å¤„ç†ã€‚</li>
<li>NARæ¨¡å‹åŸºäºARæ¨¡å‹çš„è¾“å‡ºé¢„æµ‹è¯¦ç»†æ ‡è®°ï¼Œä½“ç°äº†è¯­éŸ³å’Œè¯­ä¹‰çš„ç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>æå‡ºçš„Parallel GPTæ¶æ„æ—¨åœ¨æé«˜é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³çš„åˆæˆè´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨è‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹çš„æ¼”ç¤ºå’Œæ›´å¤šç»†èŠ‚å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://t1235-ch.github.io/pgpt/%E6%9F%A5%E7%9C%8B%E3%80%82">https://t1235-ch.github.io/pgpt/æŸ¥çœ‹ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.04141v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.04141v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.04141v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2508.04141v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TPTT-Transforming-Pretrained-Transformers-into-Titans"><a href="#TPTT-Transforming-Pretrained-Transformers-into-Titans" class="headerlink" title="TPTT: Transforming Pretrained Transformers into Titans"></a>TPTT: Transforming Pretrained Transformers into Titans</h2><p><strong>Authors:Fabien Furfaro</strong></p>
<p>Transformer-based large language models (LLMs) have achieved strong performance across many natural language processing tasks. Nonetheless, their quadratic computational and memory requirements, particularly in self-attention layers, pose challenges for efficient inference on long contexts and for deployment in resource-limited environments. We present TPTT (Transforming Pretrained Transformers into Titans), a framework designed to augment pretrained Transformers with linearized attention (LiZA) and internal memory gating via Memory as Gate (MaG), applied without full retraining. TPTT supports parameter-efficient fine-tuning (LoRA) and integrates with standard toolkits such as Hugging Face Transformers. We evaluated TPTT on several pretrained models, including Llama-1B, OlMoE-1B-7B, Qwen2.5-1.5B, Gemma3-270m, OpenELM-1.3B, and Mistral-7B, in order to assess applicability across architectures of different scales. Experiments on models with approximately 1 billion parameters, evaluated primarily on the MMLU benchmark, suggest potential improvements in both efficiency and accuracy compared to baseline models. For example, Titans-Llama-1B exhibited up to a 20% relative increase in Exact Match scores in one-shot evaluation. An additional finding is that it is possible to convert a quadratic-attention model into a purely linear-attention model using the DeltaProduct mechanism. All training runs were carried out with modest computational resources. These preliminary findings indicate that TPTT may help adapt pretrained LLMs for long-context tasks with limited overhead. Further studies on larger models and a broader set of benchmarks will be necessary to evaluate the generality and robustness of the framework. Code is available at <a target="_blank" rel="noopener" href="https://github.com/fabienfrfr/tptt">https://github.com/fabienfrfr/tptt</a> . Python package at <a target="_blank" rel="noopener" href="https://pypi.org/project/tptt/">https://pypi.org/project/tptt/</a> . </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„äºŒæ¬¡è®¡ç®—å’Œå†…å­˜è¦æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œå¯¹é•¿æ–‡æœ¬çš„é«˜æ•ˆæ¨ç†å’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²æå‡ºäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TPTTï¼ˆå°†é¢„è®­ç»ƒçš„Transformerè½¬åŒ–ä¸ºå·¨äººï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡çº¿æ€§æ³¨æ„åŠ›ï¼ˆLiZAï¼‰å’Œé€šè¿‡å†…å­˜ä½œä¸ºé—¨æ§æœºåˆ¶ï¼ˆMaGï¼‰çš„å†…éƒ¨è®°å¿†é—¨æ§æ¥å¢å¼ºé¢„è®­ç»ƒçš„Transformerï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢å†è®­ç»ƒã€‚TPTTæ”¯æŒå‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆLoRAï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ä¸Hugging Face Transformersç­‰æ ‡å‡†å·¥å…·åŒ…é›†æˆã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸Šè¯„ä¼°äº†TPTTï¼ŒåŒ…æ‹¬Llama-1Bã€OlMoE-1B-7Bã€Qwen2.5-1.5Bã€Gemma3-270mã€OpenELM-1.3Bå’ŒMistral-7Bç­‰æ¨¡å‹ï¼Œä»¥è¯„ä¼°å…¶åœ¨ä¸åŒè§„æ¨¡æ¶æ„ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨MMLUåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°çš„çº¦å«1äº¿å‚æ•°çš„æ¨¡å‹å®éªŒè¡¨æ˜ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒTPTTåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½æœ‰æ½œåœ¨çš„æå‡ã€‚ä¾‹å¦‚ï¼ŒTitans-Llama-1Båœ¨ä¸€æ¬¡è¯„ä¼°ä¸­çš„ç²¾ç¡®åŒ¹é…å¾—åˆ†æé«˜äº†é«˜è¾¾20%ã€‚å¦ä¸€ä¸ªå‘ç°æ˜¯ä½¿ç”¨DeltaProductæœºåˆ¶å°†äºŒæ¬¡æ³¨æ„åŠ›æ¨¡å‹è½¬æ¢ä¸ºçº¯çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹çš„å¯èƒ½æ€§ã€‚æ‰€æœ‰çš„è®­ç»ƒè¿è¡Œéƒ½æ˜¯ä½¿ç”¨é€‚ä¸­çš„è®¡ç®—èµ„æºå®Œæˆçš„ã€‚è¿™äº›åˆæ­¥ç»“æœè¡¨æ˜ï¼ŒTPTTå¯èƒ½æœ‰åŠ©äºé€‚åº”é•¿æ–‡æœ¬ä»»åŠ¡çš„é¢„è®­ç»ƒLLMï¼Œå¹¶ä¸”å…·æœ‰æœ‰é™çš„é¢å¤–å¼€é”€ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ¡†æ¶çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ï¼Œè¿˜éœ€è¦åœ¨æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¹¿æ³›çš„åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚ä»£ç åœ°å€ä¸º<a target="_blank" rel="noopener" href="https://github.com/fabienfrfr/tptt%E3%80%82Python%E5%8C%85%E5%9C%B0%E5%9D%80%E4%B8%BAhttps://pypi.org/project/tptt/%E3%80%82">https://github.com/fabienfrfr/tpttã€‚PythonåŒ…åœ°å€ä¸ºhttps://pypi.org/project/tptt/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17671v2">PDF</a> 14 pages, 2 figure</p>
<p><strong>Summary</strong><br>     é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†è®¡ç®—ä¸å†…å­˜èµ„æºéœ€æ±‚é™åˆ¶äº†å…¶åœ¨é•¿æ–‡æœ¬åœºæ™¯åŠèµ„æºå—é™ç¯å¢ƒä¸‹çš„æ•ˆç‡éƒ¨ç½²èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºTransforming Pretrained Transformers into Titansï¼ˆTPTTï¼‰æ¡†æ¶ï¼Œé€šè¿‡çº¿æ€§æ³¨æ„åŠ›ï¼ˆLiZAï¼‰ä¸å†…å­˜é—¨æ§æŠ€æœ¯ï¼ˆMaGï¼‰å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œæ— éœ€å…¨é‡é‡è®­å³å¯åº”ç”¨ã€‚TPTTæ”¯æŒå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰ï¼Œå¹¶èƒ½ä¸Hugging Face Transformersç­‰æ ‡å‡†å·¥å…·é›†é›†æˆã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­å…·æœ‰æ½œåœ¨æå‡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„æ½œåŠ›ã€‚ä¾‹å¦‚ï¼ŒLlama-1Bæ¨¡å‹åœ¨ä¸€æ¬¡è¯„ä¼°ä¸­ç²¾ç¡®åŒ¹é…å¾—åˆ†æé«˜äº†é«˜è¾¾ç›¸å¯¹ç™¾åˆ†ä¹‹äºŒåã€‚åˆæ­¥å‘ç°è¡¨æ˜ï¼Œä½¿ç”¨DeltaProductæœºåˆ¶å¯å°†äºŒæ¬¡æ³¨æ„åŠ›æ¨¡å‹è½¬æ¢ä¸ºçº¯çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ã€‚æ‰€æœ‰è®­ç»ƒè¿è¡Œéƒ½åœ¨ä½¿ç”¨é€‚åº¦è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹å®Œæˆã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚PythonåŒ…å·²å‘å¸ƒåœ¨PyPIä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TPTTæ¡†æ¶æ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒTransformeræ¨¡å‹åœ¨é•¿æ–‡æœ¬åœºæ™¯å’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>TPTTç»“åˆäº†çº¿æ€§æ³¨æ„åŠ›ï¼ˆLiZAï¼‰å’Œå†…å­˜é—¨æ§æŠ€æœ¯ï¼ˆMaGï¼‰ä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œæ— éœ€å…¨é‡é‡è®­ã€‚</li>
<li>TPTTæ”¯æŒå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRAï¼‰ï¼Œå¹¶èƒ½ä¸æ ‡å‡†å·¥å…·é›†é›†æˆï¼Œå¦‚Hugging Face Transformersã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTPTTåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­æœ‰æœ›æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨DeltaProductæœºåˆ¶å¯å°†äºŒæ¬¡æ³¨æ„åŠ›æ¨¡å‹è½¬æ¢ä¸ºçº¯çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2506.17671v2/page_0_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bringing-Attention-to-CAD-Boundary-Representation-Learning-via-Transformer"><a href="#Bringing-Attention-to-CAD-Boundary-Representation-Learning-via-Transformer" class="headerlink" title="Bringing Attention to CAD: Boundary Representation Learning via   Transformer"></a>Bringing Attention to CAD: Boundary Representation Learning via   Transformer</h2><p><strong>Authors:Qiang Zou, Lizhen Zhu</strong></p>
<p>The recent rise of generative artificial intelligence (AI), powered by Transformer networks, has achieved remarkable success in natural language processing, computer vision, and graphics. However, the application of Transformers in computer-aided design (CAD), particularly for processing boundary representation (B-rep) models, remains largely unexplored. To bridge this gap, we propose a novel approach for adapting Transformers to B-rep learning, called the Boundary Representation Transformer (BRT). B-rep models pose unique challenges due to their irregular topology and continuous geometric definitions, which are fundamentally different from the structured and discrete data Transformers are designed for. To address this, BRT proposes a continuous geometric embedding method that encodes B-rep surfaces (trimmed and untrimmed) into Bezier triangles, preserving their shape and continuity without discretization. Additionally, BRT employs a topology-aware embedding method that organizes these geometric embeddings into a sequence of discrete tokens suitable for Transformers, capturing both geometric and topological characteristics within B-rep models. This enables the Transformerâ€™s attention mechanism to effectively learn shape patterns and contextual semantics of boundary elements in a B-rep model. Extensive experiments demonstrate that BRT achieves state-of-the-art performance in part classification and feature recognition tasks. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä»¥Transformerç½‘ç»œä¸ºåŠ¨åŠ›çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç„¶è€Œï¼ŒTransformeråœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰æ¨¡å‹æ–¹é¢ï¼Œä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Transformeré€‚åº”B-repå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºè¾¹ç•Œè¡¨ç¤ºè½¬æ¢å™¨ï¼ˆBRTï¼‰ã€‚B-repæ¨¡å‹ç”±äºå…¶ä¸è§„åˆ™æ‹“æ‰‘å’Œè¿ç»­å‡ ä½•å®šä¹‰è€Œå…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œè¿™äº›ä¸Transformeræ‰€è®¾è®¡çš„ç»“æ„åŒ–ç¦»æ•£æ•°æ®å­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒBRTæå‡ºäº†ä¸€ç§è¿ç»­å‡ ä½•åµŒå…¥æ–¹æ³•ï¼Œå°†B-repè¡¨é¢ï¼ˆå¸¦ä¿®é¥°å’Œä¸å¸¦ä¿®é¥°ï¼‰ç¼–ç ä¸ºBezierä¸‰è§’å½¢ï¼Œä¿ç•™å…¶å½¢çŠ¶å’Œè¿ç»­æ€§ï¼Œæ— éœ€ç¦»æ•£åŒ–ã€‚æ­¤å¤–ï¼ŒBRTé‡‡ç”¨äº†ä¸€ç§æ‹“æ‰‘æ„ŸçŸ¥åµŒå…¥æ–¹æ³•ï¼Œå°†è¿™äº›å‡ ä½•åµŒå…¥ç»„ç»‡æˆé€‚åˆTransformerçš„ç¦»æ•£ä»¤ç‰Œåºåˆ—ï¼Œæ•æ‰B-repæ¨¡å‹ä¸­çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾ã€‚è¿™ä½¿å¾—Transformerçš„æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ B-repæ¨¡å‹ä¸­è¾¹ç•Œå…ƒç´ çš„å½¢çŠ¶æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBRTåœ¨é›¶ä»¶åˆ†ç±»å’Œç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07134v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºTransformerç½‘ç»œçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰æ¨¡å‹åº”ç”¨æ–¹é¢ä»é²œæœ‰æ¢ç´¢ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†é€‚åº”B-repå­¦ä¹ çš„å…¨æ–°æ–¹æ³•â€”â€”è¾¹ç•Œè¡¨ç¤ºè½¬æ¢å™¨ï¼ˆBRTï¼‰ã€‚BRTé€šè¿‡è¿ç»­å‡ ä½•åµŒå…¥æ–¹æ³•å°†B-repæ›²é¢ç¼–ç ä¸ºBezierä¸‰è§’å½¢ï¼Œå¹¶ä¿ç•™å…¶å½¢çŠ¶å’Œè¿ç»­æ€§ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æ‹“æ‰‘æ„ŸçŸ¥åµŒå…¥æ³•å°†å‡ ä½•åµŒå…¥è½¬æ¢ä¸ºé€‚åˆTransformerçš„ç¦»æ•£ä»¤ç‰Œåºåˆ—ï¼Œæ•æ‰B-repæ¨¡å‹çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾ã€‚è¿™ä½¿å¾—Transformerçš„å…³æ³¨æœºåˆ¶èƒ½æœ‰æ•ˆå­¦ä¹ B-repæ¨¡å‹ä¸­è¾¹ç•Œå…ƒç´ çš„å½¢çŠ¶æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼ŒBRTåœ¨é›¶ä»¶åˆ†ç±»å’Œç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformerç½‘ç»œåœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„è¾¹ç•Œè¡¨ç¤ºï¼ˆB-repï¼‰æ¨¡å‹åº”ç”¨æ–¹é¢ä»æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>BRTæ˜¯ä¸€ç§é€‚åº”B-repå­¦ä¹ çš„å…¨æ–°æ–¹æ³•ã€‚</li>
<li>BRTé€šè¿‡è¿ç»­å‡ ä½•åµŒå…¥æ–¹æ³•å°†B-repæ›²é¢ç¼–ç ä¸ºBezierä¸‰è§’å½¢ï¼Œä¿ç•™å…¶å½¢çŠ¶å’Œè¿ç»­æ€§ã€‚</li>
<li>BRTåˆ©ç”¨æ‹“æ‰‘æ„ŸçŸ¥åµŒå…¥æ³•å°†å‡ ä½•åµŒå…¥è½¬æ¢ä¸ºé€‚åˆTransformerçš„ç¦»æ•£ä»¤ç‰Œåºåˆ—ã€‚</li>
<li>BRTèƒ½æ•æ‰B-repæ¨¡å‹çš„å‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾ã€‚</li>
<li>Transformerçš„å…³æ³¨æœºåˆ¶é€šè¿‡BRTèƒ½æœ‰æ•ˆå­¦ä¹ B-repæ¨¡å‹ä¸­è¾¹ç•Œå…ƒç´ çš„å½¢çŠ¶æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒBRTåœ¨é›¶ä»¶åˆ†ç±»å’Œç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2504.07134v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2504.07134v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2504.07134v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2504.07134v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2504.07134v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EQ-Knight-A-Memory-Augmented-LLM-Agent-for-Strategic-Affective-Gaming-in-Debt-Recovery"><a href="#EQ-Knight-A-Memory-Augmented-LLM-Agent-for-Strategic-Affective-Gaming-in-Debt-Recovery" class="headerlink" title="EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming   in Debt Recovery"></a>EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming   in Debt Recovery</h2><p><strong>Authors:Yunbo Long, Yuhan Liu, Liming Xu, Alexandra Brintrup</strong></p>
<p>Large language model-based chatbots have enhanced engagement in financial negotiations, but their overreliance on passive empathy introduces critical risks in credit collection. While empathy-driven approaches preserve client satisfaction in benign cases, they fail catastrophically against dishonest debtorsâ€“individuals who exploit conciliatory tactics to manipulate terms or evade repayment. Blindly prioritizing â€œcustomer experienceâ€ in such scenarios leads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic exploitation. To address this, we propose EQ-Knight, an LLM agent that dynamically optimizes emotional strategy to defend creditor interests. Unlike naive empathy-centric bots, EQ-Knight integrates emotion memory and game-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and predict debtor emotional states. By analyzing both real-time and historical emotional cues, EQ-Knight strategically counters negative emotions (e.g., aggression, feigned distress) while preserving productive debtor relationships. Experiments demonstrate EQ-Knightâ€™s superiority over conventional LLM negotiators: it achieves a 32% reduction in concession losses without compromising recovery rates, particularly in adversarial cases where debtors weaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce concessions. For credit agencies, EQ-Knight transforms LLMs from high-risk â€œpeople-pleasersâ€ into strategic emotion-defendersâ€“balancing emotional intelligence with tactical rigor to enforce accountability and deter exploitation. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººåœ¨è´¢åŠ¡è°ˆåˆ¤ä¸­çš„å‚ä¸åº¦æœ‰æ‰€æé«˜ï¼Œä½†å®ƒä»¬è¿‡äºä¾èµ–è¢«åŠ¨å…±æƒ…ï¼Œåœ¨æ”¶æ¬¾æ–¹é¢å­˜åœ¨é‡å¤§é£é™©ã€‚åœ¨è‰¯æ€§æƒ…å†µä¸‹ï¼Œä»¥å…±æƒ…é©±åŠ¨çš„æ–¹æ³•å¯ä»¥ä¿æŒå®¢æˆ·æ»¡æ„åº¦ï¼Œä½†å®ƒä»¬å¯¹å¤±ä¿¡å€ºåŠ¡äººï¼ˆå³åˆ©ç”¨å’Œè§£ç­–ç•¥æ¥æ“çºµæ¡ä»¶æˆ–é€ƒé¿è¿˜æ¬¾çš„ä¸ªäººï¼‰å´äº§ç”Ÿç¾éš¾æ€§çš„å¤±è´¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ç›²ç›®ä¼˜å…ˆé‡è§†â€œå®¢æˆ·ä½“éªŒâ€ï¼Œä¼šå¯¼è‡´å€ºæƒäººé¢ä¸´æ¼æ´ï¼Œå¦‚æ”¶å…¥æŸå¤±ã€é“å¾·é£é™©å’Œç³»ç»Ÿæ€§å‰¥å‰Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EQ-Knightè¿™ä¸€LLMä»£ç†ï¼Œå®ƒèƒ½å¤ŸåŠ¨æ€ä¼˜åŒ–æƒ…ç»ªç­–ç•¥æ¥ç»´æŠ¤å€ºæƒäººçš„åˆ©ç›Šã€‚ä¸åŒäºç®€å•çš„ä»¥å…±æƒ…ä¸ºä¸­å¿ƒçš„æœºå™¨äººï¼ŒEQ-Knightç»“åˆäº†æƒ…ç»ªè®°å¿†å’Œæ¸¸æˆç†è®ºæ¨ç†ï¼Œå€ŸåŠ©éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰è·Ÿè¸ªå’Œé¢„æµ‹å€ºåŠ¡äººçš„æƒ…ç»ªçŠ¶æ€ã€‚é€šè¿‡å®æ—¶å’Œå†å²æƒ…ç»ªçº¿ç´¢çš„åˆ†æï¼ŒEQ-Knightèƒ½å¤Ÿæˆ˜ç•¥æ€§åœ°åº”å¯¹æ¶ˆææƒ…ç»ªï¼ˆå¦‚æ”»å‡»æ€§ã€å‡è£…çš„ç—›è‹¦ï¼‰ï¼ŒåŒæ—¶ä¿æŒä¸å€ºåŠ¡äººçš„è‰¯å¥½å…³ç³»ã€‚å®éªŒè¯æ˜ï¼ŒEQ-Knightä¼˜äºä¼ ç»Ÿçš„LLMè°ˆåˆ¤è€…ï¼šå®ƒåœ¨è®©æ­¥æŸå¤±æ–¹é¢å‡å°‘äº†32%ï¼ŒåŒæ—¶ä¸å¦¥åäºå›æ”¶ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å€ºåŠ¡äººåˆ©ç”¨æ¶ˆææƒ…ç»ªï¼ˆå¦‚æå“ã€æ–½åŠ å†…ç–šæ„Ÿï¼‰æ¥è¿«ä½¿è®©æ­¥çš„å¯¹ç«‹æƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å¯¹äºä¿¡è´·æœºæ„è€Œè¨€ï¼ŒEQ-Knightèƒ½å¤Ÿå°†LLMä»é«˜é£é™©çš„äººé™…å–æ‚¦è€…è½¬å˜ä¸ºæˆ˜ç•¥æ€§çš„æƒ…ç»ªæå«è€…â€”â€”å¹³è¡¡æƒ…å•†ä¸æˆ˜æœ¯ä¸¥è°¨æ€§ï¼Œä»¥æ‰§è¡Œé—®è´£åˆ¶å¹¶éåˆ¶å‰¥å‰Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21080v4">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººåœ¨è´¢åŠ¡è°ˆåˆ¤ä¸­çš„å‚ä¸åº¦æœ‰æ‰€æå‡ï¼Œä½†è¿‡åº¦ä¾èµ–è¢«åŠ¨å…±æƒ…åœ¨ä¿¡è´·å›æ”¶æ–¹é¢å¸¦æ¥å·¨å¤§é£é™©ã€‚å°½ç®¡åœ¨è‰¯æ€§æƒ…å†µä¸‹ï¼Œå…±æƒ…é©±åŠ¨çš„æ–¹æ³•èƒ½ä¿æŒå®¢æˆ·æ»¡æ„åº¦ï¼Œä½†å¯¹ä¸è¯šå®å€ºåŠ¡äººæ¥è¯´ï¼Œä»–ä»¬åˆ©ç”¨å’Œè§£ç­–ç•¥æ¥æ“çºµæ¡æ¬¾æˆ–é€ƒé¿è¿˜æ¬¾ã€‚ç›²ç›®ä¼˜å…ˆè€ƒè™‘å®¢æˆ·ä½“éªŒå¯¼è‡´å€ºæƒäººé¢ä¸´æ”¶å…¥æŸå¤±ã€é“å¾·é£é™©å’Œç³»ç»Ÿæ€§å‰¥å‰Šç­‰è„†å¼±æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºEQ-Knightè¿™ä¸€LLMæ™ºèƒ½ä½“ï¼Œå®ƒèƒ½åŠ¨æ€ä¼˜åŒ–æƒ…æ„Ÿç­–ç•¥ä»¥ç»´æŠ¤å€ºæƒäººåˆ©ç›Šã€‚ä¸åŒäºç®€å•çš„å…±æƒ…ä¸­å¿ƒæœºå™¨äººï¼ŒEQ-Knightæ•´åˆæƒ…æ„Ÿè®°å¿†å’Œæ¸¸æˆç†è®ºæ¨ç†ï¼Œå€ŸåŠ©éšé©¬å°”å¯å¤«æ¨¡å‹è·Ÿè¸ªå’Œé¢„æµ‹å€ºåŠ¡äººæƒ…æ„ŸçŠ¶æ€ã€‚é€šè¿‡åˆ†æå®æ—¶å’Œå†å²æƒ…æ„Ÿçº¿ç´¢ï¼ŒEQ-Knightèƒ½æˆ˜ç•¥æ€§åœ°åº”å¯¹è´Ÿé¢æƒ…ç»ªï¼ŒåŒæ—¶ç»´æŒä¸å€ºåŠ¡äººçš„è‰¯å¥½å…³ç³»ã€‚å®éªŒè¯æ˜ï¼ŒEQ-Knightç›¸è¾ƒäºä¼ ç»ŸLLMè°ˆåˆ¤è€…æ›´å…·ä¼˜åŠ¿ï¼Œåœ¨å€ºåŠ¡äººä½¿ç”¨è´Ÿé¢æƒ…ç»ªï¼ˆå¦‚å¨èƒå’Œå†…ç–šï¼‰èƒè¿«è®©æ­¥çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†è®©æ­¥æŸå¤±å‡å°‘32%ï¼Œä¸”å›æ”¶ç‡æœªå—å½±å“ã€‚å¯¹ä¿¡è´·æœºæ„è€Œè¨€ï¼ŒEQ-Knightå°†LLMä»é«˜é£é™©çš„äººæƒ…æœºå™¨è½¬å˜ä¸ºæˆ˜ç•¥æƒ…ç»ªé˜²å¾¡è€…ï¼Œå¹³è¡¡æƒ…æ„Ÿæ™ºèƒ½ä¸æˆ˜æœ¯ä¸¥è°¨æ€§ï¼Œä»¥æ‰§è¡Œé—®è´£åˆ¶å¹¶éåˆ¶å‰¥å‰Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èŠå¤©æœºå™¨äººåœ¨è´¢åŠ¡è°ˆåˆ¤ä¸­å¢å¼ºå‚ä¸åº¦çš„åŒæ—¶ï¼Œè¿‡åº¦ä¾èµ–è¢«åŠ¨å…±æƒ…åœ¨ä¿¡è´·å›æ”¶æ–¹é¢å­˜åœ¨é£é™©ã€‚</li>
<li>å…±æƒ…é©±åŠ¨çš„æ–¹æ³•åœ¨è‰¯æ€§æƒ…å†µä¸‹èƒ½ç»´æŒå®¢æˆ·æ»¡æ„åº¦ï¼Œä½†å¯¹ä¸è¯šå®å€ºåŠ¡äººå®¹æ˜“é­å—æ“çºµå’Œå‰¥å‰Šã€‚</li>
<li>å€ºæƒäººé¢ä¸´å› ç›²ç›®ä¼˜å…ˆè€ƒè™‘å®¢æˆ·ä½“éªŒè€Œå¯¼è‡´çš„æ”¶å…¥æŸå¤±ã€é“å¾·é£é™©å’Œç³»ç»Ÿæ€§å‰¥å‰Šç­‰é—®é¢˜ã€‚</li>
<li>EQ-Knighté€šè¿‡æ•´åˆæƒ…æ„Ÿè®°å¿†å’Œæ¸¸æˆç†è®ºæ¨ç†ï¼ŒåŠ¨æ€ä¼˜åŒ–æƒ…æ„Ÿç­–ç•¥ä»¥ç»´æŠ¤å€ºæƒäººåˆ©ç›Šã€‚</li>
<li>EQ-Knightå€ŸåŠ©éšé©¬å°”å¯å¤«æ¨¡å‹è·Ÿè¸ªå’Œé¢„æµ‹å€ºåŠ¡äººæƒ…æ„ŸçŠ¶æ€ï¼Œå®ç°æˆ˜ç•¥åº”å¯¹è´Ÿé¢æƒ…ç»ªåŒæ—¶ç»´æŒä¸å€ºåŠ¡äººçš„è‰¯å¥½å…³ç³»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒEQ-Knightç›¸è¾ƒäºä¼ ç»ŸLLMè°ˆåˆ¤è€…æ›´å…·ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨ä¸å½±å“å›æ”¶ç‡çš„æƒ…å†µä¸‹å‡å°‘è®©æ­¥æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2503.21080v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2503.21080v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2503.21080v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2503.21080v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2503.21080v4/page_3_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InsBank-Evolving-Instruction-Subset-for-Ongoing-Alignment"><a href="#InsBank-Evolving-Instruction-Subset-for-Ongoing-Alignment" class="headerlink" title="InsBank: Evolving Instruction Subset for Ongoing Alignment"></a>InsBank: Evolving Instruction Subset for Ongoing Alignment</h2><p><strong>Authors:Jiayi Shi, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Huan Ren, Yao Hu, Kan Li</strong></p>
<p>Large language models (LLMs) typically undergo instruction tuning to enhance alignment. Recent studies emphasize that quality and diversity of instruction data are more crucial than quantity, highlighting the need to select diverse, high-quality subsets to reduce training costs. However, how to evolve these selected subsets alongside the development of new instruction data remains insufficiently explored. To achieve LLMsâ€™ ongoing alignment, we introduce Instruction Bank (\textbf{InsBank}), a continuously updated repository that integrates the latest valuable instruction data. We further propose Progressive Instruction Bank Evolution (\textbf{PIBE}), a novel framework designed to evolve InsBank effectively and efficiently over time. PIBE employs a gradual data selection strategy to maintain long-term efficiency, leveraging a representation-based diversity score to capture relationships between data points and retain historical information for comprehensive diversity evaluation. This also allows for flexible combination of diversity and quality scores during data selection and ranking. Extensive experiments demonstrate that PIBE significantly outperforms baselines in InsBank evolution and is able to extract budget-specific subsets, demonstrating its effectiveness and adaptability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸é€šè¿‡æŒ‡ä»¤è°ƒæ•´æ¥å¢å¼ºå¯¹é½æ•ˆæœã€‚æœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒï¼ŒæŒ‡ä»¤æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§æ¯”æ•°é‡æ›´é‡è¦ï¼Œè¿™å‡¸æ˜¾äº†é€‰æ‹©å¤šæ ·ã€é«˜è´¨é‡å­é›†çš„éœ€è¦ï¼Œä»¥é™ä½è®­ç»ƒæˆæœ¬ã€‚ç„¶è€Œï¼Œå¦‚ä½•éšç€æ–°æŒ‡ä»¤æ•°æ®çš„å‘å±•è€Œå‘å±•è¿™äº›é€‰æ‹©çš„å­é›†å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å®ç°LLMçš„æŒç»­å¯¹é½ï¼Œæˆ‘ä»¬å¼•å…¥äº†æŒ‡ä»¤åº“ï¼ˆInsBankï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­æ›´æ–°çš„å­˜å‚¨åº“ï¼Œé›†æˆäº†æœ€æ–°çš„å®è´µæŒ‡ä»¤æ•°æ®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ¸è¿›å¼æŒ‡ä»¤åº“è¿›åŒ–ï¼ˆPIBEï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨éšæ—¶é—´æœ‰æ•ˆåœ°å®ç°InsBankçš„è¿›åŒ–ã€‚PIBEé‡‡ç”¨é€æ­¥æ•°æ®é€‰æ‹©ç­–ç•¥æ¥ç»´æŒé•¿æœŸæ•ˆç‡ï¼Œåˆ©ç”¨åŸºäºè¡¨ç¤ºçš„å¤šæ ·æ€§åˆ†æ•°æ¥æ•æ‰æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»å¹¶ä¿ç•™å†å²ä¿¡æ¯è¿›è¡Œå…¨é¢çš„å¤šæ ·æ€§è¯„ä¼°ã€‚è¿™ä¹Ÿå…è®¸åœ¨æ•°æ®é€‰æ‹©å’Œæ’åè¿‡ç¨‹ä¸­çµæ´»ç»„åˆå¤šæ ·æ€§å’Œè´¨é‡åˆ†æ•°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPIBEåœ¨InsBankè¿›åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œå¹¶ä¸”èƒ½å¤Ÿæå–ç‰¹å®šé¢„ç®—çš„å­é›†ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11419v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMçš„æŒ‡ä»¤è°ƒæ•´æ˜¯å…³æ³¨è´¨é‡å’Œå¤šæ ·æ€§çš„æ–°æ—¶ä»£çš„ç ”ç©¶é‡ç‚¹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬æå‡ºä¸€ç§æ–°é¢–æ¡†æ¶Progressive Instruction Bank Evolutionï¼ˆPIBEï¼‰ï¼Œç”¨äºæœ‰æ•ˆåœ°é›†æˆå’Œæ›´æ–°é«˜è´¨é‡æŒ‡ä»¤æ•°æ®ã€‚PIBEä½¿ç”¨é€æ­¥çš„æ•°æ®é€‰æ‹©ç­–ç•¥ç»´æŒé•¿æœŸæ•ˆç‡ï¼Œç»“åˆåŸºäºè¡¨ç°çš„å¤šæ ·æ€§å’Œè´¨é‡åˆ†æ•°è¿›è¡Œæ•°æ®ç­›é€‰å’Œæ’åï¼Œä»è€Œåœ¨èµ„æºé¢„ç®—æ–¹é¢å±•ç¤ºå‡ºäº†å…¶çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯ä¸ºäº†åœ¨é•¿æœŸå‘å±•è¿‡ç¨‹ä¸­ç»´æŒLLMçš„æŒç»­å¯¹é½çŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„ç ”ç©¶é‡ç‚¹å·²ä»æ•°é‡è½¬å‘æŒ‡ä»¤æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>ä¸ºå®ç°LLMçš„æŒç»­å¯¹é½ï¼Œå¼•å…¥äº†Instruction Bankï¼ˆInsBankï¼‰è¿™ä¸€æŒç»­æ›´æ–°çš„æŒ‡ä»¤æ•°æ®å­˜å‚¨åº“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶Progressive Instruction Bank Evolutionï¼ˆPIBEï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆå’Œé«˜æ•ˆåœ°æ¨è¿›InsBankçš„è¿›åŒ–ã€‚</li>
<li>PIBEé‡‡ç”¨é€æ­¥æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œä»¥ç»´æŒé•¿æœŸæ•ˆç‡ã€‚</li>
<li>PIBEåˆ©ç”¨è¡¨ç°å¤šæ ·æ€§åˆ†æ•°è¿›è¡Œæ•°æ®ç­›é€‰å’Œæ’åï¼ŒåŒæ—¶è€ƒè™‘å¤šæ ·æ€§å’Œè´¨é‡å› ç´ ã€‚</li>
<li>PIBEèƒ½å¤Ÿåœ¨é¢„ç®—é™åˆ¶ä¸‹æå–ç‰¹å®šå­é›†ï¼Œè¡¨ç°å‡ºçµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2502.11419v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2502.11419v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2502.11419v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ROSE-A-Reward-Oriented-Data-Selection-Framework-for-LLM-Task-Specific-Instruction-Tuning"><a href="#ROSE-A-Reward-Oriented-Data-Selection-Framework-for-LLM-Task-Specific-Instruction-Tuning" class="headerlink" title="ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific   Instruction Tuning"></a>ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific   Instruction Tuning</h2><p><strong>Authors:Yang Wu, Huayi Zhang, Yizheng Jiao, Lin Ma, Xiaozhong Liu, Jinhong Yu, Dongyu Zhang, Dezhi Yu, Wei Xu</strong></p>
<p>Instruction tuning has underscored the significant potential of large language models (LLMs) in producing more human controllable and effective outputs in various domains. In this work, we focus on the data selection problem for task-specific instruction tuning of LLMs. Prevailing methods primarily rely on the crafted similarity metrics to select training data that aligns with the test data distribution. The goal is to minimize instruction tuning loss on the test data, ultimately improving performance on the target task. However, it has been widely observed that instruction tuning loss (i.e., cross-entropy loss for next token prediction) in LLMs often fails to exhibit a monotonic relationship with actual task performance. This misalignment undermines the effectiveness of current data selection methods for task-specific instruction tuning. To address this issue, we introduce ROSE, a novel Reward-Oriented inStruction data sElection method which leverages pairwise preference loss as a reward signal to optimize data selection for task-specific instruction tuning. Specifically, ROSE adapts an influence formulation to approximate the influence of training data points relative to a few-shot preference validation set to select the most task-related training data points. Experimental results show that by selecting just 5% of the training data using ROSE, our approach can achieve competitive results compared to fine-tuning with the full training dataset, and it surpasses other state-of-the-art data selection methods for task-specific instruction tuning. Our qualitative analysis further confirms the robust generalizability of our method across multiple benchmark datasets and diverse model architectures. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå·²ç»å‡¸æ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¢†åŸŸäº§ç”Ÿæ›´å¤šäººç±»å¯æ§å’Œæœ‰æ•ˆè¾“å‡ºçš„æ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºLLMä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒåˆ¶çš„æ•°é€‰æ‹©é—®é¢˜ã€‚æµè¡Œçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†æ¥é€‰æ‹©ä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒä¸€è‡´çš„è®­ç»ƒæ•°æ®ã€‚ç›®æ ‡æ˜¯å‡å°æµ‹è¯•æ•°æ®ä¸Šçš„æŒ‡ä»¤å¾®è°ƒæŸå¤±ï¼Œæœ€ç»ˆæé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œäººä»¬æ™®éè§‚å¯Ÿåˆ°ï¼ŒLLMä¸­çš„æŒ‡ä»¤å¾®è°ƒæŸå¤±ï¼ˆå³ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„äº¤å‰ç†µæŸå¤±ï¼‰ä¸å®é™…ä»»åŠ¡æ€§èƒ½ä¹‹é—´å¾€å¾€æ²¡æœ‰å•è°ƒå…³ç³»ã€‚è¿™ç§é”™ä½é™ä½äº†å½“å‰æ•°æ®é€‰æ‹©æ–¹æ³•åœ¨ç‰¹å®šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ROSEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¥–åŠ±å¯¼å‘æŒ‡ä»¤æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æˆå¯¹åå¥½æŸå¤±ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä¼˜åŒ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡æŒ‡ä»¤çš„æ•°æ®é€‰æ‹©ã€‚å…·ä½“æ¥è¯´ï¼ŒROSEé‡‡ç”¨ä¸€ç§å½±å“å…¬å¼æ¥è¿‘ä¼¼è®­ç»ƒæ•°æ®ç‚¹ä¸å°‘é‡åå¥½éªŒè¯é›†çš„å½±å“å…³ç³»ï¼Œä»¥é€‰æ‹©æœ€ç›¸å…³çš„è®­ç»ƒæ•°æ®ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ROSEä»…é€‰æ‹©5%çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°±å¯ä»¥å–å¾—ä¸å…¨è®­ç»ƒæ•°æ®é›†å¾®è°ƒç›¸ç«äº‰çš„ç»“æœï¼Œå¹¶ä¸”è¶…è¶Šäº†å…¶ä»–é’ˆå¯¹ç‰¹å®šä»»åŠ¡æŒ‡ä»¤çš„å…ˆè¿›æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®šæ€§åˆ†æè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œå„ç§æ¨¡å‹æ¶æ„ä¸­çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00631v2">PDF</a> EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ä¸­çš„æ•°æ®é€‰æ‹©é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹å·¥æ„å»ºçš„ç›¸ä¼¼æ€§åº¦é‡æ¥é€‰æ‹©ä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒå¯¹é½çš„è®­ç»ƒæ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ROSEæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æˆå¯¹åå¥½æŸå¤±ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œé€šè¿‡å½±å“å…¬å¼æ¥è¿‘ä¼¼é€‰æ‹©ä¸å°‘æ•°åå¥½éªŒè¯é›†ç›¸å…³çš„è®­ç»ƒæ•°æ®ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROSEåªéœ€é€‰æ‹©5%çš„è®­ç»ƒæ•°æ®å³å¯è·å¾—ä¸å…¨æ•°æ®é›†å¾®è°ƒç›¸å½“çš„ç»“æœï¼Œå¹¶è¶…è¶Šäº†å…¶ä»–å…ˆè¿›çš„æ•°æ®é€‰æ‹©æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ä¸­å±•ç°å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰æ•°æ®é€‰æ‹©æ–¹æ³•ä¸»è¦ä¾èµ–ä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒçš„ç›¸ä¼¼æ€§åº¦é‡æ¥é€‰æ‹©è®­ç»ƒæ•°æ®ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´æŸå¤±ä¸å®é™…ä»»åŠ¡æ€§èƒ½ä¹‹é—´ç»å¸¸å­˜åœ¨ä¸åŒ¹é…ï¼Œå¯¼è‡´ç°æœ‰æ•°æ®é€‰æ‹©æ–¹æ³•æ•ˆæœä¸ä½³ã€‚</li>
<li>ROSEæ–¹æ³•åˆ©ç”¨æˆå¯¹åå¥½æŸå¤±ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥ä¼˜åŒ–æ•°æ®é€‰æ‹©ã€‚</li>
<li>ROSEé€šè¿‡å½±å“å…¬å¼é€‰æ‹©æœ€ç›¸å…³çš„è®­ç»ƒæ•°æ®ç‚¹ï¼Œç›¸å¯¹å°‘æ•°åå¥½éªŒè¯é›†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒROSEä»…é€‰æ‹©5%çš„è®­ç»ƒæ•°æ®å³å¯è·å¾—ä¸å…¨æ•°æ®é›†å¾®è°ƒç›¸å½“çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2412.00631v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2412.00631v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2412.00631v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Soft-TransFormers-for-Continual-Learning"><a href="#Soft-TransFormers-for-Continual-Learning" class="headerlink" title="Soft-TransFormers for Continual Learning"></a>Soft-TransFormers for Continual Learning</h2><p><strong>Authors:Haeyong Kang, Chang D. Yoo</strong></p>
<p>Inspired by the Well-initialized Lottery Ticket Hypothesis (WLTH), which provides suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF sequentially learns and selects an optimal soft-network for each task. During sequential training in CL, a well-initialized Soft-TF mask optimizes the weights of sparse layers to obtain task-adaptive soft (real-valued) networks, while keeping the well-pre-trained layer parameters frozen. In inference, the identified task-adaptive network of Soft-TF masks the parameters of the pre-trained network, mapping to an optimal solution for each task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves the knowledge of the pre-trained network. Extensive experiments on the Vision Transformer (ViT) and the Language Transformer (Bert) demonstrate the effectiveness of Soft-TF, achieving state-of-the-art performance across Vision and Language Class Incremental Learning (CIL) scenarios. </p>
<blockquote>
<p>å—åˆå§‹çŠ¶æ€è‰¯å¥½çš„å½©ç¥¨ç¥¨æ ¹å‡è®¾ï¼ˆWLTHï¼‰çš„å¯å‘ï¼Œè¯¥å‡è®¾æä¾›äº†æ¬¡ä¼˜çš„å¾®è°ƒè§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å®Œå…¨å¾®è°ƒè¿ç»­å­¦ä¹ ï¼ˆCLï¼‰æ–¹æ³•ï¼Œç§°ä¸ºSoft-TransFormersï¼ˆSoft-TFï¼‰ã€‚Soft-TFæŒ‰é¡ºåºå­¦ä¹ å¹¶ä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©æœ€ä½³è½¯ç½‘ç»œã€‚åœ¨è¿ç»­å­¦ä¹ ï¼ˆCLï¼‰çš„åºåˆ—è®­ç»ƒä¸­ï¼Œè‰¯å¥½åˆå§‹åŒ–çš„Soft-TFæ©ç ä¼šä¼˜åŒ–ç¨€ç–å±‚çš„æƒé‡ï¼Œä»¥è·å¾—ä»»åŠ¡é€‚åº”æ€§çš„è½¯ï¼ˆå®å€¼ï¼‰ç½‘ç»œï¼ŒåŒæ—¶ä¿æŒé¢„å…ˆè®­ç»ƒå¥½çš„å±‚å‚æ•°ä¸å˜ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒSoft-TFæ‰€ç¡®å®šçš„ä¸ä»»åŠ¡ç›¸é€‚åº”çš„ç½‘ç»œä¼šæ©ç›–é¢„è®­ç»ƒç½‘ç»œçš„å‚æ•°ï¼Œä¸ºæ¯ä¸€ä¸ªä»»åŠ¡æ˜ å°„å‡ºæœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œå¹¶å°½é‡å‡å°‘ç¾éš¾æ€§é—å¿˜ï¼ˆCFï¼‰â€”â€”è½¯æ©ç ä¼šä¿ç•™é¢„è®­ç»ƒç½‘ç»œçš„çŸ¥è¯†ã€‚åœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œè¯­è¨€è½¬æ¢å™¨ï¼ˆBertï¼‰ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†Soft-TFçš„æœ‰æ•ˆæ€§ï¼Œåœ¨è§†è§‰å’Œè¯­è¨€ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆCILï¼‰åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16073v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åŸºäºWell-initialized Lottery Ticket Hypothesisï¼ˆWLTHï¼‰æå‡ºäº†å…¨æ–°çš„å®Œå…¨ç²¾ç»†è°ƒæ•´çš„æŒç»­å­¦ä¹ æ–¹æ³•â€”â€”Soft-TransFormersï¼ˆSoft-TFï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤ŸæŒ‰é¡ºåºå­¦ä¹ å’Œé€‰æ‹©æ¯ä¸ªä»»åŠ¡çš„æœ€ä¼˜è½¯ç½‘ç»œã€‚åœ¨æŒç»­å­¦ä¹ ä¸­è¿›è¡Œé¡ºåºè®­ç»ƒæ—¶ï¼ŒSoft-TFé€šè¿‡ä¼˜åŒ–ç¨€ç–å±‚çš„æƒé‡æ¥è·å¾—ä»»åŠ¡é€‚åº”æ€§çš„è½¯ï¼ˆå®å€¼ï¼‰ç½‘ç»œï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒå±‚å‚æ•°å†»ç»“ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒSoft-TFæ‰€ç¡®å®šçš„ä»»åŠ¡é€‚åº”æ€§ç½‘ç»œä¼šé®è”½é¢„è®­ç»ƒç½‘ç»œçš„å‚æ•°ï¼Œä¸ºæ¯é¡¹ä»»åŠ¡æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œå¹¶æœ€å°åŒ–ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒè¯æ˜ï¼ŒSoft-TFåœ¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å’Œè¯­è¨€å˜å‹å™¨ï¼ˆBertï¼‰ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è§†è§‰å’Œè¯­è¨€ç±»åˆ«å¢é‡å­¦ä¹ åœºæ™¯ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºWell-initialized Lottery Ticket Hypothesisï¼ˆWLTHï¼‰ï¼Œæå‡ºå…¨æ–°çš„æŒç»­å­¦ä¹ æ–¹æ³•â€”â€”Soft-TransFormersï¼ˆSoft-TFï¼‰ã€‚</li>
<li>Soft-TFèƒ½å¤ŸæŒ‰é¡ºåºå­¦ä¹ å’Œé€‰æ‹©æ¯ä¸ªä»»åŠ¡çš„æœ€ä¼˜è½¯ç½‘ç»œã€‚</li>
<li>åœ¨æŒç»­å­¦ä¹ ä¸­è¿›è¡Œé¡ºåºè®­ç»ƒæ—¶ï¼ŒSoft-TFé€šè¿‡ä¼˜åŒ–ç¨€ç–å±‚çš„æƒé‡æ¥è·å¾—ä»»åŠ¡é€‚åº”æ€§çš„è½¯ç½‘ç»œï¼ŒåŒæ—¶ä¿æŒé¢„è®­ç»ƒå±‚å‚æ•°å†»ç»“ã€‚</li>
<li>Soft-TFé€šè¿‡é®è”½é¢„è®­ç»ƒç½‘ç»œçš„å‚æ•°æ¥é€‚åº”ä¸åŒä»»åŠ¡ï¼Œæ‰¾åˆ°æ¯ä¸ªä»»åŠ¡çš„æœ€ä¼˜è§£ã€‚</li>
<li>Soft-TFæ–¹æ³•èƒ½å¤Ÿæœ€å°åŒ–ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>Soft-TFåœ¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å’Œè¯­è¨€å˜å‹å™¨ï¼ˆBertï¼‰ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.16073v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.16073v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.16073v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ACING-Actor-Critic-for-Instruction-Learning-in-Black-Box-LLMs"><a href="#ACING-Actor-Critic-for-Instruction-Learning-in-Black-Box-LLMs" class="headerlink" title="ACING: Actor-Critic for Instruction Learning in Black-Box LLMs"></a>ACING: Actor-Critic for Instruction Learning in Black-Box LLMs</h2><p><strong>Authors:Salma Kharrat, Fares Fourati, Marco Canini</strong></p>
<p>The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at <a target="_blank" rel="noopener" href="https://github.com/salmakh1/ACING">https://github.com/salmakh1/ACING</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®Œæˆä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæŒ‡ä»¤çš„è´¨é‡ï¼Œè€Œè¿™é€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥ç²¾åŠ›æ¥åˆ¶å®šã€‚è¿™å¼ºè°ƒäº†è‡ªåŠ¨æŒ‡ä»¤ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚ç„¶è€Œï¼Œåœ¨ä¸é»‘ç›’LLMï¼ˆæ¨¡å‹å‚æ•°å’Œæ¢¯åº¦ä¸å¯è®¿é—®ï¼‰ä¸€èµ·å·¥ä½œæ—¶ï¼Œä¼˜åŒ–æŒ‡ä»¤å…·æœ‰ç‰¹åˆ«å¤§çš„æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ACINGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¼”å‘˜è¯„è®ºå®¶æ¡†æ¶ï¼Œå®ƒå°†æŒ‡ä»¤ä¼˜åŒ–å…¬å¼åŒ–ä¸ºä¸€ä¸ªæ— çŠ¶æ€ã€è¿ç»­åŠ¨ä½œçš„é—®é¢˜ï¼Œä»…ä½¿ç”¨é»‘ç›’åé¦ˆå°±èƒ½æ¢ç´¢æ— é™çš„æŒ‡ä»¤ç©ºé—´ã€‚åœ¨æŒ‡ä»¤å½’çº³ä»»åŠ¡çš„76%ä¸­ï¼ŒACINGè‡ªåŠ¨å‘ç°çš„æç¤ºä¼˜äºäººç±»ç¼–å†™çš„æç¤ºï¼Œåœ¨æ¶µç›–æŒ‡ä»¤å½’çº³ã€æ‘˜è¦å’Œé“¾å¼æ€ç»´æ¨ç†çš„33é¡¹ä»»åŠ¡ä¸­ï¼Œç›¸è¾ƒäºæœ€ä½³è‡ªåŠ¨åŸºçº¿æœ‰é«˜è¾¾33åˆ†çš„æå‡ï¼Œä¸­ä½æ•°æ”¹å–„ä¸º10åˆ†ã€‚å¤§é‡çš„æ¶ˆèå®éªŒçªæ˜¾äº†å…¶ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚æœ‰å…³ACINGçš„å®ç°å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/salmakh1/ACING%E3%80%82">https://github.com/salmakh1/ACINGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12736v2">PDF</a> Accepted at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæŒ‡ä»¤çš„è´¨é‡ï¼Œè¿™é€šå¸¸éœ€è¦å¤§é‡çš„äººåŠ›æ¥åˆ¶å®šæŒ‡ä»¤ï¼Œè¿™å¼ºè°ƒäº†è‡ªåŠ¨æŒ‡ä»¤ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚ç„¶è€Œï¼Œåœ¨ä¸é»‘ç›’LLMåˆä½œæ—¶ï¼Œç”±äºæ¨¡å‹å‚æ•°å’Œæ¢¯åº¦æ— æ³•è®¿é—®ï¼ŒæŒ‡ä»¤ä¼˜åŒ–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†åä¸ºâ€œACINGâ€çš„æ¼”å‘˜è¯„è®ºå®¶å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†æŒ‡ä»¤ä¼˜åŒ–è¡¨è¿°ä¸ºä¸€ä¸ªæ— çŠ¶æ€ã€è¿ç»­åŠ¨ä½œçš„é—®é¢˜ï¼Œä»…ä½¿ç”¨é»‘ç›’åé¦ˆå³å¯æ¢ç´¢æ— é™çš„æŒ‡ä»¤ç©ºé—´ã€‚åœ¨76%çš„æŒ‡ä»¤æ„Ÿåº”ä»»åŠ¡ä¸­ï¼Œè‡ªåŠ¨å‘ç°çš„æç¤ºè¯è¡¨ç°ä¼˜äºäººä¸ºç¼–å†™çš„æç¤ºè¯ï¼Œåœ¨åŒ…æ‹¬æŒ‡ä»¤æ„Ÿåº”åœ¨å†…çš„å¤šä¸ªä»»åŠ¡ä¸Šè·å¾—æœ€å¤§è‡³è¶…è¿‡æœ€é«˜è‡ªåŠ¨åŒ–åŸºå‡†ç‚¹è¶…è¿‡ä¸‰ç‚¹çš„ç§¯åˆ†å¢é•¿å€¼åŠåç‚¹ä¸­ä½æ•°æ”¹å–„ï¼Œè¡¨æ˜å…¶åœ¨å¤šä»»åŠ¡çš„é²æ£’æ€§å’Œæ•ˆç‡æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚é€šè¿‡åœ¨â€œ<a target="_blank" rel="noopener" href="https://github.com/salmakh1/ACING%E2%80%9D%E4%B8%8A%E5%AE%9E%E7%8E%B0%E3%80%82">https://github.com/salmakh1/ACINGâ€ä¸Šå®ç°ã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ€§å–å†³äºæŒ‡ä»¤çš„è´¨é‡ï¼Œè¿™éœ€è¦å¤§é‡çš„æ‰‹å·¥å®šåˆ¶å·¥ä½œï¼Œå¼ºè°ƒäº†è‡ªåŠ¨æŒ‡ä»¤ä¼˜åŒ–çš„é‡è¦æ€§ã€‚</li>
<li>åœ¨é»‘ç›’LLMç¯å¢ƒä¸­ä¼˜åŒ–æŒ‡ä»¤å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¨¡å‹å‚æ•°å’Œæ¢¯åº¦æ— æ³•è®¿é—®ã€‚</li>
<li>ACINGæ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨ä¼˜åŒ–æŒ‡ä»¤ï¼Œå°†æŒ‡ä»¤ä¼˜åŒ–è¡¨è¿°ä¸ºæ— çŠ¶æ€ã€è¿ç»­åŠ¨ä½œçš„é—®é¢˜ã€‚</li>
<li>ACINGèƒ½å¤Ÿåœ¨å¹¿æ³›çš„æŒ‡ä»¤æ„Ÿåº”ä»»åŠ¡ä¸­è‡ªåŠ¨å‘ç°è¶…è¶Šäººå·¥åˆ¶å®šçš„æŒ‡ä»¤æç¤ºã€‚</li>
<li>ACINGåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚æŒ‡ä»¤æ„Ÿåº”ã€æ‘˜è¦å’Œé“¾å¼æ€ç»´æ¨ç†ç­‰ï¼Œå¹¶ä¸”åœ¨å¤šæ•°ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡æœ€é«˜è‡ªåŠ¨åŒ–åŸºå‡†ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.12736v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.12736v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.12736v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.12736v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2411.12736v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents"><a href="#Transforming-Wearable-Data-into-Personal-Health-Insights-using-Large-Language-Model-Agents" class="headerlink" title="Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents"></a>Transforming Wearable Data into Personal Health Insights using Large   Language Model Agents</h2><p><strong>Authors:Mike A. Merrill, Akshay Paruchuri, Naghmeh Rezaei, Geza Kovacs, Javier Perez, Yun Liu, Erik Schenck, Nova Hammerquist, Jake Sunshine, Shyam Tailor, Kumar Ayush, Hao-Wei Su, Qian He, Cory Y. McLean, Mark Malhotra, Shwetak Patel, Jiening Zhan, Tim Althoff, Daniel McDuff, Xin Liu</strong></p>
<p>Deriving personalized insights from popular wearable trackers requires complex numerical reasoning that challenges standard LLMs, necessitating tool-based approaches like code generation. Large language model (LLM) agents present a promising yet largely untapped solution for this analysis at scale. We introduce the Personal Health Insights Agent (PHIA), a system leveraging multistep reasoning with code generation and information retrieval to analyze and interpret behavioral health data. To test its capabilities, we create and share two benchmark datasets with over 4000 health insights questions. A 650-hour human expert evaluation shows that PHIA significantly outperforms a strong code generation baseline, achieving 84% accuracy on objective, numerical questions and, for open-ended ones, earning 83% favorable ratings while being twice as likely to achieve the highest quality rating. This work can advance behavioral health by empowering individuals to understand their data, enabling a new era of accessible, personalized, and data-driven wellness for the wider population. </p>
<blockquote>
<p>ä»æµè¡Œçš„å¯ç©¿æˆ´è·Ÿè¸ªå™¨ä¸­è·å–ä¸ªæ€§åŒ–æ´å¯Ÿéœ€è¦å¤æ‚çš„æ•°å€¼æ¨ç†ï¼Œè¿™æŒ‘æˆ˜äº†æ ‡å‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œéœ€è¦åŸºäºå·¥å…·çš„æ–¹æ³•ï¼Œå¦‚ä»£ç ç”Ÿæˆã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†äººä¸ºè¿™ç§å¤§è§„æ¨¡åˆ†ææä¾›äº†æœ‰å‰æ™¯ä¸”å°šæœªå……åˆ†åˆ©ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸ªäººå¥åº·æ´å¯Ÿä»£ç†ï¼ˆPHIAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†å’Œä»£ç ç”Ÿæˆä»¥åŠä¿¡æ¯æ£€ç´¢æ¥åˆ†æå¹¶è§£é‡Šè¡Œä¸ºå¥åº·æ•°æ®ã€‚ä¸ºäº†æµ‹è¯•å…¶èƒ½åŠ›ï¼Œæˆ‘ä»¬åˆ›å»ºå¹¶åˆ†äº«äº†åŒ…å«è¶…è¿‡4000ä¸ªå¥åº·æ´å¯Ÿé—®é¢˜çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ã€‚ä¸€é¡¹ä¸ºæœŸ650å°æ—¶çš„äººç±»ä¸“å®¶è¯„ä¼°è¡¨æ˜ï¼ŒPHIAæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„ä»£ç ç”ŸæˆåŸºçº¿ï¼Œåœ¨å®¢è§‚æ•°å€¼é—®é¢˜ä¸Šè¾¾åˆ°84%çš„å‡†ç¡®ç‡ï¼Œåœ¨å¼€æ”¾æ€§é—®é¢˜ä¸Šè·å¾—83%çš„å¥½è¯„ï¼Œå¹¶ä¸”æœ€æœ‰å¯èƒ½è·å¾—æœ€é«˜è´¨é‡è¯„åˆ†ã€‚è¿™é¡¹å·¥ä½œå¯ä»¥é€šè¿‡å¸®åŠ©ä¸ªäººç†è§£ä»–ä»¬çš„æ•°æ®ï¼Œä¸ºæ›´å¹¿æ³›çš„äººç¾¤å¼€å¯ä¸€ä¸ªå¯è®¿é—®ã€ä¸ªæ€§åŒ–ã€æ•°æ®é©±åŠ¨çš„å¥åº·æ–°æ—¶ä»£ï¼Œä»è€Œä¿ƒè¿›è¡Œä¸ºå¥åº·çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06464v3">PDF</a> 53 pages, 7 main figures, 2 main tables, accepted to Nature   Communications</p>
<p><strong>Summary</strong><br>ç©¿æˆ´å¼è¿½è¸ªå™¨äº§ç”Ÿçš„ä¸ªæ€§åŒ–æ´å¯Ÿéœ€è¦å¤æ‚çš„æ•°å€¼æ¨ç†ï¼Œè¿™æŒ‘æˆ˜äº†æ ‡å‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œéœ€è¦åŸºäºå·¥å…·çš„æ–¹æ³•å¦‚ä»£ç ç”Ÿæˆæ¥è§£å†³ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸ªäººå¥åº·æ´å¯Ÿä»£ç†ï¼ˆPHIAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œåˆ©ç”¨å¤šæ­¥éª¤æ¨ç†å’Œä»£ç ç”Ÿæˆæ¥åˆ†æå’Œè§£é‡Šè¡Œä¸ºå¥åº·æ•°æ®ã€‚æµ‹è¯•æ˜¾ç¤ºï¼ŒPHIAåœ¨å®¢è§‚æ•°å€¼é—®é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†84%å’Œ83%ï¼Œå¹¶ä¸”æ›´æœ‰å¯èƒ½è·å¾—æœ€é«˜è´¨é‡è¯„åˆ†ï¼Œæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„ä»£ç ç”ŸæˆåŸºçº¿ã€‚è¿™ä¸ºè¡Œä¸ºå¥åº·é¢†åŸŸå¸¦æ¥äº†ä¸ªæ€§åŒ–çš„æ•°æ®é©±åŠ¨ç¦åˆ©çš„æ–°æ—¶ä»£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©¿æˆ´å¼è¿½è¸ªå™¨äº§ç”Ÿçš„æ•°æ®éœ€è¦å¤æ‚çš„æ•°å€¼æ¨ç†æ¥è½¬åŒ–ä¸ºä¸ªæ€§åŒ–æ´å¯Ÿã€‚</li>
<li>æ ‡å‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›æ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸ªäººå¥åº·æ´å¯Ÿä»£ç†ï¼ˆPHIAï¼‰æ˜¯ä¸€ä¸ªå¤šæ­¥éª¤æ¨ç†ç³»ç»Ÿï¼Œåˆ©ç”¨ä»£ç ç”Ÿæˆå’Œä¿¡æ¯æ£€ç´¢æ¥åˆ†æå¥åº·æ•°æ®ã€‚</li>
<li>PHIAåœ¨å®¢è§‚æ•°å€¼é—®é¢˜å’Œå¼€æ”¾æ€§é—®é¢˜çš„å‡†ç¡®ç‡å¾ˆé«˜ã€‚</li>
<li>PHIAæ˜¾è‘—ä¼˜äºåŸºçº¿ä»£ç ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>PHIAå¯ä»¥ä¸ºä¸ªä½“æä¾›ç†è§£å…¶æ•°æ®çš„å·¥å…·ï¼Œæ¨åŠ¨è¡Œä¸ºå¥åº·é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.06464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2406.06464v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2406.06464v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2406.06464v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_LLM/2406.06464v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Few-Shot/2508.16833v1/page_0_0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Few-shot Human Action Anomaly Detection via a Unified Contrastive   Learning Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_R1_Reasoning/2509.01321v1/page_0_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  SimpleTIR End-to-End Reinforcement Learning for Multi-Turn   Tool-Integrated Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
