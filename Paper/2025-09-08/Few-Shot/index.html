<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Few-shot Human Action Anomaly Detection via a Unified Contrastive   Learning Framework">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-332aca5a95e994063881c7c6b76808bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805272&auth_key=1759805272-0-0-eaa7cada8ff724e46083e4270ee23e26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Framework"><a href="#Few-shot-Human-Action-Anomaly-Detection-via-a-Unified-Contrastive-Learning-Framework" class="headerlink" title="Few-shot Human Action Anomaly Detection via a Unified Contrastive   Learning Framework"></a>Few-shot Human Action Anomaly Detection via a Unified Contrastive   Learning Framework</h2><p><strong>Authors:Koichiro Kamide, Shunsuke Sakai, Shun Maeda, Chunzhi Gu, Chao Zhang</strong></p>
<p>Human Action Anomaly Detection (HAAD) aims to identify anomalous actions given only normal action data during training. Existing methods typically follow a one-model-per-category paradigm, requiring separate training for each action category and a large number of normal samples. These constraints hinder scalability and limit applicability in real-world scenarios, where data is often scarce or novel categories frequently appear. To address these limitations, we propose a unified framework for HAAD that is compatible with few-shot scenarios. Our method constructs a category-agnostic representation space via contrastive learning, enabling AD by comparing test samples with a given small set of normal examples (referred to as the support set). To improve inter-category generalization and intra-category robustness, we introduce a generative motion augmentation strategy harnessing a diffusion-based foundation model for creating diverse and realistic training samples. Notably, to the best of our knowledge, our work is the first to introduce such a strategy specifically tailored to enhance contrastive learning for action AD. Extensive experiments on the HumanAct12 dataset demonstrate the state-of-the-art effectiveness of our approach under both seen and unseen category settings, regarding training efficiency and model scalability for few-shot HAAD. </p>
<blockquote>
<p>äººç±»è¡Œä¸ºå¼‚å¸¸æ£€æµ‹ï¼ˆHAADï¼‰æ—¨åœ¨ä»…åœ¨è®­ç»ƒæœŸé—´ç»™å®šæ­£å¸¸è¡Œä¸ºæ•°æ®çš„æƒ…å†µä¸‹è¯†åˆ«å¼‚å¸¸è¡Œä¸ºã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éµå¾ªä¸€ç§é’ˆå¯¹æ¯ä¸ªç±»åˆ«çš„æ¨¡å‹èŒƒå¼ï¼Œéœ€è¦ä¸ºæ¯ä¸ªåŠ¨ä½œç±»åˆ«è¿›è¡Œå•ç‹¬è®­ç»ƒï¼Œå¹¶ä¸”éœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬ã€‚è¿™äº›çº¦æŸé˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œå¹¶é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå› ä¸ºæ•°æ®é€šå¸¸ç¨€ç¼ºæˆ–ç»å¸¸å‡ºç°æ–°çš„ç±»åˆ«ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„HAADæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä¸å°‘æ•°åœºæ™¯å…¼å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºäº†ä¸€ä¸ªç±»åˆ«æ— å…³çš„è¡¨ç¤ºç©ºé—´ï¼Œé€šè¿‡æ¯”è¾ƒæµ‹è¯•æ ·æœ¬å’Œç»™å®šçš„ä¸€ç»„æ­£å¸¸æ ·æœ¬ï¼ˆç§°ä¸ºæ”¯æŒé›†ï¼‰æ¥å®ç°å¼‚å¸¸æ£€æµ‹ã€‚ä¸ºäº†æé«˜è·¨ç±»åˆ«æ³›åŒ–å’Œç±»åˆ«å†…ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£çš„åŸºç¡€æ¨¡å‹çš„ç”Ÿæˆè¿åŠ¨å¢å¼ºç­–ç•¥ï¼Œç”¨äºåˆ›å»ºå¤šæ ·ä¸”é€¼çœŸçš„è®­ç»ƒæ ·æœ¬ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡å¼•å…¥è¿™æ ·ä¸€ç§ä¸“é—¨å¢å¼ºåŠ¨ä½œå¼‚å¸¸æ£€æµ‹å¯¹æ¯”å­¦ä¹ çš„ç­–ç•¥ã€‚åœ¨HumanAct12æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯è§å’Œä¸å¯è§ç±»åˆ«è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼Œæé«˜äº†å°‘æ•°æ ·æœ¬HAADçš„è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹æ¯”å­¦ä¹ çš„äººç±»è¡Œä¸ºå¼‚å¸¸æ£€æµ‹ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é’ˆå¯¹å°‘æ ·æœ¬æƒ…å†µä¸‹çš„äººç±»è¡Œä¸ºå¼‚å¸¸æ£€æµ‹é—®é¢˜ï¼Œé€šè¿‡æ„å»ºç±»åˆ«æ— å…³çš„è¡¨ç¤ºç©ºé—´å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œå¯¹æ¯”æ£€æµ‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼è¿åŠ¨å¢å¼ºç­–ç•¥ï¼Œä»¥æé«˜è·¨ç±»åˆ«æ³›åŒ–å’Œç±»åˆ«å†…é²æ£’æ€§ã€‚åœ¨HumanAct12æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯è§å’Œä¸å¯è§ç±»åˆ«è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³äººç±»è¡Œä¸ºå¼‚å¸¸æ£€æµ‹ä¸­çš„å°‘æ ·æœ¬é—®é¢˜ï¼Œæå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€‚ç”¨äºå¤šç§åœºæ™¯ã€‚</li>
<li>é€šè¿‡æ„å»ºç±»åˆ«æ— å…³çš„è¡¨ç¤ºç©ºé—´ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œå¯¹æ¯”æ£€æµ‹ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆå¼è¿åŠ¨å¢å¼ºç­–ç•¥ï¼Œæé«˜è·¨ç±»åˆ«æ³›åŒ–å’Œç±»åˆ«å†…é²æ£’æ€§ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€æ¨¡å‹è¿›è¡Œè®­ç»ƒæ ·æœ¬çš„åˆ›å»ºï¼Œç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„æ ·æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨HumanAct12æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d6d99b8cb636ac9cd5667a8fe2b4dc3.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f07ceae1659397c7b1338a3f82de6df~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805287&auth_key=1759805287-0-0-5baa02e93f0f2c0acdd984941e84cfe6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-09d7701d71bf866df5d4b66f8a03eb7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fc18f1a47a36ed22b321a0c67cda110.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-44235dce0f7d2481063b60d1c8375784~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805308&auth_key=1759805308-0-0-ab9019cda304ff6ac08b1dc11e99bc36&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Pattern-Detection-via-Template-Matching-and-Regression"><a href="#Few-Shot-Pattern-Detection-via-Template-Matching-and-Regression" class="headerlink" title="Few-Shot Pattern Detection via Template Matching and Regression"></a>Few-Shot Pattern Detection via Template Matching and Regression</h2><p><strong>Authors:Eunchan Jo, Dahyun Kang, Sanghyun Kim, Yunseon Choi, Minsu Cho</strong></p>
<p>We address the problem of few-shot pattern detection, which aims to detect all instances of a given pattern, typically represented by a few exemplars, from an input image. Although similar problems have been studied in few-shot object counting and detection (FSCD), previous methods and their benchmarks have narrowed patterns of interest to object categories and often fail to localize non-object patterns. In this work, we propose a simple yet effective detector based on template matching and regression, dubbed TMR. While previous FSCD methods typically represent target exemplars as spatially collapsed prototypes and lose structural information, we revisit classic template matching and regression. It effectively preserves and leverages the spatial layout of exemplars through a minimalistic structure with a small number of learnable convolutional or projection layers on top of a frozen backbone We also introduce a new dataset, dubbed RPINE, which covers a wider range of patterns than existing object-centric datasets. Our method outperforms the state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation. </p>
<blockquote>
<p>æˆ‘ä»¬è§£å†³å°æ ·å›¾æ¡ˆæ£€æµ‹çš„é—®é¢˜ï¼Œå…¶ç›®çš„æ˜¯ä»è¾“å…¥å›¾åƒä¸­æ£€æµ‹ç»™å®šå›¾æ¡ˆçš„æ‰€æœ‰å®ä¾‹ï¼Œè¿™äº›å›¾æ¡ˆé€šå¸¸é€šè¿‡å‡ ä¸ªæ ·æœ¬è¡¨ç¤ºã€‚å°½ç®¡ç±»ä¼¼çš„é—®é¢˜å·²ç»åœ¨å°æ ·æœ¬ç›®æ ‡è®¡æ•°å’Œæ£€æµ‹ï¼ˆFSCDï¼‰ä¸­è¿›è¡Œäº†ç ”ç©¶ï¼Œä½†ä¹‹å‰çš„æ–¹æ³•åŠå…¶åŸºå‡†æµ‹è¯•å°†å›¾æ¡ˆå…´è¶£å±€é™äºç›®æ ‡ç±»åˆ«ï¼Œå¹¶ä¸”å¾€å¾€æ— æ³•å®šä½éç›®æ ‡å›¾æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ¿åŒ¹é…å’Œå›å½’çš„ç®€å•æœ‰æ•ˆçš„æ£€æµ‹å™¨ï¼Œè¢«ç§°ä¸ºTMRã€‚è™½ç„¶ä¹‹å‰çš„FSCDæ–¹æ³•é€šå¸¸å°†ç›®æ ‡æ ·æœ¬è¡¨ç¤ºä¸ºç©ºé—´å¡Œé™·çš„åŸå‹å¹¶ä¸¢å¤±ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡æ–°ç ”ç©¶äº†ç»å…¸çš„æ¨¡æ¿åŒ¹é…å’Œå›å½’ã€‚å®ƒé€šè¿‡æç®€çš„ç»“æ„æœ‰æ•ˆåœ°ä¿ç•™å¹¶åˆ©ç”¨äº†æ ·æœ¬çš„ç©ºé—´å¸ƒå±€ï¼Œè¯¥ç»“æ„å…·æœ‰å°‘é‡çš„å¯å­¦ä¹ å·ç§¯å±‚æˆ–æŠ•å½±å±‚ï¼Œå»ºç«‹åœ¨å†»ç»“çš„backboneä¹‹ä¸Šã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œåä¸ºRPINEï¼Œå®ƒæ¯”ç°æœ‰çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®é›†æ¶µç›–äº†æ›´å¹¿æ³›çš„å›¾æ¡ˆèŒƒå›´ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨RPINEã€FSCD-147å’ŒFSCD-LVISè¿™ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17636v1">PDF</a> Accepted to ICCV 2025 (highlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³äº†å°‘æ ·æœ¬æ¨¡å¼æ£€æµ‹çš„é—®é¢˜ï¼Œæ—¨åœ¨ä»è¾“å…¥å›¾åƒä¸­æ£€æµ‹å‡ºç»™å®šæ¨¡å¼çš„æ‰€æœ‰å®ä¾‹ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ¿åŒ¹é…å’Œå›å½’çš„æ£€æµ‹å™¨TMRï¼Œè¯¥æ£€æµ‹å™¨èƒ½æœ‰æ•ˆä¿ç•™å’Œåˆ©ç”¨èŒƒä¾‹çš„ç©ºé—´å¸ƒå±€ä¿¡æ¯ï¼Œä»è€Œè§£å†³ä¹‹å‰æ–¹æ³•å¿½è§†éå¯¹è±¡æ¨¡å¼å®šä½çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†RPINEï¼Œè¦†ç›–çš„æ¨¡å¼èŒƒå›´æ›´å¹¿ã€‚å®éªŒè¡¨æ˜ï¼ŒTMRåœ¨RPINEã€FSCD-147å’ŒFSCD-LVISä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨è·¨æ•°æ®é›†è¯„ä¼°ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« è§£å†³äº†å°‘æ ·æœ¬æ¨¡å¼æ£€æµ‹é—®é¢˜ï¼Œæ—¨åœ¨ä»è¾“å…¥å›¾åƒä¸­æ£€æµ‹å‡ºç»™å®šæ¨¡å¼çš„æ‰€æœ‰å®ä¾‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ¿åŒ¹é…å’Œå›å½’çš„æ£€æµ‹å™¨TMRï¼Œæœ‰æ•ˆä¿ç•™å’Œåˆ©ç”¨èŒƒä¾‹çš„ç©ºé—´å¸ƒå±€ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†RPINEï¼ŒåŒ…å«æ›´å¹¿æ³›çš„æ¨¡å¼èŒƒå›´ã€‚</li>
<li>TMRåœ¨RPINEã€FSCD-147å’ŒFSCD-LVISä¸‰ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºä¹‹å‰çš„æ–¹æ³•åœ¨è¡¨ç¤ºç›®æ ‡èŒƒä¾‹æ—¶å®¹æ˜“ä¸¢å¤±ç»“æ„ä¿¡æ¯ï¼Œè€ŒTMRé€šè¿‡æç®€ç»“æ„æœ‰æ•ˆè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TMRé€šè¿‡é¡¶éƒ¨å¯å­¦ä¹ çš„å·ç§¯å±‚æˆ–æŠ•å½±å±‚ï¼Œåœ¨å†»ç»“çš„éª¨å¹²ç½‘ä¸Šè¿›è¡Œå·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6e830d4830c050e0d7bd7f1d0698ddf2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805315&auth_key=1759805315-0-0-ca55cd9ba2f32d6fcef2b8338eed036e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-a87a279f2399b07fb66fe8b74ef2e346.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-de063c2a55dc94af7d315706ae00ad44~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805330&auth_key=1759805330-0-0-81e0354b28d4e424f2546755d629fb76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1229521e5c5e949b3408bea447966735~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805337&auth_key=1759805337-0-0-9e83e356408385a51541515e6574aafd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f812ea528fea37338477165ec9f7ac9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805345&auth_key=1759805345-0-0-a00115d71d933b47320ff3c7da8b011c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReProCon-Scalable-and-Resource-Efficient-Few-Shot-Biomedical-Named-Entity-Recognition"><a href="#ReProCon-Scalable-and-Resource-Efficient-Few-Shot-Biomedical-Named-Entity-Recognition" class="headerlink" title="ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named   Entity Recognition"></a>ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named   Entity Recognition</h2><p><strong>Authors:Jeongkyun Yoo, Nela Riddle, Andrew Hoblitzell</strong></p>
<p>Named Entity Recognition (NER) in biomedical domains faces challenges due to data scarcity and imbalanced label distributions, especially with fine-grained entity types. We propose ReProCon, a novel few-shot NER framework that combines multi-prototype modeling, cosine-contrastive learning, and Reptile meta-learning to tackle these issues. By representing each category with multiple prototypes, ReProCon captures semantic variability, such as synonyms and contextual differences, while a cosine-contrastive objective ensures strong interclass separation. Reptile meta-updates enable quick adaptation with little data. Using a lightweight fastText + BiLSTM encoder with much lower memory usage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines (around 99 percent of BERT performance). The model remains stable with a label budget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19 to 50 categories, outperforming baselines such as SpanProto and CONTaiNER, which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight the importance of multi-prototype modeling and contrastive learning in managing class imbalance. Despite difficulties with label ambiguity, ReProCon demonstrates state-of-the-art performance in resource-limited settings, making it suitable for biomedical applications. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ç”±äºæ•°æ®ç¨€ç¼ºå’Œæ ‡ç­¾åˆ†å¸ƒä¸å¹³è¡¡è€Œé¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç»†ç²’åº¦çš„å®ä½“ç±»å‹ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ReProConï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å°‘æ ·æœ¬NERæ¡†æ¶ï¼Œç»“åˆäº†å¤šåŸå‹å»ºæ¨¡ã€ä½™å¼¦å¯¹æ¯”å­¦ä¹ å’ŒReptileå…ƒå­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ReProConé€šè¿‡å¤šä¸ªåŸå‹è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œä»è€Œæ•æ‰è¯­ä¹‰å˜åŒ–ï¼Œå¦‚åŒä¹‰è¯å’Œä¸Šä¸‹æ–‡å·®å¼‚ï¼ŒåŒæ—¶ä½™å¼¦å¯¹æ¯”ç›®æ ‡ç¡®ä¿ç±»é—´å¼ºåˆ†ç¦»ã€‚Reptileå…ƒæ›´æ–°ä½¿æ¨¡å‹èƒ½å¤Ÿç”¨å°‘é‡æ•°æ®è¿›è¡Œå¿«é€Ÿé€‚åº”ã€‚ä½¿ç”¨å†…å­˜ä½¿ç”¨æ›´ä½çš„è½»é‡çº§fastText + BiLSTMç¼–ç å™¨ï¼ŒReProConçš„å®è§‚F1åˆ†æ•°æ¥è¿‘BERTåŸºå‡†ï¼ˆçº¦99%çš„BERTæ€§èƒ½ï¼‰ã€‚åœ¨æ ‡ç­¾é¢„ç®—ä¸º30%çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä¿æŒç¨³å®šï¼Œåœ¨æ‰©å±•ä»19ä¸ªç±»åˆ«åˆ°50ä¸ªç±»åˆ«æ—¶ï¼ŒF1åˆ†æ•°ä»…ä¸‹é™7.8%ï¼Œä¼˜äºSpanProtoå’ŒCONTaiNERç­‰åŸºçº¿æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨Few-NERDä¸­çš„æ€§èƒ½ä¸‹é™10%åˆ°32%ã€‚æ¶ˆèç ”ç©¶å¼ºè°ƒäº†å¤šåŸå‹å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ åœ¨ç®¡ç†ç±»åˆ«ä¸å¹³è¡¡ä¸­çš„é‡è¦æ€§ã€‚å°½ç®¡å­˜åœ¨æ ‡ç­¾æ¨¡ç³Šæ€§çš„å›°éš¾ï¼ŒReProConåœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºç”Ÿç‰©åŒ»å­¦åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16833v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­çš„å°æ ·æœ¬å­¦ä¹ é—®é¢˜ï¼Œæå‡ºçš„ReProConæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šåŸå‹å»ºæ¨¡ã€ä½™å¼¦å¯¹æ¯”å­¦ä¹ å’ŒReptileå…ƒå­¦ä¹ ï¼Œä»¥åº”å¯¹æ•°æ®ç¨€ç¼ºå’Œæ ‡ç­¾åˆ†å¸ƒä¸å¹³è¡¡çš„æŒ‘æˆ˜ã€‚ReProConé€šè¿‡å¤šä¸ªåŸå‹è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œæ•è·è¯­ä¹‰å˜åŒ–ï¼Œå¦‚åŒä¹‰è¯å’Œä¸Šä¸‹æ–‡å·®å¼‚ã€‚ä½™å¼¦å¯¹æ¯”ç›®æ ‡ç¡®ä¿ç±»é—´å¼ºåˆ†ç¦»ï¼Œè€ŒReptileå…ƒæ›´æ–°åˆ™å®ç°äº†å¿«é€Ÿé€‚åº”å°‘é‡æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒReProConåœ¨å®è§‚F1åˆ†æ•°ä¸Šæ¥è¿‘BERTåŸºå‡†æµ‹è¯•ï¼Œä¸”åœ¨æ ‡ç­¾é¢„ç®—æœ‰é™å’Œç±»åˆ«æ‰©å±•æƒ…å†µä¸‹è¡¨ç°ç¨³å®šï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReProConæ˜¯ä¸€ä¸ªé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå‘½åå®ä½“è¯†åˆ«çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ•°æ®ç¨€ç¼ºå’Œæ ‡ç­¾åˆ†å¸ƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å¤šåŸå‹å»ºæ¨¡ã€ä½™å¼¦å¯¹æ¯”å­¦ä¹ å’ŒReptileå…ƒå­¦ä¹ ï¼Œä»¥æ•æ‰è¯­ä¹‰å˜åŒ–å’Œå¿«é€Ÿé€‚åº”å°‘é‡æ•°æ®ã€‚</li>
<li>ReProConé€šè¿‡å¤šä¸ªåŸå‹è¡¨ç¤ºæ¯ä¸ªç±»åˆ«ï¼Œä»¥æ•è·åŒä¹‰è¯å’Œä¸Šä¸‹æ–‡å·®å¼‚ã€‚</li>
<li>ä½™å¼¦å¯¹æ¯”å­¦ä¹ ç¡®ä¿äº†ç±»é—´çš„å¼ºåˆ†ç¦»ã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§çš„fastText + BiLSTMç¼–ç å™¨ï¼ŒReProConå®ç°äº†æ¥è¿‘BERTåŸºå‡†çš„å®è§‚F1åˆ†æ•°ï¼Œä¸”å†…å­˜ä½¿ç”¨è¾ƒä½ã€‚</li>
<li>ReProConåœ¨æœ‰é™çš„æ ‡ç­¾é¢„ç®—å’Œç±»åˆ«æ‰©å±•æƒ…å†µä¸‹è¡¨ç°ç¨³å®šï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œå¦‚SpanProtoå’ŒCONTaiNERã€‚</li>
<li>æ¶ˆèç ”ç©¶å¼ºè°ƒäº†å¤šåŸå‹å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ åœ¨åº”å¯¹ç±»åˆ«ä¸å¹³è¡¡ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-332aca5a95e994063881c7c6b76808bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c4121207131270aa23f2aaa6da7b106.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-2befa7177d04490f5e5b387ccbcb7459~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805367&auth_key=1759805367-0-0-9075dc800059102528ee1a48dd7ed3c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-59934f4a181bda8c5a4aa9d5e20374b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805374&auth_key=1759805374-0-0-e8eef47738e39c54269eb8bf917cb36b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4b6f0f05701e2ca7e74f43786381f1ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805380&auth_key=1759805380-0-0-001e50b524b83bff845f7c957018053f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f12201cb2b57ee7c55816f0a05c55b05~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805387&auth_key=1759805387-0-0-4be37a893b4c2edb090e13f2d26cdbe4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bridging-Generalization-and-Personalization-in-Wearable-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning"><a href="#Bridging-Generalization-and-Personalization-in-Wearable-Human-Activity-Recognition-via-On-Device-Few-Shot-Learning" class="headerlink" title="Bridging Generalization and Personalization in Wearable Human Activity   Recognition via On-Device Few-Shot Learning"></a>Bridging Generalization and Personalization in Wearable Human Activity   Recognition via On-Device Few-Shot Learning</h2><p><strong>Authors:Pixi Kang, Julian Moosmann, Mengxi Liu, Bo Zhou, Michele Magno, Paul Lukowicz, Sizhen Bian</strong></p>
<p>Human Activity Recognition (HAR) with wearable devices requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in wearable HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73%, 17.38%, and 3.70%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization \footnote{<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023%7D">https://github.com/kangpx/onlineTiny2023}</a>. </p>
<blockquote>
<p>äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰æŠ€æœ¯é€šè¿‡å¯ç©¿æˆ´è®¾å¤‡å®ç°ï¼Œéœ€è¦å…¼é¡¾è·¨ä¸åŒç”¨æˆ·çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›å’Œé’ˆå¯¹ä¸ªäººçš„é«˜æ•ˆä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„HARæ¨¡å‹åœ¨é¢å¯¹ç”¨æˆ·ç‰¹å®šå˜åŒ–æ—¶å¾€å¾€æ— æ³•æ³›åŒ–ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åœ¨è®¾å¤‡ç«¯çš„å°æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨å¯ç©¿æˆ´çš„HARä¸­å¼¥æ³›åŒ–å’Œä¸ªæ€§åŒ–ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªè·¨ç”¨æˆ·çš„é€šç”¨è¡¨ç¤ºï¼Œç„¶åä»…é€šè¿‡å°‘é‡æ ‡è®°æ ·æœ¬å¿«é€Ÿé€‚åº”æ–°ç”¨æˆ·ï¼Œç›´æ¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ›´æ–°è½»é‡çº§åˆ†ç±»å™¨å±‚ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å…·æœ‰æœ€å°è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„ç¨³å¥åœ¨è®¾å¤‡ç«¯å­¦ä¹ ï¼Œä½¿å…¶æˆä¸ºå®é™…éƒ¨ç½²çš„å®é™…é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨èƒ½æ•ˆé«˜çš„RISC-V GAP9å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¹¶åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆRecGymã€QVAR-Gestureã€Ultrasound-Gestureï¼‰ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œéƒ¨ç½²åçš„é€‚åº”åˆ†åˆ«æé«˜äº†3.73%ã€17.38%å’Œ3.70%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ— ç¼ç»“åˆæ³›åŒ–å’Œä¸ªæ€§åŒ–ï¼Œå°æ ·æœ¬åœ¨è®¾å¤‡ç«¯å­¦ä¹ å¯å®ç°å¯æ‰©å±•çš„ã€ç”¨æˆ·æ„ŸçŸ¥çš„å’Œèƒ½æ•ˆé«˜çš„å¯ç©¿æˆ´äººç±»æ´»åŠ¨è¯†åˆ«ã€‚[^<a target="_blank" rel="noopener" href="https://github.com/kangpx/onlineTiny2023]%E3%80%82">https://github.com/kangpx/onlineTiny2023]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15413v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åœ¨è®¾å¤‡ç«¯è¿›è¡Œå°æ ·æœ¬å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯ç©¿æˆ´è®¾å¤‡ä¸­äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰çš„ç”¨æˆ·ç‰¹å®šå˜åŒ–å¸¦æ¥çš„æ³›åŒ–é—®é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®­ç»ƒè·¨ç”¨æˆ·çš„é€šç”¨è¡¨ç¤ºï¼Œç„¶åä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ ·æœ¬å¯¹æ–°ç”¨æˆ·è¿›è¡Œå¿«é€Ÿé€‚åº”ï¼Œç›´æ¥åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ›´æ–°è½»é‡çº§åˆ†ç±»å™¨å±‚ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å…·æœ‰æœ€å°è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„åœ¨è®¾å¤‡ç«¯ç¨³å¥å­¦ä¹ ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹åœ¨è®¾å¤‡ç«¯è¿›è¡Œå°æ ·æœ¬å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯ç©¿æˆ´è®¾å¤‡ä¸­äººç±»æ´»åŠ¨è¯†åˆ«çš„æ³›åŒ–é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†è·¨ç”¨æˆ·çš„é€šç”¨è¡¨ç¤ºå’Œé’ˆå¯¹ä¸ªäººçš„å¿«é€Ÿé€‚åº”ï¼Œåªéœ€å°‘é‡æ ‡è®°æ ·æœ¬ã€‚</li>
<li>æ¡†æ¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæ›´æ–°è½»é‡çº§åˆ†ç±»å™¨å±‚ï¼Œå‡å°‘è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚</li>
<li>æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œéƒ¨ç½²ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ˆRecGymã€QVAR-Gestureã€Ultrasound-Gestureï¼‰ã€‚</li>
<li>éƒ¨ç½²åçš„é€‚åº”æé«˜äº†å‡†ç¡®æ€§ï¼Œè¡¨æ˜å°æ ·æœ¬åœ¨è®¾å¤‡ç«¯å­¦ä¹ å¯ä»¥æé«˜å¯ç©¿æˆ´äººç±»æ´»åŠ¨è¯†åˆ«çš„å¯æ‰©å±•æ€§ã€ç”¨æˆ·æ„è¯†å’Œèƒ½æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-6b74da2ba1755dd5f213a2a5be0ff0e1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805394&auth_key=1759805394-0-0-a780d9c16663e674717b2d2ae2cedf78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-177563e2614d7637f02aafcadad75ddf.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4c85be1d1894f00f3fce99ca979eedd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805409&auth_key=1759805409-0-0-9742c8bc3a7fc66377942bf31fa2d81b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-47eaf1277fb857ff07ab2ab1af34b59c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction"><a href="#Two-Stage-Quranic-QA-via-Ensemble-Retrieval-and-Instruction-Tuned-Answer-Extraction" class="headerlink" title="Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction"></a>Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer   Extraction</h2><p><strong>Authors:Mohamed Basem, Islam Oshallah, Ali Hamdi, Khaled Shaban, Hozaifa Kassab</strong></p>
<p>Quranic Question Answering presents unique challenges due to the linguistic complexity of Classical Arabic and the semantic richness of religious texts. In this paper, we propose a novel two-stage framework that addresses both passage retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned Arabic language models to achieve superior ranking performance. For answer extraction, we employ instruction-tuned large language models with few-shot prompting to overcome the limitations of fine-tuning on small datasets. Our approach achieves state-of-the-art results on the Quran QA 2023 Shared Task, with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of 0.669 for extraction, substantially outperforming previous methods. These results demonstrate that combining model ensembling and instruction-tuned language models effectively addresses the challenges of low-resource question answering in specialized domains. </p>
<blockquote>
<p>å¤å…°ç»é—®ç­”å­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºå¤å…¸é˜¿æ‹‰ä¼¯è¯­çš„å¤æ‚æ€§å’Œå®—æ•™æ–‡æœ¬ä¸°å¯Œçš„è¯­ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè§£å†³äº†æ®µè½æ£€ç´¢å’Œç­”æ¡ˆæå–ä¸¤ä¸ªé—®é¢˜ã€‚å¯¹äºæ®µè½æ£€ç´¢ï¼Œæˆ‘ä»¬ç»„åˆäº†ç»è¿‡ç²¾ç»†è°ƒæ•´çš„é˜¿æ‹‰ä¼¯è¯­æ¨¡å‹ï¼Œä»¥å®ç°å‡ºè‰²çš„æ’åæ€§èƒ½ã€‚å¯¹äºç­”æ¡ˆæå–ï¼Œæˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°æ ·æœ¬æç¤ºï¼Œä»¥å…‹æœåœ¨å°æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ—¶çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤å…°ç»é—®ç­”å…±äº«ä»»åŠ¡2023ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œæ£€ç´¢çš„MAP@10ä¸º0.3128ï¼ŒMRR@10ä¸º0.5763ï¼Œæå–çš„pAP@10ä¸º0.669ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹ç»„åˆå’ŒæŒ‡ä»¤è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç›¸ç»“åˆæœ‰æ•ˆåœ°è§£å†³äº†ç‰¹å®šé¢†åŸŸä½èµ„æºé—®ç­”ä¸­çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06971v2">PDF</a> 8 pages , 4 figures , Accepted in Aiccsa 2025 ,   <a target="_blank" rel="noopener" href="https://conferences.sigappfr.org/aiccsa2025/">https://conferences.sigappfr.org/aiccsa2025/</a></p>
<p><strong>Summary</strong></p>
<p>è®ºæ–‡æå‡ºä¸€ç§æ–°é¢–çš„åˆ†ä¸ºä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œåº”å¯¹åŸºäºã€Šå¤å…°ç»ã€‹çš„é—®é¢˜å›ç­”ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç²¾ç»†è°ƒæ•´çš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†å‡ºè‰²çš„æ’åæ€§èƒ½ï¼Œå¹¶é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç­”æ¡ˆæå–ã€‚è¯¥æ–¹æ³•åœ¨Quran QA 2023å…±äº«ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚è¯¥ç ”ç©¶ç»“æœè¯æ˜ç»“åˆæ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤å¾®è°ƒçš„è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆè§£å†³ä½èµ„æºé—®ç­”ä¸­çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶é’ˆå¯¹ã€Šå¤å…°ç»ã€‹é—®ç­”ä¸­çš„è¯­è¨€å¤æ‚æ€§å’Œè¯­ä¹‰ä¸°å¯Œæ€§æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé¦–å…ˆè¿›è¡Œæ®µè½æ£€ç´¢ï¼Œç„¶åè¿›è¡Œç­”æ¡ˆæå–ã€‚</li>
<li>æ®µè½æ£€ç´¢ä½¿ç”¨äº†ç²¾ç»†è°ƒæ•´çš„é˜¿æ‹‰ä¼¯è¯­è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†ä¼˜ç§€çš„æ’åæ€§èƒ½ã€‚</li>
<li>ç­”æ¡ˆæå–éƒ¨åˆ†é‡‡ç”¨äº†æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å°‘æ ·æœ¬æç¤ºæ¥å…‹æœåœ¨å°å‹æ•°æ®é›†ä¸Šç²¾ç»†è°ƒæ•´çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Quran QA 2023å…±äº«ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³ç»“æœï¼Œå±•ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ç»“åˆæ¨¡å‹é›†æˆå’ŒæŒ‡ä»¤å¾®è°ƒçš„è¯­è¨€æ¨¡å‹æ˜¯è§£å†³ä½èµ„æºé—®ç­”ä¸­çš„æŒ‘æˆ˜çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f05f22ffd457bb9d501d06881a416c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbe79d6b2e26d011af0d700a5ba6b103.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f321c90d5d59ae815000de08ae58627~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805436&auth_key=1759805436-0-0-4ab0ef464032488aa64e91291c7f8a8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-73161a8e1213e5d5516a717cdd0a688e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4aa54de28abfc814aeda385b56571b6e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Advancing-Dialectal-Arabic-to-Modern-Standard-Arabic-Machine-Translation"><a href="#Advancing-Dialectal-Arabic-to-Modern-Standard-Arabic-Machine-Translation" class="headerlink" title="Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation"></a>Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation</h2><p><strong>Authors:Abdullah Alabdullah, Lifeng Han, Chenghua Lin</strong></p>
<p>Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday communication in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: (i) a comprehensive evaluation of training-free prompting techniques, and (ii) the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. Ara-TEaR is designed as a three-stage self-refinement prompting process, targeting frequent meaning-transfer and adaptation errors in DA-MSA translation. In this evaluation, GPT-4o achieved the highest performance across all prompting settings. For fine-tuning LLMs, a quantized Gemma2-9B model achieved a chrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% chrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies. </p>
<blockquote>
<p>æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ï¼ˆDAï¼‰å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ„æˆäº†æŒç»­çš„æŒ‘æˆ˜ã€‚é˜¿æ‹‰ä¼¯ä¸–ç•Œçš„æ—¥å¸¸æ²Ÿé€šå¤§å¤šå‘ç”Ÿåœ¨ä¸ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰å­˜åœ¨æ˜¾è‘—å·®å¼‚çš„æ–¹è¨€ä¸­ã€‚è¿™ç§è¯­è¨€é¸¿æ²Ÿé˜»ç¢äº†é˜¿æ‹‰ä¼¯è¯­æœºå™¨ç¿»è¯‘çš„è¿›æ­¥ã€‚æœ¬æ–‡é’ˆå¯¹è±ä¸‡æ‰˜ã€åŸƒåŠå’Œæµ·æ¹¾æ–¹è¨€çš„DA-MSAç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™å’Œè®¡ç®—å—é™çš„ç¯å¢ƒä¸­ï¼Œæå‡ºäº†ä¸¤é¡¹æ ¸å¿ƒè´¡çŒ®ï¼šï¼ˆiï¼‰å¯¹æ— è®­ç»ƒæç¤ºæŠ€æœ¯çš„å…¨é¢è¯„ä¼°ï¼Œï¼ˆiiï¼‰èµ„æºé«˜æ•ˆå¾®è°ƒç®¡é“çš„å¼€å‘ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šè¯„ä¼°äº†æç¤ºç­–ç•¥ï¼Œå‘ç°å°‘æç¤ºæ³•å§‹ç»ˆä¼˜äºé›¶æç¤ºæ³•ã€æ€ç»´é“¾å’Œæˆ‘ä»¬æå‡ºçš„Ara-TEaRæ–¹æ³•ã€‚Ara-TEaRè¢«è®¾è®¡ä¸ºä¸€ä¸ªä¸‰é˜¶æ®µçš„è‡ªæˆ‘å®Œå–„æç¤ºè¿‡ç¨‹ï¼Œæ—¨åœ¨è§£å†³DA-MSAç¿»è¯‘ä¸­å¸¸è§çš„æ„ä¹‰è½¬æ¢å’Œé€‚åº”é”™è¯¯ã€‚åœ¨æ­¤è¯„ä¼°ä¸­ï¼ŒGPT-4oåœ¨æ‰€æœ‰æç¤ºè®¾ç½®ä¸­è¡¨ç°æœ€ä½³ã€‚åœ¨å¾®è°ƒLLMæ–¹é¢ï¼Œé‡åŒ–åçš„Gemma2-9Bæ¨¡å‹åœ¨chrF++ä¸Šå–å¾—äº†49.88çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†é›¶æ­¥GPT-4oï¼ˆ44.58ï¼‰ã€‚è”åˆå¤šæ–¹è¨€è®­ç»ƒæ¨¡å‹åœ¨chrF++ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å•æ–¹è¨€è®­ç»ƒæ¨¡å‹è¶…è¿‡10%ï¼Œè€Œ4ä½é‡åŒ–å‡å°‘äº†60%çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±ä¸åˆ°1%ã€‚æˆ‘ä»¬çš„å®éªŒçš„ç»“æœå’Œè§è§£æä¾›äº†ä¸€ä¸ªå®é™…çš„è“å›¾ï¼Œç”¨äºæ”¹å–„æ–¹è¨€åœ¨é˜¿æ‹‰ä¼¯NLPä¸­çš„åŒ…å®¹æ€§ï¼Œè¡¨æ˜å³ä½¿åœ¨æœ‰é™çš„èµ„æºä¸‹ä¹Ÿèƒ½å®ç°é«˜è´¨é‡çš„DA-MSAæœºå™¨ç¿»è¯‘ï¼Œå¹¶ä¸ºæ›´å…·åŒ…å®¹æ€§çš„è¯­è¨€æŠ€æœ¯é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20301v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹é˜¿æ‹‰ä¼¯æ–¹è¨€ï¼ˆDAï¼‰ä¸ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰ä¹‹é—´çš„ç¿»è¯‘æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç³»åˆ—è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« è¯¦ç»†è¯„ä¼°äº†æ— éœ€è®­ç»ƒæç¤ºæŠ€æœ¯ï¼Œå¹¶å¼€å‘äº†ä¸€ç§èµ„æºé«˜æ•ˆçš„å¾®è°ƒç®¡é“ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå°‘æ ·æœ¬æç¤ºæ³•è¡¨ç°æœ€ä½³ï¼ŒGPT-4oåœ¨æ‰€æœ‰æç¤ºè®¾ç½®ä¸­çš„æ€§èƒ½æœ€é«˜ã€‚å¯¹äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé‡åŒ–åçš„Gemma2-9Bæ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚è”åˆå¤šæ–¹è¨€è®­ç»ƒæ¨¡å‹æ¯”å•æ–¹è¨€æ¨¡å‹æ€§èƒ½é«˜å‡ºè¶…è¿‡10%ï¼Œè€Œ4ä½é‡åŒ–å‡å°‘äº†60%çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±ä¸åˆ°1%ã€‚è¿™äº›ç»“æœæä¾›äº†æ”¹å–„é˜¿æ‹‰ä¼¯è¯­NLPä¸­æ–¹è¨€åŒ…å®¹æ€§çš„å®ç”¨è“å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯æ–¹è¨€ï¼ˆDAï¼‰ä¸è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¹‹é—´çš„æŒ‘æˆ˜ï¼šç”±äºé˜¿æ‹‰ä¼¯ä¸–ç•Œä¸­çš„æ—¥å¸¸æ²Ÿé€šä¸»è¦å‘ç”Ÿåœ¨ä¸ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ï¼ˆMSAï¼‰å·®å¼‚æ˜¾è‘—çš„æ–¹è¨€ä¸­ï¼Œè¿™é˜»ç¢äº†é˜¿æ‹‰ä¼¯æœºå™¨ç¿»è¯‘çš„è¿›æ­¥ã€‚</li>
<li>å¯¹æ— éœ€è®­ç»ƒæç¤ºæŠ€æœ¯çš„å…¨é¢è¯„ä¼°ï¼šæ–‡ç« è¯„ä¼°äº†å¤šç§æ— éœ€è®­ç»ƒçš„æç¤ºæŠ€æœ¯ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬æç¤ºã€é›¶æ ·æœ¬å’Œé“¾å¼æ€ç»´ç­‰ã€‚ç»“æœæ˜¾ç¤ºå°‘æ ·æœ¬æç¤ºæ³•è¡¨ç°æœ€ä½³ã€‚</li>
<li>Ara-TEaRæ–¹æ³•çš„å¼•å…¥ï¼šä½œä¸ºä¸€ç§ä¸‰é˜¶æ®µè‡ªæˆ‘å®Œå–„æç¤ºè¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•é’ˆå¯¹æ–¹è¨€ä¸ç°ä»£æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­ç¿»è¯‘ä¸­çš„å¸¸è§æ„ä¹‰è½¬ç§»å’Œé€‚åº”é”™è¯¯ã€‚</li>
<li>GPT-4oåœ¨æç¤ºè®¾ç½®ä¸­çš„æœ€ä½³æ€§èƒ½ï¼šåœ¨è¯„ä¼°ä¸­ï¼ŒGPT-4oåœ¨æ‰€æœ‰æç¤ºè®¾ç½®ä¸­çš„è¡¨ç°æœ€ä½³ã€‚</li>
<li>é‡åŒ–æ¨¡å‹åœ¨å¾®è°ƒä¸­çš„ä¼˜åŠ¿ï¼šé‡åŒ–åçš„Gemma2-9Bæ¨¡å‹åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¸”å†…å­˜ä½¿ç”¨å‡å°‘äº†60%ï¼Œæ€§èƒ½æŸå¤±æå°ã€‚</li>
<li>è”åˆå¤šæ–¹è¨€è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼šè”åˆå¤šæ–¹è¨€ï¼ˆå¦‚Levantineã€åŸƒåŠå’Œæµ·æ¹¾æ–¹è¨€ï¼‰è®­ç»ƒçš„æ¨¡å‹æ¯”å•æ–¹è¨€æ¨¡å‹æ€§èƒ½æ›´é«˜ï¼Œè¶…å‡º10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-69f3c63c2a42d58661dc7fbf8596bbf1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805456&auth_key=1759805456-0-0-31f9d10da6bb7a672bd476a5100cca97&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Label-Semantics-Language-Guided-Action-Anatomy-for-Few-shot-Action-Recognition"><a href="#Beyond-Label-Semantics-Language-Guided-Action-Anatomy-for-Few-shot-Action-Recognition" class="headerlink" title="Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot   Action Recognition"></a>Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot   Action Recognition</h2><p><strong>Authors:Zefeng Qian, Xincheng Yao, Yifei Huang, Chongyang Zhang, Jiangyong Ying, Hong Sun</strong></p>
<p>Few-shot action recognition (FSAR) aims to classify human actions in videos with only a small number of labeled samples per category. The scarcity of training data has driven recent efforts to incorporate additional modalities, particularly text. However, the subtle variations in human posture, motion dynamics, and the object interactions that occur during different phases, are critical inherent knowledge of actions that cannot be fully exploited by action labels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a novel framework that goes beyond label semantics by leveraging Large Language Models (LLMs) to dissect the essential representational characteristics hidden beneath action labels. Guided by the prior knowledge encoded in LLM, LGA effectively captures rich spatiotemporal cues in few-shot scenarios. Specifically, for text, we prompt an off-the-shelf LLM to anatomize labels into sequences of atomic action descriptions, focusing on the three core elements of action (subject, motion, object). For videos, a Visual Anatomy Module segments actions into atomic video phases to capture the sequential structure of actions. A fine-grained fusion strategy then integrates textual and visual features at the atomic level, resulting in more generalizable prototypes. Finally, we introduce a Multimodal Matching mechanism, comprising both video-video and video-text matching, to ensure robust few-shot classification. Experimental results demonstrate that LGA achieves state-of-the-art performance across multipe FSAR benchmarks. </p>
<blockquote>
<p>å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰æ—¨åœ¨åˆ©ç”¨æ¯ä¸€ç±»åˆ«ä»…æœ‰å°‘é‡æ ‡è®°æ ·æœ¬å¯¹è§†é¢‘ä¸­çš„è¡Œä¸ºè¿›è¡Œåˆ†ç±»ã€‚ç”±äºè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œæœ€è¿‘çš„ç ”ç©¶åŠªåŠ›èå…¥äº†é¢å¤–çš„æ¨¡å¼ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ã€‚ç„¶è€Œï¼Œäººç±»å§¿åŠ¿ã€è¿åŠ¨åŠ¨æ€ä»¥åŠåœ¨å„ä¸ªé˜¶æ®µå‘ç”Ÿçš„å¯¹è±¡äº¤äº’çš„ç»†å¾®å˜åŒ–ï¼Œæ˜¯åŠ¨ä½œçš„å…³é”®å›ºæœ‰çŸ¥è¯†ï¼Œä»…å‡­åŠ¨ä½œæ ‡ç­¾æ˜¯æ— æ³•å®Œå…¨åˆ©ç”¨çš„ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Language-Guided Action Anatomyï¼ˆLGAï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å‰–æéšè—åœ¨åŠ¨ä½œæ ‡ç­¾ä¸‹çš„å…³é”®è¡¨å¾ç‰¹å¾ï¼Œè¶…è¶Šäº†æ ‡ç­¾è¯­ä¹‰ã€‚åœ¨LLMç¼–ç çš„å…ˆéªŒçŸ¥è¯†çš„å¼•å¯¼ä¸‹ï¼ŒLGAæœ‰æ•ˆåœ°æ•è·äº†å°‘æ ·æœ¬åœºæ™¯ä¸­çš„ä¸°å¯Œæ—¶ç©ºçº¿ç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ–‡æœ¬ï¼Œæˆ‘ä»¬æç¤ºç°æˆçš„LLMå°†æ ‡ç­¾è§£å‰–æˆåŸå­åŠ¨ä½œæè¿°çš„åºåˆ—ï¼Œä¾§é‡äºåŠ¨ä½œçš„ä¸‰ä¸ªæ ¸å¿ƒå…ƒç´ ï¼ˆä¸»ä½“ã€è¿åŠ¨ã€å¯¹è±¡ï¼‰ã€‚å¯¹äºè§†é¢‘ï¼Œè§†è§‰è§£å‰–æ¨¡å—å°†åŠ¨ä½œåˆ†å‰²æˆåŸå­è§†é¢‘é˜¶æ®µï¼Œä»¥æ•è·åŠ¨ä½œçš„åºåˆ—ç»“æ„ã€‚ä¸€ç§ç²¾ç»†çš„èåˆç­–ç•¥åœ¨åŸå­å±‚é¢æ•´åˆäº†æ–‡æœ¬å’Œè§†é¢‘ç‰¹å¾ï¼Œäº§ç”Ÿäº†æ›´å…·æ¦‚æ‹¬æ€§çš„åŸå‹ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€åŒ¹é…æœºåˆ¶ï¼ŒåŒ…æ‹¬è§†é¢‘-è§†é¢‘å’Œè§†é¢‘-æ–‡æœ¬åŒ¹é…ï¼Œä»¥ç¡®ä¿å¯é çš„å°‘æ ·æœ¬åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLGAåœ¨å¤šä¸ªäººä½“å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16287v2">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºLanguage-Guided Action Anatomyï¼ˆLGAï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºè§£å†³å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«é—®é¢˜ã€‚è¯¥æ¡†æ¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹åŠ¨ä½œæ ‡ç­¾è¿›è¡Œè§£æ„ï¼Œæ•æ‰åŠ¨ä½œçš„å…³é”®ä»£è¡¨æ€§ç‰¹å¾ã€‚é€šè¿‡è§†è§‰è§£å‰–æ¨¡å—å’Œè§†é¢‘ç‰‡æ®µåŒ¹é…æœºåˆ¶ï¼Œå®ç°æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾çš„æ·±åº¦èåˆï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨å¤šåŠ¨ä½œåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›å’Œè¯†åˆ«æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLGAåœ¨å¤šä¸ªFSARåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>LGAæ¡†æ¶ç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æ¥è§£å†³å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«é—®é¢˜ã€‚</li>
<li>LGAåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£æ„åŠ¨ä½œæ ‡ç­¾ï¼ŒæŒ–æ˜åŠ¨ä½œçš„å†…åœ¨çŸ¥è¯†ã€‚</li>
<li>LGAé€šè¿‡è§†è§‰è§£å‰–æ¨¡å—å°†åŠ¨ä½œåˆ†è§£ä¸ºåŸå­è§†é¢‘é˜¶æ®µï¼Œæ•æ‰åŠ¨ä½œçš„åºåˆ—ç»“æ„ã€‚</li>
<li>LGAé‡‡ç”¨ç²¾ç»†çš„èåˆç­–ç•¥ï¼Œåœ¨åŸå­å±‚é¢æ•´åˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ã€‚</li>
<li>LGAå¼•å…¥è§†é¢‘ç‰‡æ®µå’Œè§†é¢‘æ–‡æœ¬åŒ¹é…æœºåˆ¶ï¼Œç¡®ä¿ç¨³å¥çš„å°‘æ ·æœ¬åˆ†ç±»ã€‚</li>
<li>LGAåœ¨å¤šä¸ªFSARåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6c0f616a0b12f82e2de9913edddab0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17082d0b828f4d523f6bbf0ff9d70df4.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-d9d23a893d25159a2c7171e22da4e626~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805478&auth_key=1759805478-0-0-575e708969d0f993c981754cddf526ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8389078680ac640d18931224e9c14567~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805486&auth_key=1759805486-0-0-5723adadb848cde8f233eb4f493e57d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings"><a href="#Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings" class="headerlink" title="Multimodal Medical Image Binding via Shared Text Embeddings"></a>Multimodal Medical Image Binding via Shared Text Embeddings</h2><p><strong>Authors:Yunhao Liu, Suyang Xi, Shiqi Liu, Hong Ding, Chicheng Jin, Chong Zhong, Junjun He, Catherine C. Liu, Yiqing Shen</strong></p>
<p>Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bindâ€™s effectiveness in achieving cross-image-modal alignment for medical analysis. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ¨¡å¼çš„èåˆï¼Œä»¥æ•æ‰äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è¯Šæ–­å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆã€‚å› æ­¤ï¼Œåœ¨è¿™äº›ä¸åŒçš„æ¨¡å¼ä¹‹é—´å®ç°å¯¹é½çš„ç‰¹å¾è¡¨ç¤ºå¯¹äºæœ‰æ•ˆçš„å¤šæ¨¡æ€åˆ†æè‡³å…³é‡è¦ã€‚è™½ç„¶å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åŠå…¶å˜ä½“å·²ç»å®ç°äº†å›¾åƒæ–‡æœ¬çš„å¯¹é½ï¼Œä½†å®ƒä»¬éœ€è¦åœ¨ä»»æ„ä¸¤ä¸ªæ¨¡æ€ä¹‹é—´æ˜ç¡®é…å¯¹çš„æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦ç¯å¢ƒä¸­å¾ˆéš¾è·å–ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´æ— ç¼å¯¹é½å¤šä¸ªåŒ»å­¦æˆåƒæ¨¡å¼ï¼Œè€Œæ— éœ€åœ¨ä»»æ„ä¸¤ä¸ªåŒ»å­¦å›¾åƒæ¨¡å¼ä¹‹é—´è¦æ±‚æ˜ç¡®çš„é…å¯¹æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºä¸åŒå›¾åƒå¯ä»¥è‡ªç„¶ç»‘å®šæ–‡æœ¬çš„çœ‹æ³•ï¼ŒM\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½å…¶ç‰¹å®šäºæ¨¡æ€çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹å®šäºæ¨¡æ€çš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦åˆ°ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªå…±äº«çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚åœ¨Xå°„çº¿ã€CTã€è§†ç½‘è†œã€å¿ƒç”µå›¾å’Œç—…ç†å›¾åƒç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸CLIPç±»æ¨¡å‹ç›¸æ¯”ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†M\textsuperscript{3}Bindåœ¨å®ç°åŒ»å­¦åˆ†æä¸­çš„è·¨å›¾åƒæ¨¡æ€å¯¹é½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18072v2">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åŒ»ç–—å›¾åƒåˆ†æä¸­å¯¹å¤šç§æˆåƒæ¨¡å¼é›†æˆçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºä¸ºå®ç°è·¨ä¸åŒæ¨¡å¼çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶â€”â€”å¤šæ¨¡æ€åŒ»å­¦å›¾åƒä¸æ–‡æœ¬ç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— éœ€ä»»æ„ä¸¤ç§åŒ»å­¦å›¾åƒæ¨¡å¼ä¹‹é—´æ˜ç¡®é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´å®ç°å¤šåŒ»å­¦æˆåƒæ¨¡å¼çš„æ— ç¼å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒéªŒè¯äº†å…¶åœ¨åŒ»å­¦åˆ†æä¸­å®ç°è·¨å›¾åƒæ¨¡æ€å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†æå—ç›Šäºå¤šæˆåƒæ¨¡å¼çš„é›†æˆï¼Œä»¥è·å–äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä¿ƒè¿›æ›´å‡†ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ã€‚</li>
<li>è·¨ä¸åŒåŒ»å­¦æˆåƒæ¨¡å¼å®ç°å¯¹é½ç‰¹å¾è¡¨ç¤ºæ˜¯æœ‰æ•ˆå¤šæ¨¡æ€åˆ†æçš„å…³é”®ã€‚</li>
<li>M\textsuperscript{3}Bindæ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå¯åœ¨æ— éœ€ä»»æ„ä¸¤ç§åŒ»å­¦å›¾åƒæ¨¡å¼ä¹‹é—´æ˜ç¡®é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ç°æ— ç¼å¯¹é½ã€‚</li>
<li>M\textsuperscript{3}BindåŸºäºä¸åŒå›¾åƒè‡ªç„¶ç»‘å®šæ–‡æœ¬çš„è§‚å¿µï¼Œé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚</li>
<li>M\textsuperscript{3}Bindé€šè¿‡è’¸é¦æ¨¡æ€ç‰¹å®šæ–‡æœ¬ç¼–ç å™¨åˆ°ç»Ÿä¸€æ¨¡å‹ï¼Œåˆ›å»ºå…±äº«æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7ef015a129faca98e01aaedfced5ec22~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805495&auth_key=1759805495-0-0-44635a0bc07c7f37a464b135d824c455&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-4c09dc1487f22721e705fc5f091bc94c.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-37c752e54b84b29b76715a071871650d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805509&auth_key=1759805509-0-0-017d0fd09c98793bec4bf8e66a51b9dd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Vision-Language-Agent-System-for-Compositional-Reasoning-with-VLM-assisted-Script-and-Executable-Generation"><a href="#A-Vision-Language-Agent-System-for-Compositional-Reasoning-with-VLM-assisted-Script-and-Executable-Generation" class="headerlink" title="A Vision-Language Agent System for Compositional Reasoning with   VLM-assisted Script and Executable Generation"></a>A Vision-Language Agent System for Compositional Reasoning with   VLM-assisted Script and Executable Generation</h2><p><strong>Authors:Yichang Xu, Gaowen Liu, Ramana Rao Kompella, Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Furkan Tekin, Zachary Yahn, Ling Liu</strong></p>
<p>The advancement in large language models (LLMs) and large vision models has fueled the rapid progress in multi-modal vision-text reasoning capabilities. However, existing vision-language models (VLMs) to date offer poor performance for compositional reasoning. This paper presents VLAgent, a vision-language agent system for vision-text compositional reasoning with three novel features. First, VLAgent leverages a pre-trained LLM with few-shot context learning to generate the planning script for each compositional reasoning task and provides a backend engine to generate and perform executable runtime, which maps the planning script into executable code using the VLAgent library for VLAgent executor. Second, VLAgent introduces the SS-parser, which identifies and corrects logic errors embedded in the LLM-generated planning script, to further enhance the quality of script-executable mapping. Third, VLAgent introduces the compositional reasoning output verifier, which validates and refines the output of complex compositional reasoning steps, by leveraging complementary reasoning techniques, e.g., ensemble learning and caption analysis. Extensive experiments are conducted on six visual benchmarks and compared to a dozen of the SoTA visual reasoning models. The results show that VLAgent outperforms existing representative approaches for compositional text-visual reasoning. Our code and datasets with outputs will be made available upon acceptance. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰æ¨¡å‹çš„è¿›æ­¥ï¼Œå¤šæ¨¡æ€è§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›çš„å¿«é€Ÿå‘å±•å¾—åˆ°äº†æ¨åŠ¨ã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢çš„ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç»„åˆæ¨ç†æ–¹é¢çš„è¡¨ç°è¾ƒå·®ã€‚æœ¬æ–‡æå‡ºäº†VLAgentï¼Œä¸€ä¸ªç”¨äºè§†è§‰æ–‡æœ¬ç»„åˆæ¨ç†çš„è§†è§‰è¯­è¨€ä»£ç†ç³»ç»Ÿï¼Œå…·æœ‰ä¸‰ä¸ªæ–°é¢–çš„ç‰¹ç‚¹ã€‚é¦–å…ˆï¼ŒVLAgentåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä¸ºæ¯é¡¹ç»„åˆæ¨ç†ä»»åŠ¡ç”Ÿæˆè§„åˆ’è„šæœ¬ï¼Œå¹¶æä¾›åç«¯å¼•æ“æ¥ç”Ÿæˆå¹¶æ‰§è¡Œè¿è¡Œæ—¶è„šæœ¬ï¼Œä½¿ç”¨VLAgentåº“å°†è§„åˆ’è„šæœ¬æ˜ å°„ä¸ºå¯æ‰§è¡Œä»£ç ä»¥ä¾›VLAgentæ‰§è¡Œå™¨ä½¿ç”¨ã€‚å…¶æ¬¡ï¼ŒVLAgentå¼•å…¥äº†SSè§£æå™¨ï¼Œç”¨äºè¯†åˆ«å’Œçº æ­£å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§„åˆ’è„šæœ¬ä¸­çš„é€»è¾‘é”™è¯¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜è„šæœ¬å¯æ‰§è¡Œæ˜ å°„çš„è´¨é‡ã€‚ç¬¬ä¸‰ï¼ŒVLAgentå¼•å…¥äº†ç»„åˆæ¨ç†è¾“å‡ºéªŒè¯å™¨ï¼Œå®ƒé€šè¿‡åˆ©ç”¨äº’è¡¥æ¨ç†æŠ€æœ¯ï¼ˆå¦‚é›†æˆå­¦ä¹ å’Œå­—å¹•åˆ†æï¼‰æ¥éªŒè¯å’Œä¼˜åŒ–å¤æ‚ç»„åˆæ¨ç†æ­¥éª¤çš„è¾“å‡ºã€‚åœ¨å…­ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶ä¸ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„è§†è§‰æ¨ç†æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒVLAgentåœ¨ç»„åˆæ–‡æœ¬è§†è§‰æ¨ç†æ–¹é¢ä¼˜äºç°æœ‰ä»£è¡¨æ€§æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒåŒ…å«è¾“å‡ºçš„æ•°æ®é›†å°†åœ¨æ¥å—åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07778v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†å¤šæ¨¡æ€è§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›çš„å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç»„åˆæ¨ç†æ–¹é¢çš„è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†VLAgentï¼Œä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªæ–°ç‰¹æ€§çš„è§†è§‰è¯­è¨€ä»£ç†ç³»ç»Ÿï¼Œç”¨äºè§†è§‰æ–‡æœ¬ç»„åˆæ¨ç†ã€‚VLAgentåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä¸ºæ¯é¡¹ç»„åˆæ¨ç†ä»»åŠ¡ç”Ÿæˆè§„åˆ’è„šæœ¬ï¼Œå¹¶æä¾›åç«¯å¼•æ“æ¥ç”Ÿæˆå¹¶æ‰§è¡Œè¿è¡Œæ—¶è„šæœ¬ã€‚æ­¤å¤–ï¼ŒVLAgentè¿˜å¼•å…¥äº†SSè§£æå™¨ï¼Œç”¨äºè¯†åˆ«å’Œçº æ­£å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è§„åˆ’è„šæœ¬ä¸­çš„é€»è¾‘é”™è¯¯ã€‚æœ€åï¼ŒVLAgentå¼•å…¥äº†ç»„åˆæ¨ç†è¾“å‡ºéªŒè¯å™¨ï¼Œé€šè¿‡åˆ©ç”¨äº’è¡¥æ¨ç†æŠ€æœ¯ï¼ˆå¦‚é›†æˆå­¦ä¹ å’Œå­—å¹•åˆ†æï¼‰æ¥éªŒè¯å’Œç»†åŒ–å¤æ‚ç»„åˆæ¨ç†æ­¥éª¤çš„è¾“å‡ºã€‚åœ¨å…­ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVLAgentåœ¨ç»„åˆæ–‡æœ¬è§†è§‰æ¨ç†æ–¹é¢ä¼˜äºç°æœ‰ä»£è¡¨æ€§æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†å¤šæ¨¡æ€è§†è§‰æ–‡æœ¬æ¨ç†çš„å‘å±•ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»„åˆæ¨ç†æ–¹é¢çš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>VLAgentç³»ç»Ÿå…·æœ‰ä¸‰ä¸ªæ–°ç‰¹æ€§ï¼ŒåŒ…æ‹¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œç”Ÿæˆè§„åˆ’è„šæœ¬å¹¶æ˜ å°„ä¸ºå¯æ‰§è¡Œä»£ç ã€‚</li>
<li>VLAgentå¼•å…¥äº†SSè§£æå™¨æ¥è¯†åˆ«å’Œçº æ­£è§„åˆ’è„šæœ¬ä¸­çš„é€»è¾‘é”™è¯¯ã€‚</li>
<li>VLAgentåŒ…å«ç»„åˆæ¨ç†è¾“å‡ºéªŒè¯å™¨ï¼Œç”¨äºéªŒè¯å’Œç»†åŒ–å¤æ‚ç»„åˆæ¨ç†æ­¥éª¤çš„è¾“å‡ºã€‚</li>
<li>VLAgentåœ¨å…­ä¸ªè§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-35d5c54a34c4932706e9cf4545e518b1~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805517&auth_key=1759805517-0-0-4f32ce84724b43cad45264046ab4ef03&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-52bc9cbcc5ef77783ad7ff4a1b38741a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805525&auth_key=1759805525-0-0-ae30d5fd898af8fa876cb8d2e9696374&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e5426f2e01b217ebb0595a9d7b82b314~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805532&auth_key=1759805532-0-0-da798625af72a61767ba9f19316bf1ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-e97e30bc5b766f54e1d5ea280aae9b35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40399f8d3d6a5154d3adbc3f373ac5e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d263a505a633bdc1a5c8025f8d70581.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition"><a href="#Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition" class="headerlink" title="Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition"></a>Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition</h2><p><strong>Authors:Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°å‡ºä»¤äººç©ç›®çš„è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œå¦‚å›¾åƒè¯†åˆ«å’Œå¯¹è±¡å®šä½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç²¾ç»†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æ—¥å¸¸åœºæ™¯ä¸­ï¼Œäººä»¬é‡åˆ°è®¾è®¡ææ–™ï¼Œå¦‚æ‚å¿—ã€å­—ä½“æ•™ç¨‹ã€ç ”ç©¶è®ºæ–‡æˆ–å“ç‰Œå†…å®¹ï¼Œå¯èƒ½ä¼šå¸Œæœ›è¯†åˆ«æ–‡æœ¬ä¸­ç¾è§‚çš„å­—ä½“ã€‚è€ƒè™‘åˆ°å®ƒä»¬çš„å¤šæ¨¡å¼èƒ½åŠ›å’Œè‡ªç”±å¯è®¿é—®æ€§ï¼Œè®¸å¤šVLMsé€šå¸¸è¢«è®¤ä¸ºæ˜¯å­—ä½“è¯†åˆ«çš„æ½œåœ¨å·¥å…·ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šVLMsæ˜¯å¦çœŸçš„å…·å¤‡å­—ä½“è¯†åˆ«çš„èƒ½åŠ›ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«15ç§å¸¸ç”¨å­—ä½“çš„ç´§å‡‘ä¸”ç»“æ„è‰¯å¥½çš„æ•°æ®é›†ã€‚FRBåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šï¼ˆiï¼‰ä¸€ä¸ªç®€å•ç‰ˆæœ¬ï¼Œå…¶ä¸­10ä¸ªå¥å­ä»¥ä¸åŒçš„å­—ä½“å‘ˆç°ï¼›ï¼ˆiiï¼‰ä¸€ä¸ªå›°éš¾ç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±è¿™15ç§å­—ä½“çš„åç§°ç»„æˆï¼Œå¼•å…¥ä¸€ç§æ–¯ç‰¹é²æ™®æ•ˆåº”ï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹å„ç§VLMåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ä»¥ä¸‹å…³é”®å‘ç°ï¼šï¼ˆiï¼‰å½“å‰VLMåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹æ— æ³•è¾¾åˆ°æ»¡æ„çš„æ€§èƒ½ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°æ–‡æœ¬ä¿¡æ¯å¼•å…¥çš„æ–¯ç‰¹é²æ™®æ•ˆåº”çš„å½±å“ã€‚ï¼ˆiiï¼‰å°æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåœ¨ä¸åŒVLMä¸­æä¾›æœ€å°çš„æ”¹å–„å­—ä½“è¯†åˆ«å‡†ç¡®æ€§çš„å¥½å¤„ã€‚ï¼ˆiiiï¼‰æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23768v3">PDF</a> Accepted to COLM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒè¯†åˆ«ã€ç‰©ä½“å®šä½ç­‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶åœ¨ç²¾ç»†ä»»åŠ¡ä¸­çš„æ•ˆæœä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚å¯¹äºæ—¥å¸¸åœºæ™¯ä¸­é‡åˆ°çš„å­—ä½“è¯†åˆ«é—®é¢˜ï¼Œä¾‹å¦‚æ‚å¿—ã€æ’ç‰ˆæ•™ç¨‹ã€ç ”ç©¶è®ºæ–‡æˆ–å“ç‰Œå†…å®¹ä¸­çš„å­—ä½“ï¼Œäººä»¬å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å‡ºæ–‡æœ¬ä¸­ä½¿ç”¨çš„ç¾è§‚å­—ä½“ã€‚è€ƒè™‘åˆ°VLMsçš„å¤šæ¨¡æ€èƒ½åŠ›å’Œå¼€æ”¾æ€§è®¿é—®æ€§ï¼Œå…¶è¢«è§†ä¸ºæ½œåœ¨å­—ä½“è¯†åˆ«å·¥å…·ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•é›†ï¼ˆFRBï¼‰ï¼ŒåŒ…å«15ç§å¸¸ç”¨å­—ä½“å’Œä¸¤ä¸ªç‰ˆæœ¬ï¼šä¸€ä¸ªç®€å•ç‰ˆæœ¬ï¼Œå…¶ä¸­10å¥è¯ç”¨ä¸åŒçš„å­—ä½“å‘ˆç°ï¼›ä¸€ä¸ªå›°éš¾ç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±è¿™15ç§å­—ä½“çš„åç§°ç»„æˆï¼Œå¼•å…¥æ–¯ç‰¹é²æ™®æ•ˆåº”æŒ‘æˆ˜æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹å„ç§VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œè®¸å¤šå…ˆè¿›æ¨¡å‹æ— æ³•è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œä¸”å®¹æ˜“å—åˆ°æ–¯ç‰¹é²æ™®æ•ˆåº”çš„å½±å“ã€‚æ­¤å¤–ï¼Œå°æ ·å­¦ä¹ æ³•å’ŒChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå¯¹æ”¹è¿›ä¸åŒVLMsçš„å­—ä½“è¯†åˆ«å‡†ç¡®åº¦ä»…æä¾›æœ‰é™å¸®åŠ©ã€‚æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>è®¸å¤šå…ˆè¿›æ¨¡å‹åœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå®¹æ˜“å—åˆ°æ–¯ç‰¹é²æ™®æ•ˆåº”çš„å½±å“ã€‚</li>
<li>å°æ ·å­¦ä¹ æ³•å’ŒChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå¯¹æ”¹è¿›å­—ä½“è¯†åˆ«å‡†ç¡®åº¦çš„ä½œç”¨æœ‰é™ã€‚</li>
<li>æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</li>
<li>FRBæµ‹è¯•é›†ä¸ºè¯„ä¼°VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„æ€§èƒ½æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚</li>
<li>VLMsåœ¨æ—¥å¸¸åœºæ™¯ä¸­çš„å®é™…åº”ç”¨èƒ½åŠ›ä»éœ€è¿›ä¸€æ­¥ç ”ç©¶å’Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a8d14296de5f2fe68e0d914bd1bdec9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805561&auth_key=1759805561-0-0-5aa22518e4020b3a1f84f75a35574dd6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09797cf823fa1543e9098f0df2c1abe6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805568&auth_key=1759805568-0-0-64eee8541e04701e07da60885763bcfc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-689f255139b611ef5d349dcc73baf099~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805575&auth_key=1759805575-0-0-24dec737f84503992c09be5c411e6db1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-73c08be47c36f087923de6cb182eac21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b02f4b80fa36aeba1763c3c0d79e2562.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Legacy-Learning-Strategy-Based-on-Few-Shot-Font-Generation-Models-for-Automatic-Text-Design-in-Metaverse-Content"><a href="#Legacy-Learning-Strategy-Based-on-Few-Shot-Font-Generation-Models-for-Automatic-Text-Design-in-Metaverse-Content" class="headerlink" title="Legacy Learning Strategy Based on Few-Shot Font Generation Models for   Automatic Text Design in Metaverse Content"></a>Legacy Learning Strategy Based on Few-Shot Font Generation Models for   Automatic Text Design in Metaverse Content</h2><p><strong>Authors:Younghwi Kim, Dohee Kim, Seok Chan Jeong, Sunghyun Sim</strong></p>
<p>The metaverse consists of hardware, software, and content, among which text design plays a critical role in enhancing user immersion and usability as a content element. However, in languages such as Korean and Chinese that require thousands of unique glyphs, creating new text designs involves high costs and complexity. To address this, this study proposes a training strategy called Legacy Learning, which recombines and transforms structures based on existing text design models. This approach enables the generation of new text designs and improves quality without manual design processes. To evaluate Legacy Learning, it was applied to Korean and Chinese text designs. Additionally, we compared results before and after on seven state of the art text generation models. As a result, text designs generated using Legacy Learning showed over a 30% difference in Frechet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS) metrics compared to the originals, and also exhibited meaningful style variations in visual comparisons. Furthermore, the repeated learning process improved the structural consistency of the generated characters, and an OCR based evaluation showed increasing recognition accuracy across iterations, indicating improved legibility of the generated glyphs. In addition, a System Usability Scale (SUS) survey was conducted to evaluate usability among metaverse content designers and general users. The expert group recorded a score of 95.78 (â€œBest Imaginableâ€), while the non expert group scored 76.42 (â€œExcellentâ€), indicating an overall high level of usability. These results suggest that Legacy Learning can significantly improve both the production efficiency and quality of text design in the metaverse environment. </p>
<blockquote>
<p>å…ƒå®‡å®™ç”±ç¡¬ä»¶ã€è½¯ä»¶å’Œå†…å®¹ç»„æˆï¼Œå…¶ä¸­æ–‡æœ¬è®¾è®¡ä½œä¸ºå†…å®¹å…ƒç´ ï¼Œåœ¨å¢å¼ºç”¨æˆ·æ²‰æµ¸æ„Ÿå’Œæ˜“ç”¨æ€§æ–¹é¢æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œåœ¨è¯¸å¦‚éŸ©è¯­å’Œä¸­æ–‡ç­‰éœ€è¦æ•°åƒä¸ªç‹¬ç‰¹å­—ç¬¦çš„è¯­è¨€ä¸­ï¼Œåˆ›å»ºæ–°çš„æ–‡æœ¬è®¾è®¡æ¶‰åŠé«˜æ˜‚çš„æˆæœ¬å’Œå¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºLegacy Learningçš„è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºç°æœ‰çš„æ–‡æœ¬è®¾è®¡æ¨¡å‹è¿›è¡Œé‡ç»„å’Œè½¬æ¢ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ–°çš„æ–‡æœ¬è®¾è®¡ï¼Œå¹¶åœ¨æ— éœ€æ‰‹åŠ¨è®¾è®¡æµç¨‹çš„æƒ…å†µä¸‹æé«˜è´¨é‡ã€‚ä¸ºäº†è¯„ä¼°Legacy Learningï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºéŸ©è¯­å’Œä¸­æ–‡çš„æ–‡æœ¬è®¾è®¡ï¼Œå¹¶åœ¨ä¸ƒä¸ªæœ€å…ˆè¿›çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸Šè¿›è¡Œäº†åº”ç”¨å‰åçš„æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨Legacy Learningç”Ÿæˆçš„æ–‡æœ¬è®¾è®¡åœ¨Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒLearned Perceptual Image Patch Similarityï¼ˆLPIPSï¼‰æŒ‡æ ‡ä¸Šä¸åŸå§‹è®¾è®¡ç›¸æ¯”æœ‰è¶…è¿‡30%çš„å·®å¼‚ï¼ŒåŒæ—¶åœ¨è§†è§‰æ¯”è¾ƒä¸­ä¹Ÿè¡¨ç°å‡ºæœ‰æ„ä¹‰çš„é£æ ¼å˜åŒ–ã€‚æ­¤å¤–ï¼Œé‡å¤å­¦ä¹ è¿‡ç¨‹æé«˜äº†ç”Ÿæˆå­—ç¬¦çš„ç»“æ„ä¸€è‡´æ€§ï¼ŒOCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰è¯„ä¼°æ˜¾ç¤ºè¯†åˆ«å‡†ç¡®ç‡éšç€è¿­ä»£è€Œæé«˜ï¼Œè¡¨æ˜ç”Ÿæˆçš„å­—ç¬¦å¯è¯†åˆ«æ€§æœ‰æ‰€æé«˜ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨ï¼ˆSUSï¼‰è°ƒæŸ¥ï¼Œä»¥è¯„ä¼°å…ƒå®‡å®™å†…å®¹è®¾è®¡å¸ˆå’Œä¸€èˆ¬ç”¨æˆ·çš„å¯ç”¨æ€§ã€‚ä¸“å®¶ç»„å¾—åˆ†ä¸º95.78ï¼ˆæœ€é«˜åˆ†ï¼‰ï¼Œéä¸“å®¶ç»„å¾—åˆ†ä¸º76.42ï¼ˆä¼˜ç§€ï¼‰ï¼Œæ˜¾ç¤ºå‡ºæ•´ä½“è¾ƒé«˜çš„å¯ç”¨æ€§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLegacy Learningå¯ä»¥æ˜¾è‘—æé«˜å…ƒå®‡å®™ç¯å¢ƒä¸­æ–‡æœ¬è®¾è®¡çš„ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16900v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åœ¨å…ƒå®‡å®™ç¯å¢ƒä¸­ï¼Œæ–‡æœ¬è®¾è®¡å¯¹äºå¢å¼ºç”¨æˆ·æ²‰æµ¸æ„Ÿå’Œå¯ç”¨æ€§çš„é‡è¦æ€§ã€‚é’ˆå¯¹å¦‚éŸ©è¯­å’Œä¸­æ–‡ç­‰éœ€è¦ç‹¬ç‰¹å­—ç¬¦çš„è¯­è¨€ï¼Œæå‡ºäº†Legacy Learningè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºç°æœ‰æ–‡æœ¬è®¾è®¡æ¨¡å‹è¿›è¡Œé‡ç»„å’Œè½¬æ¢ï¼Œä»¥ç”Ÿæˆæ–°çš„æ–‡æœ¬è®¾è®¡å¹¶æé«˜è´¨é‡ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡è¿‡ç¨‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLegacy Learningåœ¨FIDå’ŒLPIPSæŒ‡æ ‡ä¸Šç›¸è¾ƒäºåŸå§‹è®¾è®¡æœ‰æ˜¾è‘—æ”¹å–„ï¼Œç”Ÿæˆçš„å­—ç¬¦ç»“æ„æ›´åŠ ä¸€è‡´ï¼Œè¯†åˆ«å‡†ç¡®ç‡ä¹Ÿæœ‰æ‰€æé«˜ã€‚åŒæ—¶ï¼Œé€šè¿‡ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨è¯„ä¼°ï¼Œè¯¥ç­–ç•¥åœ¨å…ƒå®‡å®™å†…å®¹è®¾è®¡å¸ˆå’Œæ™®é€šç”¨æˆ·ä¸­çš„å¯ç”¨æ€§å‡è¡¨ç°ä¼˜ç§€ã€‚æ€»ä¹‹ï¼ŒLegacy Learningå¯æ˜¾è‘—æé«˜æ–‡æœ¬è®¾è®¡çš„ç”Ÿäº§æ•ˆç‡å’Œè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è®¾è®¡åœ¨å…ƒå®‡å®™ç¯å¢ƒä¸­å¯¹å¢å¼ºç”¨æˆ·æ²‰æµ¸æ„Ÿå’Œå¯ç”¨æ€§è‡³å…³é‡è¦ã€‚</li>
<li>Legacy Learningæ˜¯ä¸€ç§åŸºäºç°æœ‰æ–‡æœ¬è®¾è®¡æ¨¡å‹çš„è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–°çš„æ–‡æœ¬è®¾è®¡å¹¶æé«˜å…¶è´¨é‡ã€‚</li>
<li>Legacy Learningç­–ç•¥åœ¨éŸ©è¯­å’Œä¸­æ–‡çš„æ–‡æœ¬è®¾è®¡ä¸­å¾—åˆ°äº†åº”ç”¨ã€‚</li>
<li>é€šè¿‡FIDå’ŒLPIPSæŒ‡æ ‡è¯„ä¼°ï¼ŒLegacy Learningåœ¨ç”Ÿæˆæ–‡æœ¬è®¾è®¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>Legacy Learningèƒ½å¤Ÿæ”¹å–„ç”Ÿæˆçš„å­—ç¬¦ç»“æ„ä¸€è‡´æ€§ï¼Œæé«˜å­—ç¬¦è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
<li>ç³»ç»Ÿå¯ç”¨æ€§é‡è¡¨è¯„ä¼°æ˜¾ç¤ºï¼ŒLegacy Learningåœ¨å…ƒå®‡å®™å†…å®¹è®¾è®¡å¸ˆå’Œæ™®é€šç”¨æˆ·ä¸­çš„å¯ç”¨æ€§å‡è¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-58d76c206bec2494740e03d1e4b5533a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805596&auth_key=1759805596-0-0-f8ae9bca5f4d9392b1621beeaf696b3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d4a5b70f7e016e60a6cfa0cc26a189e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805603&auth_key=1759805603-0-0-da7a4748f9451a28ff06634a854564f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="When-predict-can-also-explain-few-shot-prediction-to-select-better-neural-latents"><a href="#When-predict-can-also-explain-few-shot-prediction-to-select-better-neural-latents" class="headerlink" title="When predict can also explain: few-shot prediction to select better   neural latents"></a>When predict can also explain: few-shot prediction to select better   neural latents</h2><p><strong>Authors:Kabir Dabholkar, Omri Barak</strong></p>
<p>Latent variable models serve as powerful tools to infer underlying dynamics from observed neural activity. Ideally, the inferred dynamics should align with true ones. However, due to the absence of ground truth data, prediction benchmarks are often employed as proxies. One widely-used method, $\textit{co-smoothing}$, involves jointly estimating latent variables and predicting observations along held-out channels to assess model performance. In this study, we reveal the limitations of the co-smoothing prediction framework and propose a remedy. Using a student-teacher setup, we demonstrate that models with high co-smoothing can have arbitrary extraneous dynamics in their latent representations. To address this, we introduce a secondary metric â€“ $\textit{few-shot co-smoothing}$, performing regression from the latent variables to held-out neurons in the data using fewer trials. Our results indicate that among models with near-optimal co-smoothing, those with extraneous dynamics underperform in the few-shot co-smoothing compared to &#96;minimalâ€™ models that are devoid of such dynamics. We provide analytical insights into the origin of this phenomenon and further validate our findings on four standard neural datasets using a state-of-the-art method: STNDT. In the absence of ground truth, we suggest a novel measure to validate our approach. By cross-decoding the latent variables of all model pairs with high co-smoothing, we identify models with minimal extraneous dynamics. We find a correlation between few-shot co-smoothing performance and this new measure. In summary, we present a novel prediction metric designed to yield latent variables that more accurately reflect the ground truth, offering a significant improvement for latent dynamics inference. </p>
<blockquote>
<p>æ½œåœ¨å˜é‡æ¨¡å‹æ˜¯æ¨æ–­è§‚å¯Ÿåˆ°çš„ç¥ç»æ´»åŠ¨æ½œåœ¨åŠ¨æ€çš„å¼ºå¤§å·¥å…·ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ¨æ–­å‡ºçš„åŠ¨æ€åº”ä¸çœŸå®æƒ…å†µç›¸ç¬¦ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹çœŸå®æ•°æ®ï¼Œé€šå¸¸ä½¿ç”¨é¢„æµ‹åŸºå‡†ä½œä¸ºä»£ç†ã€‚ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•ï¼Œå³â€œååŒå¹³æ»‘æ³•â€ï¼Œæ¶‰åŠè”åˆä¼°è®¡æ½œåœ¨å˜é‡å¹¶é¢„æµ‹ä¿ç•™é€šé“çš„è§‚å¯Ÿç»“æœï¼Œä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ååŒå¹³æ»‘é¢„æµ‹æ¡†æ¶çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¡¥æ•‘æªæ–½ã€‚æˆ‘ä»¬ä½¿ç”¨å­¦ç”Ÿ-æ•™å¸ˆè®¾ç½®ï¼Œè¯æ˜äº†å…·æœ‰é«˜ååŒå¹³æ»‘çš„æ¨¡å‹åœ¨å…¶æ½œåœ¨è¡¨ç¤ºä¸­å¯èƒ½å­˜åœ¨ä»»æ„çš„é¢å¤–åŠ¨æ€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¬¡è¦æŒ‡æ ‡â€”â€”â€œå°æ ·æœ¬ååŒå¹³æ»‘â€ï¼Œä½¿ç”¨è¾ƒå°‘çš„è¯•éªŒæ¬¡æ•°ä»æ½œåœ¨å˜é‡å›å½’æ•°æ®ä¸­çš„ä¿ç•™ç¥ç»å…ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æ¥è¿‘æœ€ä½³ååŒå¹³æ»‘çš„æ¨¡å‹ä¸­ï¼Œå­˜åœ¨é¢å¤–åŠ¨æ€çš„æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬ååŒå¹³æ»‘æ–¹é¢çš„è¡¨ç°è¾ƒå·®ï¼Œä¸æ²¡æœ‰æ­¤ç±»åŠ¨æ€çš„â€œæœ€å°â€æ¨¡å‹ç›¸æ¯”ã€‚æˆ‘ä»¬å¯¹è¿™ç§ç°è±¡çš„èµ·æºè¿›è¡Œäº†æ·±å…¥çš„åˆ†ææ´å¯Ÿï¼Œå¹¶ä½¿ç”¨æœ€å…ˆè¿›çš„STNDTæ–¹æ³•è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬åœ¨å››ä¸ªæ ‡å‡†ç¥ç»æ•°æ®é›†ä¸Šçš„å‘ç°ã€‚åœ¨æ²¡æœ‰çœŸå®æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æªæ–½æ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ã€‚é€šè¿‡äº¤å‰è§£ç æ‰€æœ‰é«˜ååŒå¹³æ»‘æ¨¡å‹å¯¹çš„æ½œåœ¨å˜é‡ï¼Œæˆ‘ä»¬ç¡®å®šäº†å…·æœ‰æœ€å°‘é¢å¤–åŠ¨æ€çš„æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°å°‘é‡æ ·æœ¬ååŒå¹³æ»‘è¡¨ç°ä¸æ­¤æ–°åº¦é‡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹é¢„æµ‹æŒ‡æ ‡ï¼Œæ—¨åœ¨äº§ç”Ÿæ›´å‡†ç¡®åœ°åæ˜ çœŸå®æƒ…å†µçš„æ½œåœ¨å˜é‡ï¼Œä¸ºæ½œåœ¨åŠ¨æ€æ¨æ–­æä¾›äº†é‡å¤§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14425v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ­ç¤ºäº†ç°æœ‰é¢„æµ‹æ¡†æ¶â€”â€”ååŒå¹³æ»‘çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶é€šè¿‡å­¦ç”Ÿ-æ•™å¸ˆè®¾ç½®æ­ç¤ºäº†å³ä½¿å…·æœ‰è¾ƒé«˜ååŒå¹³æ»‘æ€§èƒ½çš„æ¨¡å‹ä¹Ÿå¯èƒ½åœ¨æ½œåœ¨è¡¨ç¤ºä¸­å­˜åœ¨ä»»æ„é¢å¤–åŠ¨åŠ›å­¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†æ¬¡è¦æŒ‡æ ‡â€”â€”å°‘é•œå¤´ååŒå¹³æ»‘ï¼Œé€šè¿‡è¾ƒå°‘çš„è¯•éªŒæ¬¡æ•°ä»æ½œåœ¨å˜é‡å›å½’æ•°æ®ä¸­çš„æœªè§‚æµ‹ç¥ç»å…ƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ¥è¿‘æœ€ä½³ååŒå¹³æ»‘çš„æ¨¡å‹ä¸­ï¼Œå­˜åœ¨é¢å¤–åŠ¨åŠ›å­¦çš„æ¨¡å‹åœ¨å°‘é•œå¤´ååŒå¹³æ»‘æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚ç ”ç©¶æä¾›äº†å¯¹æ­¤ç°è±¡çš„è§£ææ´å¯Ÿï¼Œå¹¶åœ¨å››ä¸ªæ ‡å‡†ç¥ç»æ•°æ®é›†ä¸Šä½¿ç”¨æœ€æ–°æ–¹æ³•éªŒè¯äº†å‘ç°ã€‚é’ˆå¯¹ç¼ºä¹çœŸå®æ•°æ®çš„æƒ…å†µï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§éªŒè¯æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§è¡¡é‡æ ‡å‡†â€”â€”é€šè¿‡äº¤å‰è§£ç æ‰€æœ‰é«˜ååŒå¹³æ»‘æ¨¡å‹å¯¹çš„æ½œåœ¨å˜é‡æ¥è¯†åˆ«å…·æœ‰æœ€å°‘é¢å¤–åŠ¨åŠ›å­¦çš„æ¨¡å‹ã€‚å‘ç°å°‘é•œå¤´ååŒå¹³æ»‘æ€§èƒ½ä¸æ–°è¡¡é‡æ ‡å‡†ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚æ€»ä¹‹ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„é¢„æµ‹æŒ‡æ ‡ï¼Œæ—¨åœ¨äº§ç”Ÿæ›´å‡†ç¡®åœ°åæ˜ çœŸå®æƒ…å†µçš„æ½œåœ¨å˜é‡ï¼Œä¸ºæ½œåœ¨åŠ¨åŠ›å­¦æ¨æ–­æä¾›äº†é‡å¤§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰é¢„æµ‹æ¡†æ¶ååŒå¹³æ»‘å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å‡†ç¡®åæ˜ æ¨¡å‹çš„æ½œåœ¨åŠ¨åŠ›å­¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢„æµ‹æŒ‡æ ‡â€”â€”å°‘é•œå¤´ååŒå¹³æ»‘ï¼Œé€šè¿‡è¾ƒå°‘çš„è¯•éªŒæ¬¡æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°å­˜åœ¨é¢å¤–åŠ¨åŠ›å­¦çš„æ¨¡å‹åœ¨å°‘é•œå¤´ååŒå¹³æ»‘æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚</li>
<li>æä¾›äº†å¯¹è¿™ç§ç°è±¡çš„è§£ææ´å¯Ÿï¼Œæ­ç¤ºäº†é¢å¤–åŠ¨åŠ›å­¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åœ¨å››ä¸ªæ ‡å‡†ç¥ç»æ•°æ®é›†ä¸ŠéªŒè¯äº†ç ”ç©¶çš„å‘ç°å’Œæ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¡¡é‡æ ‡å‡†æ¥éªŒè¯æ¨¡å‹æ˜¯å¦å…·æœ‰æœ€å°‘é¢å¤–åŠ¨åŠ›å­¦çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0308f411fea1304bbac7920ff288b6e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805610&auth_key=1759805610-0-0-7bc6f95780838d811acbd2b018763e6d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-198e2ce381e9f1419e2844e7dcc9c90a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805618&auth_key=1759805618-0-0-255320bcd4567dfb9fd1ad8639f47ea3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-23f9e76e1a9277c4d4ae010f784741d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805625&auth_key=1759805625-0-0-ed03f8e9d89ae246bf3a2b118560a50b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ff69483cb2444b3cf85d8a4abe6d35f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909546&auth_key=1759909546-0-0-7840ce82eef73d8b7432dfcc4baa2ffb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-52b235d3f37de92834bbb5c6fdd9ccf9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909553&auth_key=1759909553-0-0-b6957a52aecde457a2667f8e65a9b88d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  FedMVP Federated Multimodal Visual Prompt Tuning for Vision-Language   Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-2f52ab41d0474014d3ee4d6d83f05037~resize:0:q75.jpg?source=1f5c5e47&expiration=1759804690&auth_key=1759804690-0-0-04e2a7ac57ee24247eb414bec33df402&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  GPT-FT An Efficient Automated Feature Transformation Using GPT for   Sequence Reconstruction and Performance Enhancement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
