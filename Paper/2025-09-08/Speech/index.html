<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Towards Inclusive Communication A Unified LLM-Based Framework for Sign   Language, Lip Movements, and Audio Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-0ba0dc22cd5f42e6d2c5215b38bfcfff~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910386&auth_key=1759910386-0-0-fec2095dc5d8e69e13f63a2979cdeca8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="Towards-Inclusive-Communication-A-Unified-LLM-Based-Framework-for-Sign-Language-Lip-Movements-and-Audio-Understanding"><a href="#Towards-Inclusive-Communication-A-Unified-LLM-Based-Framework-for-Sign-Language-Lip-Movements-and-Audio-Understanding" class="headerlink" title="Towards Inclusive Communication: A Unified LLM-Based Framework for Sign   Language, Lip Movements, and Audio Understanding"></a>Towards Inclusive Communication: A Unified LLM-Based Framework for Sign   Language, Lip Movements, and Audio Understanding</h2><p><strong>Authors:Jeong Hun Yeo, Hyeongseop Rha, Sungjune Park, Junil Won, Yong Man Ro</strong></p>
<p>Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such systems remain inherently inaccessible to individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we introduce the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that explicitly modeling lip movements as a separate modality significantly improves SLT performance. </p>
<blockquote>
<p>éŸ³é¢‘æ˜¯äººç±»æ²Ÿé€šçš„ä¸»è¦æ–¹å¼ï¼Œå¹¶æ¨åŠ¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿå¯¹äºè‹å“‘äººå£«æ¥è¯´ä»ç„¶å¤©ç”Ÿæ— æ³•è®¿é—®ã€‚æ‰‹è¯­å’Œå”‡è¯»ç­‰è§†è§‰æ›¿ä»£æ–¹æ¡ˆæä¾›äº†æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ³•ï¼Œæœ€è¿‘åœ¨æ‰‹è¯­ç¿»è¯‘ï¼ˆSLTï¼‰å’Œè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æ–¹é¢çš„è¿›å±•æé«˜äº†æ— å£°æ²Ÿé€šçš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å¼å¤§å¤šè¢«å­¤ç«‹ç ”ç©¶ï¼Œå®ƒä»¬åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­çš„æ•´åˆä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ‰‹è¯­ã€å”‡åŠ¨å’ŒéŸ³é¢‘ç­‰å¤šç§ç»„åˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šï¼ˆiï¼‰è®¾è®¡ä¸€ç§ç»Ÿä¸€ã€ä¸æ¨¡æ€æ— å…³çš„æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¼‚æ„è¾“å…¥ï¼›ï¼ˆiiï¼‰æ¢ç´¢æœªè¢«å……åˆ†ç ”ç©¶çš„æ¨¡æ€ä¹‹é—´çš„ååŒä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å”‡åŠ¨ä½œä¸ºéæ‰‹åŠ¨çº¿ç´¢åœ¨æ‰‹è¯­ç†è§£ä¸­çš„ä½œç”¨ï¼›ï¼ˆiiiï¼‰è¾¾åˆ°æˆ–è¶…è¿‡é’ˆå¯¹å•ä¸ªä»»åŠ¡ä¸“ä¸šåŒ–çš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨SLTã€VSRã€ASRå’ŒAVSRæ–¹é¢çš„æ€§èƒ½è¾¾åˆ°æˆ–ä¼˜äºç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå°†å”‡åŠ¨æ˜ç¡®å»ºæ¨¡ä¸ºå•ç‹¬çš„æ¨¡å¼å¯ä»¥æ˜¾è‘—æé«˜SLTæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20476v1">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/JeongHun0716/UniSLA">https://github.com/JeongHun0716/UniSLA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†åŒ…æ‹¬æ‰‹è¯­ã€å˜´å”‡åŠ¨ä½œå’ŒéŸ³é¢‘åœ¨å†…çš„å¤šç§ç»„åˆï¼Œç”¨äºç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚ç ”ç©¶é‡ç‚¹åŒ…æ‹¬è®¾è®¡ä¸€ç§ç»Ÿä¸€ã€æ¨¡å¼æ— å…³çš„ç»“æ„ï¼Œä»¥æœ‰æ•ˆå¤„ç†å¼‚æ„è¾“å…¥ï¼›æ¢ç´¢å„æ¨¡å¼é—´å°¤å…¶æ˜¯å˜´å”‡åŠ¨ä½œåœ¨éæ‰‹åŠ¨æç¤ºæ‰‹è¯­ç†è§£ä¸­çš„è§’è‰²ï¼›ä»¥åŠå®ç°ä¸æˆ–ä¼˜äºé’ˆå¯¹ä¸ªåˆ«ä»»åŠ¡çš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œç ”ç©¶è€…åœ¨æ‰‹è¯­ç¿»è¯‘ã€è§†è§‰è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè§†å¬è¯­éŸ³è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†ä¸æœ€æ–°æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ†ææ˜¾ç¤ºï¼Œå°†å˜´å”‡åŠ¨ä½œä½œä¸ºç‹¬ç«‹æ¨¡å¼è¿›è¡Œå»ºæ¨¡å¯ä»¥æ˜¾è‘—æé«˜æ‰‹è¯­ç¿»è¯‘æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªèƒ½å¤Ÿå¤„ç†æ‰‹è¯­ã€å˜´å”‡åŠ¨ä½œå’ŒéŸ³é¢‘ç­‰å¤šç§æ¨¡å¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åŒ…æ‹¬è®¾è®¡ä¸€ç§æœ‰æ•ˆçš„ç»Ÿä¸€æ¶æ„ï¼Œæ¢ç´¢å„æ¨¡å¼é—´çš„ååŒä½œç”¨ï¼Œå¹¶è¾¾åˆ°æˆ–è¶…è¶Šé’ˆå¯¹ä¸ªåˆ«ä»»åŠ¡çš„æœ€æ–°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åˆ†æå‘ç°ï¼Œå°†å˜´å”‡åŠ¨ä½œä½œä¸ºç‹¬ç«‹æ¨¡å¼è¿›è¡Œå»ºæ¨¡å¯ä»¥æ˜¾è‘—æé«˜æ‰‹è¯­ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æ‰‹è¯­ç¿»è¯‘ã€è§†è§‰è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè§†å¬è¯­éŸ³è¯†åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†è§‰ä¿¡æ¯ï¼ˆå¦‚æ‰‹è¯­å’Œå˜´å”‡åŠ¨ä½œï¼‰åœ¨éŸ³é¢‘æ— æ³•è®¿é—®æˆ–ç¼ºå¤±æƒ…å†µä¸‹çš„é‡è¦æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºæ•´åˆä¸åŒæ²Ÿé€šæ¨¡å¼æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å¹³å°ï¼Œå¢å¼ºäº†æ²Ÿé€šéšœç¢äººå£«çš„æ²Ÿé€šèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-41281b3c337725dd3772f555a2f67b2f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910395&auth_key=1759910395-0-0-9d3b978f3a7192b37cc3f2d4ac15d9ba&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a8808baa9e4db5ec4e7fef6b33f8c59~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910403&auth_key=1759910403-0-0-c7a975652639bb9cf02772607b4229eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f7da9f03586d94b0ea4e99de062d1bc~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910411&auth_key=1759910411-0-0-cefe94b743c800da30f595ee6ccc60b2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Unifying-Diarization-Separation-and-ASR-with-Multi-Speaker-Encoder"><a href="#Unifying-Diarization-Separation-and-ASR-with-Multi-Speaker-Encoder" class="headerlink" title="Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder"></a>Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder</h2><p><strong>Authors:Muhammad Shakeel, Yui Sudo, Yifan Peng, Chyi-Jiunn Lin, Shinji Watanabe</strong></p>
<p>This paper presents a unified multi-speaker encoder (UME), a novel architecture that jointly learns representations for speaker diarization (SD), speech separation (SS), and multi-speaker automatic speech recognition (ASR) tasks using a shared speech foundational encoder. We leverage the hidden representations from multiple layers of UME as a residual weighted-sum encoding (RWSE) to effectively use information from different semantic levels, contributing to bottom-up alignment between tasks. This joint training approach captures the inherent interdependencies among the tasks, enhancing overall performance on overlapping speech data. Our evaluations demonstrate that UME substantially improves over the single-task baselines dedicated to SD, SS, and multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms the previous studies, achieving diarization error rates of 1.37% and 2.29% on Libri2Mix and Libri3Mix evaluation sets, respectively. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šè¯´è¯äººç¼–ç å™¨ï¼ˆUMEï¼‰çš„æ–°æ¶æ„ï¼Œè¯¥æ¶æ„é€šè¿‡å…±äº«çš„åŸºç¡€è¯­éŸ³ç¼–ç å™¨è”åˆå­¦ä¹ è¯´è¯äººåˆ†å›¾ï¼ˆSDï¼‰ã€è¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰å’Œå¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„è¡¨ç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨UMEå¤šå±‚éšè—è¡¨ç¤ºä½œä¸ºæ®‹å·®åŠ æƒå’Œç¼–ç ï¼ˆRWSEï¼‰ï¼Œæœ‰æ•ˆåˆ©ç”¨æ¥è‡ªä¸åŒè¯­ä¹‰çº§åˆ«çš„ä¿¡æ¯ï¼Œä¿ƒè¿›ä»»åŠ¡ä¹‹é—´çš„è‡ªä¸‹è€Œä¸Šçš„å¯¹é½ã€‚è¿™ç§è”åˆè®­ç»ƒæ–¹æ³•æ•è·ä»»åŠ¡ä¹‹é—´çš„å†…åœ¨ç›¸äº’ä¾èµ–æ€§ï¼Œæé«˜äº†é‡å è¯­éŸ³æ•°æ®ä¸Šçš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨LibriMixè¯„ä¼°é›†ä¸Šï¼Œä¸ä¸“é—¨ç”¨äºSDã€SSå’Œå¤šè¯´è¯äººASRçš„å•ä»»åŠ¡åŸºçº¿ç›¸æ¯”ï¼ŒUMEæœ‰æ˜¾è‘—æ”¹å–„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºSDï¼ŒUMEçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„ç ”ç©¶ï¼Œåœ¨Libri2Mixå’ŒLibri3Mixè¯„ä¼°é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†åˆ†å›¾é”™è¯¯ç‡ä¸º1.37%å’Œ2.29%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20474v1">PDF</a> Accepted to IEEE ASRU 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šè¯´è¯äººç¼–ç å™¨ï¼ˆUMEï¼‰çš„æ–°æ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥è”åˆå­¦ä¹ è¯´è¯äººåˆ†æ®µï¼ˆSDï¼‰ã€è¯­éŸ³åˆ†ç¦»ï¼ˆSSï¼‰å’Œå¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„è¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨UMEå¤šå±‚éšè—è¡¨ç¤ºä½œä¸ºæ®‹å·®åŠ æƒå’Œç¼–ç ï¼ˆRWSEï¼‰ï¼Œæœ‰æ•ˆä½¿ç”¨ä¸åŒè¯­ä¹‰å±‚æ¬¡çš„ä¿¡æ¯ï¼Œå®ç°ä»»åŠ¡ä¹‹é—´çš„è‡ªä¸‹è€Œä¸Šçš„å¯¹é½ã€‚è”åˆè®­ç»ƒæ–¹å¼æ•æ‰ä»»åŠ¡ä¹‹é—´çš„å†…åœ¨ç›¸äº’ä¾èµ–æ€§ï¼Œæé«˜é‡å è¯­éŸ³æ•°æ®çš„æ•´ä½“æ€§èƒ½ã€‚åœ¨LibriMixè¯„ä¼°é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒUMEåœ¨SDã€SSå’Œå¤šè¯´è¯äººASRçš„å•ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸Šæœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯åœ¨SDæ–¹é¢ï¼ŒUMEçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„ç ”ç©¶ï¼Œåœ¨Libri2Mixå’ŒLibri3Mixè¯„ä¼°é›†ä¸Šçš„åˆ†æ®µé”™è¯¯ç‡åˆ†åˆ«ä¸º1.37%å’Œ2.29%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ç»Ÿä¸€çš„å¤šè¯´è¯äººç¼–ç å™¨ï¼ˆUMEï¼‰æ–°æ¶æ„ï¼Œå¯è”åˆå­¦ä¹ å¤šç§è¯­éŸ³ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨æ®‹å·®åŠ æƒå’Œç¼–ç ï¼ˆRWSEï¼‰èåˆå¤šå±‚éšè—è¡¨ç¤ºï¼Œä»¥æé«˜ä¸åŒè¯­ä¹‰å±‚æ¬¡çš„ä¿¡æ¯åˆ©ç”¨ã€‚</li>
<li>é€šè¿‡è‡ªä¸‹è€Œä¸Šçš„å¯¹é½æ–¹å¼å®ç°ä»»åŠ¡é—´çš„å¯¹é½ã€‚</li>
<li>è”åˆè®­ç»ƒæ–¹å¼æ•æ‰ä»»åŠ¡é—´çš„å†…åœ¨ç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>UMEåœ¨LibriMixè¯„ä¼°é›†ä¸Šçš„è¡¨ç°ä¼˜äºå•ä»»åŠ¡åŸºå‡†æµ‹è¯•ã€‚</li>
<li>åœ¨è¯´è¯äººåˆ†æ®µï¼ˆSDï¼‰ä»»åŠ¡ä¸Šï¼ŒUMEè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œé”™è¯¯ç‡ä½è‡³1.37%å’Œ2.29%ã€‚</li>
<li>UMEæœ‰åŠ©äºæé«˜å¤„ç†é‡å è¯­éŸ³æ•°æ®çš„æ•´ä½“æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8a84cd8e87143ff821ebe6cf8ae0ff0a.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b58c8fcfe12120cd5b621083881fa96a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910501&auth_key=1759910501-0-0-6e2cafce8ab525da0f43e37e311c58e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-088a837dd9b78056508788a49fe701a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910508&auth_key=1759910508-0-0-af02dba654f8fdb4505f16458767c1f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-86b15f999f7f39723e4e12d86d610a23.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-0ef812b600bf3bb3250269e04d07108d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910522&auth_key=1759910522-0-0-7bf46026f4c5c11ba4f5f8e7d4fdda60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-058c626a206201a25453de0fb3e47ad3.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-adbd955e155e3bc617f56405cf5335f4~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910536&auth_key=1759910536-0-0-5a8690fbf2a708b73518189da4b673f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FLASepformer-Efficient-Speech-Separation-with-Gated-Focused-Linear-Attention-Transformer"><a href="#FLASepformer-Efficient-Speech-Separation-with-Gated-Focused-Linear-Attention-Transformer" class="headerlink" title="FLASepformer: Efficient Speech Separation with Gated Focused Linear   Attention Transformer"></a>FLASepformer: Efficient Speech Separation with Gated Focused Linear   Attention Transformer</h2><p><strong>Authors:Haoxu Wang, Yiheng Jiang, Gang Qiao, Pengteng Shi, Biao Tian</strong></p>
<p>Speech separation always faces the challenge of handling prolonged time sequences. Past methods try to reduce sequence lengths and use the Transformer to capture global information. However, due to the quadratic time complexity of the attention module, memory usage and inference time still increase significantly with longer segments. To tackle this, we introduce Focused Linear Attention and build FLASepformer with linear complexity for efficient speech separation. Inspired by SepReformer and TF-Locoformer, we have two variants: FLA-SepReformer and FLA-TFLocoformer. We also add a new Gated module to improve performance further. Experimental results on various datasets show that FLASepformer matches state-of-the-art performance with less memory consumption and faster inference. FLA-SepReformer-T&#x2F;B&#x2F;L increases speed by 2.29x, 1.91x, and 1.49x, with 15.8%, 20.9%, and 31.9% GPU memory usage, proving our modelâ€™s effectiveness. </p>
<blockquote>
<p>è¯­éŸ³åˆ†ç¦»å§‹ç»ˆé¢ä¸´å¤„ç†é•¿æ—¶é—´åºåˆ—çš„æŒ‘æˆ˜ã€‚è¿‡å»çš„æ–¹æ³•è¯•å›¾å‡å°‘åºåˆ—é•¿åº¦å¹¶ä½¿ç”¨Transformeræ•æ‰å…¨å±€ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç”±äºæ³¨æ„åŠ›æ¨¡å—çš„äºŒæ¬¡æ—¶é—´å¤æ‚åº¦ï¼Œéšç€æ®µè½çš„å¢é•¿ï¼Œå†…å­˜ä½¿ç”¨å’Œæ¨ç†æ—¶é—´ä»ç„¶æ˜¾è‘—å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†èšç„¦çº¿æ€§æ³¨æ„åŠ›ï¼Œå¹¶æ„å»ºäº†å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„FLASepformerï¼Œä»¥å®ç°é«˜æ•ˆçš„è¯­éŸ³åˆ†ç¦»ã€‚å—SepReformerå’ŒTF-Locoformerçš„å¯å‘ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§å˜ä½“ï¼šFLA-SepReformerå’ŒFLA-TFLocoformerã€‚æˆ‘ä»¬è¿˜æ·»åŠ äº†ä¸€ä¸ªæ–°çš„é—¨æ§æ¨¡å—æ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFLASepformerå…·æœ‰è¾ƒå°çš„å†…å­˜æ¶ˆè€—å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚FLA-SepReformer-T&#x2F;B&#x2F;Lçš„é€Ÿåº¦æé«˜äº†2.29å€ã€1.91å€å’Œ1.49å€ï¼ŒGPUå†…å­˜ä½¿ç”¨ç‡åˆ†åˆ«é™ä½äº†15.8%ã€20.9%å’Œ31.9%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19528v1">PDF</a> Accepted by Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³åˆ†ç¦»ä¸­é•¿æœŸåºåˆ—å¤„ç†éš¾é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥èšç„¦çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ„å»ºå…·æœ‰çº¿æ€§å¤æ‚åº¦çš„FLASepformeræ¨¡å‹ï¼Œæé«˜è¯­éŸ³åˆ†ç¦»æ•ˆç‡ã€‚æ¨¡å‹æœ‰ä¸¤ç§å˜ä½“ï¼šFLA-SepReformerå’ŒFLA-TFLocoformerï¼Œå¹¶æ·»åŠ æ–°çš„Gatedæ¨¡å—è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFLASepformerè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶é™ä½å†…å­˜æ¶ˆè€—å¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆ†ç¦»é¢ä¸´å¤„ç†é•¿æœŸåºåˆ—çš„æŒ‘æˆ˜ã€‚</li>
<li>è¿‡å»çš„æ–¹æ³•è¯•å›¾é€šè¿‡å‡å°‘åºåˆ—é•¿åº¦å’Œä½¿ç”¨Transformeræ•æ‰å…¨å±€ä¿¡æ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>ç”±äºæ³¨æ„åŠ›æ¨¡å—çš„äºŒæ¬¡æ—¶é—´å¤æ‚æ€§ï¼Œå†…å­˜ä½¿ç”¨é‡å’Œæ¨ç†æ—¶é—´ä»ç„¶ä¼šéšç€åºåˆ—é•¿åº¦çš„å¢åŠ è€Œæ˜¾è‘—å¢åŠ ã€‚</li>
<li>å¼•å…¥èšç„¦çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ„å»ºå…·æœ‰çº¿æ€§å¤æ‚åº¦çš„FLASepformeræ¨¡å‹ï¼Œä»¥æé«˜è¯­éŸ³åˆ†ç¦»æ•ˆç‡ã€‚</li>
<li>æœ‰ä¸¤ç§æ¨¡å‹å˜ä½“ï¼šFLA-SepReformerå’ŒFLA-TFLocoformerã€‚</li>
<li>æ·»åŠ æ–°çš„Gatedæ¨¡å—è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2ad7ed81a9d481304f0757e53b0eb394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee014d67ce518bf2c446b38edd8bf95c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0f8afe0491ffaea9c0ea76f6922caa8.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b335bc4d3e8bcea09ed398c8d378eab2~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910567&auth_key=1759910567-0-0-8954a8549764ef9b86dbdb09e4748670&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="WildSpoof-Challenge-Evaluation-Plan"><a href="#WildSpoof-Challenge-Evaluation-Plan" class="headerlink" title="WildSpoof Challenge Evaluation Plan"></a>WildSpoof Challenge Evaluation Plan</h2><p><strong>Authors:Yihan Wu, Jee-weon Jung, Hye-jin Shim, Xin Cheng, Xin Wang</strong></p>
<p>The WildSpoof Challenge aims to advance the use of in-the-wild data in two intertwined speech processing tasks. It consists of two parallel tracks: (1) Text-to-Speech (TTS) synthesis for generating spoofed speech, and (2) Spoofing-robust Automatic Speaker Verification (SASV) for detecting spoofed speech. While the organizers coordinate both tracks and define the data protocols, participants treat them as separate and independent tasks. The primary objectives of the challenge are: (i) to promote the use of in-the-wild data for both TTS and SASV, moving beyond conventional clean and controlled datasets and considering real-world scenarios; and (ii) to encourage interdisciplinary collaboration between the spoofing generation (TTS) and spoofing detection (SASV) communities, thereby fostering the development of more integrated, robust, and realistic systems. </p>
<blockquote>
<p>WildSpoofæŒ‘æˆ˜æ—¨åœ¨æ¨è¿›é‡å¤–æ•°æ®åœ¨ä¸¤ä¸ªç›¸äº’å…³è”çš„è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å®ƒåŒ…å«ä¸¤ä¸ªå¹¶è¡Œè½¨é“ï¼šï¼ˆ1ï¼‰ç”¨äºç”Ÿæˆæ¬ºéª—è¯­éŸ³çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆï¼Œï¼ˆ2ï¼‰ç”¨äºæ£€æµ‹æ¬ºéª—è¯­éŸ³çš„æ¬ºéª—ç¨³å¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆSASVï¼‰ã€‚è™½ç„¶ç»„ç»‡è€…åè°ƒè¿™ä¸¤ä¸ªè½¨é“å¹¶å®šä¹‰æ•°æ®åè®®ï¼Œä½†å‚ä¸è€…å°†å…¶è§†ä¸ºç‹¬ç«‹ä¸”ç‹¬ç«‹çš„ä»»åŠ¡ã€‚è¯¥æŒ‘æˆ˜çš„ä¸»è¦ç›®æ ‡æ˜¯ï¼šï¼ˆiï¼‰ä¿ƒè¿›é‡å¤–æ•°æ®åœ¨TTSå’ŒSASVä¸¤è€…ä¸­çš„åº”ç”¨ï¼Œè¶…è¶Šä¼ ç»Ÿçš„å¹²å‡€ä¸”å—æ§çš„æ•°æ®é›†ï¼Œå¹¶è€ƒè™‘çœŸå®åœºæ™¯ï¼›ï¼ˆiiï¼‰é¼“åŠ±æ¬ºéª—ç”Ÿæˆï¼ˆTTSï¼‰å’Œæ¬ºéª—æ£€æµ‹ï¼ˆSASVï¼‰ç¤¾åŒºä¹‹é—´çš„è·¨å­¦ç§‘åˆä½œï¼Œä»è€Œä¿ƒè¿›å¼€å‘æ›´é›†æˆã€æ›´ç¨³å¥å’Œæ›´ç°å®çš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16858v1">PDF</a> ICASSP 2026 challenge</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†â€œWildSpoofæŒ‘æˆ˜â€çš„ç›®æ ‡å’Œæ„æˆï¼Œæ—¨åœ¨æ¨è¿›é‡å¤–æ•°æ®åœ¨ä¸¤é¡¹äº¤ç»‡çš„è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚è¯¥æŒ‘æˆ˜åŒ…æ‹¬ä¸¤ä¸ªå¹¶è¡Œè½¨é“ï¼šï¼ˆ1ï¼‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç”Ÿæˆä¼ªé€ è¯­éŸ³ï¼›ï¼ˆ2ï¼‰æ£€æµ‹ä¼ªé€ è¯­éŸ³çš„é˜²ä¼ªé€ è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆSASVï¼‰ã€‚æŒ‘æˆ˜çš„ä¸»è¦ç›®æ ‡æ˜¯æ¨åŠ¨é‡å¤–æ•°æ®åœ¨TTSå’ŒSASVä¸­çš„åº”ç”¨ï¼Œå¹¶é¼“åŠ±æ¬ºéª—ç”Ÿæˆï¼ˆTTSï¼‰å’Œæ¬ºéª—æ£€æµ‹ï¼ˆSASVï¼‰ç¤¾åŒºä¹‹é—´çš„è·¨å­¦ç§‘åˆä½œï¼Œä»è€Œä¿ƒè¿›å¼€å‘æ›´åŠ é›†æˆã€ç¨³å¥å’Œç°å®çš„ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WildSpoof Challengeæ—¨åœ¨æ¨è¿›é‡å¤–æ•°æ®åœ¨ä¸¤é¡¹äº¤ç»‡çš„è¯­éŸ³å¤„ç†ä»»åŠ¡çš„åº”ç”¨ã€‚</li>
<li>æŒ‘æˆ˜åŒ…æ‹¬ä¸¤ä¸ªå¹¶è¡Œè½¨é“ï¼šæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä»¥åŠæ£€æµ‹ä¼ªé€ è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>è¯¥æŒ‘æˆ˜çš„ä¸»è¦ç›®æ ‡æ˜¯æ¨åŠ¨é‡å¤–æ•°æ®åœ¨è¯­éŸ³åˆæˆå’Œè¯­éŸ³è¯†åˆ«é¢†åŸŸçš„ä½¿ç”¨ï¼Œå¹¶é¼“åŠ±è·¨å­¦ç§‘åˆä½œã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é‡å¤–æ•°æ®ï¼Œè¯¥æŒ‘æˆ˜å¸Œæœ›æ¨åŠ¨æ›´åŠ é›†æˆã€ç¨³å¥å’Œç°å®çš„ç³»ç»Ÿå¼€å‘ã€‚</li>
<li>è¯¥æŒ‘æˆ˜æ—¨åœ¨è€ƒè™‘çœŸå®åœºæ™¯ä¸­çš„æ•°æ®åè®®åº”ç”¨ï¼Œè€Œéä»…ä»…ä¾èµ–äºä¼ ç»Ÿçš„æ¸…æ´å’Œæ§åˆ¶æ•°æ®é›†ã€‚</li>
<li>åœ¨è¿™ä¸ªæŒ‘æˆ˜ä¸­ï¼Œå°½ç®¡æœ‰ä¸¤ä¸ªä¸åŒçš„è½¨é“ï¼Œä½†ç»„ç»‡è€…åœ¨åè°ƒå’Œå®šä¹‰æ•°æ®åè®®æ–¹é¢æ˜¯å‘æŒ¥æ ¸å¿ƒä½œç”¨çš„ã€‚å‚ä¸è€…åˆ™å°†è¿™ä¸¤ä¸ªè½¨é“è§†ä¸ºç‹¬ç«‹çš„ä»»åŠ¡æ¥å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ca9f1dd1342c9c63c5838587feb8385.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-3b4e0778b9947ef17bc786b42631388e~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910582&auth_key=1759910582-0-0-a966047945d8e4605dd89026eae851f1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue"><a href="#OSUM-EChat-Enhancing-End-to-End-Empathetic-Spoken-Chatbot-via-Understanding-Driven-Spoken-Dialogue" class="headerlink" title="OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue"></a>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via   Understanding-Driven Spoken Dialogue</h2><p><strong>Authors:Xuelong Geng, Qijie Shao, Hongfei Xue, Shuiyuan Wang, Hanke Xie, Zhao Guo, Yi Zhao, Guojian Li, Wenjie Tian, Chengyou Wang, Zhixian Zhao, Kangxiang Xia, Ziyu Zhang, Zhennan Lin, Tianlun Zuo, Mingchen Shao, Yuang Cao, Guobin Ma, Longhao Li, Yuhang Dai, Dehui Gao, Dake Guo, Lei Xie</strong></p>
<p>Empathy is crucial in enabling natural interactions within spoken dialogue systems, allowing machines to recognize and respond appropriately to paralinguistic cues such as age, gender, and emotion. Recent advancements in end-to-end speech language models, which unify speech understanding and generation, provide promising solutions. However, several challenges persist, including an over-reliance on large-scale dialogue datasets, insufficient extraction of paralinguistic cues vital for conveying empathy, and the lack of empathy-specific datasets and evaluation frameworks. To address these issues, we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system designed to enhance empathetic interactions, particularly in resource-limited settings. OSUM-EChat introduces two key innovations: (1) a three-stage understanding-driven spoken dialogue training strategy that extends the capabilities of a large speech understanding model to spoken dialogue tasks, and (2) a linguistic-paralinguistic dual thinking mechanism that integrates paralinguistic understanding through a chain of thought with dialogue generation, enabling the system to produce more empathetic responses. This approach reduces reliance on large-scale dialogue datasets while maintaining high-quality empathetic interactions. Additionally, we introduce the EChat-200K dataset, a rich corpus of empathetic speech-to-speech dialogues, and the EChat-eval benchmark, a comprehensive framework for evaluating the empathetic capabilities of dialogue systems. Experimental results demonstrate that OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic responsiveness, validating its effectiveness. </p>
<blockquote>
<p>åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’ï¼Œå…±æƒ…è‡³å…³é‡è¦ã€‚å®ƒå…è®¸æœºå™¨è¯†åˆ«å¹¶é€‚å½“å›åº”è¯¸å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿç­‰å‰¯è¯­è¨€çº¿ç´¢ã€‚ç«¯åˆ°ç«¯çš„æœ€è¿‘è¿›å±•ä¸ºè¯­è¨€æ¨¡å‹å°†è¯­éŸ³ç†è§£å’Œç”Ÿæˆç»Ÿä¸€èµ·æ¥ï¼Œæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡äºä¾èµ–å¤§è§„æ¨¡çš„å¯¹è¯æ•°æ®é›†ã€æœªèƒ½å……åˆ†æå–å¯¹äºä¼ è¾¾å…±æƒ…è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OSUM-EChatï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå…±æƒ…äº’åŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatæœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç†è§£é©±åŠ¨å¼å¯¹è¯è®­ç»ƒç­–ç•¥ï¼Œå®ƒæ‰©å±•äº†å¤§å‹è¯­éŸ³ç†è§£æ¨¡å‹åœ¨å£è¯­å¯¹è¯ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä¸€ç§è¯­è¨€å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ï¼Œå®ƒå°†å‰¯è¯­è¨€ç†è§£èå…¥ä¸€ç³»åˆ—æ€ç»´ä¸å¯¹è¯ç”Ÿæˆä¹‹ä¸­ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å›åº”ã€‚è¿™ç§æ–¹æ³•é™ä½äº†å¯¹å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº’åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†EChat-200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸°å¯Œçš„å…±æƒ…è¯­éŸ³å¯¹è¯è¯­æ–™åº“ï¼Œä»¥åŠEChat-evalåŸºå‡†æµ‹è¯•ï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°å¯¹è¯ç³»ç»Ÿå…±æƒ…èƒ½åŠ›çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09600v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæœºå™¨é€šè¿‡è¯†åˆ«å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿç­‰å‰¯è¯­è¨€çº¿ç´¢æ¥ä½œå‡ºæ°å½“å›åº”çš„èƒ½åŠ›ã€‚è¿‘æœŸç«¯åˆ°ç«¯çš„è¯­éŸ³è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¸ºç»Ÿä¸€è¯­éŸ³ç†è§£å’Œç”Ÿæˆæä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚è¿‡äºä¾èµ–å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†ã€æœªèƒ½å……åˆ†æå–å¯¹è¡¨è¾¾å…±æƒ…è‡³å…³é‡è¦çš„å‰¯è¯­è¨€çº¿ç´¢ä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†OSUM-EChatè¿™ä¸€å¼€æºçš„ç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºå…±æƒ…äº’åŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚OSUM-EChatå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯ä»¥ç†è§£ä¸ºä¸»å¯¼çš„å£è¯­å¯¹è¯ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ‰©å±•äº†å¤§è§„æ¨¡è¯­éŸ³ç†è§£æ¨¡å‹åœ¨å£è¯­å¯¹è¯ä»»åŠ¡ä¸­çš„åº”ç”¨èƒ½åŠ›ï¼›äºŒæ˜¯è¯­è¨€-å‰¯è¯­è¨€åŒé‡æ€è€ƒæœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†å‰¯è¯­è¨€ç†è§£ä¸æ€ç»´é“¾ç›¸ç»“åˆï¼Œæ¨åŠ¨å¯¹è¯ç”Ÿæˆï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿäº§ç”Ÿæ›´å…·å…±æƒ…çš„å›åº”ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†çš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„å…±æƒ…äº’åŠ¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†EChat-200Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸°å¯Œçš„å…±æƒ…æ€§è¯­éŸ³å¯¹è¯è¯­æ–™åº“ï¼Œä»¥åŠEChat-evalåŸºå‡†æµ‹è¯•ï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°å¯¹è¯ç³»ç»Ÿå…±æƒ…èƒ½åŠ›çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSUM-EChatåœ¨å…±æƒ…å“åº”æ–¹é¢ä¼˜äºç«¯åˆ°ç«¯çš„å£è¯­å¯¹è¯æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…±æƒ…åœ¨å£è¯­å¯¹è¯ç³»ç»Ÿä¸­å®ç°è‡ªç„¶äº¤äº’è‡³å…³é‡è¦ï¼Œæœºå™¨éœ€è¦è¯†åˆ«å¹¶å“åº”å‰¯è¯­è¨€çº¿ç´¢å¦‚å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸ºç»Ÿä¸€è¯­éŸ³ç†è§£å’Œç”Ÿæˆæä¾›äº†å‰æ™¯ã€‚</li>
<li>å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ä¾èµ–å¤§è§„æ¨¡å¯¹è¯æ•°æ®é›†ã€æœªèƒ½å……åˆ†æå–å‰¯è¯­è¨€çº¿ç´¢ä»¥åŠç¼ºä¹é’ˆå¯¹å…±æƒ…çš„ç‰¹å®šæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>OSUM-EChatæ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå…±æƒ…äº’åŠ¨çš„å¼€æºç«¯åˆ°ç«¯å£è¯­å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>OSUM-EChatå¼•å…¥äº†ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥å’ŒåŒé‡æ€è€ƒæœºåˆ¶ï¼Œä»¥æé«˜å…±æƒ…äº’åŠ¨çš„è´¨é‡å’Œå‡å°‘å¯¹æ•°æ®é›†çš„ä¾èµ–ã€‚</li>
<li>EChat-200Kæ•°æ®é›†æ˜¯ä¸€ä¸ªä¸°å¯Œçš„å…±æƒ…æ€§è¯­éŸ³å¯¹è¯è¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d386e2fc84fcc3e17c4d8e633ad8089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74141b369f2361d8250ad455babb0565.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-e8736de6347a296ecdc6e30496e4b354~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910604&auth_key=1759910604-0-0-4acce2de7c2c876605e974d046b2a3c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4bd9a04fbafc7b2d741a1d510deeaf94~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910610&auth_key=1759910610-0-0-d526e795a9f0f2d163df58731e84e5f5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1532fe434229d1e26038d8d42cfc6836~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910617&auth_key=1759910617-0-0-6ca12e8d267c63577daa5971afc339cd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-28c3b03132ebd4dd671c6c43f74be212.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Conan-A-Chunkwise-Online-Network-for-Zero-Shot-Adaptive-Voice-Conversion"><a href="#Conan-A-Chunkwise-Online-Network-for-Zero-Shot-Adaptive-Voice-Conversion" class="headerlink" title="Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice   Conversion"></a>Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice   Conversion</h2><p><strong>Authors:Yu Zhang, Baotong Tian, Zhiyao Duan</strong></p>
<p>Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics. To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the voice timbre and styles of reference speech. Conan comprises three core components: 1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics. Audio samples can be found at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/ConanDemo">https://aaronz345.github.io/ConanDemo</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬åœ¨çº¿è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰åœ¨å®æ—¶é€šä¿¡å’Œå¨±ä¹æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„VCæ¨¡å‹åœ¨å®æ—¶çº¦æŸä¸‹å¾ˆéš¾ä¿æŒè¯­ä¹‰ä¿çœŸï¼Œå®ç°è‡ªç„¶è½¬æ¢çš„è¯­éŸ³ï¼Œå¹¶æœ‰æ•ˆåœ°é€‚åº”æœªè§è¿‡çš„è¯´è¯äººç‰¹å¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Conanï¼Œä¸€ç§åˆ†å—çš„åœ¨çº¿é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œå®ƒèƒ½ä¿æŒæºå†…å®¹çš„åŒæ—¶åŒ¹é…å‚è€ƒè¯­éŸ³çš„éŸ³è‰²å’Œé£æ ¼ã€‚ConanåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1ï¼‰æµå†…å®¹æå–å™¨ï¼Œå®ƒåˆ©ç”¨Emformerè¿›è¡Œä½å»¶è¿Ÿæµå†…å®¹ç¼–ç ï¼›2ï¼‰è‡ªé€‚åº”é£æ ¼ç¼–ç å™¨ï¼Œä»å‚è€ƒè¯­éŸ³ä¸­æå–ç²¾ç»†çš„é£æ ¼ç‰¹å¾ï¼Œä»¥å¢å¼ºé£æ ¼é€‚åº”ï¼›3ï¼‰å› æœæ··æ´—vocoderï¼Œå®ƒä½¿ç”¨åƒç´ æ··æ´—æœºåˆ¶å®ç°äº†å…¨å› æœHiFiGANã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒConanåœ¨ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://aaronz345.github.io/ConanDemo%E6%89%BE%E5%88%B0%E3%80%82">https://aaronz345.github.io/ConanDemoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14534v4">PDF</a> Accepted by ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>é›¶æ ·æœ¬åœ¨çº¿è¯­éŸ³è½¬æ¢æŠ€æœ¯ï¼ˆVCï¼‰åœ¨å®æ—¶é€šä¿¡å’Œå¨±ä¹é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰VCæ¨¡å‹éš¾ä»¥åœ¨å®æ—¶çº¦æŸä¸‹ä¿æŒè¯­ä¹‰ä¿çœŸã€å®ç°è‡ªç„¶çš„å£°éŸ³è½¬æ¢ä»¥åŠæœ‰æ•ˆé€‚åº”æœªè§è¿‡çš„è¯´è¯äººç‰¹å¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºConanæ¨¡å‹ï¼Œä¸€ç§åˆ†å—åœ¨çº¿é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™æºå†…å®¹çš„åŒæ—¶åŒ¹é…å‚è€ƒè¯­éŸ³çš„éŸ³è‰²å’Œé£æ ¼ã€‚ConanåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1) æµå†…å®¹æå–å™¨ï¼Œåˆ©ç”¨Emformerè¿›è¡Œä½å»¶è¿Ÿæµå†…å®¹ç¼–ç ï¼›2) é€‚åº”æ€§é£æ ¼ç¼–ç å™¨ï¼Œä»å‚è€ƒè¯­éŸ³ä¸­æå–ç²¾ç»†çš„é£æ ¼ç‰¹å¾ä»¥å¢å¼ºé£æ ¼é€‚åº”ï¼›3) å› æœæ´—ç‰ŒVocoderï¼Œé‡‡ç”¨åƒç´ æ´—ç‰Œæœºåˆ¶å®ç°å…¨å› æœHiFiGANã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒConanåœ¨ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é›¶æ ·æœ¬åœ¨çº¿è¯­éŸ³è½¬æ¢æŠ€æœ¯åœ¨å®æ—¶é€šä¿¡å’Œå¨±ä¹ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>å½“å‰VCæ¨¡å‹é¢ä¸´ä¿æŒè¯­ä¹‰ä¿çœŸã€è‡ªç„¶å£°éŸ³è½¬æ¢å’Œé€‚åº”æœªè§è¯´è¯äººç‰¹å¾çš„æŒ‘æˆ˜ã€‚</li>
<li>Conanæ¨¡å‹æ˜¯ä¸€ç§åˆ†å—åœ¨çº¿é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œèƒ½ä¿ç•™æºå†…å®¹å¹¶åŒ¹é…å‚è€ƒè¯­éŸ³çš„éŸ³è‰²å’Œé£æ ¼ã€‚</li>
<li>ConanåŒ…æ‹¬æµå†…å®¹æå–å™¨ã€é€‚åº”æ€§é£æ ¼ç¼–ç å™¨å’Œå› æœæ´—ç‰ŒVocoderä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>æµå†…å®¹æå–å™¨åˆ©ç”¨Emformerè¿›è¡Œä½å»¶è¿Ÿæµå†…å®¹ç¼–ç ã€‚</li>
<li>é€‚åº”æ€§é£æ ¼ç¼–ç å™¨ä»å‚è€ƒè¯­éŸ³ä¸­æå–ç²¾ç»†é£æ ¼ç‰¹å¾ï¼Œå¢å¼ºé£æ ¼é€‚åº”ã€‚</li>
<li>å› æœæ´—ç‰ŒVocoderé‡‡ç”¨åƒç´ æ´—ç‰Œæœºåˆ¶å®ç°å…¨å› æœHiFiGANã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4d8151b14dcf2584bc1a2528fd0976d6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910632&auth_key=1759910632-0-0-ca06a0f2631bab43dd39a042b7e3176b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c8c6ea524c0363526c663c11de2c1c6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910639&auth_key=1759910639-0-0-88ceed831985a4a36b5bce0bf3ad83ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0bc54b1a69379010035b9264ea30507a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910646&auth_key=1759910646-0-0-522ff6105a217ac278906a72ea700d2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-53882a2381b272611a58d05b5271b05b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910652&auth_key=1759910652-0-0-413361ac788cfd64e34a800ae450034d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-787a8a1db1fdb59535a983d0204c5624~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910659&auth_key=1759910659-0-0-1dd7cc1755ae0107ca4cfc1bf5d2ca37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-5094fbb102e8a6ef4c64230d7334e976.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech"><a href="#IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech" class="headerlink" title="IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech"></a>IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h2><p><strong>Authors:Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, Jingchen Shu</strong></p>
<p>Existing autoregressive large-scale text-to-speech (TTS) models have advantages in speech naturalness, but their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This becomes a significant limitation in applications requiring strict audio-visual synchronization, such as video dubbing. This paper introduces IndexTTS2, which proposes a novel, general, and autoregressive model-friendly method for speech duration control. The method supports two generation modes: one explicitly specifies the number of generated tokens to precisely control speech duration; the other freely generates speech in an autoregressive manner without specifying the number of tokens, while faithfully reproducing the prosodic features of the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control over timbre and emotion. In the zero-shot setting, the model can accurately reconstruct the target timbre (from the timbre prompt) while perfectly reproducing the specified emotional tone (from the style prompt). To enhance speech clarity in highly emotional expressions, we incorporate GPT latent representations and design a novel three-stage training paradigm to improve the stability of the generated speech. Additionally, to lower the barrier for emotional control, we designed a soft instruction mechanism based on text descriptions by fine-tuning Qwen3, effectively guiding the generation of speech with the desired emotional orientation. Finally, experimental results on multiple datasets show that IndexTTS2 outperforms state-of-the-art zero-shot TTS models in terms of word error rate, speaker similarity, and emotional fidelity. Audio samples are available at: <a target="_blank" rel="noopener" href="https://index-tts.github.io/index-tts2.github.io/">https://index-tts.github.io/index-tts2.github.io/</a> </p>
<blockquote>
<p>ç°æœ‰çš„è‡ªå›å½’å¤§è§„æ¨¡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹åœ¨è¯­éŸ³è‡ªç„¶åº¦æ–¹é¢å…·ä¼˜åŠ¿ï¼Œä½†å…¶é€ä»¤ç‰Œç”Ÿæˆæœºåˆ¶ä½¿å¾—éš¾ä»¥ç²¾ç¡®æ§åˆ¶åˆæˆè¯­éŸ³çš„æŒç»­æ—¶é—´ã€‚è¿™åœ¨éœ€è¦ä¸¥æ ¼éŸ³è§†é¢‘åŒæ­¥çš„åº”ç”¨ä¸­æˆä¸ºä¸€å¤§å±€é™ï¼Œä¾‹å¦‚è§†é¢‘é…éŸ³ã€‚æœ¬æ–‡ä»‹ç»äº†IndexTTS2ï¼Œå®ƒæå‡ºäº†ä¸€ç§æ–°é¢–ã€é€šç”¨ã€é€‚ç”¨äºè‡ªå›å½’æ¨¡å‹çš„è¯­éŸ³æŒç»­æ—¶é—´æ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šä¸€ç§æ˜ç¡®æŒ‡å®šç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ä»¥ç²¾ç¡®æ§åˆ¶è¯­éŸ³æŒç»­æ—¶é—´ï¼›å¦ä¸€ç§ä»¥è‡ªå›å½’çš„æ–¹å¼è‡ªç”±ç”Ÿæˆè¯­éŸ³ï¼Œæ— éœ€æŒ‡å®šä»¤ç‰Œæ•°é‡ï¼ŒåŒæ—¶å¿ å®å†ç°è¾“å…¥æç¤ºçš„éŸµå¾‹ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒIndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œå®ç°å¯¹éŸ³è‰²å’Œæƒ…æ„Ÿçš„ç‹¬ç«‹æ§åˆ¶ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œæ¨¡å‹å¯ä»¥å‡†ç¡®é‡å»ºç›®æ ‡éŸ³è‰²ï¼ˆæ¥è‡ªéŸ³è‰²æç¤ºï¼‰ï¼ŒåŒæ—¶å®Œç¾å†ç°æŒ‡å®šçš„æƒ…æ„ŸåŸºè°ƒï¼ˆæ¥è‡ªé£æ ¼æç¤ºï¼‰ã€‚ä¸ºæé«˜é«˜åº¦æƒ…æ„Ÿè¡¨è¾¾ä¸­çš„è¯­éŸ³æ¸…æ™°åº¦ï¼Œæˆ‘ä»¬èå…¥äº†GPTæ½œåœ¨è¡¨å¾ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°å‹ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œä»¥æé«˜ç”Ÿæˆè¯­éŸ³çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†é™ä½æƒ…æ„Ÿæ§åˆ¶çš„éšœç¢ï¼Œæˆ‘ä»¬åŸºäºæ–‡æœ¬æè¿°è®¾è®¡äº†æŸ”å’Œçš„æŒ‡ä»¤æœºåˆ¶ï¼Œé€šè¿‡å¾®è°ƒQwen3ï¼Œæœ‰æ•ˆåœ°å¼•å¯¼è¯­éŸ³æœç€æœŸæœ›çš„æƒ…æ„Ÿæ–¹å‘ç”Ÿæˆã€‚æœ€åï¼Œå¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨è¯é”™è¯¯ç‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦æ–¹é¢ä¼˜äºé›¶æ ·æœ¬TTSæ¨¡å‹ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://index-tts.github.io/index-tts2.github.io/]%E6%9F%A5%E7%9C%8B%E3%80%82">https://index-tts.github.io/index-tts2.github.io/]æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21619v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹IndexTTS2ï¼Œè¯¥æ¨¡å‹å…·æœ‰å¯¹è¯­éŸ³æ—¶é•¿çš„ç²¾ç¡®æ§åˆ¶èƒ½åŠ›ï¼Œé€‚ç”¨äºéœ€è¦ä¸¥æ ¼éŸ³è§†é¢‘åŒæ­¥çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚è§†é¢‘é…éŸ³ã€‚IndexTTS2æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šä¸€ç§æ˜¯é€šè¿‡æŒ‡å®šç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡æ¥ç²¾ç¡®æ§åˆ¶è¯­éŸ³æ—¶é•¿ï¼›å¦ä¸€ç§æ˜¯ä»¥è‡ªå›å½’çš„æ–¹å¼è‡ªç”±ç”Ÿæˆè¯­éŸ³ï¼ŒåŒæ—¶å¿ å®ä¿ç•™è¾“å…¥æç¤ºçš„éŸµå¾‹ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒIndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ§åˆ¶éŸ³è‰²å’Œæƒ…æ„Ÿã€‚é€šè¿‡èå…¥GPTæ½œåœ¨è¡¨å¾å¹¶é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œæé«˜äº†åœ¨é«˜åº¦æƒ…æ„Ÿè¡¨è¾¾æ—¶çš„è¯­éŸ³æ¸…æ™°åº¦ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ–‡æœ¬æè¿°çš„è½¯æŒ‡ä»¤æœºåˆ¶ï¼Œé€šè¿‡å¾®è°ƒQwen3ï¼Œæœ‰æ•ˆå¼•å¯¼ç”Ÿæˆå…·æœ‰æ‰€éœ€æƒ…æ„Ÿå€¾å‘çš„è¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨è¯é”™è¯¯ç‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦ç­‰æ–¹é¢ä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>IndexTTS2æ¨¡å‹å¼•å…¥äº†ä¸€ç§æ–°çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶åˆæˆè¯­éŸ³çš„æ—¶é•¿ã€‚</li>
<li>æ¨¡å‹æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼Œä¸€ç§å¯æŒ‡å®šç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡æ¥æ§åˆ¶è¯­éŸ³æ—¶é•¿ï¼Œå¦ä¸€ç§åˆ™ä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆè¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™è¾“å…¥æç¤ºçš„éŸµå¾‹ç‰¹å¾ã€‚</li>
<li>IndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œå…è®¸ç‹¬ç«‹æ§åˆ¶éŸ³è‰²å’Œæƒ…æ„Ÿã€‚</li>
<li>é€šè¿‡èå…¥GPTæ½œåœ¨è¡¨å¾å’Œä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼æé«˜è¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>é‡‡ç”¨åŸºäºæ–‡æœ¬æè¿°çš„è½¯æŒ‡ä»¤æœºåˆ¶ï¼Œé€šè¿‡å¾®è°ƒQwen3æœ‰æ•ˆå¼•å¯¼ç”Ÿæˆå…·æœ‰æ‰€éœ€æƒ…æ„Ÿå€¾å‘çš„è¯­éŸ³ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨è¯é”™è¯¯ç‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-143a226c48b39cf9c70506b021559f71~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910673&auth_key=1759910673-0-0-1e3d1e61c731121d0456119cb540e55e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b400bb0c5bd3e546ea1942b196a1c31b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910680&auth_key=1759910680-0-0-a1436f981ea704bae0469143047f0de5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4daaad6462edf136227e6ff210a71a6d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910687&auth_key=1759910687-0-0-bdb6e39cf4ff96c9f427e8ebb51532f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb462602d929561088e608ef54d54295~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910694&auth_key=1759910694-0-0-7e413e55b1cc4b2bf95bb4bc9068c3d6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e2e66d6a1c2fdbaf53b74ea127d1d555~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910700&auth_key=1759910700-0-0-a8a2777244be8fd36d60ead0f3d6527b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CAARMA-Class-Augmentation-with-Adversarial-Mixup-Regularization"><a href="#CAARMA-Class-Augmentation-with-Adversarial-Mixup-Regularization" class="headerlink" title="CAARMA: Class Augmentation with Adversarial Mixup Regularization"></a>CAARMA: Class Augmentation with Adversarial Mixup Regularization</h2><p><strong>Authors:Massa Baali, Xiang Li, Hao Chen, Syed Abdul Hannan, Rita Singh, Bhiksha Raj</strong></p>
<p>Speaker verification is a typical zero-shot learning task, where inference of unseen classes is performed by comparing embeddings of test instances to known examples. The models performing inference must hence naturally generate embeddings that cluster same-class instances compactly, while maintaining separation across classes. In order to learn to do so, they are typically trained on a large number of classes (speakers), often using specialized losses. However real-world speaker datasets often lack the class diversity needed to effectively learn this in a generalizable manner. We introduce CAARMA, a class augmentation framework that addresses this problem by generating synthetic classes through data mixing in the embedding space, expanding the number of training classes. To ensure the authenticity of the synthetic classes we adopt a novel adversarial refinement mechanism that minimizes categorical distinctions between synthetic and real classes. We evaluate CAARMA on multiple speaker verification tasks, as well as other representative zero-shot comparison-based speech analysis tasks and obtain consistent improvements: our framework demonstrates a significant improvement of 8% over all baseline models. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/massabaali7/CAARMA/">https://github.com/massabaali7/CAARMA/</a> </p>
<blockquote>
<p>è¯´è¯äººéªŒè¯æ˜¯ä¸€ä¸ªå…¸å‹çš„é›¶æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼Œé€šè¿‡æ¯”è¾ƒæµ‹è¯•å®ä¾‹çš„åµŒå…¥å’Œå·²çŸ¥æ ·æœ¬çš„åµŒå…¥æ¥æ¨æ–­æœªè§è¿‡çš„ç±»åˆ«ã€‚å› æ­¤ï¼Œæ‰§è¡Œæ¨æ–­çš„æ¨¡å‹å¿…é¡»è‡ªç„¶åœ°ç”ŸæˆåµŒå…¥ï¼Œè¿™äº›åµŒå…¥èƒ½å¤Ÿç´§å¯†åœ°èšç±»åŒç±»å®ä¾‹ï¼ŒåŒæ—¶ä¿æŒä¸åŒç±»åˆ«ä¹‹é—´çš„åˆ†ç¦»ã€‚ä¸ºäº†å­¦ä¹ å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå®ƒä»¬é€šå¸¸ä¼šåœ¨å¤§é‡ç±»åˆ«ï¼ˆè¯´è¯äººï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç»å¸¸ä½¿ç”¨ä¸“é—¨çš„æŸå¤±å‡½æ•°ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„è¯´è¯äººæ•°æ®é›†é€šå¸¸ç¼ºä¹ä»¥å¯æ³›åŒ–çš„æ–¹å¼æœ‰æ•ˆå­¦ä¹ æ‰€éœ€çš„ç±»åˆ«å¤šæ ·æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†CAARMAï¼Œè¿™æ˜¯ä¸€ä¸ªç±»å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ•°æ®æ··åˆç”Ÿæˆåˆæˆç±»æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œæ‰©å¤§è®­ç»ƒç±»åˆ«çš„æ•°é‡ã€‚ä¸ºäº†ç¡®ä¿åˆæˆç±»çš„çœŸå®æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„å¯¹æŠ—ç»†åŒ–æœºåˆ¶ï¼Œæœ€å°åŒ–åˆæˆç±»å’ŒçœŸå®ç±»ä¹‹é—´çš„ç±»åˆ«å·®å¼‚ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè¯´è¯äººéªŒè¯ä»»åŠ¡ä»¥åŠå…¶ä»–å…·æœ‰ä»£è¡¨æ€§çš„åŸºäºé›¶æ ·æœ¬æ¯”è¾ƒçš„è¯­éŸ³åˆ†æä»»åŠ¡ä¸Šè¯„ä¼°äº†CAARMAï¼Œå¹¶è·å¾—äº†æŒç»­æ€§çš„æ”¹è¿›ï¼šæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰åŸºçº¿æ¨¡å‹ä¸Šå®ç°äº†8%çš„æ˜¾è‘—æ”¹è¿›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/massabaali7/CAARMA/">https://github.com/massabaali7/CAARMA/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16718v2">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong>ï¼š<br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºCAARMAçš„ç±»å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè§£å†³ç°å®è¯´è¯äººæ•°æ®é›†ä¸­ç±»å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ•°æ®æ··åˆç”Ÿæˆåˆæˆç±»ï¼Œæ‰©å¤§è®­ç»ƒç±»çš„æ•°é‡ï¼Œå¹¶é‡‡ç”¨æ–°çš„å¯¹æŠ—ç»†åŒ–æœºåˆ¶ç¡®ä¿åˆæˆç±»çš„çœŸå®æ€§ï¼Œä»è€Œæœ€å°åŒ–åˆæˆç±»å’ŒçœŸå®ç±»ä¹‹é—´çš„ç±»åˆ«å·®å¼‚ã€‚åœ¨å¤šä¸ªè¯´è¯äººéªŒè¯ä»»åŠ¡ä»¥åŠå…¶ä»–åŸºäºé›¶æ ·æœ¬æ¯”è¾ƒçš„è¯­éŸ³åˆ†æä»»åŠ¡ä¸Šï¼ŒCAARMAæ¡†æ¶è¡¨ç°å‡ºäº†ä¸€è‡´çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå…¶æ”¹è¿›å¹…åº¦è¾¾åˆ°äº†æ˜¾è‘—çš„8%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬ä»‹ç»äº†CAARMAæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°å®è¯´è¯äººæ•°æ®é›†ä¸­ç±»å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>CAARMAæ¡†æ¶é€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ•°æ®æ··åˆç”Ÿæˆåˆæˆç±»ï¼Œæ‰©å¤§è®­ç»ƒç±»çš„æ•°é‡ã€‚</li>
<li>é‡‡ç”¨æ–°çš„å¯¹æŠ—ç»†åŒ–æœºåˆ¶ç¡®ä¿åˆæˆç±»çš„çœŸå®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½æœ€å°åŒ–åˆæˆç±»å’ŒçœŸå®ç±»ä¹‹é—´çš„ç±»åˆ«å·®å¼‚ã€‚</li>
<li>åœ¨å¤šä¸ªè¯´è¯äººéªŒè¯ä»»åŠ¡ä¸Šï¼ŒCAARMAæ¡†æ¶è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰8%çš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>CAARMAæ¡†æ¶ä¹Ÿé€‚ç”¨äºå…¶ä»–åŸºäºé›¶æ ·æœ¬æ¯”è¾ƒçš„è¯­éŸ³åˆ†æä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0ba0dc22cd5f42e6d2c5215b38bfcfff~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910707&auth_key=1759910707-0-0-47150d73048bb5863ddd741714130056&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-ed5869c24dad3c8eba663d1a3ac909f3.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c55e60ed2c5c239c682ea91f7a04ce82~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910721&auth_key=1759910721-0-0-8abd98afa1bef3d969464f9f96ae70c0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NeuroAMP-A-Novel-End-to-end-General-Purpose-Deep-Neural-Amplifier-for-Personalized-Hearing-Aids"><a href="#NeuroAMP-A-Novel-End-to-end-General-Purpose-Deep-Neural-Amplifier-for-Personalized-Hearing-Aids" class="headerlink" title="NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for   Personalized Hearing Aids"></a>NeuroAMP: A Novel End-to-end General Purpose Deep Neural Amplifier for   Personalized Hearing Aids</h2><p><strong>Authors:Shafique Ahmed, Ryandhimas E. Zezario, Hui-Guan Yuan, Amir Hussain, Hsin-Min Wang, Wei-Ho Chung, Yu Tsao</strong></p>
<p>The prevalence of hearing aids is increasing. However, optimizing the amplification processes of hearing aids remains challenging due to the complexity of integrating multiple modular components in traditional methods. To address this challenge, we present NeuroAMP, a novel deep neural network designed for end-to-end, personalized amplification in hearing aids. NeuroAMP leverages both spectral features and the listenerâ€™s audiogram as inputs, and we investigate four architectures: Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Convolutional Recurrent Neural Network (CRNN), and Transformer. We also introduce Denoising NeuroAMP, an extension that integrates noise reduction along with amplification capabilities for improved performance in real-world scenarios. To enhance generalization, a comprehensive data augmentation strategy was employed during training on diverse speech (TIMIT and TMHINT) and music (Cadenza Challenge MUSIC) datasets. Evaluation using the Hearing Aid Speech Perception Index (HASPI), Hearing Aid Speech Quality Index (HASQI), and Hearing Aid Audio Quality Index (HAAQI) demonstrates that the Transformer architecture within NeuroAMP achieves the best performance, with SRCC scores of 0.9927 (HASQI) and 0.9905 (HASPI) on TIMIT, and 0.9738 (HAAQI) on the Cadenza Challenge MUSIC dataset. Notably, our data augmentation strategy maintains high performance on unseen datasets (e.g., VCTK, MUSDB18-HQ). Furthermore, Denoising NeuroAMP outperforms both the conventional NAL-R+WDRC approach and a two-stage baseline on the VoiceBank+DEMAND dataset, achieving a 10% improvement in both HASPI (0.90) and HASQI (0.59) scores. These results highlight the potential of NeuroAMP and Denoising NeuroAMP to deliver notable improvements in personalized hearing aid amplification. </p>
<blockquote>
<p>åŠ©å¬å™¨çš„æ™®åŠç‡æ­£åœ¨ä¸æ–­æé«˜ã€‚ç„¶è€Œï¼Œç”±äºä¼ ç»Ÿæ–¹æ³•ä¸­æ•´åˆå¤šä¸ªæ¨¡å—åŒ–ç»„ä»¶çš„å¤æ‚æ€§ï¼Œä¼˜åŒ–åŠ©å¬å™¨çš„æ”¾å¤§è¿‡ç¨‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeuroAMPï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºåŠ©å¬å™¨ç«¯åˆ°ç«¯ä¸ªæ€§åŒ–æ”¾å¤§è€Œè®¾è®¡çš„æ–°å‹æ·±åº¦ç¥ç»ç½‘ç»œã€‚NeuroAMPåˆ©ç”¨é¢‘è°±ç‰¹å¾å’Œå¬è€…çš„å¬åŠ›å›¾ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å››ç§æ¶æ„ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ã€å·ç§¯å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰å’ŒTransformerã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Denoising NeuroAMPï¼Œå®ƒæ˜¯NeuroAMPçš„æ‰©å±•ï¼Œé›†æˆäº†é™å™ªåŠŸèƒ½ï¼Œä»¥æé«˜ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´é‡‡ç”¨äº†å…¨é¢çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè®­ç»ƒæ•°æ®é›†åŒ…æ‹¬å„ç§è¯­éŸ³ï¼ˆTIMITå’ŒTMHINTï¼‰å’ŒéŸ³ä¹ï¼ˆCadenza Challenge MUSICï¼‰æ•°æ®é›†ã€‚é€šè¿‡å¬åŠ›è¾…åŠ©è®¾å¤‡è¯­éŸ³æ„ŸçŸ¥æŒ‡æ•°ï¼ˆHASPIï¼‰ã€å¬åŠ›è¾…åŠ©è®¾å¤‡è¯­éŸ³è´¨é‡æŒ‡æ•°ï¼ˆHASQIï¼‰å’Œå¬åŠ›è¾…åŠ©è®¾å¤‡éŸ³é¢‘è´¨é‡æŒ‡æ•°ï¼ˆHAAQIï¼‰çš„è¯„ä¼°è¡¨æ˜ï¼ŒNeuroAMPä¸­çš„Transformeræ¶æ„è¡¨ç°æœ€ä½³ï¼ŒTIMITä¸Šçš„HASQIå’ŒHASPIçš„SRCCåˆ†æ•°åˆ†åˆ«ä¸º0.9927å’Œ0.9905ï¼ŒCadenza Challenge MUSICæ•°æ®é›†ä¸Šçš„HAAQIåˆ†æ•°ä¸º0.9738ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®å¢å¼ºç­–ç•¥åœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šï¼ˆä¾‹å¦‚VCTKã€MUSDB18-HQï¼‰ä¹Ÿä¿æŒäº†é«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDenoising NeuroAMPåœ¨VoiceBank+DEMANDæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„NAL-R+WDRCæ–¹æ³•å’Œä¸¤é˜¶æ®µåŸºçº¿æ–¹æ³•ï¼ŒHASPIå’ŒHASQIå¾—åˆ†å‡æé«˜äº†10%ã€‚è¿™äº›ç»“æœçªå‡ºäº†NeuroAMPå’ŒDenoising NeuroAMPåœ¨ä¸ªæ€§åŒ–åŠ©å¬å™¨æ”¾å¤§æ–¹é¢çš„æ½œåŠ›ï¼Œæœ‰æœ›å¸¦æ¥æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10822v2">PDF</a> Accepted for publication in IEEE Transactions on Artificial   Intelligence</p>
<p><strong>Summary</strong><br>     éšç€åŠ©å¬å™¨æ™®åŠç‡çš„æé«˜ï¼Œä¼˜åŒ–å…¶æ”¾å¤§è¿‡ç¨‹é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†NeuroAMPè¿™ä¸€æ–°å‹çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç”¨äºå®ç°ç«¯åˆ°ç«¯çš„ä¸ªæ€§åŒ–åŠ©å¬å™¨æ”¾å¤§ã€‚NeuroAMPç»“åˆäº†é¢‘è°±ç‰¹å¾å’Œå¬è€…çš„å¬åŠ›å›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç ”ç©¶äº†å››ç§æ¶æ„ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†é›†æˆé™å™ªåŠŸèƒ½çš„Denoising NeuroAMPï¼Œä»¥æé«˜çœŸå®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚é€šè¿‡æ•°æ®å¢å¼ºç­–ç•¥åŠå¤šç§æ•°æ®é›†çš„è®­ç»ƒï¼Œè¯„ä¼°ç»“æœè¡¨æ˜Transformeræ¶æ„çš„NeuroAMPè¡¨ç°æœ€ä½³ã€‚Denoising NeuroAMPç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå±•ç°å‡ºä¸ªæ€§åŒ–åŠ©å¬å™¨æ”¾å¤§çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ©å¬å™¨æ™®åŠç‡æé«˜ï¼Œä¼˜åŒ–æ”¾å¤§è¿‡ç¨‹æˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>NeuroAMPæ˜¯ä¸€ç§æ–°å‹çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç”¨äºä¸ªæ€§åŒ–åŠ©å¬å™¨æ”¾å¤§ã€‚</li>
<li>NeuroAMPç»“åˆäº†é¢‘è°±ç‰¹å¾å’Œå¬è€…çš„å¬åŠ›å›¾ä½œä¸ºè¾“å…¥ï¼Œç ”ç©¶äº†CNNã€LSTMã€CRNNå’ŒTransformerå››ç§æ¶æ„ã€‚</li>
<li>Denoising NeuroAMPé›†æˆäº†é™å™ªåŠŸèƒ½ï¼Œæé«˜äº†çœŸå®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®å¢å¼ºç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Transformeræ¶æ„çš„NeuroAMPåœ¨å¤šç§è¯„ä¼°ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e5f09e1255994af242a04eb9d16c7ad.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-08935d6d6e3e32997c711a4654914172~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910736&auth_key=1759910736-0-0-d70e6e7c18248a4ef29822f93f427a17&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-41e8ed5d2b476a7c6250986b336726f2.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-146490e426056dbe69c027f2605992ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910749&auth_key=1759910749-0-0-89c9c49a29bd9accabeb0247b002f110&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7b9f236dc672e0f996590d5e6651994c~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910756&auth_key=1759910756-0-0-67ed2ee0d279f33e12cf86eb9f2aca2b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹æ€§å¯¹äºé€‚åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿè‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³å¯¹åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥çš„é‡è§†ç¨‹åº¦æ—¥ç›Šå¢åŠ ï¼Œè¿™å¯èƒ½ä¸ºæ¸¸æˆå’Œè™šæ‹Ÿç°å®æä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå®ƒå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è°ƒæ•´åˆæˆçš„æ¢…å°”é¢‘è°±å›¾ï¼Œä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ‰€æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸å½±å“è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œæ ‡å¿—ç€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> ç´¢å¼•æœ¯è¯­-è¯­éŸ³åˆæˆï¼Œåœºæ™¯æç¤ºï¼Œç©ºé—´æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v4">PDF</a> Accepted by APSIPA ASC2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ–‡æœ¬è½¬è¯­éŸ³åˆæˆæ–¹æ³•â€”â€”å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬è½¬è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥èå…¥åˆæˆç®¡é“ï¼Œæ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚åŒæ—¶æå‡ºä¸€ç§æ··å“åˆ†ç±»ä¸ç²¾ç‚¼æŠ€æœ¯ï¼Œè°ƒæ•´åˆæˆé¢‘è°±å›¾ï¼Œå¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å®ç°äº†é«˜è´¨é‡åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸”ä¸å½±å“è¯­éŸ³çš„è‡ªç„¶æ€§ï¼Œæ ‡å¿—ç€è¯­å¢ƒæ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸçš„é‡è¦è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆçš„é£æ ¼ä¸ç‰¹æ€§æ§åˆ¶å¯¹äºé€‚åº”ç‰¹å®šè¯­å¢ƒå’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿçš„TTSæŠ€æœ¯ä¸»è¦å…³æ³¨è¯­éŸ³çš„æŠ€æœ¯æ€§æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ï¼Œä½†å¿½è§†äº†åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥ã€‚</li>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€TTSæ–¹æ³•â€”â€”å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬è½¬è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚</li>
<li>I2TTSé€šè¿‡å¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºèå…¥è¯­éŸ³åˆæˆè¿‡ç¨‹ï¼Œå®ç°æ›´ç²¾å‡†çš„è¯­å¢ƒæ„ŸçŸ¥ã€‚</li>
<li>I2TTSæå‡ºçš„æ··å“åˆ†ç±»ä¸ç²¾ç‚¼æŠ€æœ¯èƒ½è°ƒæ•´åˆæˆé¢‘è°±å›¾ï¼Œå¢å¼ºè¯­éŸ³çš„æ²‰æµ¸å¼ä½“éªŒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒI2TTSæ¨¡å‹èƒ½åœ¨ä¸æŸå¤±è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡åœºæ™¯å’Œç©ºé—´åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-172a80751488cbda6eb94673dff1d38f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c12fa7c93134b1fe314aa6ec280ebd28.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-b580801026f76554a6484f6287279b9f~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910779&auth_key=1759910779-0-0-52b25ab216e8d577e0a4c17200d01a77&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-13ceced552a9794056e5109a0a06c845.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Neural-Speech-Codec-for-Noise-Robust-Speech-Coding"><a href="#A-Neural-Speech-Codec-for-Noise-Robust-Speech-Coding" class="headerlink" title="A Neural Speech Codec for Noise Robust Speech Coding"></a>A Neural Speech Codec for Noise Robust Speech Coding</h2><p><strong>Authors:Jiayi Huang, Zeyu Yan, Wenbin Jiang, He Wang, Fei Wen</strong></p>
<p>This paper considers the joint compression and enhancement problem for speech signal in the presence of noise. Recently, the SoundStream codec, which relies on end-to-end joint training of an encoder-decoder pair and a residual vector quantizer by a combination of adversarial and reconstruction losses,has shown very promising performance, especially in subjective perception quality. In this work, we provide a theoretical result to show that, to simultaneously achieve low distortion and high perception in the presence of noise, there exist an optimal two-stage optimization procedure for the joint compression and enhancement problem. This procedure firstly optimizes an encoder-decoder pair using only distortion loss and then fixes the encoder to optimize a perceptual decoder using perception loss. Based on this result, we construct a two-stage training framework for joint compression and enhancement of noisy speech signal. Unlike existing training methods which are heuristic, the proposed two-stage training method has a theoretical foundation. Finally, experimental results for various noise and bit-rate conditions are provided. The results demonstrate that a codec trained by the proposed framework can outperform SoundStream and other representative codecs in terms of both objective and subjective evaluation metrics. Code is available at \textit{<a target="_blank" rel="noopener" href="https://github.com/jscscloris/SEStream%7D">https://github.com/jscscloris/SEStream}</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å™ªå£°å­˜åœ¨ä¸‹çš„è¯­éŸ³ä¿¡å·çš„è”åˆå‹ç¼©ä¸å¢å¼ºé—®é¢˜ã€‚æœ€è¿‘ï¼ŒSoundStreamç¼–è§£ç å™¨å‡­å€Ÿå…¶ç«¯åˆ°ç«¯çš„è”åˆè®­ç»ƒæ–¹å¼ï¼ŒåŒ…æ‹¬ç¼–ç å™¨-è§£ç å™¨å¯¹å’Œæ®‹å·®çŸ¢é‡é‡åŒ–å™¨çš„ç»“åˆï¼Œä»¥åŠå¯¹æŠ—å’Œé‡å»ºæŸå¤±çš„ç»„åˆï¼Œè¡¨ç°å‡ºäº†éå¸¸æœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸»è§‚æ„ŸçŸ¥è´¨é‡æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜ï¼Œä¸ºäº†åœ¨å™ªå£°å­˜åœ¨çš„åŒæ—¶å®ç°ä½å¤±çœŸå’Œé«˜æ„ŸçŸ¥ï¼Œé’ˆå¯¹è”åˆå‹ç¼©å’Œå¢å¼ºé—®é¢˜ï¼Œå­˜åœ¨ä¸€ä¸ªæœ€ä¼˜çš„ä¸¤é˜¶æ®µä¼˜åŒ–ç¨‹åºã€‚è¯¥ç¨‹åºé¦–å…ˆä»…ä½¿ç”¨å¤±çœŸæŸå¤±ä¼˜åŒ–ç¼–ç å™¨-è§£ç å™¨å¯¹ï¼Œç„¶åå›ºå®šç¼–ç å™¨ï¼Œä½¿ç”¨æ„ŸçŸ¥æŸå¤±ä¼˜åŒ–æ„ŸçŸ¥è§£ç å™¨ã€‚åŸºäºè¿™ä¸€ç»“æœï¼Œæˆ‘ä»¬æ„å»ºäº†ç”¨äºå™ªå£°è¯­éŸ³ä¿¡å·çš„è”åˆå‹ç¼©å’Œå¢å¼ºçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚ä¸ç°æœ‰çš„å¯å‘å¼è®­ç»ƒæ–¹æ³•ä¸åŒï¼Œæ‰€æå‡ºçš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å…·æœ‰ç†è®ºåŸºç¡€ã€‚æœ€åï¼Œæä¾›äº†å„ç§å™ªå£°å’Œæ¯”ç‰¹ç‡æ¡ä»¶ä¸‹çš„å®éªŒç»“æœã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ‰€ææ¡†æ¶è®­ç»ƒçš„ç¼–è§£ç å™¨åœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä»·æŒ‡æ ‡ä¸Šå‡ä¼˜äºSoundStreamå’Œå…¶ä»–ä»£è¡¨æ€§ç¼–è§£ç å™¨ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jscscloris/SEStream%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jscscloris/SEStreamä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04132v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³ä¿¡å·è”åˆå‹ç¼©ä¸å¢å¼ºé—®é¢˜ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶çš„è”åˆå‹ç¼©ä¸å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨å®ç°ä½å¤±çœŸå’Œé«˜æ„ŸçŸ¥è´¨é‡ã€‚é¦–å…ˆä¼˜åŒ–ç¼–ç å™¨-è§£ç å™¨å¯¹ï¼Œä»…ä½¿ç”¨å¤±çœŸæŸå¤±ï¼Œç„¶åå›ºå®šç¼–ç å™¨ä»¥ä¼˜åŒ–æ„ŸçŸ¥è§£ç å™¨ï¼Œä»è€Œæé«˜æ„ŸçŸ¥è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶è®­ç»ƒçš„ç¼–ç å™¨åœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä»·æŒ‡æ ‡ä¸Šå‡ä¼˜äºSoundStreamç­‰å…¶ä»–ä»£è¡¨æ€§ç¼–ç å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç ”ç©¶å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³ä¿¡å·è”åˆå‹ç¼©ä¸å¢å¼ºé—®é¢˜ã€‚</li>
<li>SoundStreamç¼–ç å™¨åœ¨ä¸»è§‚æ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ä½å¤±çœŸå’Œé«˜æ„ŸçŸ¥è´¨é‡çš„è¯­éŸ³ä¿¡å·è”åˆå‹ç¼©ä¸å¢å¼ºã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä¼˜åŒ–ç¼–ç å™¨-è§£ç å™¨å¯¹ï¼Œä»…ä½¿ç”¨å¤±çœŸæŸå¤±ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå›ºå®šç¼–ç å™¨ï¼Œä¼˜åŒ–æ„ŸçŸ¥è§£ç å™¨ï¼Œæé«˜æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶è®­ç»ƒçš„ç¼–ç å™¨åœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä»·æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ç¼–ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.04132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c9d1dc6e2572963e163383dad727cfb6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910793&auth_key=1759910793-0-0-cd08ed364b17d88560971f460e32dd4c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7e45862a513a60890562a5cd50ca2fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759910800&auth_key=1759910800-0-0-a70f8a752fda44fb62aac0831e2634c9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5400a337ce684bd7d16d13bdeaee8f68.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  FreeSplatter Pose-free Gaussian Splatting for Sparse-view 3D   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fb2d7f5ff7a76ab26b459a5917f931c7.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Box-Level Class-Balanced Sampling for Active Object Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
