<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  FedMVP Federated Multimodal Visual Prompt Tuning for Vision-Language   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-52b235d3f37de92834bbb5c6fdd9ccf9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909553&auth_key=1759909553-0-0-b6957a52aecde457a2667f8e65a9b88d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    31 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="FedMVP-Federated-Multimodal-Visual-Prompt-Tuning-for-Vision-Language-Models"><a href="#FedMVP-Federated-Multimodal-Visual-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="FedMVP: Federated Multimodal Visual Prompt Tuning for Vision-Language   Models"></a>FedMVP: Federated Multimodal Visual Prompt Tuning for Vision-Language   Models</h2><p><strong>Authors:Mainak Singha, Subhankar Roy, Sarthak Mehrotra, Ankit Jha, Moloud Abdar, Biplab Banerjee, Elisa Ricci</strong></p>
<p>In federated learning, textual prompt tuning adapts Vision-Language Models (e.g., CLIP) by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. After training, only the prompts are shared by the clients with the central server for aggregation. However, textual prompt tuning suffers from overfitting to known concepts, limiting its generalizability to unseen concepts. To address this limitation, we propose Multimodal Visual Prompt Tuning (FedMVP) that conditions the prompts on multimodal contextual information - derived from the input image and textual attribute features of a class. At the core of FedMVP is a PromptFormer module that synergistically aligns textual and visual features through a cross-attention mechanism. The dynamically generated multimodal visual prompts are then input to the frozen vision encoder of CLIP, and trained with a combination of CLIP similarity loss and a consistency loss. Extensive evaluation on 20 datasets, spanning three generalization settings, demonstrates that FedMVP not only preserves performance on in-distribution classes and domains, but also displays higher generalizability to unseen classes and domains, surpassing state-of-the-art methods by a notable margin of +1.57% - 2.26%. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mainaksingha01/FedMVP">https://github.com/mainaksingha01/FedMVP</a>. </p>
<blockquote>
<p>åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œæ–‡æœ¬æç¤ºè°ƒæ•´æ–¹æ³•é€šè¿‡è°ƒæ•´è½»é‡çº§è¾“å…¥ä»¤ç‰Œï¼ˆæˆ–æç¤ºï¼‰æ¥é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œæƒé‡å†»ç»“ï¼Œåœ¨æœ¬åœ°å®¢æˆ·ç«¯æ•°æ®ä¸Šåº”ç”¨ã€‚è®­ç»ƒåï¼Œåªæœ‰æç¤ºè¢«å®¢æˆ·ç«¯ä¸ä¸­å¤®æœåŠ¡å™¨å…±äº«ä»¥è¿›è¡Œèšåˆã€‚ç„¶è€Œï¼Œæ–‡æœ¬æç¤ºè°ƒæ•´é¢ä¸´å¯¹å·²çŸ¥æ¦‚å¿µçš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªè§æ¦‚å¿µä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è§†è§‰æç¤ºè°ƒæ•´ï¼ˆFedMVPï¼‰ï¼Œå®ƒé€šè¿‡å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è®¾ç½®æç¤ºï¼Œè¿™äº›ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æºäºè¾“å…¥å›¾åƒå’Œæ–‡æœ¬å±æ€§ç‰¹å¾çš„ç±»åˆ«ã€‚FedMVPçš„æ ¸å¿ƒæ˜¯PromptFormeræ¨¡å—ï¼Œå®ƒé€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶ååŒå¯¹é½æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ã€‚ç„¶åï¼ŒåŠ¨æ€ç”Ÿæˆçš„å¤šæ¨¡æ€è§†è§‰æç¤ºè¢«è¾“å…¥åˆ°CLIPçš„å†»ç»“è§†è§‰ç¼–ç å™¨ï¼Œå¹¶ç”¨CLIPç›¸ä¼¼æ€§æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±çš„ç»„åˆè¿›è¡Œè®­ç»ƒã€‚åœ¨è·¨è¶Šä¸‰ç§æ³›åŒ–è®¾ç½®çš„20ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFedMVPä¸ä»…åœ¨å†…éƒ¨åˆ†å¸ƒç±»åˆ«å’Œé¢†åŸŸä¸Šä¿æŒæ€§èƒ½ï¼Œè€Œä¸”åœ¨æœªè§ç±»åˆ«å’Œé¢†åŸŸä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…å‡ºæœ€æ–°æ–¹æ³•ä¸€ä¸ªæ˜æ˜¾çš„å¹…åº¦ï¼ˆæé«˜+ 1.57ï¼…è‡³+ 2.26ï¼…ï¼‰ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/mainaksingha01/FedMVP%E3%80%82">https://github.com/mainaksingha01/FedMVPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20860v2">PDF</a> Accepted in ICCV 2025</p>
<p><strong>Summary</strong><br>     è”é‚¦å­¦ä¹ ä¸­ï¼Œæ–‡æœ¬æç¤ºè°ƒæ•´é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ï¼Œé€šè¿‡è°ƒæ•´è½»é‡çº§è¾“å…¥ä»¤ç‰Œï¼ˆæˆ–æç¤ºï¼‰åœ¨æœ¬åœ°å®¢æˆ·ç«¯æ•°æ®ä¸Šï¼ŒåŒæ—¶ä¿æŒç½‘ç»œæƒé‡å†»ç»“ã€‚è®­ç»ƒåï¼Œåªæœ‰æç¤ºè¢«å®¢æˆ·ç«¯ä¸ä¸­å¤®æœåŠ¡å™¨å…±äº«ä»¥è¿›è¡Œèšåˆã€‚ç„¶è€Œï¼Œæ–‡æœ¬æç¤ºè°ƒæ•´ä¼šè¿‡åº¦æ‹Ÿåˆå·²çŸ¥æ¦‚å¿µï¼Œé™åˆ¶äº†å…¶å¯¹æœªè§æ¦‚å¿µçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºå¤šæ¨¡å¼è§†è§‰æç¤ºè°ƒæ•´ï¼ˆFedMVPï¼‰ï¼Œè¯¥è°ƒæ•´åŸºäºè¾“å…¥å›¾åƒå’Œç±»åˆ«æ–‡æœ¬å±æ€§ç‰¹å¾çš„å¤šæ¨¡å¼ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆæç¤ºã€‚FedMVPçš„æ ¸å¿ƒæ˜¯PromptFormeræ¨¡å—ï¼Œå®ƒé€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶ååŒå¯¹é½æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ã€‚ç„¶åï¼Œå°†åŠ¨æ€ç”Ÿæˆçš„å¤šæ¨¡å¼è§†è§‰æç¤ºè¾“å…¥åˆ°CLIPçš„å†»ç»“è§†è§‰ç¼–ç å™¨ï¼Œå¹¶ç”¨CLIPç›¸ä¼¼æ€§æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±çš„ç»„åˆè¿›è¡Œè®­ç»ƒã€‚åœ¨è·¨è¶Šä¸‰ç§æ³›åŒ–è®¾ç½®çš„20ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFedMVPä¸ä»…ä¿æŒäº†åŒç±»å’ŒåŸŸå†…çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨æœªè§ç±»åˆ«å’ŒåŸŸå†…è¡¨ç°å‡ºæ›´é«˜çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…å‡ºæœ€æ–°æ–¹æ³•æ˜¾è‘—çš„ä¼˜åŠ¿ä¸º+1.57% - 2.26%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ä¸­ï¼Œæ–‡æœ¬æç¤ºè°ƒæ•´æ˜¯é€‚åº”Vision-Languageæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„ä¸€ç§ç­–ç•¥ï¼Œé€šè¿‡è°ƒæ•´æœ¬åœ°æ•°æ®ä¸Šçš„è½»é‡çº§è¾“å…¥ä»¤ç‰Œï¼ˆæˆ–æç¤ºï¼‰ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œæƒé‡ä¸å˜ã€‚</li>
<li>æ–‡æœ¬æç¤ºè°ƒæ•´é¢ä¸´è¿‡åº¦æ‹Ÿåˆå·²çŸ¥æ¦‚å¿µçš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨æœªè§æ¦‚å¿µä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæå‡ºäº†å¤šæ¨¡å¼è§†è§‰æç¤ºè°ƒæ•´ï¼ˆFedMVPï¼‰ï¼Œè¯¥ç­–ç•¥åŸºäºè¾“å…¥å›¾åƒå’Œæ–‡æœ¬å±æ€§ç‰¹å¾çš„å¤šæ¨¡å¼ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆæç¤ºã€‚</li>
<li>FedMVPçš„æ ¸å¿ƒæ˜¯PromptFormeræ¨¡å—ï¼Œå®ƒé€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶ååŒæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾çš„å¯¹é½ã€‚</li>
<li>FedMVPåœ¨å¹¿æ³›çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå¯¹æœªè§ç±»åˆ«å’ŒåŸŸçš„é«˜æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>FedMVPç»“åˆäº†CLIPç›¸ä¼¼æ€§æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f49b08b0406dd0612c31ef3c9655556d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ed2dc6b312ba1a164f57c8a4b1385da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64738a865674f067f8b8f0b22921381e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1891cc064349b39605dfce0f608c636d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Disease-Detection-from-Retinal-Fundus-Images"><a href="#A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Disease-Detection-from-Retinal-Fundus-Images" class="headerlink" title="A Hybrid Fully Convolutional CNN-Transformer Model for Inherently   Interpretable Disease Detection from Retinal Fundus Images"></a>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently   Interpretable Disease Detection from Retinal Fundus Images</h2><p><strong>Authors:Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens</strong></p>
<p>In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for retinal disease detection. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the modeâ€™s decision process. We evaluated our method on two medical tasks focused on disease detection using color fundus images. Our model achieves state-of-the-art predictive performance compared to black-box and interpretable models and provides class-specific sparse evidence maps in a single forward pass. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer">https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer</a>. </p>
<blockquote>
<p>åœ¨è®¸å¤šåŒ»å­¦æˆåƒä»»åŠ¡ä¸­ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰èƒ½å¤Ÿé«˜æ•ˆåœ°åˆ†å±‚æå–å±€éƒ¨ç‰¹å¾ã€‚æœ€è¿‘ï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å…¨å±€ä¾èµ–æ€§çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†åœ¨å·ç§¯æ‰€å›ºæœ‰çš„ç©ºé—´å®šä½æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚å› æ­¤ï¼Œä¸ºäº†ç»“åˆä¸¤ç§æ¶æ„çš„ä¼˜ç‚¹ï¼Œå·²ç»å¼€å‘å‡ºäº†ç»“åˆCNNå’ŒViTçš„æ··åˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ··åˆæ¨¡å‹éš¾ä»¥è§£é‡Šï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯è§£é‡Šè®¾è®¡çš„æ··åˆå…¨å·ç§¯CNN-Transformeræ¶æ„ï¼Œç”¨äºè§†ç½‘è†œç–¾ç—…æ£€æµ‹ã€‚ä¸å¹¿æ³›ç”¨äºViTçš„åéªŒæ˜¾è‘—æ€§æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆå¿ å®ä¸”å®šä½çš„è¯æ®å›¾ï¼Œç›´æ¥åæ˜ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»¥ç–¾ç—…æ£€æµ‹ä¸ºé‡ç‚¹çš„åŒ»å­¦ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨çš„æ˜¯å½©è‰²çœ¼åº•å›¾åƒã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸é»‘ç®±å’Œå¯è§£é‡Šæ¨¡å‹ç›¸æ¯”ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„é¢„æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­æä¾›äº†ç‰¹å®šç±»åˆ«çš„ç¨€ç–è¯æ®å›¾ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer">https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08481v4">PDF</a> Accepted at the Workshop on Interpretability of Machine Intelligence   in Medical Image Computing (IMIMIC) at MICCAI 2025 for oral presentation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä¸ºè§†ç½‘è†œç–¾ç—…æ£€æµ‹è€Œè®¾è®¡çš„å¯è§£é‡Šçš„æ··åˆå…¨å·ç§¯CNN-Transformeræ¶æ„ã€‚è¯¥æ¶æ„ç»“åˆäº†CNNå’ŒViTçš„ä¼˜ç‚¹ï¼Œç”Ÿæˆå¿ å®ä¸”å®šä½çš„è¯æ®å›¾ï¼Œç›´æ¥åæ˜ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚åœ¨é¢œè‰²çœ¼åº•å›¾åƒçš„ç–¾ç—…æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œè¯¥æ¨¡å‹å®ç°äº†ä¸é»‘ç®±å’Œå¯è§£é‡Šæ¨¡å‹ç›¸æ¯”çš„å…ˆè¿›é¢„æµ‹æ€§èƒ½ï¼Œå¹¶æä¾›å•å‰å‘ä¼ é€’çš„ç±»ç‰¹å®šç¨€ç–è¯æ®å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆCNN-Transformeræ¶æ„ï¼Œæ—¨åœ¨å®ç°è§†ç½‘è†œç–¾ç—…æ£€æµ‹çš„æ¨¡å‹è®¾è®¡ã€‚</li>
<li>è¯¥æ¶æ„ç»“åˆäº†CNNå’ŒViTçš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>è¯¥æ¨¡å‹ç”Ÿæˆå¿ å®ä¸”å®šä½çš„è¯æ®å›¾ï¼Œç›´æ¥åæ˜ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹åœ¨é¢œè‰²çœ¼åº•å›¾åƒçš„ç–¾ç—…æ£€æµ‹ä»»åŠ¡ä¸Šå®ç°äº†å…ˆè¿›é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹ç›¸æ¯”é»‘ç®±å’Œå¯è§£é‡Šæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹æä¾›å•å‰å‘ä¼ é€’çš„ç±»ç‰¹å®šç¨€ç–è¯æ®å›¾ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8c7550912d2a4b08c18d135f1ff7f793~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909588&auth_key=1759909588-0-0-9337f68cf26ed20ea21c4e18ff8447a1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://picx.zhimg.com/v2-54eaf7f54bd1abc6786ebfd96e0483df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2627bac71ad5dee892a68c1534df5db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a27d3ca4517233ac30fe1f1d7534631.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transferable-Mask-Transformer-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation"><a href="#Transferable-Mask-Transformer-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation" class="headerlink" title="Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation"></a>Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation</h2><p><strong>Authors:Jianhua Liu, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Ruizhe Zhao, Guan Wang, Yang Li</strong></p>
<p>Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTsâ€™ attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMTâ€™s superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰çš„æœ€æ–°è¿›å±•ä¸ºè¯­ä¹‰åˆ†å‰²è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œåœ¨å°†é¢„è®­ç»ƒçš„ViTé€‚åº”åˆ°æ–°çš„ç›®æ ‡åŸŸæ—¶ï¼Œç”±äºåˆ†å¸ƒè½¬ç§»ï¼Œæ€§èƒ½å¾€å¾€ä¼šå‡ºç°æ˜¾è‘—ä¸‹é™ï¼Œå¯¼è‡´å…¨å±€æ³¨æ„åŠ›ä¸ä½³ã€‚ç”±äºè‡ªæ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯æ•°æ®é©±åŠ¨çš„ï¼Œå› æ­¤å½“æºåŸŸå’Œç›®æ ‡åŸŸåœ¨çº¹ç†ã€å°ºåº¦æˆ–å¯¹è±¡å…±ç°æ¨¡å¼ä¸Šå­˜åœ¨å·®å¼‚æ—¶ï¼Œå®ƒä»¬å¯èƒ½æ— æ³•æœ‰æ•ˆåœ°å…³æ³¨å…³é”®å¯¹è±¡ã€‚è™½ç„¶å…¨å±€å’Œè¡¥ä¸çº§åˆ«çš„åŸŸé€‚åº”æ–¹æ³•æä¾›äº†éƒ¨åˆ†è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºä¸åŒå›¾åƒåŒºåŸŸåœ¨è½¬ç§»æ€§ä¸Šçš„ç©ºé—´å¼‚è´¨æ€§ï¼ŒåŠ¨æ€å½¢çŠ¶åŒºåŸŸçš„åŒºåŸŸçº§é€‚åº”è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†Transferable Mask Transformerï¼ˆTMTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯­ä¹‰åˆ†å‰²çš„æ–°å‹åŒºåŸŸçº§é€‚åº”æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç©ºé—´è½¬ç§»æ€§åˆ†ææ¥å¯¹é½è·¨åŸŸè¡¨ç¤ºã€‚TMTç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰åŸºäºè‡ªé€‚åº”èšç±»çš„è½¬ç§»æ€§ä¼°è®¡å™¨ï¼ˆACTEï¼‰ï¼Œå®ƒåŠ¨æ€åœ°å°†å›¾åƒåˆ†å‰²æˆç»“æ„å’Œè¯­ä¹‰ä¸Šè¿è´¯çš„åŒºåŸŸï¼Œä»¥è¿›è¡Œå±€éƒ¨åŒ–çš„è½¬ç§»æ€§è¯„ä¼°ï¼›ï¼ˆ2ï¼‰å¯è½¬ç§»çš„æ©æ¨¡æ³¨æ„åŠ›ï¼ˆTMAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åŒºåŸŸç‰¹å®šçš„è½¬ç§»å›¾é›†æˆåˆ°ViTçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¼˜å…ˆé€‚åº”ä½è½¬ç§»æ€§å’Œé«˜è¯­ä¹‰ä¸ç¡®å®šæ€§çš„åŒºåŸŸã€‚åœ¨20ä¸ªè·¨åŸŸå¯¹ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†TMTçš„ä¼˜è¶Šæ€§ï¼Œä¸å¾®è°ƒç›¸æ¯”ï¼Œå¹³å‡MIoUæé«˜äº†2%ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”æé«˜äº†1.28%ã€‚æºä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05774v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è§†è§‰Transformeråœ¨è¯­ä¹‰åˆ†å‰²ä¸­çš„è·¨åŸŸé€‚åº”æ€§æŒ‘æˆ˜ï¼Œæå‡ºTransferable Mask Transformerï¼ˆTMTï¼‰æ¡†æ¶ï¼ŒåŒ…å«Adaptive Cluster-based Transferability Estimatorï¼ˆACTEï¼‰å’ŒTransferable Masked Attentionï¼ˆTMAï¼‰æ¨¡å—ã€‚ACTEåŠ¨æ€åˆ†å‰²å›¾åƒåŒºåŸŸä»¥è¯„ä¼°å±€éƒ¨è¿ç§»æ€§ï¼Œè€ŒTMAåˆ™å°†åŒºåŸŸç‰¹å®šçš„è¿ç§»æ€§åœ°å›¾æ•´åˆåˆ°ViTsçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¼˜å…ˆé€‚åº”ä½è¿ç§»æ€§å’Œé«˜è¯­ä¹‰ä¸ç¡®å®šæ€§çš„åŒºåŸŸã€‚åœ¨20ä¸ªè·¨åŸŸå¯¹ä¸Šçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºTMTå…·æœ‰ä¼˜åŠ¿ï¼Œå¹³å‡ä¸å¸¸è§„å¾®è°ƒç›¸æ¯”æé«˜äº†2%çš„MIoUï¼Œå¹¶ä¸”æ¯”æœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†1.28%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†åœ¨è·¨åŸŸé€‚åº”æ—¶é¢ä¸´æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨æºåŸŸå’Œç›®æ ‡åŸŸå­˜åœ¨çº¹ç†ã€å°ºåº¦æˆ–å¯¹è±¡å…±ç°æ¨¡å¼å·®å¼‚æ—¶ï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆå…³æ³¨å…³é”®å¯¹è±¡ã€‚</li>
<li>å½“å‰å…¨çƒå’Œè¡¥ä¸çº§åˆ«çš„åŸŸé€‚åº”æ–¹æ³•æä¾›éƒ¨åˆ†è§£å†³æ–¹æ¡ˆï¼Œä½†åŒºåŸŸçº§åˆ«çš„é€‚åº”å¾ˆé‡è¦ï¼Œå› ä¸ºä¸åŒå›¾åƒåŒºåŸŸçš„è½¬ç§»èƒ½åŠ›å­˜åœ¨ç©ºé—´å¼‚è´¨æ€§ã€‚</li>
<li>å¼•å…¥Transferable Mask Transformer (TMT)æ¡†æ¶ï¼ŒåŒ…æ‹¬Adaptive Cluster-based Transferability Estimator (ACTE) å’Œ Transferable Masked Attention (TMA) æ¨¡å—ã€‚</li>
<li>ACTEèƒ½å¤ŸåŠ¨æ€åˆ†å‰²å›¾åƒä»¥è¯„ä¼°å±€éƒ¨è¿ç§»æ€§ã€‚</li>
<li>TMAæ•´åˆäº†åŒºåŸŸç‰¹å®šçš„è¿ç§»æ€§åœ°å›¾åˆ°ViTsçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä¼˜å…ˆé€‚åº”ä½è¿ç§»æ€§å’Œé«˜è¯­ä¹‰ä¸ç¡®å®šæ€§çš„åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-67d6c893b024ab29298c4f75ba353f2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909617&auth_key=1759909617-0-0-55427cd4f69434b0cd4e5294a33ab3a3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pica.zhimg.com/v2-089b47148df110b8d475f038b52edf48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35a73742e1ec071ba686999e1cd633ff.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-c87fbd0597e0047e117fe093a644208b~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909637&auth_key=1759909637-0-0-30b572f86bc9f21664718f6fd70095ac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition"><a href="#Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition" class="headerlink" title="Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition"></a>Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition</h2><p><strong>Authors:Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¡¨ç°å‡ºæ˜¾è‘—çš„è§†é¢‘å’Œè¯­è¨€èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ï¼ˆå¦‚å›¾åƒè¯†åˆ«å’Œå¯¹è±¡å®šä½ï¼‰ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç²¾ç»†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æ—¥å¸¸åœºæ™¯ä¸­ï¼Œä¸ªäººåœ¨é‡åˆ°è®¾è®¡ææ–™ï¼ˆå¦‚æ‚å¿—ã€æ’ç‰ˆæ•™ç¨‹ã€ç ”ç©¶è®ºæ–‡æˆ–å“ç‰Œå†…å®¹ï¼‰æ—¶ï¼Œå¯èƒ½å¸Œæœ›è¯†åˆ«æ–‡æœ¬ä¸­è§†è§‰ä¸Šç¾è§‚çš„å­—ä½“ã€‚è€ƒè™‘åˆ°å®ƒä»¬çš„å¤šæ¨¡å¼èƒ½åŠ›å’Œå…è´¹è®¿é—®æ€§ï¼Œè®¸å¤šVLMsé€šå¸¸è¢«è®¤ä¸ºæ˜¯å­—ä½“è¯†åˆ«çš„æ½œåœ¨å·¥å…·ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šVLMsæ˜¯å¦çœŸçš„å…·å¤‡è¯†åˆ«å­—ä½“çš„èƒ½åŠ›ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«15ç§å¸¸ç”¨å­—ä½“çš„ç´§å‡‘ä¸”ç»“æ„è‰¯å¥½çš„æ•°æ®é›†ã€‚FRBåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šï¼ˆiï¼‰ç®€æ˜“ç‰ˆæœ¬ï¼Œå…¶ä¸­10ä¸ªå¥å­ä»¥ä¸åŒçš„å­—ä½“å‘ˆç°ï¼›ï¼ˆiiï¼‰å›°éš¾ç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±ä¸Šè¿°çš„15ç§å­—ä½“çš„åç§°ç»„æˆï¼Œå¼•å…¥ä¸€ç§æ–¯ç‰¹é²æ™®æ•ˆåº”ï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹å„ç§VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹å…³é”®å‘ç°ï¼šï¼ˆiï¼‰å½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹æœªèƒ½å–å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“å—åˆ°æ–‡æœ¬ä¿¡æ¯å¼•å…¥çš„æ–¯ç‰¹é²æ™®æ•ˆåº”çš„å½±å“ã€‚ï¼ˆiiï¼‰åœ¨æ”¹å–„ä¸åŒVLMçš„å­—è¿¹è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢ï¼Œå°æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰æä¾›çš„å¸®åŠ©å¾®ä¹å…¶å¾®ã€‚ï¼ˆiiiï¼‰æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23768v3">PDF</a> Accepted to COLM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒè¯†åˆ«ã€ç‰©ä½“å®šä½ç­‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†ä»»åŠ¡ä¸Šçš„æ•ˆæœä»æœ‰å¾…æ¢ç´¢ã€‚å¯¹äºæ—¥å¸¸ç”Ÿæ´»ä¸­çš„è®¾è®¡ææ–™ï¼Œå¦‚æ‚å¿—ã€æ’ç‰ˆæ•™ç¨‹ç­‰ï¼Œäººä»¬å¸Œæœ›è¯†åˆ«æ–‡æœ¬ä¸­çš„ç¾è§‚å­—ä½“ã€‚è€ƒè™‘åˆ°VLMsçš„å¤šæ¨¡æ€èƒ½åŠ›å’Œå¼€æ”¾æ€§è®¿é—®ç‰¹æ€§ï¼Œå…¶è¢«è§†ä¸ºå­—ä½“è¯†åˆ«çš„æ½œåœ¨å·¥å…·ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œè®¸å¤šé¡¶å°–æ¨¡å‹æœªèƒ½å–å¾—æ»¡æ„è¡¨ç°ï¼Œä¸”æ˜“å—æ–‡æœ¬ä¿¡æ¯çš„å¹²æ‰°ã€‚æ­¤å¤–ï¼Œå°‘é‡å­¦ä¹ å’ŒChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºå¯¹æå‡ä¸åŒVLMsçš„å­—ä½“è¯†åˆ«å‡†ç¡®åº¦æ•ˆæœæœ‰é™ã€‚æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•æ‰è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å­—ä½“è¯†åˆ«ç­‰ç²¾ç»†ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œé¡¶å°–æ¨¡å‹éš¾ä»¥è¾¾åˆ°æ»¡æ„çš„æ•ˆæœã€‚</li>
<li>å°‘é‡å­¦ä¹ å’ŒChain-of-Thoughtï¼ˆCoTï¼‰æç¤ºåœ¨æ”¹å–„å­—ä½“è¯†åˆ«æ–¹é¢çš„ä½œç”¨æœ‰é™ã€‚</li>
<li>VLMså®¹æ˜“å—åˆ°æ–‡æœ¬ä¿¡æ¯çš„å¹²æ‰°ï¼Œè¿™åœ¨ç ”ç©¶ä¸­è¡¨ç°ä¸ºä¸€ç§â€œæ–¯ç‰¹é²æ™®æ•ˆåº”â€ã€‚</li>
<li>æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•æ‰è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™ã€‚</li>
<li>Font Recognition Benchmarkï¼ˆFRBï¼‰çš„å¼•å…¥ä¸ºè¯„ä¼°VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½æä¾›äº†æœ‰æ•ˆå·¥å…·ã€‚</li>
<li>FRBåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼Œåˆ†åˆ«ä¾§é‡äºä¸åŒéš¾åº¦çº§åˆ«çš„å­—ä½“è¯†åˆ«ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a8d14296de5f2fe68e0d914bd1bdec9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909645&auth_key=1759909645-0-0-b4b473fe33195fd725efd8819b527278&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-09797cf823fa1543e9098f0df2c1abe6~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909653&auth_key=1759909653-0-0-4556c18584b0cfcb884f60adf90e7460&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-689f255139b611ef5d349dcc73baf099~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909660&auth_key=1759909660-0-0-17a9765f407fd65742a6702ddb8494e5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-73c08be47c36f087923de6cb182eac21~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909666&auth_key=1759909666-0-0-b9a13b123cad0eb30a6fc5cec74125d1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b02f4b80fa36aeba1763c3c0d79e2562~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909674&auth_key=1759909674-0-0-f396c3382109244e305916eb5a3e3c3f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Inspiring-the-Next-Generation-of-Segment-Anything-Models-Comprehensively-Evaluate-SAM-and-SAM-2-with-Diverse-Prompts-Towards-Context-Dependent-Concepts-under-Different-Scenes"><a href="#Inspiring-the-Next-Generation-of-Segment-Anything-Models-Comprehensively-Evaluate-SAM-and-SAM-2-with-Diverse-Prompts-Towards-Context-Dependent-Concepts-under-Different-Scenes" class="headerlink" title="Inspiring the Next Generation of Segment Anything Models:   Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards   Context-Dependent Concepts under Different Scenes"></a>Inspiring the Next Generation of Segment Anything Models:   Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards   Context-Dependent Concepts under Different Scenes</h2><p><strong>Authors:Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang, Chenyang Yu, Hanqi Liu, Jiaming Zuo, Jinsong Ouyang, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</strong></p>
<p>As large-scale foundation models trained on billions of imageâ€“mask pairs covering a vast diversity of scenes, objects, and contexts, SAM and its upgraded version, SAM<del>2, have significantly influenced multiple fields within computer vision. Leveraging such unprecedented data diversity, they exhibit strong open-world segmentation capabilities, with SAM</del>2 further enhancing these capabilities to support high-quality video segmentation. While SAMs (SAM and SAM<del>2) have demonstrated excellent performance in segmenting context-independent concepts like people, cars, and roads, they overlook more challenging context-dependent (CD) concepts, such as visual saliency, camouflage, industrial defects, and medical lesions. CD concepts rely heavily on global and local contextual information, making them susceptible to shifts in different contexts, which requires strong discriminative capabilities from the model. The lack of comprehensive evaluation of SAMs limits understanding of their performance boundaries, which may hinder the design of future models. In this paper, we conduct a thorough evaluation of SAMs on 11 CD concepts across 2D and 3D images and videos in various visual modalities within natural, medical, and industrial scenes. We develop a unified evaluation framework for SAM and SAM</del>2 that supports manual, automatic, and intermediate self-prompting, aided by our specific prompt generation and interaction strategies. We further explore the potential of SAM~2 for in-context learning and introduce prompt robustness testing to simulate real-world imperfect prompts. Finally, we analyze the benefits and limitations of SAMs in understanding CD concepts and discuss their future development in segmentation tasks. </p>
<blockquote>
<p>ä½œä¸ºè®­ç»ƒåœ¨è¦†ç›–å¹¿æ³›åœºæ™¯ã€ç‰©ä½“å’Œä¸Šä¸‹æ–‡çš„æ•°åäº¿å›¾åƒ-æ©è†œå¯¹ä¸Šçš„å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ï¼ŒSAMåŠå…¶å‡çº§ç‰ˆSAM<del>2å·²ç»å¯¹è®¡ç®—æœºè§†è§‰çš„å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚å®ƒä»¬åˆ©ç”¨å‰æ‰€æœªæœ‰çš„æ•°æ®å¤šæ ·æ€§ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œåˆ†å‰²èƒ½åŠ›ï¼Œè€ŒSAM</del>2åˆ™è¿›ä¸€æ­¥å¢å¼ºäº†è¿™äº›èƒ½åŠ›ï¼Œä»¥æ”¯æŒé«˜è´¨é‡çš„è§†é¢‘åˆ†å‰²ã€‚è™½ç„¶SAMï¼ˆåŒ…æ‹¬SAMå’ŒSAM<del>2ï¼‰åœ¨åˆ†å‰²ä¸Šä¸‹æ–‡ç‹¬ç«‹æ¦‚å¿µï¼ˆå¦‚äººã€æ±½è½¦å’Œé“è·¯ï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†æ›´å…·æŒ‘æˆ˜æ€§çš„ä¸Šä¸‹æ–‡ç›¸å…³ï¼ˆCDï¼‰æ¦‚å¿µï¼Œå¦‚è§†è§‰æ˜¾è‘—æ€§ã€ä¼ªè£…ã€å·¥ä¸šç¼ºé™·å’ŒåŒ»å­¦ç—…ç¶ã€‚CDæ¦‚å¿µä¸¥é‡ä¾èµ–äºå…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå› æ­¤å®ƒä»¬åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„å˜åŒ–å¾ˆå¤§ï¼Œè¿™è¦æ±‚æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è¾¨åˆ«èƒ½åŠ›ã€‚ç¼ºä¹å¯¹SAMçš„å…¨é¢è¯„ä¼°é™åˆ¶äº†å¯¹å…¶æ€§èƒ½è¾¹ç•Œçš„ç†è§£ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢æœªæ¥æ¨¡å‹çš„è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹SAMåœ¨æ¶µç›–è‡ªç„¶ã€åŒ»å­¦å’Œå·¥ä¸šåœºæ™¯çš„2Då’Œ3Då›¾åƒä»¥åŠè§†é¢‘ä¸­çš„å„ç§è§†è§‰æ¨¡æ€çš„11ä¸ªCDæ¦‚å¿µè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬ä¸ºSAMå’ŒSAM</del>2å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒæ‰‹åŠ¨ã€è‡ªåŠ¨å’Œä¸­é—´è‡ªæˆ‘æç¤ºï¼Œè¾…ä»¥æˆ‘ä»¬ç‰¹å®šçš„æç¤ºç”Ÿæˆå’Œäº¤äº’ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†SAM~2åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥æç¤ºç¨³å¥æ€§æµ‹è¯•æ¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­ä¸å®Œç¾çš„æç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†SAMåœ¨ç†è§£CDæ¦‚å¿µæ–¹é¢çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬åœ¨åˆ†å‰²ä»»åŠ¡çš„æœªæ¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01240v3">PDF</a> Under submission to International Journal of Computer Vision (IJCV)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>SAMåŠå…¶å‡çº§ç‰ˆSAM<del>2æ¨¡å‹ï¼Œé€šè¿‡è®­ç»ƒåœ¨æ¶µç›–å¹¿æ³›åœºæ™¯ã€ç‰©ä½“å’Œä¸Šä¸‹æ–‡çš„å¤§é‡å›¾åƒ-æ©è†œå¯¹ä¸Šï¼Œå·²å¯¹è®¡ç®—æœºè§†è§‰çš„å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚å®ƒä»¬åˆ©ç”¨å‰æ‰€æœªæœ‰çš„æ•°æ®å¤šæ ·æ€§ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œåˆ†å‰²èƒ½åŠ›ï¼Œè€ŒSAM</del>2åˆ™è¿›ä¸€æ­¥æ”¯æŒé«˜è´¨é‡è§†é¢‘åˆ†å‰²ï¼Œå¢å¼ºäº†è¿™äº›èƒ½åŠ›ã€‚SAMç³»åˆ—æ¨¡å‹åœ¨åˆ†å‰²ä¸Šä¸‹æ–‡ç‹¬ç«‹æ¦‚å¿µï¼ˆå¦‚äººã€è½¦å’Œé“è·¯ï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä¸Šä¸‹æ–‡ä¾èµ–ï¼ˆCDï¼‰æ¦‚å¿µä¸Šæœ‰æ‰€å¿½è§†ï¼Œå¦‚è§†è§‰æ˜¾è‘—æ€§ã€ä¼ªè£…ã€å·¥ä¸šç¼ºé™·å’ŒåŒ»ç–—ç—…å˜ã€‚CDæ¦‚å¿µä¸¥é‡ä¾èµ–äºå…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå› æ­¤åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å®¹æ˜“å‘ç”Ÿå˜åŒ–ï¼Œè¦æ±‚æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è¾¨åˆ«èƒ½åŠ›ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹SAMç³»åˆ—æ¨¡å‹åœ¨è·¨è¶Šè‡ªç„¶ã€åŒ»ç–—å’Œå·¥ä¸šåœºæ™¯çš„11ä¸ªCDæ¦‚å¿µä¸Šçš„äºŒç»´å’Œä¸‰ç»´å›¾åƒåŠè§†é¢‘è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬ä¸ºSAMå’ŒSAM<del>2å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒæ‰‹åŠ¨ã€è‡ªåŠ¨å’Œä¸­é—´è‡ªæç¤ºï¼Œè¾…ä»¥æˆ‘ä»¬çš„ç‰¹å®šæç¤ºç”Ÿæˆå’Œäº¤äº’ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†SAM</del>2åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥æç¤ºç¨³å¥æ€§æµ‹è¯•æ¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­ä¸å®Œç¾çš„æç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†SAMç³»åˆ—æ¨¡å‹åœ¨ç†è§£CDæ¦‚å¿µæ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬åœ¨åˆ†å‰²ä»»åŠ¡ä¸­çš„æœªæ¥å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SAMå’ŒSAM~2æ¨¡å‹é€šè¿‡è®­ç»ƒåœ¨å¹¿æ³›çš„å›¾åƒ-æ©è†œå¯¹æ•°æ®ä¸Šï¼Œå·²å¯¹è®¡ç®—æœºè§†è§‰çš„å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†å½±å“ã€‚</li>
<li>SAM~2æ”¯æŒé«˜è´¨é‡è§†é¢‘åˆ†å‰²ï¼Œå±•ç°äº†å¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œåˆ†å‰²èƒ½åŠ›ã€‚</li>
<li>SAMç³»åˆ—æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ç‹¬ç«‹æ¦‚å¿µçš„åˆ†å‰²ä¸Šè¡¨ç°å‡ºä¼˜ç§€æ€§èƒ½ï¼Œä½†åœ¨ä¸Šä¸‹æ–‡ä¾èµ–ï¼ˆCDï¼‰æ¦‚å¿µä¸Šæœ‰æ‰€ä¸è¶³ã€‚</li>
<li>CDæ¦‚å¿µä¾èµ–äºå…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™å¯¹æ¨¡å‹çš„è¾¨åˆ«èƒ½åŠ›æå‡ºäº†é«˜è¦æ±‚ã€‚</li>
<li>æœ¬æ–‡å¯¹SAMç³»åˆ—æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„CDæ¦‚å¿µè¯„ä¼°ï¼Œæ¶‰åŠè‡ªç„¶ã€åŒ»ç–—å’Œå·¥ä¸šåœºæ™¯çš„äºŒç»´å’Œä¸‰ç»´å›¾åƒåŠè§†é¢‘ã€‚</li>
<li>ç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ”¯æŒæ‰‹åŠ¨ã€è‡ªåŠ¨å’Œä¸­é—´è‡ªæç¤ºï¼Œæœ‰åŠ©äºæ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8b884398f7eb35f4185c9ef14f904d00~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909681&auth_key=1759909681-0-0-d38c96e33924fe28dd165f597509d936&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9eb8b4dfa27683f091141d1c4ed28eb9~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909689&auth_key=1759909689-0-0-d6288039cda7d5475afe16b3da017c06&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d2495809c321baddcafc6f58cc0da591~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909697&auth_key=1759909697-0-0-b09a701ae6d1b74cebb4f6abcd2cbe40&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ALow-Cost-Real-Time-Framework-for-Industrial-Action-Recognition-Using-Foundation-Models"><a href="#ALow-Cost-Real-Time-Framework-for-Industrial-Action-Recognition-Using-Foundation-Models" class="headerlink" title="ALow-Cost Real-Time Framework for Industrial Action Recognition Using   Foundation Models"></a>ALow-Cost Real-Time Framework for Industrial Action Recognition Using   Foundation Models</h2><p><strong>Authors:Zhicheng Wang, Wensheng Liang, Ruiyan Zhuang, Shuai Li, Jianwei Tan, Xiaoguang Ma</strong></p>
<p>Action recognition (AR) in industrial environments â€“ particularly for identifying actions and operational gestures â€“ faces persistent challenges due to high deployment costs, poor cross-scenario generalization, and limited real-time performance. To address these issues, we propose a low-cost real-time framework for industrial action recognition using foundation models, denoted as LRIAR, to enhance recognition accuracy and transferability while minimizing human annotation and computational overhead. The proposed framework constructs an automatically labeled dataset by coupling Grounding DINO with the pretrained BLIP-2 image encoder, enabling efficient and scalable action labeling. Leveraging the constructed dataset, we train YOLOv5 for real-time action detection, and a Vision Transformer (ViT) classifier is deceloped via LoRA-based fine-tuning for action classification. Extensive experiments conducted in real-world industrial settings validate the effectiveness of LRIAR, demonstrating consistent improvements over state-of-the-art methods in recognition accuracy, scenario generalization, and deployment efficiency. </p>
<blockquote>
<p>åœ¨å·¥ä¸šç¯å¢ƒä¸­è¿›è¡ŒåŠ¨ä½œè¯†åˆ«ï¼ˆARï¼‰â€”â€”ç‰¹åˆ«æ˜¯è¯†åˆ«åŠ¨ä½œå’Œæ“ä½œæ‰‹åŠ¿â€”â€”ç”±äºéƒ¨ç½²æˆæœ¬é«˜ã€è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›å·®ä»¥åŠå®æ—¶æ€§èƒ½æœ‰é™ï¼Œä¸€ç›´é¢ä¸´ç€æŒç»­æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„å·¥ä¸šåŠ¨ä½œè¯†åˆ«å®æ—¶ä½æˆæœ¬æ¡†æ¶ï¼Œç§°ä¸ºLRIARï¼Œä»¥æé«˜è¯†åˆ«ç²¾åº¦å’Œå¯è¿ç§»æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–äººå·¥æ ‡æ³¨å’Œè®¡ç®—å¼€é”€ã€‚æ‰€æå‡ºçš„æ¡†æ¶é€šè¿‡ç»“åˆGrounding DINOå’Œé¢„è®­ç»ƒçš„BLIP-2å›¾åƒç¼–ç å™¨ï¼Œæ„å»ºè‡ªåŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œå®ç°é«˜æ•ˆä¸”å¯æ‰©å±•çš„åŠ¨ä½œæ ‡æ³¨ã€‚åˆ©ç”¨æ„å»ºçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒYOLOv5è¿›è¡Œå®æ—¶åŠ¨ä½œæ£€æµ‹ï¼Œå¹¶é€šè¿‡LoRAå¾®è°ƒå¼€å‘Vision Transformerï¼ˆViTï¼‰åˆ†ç±»å™¨è¿›è¡ŒåŠ¨ä½œåˆ†ç±»ã€‚åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„å¹¿æ³›å®éªŒéªŒè¯äº†LRIARçš„æœ‰æ•ˆæ€§ï¼Œåœ¨è¯†åˆ«ç²¾åº¦ã€åœºæ™¯æ³›åŒ–å’Œéƒ¨ç½²æ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•çš„æŒç»­æ€§æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08420v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå·¥ä¸šç¯å¢ƒä¸­åŠ¨ä½œè¯†åˆ«é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é«˜éƒ¨ç½²æˆæœ¬ã€è·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›å·®å’Œå®æ—¶æ€§èƒ½æœ‰é™ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„ä½æˆæœ¬å®æ—¶å·¥ä¸šåŠ¨ä½œè¯†åˆ«æ¡†æ¶ï¼Œç§°ä¸ºLRIARã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆGrounding DINOå’Œé¢„è®­ç»ƒçš„BLIP-2å›¾åƒç¼–ç å™¨ï¼Œæ„å»ºè‡ªåŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œå®ç°é«˜æ•ˆå¯ä¼¸ç¼©çš„åŠ¨ä½œæ ‡æ³¨ã€‚åˆ©ç”¨æ„å»ºçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒYOLOv5è¿›è¡Œå®æ—¶åŠ¨ä½œæ£€æµ‹ï¼Œå¹¶é€šè¿‡LoRAå¾®è°ƒæŠ€æœ¯å¼€å‘Vision Transformerï¼ˆViTï¼‰åˆ†ç±»å™¨è¿›è¡ŒåŠ¨ä½œåˆ†ç±»ã€‚åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„å¹¿æ³›å®éªŒéªŒè¯äº†LRIARçš„æœ‰æ•ˆæ€§ï¼Œåœ¨è¯†åˆ«ç²¾åº¦ã€åœºæ™¯æ³›åŒ–å’Œéƒ¨ç½²æ•ˆç‡æ–¹é¢å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRIARæ¡†æ¶æ—¨åœ¨è§£å†³å·¥ä¸šç¯å¢ƒä¸­åŠ¨ä½œè¯†åˆ«çš„é«˜éƒ¨ç½²æˆæœ¬ã€æ³›åŒ–èƒ½åŠ›å·®å’Œå®æ—¶æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨Grounding DINOå’ŒBLIP-2å›¾åƒç¼–ç å™¨æ„å»ºè‡ªåŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œæé«˜æ ‡æ³¨æ•ˆç‡å’Œæ•°æ®è´¨é‡ã€‚</li>
<li>é‡‡ç”¨YOLOv5è¿›è¡Œå®æ—¶åŠ¨ä½œæ£€æµ‹ï¼Œç¡®ä¿åŠ¨ä½œçš„å‡†ç¡®æ•æ‰ã€‚</li>
<li>åˆ©ç”¨Vision Transformerï¼ˆViTï¼‰åˆ†ç±»å™¨è¿›è¡ŒåŠ¨ä½œåˆ†ç±»ï¼Œé€šè¿‡LoRAå¾®è°ƒæŠ€æœ¯æé«˜åˆ†ç±»ç²¾åº¦ã€‚</li>
<li>LRIARæ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸åŒå·¥ä¸šåœºæ™¯ä¸­è¿›è¡Œæœ‰æ•ˆè¯†åˆ«ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†LRIARåœ¨è¯†åˆ«ç²¾åº¦ã€åœºæ™¯æ³›åŒ–èƒ½åŠ›å’Œéƒ¨ç½²æ•ˆç‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c41fe35f0d1abb1d9301336f69f6418d~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909705&auth_key=1759909705-0-0-5f75e9c7afcdbb13256dc85166565a95&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-30d7cca7b3fd964ffe9134fba1a051bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52b235d3f37de92834bbb5c6fdd9ccf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e565154b163aa62aa797222cde3697d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af3950215ceaf905a9fad35abeb840e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a15cad09e7138cf84a3d5acc69a4298.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-18dc2750cf02f751e297ecb33346c3b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909745&auth_key=1759909745-0-0-9ebdfef6ac9af2dbd84f822db58812fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-3ef60c45801f75147fa9f014d463c839.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-ca164bf58ba8c8f64c12fcfe43a41624~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909758&auth_key=1759909758-0-0-33bfc2c6a6c3251a290eb4ede177347f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Mixture-of-Exemplars-Approach-for-Efficient-Out-of-Distribution-Detection-with-Foundation-Models"><a href="#A-Mixture-of-Exemplars-Approach-for-Efficient-Out-of-Distribution-Detection-with-Foundation-Models" class="headerlink" title="A Mixture of Exemplars Approach for Efficient Out-of-Distribution   Detection with Foundation Models"></a>A Mixture of Exemplars Approach for Efficient Out-of-Distribution   Detection with Foundation Models</h2><p><strong>Authors:Evelyn Mannix, Howard Bondell</strong></p>
<p>One of the early weaknesses identified in deep neural networks trained for image classification tasks was their inability to provide low confidence predictions on out-of-distribution (OOD) data that was significantly different from the in-distribution (ID) data used to train them. Representation learning, where neural networks are trained in specific ways that improve their ability to detect OOD examples, has emerged as a promising solution. However, these approaches require long training times and can add additional overhead to detect OOD examples. Recent developments in Vision Transformer (ViT) foundation models$\unicode{x2013}$large networks trained on large and diverse datasets with self-supervised approaches$\unicode{x2013}$also show strong performance in OOD detection, and could address these challenges. This paper presents Mixture of Exemplars (MoLAR), an efficient approach to tackling OOD detection challenges that is designed to maximise the benefit of training a classifier with a high quality, frozen, pretrained foundation model backbone. MoLAR provides strong OOD detection performance when only comparing the similarity of OOD examples to the exemplars, a small set of images chosen to be representative of the dataset, leading to significantly reduced overhead for OOD detection inference over other methods that provide best performance when the full ID dataset is used. Extensive experiments demonstrate the improved OOD detection performance of MoLAR in comparison to comparable approaches in both supervised and semi-supervised settings, and code is available at github.com&#x2F;emannix&#x2F;molar-mixture-of-exemplars. </p>
<blockquote>
<p>æ—©æœŸåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ—¶ï¼Œå‘ç°çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬æ— æ³•åœ¨å¯¹è¿œç¦»è®­ç»ƒæ‰€ç”¨çš„å†…éƒ¨æ•°æ®åˆ†å¸ƒï¼ˆIDï¼‰ä¸”æˆªç„¶ä¸åŒçš„å¤–éƒ¨æ•°æ®åˆ†å¸ƒï¼ˆOODï¼‰æ•°æ®ä¸Šæä¾›ä½ç½®ä¿¡åº¦é¢„æµ‹ã€‚è¡¨ç°å­¦ä¹ ï¼ˆrepresentation learningï¼‰åº”è¿è€Œç”Ÿä¸ºä¸€ç§é¢‡å…·å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç‰¹å®šçš„è®­ç»ƒç¥ç»ç½‘ç»œæ–¹å¼æé«˜å…¶æ£€æµ‹OODæ ·æœ¬çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦é•¿æ—¶é—´çš„è®­ç»ƒå¹¶ä¸”ä¼šé¢å¤–å¢åŠ æ£€æµ‹OODæ ·æœ¬çš„å¼€é”€ã€‚è¿‘æœŸï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼Œç®€ç§°ViTï¼‰åŸºç¡€æ¨¡å‹çš„ç ”å‘å±•ç¤ºäº†åœ¨OODæ£€æµ‹ä¸­çš„å¼ºåŠ²æ€§èƒ½ã€‚è¿™äº›åŸºç¡€æ¨¡å‹é€šè¿‡åœ¨å¤§å‹ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šä½¿ç”¨è‡ªç›‘ç£æ–¹å¼è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºMixture of Exemplars (MoLAR)çš„æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹çš„ä¼˜åŠ¿è¿›è¡Œé«˜æ•ˆçš„OODæ£€æµ‹æŒ‘æˆ˜ã€‚MoLARè®¾è®¡ä¹‹åˆå°±æ—¨åœ¨é€šè¿‡ç›¸ä¼¼æ€§å¯¹æ¯”æ¥åˆ¤æ–­æ˜¯å¦ä¸ºOODæ ·æœ¬ï¼Œåªå¯¹æ¯”ä¸€å°éƒ¨åˆ†å…·æœ‰ä»£è¡¨æ€§çš„å›¾åƒæ ·æœ¬é›†å³å¯å®ç°å‡ºè‰²çš„OODæ£€æµ‹æ€§èƒ½ã€‚ç›¸è¾ƒäºä½¿ç”¨å…¨IDæ•°æ®é›†è¡¨ç°æœ€ä½³çš„å…¶ä»–æ–¹æ³•è€Œè¨€ï¼Œè¿™æ— ç–‘å¤§å¤§å‡å°‘äº†OODæ£€æµ‹æ¨ç†çš„å¼€é”€ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜äº†MoLARç›¸è¾ƒäºå…¶ä»–å¯æ¯”æ–¹æ³•åœ¨æ— ç›‘ç£åŠåŠç›‘ç£è®¾ç½®ä¸‹çš„å‡ºè‰²OODæ£€æµ‹æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/emannix/molar-mixture-of-exemplars">github.com&#x2F;emannix&#x2F;molar-mixture-of-exemplars</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.17093v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†å›¾åƒåˆ†ç±»ä»»åŠ¡æ—¶çš„ä¸€ä¸ªæ—©æœŸå¼±ç‚¹ï¼Œå³å®ƒä»¬æ— æ³•å¯¹ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒå·®å¼‚è¾ƒå¤§çš„æ•°æ®è¿›è¡Œä½ç½®ä¿¡åº¦é¢„æµ‹ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä½¿ç”¨è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒç¥ç»ç½‘ç»œä»¥æ£€æµ‹å¼‚å¸¸æ•°æ®çš„æ–¹æ³•ã€‚æœ€è¿‘ï¼ŒVision Transformerï¼ˆViTï¼‰åŸºç¡€æ¨¡å‹çš„å‡ºç°ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMixture of Exemplarsï¼ˆMoLARï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä½¿ç”¨é«˜è´¨é‡ã€å†»ç»“çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è®­ç»ƒåˆ†ç±»å™¨çš„ä¼˜åŠ¿ã€‚MoLARé€šè¿‡æ¯”è¾ƒå¼‚å¸¸æ•°æ®ä¸æ ·æœ¬çš„ç›¸ä¼¼æ€§è¿›è¡ŒOODæ£€æµ‹ï¼Œè¿™äº›æ ·æœ¬æ˜¯ä»å…·æœ‰ä»£è¡¨æ€§çš„å›¾åƒé›†ä¸­é€‰æ‹©çš„å°‘é‡å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒMoLARåœ¨ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­ä¸å…¶ä»–ç±»ä¼¼æ–¹æ³•çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ›´å¼ºçš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚åŒæ—¶æä¾›äº†å¯¹åº”çš„ä»£ç åº“åœ°å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æå–çš„ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†å›¾åƒåˆ†ç±»ä»»åŠ¡æ—¶é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œå®ƒä»¬éš¾ä»¥å¯¹ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒçš„æ•°æ®è¿›è¡Œä½ç½®ä¿¡åº¦é¢„æµ‹ã€‚</li>
<li>è¡¨ç¤ºå­¦ä¹ æ–¹æ³•èƒ½å¤Ÿè®­ç»ƒç¥ç»ç½‘ç»œä»¥æ£€æµ‹å¼‚å¸¸æ•°æ®åˆ†å¸ƒï¼ˆOODï¼‰ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰åŸºç¡€æ¨¡å‹ä¸ºè§£å†³OODæ£€æµ‹é—®é¢˜æä¾›äº†æœ‰æ•ˆæ”¯æŒã€‚</li>
<li>Mixture of Exemplarsï¼ˆMoLARï¼‰æ˜¯ä¸€ç§è®¾è®¡ç”¨æ¥æœ€å¤§åŒ–ä½¿ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è®­ç»ƒåˆ†ç±»å™¨ä¼˜åŠ¿çš„æ–¹æ³•ã€‚</li>
<li>MoLARé€šè¿‡æ¯”è¾ƒå¼‚å¸¸æ•°æ®ä¸æ ·æœ¬çš„ç›¸ä¼¼æ€§è¿›è¡ŒOODæ£€æµ‹ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å¼‚å¸¸æ£€æµ‹çš„æ¨ç†å¼€é”€ã€‚</li>
<li>å®éªŒè¡¨æ˜MoLARåœ¨ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´å¥½çš„OODæ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.17093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a15fc0526b0b70fc9d56edee45231b23~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909765&auth_key=1759909765-0-0-26e207992cb3a7c7d29871329ecdf41c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic1.zhimg.com/v2-2d3001d8deca4df2f0fb6bfa66bbe901.jpg" align="middle">
<img src="https://pic-private.zhihu.com/v2-4fbeee710bd89f11ff76076d5594bcdb~resize:0:q75.jpg?source=1f5c5e47&expiration=1759909779&auth_key=1759909779-0-0-dfdc352476336aa01fc045b78219db37&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fb2d7f5ff7a76ab26b459a5917f931c7.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Box-Level Class-Balanced Sampling for Active Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-332aca5a95e994063881c7c6b76808bd~resize:0:q75.jpg?source=1f5c5e47&expiration=1759805272&auth_key=1759805272-0-0-eaa7cada8ff724e46083e4270ee23e26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Few-shot Human Action Anomaly Detection via a Unified Contrastive   Learning Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31180k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
