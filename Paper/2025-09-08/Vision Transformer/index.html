<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-09-08  FedMVP Federated Multimodal Visual Prompt Tuning for Vision-Language   Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-08-更新"><a href="#2025-09-08-更新" class="headerlink" title="2025-09-08 更新"></a>2025-09-08 更新</h1><h2 id="FedMVP-Federated-Multimodal-Visual-Prompt-Tuning-for-Vision-Language-Models"><a href="#FedMVP-Federated-Multimodal-Visual-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="FedMVP: Federated Multimodal Visual Prompt Tuning for Vision-Language   Models"></a>FedMVP: Federated Multimodal Visual Prompt Tuning for Vision-Language   Models</h2><p><strong>Authors:Mainak Singha, Subhankar Roy, Sarthak Mehrotra, Ankit Jha, Moloud Abdar, Biplab Banerjee, Elisa Ricci</strong></p>
<p>In federated learning, textual prompt tuning adapts Vision-Language Models (e.g., CLIP) by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. After training, only the prompts are shared by the clients with the central server for aggregation. However, textual prompt tuning suffers from overfitting to known concepts, limiting its generalizability to unseen concepts. To address this limitation, we propose Multimodal Visual Prompt Tuning (FedMVP) that conditions the prompts on multimodal contextual information - derived from the input image and textual attribute features of a class. At the core of FedMVP is a PromptFormer module that synergistically aligns textual and visual features through a cross-attention mechanism. The dynamically generated multimodal visual prompts are then input to the frozen vision encoder of CLIP, and trained with a combination of CLIP similarity loss and a consistency loss. Extensive evaluation on 20 datasets, spanning three generalization settings, demonstrates that FedMVP not only preserves performance on in-distribution classes and domains, but also displays higher generalizability to unseen classes and domains, surpassing state-of-the-art methods by a notable margin of +1.57% - 2.26%. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mainaksingha01/FedMVP">https://github.com/mainaksingha01/FedMVP</a>. </p>
<blockquote>
<p>在联邦学习中，文本提示调整方法通过调整轻量级输入令牌（或提示）来适应视觉语言模型（例如CLIP），同时保持网络权重冻结，在本地客户端数据上应用。训练后，只有提示被客户端与中央服务器共享以进行聚合。然而，文本提示调整面临对已知概念的过度拟合问题，这限制了其在未见概念上的泛化能力。为了解决这个问题，我们提出了多模态视觉提示调整（FedMVP），它通过多模态上下文信息来设置提示，这些上下文信息来源于输入图像和文本属性特征的类别。FedMVP的核心是PromptFormer模块，它通过交叉注意机制协同对齐文本和视觉特征。然后，动态生成的多模态视觉提示被输入到CLIP的冻结视觉编码器，并用CLIP相似性损失和一致性损失的组合进行训练。在跨越三种泛化设置的20个数据集上的广泛评估表明，FedMVP不仅在内部分布类别和领域上保持性能，而且在未见类别和领域上表现出更高的泛化能力，超出最新方法一个明显的幅度（提高+ 1.57％至+ 2.26％）。代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/mainaksingha01/FedMVP%E3%80%82">https://github.com/mainaksingha01/FedMVP。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20860v2">PDF</a> Accepted in ICCV 2025</p>
<p><strong>Summary</strong><br>     联邦学习中，文本提示调整适应视觉语言模型（如CLIP），通过调整轻量级输入令牌（或提示）在本地客户端数据上，同时保持网络权重冻结。训练后，只有提示被客户端与中央服务器共享以进行聚合。然而，文本提示调整会过度拟合已知概念，限制了其对未见概念的泛化能力。为解决此局限性，我们提出多模式视觉提示调整（FedMVP），该调整基于输入图像和类别文本属性特征的多模式上下文信息来生成提示。FedMVP的核心是PromptFormer模块，它通过交叉注意机制协同对齐文本和视觉特征。然后，将动态生成的多模式视觉提示输入到CLIP的冻结视觉编码器，并用CLIP相似性损失和一致性损失的组合进行训练。在跨越三种泛化设置的20个数据集上的广泛评估表明，FedMVP不仅保持了同类和域内的性能，而且在未见类别和域内表现出更高的泛化能力，超出最新方法显著的优势为+1.57% - 2.26%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习中，文本提示调整是适应Vision-Language模型（如CLIP）的一种策略，通过调整本地数据上的轻量级输入令牌（或提示），同时保持网络权重不变。</li>
<li>文本提示调整面临过度拟合已知概念的挑战，限制了其在未见概念上的泛化能力。</li>
<li>为了解决这一局限性，提出了多模式视觉提示调整（FedMVP），该策略基于输入图像和文本属性特征的多模式上下文信息来生成提示。</li>
<li>FedMVP的核心是PromptFormer模块，它通过交叉注意机制协同文本和视觉特征的对齐。</li>
<li>FedMVP在广泛的数据集上进行了评估，显示出对未见类别和域的高泛化能力，并超越了现有方法的性能。</li>
<li>FedMVP结合了CLIP相似性损失和一致性损失进行训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.20860v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.20860v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.20860v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.20860v2/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Disease-Detection-from-Retinal-Fundus-Images"><a href="#A-Hybrid-Fully-Convolutional-CNN-Transformer-Model-for-Inherently-Interpretable-Disease-Detection-from-Retinal-Fundus-Images" class="headerlink" title="A Hybrid Fully Convolutional CNN-Transformer Model for Inherently   Interpretable Disease Detection from Retinal Fundus Images"></a>A Hybrid Fully Convolutional CNN-Transformer Model for Inherently   Interpretable Disease Detection from Retinal Fundus Images</h2><p><strong>Authors:Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens</strong></p>
<p>In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for retinal disease detection. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the mode’s decision process. We evaluated our method on two medical tasks focused on disease detection using color fundus images. Our model achieves state-of-the-art predictive performance compared to black-box and interpretable models and provides class-specific sparse evidence maps in a single forward pass. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer">https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer</a>. </p>
<blockquote>
<p>在许多医学成像任务中，卷积神经网络（CNN）能够高效地分层提取局部特征。最近，使用自注意力机制捕捉全局依赖性的视觉变压器（ViT）越来越受欢迎，但在卷积所固有的空间定位方面仍存在不足。因此，为了结合两种架构的优点，已经开发出了结合CNN和ViT的混合模型。然而，这种混合模型难以解释，阻碍了它们在医学成像中的应用。在这项工作中，我们引入了一种可解释设计的混合全卷积CNN-Transformer架构，用于视网膜疾病检测。与广泛用于ViT的后验显著性方法不同，我们的方法生成忠实且定位的证据图，直接反映模型的决策过程。我们在两个以疾病检测为重点的医学任务上评估了我们的方法，使用的是彩色眼底图像。我们的模型与黑箱和可解释模型相比，达到了最先进的预测性能，并在单次前向传递中提供了特定类别的稀疏证据图。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer">https://github.com/kdjoumessi/Self-Explainable-CNN-Transformer</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08481v4">PDF</a> Accepted at the Workshop on Interpretability of Machine Intelligence   in Medical Image Computing (IMIMIC) at MICCAI 2025 for oral presentation</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种为视网膜疾病检测而设计的可解释的混合全卷积CNN-Transformer架构。该架构结合了CNN和ViT的优点，生成忠实且定位的证据图，直接反映模型的决策过程。在颜色眼底图像的疾病检测任务上，该模型实现了与黑箱和可解释模型相比的先进预测性能，并提供单前向传递的类特定稀疏证据图。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种混合CNN-Transformer架构，旨在实现视网膜疾病检测的模型设计。</li>
<li>该架构结合了CNN和ViT的优点，旨在提高模型的性能和可解释性。</li>
<li>该模型生成忠实且定位的证据图，直接反映模型的决策过程。</li>
<li>模型在颜色眼底图像的疾病检测任务上实现了先进预测性能。</li>
<li>模型相比黑箱和可解释模型表现出更高的性能。</li>
<li>模型提供单前向传递的类特定稀疏证据图，有助于提高模型的可解释性和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.08481v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.08481v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.08481v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.08481v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transferable-Mask-Transformer-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation"><a href="#Transferable-Mask-Transformer-Cross-domain-Semantic-Segmentation-with-Region-adaptive-Transferability-Estimation" class="headerlink" title="Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation"></a>Transferable Mask Transformer: Cross-domain Semantic Segmentation with   Region-adaptive Transferability Estimation</h2><p><strong>Authors:Jianhua Liu, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Ruizhe Zhao, Guan Wang, Yang Li</strong></p>
<p>Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTs’ attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMT’s superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available. </p>
<blockquote>
<p>Vision Transformer（ViT）的最新进展为语义分割设定了新的基准。然而，在将预训练的ViT适应到新的目标域时，由于分布转移，性能往往会出现显著下降，导致全局注意力不佳。由于自注意力机制本质上是数据驱动的，因此当源域和目标域在纹理、尺度或对象共现模式上存在差异时，它们可能无法有效地关注关键对象。虽然全局和补丁级别的域适应方法提供了部分解决方案，但由于不同图像区域在转移性上的空间异质性，动态形状区域的区域级适应至关重要。我们提出了Transferable Mask Transformer（TMT），这是一种用于语义分割的新型区域级适应框架，它通过空间转移性分析来对齐跨域表示。TMT由两个关键组件构成：（1）基于自适应聚类的转移性估计器（ACTE），它动态地将图像分割成结构和语义上连贯的区域，以进行局部化的转移性评估；（2）可转移的掩模注意力（TMA）模块，该模块将区域特定的转移图集成到ViT的注意力机制中，优先适应低转移性和高语义不确定性的区域。在20个跨域对上的综合评估证明了TMT的优越性，与微调相比，平均MIoU提高了2%，与最新基线相比提高了1.28%。源代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05774v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对视觉Transformer在语义分割中的跨域适应性挑战，提出Transferable Mask Transformer（TMT）框架，包含Adaptive Cluster-based Transferability Estimator（ACTE）和Transferable Masked Attention（TMA）模块。ACTE动态分割图像区域以评估局部迁移性，而TMA则将区域特定的迁移性地图整合到ViTs的注意力机制中，优先适应低迁移性和高语义不确定性的区域。在20个跨域对上的综合评估显示TMT具有优势，平均与常规微调相比提高了2%的MIoU，并且比最先进的基线提高了1.28%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) 在语义分割方面取得了最新进展，但在跨域适应时面临性能下降问题。</li>
<li>自注意力机制在源域和目标域存在纹理、尺度或对象共现模式差异时，可能无法有效关注关键对象。</li>
<li>当前全球和补丁级别的域适应方法提供部分解决方案，但区域级别的适应很重要，因为不同图像区域的转移能力存在空间异质性。</li>
<li>引入Transferable Mask Transformer (TMT)框架，包括Adaptive Cluster-based Transferability Estimator (ACTE) 和 Transferable Masked Attention (TMA) 模块。</li>
<li>ACTE能够动态分割图像以评估局部迁移性。</li>
<li>TMA整合了区域特定的迁移性地图到ViTs的注意力机制中，优先适应低迁移性和高语义不确定性的区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.05774v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.05774v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.05774v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2504.05774v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition"><a href="#Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition" class="headerlink" title="Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition"></a>Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition</h2><p><strong>Authors:Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features. </p>
<blockquote>
<p>现代视觉语言模型（VLMs）表现出显著的视频和语言能力，在各种任务（如图像识别和对象定位）中取得了令人印象深刻的性能。然而，它们在精细任务中的有效性仍然是一个悬而未决的问题。在日常场景中，个人在遇到设计材料（如杂志、排版教程、研究论文或品牌内容）时，可能希望识别文本中视觉上美观的字体。考虑到它们的多模式能力和免费访问性，许多VLMs通常被认为是字体识别的潜在工具。这引发了一个基本问题：VLMs是否真的具备识别字体的能力？为了调查这一点，我们引入了字体识别基准测试（FRB），这是一个包含15种常用字体的紧凑且结构良好的数据集。FRB包括两个版本：（i）简易版本，其中10个句子以不同的字体呈现；（ii）困难版本，其中每个文本样本由上述的15种字体的名称组成，引入一种斯特鲁普效应，挑战模型的感知能力。通过对各种VLMs在字体识别任务上的广泛评估，我们得出以下关键发现：（i）当前VLMs在字体识别方面的能力有限，许多最先进的模型未能取得令人满意的性能，并且很容易受到文本信息引入的斯特鲁普效应的影响。（ii）在改善不同VLM的字迹识别准确性方面，小样本学习和思维链提示（CoT）提供的帮助微乎其微。（iii）注意力分析揭示了VLM在捕获语义特征方面的内在局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23768v3">PDF</a> Accepted to COLM 2025</p>
<p><strong>摘要</strong></p>
<p>现代视觉语言模型（VLMs）在图像识别、物体定位等方面表现出强大的视觉和语言能力，但在精细任务上的效果仍有待探索。对于日常生活中的设计材料，如杂志、排版教程等，人们希望识别文本中的美观字体。考虑到VLMs的多模态能力和开放性访问特性，其被视为字体识别的潜在工具。然而，我们的研究发现，当前VLMs在字体识别方面的能力有限，许多顶尖模型未能取得满意表现，且易受文本信息的干扰。此外，少量学习和Chain-of-Thought（CoT）提示对提升不同VLMs的字体识别准确度效果有限。注意力分析揭示了VLMs在捕捉语义特征方面的内在局限。</p>
<p><strong>要点</strong></p>
<ol>
<li>现代视觉语言模型在字体识别等精细任务上的表现有限，顶尖模型难以达到满意的效果。</li>
<li>少量学习和Chain-of-Thought（CoT）提示在改善字体识别方面的作用有限。</li>
<li>VLMs容易受到文本信息的干扰，这在研究中表现为一种“斯特鲁普效应”。</li>
<li>注意力分析揭示了VLMs在捕捉语义特征方面的内在局限。</li>
<li>Font Recognition Benchmark（FRB）的引入为评估VLMs在字体识别任务上的性能提供了有效工具。</li>
<li>FRB包括两个版本，分别侧重于不同难度级别的字体识别任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2503.23768v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2503.23768v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2503.23768v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2503.23768v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2503.23768v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Inspiring-the-Next-Generation-of-Segment-Anything-Models-Comprehensively-Evaluate-SAM-and-SAM-2-with-Diverse-Prompts-Towards-Context-Dependent-Concepts-under-Different-Scenes"><a href="#Inspiring-the-Next-Generation-of-Segment-Anything-Models-Comprehensively-Evaluate-SAM-and-SAM-2-with-Diverse-Prompts-Towards-Context-Dependent-Concepts-under-Different-Scenes" class="headerlink" title="Inspiring the Next Generation of Segment Anything Models:   Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards   Context-Dependent Concepts under Different Scenes"></a>Inspiring the Next Generation of Segment Anything Models:   Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards   Context-Dependent Concepts under Different Scenes</h2><p><strong>Authors:Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang, Chenyang Yu, Hanqi Liu, Jiaming Zuo, Jinsong Ouyang, Weisi Lin, Georges El Fakhri, Huchuan Lu, Xiaofeng Liu</strong></p>
<p>As large-scale foundation models trained on billions of image–mask pairs covering a vast diversity of scenes, objects, and contexts, SAM and its upgraded version, SAM<del>2, have significantly influenced multiple fields within computer vision. Leveraging such unprecedented data diversity, they exhibit strong open-world segmentation capabilities, with SAM</del>2 further enhancing these capabilities to support high-quality video segmentation. While SAMs (SAM and SAM<del>2) have demonstrated excellent performance in segmenting context-independent concepts like people, cars, and roads, they overlook more challenging context-dependent (CD) concepts, such as visual saliency, camouflage, industrial defects, and medical lesions. CD concepts rely heavily on global and local contextual information, making them susceptible to shifts in different contexts, which requires strong discriminative capabilities from the model. The lack of comprehensive evaluation of SAMs limits understanding of their performance boundaries, which may hinder the design of future models. In this paper, we conduct a thorough evaluation of SAMs on 11 CD concepts across 2D and 3D images and videos in various visual modalities within natural, medical, and industrial scenes. We develop a unified evaluation framework for SAM and SAM</del>2 that supports manual, automatic, and intermediate self-prompting, aided by our specific prompt generation and interaction strategies. We further explore the potential of SAM~2 for in-context learning and introduce prompt robustness testing to simulate real-world imperfect prompts. Finally, we analyze the benefits and limitations of SAMs in understanding CD concepts and discuss their future development in segmentation tasks. </p>
<blockquote>
<p>作为训练在覆盖广泛场景、物体和上下文的数十亿图像-掩膜对上的大规模基础模型，SAM及其升级版SAM<del>2已经对计算机视觉的多个领域产生了显著影响。它们利用前所未有的数据多样性，展现出强大的开放世界分割能力，而SAM</del>2则进一步增强了这些能力，以支持高质量的视频分割。虽然SAM（包括SAM和SAM<del>2）在分割上下文独立概念（如人、汽车和道路）方面表现出卓越的性能，但它们忽略了更具挑战性的上下文相关（CD）概念，如视觉显著性、伪装、工业缺陷和医学病灶。CD概念严重依赖于全局和局部上下文信息，因此它们在不同上下文中的变化很大，这要求模型具备强大的辨别能力。缺乏对SAM的全面评估限制了对其性能边界的理解，这可能会阻碍未来模型的设计。在本文中，我们对SAM在涵盖自然、医学和工业场景的2D和3D图像以及视频中的各种视觉模态的11个CD概念进行了全面评估。我们为SAM和SAM</del>2开发了一个统一的评估框架，该框架支持手动、自动和中间自我提示，辅以我们特定的提示生成和交互策略。我们进一步探索了SAM~2在上下文学习中的潜力，并引入提示稳健性测试来模拟现实世界中不完美的提示。最后，我们分析了SAM在理解CD概念方面的优点和局限性，并讨论了它们在分割任务的未来发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01240v3">PDF</a> Under submission to International Journal of Computer Vision (IJCV)</p>
<p><strong>摘要</strong></p>
<p>SAM及其升级版SAM<del>2模型，通过训练在涵盖广泛场景、物体和上下文的大量图像-掩膜对上，已对计算机视觉的多个领域产生了显著影响。它们利用前所未有的数据多样性，展现出强大的开放世界分割能力，而SAM</del>2则进一步支持高质量视频分割，增强了这些能力。SAM系列模型在分割上下文独立概念（如人、车和道路）方面表现出卓越性能，但在更具挑战性的上下文依赖（CD）概念上有所忽视，如视觉显著性、伪装、工业缺陷和医疗病变。CD概念严重依赖于全局和局部上下文信息，因此在不同上下文中容易发生变化，要求模型具备强大的辨别能力。本文中，我们对SAM系列模型在跨越自然、医疗和工业场景的11个CD概念上的二维和三维图像及视频进行了全面评估。我们为SAM和SAM<del>2开发了一个统一的评估框架，支持手动、自动和中间自提示，辅以我们的特定提示生成和交互策略。我们进一步探索了SAM</del>2在上下文学习中的潜力，并引入提示稳健性测试来模拟现实世界中不完美的提示。最后，我们分析了SAM系列模型在理解CD概念方面的优势和局限性，并讨论了它们在分割任务中的未来发展。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SAM和SAM~2模型通过训练在广泛的图像-掩膜对数据上，已对计算机视觉的多个领域产生了影响。</li>
<li>SAM~2支持高质量视频分割，展现了强大的开放世界分割能力。</li>
<li>SAM系列模型在上下文独立概念的分割上表现出优秀性能，但在上下文依赖（CD）概念上有所不足。</li>
<li>CD概念依赖于全局和局部上下文信息，这对模型的辨别能力提出了高要求。</li>
<li>本文对SAM系列模型进行了全面的CD概念评估，涉及自然、医疗和工业场景的二维和三维图像及视频。</li>
<li>统一的评估框架支持手动、自动和中间自提示，有助于更全面地评估模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01240">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2412.01240v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2412.01240v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2412.01240v3/page_3_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ALow-Cost-Real-Time-Framework-for-Industrial-Action-Recognition-Using-Foundation-Models"><a href="#ALow-Cost-Real-Time-Framework-for-Industrial-Action-Recognition-Using-Foundation-Models" class="headerlink" title="ALow-Cost Real-Time Framework for Industrial Action Recognition Using   Foundation Models"></a>ALow-Cost Real-Time Framework for Industrial Action Recognition Using   Foundation Models</h2><p><strong>Authors:Zhicheng Wang, Wensheng Liang, Ruiyan Zhuang, Shuai Li, Jianwei Tan, Xiaoguang Ma</strong></p>
<p>Action recognition (AR) in industrial environments – particularly for identifying actions and operational gestures – faces persistent challenges due to high deployment costs, poor cross-scenario generalization, and limited real-time performance. To address these issues, we propose a low-cost real-time framework for industrial action recognition using foundation models, denoted as LRIAR, to enhance recognition accuracy and transferability while minimizing human annotation and computational overhead. The proposed framework constructs an automatically labeled dataset by coupling Grounding DINO with the pretrained BLIP-2 image encoder, enabling efficient and scalable action labeling. Leveraging the constructed dataset, we train YOLOv5 for real-time action detection, and a Vision Transformer (ViT) classifier is deceloped via LoRA-based fine-tuning for action classification. Extensive experiments conducted in real-world industrial settings validate the effectiveness of LRIAR, demonstrating consistent improvements over state-of-the-art methods in recognition accuracy, scenario generalization, and deployment efficiency. </p>
<blockquote>
<p>在工业环境中进行动作识别（AR）——特别是识别动作和操作手势——由于部署成本高、跨场景泛化能力差以及实时性能有限，一直面临着持续挑战。为了解决这些问题，我们提出了一种利用基础模型的工业动作识别实时低成本框架，称为LRIAR，以提高识别精度和可迁移性，同时最小化人工标注和计算开销。所提出的框架通过结合Grounding DINO和预训练的BLIP-2图像编码器，构建自动标注数据集，实现高效且可扩展的动作标注。利用构建的数据集，我们训练YOLOv5进行实时动作检测，并通过LoRA微调开发Vision Transformer（ViT）分类器进行动作分类。在真实工业环境中的广泛实验验证了LRIAR的有效性，在识别精度、场景泛化和部署效率方面均表现出优于现有最新方法的持续性改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08420v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于工业环境中动作识别面临的挑战，如高部署成本、跨场景泛化能力差和实时性能有限等问题，我们提出了一种利用基础模型的低成本实时工业动作识别框架，称为LRIAR。该框架通过结合Grounding DINO和预训练的BLIP-2图像编码器，构建自动标注数据集，实现高效可伸缩的动作标注。利用构建的数据集，我们训练YOLOv5进行实时动作检测，并通过LoRA微调技术开发Vision Transformer（ViT）分类器进行动作分类。在真实工业环境中的广泛实验验证了LRIAR的有效性，在识别精度、场景泛化和部署效率方面均表现出优于现有技术的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRIAR框架旨在解决工业环境中动作识别的高部署成本、泛化能力差和实时性能不足的问题。</li>
<li>利用Grounding DINO和BLIP-2图像编码器构建自动标注数据集，提高标注效率和数据质量。</li>
<li>采用YOLOv5进行实时动作检测，确保动作的准确捕捉。</li>
<li>利用Vision Transformer（ViT）分类器进行动作分类，通过LoRA微调技术提高分类精度。</li>
<li>LRIAR框架具有良好的泛化能力，能在不同工业场景中进行有效识别。</li>
<li>实验结果证明了LRIAR在识别精度、场景泛化能力和部署效率方面的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08420">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2403.08420v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Mixture-of-Exemplars-Approach-for-Efficient-Out-of-Distribution-Detection-with-Foundation-Models"><a href="#A-Mixture-of-Exemplars-Approach-for-Efficient-Out-of-Distribution-Detection-with-Foundation-Models" class="headerlink" title="A Mixture of Exemplars Approach for Efficient Out-of-Distribution   Detection with Foundation Models"></a>A Mixture of Exemplars Approach for Efficient Out-of-Distribution   Detection with Foundation Models</h2><p><strong>Authors:Evelyn Mannix, Howard Bondell</strong></p>
<p>One of the early weaknesses identified in deep neural networks trained for image classification tasks was their inability to provide low confidence predictions on out-of-distribution (OOD) data that was significantly different from the in-distribution (ID) data used to train them. Representation learning, where neural networks are trained in specific ways that improve their ability to detect OOD examples, has emerged as a promising solution. However, these approaches require long training times and can add additional overhead to detect OOD examples. Recent developments in Vision Transformer (ViT) foundation models$\unicode{x2013}$large networks trained on large and diverse datasets with self-supervised approaches$\unicode{x2013}$also show strong performance in OOD detection, and could address these challenges. This paper presents Mixture of Exemplars (MoLAR), an efficient approach to tackling OOD detection challenges that is designed to maximise the benefit of training a classifier with a high quality, frozen, pretrained foundation model backbone. MoLAR provides strong OOD detection performance when only comparing the similarity of OOD examples to the exemplars, a small set of images chosen to be representative of the dataset, leading to significantly reduced overhead for OOD detection inference over other methods that provide best performance when the full ID dataset is used. Extensive experiments demonstrate the improved OOD detection performance of MoLAR in comparison to comparable approaches in both supervised and semi-supervised settings, and code is available at github.com&#x2F;emannix&#x2F;molar-mixture-of-exemplars. </p>
<blockquote>
<p>早期在图像分类任务中训练深度神经网络时，发现的一个弱点是它们无法在对远离训练所用的内部数据分布（ID）且截然不同的外部数据分布（OOD）数据上提供低置信度预测。表现学习（representation learning）应运而生为一种颇具前景的解决方案，通过特定的训练神经网络方式提高其检测OOD样本的能力。然而，这些方法需要长时间的训练并且会额外增加检测OOD样本的开销。近期，视觉转换器（Vision Transformer，简称ViT）基础模型的研发展示了在OOD检测中的强劲性能。这些基础模型通过在大型、多样化的数据集上使用自监督方式进行大规模训练。本文提出了一个名为Mixture of Exemplars (MoLAR)的方法，旨在有效利用预训练的基础模型的优势进行高效的OOD检测挑战。MoLAR设计之初就旨在通过相似性对比来判断是否为OOD样本，只对比一小部分具有代表性的图像样本集即可实现出色的OOD检测性能。相较于使用全ID数据集表现最佳的其他方法而言，这无疑大大减少了OOD检测推理的开销。广泛的实验证明了MoLAR相较于其他可比方法在无监督及半监督设置下的出色OOD检测性能。代码可通过<a target="_blank" rel="noopener" href="https://github.com/emannix/molar-mixture-of-exemplars">github.com&#x2F;emannix&#x2F;molar-mixture-of-exemplars</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.17093v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了深度学习模型在处理图像分类任务时的一个早期弱点，即它们无法对与训练数据分布差异较大的数据进行低置信度预测。为此，研究者提出了使用表示学习方法来训练神经网络以检测异常数据的方法。最近，Vision Transformer（ViT）基础模型的出现为解决这一问题提供了有力支持。本文提出了一种名为Mixture of Exemplars（MoLAR）的有效方法，旨在最大化使用高质量、冻结的预训练基础模型训练分类器的优势。MoLAR通过比较异常数据与样本的相似性进行OOD检测，这些样本是从具有代表性的图像集中选择的少量图像。实验表明，MoLAR在监督学习和半监督学习环境中与其他类似方法的比较中表现出更强的异常检测性能。同时提供了对应的代码库地址。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是提取的七个关键见解：</p>
<ol>
<li>深度学习模型在处理图像分类任务时面临的一个挑战是，它们难以对与训练数据分布不同的数据进行低置信度预测。</li>
<li>表示学习方法能够训练神经网络以检测异常数据分布（OOD）。</li>
<li>Vision Transformer（ViT）基础模型为解决OOD检测问题提供了有效支持。</li>
<li>Mixture of Exemplars（MoLAR）是一种设计用来最大化使用预训练基础模型训练分类器优势的方法。</li>
<li>MoLAR通过比较异常数据与样本的相似性进行OOD检测，这种方法显著减少了异常检测的推理开销。</li>
<li>实验表明MoLAR在监督学习和半监督学习环境中与其他方法相比具有更好的OOD检测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.17093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2311.17093v6/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2311.17093v6/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Vision Transformer/2311.17093v6/page_3_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_检测_分割_跟踪/2407.15199v2/page_0_0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-09-08  Box-Level Class-Balanced Sampling for Active Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_Few-Shot/2508.16833v1/page_0_0.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-09-08  Few-shot Human Action Anomaly Detection via a Unified Contrastive   Learning Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
