<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Masked Autoencoder Pretraining and BiXLSTM ResNet Architecture for   PET/CT Tumor Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20537v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-08-æ›´æ–°"><a href="#2025-09-08-æ›´æ–°" class="headerlink" title="2025-09-08 æ›´æ–°"></a>2025-09-08 æ›´æ–°</h1><h2 id="Masked-Autoencoder-Pretraining-and-BiXLSTM-ResNet-Architecture-for-PET-CT-Tumor-Segmentation"><a href="#Masked-Autoencoder-Pretraining-and-BiXLSTM-ResNet-Architecture-for-PET-CT-Tumor-Segmentation" class="headerlink" title="Masked Autoencoder Pretraining and BiXLSTM ResNet Architecture for   PET&#x2F;CT Tumor Segmentation"></a>Masked Autoencoder Pretraining and BiXLSTM ResNet Architecture for   PET&#x2F;CT Tumor Segmentation</h2><p><strong>Authors:Moona Mazher, Steven A Niederer, Abdul Qayyum</strong></p>
<p>The accurate segmentation of lesions in whole-body PET&#x2F;CT imaging is es-sential for tumor characterization, treatment planning, and response assess-ment, yet current manual workflows are labor-intensive and prone to inter-observer variability. Automated deep learning methods have shown promise but often remain limited by modality specificity, isolated time points, or in-sufficient integration of expert knowledge. To address these challenges, we present a two-stage lesion segmentation framework developed for the fourth AutoPET Challenge. In the first stage, a Masked Autoencoder (MAE) is em-ployed for self-supervised pretraining on unlabeled PET&#x2F;CT and longitudinal CT scans, enabling the extraction of robust modality-specific representations without manual annotations. In the second stage, the pretrained encoder is fine-tuned with a bidirectional XLSTM architecture augmented with ResNet blocks and a convolutional decoder. By jointly leveraging anatomical (CT) and functional (PET) information as complementary input channels, the model achieves improved temporal and spatial feature integration. Evalua-tion on the AutoPET Task 1 dataset demonstrates that self-supervised pre-training significantly enhances segmentation accuracy, achieving a Dice score of 0.582 compared to 0.543 without pretraining. These findings high-light the potential of combining self-supervised learning with multimodal fu-sion for robust and generalizable PET&#x2F;CT lesion segmentation. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/RespectKnowledge/AutoPet_2025_BxLSTM_UNET_Segmentation">https://github.com/RespectKnowledge/AutoPet_2025_BxLSTM_UNET_Segmentation</a> </p>
<blockquote>
<p>åœ¨å…¨èº«PET&#x2F;CTæˆåƒä¸­ï¼Œç—…å˜çš„ç²¾ç¡®åˆ†å‰²å¯¹äºè‚¿ç˜¤ç‰¹å¾æè¿°ã€æ²»ç–—è®¡åˆ’åˆ¶å®šå’Œç–—æ•ˆè¯„ä¼°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ‰‹åŠ¨å·¥ä½œæµç¨‹åŠ³åŠ¨å¼ºåº¦å¤§ï¼Œä¸”å­˜åœ¨è§‚å¯Ÿè€…é—´å˜å¼‚çš„å¯èƒ½æ€§ã€‚è™½ç„¶è‡ªåŠ¨æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»å±•ç°å‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†é€šå¸¸å—åˆ°æ¨¡æ€ç‰¹å¼‚æ€§ã€å­¤ç«‹æ—¶é—´ç‚¹æˆ–ä¸“å®¶çŸ¥è¯†æ•´åˆä¸è¶³çš„é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02602v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å…¨èº«PET&#x2F;CTå½±åƒçš„ä¸¤é˜¶æ®µç—…å˜åˆ†å‰²æ¡†æ¶ï¼Œä»¥è§£å†³è‚¿ç˜¤ç‰¹å¾åŒ–ã€æ²»ç–—è§„åˆ’å’Œç–—æ•ˆè¯„ä¼°ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨Masked Autoencoderè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œæå–ç¨³å¥çš„æ¨¡æ€ç‰¹å¼‚æ€§è¡¨ç¤ºï¼›ç¬¬äºŒé˜¶æ®µä½¿ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œç»“åˆåŒå‘XLSTMæ¶æ„å’Œå·ç§¯è§£ç å™¨è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚è¯¥æ¨¡å‹åˆ©ç”¨è§£å‰–ï¼ˆCTï¼‰å’ŒåŠŸèƒ½ï¼ˆPETï¼‰ä¿¡æ¯ä½œä¸ºäº’è¡¥è¾“å…¥é€šé“ï¼Œå®ç°äº†æ—¶ç©ºç‰¹å¾çš„é›†æˆã€‚åœ¨AutoPET Task 1æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒæ˜¾è‘—æé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ï¼ŒDiceå¾—åˆ†ä»0.543æå‡è‡³0.582ã€‚è¿™è¡¨æ˜è‡ªç›‘ç£å­¦ä¹ ä¸å¤šæ¨¡æ€èåˆçš„ç»“åˆåœ¨PET&#x2F;CTç—…å˜åˆ†å‰²ä¸­å…·æœ‰æ½œåŠ›å’Œå¹¿é˜”å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PET&#x2F;CTå½±åƒçš„ç²¾ç¡®ç—…å˜åˆ†å‰²å¯¹äºè‚¿ç˜¤è¡¨å¾ã€æ²»ç–—è§„åˆ’å’Œç–—æ•ˆè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ‰‹åŠ¨å·¥ä½œæµç¨‹åŠ³åŠ¨å¼ºåº¦å¤§ï¼Œä¸”å­˜åœ¨è§‚å¯Ÿè€…é—´å˜å¼‚çš„é—®é¢˜ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ·±åº¦å­¦ä¹ æ–¹æ³•è™½å…·æ½œåŠ›ï¼Œä½†ä»é¢ä¸´æ¨¡æ€ç‰¹å¼‚æ€§ã€å­¤ç«‹æ—¶é—´ç‚¹æˆ–ç¼ºä¹ä¸“å®¶çŸ¥è¯†æ•´åˆç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µç—…å˜åˆ†å‰²æ¡†æ¶ç»“åˆäº†è‡ªç›‘ç£é¢„è®­ç»ƒå’ŒåŒå‘XLSTMæ¶æ„ï¼Œä»¥æé«˜PET&#x2F;CTå½±åƒçš„åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒåœ¨æ— æ ‡ç­¾PET&#x2F;CTå’Œçºµå‘CTæ‰«ææ•°æ®ä¸Šæœ‰æ•ˆæå–äº†ç¨³å¥çš„æ¨¡æ€ç‰¹å¼‚æ€§è¡¨ç¤ºã€‚</li>
<li>ç»“åˆè§£å‰–ï¼ˆCTï¼‰å’ŒåŠŸèƒ½ï¼ˆPETï¼‰ä¿¡æ¯ä½œä¸ºäº’è¡¥è¾“å…¥é€šé“ï¼Œå®ç°äº†æ—¶ç©ºç‰¹å¾çš„é›†æˆï¼Œè¿›ä¸€æ­¥æé«˜äº†åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.02602v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.02602v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.02602v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.02602v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.02602v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GPSToken-Gaussian-Parameterized-Spatially-adaptive-Tokenization-for-Image-Representation-and-Generation"><a href="#GPSToken-Gaussian-Parameterized-Spatially-adaptive-Tokenization-for-Image-Representation-and-Generation" class="headerlink" title="GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for   Image Representation and Generation"></a>GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for   Image Representation and Generation</h2><p><strong>Authors:Zhengqiang Zhang, Rongyuan Wu, Lingchen Sun, Lei Zhang</strong></p>
<p>Effective and efficient tokenization plays an important role in image representation and generation. Conventional methods, constrained by uniform 2D&#x2F;1D grid tokenization, are inflexible to represent regions with varying shapes and textures and at different locations, limiting their efficacy of feature representation. In this work, we propose $\textbf{GPSToken}$, a novel $\textbf{G}$aussian $\textbf{P}$arameterized $\textbf{S}$patially-adaptive $\textbf{Token}$ization framework, to achieve non-uniform image tokenization by leveraging parametric 2D Gaussians to dynamically model the shape, position, and textures of different image regions. We first employ an entropy-driven algorithm to partition the image into texture-homogeneous regions of variable sizes. Then, we parameterize each region as a 2D Gaussian (mean for position, covariance for shape) coupled with texture features. A specialized transformer is trained to optimize the Gaussian parameters, enabling continuous adaptation of position&#x2F;shape and content-aware feature extraction. During decoding, Gaussian parameterized tokens are reconstructed into 2D feature maps through a differentiable splatting-based renderer, bridging our adaptive tokenization with standard decoders for end-to-end training. GPSToken disentangles spatial layout (Gaussian parameters) from texture features to enable efficient two-stage generation: structural layout synthesis using lightweight networks, followed by structure-conditioned texture generation. Experiments demonstrate the state-of-the-art performance of GPSToken, which achieves rFID and FID scores of 0.65 and 1.50 on image reconstruction and generation tasks using 128 tokens, respectively. Codes and models of GPSToken can be found at $\href{<a target="_blank" rel="noopener" href="https://github.com/xtudbxk/GPSToken%7D%7Bhttps://github.com/xtudbxk/GPSToken%7D$">https://github.com/xtudbxk/GPSToken}{https://github.com/xtudbxk/GPSToken}$</a>. </p>
<blockquote>
<p>æœ‰æ•ˆä¸”é«˜æ•ˆçš„ä»¤ç‰ŒåŒ–åœ¨å›¾åƒè¡¨ç¤ºå’Œç”Ÿæˆä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•å—åˆ°ç»Ÿä¸€2D&#x2F;1Dç½‘æ ¼ä»¤ç‰ŒåŒ–çš„é™åˆ¶ï¼Œéš¾ä»¥è¡¨ç¤ºå½¢çŠ¶å’Œçº¹ç†å„å¼‚ä»¥åŠä½ç½®ä¸åŒçš„åŒºåŸŸï¼Œä»è€Œé™åˆ¶äº†å…¶ç‰¹å¾è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>GPSToken</strong>è¿™ä¸€æ–°å‹çš„é«˜æ–¯å‚æ•°åŒ–ç©ºé—´è‡ªé€‚åº”ä»¤ç‰ŒåŒ–æ¡†æ¶ï¼Œåˆ©ç”¨å‚æ•°åŒ–2Dé«˜æ–¯æ¨¡å‹æ¥åŠ¨æ€æ¨¡æ‹Ÿä¸åŒå›¾åƒåŒºåŸŸçš„å½¢çŠ¶ã€ä½ç½®å’Œçº¹ç†ï¼Œä»è€Œå®ç°éå‡åŒ€å›¾åƒä»¤ç‰ŒåŒ–ã€‚æˆ‘ä»¬é¦–å…ˆé‡‡ç”¨ä¸€ç§åŸºäºç†µçš„ç®—æ³•å°†å›¾åƒåˆ†å‰²æˆä¸åŒå¤§å°çš„çº¹ç†å‡åŒ€åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ¯ä¸ªåŒºåŸŸå‚æ•°åŒ–ä¸ºä¸€ä¸ªåŒ…å«çº¹ç†ç‰¹å¾çš„äºŒç»´é«˜æ–¯ï¼ˆå‡å€¼ç”¨äºä½ç½®ï¼Œåæ–¹å·®ç”¨äºå½¢çŠ¶ï¼‰ã€‚è®­ç»ƒäº†ä¸“é—¨çš„å˜å‹å™¨æ¥ä¼˜åŒ–é«˜æ–¯å‚æ•°ï¼Œå®ç°äº†ä½ç½®&#x2F;å½¢çŠ¶çš„æŒç»­è‡ªé€‚åº”å’Œå†…å®¹æ„ŸçŸ¥ç‰¹å¾æå–ã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å¯å¾®åˆ†çš„åŸºäºå¹³é“ºçš„æ¸²æŸ“å™¨å°†é«˜æ–¯å‚æ•°åŒ–ä»¤ç‰Œé‡å»ºä¸ºäºŒç»´ç‰¹å¾å›¾ï¼Œä½¿æˆ‘ä»¬çš„è‡ªé€‚åº”ä»¤ç‰ŒåŒ–ä¸æ ‡å‡†è§£ç å™¨ç›¸ç»“åˆï¼Œå®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚GPSTokenå°†ç©ºé—´å¸ƒå±€ï¼ˆé«˜æ–¯å‚æ•°ï¼‰ä¸çº¹ç†ç‰¹å¾åˆ†ç¦»ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸¤ä¸ªé˜¶æ®µç”Ÿæˆï¼šä½¿ç”¨è½»é‡çº§ç½‘ç»œè¿›è¡Œç»“æ„å¸ƒå±€åˆæˆï¼Œç„¶åè¿›è¡Œç»“æ„æ¡ä»¶çº¹ç†ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒGPSTokençš„æ€§èƒ½å¤„äºé¢†å…ˆæ°´å¹³ï¼Œåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸­ä½¿ç”¨128ä¸ªä»¤ç‰Œå®ç°äº†rFIDå’ŒFIDå¾—åˆ†åˆ†åˆ«ä¸º0.65å’Œ1.50ã€‚GPSTokençš„ä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/xtudbxk/GPSToken%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xtudbxk/GPSTokenæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01109v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºé«˜æ–¯å‚æ•°åŒ–çš„ç©ºé—´è‡ªé€‚åº”ä»¤ç‰ŒåŒ–æ¡†æ¶GPSTokenï¼Œç”¨äºå®ç°éå‡åŒ€å›¾åƒä»¤ç‰ŒåŒ–ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å‚æ•°åŒ–äºŒç»´é«˜æ–¯åŠ¨æ€å»ºæ¨¡å›¾åƒä¸åŒåŒºåŸŸçš„å½¢çŠ¶ã€ä½ç½®å’Œçº¹ç†ï¼Œå®ç°äº†é«˜æ•ˆå’Œæœ‰æ•ˆçš„å›¾åƒè¡¨ç¤ºå’Œç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åŸºäºç†µé©±åŠ¨çš„ç®—æ³•å°†å›¾åƒåˆ’åˆ†ä¸ºçº¹ç†å‡åŒ€çš„å¯å˜åŒºåŸŸï¼Œå¹¶å°†æ¯ä¸ªåŒºåŸŸå‚æ•°åŒ–ä¸ºäºŒç»´é«˜æ–¯æ¨¡å‹ï¼Œä¸çº¹ç†ç‰¹å¾ç›¸ç»“åˆã€‚è®­ç»ƒä¸“ç”¨è½¬æ¢å™¨ä»¥ä¼˜åŒ–é«˜æ–¯å‚æ•°ï¼Œå®ç°ä½ç½®&#x2F;å½¢çŠ¶çš„è¿ç»­é€‚åº”å’Œå†…å®¹æ„ŸçŸ¥ç‰¹å¾æå–ã€‚è§£ç è¿‡ç¨‹ä¸­ï¼Œé«˜æ–¯å‚æ•°åŒ–ä»¤ç‰Œé€šè¿‡å¯å¾®åˆ†çš„å¹³é“ºæ¸²æŸ“å™¨é‡å»ºä¸ºäºŒç»´ç‰¹å¾å›¾ï¼Œå°†è‡ªé€‚åº”ä»¤ç‰ŒåŒ–ä¸æ ‡å‡†è§£ç å™¨ç›¸ç»“åˆè¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚GPSTokenå°†ç©ºé—´å¸ƒå±€ï¼ˆé«˜æ–¯å‚æ•°ï¼‰ä¸çº¹ç†ç‰¹å¾åˆ†å¼€ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¸¤é˜¶æ®µç”Ÿæˆï¼šç»“æ„å¸ƒå±€çš„åˆæˆä½¿ç”¨è½»é‡çº§ç½‘ç»œï¼Œç„¶åè¿›è¡Œç»“æ„æ¡ä»¶ä¸‹çš„çº¹ç†ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒGPSTokenåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œä½¿ç”¨128ä¸ªä»¤ç‰Œåˆ†åˆ«å®ç°äº†0.65çš„rFIDå’Œ1.50çš„FIDåˆ†æ•°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜æ–¯å‚æ•°åŒ–ç©ºé—´è‡ªé€‚åº”ä»¤ç‰ŒåŒ–æ¡†æ¶GPSTokenï¼Œå®ç°äº†éå‡åŒ€å›¾åƒä»¤ç‰ŒåŒ–ã€‚</li>
<li>åˆ©ç”¨å‚æ•°åŒ–äºŒç»´é«˜æ–¯åŠ¨æ€å»ºæ¨¡å›¾åƒä¸åŒåŒºåŸŸçš„å½¢çŠ¶ã€ä½ç½®å’Œçº¹ç†ã€‚</li>
<li>é‡‡ç”¨äº†åŸºäºç†µé©±åŠ¨çš„ç®—æ³•å°†å›¾åƒåˆ’åˆ†ä¸ºçº¹ç†å‡åŒ€çš„å¯å˜åŒºåŸŸã€‚</li>
<li>é€šè¿‡è®­ç»ƒä¸“ç”¨è½¬æ¢å™¨ä¼˜åŒ–é«˜æ–¯å‚æ•°ï¼Œå®ç°äº†ä½ç½®&#x2F;å½¢çŠ¶çš„è¿ç»­é€‚åº”å’Œå†…å®¹æ„ŸçŸ¥ç‰¹å¾æå–ã€‚</li>
<li>é€šè¿‡å¯å¾®åˆ†çš„å¹³é“ºæ¸²æŸ“å™¨å°†é«˜æ–¯å‚æ•°åŒ–ä»¤ç‰Œé‡å»ºä¸ºäºŒç»´ç‰¹å¾å›¾ï¼Œä¾¿äºä¸æ ‡å‡†è§£ç å™¨ç»“åˆè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚</li>
<li>GPSTokenå®ç°äº†é«˜æ•ˆçš„ä¸¤é˜¶æ®µç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç»“æ„å¸ƒå±€çš„åˆæˆå’Œçº¹ç†ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01109v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01109v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01109v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01109v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpectMamba-Integrating-Frequency-and-State-Space-Models-for-Enhanced-Medical-Image-Detection"><a href="#SpectMamba-Integrating-Frequency-and-State-Space-Models-for-Enhanced-Medical-Image-Detection" class="headerlink" title="SpectMamba: Integrating Frequency and State Space Models for Enhanced   Medical Image Detection"></a>SpectMamba: Integrating Frequency and State Space Models for Enhanced   Medical Image Detection</h2><p><strong>Authors:Yao Wang, Dong Yang, Zhi Qiao, Wenjian Huang, Liuzhi Yang, Zhen Qian</strong></p>
<p>Abnormality detection in medical imaging is a critical task requiring both high efficiency and accuracy to support effective diagnosis. While convolutional neural networks (CNNs) and Transformer-based models are widely used, both face intrinsic challenges: CNNs have limited receptive fields, restricting their ability to capture broad contextual information, and Transformers encounter prohibitive computational costs when processing high-resolution medical images. Mamba, a recent innovation in natural language processing, has gained attention for its ability to process long sequences with linear complexity, offering a promising alternative. Building on this foundation, we present SpectMamba, the first Mamba-based architecture designed for medical image detection. A key component of SpectMamba is the Hybrid Spatial-Frequency Attention (HSFA) block, which separately learns high- and low-frequency features. This approach effectively mitigates the loss of high-frequency information caused by frequency bias and correlates frequency-domain features with spatial features, thereby enhancing the modelâ€™s ability to capture global context. To further improve long-range dependencies, we propose the Visual State-Space Module (VSSM) and introduce a novel Hilbert Curve Scanning technique to strengthen spatial correlations and local dependencies, further optimizing the Mamba framework. Comprehensive experiments show that SpectMamba achieves state-of-the-art performance while being both effective and efficient across various medical image detection tasks. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒä¸­çš„å¼‚å¸¸æ£€æµ‹æ˜¯ä¸€é¡¹æ—¢éœ€è¦é«˜æ•ˆç‡åˆéœ€è¦å‡†ç¡®æ€§çš„é‡è¦ä»»åŠ¡ï¼Œä»¥æ”¯æŒæœ‰æ•ˆçš„è¯Šæ–­ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŸºäºTransformerçš„æ¨¡å‹å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä½†å®ƒä»¬éƒ½é¢ä¸´ç€å›ºæœ‰çš„æŒ‘æˆ˜ï¼šCNNçš„æ„Ÿé‡æœ‰é™ï¼Œé™åˆ¶äº†å®ƒä»¬æ•è·å¹¿æ³›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼›è€ŒTransformeråœ¨å¤„ç†é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ—¶é­é‡äº†è¿‡é«˜çš„è®¡ç®—æˆæœ¬ã€‚ä½œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€é¡¹æœ€æ–°åˆ›æ–°ï¼ŒMambaå› å…¶ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ï¼Œå®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“ä¸ºåŒ»å­¦å›¾åƒæ£€æµ‹è®¾è®¡çš„Mambaæ¶æ„â€”â€”SpectMambaã€‚SpectMambaçš„å…³é”®ç»„ä»¶æ˜¯æ··åˆç©ºé—´é¢‘ç‡æ³¨æ„åŠ›ï¼ˆHSFAï¼‰å—ï¼Œè¯¥å—åˆ†åˆ«å­¦ä¹ é«˜é¢‘å’Œä½é¢‘ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†ç”±é¢‘ç‡åå·®å¯¼è‡´çš„é«˜é¢‘ä¿¡æ¯æŸå¤±ï¼Œå¹¶å°†é¢‘åŸŸç‰¹å¾ä¸ç©ºé—´ç‰¹å¾ç›¸å…³è”ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹æ•è·å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„è¿œç¨‹ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆVSSMï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„Hilbertæ›²çº¿æ‰«ææŠ€æœ¯æ¥åŠ å¼ºç©ºé—´å…³è”å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†Mambaæ¡†æ¶ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒSpectMambaåœ¨å¤šç§åŒ»å­¦å›¾åƒæ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ—¢æœ‰æ•ˆåˆé«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹éœ€è¦é«˜æ•ˆä¸”å‡†ç¡®çš„ä»»åŠ¡ä»¥æ»¡è¶³æœ‰æ•ˆè¯Šæ–­çš„éœ€æ±‚ã€‚CNNå’ŒåŸºäºTransformerçš„æ¨¡å‹è™½ç„¶å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚Mambaåœ¨å¤„ç†é•¿åºåˆ—æ—¶å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„èƒ½åŠ›è€Œå¼•èµ·å…³æ³¨ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSpectMambaï¼Œé¦–ä¸ªç”¨äºåŒ»å­¦å›¾åƒæ£€æµ‹çš„Mambaæ¶æ„ã€‚å®ƒé‡‡ç”¨æ··åˆæ—¶ç©ºé¢‘æ³¨æ„ï¼ˆHSFAï¼‰å—ï¼Œåˆ†åˆ«å­¦ä¹ é«˜ä½é¢‘ç‰¹å¾ï¼Œå¹¶ä»‹ç»è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆVSSMï¼‰å’ŒHilbertæ›²çº¿æ‰«ææŠ€æœ¯ï¼Œä¼˜åŒ–Mambaæ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒSpectMambaåœ¨å¤šç§åŒ»å­¦å›¾åƒæ£€æµ‹ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹è¦æ±‚é«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ä»¥æ”¯æŒæœ‰æ•ˆè¯Šæ–­ã€‚</li>
<li>CNNå’ŒåŸºäºTransformerçš„æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒæ£€æµ‹ä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Mambaå› å…¶å¤„ç†é•¿åºåˆ—çš„çº¿æ€§å¤æ‚åº¦èƒ½åŠ›è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>SpectMambaæ˜¯é¦–ä¸ªç”¨äºåŒ»å­¦å›¾åƒæ£€æµ‹çš„Mambaæ¶æ„ã€‚</li>
<li>SpectMambaé‡‡ç”¨æ··åˆæ—¶ç©ºé¢‘æ³¨æ„ï¼ˆHSFAï¼‰å—ï¼Œåˆ†åˆ«å­¦ä¹ é«˜ä½é¢‘ç‰¹å¾ã€‚</li>
<li>è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆVSSMï¼‰å’ŒHilbertæ›²çº¿æ‰«ææŠ€æœ¯ç”¨äºä¼˜åŒ–Mambaæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01080v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01080v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BSNeRF-Broadband-Spectral-Neural-Radiance-Fields-for-Snapshot-Multispectral-Light-field-Imaging"><a href="#BSNeRF-Broadband-Spectral-Neural-Radiance-Fields-for-Snapshot-Multispectral-Light-field-Imaging" class="headerlink" title="BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot   Multispectral Light-field Imaging"></a>BSNeRF: Broadband Spectral Neural Radiance Fields for Snapshot   Multispectral Light-field Imaging</h2><p><strong>Authors:Erqi Huang, John Restrepo, Xun Cao, Ivo Ihrke</strong></p>
<p>Snapshot Multispectral Light-field Imaging (SMLI) is an emerging computational imaging technique that captures high-dimensional data (x, y, z, $\theta$, $\phi$, $\lambda$) in a single shot using a low-dimensional sensor. The accuracy of high-dimensional data reconstruction depends on representing the spectrum using neural radiance field models, which requires consideration of broadband spectral decoupling during optimization. Currently, some SMLI approaches avoid the challenge of model decoupling by either reducing light-throughput or prolonging imaging time. In this work, we propose a broadband spectral neural radiance field (BSNeRF) for SMLI systems. Experiments show that our model successfully decouples a broadband multiplexed spectrum. Consequently, this approach enhances multispectral light-field image reconstruction and further advances plenoptic imaging. </p>
<blockquote>
<p>ç¬æ—¶å¤šå…‰è°±å…‰åœºæˆåƒï¼ˆSMLIï¼‰æ˜¯ä¸€ç§æ–°å…´çš„è®¡ç®—æˆåƒæŠ€æœ¯ï¼Œå®ƒä½¿ç”¨ä½ç»´ä¼ æ„Ÿå™¨åœ¨ä¸€æ¬¡æ‹æ‘„ä¸­æ•è·é«˜ç»´æ•°æ®ï¼ˆxï¼Œyï¼Œzï¼ŒÎ¸ï¼ŒÏ†ï¼ŒÎ»ï¼‰ã€‚é«˜ç»´æ•°æ®é‡å»ºçš„å‡†ç¡®æ€§å–å†³äºä½¿ç”¨ç¥ç»è¾å°„åœºæ¨¡å‹è¡¨ç¤ºå…‰è°±ï¼Œè¿™éœ€è¦åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è€ƒè™‘å®½è°±è§£è€¦ã€‚ç›®å‰ï¼Œä¸€äº›SMLIæ–¹æ³•é€šè¿‡é™ä½å…‰é€šé‡æˆ–å»¶é•¿æˆåƒæ—¶é—´ï¼Œé¿å…æ¨¡å‹è§£è€¦çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºSMLIç³»ç»Ÿæå‡ºäº†å®½è°±ç¥ç»è¾å°„åœºï¼ˆBSNeRFï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æˆåŠŸå®ç°äº†å®½é¢‘å¤šè·¯å¤ç”¨å…‰è°±çš„è§£è€¦ã€‚å› æ­¤ï¼Œæ­¤æ–¹æ³•å¢å¼ºäº†å¤šå…‰è°±å…‰åœºå›¾åƒé‡å»ºï¼Œå¹¶æ¨åŠ¨äº†å…¨å…‰æˆåƒçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.01070v1">PDF</a> Presented in ISCS25</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åä¸ºå¿«ç…§å¤šå…‰è°±å…‰åœºæˆåƒï¼ˆSMLIï¼‰çš„è®¡ç®—æˆåƒæŠ€æœ¯æ­£åœ¨å…´èµ·ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å•æ¬¡æ‹æ‘„ä¸­ä½¿ç”¨ä½ç»´ä¼ æ„Ÿå™¨æ•è·é«˜ç»´æ•°æ®ï¼ˆxï¼Œyï¼Œzï¼ŒÎ¸ï¼ŒÏ†ï¼ŒÎ»ï¼‰ã€‚é«˜ç»´æ•°æ®é‡å»ºçš„å‡†ç¡®æ€§ä¾èµ–äºä½¿ç”¨ç¥ç»è¾å°„åœºæ¨¡å‹è¡¨ç¤ºå…‰è°±ï¼Œè¿™éœ€è¦ä¼˜åŒ–è¿‡ç¨‹ä¸­è€ƒè™‘å®½è°±è§£è€¦ã€‚ç›®å‰ä¸€äº›SMLIæ–¹æ³•é€šè¿‡é™ä½å…‰é€šé‡æˆ–å»¶é•¿æˆåƒæ—¶é—´æ¥é¿å…æ¨¡å‹è§£è€¦çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºSMLIç³»ç»Ÿçš„å®½å¸¦è°±ç¥ç»è¾å°„åœºï¼ˆBSNeRFï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æˆåŠŸå®ç°äº†å®½å¸¦å¤šè·¯å¤ç”¨å…‰è°±çš„è§£è€¦ã€‚å› æ­¤ï¼Œæ­¤æ–¹æ³•æé«˜äº†å¤šå…‰è°±å…‰åœºå›¾åƒçš„é‡å»ºè´¨é‡ï¼Œå¹¶æ¨åŠ¨äº†å…¨å…‰æˆåƒçš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿«ç…§å¤šå…‰è°±å…‰åœºæˆåƒï¼ˆSMLIï¼‰æ˜¯æ–°å…´çš„è®¡ç®—æˆåƒæŠ€æœ¯ã€‚</li>
<li>SMLIæŠ€æœ¯èƒ½å¤Ÿåœ¨å•æ¬¡æ‹æ‘„ä¸­æ•è·é«˜ç»´æ•°æ®ã€‚</li>
<li>é«˜ç»´æ•°æ®é‡å»ºçš„å‡†ç¡®æ€§å—ç¥ç»è¾å°„åœºæ¨¡å‹è¡¨ç¤ºå…‰è°±çš„å½±å“ã€‚</li>
<li>å®½è°±è§£è€¦åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ˜¯å¿…è¦çš„ã€‚</li>
<li>å½“å‰ä¸€äº›SMLIæ–¹æ³•é€šè¿‡é™ä½å…‰é€šé‡æˆ–å»¶é•¿æˆåƒæ—¶é—´æ¥è§£å†³æ¨¡å‹è§£è€¦çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºå®½å¸¦è°±ç¥ç»è¾å°„åœºï¼ˆBSNeRFï¼‰çš„æ–¹æ³•ï¼Œç”¨äºSMLIç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01070v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.01070v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Early-Detection-AI-Based-Five-Year-Forecasting-of-Breast-Cancer-Risk-Using-Digital-Breast-Tomosynthesis-Imaging"><a href="#Towards-Early-Detection-AI-Based-Five-Year-Forecasting-of-Breast-Cancer-Risk-Using-Digital-Breast-Tomosynthesis-Imaging" class="headerlink" title="Towards Early Detection: AI-Based Five-Year Forecasting of Breast Cancer   Risk Using Digital Breast Tomosynthesis Imaging"></a>Towards Early Detection: AI-Based Five-Year Forecasting of Breast Cancer   Risk Using Digital Breast Tomosynthesis Imaging</h2><p><strong>Authors:Manon A. Dorster, Felix J. Dorfner, Mason C. Cleveland, Melisa S. Guelen, Jay Patel, Dania Daye, Jean-Philippe Thiran, Albert E. Kim, Christopher P. Bridge</strong></p>
<p>As early detection of breast cancer strongly favors successful therapeutic outcomes, there is major commercial interest in optimizing breast cancer screening. However, current risk prediction models achieve modest performance and do not incorporate digital breast tomosynthesis (DBT) imaging, which was FDA-approved for breast cancer screening in 2011. To address this unmet need, we present a deep learning (DL)-based framework capable of forecasting an individual patientâ€™s 5-year breast cancer risk directly from screening DBT. Using an unparalleled dataset of 161,753 DBT examinations from 50,590 patients, we trained a risk predictor based on features extracted using the Meta AI DINOv2 image encoder, combined with a cumulative hazard layer, to assess a patientâ€™s likelihood of developing breast cancer over five years. On a held-out test set, our best-performing model achieved an AUROC of 0.80 on predictions within 5 years. These findings reveal the high potential of DBT-based DL approaches to complement traditional risk assessment tools, and serve as a promising basis for additional investigation to validate and enhance our work. </p>
<blockquote>
<p>ç”±äºä¹³è…ºç™Œçš„æ—©æœŸæ£€æµ‹å¯¹æˆåŠŸæ²»ç–—ç»“æœæœ‰ç€æå¤§çš„ç§¯æå½±å“ï¼Œä¼˜åŒ–ä¹³è…ºç™Œç­›æŸ¥æœ‰ç€å·¨å¤§çš„å•†ä¸šä»·å€¼ã€‚ç„¶è€Œï¼Œå½“å‰çš„é£é™©é¢„æµ‹æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå¹¶æœªèå…¥æ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼ˆDBTï¼‰æˆåƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯äº2011å¹´è·å¾—FDAæ‰¹å‡†ç”¨äºä¹³è…ºç™Œç­›æŸ¥ã€‚ä¸ºæ»¡è¶³è¿™ä¸€æœªè¢«æ»¡è¶³çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»ç­›æŸ¥DBTé¢„æµ‹ä¸ªä½“æ‚£è€…5å¹´çš„ä¹³è…ºç™Œé£é™©ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«50,590åæ‚£è€…çš„161,753æ¬¡DBTæ£€æŸ¥ç»„æˆçš„æ— ä¸ä¼¦æ¯”çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä»¥Meta AI DINOv2å›¾åƒç¼–ç å™¨æå–çš„ç‰¹å¾ä¸ºåŸºç¡€ç»“åˆç´¯ç§¯é£é™©å±‚è®­ç»ƒé£é™©é¢„æµ‹å™¨ï¼Œä»¥è¯„ä¼°æ‚£è€…æœªæ¥äº”å¹´å†…æ‚£ä¹³è…ºç™Œçš„å¯èƒ½æ€§ã€‚åœ¨ç‹¬ç«‹çš„æµ‹è¯•é›†ä¸Šï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨äº”å¹´å†…é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†0.8çš„AUROCã€‚è¿™äº›å‘ç°æ­ç¤ºäº†åŸºäºDBTçš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¡¥å……ä¼ ç»Ÿé£é™©è¯„ä¼°å·¥å…·æ–¹é¢çš„é«˜æ½œåŠ›ï¼Œå¹¶ä¸ºæˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯å’Œæ”¹è¿›å·¥ä½œæä¾›äº†æœ‰å‰æ™¯çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00900v1">PDF</a> Deep Breath Workshop, MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ç»“åˆæ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼ˆDBTï¼‰æˆåƒæ¥é¢„æµ‹ä¸ªä½“äº”å¹´å†…ä¹³è…ºç™Œé£é™©çš„ç ”ç©¶ã€‚åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„é£é™©é¢„æµ‹æ¨¡å‹ï¼Œå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ï¼Œæœ‰æœ›ä½œä¸ºä¼ ç»Ÿé£é™©è¯„ä¼°å·¥å…·çš„è¡¥å……ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯å¼ºè°ƒæ—©æœŸæ£€æµ‹å¯¹ä¹³è…ºç™Œæ²»ç–—æˆåŠŸçš„é‡è¦æ€§ï¼Œä»¥åŠå•†ä¸šå¸‚åœºå¯¹ä¼˜åŒ–ä¹³è…ºç™Œç­›æŸ¥çš„å…´è¶£ã€‚</li>
<li>å½“å‰é£é™©é¢„æµ‹æ¨¡å‹æ€§èƒ½æœ‰é™ï¼Œæœªå……åˆ†åˆ©ç”¨æ•°å­—ä¹³è…ºæ–­å±‚åˆæˆï¼ˆDBTï¼‰æˆåƒæŠ€æœ¯ã€‚</li>
<li>é‡‡ç”¨äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œç›´æ¥ä»ç­›æŸ¥DBTå›¾åƒé¢„æµ‹ä¸ªäººäº”å¹´å†…ä¹³è…ºç™Œé£é™©ã€‚</li>
<li>ä½¿ç”¨äº†å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆåŒ…æ‹¬æ¥è‡ªäº”ä¸‡å¤šåæ‚£è€…çš„16ä¸‡å¤šæ¬¡DBTæ£€æŸ¥ï¼‰è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨Meta AI DINOv2å›¾åƒç¼–ç å™¨æå–ç‰¹å¾ï¼Œå¹¶ç»“åˆç´¯ç§¯å±å®³å±‚è¯„ä¼°é£é™©ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„é¢„æµ‹æ€§èƒ½ï¼ˆAUROCä¸º0.80ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00900v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00900v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00900v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Can-General-Purpose-Omnimodels-Compete-with-Specialists-A-Case-Study-in-Medical-Image-Segmentation"><a href="#Can-General-Purpose-Omnimodels-Compete-with-Specialists-A-Case-Study-in-Medical-Image-Segmentation" class="headerlink" title="Can General-Purpose Omnimodels Compete with Specialists? A Case Study in   Medical Image Segmentation"></a>Can General-Purpose Omnimodels Compete with Specialists? A Case Study in   Medical Image Segmentation</h2><p><strong>Authors:Yizhe Zhang, Qiang Chen, Tao Zhou</strong></p>
<p>The emergence of powerful, general-purpose omnimodels capable of processing diverse data modalities has raised a critical question: can these <code>jack-of-all-trades&#39;&#39; systems perform on par with highly specialized models in knowledge-intensive domains? This work investigates this question within the high-stakes field of medical image segmentation. We conduct a comparative study analyzing the zero-shot performance of a state-of-the-art omnimodel (Gemini 2.5 Pro, the </code>Nano Bananaâ€™â€™ model) against domain-specific deep learning models on three distinct tasks: polyp (endoscopy), retinal vessel (fundus), and breast tumor segmentation (ultrasound). Our study focuses on performance at the extremes by curating subsets of the <code>easiest&#39;&#39; and </code>hardestâ€™â€™ cases based on the specialist modelsâ€™ accuracy. Our findings reveal a nuanced and task-dependent landscape. For polyp and breast tumor segmentation, specialist models excel on easy samples, but the omnimodel demonstrates greater robustness on hard samples where specialists fail catastrophically. Conversely, for the fine-grained task of retinal vessel segmentation, the specialist model maintains superior performance across both easy and hard cases. Intriguingly, qualitative analysis suggests omnimodels may possess higher sensitivity, identifying subtle anatomical features missed by human annotators. Our results indicate that while current omnimodels are not yet a universal replacement for specialists, their unique strengths suggest a potential complementary role with specialist models, particularly in enhancing robustness on challenging edge cases. </p>
<blockquote>
<p>é€šç”¨å¤šåŠŸèƒ½å…¨æ¨¡å‹çš„å¼ºå¤§å‡ºç°ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§æ•°æ®æ¨¡æ€ï¼Œå¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šè¿™äº›â€œæ— æ‰€ä¸èƒ½â€çš„ç³»ç»Ÿæ˜¯å¦èƒ½åœ¨çŸ¥è¯†å¯†é›†å‹é¢†åŸŸä¸é«˜åº¦ä¸“ä¸šåŒ–çš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Ÿæœ¬ç ”ç©¶åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²çš„é«˜é£é™©é¢†åŸŸæ¢è®¨äº†è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹æ¯”è¾ƒç ”ç©¶ï¼Œåˆ†æäº†ä¸€ç§æœ€å…ˆè¿›çš„å…¨æ¨¡å‹ï¼ˆGemini 2.5 Proï¼Œâ€œNano Bananaâ€æ¨¡å‹ï¼‰åœ¨ä¸‰ä¸ªä¸åŒä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ï¼šæ¯è‚‰ï¼ˆå†…çª¥é•œï¼‰ã€è§†ç½‘è†œè¡€ç®¡ï¼ˆçœ¼åº•ï¼‰å’Œä¹³è…ºç™Œè‚¿ç˜¤åˆ†å‰²ï¼ˆè¶…å£°ï¼‰ï¼Œå¹¶ä¸ç‰¹å®šé¢†åŸŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶é‡ç‚¹æ˜¯é€šè¿‡åŸºäºä¸“ä¸šæ¨¡å‹çš„å‡†ç¡®æ€§æ•´ç†â€œæœ€å®¹æ˜“â€å’Œâ€œæœ€å›°éš¾â€çš„æ¡ˆä¾‹å­é›†æ¥å…³æ³¨æç«¯æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸€ä¸ªå¾®å¦™ä¸”ä¾èµ–äºä»»åŠ¡çš„æ™¯è§‚ã€‚åœ¨æ¯è‚‰å’Œä¹³è…ºç™Œè‚¿ç˜¤åˆ†å‰²æ–¹é¢ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•æ ·æœ¬ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å…¨æ¨¡å‹åœ¨å›°éš¾æ ·æœ¬ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ï¼Œåœ¨è¿™äº›å›°éš¾æ ·æœ¬ä¸Šä¸“ä¸šæ¨¡å‹ä¼šé­é‡ç¾éš¾æ€§çš„å¤±è´¥ã€‚ç›¸åï¼Œå¯¹äºç²¾ç»†çš„è§†ç½‘è†œè¡€ç®¡åˆ†å‰²ä»»åŠ¡ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•å’Œå›°éš¾æ¡ˆä¾‹ä¸­å‡ä¿æŒå“è¶Šæ€§èƒ½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå®šæ€§åˆ†æè¡¨æ˜å…¨æ¨¡å‹å¯èƒ½å…·æœ‰æ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºäººç±»æ³¨é‡Šå™¨é—æ¼çš„ç»†å¾®è§£å‰–ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„å…¨æ¨¡å‹å°šæœªæˆä¸ºä¸“ä¸šæ¨¡å‹çš„é€šç”¨æ›¿ä»£å“ï¼Œä½†å®ƒä»¬ç‹¬ç‰¹çš„ä¼˜åŠ¿è¡¨æ˜å¯èƒ½ä¸ä¸“ä¸šæ¨¡å‹äº’è¡¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜æŒ‘æˆ˜æ€§è¾¹ç¼˜æ¡ˆä¾‹çš„ç¨³å¥æ€§æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00866v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–°å…´çš„å¼ºå¤§é€šç”¨å‹å…¨èƒ½æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæ˜¯å¦å¯ä¸é«˜åº¦ä¸“ä¸šåŒ–çš„æ¨¡å‹ç›¸æå¹¶è®ºçš„é—®é¢˜ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„å…¨èƒ½æ¨¡å‹ï¼ˆGemini 2.5 Proï¼Œâ€œNano Bananaâ€æ¨¡å‹ï¼‰ä¸ç‰¹å®šé¢†åŸŸçš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¸‰ä¸ªä¸åŒä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½è¿›è¡Œæ¯”è¾ƒç ”ç©¶ï¼Œå‘ç°ç»“æœå‘ˆç°å‡ºå¾®å¦™çš„ã€ä»»åŠ¡ä¾èµ–æ€§çš„æ ¼å±€ã€‚åœ¨æ¯è‚‰å’Œä¹³è…ºç™Œåˆ†å‰²æ–¹é¢ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•æ ·æœ¬ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œå…¨èƒ½æ¨¡å‹åœ¨å›°éš¾æ ·æœ¬ä¸Šå±•ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ï¼Œä¸“ä¸šæ¨¡å‹ä¼šé­é‡ç¾éš¾æ€§çš„å¤±è´¥ã€‚ç›¸åï¼Œå¯¹äºç²¾ç»†çš„è§†ç½‘è†œè¡€ç®¡åˆ†å‰²ä»»åŠ¡ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•å’Œå›°éš¾æ¡ˆä¾‹ä¸­å‡ä¿æŒå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®šæ€§åˆ†ææ˜¾ç¤ºå…¨èƒ½æ¨¡å‹å¯èƒ½å…·æœ‰è¾ƒé«˜çš„æ•æ„Ÿæ€§ï¼Œèƒ½å¤Ÿè¯†åˆ«å‡ºäººç±»æ³¨é‡Šå™¨é—æ¼çš„ç»†å¾®è§£å‰–ç‰¹å¾ã€‚å› æ­¤ï¼Œå°½ç®¡å…¨èƒ½æ¨¡å‹å°šæœªæˆä¸ºå–ä»£ä¸“å®¶çš„æ™®éé€‰æ‹©ï¼Œä½†å…¶ç‹¬ç‰¹ä¼˜åŠ¿è¡¨æ˜å…¶ä½œä¸ºä¸“å®¶æ¨¡å‹çš„è¡¥å……è§’è‰²å…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜æŒ‘æˆ˜æ€§è¾¹ç¼˜æ¡ˆä¾‹çš„ç¨³å¥æ€§æ–¹é¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…¨èƒ½æ¨¡å‹ä¸ä¸“ä¸šæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„æ€§èƒ½å¯¹æ¯”æ˜¯ä¸€ä¸ªå…³é”®ç ”ç©¶é—®é¢˜ã€‚</li>
<li>åœ¨æ¯è‚‰å’Œä¹³è…ºç™Œåˆ†å‰²æ–¹é¢ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•æ ·æœ¬ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè€Œå…¨èƒ½æ¨¡å‹åœ¨å›°éš¾æ ·æœ¬ä¸Šæ›´ç¨³å¥ã€‚</li>
<li>å¯¹äºè§†ç½‘è†œè¡€ç®¡åˆ†å‰²çš„ç²¾ç»†ä»»åŠ¡ï¼Œä¸“ä¸šæ¨¡å‹åœ¨ç®€å•å’Œå›°éš¾æƒ…å†µä¸‹å‡è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>å…¨èƒ½æ¨¡å‹å¯èƒ½å…·æœ‰è¾ƒé«˜çš„æ•æ„Ÿæ€§ï¼Œèƒ½è¯†åˆ«å‡ºäººç±»æ³¨é‡Šå™¨é—æ¼çš„ç»†å¾®è§£å‰–ç‰¹å¾ã€‚</li>
<li>å…¨èƒ½æ¨¡å‹å°šæœªæ™®éå–ä»£ä¸“ä¸šæ¨¡å‹ï¼Œä½†åœ¨å¤„ç†æŒ‘æˆ˜æ€§æ¡ˆä¾‹æ—¶å…·æœ‰æ½œåœ¨çš„è¡¥å……ä½œç”¨ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸åŒä»»åŠ¡ä¸­æ¨¡å‹æ€§èƒ½çš„å·®å¼‚æ€§ï¼Œè¡¨æ˜éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡é€‰æ‹©é€‚å½“çš„æ¨¡å‹ã€‚</li>
<li>å®šæ€§åˆ†æä¸å®šé‡è¯„ä¼°ç›¸ç»“åˆå¯¹äºå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00866v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00866v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00866v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Encoder-Only-Image-Registration"><a href="#Encoder-Only-Image-Registration" class="headerlink" title="Encoder-Only Image Registration"></a>Encoder-Only Image Registration</h2><p><strong>Authors:Xiang Chen, Renjiu Hu, Jinwei Zhang, Yuxi Zhang, Xinyao Yue, Min Liu, Yaonan Wang, Hang Zhang</strong></p>
<p>Learning-based techniques have significantly improved the accuracy and speed of deformable image registration. However, challenges such as reducing computational complexity and handling large deformations persist. To address these challenges, we analyze how convolutional neural networks (ConvNets) influence registration performance using the Horn-Schunck optical flow equation. Supported by prior studies and our empirical experiments, we observe that ConvNets play two key roles in registration: linearizing local intensities and harmonizing global contrast variations. Based on these insights, we propose the Encoder-Only Image Registration (EOIR) framework, designed to achieve a better accuracy-efficiency trade-off. EOIR separates feature learning from flow estimation, employing only a 3-layer ConvNet for feature extraction and a set of 3-layer flow estimators to construct a Laplacian feature pyramid, progressively composing diffeomorphic deformations under a large-deformation model. Results on five datasets across different modalities and anatomical regions demonstrate EOIRâ€™s effectiveness, achieving superior accuracy-efficiency and accuracy-smoothness trade-offs. With comparable accuracy, EOIR provides better efficiency and smoothness, and vice versa. The source code of EOIR is publicly available on <a target="_blank" rel="noopener" href="https://github.com/XiangChen1994/EOIR">https://github.com/XiangChen1994/EOIR</a>. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„æŠ€æœ¯æ˜¾è‘—æé«˜äº†å¯å˜å½¢å›¾åƒé…å‡†çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚ç„¶è€Œï¼Œå‡å°‘è®¡ç®—å¤æ‚æ€§å’Œå¤„ç†å¤§å˜å½¢ç­‰æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ†æäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰å¦‚ä½•ä½¿ç”¨Horn-Schunckå…‰æµæ–¹ç¨‹å½±å“é…å‡†æ€§èƒ½ã€‚é€šè¿‡å…ˆå‰çš„ç ”ç©¶å’Œæˆ‘ä»¬çš„ç»éªŒå®éªŒï¼Œæˆ‘ä»¬å‘ç°ConvNetsåœ¨é…å‡†ä¸­æ‰®æ¼”ä¸¤ä¸ªå…³é”®è§’è‰²ï¼šçº¿æ€§åŒ–å±€éƒ¨å¼ºåº¦å’Œåè°ƒå…¨å±€å¯¹æ¯”åº¦å˜åŒ–ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä»…ç¼–ç å™¨å›¾åƒé…å‡†ï¼ˆEOIRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ›´å¥½çš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ã€‚EOIRå°†ç‰¹å¾å­¦ä¹ ä»æµä¼°è®¡ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä»…ä½¿ç”¨3å±‚ConvNetè¿›è¡Œç‰¹å¾æå–å’Œä¸€ç»„3å±‚æµä¼°è®¡å™¨æ¥æ„å»ºæ‹‰æ™®æ‹‰æ–¯ç‰¹å¾é‡‘å­—å¡”ï¼Œåœ¨å¤§å˜å½¢æ¨¡å‹ä¸‹é€æ­¥ç»„æˆå¾®åˆ†åŒèƒšå˜å½¢ã€‚åœ¨äº”ä¸ªä¸åŒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸçš„æ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒEOIRçš„æœ‰æ•ˆæ€§å®ç°äº†ä¼˜è¶Šçš„å‡†ç¡®åº¦-æ•ˆç‡å’Œå‡†ç¡®åº¦-å¹³æ»‘åº¦æƒè¡¡ã€‚åœ¨å…·æœ‰å¯æ¯”çš„å‡†ç¡®åº¦çš„æƒ…å†µä¸‹ï¼ŒEOIRæä¾›äº†æ›´å¥½çš„æ•ˆç‡å’Œå¹³æ»‘åº¦ï¼Œåä¹‹äº¦ç„¶ã€‚EOIRçš„æºä»£ç å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/XiangChen1994/EOIR%E3%80%82">https://github.com/XiangChen1994/EOIRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00451v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨å¯å˜å½¢å›¾åƒé…å‡†æ–¹é¢æ˜¾è‘—æé«˜å‡†ç¡®æ€§å’Œé€Ÿåº¦ï¼Œä½†ä»æœ‰æŒ‘æˆ˜å¦‚å‡å°‘è®¡ç®—å¤æ‚æ€§åŠå¤„ç†å¤§å˜å½¢çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡åˆ†æå·ç§¯ç¥ç»ç½‘ç»œå¯¹åŸºäºHorn-Schunckå…‰æµæ–¹ç¨‹æ³¨å†Œæ€§èƒ½çš„å½±å“ï¼Œæˆ‘ä»¬å‘ç°ConvNetsæ‰®æ¼”äº†å…³é”®è§’è‰²ï¼Œå®ƒä»¬æœ‰åŠ©äºå®ç°æœ¬åœ°å¼ºåº¦çš„çº¿æ€§åŒ–å’Œå…¨å±€å¯¹æ¯”åº¦å˜åŒ–çš„è°ƒå’Œã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†åªé‡‡ç”¨Encoderçš„å›¾åƒæ³¨å†Œï¼ˆEOIRï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ›´ä½³çš„ç²¾å‡†åº¦ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚EOIRé€šè¿‡å°†ç‰¹å¾å­¦ä¹ ä¸æµåŠ¨ä¼°è®¡åˆ†å¼€ï¼Œä½¿ç”¨ä»…åŒ…å«ä¸‰å±‚ConvNetçš„ç‰¹å¾æå–å™¨å’Œä¸€ç»„ä¸‰å±‚æµåŠ¨ä¼°è®¡å™¨æ„å»ºæ‹‰æ™®æ‹‰æ–¯ç‰¹å¾é‡‘å­—å¡”ï¼Œåœ¨å¤§å˜å½¢æ¨¡å‹ä¸‹é€æ­¥ç»„åˆå¾®åˆ†åŒèƒšå˜å½¢ã€‚åœ¨äº”ç»„ä¸åŒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸçš„æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜EOIRçš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†å‡ºè‰²çš„ç²¾å‡†åº¦ä¸æ•ˆç‡å’Œç²¾å‡†åº¦ä¸å¹³æ»‘åº¦çš„å¹³è¡¡ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiangChen1994/EOIR">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŸºäºå­¦ä¹ çš„æ–¹æ³•å¢å¼ºäº†å¯å˜å½¢å›¾åƒé…å‡†çš„å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚</li>
<li>é¢ä¸´å‡å°‘è®¡ç®—å¤æ‚æ€§åŠå¤„ç†å¤§å˜å½¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvNetsï¼‰åœ¨å›¾åƒæ³¨å†Œä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œçº¿æ€§åŒ–å±€éƒ¨å¼ºåº¦å¹¶è°ƒå’Œå…¨å±€å¯¹æ¯”åº¦å˜åŒ–ã€‚</li>
<li>æå‡ºäº†Encoder-Only Image Registration (EOIR)æ¡†æ¶ï¼Œå®ç°äº†ç²¾å‡†åº¦ä¸æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>EOIRæ¡†æ¶é€šè¿‡åˆ†ç¦»ç‰¹å¾å­¦ä¹ ä¸æµåŠ¨ä¼°è®¡ï¼Œä½¿ç”¨ä¸‰å±‚ConvNetå’ŒæµåŠ¨ä¼°è®¡å™¨æ„å»ºæ‹‰æ™®æ‹‰æ–¯ç‰¹å¾é‡‘å­—å¡”ã€‚</li>
<li>EOIRåœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†ç²¾å‡†åº¦ã€æ•ˆç‡å’Œå¹³æ»‘åº¦çš„è‰¯å¥½å¹³è¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00451v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00451v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00451v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00451v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00451v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-and-Multi-centric-Head-and-Neck-Cancer-Dataset-for-Tumor-Segmentation-and-Outcome-Prediction"><a href="#A-Multimodal-and-Multi-centric-Head-and-Neck-Cancer-Dataset-for-Tumor-Segmentation-and-Outcome-Prediction" class="headerlink" title="A Multimodal and Multi-centric Head and Neck Cancer Dataset for Tumor   Segmentation and Outcome Prediction"></a>A Multimodal and Multi-centric Head and Neck Cancer Dataset for Tumor   Segmentation and Outcome Prediction</h2><p><strong>Authors:Numan Saeed, Salma Hassan, Shahad Hardan, Ahmed Aly, Darya Taratynova, Umair Nawaz, Ufaq Khan, Muhammad Ridzuan, Vincent Andrearczyk, Adrien Depeursinge, Mathieu Hatt, Thomas Eugene, RaphaÃ«l Metz, MÃ©lanie Dore, Gregory Delpon, Vijay Ram Kumar Papineni, Kareem Wahid, Cem Dede, Alaa Mohamed Shawky Ali, Carlos Sjogreen, Mohamed Naser, Clifton D. Fuller, Valentin Oreiller, Mario Jreige, John O. Prior, Catherine Cheze Le Rest, Olena Tankyevych, Pierre Decazes, Su Ruan, Stephanie Tanadini-Lang, Martin ValliÃ¨res, Hesham Elhalawani, Ronan Abgral, Romain Floch, Kevin Kerleguer, Ulrike Schick, Maelle Mauguen, Arman Rahmim, Mohammad Yaqub</strong></p>
<p>We describe a publicly available multimodal dataset of annotated Positron Emission Tomography&#x2F;Computed Tomography (PET&#x2F;CT) studies for head and neck cancer research. The dataset includes 1123 FDG-PET&#x2F;CT studies from patients with histologically confirmed head and neck cancer, acquired from 10 international medical centers. All examinations consisted of co-registered PET&#x2F;CT scans with varying acquisition protocols, reflecting real-world clinical diversity across institutions. Primary gross tumor volumes (GTVp) and involved lymph nodes (GTVn) were manually segmented by experienced radiation oncologists and radiologists following standardized guidelines and quality control measures. We provide anonymized NifTi files of all studies, along with expert-annotated segmentation masks, radiotherapy dose distribution for a subset of patients, and comprehensive clinical metadata. This metadata includes TNM staging, HPV status, demographics (age and gender), long-term follow-up outcomes, survival times, censoring indicators, and treatment information. We demonstrate how this dataset can be used for three key clinical tasks: automated tumor segmentation, recurrence-free survival prediction, and HPV status classification, providing benchmark results using state-of-the-art deep learning models, including UNet, SegResNet, and multimodal prognostic frameworks. </p>
<blockquote>
<p>æˆ‘ä»¬æè¿°äº†ä¸€ä¸ªå…¬å…±å¯ç”¨çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†ç»ç»„ç»‡ç—…ç†å­¦è¯å®å¤´é¢ˆç™Œæ‚£è€…çš„Â¹Â¹Â¹Â³C PET&#x2F;CTæ‰«æç ”ç©¶æ•°æ®ï¼Œæ¶µç›–äº†ä»åä¸ªå›½é™…åŒ»å­¦ä¸­å¿ƒçš„ä¸€åƒä¸€ç™¾äºŒåä¸‰é¡¹æ£€æŸ¥ç ”ç©¶ã€‚æ‰€æœ‰çš„æ£€æŸ¥å‡åŒ…å«åŒæ—¶æ³¨å†Œåçš„PET&#x2F;CTæ‰«æï¼Œé‡‡ç”¨äº†ä¸åŒçš„é‡‡é›†åè®®ï¼Œåæ˜ äº†æœºæ„é—´çœŸå®ä¸–ç•Œçš„ä¸´åºŠå¤šæ ·æ€§ã€‚åŸå‘è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVpï¼‰å’Œå—ç´¯æ·‹å·´ç»“ï¼ˆGTVnï¼‰æ˜¯ç”±ç»éªŒä¸°å¯Œçš„æ”¾å°„è‚¿ç˜¤å­¦å®¶å’Œæ”¾å°„ç§‘åŒ»ç”ŸæŒ‰ç…§æ ‡å‡†åŒ–å‡†åˆ™å’Œè´¨é‡ä¿è¯æªæ–½è¿›è¡Œæ‰‹åŠ¨åˆ†å‰²çš„ã€‚æˆ‘ä»¬æä¾›äº†æ‰€æœ‰ç ”ç©¶çš„åŒ¿ååŒ–NifTiæ–‡ä»¶ï¼Œä»¥åŠä¸“å®¶æ³¨é‡Šçš„åˆ†å‰²æ©æ¨¡ã€éƒ¨åˆ†æ‚£è€…çš„æ”¾å°„æ²»ç–—å‰‚é‡åˆ†å¸ƒå’Œå…¨é¢çš„ä¸´åºŠå…ƒæ•°æ®ã€‚è¿™äº›å…ƒæ•°æ®åŒ…æ‹¬TNMåˆ†æœŸã€HPVçŠ¶æ€ã€äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ï¼ˆå¹´é¾„å’Œæ€§åˆ«ï¼‰ã€é•¿æœŸéšè®¿ç»“æœã€ç”Ÿå­˜æ—¶é—´ã€å®¡æŸ¥æŒ‡æ ‡å’Œæ²»ç–—ä¿¡æ¯ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨ä¸‰é¡¹å…³é”®ä¸´åºŠä»»åŠ¡ä¸­çš„åº”ç”¨ï¼šè‡ªåŠ¨è‚¿ç˜¤åˆ†å‰²ã€æ— å¤å‘ç”Ÿå­˜é¢„æµ‹å’ŒHPVçŠ¶æ€åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬UNetã€SegResNetå’Œå¤šæ¨¡æ€é¢„åæ¡†æ¶ç­‰ï¼Œä¸ºè¯¥æ•°æ®é›†æä¾›äº†åŸºå‡†æµ‹è¯•ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00367v2">PDF</a> 10 pages, 5 figures. Numan Saeed is the corresponding author. Numan   Saeed, Salma Hassan and Shahad Hardan contributed equally to this work.   Project page: <a target="_blank" rel="noopener" href="https://hecktor25.grand-challenge.org/">https://hecktor25.grand-challenge.org/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«ç»ç»„ç»‡ç—…ç†å­¦è¯å®å¤´é¢ˆéƒ¨ç™Œç—‡æ‚£è€…çš„PET&#x2F;CTç ”ç©¶ã€‚æ•°æ®é›†ç”±æ¥è‡ªå›½é™…åŒ»ç–—ä¸­å¿ƒçš„PET&#x2F;CTæ‰«æç»„æˆï¼ŒåŒ…å«æ‰‹åŠ¨åˆ†å‰²çš„ä¸»è¦è‚¿ç˜¤ä½“ç§¯å’Œæ¶‰åŠçš„æ·‹å·´ç»“ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†åŒ¿ååŒ–çš„NifTiæ–‡ä»¶ã€ä¸“å®¶æ³¨é‡Šçš„åˆ†å‰²æ©æ¨¡ã€éƒ¨åˆ†æ‚£è€…çš„æ”¾ç–—å‰‚é‡åˆ†å¸ƒå’Œå…¨é¢çš„ä¸´åºŠå…ƒæ•°æ®ã€‚æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè‡ªåŠ¨è‚¿ç˜¤åˆ†å‰²ã€æ— å¤å‘ç”Ÿå­˜é¢„æµ‹å’ŒHPVçŠ¶æ€åˆ†ç±»ç­‰ä¸‰é¡¹å…³é”®ä¸´åºŠä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å‰æ²¿çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æä¾›åŸºå‡†æµ‹è¯•ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æè¿°äº†ä¸€ä¸ªå…¬å¼€çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…å«å¤´é¢ˆéƒ¨ç™Œç—‡æ‚£è€…çš„PET&#x2F;CTç ”ç©¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ªä¸åŒå›½é™…åŒ»ç–—ä¸­å¿ƒçš„å¼‚æ„å›¾åƒï¼Œåæ˜ äº†çœŸå®ä¸–ç•Œçš„ä¸´åºŠå¤šæ ·æ€§ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº†è‚¿ç˜¤å’Œæ·‹å·´ç»“çš„æ‰‹åŠ¨åˆ†å‰²ï¼Œä»¥åŠè¯¦ç»†çš„ä¸´åºŠå…ƒæ•°æ®ã€‚</li>
<li>æä¾›äº†åŒ¿ååŒ–çš„å›¾åƒæ–‡ä»¶å’Œä¸“å®¶æ³¨é‡Šçš„åˆ†å‰²æ©æ¨¡ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºè‡ªåŠ¨è‚¿ç˜¤åˆ†å‰²ã€æ— å¤å‘ç”Ÿå­˜é¢„æµ‹å’ŒHPVçŠ¶æ€åˆ†ç±»ç­‰ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚UNetã€SegResNetå’Œå¤šæ¨¡æ€é¢„åæ¡†æ¶ï¼‰ä¸ºè¿™äº›ä»»åŠ¡æä¾›äº†åŸºå‡†æµ‹è¯•ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00367v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MorphGen-Morphology-Guided-Representation-Learning-for-Robust-Single-Domain-Generalization-in-Histopathological-Cancer-Classification"><a href="#MorphGen-Morphology-Guided-Representation-Learning-for-Robust-Single-Domain-Generalization-in-Histopathological-Cancer-Classification" class="headerlink" title="MorphGen: Morphology-Guided Representation Learning for Robust   Single-Domain Generalization in Histopathological Cancer Classification"></a>MorphGen: Morphology-Guided Representation Learning for Robust   Single-Domain Generalization in Histopathological Cancer Classification</h2><p><strong>Authors:Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, Jia Wu</strong></p>
<p>Domain generalization in computational histopathology is hindered by heterogeneity in whole slide images (WSIs), caused by variations in tissue preparation, staining, and imaging conditions across institutions. Unlike machine learning systems, pathologists rely on domain-invariant morphological cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia, chromatin texture, spatial disorganization), structural atypia (abnormal architecture and gland formation), and overall morphological atypia that remain diagnostic across diverse settings. Motivated by this, we hypothesize that explicitly modeling biologically robust nuclear morphology and spatial organization will enable the learning of cancer representations that are resilient to domain shifts. We propose MorphGen (Morphology-Guided Generalization), a method that integrates histopathology images, augmentations, and nuclear segmentation masks within a supervised contrastive learning framework. By aligning latent representations of images and nuclear masks, MorphGen prioritizes diagnostic features such as nuclear and morphological atypia and spatial organization over staining artifacts and domain-specific features. To further enhance out-of-distribution robustness, we incorporate stochastic weight averaging (SWA), steering optimization toward flatter minima. Attention map analyses revealed that MorphGen primarily relies on nuclear morphology, cellular composition, and spatial cell organization within tumors or normal regions for final classification. Finally, we demonstrate resilience of the learned representations to image corruptions (such as staining artifacts) and adversarial attacks, showcasing not only OOD generalization but also addressing critical vulnerabilities in current deep learning systems for digital pathology. Code, datasets, and trained models are available at: <a target="_blank" rel="noopener" href="https://github.com/hikmatkhan/MorphGen">https://github.com/hikmatkhan/MorphGen</a> </p>
<blockquote>
<p>åœ¨è®¡ç®—ç—…ç†å­¦ä¸­ï¼Œé¢†åŸŸæ³›åŒ–å—åˆ°å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰å¼‚è´¨æ€§çš„é˜»ç¢ï¼Œè¿™ç§å¼‚è´¨æ€§æ˜¯ç”±ä¸åŒæœºæ„åœ¨ç»„ç»‡åˆ¶å¤‡ã€æŸ“è‰²å’Œæˆåƒæ¡ä»¶ä¸Šçš„å˜åŒ–å¼•èµ·çš„ã€‚ç—…ç†åŒ»å¸ˆä¸åŒäºæœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œä»–ä»¬ä¾èµ–äºè·¨ä¸åŒè®¾ç½®ä»ç„¶å…·æœ‰è¯Šæ–­æ€§çš„é¢†åŸŸä¸å˜å½¢æ€çº¿ç´¢ï¼Œå¦‚æ ¸å¼‚å‹æ€§ï¼ˆå¢å¤§ã€è½®å»“ä¸è§„åˆ™ã€è¶…æŸ“è‰²ã€æŸ“è‰²è´¨çº¹ç†ã€ç©ºé—´æ— åºï¼‰ã€ç»“æ„å¼‚å‹æ€§ï¼ˆå¼‚å¸¸ç»“æ„å’Œè…ºä½“å½¢æˆï¼‰ä»¥åŠæ•´ä½“å½¢æ€å¼‚å‹æ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾é€šè¿‡æ˜¾å¼å»ºæ¨¡ç”Ÿç‰©å­¦ä¸Šç¨³å¥çš„æ ¸å½¢æ€å’Œç©ºé—´ç»„ç»‡ï¼Œå°†èƒ½å¤Ÿå­¦ä¹ å¯¹é¢†åŸŸå˜åŒ–å…·æœ‰å¼¹æ€§çš„ç™Œç—‡è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº†MorphGenï¼ˆå½¢æ€å­¦å¼•å¯¼æ³›åŒ–ï¼‰æ–¹æ³•ï¼Œå®ƒåœ¨ä¸€ä¸ªæœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶å†…æ•´åˆäº†ç—…ç†ç»„ç»‡å­¦å›¾åƒã€å¢å¼ºå›¾åƒå’Œæ ¸åˆ†å‰²æ©è†œã€‚é€šè¿‡å¯¹é½å›¾åƒå’Œæ ¸æ©è†œçš„æ½œåœ¨è¡¨ç¤ºï¼ŒMorphGenä¼˜å…ˆè€ƒè™‘æ ¸å’Œå½¢æ€å¼‚å‹æ€§ä»¥åŠç©ºé—´ç»„ç»‡ç­‰è¯Šæ–­ç‰¹å¾ï¼Œè€Œä¸æ˜¯æŸ“è‰²ä¼ªå½±å’Œé¢†åŸŸç‰¹å®šç‰¹å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆ†å¸ƒå¤–çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†éšæœºæƒé‡å¹³å‡ï¼ˆSWAï¼‰ï¼Œå°†ä¼˜åŒ–è½¬å‘è¾ƒå¹³å¦çš„æœ€å°å€¼ã€‚æ³¨æ„åŠ›å›¾åˆ†æè¡¨æ˜ï¼ŒMorphGenä¸»è¦ä¾èµ–äºæ ¸å½¢æ€ã€è‚¿ç˜¤å†…æˆ–æ­£å¸¸åŒºåŸŸçš„ç»†èƒç»„æˆå’Œç»†èƒç©ºé—´ç»„ç»‡æ¥è¿›è¡Œæœ€ç»ˆåˆ†ç±»ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€å­¦è¡¨ç¤ºçš„éŸ§æ€§èƒ½å¤Ÿå¯¹æŠ—å›¾åƒè…èš€ï¼ˆå¦‚æŸ“è‰²ä¼ªå½±ï¼‰å’Œå¯¹æŠ—æ€§æ”»å‡»ï¼Œå±•ç¤ºäº†å…¶åœ¨é¢†åŸŸå¤–çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è§£å†³äº†å½“å‰æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨æ•°å­—ç—…ç†å­¦ä¸­çš„å…³é”®è„†å¼±æ€§ã€‚ä»£ç ã€æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/hikmatkhan/MorphGen">https://github.com/hikmatkhan/MorphGen</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00311v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨è®¡ç®—ç»„ç»‡ç—…ç†å­¦ä¸­çš„é¢†åŸŸæ³›åŒ–é—®é¢˜ï¼ŒæŒ‡å‡ºå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„å¼‚è´¨æ€§æ˜¯é˜»ç¢å› ç´ ä¹‹ä¸€ã€‚æ–‡ç« æå‡ºMorphGenæ–¹æ³•ï¼Œé€šè¿‡æ•´åˆç»„ç»‡ç—…ç†å­¦å›¾åƒã€å¢å¼ºæŠ€æœ¯å’Œæ ¸åˆ†æ®µæ©è†œï¼Œåœ¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­æ˜¾å¼å»ºæ¨¡ç”Ÿç‰©å­¦ç¨³å®šçš„æ ¸å½¢æ€å’Œç©ºé—´ç»„ç»‡ï¼Œä»¥æé«˜ç™Œç—‡è¡¨ç¤ºçš„åŸŸè½¬ç§»æŠ—æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥éšæœºæƒé‡å¹³å‡ï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚MorphGenä¸»è¦ä¾èµ–æ ¸å½¢æ€å­¦ã€ç»†èƒç»„æˆå’Œè‚¿ç˜¤æˆ–æ­£å¸¸åŒºåŸŸçš„ç»†èƒç©ºé—´ç»„ç»‡è¿›è¡Œåˆ†ç±»ã€‚è¯¥æ–¹æ³•ä¸ä»…å±•ç¤ºäº†å¯¹å›¾åƒè…èš€å’Œå¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ï¼Œè¿˜è§£å†³äº†å½“å‰æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨æ•°å­—ç—…ç†å­¦ä¸­çš„å…³é”®è„†å¼±æ€§é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç»„ç»‡ç—…ç†å­¦ä¸­çš„é¢†åŸŸæ³›åŒ–å—åˆ°å…¨å¹»ç¯ç‰‡å›¾åƒå¼‚è´¨æ€§çš„é˜»ç¢ã€‚</li>
<li>æ ¸å½¢æ€å’Œç©ºé—´ç»„ç»‡æ˜¯ç—…ç†è¯Šæ–­ä¸­è·¨è¶Šä¸åŒè®¾ç½®çš„é‡è¦ä¸å˜ç‰¹å¾ã€‚</li>
<li>MorphGenæ–¹æ³•é€šè¿‡æ•´åˆå›¾åƒã€å¢å¼ºæŠ€æœ¯å’Œæ ¸åˆ†æ®µæ©è†œï¼Œåœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­å»ºæ¨¡è¿™äº›ç‰¹å¾ã€‚</li>
<li>ç›‘ç£å¯¹æ¯”å­¦ä¹ å¼ºè°ƒè¯Šæ–­ç‰¹å¾ï¼Œå¦‚æ ¸å’Œå½¢æ€çš„ä¸å…¸å‹æ€§ä»¥åŠç©ºé—´ç»„ç»‡ï¼ŒåŒæ—¶å¿½ç•¥æŸ“è‰²ä¼ªå½±å’Œç‰¹å®šåŸŸç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¼•å…¥éšæœºæƒé‡å¹³å‡ï¼ˆSWAï¼‰ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ³¨æ„åŠ›å›¾åˆ†ææ˜¾ç¤ºMorphGenä¸»è¦ä¾èµ–æ ¸å½¢æ€å­¦ã€ç»†èƒç»„æˆå’Œè‚¿ç˜¤æˆ–æ­£å¸¸åŒºåŸŸçš„ç»†èƒç©ºé—´ç»„ç»‡è¿›è¡Œåˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00311v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00311v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multimodal-Deep-Learning-for-Phyllodes-Tumor-Classification-from-Ultrasound-and-Clinical-Data"><a href="#Multimodal-Deep-Learning-for-Phyllodes-Tumor-Classification-from-Ultrasound-and-Clinical-Data" class="headerlink" title="Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data"></a>Multimodal Deep Learning for Phyllodes Tumor Classification from   Ultrasound and Clinical Data</h2><p><strong>Authors:Farhan Fuad Abir, Abigail Elliott Daly, Kyle Anderman, Tolga Ozmen, Laura J. Brattain</strong></p>
<p>Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are difficult to classify preoperatively due to their radiological similarity to benign fibroadenomas. This often leads to unnecessary surgical excisions. To address this, we propose a multimodal deep learning framework that integrates breast ultrasound (BUS) images with structured clinical data to improve diagnostic accuracy. We developed a dual-branch neural network that extracts and fuses features from ultrasound images and patient metadata from 81 subjects with confirmed PTs. Class-aware sampling and subject-stratified 5-fold cross-validation were applied to prevent class imbalance and data leakage. The results show that our proposed multimodal method outperforms unimodal baselines in classifying benign versus borderline&#x2F;malignant PTs. Among six image encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and 0.7294, respectively. This study demonstrates the potential of multimodal AI to serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and improving clinical decision-making in breast tumor management. </p>
<blockquote>
<p>å¶çŠ¶è‚¿ç˜¤ï¼ˆPTsï¼‰æ˜¯ä¸€ç§ç½•è§çš„ä¹³è…ºçº¤ç»´ä¸Šçš®æ€§ç—…å˜ï¼Œç”±äºå…¶æ”¾å°„å­¦ä¸è‰¯æ€§çº¤ç»´è…ºç˜¤ç›¸ä¼¼ï¼Œæœ¯å‰éš¾ä»¥åˆ†ç±»ã€‚è¿™å¸¸å¸¸å¯¼è‡´ä¸å¿…è¦çš„æ‰‹æœ¯åˆ‡é™¤ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¹³è…ºè¶…å£°ï¼ˆBUSï¼‰å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒåˆ†æ”¯ç¥ç»ç½‘ç»œï¼Œä»è¶…å£°å›¾åƒå’Œæ¥è‡ª81åå·²ç¡®è¯ŠPTsæ‚£è€…çš„æ‚£è€…å…ƒæ•°æ®ä¸­æå–å¹¶èåˆç‰¹å¾ã€‚åº”ç”¨ç±»åˆ«æ„ŸçŸ¥é‡‡æ ·å’Œåˆ†å±‚5æŠ˜äº¤å‰éªŒè¯ï¼Œä»¥é˜²æ­¢ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®æ³„éœ²ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å¤šæ¨¡å¼æ–¹æ³•åœ¨åˆ†ç±»è‰¯æ€§ä¸è¾¹ç•Œæ€§æˆ–æ¶æ€§PTsæ—¶ä¼˜äºå•æ¨¡å¼åŸºçº¿ã€‚åœ¨å…­ç§å›¾åƒç¼–ç å™¨ä¸­ï¼ŒConvNeXtå’ŒResNet18åœ¨å¤šæ¨¡å¼è®¾ç½®ä¸­è¡¨ç°æœ€ä½³ï¼ŒAUC-ROCå¾—åˆ†åˆ†åˆ«ä¸º0.9427å’Œ0.9349ï¼ŒF1å¾—åˆ†åˆ†åˆ«ä¸º0.6720å’Œ0.7294ã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡å¼äººå·¥æ™ºèƒ½ä½œä¸ºéä¾µå…¥æ€§è¯Šæ–­å·¥å…·çš„æ½œåŠ›ï¼Œå¯ä»¥å‡å°‘ä¸å¿…è¦çš„æ´»æ£€ï¼Œæé«˜ä¹³è…ºè‚¿ç˜¤ç®¡ç†çš„ä¸´åºŠå†³ç­–æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00213v1">PDF</a> IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS   BSN 2025)</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¹³è…ºè¶…å£°å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶å¼€å‘äº†åŒåˆ†æ”¯ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿä»ç¡®è¯Šçš„ç—…ä¾‹ä¸­æå–å’Œèåˆæ¥è‡ªè¶…å£°å›¾åƒå’Œæ‚£è€…å…ƒæ•°æ®çš„ç‰¹å¾ã€‚è¯¥ç ”ç©¶çš„ç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€æ–¹æ³•ç›¸è¾ƒäºå•æ¨¡æ€åŸºçº¿åœ¨åŒºåˆ†è‰¯æ€§ä¸äº¤ç•Œæ€§æˆ–æ¶æ€§å¶çŠ¶è‚¿ç˜¤æ–¹é¢è¡¨ç°æ›´ä½³ã€‚å…¶ä¸­ï¼ŒConvNeXtå’ŒResNet18åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­è¡¨ç°æœ€ä½³ï¼ŒAUC-ROCå¾—åˆ†åˆ†åˆ«ä¸º0.9427å’Œ0.9349ï¼ŒF1åˆ†æ•°åˆ†åˆ«ä¸º0.6720å’Œ0.7294ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€äººå·¥æ™ºèƒ½å…·æœ‰ä½œä¸ºéä¾µå…¥æ€§è¯Šæ–­å·¥å…·çš„æ½œåŠ›ï¼Œæœ‰æœ›é™ä½ä¸å¿…è¦çš„æ´»æ£€æ¬¡æ•°ï¼Œæé«˜ä¹³è…ºè‚¿ç˜¤ç®¡ç†çš„ä¸´åºŠå†³ç­–æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ç»“åˆäº†ä¹³è…ºè¶…å£°å›¾åƒå’Œç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œæ—¨åœ¨æé«˜å¶çŠ¶è‚¿ç˜¤çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>åŒåˆ†æ”¯ç¥ç»ç½‘ç»œèƒ½å¤Ÿä»ç¡®è¯Šçš„ç—…ä¾‹ä¸­æå–å¹¶èåˆè¶…å£°å›¾åƒå’Œæ‚£è€…å…ƒæ•°æ®ç‰¹å¾ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç±»æ„ŸçŸ¥é‡‡æ ·å’Œåˆ†å±‚äº”é‡äº¤å‰éªŒè¯æ¥é˜²æ­¢ç±»åˆ«ä¸å¹³è¡¡å’Œæ•°æ®æ³„éœ²ã€‚</li>
<li>å¤šæ¨¡æ€æ–¹æ³•ç›¸è¾ƒäºå•æ¨¡æ€åŸºçº¿åœ¨åŒºåˆ†è‰¯æ€§ä¸äº¤ç•Œæ€§æˆ–æ¶æ€§å¶çŠ¶è‚¿ç˜¤æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
<li>ConvNeXtå’ŒResNet18åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­è¡¨ç°æœ€ä½³ï¼ŒAUC-ROCå’ŒF1åˆ†æ•°å‡è¾ƒé«˜ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜å¤šæ¨¡æ€äººå·¥æ™ºèƒ½å…·æœ‰ä½œä¸ºéä¾µå…¥æ€§è¯Šæ–­å·¥å…·çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00213v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00213v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00213v1/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00213v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00213v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2509.00213v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Multi-Stage-Fine-Tuning-and-Ensembling-Strategy-for-Pancreatic-Tumor-Segmentation-in-Diagnostic-and-Therapeutic-MRI"><a href="#A-Multi-Stage-Fine-Tuning-and-Ensembling-Strategy-for-Pancreatic-Tumor-Segmentation-in-Diagnostic-and-Therapeutic-MRI" class="headerlink" title="A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor   Segmentation in Diagnostic and Therapeutic MRI"></a>A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor   Segmentation in Diagnostic and Therapeutic MRI</h2><p><strong>Authors:Omer Faruk Durugol, Maximilian Rokuss, Yannick Kirchhoff, Klaus H. Maier-Hein</strong></p>
<p>Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is critical for clinical workflows but is hindered by poor tumor-tissue contrast and a scarcity of annotated data. This paper details our submission to the PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the nnU-Net framework and leverages a deep, multi-stage cascaded pre-training strategy, starting from a general anatomical foundation model and sequentially fine-tuning on CT pancreatic lesion datasets and the target MRI modalities. Through extensive five-fold cross-validation, we systematically evaluated data augmentation schemes and training schedules. Our analysis revealed a critical trade-off, where aggressive data augmentation produced the highest volumetric accuracy, while default augmentations yielded superior boundary precision (achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1). For our final submission, we exploited this finding by constructing custom, heterogeneous ensembles of specialist models, essentially creating a mix of experts. This metric-aware ensembling strategy proved highly effective, achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523 for Task 2. Our work presents a robust methodology for developing specialized, high-performance models in the context of limited data and complex medical imaging tasks (Team MIC-DKFZ). </p>
<blockquote>
<p>è‡ªåŠ¨åˆ†å‰²èƒ°è…ºå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰çš„MRIå¯¹äºä¸´åºŠå·¥ä½œæµç¨‹è‡³å…³é‡è¦ï¼Œä½†ç”±äºè‚¿ç˜¤ç»„ç»‡å¯¹æ¯”åº¦å·®å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºï¼Œè¿™ä¸€ä»»åŠ¡å—åˆ°é˜»ç¢ã€‚æœ¬æ–‡è¯¦ç»†æè¿°äº†æˆ‘ä»¬çš„é’ˆå¯¹PANTHERæŒ‘æˆ˜æäº¤çš„æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŒæ—¶è§£å†³äº†è¯Šæ–­T1åŠ æƒï¼ˆä»»åŠ¡1ï¼‰å’Œæ²»ç–—T2åŠ æƒï¼ˆä»»åŠ¡2ï¼‰çš„åˆ†å‰²é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨nnU-Netæ¡†æ¶ä¹‹ä¸Šï¼Œé‡‡ç”¨æ·±åº¦å¤šé˜¶æ®µçº§è”é¢„è®­ç»ƒç­–ç•¥ï¼Œä»ä¸€èˆ¬è§£å‰–åŸºç¡€æ¨¡å‹å¼€å§‹ï¼Œå¹¶åœ¨CTèƒ°è…ºç—…å˜æ•°æ®é›†å’Œç›®æ ‡MRIæ¨¡å¼ä¸Šé¡ºåºå¾®è°ƒã€‚é€šè¿‡äº”å€äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æ•°æ®å¢å¼ºæ–¹æ¡ˆå’Œè®­ç»ƒæ—¶é—´è¡¨ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„æƒè¡¡ï¼šæ¿€è¿›çš„æ•°æ®å¢å¼ºäº§ç”Ÿäº†æœ€é«˜çš„ä½“ç§¯ç²¾åº¦ï¼Œè€Œé»˜è®¤çš„æ•°æ®å¢å¼ºäº§ç”Ÿäº†æ›´é«˜çš„è¾¹ç•Œç²¾åº¦ï¼ˆåœ¨ä»»åŠ¡1ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¹³å‡å¯¹ç§°è·ç¦»ï¼ˆMASDï¼‰ä¸º5.46æ¯«ç±³å’ŒHD95ä¸º17.33æ¯«ç±³ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„æœ€ç»ˆæäº¤ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ„å»ºä¸“ä¸šçš„å¼‚è´¨æ¨¡å‹ç»„åˆï¼Œåˆ©ç”¨äº†è¿™ä¸€å‘ç°ï¼Œå®è´¨ä¸Šåˆ›å»ºäº†ä¸“å®¶æ··åˆã€‚è¿™ç§æŒ‡æ ‡æ„ŸçŸ¥çš„é›†æˆç­–ç•¥è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼Œåœ¨ä»»åŠ¡1å’Œä»»åŠ¡2ä¸­åˆ†åˆ«è¾¾åˆ°äº†äº¤å‰éªŒè¯è‚¿ç˜¤Diceç³»æ•°çš„æœ€é«˜åˆ†0.661å’Œ0.523ã€‚ï¼ˆå›¢é˜ŸMIC-DKFZï¼‰æˆ‘ä»¬çš„å·¥ä½œåœ¨æœ‰é™æ•°æ®å’Œå¤æ‚åŒ»å­¦æˆåƒä»»åŠ¡çš„èƒŒæ™¯ä¸‹ï¼Œæå‡ºäº†ä¸€ç§ç¨³å¥çš„æ–¹æ³•ï¼Œç”¨äºå¼€å‘ä¸“ä¸šã€é«˜æ€§èƒ½çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21775v1">PDF</a> 11 pages, 1 figure, PANTHER Challenge submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹èƒ°è…ºå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰çš„MRIè‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨PANTHERæŒ‘æˆ˜ä¸­æå‡ºäº†é’ˆå¯¹è¯Šæ–­T1åŠ æƒï¼ˆä»»åŠ¡1ï¼‰å’Œæ²»ç–—T2åŠ æƒï¼ˆä»»åŠ¡2ï¼‰åˆ†å‰²çš„è§£å†³ç­–ç•¥ã€‚è¯¥ç ”ç©¶åŸºäºnnU-Netæ¡†æ¶ï¼Œé‡‡ç”¨æ·±åº¦å¤šé˜¶æ®µçº§è”é¢„è®­ç»ƒç­–ç•¥ï¼Œä»ä¸€èˆ¬è§£å‰–åŸºç¡€æ¨¡å‹å¼€å§‹ï¼Œå¹¶åœ¨CTèƒ°è…ºç—…å˜æ•°æ®é›†å’Œç›®æ ‡MRIæ¨¡å¼ä¸Šè¿›è¡Œé¡ºåºå¾®è°ƒã€‚ç ”ç©¶é€šè¿‡äº”æŠ˜äº¤å‰éªŒè¯è¯„ä¼°äº†æ•°æ®å¢å¼ºæ–¹æ¡ˆå’Œè®­ç»ƒè®¡åˆ’ï¼Œå‘ç°æ¿€è¿›çš„æ•°æ®å¢å¼ºäº§ç”Ÿæœ€é«˜çš„ä½“ç§¯ç²¾åº¦ï¼Œè€Œé»˜è®¤å¢å¼ºåˆ™äº§ç”Ÿæ›´é«˜çš„è¾¹ç•Œç²¾åº¦ã€‚æœ€ç»ˆï¼Œå›¢é˜Ÿé‡‡ç”¨æ··åˆä¸“å®¶æ¨¡å‹çš„æ–¹å¼æ„å»ºè‡ªå®šä¹‰çš„å¼‚è´¨æ¨¡å‹é›†åˆï¼Œé‡‡ç”¨æŒ‡æ ‡æ„ŸçŸ¥é›†æˆç­–ç•¥ï¼Œåœ¨ä»»åŠ¡1ä¸­è¾¾åˆ°0.661çš„è‚¿ç˜¤Diceåˆ†æ•°ï¼Œä»»åŠ¡2è¾¾åˆ°0.523ã€‚è¯¥ç ”ç©¶ä¸ºæœ‰é™æ•°æ®å’Œå¤æ‚åŒ»å­¦æˆåƒä»»åŠ¡ä¸‹å¼€å‘é«˜æ€§èƒ½æ¨¡å‹æä¾›äº†ç¨³å¥çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–åˆ†å‰²èƒ°è…ºå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰ä»MRIå¯¹äºä¸´åºŠå·¥ä½œæµç¨‹è‡³å…³é‡è¦ï¼Œä½†å—åˆ°è‚¿ç˜¤ç»„ç»‡å¯¹æ¯”åº¦å·®å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåœ¨PANTHERæŒ‘æˆ˜ä¸­æå‡ºäº†é’ˆå¯¹ä»»åŠ¡1ï¼ˆè¯Šæ–­T1åŠ æƒï¼‰å’Œä»»åŠ¡2ï¼ˆæ²»ç–—T2åŠ æƒï¼‰çš„åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>åŸºäºnnU-Netæ¡†æ¶ï¼Œé‡‡ç”¨æ·±åº¦å¤šé˜¶æ®µçº§è”é¢„è®­ç»ƒç­–ç•¥ï¼Œä»ä¸€èˆ¬è§£å‰–æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¾®è°ƒã€‚</li>
<li>è¯„ä¼°äº†æ•°æ®å¢å¼ºæ–¹æ¡ˆå’Œè®­ç»ƒè®¡åˆ’ï¼Œå‘ç°æ¿€è¿›æ•°æ®å¢å¼ºæé«˜ä½“ç§¯ç²¾åº¦ï¼Œè€Œé»˜è®¤å¢å¼ºæé«˜è¾¹ç•Œç²¾åº¦ã€‚</li>
<li>å›¢é˜Ÿé‡‡ç”¨æ··åˆä¸“å®¶æ¨¡å‹çš„æ–¹å¼æ„å»ºè‡ªå®šä¹‰çš„å¼‚è´¨æ¨¡å‹é›†åˆï¼Œå®ç°é«˜æ€§èƒ½æ¨¡å‹å¼€å‘ã€‚</li>
<li>é‡‡ç”¨æŒ‡æ ‡æ„ŸçŸ¥é›†æˆç­–ç•¥ï¼Œåœ¨ä»»åŠ¡1ä¸­è¾¾åˆ°è‚¿ç˜¤Diceåˆ†æ•°0.661ï¼Œä»»åŠ¡2è¾¾åˆ°0.523ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21775v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21775v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CAD2DMD-SET-Synthetic-Generation-Tool-of-Digital-Measurement-Device-CAD-Model-Datasets-for-fine-tuning-Large-Vision-Language-Models"><a href="#CAD2DMD-SET-Synthetic-Generation-Tool-of-Digital-Measurement-Device-CAD-Model-Datasets-for-fine-tuning-Large-Vision-Language-Models" class="headerlink" title="CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD   Model Datasets for fine-tuning Large Vision-Language Models"></a>CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD   Model Datasets for fine-tuning Large Vision-Language Models</h2><p><strong>Authors:JoÃ£o Valente, Atabak Dehban, Rodrigo Ventura</strong></p>
<p>Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRAâ€™s of these models with CAD2DMD-SETâ€™s generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›å±•åœ¨å„ç§è·¨æ¨¡æ€ä»»åŠ¡ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥å¤„ç†ä»æ•°å­—æµ‹é‡è®¾å¤‡ï¼ˆDMDsï¼‰è¯»å–æ•°å€¼ç­‰å¹³å‡¡åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œä¸­å­˜åœ¨æ‚ä¹±ã€é®æŒ¡ã€æç«¯è§†è§’å’Œè¿åŠ¨æ¨¡ç³Šç­‰å¸¸è§æƒ…å†µï¼Œè¿™äº›æƒ…å†µåœ¨å¤´æˆ´å¼ç›¸æœºå’Œå¢å¼ºç°å®ï¼ˆARï¼‰åº”ç”¨ä¸­å¾ˆå¸¸è§ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œè¿™é¡¹å·¥ä½œå¼•å…¥äº†CAD2DMD-SETï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆå·¥å…·ï¼Œæ—¨åœ¨æ”¯æŒæ¶‰åŠDMDçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å·¥å…·é€šè¿‡åˆ©ç”¨3D CADæ¨¡å‹ã€é«˜çº§æ¸²æŸ“å’Œé«˜ä¿çœŸå›¾åƒç»„åˆï¼Œå¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–ã€å¸¦æœ‰VQAæ ‡ç­¾çš„åˆæˆDMDæ•°æ®é›†ï¼Œé€‚ç”¨äºå¾®è°ƒLVLMsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†DMDBenchï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«1000ä¸ªæ³¨é‡Šè¿‡çš„çœŸå®ä¸–ç•Œå›¾åƒçš„ç²¾é€‰éªŒè¯é›†ï¼Œæ—¨åœ¨åœ¨å®é™…çº¦æŸä¸‹è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚ä½¿ç”¨å¹³å‡æ ‡å‡†åŒ–è±æ–‡æ–¯å¦ç›¸ä¼¼åº¦ï¼ˆANLSï¼‰å¯¹ä¸‰é¡¹æœ€å…ˆè¿›çš„LVLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨CAD2DMD-SETç”Ÿæˆçš„æ•°æ®é›†å¯¹LoRAæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æå‡æ•ˆæœï¼Œå…¶ä¸­InternVLçš„å¾—åˆ†æé«˜äº†ä¸¤å€ï¼ŒåŒæ—¶åœ¨å…¶ä»–ä»»åŠ¡ä¸Šä¹Ÿæ²¡æœ‰å‡ºç°æ€§èƒ½ä¸‹é™ã€‚è¿™è¡¨æ˜CAD2DMD-SETè®­ç»ƒæ•°æ®é›†åœ¨åº”å¯¹ä¹‹å‰æåˆ°çš„æŒ‘æˆ˜æ¡ä»¶ä¸‹ï¼Œæ˜¾è‘—æé«˜äº†LVLMsçš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚ä¸€æ—¦æœ€ç»ˆç‰ˆæœ¬çš„æ‰‹ç¨¿å‡†å¤‡å®Œæ¯•ï¼ŒCAD2DMD-SETå·¥å…·å°†ä½œä¸ºå¼€æºå‘å¸ƒï¼Œå…è®¸ç¤¾åŒºæ·»åŠ ä¸åŒçš„æµ‹é‡è®¾å¤‡å¹¶ç”Ÿæˆè‡ªå·±çš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21732v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨æ•°å­—æµ‹é‡è®¾å¤‡ï¼ˆDMDï¼‰è¯»å–å€¼æ–¹é¢çš„ä¸è¶³ï¼Œå¼€å‘äº†CAD2DMD-SETåˆæˆæ•°æ®ç”Ÿæˆå·¥å…·ï¼Œå¯æ”¯æŒæ¶‰åŠDMDçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚é€šè¿‡3D CADæ¨¡å‹ã€é«˜çº§æ¸²æŸ“å’Œé«˜ä¿çœŸå›¾åƒç»„åˆæŠ€æœ¯ï¼Œè¯¥å·¥å…·ç”Ÿæˆå¤šæ ·åŒ–çš„ã€å¸¦æœ‰VQAæ ‡ç­¾çš„åˆæˆDMDæ•°æ®é›†ï¼Œé€‚ç”¨äºç²¾ç»†è°ƒæ•´LVLMsã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†DMDBenchï¼Œä¸€ä¸ªåŒ…å«1000ä¸ªæ³¨é‡Šè¿‡çš„çœŸå®ä¸–ç•Œå›¾åƒçš„è¯„ä»·é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å®é™…çº¦æŸä¸‹çš„æ€§èƒ½ã€‚ä½¿ç”¨å¹³å‡æ ‡å‡†åŒ–è±æ–‡æ–¯å¦ç›¸ä¼¼æ€§ï¼ˆANLSï¼‰è¯„ä¼°ä¸‰ç§å…ˆè¿›çš„LVLMsï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨CAD2DMD-SETç”Ÿæˆçš„æ•°æ®é›†å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒï¼Œç»“æœæ˜¾ç¤ºInternVLçš„æ€§èƒ½æé«˜äº†200%ï¼ŒåŒæ—¶ä¸å½±å“å…¶ä»–ä»»åŠ¡ã€‚è¿™è¡¨æ˜CAD2DMD-SETè®­ç»ƒæ•°æ®é›†æ˜¾è‘—æé«˜äº†LVLMsåœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è¯»å–æ•°å­—æµ‹é‡è®¾å¤‡ï¼ˆDMDï¼‰çš„å€¼æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œçš„å¤æ‚ç¯å¢ƒä¸­ã€‚</li>
<li>CAD2DMD-SETæ˜¯ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆå·¥å…·ï¼Œç”¨äºæ”¯æŒæ¶‰åŠDMDçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚</li>
<li>CAD2DMD-SETåˆ©ç”¨3D CADæ¨¡å‹ã€é«˜çº§æ¸²æŸ“å’Œé«˜ä¿çœŸå›¾åƒç»„åˆæŠ€æœ¯ï¼Œç”Ÿæˆé€‚åˆç²¾ç»†è°ƒæ•´LVLMsçš„å¤šæ ·åŒ–åˆæˆDMDæ•°æ®é›†ã€‚</li>
<li>DMDBenchæ˜¯ä¸€ä¸ªçœŸå®ä¸–ç•Œå›¾åƒè¯„ä»·é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å®ç”¨æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨CAD2DMD-SETç”Ÿæˆçš„æ•°æ®é›†å¯¹ä¸‰ç§å…ˆè¿›çš„LVLMsè¿›è¡Œå¾®è°ƒï¼Œç»“æœæ˜¾ç¤ºæ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>InternVLæ¨¡å‹åœ¨ä½¿ç”¨CAD2DMD-SETè®­ç»ƒæ•°æ®é›†åï¼Œæ€§èƒ½æå‡200%ï¼ŒåŒæ—¶ä¸å½±å“å…¶ä»–ä»»åŠ¡çš„æ‰§è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21732v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21732v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21732v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21732v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Integrating-Pathology-and-CT-Imaging-for-Personalized-Recurrence-Risk-Prediction-in-Renal-Cancer"><a href="#Integrating-Pathology-and-CT-Imaging-for-Personalized-Recurrence-Risk-Prediction-in-Renal-Cancer" class="headerlink" title="Integrating Pathology and CT Imaging for Personalized Recurrence Risk   Prediction in Renal Cancer"></a>Integrating Pathology and CT Imaging for Personalized Recurrence Risk   Prediction in Renal Cancer</h2><p><strong>Authors:DaniÃ«l Boeke, Cedrik Blommestijn, Rebecca N. Wray, Kalina Chupetlovska, Shangqi Gao, Zeyu Gao, Regina G. H. Beets-Tan, Mireia Crispin-Ortuzar, James O. Jones, Wilson Silva, Ines P. Machado</strong></p>
<p>Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is essential for guiding postoperative surveillance and treatment. The Leibovich score remains widely used for stratifying distant recurrence risk but offers limited patient-level resolution and excludes imaging information. This study evaluates multimodal recurrence prediction by integrating preoperative computed tomography (CT) and postoperative histopathology whole-slide images (WSIs). A modular deep learning framework with pretrained encoders and Cox-based survival modeling was tested across unimodal, late fusion, and intermediate fusion setups. In a real-world ccRCC cohort, WSI-based models consistently outperformed CT-only models, underscoring the prognostic strength of pathology. Intermediate fusion further improved performance, with the best model (TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random tie-breaking narrowed the gap between the clinical baseline and learned models, suggesting discretization may overstate individualized performance. Using simple embedding concatenation, radiology added value primarily through fusion. These findings demonstrate the feasibility of foundation model-based multimodal integration for personalized ccRCC risk prediction. Future work should explore more expressive fusion strategies, larger multimodal datasets, and general-purpose CT encoders to better match pathology modeling capacity. </p>
<blockquote>
<p>åœ¨é€æ˜ç»†èƒè‚¾ç»†èƒç™Œï¼ˆccRCCï¼‰ä¸­ï¼Œå¤å‘é£é™©è¯„ä¼°å¯¹äºæŒ‡å¯¼æœ¯åç›‘æµ‹å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚Leibovichè¯„åˆ†ä»å¹¿æ³›åº”ç”¨äºåˆ†å±‚è¿œè·ç¦»å¤å‘é£é™©çš„è¯„ä¼°ï¼Œä½†å…¶åœ¨æ‚£è€…å±‚é¢çš„åˆ†è¾¨ç‡æœ‰é™ï¼Œä¸”ä¸åŒ…æ‹¬æˆåƒä¿¡æ¯ã€‚æœ¬ç ”ç©¶é€šè¿‡æ•´åˆæœ¯å‰è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œæœ¯åç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æ¥è¯„ä¼°å¤šæ¨¡å¼å¤å‘é¢„æµ‹ã€‚é‡‡ç”¨æ¨¡å—åŒ–æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«é¢„è®­ç»ƒç¼–ç å™¨å’ŒåŸºäºCoxçš„ç”Ÿå­˜å»ºæ¨¡ï¼Œæµ‹è¯•äº†å•æ¨¡æ€ã€åæœŸèåˆå’Œä¸­é—´èåˆç­‰å¤šç§è®¾ç½®ã€‚åœ¨çœŸå®ä¸–ç•Œçš„ccRCCé˜Ÿåˆ—ä¸­ï¼ŒåŸºäºWSIçš„æ¨¡å‹å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨CTçš„æ¨¡å‹ï¼Œçªæ˜¾äº†ç—…ç†å­¦çš„é¢„åå¼ºåº¦ã€‚ä¸­é—´èåˆè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå…¶ä¸­æœ€ä½³æ¨¡å‹ï¼ˆé‡‡ç”¨ResNet-18çš„TITAN-CONCHï¼‰æ¥è¿‘è°ƒæ•´åçš„Leibovichè¯„åˆ†ã€‚éšæœºè§£å†³å¹³å±€é—®é¢˜ç¼©å°äº†ä¸´åºŠåŸºçº¿ä¸å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œè¡¨æ˜ç¦»æ•£åŒ–å¯èƒ½ä¼šå¤¸å¤§ä¸ªä½“åŒ–æ€§èƒ½ã€‚é€šè¿‡ç®€å•çš„åµŒå…¥æ‹¼æ¥ï¼Œæ”¾å°„å­¦ä¸»è¦é€šè¿‡èåˆå¢åŠ äº†ä»·å€¼ã€‚è¿™äº›å‘ç°è¯æ˜äº†åŸºäºåŸºç¡€æ¨¡å‹çš„å¤šæ¨¡å¼æ•´åˆåœ¨ä¸ªæ€§åŒ–ccRCCé£é™©é¢„æµ‹ä¸­çš„å¯è¡Œæ€§ã€‚æœªæ¥çš„å·¥ä½œåº”æ¢ç´¢æ›´å…·è¡¨ç°åŠ›çš„èåˆç­–ç•¥ã€æ›´å¤§çš„å¤šæ¨¡å¼æ•°æ®é›†å’Œä¸ç—…ç†å­¦å»ºæ¨¡èƒ½åŠ›ç›¸åŒ¹é…çš„é€šç”¨CTç¼–ç å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21581v1">PDF</a> 12 pages, 2 figures, 1 table. Accepted at the Multimodal Learning and   Fusion Across Scales for Clinical Decision Support (ML-CDS) Workshop, MICCAI   2025. This is the submitted version with authors, affiliations, and   acknowledgements included; it has not undergone peer review or revisions. The   final version will appear in the Springer Lecture Notes in Computer Science   (LNCS) proceedings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨å¤šæ¨¡æ€é¢„æµ‹å¤å‘é£é™©çš„æ–¹æ³•ï¼Œç»“åˆäº†æœ¯å‰è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œæœ¯åç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰ã€‚é‡‡ç”¨æ¨¡å—åŒ–æ·±åº¦å­¦ä¹ æ¡†æ¶å’ŒCoxç”Ÿå­˜æ¨¡å‹ï¼Œæµ‹è¯•äº†å•æ¨¡æ€ã€åæœŸèåˆå’Œä¸­é—´èåˆç­‰å¤šç§è®¾ç½®ã€‚åœ¨çœŸå®ä¸–ç•Œè‚¾é€æ˜ç»†èƒç™Œé˜Ÿåˆ—ä¸­ï¼ŒåŸºäºWSIçš„æ¨¡å‹è¡¨ç°ä¼˜äºä»…ä½¿ç”¨CTçš„æ¨¡å‹ï¼Œçªæ˜¾äº†ç—…ç†å­¦çš„é¢„åä»·å€¼ã€‚ä¸­é—´èåˆè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œæœ€ä½³æ¨¡å‹æ¥è¿‘è°ƒæ•´åçš„Leibovichè¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤å‘é£é™©è¯„ä¼°åœ¨è‚¾é€æ˜ç»†èƒç™Œï¼ˆccRCCï¼‰ä¸­éå¸¸é‡è¦ï¼Œç”¨äºæŒ‡å¯¼æœ¯åç›‘æµ‹å’Œæ²»ç–—ã€‚</li>
<li>Leibovichè¯„åˆ†è™½å¹¿æ³›åº”ç”¨äºåˆ†å±‚è¯„ä¼°è¿œå¤„å¤å‘é£é™©ï¼Œä½†æ‚£è€…å±‚é¢åˆ†è¾¨ç‡æœ‰é™ï¼Œä¸”æœªåŒ…å«æˆåƒä¿¡æ¯ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€é¢„æµ‹å¤å‘é£é™©ï¼Œç»“åˆäº†æœ¯å‰CTå’Œæœ¯åç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒã€‚</li>
<li>ä½¿ç”¨æ¨¡å—åŒ–æ·±åº¦å­¦ä¹ æ¡†æ¶å’ŒCoxç”Ÿå­˜æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œæ˜¾ç¤ºåŸºäºWSIçš„æ¨¡å‹æ€§èƒ½ä¼˜äºä»…ä½¿ç”¨CTçš„æ¨¡å‹ã€‚</li>
<li>ä¸­é—´èåˆç­–ç•¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œæœ€ä½³æ¨¡å‹æ¥è¿‘è°ƒæ•´åçš„Leibovichè¯„åˆ†ã€‚</li>
<li>éšæœºå¹³å±€æœºåˆ¶ç¼©å°äº†ä¸´åºŠåŸºçº¿ä¸å­¦ä¹ æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œè¡¨æ˜ç¦»æ•£åŒ–å¯èƒ½å¤¸å¤§ä¸ªæ€§åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21581v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21581v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Temporal-Flow-Matching-for-Learning-Spatio-Temporal-Trajectories-in-4D-Longitudinal-Medical-Imaging"><a href="#Temporal-Flow-Matching-for-Learning-Spatio-Temporal-Trajectories-in-4D-Longitudinal-Medical-Imaging" class="headerlink" title="Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D   Longitudinal Medical Imaging"></a>Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D   Longitudinal Medical Imaging</h2><p><strong>Authors:Nico Albert Disch, Yannick Kirchhoff, Robin Peretzke, Maximilian Rokuss, Saikat Roy, Constantin Ulrich, David Zimmerer, Klaus Maier-Hein</strong></p>
<p>Understanding temporal dynamics in medical imaging is crucial for applications such as disease progression modeling, treatment planning and anatomical development tracking. However, most deep learning methods either consider only single temporal contexts, or focus on tasks like classification or regression, limiting their ability for fine-grained spatial predictions. While some approaches have been explored, they are often limited to single timepoints, specific diseases or have other technical restrictions. To address this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified generative trajectory method that (i) aims to learn the underlying temporal distribution, (ii) by design can fall back to a nearest image predictor, i.e. predicting the last context image (LCI), as a special case, and (iii) supports $3D$ volumes, multiple prior scans, and irregular sampling. Extensive benchmarks on three public longitudinal datasets show that TFM consistently surpasses spatio-temporal methods from natural imaging, establishing a new state-of-the-art and robust baseline for $4D$ medical image prediction. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ç†è§£æ—¶é—´åŠ¨æ€å¯¹äºç–¾ç—…è¿›å±•å»ºæ¨¡ã€æ²»ç–—è®¡åˆ’æ‹Ÿå®šå’Œè§£å‰–å‘è‚²è·Ÿè¸ªç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ–¹æ³•è¦ä¹ˆåªè€ƒè™‘å•ä¸€çš„æ—¶æ€èƒŒæ™¯ï¼Œè¦ä¹ˆä¸“æ³¨äºåˆ†ç±»æˆ–å›å½’ç­‰ä»»åŠ¡ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬è¿›è¡Œç²¾ç»†ç©ºé—´é¢„æµ‹çš„èƒ½åŠ›ã€‚è™½ç„¶æœ‰ä¸€äº›æ–¹æ³•å·²ç»è¢«æ¢ç´¢å‡ºæ¥ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…é™äºå•ä¸€æ—¶é—´ç‚¹ã€ç‰¹å®šç–¾ç—…æˆ–å…¶ä»–æŠ€æœ¯é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€åŸºæœ¬å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶é—´æµåŒ¹é…ï¼ˆTFMï¼‰è¿™ä¸€ç»Ÿä¸€çš„ç”Ÿæˆè½¨è¿¹æ–¹æ³•ï¼Œå…¶ç›®æ ‡åŒ…æ‹¬ï¼šï¼ˆä¸€ï¼‰å­¦ä¹ æ½œåœ¨çš„æ—¶æ€åˆ†å¸ƒï¼›ï¼ˆäºŒï¼‰è®¾è®¡æ—¶èƒ½å¤Ÿå›é€€åˆ°æœ€æ¥è¿‘çš„å›¾åƒé¢„æµ‹å™¨ï¼Œå³é¢„æµ‹æœ€åä¸€ä¸ªä¸Šä¸‹æ–‡å›¾åƒï¼ˆLCIï¼‰ä½œä¸ºç‰¹æ®Šæƒ…å†µï¼›ï¼ˆä¸‰ï¼‰æ”¯æŒä¸‰ç»´ä½“ç§¯ã€å¤šä¸ªå…ˆå‰æ‰«æå’Œä¸è§„åˆ™é‡‡æ ·ã€‚åœ¨ä¸‰ä¸ªå…¬å…±çºµå‘æ•°æ®é›†ä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒTFMå§‹ç»ˆè¶…è¶Šäº†è‡ªç„¶æˆåƒä¸­çš„æ—¶ç©ºæ–¹æ³•ï¼Œä¸ºå››ç»´åŒ»å­¦å›¾åƒé¢„æµ‹å»ºç«‹äº†æ–°çš„å…ˆè¿›å’Œç¨³å¥çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21580v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦æˆåƒä¸­çš„æ—¶é—´åŠ¨æ€ç†è§£å¯¹äºç–¾ç—…è¿›å±•å»ºæ¨¡ã€æ²»ç–—è§„åˆ’å’Œè§£å‰–å‘è‚²è·Ÿè¸ªç­‰åº”ç”¨çš„é‡è¦æ€§ã€‚é’ˆå¯¹ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚åªèƒ½å¤„ç†å•ä¸€æ—¶é—´ä¸Šä¸‹æ–‡æˆ–ä¸“æ³¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡è€Œæ— æ³•è¿›è¡Œç²¾ç»†çš„ç©ºé—´é¢„æµ‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„ç”Ÿæˆè½¨è¿¹æ–¹æ³•â€”â€”Temporal Flow Matchingï¼ˆTFMï¼‰ã€‚TFMæ—¨åœ¨å­¦ä¹ æ½œåœ¨çš„æ—¶ç©ºåˆ†å¸ƒï¼Œå¹¶å¯åœ¨å¿…è¦æ—¶é€€å›åˆ°æœ€è¿‘å›¾åƒé¢„æµ‹å™¨è¿›è¡Œé¢„æµ‹ã€‚è¯¥æ–¹æ³•æ”¯æŒä¸‰ç»´ä½“ç§¯ã€å¤šä¸ªå…ˆéªŒæ‰«æå’Œéè§„åˆ™é‡‡æ ·ã€‚åœ¨ä¸‰ä¸ªå…¬å…±çºµå‘æ•°æ®é›†ä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒTFMåœ¨æ—¶ç©ºæ–¹æ³•ä¸Šè¡¨ç°æ›´ä¼˜ç§€ï¼Œå»ºç«‹äº†æ–°çš„å››ç»´åŒ»å­¦å›¾åƒé¢„æµ‹å…ˆè¿›åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒä¸­çš„æ—¶é—´åŠ¨æ€ç†è§£å¯¹äºç–¾ç—…è¿›å±•å»ºæ¨¡ã€æ²»ç–—è§„åˆ’å’Œè§£å‰–å‘è‚²è·Ÿè¸ªè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å¤„ç†å•ä¸€æ—¶é—´ä¸Šä¸‹æ–‡æˆ–ä¸“æ³¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚</li>
<li>TFMæ˜¯ä¸€ç§ç»Ÿä¸€çš„ç”Ÿæˆè½¨è¿¹æ–¹æ³•ï¼Œæ—¨åœ¨å­¦ä¹ æ½œåœ¨çš„æ—¶ç©ºåˆ†å¸ƒã€‚</li>
<li>TFMæ”¯æŒä¸‰ç»´ä½“ç§¯ã€å¤šä¸ªå…ˆéªŒæ‰«æå’Œéè§„åˆ™é‡‡æ ·ï¼Œå¢å¼ºäº†æ–¹æ³•çš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚</li>
<li>TFMå…·æœ‰é€€åŒ–æœºåˆ¶ï¼Œåœ¨æ— æ³•ç²¾ç¡®é¢„æµ‹æœªæ¥å›¾åƒæ—¶ï¼Œå¯é€€å›åˆ°æœ€è¿‘å›¾åƒé¢„æµ‹å™¨è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±çºµå‘æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜TFMè¡¨ç°ä¼˜äºå…¶ä»–æ—¶ç©ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21580v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21580v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21580v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21580v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21580v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21580v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Lightweight-MRI-Based-Automated-Segmentation-of-Pancreatic-Cancer-with-Auto3DSeg"><a href="#Lightweight-MRI-Based-Automated-Segmentation-of-Pancreatic-Cancer-with-Auto3DSeg" class="headerlink" title="Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with   Auto3DSeg"></a>Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with   Auto3DSeg</h2><p><strong>Authors:Keshav Jha, William Sharp, Dominic LaBella</strong></p>
<p>Accurate delineation of pancreatic tumors is critical for diagnosis, treatment planning, and outcome assessment, yet automated segmentation remains challenging due to anatomical variability and limited dataset availability. In this study, SegResNet models, as part of the Auto3DSeg architecture, were trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold cross-validation with STAPLE ensembling after focusing on an anatomically relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI with expert annotated pancreas and tumor labels. The Pancreatic Tumor Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases with expert annotated pancreas and tumor labels. Algorithm-automated segmentation performance of pancreatic tumor was assessed using Dice Similarity Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1, the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203 mm. These findings illustrate the challenges of MRI-based pancreatic tumor segmentation with small datasets, highlighting variability introduced by different MRI sequences. Despite modest performance, the results demonstrate potential for automated delineation and emphasize the need for larger, standardized MRI datasets to improve model robustness and clinical utility. </p>
<blockquote>
<p>èƒ°è…ºè‚¿ç˜¤çš„å‡†ç¡®è½®å»“æç»˜å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–—æ•ˆè¯„ä¼°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè§£å‰–ç»“æ„å·®å¼‚å’Œå¯ç”¨æ•°æ®é›†æœ‰é™ï¼Œè‡ªåŠ¨åŒ–åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸­ï¼ŒSegResNetæ¨¡å‹ä½œä¸ºAuto3DSegæ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨åŸºäºMRIçš„èƒ°è…ºè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œä½œä¸º2025å¹´PANTHERæŒ‘æˆ˜èµ›çš„ä¸€éƒ¨åˆ†ã€‚ç®—æ³•æ–¹æ³•åŒ…æ‹¬åœ¨å…³æ³¨è§£å‰–ä¸Šç›¸å…³æ„Ÿå…´è¶£åŒºåŸŸåè¿›è¡Œ5å€äº¤å‰éªŒè¯å’ŒSTAPLEé›†æˆã€‚èƒ°è…ºè‚¿ç˜¤åˆ†å‰²è¯Šæ–­MRIä»»åŠ¡1çš„è®­ç»ƒé›†åŒ…æ‹¬91ä¾‹T1åŠ æƒåŠ¨è„‰å¢å¼ºMRIï¼Œå¸¦æœ‰ä¸“å®¶æ³¨é‡Šçš„èƒ°è…ºå’Œè‚¿ç˜¤æ ‡ç­¾ã€‚èƒ°è…ºè‚¿ç˜¤åˆ†å‰²MR-Linacä»»åŠ¡2çš„è®­ç»ƒé›†ä½¿ç”¨äº†50ä¾‹T2åŠ æƒMR-Linacç—…ä¾‹ï¼ŒåŒæ ·å¸¦æœ‰ä¸“å®¶æ³¨é‡Šçš„èƒ°è…ºå’Œè‚¿ç˜¤æ ‡ç­¾ã€‚ä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ã€5mm DSCã€95thç™¾åˆ†ä½Hausdorffè·ç¦»ï¼ˆHD95ï¼‰ã€å¹³å‡è¡¨é¢è·ç¦»ï¼ˆMASDï¼‰å’Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰æ¥è¯„ä¼°ç®—æ³•å¯¹èƒ°è…ºè‚¿ç˜¤è‡ªåŠ¨åŒ–åˆ†å‰²çš„æ€§èƒ½ã€‚å¯¹äºä»»åŠ¡1ï¼Œè¯¥ç®—æ³•å®ç°äº†DSC 0.56ï¼Œ5mm DSC 0.73ï¼ŒHD95 41.1mmï¼ŒMASD 26.0mmï¼ŒRMSE 5164mmã€‚å¯¹äºä»»åŠ¡2ï¼Œæ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼ŒDSCä¸º0.33ï¼Œ5mm DSCä¸º0.50ï¼ŒHD95ä¸º20.1mmï¼ŒMASDä¸º7.2mmï¼ŒRMSEä¸º17,203mmã€‚è¿™äº›å‘ç°è¯´æ˜äº†åŸºäºMRIçš„èƒ°è…ºè‚¿ç˜¤åˆ†å‰²ä½¿ç”¨å°æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†ä¸åŒMRIåºåˆ—å¼•å…¥çš„å˜å¼‚æ€§ã€‚å°½ç®¡æ€§èƒ½è¡¨ç°ä¸€èˆ¬ï¼Œä½†ç»“æœè¯æ˜äº†è‡ªåŠ¨åŒ–è½®å»“æç»˜çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´å¤§ã€æ ‡å‡†åŒ–çš„MRIæ•°æ®é›†æ¥æé«˜æ¨¡å‹ç¨³å¥æ€§å’Œä¸´åºŠå®ç”¨æ€§çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21227v1">PDF</a> 11 pages, 3 figures, 3 tables, MICCAI</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨SegResNetæ¨¡å‹ä½œä¸ºAuto3DSegæ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œå¯¹åŸºäºMRIçš„èƒ°è…ºè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡è¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œå‚åŠ 2025å¹´PANTHERæŒ‘æˆ˜èµ›ã€‚ç®—æ³•åœ¨T1åŠ æƒåŠ¨è„‰å¢å¼ºMRIçš„Pancreatic Tumor Segmentation on Diagnostic MRIä»»åŠ¡1è®­ç»ƒé›†å’ŒT2åŠ æƒMR-Linacç—…ä¾‹çš„Pancreatic Tumor Segmentation on MR-Linacä»»åŠ¡2è®­ç»ƒé›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚è™½ç„¶é¢ä¸´æ•°æ®é›†å°çš„æŒ‘æˆ˜ï¼Œä½†è¯¥ç ”ç©¶å±•ç¤ºäº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´å¤§ã€æ ‡å‡†åŒ–çš„MRIæ•°æ®é›†æ¥æé«˜æ¨¡å‹ç¨³å¥æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ°è…ºè‚¿ç˜¤çš„å‡†ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œç»“æœè¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†è‡ªåŠ¨åŒ–åˆ†å‰²ç”±äºè§£å‰–å˜å¼‚å’Œæœ‰é™çš„æ•°æ®é›†å¯ç”¨æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†SegResNetæ¨¡å‹ä½œä¸ºAuto3DSegæ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œå‚ä¸2025å¹´PANTHERæŒ‘æˆ˜èµ›ä¸­çš„ä¸¤ä¸ªMRIèƒ°è…ºè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>åœ¨è¯Šæ–­MRIçš„èƒ°è…ºè‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç®—æ³•å–å¾—äº†ç›¸å¯¹è¾ƒå¥½çš„æ€§èƒ½ï¼Œä½¿ç”¨ä¸“å®¶æ ‡æ³¨çš„èƒ°è…ºå’Œè‚¿ç˜¤æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨MR-Linacä»»åŠ¡ä¸­ï¼Œç®—æ³•æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œè¿™å¯èƒ½ä¸ä¸åŒMRIåºåˆ—å¼•å…¥çš„å˜å¼‚æ€§æœ‰å…³ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è¯„ä¼°ä¸åŒæŒ‡æ ‡ï¼ˆå¦‚Diceç›¸ä¼¼ç³»æ•°ã€Hausdorffè·ç¦»ç­‰ï¼‰æ¥è¡¡é‡ç®—æ³•è‡ªåŠ¨åŒ–åˆ†å‰²çš„æ€§èƒ½ã€‚</li>
<li>è™½ç„¶æ€§èƒ½ç›¸å¯¹æ¸©å’Œï¼Œä½†ç»“æœå±•ç¤ºäº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21227v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21227v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21227v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21227v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21227v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.21227v1/page_4_2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Dino-U-Net-Exploiting-High-Fidelity-Dense-Features-from-Foundation-Models-for-Medical-Image-Segmentation"><a href="#Dino-U-Net-Exploiting-High-Fidelity-Dense-Features-from-Foundation-Models-for-Medical-Image-Segmentation" class="headerlink" title="Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation   Models for Medical Image Segmentation"></a>Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation   Models for Medical Image Segmentation</h2><p><strong>Authors:Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao</strong></p>
<p>Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the modelâ€™s rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/yifangao112/DinoUNet">https://github.com/yifangao112/DinoUNet</a>. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†å¼ºå¤§çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°è½¬ç§»å…¶å­¦ä¹ è¡¨ç¤ºä»¥åº”ç”¨äºç²¾ç¡®çš„ä¸´åºŠåº”ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Dino U-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨åˆ©ç”¨DINOv3è§†è§‰åŸºç¡€æ¨¡å‹çš„é«˜ä¿çœŸå¯†é›†ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ¶æ„å¼•å…¥äº†ä¸€ä¸ªåŸºäºå†»ç»“çš„DINOv3éª¨å¹²ç½‘çš„ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨é‡‡ç”¨ä¸“ç”¨é€‚é…å™¨èåˆæ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾ä¸ä½çº§ç©ºé—´ç»†èŠ‚ã€‚ä¸ºäº†åœ¨é™ç»´è¿‡ç¨‹ä¸­ä¿æŒè¿™äº›è¡¨ç¤ºçš„è´¨é‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°å‹ä¿çœŸåº¦æ„ŸçŸ¥æŠ•å½±æ¨¡å—ï¼ˆFAPMï¼‰ï¼Œè¯¥æ¨¡å—å¯ä»¥æœ‰æ•ˆåœ°æ”¹è¿›å¹¶æŠ•å½±ç‰¹å¾ä»¥ä¾›è§£ç å™¨ä½¿ç”¨ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒDino U-Netè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å„ç§æˆåƒæ¨¡å¼ä¸‹å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¯æ˜å…·æœ‰å¾ˆé«˜çš„å¯æ‰©å±•æ€§ï¼Œéšç€éª¨å¹²ç½‘æ¨¡å‹å¤§å°å¢åŠ åˆ°7äº¿å‚æ•°å˜ä½“ï¼Œåˆ†å‰²ç²¾åº¦æŒç»­æé«˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨é€šç”¨åŸºç¡€æ¨¡å‹çš„ä¼˜è´¨ã€å¯†é›†é¢„è®­ç»ƒç‰¹å¾æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å‚æ•°åŒ–çš„æ–¹æ³•æ¥æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/yifangao112/DinoUNet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yifangao112/DinoUNetè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20909v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†Dino U-Netæ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒæ•°æ®é›†é¢„è®­ç»ƒçš„DINOv3æ¨¡å‹çš„é«˜ä¿çœŸå¯†é›†ç‰¹å¾ï¼Œé€šè¿‡ç‰¹æ®Šé€‚é…å™¨èåˆæ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾ä¸ä½çº§ç©ºé—´ç»†èŠ‚ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„ä¿çœŸåº¦æ„ŸçŸ¥æŠ•å½±æ¨¡å—ï¼ˆFAPMï¼‰ï¼Œä»¥åœ¨é™ç»´è¿‡ç¨‹ä¸­ä¿æŒç‰¹å¾è´¨é‡ã€‚åœ¨ä¸ƒä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDino U-Netè¡¨ç°å“è¶Šï¼Œåœ¨å„ç§æˆåƒæ¨¡æ€ä¸Šå‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é«˜åº¦å¯æ‰©å±•ï¼Œéšç€åŸºç¡€æ¨¡å‹è§„æ¨¡å¢å¤§åˆ°7äº¿å‚æ•°å˜ä½“ï¼Œåˆ†å‰²ç²¾åº¦æŒç»­æé«˜ã€‚ç ”ç©¶è¯æ˜äº†åˆ©ç”¨é€šç”¨åŸºç¡€æ¨¡å‹çš„å…ˆè¿›é¢„è®­ç»ƒç‰¹å¾ï¼Œå¯¹æå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®åº¦å…·æœ‰é«˜æ•ˆå’Œå‚æ•°é«˜æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dino U-Netåˆ©ç”¨é¢„è®­ç»ƒçš„DINOv3æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>è¯¥æ¶æ„é€šè¿‡ç‰¹æ®Šé€‚é…å™¨èåˆæ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾å’Œä½çº§ç©ºé—´ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¿çœŸåº¦æ„ŸçŸ¥æŠ•å½±æ¨¡å—ï¼ˆFAPMï¼‰ï¼Œä»¥åœ¨é™ç»´è¿‡ç¨‹ä¸­ä¿æŒç‰¹å¾è´¨é‡ã€‚</li>
<li>åœ¨ä¸ƒä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDino U-Netè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>éšç€åŸºç¡€æ¨¡å‹è§„æ¨¡å¢å¤§ï¼Œåˆ†å‰²ç²¾åº¦æŒç»­æé«˜ï¼Œè¯æ˜è¯¥æ¡†æ¶é«˜åº¦å¯æ‰©å±•ã€‚</li>
<li>åˆ©ç”¨é€šç”¨åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒç‰¹å¾ï¼Œèƒ½é«˜æ•ˆæå‡åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20909v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20909v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20909v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20909v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20909v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20909v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Domain-Adaptation-Techniques-for-Natural-and-Medical-Image-Classification"><a href="#Domain-Adaptation-Techniques-for-Natural-and-Medical-Image-Classification" class="headerlink" title="Domain Adaptation Techniques for Natural and Medical Image   Classification"></a>Domain Adaptation Techniques for Natural and Medical Image   Classification</h2><p><strong>Authors:Ahmad Chaddad, Yihang Wu, Reem Kateb, Christian Desrosiers</strong></p>
<p>Domain adaptation (DA) techniques have the potential in machine learning to alleviate distribution differences between training and test sets by leveraging information from source domains. In image classification, most advances in DA have been made using natural images rather than medical data, which are harder to work with. Moreover, even for natural images, the use of mainstream datasets can lead to performance bias. {With the aim of better understanding the benefits of DA for both natural and medical images, this study performs 557 simulation studies using seven widely-used DA techniques for image classification in five natural and eight medical datasets that cover various scenarios, such as out-of-distribution, dynamic data streams, and limited training samples.} Our experiments yield detailed results and insightful observations highlighting the performance and medical applicability of these techniques. Notably, our results have shown the outstanding performance of the Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved feasible classification accuracy (91.2%) in the COVID-19 dataset using Resnet50 and showed an important accuracy improvement in the dynamic data stream DA scenario (+6.7%) compared to the baseline. Our results also demonstrate that DSAN exhibits remarkable level of explainability when evaluated on COVID-19 and skin cancer datasets. These results contribute to the understanding of DA techniques and offer valuable insight into the effective adaptation of models to medical data. </p>
<blockquote>
<p>é¢†åŸŸè‡ªé€‚åº”ï¼ˆDAï¼‰æŠ€æœ¯åœ¨æœºå™¨å­¦ä¹ ä¸­æœ‰æ½œåŠ›é€šè¿‡åˆ©ç”¨æºåŸŸçš„ä¿¡æ¯æ¥ç¼“è§£è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚åœ¨å›¾åƒåˆ†ç±»ä¸­ï¼Œå¤§å¤šæ•°DAæ–¹é¢çš„è¿›å±•éƒ½æ˜¯ä½¿ç”¨è‡ªç„¶å›¾åƒè€ŒéåŒ»å­¦æ•°æ®å–å¾—çš„ï¼ŒåŒ»å­¦æ•°æ®æ›´éš¾å¤„ç†ã€‚æ­¤å¤–ï¼Œå³ä½¿å¯¹äºè‡ªç„¶å›¾åƒï¼Œä¸»æµæ•°æ®é›†çš„ä½¿ç”¨ä¹Ÿå¯èƒ½å¯¼è‡´æ€§èƒ½åè§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ›´å¥½åœ°äº†è§£DAå¯¹äºè‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒçš„å¥½å¤„ï¼Œå¯¹ä¸ƒç§å¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†ç±»çš„DAæŠ€æœ¯è¿›è¡Œäº†557é¡¹æ¨¡æ‹Ÿç ”ç©¶ï¼Œæ¶‰åŠäº”ä¸ªè‡ªç„¶æ•°æ®é›†å’Œå…«ä¸ªåŒ»å­¦æ•°æ®é›†ï¼Œæ¶µç›–å„ç§åœºæ™¯ï¼Œå¦‚è·¨åˆ†å¸ƒã€åŠ¨æ€æ•°æ®æµå’Œæœ‰é™çš„è®­ç»ƒæ ·æœ¬ã€‚æˆ‘ä»¬çš„å®éªŒäº§ç”Ÿäº†è¯¦ç»†çš„ç»“æœå’Œæ·±åˆ»çš„è§‚å¯Ÿç»“æœï¼Œçªå‡ºäº†è¿™äº›æŠ€æœ¯åœ¨æ€§èƒ½å’ŒåŒ»å­¦åº”ç”¨æ–¹é¢çš„è¡¨ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†Deep Subdomain Adaptation Networkï¼ˆDSANï¼‰ç®—æ³•çš„å‡ºè‰²æ€§èƒ½ã€‚è¯¥ç®—æ³•åœ¨COVID-19æ•°æ®é›†ä¸Šä½¿ç”¨Resnet50å–å¾—äº†å¯è¡Œçš„åˆ†ç±»å‡†ç¡®ç‡ï¼ˆ91.2%ï¼‰ï¼Œå¹¶åœ¨åŠ¨æ€æ•°æ®æµDAåœºæ™¯ä¸­å®ç°äº†ä¸åŸºçº¿ç›¸æ¯”çš„é‡è¦å‡†ç¡®ç‡æ”¹è¿›ï¼ˆ+6.7%ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œåœ¨COVID-19å’Œçš®è‚¤ç™Œæ•°æ®é›†ä¸Šè¯„ä¼°æ—¶ï¼ŒDSANè¡¨ç°å‡ºæé«˜çš„å¯è§£é‡Šæ€§ã€‚è¿™äº›ç»“æœæœ‰åŠ©äºäº†è§£DAæŠ€æœ¯ï¼Œå¹¶ä¸ºæ¨¡å‹åœ¨åŒ»å­¦æ•°æ®ä¸Šçš„æœ‰æ•ˆé€‚åº”æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20537v1">PDF</a> Accepted in Information Sciences</p>
<p><strong>æ‘˜è¦</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨æœºå™¨å­¦ä¹ ä¸­çš„åŸŸé€‚åº”ï¼ˆDAï¼‰æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨æºåŸŸçš„ä¿¡æ¯æ¥ç¼“è§£è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚åœ¨å›¾åƒåˆ†ç±»æ–¹é¢ï¼Œå¤§å¤šæ•°DAçš„è¿›å±•éƒ½æ˜¯ä½¿ç”¨è‡ªç„¶å›¾åƒè€ŒéåŒ»å­¦æ•°æ®æ¥å®Œæˆçš„ï¼ŒåŒ»å­¦æ•°æ®æ›´éš¾å¤„ç†ã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯è‡ªç„¶å›¾åƒï¼Œä½¿ç”¨ä¸»æµæ•°æ®é›†ä¹Ÿå¯èƒ½å¯¼è‡´æ€§èƒ½åè§ã€‚æœ¬ç ”ç©¶ä¸ºäº†æ›´æ·±å…¥åœ°äº†è§£DAåœ¨è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¸­çš„ä¼˜åŠ¿ï¼Œè¿›è¡Œäº†557æ¬¡æ¨¡æ‹Ÿç ”ç©¶ï¼Œä½¿ç”¨äº†ä¸ƒç§å¹¿æ³›ä½¿ç”¨çš„DAæŠ€æœ¯ï¼Œæ¶‰åŠäº”ä¸ªè‡ªç„¶å’Œå…«ä¸ªåŒ»å­¦æ•°æ®é›†ï¼Œæ¶µç›–äº†å„ç§åœºæ™¯ï¼Œå¦‚è¶…å‡ºåˆ†å¸ƒèŒƒå›´ã€åŠ¨æ€æ•°æ®æµå’Œæœ‰é™çš„è®­ç»ƒæ ·æœ¬ã€‚å®éªŒäº§ç”Ÿäº†è¯¦ç»†çš„ç»“æœå’Œæ·±åˆ»çš„è§‚å¯Ÿç»“æœï¼Œçªå‡ºäº†è¿™äº›æŠ€æœ¯åœ¨æ€§èƒ½å’ŒåŒ»å­¦åº”ç”¨æ–¹é¢çš„è¡¨ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDeep Subdomain Adaptation Networkï¼ˆDSANï¼‰ç®—æ³•è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚è¯¥ç®—æ³•åœ¨COVID-19æ•°æ®é›†ä¸Šä½¿ç”¨Resnet50è¾¾åˆ°äº†å¯è¡Œçš„åˆ†ç±»ç²¾åº¦ï¼ˆ91.2%ï¼‰ï¼Œå¹¶ä¸”åœ¨åŠ¨æ€æ•°æ®æµDAåœºæ™¯ä¸­ç›¸æ¯”åŸºçº¿æ–¹æ³•ç²¾åº¦æé«˜äº†+6.7%ã€‚ç ”ç©¶ç»“æœè¿˜è¡¨æ˜ï¼ŒDSANåœ¨COVID-19å’Œçš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å¯è§£é‡Šæ€§è¡¨ç°å“è¶Šã€‚è¿™äº›ç»“æœæœ‰åŠ©äºç†è§£DAæŠ€æœ¯ï¼Œå¹¶ä¸ºæ¨¡å‹åœ¨åŒ»å­¦æ•°æ®ä¸Šçš„æœ‰æ•ˆé€‚åº”æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶é€šè¿‡557æ¬¡æ¨¡æ‹Ÿç ”ç©¶è¯„ä¼°äº†ä¸ƒç§å¹¿æ³›ä½¿ç”¨çš„DAæŠ€æœ¯åœ¨å›¾åƒåˆ†ç±»ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠäº”ä¸ªè‡ªç„¶å’Œå…«ä¸ªåŒ»å­¦æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§åœºæ™¯ï¼Œå¦‚è¶…å‡ºåˆ†å¸ƒã€åŠ¨æ€æ•°æ®æµå’Œæœ‰é™çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>DSANç®—æ³•åœ¨COVID-19æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„åˆ†ç±»ç²¾åº¦ï¼ˆ91.2%ï¼‰ï¼Œå¹¶ä¸”åœ¨åŠ¨æ€æ•°æ®æµåœºæ™¯ä¸­ç›¸æ¯”å…¶ä»–æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>DSANåœ¨è§£é‡Šæ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨COVID-19å’Œçš®è‚¤ç™Œæ•°æ®é›†ä¸Šã€‚</li>
<li>DAæŠ€æœ¯å¯¹äºå¤„ç†åŒ»å­¦å›¾åƒå…·æœ‰æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆé€‚åº”åŒ»å­¦æ•°æ®ã€‚</li>
<li>ç ”ç©¶ç»“æœæœ‰åŠ©äºæ·±å…¥ç†è§£DAæŠ€æœ¯åœ¨å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20537v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20537v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20537v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-What-is-Worth-Learning-Active-and-Sequential-Domain-Adaptation-for-Multi-modal-Gross-Tumor-Volume-Segmentation"><a href="#Learning-What-is-Worth-Learning-Active-and-Sequential-Domain-Adaptation-for-Multi-modal-Gross-Tumor-Volume-Segmentation" class="headerlink" title="Learning What is Worth Learning: Active and Sequential Domain Adaptation   for Multi-modal Gross Tumor Volume Segmentation"></a>Learning What is Worth Learning: Active and Sequential Domain Adaptation   for Multi-modal Gross Tumor Volume Segmentation</h2><p><strong>Authors:Jingyun Yang, Guoqing Zhang, Jingge Wang, Yang Li</strong></p>
<p>Accurate gross tumor volume segmentation on multi-modal medical data is critical for radiotherapy planning in nasopharyngeal carcinoma and glioblastoma. Recent advances in deep neural networks have brought promising results in medical image segmentation, leading to an increasing demand for labeled data. Since labeling medical images is time-consuming and labor-intensive, active learning has emerged as a solution to reduce annotation costs by selecting the most informative samples to label and adapting high-performance models with as few labeled samples as possible. Previous active domain adaptation (ADA) methods seek to minimize sample redundancy by selecting samples that are farthest from the source domain. However, such one-off selection can easily cause negative transfer, and access to source medical data is often limited. Moreover, the query strategy for multi-modal medical data remains unexplored. In this work, we propose an active and sequential domain adaptation framework for dynamic multi-modal sample selection in ADA. We derive a query strategy to prioritize labeling and training on the most valuable samples based on their informativeness and representativeness. Empirical validation on diverse gross tumor volume segmentation tasks demonstrates that our method achieves favorable segmentation performance, significantly outperforming state-of-the-art ADA methods. Code is available at the git repository: \href{<a target="_blank" rel="noopener" href="https://github.com/Hiyoochan/mmActS%7D%7BmmActS%7D">https://github.com/Hiyoochan/mmActS}{mmActS}</a>. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€åŒ»å­¦æ•°æ®ä¸Šè¿›è¡Œç²¾ç¡®çš„è‚¿ç˜¤ä½“ç§¯åˆ†å‰²å¯¹äºé¼»å’½ç™Œå’Œèƒ¶è´¨æ¯ç»†èƒç˜¤çš„æ”¾å°„æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æ·±åº¦ç¥ç»ç½‘ç»œé¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²å¸¦æ¥äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå¯¼è‡´å¯¹æ ‡è®°æ•°æ®çš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚ç”±äºåŒ»å­¦å›¾åƒæ ‡æ³¨è€—æ—¶ä¸”åŠ³åŠ¨å¯†é›†å‹ï¼Œä¸»åŠ¨å­¦ä¹ ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆå·²ç»å‡ºç°ï¼Œé€šè¿‡é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶åˆ©ç”¨å°½å¯èƒ½å°‘çš„æ ‡æ³¨æ ·æœ¬é€‚åº”é«˜æ€§èƒ½æ¨¡å‹ï¼Œä»è€Œå‡å°‘æ ‡æ³¨æˆæœ¬ã€‚ä¹‹å‰çš„ä¸»åŠ¨åŸŸé€‚åº”ï¼ˆADAï¼‰æ–¹æ³•è¯•å›¾é€šè¿‡é€‰æ‹©è·ç¦»æºåŸŸæœ€è¿œçš„æ ·æœ¬æ¥å‡å°‘æ ·æœ¬å†—ä½™ã€‚ç„¶è€Œï¼Œè¿™ç§ä¸€æ¬¡æ€§é€‰æ‹©å¾ˆå®¹æ˜“å¯¼è‡´è´Ÿè¿ç§»ï¼Œå¹¶ä¸”å¾€å¾€éš¾ä»¥è·å–æºåŒ»å­¦æ•°æ®ã€‚æ­¤å¤–ï¼Œå¤šæ¨¡æ€åŒ»å­¦æ•°æ®çš„æŸ¥è¯¢ç­–ç•¥ä»ç„¶æœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸»åŠ¨å’Œåºè´¯çš„åŸŸé€‚åº”æ¡†æ¶ï¼Œç”¨äºADAä¸­çš„åŠ¨æ€å¤šæ¨¡æ€æ ·æœ¬é€‰æ‹©ã€‚æˆ‘ä»¬æ ¹æ®æ ·æœ¬çš„ä¿¡æ¯é‡å’Œä»£è¡¨æ€§åˆ¶å®šäº†ä¸€ä¸ªæŸ¥è¯¢ç­–ç•¥ï¼Œä¼˜å…ˆæ ‡æ³¨å’Œè®­ç»ƒæœ€æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚åœ¨å¤šç§è‚¿ç˜¤ä½“ç§¯åˆ†å‰²ä»»åŠ¡ä¸Šçš„å®è¯éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è‰¯å¥½çš„åˆ†å‰²æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„ADAæ–¹æ³•ã€‚ä»£ç å¯åœ¨gitä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Hiyoochan/mmActS">mmActS</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20528v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€åŒ»å­¦æ•°æ®çš„ä¸»åŠ¨åºåˆ—åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€é€‰æ‹©æ ·æœ¬ã€‚è¯¥æ¡†æ¶é€šè¿‡è€ƒè™‘æ ·æœ¬çš„ä¿¡æ¯æ€§å’Œä»£è¡¨æ€§æ¥ä¼˜å…ˆæ ‡è®°å’Œè®­ç»ƒæœ€æœ‰ä»·å€¼çš„æ ·æœ¬ï¼Œä»è€Œæé«˜æ”¾å°„æ²»ç–—è®¡åˆ’ä¸­çš„è‚¿ç˜¤ä½“ç§¯åˆ†å‰²å‡†ç¡®æ€§ã€‚ç»éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§è‚¿ç˜¤ä½“ç§¯åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰ADAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®çš„å¤šæ¨¡æ€åŒ»å­¦æ•°æ®è‚¿ç˜¤ä½“ç§¯åˆ†å‰²å¯¹é¼»å’½ç™Œå’Œèƒ¶è´¨æ¯ç»†èƒç˜¤çš„æ”¾å°„æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢çš„æœ€æ–°è¿›å±•å¯¼è‡´äº†æ ‡æ³¨æ•°æ®éœ€æ±‚çš„å¢åŠ ã€‚</li>
<li>åŒ»å­¦å›¾åƒæ ‡æ³¨è€—æ—¶ä¸”åŠ³åŠ›å¯†é›†ï¼Œå› æ­¤å‡ºç°äº†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•æ¥å‡å°‘æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>ä»¥å¾€çš„ä¸»åŠ¨åŸŸé€‚åº”ï¼ˆADAï¼‰æ–¹æ³•é€šè¿‡é€‰æ‹©è·ç¦»æºåŸŸæœ€è¿œçš„æ ·æœ¬æ¥å‡å°‘æ ·æœ¬å†—ä½™ï¼Œä½†è¿™ç§æ–¹æ³•å®¹æ˜“å¯¼è‡´è´Ÿé¢è¿ç§»ï¼Œå¹¶ä¸”è·å–æºåŒ»å­¦æ•°æ®å¾€å¾€å—é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸»åŠ¨åºåˆ—åŸŸè‡ªé€‚åº”æ¡†æ¶ï¼Œç”¨äºåŠ¨æ€å¤šæ¨¡æ€æ ·æœ¬é€‰æ‹©ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†æ ·æœ¬çš„ä¿¡æ¯æ€§å’Œä»£è¡¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§è‚¿ç˜¤ä½“ç§¯åˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®è¯éªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„ADAæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20528v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20528v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20528v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.20528v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Event-Enriched-Image-Analysis-Grand-Challenge-at-ACM-Multimedia-2025"><a href="#Event-Enriched-Image-Analysis-Grand-Challenge-at-ACM-Multimedia-2025" class="headerlink" title="Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025"></a>Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025</h2><p><strong>Authors:Thien-Phuc Tran, Minh-Quang Nguyen, Minh-Triet Tran, Tam V. Nguyen, Trong-Le Do, Duy-Nam Ly, Viet-Tham Huynh, Khanh-Duy Le, Mai-Khiem Tran, Trung-Nghia Le</strong></p>
<p>The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM Multimedia 2025, introduces the first large-scale benchmark for event-level multimodal understanding. Traditional captioning and retrieval tasks largely focus on surface-level recognition of people, objects, and scenes, often overlooking the contextual and semantic dimensions that define real-world events. EVENTA addresses this gap by integrating contextual, temporal, and semantic information to capture the who, when, where, what, and why behind an image. Built upon the OpenEvents V1 dataset, the challenge features two tracks: Event-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval. A total of 45 teams from six countries participated, with evaluation conducted through Public and Private Test phases to ensure fairness and reproducibility. The top three teams were invited to present their solutions at ACM Multimedia 2025. EVENTA establishes a foundation for context-aware, narrative-driven multimedia AI, with applications in journalism, media analysis, cultural archiving, and accessibility. Further details about the challenge are available at the official homepage: <a target="_blank" rel="noopener" href="https://ltnghia.github.io/eventa/eventa-2025">https://ltnghia.github.io/eventa/eventa-2025</a>. </p>
<blockquote>
<p>åœ¨ACMå¤šåª’ä½“2025ä¸»åŠçš„äº‹ä»¶ä¸°å¯Œå›¾åƒåˆ†æï¼ˆEVENTAï¼‰å¤§èµ›ä¸­ï¼Œå¼•å…¥äº†äº‹ä»¶çº§åˆ«å¤šæ¨¡å¼ç†è§£çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚ä¼ ç»Ÿçš„æè¿°å’Œæ£€ç´¢ä»»åŠ¡ä¸»è¦å…³æ³¨å¯¹äººç‰©ã€ç‰©ä½“å’Œåœºæ™¯çš„è¡¨å±‚è¯†åˆ«ï¼Œå¾€å¾€å¿½è§†äº†å®šä¹‰çœŸå®ä¸–ç•Œäº‹ä»¶çš„ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ç»´åº¦ã€‚EVENTAé€šè¿‡æ•´åˆä¸Šä¸‹æ–‡ã€æ—¶é—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œæ•æ‰å›¾åƒçš„â€œè°ã€ä½•æ—¶ã€ä½•åœ°ã€ä½•äº‹å’Œä¸ºä½•â€æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚è¯¥æŒ‘æˆ˜åŸºäºOpenEvents V1æ•°æ®é›†ï¼Œè®¾æœ‰ä¸¤ä¸ªèµ›é“ï¼šäº‹ä»¶ä¸°å¯Œå›¾åƒæ£€ç´¢å’Œæè¿°ï¼Œä»¥åŠåŸºäºäº‹ä»¶çš„å›¾åƒæ£€ç´¢ã€‚å…±æœ‰æ¥è‡ªå…­ä¸ªå›½å®¶çš„45æ”¯é˜Ÿä¼å‚èµ›ï¼Œè¯„ä¼°åˆ†ä¸ºå…¬å¼€æµ‹è¯•é˜¶æ®µå’Œç§ä¸‹æµ‹è¯•é˜¶æ®µè¿›è¡Œï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§å’Œå¯é‡å¤æ€§ã€‚å‰ä¸‰åé˜Ÿä¼è¢«é‚€è¯·åœ¨ACMå¤šåª’ä½“2025ä¸Šå±•ç¤ºä»–ä»¬çš„è§£å†³æ–¹æ¡ˆã€‚EVENTAä¸ºè¯­å¢ƒæ„ŸçŸ¥ã€å™äº‹é©±åŠ¨çš„å¤šåª’ä½“äººå·¥æ™ºèƒ½å¥ å®šäº†åŸºç¡€ï¼Œåœ¨æ–°é—»ã€åª’ä½“åˆ†æã€æ–‡åŒ–å½’æ¡£å’Œå¯è®¿é—®æ€§ç­‰æ–¹é¢æœ‰åº”ç”¨ã€‚æœ‰å…³è¯¥æŒ‘æˆ˜çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®å®˜æ–¹ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://ltnghia.github.io/eventa/eventa-2025%E3%80%82">https://ltnghia.github.io/eventa/eventa-2025ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18904v1">PDF</a> ACM Multimedia 2025</p>
<p><strong>Summary</strong><br>     äº‹ä»¶ä¸°å¯Œçš„å›¾åƒåˆ†æï¼ˆEVENTAï¼‰æŒ‘æˆ˜èµ›äºACMå¤šåª’ä½“ä¼šè®®ä¸»åŠï¼Œå»ºç«‹é¦–ä¸ªå¤§è§„æ¨¡äº‹ä»¶çº§åˆ«å¤šåª’ä½“ç†è§£åŸºå‡†æµ‹è¯•ã€‚è¯¥æŒ‘æˆ˜é€šè¿‡é›†æˆä¸Šä¸‹æ–‡ã€æ—¶é—´å’Œè¯­ä¹‰ä¿¡æ¯æ•æ‰å›¾åƒçš„â€œè°â€ã€â€œä½•æ—¶â€ã€â€œä½•åœ°â€ã€â€œä½•äº‹â€å’Œâ€œä¸ºä½•â€ï¼Œè§£å†³äº†ä¼ ç»Ÿæ ‡æ³¨å’Œæ£€ç´¢ä»»åŠ¡å¿½ç•¥çš„ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ç»´åº¦é—®é¢˜ã€‚è¯¥æŒ‘æˆ˜åŒ…æ‹¬ä¸¤ä¸ªèµ›é“ï¼šäº‹ä»¶ä¸°å¯Œå›¾åƒæ£€ç´¢å’Œæ ‡æ³¨ï¼Œä»¥åŠåŸºäºäº‹ä»¶çš„å›¾åƒæ£€ç´¢ã€‚å…±æœ‰æ¥è‡ªå…­ä¸ªå›½å®¶çš„45æ”¯é˜Ÿä¼å‚ä¸æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å…¬å¼€å’Œç§ä¸‹æµ‹è¯•é˜¶æ®µè¿›è¡Œå…¬å¹³å’Œå¯é‡å¤æ€§çš„è¯„ä¼°ã€‚æ’åå‰ä¸‰çš„å›¢é˜Ÿå—é‚€åœ¨ACMå¤šåª’ä½“ä¼šè®®ä¸Šè¿›è¡Œæ–¹æ¡ˆå±•ç¤ºã€‚EVENTAä¸ºè¯­å¢ƒæ„ŸçŸ¥ã€å™äº‹é©±åŠ¨çš„å¤šåª’ä½“äººå·¥æ™ºèƒ½å»ºç«‹äº†åŸºç¡€ï¼Œå¹¶åº”ç”¨äºæ–°é—»ä¸šã€åª’ä½“åˆ†æã€æ–‡åŒ–å½’æ¡£å’Œå¯è®¿é—®æ€§ç­‰é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EVENTAæ˜¯é¦–ä¸ªå¤§è§„æ¨¡äº‹ä»¶çº§åˆ«å¤šåª’ä½“ç†è§£çš„åŸºå‡†æµ‹è¯•ï¼ŒäºACMå¤šåª’ä½“ä¼šè®®ä¸¾åŠã€‚</li>
<li>è¯¥æŒ‘æˆ˜è§£å†³äº†ä¼ ç»Ÿå›¾åƒæ ‡æ³¨å’Œæ£€ç´¢ä»»åŠ¡å¿½ç•¥çš„ä¸Šä¸‹æ–‡å’Œè¯­ä¹‰ç»´åº¦é—®é¢˜ã€‚</li>
<li>EVENTAé€šè¿‡é›†æˆä¸Šä¸‹æ–‡ã€æ—¶é—´å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œæ·±å…¥æ•æ‰å›¾åƒå†…å®¹ã€‚</li>
<li>æŒ‘æˆ˜åŒ…æ‹¬ä¸¤ä¸ªèµ›é“ï¼šäº‹ä»¶ä¸°å¯Œå›¾åƒæ£€ç´¢å’Œæ ‡æ³¨ï¼Œä»¥åŠåŸºäºäº‹ä»¶çš„å›¾åƒæ£€ç´¢ã€‚</li>
<li>å…±æœ‰æ¥è‡ªå…­ä¸ªå›½å®¶çš„45æ”¯é˜Ÿä¼å‚ä¸æŒ‘æˆ˜ï¼Œé€šè¿‡å…¬å¼€å’Œç§ä¸‹æµ‹è¯•é˜¶æ®µè¿›è¡Œå…¬å¹³è¯„ä¼°ã€‚</li>
<li>EVENTAçš„æˆç«‹ä¸ºè¯­å¢ƒæ„ŸçŸ¥ã€å™äº‹é©±åŠ¨çš„å¤šåª’ä½“äººå·¥æ™ºèƒ½æä¾›äº†åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18904v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18904v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18904v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18904v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SWiFT-Soft-Mask-Weight-Fine-tuning-for-Bias-Mitigation"><a href="#SWiFT-Soft-Mask-Weight-Fine-tuning-for-Bias-Mitigation" class="headerlink" title="SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation"></a>SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation</h2><p><strong>Authors:Junyu Yan, Feng Chen, Yuyang Xue, Yuning Du, Konstantinos Vilouras, Sotirios A. Tsaftaris, Steven McDonagh</strong></p>
<p>Recent studies have shown that Machine Learning (ML) models can exhibit bias in real-world scenarios, posing significant challenges in ethically sensitive domains such as healthcare. Such bias can negatively affect model fairness, model generalization abilities and further risks amplifying social discrimination. There is a need to remove biases from trained models. Existing debiasing approaches often necessitate access to original training data and need extensive model retraining; they also typically exhibit trade-offs between model fairness and discriminative performance. To address these challenges, we propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that efficiently improves fairness while preserving discriminative performance with much less debiasing costs. Notably, SWiFT requires only a small external dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to first find the relative, and yet distinct, contributions of model parameters to both bias and predictive performance. Then, a two-step fine-tuning process updates each parameter with different gradient flows defined by its contribution. Extensive experiments with three bias sensitive attributes (gender, skin tone, and age) across four dermatological and two chest X-ray datasets demonstrate that SWiFT can consistently reduce model bias while achieving competitive or even superior diagnostic accuracy under common fairness and accuracy metrics, compared to the state-of-the-art. Specifically, we demonstrate improved model generalization ability as evidenced by superior performance on several out-of-distribution (OOD) datasets. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¯èƒ½ä¼šè¡¨ç°å‡ºåè§ï¼Œè¿™åœ¨åŒ»ç–—ç­‰ä¼¦ç†æ•æ„Ÿé¢†åŸŸå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è¿™ç§åè§å¯èƒ½ä¼šå¯¹æ¨¡å‹çš„å…¬å¹³æ€§ã€æ¨¡å‹æ³›åŒ–èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¹¶å¯èƒ½è¿›ä¸€æ­¥åŠ å‰§ç¤¾ä¼šæ­§è§†ã€‚éœ€è¦ä»è®­ç»ƒæ¨¡å‹ä¸­æ¶ˆé™¤åè§ã€‚ç°æœ‰çš„å»åæ–¹æ³•é€šå¸¸éœ€è¦è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ï¼Œå¹¶éœ€è¦å¤§é‡æ¨¡å‹é‡æ–°è®­ç»ƒï¼›å®ƒä»¬é€šå¸¸åœ¨æ¨¡å‹å…¬å¹³æ€§å’Œåˆ¤åˆ«æ€§èƒ½ä¹‹é—´è¡¨ç°å‡ºæƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Soft-Mask Weight Fine-Tuningï¼ˆSWiFTï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å»åæ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„å…¬å¹³æ€§ï¼ŒåŒæ—¶ä¿æŒå…¶åˆ¤åˆ«æ€§èƒ½ï¼Œå¹¶ä¸”å»åæˆæœ¬è¾ƒä½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSWiFTä»…éœ€ä¸€ä¸ªå¤–éƒ¨å°å‹æ•°æ®é›†å’Œå‡ æ¬¡æ¨¡å‹å¾®è°ƒå‘¨æœŸã€‚SWiFTèƒŒåçš„ç†å¿µæ˜¯é¦–å…ˆæ‰¾å‡ºæ¨¡å‹å‚æ•°å¯¹åè§å’Œé¢„æµ‹æ€§èƒ½çš„ç›¸å¯¹ä¸”ç‹¬ç‰¹çš„è´¡çŒ®ã€‚ç„¶åï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„å¾®è°ƒè¿‡ç¨‹ä¼šæ ¹æ®æ¯ä¸ªå‚æ•°çš„è´¡çŒ®ï¼Œä½¿ç”¨ä¸åŒçš„æ¢¯åº¦æµæ¥æ›´æ–°å®ƒã€‚åœ¨å››ä¸ªçš®è‚¤ç§‘å’Œä¸¤ä¸ªèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šè¿›è¡Œçš„é’ˆå¯¹ä¸‰ä¸ªæ•æ„Ÿå±æ€§ï¼ˆæ€§åˆ«ã€è‚¤è‰²å’Œå¹´é¾„ï¼‰çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSWiFTèƒ½å¤ŸæŒç»­å‡å°‘æ¨¡å‹åè§ï¼ŒåŒæ—¶åœ¨å¸¸è§çš„å…¬å¹³æ€§å’Œå‡†ç¡®æ€§æŒ‡æ ‡ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç”šè‡³æ›´é«˜çš„è¯Šæ–­å‡†ç¡®ç‡ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å…¶åœ¨å‡ ä¸ªç¦»ç¾¤æ•°æ®é›†ä¸Šçš„å“è¶Šè¡¨ç°è¯æ˜äº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å¢å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18826v2">PDF</a> Accepted for publication at the Journal of Machine Learning for   Biomedical Imaging (MELBA) <a target="_blank" rel="noopener" href="https://melba-journal.org/2025:015">https://melba-journal.org/2025:015</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¯èƒ½è¡¨ç°å‡ºåè§ï¼Œè¿™åœ¨åŒ»ç–—ç­‰ä¼¦ç†æ•æ„Ÿé¢†åŸŸå¸¦æ¥äº†æŒ‘æˆ˜ã€‚åè§ä¼šå½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¯èƒ½åŠ å‰§ç¤¾ä¼šæ­§è§†ã€‚éœ€è¦æ¶ˆé™¤è®­ç»ƒæ¨¡å‹ä¸­çš„åè§ã€‚ç°æœ‰çš„å»åæ–¹æ³•é€šå¸¸éœ€è¦è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®å¹¶è¿›è¡Œå¤§é‡çš„æ¨¡å‹å†è®­ç»ƒï¼Œå¹¶ä¸”é€šå¸¸åœ¨æ¨¡å‹å…¬å¹³æ€§å’Œè¾¨åˆ«æ€§èƒ½ä¹‹é—´è¡¨ç°å‡ºæƒè¡¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Soft-Mask Weight Fine-Tuningï¼ˆSWiFTï¼‰å»åæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒè¾¨åˆ«æ€§èƒ½çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°æé«˜äº†æ¨¡å‹çš„å…¬å¹³æ€§ï¼Œå¹¶ä¸”å»åæˆæœ¬è¾ƒä½ã€‚SWiFTä»…éœ€ä¸€ä¸ªå°å‹å¤–éƒ¨æ•°æ®é›†å’Œå‡ æ¬¡æ¨¡å‹å¾®è°ƒå‘¨æœŸã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯å…ˆæ‰¾å‡ºæ¨¡å‹å‚æ•°å¯¹åè§å’Œé¢„æµ‹æ€§èƒ½çš„ç›¸å¯¹ä½†ç‹¬ç‰¹çš„è´¡çŒ®ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªä¸¤æ­¥éª¤å¾®è°ƒè¿‡ç¨‹ï¼Œæ ¹æ®æ¯ä¸ªå‚æ•°çš„è´¡çŒ®ï¼Œç”¨ä¸åŒçš„æ¢¯åº¦æµè¿›è¡Œæ›´æ–°ã€‚åœ¨å¤šä¸ªçš®è‚¤ç—…å˜å’Œèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSWiFTåœ¨å¸¸è§çš„å…¬å¹³æ€§å’Œå‡†ç¡®æ€§æŒ‡æ ‡ä¸‹ï¼Œå¯ä»¥æŒç»­å‡å°‘æ¨¡å‹åè§ï¼Œå¹¶ä¸”åœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ç”šè‡³æ›´ä¼˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æé«˜çš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œè¿™åœ¨è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®é›†ä¸Šçš„è¡¨ç°å¾—åˆ°äº†è¯å®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å¯èƒ½å‡ºç°åè§ï¼Œè¿™åœ¨åŒ»ç–—ç­‰ä¼¦ç†æ•æ„Ÿé¢†åŸŸå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>åè§ä¼šå½±å“æ¨¡å‹çš„å…¬å¹³æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¯èƒ½åŠ å‰§ç¤¾ä¼šæ­§è§†ã€‚</li>
<li>ç°æœ‰å»åæ–¹æ³•é€šå¸¸éœ€è¦è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®å¹¶è¿›è¡Œå¤§é‡å†è®­ç»ƒï¼Œä¸”åœ¨æ¨¡å‹å…¬å¹³æ€§å’Œæ€§èƒ½é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>æå‡ºçš„Soft-Mask Weight Fine-Tuning (SWiFT)æ¡†æ¶æ—¨åœ¨é€šè¿‡å¾®è°ƒæ¨¡å‹å‚æ•°å»åï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„è¾¨åˆ«æ€§èƒ½å’Œå…¬å¹³æ€§ã€‚</li>
<li>SWiFTä»…éœ€ä¸€ä¸ªå°å¤–éƒ¨æ•°æ®é›†å’Œå°‘é‡å¾®è°ƒå‘¨æœŸï¼Œé™ä½äº†å»åæˆæœ¬ã€‚</li>
<li>SWiFTé€šè¿‡è¯†åˆ«æ¨¡å‹å‚æ•°å¯¹åè§å’Œé¢„æµ‹æ€§èƒ½çš„è´¡çŒ®ï¼Œè¿›è¡Œä¸¤æ­¥å¾®è°ƒè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18826v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18826v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18826v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_åŒ»å­¦å›¾åƒ/2508.18826v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_TTS/2409.10969v2/page_5_2.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  Enhancing Code-switched Text-to-Speech Synthesis Capability in Large   Language Models with only Monolingual Corpora
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5400a337ce684bd7d16d13bdeaee8f68.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-08  FreeSplatter Pose-free Gaussian Splatting for Sparse-view 3D   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
