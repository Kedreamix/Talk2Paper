<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-05-03  T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-03-更新"><a href="#2025-05-03-更新" class="headerlink" title="2025-05-03 更新"></a>2025-05-03 更新</h1><h2 id="T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT"><a href="#T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT" class="headerlink" title="T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT"></a>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT</h2><p><strong>Authors:Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li</strong></p>
<p>Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a> </p>
<blockquote>
<p>最近的大型语言模型进展展示了思维链（CoT）和强化学习（RL）如何提升性能。然而，将此类推理策略应用于视觉生成领域仍待大量探索。在本文中，我们提出了T2I-R1，这是一种新颖的推理增强文本到图像生成模型，通过双级CoT推理过程的强化学习驱动。具体来说，我们确定了两个级别的CoT，可以用来增强生成的不同阶段：（1）语义级的CoT用于提示的高级规划和（2）令牌级的CoT用于斑块对斑块生成过程中的低级像素处理。为了更好地协调这两个级别的CoT，我们引入了BiCoT-GRPO，采用生成奖励集合，可以在同一训练步骤中无缝优化两种生成CoT。通过将我们的推理策略应用于基线模型Janus-Pro，我们在T2I-CompBench上实现了13%的改进，在WISE基准测试上实现了19%的改进，甚至超越了最先进的模型FLUX.1。代码可在<a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CaraJ7/T2I-R1找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00703v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型结合强化学习与双重思维链（CoT）推理过程（语义级和token级）的文本到图像生成模型T2I-R1。通过引入BiCoT-GRPO来协调这两种层次的推理，实现在同一训练步骤中对两种推理的优化。将推理策略应用于基线模型Janus-Pro后，在T2I-CompBench和WISE基准测试中取得了显著的性能提升，超越了现有最佳模型FLUX。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I-R1模型结合了强化学习（RL）与双重思维链（CoT）推理过程，用于文本到图像的生成任务。</li>
<li>模型包含两个层次的CoT推理：语义级CoT用于高级规划提示，token级CoT用于低级的像素处理。</li>
<li>BiCoT-GRPO被引入以协调这两个层次的CoT推理，实现优化生成过程。</li>
<li>T2I-R1模型在基线模型Janus-Pro的基础上应用推理策略，实现了显著的性能提升。</li>
<li>在T2I-CompBench和WISE基准测试中，T2I-R1模型超越了现有最佳模型FLUX。</li>
<li>模型性能的提升得益于强化学习和双重思维链推理的结合，这种结合有助于改进模型的决策过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00703">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-638cb2be2e21963d60c5f4ae260eac12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b799cc52d19d12cf4d975e022450830b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be9d027594a5d021598e8c542bd13e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f9eb0339ee5839957df728e9b36af8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rethinking-Memory-in-AI-Taxonomy-Operations-Topics-and-Future-Directions"><a href="#Rethinking-Memory-in-AI-Taxonomy-Operations-Topics-and-Future-Directions" class="headerlink" title="Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future   Directions"></a>Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future   Directions</h2><p><strong>Authors:Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan</strong></p>
<p>Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI%7D%7Bhttps://github.com/Elvin-Yiming-Du/Survey/_Memory/_in/_AI%7D.%7D">https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}</a>. </p>
<blockquote>
<p>记忆是人工智能系统的基础组成部分，支撑着基于大语言模型的智能体。尽管先前的调查主要关注大型语言模型中的记忆应用，但它们经常忽略底层内存动态的原子操作。在这项调查中，我们首先将内存表示分类为参数表示、上下文结构化表示和上下文非结构化表示，然后介绍了六种基本内存操作：整合、更新、索引、遗忘、检索和压缩。我们系统地将这些操作映射到长期记忆、长上下文环境、参数修改和多源记忆中最相关的主题。通过原子操作和表示类型的透镜重新审视内存系统，本调查为与人工智能相关的研究、基准数据集和工具提供了结构化和动态的视角，并阐明了基于大型语言模型的智能体中功能性交互的同时概述了未来研究的前景。\footnote{论文清单、数据集、方法和工具可访问链接：<a target="_blank" rel="noopener" href="https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI%7D%E3%80%82">https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00675v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMs基于代理的智能系统以记忆为关键要素。现有调查多关注LLM中的记忆应用，却忽略了支撑其功能的原子操作。本文通过分类记忆表征（参数型、上下文结构型和上下文非结构型）及介绍六种基本记忆操作（整合、更新、索引、遗忘、检索和压缩），将其与长期记忆、长语境、参数修改和多源记忆等相关研究主题相联系。本文重新构建AI中的记忆系统视角，以原子操作和表征类型作为观察视角，对与AI中的记忆相关的研究、基准数据集和工具进行结构化且动态的阐述，同时明确了LLMs代理的功能交互，并指出了未来研究的潜在方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文调研了LLM智能系统中基于记忆的基本要素，聚焦于先前被忽视的记忆原子操作研究。</li>
<li>文章提出了三类记忆表征方式：参数型、上下文结构型和上下文非结构型。</li>
<li>文章详细介绍了六种关键记忆操作：整合、更新、索引、遗忘、检索和压缩。</li>
<li>文章首次系统性地将这些操作映射到相关的研究主题上，包括长期记忆和语境敏感性研究等。</li>
<li>该调查为读者提供了一个清晰框架来理解AI系统中的记忆结构，特别是在LLM背景下的记忆功能交互。</li>
<li>文章提供了相关领域的基准数据集和方法论工具，以帮助未来研究工作的开展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a549b77d086c21c0ff822ca6c2130c3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-946f3c1019ea72cc736ce962cf97c8a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b420821e103b84cc46563e0e02f9c055.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e54e0efd46c82335642a0adb428e744.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeepCritic-Deliberate-Critique-with-Large-Language-Models"><a href="#DeepCritic-Deliberate-Critique-with-Large-Language-Models" class="headerlink" title="DeepCritic: Deliberate Critique with Large Language Models"></a>DeepCritic: Deliberate Critique with Large Language Models</h2><p><strong>Authors:Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen</strong></p>
<p>As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback. </p>
<blockquote>
<p>随着大型语言模型（LLM）的快速发展，对其输出提供准确的反馈和可扩展的监督成为一个紧迫且关键的问题。利用LLM作为批评模型来实现自动化监督是一个有前景的解决方案。在这项工作中，我们专注于研究和提高LLM的数学批判能力。当前的LLM评论家对每一步的评论过于肤浅，导致判断准确性低，难以为LLM生成器提供足够的反馈来纠正错误。为了解决这一问题，我们提出了一种新颖有效的两阶段框架来开发能够有针对性地对数学解决方案的每一步推理进行批判的LLM评论家。在第一阶段，我们利用Qwen2.5-72B-Instruct生成4.5K长形式的评论作为监督微调种子数据。每条种子评论都包含有针对性的步骤批判，包括多视角验证以及每个推理步骤的初步评论的深入批判。然后，我们对微调后的模型使用PRM800K的现有手工标注数据或我们通过蒙特卡洛采样法估计的正确性自动标注的数据进行强化学习，以进一步激励其批判能力。我们基于Qwen2.5-7B-Instruct开发的批评模型不仅在各种错误识别基准测试中显著优于现有LLM评论家（包括相同规模的DeepSeek-R1-distill模型和GPT-4o），而且更有效地帮助LLM生成器通过更详细的反馈来修正错误步骤。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00662v1">PDF</a> Work in progress. Data and models are available at   <a target="_blank" rel="noopener" href="https://github.com/RUCBM/DeepCritic">https://github.com/RUCBM/DeepCritic</a></p>
<p><strong>Summary</strong>：随着大型语言模型（LLM）的快速发展，对其输出进行准确的反馈和可扩展的监督成为了一个紧迫且关键的问题。利用LLM作为批判模型以实现自动化监督是一个有前景的解决方案。本研究专注于研究和提高LLM的数学批判能力。当前LLM的批判过于肤浅，导致判断准确性低，难以向LLM生成器提供足够的反馈以纠正错误。为解决此问题，研究提出了一种新型的两阶段框架来开发能够有针对性地对数学解决方案的每个推理步骤进行批判的LLM批判模型。第一阶段利用Qwen2.5-72B-Instruct生成长评论文据，用于监督微调。然后对模型进行强化学习，使用人类标注的数据或自动注释数据来进一步激励其批判能力。开发的批判模型不仅显著优于现有LLM批判模型，更有效地帮助LLM生成器通过更详细的反馈来修正错误步骤。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLMs在自动监督方面展现出巨大潜力，特别是在数学批判能力上。</li>
<li>当前LLM的批判反馈过于肤浅，导致判断准确性低。</li>
<li>研究提出了一种新型的两阶段框架来开发增强数学批判能力的LLM模型。</li>
<li>第一阶段利用Qwen2.5-72B-Instruct生成的长评论文据用于监督微调模型。</li>
<li>强化学习用于进一步提升模型的批判能力，使用人类或自动注释数据。</li>
<li>开发的批判模型在错误识别基准测试中显著优于其他LLM批判模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00662">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-106a5c46b9bc0bd6f929d45431ed4661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9a1db7a7e0b1968e52ace7210f70f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f65a9f4e0212542596056ca1ea36e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9506250eeab1f12b11547f2bd8cd47a3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Investigating-Task-Arithmetic-for-Zero-Shot-Information-Retrieval"><a href="#Investigating-Task-Arithmetic-for-Zero-Shot-Information-Retrieval" class="headerlink" title="Investigating Task Arithmetic for Zero-Shot Information Retrieval"></a>Investigating Task Arithmetic for Zero-Shot Information Retrieval</h2><p><strong>Authors:Marco Braga, Pranav Kasela, Alessandro Raganato, Gabriella Pasi</strong></p>
<p>Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR">https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各种自然语言处理任务中表现出了令人印象深刻的零样本性能，包括文档重新排序。然而，它们在未见任务和数据集上的效果会降低，这主要是由于词汇和词分布的变化。在本文中，我们研究了任务算术（Task Arithmetic）技术，这是一种通过简单的数学运算（如加法或减法）结合在不同任务或数据集上预训练的LLM权重，以适应检索模型而无需额外的微调技术。我们的方法能够将多样化的任务和领域知识合成到一个单一模型中，从而在不同的检索上下文中实现有效的零样本适应。在公开可用的科学、生物医学和多语言数据集上的大量实验表明，我们的方法在NDCG@10和P@10上的重新排序性能提高了最多达18%和15%。除了这些经验性收益之外，我们的分析还深入探讨了任务算术作为零样本学习和模型适应的实际策略的优缺点。我们在<a target="_blank" rel="noopener" href="https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR公开提供我们的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00649v1">PDF</a> Accepted in SIGIR ‘25</p>
<p><strong>Summary</strong></p>
<p>LLM在多种NLP任务中表现出零样本性能。本文通过任务算术技术结合在不同任务或领域上预训练的LLM权重，实现模型的有效零样本适应。通过公开可用的科学、生物医学和多语种数据集的实验表明，该方法可提高重排性能，并提供了对任务算术作为零样本学习和模型适应策略的优缺点分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多种NLP任务中显示出零样本性能，但其在未见任务和领域上的效果会下降。</li>
<li>任务算术技术结合了在不同任务或领域上预训练的LLM权重，通过简单的数学运算（如加减）来适应检索模型，无需额外的微调。</li>
<li>任务算术能够合成不同的任务和领域知识到一个单一模型中，使模型在不同的检索上下文中实现有效的零样本适应。</li>
<li>在公开数据集上的实验表明，该方法能提高重排性能，最高可提高18%的NDCG@10和15%的P@10。</li>
<li>除了这些实证收益外，本文还深入分析了任务算术的优势和局限性，为理解和应用该技术提供了重要见解。</li>
<li>论文提供了代码公开访问，便于其他研究者使用和扩展该技术的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00649">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5109dc743cde1b422b6610ee27709de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af5d549d9cf7bcab99bbddb4d9b90779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a538ec85613d222521827fe9775d00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ed1e584f0dc23adc6b9675833ef5afa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94e1cc01bf574647f38f3849d3aa618c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Illusion-of-Role-Separation-Hidden-Shortcuts-in-LLM-Role-Learning-and-How-to-Fix-Them"><a href="#The-Illusion-of-Role-Separation-Hidden-Shortcuts-in-LLM-Role-Learning-and-How-to-Fix-Them" class="headerlink" title="The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning   (and How to Fix Them)"></a>The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning   (and How to Fix Them)</h2><p><strong>Authors:Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang</strong></p>
<p>Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role – a concept we call \emph{role separation} – is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model’s input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers. </p>
<blockquote>
<p>大型语言模型（LLM）在实践中越来越普遍地集成了多种输入角色（如系统指令、用户查询、外部工具输出）。确保模型准确区分每个角色的信息——我们称之为“角色分离”的概念——对于保持多角色行为的一致性至关重要。尽管最近的工作往往针对最先进的提示注入防御，但尚不清楚这些方法是否真的教会了LLM区分角色，还是仅仅记住了已知触发因素。在本文中，我们研究了“角色分离学习”：教授LLM稳健地区分系统和用户令牌的过程。通过一个简单且受控的实验框架，我们发现精细调整后的模型在识别角色时通常依赖于两个代理：（1）任务类型利用，（2）接近文本开始位置。虽然数据增强可以部分缓解这些捷径，但它通常导致迭代修补而不是更深入的问题解决。为了解决这一问题，我们提出通过调整模型输入编码中的令牌级线索来加强标记角色边界的“不变信号”。特别是，调整位置ID有助于模型学习更清晰的区分度，并减少对表面代理的依赖。通过关注这种以机制为中心的观点，我们的工作阐明了LLM如何更可靠地保持一致的多角色行为，而不仅仅是记住已知的提示或触发因素。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在实践中越来越普遍地整合多重输入角色，如系统指令、用户查询和外部工具输出。确保模型准确区分各种角色的信息，即我们称之为的角色分离，对于保持多角色的行为一致性至关重要。尽管最近有研究表明瞄准先进的提示注入防御，但仍不清楚这些方法是否真正教会LLM区分角色，还是仅仅记住已知触发因素。本文通过考察角色分离学习过程，研究如何稳健地区分系统和用户符号。通过简单、受控的实验框架，我们发现微调模型通常依赖两种角色识别的代理方式：任务类型利用和接近文本开始位置。虽然数据增强可以部分缓解这些捷径，但它通常导致迭代修补而不是深度修复。为解决这一问题，我们提出通过调整模型输入编码中的符号级线索来强化标记角色边界的不变信号。特别是，通过操作位置ID有助于模型学习更清晰的区别，并减少对表面代理的依赖。我们的工作从机制中心的角度揭示了LLM如何更可靠地保持一致的多角色行为，而不会仅仅记住已知的提示或触发因素。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在实践中需要整合多重输入角色，如系统指令、用户查询等。</li>
<li>角色分离是确保LLM在多角色环境中表现一致性的关键。</li>
<li>现有方法可能无法真正教会LLM区分角色，而是可能只是记住已知触发因素。</li>
<li>通过对角色分离学习过程的研究，发现微调模型通常依赖任务类型利用和接近文本开始位置两种角色识别方式。</li>
<li>数据增强可以缓解这些识别方式的局限性，但通常只是迭代修补而非深度修复问题。</li>
<li>强化不变信号以标记角色边界是一种有效的解决方案，可以通过调整模型输入编码中的符号级线索来实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bb5115c87b639f59c2e67a2edd614c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b988e847b23baffd6cfea95c168d7b31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-914573a51498d54b024801d1b2068423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0baffb8445e2220c8f3ccc8368d84822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de6ffbced77b5a67022d511019f2726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2216c66950fbdcdd0221f85fe69f4aed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FineScope-Precision-Pruning-for-Domain-Specialized-Large-Language-Models-Using-SAE-Guided-Self-Data-Cultivation"><a href="#FineScope-Precision-Pruning-for-Domain-Specialized-Large-Language-Models-Using-SAE-Guided-Self-Data-Cultivation" class="headerlink" title="FineScope : Precision Pruning for Domain-Specialized Large Language   Models Using SAE-Guided Self-Data Cultivation"></a>FineScope : Precision Pruning for Domain-Specialized Large Language   Models Using SAE-Guided Self-Data Cultivation</h2><p><strong>Authors:Chaitali Bhattacharyya, Yeseong Kim</strong></p>
<p>Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released. </p>
<blockquote>
<p>训练大型语言模型（LLM）需要从零开始消耗大量的计算资源，这激发了人们对开发更小、特定领域的LLM的兴趣，这些模型既能保持高效率又能实现强大的任务性能。中等规模的模型，如LLaMA等，已经成为特定领域适应性的起点，但在专业数据集上进行测试时，它们往往会出现精度下降的情况。我们引入了FineScope，一个从更大的预训练模型派生出紧凑、域优化LLM的框架。FineScope利用稀疏自动编码器（SAE）框架，受其能产生可解释特征表示能力的启发，从大型数据集中提取域特定子集。我们应用带有域特定约束的结构化修剪，确保修剪后的模型保留目标域的必要知识。为了进一步提高性能，这些修剪后的模型经历了自我数据蒸馏，利用SAE策划的数据集恢复在修剪过程中丢失的关键域特定信息。大量的实验和消融研究表明，FineScope具有高度的竞争力，在特定领域的任务中优于几种大规模的最先进LLM。此外，我们的结果表明，使用FineScope的修剪模型在使用SAE策划数据集进行微调时可以恢复其大部分原始性能。此外，将这些数据集用于微调预训练的LLM而不进行修剪也可以提高其特定领域的准确性，这凸显了我们方法的稳健性。代码将被发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00624v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）训练需要大量计算资源，因此研究者开始关注开发更小、针对特定领域的LLM，以兼顾效率和任务性能。LLaMA等中型模型虽为领域特定适配提供了起点，但在专业数据集上测试时会出现精度下降的问题。本研究提出了FineScope框架，用于从较大的预训练模型中推导出紧凑、针对特定领域的LLM。FineScope利用受启发于可产生可解释特征表示的稀疏自动编码器（SAE）框架，从大型数据集中提取特定领域的子集。通过应用带有特定领域约束的结构化修剪，确保修剪后的模型保留目标领域的关键知识。为进一步增强性能，这些修剪后的模型进行自我数据蒸馏，利用SAE策划的数据集恢复在修剪过程中丢失的关键领域特定信息。实验和消融研究证明，FineScope在特定领域的任务中实现了高度竞争的性能，超越了若干大规模先进LLM。此外，结果还表明，使用SAE策划的数据集微调FineScope修剪的模型，可使其恢复大部分原始性能。同时，将这些数据集应用于未经修剪的预训练LLM的微调，也能提高其在特定领域的精度，凸显了此方法的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM训练需大量计算资源，促使研究者关注更小、针对特定领域的LLM。</li>
<li>中型LLM在特定数据集上可能存在精度下降的问题。</li>
<li>FineScope框架可从大型预训练模型中推导出紧凑、针对特定领域的LLM。</li>
<li>FineScope利用Sparse Autoencoder（SAE）框架提取领域特定数据子集。</li>
<li>通过结构化修剪和领域特定约束，确保模型保留关键领域知识。</li>
<li>修剪后的模型通过自我数据蒸馏增强性能，利用SAE策划的数据集恢复丢失信息。</li>
<li>FineScope在特定领域任务中表现优异，提高模型性能的同时保持模型紧凑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00624">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5fc9813480c33677818a2e8dc20af7fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8798fd5bf3afb36791bcc49e554e3b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c40547cbfb0993856bd878e699c7cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb197e89a0d5de4c2d744a488fa0492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a50cb74f0d4f73038a372959c071ab97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d443adb39c31a4b5cec2b170d49ff1d2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Block-Circulant-Adapter-for-Large-Language-Models"><a href="#Block-Circulant-Adapter-for-Large-Language-Models" class="headerlink" title="Block Circulant Adapter for Large Language Models"></a>Block Circulant Adapter for Large Language Models</h2><p><strong>Authors:Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang</strong></p>
<p>Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks. </p>
<blockquote>
<p>微调大型语言模型（LLM）因模型规模庞大而具有挑战性。最近的基于傅里叶域的方法显示出降低微调成本的潜力。我们提出了一种基于块循环矩阵的微调方法，并采用稳定的训练启发式策略，以利用循环矩阵和一维傅里叶变换的属性来降低存储和计算成本。实验表明，我们的方法使用的参数数量比VeRA少14倍，比LoRA少16倍，并且与FourierFT相比，浮点运算减少了32倍，同时保持了接近或更好的任务性能。我们的方法为解决下游任务时的模型微调提出了一种颇具前景的基于频域的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00582v1">PDF</a> to appear in Proceedings of the 2025 International Joint Conference   on Artificial Intelligence (IJCAI-2025)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的微调因模型规模巨大而具有挑战性。本研究提出一种基于循环矩阵和稳定训练启发式方法的微调方法，利用循环矩阵和一维傅里叶变换的特性来降低存储和计算成本。实验结果表明，该方法参数使用数量相较于VeRA减少了14倍，相较于LoRA减少了16倍，且相较于FourierFT的浮点运算量减少了32倍，同时保持或提高了任务性能。本研究为在频率域微调大型模型提供了可行的途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的微调因模型规模巨大而具有挑战性。</li>
<li>研究提出了一种基于循环矩阵和稳定训练启发式方法的微调方法。</li>
<li>该方法利用循环矩阵和一维傅里叶变换的特性来降低存储和计算成本。</li>
<li>实验结果显示，该方法相较于其他方法，参数使用数量和浮点运算量显著减少。</li>
<li>该方法能够保持或提高任务性能。</li>
<li>研究为在频率域微调大型语言模型提供了新的视角和途径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6c2737c33d0e279e07c76174abef1ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b6aa3a6ba2434503629f66d71ef62c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5602ffdeb5b5186a3be8a950c52c378d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72ee2a14839f3a480f254ee00afc0c3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00613521c76e9cb54ccaa1df4e8ec9c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3009bc1ba3244770d00e16915847109e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07f1d510a7f45420f0d2a94b203bb0c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ac91cf37c12099b13d8c2f7e1006d05.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Partial-SMILES-Validation-Scheme-for-Enhanced-Drug-Design-in-Reinforcement-Learning-Frameworks"><a href="#Leveraging-Partial-SMILES-Validation-Scheme-for-Enhanced-Drug-Design-in-Reinforcement-Learning-Frameworks" class="headerlink" title="Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks"></a>Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks</h2><p><strong>Authors:Xinyu Wang, Jinbo Bi, Minghu Song</strong></p>
<p>SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery. </p>
<blockquote>
<p>SMILES表达式基础的分子生成在药物发现中已成为一种强大的方法。结合大型语言模型（LLM）的深度强化学习（RL）已被纳入分子生成过程，旨在提高候选分子的可能性匹配得分。然而，这种方法面临的一个关键挑战是强化学习阶段的灾难性遗忘问题，在预训练期间通常超过99%的分子有效性知识会显著下降。目前应用于药物发现的强化学习算法（如REINVENT）使用先验模型作为锚点来保留预训练知识，但这些方法缺乏稳健的探索机制。为了解决这些问题，我们提出了部分SMILES验证-PPO（PSV-PPO），这是一种新型的强化学习算法，它结合了实时部分SMILES验证来防止灾难性遗忘，同时鼓励探索。与传统的只在生成整个序列后进行分子结构验证的强化学习方法不同，PSV-PPO在每一步的自回归过程中进行逐步验证，不仅评估所选的标记候选者，还评估由先前部分序列产生的所有潜在分支。这能够在所有潜在路径中早期检测到无效的局部SMILES。因此，即使在化学空间的广泛激烈探索中，PSV-PPO也能保持较高的有效性率。我们在PMO和GuacaMol基准数据集上的实验表明，PSV-PPO在保持竞争力的探索和优化性能的同时，显著减少了生成的无效结构数量。虽然我们的工作主要集中在保持有效性上，但PSV-PPO的框架在未来研究中可以扩展，以融入更多有价值的领域知识，进一步改进药物发现中的强化学习应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00530v1">PDF</a> 17 pages, 5 main figures, 2 appendix figures. Submitted to ICML 2025</p>
<p><strong>摘要</strong></p>
<p>SMILES语言模型在药物发现中的分子生成方法具有强大的潜力。结合深度强化学习（RL）与大型语言模型（LLM），能够在生成分子候选物时获得高匹配度。然而，RL阶段存在灾难性遗忘的挑战，如分子有效性知识在预训练时往往超过99%，但在RL过程中会显著下降。针对这一问题，本文提出一种新型的强化学习算法——Partial SMILES Validation-PPO（PSV-PPO）。该算法通过实时部分SMILES验证来防止灾难性遗忘，同时鼓励探索。与传统的RL方法不同，PSV-PPO在生成整个序列之前的每个自回归步骤中执行逐步验证，不仅评估所选择的令牌候选者，还评估源自先前部分序列的所有潜在分支。这能够早期检测所有潜在路径中的无效部分SMILES。实验结果表明，PSV-PPO在维持高有效性率的同时，减少了无效生成结构的数量，在PMO和GuacaMol基准数据集上保持了有竞争力的探索和优化性能。虽然我们的工作主要集中在保持有效性上，但PSV-PPO框架未来可纳入其他有价值的领域知识，进一步推动药物发现中的强化学习应用。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SMILES语言模型在药物发现中的分子生成具有强大潜力，结合深度强化学习与大型语言模型可获得高匹配度。</li>
<li>RL阶段存在灾难性遗忘的挑战，导致分子有效性知识在训练过程中显著下降。</li>
<li>提出新型RL算法PSV-PPO，通过实时部分SMILES验证来防止灾难性遗忘，同时鼓励探索。</li>
<li>PSV-PPO在生成序列的每个步骤进行验证，评估所选令牌及其潜在分支，早期检测无效分子结构。</li>
<li>实验结果表明，PSV-PPO在维持高有效性率的同时，减少了无效生成结构的数量。</li>
<li>PSV-PPO在PMO和GuacaMol基准数据集上具有竞争力，表明其在探索和优化方面的效能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00530">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3042b86f43099b2765eb759d42ec1397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f76f6ba4238a1c94e3fc946303164f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdb51657e0f14f212597066eebc721c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c65519b3d1a9b97bc75bf877d38f37ae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-Ablating-Transformers-More-Interpretability-Less-Sparsity"><a href="#Self-Ablating-Transformers-More-Interpretability-Less-Sparsity" class="headerlink" title="Self-Ablating Transformers: More Interpretability, Less Sparsity"></a>Self-Ablating Transformers: More Interpretability, Less Sparsity</h2><p><strong>Authors:Jeremias Ferrao, Luhan Mikaelson, Keenan Pepper, Natalia Perez-Campanero Antolin</strong></p>
<p>A growing intuition in machine learning suggests a link between sparsity and interpretability. We introduce a novel self-ablation mechanism to investigate this connection ante-hoc in the context of language transformers. Our approach dynamically enforces a k-winner-takes-all constraint, forcing the model to demonstrate selective activation across neuron and attention units. Unlike post-hoc methods that analyze already-trained models, our approach integrates interpretability directly into model training, promoting feature localization from inception. Training small models on the TinyStories dataset and employing interpretability tests, we find that self-ablation leads to more localized circuits, concentrated feature representations, and increased neuron specialization without compromising language modelling performance. Surprisingly, our method also decreased overall sparsity, indicating that self-ablation promotes specialization rather than widespread inactivity. This reveals a complex interplay between sparsity and interpretability, where decreased global sparsity can coexist with increased local specialization, leading to enhanced interpretability. To facilitate reproducibility, we make our code available at <a target="_blank" rel="noopener" href="https://github.com/keenanpepper/self-ablating-transformers">https://github.com/keenanpepper/self-ablating-transformers</a>. </p>
<blockquote>
<p>机器学习领域日益增长的直觉表明稀疏性与可解释性之间存在联系。我们引入了一种新型的自消融机制，在语言转换器的背景下，对这种联系进行前瞻性研究。我们的方法动态实施“胜者为王”约束，迫使模型在神经元和注意力单位之间表现出选择性激活。不同于分析已训练模型的后方法，我们的方法直接将可解释性融入模型训练，从而促进从初始阶段就开始的特征定位。在TinyStories数据集上训练小型模型并对其进行可解释性测试，我们发现自消融导致了更局部的电路、集中的特征表示以及增强神经元专业化，同时不会损害语言建模性能。令人惊讶的是，我们的方法还降低了总体稀疏性，这表明自消融促进了专业化而非普遍不活跃。这揭示出稀疏性与可解释性之间的复杂相互作用，其中全球稀疏性的减少可以与局部专业化的增强共存，从而提高可解释性。为便于复现，我们将代码放在<a target="_blank" rel="noopener" href="https://github.com/keenanpepper/self-ablating-transformers%E4%B8%8A%E3%80%82">https://github.com/keenanpepper/self-ablating-transformers上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00509v1">PDF</a> Poster Presentation at Building Trust Workshop at ICLR 2025</p>
<p><strong>Summary</strong><br>     机器学习领域逐渐意识到稀疏性与可解释性之间的联系。本研究引入了一种新型的自消融机制，在语言模型训练过程中直接探究这种联系。通过动态实施“胜者全取”原则，模型表现出神经元和注意力单元的选择性激活。本研究在TinyStories数据集上训练小型模型并进行可解释性测试，发现自消融导致电路更集中、特征表示更集中，神经元专业化增强，且不影响语言建模性能。令人惊讶的是，自消融方法还降低了总体稀疏性，表明自消融促进专业化而非广泛不活跃。这揭示了稀疏性与可解释性之间的复杂相互作用，全局稀疏性的降低可与局部专业化的增加共存，从而提高可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习领域正逐渐认识到稀疏性与可解释性之间的联系。</li>
<li>研究通过自消融机制探究稀疏性与可解释性的联系，直接在模型训练过程中提升可解释性。</li>
<li>自消融机制通过动态实施“胜者全取”原则，使模型在神经元和注意力单元上表现出选择性激活。</li>
<li>在TinyStories数据集上进行的测试表明，自消融能提高模型的可解释性，且不影响语言建模性能。</li>
<li>自消融机制促进了神经元的专业化，使得电路和特征表示更为集中。</li>
<li>自消融方法降低了模型的总体稀疏性，显示出自消融促进模型的专业化而非广泛不活跃的特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00509">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f1c90c21ce5f705c6cc3f556d0078e20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245855e2e02e5faf746f02250b4c1a39.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HalluMix-A-Task-Agnostic-Multi-Domain-Benchmark-for-Real-World-Hallucination-Detection"><a href="#HalluMix-A-Task-Agnostic-Multi-Domain-Benchmark-for-Real-World-Hallucination-Detection" class="headerlink" title="HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World   Hallucination Detection"></a>HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World   Hallucination Detection</h2><p><strong>Authors:Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu</strong></p>
<p>As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84. </p>
<blockquote>
<p>随着大型语言模型（LLM）在高风险领域的部署越来越多，检测虚构内容——即没有基于支持证据的文本——已经成为一项关键挑战。现有的虚构检测基准测试通常是合成生成的，主要集中在提取式问答上，未能捕捉到涉及多文档上下文和完整句子输出的现实世界的复杂性。我们引入了HalluMix基准测试，这是一个多样化、任务无关的数据集，包含来自各个领域的例子和格式。使用这个基准测试，我们评估了七个虚构检测系统——包括开源和闭源系统——突出了在不同任务、文档长度和输入表示之间的性能差异。我们的分析突出了长上下文与短上下文之间的巨大性能差距，对现实世界中的检索增强生成（RAG）实现具有关键影响。Quotient Detections取得了最佳的整体性能，准确率为0.82，F1分数为0.84。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00506v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在高风险领域的应用日益普遍，检测虚构内容——即没有基于支持证据的文本——已成为一项关键挑战。现有虚构检测基准测试通常合成生成，专注于提取性问题回答，未能捕捉涉及多文档背景和完整句子输出的现实世界的复杂性。我们推出HalluMix Benchmark，一个多样且任务无关的数据集，涵盖各种领域和格式的例子。利用此基准测试，我们评估了七个虚构检测系统的性能——包括开源和闭源系统——突显了不同任务、文档长度和输入表示之间的性能差异。分析指出短背景和长背景之间的性能差距显著，对现实世界检索增强生成（RAG）实施具有关键影响。Quotient Detections取得最佳总体性能，准确率为0.82，F1分数为0.84。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在高风险领域的应用中，检测虚构内容至关重要。</li>
<li>现有虚构检测基准测试存在局限性，无法充分反映现实世界的复杂性。</li>
<li>推出HalluMix Benchmark数据集，涵盖多种领域和格式，用于评估虚构检测系统。</li>
<li>评估了七个虚构检测系统的性能，发现任务、文档长度和输入表示之间的性能差异显著。</li>
<li>虚构检测在短背景和长背景下的性能差距显著，对RAG实施具有重要影响。</li>
<li>Quotient Detections在虚构检测方面表现最佳，准确率和F1分数较高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-183171daa384eb83a942a9fb53d4a956.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a0c1aa376876fdb4068f6258fc7f9e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdc915738c0d44091d5bb95e7974628a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44149c38cb242c8222cda09b6167e149.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a92ad2df55ad1a0b1d6e5694b2db1db9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Be-Trusted-for-Evaluating-RAG-Systems-A-Survey-of-Methods-and-Datasets"><a href="#Can-LLMs-Be-Trusted-for-Evaluating-RAG-Systems-A-Survey-of-Methods-and-Datasets" class="headerlink" title="Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and   Datasets"></a>Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and   Datasets</h2><p><strong>Authors:Lorenz Brehme, Thomas Ströhle, Ruth Breu</strong></p>
<p>Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do’s and don’ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations. </p>
<blockquote>
<p>近年来，基于检索的增强生成（RAG）技术取得了显著进展。RAG系统的复杂性涉及索引、检索和生成等多个组件以及其他众多参数，为系统评估和质量提升带来了巨大挑战。先前的研究强调，对RAG系统进行评估对于记录进展、比较配置以及识别特定领域应用的有效方法至关重要。本研究系统地回顾了63篇学术论文，以全面概述最先进的RAG评估方法，重点关注四个关键领域：数据集、检索器、索引和数据库以及生成器组件。我们观察到利用大型语言模型（LLM）自动评估RAG系统各组件的可行性，该模型既能生成评估数据集又能进行评估。此外，我们发现进一步的实际研究对于为企业提供实施和评估RAG系统的明确指导至关重要。通过综合RAG关键组件的评估方法并强调针对基准测试创建和适应特定领域的数据集，我们为推进系统评估方法和提高RAG系统评估的严谨性做出了贡献。此外，通过检查利用LLM的自动化方法与人类判断之间的相互作用，我们为关于平衡自动化和人类输入的持续讨论做出了贡献，澄清了它们各自的贡献、局限性和在实现稳健和可靠评估方面的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20119v2">PDF</a> 8 Pages. This paper has been accepted for presentation at the IEEE   Swiss Conference on Data Science (SDS25)</p>
<p><strong>Summary</strong>：</p>
<p>近期，检索增强生成（RAG）技术取得了显著进展。由于其系统的复杂性涉及索引、检索、生成等多个组件及众多参数，为系统评估和质量提升带来了巨大挑战。本研究通过系统综述了63篇学术论文，全面概述了最先进的RAG评估方法，重点关注数据集、检索器、索引和数据库以及生成器组件等四个关键领域。研究观察到利用大型语言模型（LLM）自动评估RAG系统各组件的可行性。此外，还发现需要更多实践研究为公司提供实施和评估RAG系统的明确指导。通过合成关键RAG组件的评估方法并强调针对基准测试的域特定数据集的创建和适应性改进，研究促进了系统评估方法的进步，提高了RAG系统的评估严谨性。同时，通过检查利用LLM的自动化方法和人类判断之间的相互作用，研究为平衡自动化和人类输入的持续讨论做出了贡献，明确了各自的贡献、局限性和实现稳健和可靠评估的挑战。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>RAG系统因其复杂性和多组件结构面临评估挑战。</li>
<li>学术界对于RAG评估方法进行了广泛研究，涵盖数据集、检索器、索引和数据库以及生成器组件等关键领域。</li>
<li>利用LLM进行自动评估是可行的，并为RAG系统的评估提供了新的方向。</li>
<li>实践研究对于指导公司实施和评估RAG系统至关重要。</li>
<li>合成关键RAG组件的评估方法并强调域特定数据集的适应性改进有助于推动系统评估方法的进步。</li>
<li>在自动化和人类判断之间找到平衡是实现稳健和可靠评估的关键。</li>
<li>LLM在自动化评估中的潜力和挑战值得进一步探讨。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20119">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e84c9e9b6d7be96f535aff156ca8c2e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf8cc311b3f1172a91c452052296ec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff183a9424052ed8145b17bcfe614c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2b4c9dc4f16e084cfd66ea08738bec1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81b6cac17039c9186ea21f955e9cf302.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LGD-Leveraging-Generative-Descriptions-for-Zero-Shot-Referring-Image-Segmentation"><a href="#LGD-Leveraging-Generative-Descriptions-for-Zero-Shot-Referring-Image-Segmentation" class="headerlink" title="LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image   Segmentation"></a>LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image   Segmentation</h2><p><strong>Authors:Jiachen Li, Qing Xie, Renshu Gu, Jinyu Xu, Yongjian Liu, Xiaohan Yu</strong></p>
<p>Zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. Previous works address this challenge by utilizing Vision-Language Models and mask proposal networks for region-text matching. However, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. To alleviate this issue, we present LGD (Leveraging Generative Descriptions), a framework that utilizes the advanced language generation capabilities of Multi-Modal Large Language Models to enhance region-text matching performance in Vision-Language Models. Specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the Multi-Modal Large Language Models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. Secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. The proposed method achieves new state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU compared to previous methods. </p>
<blockquote>
<p>零镜头引用图像分割旨在基于引用表达式定位并分割目标区域，其主要挑战在于在没有训练的情况下实现视觉和文本模态之间的语义对齐和匹配。以前的工作通过利用视觉语言模型和掩膜提案网络进行区域文本匹配来解决这一挑战。然而，这种范式可能导致由于自由形式引用表达式的固有模糊性和多样性而导致目标定位不正确。为了缓解这个问题，我们提出了LGD（利用生成描述），这是一个利用多模态大型语言模型的先进语言生成能力来提高视觉语言模型中区域文本匹配性能的框架。具体来说，我们首先设计两种提示，属性提示和周围提示，来引导多模态大型语言模型生成与指代对象的关键属性以及周围对象的细节相关的描述，分别称为属性描述和周围描述。其次，引入了三种视觉文本匹配分数来评估实例级视觉特征和文本特征之间的相似性，这决定了与引用表达式最相关的掩膜。所提出的方法在RefCOCO、RefCOCO+和RefCOCOg三个公共数据集上实现了最新最先进的性能，与以前的方法相比，在IoU和mIoU方面分别提高了9.97%和11.29%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14467v2">PDF</a> </p>
<p><strong>Summary</strong>：该研究提出了一个利用多模态大型语言模型提升零样本指向性图像分割性能的框架。它利用视觉语言模型和语言生成能力来生成与关键属性和周围对象相关的描述，提高视觉文本匹配的性能。在三个公共数据集上实现了最高性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>零样本指向性图像分割旨在基于描述性语句定位并分割目标区域。</li>
<li>主要挑战在于在没有训练的情况下实现视觉和文本语义的匹配和对应。</li>
<li>研究者利用视觉语言模型和掩模提议网络进行区域文本匹配来解决这一挑战。</li>
<li>由于自由形式的描述性语句固有的模糊性和多样性，现有方法可能导致目标定位错误。</li>
<li>研究提出了一种名为LGD的新框架，利用多模态大型语言模型的生成能力来提高区域文本匹配性能。</li>
<li>LGD通过设计属性提示和周围提示来引导大型语言模型生成与关键属性和周围对象相关的描述。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1d50972146b0cc49a11c4dd33948f05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>强化学习（RL）可以直接提升大语言模型的推理能力，而无需过多依赖监督微调（SFT）。在这项工作中，我们重新审视了传统的策略梯度（PG）机制，并提出了一种极简的RL方法，称为组策略梯度（GPG）。与常规方法不同，GPG直接优化原始的RL目标，从而无需替代损失函数。通过消除批评者和参考模型，避免KL散度约束，并解决优势和梯度估计偏差的问题，我们的方法大大简化了与组相对策略优化（GRPO）相比的训练过程。我们的方法在不依赖辅助技术或调整的情况下实现了卓越的性能。如图1所示，大量实验表明，我们的方法不仅降低了计算成本，而且在各种单模态和多模态任务上均优于GRPO。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPG上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v3">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习（RL）可直接提升大语言模型的推理能力，无需过多依赖监督微调（SFT）。本研究重新审视了传统的策略梯度（PG）机制，并提出了一种极简的RL方法——群组策略梯度（GPG）。GPG直接优化原始的RL目标，从而无需替代损失函数。通过消除批评者和参考模型，避免KL散度约束，并解决优势和梯度估计偏差的问题，该方法大大简化了与群组相对策略优化（GRPO）的训练过程。该方法在不依赖辅助技术或调整的情况下实现了卓越的性能。如图一所示，大量实验证明，该方法不仅降低了计算成本，而且在各种单模态和多模态任务上持续优于GRPO。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习可以增强大语言模型的推理能力，且不需要依赖大量的监督微调。</li>
<li>研究者提出了一种新的强化学习方法——群组策略梯度（GPG）。</li>
<li>GPG直接优化原始的RL目标，无需使用替代损失函数。</li>
<li>GPG简化了训练过程，与传统的策略优化方法相比，它消除了复杂组件并解决了相关的问题。</li>
<li>GPG方法在各种任务上表现优越，包括单模态和多模态任务。</li>
<li>该方法的计算成本较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0154bfaf18875c7cae4ded2d987de76b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb3eeaa103c7e595684cf660d6970abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bfb126d0f4a84aad9796b7bb3580cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9977e8d13e79fe1db75e4260ba5d161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea27ba65c8fe28125622d097454df66c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="KAP-MLLM-assisted-OCR-Text-Enhancement-for-Hybrid-Retrieval-in-Chinese-Non-Narrative-Documents"><a href="#KAP-MLLM-assisted-OCR-Text-Enhancement-for-Hybrid-Retrieval-in-Chinese-Non-Narrative-Documents" class="headerlink" title="KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese   Non-Narrative Documents"></a>KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese   Non-Narrative Documents</h2><p><strong>Authors:Hsin-Ling Hsu, Ping-Sheng Lin, Jing-Di Lin, Jengnan Tzeng</strong></p>
<p>Hybrid Retrieval systems, combining Sparse and Dense Retrieval methods, struggle with Traditional Chinese non-narrative documents due to their complex formatting, rich vocabulary, and the insufficient understanding of Chinese synonyms by common embedding models. Previous approaches inadequately address the dual needs of these systems, focusing mainly on general text quality improvement rather than optimizing for retrieval. We propose Knowledge-Aware Preprocessing (KAP), a novel framework that transforms noisy OCR outputs into retrieval-optimized text. KAP adopts a two-stage approach: it first extracts text using OCR, then employs Multimodal Large Language Models to refine the output by integrating visual information from the original documents. This design reduces OCR noise, reconstructs structural elements, and formats the text to satisfy the distinct requirements of sparse and dense retrieval. Empirical results demonstrate that KAP consistently and significantly outperforms conventional preprocessing approaches. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/JustinHsu1019/KAP">https://github.com/JustinHsu1019/KAP</a>. </p>
<blockquote>
<p>混合检索系统结合了稀疏和密集检索方法，由于传统中文非叙事文档的复杂格式、丰富词汇，以及常见嵌入模型对中文同义词理解不足，因此它们在这方面面临挑战。之前的方法不能满足这些系统的双重需求，主要集中在提高一般文本质量上，而不是优化检索。我们提出了知识感知预处理（KAP），这是一种将嘈杂的OCR输出转换为优化检索文本的新型框架。KAP采用两阶段方法：首先使用OCR提取文本，然后采用多模态大型语言模型对输出进行细化，通过集成原始文档中的视觉信息来完善输出。这种设计降低了OCR噪声，重建了结构元素，并将文本格式化为满足稀疏和密集检索的特定要求。经验结果表明，KAP始终显著优于传统预处理技术。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/JustinHsu1019/KAP">https://github.com/JustinHsu1019/KAP</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08452v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了混合检索系统在传统中文非叙事文档中的挑战，并提出了知识感知预处理（KAP）框架。该框架通过OCR技术提取文本，并利用多模态大型语言模型整合原始文档中的视觉信息，以优化输出。KAP的设计旨在减少OCR噪声，重建结构元素，并格式化文本以满足稀疏和密集检索的特定要求。实证结果表明，KAP在预处理方面表现优异，显著优于传统方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>混合检索系统在处理传统中文非叙事文档时面临挑战，主要由于文档复杂格式、丰富词汇及常用嵌入模型对中文同义词理解不足。</li>
<li>现有方法主要关注通用文本质量提升，未能满足优化检索的双重需求。</li>
<li>提出知识感知预处理（KAP）框架，通过两阶段方法转换噪声OCR输出为检索优化文本。</li>
<li>KAP首先使用OCR提取文本，然后利用多模态大型语言模型结合原始文档的视觉信息进行优化。</li>
<li>KAP设计降低OCR噪声，重建结构元素，满足稀疏和密集检索的特定要求。</li>
<li>实证研究证明KAP在预处理方面表现优异，显著优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4930f1db2c9231a7c063b0de6400a02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c917a27fce3855290b205bd7b5ee9ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dadeee1049b9b472e8a8cd4c93102c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e298983ca557f7799ab60845fd02979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c59807177c5ef3d143ac6f921676ddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a935ac683d9d34b85b6bfa0a6729148e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8922e96d47dda4f1d13e4383bce0b019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a21bfa3258378e38d37a45f20f966cfe.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="UoR-NCL-at-SemEval-2025-Task-1-Using-Generative-LLMs-and-CLIP-Models-for-Multilingual-Multimodal-Idiomaticity-Representation"><a href="#UoR-NCL-at-SemEval-2025-Task-1-Using-Generative-LLMs-and-CLIP-Models-for-Multilingual-Multimodal-Idiomaticity-Representation" class="headerlink" title="UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models   for Multilingual Multimodal Idiomaticity Representation"></a>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models   for Multilingual Multimodal Idiomaticity Representation</h2><p><strong>Authors:Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang</strong></p>
<p>SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at <a target="_blank" rel="noopener" href="https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL">https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL</a>. </p>
<blockquote>
<p>SemEval-2025 Task 1的重点是根据给定的具有英语和巴西葡萄牙语中习惯用法的复合名词来排列图像。为了应对这一挑战，这项工作使用生成式大型语言模型（LLM）和多语言CLIP模型，以增强习惯用法的复合名词表示。LLM为潜在的习惯用语复合词生成习惯用法意义，丰富了它们的语义解释。这些意义随后使用多语言CLIP模型进行编码，作为图像排序的表示。对比学习和数据增强技术被应用于微调这些嵌入，以提高性能。实验结果表明，通过这种方法提取的多模式表示优于仅基于原始名义复合词的表示。微调方法显示出有希望的成果，但不如不使用微调直接使用嵌入有效。本文使用的源代码可在<a target="_blank" rel="noopener" href="https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL">https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20984v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了SemEval-2025任务1，该任务聚焦于根据含有英语和巴西葡萄牙语中习语含义的名词短语对图像进行排序。为解决这一挑战，该研究采用生成式大型语言模型（LLMs）和多语种CLIP模型，以增强习语短语的表现。LLMs为潜在习语生成含义，丰富了语义解释。随后使用多语种CLIP模型对这些含义进行编码，作为图像排序的代表。该研究还应用了对比学习和数据增强技术来微调这些嵌入，以提高性能。实验结果表明，通过该方法提取的多模式表示优于仅基于原始名词化合物的表示。微调方法显示出有希望的结果，但不如不使用微调的嵌入方法有效。本文使用的源代码可在<a target="_blank" rel="noopener" href="https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL">链接</a>找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SemEval-2025 Task 1集中于根据含有习语含义的名词短语对图像进行排序。</li>
<li>研究使用生成式大型语言模型（LLMs）和多语种CLIP模型以增强习语表现。</li>
<li>LLMs能够生成潜在习语的含义，丰富语义解释。</li>
<li>多语种CLIP模型用于编码这些含义，作为图像排序的代表。</li>
<li>对比学习和数据增强技术用于微调嵌入，以提高性能。</li>
<li>实验结果显示多模式表示方法优于仅基于原始名词化合物的表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20984">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d077575da05e3cb402a96a2a4df7c574.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60f0a6ab3c54c56c049bbbad331333e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86fa327fa1f274f4b66b72300a16ed07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f1d315f2cffe1cca9d274aa127bc131.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2499f74813f50c21247d055952a6ffdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b69a62c1fbd50a7fe320cd222973ebe.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MotionGlot-A-Multi-Embodied-Motion-Generation-Model"><a href="#MotionGlot-A-Multi-Embodied-Motion-Generation-Model" class="headerlink" title="MotionGlot: A Multi-Embodied Motion Generation Model"></a>MotionGlot: A Multi-Embodied Motion Generation Model</h2><p><strong>Authors:Sudarshan Harithas, Srinath Sridhar</strong></p>
<p>This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications. </p>
<blockquote>
<p>本文介绍了MotionGlot模型，该模型能够在多种具有不同动作维度的载体上生成动作，如四足机器人和人体。通过借鉴大型语言模型（LLM）常用的训练流程，我们引入了专为运动相关任务设计的指令调整模板。我们的方法表明，LLM训练的基本原理可以成功适应不同载体和不同动作维度的广泛运动生成任务。我们在一组6个任务上展示了MotionGlot的各种功能，并报告了跨任务的平均改进率为35.3%。此外，我们还提供了两个新的数据集：（1）专家控制的四足运动数据集，包含大约48,000条带有方向性文本注释的轨迹；（2）包含超过23,000个情境文本提示的人类运动生成任务数据集。最后，我们通过硬件实验验证了我们的系统在现实世界应用中的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16623v2">PDF</a> </p>
<p><strong>Summary</strong><br>MotionGlot模型可生成跨多种行动维度的不同实体的动作，如四足机器人和人体。该研究采用大型语言模型（LLM）的常见训练程序，并引入针对运动相关任务的指令调整模板。研究证明LLM训练原理可成功适应不同行动维度的多个实体的广泛运动生成任务。MotionGlot在6项任务上的平均改进率为35.3%。此外，研究还贡献了两个新数据集：包含约48000条与方向文本注释配对轨迹的专家控制四足行走数据集，以及包含超过23000条用于人体运动生成任务的情境文本提示数据集。最后，通过硬件实验验证了系统的现实应用能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MotionGlot模型可以生成跨多种实体的动作，这些实体具有不同的行动维度，如四足机器人和人体。</li>
<li>该研究利用大型语言模型的训练程序，并引入指令调整模板，为运动相关任务提供定制解决方案。</li>
<li>研究证明了LLM训练原理可以成功适应多种运动生成任务，这些任务涉及不同行动维度的多个实体。</li>
<li>MotionGlot在多个任务上的平均性能改进达到35.3%。</li>
<li>研究贡献了两个新数据集，包括专家控制的四足行走数据集和用于人体运动生成任务的情境文本提示数据集。</li>
<li>通过硬件实验验证了系统的现实应用能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16623">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20160fe1ccc96901d06b1ef6060f0f5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da234cca253b0b543dfe4a40c12881c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd1a422d9cc0ecb9996d7bfde8b04c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-962e88927e82637906fc09bf0c95f144.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56cc93f2573a474d64bfe26488cb5ff7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reward-Augmented-Data-Enhances-Direct-Preference-Alignment-of-LLMs"><a href="#Reward-Augmented-Data-Enhances-Direct-Preference-Alignment-of-LLMs" class="headerlink" title="Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"></a>Reward-Augmented Data Enhances Direct Preference Alignment of LLMs</h2><p><strong>Authors:Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang</strong></p>
<p>Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/shenao-zhang/reward-augmented-preference">https://github.com/shenao-zhang/reward-augmented-preference</a>. </p>
<blockquote>
<p>偏好对齐在大语言模型（LLM）中的应用显著提高了它们遵循人类指令和意图的能力。然而，现有的直接对齐算法主要关注相对偏好，往往忽视响应的定性方面，尽管可以访问包含来自判断模型的奖励分数的AI反馈偏好数据。努力最大化所选和略为逊色的被拒绝响应之间的隐含奖励差距可能会导致过度拟合和不必要的对高质量被拒绝响应的遗忘。对奖励分数的无知也驱使LLM不加区别地偏爱低质量的选定响应，并无法推广到数据稀缺的最佳响应。为了克服这些缺点，我们的研究引入了奖励条件LLM策略，能够识别并从数据集内的整个响应质量谱中学习，有助于推断出更优化的区域。我们提出了一种有效而简单的数据重新标记方法，该方法根据质量分数对偏好对进行条件处理，以构建奖励增强数据集。在不同基准测试和多种模型上的实验表明，我们的方法始终如一地大幅提高了DPO。通过全面的消融研究，我们证明了我们的方法不仅最大限度地利用了偏好数据，而且缓解了遗忘问题，证明了其在单纯的数据扩展之外的广泛有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/shenao-zhang/reward-augmented-preference%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shenao-zhang/reward-augmented-preference找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08067v4">PDF</a> Published at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的偏好对齐显著提高了其遵循人类指令和意图的能力。然而，现有的直接对齐算法主要关注相对偏好，忽视响应的定性方面，尽管在AI反馈期间可以访问包含判断模型奖励分数的偏好数据。努力最大化所选和略为逊色的拒绝响应之间的隐性奖励差距可能导致过度拟合和不必要的对高质量拒绝响应的遗忘。忽视奖励分数还导致LLM不加区别地偏爱低质量的选定响应，并且无法概括数据稀缺的最佳响应。为了克服这些缺点，我们的研究引入了以奖励为条件的LLM策略，能够辨别并学习数据集内响应质量的全谱，有助于推断出更优化的区域。我们提出了一种有效而简单的数据重新标记方法，根据质量分数对偏好对进行条件处理，以构建奖励增强数据集。在不同基准测试和模型的实验表明，我们的方法持续且大幅提升了DPO。通过全面的消除研究，我们证明了我们的方法不仅最大限度地利用了偏好数据，还缓解了遗忘问题，这证明了其广泛的实用性超越了单纯的数据扩展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>偏好对齐在大型语言模型（LLM）中显著提高了遵循人类指令和意图的能力。</li>
<li>现有直接对齐算法主要关注相对偏好，但忽视响应的定性方面。</li>
<li>过度追求隐性奖励差距可能导致过度拟合和对高质量拒绝响应的遗忘。</li>
<li>忽视奖励分数会导致LLM不加区别地偏爱低质量响应，且无法概括最佳响应。</li>
<li>研究引入了奖励增强的大型语言模型策略，能学习响应质量的全谱并推断出更优化的区域。</li>
<li>提出了一种有效且简单的数据重新标记方法，构建了奖励增强数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d39ff82a3fae9a1e58f47209862ea203.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d57ddba54ee6c2055dac69c3c3528e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4068b68258a092e04f7cf43fed9616b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a595d5c489d826aa4b9b7eab8b51688.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Automated-Review-Generation-Method-Based-on-Large-Language-Models"><a href="#Automated-Review-Generation-Method-Based-on-Large-Language-Models" class="headerlink" title="Automated Review Generation Method Based on Large Language Models"></a>Automated Review Generation Method Based on Large Language Models</h2><p><strong>Authors:Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong</strong></p>
<p>Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers’ processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users’ domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts’ properties. Through multi-layered quality control, we effectively mitigated LLMs’ hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5% with 95% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations. </p>
<blockquote>
<p>文献研究对于科学工作至关重要，但随着信息量的激增，研究者面临处理信息的能力瓶颈挑战。我们提出了一种基于大型语言模型（LLM）的自动摘要生成方法，以提高效率，减轻认知负担。我们的统计验证评估框架显示，生成的摘要与人工摘要质量相当甚至更好，且广泛应用于各个领域，无需用户具备专业知识。在丙烷脱氢（PDH）催化剂的应用中，我们的方法迅速分析了343篇文章，平均每篇文章每个LLM账户只需几秒钟，生成了涵盖35个主题的全面摘要。通过对1041篇文章进行深入分析，为催化剂的性能提供了见解。通过多层次的质量控制，我们有效缓解了LLM的幻觉问题。专家验证确认了我们系统的准确性和引用完整性，同时表明幻觉风险已降低到低于0.5%，并具有95%的信心。发布的Windows应用程序可实现一键生成摘要，提高研究生产力和文献推荐效率，为更广泛的科学探索奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20906v5">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration">https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration</a> Data:   <a target="_blank" rel="noopener" href="https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData">https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData</a> This research   has been invited for a Short Oral presentation at the 18th ICC -   International Congress on Catalysis, taking place in Lyon, France from July   14-19, 2024 Published at <a target="_blank" rel="noopener" href="https://doi.org/10.1093/nsr/nwaf169">https://doi.org/10.1093/nsr/nwaf169</a> for newer   edition</p>
<p><strong>Summary</strong><br>基于文献信息爆炸式增长所带来的挑战，研究者提出了一种基于大型语言模型（LLM）的自动化摘要生成方法，旨在提高研究效率并降低认知负荷。该方法在丙烷脱氢（PDH）催化剂研究中的应用，展示了其在快速分析大量文献中的效率和准确性。该方法能有效降低LLM产生的幻觉风险，同时提供专家验证确认其准确性和引文完整性。该方法的推广使用有望提高研究生产力和文献推荐效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）用于自动化摘要生成，解决文献信息量大带来的挑战。</li>
<li>该方法在丙烷脱氢催化剂研究中的应用展示了其高效性和准确性。</li>
<li>通过多层质量控制有效降低了LLM产生的幻觉风险。</li>
<li>专家验证确认了该方法的准确性和引文完整性。</li>
<li>该方法提高了研究生产力和文献推荐效率。</li>
<li>该方法具有广泛的应用性，适用于不同研究领域，无需用户具备特定领域知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-23e7918f921284540f280c1fe5adad04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1161d974cf143db871edff25ba7d31a9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EvoPrompt-Connecting-LLMs-with-Evolutionary-Algorithms-Yields-Powerful-Prompt-Optimizers"><a href="#EvoPrompt-Connecting-LLMs-with-Evolutionary-Algorithms-Yields-Powerful-Prompt-Optimizers" class="headerlink" title="EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful   Prompt Optimizers"></a>EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful   Prompt Optimizers</h2><p><strong>Authors:Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang</strong></p>
<p>Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms. </p>
<blockquote>
<p>大型语言模型（LLM）在各种任务中表现出色，但它们依赖于精心设计的提示，通常需要大量的人工努力。为了自动化这个过程，本文提出了一种用于离散提示优化的新型框架，名为EvoPrompt，它借鉴了进化算法（EA）的思想，因为进化算法具有良好的性能和快速的收敛速度。为了使EA能够在离散提示上工作，这些离散提示是自然语言表达式，需要连贯且人类可读，我们将LLM与EA连接起来。这种方法使我们能够同时利用LLM的强大语言处理能力和EA的高效优化性能。具体来说，EvoPrompt不涉及任何梯度或参数，它从提示种群开始，基于进化算子使用LLM迭代生成新的提示，根据开发集改进种群。我们对封闭和开源的LLM进行了提示优化，包括GPT-3.5和Alpaca，在涵盖语言理解、生成任务以及BIG-Bench Hard（BBH）任务的31个数据集上进行优化。EvoPrompt显著优于人工设计的提示和现有的自动提示生成方法（例如在BBH上高达25%）。此外，EvoPrompt证明将LLM与EA连接会产生协同效应，这可能激发关于LLM和常规算法组合进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08532v3">PDF</a> International Conference on Learning Representations (ICLR) 2024</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在各种任务中表现出色，但它们依赖于精心设计的提示，这需要大量人力。本文提出了一种用于离散提示优化的新型框架EvoPrompt，它借鉴了进化算法（EA）的思想，具有良好的性能和快速的收敛速度。通过将LLM与EA相结合，EvoPrompt能够同时利用LLM的强大语言处理能力和EA的高效优化性能。EvoPrompt在多个数据集上优化了闭源和开源LLM的提示，包括GPT-3.5和Alpaca等，涵盖了语言理解、生成任务以及BIG-Bench Hard（BBH）任务。相较于人工设计的提示和现有的自动提示生成方法，EvoPrompt表现出显著的优势（在BBH任务上提升了高达25%）。此外，EvoPrompt展示了连接LLM与EA的协同作用潜力，为未来研究结合LLM和常规算法提供了启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）依赖精心设计的人工提示。</li>
<li>EvoPrompt是一种利用进化算法思想进行离散提示优化的新型框架。</li>
<li>EvoPrompt结合了LLM的强大语言处理能力与进化算法的高效优化性能。</li>
<li>EvoPrompt优化了包括GPT-3.5和Alpaca在内的多种LLM的提示。</li>
<li>EvoPrompt在多种任务上表现优于人工设计的提示和其他自动提示生成方法。</li>
<li>EvoPrompt显著提升了在BIG-Bench Hard（BBH）任务上的性能（提升了高达25%）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.08532">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-13bfd6b6ad9466ba0f65bb9a5fef2eb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4b3c7d5f397c661667d5255b2cc69fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2025aba69bb3d7a0bc0023b2ce467b78.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-89c3b48ed5f1c13900650e20c433fac2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-03  Visual Test-time Scaling for GUI Agent Grounding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3d20063c6db3cc2bc0baa4652fc4fa1f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-05-03  T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
