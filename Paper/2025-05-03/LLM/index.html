<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-03  T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-03-æ›´æ–°"><a href="#2025-05-03-æ›´æ–°" class="headerlink" title="2025-05-03 æ›´æ–°"></a>2025-05-03 æ›´æ–°</h1><h2 id="T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT"><a href="#T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT" class="headerlink" title="T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT"></a>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT</h2><p><strong>Authors:Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li</strong></p>
<p>Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›å±•å±•ç¤ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¦‚ä½•æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†æ­¤ç±»æ¨ç†ç­–ç•¥åº”ç”¨äºè§†è§‰ç”Ÿæˆé¢†åŸŸä»å¾…å¤§é‡æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†T2I-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¨ç†å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡åŒçº§CoTæ¨ç†è¿‡ç¨‹çš„å¼ºåŒ–å­¦ä¹ é©±åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œå¯ä»¥ç”¨æ¥å¢å¼ºç”Ÿæˆçš„ä¸åŒé˜¶æ®µï¼šï¼ˆ1ï¼‰è¯­ä¹‰çº§çš„CoTç”¨äºæç¤ºçš„é«˜çº§è§„åˆ’å’Œï¼ˆ2ï¼‰ä»¤ç‰Œçº§çš„CoTç”¨äºæ–‘å—å¯¹æ–‘å—ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½çº§åƒç´ å¤„ç†ã€‚ä¸ºäº†æ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œæˆ‘ä»¬å¼•å…¥äº†BiCoT-GRPOï¼Œé‡‡ç”¨ç”Ÿæˆå¥–åŠ±é›†åˆï¼Œå¯ä»¥åœ¨åŒä¸€è®­ç»ƒæ­¥éª¤ä¸­æ— ç¼ä¼˜åŒ–ä¸¤ç§ç”ŸæˆCoTã€‚é€šè¿‡å°†æˆ‘ä»¬çš„æ¨ç†ç­–ç•¥åº”ç”¨äºåŸºçº¿æ¨¡å‹Janus-Proï¼Œæˆ‘ä»¬åœ¨T2I-CompBenchä¸Šå®ç°äº†13%çš„æ”¹è¿›ï¼Œåœ¨WISEåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†19%çš„æ”¹è¿›ï¼Œç”šè‡³è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹FLUX.1ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CaraJ7/T2I-R1æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00703v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸åŒé‡æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ï¼ˆè¯­ä¹‰çº§å’Œtokençº§ï¼‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹T2I-R1ã€‚é€šè¿‡å¼•å…¥BiCoT-GRPOæ¥åè°ƒè¿™ä¸¤ç§å±‚æ¬¡çš„æ¨ç†ï¼Œå®ç°åœ¨åŒä¸€è®­ç»ƒæ­¥éª¤ä¸­å¯¹ä¸¤ç§æ¨ç†çš„ä¼˜åŒ–ã€‚å°†æ¨ç†ç­–ç•¥åº”ç”¨äºåŸºçº¿æ¨¡å‹Janus-Proåï¼Œåœ¨T2I-CompBenchå’ŒWISEåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ç°æœ‰æœ€ä½³æ¨¡å‹FLUXã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I-R1æ¨¡å‹ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸åŒé‡æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹åŒ…å«ä¸¤ä¸ªå±‚æ¬¡çš„CoTæ¨ç†ï¼šè¯­ä¹‰çº§CoTç”¨äºé«˜çº§è§„åˆ’æç¤ºï¼Œtokençº§CoTç”¨äºä½çº§çš„åƒç´ å¤„ç†ã€‚</li>
<li>BiCoT-GRPOè¢«å¼•å…¥ä»¥åè°ƒè¿™ä¸¤ä¸ªå±‚æ¬¡çš„CoTæ¨ç†ï¼Œå®ç°ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>T2I-R1æ¨¡å‹åœ¨åŸºçº¿æ¨¡å‹Janus-Proçš„åŸºç¡€ä¸Šåº”ç”¨æ¨ç†ç­–ç•¥ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨T2I-CompBenchå’ŒWISEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒT2I-R1æ¨¡å‹è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ¨¡å‹FLUXã€‚</li>
<li>æ¨¡å‹æ€§èƒ½çš„æå‡å¾—ç›Šäºå¼ºåŒ–å­¦ä¹ å’ŒåŒé‡æ€ç»´é“¾æ¨ç†çš„ç»“åˆï¼Œè¿™ç§ç»“åˆæœ‰åŠ©äºæ”¹è¿›æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-638cb2be2e21963d60c5f4ae260eac12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b799cc52d19d12cf4d975e022450830b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be9d027594a5d021598e8c542bd13e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f9eb0339ee5839957df728e9b36af8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rethinking-Memory-in-AI-Taxonomy-Operations-Topics-and-Future-Directions"><a href="#Rethinking-Memory-in-AI-Taxonomy-Operations-Topics-and-Future-Directions" class="headerlink" title="Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future   Directions"></a>Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future   Directions</h2><p><strong>Authors:Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan</strong></p>
<p>Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI%7D%7Bhttps://github.com/Elvin-Yiming-Du/Survey/_Memory/_in/_AI%7D.%7D">https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}</a>. </p>
<blockquote>
<p>è®°å¿†æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„åŸºç¡€ç»„æˆéƒ¨åˆ†ï¼Œæ”¯æ’‘ç€åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ã€‚å°½ç®¡å…ˆå‰çš„è°ƒæŸ¥ä¸»è¦å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è®°å¿†åº”ç”¨ï¼Œä½†å®ƒä»¬ç»å¸¸å¿½ç•¥åº•å±‚å†…å­˜åŠ¨æ€çš„åŸå­æ“ä½œã€‚åœ¨è¿™é¡¹è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å†…å­˜è¡¨ç¤ºåˆ†ç±»ä¸ºå‚æ•°è¡¨ç¤ºã€ä¸Šä¸‹æ–‡ç»“æ„åŒ–è¡¨ç¤ºå’Œä¸Šä¸‹æ–‡éç»“æ„åŒ–è¡¨ç¤ºï¼Œç„¶åä»‹ç»äº†å…­ç§åŸºæœ¬å†…å­˜æ“ä½œï¼šæ•´åˆã€æ›´æ–°ã€ç´¢å¼•ã€é—å¿˜ã€æ£€ç´¢å’Œå‹ç¼©ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å°†è¿™äº›æ“ä½œæ˜ å°„åˆ°é•¿æœŸè®°å¿†ã€é•¿ä¸Šä¸‹æ–‡ç¯å¢ƒã€å‚æ•°ä¿®æ”¹å’Œå¤šæºè®°å¿†ä¸­æœ€ç›¸å…³çš„ä¸»é¢˜ã€‚é€šè¿‡åŸå­æ“ä½œå’Œè¡¨ç¤ºç±»å‹çš„é€é•œé‡æ–°å®¡è§†å†…å­˜ç³»ç»Ÿï¼Œæœ¬è°ƒæŸ¥ä¸ºä¸äººå·¥æ™ºèƒ½ç›¸å…³çš„ç ”ç©¶ã€åŸºå‡†æ•°æ®é›†å’Œå·¥å…·æä¾›äº†ç»“æ„åŒ–å’ŒåŠ¨æ€çš„è§†è§’ï¼Œå¹¶é˜æ˜äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ä¸­åŠŸèƒ½æ€§äº¤äº’çš„åŒæ—¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„å‰æ™¯ã€‚\footnote{è®ºæ–‡æ¸…å•ã€æ•°æ®é›†ã€æ–¹æ³•å’Œå·¥å…·å¯è®¿é—®é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI%7D%E3%80%82">https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00675v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåŸºäºä»£ç†çš„æ™ºèƒ½ç³»ç»Ÿä»¥è®°å¿†ä¸ºå…³é”®è¦ç´ ã€‚ç°æœ‰è°ƒæŸ¥å¤šå…³æ³¨LLMä¸­çš„è®°å¿†åº”ç”¨ï¼Œå´å¿½ç•¥äº†æ”¯æ’‘å…¶åŠŸèƒ½çš„åŸå­æ“ä½œã€‚æœ¬æ–‡é€šè¿‡åˆ†ç±»è®°å¿†è¡¨å¾ï¼ˆå‚æ•°å‹ã€ä¸Šä¸‹æ–‡ç»“æ„å‹å’Œä¸Šä¸‹æ–‡éç»“æ„å‹ï¼‰åŠä»‹ç»å…­ç§åŸºæœ¬è®°å¿†æ“ä½œï¼ˆæ•´åˆã€æ›´æ–°ã€ç´¢å¼•ã€é—å¿˜ã€æ£€ç´¢å’Œå‹ç¼©ï¼‰ï¼Œå°†å…¶ä¸é•¿æœŸè®°å¿†ã€é•¿è¯­å¢ƒã€å‚æ•°ä¿®æ”¹å’Œå¤šæºè®°å¿†ç­‰ç›¸å…³ç ”ç©¶ä¸»é¢˜ç›¸è”ç³»ã€‚æœ¬æ–‡é‡æ–°æ„å»ºAIä¸­çš„è®°å¿†ç³»ç»Ÿè§†è§’ï¼Œä»¥åŸå­æ“ä½œå’Œè¡¨å¾ç±»å‹ä½œä¸ºè§‚å¯Ÿè§†è§’ï¼Œå¯¹ä¸AIä¸­çš„è®°å¿†ç›¸å…³çš„ç ”ç©¶ã€åŸºå‡†æ•°æ®é›†å’Œå·¥å…·è¿›è¡Œç»“æ„åŒ–ä¸”åŠ¨æ€çš„é˜è¿°ï¼ŒåŒæ—¶æ˜ç¡®äº†LLMsä»£ç†çš„åŠŸèƒ½äº¤äº’ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡è°ƒç ”äº†LLMæ™ºèƒ½ç³»ç»Ÿä¸­åŸºäºè®°å¿†çš„åŸºæœ¬è¦ç´ ï¼Œèšç„¦äºå…ˆå‰è¢«å¿½è§†çš„è®°å¿†åŸå­æ“ä½œç ”ç©¶ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸‰ç±»è®°å¿†è¡¨å¾æ–¹å¼ï¼šå‚æ•°å‹ã€ä¸Šä¸‹æ–‡ç»“æ„å‹å’Œä¸Šä¸‹æ–‡éç»“æ„å‹ã€‚</li>
<li>æ–‡ç« è¯¦ç»†ä»‹ç»äº†å…­ç§å…³é”®è®°å¿†æ“ä½œï¼šæ•´åˆã€æ›´æ–°ã€ç´¢å¼•ã€é—å¿˜ã€æ£€ç´¢å’Œå‹ç¼©ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡ç³»ç»Ÿæ€§åœ°å°†è¿™äº›æ“ä½œæ˜ å°„åˆ°ç›¸å…³çš„ç ”ç©¶ä¸»é¢˜ä¸Šï¼ŒåŒ…æ‹¬é•¿æœŸè®°å¿†å’Œè¯­å¢ƒæ•æ„Ÿæ€§ç ”ç©¶ç­‰ã€‚</li>
<li>è¯¥è°ƒæŸ¥ä¸ºè¯»è€…æä¾›äº†ä¸€ä¸ªæ¸…æ™°æ¡†æ¶æ¥ç†è§£AIç³»ç»Ÿä¸­çš„è®°å¿†ç»“æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨LLMèƒŒæ™¯ä¸‹çš„è®°å¿†åŠŸèƒ½äº¤äº’ã€‚</li>
<li>æ–‡ç« æä¾›äº†ç›¸å…³é¢†åŸŸçš„åŸºå‡†æ•°æ®é›†å’Œæ–¹æ³•è®ºå·¥å…·ï¼Œä»¥å¸®åŠ©æœªæ¥ç ”ç©¶å·¥ä½œçš„å¼€å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a549b77d086c21c0ff822ca6c2130c3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-946f3c1019ea72cc736ce962cf97c8a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b420821e103b84cc46563e0e02f9c055.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e54e0efd46c82335642a0adb428e744.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeepCritic-Deliberate-Critique-with-Large-Language-Models"><a href="#DeepCritic-Deliberate-Critique-with-Large-Language-Models" class="headerlink" title="DeepCritic: Deliberate Critique with Large Language Models"></a>DeepCritic: Deliberate Critique with Large Language Models</h2><p><strong>Authors:Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen</strong></p>
<p>As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å…¶è¾“å‡ºæä¾›å‡†ç¡®çš„åé¦ˆå’Œå¯æ‰©å±•çš„ç›‘ç£æˆä¸ºä¸€ä¸ªç´§è¿«ä¸”å…³é”®çš„é—®é¢˜ã€‚åˆ©ç”¨LLMä½œä¸ºæ‰¹è¯„æ¨¡å‹æ¥å®ç°è‡ªåŠ¨åŒ–ç›‘ç£æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºç ”ç©¶å’Œæé«˜LLMçš„æ•°å­¦æ‰¹åˆ¤èƒ½åŠ›ã€‚å½“å‰çš„LLMè¯„è®ºå®¶å¯¹æ¯ä¸€æ­¥çš„è¯„è®ºè¿‡äºè‚¤æµ…ï¼Œå¯¼è‡´åˆ¤æ–­å‡†ç¡®æ€§ä½ï¼Œéš¾ä»¥ä¸ºLLMç”Ÿæˆå™¨æä¾›è¶³å¤Ÿçš„åé¦ˆæ¥çº æ­£é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–æœ‰æ•ˆçš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥å¼€å‘èƒ½å¤Ÿæœ‰é’ˆå¯¹æ€§åœ°å¯¹æ•°å­¦è§£å†³æ–¹æ¡ˆçš„æ¯ä¸€æ­¥æ¨ç†è¿›è¡Œæ‰¹åˆ¤çš„LLMè¯„è®ºå®¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨Qwen2.5-72B-Instructç”Ÿæˆ4.5Ké•¿å½¢å¼çš„è¯„è®ºä½œä¸ºç›‘ç£å¾®è°ƒç§å­æ•°æ®ã€‚æ¯æ¡ç§å­è¯„è®ºéƒ½åŒ…å«æœ‰é’ˆå¯¹æ€§çš„æ­¥éª¤æ‰¹åˆ¤ï¼ŒåŒ…æ‹¬å¤šè§†è§’éªŒè¯ä»¥åŠæ¯ä¸ªæ¨ç†æ­¥éª¤çš„åˆæ­¥è¯„è®ºçš„æ·±å…¥æ‰¹åˆ¤ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å¾®è°ƒåçš„æ¨¡å‹ä½¿ç”¨PRM800Kçš„ç°æœ‰æ‰‹å·¥æ ‡æ³¨æ•°æ®æˆ–æˆ‘ä»¬é€šè¿‡è’™ç‰¹å¡æ´›é‡‡æ ·æ³•ä¼°è®¡çš„æ­£ç¡®æ€§è‡ªåŠ¨æ ‡æ³¨çš„æ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥æ¿€åŠ±å…¶æ‰¹åˆ¤èƒ½åŠ›ã€‚æˆ‘ä»¬åŸºäºQwen2.5-7B-Instructå¼€å‘çš„æ‰¹è¯„æ¨¡å‹ä¸ä»…åœ¨å„ç§é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰LLMè¯„è®ºå®¶ï¼ˆåŒ…æ‹¬ç›¸åŒè§„æ¨¡çš„DeepSeek-R1-distillæ¨¡å‹å’ŒGPT-4oï¼‰ï¼Œè€Œä¸”æ›´æœ‰æ•ˆåœ°å¸®åŠ©LLMç”Ÿæˆå™¨é€šè¿‡æ›´è¯¦ç»†çš„åé¦ˆæ¥ä¿®æ­£é”™è¯¯æ­¥éª¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00662v1">PDF</a> Work in progress. Data and models are available at   <a target="_blank" rel="noopener" href="https://github.com/RUCBM/DeepCritic">https://github.com/RUCBM/DeepCritic</a></p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å…¶è¾“å‡ºè¿›è¡Œå‡†ç¡®çš„åé¦ˆå’Œå¯æ‰©å±•çš„ç›‘ç£æˆä¸ºäº†ä¸€ä¸ªç´§è¿«ä¸”å…³é”®çš„é—®é¢˜ã€‚åˆ©ç”¨LLMä½œä¸ºæ‰¹åˆ¤æ¨¡å‹ä»¥å®ç°è‡ªåŠ¨åŒ–ç›‘ç£æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºç ”ç©¶å’Œæé«˜LLMçš„æ•°å­¦æ‰¹åˆ¤èƒ½åŠ›ã€‚å½“å‰LLMçš„æ‰¹åˆ¤è¿‡äºè‚¤æµ…ï¼Œå¯¼è‡´åˆ¤æ–­å‡†ç¡®æ€§ä½ï¼Œéš¾ä»¥å‘LLMç”Ÿæˆå™¨æä¾›è¶³å¤Ÿçš„åé¦ˆä»¥çº æ­£é”™è¯¯ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥å¼€å‘èƒ½å¤Ÿæœ‰é’ˆå¯¹æ€§åœ°å¯¹æ•°å­¦è§£å†³æ–¹æ¡ˆçš„æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œæ‰¹åˆ¤çš„LLMæ‰¹åˆ¤æ¨¡å‹ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨Qwen2.5-72B-Instructç”Ÿæˆé•¿è¯„è®ºæ–‡æ®ï¼Œç”¨äºç›‘ç£å¾®è°ƒã€‚ç„¶åå¯¹æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨äººç±»æ ‡æ³¨çš„æ•°æ®æˆ–è‡ªåŠ¨æ³¨é‡Šæ•°æ®æ¥è¿›ä¸€æ­¥æ¿€åŠ±å…¶æ‰¹åˆ¤èƒ½åŠ›ã€‚å¼€å‘çš„æ‰¹åˆ¤æ¨¡å‹ä¸ä»…æ˜¾è‘—ä¼˜äºç°æœ‰LLMæ‰¹åˆ¤æ¨¡å‹ï¼Œæ›´æœ‰æ•ˆåœ°å¸®åŠ©LLMç”Ÿæˆå™¨é€šè¿‡æ›´è¯¦ç»†çš„åé¦ˆæ¥ä¿®æ­£é”™è¯¯æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsåœ¨è‡ªåŠ¨ç›‘ç£æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ‰¹åˆ¤èƒ½åŠ›ä¸Šã€‚</li>
<li>å½“å‰LLMçš„æ‰¹åˆ¤åé¦ˆè¿‡äºè‚¤æµ…ï¼Œå¯¼è‡´åˆ¤æ–­å‡†ç¡®æ€§ä½ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥å¼€å‘å¢å¼ºæ•°å­¦æ‰¹åˆ¤èƒ½åŠ›çš„LLMæ¨¡å‹ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨Qwen2.5-72B-Instructç”Ÿæˆçš„é•¿è¯„è®ºæ–‡æ®ç”¨äºç›‘ç£å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ‰¹åˆ¤èƒ½åŠ›ï¼Œä½¿ç”¨äººç±»æˆ–è‡ªåŠ¨æ³¨é‡Šæ•°æ®ã€‚</li>
<li>å¼€å‘çš„æ‰¹åˆ¤æ¨¡å‹åœ¨é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–LLMæ‰¹åˆ¤æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-106a5c46b9bc0bd6f929d45431ed4661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9a1db7a7e0b1968e52ace7210f70f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f65a9f4e0212542596056ca1ea36e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9506250eeab1f12b11547f2bd8cd47a3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Investigating-Task-Arithmetic-for-Zero-Shot-Information-Retrieval"><a href="#Investigating-Task-Arithmetic-for-Zero-Shot-Information-Retrieval" class="headerlink" title="Investigating Task Arithmetic for Zero-Shot Information Retrieval"></a>Investigating Task Arithmetic for Zero-Shot Information Retrieval</h2><p><strong>Authors:Marco Braga, Pranav Kasela, Alessandro Raganato, Gabriella Pasi</strong></p>
<p>Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR">https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ŒåŒ…æ‹¬æ–‡æ¡£é‡æ–°æ’åºã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœªè§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ•ˆæœä¼šé™ä½ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè¯æ±‡å’Œè¯åˆ†å¸ƒçš„å˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»»åŠ¡ç®—æœ¯ï¼ˆTask Arithmeticï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç®€å•çš„æ•°å­¦è¿ç®—ï¼ˆå¦‚åŠ æ³•æˆ–å‡æ³•ï¼‰ç»“åˆåœ¨ä¸åŒä»»åŠ¡æˆ–æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„LLMæƒé‡ï¼Œä»¥é€‚åº”æ£€ç´¢æ¨¡å‹è€Œæ— éœ€é¢å¤–çš„å¾®è°ƒæŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå°†å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œé¢†åŸŸçŸ¥è¯†åˆæˆåˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨ä¸åŒçš„æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬é€‚åº”ã€‚åœ¨å…¬å¼€å¯ç”¨çš„ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’Œå¤šè¯­è¨€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨NDCG@10å’ŒP@10ä¸Šçš„é‡æ–°æ’åºæ€§èƒ½æé«˜äº†æœ€å¤šè¾¾18%å’Œ15%ã€‚é™¤äº†è¿™äº›ç»éªŒæ€§æ”¶ç›Šä¹‹å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¿˜æ·±å…¥æ¢è®¨äº†ä»»åŠ¡ç®—æœ¯ä½œä¸ºé›¶æ ·æœ¬å­¦ä¹ å’Œæ¨¡å‹é€‚åº”çš„å®é™…ç­–ç•¥çš„ä¼˜ç¼ºç‚¹ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IRå…¬å¼€æä¾›æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00649v1">PDF</a> Accepted in SIGIR â€˜25</p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨å¤šç§NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºé›¶æ ·æœ¬æ€§èƒ½ã€‚æœ¬æ–‡é€šè¿‡ä»»åŠ¡ç®—æœ¯æŠ€æœ¯ç»“åˆåœ¨ä¸åŒä»»åŠ¡æˆ–é¢†åŸŸä¸Šé¢„è®­ç»ƒçš„LLMæƒé‡ï¼Œå®ç°æ¨¡å‹çš„æœ‰æ•ˆé›¶æ ·æœ¬é€‚åº”ã€‚é€šè¿‡å…¬å¼€å¯ç”¨çš„ç§‘å­¦ã€ç”Ÿç‰©åŒ»å­¦å’Œå¤šè¯­ç§æ•°æ®é›†çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æé«˜é‡æ’æ€§èƒ½ï¼Œå¹¶æä¾›äº†å¯¹ä»»åŠ¡ç®—æœ¯ä½œä¸ºé›¶æ ·æœ¬å­¦ä¹ å’Œæ¨¡å‹é€‚åº”ç­–ç•¥çš„ä¼˜ç¼ºç‚¹åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šç§NLPä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºé›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†å…¶åœ¨æœªè§ä»»åŠ¡å’Œé¢†åŸŸä¸Šçš„æ•ˆæœä¼šä¸‹é™ã€‚</li>
<li>ä»»åŠ¡ç®—æœ¯æŠ€æœ¯ç»“åˆäº†åœ¨ä¸åŒä»»åŠ¡æˆ–é¢†åŸŸä¸Šé¢„è®­ç»ƒçš„LLMæƒé‡ï¼Œé€šè¿‡ç®€å•çš„æ•°å­¦è¿ç®—ï¼ˆå¦‚åŠ å‡ï¼‰æ¥é€‚åº”æ£€ç´¢æ¨¡å‹ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚</li>
<li>ä»»åŠ¡ç®—æœ¯èƒ½å¤Ÿåˆæˆä¸åŒçš„ä»»åŠ¡å’Œé¢†åŸŸçŸ¥è¯†åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œä½¿æ¨¡å‹åœ¨ä¸åŒçš„æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­å®ç°æœ‰æ•ˆçš„é›¶æ ·æœ¬é€‚åº”ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æé«˜é‡æ’æ€§èƒ½ï¼Œæœ€é«˜å¯æé«˜18%çš„NDCG@10å’Œ15%çš„P@10ã€‚</li>
<li>é™¤äº†è¿™äº›å®è¯æ”¶ç›Šå¤–ï¼Œæœ¬æ–‡è¿˜æ·±å…¥åˆ†æäº†ä»»åŠ¡ç®—æœ¯çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºç†è§£å’Œåº”ç”¨è¯¥æŠ€æœ¯æä¾›äº†é‡è¦è§è§£ã€‚</li>
<li>è®ºæ–‡æä¾›äº†ä»£ç å…¬å¼€è®¿é—®ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ‰©å±•è¯¥æŠ€æœ¯çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5109dc743cde1b422b6610ee27709de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af5d549d9cf7bcab99bbddb4d9b90779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a538ec85613d222521827fe9775d00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ed1e584f0dc23adc6b9675833ef5afa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94e1cc01bf574647f38f3849d3aa618c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Illusion-of-Role-Separation-Hidden-Shortcuts-in-LLM-Role-Learning-and-How-to-Fix-Them"><a href="#The-Illusion-of-Role-Separation-Hidden-Shortcuts-in-LLM-Role-Learning-and-How-to-Fix-Them" class="headerlink" title="The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning   (and How to Fix Them)"></a>The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning   (and How to Fix Them)</h2><p><strong>Authors:Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang</strong></p>
<p>Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role â€“ a concept we call \emph{role separation} â€“ is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the modelâ€™s input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®è·µä¸­è¶Šæ¥è¶Šæ™®éåœ°é›†æˆäº†å¤šç§è¾“å…¥è§’è‰²ï¼ˆå¦‚ç³»ç»ŸæŒ‡ä»¤ã€ç”¨æˆ·æŸ¥è¯¢ã€å¤–éƒ¨å·¥å…·è¾“å‡ºï¼‰ã€‚ç¡®ä¿æ¨¡å‹å‡†ç¡®åŒºåˆ†æ¯ä¸ªè§’è‰²çš„ä¿¡æ¯â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œè§’è‰²åˆ†ç¦»â€çš„æ¦‚å¿µâ€”â€”å¯¹äºä¿æŒå¤šè§’è‰²è¡Œä¸ºçš„ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘çš„å·¥ä½œå¾€å¾€é’ˆå¯¹æœ€å…ˆè¿›çš„æç¤ºæ³¨å…¥é˜²å¾¡ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›æ–¹æ³•æ˜¯å¦çœŸçš„æ•™ä¼šäº†LLMåŒºåˆ†è§’è‰²ï¼Œè¿˜æ˜¯ä»…ä»…è®°ä½äº†å·²çŸ¥è§¦å‘å› ç´ ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†â€œè§’è‰²åˆ†ç¦»å­¦ä¹ â€ï¼šæ•™æˆLLMç¨³å¥åœ°åŒºåˆ†ç³»ç»Ÿå’Œç”¨æˆ·ä»¤ç‰Œçš„è¿‡ç¨‹ã€‚é€šè¿‡ä¸€ä¸ªç®€å•ä¸”å—æ§çš„å®éªŒæ¡†æ¶ï¼Œæˆ‘ä»¬å‘ç°ç²¾ç»†è°ƒæ•´åçš„æ¨¡å‹åœ¨è¯†åˆ«è§’è‰²æ—¶é€šå¸¸ä¾èµ–äºä¸¤ä¸ªä»£ç†ï¼šï¼ˆ1ï¼‰ä»»åŠ¡ç±»å‹åˆ©ç”¨ï¼Œï¼ˆ2ï¼‰æ¥è¿‘æ–‡æœ¬å¼€å§‹ä½ç½®ã€‚è™½ç„¶æ•°æ®å¢å¼ºå¯ä»¥éƒ¨åˆ†ç¼“è§£è¿™äº›æ·å¾„ï¼Œä½†å®ƒé€šå¸¸å¯¼è‡´è¿­ä»£ä¿®è¡¥è€Œä¸æ˜¯æ›´æ·±å…¥çš„é—®é¢˜è§£å†³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è°ƒæ•´æ¨¡å‹è¾“å…¥ç¼–ç ä¸­çš„ä»¤ç‰Œçº§çº¿ç´¢æ¥åŠ å¼ºæ ‡è®°è§’è‰²è¾¹ç•Œçš„â€œä¸å˜ä¿¡å·â€ã€‚ç‰¹åˆ«æ˜¯ï¼Œè°ƒæ•´ä½ç½®IDæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ›´æ¸…æ™°çš„åŒºåˆ†åº¦ï¼Œå¹¶å‡å°‘å¯¹è¡¨é¢ä»£ç†çš„ä¾èµ–ã€‚é€šè¿‡å…³æ³¨è¿™ç§ä»¥æœºåˆ¶ä¸ºä¸­å¿ƒçš„è§‚ç‚¹ï¼Œæˆ‘ä»¬çš„å·¥ä½œé˜æ˜äº†LLMå¦‚ä½•æ›´å¯é åœ°ä¿æŒä¸€è‡´çš„å¤šè§’è‰²è¡Œä¸ºï¼Œè€Œä¸ä»…ä»…æ˜¯è®°ä½å·²çŸ¥çš„æç¤ºæˆ–è§¦å‘å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®è·µä¸­è¶Šæ¥è¶Šæ™®éåœ°æ•´åˆå¤šé‡è¾“å…¥è§’è‰²ï¼Œå¦‚ç³»ç»ŸæŒ‡ä»¤ã€ç”¨æˆ·æŸ¥è¯¢å’Œå¤–éƒ¨å·¥å…·è¾“å‡ºã€‚ç¡®ä¿æ¨¡å‹å‡†ç¡®åŒºåˆ†å„ç§è§’è‰²çš„ä¿¡æ¯ï¼Œå³æˆ‘ä»¬ç§°ä¹‹ä¸ºçš„è§’è‰²åˆ†ç¦»ï¼Œå¯¹äºä¿æŒå¤šè§’è‰²çš„è¡Œä¸ºä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜ç„å‡†å…ˆè¿›çš„æç¤ºæ³¨å…¥é˜²å¾¡ï¼Œä½†ä»ä¸æ¸…æ¥šè¿™äº›æ–¹æ³•æ˜¯å¦çœŸæ­£æ•™ä¼šLLMåŒºåˆ†è§’è‰²ï¼Œè¿˜æ˜¯ä»…ä»…è®°ä½å·²çŸ¥è§¦å‘å› ç´ ã€‚æœ¬æ–‡é€šè¿‡è€ƒå¯Ÿè§’è‰²åˆ†ç¦»å­¦ä¹ è¿‡ç¨‹ï¼Œç ”ç©¶å¦‚ä½•ç¨³å¥åœ°åŒºåˆ†ç³»ç»Ÿå’Œç”¨æˆ·ç¬¦å·ã€‚é€šè¿‡ç®€å•ã€å—æ§çš„å®éªŒæ¡†æ¶ï¼Œæˆ‘ä»¬å‘ç°å¾®è°ƒæ¨¡å‹é€šå¸¸ä¾èµ–ä¸¤ç§è§’è‰²è¯†åˆ«çš„ä»£ç†æ–¹å¼ï¼šä»»åŠ¡ç±»å‹åˆ©ç”¨å’Œæ¥è¿‘æ–‡æœ¬å¼€å§‹ä½ç½®ã€‚è™½ç„¶æ•°æ®å¢å¼ºå¯ä»¥éƒ¨åˆ†ç¼“è§£è¿™äº›æ·å¾„ï¼Œä½†å®ƒé€šå¸¸å¯¼è‡´è¿­ä»£ä¿®è¡¥è€Œä¸æ˜¯æ·±åº¦ä¿®å¤ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è°ƒæ•´æ¨¡å‹è¾“å…¥ç¼–ç ä¸­çš„ç¬¦å·çº§çº¿ç´¢æ¥å¼ºåŒ–æ ‡è®°è§’è‰²è¾¹ç•Œçš„ä¸å˜ä¿¡å·ã€‚ç‰¹åˆ«æ˜¯ï¼Œé€šè¿‡æ“ä½œä½ç½®IDæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ›´æ¸…æ™°çš„åŒºåˆ«ï¼Œå¹¶å‡å°‘å¯¹è¡¨é¢ä»£ç†çš„ä¾èµ–ã€‚æˆ‘ä»¬çš„å·¥ä½œä»æœºåˆ¶ä¸­å¿ƒçš„è§’åº¦æ­ç¤ºäº†LLMå¦‚ä½•æ›´å¯é åœ°ä¿æŒä¸€è‡´çš„å¤šè§’è‰²è¡Œä¸ºï¼Œè€Œä¸ä¼šä»…ä»…è®°ä½å·²çŸ¥çš„æç¤ºæˆ–è§¦å‘å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®è·µä¸­éœ€è¦æ•´åˆå¤šé‡è¾“å…¥è§’è‰²ï¼Œå¦‚ç³»ç»ŸæŒ‡ä»¤ã€ç”¨æˆ·æŸ¥è¯¢ç­‰ã€‚</li>
<li>è§’è‰²åˆ†ç¦»æ˜¯ç¡®ä¿LLMåœ¨å¤šè§’è‰²ç¯å¢ƒä¸­è¡¨ç°ä¸€è‡´æ€§çš„å…³é”®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¯èƒ½æ— æ³•çœŸæ­£æ•™ä¼šLLMåŒºåˆ†è§’è‰²ï¼Œè€Œæ˜¯å¯èƒ½åªæ˜¯è®°ä½å·²çŸ¥è§¦å‘å› ç´ ã€‚</li>
<li>é€šè¿‡å¯¹è§’è‰²åˆ†ç¦»å­¦ä¹ è¿‡ç¨‹çš„ç ”ç©¶ï¼Œå‘ç°å¾®è°ƒæ¨¡å‹é€šå¸¸ä¾èµ–ä»»åŠ¡ç±»å‹åˆ©ç”¨å’Œæ¥è¿‘æ–‡æœ¬å¼€å§‹ä½ç½®ä¸¤ç§è§’è‰²è¯†åˆ«æ–¹å¼ã€‚</li>
<li>æ•°æ®å¢å¼ºå¯ä»¥ç¼“è§£è¿™äº›è¯†åˆ«æ–¹å¼çš„å±€é™æ€§ï¼Œä½†é€šå¸¸åªæ˜¯è¿­ä»£ä¿®è¡¥è€Œéæ·±åº¦ä¿®å¤é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–ä¸å˜ä¿¡å·ä»¥æ ‡è®°è§’è‰²è¾¹ç•Œæ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´æ¨¡å‹è¾“å…¥ç¼–ç ä¸­çš„ç¬¦å·çº§çº¿ç´¢æ¥å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb5115c87b639f59c2e67a2edd614c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b988e847b23baffd6cfea95c168d7b31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-914573a51498d54b024801d1b2068423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0baffb8445e2220c8f3ccc8368d84822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de6ffbced77b5a67022d511019f2726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2216c66950fbdcdd0221f85fe69f4aed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FineScope-Precision-Pruning-for-Domain-Specialized-Large-Language-Models-Using-SAE-Guided-Self-Data-Cultivation"><a href="#FineScope-Precision-Pruning-for-Domain-Specialized-Large-Language-Models-Using-SAE-Guided-Self-Data-Cultivation" class="headerlink" title="FineScope : Precision Pruning for Domain-Specialized Large Language   Models Using SAE-Guided Self-Data Cultivation"></a>FineScope : Precision Pruning for Domain-Specialized Large Language   Models Using SAE-Guided Self-Data Cultivation</h2><p><strong>Authors:Chaitali Bhattacharyya, Yeseong Kim</strong></p>
<p>Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦ä»é›¶å¼€å§‹æ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™æ¿€å‘äº†äººä»¬å¯¹å¼€å‘æ›´å°ã€ç‰¹å®šé¢†åŸŸçš„LLMçš„å…´è¶£ï¼Œè¿™äº›æ¨¡å‹æ—¢èƒ½ä¿æŒé«˜æ•ˆç‡åˆèƒ½å®ç°å¼ºå¤§çš„ä»»åŠ¡æ€§èƒ½ã€‚ä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ï¼Œå¦‚LLaMAç­‰ï¼Œå·²ç»æˆä¸ºç‰¹å®šé¢†åŸŸé€‚åº”æ€§çš„èµ·ç‚¹ï¼Œä½†åœ¨ä¸“ä¸šæ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šå‡ºç°ç²¾åº¦ä¸‹é™çš„æƒ…å†µã€‚æˆ‘ä»¬å¼•å…¥äº†FineScopeï¼Œä¸€ä¸ªä»æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹æ´¾ç”Ÿå‡ºç´§å‡‘ã€åŸŸä¼˜åŒ–LLMçš„æ¡†æ¶ã€‚FineScopeåˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¡†æ¶ï¼Œå—å…¶èƒ½äº§ç”Ÿå¯è§£é‡Šç‰¹å¾è¡¨ç¤ºèƒ½åŠ›çš„å¯å‘ï¼Œä»å¤§å‹æ•°æ®é›†ä¸­æå–åŸŸç‰¹å®šå­é›†ã€‚æˆ‘ä»¬åº”ç”¨å¸¦æœ‰åŸŸç‰¹å®šçº¦æŸçš„ç»“æ„åŒ–ä¿®å‰ªï¼Œç¡®ä¿ä¿®å‰ªåçš„æ¨¡å‹ä¿ç•™ç›®æ ‡åŸŸçš„å¿…è¦çŸ¥è¯†ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œè¿™äº›ä¿®å‰ªåçš„æ¨¡å‹ç»å†äº†è‡ªæˆ‘æ•°æ®è’¸é¦ï¼Œåˆ©ç”¨SAEç­–åˆ’çš„æ•°æ®é›†æ¢å¤åœ¨ä¿®å‰ªè¿‡ç¨‹ä¸­ä¸¢å¤±çš„å…³é”®åŸŸç‰¹å®šä¿¡æ¯ã€‚å¤§é‡çš„å®éªŒå’Œæ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒFineScopeå…·æœ‰é«˜åº¦çš„ç«äº‰åŠ›ï¼Œåœ¨ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸­ä¼˜äºå‡ ç§å¤§è§„æ¨¡çš„æœ€å…ˆè¿›LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FineScopeçš„ä¿®å‰ªæ¨¡å‹åœ¨ä½¿ç”¨SAEç­–åˆ’æ•°æ®é›†è¿›è¡Œå¾®è°ƒæ—¶å¯ä»¥æ¢å¤å…¶å¤§éƒ¨åˆ†åŸå§‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå°†è¿™äº›æ•°æ®é›†ç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„LLMè€Œä¸è¿›è¡Œä¿®å‰ªä¹Ÿå¯ä»¥æé«˜å…¶ç‰¹å®šé¢†åŸŸçš„å‡†ç¡®æ€§ï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„ç¨³å¥æ€§ã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00624v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œå› æ­¤ç ”ç©¶è€…å¼€å§‹å…³æ³¨å¼€å‘æ›´å°ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„LLMï¼Œä»¥å…¼é¡¾æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½ã€‚LLaMAç­‰ä¸­å‹æ¨¡å‹è™½ä¸ºé¢†åŸŸç‰¹å®šé€‚é…æä¾›äº†èµ·ç‚¹ï¼Œä½†åœ¨ä¸“ä¸šæ•°æ®é›†ä¸Šæµ‹è¯•æ—¶ä¼šå‡ºç°ç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†FineScopeæ¡†æ¶ï¼Œç”¨äºä»è¾ƒå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­æ¨å¯¼å‡ºç´§å‡‘ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„LLMã€‚FineScopeåˆ©ç”¨å—å¯å‘äºå¯äº§ç”Ÿå¯è§£é‡Šç‰¹å¾è¡¨ç¤ºçš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¡†æ¶ï¼Œä»å¤§å‹æ•°æ®é›†ä¸­æå–ç‰¹å®šé¢†åŸŸçš„å­é›†ã€‚é€šè¿‡åº”ç”¨å¸¦æœ‰ç‰¹å®šé¢†åŸŸçº¦æŸçš„ç»“æ„åŒ–ä¿®å‰ªï¼Œç¡®ä¿ä¿®å‰ªåçš„æ¨¡å‹ä¿ç•™ç›®æ ‡é¢†åŸŸçš„å…³é”®çŸ¥è¯†ã€‚ä¸ºè¿›ä¸€æ­¥å¢å¼ºæ€§èƒ½ï¼Œè¿™äº›ä¿®å‰ªåçš„æ¨¡å‹è¿›è¡Œè‡ªæˆ‘æ•°æ®è’¸é¦ï¼Œåˆ©ç”¨SAEç­–åˆ’çš„æ•°æ®é›†æ¢å¤åœ¨ä¿®å‰ªè¿‡ç¨‹ä¸­ä¸¢å¤±çš„å…³é”®é¢†åŸŸç‰¹å®šä¿¡æ¯ã€‚å®éªŒå’Œæ¶ˆèç ”ç©¶è¯æ˜ï¼ŒFineScopeåœ¨ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸­å®ç°äº†é«˜åº¦ç«äº‰çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†è‹¥å¹²å¤§è§„æ¨¡å…ˆè¿›LLMã€‚æ­¤å¤–ï¼Œç»“æœè¿˜è¡¨æ˜ï¼Œä½¿ç”¨SAEç­–åˆ’çš„æ•°æ®é›†å¾®è°ƒFineScopeä¿®å‰ªçš„æ¨¡å‹ï¼Œå¯ä½¿å…¶æ¢å¤å¤§éƒ¨åˆ†åŸå§‹æ€§èƒ½ã€‚åŒæ—¶ï¼Œå°†è¿™äº›æ•°æ®é›†åº”ç”¨äºæœªç»ä¿®å‰ªçš„é¢„è®­ç»ƒLLMçš„å¾®è°ƒï¼Œä¹Ÿèƒ½æé«˜å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„ç²¾åº¦ï¼Œå‡¸æ˜¾äº†æ­¤æ–¹æ³•çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè®­ç»ƒéœ€å¤§é‡è®¡ç®—èµ„æºï¼Œä¿ƒä½¿ç ”ç©¶è€…å…³æ³¨æ›´å°ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„LLMã€‚</li>
<li>ä¸­å‹LLMåœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¯èƒ½å­˜åœ¨ç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>FineScopeæ¡†æ¶å¯ä»å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ä¸­æ¨å¯¼å‡ºç´§å‡‘ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„LLMã€‚</li>
<li>FineScopeåˆ©ç”¨Sparse Autoencoderï¼ˆSAEï¼‰æ¡†æ¶æå–é¢†åŸŸç‰¹å®šæ•°æ®å­é›†ã€‚</li>
<li>é€šè¿‡ç»“æ„åŒ–ä¿®å‰ªå’Œé¢†åŸŸç‰¹å®šçº¦æŸï¼Œç¡®ä¿æ¨¡å‹ä¿ç•™å…³é”®é¢†åŸŸçŸ¥è¯†ã€‚</li>
<li>ä¿®å‰ªåçš„æ¨¡å‹é€šè¿‡è‡ªæˆ‘æ•°æ®è’¸é¦å¢å¼ºæ€§èƒ½ï¼Œåˆ©ç”¨SAEç­–åˆ’çš„æ•°æ®é›†æ¢å¤ä¸¢å¤±ä¿¡æ¯ã€‚</li>
<li>FineScopeåœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ä¿æŒæ¨¡å‹ç´§å‡‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fc9813480c33677818a2e8dc20af7fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8798fd5bf3afb36791bcc49e554e3b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c40547cbfb0993856bd878e699c7cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb197e89a0d5de4c2d744a488fa0492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a50cb74f0d4f73038a372959c071ab97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d443adb39c31a4b5cec2b170d49ff1d2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Block-Circulant-Adapter-for-Large-Language-Models"><a href="#Block-Circulant-Adapter-for-Large-Language-Models" class="headerlink" title="Block Circulant Adapter for Large Language Models"></a>Block Circulant Adapter for Large Language Models</h2><p><strong>Authors:Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang</strong></p>
<p>Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks. </p>
<blockquote>
<p>å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› æ¨¡å‹è§„æ¨¡åºå¤§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„åŸºäºå‚…é‡Œå¶åŸŸçš„æ–¹æ³•æ˜¾ç¤ºå‡ºé™ä½å¾®è°ƒæˆæœ¬çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå—å¾ªç¯çŸ©é˜µçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹¶é‡‡ç”¨ç¨³å®šçš„è®­ç»ƒå¯å‘å¼ç­–ç•¥ï¼Œä»¥åˆ©ç”¨å¾ªç¯çŸ©é˜µå’Œä¸€ç»´å‚…é‡Œå¶å˜æ¢çš„å±æ€§æ¥é™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨çš„å‚æ•°æ•°é‡æ¯”VeRAå°‘14å€ï¼Œæ¯”LoRAå°‘16å€ï¼Œå¹¶ä¸”ä¸FourierFTç›¸æ¯”ï¼Œæµ®ç‚¹è¿ç®—å‡å°‘äº†32å€ï¼ŒåŒæ—¶ä¿æŒäº†æ¥è¿‘æˆ–æ›´å¥½çš„ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè§£å†³ä¸‹æ¸¸ä»»åŠ¡æ—¶çš„æ¨¡å‹å¾®è°ƒæå‡ºäº†ä¸€ç§é¢‡å…·å‰æ™¯çš„åŸºäºé¢‘åŸŸçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00582v1">PDF</a> to appear in Proceedings of the 2025 International Joint Conference   on Artificial Intelligence (IJCAI-2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒå› æ¨¡å‹è§„æ¨¡å·¨å¤§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå¾ªç¯çŸ©é˜µå’Œç¨³å®šè®­ç»ƒå¯å‘å¼æ–¹æ³•çš„å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨å¾ªç¯çŸ©é˜µå’Œä¸€ç»´å‚…é‡Œå¶å˜æ¢çš„ç‰¹æ€§æ¥é™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å‚æ•°ä½¿ç”¨æ•°é‡ç›¸è¾ƒäºVeRAå‡å°‘äº†14å€ï¼Œç›¸è¾ƒäºLoRAå‡å°‘äº†16å€ï¼Œä¸”ç›¸è¾ƒäºFourierFTçš„æµ®ç‚¹è¿ç®—é‡å‡å°‘äº†32å€ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†ä»»åŠ¡æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸ºåœ¨é¢‘ç‡åŸŸå¾®è°ƒå¤§å‹æ¨¡å‹æä¾›äº†å¯è¡Œçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒå› æ¨¡å‹è§„æ¨¡å·¨å¤§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¾ªç¯çŸ©é˜µå’Œç¨³å®šè®­ç»ƒå¯å‘å¼æ–¹æ³•çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¾ªç¯çŸ©é˜µå’Œä¸€ç»´å‚…é‡Œå¶å˜æ¢çš„ç‰¹æ€§æ¥é™ä½å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œå‚æ•°ä½¿ç”¨æ•°é‡å’Œæµ®ç‚¹è¿ç®—é‡æ˜¾è‘—å‡å°‘ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä¿æŒæˆ–æé«˜ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ä¸ºåœ¨é¢‘ç‡åŸŸå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’å’Œé€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c2737c33d0e279e07c76174abef1ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b6aa3a6ba2434503629f66d71ef62c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5602ffdeb5b5186a3be8a950c52c378d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72ee2a14839f3a480f254ee00afc0c3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00613521c76e9cb54ccaa1df4e8ec9c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3009bc1ba3244770d00e16915847109e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07f1d510a7f45420f0d2a94b203bb0c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ac91cf37c12099b13d8c2f7e1006d05.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Leveraging-Partial-SMILES-Validation-Scheme-for-Enhanced-Drug-Design-in-Reinforcement-Learning-Frameworks"><a href="#Leveraging-Partial-SMILES-Validation-Scheme-for-Enhanced-Drug-Design-in-Reinforcement-Learning-Frameworks" class="headerlink" title="Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks"></a>Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks</h2><p><strong>Authors:Xinyu Wang, Jinbo Bi, Minghu Song</strong></p>
<p>SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery. </p>
<blockquote>
<p>SMILESè¡¨è¾¾å¼åŸºç¡€çš„åˆ†å­ç”Ÿæˆåœ¨è¯ç‰©å‘ç°ä¸­å·²æˆä¸ºä¸€ç§å¼ºå¤§çš„æ–¹æ³•ã€‚ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«çº³å…¥åˆ†å­ç”Ÿæˆè¿‡ç¨‹ï¼Œæ—¨åœ¨æé«˜å€™é€‰åˆ†å­çš„å¯èƒ½æ€§åŒ¹é…å¾—åˆ†ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¼ºåŒ–å­¦ä¹ é˜¶æ®µçš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´é€šå¸¸è¶…è¿‡99%çš„åˆ†å­æœ‰æ•ˆæ€§çŸ¥è¯†ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç›®å‰åº”ç”¨äºè¯ç‰©å‘ç°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚REINVENTï¼‰ä½¿ç”¨å…ˆéªŒæ¨¡å‹ä½œä¸ºé”šç‚¹æ¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼Œä½†è¿™äº›æ–¹æ³•ç¼ºä¹ç¨³å¥çš„æ¢ç´¢æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†éƒ¨åˆ†SMILESéªŒè¯-PPOï¼ˆPSV-PPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒç»“åˆäº†å®æ—¶éƒ¨åˆ†SMILESéªŒè¯æ¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶é¼“åŠ±æ¢ç´¢ã€‚ä¸ä¼ ç»Ÿçš„åªåœ¨ç”Ÿæˆæ•´ä¸ªåºåˆ—åè¿›è¡Œåˆ†å­ç»“æ„éªŒè¯çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒPSV-PPOåœ¨æ¯ä¸€æ­¥çš„è‡ªå›å½’è¿‡ç¨‹ä¸­è¿›è¡Œé€æ­¥éªŒè¯ï¼Œä¸ä»…è¯„ä¼°æ‰€é€‰çš„æ ‡è®°å€™é€‰è€…ï¼Œè¿˜è¯„ä¼°ç”±å…ˆå‰éƒ¨åˆ†åºåˆ—äº§ç”Ÿçš„æ‰€æœ‰æ½œåœ¨åˆ†æ”¯ã€‚è¿™èƒ½å¤Ÿåœ¨æ‰€æœ‰æ½œåœ¨è·¯å¾„ä¸­æ—©æœŸæ£€æµ‹åˆ°æ— æ•ˆçš„å±€éƒ¨SMILESã€‚å› æ­¤ï¼Œå³ä½¿åœ¨åŒ–å­¦ç©ºé—´çš„å¹¿æ³›æ¿€çƒˆæ¢ç´¢ä¸­ï¼ŒPSV-PPOä¹Ÿèƒ½ä¿æŒè¾ƒé«˜çš„æœ‰æ•ˆæ€§ç‡ã€‚æˆ‘ä»¬åœ¨PMOå’ŒGuacaMolåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPSV-PPOåœ¨ä¿æŒç«äº‰åŠ›çš„æ¢ç´¢å’Œä¼˜åŒ–æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆçš„æ— æ•ˆç»“æ„æ•°é‡ã€‚è™½ç„¶æˆ‘ä»¬çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿æŒæœ‰æ•ˆæ€§ä¸Šï¼Œä½†PSV-PPOçš„æ¡†æ¶åœ¨æœªæ¥ç ”ç©¶ä¸­å¯ä»¥æ‰©å±•ï¼Œä»¥èå…¥æ›´å¤šæœ‰ä»·å€¼çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¿›ä¸€æ­¥æ”¹è¿›è¯ç‰©å‘ç°ä¸­çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00530v1">PDF</a> 17 pages, 5 main figures, 2 appendix figures. Submitted to ICML 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>SMILESè¯­è¨€æ¨¡å‹åœ¨è¯ç‰©å‘ç°ä¸­çš„åˆ†å­ç”Ÿæˆæ–¹æ³•å…·æœ‰å¼ºå¤§çš„æ½œåŠ›ã€‚ç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆåˆ†å­å€™é€‰ç‰©æ—¶è·å¾—é«˜åŒ¹é…åº¦ã€‚ç„¶è€Œï¼ŒRLé˜¶æ®µå­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå¦‚åˆ†å­æœ‰æ•ˆæ€§çŸ¥è¯†åœ¨é¢„è®­ç»ƒæ—¶å¾€å¾€è¶…è¿‡99%ï¼Œä½†åœ¨RLè¿‡ç¨‹ä¸­ä¼šæ˜¾è‘—ä¸‹é™ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”Partial SMILES Validation-PPOï¼ˆPSV-PPOï¼‰ã€‚è¯¥ç®—æ³•é€šè¿‡å®æ—¶éƒ¨åˆ†SMILESéªŒè¯æ¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶é¼“åŠ±æ¢ç´¢ã€‚ä¸ä¼ ç»Ÿçš„RLæ–¹æ³•ä¸åŒï¼ŒPSV-PPOåœ¨ç”Ÿæˆæ•´ä¸ªåºåˆ—ä¹‹å‰çš„æ¯ä¸ªè‡ªå›å½’æ­¥éª¤ä¸­æ‰§è¡Œé€æ­¥éªŒè¯ï¼Œä¸ä»…è¯„ä¼°æ‰€é€‰æ‹©çš„ä»¤ç‰Œå€™é€‰è€…ï¼Œè¿˜è¯„ä¼°æºè‡ªå…ˆå‰éƒ¨åˆ†åºåˆ—çš„æ‰€æœ‰æ½œåœ¨åˆ†æ”¯ã€‚è¿™èƒ½å¤Ÿæ—©æœŸæ£€æµ‹æ‰€æœ‰æ½œåœ¨è·¯å¾„ä¸­çš„æ— æ•ˆéƒ¨åˆ†SMILESã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPSV-PPOåœ¨ç»´æŒé«˜æœ‰æ•ˆæ€§ç‡çš„åŒæ—¶ï¼Œå‡å°‘äº†æ— æ•ˆç”Ÿæˆç»“æ„çš„æ•°é‡ï¼Œåœ¨PMOå’ŒGuacaMolåŸºå‡†æ•°æ®é›†ä¸Šä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æ¢ç´¢å’Œä¼˜åŒ–æ€§èƒ½ã€‚è™½ç„¶æˆ‘ä»¬çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿æŒæœ‰æ•ˆæ€§ä¸Šï¼Œä½†PSV-PPOæ¡†æ¶æœªæ¥å¯çº³å…¥å…¶ä»–æœ‰ä»·å€¼çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¿›ä¸€æ­¥æ¨åŠ¨è¯ç‰©å‘ç°ä¸­çš„å¼ºåŒ–å­¦ä¹ åº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SMILESè¯­è¨€æ¨¡å‹åœ¨è¯ç‰©å‘ç°ä¸­çš„åˆ†å­ç”Ÿæˆå…·æœ‰å¼ºå¤§æ½œåŠ›ï¼Œç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯è·å¾—é«˜åŒ¹é…åº¦ã€‚</li>
<li>RLé˜¶æ®µå­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´åˆ†å­æœ‰æ•ˆæ€§çŸ¥è¯†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>æå‡ºæ–°å‹RLç®—æ³•PSV-PPOï¼Œé€šè¿‡å®æ—¶éƒ¨åˆ†SMILESéªŒè¯æ¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼ŒåŒæ—¶é¼“åŠ±æ¢ç´¢ã€‚</li>
<li>PSV-PPOåœ¨ç”Ÿæˆåºåˆ—çš„æ¯ä¸ªæ­¥éª¤è¿›è¡ŒéªŒè¯ï¼Œè¯„ä¼°æ‰€é€‰ä»¤ç‰ŒåŠå…¶æ½œåœ¨åˆ†æ”¯ï¼Œæ—©æœŸæ£€æµ‹æ— æ•ˆåˆ†å­ç»“æ„ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPSV-PPOåœ¨ç»´æŒé«˜æœ‰æ•ˆæ€§ç‡çš„åŒæ—¶ï¼Œå‡å°‘äº†æ— æ•ˆç”Ÿæˆç»“æ„çš„æ•°é‡ã€‚</li>
<li>PSV-PPOåœ¨PMOå’ŒGuacaMolåŸºå‡†æ•°æ®é›†ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œè¡¨æ˜å…¶åœ¨æ¢ç´¢å’Œä¼˜åŒ–æ–¹é¢çš„æ•ˆèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3042b86f43099b2765eb759d42ec1397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f76f6ba4238a1c94e3fc946303164f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdb51657e0f14f212597066eebc721c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c65519b3d1a9b97bc75bf877d38f37ae.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-Ablating-Transformers-More-Interpretability-Less-Sparsity"><a href="#Self-Ablating-Transformers-More-Interpretability-Less-Sparsity" class="headerlink" title="Self-Ablating Transformers: More Interpretability, Less Sparsity"></a>Self-Ablating Transformers: More Interpretability, Less Sparsity</h2><p><strong>Authors:Jeremias Ferrao, Luhan Mikaelson, Keenan Pepper, Natalia Perez-Campanero Antolin</strong></p>
<p>A growing intuition in machine learning suggests a link between sparsity and interpretability. We introduce a novel self-ablation mechanism to investigate this connection ante-hoc in the context of language transformers. Our approach dynamically enforces a k-winner-takes-all constraint, forcing the model to demonstrate selective activation across neuron and attention units. Unlike post-hoc methods that analyze already-trained models, our approach integrates interpretability directly into model training, promoting feature localization from inception. Training small models on the TinyStories dataset and employing interpretability tests, we find that self-ablation leads to more localized circuits, concentrated feature representations, and increased neuron specialization without compromising language modelling performance. Surprisingly, our method also decreased overall sparsity, indicating that self-ablation promotes specialization rather than widespread inactivity. This reveals a complex interplay between sparsity and interpretability, where decreased global sparsity can coexist with increased local specialization, leading to enhanced interpretability. To facilitate reproducibility, we make our code available at <a target="_blank" rel="noopener" href="https://github.com/keenanpepper/self-ablating-transformers">https://github.com/keenanpepper/self-ablating-transformers</a>. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ é¢†åŸŸæ—¥ç›Šå¢é•¿çš„ç›´è§‰è¡¨æ˜ç¨€ç–æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´å­˜åœ¨è”ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªæ¶ˆèæœºåˆ¶ï¼Œåœ¨è¯­è¨€è½¬æ¢å™¨çš„èƒŒæ™¯ä¸‹ï¼Œå¯¹è¿™ç§è”ç³»è¿›è¡Œå‰ç»æ€§ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŠ¨æ€å®æ–½â€œèƒœè€…ä¸ºç‹â€çº¦æŸï¼Œè¿«ä½¿æ¨¡å‹åœ¨ç¥ç»å…ƒå’Œæ³¨æ„åŠ›å•ä½ä¹‹é—´è¡¨ç°å‡ºé€‰æ‹©æ€§æ¿€æ´»ã€‚ä¸åŒäºåˆ†æå·²è®­ç»ƒæ¨¡å‹çš„åæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å°†å¯è§£é‡Šæ€§èå…¥æ¨¡å‹è®­ç»ƒï¼Œä»è€Œä¿ƒè¿›ä»åˆå§‹é˜¶æ®µå°±å¼€å§‹çš„ç‰¹å¾å®šä½ã€‚åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒå°å‹æ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œå¯è§£é‡Šæ€§æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°è‡ªæ¶ˆèå¯¼è‡´äº†æ›´å±€éƒ¨çš„ç”µè·¯ã€é›†ä¸­çš„ç‰¹å¾è¡¨ç¤ºä»¥åŠå¢å¼ºç¥ç»å…ƒä¸“ä¸šåŒ–ï¼ŒåŒæ—¶ä¸ä¼šæŸå®³è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜é™ä½äº†æ€»ä½“ç¨€ç–æ€§ï¼Œè¿™è¡¨æ˜è‡ªæ¶ˆèä¿ƒè¿›äº†ä¸“ä¸šåŒ–è€Œéæ™®éä¸æ´»è·ƒã€‚è¿™æ­ç¤ºå‡ºç¨€ç–æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ï¼Œå…¶ä¸­å…¨çƒç¨€ç–æ€§çš„å‡å°‘å¯ä»¥ä¸å±€éƒ¨ä¸“ä¸šåŒ–çš„å¢å¼ºå…±å­˜ï¼Œä»è€Œæé«˜å¯è§£é‡Šæ€§ã€‚ä¸ºä¾¿äºå¤ç°ï¼Œæˆ‘ä»¬å°†ä»£ç æ”¾åœ¨<a target="_blank" rel="noopener" href="https://github.com/keenanpepper/self-ablating-transformers%E4%B8%8A%E3%80%82">https://github.com/keenanpepper/self-ablating-transformersä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00509v1">PDF</a> Poster Presentation at Building Trust Workshop at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ é¢†åŸŸé€æ¸æ„è¯†åˆ°ç¨€ç–æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„è”ç³»ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªæ¶ˆèæœºåˆ¶ï¼Œåœ¨è¯­è¨€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ç›´æ¥æ¢ç©¶è¿™ç§è”ç³»ã€‚é€šè¿‡åŠ¨æ€å®æ–½â€œèƒœè€…å…¨å–â€åŸåˆ™ï¼Œæ¨¡å‹è¡¨ç°å‡ºç¥ç»å…ƒå’Œæ³¨æ„åŠ›å•å…ƒçš„é€‰æ‹©æ€§æ¿€æ´»ã€‚æœ¬ç ”ç©¶åœ¨TinyStoriesæ•°æ®é›†ä¸Šè®­ç»ƒå°å‹æ¨¡å‹å¹¶è¿›è¡Œå¯è§£é‡Šæ€§æµ‹è¯•ï¼Œå‘ç°è‡ªæ¶ˆèå¯¼è‡´ç”µè·¯æ›´é›†ä¸­ã€ç‰¹å¾è¡¨ç¤ºæ›´é›†ä¸­ï¼Œç¥ç»å…ƒä¸“ä¸šåŒ–å¢å¼ºï¼Œä¸”ä¸å½±å“è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè‡ªæ¶ˆèæ–¹æ³•è¿˜é™ä½äº†æ€»ä½“ç¨€ç–æ€§ï¼Œè¡¨æ˜è‡ªæ¶ˆèä¿ƒè¿›ä¸“ä¸šåŒ–è€Œéå¹¿æ³›ä¸æ´»è·ƒã€‚è¿™æ­ç¤ºäº†ç¨€ç–æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„å¤æ‚ç›¸äº’ä½œç”¨ï¼Œå…¨å±€ç¨€ç–æ€§çš„é™ä½å¯ä¸å±€éƒ¨ä¸“ä¸šåŒ–çš„å¢åŠ å…±å­˜ï¼Œä»è€Œæé«˜å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ é¢†åŸŸæ­£é€æ¸è®¤è¯†åˆ°ç¨€ç–æ€§ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è‡ªæ¶ˆèæœºåˆ¶æ¢ç©¶ç¨€ç–æ€§ä¸å¯è§£é‡Šæ€§çš„è”ç³»ï¼Œç›´æ¥åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æå‡å¯è§£é‡Šæ€§ã€‚</li>
<li>è‡ªæ¶ˆèæœºåˆ¶é€šè¿‡åŠ¨æ€å®æ–½â€œèƒœè€…å…¨å–â€åŸåˆ™ï¼Œä½¿æ¨¡å‹åœ¨ç¥ç»å…ƒå’Œæ³¨æ„åŠ›å•å…ƒä¸Šè¡¨ç°å‡ºé€‰æ‹©æ€§æ¿€æ´»ã€‚</li>
<li>åœ¨TinyStoriesæ•°æ®é›†ä¸Šè¿›è¡Œçš„æµ‹è¯•è¡¨æ˜ï¼Œè‡ªæ¶ˆèèƒ½æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä¸”ä¸å½±å“è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚</li>
<li>è‡ªæ¶ˆèæœºåˆ¶ä¿ƒè¿›äº†ç¥ç»å…ƒçš„ä¸“ä¸šåŒ–ï¼Œä½¿å¾—ç”µè·¯å’Œç‰¹å¾è¡¨ç¤ºæ›´ä¸ºé›†ä¸­ã€‚</li>
<li>è‡ªæ¶ˆèæ–¹æ³•é™ä½äº†æ¨¡å‹çš„æ€»ä½“ç¨€ç–æ€§ï¼Œæ˜¾ç¤ºå‡ºè‡ªæ¶ˆèä¿ƒè¿›æ¨¡å‹çš„ä¸“ä¸šåŒ–è€Œéå¹¿æ³›ä¸æ´»è·ƒçš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1c90c21ce5f705c6cc3f556d0078e20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245855e2e02e5faf746f02250b4c1a39.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HalluMix-A-Task-Agnostic-Multi-Domain-Benchmark-for-Real-World-Hallucination-Detection"><a href="#HalluMix-A-Task-Agnostic-Multi-Domain-Benchmark-for-Real-World-Hallucination-Detection" class="headerlink" title="HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World   Hallucination Detection"></a>HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World   Hallucination Detection</h2><p><strong>Authors:Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu</strong></p>
<p>As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œæ£€æµ‹è™šæ„å†…å®¹â€”â€”å³æ²¡æœ‰åŸºäºæ”¯æŒè¯æ®çš„æ–‡æœ¬â€”â€”å·²ç»æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„è™šæ„æ£€æµ‹åŸºå‡†æµ‹è¯•é€šå¸¸æ˜¯åˆæˆç”Ÿæˆçš„ï¼Œä¸»è¦é›†ä¸­åœ¨æå–å¼é—®ç­”ä¸Šï¼Œæœªèƒ½æ•æ‰åˆ°æ¶‰åŠå¤šæ–‡æ¡£ä¸Šä¸‹æ–‡å’Œå®Œæ•´å¥å­è¾“å‡ºçš„ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†HalluMixåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ ·åŒ–ã€ä»»åŠ¡æ— å…³çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå„ä¸ªé¢†åŸŸçš„ä¾‹å­å’Œæ ¼å¼ã€‚ä½¿ç”¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸ƒä¸ªè™šæ„æ£€æµ‹ç³»ç»Ÿâ€”â€”åŒ…æ‹¬å¼€æºå’Œé—­æºç³»ç»Ÿâ€”â€”çªå‡ºäº†åœ¨ä¸åŒä»»åŠ¡ã€æ–‡æ¡£é•¿åº¦å’Œè¾“å…¥è¡¨ç¤ºä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚æˆ‘ä»¬çš„åˆ†æçªå‡ºäº†é•¿ä¸Šä¸‹æ–‡ä¸çŸ­ä¸Šä¸‹æ–‡ä¹‹é—´çš„å·¨å¤§æ€§èƒ½å·®è·ï¼Œå¯¹ç°å®ä¸–ç•Œä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å®ç°å…·æœ‰å…³é”®å½±å“ã€‚Quotient Detectionså–å¾—äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º0.82ï¼ŒF1åˆ†æ•°ä¸º0.84ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00506v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šæ™®éï¼Œæ£€æµ‹è™šæ„å†…å®¹â€”â€”å³æ²¡æœ‰åŸºäºæ”¯æŒè¯æ®çš„æ–‡æœ¬â€”â€”å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰è™šæ„æ£€æµ‹åŸºå‡†æµ‹è¯•é€šå¸¸åˆæˆç”Ÿæˆï¼Œä¸“æ³¨äºæå–æ€§é—®é¢˜å›ç­”ï¼Œæœªèƒ½æ•æ‰æ¶‰åŠå¤šæ–‡æ¡£èƒŒæ™¯å’Œå®Œæ•´å¥å­è¾“å‡ºçš„ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æ¨å‡ºHalluMix Benchmarkï¼Œä¸€ä¸ªå¤šæ ·ä¸”ä»»åŠ¡æ— å…³çš„æ•°æ®é›†ï¼Œæ¶µç›–å„ç§é¢†åŸŸå’Œæ ¼å¼çš„ä¾‹å­ã€‚åˆ©ç”¨æ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸ƒä¸ªè™šæ„æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½â€”â€”åŒ…æ‹¬å¼€æºå’Œé—­æºç³»ç»Ÿâ€”â€”çªæ˜¾äº†ä¸åŒä»»åŠ¡ã€æ–‡æ¡£é•¿åº¦å’Œè¾“å…¥è¡¨ç¤ºä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚åˆ†ææŒ‡å‡ºçŸ­èƒŒæ™¯å’Œé•¿èƒŒæ™¯ä¹‹é—´çš„æ€§èƒ½å·®è·æ˜¾è‘—ï¼Œå¯¹ç°å®ä¸–ç•Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å®æ–½å…·æœ‰å…³é”®å½±å“ã€‚Quotient Detectionså–å¾—æœ€ä½³æ€»ä½“æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º0.82ï¼ŒF1åˆ†æ•°ä¸º0.84ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œæ£€æµ‹è™šæ„å†…å®¹è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è™šæ„æ£€æµ‹åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†åæ˜ ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
<li>æ¨å‡ºHalluMix Benchmarkæ•°æ®é›†ï¼Œæ¶µç›–å¤šç§é¢†åŸŸå’Œæ ¼å¼ï¼Œç”¨äºè¯„ä¼°è™šæ„æ£€æµ‹ç³»ç»Ÿã€‚</li>
<li>è¯„ä¼°äº†ä¸ƒä¸ªè™šæ„æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå‘ç°ä»»åŠ¡ã€æ–‡æ¡£é•¿åº¦å’Œè¾“å…¥è¡¨ç¤ºä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>è™šæ„æ£€æµ‹åœ¨çŸ­èƒŒæ™¯å’Œé•¿èƒŒæ™¯ä¸‹çš„æ€§èƒ½å·®è·æ˜¾è‘—ï¼Œå¯¹RAGå®æ–½å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>Quotient Detectionsåœ¨è™šæ„æ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-183171daa384eb83a942a9fb53d4a956.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a0c1aa376876fdb4068f6258fc7f9e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdc915738c0d44091d5bb95e7974628a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44149c38cb242c8222cda09b6167e149.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a92ad2df55ad1a0b1d6e5694b2db1db9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Be-Trusted-for-Evaluating-RAG-Systems-A-Survey-of-Methods-and-Datasets"><a href="#Can-LLMs-Be-Trusted-for-Evaluating-RAG-Systems-A-Survey-of-Methods-and-Datasets" class="headerlink" title="Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and   Datasets"></a>Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and   Datasets</h2><p><strong>Authors:Lorenz Brehme, Thomas StrÃ¶hle, Ruth Breu</strong></p>
<p>Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the doâ€™s and donâ€™ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚RAGç³»ç»Ÿçš„å¤æ‚æ€§æ¶‰åŠç´¢å¼•ã€æ£€ç´¢å’Œç”Ÿæˆç­‰å¤šä¸ªç»„ä»¶ä»¥åŠå…¶ä»–ä¼—å¤šå‚æ•°ï¼Œä¸ºç³»ç»Ÿè¯„ä¼°å’Œè´¨é‡æå‡å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚å…ˆå‰çš„ç ”ç©¶å¼ºè°ƒï¼Œå¯¹RAGç³»ç»Ÿè¿›è¡Œè¯„ä¼°å¯¹äºè®°å½•è¿›å±•ã€æ¯”è¾ƒé…ç½®ä»¥åŠè¯†åˆ«ç‰¹å®šé¢†åŸŸåº”ç”¨çš„æœ‰æ•ˆæ–¹æ³•è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°å›é¡¾äº†63ç¯‡å­¦æœ¯è®ºæ–‡ï¼Œä»¥å…¨é¢æ¦‚è¿°æœ€å…ˆè¿›çš„RAGè¯„ä¼°æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨å››ä¸ªå…³é”®é¢†åŸŸï¼šæ•°æ®é›†ã€æ£€ç´¢å™¨ã€ç´¢å¼•å’Œæ•°æ®åº“ä»¥åŠç”Ÿæˆå™¨ç»„ä»¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è¯„ä¼°RAGç³»ç»Ÿå„ç»„ä»¶çš„å¯è¡Œæ€§ï¼Œè¯¥æ¨¡å‹æ—¢èƒ½ç”Ÿæˆè¯„ä¼°æ•°æ®é›†åˆèƒ½è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è¿›ä¸€æ­¥çš„å®é™…ç ”ç©¶å¯¹äºä¸ºä¼ä¸šæä¾›å®æ–½å’Œè¯„ä¼°RAGç³»ç»Ÿçš„æ˜ç¡®æŒ‡å¯¼è‡³å…³é‡è¦ã€‚é€šè¿‡ç»¼åˆRAGå…³é”®ç»„ä»¶çš„è¯„ä¼°æ–¹æ³•å¹¶å¼ºè°ƒé’ˆå¯¹åŸºå‡†æµ‹è¯•åˆ›å»ºå’Œé€‚åº”ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¸ºæ¨è¿›ç³»ç»Ÿè¯„ä¼°æ–¹æ³•å’Œæé«˜RAGç³»ç»Ÿè¯„ä¼°çš„ä¸¥è°¨æ€§åšå‡ºäº†è´¡çŒ®ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ£€æŸ¥åˆ©ç”¨LLMçš„è‡ªåŠ¨åŒ–æ–¹æ³•ä¸äººç±»åˆ¤æ–­ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œæˆ‘ä»¬ä¸ºå…³äºå¹³è¡¡è‡ªåŠ¨åŒ–å’Œäººç±»è¾“å…¥çš„æŒç»­è®¨è®ºåšå‡ºäº†è´¡çŒ®ï¼Œæ¾„æ¸…äº†å®ƒä»¬å„è‡ªçš„è´¡çŒ®ã€å±€é™æ€§å’Œåœ¨å®ç°ç¨³å¥å’Œå¯é è¯„ä¼°æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20119v2">PDF</a> 8 Pages. This paper has been accepted for presentation at the IEEE   Swiss Conference on Data Science (SDS25)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç”±äºå…¶ç³»ç»Ÿçš„å¤æ‚æ€§æ¶‰åŠç´¢å¼•ã€æ£€ç´¢ã€ç”Ÿæˆç­‰å¤šä¸ªç»„ä»¶åŠä¼—å¤šå‚æ•°ï¼Œä¸ºç³»ç»Ÿè¯„ä¼°å’Œè´¨é‡æå‡å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿç»¼è¿°äº†63ç¯‡å­¦æœ¯è®ºæ–‡ï¼Œå…¨é¢æ¦‚è¿°äº†æœ€å…ˆè¿›çš„RAGè¯„ä¼°æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨æ•°æ®é›†ã€æ£€ç´¢å™¨ã€ç´¢å¼•å’Œæ•°æ®åº“ä»¥åŠç”Ÿæˆå™¨ç»„ä»¶ç­‰å››ä¸ªå…³é”®é¢†åŸŸã€‚ç ”ç©¶è§‚å¯Ÿåˆ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è¯„ä¼°RAGç³»ç»Ÿå„ç»„ä»¶çš„å¯è¡Œæ€§ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°éœ€è¦æ›´å¤šå®è·µç ”ç©¶ä¸ºå…¬å¸æä¾›å®æ–½å’Œè¯„ä¼°RAGç³»ç»Ÿçš„æ˜ç¡®æŒ‡å¯¼ã€‚é€šè¿‡åˆæˆå…³é”®RAGç»„ä»¶çš„è¯„ä¼°æ–¹æ³•å¹¶å¼ºè°ƒé’ˆå¯¹åŸºå‡†æµ‹è¯•çš„åŸŸç‰¹å®šæ•°æ®é›†çš„åˆ›å»ºå’Œé€‚åº”æ€§æ”¹è¿›ï¼Œç ”ç©¶ä¿ƒè¿›äº†ç³»ç»Ÿè¯„ä¼°æ–¹æ³•çš„è¿›æ­¥ï¼Œæé«˜äº†RAGç³»ç»Ÿçš„è¯„ä¼°ä¸¥è°¨æ€§ã€‚åŒæ—¶ï¼Œé€šè¿‡æ£€æŸ¥åˆ©ç”¨LLMçš„è‡ªåŠ¨åŒ–æ–¹æ³•å’Œäººç±»åˆ¤æ–­ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œç ”ç©¶ä¸ºå¹³è¡¡è‡ªåŠ¨åŒ–å’Œäººç±»è¾“å…¥çš„æŒç»­è®¨è®ºåšå‡ºäº†è´¡çŒ®ï¼Œæ˜ç¡®äº†å„è‡ªçš„è´¡çŒ®ã€å±€é™æ€§å’Œå®ç°ç¨³å¥å’Œå¯é è¯„ä¼°çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>RAGç³»ç»Ÿå› å…¶å¤æ‚æ€§å’Œå¤šç»„ä»¶ç»“æ„é¢ä¸´è¯„ä¼°æŒ‘æˆ˜ã€‚</li>
<li>å­¦æœ¯ç•Œå¯¹äºRAGè¯„ä¼°æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ï¼Œæ¶µç›–æ•°æ®é›†ã€æ£€ç´¢å™¨ã€ç´¢å¼•å’Œæ•°æ®åº“ä»¥åŠç”Ÿæˆå™¨ç»„ä»¶ç­‰å…³é”®é¢†åŸŸã€‚</li>
<li>åˆ©ç”¨LLMè¿›è¡Œè‡ªåŠ¨è¯„ä¼°æ˜¯å¯è¡Œçš„ï¼Œå¹¶ä¸ºRAGç³»ç»Ÿçš„è¯„ä¼°æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
<li>å®è·µç ”ç©¶å¯¹äºæŒ‡å¯¼å…¬å¸å®æ–½å’Œè¯„ä¼°RAGç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>åˆæˆå…³é”®RAGç»„ä»¶çš„è¯„ä¼°æ–¹æ³•å¹¶å¼ºè°ƒåŸŸç‰¹å®šæ•°æ®é›†çš„é€‚åº”æ€§æ”¹è¿›æœ‰åŠ©äºæ¨åŠ¨ç³»ç»Ÿè¯„ä¼°æ–¹æ³•çš„è¿›æ­¥ã€‚</li>
<li>åœ¨è‡ªåŠ¨åŒ–å’Œäººç±»åˆ¤æ–­ä¹‹é—´æ‰¾åˆ°å¹³è¡¡æ˜¯å®ç°ç¨³å¥å’Œå¯é è¯„ä¼°çš„å…³é”®ã€‚</li>
<li>LLMåœ¨è‡ªåŠ¨åŒ–è¯„ä¼°ä¸­çš„æ½œåŠ›å’ŒæŒ‘æˆ˜å€¼å¾—è¿›ä¸€æ­¥æ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e84c9e9b6d7be96f535aff156ca8c2e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf8cc311b3f1172a91c452052296ec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff183a9424052ed8145b17bcfe614c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2b4c9dc4f16e084cfd66ea08738bec1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81b6cac17039c9186ea21f955e9cf302.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LGD-Leveraging-Generative-Descriptions-for-Zero-Shot-Referring-Image-Segmentation"><a href="#LGD-Leveraging-Generative-Descriptions-for-Zero-Shot-Referring-Image-Segmentation" class="headerlink" title="LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image   Segmentation"></a>LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image   Segmentation</h2><p><strong>Authors:Jiachen Li, Qing Xie, Renshu Gu, Jinyu Xu, Yongjian Liu, Xiaohan Yu</strong></p>
<p>Zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. Previous works address this challenge by utilizing Vision-Language Models and mask proposal networks for region-text matching. However, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. To alleviate this issue, we present LGD (Leveraging Generative Descriptions), a framework that utilizes the advanced language generation capabilities of Multi-Modal Large Language Models to enhance region-text matching performance in Vision-Language Models. Specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the Multi-Modal Large Language Models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. Secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. The proposed method achieves new state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU compared to previous methods. </p>
<blockquote>
<p>é›¶é•œå¤´å¼•ç”¨å›¾åƒåˆ†å‰²æ—¨åœ¨åŸºäºå¼•ç”¨è¡¨è¾¾å¼å®šä½å¹¶åˆ†å‰²ç›®æ ‡åŒºåŸŸï¼Œå…¶ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨æ²¡æœ‰è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å¯¹é½å’ŒåŒ¹é…ã€‚ä»¥å‰çš„å·¥ä½œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ©è†œææ¡ˆç½‘ç»œè¿›è¡ŒåŒºåŸŸæ–‡æœ¬åŒ¹é…æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼å¯èƒ½å¯¼è‡´ç”±äºè‡ªç”±å½¢å¼å¼•ç”¨è¡¨è¾¾å¼çš„å›ºæœ‰æ¨¡ç³Šæ€§å’Œå¤šæ ·æ€§è€Œå¯¼è‡´ç›®æ ‡å®šä½ä¸æ­£ç¡®ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LGDï¼ˆåˆ©ç”¨ç”Ÿæˆæè¿°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è¯­è¨€ç”Ÿæˆèƒ½åŠ›æ¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ä¸­åŒºåŸŸæ–‡æœ¬åŒ¹é…æ€§èƒ½çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡ä¸¤ç§æç¤ºï¼Œå±æ€§æç¤ºå’Œå‘¨å›´æç¤ºï¼Œæ¥å¼•å¯¼å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸æŒ‡ä»£å¯¹è±¡çš„å…³é”®å±æ€§ä»¥åŠå‘¨å›´å¯¹è±¡çš„ç»†èŠ‚ç›¸å…³çš„æè¿°ï¼Œåˆ†åˆ«ç§°ä¸ºå±æ€§æè¿°å’Œå‘¨å›´æè¿°ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ä¸‰ç§è§†è§‰æ–‡æœ¬åŒ¹é…åˆ†æ•°æ¥è¯„ä¼°å®ä¾‹çº§è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè¿™å†³å®šäº†ä¸å¼•ç”¨è¡¨è¾¾å¼æœ€ç›¸å…³çš„æ©è†œã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨IoUå’ŒmIoUæ–¹é¢åˆ†åˆ«æé«˜äº†9.97%å’Œ11.29%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14467v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æå‡é›¶æ ·æœ¬æŒ‡å‘æ€§å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ¡†æ¶ã€‚å®ƒåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œè¯­è¨€ç”Ÿæˆèƒ½åŠ›æ¥ç”Ÿæˆä¸å…³é”®å±æ€§å’Œå‘¨å›´å¯¹è±¡ç›¸å…³çš„æè¿°ï¼Œæé«˜è§†è§‰æ–‡æœ¬åŒ¹é…çš„æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†æœ€é«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é›¶æ ·æœ¬æŒ‡å‘æ€§å›¾åƒåˆ†å‰²æ—¨åœ¨åŸºäºæè¿°æ€§è¯­å¥å®šä½å¹¶åˆ†å‰²ç›®æ ‡åŒºåŸŸã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨æ²¡æœ‰è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰çš„åŒ¹é…å’Œå¯¹åº”ã€‚</li>
<li>ç ”ç©¶è€…åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ©æ¨¡æè®®ç½‘ç»œè¿›è¡ŒåŒºåŸŸæ–‡æœ¬åŒ¹é…æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ç”±äºè‡ªç”±å½¢å¼çš„æè¿°æ€§è¯­å¥å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œå¤šæ ·æ€§ï¼Œç°æœ‰æ–¹æ³•å¯èƒ½å¯¼è‡´ç›®æ ‡å®šä½é”™è¯¯ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºLGDçš„æ–°æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›æ¥æé«˜åŒºåŸŸæ–‡æœ¬åŒ¹é…æ€§èƒ½ã€‚</li>
<li>LGDé€šè¿‡è®¾è®¡å±æ€§æç¤ºå’Œå‘¨å›´æç¤ºæ¥å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸å…³é”®å±æ€§å’Œå‘¨å›´å¯¹è±¡ç›¸å…³çš„æè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1d50972146b0cc49a11c4dd33948f05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•ï¼Œç§°ä¸ºç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚ä¸å¸¸è§„æ–¹æ³•ä¸åŒï¼ŒGPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤æ‰¹è¯„è€…å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå‡ä¼˜äºGRPOã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPGä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤æ‰¹è¯„è€…å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è®­ç»ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾ä¸€æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸ŠæŒç»­ä¼˜äºGRPOã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸éœ€è¦ä¾èµ–å¤§é‡çš„ç›‘ç£å¾®è°ƒã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚</li>
<li>GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œæ— éœ€ä½¿ç”¨æ›¿ä»£æŸå¤±å‡½æ•°ã€‚</li>
<li>GPGç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä¸ä¼ ç»Ÿçš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ¶ˆé™¤äº†å¤æ‚ç»„ä»¶å¹¶è§£å†³äº†ç›¸å…³çš„é—®é¢˜ã€‚</li>
<li>GPGæ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•çš„è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0154bfaf18875c7cae4ded2d987de76b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb3eeaa103c7e595684cf660d6970abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bfb126d0f4a84aad9796b7bb3580cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9977e8d13e79fe1db75e4260ba5d161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea27ba65c8fe28125622d097454df66c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="KAP-MLLM-assisted-OCR-Text-Enhancement-for-Hybrid-Retrieval-in-Chinese-Non-Narrative-Documents"><a href="#KAP-MLLM-assisted-OCR-Text-Enhancement-for-Hybrid-Retrieval-in-Chinese-Non-Narrative-Documents" class="headerlink" title="KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese   Non-Narrative Documents"></a>KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese   Non-Narrative Documents</h2><p><strong>Authors:Hsin-Ling Hsu, Ping-Sheng Lin, Jing-Di Lin, Jengnan Tzeng</strong></p>
<p>Hybrid Retrieval systems, combining Sparse and Dense Retrieval methods, struggle with Traditional Chinese non-narrative documents due to their complex formatting, rich vocabulary, and the insufficient understanding of Chinese synonyms by common embedding models. Previous approaches inadequately address the dual needs of these systems, focusing mainly on general text quality improvement rather than optimizing for retrieval. We propose Knowledge-Aware Preprocessing (KAP), a novel framework that transforms noisy OCR outputs into retrieval-optimized text. KAP adopts a two-stage approach: it first extracts text using OCR, then employs Multimodal Large Language Models to refine the output by integrating visual information from the original documents. This design reduces OCR noise, reconstructs structural elements, and formats the text to satisfy the distinct requirements of sparse and dense retrieval. Empirical results demonstrate that KAP consistently and significantly outperforms conventional preprocessing approaches. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/JustinHsu1019/KAP">https://github.com/JustinHsu1019/KAP</a>. </p>
<blockquote>
<p>æ··åˆæ£€ç´¢ç³»ç»Ÿç»“åˆäº†ç¨€ç–å’Œå¯†é›†æ£€ç´¢æ–¹æ³•ï¼Œç”±äºä¼ ç»Ÿä¸­æ–‡éå™äº‹æ–‡æ¡£çš„å¤æ‚æ ¼å¼ã€ä¸°å¯Œè¯æ±‡ï¼Œä»¥åŠå¸¸è§åµŒå…¥æ¨¡å‹å¯¹ä¸­æ–‡åŒä¹‰è¯ç†è§£ä¸è¶³ï¼Œå› æ­¤å®ƒä»¬åœ¨è¿™æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸èƒ½æ»¡è¶³è¿™äº›ç³»ç»Ÿçš„åŒé‡éœ€æ±‚ï¼Œä¸»è¦é›†ä¸­åœ¨æé«˜ä¸€èˆ¬æ–‡æœ¬è´¨é‡ä¸Šï¼Œè€Œä¸æ˜¯ä¼˜åŒ–æ£€ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†çŸ¥è¯†æ„ŸçŸ¥é¢„å¤„ç†ï¼ˆKAPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†å˜ˆæ‚çš„OCRè¾“å‡ºè½¬æ¢ä¸ºä¼˜åŒ–æ£€ç´¢æ–‡æœ¬çš„æ–°å‹æ¡†æ¶ã€‚KAPé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆä½¿ç”¨OCRæå–æ–‡æœ¬ï¼Œç„¶åé‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¾“å‡ºè¿›è¡Œç»†åŒ–ï¼Œé€šè¿‡é›†æˆåŸå§‹æ–‡æ¡£ä¸­çš„è§†è§‰ä¿¡æ¯æ¥å®Œå–„è¾“å‡ºã€‚è¿™ç§è®¾è®¡é™ä½äº†OCRå™ªå£°ï¼Œé‡å»ºäº†ç»“æ„å…ƒç´ ï¼Œå¹¶å°†æ–‡æœ¬æ ¼å¼åŒ–ä¸ºæ»¡è¶³ç¨€ç–å’Œå¯†é›†æ£€ç´¢çš„ç‰¹å®šè¦æ±‚ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒKAPå§‹ç»ˆæ˜¾è‘—ä¼˜äºä¼ ç»Ÿé¢„å¤„ç†æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/JustinHsu1019/KAP">https://github.com/JustinHsu1019/KAP</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08452v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ··åˆæ£€ç´¢ç³»ç»Ÿåœ¨ä¼ ç»Ÿä¸­æ–‡éå™äº‹æ–‡æ¡£ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†çŸ¥è¯†æ„ŸçŸ¥é¢„å¤„ç†ï¼ˆKAPï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡OCRæŠ€æœ¯æå–æ–‡æœ¬ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆåŸå§‹æ–‡æ¡£ä¸­çš„è§†è§‰ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–è¾“å‡ºã€‚KAPçš„è®¾è®¡æ—¨åœ¨å‡å°‘OCRå™ªå£°ï¼Œé‡å»ºç»“æ„å…ƒç´ ï¼Œå¹¶æ ¼å¼åŒ–æ–‡æœ¬ä»¥æ»¡è¶³ç¨€ç–å’Œå¯†é›†æ£€ç´¢çš„ç‰¹å®šè¦æ±‚ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒKAPåœ¨é¢„å¤„ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ··åˆæ£€ç´¢ç³»ç»Ÿåœ¨å¤„ç†ä¼ ç»Ÿä¸­æ–‡éå™äº‹æ–‡æ¡£æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ–‡æ¡£å¤æ‚æ ¼å¼ã€ä¸°å¯Œè¯æ±‡åŠå¸¸ç”¨åµŒå…¥æ¨¡å‹å¯¹ä¸­æ–‡åŒä¹‰è¯ç†è§£ä¸è¶³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é€šç”¨æ–‡æœ¬è´¨é‡æå‡ï¼Œæœªèƒ½æ»¡è¶³ä¼˜åŒ–æ£€ç´¢çš„åŒé‡éœ€æ±‚ã€‚</li>
<li>æå‡ºçŸ¥è¯†æ„ŸçŸ¥é¢„å¤„ç†ï¼ˆKAPï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•è½¬æ¢å™ªå£°OCRè¾“å‡ºä¸ºæ£€ç´¢ä¼˜åŒ–æ–‡æœ¬ã€‚</li>
<li>KAPé¦–å…ˆä½¿ç”¨OCRæå–æ–‡æœ¬ï¼Œç„¶ååˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆåŸå§‹æ–‡æ¡£çš„è§†è§‰ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>KAPè®¾è®¡é™ä½OCRå™ªå£°ï¼Œé‡å»ºç»“æ„å…ƒç´ ï¼Œæ»¡è¶³ç¨€ç–å’Œå¯†é›†æ£€ç´¢çš„ç‰¹å®šè¦æ±‚ã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜KAPåœ¨é¢„å¤„ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4930f1db2c9231a7c063b0de6400a02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c917a27fce3855290b205bd7b5ee9ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dadeee1049b9b472e8a8cd4c93102c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e298983ca557f7799ab60845fd02979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c59807177c5ef3d143ac6f921676ddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a935ac683d9d34b85b6bfa0a6729148e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8922e96d47dda4f1d13e4383bce0b019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a21bfa3258378e38d37a45f20f966cfe.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="UoR-NCL-at-SemEval-2025-Task-1-Using-Generative-LLMs-and-CLIP-Models-for-Multilingual-Multimodal-Idiomaticity-Representation"><a href="#UoR-NCL-at-SemEval-2025-Task-1-Using-Generative-LLMs-and-CLIP-Models-for-Multilingual-Multimodal-Idiomaticity-Representation" class="headerlink" title="UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models   for Multilingual Multimodal Idiomaticity Representation"></a>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models   for Multilingual Multimodal Idiomaticity Representation</h2><p><strong>Authors:Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang</strong></p>
<p>SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at <a target="_blank" rel="noopener" href="https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL">https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL</a>. </p>
<blockquote>
<p>SemEval-2025 Task 1çš„é‡ç‚¹æ˜¯æ ¹æ®ç»™å®šçš„å…·æœ‰è‹±è¯­å’Œå·´è¥¿è‘¡è„ç‰™è¯­ä¸­ä¹ æƒ¯ç”¨æ³•çš„å¤åˆåè¯æ¥æ’åˆ—å›¾åƒã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œä½¿ç”¨ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šè¯­è¨€CLIPæ¨¡å‹ï¼Œä»¥å¢å¼ºä¹ æƒ¯ç”¨æ³•çš„å¤åˆåè¯è¡¨ç¤ºã€‚LLMä¸ºæ½œåœ¨çš„ä¹ æƒ¯ç”¨è¯­å¤åˆè¯ç”Ÿæˆä¹ æƒ¯ç”¨æ³•æ„ä¹‰ï¼Œä¸°å¯Œäº†å®ƒä»¬çš„è¯­ä¹‰è§£é‡Šã€‚è¿™äº›æ„ä¹‰éšåä½¿ç”¨å¤šè¯­è¨€CLIPæ¨¡å‹è¿›è¡Œç¼–ç ï¼Œä½œä¸ºå›¾åƒæ’åºçš„è¡¨ç¤ºã€‚å¯¹æ¯”å­¦ä¹ å’Œæ•°æ®å¢å¼ºæŠ€æœ¯è¢«åº”ç”¨äºå¾®è°ƒè¿™äº›åµŒå…¥ï¼Œä»¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è¿™ç§æ–¹æ³•æå–çš„å¤šæ¨¡å¼è¡¨ç¤ºä¼˜äºä»…åŸºäºåŸå§‹åä¹‰å¤åˆè¯çš„è¡¨ç¤ºã€‚å¾®è°ƒæ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æˆæœï¼Œä½†ä¸å¦‚ä¸ä½¿ç”¨å¾®è°ƒç›´æ¥ä½¿ç”¨åµŒå…¥æœ‰æ•ˆã€‚æœ¬æ–‡ä½¿ç”¨çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL">https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20984v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SemEval-2025ä»»åŠ¡1ï¼Œè¯¥ä»»åŠ¡èšç„¦äºæ ¹æ®å«æœ‰è‹±è¯­å’Œå·´è¥¿è‘¡è„ç‰™è¯­ä¸­ä¹ è¯­å«ä¹‰çš„åè¯çŸ­è¯­å¯¹å›¾åƒè¿›è¡Œæ’åºã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šè¯­ç§CLIPæ¨¡å‹ï¼Œä»¥å¢å¼ºä¹ è¯­çŸ­è¯­çš„è¡¨ç°ã€‚LLMsä¸ºæ½œåœ¨ä¹ è¯­ç”Ÿæˆå«ä¹‰ï¼Œä¸°å¯Œäº†è¯­ä¹‰è§£é‡Šã€‚éšåä½¿ç”¨å¤šè¯­ç§CLIPæ¨¡å‹å¯¹è¿™äº›å«ä¹‰è¿›è¡Œç¼–ç ï¼Œä½œä¸ºå›¾åƒæ’åºçš„ä»£è¡¨ã€‚è¯¥ç ”ç©¶è¿˜åº”ç”¨äº†å¯¹æ¯”å­¦ä¹ å’Œæ•°æ®å¢å¼ºæŠ€æœ¯æ¥å¾®è°ƒè¿™äº›åµŒå…¥ï¼Œä»¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡è¯¥æ–¹æ³•æå–çš„å¤šæ¨¡å¼è¡¨ç¤ºä¼˜äºä»…åŸºäºåŸå§‹åè¯åŒ–åˆç‰©çš„è¡¨ç¤ºã€‚å¾®è°ƒæ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†ä¸å¦‚ä¸ä½¿ç”¨å¾®è°ƒçš„åµŒå…¥æ–¹æ³•æœ‰æ•ˆã€‚æœ¬æ–‡ä½¿ç”¨çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SemEval-2025 Task 1é›†ä¸­äºæ ¹æ®å«æœ‰ä¹ è¯­å«ä¹‰çš„åè¯çŸ­è¯­å¯¹å›¾åƒè¿›è¡Œæ’åºã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šè¯­ç§CLIPæ¨¡å‹ä»¥å¢å¼ºä¹ è¯­è¡¨ç°ã€‚</li>
<li>LLMsèƒ½å¤Ÿç”Ÿæˆæ½œåœ¨ä¹ è¯­çš„å«ä¹‰ï¼Œä¸°å¯Œè¯­ä¹‰è§£é‡Šã€‚</li>
<li>å¤šè¯­ç§CLIPæ¨¡å‹ç”¨äºç¼–ç è¿™äº›å«ä¹‰ï¼Œä½œä¸ºå›¾åƒæ’åºçš„ä»£è¡¨ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ç”¨äºå¾®è°ƒåµŒå…¥ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºå¤šæ¨¡å¼è¡¨ç¤ºæ–¹æ³•ä¼˜äºä»…åŸºäºåŸå§‹åè¯åŒ–åˆç‰©çš„è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d077575da05e3cb402a96a2a4df7c574.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60f0a6ab3c54c56c049bbbad331333e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86fa327fa1f274f4b66b72300a16ed07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f1d315f2cffe1cca9d274aa127bc131.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2499f74813f50c21247d055952a6ffdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b69a62c1fbd50a7fe320cd222973ebe.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MotionGlot-A-Multi-Embodied-Motion-Generation-Model"><a href="#MotionGlot-A-Multi-Embodied-Motion-Generation-Model" class="headerlink" title="MotionGlot: A Multi-Embodied Motion Generation Model"></a>MotionGlot: A Multi-Embodied Motion Generation Model</h2><p><strong>Authors:Sudarshan Harithas, Srinath Sridhar</strong></p>
<p>This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MotionGlotæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§å…·æœ‰ä¸åŒåŠ¨ä½œç»´åº¦çš„è½½ä½“ä¸Šç”ŸæˆåŠ¨ä½œï¼Œå¦‚å››è¶³æœºå™¨äººå’Œäººä½“ã€‚é€šè¿‡å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸ç”¨çš„è®­ç»ƒæµç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºè¿åŠ¨ç›¸å…³ä»»åŠ¡è®¾è®¡çš„æŒ‡ä»¤è°ƒæ•´æ¨¡æ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼ŒLLMè®­ç»ƒçš„åŸºæœ¬åŸç†å¯ä»¥æˆåŠŸé€‚åº”ä¸åŒè½½ä½“å’Œä¸åŒåŠ¨ä½œç»´åº¦çš„å¹¿æ³›è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨ä¸€ç»„6ä¸ªä»»åŠ¡ä¸Šå±•ç¤ºäº†MotionGlotçš„å„ç§åŠŸèƒ½ï¼Œå¹¶æŠ¥å‘Šäº†è·¨ä»»åŠ¡çš„å¹³å‡æ”¹è¿›ç‡ä¸º35.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ï¼šï¼ˆ1ï¼‰ä¸“å®¶æ§åˆ¶çš„å››è¶³è¿åŠ¨æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦48,000æ¡å¸¦æœ‰æ–¹å‘æ€§æ–‡æœ¬æ³¨é‡Šçš„è½¨è¿¹ï¼›ï¼ˆ2ï¼‰åŒ…å«è¶…è¿‡23,000ä¸ªæƒ…å¢ƒæ–‡æœ¬æç¤ºçš„äººç±»è¿åŠ¨ç”Ÿæˆä»»åŠ¡æ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ç¡¬ä»¶å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16623v2">PDF</a> </p>
<p><strong>Summary</strong><br>MotionGlotæ¨¡å‹å¯ç”Ÿæˆè·¨å¤šç§è¡ŒåŠ¨ç»´åº¦çš„ä¸åŒå®ä½“çš„åŠ¨ä½œï¼Œå¦‚å››è¶³æœºå™¨äººå’Œäººä½“ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¸¸è§è®­ç»ƒç¨‹åºï¼Œå¹¶å¼•å…¥é’ˆå¯¹è¿åŠ¨ç›¸å…³ä»»åŠ¡çš„æŒ‡ä»¤è°ƒæ•´æ¨¡æ¿ã€‚ç ”ç©¶è¯æ˜LLMè®­ç»ƒåŸç†å¯æˆåŠŸé€‚åº”ä¸åŒè¡ŒåŠ¨ç»´åº¦çš„å¤šä¸ªå®ä½“çš„å¹¿æ³›è¿åŠ¨ç”Ÿæˆä»»åŠ¡ã€‚MotionGlotåœ¨6é¡¹ä»»åŠ¡ä¸Šçš„å¹³å‡æ”¹è¿›ç‡ä¸º35.3%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è´¡çŒ®äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼šåŒ…å«çº¦48000æ¡ä¸æ–¹å‘æ–‡æœ¬æ³¨é‡Šé…å¯¹è½¨è¿¹çš„ä¸“å®¶æ§åˆ¶å››è¶³è¡Œèµ°æ•°æ®é›†ï¼Œä»¥åŠåŒ…å«è¶…è¿‡23000æ¡ç”¨äºäººä½“è¿åŠ¨ç”Ÿæˆä»»åŠ¡çš„æƒ…å¢ƒæ–‡æœ¬æç¤ºæ•°æ®é›†ã€‚æœ€åï¼Œé€šè¿‡ç¡¬ä»¶å®éªŒéªŒè¯äº†ç³»ç»Ÿçš„ç°å®åº”ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MotionGlotæ¨¡å‹å¯ä»¥ç”Ÿæˆè·¨å¤šç§å®ä½“çš„åŠ¨ä½œï¼Œè¿™äº›å®ä½“å…·æœ‰ä¸åŒçš„è¡ŒåŠ¨ç»´åº¦ï¼Œå¦‚å››è¶³æœºå™¨äººå’Œäººä½“ã€‚</li>
<li>è¯¥ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç¨‹åºï¼Œå¹¶å¼•å…¥æŒ‡ä»¤è°ƒæ•´æ¨¡æ¿ï¼Œä¸ºè¿åŠ¨ç›¸å…³ä»»åŠ¡æä¾›å®šåˆ¶è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†LLMè®­ç»ƒåŸç†å¯ä»¥æˆåŠŸé€‚åº”å¤šç§è¿åŠ¨ç”Ÿæˆä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠä¸åŒè¡ŒåŠ¨ç»´åº¦çš„å¤šä¸ªå®ä½“ã€‚</li>
<li>MotionGlotåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„å¹³å‡æ€§èƒ½æ”¹è¿›è¾¾åˆ°35.3%ã€‚</li>
<li>ç ”ç©¶è´¡çŒ®äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸“å®¶æ§åˆ¶çš„å››è¶³è¡Œèµ°æ•°æ®é›†å’Œç”¨äºäººä½“è¿åŠ¨ç”Ÿæˆä»»åŠ¡çš„æƒ…å¢ƒæ–‡æœ¬æç¤ºæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡ç¡¬ä»¶å®éªŒéªŒè¯äº†ç³»ç»Ÿçš„ç°å®åº”ç”¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20160fe1ccc96901d06b1ef6060f0f5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da234cca253b0b543dfe4a40c12881c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd1a422d9cc0ecb9996d7bfde8b04c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-962e88927e82637906fc09bf0c95f144.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56cc93f2573a474d64bfe26488cb5ff7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reward-Augmented-Data-Enhances-Direct-Preference-Alignment-of-LLMs"><a href="#Reward-Augmented-Data-Enhances-Direct-Preference-Alignment-of-LLMs" class="headerlink" title="Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"></a>Reward-Augmented Data Enhances Direct Preference Alignment of LLMs</h2><p><strong>Authors:Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang</strong></p>
<p>Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/shenao-zhang/reward-augmented-preference">https://github.com/shenao-zhang/reward-augmented-preference</a>. </p>
<blockquote>
<p>åå¥½å¯¹é½åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨æ˜¾è‘—æé«˜äº†å®ƒä»¬éµå¾ªäººç±»æŒ‡ä»¤å’Œæ„å›¾çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›´æ¥å¯¹é½ç®—æ³•ä¸»è¦å…³æ³¨ç›¸å¯¹åå¥½ï¼Œå¾€å¾€å¿½è§†å“åº”çš„å®šæ€§æ–¹é¢ï¼Œå°½ç®¡å¯ä»¥è®¿é—®åŒ…å«æ¥è‡ªåˆ¤æ–­æ¨¡å‹çš„å¥–åŠ±åˆ†æ•°çš„AIåé¦ˆåå¥½æ•°æ®ã€‚åŠªåŠ›æœ€å¤§åŒ–æ‰€é€‰å’Œç•¥ä¸ºé€Šè‰²çš„è¢«æ‹’ç»å“åº”ä¹‹é—´çš„éšå«å¥–åŠ±å·®è·å¯èƒ½ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œä¸å¿…è¦çš„å¯¹é«˜è´¨é‡è¢«æ‹’ç»å“åº”çš„é—å¿˜ã€‚å¯¹å¥–åŠ±åˆ†æ•°çš„æ— çŸ¥ä¹Ÿé©±ä½¿LLMä¸åŠ åŒºåˆ«åœ°åçˆ±ä½è´¨é‡çš„é€‰å®šå“åº”ï¼Œå¹¶æ— æ³•æ¨å¹¿åˆ°æ•°æ®ç¨€ç¼ºçš„æœ€ä½³å“åº”ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†å¥–åŠ±æ¡ä»¶LLMç­–ç•¥ï¼Œèƒ½å¤Ÿè¯†åˆ«å¹¶ä»æ•°æ®é›†å†…çš„æ•´ä¸ªå“åº”è´¨é‡è°±ä¸­å­¦ä¹ ï¼Œæœ‰åŠ©äºæ¨æ–­å‡ºæ›´ä¼˜åŒ–çš„åŒºåŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆè€Œç®€å•çš„æ•°æ®é‡æ–°æ ‡è®°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®è´¨é‡åˆ†æ•°å¯¹åå¥½å¯¹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥æ„å»ºå¥–åŠ±å¢å¼ºæ•°æ®é›†ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œå¤šç§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆå¦‚ä¸€åœ°å¤§å¹…æé«˜äº†DPOã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æœ€å¤§é™åº¦åœ°åˆ©ç”¨äº†åå¥½æ•°æ®ï¼Œè€Œä¸”ç¼“è§£äº†é—å¿˜é—®é¢˜ï¼Œè¯æ˜äº†å…¶åœ¨å•çº¯çš„æ•°æ®æ‰©å±•ä¹‹å¤–çš„å¹¿æ³›æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shenao-zhang/reward-augmented-preference%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shenao-zhang/reward-augmented-preferenceæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08067v4">PDF</a> Published at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åå¥½å¯¹é½æ˜¾è‘—æé«˜äº†å…¶éµå¾ªäººç±»æŒ‡ä»¤å’Œæ„å›¾çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›´æ¥å¯¹é½ç®—æ³•ä¸»è¦å…³æ³¨ç›¸å¯¹åå¥½ï¼Œå¿½è§†å“åº”çš„å®šæ€§æ–¹é¢ï¼Œå°½ç®¡åœ¨AIåé¦ˆæœŸé—´å¯ä»¥è®¿é—®åŒ…å«åˆ¤æ–­æ¨¡å‹å¥–åŠ±åˆ†æ•°çš„åå¥½æ•°æ®ã€‚åŠªåŠ›æœ€å¤§åŒ–æ‰€é€‰å’Œç•¥ä¸ºé€Šè‰²çš„æ‹’ç»å“åº”ä¹‹é—´çš„éšæ€§å¥–åŠ±å·®è·å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œä¸å¿…è¦çš„å¯¹é«˜è´¨é‡æ‹’ç»å“åº”çš„é—å¿˜ã€‚å¿½è§†å¥–åŠ±åˆ†æ•°è¿˜å¯¼è‡´LLMä¸åŠ åŒºåˆ«åœ°åçˆ±ä½è´¨é‡çš„é€‰å®šå“åº”ï¼Œå¹¶ä¸”æ— æ³•æ¦‚æ‹¬æ•°æ®ç¨€ç¼ºçš„æœ€ä½³å“åº”ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†ä»¥å¥–åŠ±ä¸ºæ¡ä»¶çš„LLMç­–ç•¥ï¼Œèƒ½å¤Ÿè¾¨åˆ«å¹¶å­¦ä¹ æ•°æ®é›†å†…å“åº”è´¨é‡çš„å…¨è°±ï¼Œæœ‰åŠ©äºæ¨æ–­å‡ºæ›´ä¼˜åŒ–çš„åŒºåŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆè€Œç®€å•çš„æ•°æ®é‡æ–°æ ‡è®°æ–¹æ³•ï¼Œæ ¹æ®è´¨é‡åˆ†æ•°å¯¹åå¥½å¯¹è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»¥æ„å»ºå¥–åŠ±å¢å¼ºæ•°æ®é›†ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŒç»­ä¸”å¤§å¹…æå‡äº†DPOã€‚é€šè¿‡å…¨é¢çš„æ¶ˆé™¤ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æœ€å¤§é™åº¦åœ°åˆ©ç”¨äº†åå¥½æ•°æ®ï¼Œè¿˜ç¼“è§£äº†é—å¿˜é—®é¢˜ï¼Œè¿™è¯æ˜äº†å…¶å¹¿æ³›çš„å®ç”¨æ€§è¶…è¶Šäº†å•çº¯çš„æ•°æ®æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åå¥½å¯¹é½åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æ˜¾è‘—æé«˜äº†éµå¾ªäººç±»æŒ‡ä»¤å’Œæ„å›¾çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç›´æ¥å¯¹é½ç®—æ³•ä¸»è¦å…³æ³¨ç›¸å¯¹åå¥½ï¼Œä½†å¿½è§†å“åº”çš„å®šæ€§æ–¹é¢ã€‚</li>
<li>è¿‡åº¦è¿½æ±‚éšæ€§å¥–åŠ±å·®è·å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œå¯¹é«˜è´¨é‡æ‹’ç»å“åº”çš„é—å¿˜ã€‚</li>
<li>å¿½è§†å¥–åŠ±åˆ†æ•°ä¼šå¯¼è‡´LLMä¸åŠ åŒºåˆ«åœ°åçˆ±ä½è´¨é‡å“åº”ï¼Œä¸”æ— æ³•æ¦‚æ‹¬æœ€ä½³å“åº”ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†å¥–åŠ±å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ç­–ç•¥ï¼Œèƒ½å­¦ä¹ å“åº”è´¨é‡çš„å…¨è°±å¹¶æ¨æ–­å‡ºæ›´ä¼˜åŒ–çš„åŒºåŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”ç®€å•çš„æ•°æ®é‡æ–°æ ‡è®°æ–¹æ³•ï¼Œæ„å»ºäº†å¥–åŠ±å¢å¼ºæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d39ff82a3fae9a1e58f47209862ea203.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d57ddba54ee6c2055dac69c3c3528e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4068b68258a092e04f7cf43fed9616b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a595d5c489d826aa4b9b7eab8b51688.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Automated-Review-Generation-Method-Based-on-Large-Language-Models"><a href="#Automated-Review-Generation-Method-Based-on-Large-Language-Models" class="headerlink" title="Automated Review Generation Method Based on Large Language Models"></a>Automated Review Generation Method Based on Large Language Models</h2><p><strong>Authors:Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong</strong></p>
<p>Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchersâ€™ processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring usersâ€™ domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalystsâ€™ properties. Through multi-layered quality control, we effectively mitigated LLMsâ€™ hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5% with 95% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations. </p>
<blockquote>
<p>æ–‡çŒ®ç ”ç©¶å¯¹äºç§‘å­¦å·¥ä½œè‡³å…³é‡è¦ï¼Œä½†éšç€ä¿¡æ¯é‡çš„æ¿€å¢ï¼Œç ”ç©¶è€…é¢ä¸´å¤„ç†ä¿¡æ¯çš„èƒ½åŠ›ç“¶é¢ˆæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆæ–¹æ³•ï¼Œä»¥æé«˜æ•ˆç‡ï¼Œå‡è½»è®¤çŸ¥è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„ç»Ÿè®¡éªŒè¯è¯„ä¼°æ¡†æ¶æ˜¾ç¤ºï¼Œç”Ÿæˆçš„æ‘˜è¦ä¸äººå·¥æ‘˜è¦è´¨é‡ç›¸å½“ç”šè‡³æ›´å¥½ï¼Œä¸”å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œæ— éœ€ç”¨æˆ·å…·å¤‡ä¸“ä¸šçŸ¥è¯†ã€‚åœ¨ä¸™çƒ·è„±æ°¢ï¼ˆPDHï¼‰å‚¬åŒ–å‰‚çš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿…é€Ÿåˆ†æäº†343ç¯‡æ–‡ç« ï¼Œå¹³å‡æ¯ç¯‡æ–‡ç« æ¯ä¸ªLLMè´¦æˆ·åªéœ€å‡ ç§’é’Ÿï¼Œç”Ÿæˆäº†æ¶µç›–35ä¸ªä¸»é¢˜çš„å…¨é¢æ‘˜è¦ã€‚é€šè¿‡å¯¹1041ç¯‡æ–‡ç« è¿›è¡Œæ·±å…¥åˆ†æï¼Œä¸ºå‚¬åŒ–å‰‚çš„æ€§èƒ½æä¾›äº†è§è§£ã€‚é€šè¿‡å¤šå±‚æ¬¡çš„è´¨é‡æ§åˆ¶ï¼Œæˆ‘ä»¬æœ‰æ•ˆç¼“è§£äº†LLMçš„å¹»è§‰é—®é¢˜ã€‚ä¸“å®¶éªŒè¯ç¡®è®¤äº†æˆ‘ä»¬ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¼•ç”¨å®Œæ•´æ€§ï¼ŒåŒæ—¶è¡¨æ˜å¹»è§‰é£é™©å·²é™ä½åˆ°ä½äº0.5%ï¼Œå¹¶å…·æœ‰95%çš„ä¿¡å¿ƒã€‚å‘å¸ƒçš„Windowsåº”ç”¨ç¨‹åºå¯å®ç°ä¸€é”®ç”Ÿæˆæ‘˜è¦ï¼Œæé«˜ç ”ç©¶ç”Ÿäº§åŠ›å’Œæ–‡çŒ®æ¨èæ•ˆç‡ï¼Œä¸ºæ›´å¹¿æ³›çš„ç§‘å­¦æ¢ç´¢å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20906v5">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration">https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration</a> Data:   <a target="_blank" rel="noopener" href="https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData">https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData</a> This research   has been invited for a Short Oral presentation at the 18th ICC -   International Congress on Catalysis, taking place in Lyon, France from July   14-19, 2024 Published at <a target="_blank" rel="noopener" href="https://doi.org/10.1093/nsr/nwaf169">https://doi.org/10.1093/nsr/nwaf169</a> for newer   edition</p>
<p><strong>Summary</strong><br>åŸºäºæ–‡çŒ®ä¿¡æ¯çˆ†ç‚¸å¼å¢é•¿æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–æ‘˜è¦ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç ”ç©¶æ•ˆç‡å¹¶é™ä½è®¤çŸ¥è´Ÿè·ã€‚è¯¥æ–¹æ³•åœ¨ä¸™çƒ·è„±æ°¢ï¼ˆPDHï¼‰å‚¬åŒ–å‰‚ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œå±•ç¤ºäº†å…¶åœ¨å¿«é€Ÿåˆ†æå¤§é‡æ–‡çŒ®ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé™ä½LLMäº§ç”Ÿçš„å¹»è§‰é£é™©ï¼ŒåŒæ—¶æä¾›ä¸“å®¶éªŒè¯ç¡®è®¤å…¶å‡†ç¡®æ€§å’Œå¼•æ–‡å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•çš„æ¨å¹¿ä½¿ç”¨æœ‰æœ›æé«˜ç ”ç©¶ç”Ÿäº§åŠ›å’Œæ–‡çŒ®æ¨èæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºè‡ªåŠ¨åŒ–æ‘˜è¦ç”Ÿæˆï¼Œè§£å†³æ–‡çŒ®ä¿¡æ¯é‡å¤§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸™çƒ·è„±æ°¢å‚¬åŒ–å‰‚ç ”ç©¶ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¤šå±‚è´¨é‡æ§åˆ¶æœ‰æ•ˆé™ä½äº†LLMäº§ç”Ÿçš„å¹»è§‰é£é™©ã€‚</li>
<li>ä¸“å®¶éªŒè¯ç¡®è®¤äº†è¯¥æ–¹æ³•çš„å‡†ç¡®æ€§å’Œå¼•æ–‡å®Œæ•´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ç ”ç©¶ç”Ÿäº§åŠ›å’Œæ–‡çŒ®æ¨èæ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œé€‚ç”¨äºä¸åŒç ”ç©¶é¢†åŸŸï¼Œæ— éœ€ç”¨æˆ·å…·å¤‡ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23e7918f921284540f280c1fe5adad04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1161d974cf143db871edff25ba7d31a9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="EvoPrompt-Connecting-LLMs-with-Evolutionary-Algorithms-Yields-Powerful-Prompt-Optimizers"><a href="#EvoPrompt-Connecting-LLMs-with-Evolutionary-Algorithms-Yields-Powerful-Prompt-Optimizers" class="headerlink" title="EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful   Prompt Optimizers"></a>EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful   Prompt Optimizers</h2><p><strong>Authors:Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang</strong></p>
<p>Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥åŠªåŠ›ã€‚ä¸ºäº†è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç¦»æ•£æç¤ºä¼˜åŒ–çš„æ–°å‹æ¡†æ¶ï¼Œåä¸ºEvoPromptï¼Œå®ƒå€Ÿé‰´äº†è¿›åŒ–ç®—æ³•ï¼ˆEAï¼‰çš„æ€æƒ³ï¼Œå› ä¸ºè¿›åŒ–ç®—æ³•å…·æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œå¿«é€Ÿçš„æ”¶æ•›é€Ÿåº¦ã€‚ä¸ºäº†ä½¿EAèƒ½å¤Ÿåœ¨ç¦»æ•£æç¤ºä¸Šå·¥ä½œï¼Œè¿™äº›ç¦»æ•£æç¤ºæ˜¯è‡ªç„¶è¯­è¨€è¡¨è¾¾å¼ï¼Œéœ€è¦è¿è´¯ä¸”äººç±»å¯è¯»ï¼Œæˆ‘ä»¬å°†LLMä¸EAè¿æ¥èµ·æ¥ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤ŸåŒæ—¶åˆ©ç”¨LLMçš„å¼ºå¤§è¯­è¨€å¤„ç†èƒ½åŠ›å’ŒEAçš„é«˜æ•ˆä¼˜åŒ–æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒEvoPromptä¸æ¶‰åŠä»»ä½•æ¢¯åº¦æˆ–å‚æ•°ï¼Œå®ƒä»æç¤ºç§ç¾¤å¼€å§‹ï¼ŒåŸºäºè¿›åŒ–ç®—å­ä½¿ç”¨LLMè¿­ä»£ç”Ÿæˆæ–°çš„æç¤ºï¼Œæ ¹æ®å¼€å‘é›†æ”¹è¿›ç§ç¾¤ã€‚æˆ‘ä»¬å¯¹å°é—­å’Œå¼€æºçš„LLMè¿›è¡Œäº†æç¤ºä¼˜åŒ–ï¼ŒåŒ…æ‹¬GPT-3.5å’ŒAlpacaï¼Œåœ¨æ¶µç›–è¯­è¨€ç†è§£ã€ç”Ÿæˆä»»åŠ¡ä»¥åŠBIG-Bench Hardï¼ˆBBHï¼‰ä»»åŠ¡çš„31ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œä¼˜åŒ–ã€‚EvoPromptæ˜¾è‘—ä¼˜äºäººå·¥è®¾è®¡çš„æç¤ºå’Œç°æœ‰çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•ï¼ˆä¾‹å¦‚åœ¨BBHä¸Šé«˜è¾¾25%ï¼‰ã€‚æ­¤å¤–ï¼ŒEvoPromptè¯æ˜å°†LLMä¸EAè¿æ¥ä¼šäº§ç”ŸååŒæ•ˆåº”ï¼Œè¿™å¯èƒ½æ¿€å‘å…³äºLLMå’Œå¸¸è§„ç®—æ³•ç»„åˆè¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08532v3">PDF</a> International Conference on Learning Representations (ICLR) 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„æç¤ºï¼Œè¿™éœ€è¦å¤§é‡äººåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç¦»æ•£æç¤ºä¼˜åŒ–çš„æ–°å‹æ¡†æ¶EvoPromptï¼Œå®ƒå€Ÿé‰´äº†è¿›åŒ–ç®—æ³•ï¼ˆEAï¼‰çš„æ€æƒ³ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½å’Œå¿«é€Ÿçš„æ”¶æ•›é€Ÿåº¦ã€‚é€šè¿‡å°†LLMä¸EAç›¸ç»“åˆï¼ŒEvoPromptèƒ½å¤ŸåŒæ—¶åˆ©ç”¨LLMçš„å¼ºå¤§è¯­è¨€å¤„ç†èƒ½åŠ›å’ŒEAçš„é«˜æ•ˆä¼˜åŒ–æ€§èƒ½ã€‚EvoPromptåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜åŒ–äº†é—­æºå’Œå¼€æºLLMçš„æç¤ºï¼ŒåŒ…æ‹¬GPT-3.5å’ŒAlpacaç­‰ï¼Œæ¶µç›–äº†è¯­è¨€ç†è§£ã€ç”Ÿæˆä»»åŠ¡ä»¥åŠBIG-Bench Hardï¼ˆBBHï¼‰ä»»åŠ¡ã€‚ç›¸è¾ƒäºäººå·¥è®¾è®¡çš„æç¤ºå’Œç°æœ‰çš„è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•ï¼ŒEvoPromptè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼ˆåœ¨BBHä»»åŠ¡ä¸Šæå‡äº†é«˜è¾¾25%ï¼‰ã€‚æ­¤å¤–ï¼ŒEvoPromptå±•ç¤ºäº†è¿æ¥LLMä¸EAçš„ååŒä½œç”¨æ½œåŠ›ï¼Œä¸ºæœªæ¥ç ”ç©¶ç»“åˆLLMå’Œå¸¸è§„ç®—æ³•æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¾èµ–ç²¾å¿ƒè®¾è®¡çš„äººå·¥æç¤ºã€‚</li>
<li>EvoPromptæ˜¯ä¸€ç§åˆ©ç”¨è¿›åŒ–ç®—æ³•æ€æƒ³è¿›è¡Œç¦»æ•£æç¤ºä¼˜åŒ–çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>EvoPromptç»“åˆäº†LLMçš„å¼ºå¤§è¯­è¨€å¤„ç†èƒ½åŠ›ä¸è¿›åŒ–ç®—æ³•çš„é«˜æ•ˆä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>EvoPromptä¼˜åŒ–äº†åŒ…æ‹¬GPT-3.5å’ŒAlpacaåœ¨å†…çš„å¤šç§LLMçš„æç¤ºã€‚</li>
<li>EvoPromptåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºäººå·¥è®¾è®¡çš„æç¤ºå’Œå…¶ä»–è‡ªåŠ¨æç¤ºç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>EvoPromptæ˜¾è‘—æå‡äº†åœ¨BIG-Bench Hardï¼ˆBBHï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ˆæå‡äº†é«˜è¾¾25%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.08532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13bfd6b6ad9466ba0f65bb9a5fef2eb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4b3c7d5f397c661667d5255b2cc69fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2025aba69bb3d7a0bc0023b2ce467b78.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-89c3b48ed5f1c13900650e20c433fac2.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-03  Visual Test-time Scaling for GUI Agent Grounding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3d20063c6db3cc2bc0baa4652fc4fa1f.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-03  T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17548.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
