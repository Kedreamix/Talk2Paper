<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-03  T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3d20063c6db3cc2bc0baa4652fc4fa1f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-03-æ›´æ–°"><a href="#2025-05-03-æ›´æ–°" class="headerlink" title="2025-05-03 æ›´æ–°"></a>2025-05-03 æ›´æ–°</h1><h2 id="T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT"><a href="#T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT" class="headerlink" title="T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT"></a>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT</h2><p><strong>Authors:Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li</strong></p>
<p>Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a> </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å±•ç¤ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¦‚ä½•æå‡æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†æ­¤ç±»æ¨ç†ç­–ç•¥åº”ç”¨äºè§†è§‰ç”Ÿæˆé¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†T2I-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¢å¼ºæ¨ç†çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡åŒçº§CoTæ¨ç†è¿‡ç¨‹é©±åŠ¨RLå®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªå¯ä»¥åˆ©ç”¨æ¥å¢å¼ºç”Ÿæˆä¸åŒé˜¶æ®µçš„CoTçº§åˆ«ï¼šï¼ˆ1ï¼‰ç”¨äºé«˜çº§æç¤ºè§„åˆ’çš„è¯­ä¹‰çº§CoTå’Œï¼ˆ2ï¼‰ç”¨äºé€å—ç”ŸæˆæœŸé—´çš„ä½çº§åƒç´ å¤„ç†çš„æ ‡è®°çº§CoTã€‚ä¸ºäº†æ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œæˆ‘ä»¬å¼•å…¥äº†BiCoT-GRPOï¼Œé€šè¿‡ç”Ÿæˆå¥–åŠ±é›†åˆæ— ç¼ä¼˜åŒ–åŒä¸€è®­ç»ƒæ­¥éª¤å†…çš„ä¸¤ä¸ªç”ŸæˆCoTã€‚é€šè¿‡å°†æˆ‘ä»¬çš„æ¨ç†ç­–ç•¥åº”ç”¨äºåŸºçº¿æ¨¡å‹Janus-Proï¼Œæˆ‘ä»¬åœ¨T2I-CompBenchä¸Šå®ç°äº†13%çš„æ”¹è¿›ï¼Œåœ¨WISEåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†19%çš„æ”¹è¿›ï¼Œç”šè‡³è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹FLUX.1ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CaraJ7/T2I-R1æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00703v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºT2I-R1çš„æ–°å‹æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ å’ŒåŒé‡é“¾å¼æ€ç»´æ¨ç†ç­–ç•¥ã€‚é€šè¿‡è¯­ä¹‰å±‚é¢çš„é“¾å¼æ€ç»´è¿›è¡Œé«˜çº§è§„åˆ’ï¼Œå¹¶åˆ©ç”¨tokençº§åˆ«çš„é“¾å¼æ€ç»´è¿›è¡Œåƒç´ çº§çš„å›¾åƒç”Ÿæˆã€‚æ¨¡å‹ä½¿ç”¨BiCoT-GRPOæŠ€æœ¯åè°ƒè¿™ä¸¤ä¸ªçº§åˆ«çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä¼˜åŒ–äº†ç”Ÿæˆå¥–åŠ±çš„é›†åˆã€‚åº”ç”¨äºJanus-Proæ¨¡å‹åï¼ŒT2I-R1åœ¨T2I-CompBenchä¸Šæå‡äº†13%ï¼Œåœ¨WISEåŸºå‡†æµ‹è¯•ä¸­æå‡äº†19%ï¼Œè¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹FLUXã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I-R1æ˜¯ä¸€ä¸ªç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’ŒåŒé‡é“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨è¯­ä¹‰çº§å’Œtokençº§çš„åŒé‡CoTæ¨ç†è¿‡ç¨‹ï¼Œåˆ†åˆ«ç”¨äºé«˜çº§è§„åˆ’å’Œåƒç´ çº§çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>BiCoT-GRPOæŠ€æœ¯ç”¨äºåè°ƒå’Œä¼˜åŒ–è¿™ä¸¤ä¸ªçº§åˆ«çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>T2I-R1æ¨¡å‹åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹Janus-Proï¼Œåœ¨T2I-CompBenchä¸Šæå‡äº†13%ï¼Œåœ¨WISEä¸Šæå‡äº†19%ã€‚</li>
<li>è¯¥æ¨¡å‹è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹FLUXã€‚</li>
<li>æ¨¡å‹ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-638cb2be2e21963d60c5f4ae260eac12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b799cc52d19d12cf4d975e022450830b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be9d027594a5d021598e8c542bd13e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f9eb0339ee5839957df728e9b36af8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MINERVA-Evaluating-Complex-Video-Reasoning"><a href="#MINERVA-Evaluating-Complex-Video-Reasoning" class="headerlink" title="MINERVA: Evaluating Complex Video Reasoning"></a>MINERVA: Evaluating Complex Video Reasoning</h2><p><strong>Authors:Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, Tobias Weyand</strong></p>
<p>Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available under <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/neptune?tab=readme-ov-file%5C#minerva">https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨å°†é‡ç‚¹è½¬å‘è§†é¢‘åŸºå‡†æµ‹è¯•ï¼Œç„¶è€Œå¤§å¤šæ•°è§†é¢‘åŸºå‡†æµ‹è¯•åªæä¾›ç»“æœç›‘ç£ï¼Œæ²¡æœ‰ä¸­é—´æˆ–å¯è§£é‡Šæ¨ç†æ­¥éª¤ã€‚è¿™ä½¿å¾—è¯„ä¼°æ¨¡å‹æ˜¯å¦çœŸæ­£èƒ½å¤Ÿç»“åˆæ„ŸçŸ¥å’Œæ—¶é—´ä¿¡æ¯å¯¹è§†é¢‘è¿›è¡Œæ¨ç†å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæˆ–è€…åªæ˜¯å¶ç„¶å¾—åˆ°æ­£ç¡®ç­”æ¡ˆæˆ–é€šè¿‡åˆ©ç”¨è¯­è¨€åè§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬ä¸ºç°ä»£å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†ä¸€ä¸ªåä¸ºMINERVAçš„æ–°è§†é¢‘æ¨ç†æ•°æ®é›†ã€‚æ•°æ®é›†ä¸­çš„æ¯ä¸ªé—®é¢˜éƒ½æœ‰5ä¸ªç­”æ¡ˆé€‰é¡¹ï¼Œä»¥åŠè¯¦ç»†çš„æ‰‹å·¥æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯å¤šæ¨¡æ€çš„ï¼Œåœ¨è§†é¢‘é¢†åŸŸå’Œé•¿åº¦æ–¹é¢éƒ½å…·æœ‰å¤šæ ·æ€§ï¼ŒåŒ…å«å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ã€‚å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å¯¹å‰æ²¿çš„å¼€æºå’Œä¸“æœ‰æ¨¡å‹éƒ½æ„æˆäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†ç²¾ç»†çš„è¯¯å·®åˆ†æï¼Œä»¥è¯†åˆ«å„ç§æ¨¡å‹çš„å¸¸è§å¤±è´¥æ¨¡å¼ï¼Œå¹¶å¯¹æ¨ç†é”™è¯¯è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç‚¹æ¥æ¢ç´¢ä¸ºäººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ‰“åˆ†è§†é¢‘æ¨ç†è½¨è¿¹çš„æ–¹æ³•ï¼Œå¹¶å‘ç°å¤±è´¥æ¨¡å¼ä¸»è¦ä¸æ—¶é—´å®šä½æœ‰å…³ï¼Œå…¶æ¬¡æ˜¯è§†è§‰æ„ŸçŸ¥é”™è¯¯ï¼Œè€Œä¸æ˜¯é€»è¾‘æˆ–å®Œæ•´æ€§é”™è¯¯ã€‚è¯¥æ•°æ®é›†ä»¥åŠä¸é—®é¢˜ã€ç­”æ¡ˆé€‰é¡¹å’Œæ¨ç†è½¨è¿¹ç›¸å…³çš„ä¿¡æ¯å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/neptune?tab=readme%20ov%20file%23minerva%E4%B8%8B%E5%85%AC%E5%BC%BA%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/google-deepmind/neptune?tab=readme-ov-file#minervaä¸‹å…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00681v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤šæ¨¡æ€æ¨¡å‹çš„éœ€æ±‚ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§åä¸ºMINERVAçš„è§†é¢‘æ¨ç†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ˜¯å¦çœŸæ­£èƒ½å¤Ÿç»“åˆæ„ŸçŸ¥å’Œæ—¶é—´ä¿¡æ¯å¯¹è§†é¢‘è¿›è¡Œæ¨ç†ã€‚è¯¥æ•°æ®é›†åŒ…å«å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰äº”ä¸ªç­”æ¡ˆé€‰é¡¹ä»¥åŠè¯¦ç»†çš„æ‰‹å·¥æ¨ç†è½¨è¿¹ã€‚æ•°æ®é›†å…·æœ‰å¤šæ¨¡æ€æ€§ã€è§†é¢‘é¢†åŸŸå¤šæ ·æ€§å’Œé•¿åº¦å¤šæ ·æ€§ã€‚é€šè¿‡å¯¹å‰æ²¿å¼€æºå’Œä¸“æœ‰æ¨¡å‹çš„å¹¿æ³›åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶å›¢é˜Ÿå‘ç°è¯¥æ•°æ®é›†å¯¹è¿™äº›æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚åŒæ—¶è¿›è¡Œäº†ç²¾ç»†çš„é”™è¯¯åˆ†ææ¥ç¡®å®šå„ç§æ¨¡å‹çš„å¸¸è§æ•…éšœæ¨¡å¼ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ¨ç†é”™è¯¯çš„åˆ†ç±»ã€‚è¯¥æ•°æ®é›†ä»¥åŠä¸è¯¥æ•°æ®é›†ç›¸å…³çš„é—®é¢˜ã€ç­”æ¡ˆå€™é€‰å’Œæ¨ç†è½¨è¿¹å°†å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€LLMæ­£å°†é‡ç‚¹è½¬å‘è§†é¢‘åŸºå‡†æµ‹è¯•ï¼Œä½†ç°æœ‰è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦æä¾›ç»“æœç›‘ç£ï¼Œç¼ºä¹ä¸­é—´æˆ–å¯è§£é‡Šçš„æ¨ç†æ­¥éª¤ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†æ–°çš„è§†é¢‘æ¨ç†æ•°æ®é›†MINERVAï¼Œç”¨äºç°ä»£å¤šæ¨¡æ€æ¨¡å‹ã€‚</li>
<li>MINERVAæ•°æ®é›†åŒ…å«å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰äº”ä¸ªç­”æ¡ˆé€‰é¡¹å’Œè¯¦ç»†çš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>æ•°æ®é›†å…·æœ‰å¤šæ¨¡æ€æ€§ã€è§†é¢‘é¢†åŸŸå¤šæ ·æ€§å’Œé•¿åº¦å¤šæ ·æ€§ã€‚</li>
<li>åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒMINERVAæ•°æ®é›†å¯¹å‰æ²¿æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ç²¾ç»†çš„é”™è¯¯åˆ†æç¡®å®šäº†æ¨¡å‹çš„å¸¸è§å¤±è´¥æ¨¡å¼ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ¨ç†é”™è¯¯çš„åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8960e0d71f65aaa1d28ac94049936b9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f728bbcbfdddb9a0fd8cca63a5a10a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97696cdf1836a83e0815fdd18dfb92ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2523184de797db79a928773031d04292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe8e24dcb9831ed451f77a83553bad42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d2e5a184d2e8b375d24cc001ff53c00.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeepCritic-Deliberate-Critique-with-Large-Language-Models"><a href="#DeepCritic-Deliberate-Critique-with-Large-Language-Models" class="headerlink" title="DeepCritic: Deliberate Critique with Large Language Models"></a>DeepCritic: Deliberate Critique with Large Language Models</h2><p><strong>Authors:Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen</strong></p>
<p>As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å…¶è¾“å‡ºæä¾›å‡†ç¡®çš„åé¦ˆå’Œå¯æ‰©å±•çš„ç›‘ç£æˆä¸ºä¸€ä¸ªç´§è¿«ä¸”å…³é”®çš„é—®é¢˜ã€‚åˆ©ç”¨LLMä½œä¸ºæ‰¹è¯„æ¨¡å‹æ¥å®ç°è‡ªåŠ¨åŒ–ç›‘ç£æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºç ”ç©¶å’Œæé«˜LLMçš„æ•°å­¦æ‰¹è¯„èƒ½åŠ›ã€‚å½“å‰çš„LLMæ‰¹è¯„è€…æä¾›çš„æ‰¹è¯„è¿‡äºè‚¤æµ…ï¼Œå¯¼è‡´åˆ¤æ–­å‡†ç¡®æ€§ä½ï¼Œéš¾ä»¥ç»™LLMç”Ÿæˆå™¨æä¾›è¶³å¤Ÿçš„åé¦ˆæ¥çº æ­£é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–æœ‰æ•ˆçš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥å¼€å‘èƒ½å¤Ÿæœ‰é’ˆå¯¹æ€§åœ°å¯¹æ•°å­¦è§£å†³æ–¹æ¡ˆçš„æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œæ‰¹è¯„çš„LLMæ‰¹è¯„æ¨¡å‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨Qwen2.5-72B-Instructç”Ÿæˆ4.5Ké•¿å½¢å¼çš„æ‰¹è¯„ä½œä¸ºç›‘ç£å¾®è°ƒç§å­æ•°æ®ã€‚æ¯ä¸ªç§å­æ‰¹è¯„éƒ½åŒ…æ‹¬æœ‰é’ˆå¯¹æ€§çš„æ­¥éª¤æ‰¹è¯„ï¼ŒåŒ…æ‹¬å¤šè§†è§’éªŒè¯ä»¥åŠå¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤çš„åˆæ­¥æ‰¹è¯„çš„æ·±å…¥æ‰¹è¯„ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å¾®è°ƒæ¨¡å‹ä½¿ç”¨PRM800Kçš„ç°æœ‰æ‰‹å·¥æ ‡æ³¨æ•°æ®æˆ–æˆ‘ä»¬é€šè¿‡è’™ç‰¹å¡æ´›é‡‡æ ·ä¼°è®¡è‡ªåŠ¨æ ‡æ³¨çš„æ•°æ®è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥æ¿€åŠ±å…¶æ‰¹è¯„èƒ½åŠ›ã€‚æˆ‘ä»¬åŸºäºQwen2.5-7B-Instructå¼€å‘çš„æ‰¹è¯„æ¨¡å‹ä¸ä»…åœ¨å„ç§é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰LLMæ‰¹è¯„æ¨¡å‹ï¼ˆåŒ…æ‹¬ç›¸åŒè§„æ¨¡çš„DeepSeek-R1-distillæ¨¡å‹å’ŒGPT-4oï¼‰ï¼Œè€Œä¸”æ›´æœ‰æ•ˆåœ°å¸®åŠ©LLMç”Ÿæˆå™¨é€šè¿‡æ›´è¯¦ç»†çš„åé¦ˆæ¥ä¿®æ­£é”™è¯¯æ­¥éª¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00662v1">PDF</a> Work in progress. Data and models are available at   <a target="_blank" rel="noopener" href="https://github.com/RUCBM/DeepCritic">https://github.com/RUCBM/DeepCritic</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•å¸¦æ¥å¯¹å…¶è¾“å‡ºå‡†ç¡®åé¦ˆä¸ç›‘ç£çš„è¿«åˆ‡éœ€æ±‚ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºæå‡LLMsåœ¨æ•°å­¦é¢†åŸŸçš„æ‰¹åˆ¤èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒèƒ½å¤Ÿé’ˆå¯¹æ•°å­¦è§£é¢˜æ¯ä¸€æ­¥è¿›è¡Œæ·±åº¦æ‰¹åˆ¤çš„LLMæ‰¹åˆ¤æ¨¡å‹ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨Qwen2.5-72B-Instructç”Ÿæˆé•¿è¯„æ‰¹åˆ¤æ•°æ®ç”¨äºç›‘ç£å¾®è°ƒã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ‰¹åˆ¤èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæœ¬ç ”ç©¶å¼€å‘çš„æ‰¹åˆ¤æ¨¡å‹åœ¨é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰LLMæ‰¹åˆ¤æ¨¡å‹ï¼Œå¹¶ä¸ºLLMç”Ÿæˆå™¨æä¾›æ›´æœ‰æ•ˆçš„é”™è¯¯ä¿®æ­£åé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªåŠ¨åŒ–ç›‘ç£ä¸­ï¼Œåˆ©ç”¨LLMsä½œä¸ºæ‰¹åˆ¤æ¨¡å‹æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰LLMæ‰¹åˆ¤æ¨¡å‹åœ¨åˆ¤æ–­å‡†ç¡®åº¦å’Œæä¾›å……è¶³åé¦ˆæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦é¢†åŸŸã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è®­ç»ƒèƒ½å¤Ÿé’ˆå¯¹æ•°å­¦è§£é¢˜æ¯ä¸€æ­¥è¿›è¡Œæ·±åº¦æ‰¹åˆ¤çš„LLMæ‰¹åˆ¤æ¨¡å‹ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ç”Ÿæˆçš„é•¿è¯„æ•°æ®ç”¨äºç›‘ç£å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ‰¹åˆ¤èƒ½åŠ›ï¼Œä½¿ç”¨äººç±»æ ‡æ³¨æ•°æ®æˆ–è‡ªåŠ¨æ³¨é‡Šæ•°æ®ã€‚</li>
<li>å¼€å‘çš„æ‰¹åˆ¤æ¨¡å‹åœ¨é”™è¯¯è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå…¶ä»–LLMæ‰¹åˆ¤æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-106a5c46b9bc0bd6f929d45431ed4661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9a1db7a7e0b1968e52ace7210f70f98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f65a9f4e0212542596056ca1ea36e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9506250eeab1f12b11547f2bd8cd47a3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fast-and-Low-Cost-Genomic-Foundation-Models-via-Outlier-Removal"><a href="#Fast-and-Low-Cost-Genomic-Foundation-Models-via-Outlier-Removal" class="headerlink" title="Fast and Low-Cost Genomic Foundation Models via Outlier Removal"></a>Fast and Low-Cost Genomic Foundation Models via Outlier Removal</h2><p><strong>Authors:Haozheng Luo, Chenghao Qiu, Maojiang Su, Zhihan Zhou, Zoe Mehta, Guo Ye, Jerry Yao-Chieh Hu, Han Liu</strong></p>
<p>We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Empirically, transformer-based models exhibit greater robustness to adversarial perturbations compared to HyenaDNA, highlighting the impact of architectural design on vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†é¦–ä¸ªé’ˆå¯¹åŸºå› ç»„åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰çš„ç»Ÿä¸€å¯¹æŠ—æ€§æ”»å‡»åŸºå‡†æµ‹è¯•ï¼Œåä¸ºGERMã€‚ä¸åŒäºç°æœ‰çš„GFMåŸºå‡†æµ‹è¯•ï¼ŒGERMæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°GFMsåœ¨é­å—å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚åœ¨æ–¹æ³•å­¦ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨å››ç§å¹¿æ³›é‡‡ç”¨çš„æ”»å‡»ç®—æ³•å’Œä¸‰ç§é˜²å¾¡ç­–ç•¥ï¼Œè¯„ä¼°äº†äº”ç§æœ€å…ˆè¿›GFMsçš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªæ˜“äºè®¿é—®çš„ç»¼åˆæ¡†æ¶ï¼Œå¯ä»¥åˆ†æGFMåœ¨æ¨¡å‹æ¶æ„ã€é‡åŒ–æ–¹æ¡ˆå’Œè®­ç»ƒæ•°æ®é›†æ–¹é¢çš„è„†å¼±æ€§ã€‚ä»å®è¯è§’åº¦çœ‹ï¼ŒåŸºäºtransformerçš„æ¨¡å‹ä¸HyenaDNAç›¸æ¯”ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„å¯¹æŠ—æ€§æ‰°åŠ¨é²æ£’æ€§ï¼Œè¿™çªå‡ºäº†æ¶æ„è®¾è®¡å¯¹è„†å¼±æ€§çš„å½±å“ã€‚æ­¤å¤–ï¼Œå¯¹æŠ—æ€§æ”»å‡»ç»å¸¸é’ˆå¯¹ç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› ç»„åŒºåŸŸï¼Œè¿™è¡¨æ˜è¿™äº›æ¨¡å‹æœ‰æ•ˆåœ°æ•è·äº†æœ‰æ„ä¹‰çš„åºåˆ—ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00598v1">PDF</a> International Conference on Machine Learning (ICML) 2025</p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†é¦–ä¸ªé’ˆå¯¹åŸºå› ç»„åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰çš„ç»Ÿä¸€å¯¹æŠ—æ€§æ”»å‡»åŸºå‡†æµ‹è¯•ï¼Œåä¸ºGERMã€‚ä¸å…¶ä»–ç°æœ‰çš„GFMåŸºå‡†æµ‹è¯•ä¸åŒï¼ŒGERMæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿåœ°è¯„ä¼°GFMså¯¹å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å››ç§å¹¿æ³›é‡‡ç”¨çš„æ”»å‡»ç®—æ³•å’Œä¸‰ç§é˜²å¾¡ç­–ç•¥ï¼Œè¯„ä¼°äº†äº”ç§æœ€å…ˆè¿›GFMsçš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”æ˜“äºä½¿ç”¨çš„æ¡†æ¶ï¼Œç”¨äºåˆ†æGFMåœ¨æ¨¡å‹æ¶æ„ã€é‡åŒ–æ–¹æ¡ˆå’Œè®­ç»ƒæ•°æ®é›†æ–¹é¢çš„è„†å¼±æ€§ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºtransformerçš„æ¨¡å‹ç›¸è¾ƒäºHyenaDNAè¡¨ç°å‡ºæ›´å¼ºçš„å¯¹æŠ—æ€§æ‰°åŠ¨ç¨³å¥æ€§ï¼Œçªæ˜¾äº†æ¶æ„è®¾è®¡å¯¹è„†å¼±æ€§çš„å½±å“ã€‚åŒæ—¶ï¼Œå¯¹æŠ—æ€§æ”»å‡»ç»å¸¸é’ˆå¯¹ç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› ç»„åŒºåŸŸï¼Œè¡¨æ˜è¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•è·æœ‰æ„ä¹‰çš„åºåˆ—ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GERMæ˜¯é¦–ä¸ªé’ˆå¯¹åŸºå› ç»„åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰çš„ç»Ÿä¸€å¯¹æŠ—æ€§æ”»å‡»åŸºå‡†æµ‹è¯•ã€‚</li>
<li>GERMæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°GFMså¯¹å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å››ç§æ”»å‡»ç®—æ³•å’Œä¸‰ç§é˜²å¾¡ç­–ç•¥ï¼Œè¯„ä¼°äº†äº”ç§å…ˆè¿›GFMsçš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚</li>
<li>åŸºäºtransformerçš„æ¨¡å‹ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚</li>
<li>å¯¹æŠ—æ€§æ”»å‡»ç»å¸¸é’ˆå¯¹ç”Ÿç‰©å­¦ä¸Šé‡è¦çš„åŸºå› ç»„åŒºåŸŸã€‚</li>
<li>æ¨¡å‹æ¶æ„ã€é‡åŒ–æ–¹æ¡ˆå’Œè®­ç»ƒæ•°æ®é›†æ˜¯å½±å“GFMè„†å¼±æ€§çš„å…³é”®å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e6177659dead3fe04d47c27d5c0f6522.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067e38efdc5d397c2f3636df4dd2930c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2512dc6aa8635c45d9aa1daca4c8d767.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-232e4d382eb435f39d36ec2f85c54aaf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Leveraging-Partial-SMILES-Validation-Scheme-for-Enhanced-Drug-Design-in-Reinforcement-Learning-Frameworks"><a href="#Leveraging-Partial-SMILES-Validation-Scheme-for-Enhanced-Drug-Design-in-Reinforcement-Learning-Frameworks" class="headerlink" title="Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks"></a>Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks</h2><p><strong>Authors:Xinyu Wang, Jinbo Bi, Minghu Song</strong></p>
<p>SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery. </p>
<blockquote>
<p>åŸºäºSMILESçš„åˆ†å­ç”Ÿæˆæ–¹æ³•å·²ç»ä½œä¸ºè¯ç‰©å‘ç°ä¸­çš„ä¸€ç§å¼ºå¤§æ–¹æ³•è€Œå‡ºç°ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¢«çº³å…¥åˆ°åˆ†å­ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»¥åœ¨æ‰€éœ€çš„åˆ†å­å€™é€‰ç‰©çš„å¯èƒ½æ€§æ–¹é¢å®ç°é«˜åŒ¹é…åº¦ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¼ºåŒ–å­¦ä¹ é˜¶æ®µçš„ç¾éš¾æ€§é—å¿˜ï¼Œå…¶ä¸­å¦‚åˆ†å­æœ‰æ•ˆæ€§ç­‰çŸ¥è¯†ï¼ˆåœ¨é¢„è®­ç»ƒæœŸé—´é€šå¸¸è¶…è¿‡99ï¼…ï¼‰ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç›®å‰åº”ç”¨äºè¯ç‰©å‘ç°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚REINVENTï¼‰ä½¿ç”¨å…ˆéªŒæ¨¡å‹ä½œä¸ºé”šç‚¹æ¥ä¿ç•™é¢„è®­ç»ƒçŸ¥è¯†ï¼Œä½†è¿™äº›æ–¹æ³•ç¼ºä¹ç¨³å¥çš„æ¢ç´¢æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†éƒ¨åˆ†SMILESéªŒè¯-PPOï¼ˆPSV-PPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒç»“åˆäº†å®æ—¶éƒ¨åˆ†SMILESéªŒè¯ï¼Œä»¥é˜²æ­¢ç¾éš¾æ€§é—å¿˜å¹¶é¼“åŠ±æ¢ç´¢ã€‚ä¸ä¼ ç»Ÿçš„åªåœ¨ç”Ÿæˆæ•´ä¸ªåºåˆ—åè¿›è¡Œåˆ†å­ç»“æ„éªŒè¯çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒPSV-PPOåœ¨æ¯ä¸ªè‡ªå›å½’æ­¥éª¤ä¸­è¿›è¡Œé€æ­¥éªŒè¯ï¼Œä¸ä»…è¯„ä¼°æ‰€é€‰çš„ä»¤ç‰Œå€™é€‰è€…ï¼Œè¿˜è¯„ä¼°æºè‡ªå…ˆå‰éƒ¨åˆ†åºåˆ—çš„æ‰€æœ‰æ½œåœ¨åˆ†æ”¯ã€‚è¿™å¯ä»¥åœ¨æ‰€æœ‰æ½œåœ¨è·¯å¾„ä¸Šæ—©æœŸæ£€æµ‹æ— æ•ˆçš„SMILESç‰‡æ®µã€‚å› æ­¤ï¼Œå³ä½¿åœ¨åŒ–å­¦ç©ºé—´çš„å¹¿é˜”é¢†åŸŸä¸­è¿›è¡Œç§¯æçš„æ¢ç´¢æ—¶ï¼ŒPSV-PPOä¹Ÿèƒ½ä¿æŒè¾ƒé«˜çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨PMOå’ŒGuacaMolåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPSV-PPOåœ¨ä¿æŒæœ‰æ•ˆæ€§å’Œç«äº‰åŠ›æ¢ç´¢å’Œä¼˜åŒ–æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆçš„æ— æ•ˆç»“æ„æ•°é‡ã€‚è™½ç„¶æˆ‘ä»¬çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿æŒæœ‰æ•ˆæ€§ä¸Šï¼Œä½†PSV-PPOçš„æ¡†æ¶å¯ä»¥åœ¨æœªæ¥çš„ç ”ç©¶ä¸­æ‰©å±•åˆ°çº³å…¥å…¶ä»–å½¢å¼çš„å®è´µé¢†åŸŸçŸ¥è¯†ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºå¼ºåŒ–å­¦ä¹ åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00530v1">PDF</a> 17 pages, 5 main figures, 2 appendix figures. Submitted to ICML 2025</p>
<p><strong>Summary</strong>ï¼š<br>SMILESåŸºäºçš„åˆ†å­ç”Ÿæˆæ³•å·²æˆä¸ºè¯ç‰©å‘ç°ä¸­çš„å¼ºå¤§æ‰‹æ®µã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å¯å®ç°é«˜åŒ¹é…åº¦çš„åˆ†å­å€™é€‰ç”Ÿæˆã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µå­˜åœ¨ç¾éš¾æ€§é—å¿˜æŒ‘æˆ˜ï¼Œå¦‚åˆ†å­æœ‰æ•ˆæ€§ç­‰é¢„è®­ç»ƒçŸ¥è¯†ä¼šæ˜¾è‘—ä¸‹é™ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºéƒ¨åˆ†SMILESéªŒè¯PPOç®—æ³•ï¼ˆPSV-PPOï¼‰ï¼Œè¯¥ç®—æ³•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶éƒ¨åˆ†SMILESéªŒè¯ï¼Œé˜²æ­¢é—å¿˜å¹¶é¼“åŠ±æ¢ç´¢ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒPSV-PPOåœ¨æ¯ä¸€æ­¥ç”Ÿæˆæ—¶éƒ½è¿›è¡ŒéªŒè¯ï¼Œè¯„ä¼°å½“å‰é€‰æ‹©çš„æ ‡è®°å€™é€‰åŠæ‰€æœ‰æ½œåœ¨åˆ†æ”¯ï¼Œä»è€Œæ—©æœŸå‘ç°æ‰€æœ‰æ½œåœ¨è·¯å¾„ä¸­çš„æ— æ•ˆéƒ¨åˆ†SMILESã€‚å®éªŒè¯æ˜ï¼ŒPSV-PPOåœ¨ç»´æŒé«˜æœ‰æ•ˆæ€§ç‡çš„åŒæ—¶ï¼Œå…·æœ‰ç«äº‰åŠ›å¼ºçš„æ¢ç´¢å’Œä¼˜åŒ–æ€§èƒ½ã€‚è™½ç„¶æœ¬ç ”ç©¶ä¸»è¦å…³æ³¨ä¿æŒæœ‰æ•ˆæ€§ï¼Œä½†PSV-PPOæ¡†æ¶æœªæ¥å¯èå…¥æ›´å¤šæœ‰ä»·å€¼çš„é¢†åŸŸçŸ¥è¯†ï¼Œè¿›ä¸€æ­¥å¼ºåŒ–å…¶åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SMILESåŸºäºçš„åˆ†å­ç”Ÿæˆæ³•åœ¨è¯ç‰©å‘ç°ä¸­å…·æœ‰å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆç”¨äºç”Ÿæˆé«˜åŒ¹é…åº¦çš„åˆ†å­å€™é€‰ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µå­˜åœ¨ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå½±å“é¢„è®­ç»ƒçŸ¥è¯†çš„ä¿ç•™ã€‚</li>
<li>éƒ¨åˆ†SMILESéªŒè¯PPOç®—æ³•ï¼ˆPSV-PPOï¼‰æå‡ºè§£å†³æ­¤é—®é¢˜ï¼Œé€šè¿‡å®æ—¶éªŒè¯é˜²æ­¢é—å¿˜å¹¶é¼“åŠ±æ¢ç´¢ã€‚</li>
<li>PSV-PPOåœ¨æ¯ä¸€æ­¥ç”Ÿæˆæ—¶è¿›è¡ŒéªŒè¯ï¼Œæ—©æœŸå‘ç°æ— æ•ˆéƒ¨åˆ†SMILESã€‚</li>
<li>å®éªŒè¯æ˜PSV-PPOåœ¨ç»´æŒé«˜æœ‰æ•ˆæ€§ç‡çš„åŒæ—¶å…·æœ‰å¼ºçš„æ¢ç´¢å’Œä¼˜åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3042b86f43099b2765eb759d42ec1397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f76f6ba4238a1c94e3fc946303164f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdb51657e0f14f212597066eebc721c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c65519b3d1a9b97bc75bf877d38f37ae.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GEOM-Drugs-Revisited-Toward-More-Chemically-Accurate-Benchmarks-for-3D-Molecule-Generation"><a href="#GEOM-Drugs-Revisited-Toward-More-Chemically-Accurate-Benchmarks-for-3D-Molecule-Generation" class="headerlink" title="GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D   Molecule Generation"></a>GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D   Molecule Generation</h2><p><strong>Authors:Filipp Nikitin, Ian Dunn, David Ryan Koes, Olexandr Isayev</strong></p>
<p>Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and reliance on force fields inconsistent with the reference data. In this work, we revisit GEOM-Drugs and propose a corrected evaluation framework: we identify and fix issues in data preprocessing, construct chemically accurate valency tables, and introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and re-evaluate several leading models under this framework, providing updated performance metrics and practical recommendations for future benchmarking. Our results underscore the need for chemically rigorous evaluation practices in 3D molecular generation. Our recommended evaluation methods and GEOM-Drugs processing scripts are available at <a target="_blank" rel="noopener" href="https://github.com/isayevlab/geom-drugs-3dgen-evaluation">https://github.com/isayevlab/geom-drugs-3dgen-evaluation</a>. </p>
<blockquote>
<p>æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆçš„3Dåˆ†å­ç»“æ„æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼ŒGEOM-Drugsæ•°æ®é›†ä½œä¸ºä¸€ä¸ªå…³é”®çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°åè®®å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼ŒåŒ…æ‹¬ä»·æ€å®šä¹‰ä¸æ­£ç¡®ã€é”®åºè®¡ç®—ä¸­çš„é”™è¯¯ä»¥åŠä¸å‚è€ƒæ•°æ®ä¸ä¸€è‡´çš„åŠ›åœºä¾èµ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†GEOM-Drugsï¼Œå¹¶æå‡ºä¸€ä¸ªä¿®æ­£åçš„è¯„ä¼°æ¡†æ¶ï¼šæˆ‘ä»¬è¯†åˆ«å’Œä¿®å¤æ•°æ®é¢„å¤„ç†ä¸­çš„é—®é¢˜ï¼Œæ„å»ºåŒ–å­¦å‡†ç¡®çš„ä»·æ€è¡¨ï¼Œå¹¶å¼•å…¥åŸºäºGFN2-xTBçš„å‡ ä½•å’Œèƒ½é‡åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨è¯¥æ¡†æ¶ä¸‹é‡æ–°è®­ç»ƒå’Œè¯„ä¼°äº†å‡ æ¬¾é¢†å…ˆæ¨¡å‹ï¼Œæä¾›äº†æ›´æ–°çš„æ€§èƒ½æŒ‡æ ‡å’Œæœªæ¥åŸºå‡†æµ‹è¯•çš„å®ç”¨å»ºè®®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†3Dåˆ†å­ç”Ÿæˆä¸­åŒ–å­¦ä¸¥æ ¼è¯„ä¼°å®è·µçš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æ¨èçš„è¯„ä¼°æ–¹æ³•å’ŒGEOM-Drugså¤„ç†è„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/isayevlab/geom-drugs-3dgen-evaluation%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/isayevlab/geom-drugs-3dgen-evaluationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00169v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆçš„3Dåˆ†å­ç»“æ„æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼ŒGEOM-Drugsæ•°æ®é›†æ˜¯å…¶ä¸­çš„å…³é”®åŸºå‡†ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä»·åè®®å­˜åœ¨å…³é”®ç¼ºé™·ï¼ŒåŒ…æ‹¬ä»·æ€å®šä¹‰ä¸æ­£ç¡®ã€é”®åºè®¡ç®—ä¸­çš„é”™è¯¯ä»¥åŠä¸å‚è€ƒæ•°æ®ä¸ä¸€è‡´çš„åŠ›åœºä¾èµ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†GEOM-Drugsï¼Œå¹¶æå‡ºäº†ä¿®æ­£åçš„è¯„ä»·æ¡†æ¶ï¼šæˆ‘ä»¬è¯†åˆ«å’Œä¿®å¤äº†æ•°æ®é¢„å¤„ç†ä¸­çš„é—®é¢˜ï¼Œæ„å»ºäº†åŒ–å­¦å‡†ç¡®çš„ä»·æ€è¡¨ï¼Œå¹¶å¼•å…¥äº†åŸºäºGFN2-xTBçš„å‡ ä½•å’Œèƒ½é‡åŸºå‡†ã€‚æˆ‘ä»¬åœ¨è¿™ä¸ªæ¡†æ¶ä¸‹é‡æ–°è®­ç»ƒå’Œè¯„ä¼°äº†å‡ ç§é¢†å…ˆçš„æ¨¡å‹ï¼Œæä¾›äº†æ›´æ–°çš„æ€§èƒ½æŒ‡æ ‡å’Œå¯¹æœªæ¥åŸºå‡†æµ‹è¯•çš„å®ç”¨å»ºè®®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†åŒ–å­¦ä¸¥è°¨è¯„ä»·å®è·µåœ¨3Dåˆ†å­ç”Ÿæˆä¸­çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆ3Dåˆ†å­ç»“æ„æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼ŒGEOM-Drugsæ•°æ®é›†æ˜¯è¯„ä»·è¿™äº›æ¨¡å‹çš„å…³é”®åŸºå‡†ã€‚</li>
<li>å½“å‰çš„è¯„ä»·åè®®å­˜åœ¨ç¼ºé™·ï¼ŒåŒ…æ‹¬ä»·æ€å®šä¹‰ã€é”®åºè®¡ç®—ä»¥åŠåŠ›åœºä¸å‚è€ƒæ•°æ®çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†ä¿®æ­£åçš„è¯„ä»·æ¡†æ¶ï¼ŒåŒ…æ‹¬è¯†åˆ«å’Œä¿®å¤æ•°æ®é¢„å¤„ç†é—®é¢˜ï¼Œæ„å»ºåŒ–å­¦å‡†ç¡®çš„ä»·æ€è¡¨ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„å‡ ä½•å’Œèƒ½é‡åŸºå‡†ï¼ŒåŸºäºGFN2-xTBæ¨¡å‹ã€‚</li>
<li>åœ¨ä¿®æ­£åçš„æ¡†æ¶ä¸‹é‡æ–°è®­ç»ƒå’Œè¯„ä¼°äº†å¤šä¸ªé¢†å…ˆçš„æ¨¡å‹ã€‚</li>
<li>æä¾›äº†æ›´æ–°åçš„æ€§èƒ½æŒ‡æ ‡ï¼Œä»¥åæ˜ æ¨¡å‹åœ¨ä¿®æ­£åçš„è¯„ä»·æ¡†æ¶ä¸‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3fa2827fecd04b799c29b3886359ec3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50aae072186ef2e4c16f7c31ee370fc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d20063c6db3cc2bc0baa4652fc4fa1f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GDI-Bench-A-Benchmark-for-General-Document-Intelligence-with-Vision-and-Reasoning-Decoupling"><a href="#GDI-Bench-A-Benchmark-for-General-Document-Intelligence-with-Vision-and-Reasoning-Decoupling" class="headerlink" title="GDI-Bench: A Benchmark for General Document Intelligence with Vision and   Reasoning Decoupling"></a>GDI-Bench: A Benchmark for General Document Intelligence with Vision and   Reasoning Decoupling</h2><p><strong>Authors:Siqi Li, Yufan Shen, Xiangnan Chen, Jiayi Chen, Hengwei Ju, Haodong Duan, Song Mao, Hongbin Zhou, Bo Zhang, Pinlong Cai, Licheng Wen, Botian Shi, Yong Liu, Xinyu Cai, Yu Qiao</strong></p>
<p>The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these modelsâ€™ capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿè¿›æ­¥ï¼Œå®ƒå¯¹æ–‡æ¡£é¢†åŸŸäº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œå¹¶åˆ›å»ºäº†å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚è¿™ç§è¿›å±•å¼ºè°ƒéœ€è¦å¯¹è¿™äº›æ¨¡å‹åœ¨å„ç§ç‰¹å®šæ–‡æ¡£ä»»åŠ¡ä¸Šçš„èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°çš„éœ€è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸æ— æ³•æ‰¾åˆ°ç‰¹å®šæ¨¡å‹çš„å¼±ç‚¹æˆ–æŒ‡å¯¼ç³»ç»Ÿæ€§æ”¹è¿›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é€šç”¨æ–‡æ¡£æ™ºèƒ½åŸºå‡†æµ‹è¯•ï¼ˆGDI-Benchï¼‰ï¼Œæ¶µç›–9ä¸ªå…³é”®åœºæ™¯ä¸­çš„1900å¼ å›¾åƒå’Œ19ä¸ªç‰¹å®šæ–‡æ¡£ä»»åŠ¡ã€‚é€šè¿‡è§£é™¤è§†è§‰å¤æ‚æ€§å’Œæ¨ç†å¤æ‚æ€§çš„è€¦åˆï¼ŒGDI-Benchæ„å»ºäº†åˆ†çº§ä»»åŠ¡ï¼Œå…è®¸æŒ‰éš¾åº¦è¯„ä¼°æ€§èƒ½ï¼Œæœ‰åŠ©äºè¯†åˆ«æ¨¡å‹å¼±ç‚¹å¹¶æä¾›ä¼˜åŒ–æŒ‡å¯¼ã€‚æˆ‘ä»¬åœ¨å„ç§å¼€æºå’Œé—­æºæ¨¡å‹ä¸Šè¯„ä¼°GDI-Benchï¼Œåœ¨è§†è§‰å’Œæ¨ç†é¢†åŸŸè¿›è¡Œäº†è§£è€¦åˆ†æã€‚ä¾‹å¦‚ï¼ŒGPT-4oæ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºäº†è§£å†³GDI-Benchä¸­çš„ä¸åŒä»»åŠ¡å’Œé¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§GDIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ™ºèƒ½ä¿ç•™è®­ç»ƒç­–ç•¥ç¼“è§£äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä»¥å‰çš„åŸºå‡†æµ‹è¯•å’ŒGDI-Benchä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹éƒ½å°†å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00063v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•å¯¹æ–‡æ¡£é¢†åŸŸçš„å½±å“ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼ˆbenchmarksï¼‰çš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§é€šç”¨æ–‡æ¡£æ™ºèƒ½åŸºå‡†æµ‹è¯•ï¼ˆGDI-Benchï¼‰ï¼ŒåŒ…å«19ä¸ªé’ˆå¯¹æ–‡æ¡£ç‰¹å®šä»»åŠ¡çš„9ä¸ªä¸»è¦åœºæ™¯ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡åˆ†ç¦»è§†è§‰å¤æ‚æ€§å’Œæ¨ç†å¤æ‚æ€§ï¼Œå…è®¸æŒ‰éš¾åº¦è¯„ä¼°æ€§èƒ½ï¼Œä»è€Œå¸®åŠ©è¯†åˆ«æ¨¡å‹å¼±ç‚¹å¹¶æä¾›ä¼˜åŒ–æŒ‡å¯¼ã€‚ä½œè€…å¯¹å¤šç§å¼€æºå’Œé—­æºæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨è§†è§‰å’Œæ¨ç†é¢†åŸŸè¿›è¡Œäº†åˆ†ç¦»åˆ†æã€‚ä¾‹å¦‚ï¼ŒGPT-4oæ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºè§£å†³GDI-Benchä¸­çš„ä¸åŒä»»åŠ¡å’Œé¢†åŸŸé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§GDIæ¨¡å‹ï¼Œé€šè¿‡æ™ºèƒ½ä¿ç•™è®­ç»ƒç­–ç•¥ç¼“è§£ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨ä»¥å‰çš„æ ‡å‡†æµ‹è¯•å’ŒGDI-Benchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚æä¾›å…³äºæ ‡å‡†æµ‹è¯•å’Œæ¨¡å‹çš„å¼€æºèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•å¯¹æ–‡æ¡£é¢†åŸŸäº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œå‚¬ç”Ÿäº†å¤šç§åº”ç”¨åœºæ™¯ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ–‡æ¡£ç‰¹å®šä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨æ–‡æ¡£æ™ºèƒ½åŸºå‡†æµ‹è¯•ï¼ˆGDI-Benchï¼‰ï¼ŒåŒ…å«å¤šä¸ªæ–‡æ¡£ç‰¹å®šä»»åŠ¡å’Œå…³é”®åœºæ™¯ã€‚</li>
<li>GDI-Benchå…è®¸æŒ‰éš¾åº¦è¯„ä¼°æ€§èƒ½ï¼Œå¸®åŠ©è¯†åˆ«æ¨¡å‹çš„å¼±ç‚¹ï¼Œå¹¶ä¸ºä¼˜åŒ–æä¾›æŒ‡å¯¼ã€‚</li>
<li>ä½œè€…å¯¹å„ç§æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°åˆ†æï¼Œå‘ç°GPT-4oåœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è§†è§‰èƒ½åŠ›ä¸Šæœ‰å±€é™ã€‚</li>
<li>ä¸ºè§£å†³GDI-Benchä¸­çš„å¤šæ ·ä»»åŠ¡é—®é¢˜ï¼Œæå‡ºäº†GDIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ™ºèƒ½ä¿ç•™è®­ç»ƒç­–ç•¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00063">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c71cfd3a615aaa59d46b937be4e71c0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4a5bbe6a3bbaf47d6f74cbbaf43f7bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bdd1b9216ac10c4582355b621896534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-674a05bc6ee3e5285c7565f8a11abe89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caa47808d1551b7bceeadfc2019f2a6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee8e6020b78db30b4d75cd503a79c53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b56793db22f74543a5b9356c4ca7813c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138346abf92a0a5142280b55381664d6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Models-Are-Not-Pragmatically-Competent-in-Referring-Expression-Generation"><a href="#Vision-Language-Models-Are-Not-Pragmatically-Competent-in-Referring-Expression-Generation" class="headerlink" title="Vision-Language Models Are Not Pragmatically Competent in Referring   Expression Generation"></a>Vision-Language Models Are Not Pragmatically Competent in Referring   Expression Generation</h2><p><strong>Authors:Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai</strong></p>
<p>Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication. </p>
<blockquote>
<p>æŒ‡ä»£è¡¨è¾¾ç”Ÿæˆï¼ˆREGï¼‰æ˜¯è¯„ä¼°è§†è§‰è¯­è¨€ç³»ç»Ÿå®ç”¨èƒ½åŠ›çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œå®ƒè¦æ±‚ä¸ä»…è¯­ä¹‰åŸºç¡€è¦å‡†ç¡®ï¼Œè¿˜è¦éµå¾ªåˆä½œæ²Ÿé€šçš„åŸåˆ™ï¼ˆGriceï¼Œ1975ï¼‰ã€‚ç„¶è€Œï¼Œå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å½“å‰è¯„ä¼°å¾€å¾€å¿½è§†äº†å®ç”¨ç»´åº¦ï¼Œå°†REGç®€åŒ–ä¸ºåŸºäºåŒºåŸŸçš„æè¿°ä»»åŠ¡ï¼Œå¹¶å¿½ç•¥äº†Griceçš„æœ€å¤§å‡†åˆ™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å®ç”¨è§’åº¦é‡æ–°ç ”ç©¶REGï¼Œä»‹ç»äº†ä¸€ä¸ªåŒ…å«1500å¼ å›¾åƒçš„æ–°æ•°æ®é›†ï¼ˆRefOIï¼‰ï¼Œè¿™äº›å›¾åƒéƒ½ç»è¿‡ä¹¦é¢å’Œå£è¯­å‚è€ƒè¡¨è¾¾çš„æ³¨é‡Šã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„VLMçš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸‰ç§å…³é”®çš„å®ç”¨èƒ½åŠ›ç¼ºå¤±ï¼šï¼ˆ1ï¼‰æ— æ³•å”¯ä¸€ç¡®å®šæŒ‡ä»£å¯¹è±¡ï¼Œï¼ˆ2ï¼‰åŒ…å«è¿‡å¤šæˆ–æ— å…³çš„ä¿¡æ¯ï¼Œï¼ˆ3ï¼‰ä¸äººç±»å®ç”¨åå¥½ä¸ç¬¦ï¼Œä¾‹å¦‚å¾ˆå°‘ä½¿ç”¨æœ€å°ç©ºé—´çº¿ç´¢ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ ‡å‡†çš„è‡ªåŠ¨è¯„ä¼°æ— æ³•æ•æ‰åˆ°è¿™äº›å®ç”¨è§„åˆ™çš„è¿åæƒ…å†µï¼Œè€Œæ˜¯å¼ºè°ƒè¡¨é¢çº¿ç´¢è€ŒéçœŸæ­£çš„æŒ‡ä»£æˆåŠŸã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå‘¼åé‡ç‚¹å…³æ³¨å®ç”¨ä¿¡æ¯ä¸°å¯Œçš„æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶ï¼Œä½¿å…¶ä¸çœŸå®çš„äººç±»æ²Ÿé€šä¿æŒä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16060v2">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://vlm-reg.github.io/">https://vlm-reg.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç€é‡ä»‹ç»äº†æŒ‡ä»£è¡¨è¾¾å¼ç”Ÿæˆï¼ˆREGï¼‰åœ¨è¯„ä¼°è§†è§‰è¯­è¨€ç³»ç»Ÿçš„è¯­ç”¨èƒ½åŠ›æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚å½“å‰å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¯„ä¼°å¾€å¾€å¿½è§†äº†è¯­ç”¨ç»´åº¦ï¼Œæœ¬æ–‡åˆ™ä»è¯­ç”¨è§’åº¦é‡æ–°å®¡è§†REGï¼Œå¹¶å¼•å…¥æ–°çš„æ•°æ®é›†RefOIï¼ŒåŒ…å«1.5kå¼ å›¾åƒï¼Œæ ‡æ³¨äº†ä¹¦é¢å’Œå£è¯­æŒ‡ä»£è¡¨è¾¾å¼ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å…¶è¯­ç”¨èƒ½åŠ›çš„ä¸‰å¤§å¤±è´¥ä¹‹å¤„ï¼šæ— æ³•å”¯ä¸€ç¡®å®šæŒ‡ä»£å¯¹è±¡ã€åŒ…å«è¿‡å¤šæˆ–æ— å…³ä¿¡æ¯ï¼Œä»¥åŠä¸äººç±»è¯­ç”¨åå¥½çš„ä¸åŒ¹é…ï¼Œå¦‚è¿‡åº¦ä½¿ç”¨ç©ºé—´çº¿ç´¢ã€‚åŒæ—¶æŒ‡å‡ºï¼Œç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°æ— æ³•æ•æ‰è¿™äº›è¯­ç”¨è¿è§„è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»£è¡¨è¾¾å¼ç”Ÿæˆï¼ˆREGï¼‰æ˜¯è¯„ä¼°è§†è§‰è¯­è¨€ç³»ç»Ÿè¯­ç”¨èƒ½åŠ›çš„é‡è¦ä»»åŠ¡ã€‚</li>
<li>å½“å‰å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¯„ä¼°å¿½è§†äº†è¯­ç”¨ç»´åº¦ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®é›†RefOIï¼ŒåŒ…å«ä¹¦é¢å’Œå£è¯­æŒ‡ä»£è¡¨è¾¾å¼çš„å›¾åƒæ ‡æ³¨ã€‚</li>
<li>VLMsåœ¨è¯­ç”¨èƒ½åŠ›æ–¹é¢å­˜åœ¨ä¸‰å¤§å¤±è´¥ï¼šæ— æ³•å”¯ä¸€ç¡®å®šæŒ‡ä»£å¯¹è±¡ã€åŒ…å«è¿‡å¤šæˆ–æ— å…³ä¿¡æ¯ï¼Œä»¥åŠä¸äººç±»è¯­ç”¨åå¥½ä¸åŒ¹é…ã€‚</li>
<li>ç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°æ— æ³•æœ‰æ•ˆæ•æ‰è¿™äº›è¯­ç”¨è¿è§„è¡Œä¸ºã€‚</li>
<li>éœ€è¦é‡æ–°å…³æ³¨è¯­ç”¨ä¿¡æ¯é©±åŠ¨çš„æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5c5a29382b9570b83a9e362d2d5c76e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec658b1660843bf61cd35cc845ea8d64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa493bcc08fffdeb30dba69a9324fe41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfd9ad1fb0d94ba6a79c8c2e2fc6c83f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e24c06e89a7388ca156be2d99b8113a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ReasoningV-Efficient-Verilog-Code-Generation-with-Adaptive-Hybrid-Reasoning-Model"><a href="#ReasoningV-Efficient-Verilog-Code-Generation-with-Adaptive-Hybrid-Reasoning-Model" class="headerlink" title="ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model"></a>ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model</h2><p><strong>Authors:Haiyan Qin, Zhiwei Xie, Jingjing Li, Liangchen Li, Xiaotong Feng, Junzhan Liu, Wang Kang</strong></p>
<p>Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75% while preserving performance. Experimental results demonstrate ReasoningVâ€™s effectiveness with a pass@1 accuracy of 57.8% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at <a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/ReasoningV">https://github.com/BUAA-CLab/ReasoningV</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå¤§åœ°æ¨åŠ¨äº†Verilogä»£ç ç”Ÿæˆçš„å‘å±•ï¼Œä½†åœ¨æ•°æ®è´¨é‡ã€æ¨ç†èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReasoningVçš„æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œå°†è®­ç»ƒçš„å†…åœ¨èƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ç›¸ç»“åˆï¼Œç”¨äºVerilogä»£ç ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸‰é¡¹äº’è¡¥åˆ›æ–°ï¼š1ï¼‰ReasoningV-5Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5000ä¸ªåŠŸèƒ½éªŒè¯å®ä¾‹çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œé€šè¿‡PyraNetæ ·æœ¬çš„å¤šç»´è¿‡æ»¤åˆ›å»ºæ¨ç†è·¯å¾„ï¼›2ï¼‰ç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒåŸºç¡€çŸ¥è¯†ä¸å…¨å‚æ•°ä¼˜åŒ–å¢å¼ºæ¨ç†çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼›3ï¼‰è‡ªé€‚åº”æ¨ç†æœºåˆ¶ï¼Œæ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæœ€å¤šå¯å‡å°‘75%çš„ä»¤ç‰Œæ¶ˆè€—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoningVåœ¨VerilogEval-humanä¸Šçš„pass@1å‡†ç¡®ç‡ä¸º57.8%ï¼Œä¸é¢†å…ˆçš„å•†ä¸šæ¨¡å‹å¦‚Gemini-2.0-flashï¼ˆ59.5%ï¼‰ç›¸ç«äº‰ï¼Œå¹¶è¶…è¿‡ä¹‹å‰æœ€ä½³å¼€æºæ¨¡å‹çš„å‡†ç¡®ç‡æé«˜äº†10.4ä¸ªç™¾åˆ†ç‚¹ã€‚ReasoningVä¸ºæ¨è¿›AIé©±åŠ¨çš„ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†æ›´å¯é å’Œå¯è®¿é—®çš„é€”å¾„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/ReasoningV%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/BUAA-CLab/ReasoningVä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14560v3">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Verilogä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´æ•°æ®è´¨é‡ã€æ¨ç†èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReasoningVçš„æ–°å‹æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œå°†è®­ç»ƒå¥½çš„å†…åœ¨èƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ç›¸ç»“åˆï¼Œç”¨äºVerilogä»£ç ç”Ÿæˆã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰é¡¹äº’è¡¥åˆ›æ–°ï¼šé«˜è´¨é‡æ•°æ®é›†ReasoningV-5Kã€ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œè‡ªé€‚åº”æ¨ç†æœºåˆ¶ã€‚å®éªŒç»“æœè¯æ˜äº†ReasoningVçš„æœ‰æ•ˆæ€§ï¼Œåœ¨VerilogEval-humanä¸Šçš„pass@1å‡†ç¡®ç‡è¾¾åˆ°57.8%ï¼Œæ€§èƒ½é¢†å…ˆå•†ä¸šæ¨¡å‹Gemini-2.0-flashï¼Œå¹¶è¶…è¿‡äº†ä¹‹å‰æœ€å¥½çš„å¼€æºæ¨¡å‹10.4ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Verilogä»£ç ç”Ÿæˆä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ReasoningVæ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œç»“åˆè®­ç»ƒå†…åœ¨èƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ã€‚</li>
<li>å¼•å…¥é«˜è´¨é‡æ•°æ®é›†ReasoningV-5Kï¼Œé€šè¿‡å¤šç»´åº¦è¿‡æ»¤PyraNetæ ·æœ¬åˆ›å»ºã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆåŸºç¡€çŸ¥è¯†çš„å‚æ•°æœ‰æ•ˆå¾®è°ƒä¸å¢å¼ºæ¨ç†çš„å…¨å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>è‡ªé€‚åº”æ¨ç†æœºåˆ¶å¯æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œå‡å°‘ä»¤ç‰Œæ¶ˆè€—ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºReasoningVåœ¨VerilogEval-humanä¸Šçš„æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œè¶…è¿‡äº†ä¸€äº›å•†ä¸šå’Œå¼€æºæ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e7b8510a4892c9dcf2fc16f8bcb67b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a483b045bf8c322d61adf476f9845e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc9409459bb9d6267659616f2819087.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae54faf9bec6f7d91b44cd5c09ab191b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-193417937435ff370b6adf15587742be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f9491cb87d44f3743cb93377b327c1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af554e19747e2b7b7d10d2f7b0579219.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed43953374c595d83b614692912f5274.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•ï¼Œç§°ä¸ºç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚ä¸å¸¸è§„æ–¹æ³•ä¸åŒï¼ŒGPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤è¯„ä»·è€…å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºGRPOã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E3%80%82">https://github.com/AMAP-ML/GPGã€‚</a></p>
</blockquote>
<hr>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿç›´æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡åº¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®ç­‰é—®é¢˜ï¼ŒGPGç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œç›¸è¾ƒäºç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚GPGåœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¯æ˜ï¼ŒGPGä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå‡ä¼˜äºGRPOã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPGæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸éœ€è¦è¿‡åº¦ä¾èµ–ç›‘ç£å¾®è°ƒã€‚</li>
<li>ç ”ç©¶è€…æå‡ºäº†ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æç®€RLæ–¹æ³•ã€‚</li>
<li>GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œæ— éœ€ä½¿ç”¨æ›¿ä»£æŸå¤±å‡½æ•°ã€‚</li>
<li>GPGç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰å¾ˆå¤§ä¼˜åŠ¿ã€‚</li>
<li>GPGæ–¹æ³•åœ¨ä¸ä½¿ç”¨è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>GPGåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0154bfaf18875c7cae4ded2d987de76b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb3eeaa103c7e595684cf660d6970abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bfb126d0f4a84aad9796b7bb3580cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9977e8d13e79fe1db75e4260ba5d161.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea27ba65c8fe28125622d097454df66c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Instantiation-based-Formalization-of-Logical-Reasoning-Tasks-using-Language-Models-and-Logical-Solvers"><a href="#Instantiation-based-Formalization-of-Logical-Reasoning-Tasks-using-Language-Models-and-Logical-Solvers" class="headerlink" title="Instantiation-based Formalization of Logical Reasoning Tasks using   Language Models and Logical Solvers"></a>Instantiation-based Formalization of Logical Reasoning Tasks using   Language Models and Logical Solvers</h2><p><strong>Authors:Mohammad Raza, Natasa Milic-Frayling</strong></p>
<p>Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such <em>near-certain reasoning</em> as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems. </p>
<blockquote>
<p>æ¨ç†çš„ç¨³å¥æ€§å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè§£å†³è¿™ä¸€æŒ‘æˆ˜å¯¹äºäººå·¥æ™ºèƒ½é©±åŠ¨æ¨ç†ç³»ç»Ÿçš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰è‡ªéªŒè¯ï¼ˆSSVï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œè§£å†³äº†å°†è¯­è¨€æ¨¡å‹ä¸é€»è¾‘æ±‚è§£å™¨ä¸¥è°¨æ€§ç›¸ç»“åˆçš„å…³é”®æŒ‘æˆ˜ï¼šå³å‡†ç¡®åœ°å°†æ¨ç†é—®é¢˜ä»è‡ªç„¶è¯­è¨€è¡¨è¿°è½¬åŒ–ä¸ºæ±‚è§£å™¨çš„æ­£å¼è¯­è¨€ã€‚SSVä½¿ç”¨åŸºäºä¸€è‡´æ€§çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹ç”Ÿæˆçš„å…·ä½“å®ä¾‹å’Œæ±‚è§£å™¨çš„éªŒè¯ï¼Œäº§ç”Ÿå¯¹é—®é¢˜çš„å¼ºå¤§æŠ½è±¡å½¢å¼åŒ–ã€‚é™¤äº†æ˜¾è‘—æé«˜æ•´ä½“æ¨ç†ç²¾åº¦ï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯æ°´å¹³ï¼Œè¯¥æ–¹æ³•çš„å…³é”®æ–°é¢–ä¹‹å¤„åœ¨äºéªŒè¯åŠŸèƒ½ï¼Œåœ¨å¤§é‡æ¡ˆä¾‹ä¸­å‡ ä¹è¾¾åˆ°äº†å®Œç¾çš„ç²¾åº¦ï¼Œæˆ‘ä»¬åœ¨å…¬å¼€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬æå‡ºå°†è¿™ç§â€œè¿‘ä¹ç¡®å®šçš„æ¨ç†â€ä½œä¸ºå‡å°‘è®¸å¤šæƒ…å†µä¸‹å¯¹äººå·¥éªŒè¯éœ€æ±‚çš„æ–°æ–¹æ³•ï¼Œä½¿æˆ‘ä»¬æ›´æ¥è¿‘å¯é å’Œè‡ªä¸»çš„äººå·¥æ™ºèƒ½æ¨ç†ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16961v2">PDF</a> IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ç¨³å¥æ€§æŒ‘æˆ˜å¯¹äºAIé©±åŠ¨çš„æ¨ç†ç³»ç»Ÿçš„å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åä¸ºè¯­ä¹‰è‡ªéªŒè¯ï¼ˆSSVï¼‰çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†å°†è¯­è¨€æ¨¡å‹ä¸é€»è¾‘æ±‚è§£å™¨ç»“åˆçš„å…³é”®éš¾é¢˜ï¼šå°†è‡ªç„¶è¯­è¨€å‡†ç¡®è½¬åŒ–ä¸ºæ±‚è§£å™¨çš„å½¢å¼è¯­è¨€ã€‚SSVä½¿ç”¨ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹ç”Ÿæˆçš„å®ä¾‹è¿›è¡ŒæŠ½è±¡é—®é¢˜çš„å½¢å¼åŒ–ï¼Œå¹¶ç”±æ±‚è§£å™¨è¿›è¡ŒéªŒè¯ã€‚é™¤äº†æ˜¾è‘—æé«˜æ•´ä½“æ¨ç†ç²¾åº¦å¤–ï¼Œè¯¥æ–¹æ³•çš„å…³é”®æ–°é¢–ä¹‹å¤„åœ¨äºå…¶éªŒè¯åŠŸèƒ½ï¼Œå®ƒåœ¨å¹¿æ³›çš„æ¡ˆä¾‹ä¸­å‡ ä¹å…·æœ‰å®Œç¾çš„ç²¾ç¡®åº¦ã€‚æˆ‘ä»¬æå‡ºäº†è¿™ç§è¿‘ä¹ç¡®å®šçš„æ¨ç†ä½œä¸ºä¸€ç§æ–°æ–¹æ³•ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹å‡å°‘å¯¹æ‰‹åŠ¨éªŒè¯çš„éœ€æ±‚ï¼Œä½¿æˆ‘ä»¬æ›´æ¥è¿‘äºæ›´å¯é å’Œæ›´è‡ªä¸»çš„AIæ¨ç†ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ç¨³å¥æ€§å¯¹äºAIæ¨ç†ç³»ç»Ÿçš„å®é™…åº”ç”¨æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯­ä¹‰è‡ªéªŒè¯ï¼ˆSSVï¼‰æ˜¯ä¸€ç§è§£å†³è¯­è¨€æ¨¡å‹ä¸é€»è¾‘æ±‚è§£å™¨ç»“åˆçš„å…³é”®éš¾é¢˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>SSVé€šè¿‡å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºæ±‚è§£å™¨çš„å½¢å¼è¯­è¨€æ¥æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚</li>
<li>SSVä½¿ç”¨ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡æ¨¡å‹ç”Ÿæˆçš„å®ä¾‹è¿›è¡ŒæŠ½è±¡é—®é¢˜çš„å½¢å¼åŒ–ã€‚</li>
<li>SSVç”±æ±‚è§£å™¨è¿›è¡ŒéªŒè¯ï¼Œå…·æœ‰æ˜¾è‘—çš„éªŒè¯åŠŸèƒ½ã€‚</li>
<li>SSVæ–¹æ³•æé«˜äº†æ•´ä½“æ¨ç†ç²¾åº¦ï¼Œå¹¶åœ¨å¹¿æ³›çš„æ¡ˆä¾‹ä¸­å‡ ä¹å…·æœ‰å®Œç¾çš„ç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b004b8d79752e734daad1dbc4df81483.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eb50979d7887c484e122ab53d27c029.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66acb51c0892c82fc93b01cf7d99d8ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ba345f4dd45d3e192c95df3b4837fa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33d5268eec54ecfb3c58e0288bc52482.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50dbb3c0506338f99ae1ae72da684d7c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5c015040c65197a1ef8f66bcf0d13365.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-03  T2I-R1 Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-510eb9ac8f45906b22957e3984330aa6.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  Talk Before You Retrieve Agent-Led Discussions for Better RAG in   Medical QA
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
