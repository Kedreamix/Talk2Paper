<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  VITA-Audio Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-07be0c23b7fac6f4ca15a78037644159.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="VITA-Audio-Fast-Interleaved-Cross-Modal-Token-Generation-for-Efficient-Large-Speech-Language-Model"><a href="#VITA-Audio-Fast-Interleaved-Cross-Modal-Token-Generation-for-Efficient-Large-Speech-Language-Model" class="headerlink" title="VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model"></a>VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model</h2><p><strong>Authors:Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, Xing Sun</strong></p>
<p>With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks. </p>
<blockquote>
<p>éšç€äººç±»å¯¹è‡ªç„¶äººæœºäº¤äº’çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼ŒåŸºäºè¯­éŸ³çš„ç³»ç»Ÿå¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå› ä¸ºè¯­éŸ³æ˜¯æ—¥å¸¸æ²Ÿé€šä¸­æœ€å¸¸è§çš„å½¢å¼ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­éŸ³æ¨¡å‹åœ¨æµæ¨¡å¼ä¸‹ç”Ÿæˆç¬¬ä¸€ä¸ªéŸ³é¢‘ä»¤ç‰Œæ—¶ä»ç„¶å­˜åœ¨è¾ƒé«˜çš„å»¶è¿Ÿï¼Œè¿™å¯¹éƒ¨ç½²æ„æˆäº†é‡å¤§ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VITA-Audioï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å¿«é€ŸéŸ³é¢‘æ–‡æœ¬ä»¤ç‰Œç”ŸæˆåŠŸèƒ½çš„ç«¯åˆ°ç«¯å¤§å‹è¯­éŸ³æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€ä»¤ç‰Œé¢„æµ‹ï¼ˆMCTPï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥åœ¨å•ä¸ªæ¨¡å‹å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°ç”Ÿæˆå¤šä¸ªéŸ³é¢‘ä»¤ç‰Œï¼Œè¿™ä¸ä»…åŠ é€Ÿäº†æ¨ç†ï¼Œè€Œä¸”æ˜¾è‘—é™ä½äº†æµåœºæ™¯ä¸­ç”Ÿæˆç¬¬ä¸€ä¸ªéŸ³é¢‘çš„å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§å››é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œä»¥åœ¨å°½å¯èƒ½å‡å°‘è¯­éŸ³è´¨é‡æŸå¤±çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹åŠ é€Ÿã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒVITA-Audioæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨ç¬¬ä¸€æ¬¡å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­ç”ŸæˆéŸ³é¢‘è¾“å‡ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯å®ç°å®æ—¶å¯¹è¯åŠŸèƒ½ï¼Œå»¶è¿Ÿæä½ã€‚VITA-Audioå¯å®Œå…¨å¤åˆ¶ä¸”åªæ¥å—å¼€æºæ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨7Bå‚æ•°è§„æ¨¡ä¸‹å®ç°äº†3~5å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œè¯­éŸ³é—®ç­”ï¼ˆSQAï¼‰ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03739v1">PDF</a> Training and Inference Codes: <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA-Audio">https://github.com/VITA-MLLM/VITA-Audio</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†éšç€äººç±»è®¡ç®—æœºäº¤äº’éœ€æ±‚çš„å¢é•¿ï¼Œè¯­éŸ³ç³»ç»Ÿå¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¸ºäº†è§£å†³ç°æœ‰è¯­éŸ³æ¨¡å‹åœ¨æµå¼åœºæ™¯ä¸‹ç”Ÿæˆé¦–ä¸ªéŸ³é¢‘ä»¤ç‰Œæ—¶çš„é«˜å»¶è¿Ÿé—®é¢˜ï¼Œæå‡ºäº†VITA-Audioç«¯åˆ°ç«¯å¤§å‹è¯­éŸ³æ¨¡å‹ï¼Œå…·å¤‡å¿«é€ŸéŸ³é¢‘æ–‡æœ¬ä»¤ç‰Œç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è½»é‡çº§çš„è·¨æ¨¡æ€ä»¤ç‰Œé¢„æµ‹æ¨¡å—ï¼Œå®ç°äº†ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆå¤šä¸ªéŸ³é¢‘ä»¤ç‰Œï¼Œä»è€ŒåŠ é€Ÿäº†æ¨ç†å¹¶é™ä½äº†å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†å››é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œä»¥åœ¨ä¿æŒæœ€å°è¯­éŸ³è´¨é‡æŸå¤±çš„æƒ…å†µä¸‹åŠ é€Ÿæ¨¡å‹ã€‚VITA-Audioæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨é¦–æ¬¡å‰å‘ä¼ é€’ä¸­äº§ç”ŸéŸ³é¢‘è¾“å‡ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯å®ç°å®æ—¶å¯¹è¯åŠŸèƒ½å¹¶æ˜¾è‘—é™ä½å»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡ä¸º7Bæ—¶å®ç°äº†æ¨ç†é€Ÿåº¦çš„3~5å€æå‡ï¼Œå¹¶ä¸”åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­éŸ³é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€äººç±»å¯¹è®¡ç®—æœºäº¤äº’çš„è‡ªç„¶éœ€æ±‚å¢é•¿ï¼Œè¯­éŸ³ç³»ç»Ÿå¤‡å—å…³æ³¨ã€‚</li>
<li>ç°æœ‰è¯­éŸ³æ¨¡å‹åœ¨æµå¼åœºæ™¯ä¸‹ç”Ÿæˆé¦–ä¸ªéŸ³é¢‘ä»¤ç‰Œæ—¶å­˜åœ¨é«˜å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>VITA-Audioæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤§å‹è¯­éŸ³æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>VITA-Audioé€šè¿‡å¼•å…¥MCTPæ¨¡å—ï¼Œå®ç°äº†ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆå¤šä¸ªéŸ³é¢‘ä»¤ç‰Œï¼ŒåŠ é€Ÿäº†æ¨ç†å¹¶é™ä½äº†å»¶è¿Ÿã€‚</li>
<li>é‡‡ç”¨å››é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ä»¥åŠ é€Ÿæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„è¯­éŸ³è´¨é‡æŸå¤±ã€‚</li>
<li>VITA-Audioæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨é¦–æ¬¡å‰å‘ä¼ é€’ä¸­äº§ç”ŸéŸ³é¢‘è¾“å‡ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5670c48015b6d8a3c54331fb3b46f911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947cc875237490afd6f0bc1a990d6e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8a03ce8cd6e88a4e48df47b0b892da5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50d29299695b980d334c08c8692f90a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a843f2703e4d040a0a8962f1f74950d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1bfe1c238d94c64994a433eeb7803c3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WebGen-Bench-Evaluating-LLMs-on-Generating-Interactive-and-Functional-Websites-from-Scratch"><a href="#WebGen-Bench-Evaluating-LLMs-on-Generating-Interactive-and-Functional-Websites-from-Scratch" class="headerlink" title="WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional   Websites from Scratch"></a>WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional   Websites from Scratch</h2><p><strong>Authors:Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, Hongsheng Li</strong></p>
<p>LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agentâ€™s ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2%, surpassing the performance of the best proprietary model. </p>
<blockquote>
<p>åŸºäºLLMçš„ä»£ç†åœ¨å¤æ‚çš„ä»£ç åº“ä¸­ç”Ÿæˆå’Œç®¡ç†ä»£ç æ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†WebGen-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¡¡é‡åŸºäºLLMçš„ä»£ç†ä»é›¶å¼€å§‹åˆ›å»ºå¤šæ–‡ä»¶ç½‘ç«™ä»£ç åº“çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«äº†å¤šæ ·åŒ–çš„ç½‘ç«™ç”ŸæˆæŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤æ˜¯é€šè¿‡äººç±»æ³¨é‡Šè€…å’ŒGPT-4oçš„å…±åŒåŠªåŠ›åˆ›å»ºçš„ã€‚è¿™äº›æŒ‡ä»¤æ¶µç›–äº†ä¸‰ä¸ªä¸»è¦ç±»åˆ«å’Œåä¸‰ä¸ªæ¬¡è¦ç±»åˆ«ï¼Œæ¶µç›–äº†å‡ ä¹æ‰€æœ‰é‡è¦ç±»å‹çš„webåº”ç”¨ç¨‹åºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03733v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMæ¨¡å‹åœ¨ç”Ÿæˆå’Œç®¡ç†å¤æ‚ä»£ç åº“ä¸­çš„ä»£ç æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹åŸºå‡†æµ‹è¯•WebGen-Benchï¼Œæ—¨åœ¨è¯„ä¼°LLMæ¨¡å‹åˆ›å»ºå¤šæ–‡ä»¶ç½‘ç«™ä»£ç åº“çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¸°å¯Œå¤šæ ·çš„ç½‘ç«™ç”ŸæˆæŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤ç”±äººç±»æ³¨é‡Šå™¨å’ŒGPT-4oå…±åŒåˆ›å»ºï¼Œæ¶µç›–ä¸‰å¤§ç±»åˆ«å’Œåä¸‰ä¸ªå°ç±»åˆ«ï¼Œå›Šæ‹¬å‡ ä¹æ‰€æœ‰é‡è¦ç±»å‹çš„webåº”ç”¨ç¨‹åºã€‚ä¸ºè¯„ä¼°ç”Ÿæˆçš„ç½‘ç«™è´¨é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨GPT-4oé’ˆå¯¹æŒ‡ä»¤ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œç„¶åè¿›è¡Œäººå·¥è¿‡æ»¤ã€è°ƒæ•´å’Œæ•´ç†ï¼Œç¡®ä¿å‡†ç¡®æ€§ï¼Œæœ€ç»ˆç”Ÿæˆ647ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨å¼ºå¤§çš„ç½‘ç»œå¯¼èˆªä»£ç†æ‰§è¡Œæµ‹è¯•ï¼Œä»¥è‡ªåŠ¨åŒ–æµ‹è¯•å’Œæé«˜ç»“æœçš„é‡å¤æ€§ã€‚æœ¬æ–‡å¯¹ä¸‰ä¸ªé«˜æ€§èƒ½ä»£ç ä»£ç†æ¡†æ¶Bolt.diyã€OpenHandså’ŒAiderè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨å¤šä¸ªä¸“æœ‰å’Œå¼€æºLLMä½œä¸ºå¼•æ“ã€‚è¡¨ç°æœ€ä½³çš„ç»„åˆæ˜¯DeepSeek-R1é©±åŠ¨çš„Bolt.diyï¼Œåœ¨æµ‹è¯•ç”¨ä¾‹ä¸Šä»…è¾¾åˆ°27.8%çš„å‡†ç¡®ç‡ï¼Œå‡¸æ˜¾äº†åŸºå‡†æµ‹è¯•çš„è‰°å·¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†WebGen-Instructè®­ç»ƒé›†ï¼ŒåŒ…å«6667æ¡ç½‘ç«™ç”ŸæˆæŒ‡ä»¤ã€‚ä½¿ç”¨Bolt.diyä»éƒ¨åˆ†è®­ç»ƒé›†ä¸­ç”Ÿæˆçš„è½¨è¿¹è®­ç»ƒQwen2.5-Coder-32B-Instructæ¨¡å‹ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†38.2%ï¼Œè¶…è¿‡äº†æœ€ä½³ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMæ¨¡å‹åœ¨ç”Ÿæˆå¤šæ–‡ä»¶ç½‘ç«™ä»£ç åº“æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å¼•å…¥WebGen-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMæ¨¡å‹åˆ›å»ºç½‘ç«™ä»£ç çš„èƒ½åŠ›ã€‚</li>
<li>WebGen-BenchåŒ…å«ç”±äººç±»æ³¨é‡Šå™¨å’ŒGPT-4oå…±åŒåˆ›å»ºçš„ä¸°å¯Œå¤šæ ·çš„ç½‘ç«™ç”ŸæˆæŒ‡ä»¤ã€‚</li>
<li>ä½¿ç”¨GPT-4oç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶ç»“åˆäººå·¥è¿‡æ»¤ã€è°ƒæ•´å’Œæ•´ç†ï¼Œç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
<li>è‡ªåŠ¨åŒ–æµ‹è¯•é€šè¿‡å¼ºå¤§çš„ç½‘ç»œå¯¼èˆªä»£ç†æ‰§è¡Œï¼Œæé«˜ç»“æœçš„é‡å¤æ€§ã€‚</li>
<li>è¯„ä¼°äº†ä¸‰ä¸ªé«˜æ€§èƒ½ä»£ç ä»£ç†æ¡†æ¶ï¼Œè¡¨ç°æœ€ä½³çš„ç»„åˆæ˜¯DeepSeek-R1é©±åŠ¨çš„Bolt.diyï¼Œä½†å‡†ç¡®ç‡ä»…ä¸º27.8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5280e0b2c446a1a083867b39781726f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577f21e7baa5543736afadef84e3b4f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc0f0907cb346c72db628bf5fbb68ffc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c30c32e91956599c7f0d7a273f5a7939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a11be7a23316cb67ced9b17a30743e4a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PhysLLM-Harnessing-Large-Language-Models-for-Cross-Modal-Remote-Physiological-Sensing"><a href="#PhysLLM-Harnessing-Large-Language-Models-for-Cross-Modal-Remote-Physiological-Sensing" class="headerlink" title="PhysLLM: Harnessing Large Language Models for Cross-Modal Remote   Physiological Sensing"></a>PhysLLM: Harnessing Large Language Models for Cross-Modal Remote   Physiological Sensing</h2><p><strong>Authors:Yiping Xie, Bo Zhao, Mingtong Dai, Jian-Ping Zhou, Yue Sun, Tao Tan, Weicheng Xie, Linlin Shen, Zitong Yu</strong></p>
<p>Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios. </p>
<blockquote>
<p>è¿œç¨‹å…‰ä½“ç§¯æè®°æœ¯ï¼ˆrPPGï¼‰èƒ½å¤Ÿå®ç°éæ¥è§¦å¼ç”Ÿç†æµ‹é‡ï¼Œä½†ä»å—åˆ°å…‰ç…§å˜åŒ–ã€è¿åŠ¨ä¼ªå½±å’Œæœ‰é™æ—¶é—´å»ºæ¨¡çš„é«˜åº¦å½±å“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œæä¾›æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºå…¶ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„è®¾è®¡ï¼Œå®ƒä»¬åœ¨å¤„ç†rPPGä¿¡å·çš„è¿ç»­æ€§å’Œå¯¹å™ªå£°çš„æ•æ„Ÿæ€§æ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhysLLMï¼Œè¿™æ˜¯ä¸€ä¸ªååŒä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†LLMä¸ç‰¹å®šé¢†åŸŸçš„rPPGç»„ä»¶ç›¸ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼Œæå‡ºäº†Text Prototype Guidanceï¼ˆTPGï¼‰ç­–ç•¥ï¼Œé€šè¿‡å»ºç«‹è¡€æµåŠ¨åŠ›å­¦ç‰¹å¾ä¸LLMå¯è§£é‡Šè¯­ä¹‰ç©ºé—´çš„è·¨æ¨¡æ€å¯¹é½ï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†ç”Ÿç†ä¿¡å·ä¸è¯­è¨€ç¬¦å·ä¹‹é—´çš„ä»£è¡¨æ€§å·®è·ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„Dual-Domain Stationaryï¼ˆDDSï¼‰ç®—æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”æ—¶é¢‘åŸŸç‰¹å¾é‡æ–°åŠ æƒæ¥è§£å†³ä¿¡å·ä¸ç¨³å®šçš„é—®é¢˜ã€‚æœ€åï¼Œé€šè¿‡ç”Ÿç†ç»Ÿè®¡ã€ç¯å¢ƒä¸Šä¸‹æ–‡å›ç­”å’Œä»»åŠ¡æè¿°ç­‰rPPGä»»åŠ¡ç‰¹å®šçº¿ç´¢ï¼Œåˆ©ç”¨è·¨æ¨¡æ€å­¦ä¹ æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œå®ç°åŠ¨æ€é€‚åº”å…‰ç…§å˜åŒ–å’Œä¸»ä½“ç§»åŠ¨ç­‰æŒ‘æˆ˜åœºæ™¯ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPhysLLMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œåœ¨ç…§æ˜å˜åŒ–å’Œè¿åŠ¨åœºæ™¯ä¸­æ˜¾ç¤ºå‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03621v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿œç¨‹å…‰å®¹ç§¯è„‰ææ³¢æˆåƒï¼ˆrPPGï¼‰èƒ½å®ç°éæ¥è§¦ç”Ÿç†æµ‹é‡ï¼Œä½†æ˜“å—å…‰ç…§å˜åŒ–ã€è¿åŠ¨å¹²æ‰°å½±å“ï¼Œä¸”ç¼ºä¹é•¿æ—¶é—´åŠ¨æ€å»ºæ¨¡èƒ½åŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½æ“…é•¿æ•æ‰é•¿æ—¶ä¾èµ–å…³ç³»ï¼Œä½†å› å…¶æ–‡æœ¬è®¾è®¡ä¸­å¿ƒç‰¹æ€§ï¼Œå¯¹è¿ç»­ä¸”å™ªå£°æ•æ„Ÿçš„rPPGä¿¡å·å¤„ç†èƒ½åŠ›ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§èåˆLLMä¸rPPGæŠ€æœ¯ä¼˜åŠ¿çš„ååŒä¼˜åŒ–æ¡†æ¶â€”â€”PhysLLMã€‚é€šè¿‡æ–‡æœ¬åŸå‹å¼•å¯¼ç­–ç•¥ï¼ˆTPGï¼‰å»ºç«‹è·¨æ¨¡æ€å¯¹é½ï¼Œå°†è¡€æµåŠ¨åŠ›å­¦ç‰¹å¾æ˜ å°„è‡³LLMå¯ç†è§£è¯­ä¹‰ç©ºé—´ï¼›é‡‡ç”¨åŒåŸŸç¨³å®šç®—æ³•ï¼ˆDDSï¼‰è§£å†³ä¿¡å·ä¸ç¨³å®šé—®é¢˜ï¼Œè‡ªé€‚åº”è°ƒæ•´æ—¶é¢‘åŸŸç‰¹å¾æƒé‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç”Ÿç†ç»Ÿè®¡ã€ç¯å¢ƒä¸Šä¸‹æ–‡ç†è§£åŠä»»åŠ¡æè¿°ç­‰ç‰¹å®šæç¤ºæ¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPhysLLMè¾¾åˆ°äº†æœ€æ–°çš„ç²¾ç¡®åº¦ä¸ç¨³å¥æ€§æ ‡å‡†ï¼Œå±•ç°äº†åœ¨å…‰ç…§å˜åŒ–å’ŒåŠ¨æ€åœºæ™¯ä¸­çš„ä¼˜è¶Šè¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹å…‰å®¹ç§¯è„‰ææ³¢æˆåƒï¼ˆrPPGï¼‰é¢ä¸´å…‰ç…§å˜åŒ–ã€è¿åŠ¨å¹²æ‰°å’Œæœ‰é™æ—¶é—´å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ•æ‰é•¿æ—¶ä¾èµ–å…³ç³»çš„ä¼˜åŠ¿ï¼Œä½†å¯¹å¤„ç†è¿ç»­ä¸”å™ªå£°æ•æ„Ÿçš„rPPGä¿¡å·å­˜åœ¨å›°éš¾ã€‚</li>
<li>å¼•å…¥PhysLLMæ¡†æ¶ï¼Œç»“åˆLLMå’ŒrPPGæŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–å¤„ç†ç”Ÿç†ä¿¡å·ã€‚</li>
<li>é‡‡ç”¨æ–‡æœ¬åŸå‹å¼•å¯¼ç­–ç•¥ï¼ˆTPGï¼‰å»ºç«‹è·¨æ¨¡æ€å¯¹é½ï¼Œå°†ç”Ÿç†ä¿¡å·è½¬åŒ–ä¸ºè¯­è¨€æ¨¡å‹å¯ç†è§£çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>æå‡ºåŒåŸŸç¨³å®šç®—æ³•ï¼ˆDDSï¼‰è§£å†³ä¿¡å·ä¸ç¨³å®šé—®é¢˜ï¼Œå®ç°è‡ªé€‚åº”ç‰¹å¾æƒé‡è°ƒæ•´ã€‚</li>
<li>é€šè¿‡ç”Ÿç†ç»Ÿè®¡ã€ç¯å¢ƒä¸Šä¸‹æ–‡å’Œä»»åŠ¡æè¿°ç­‰ç‰¹å®šæç¤ºå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8a4d94e4cfddbbaec2f5971bf7a47c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fc04cd119fb88a1ebb572028e4acc37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f1f6d44769cba452072b24ffbd204e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5bcd05ea870d8e3c40eeb8e6261bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c4c3e9eb592d4c2e7dd2cb4ac005d83.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DyGEnc-Encoding-a-Sequence-of-Textual-Scene-Graphs-to-Reason-and-Answer-Questions-in-Dynamic-Scenes"><a href="#DyGEnc-Encoding-a-Sequence-of-Textual-Scene-Graphs-to-Reason-and-Answer-Questions-in-Dynamic-Scenes" class="headerlink" title="DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer   Questions in Dynamic Scenes"></a>DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer   Questions in Dynamic Scenes</h2><p><strong>Authors:Sergey Linok, Vadim Semenov, Anastasia Trunova, Oleg Bulichev, Dmitry Yudin</strong></p>
<p>The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com&#x2F;linukc&#x2F;DyGEnc. </p>
<blockquote>
<p>å¯¹åŠ¨æ€ç¯å¢ƒä¸­äº‹ä»¶çš„åˆ†æï¼Œåœ¨å¼€å‘èƒ½å¤Ÿä¸äººç±»äº’åŠ¨çš„æ™ºèƒ½ä»£ç†å’Œæœºå™¨äººæ–¹é¢ï¼Œæ„æˆäº†æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ã€‚ç›®å‰çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨è§†è§‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä»å›¾åƒä¸­éšå¼åœ°è·å–ä¿¡æ¯ï¼Œç¼ºä¹å¯è§£é‡Šçš„æ—¶ç©ºå¯¹è±¡è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DyGEncâ€”â€”ä¸€ç§æ–°å‹çš„åŠ¨æ€å›¾ç¼–ç æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å‹ç¼©çš„æ—¶ç©ºç»“æ„è§‚æµ‹è¡¨ç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ç›¸ç»“åˆã€‚è¿™ç§ç»“åˆçš„ç›®çš„æ˜¯æ ¹æ®ä¸€ç³»åˆ—æ–‡æœ¬åœºæ™¯å›¾è¿›è¡Œé«˜çº§é—®ç­”ã€‚åœ¨STARå’ŒAGQAæ•°æ®é›†ä¸Šçš„æ‰©å±•è¯„ä¼°è¡¨æ˜ï¼ŒDyGEncåœ¨è§£å†³æœ‰å…³äººä¸å¯¹è±¡äº¤äº’å†å²çš„é—®é¢˜æ—¶ï¼Œåœ¨æ€§èƒ½ä¸Šå¤§å¤§è¶…è¶Šäº†ç°æœ‰è§†è§‰æ–¹æ³•ï¼Œå·®è·è¾¾15-25%ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ— ç¼æ‰©å±•åˆ°å¤„ç†åŸå§‹è¾“å…¥å›¾åƒï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹æå–æ˜ç¡®çš„æ–‡æœ¬åœºæ™¯å›¾ï¼Œæ­£å¦‚åœ¨è½®å¼æ“ä½œå¹³å°æœºå™¨äººå®éªŒçš„ç»“æœæ‰€è¯å®çš„é‚£æ ·ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å‘ç°å°†æœ‰åŠ©äºå®ç°ç”¨äºé•¿æœŸæ¨ç†çš„ç¨³å¥ä¸”å‹ç¼©çš„å›¾åŸºæœºå™¨äººè®°å¿†ã€‚ä»£ç å¯åœ¨github.com&#x2F;linukc&#x2F;DyGEncæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03581v1">PDF</a> 8 pages, 5 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨åŠ¨æ€ç¯å¢ƒä¸­åˆ†æäº‹ä»¶å¯¹å‘å±•æ™ºèƒ½äº¤äº’æœºå™¨äººæå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰æ¨¡å‹ï¼Œä½†ç¼ºä¹å¯è§£é‡Šçš„ç©ºé—´æ—¶é—´ç‰©ä½“è¡¨ç¤ºã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºDyGEncæ–¹æ³•ï¼Œé›†æˆå‹ç¼©çš„ç©ºé—´æ—¶é—´ç»“æ„è§‚æµ‹è¡¨ç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ï¼Œä»¥æ”¯æŒåŸºäºæ–‡æœ¬åœºæ™¯å›¾çš„å…ˆè¿›é—®ç­”ã€‚åœ¨STARå’ŒAGQAæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDyGEncåœ¨è§£å†³æœ‰å…³äººä¸ç‰©ä½“äº¤äº’å†å²çš„é—®é¢˜æ—¶ï¼Œè¾ƒç°æœ‰è§†è§‰æ–¹æ³•çš„æ€§èƒ½æå‡å¹…åº¦è¾¾15-25%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯è½»æ¾æ‰©å±•åˆ°å¤„ç†åŸå§‹å›¾åƒï¼Œå¹¶åˆ©ç”¨åŸºç¡€æ¨¡å‹æå–æ˜ç¡®çš„æ–‡æœ¬åœºæ™¯å›¾ï¼Œå¦‚æœºå™¨äººå®éªŒæ‰€ç¤ºã€‚æœ¬æ–‡å¸Œæœ›è¿™äº›å‘ç°æœ‰åŠ©äºå®ç°ç”¨äºé•¿æœŸæ¨ç†çš„ç¨³å¥å’Œå‹ç¼©å›¾åŸºæœºå™¨äººè®°å¿†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€ç¯å¢ƒä¸­åˆ†æäº‹ä»¶å¯¹æ™ºèƒ½æœºå™¨äººä¸äººç±»äº¤äº’çš„å‘å±•æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰æ¨¡å‹ï¼Œä½†ç¼ºä¹å¯è§£é‡Šçš„ç©ºé—´æ—¶é—´ç‰©ä½“è¡¨ç¤ºã€‚</li>
<li>DyGEncæ–¹æ³•é›†æˆäº†å‹ç¼©çš„ç©ºé—´æ—¶é—´ç»“æ„è§‚æµ‹è¡¨ç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚</li>
<li>DyGEncåœ¨è§£å†³æœ‰å…³äººä¸ç‰©ä½“äº¤äº’å†å²çš„é—®é¢˜æ—¶è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œè¾ƒç°æœ‰è§†è§‰æ–¹æ³•çš„æ€§èƒ½æå‡å¹…åº¦è¾¾15-25%ã€‚</li>
<li>DyGEncå¯è½»æ¾æ‰©å±•åˆ°å¤„ç†åŸå§‹å›¾åƒï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡åŸºç¡€æ¨¡å‹æå–æ–‡æœ¬åœºæ™¯å›¾ã€‚</li>
<li>æœºå™¨äººå®éªŒè¯å®äº†DyGEncçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4530b5597b7c5e33abb342ca38e70132.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ece9bafcfda99f067d67f68b72a0e52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dd396978241690f41e7c67dfa3192dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629913bcab73f75dfcf4133406cddd23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea2718b7e47358de4c056eaefa3037ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-430caa78c16fd570c9373362a1c2020d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad1765da23aaedf3d3969b5efe429a9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b0ef76f6b8043b099b679e29e30a69.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LlamaFirewall-An-open-source-guardrail-system-for-building-secure-AI-agents"><a href="#LlamaFirewall-An-open-source-guardrail-system-for-building-secure-AI-agents" class="headerlink" title="LlamaFirewall: An open source guardrail system for building secure AI   agents"></a>LlamaFirewall: An open source guardrail system for building secure AI   agents</h2><p><strong>Authors:Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Beto de Paola, Dominik Gabi, James Crnkovich, Jean-Christophe Testud, Kat He, Rashnil Chaturvedi, Wu Zhou, Joshua Saxe</strong></p>
<p>Large language models (LLMs) have evolved from simple chatbots into autonomous agents capable of performing complex tasks such as editing production code, orchestrating workflows, and taking higher-stakes actions based on untrusted inputs like webpages and emails. These capabilities introduce new security risks that existing security measures, such as model fine-tuning or chatbot-focused guardrails, do not fully address. Given the higher stakes and the absence of deterministic solutions to mitigate these risks, there is a critical need for a real-time guardrail monitor to serve as a final layer of defense, and support system level, use case specific safety policy definition and enforcement. We introduce LlamaFirewall, an open-source security focused guardrail framework designed to serve as a final layer of defense against security risks associated with AI Agents. Our framework mitigates risks such as prompt injection, agent misalignment, and insecure code risks through three powerful guardrails: PromptGuard 2, a universal jailbreak detector that demonstrates clear state of the art performance; Agent Alignment Checks, a chain-of-thought auditor that inspects agent reasoning for prompt injection and goal misalignment, which, while still experimental, shows stronger efficacy at preventing indirect injections in general scenarios than previously proposed approaches; and CodeShield, an online static analysis engine that is both fast and extensible, aimed at preventing the generation of insecure or dangerous code by coding agents. Additionally, we include easy-to-use customizable scanners that make it possible for any developer who can write a regular expression or an LLM prompt to quickly update an agentâ€™s security guardrails. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»ä»ç®€å•çš„èŠå¤©æœºå™¨äººè¿›åŒ–æˆèƒ½å¤Ÿæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ï¼Œå¦‚ç¼–è¾‘ç”Ÿäº§ä»£ç ã€åè°ƒå·¥ä½œæµç¨‹å’ŒåŸºäºä¸å¯ä¿¡çš„è¾“å…¥ï¼ˆå¦‚ç½‘é¡µå’Œç”µå­é‚®ä»¶ï¼‰é‡‡å–æ›´é«˜é£é™©çš„è¡ŒåŠ¨ã€‚è¿™äº›åŠŸèƒ½å¼•å…¥äº†æ–°çš„å®‰å…¨é£é™©ï¼Œç°æœ‰çš„å®‰å…¨æªæ–½ï¼Œå¦‚æ¨¡å‹å¾®è°ƒæˆ–é’ˆå¯¹èŠå¤©æœºå™¨äººçš„é˜²æŠ¤æ ï¼Œå¹¶ä¸èƒ½å®Œå…¨è§£å†³è¿™äº›é—®é¢˜ã€‚è€ƒè™‘åˆ°é£é™©è¾ƒé«˜ä¸”ç¼ºä¹ç¡®å®šæ€§çš„è§£å†³æ–¹æ¡ˆæ¥å‡è½»è¿™äº›é£é™©ï¼Œå®æ—¶é˜²æŠ¤æ ç›‘æ§å™¨ä½œä¸ºæœ€åä¸€é“é˜²çº¿è‡³å…³é‡è¦ï¼Œå¹¶ä¸”éœ€è¦æ”¯æŒç³»ç»Ÿçº§åˆ«ã€é’ˆå¯¹ç”¨ä¾‹çš„å®‰å…¨æ”¿ç­–å®šä¹‰å’Œæ‰§è¡Œã€‚æˆ‘ä»¬æ¨å‡ºäº†LlamaFirewallï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å®‰å…¨é˜²æŠ¤æ æ¡†æ¶ï¼Œæ—¨åœ¨ä½œä¸ºå¯¹æŠ—ä¸äººå·¥æ™ºèƒ½ä»£ç†ç›¸å…³çš„å®‰å…¨é£é™©çš„æœ€åé˜²çº¿ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¸‰ä¸ªå¼ºå¤§çš„é˜²æŠ¤æ æ¥å‡è½»é£é™©ï¼šPromptGuard 2ï¼Œä¸€ç§é€šç”¨è¶Šç‹±æ£€æµ‹å™¨ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼›Agent Alignment Checksï¼Œä¸€ç§æ€ç»´é“¾å®¡è®¡å‘˜ï¼Œæ£€æŸ¥ä»£ç†æ¨ç†æ˜¯å¦å­˜åœ¨æç¤ºæ³¨å…¥å’Œç›®æ ‡é”™ä½ï¼Œå°½ç®¡å®ƒä»ç„¶å¤„äºå®éªŒé˜¶æ®µï¼Œä½†åœ¨ä¸€èˆ¬åœºæ™¯ä¸­é˜²æ­¢é—´æ¥æ³¨å…¥æ–¹é¢æ˜¾ç¤ºå‡ºæ¯”ä»¥å‰æå‡ºçš„æ–¹æ³•æ›´å¼ºçš„æœ‰æ•ˆæ€§ï¼›ä»¥åŠCodeShieldï¼Œä¸€ä¸ªå¿«é€Ÿä¸”å¯æ‰©å±•çš„åœ¨çº¿é™æ€åˆ†æå¼•æ“ï¼Œæ—¨åœ¨é˜²æ­¢ç¼–ç ä»£ç†ç”Ÿæˆä¸å®‰å…¨æˆ–å±é™©çš„ä»£ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åŒ…æ‹¬æ˜“äºä½¿ç”¨ä¸”å¯å®šåˆ¶åŒ–çš„æ‰«æå™¨ï¼Œä½¿ä»»ä½•èƒ½å¤Ÿç¼–å†™æ­£åˆ™è¡¨è¾¾å¼æˆ–LLMæç¤ºçš„å¼€å‘äººå‘˜éƒ½å¯ä»¥å¿«é€Ÿæ›´æ–°ä»£ç†çš„å®‰å…¨é˜²æŠ¤æ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03574v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ä»ç®€å•çš„èŠå¤©æœºå™¨äººè¿›åŒ–ä¸ºèƒ½å¤Ÿæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ï¼Œå¦‚ç¼–è¾‘ç”Ÿäº§ä»£ç ã€åè°ƒå·¥ä½œæµç¨‹å’Œæ ¹æ®ä¸å¯ä¿¡çš„è¾“å…¥ï¼ˆå¦‚ç½‘é¡µå’Œç”µå­é‚®ä»¶ï¼‰é‡‡å–é«˜é£é™©è¡ŒåŠ¨ã€‚è¿™äº›åŠŸèƒ½å¼•å…¥äº†æ–°çš„å®‰å…¨é£é™©ï¼Œç°æœ‰çš„å®‰å…¨æªæ–½ï¼ˆå¦‚æ¨¡å‹å¾®è°ƒæˆ–é’ˆå¯¹èŠå¤©æœºå™¨äººçš„é˜²æŠ¤æ ï¼‰å¹¶æœªå®Œå…¨è§£å†³è¿™äº›é—®é¢˜ã€‚é‰´äºé£é™©è¾ƒé«˜ä¸”ç¼ºä¹ç¡®å®šçš„è§£å†³æ–¹æ¡ˆæ¥å‡è½»è¿™äº›é£é™©ï¼Œæ€¥éœ€ä¸€ç§å®æ—¶é˜²æŠ¤æ ç›‘æ§å™¨ä½œä¸ºæœ€åä¸€å±‚é˜²å¾¡ï¼Œå¹¶æ”¯æŒç³»ç»Ÿçº§åˆ«ã€ç”¨ä¾‹ç‰¹å®šçš„å®‰å…¨ç­–ç•¥å®šä¹‰å’Œæ‰§è¡Œã€‚æˆ‘ä»¬å¼•å…¥äº†LlamaFirewallï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºå®‰å…¨çš„é˜²æŠ¤æ æ¡†æ¶ï¼Œæ—¨åœ¨ä½œä¸ºå¯¹æŠ—ä¸AIä»£ç†ç›¸å…³çš„å®‰å…¨é£é™©çš„æœ€åé˜²çº¿ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¸‰ä¸ªå¼ºå¤§çš„é˜²æŠ¤æ æ¥å‡è½»é£é™©ï¼ŒåŒ…æ‹¬PromptGuard 2é€šç”¨è¶Šç‹±æ£€æµ‹å™¨ã€Agent Alignment Checksæ€ç»´é“¾å®¡è®¡å™¨å’ŒCodeShieldåœ¨çº¿é™æ€åˆ†æå¼•æ“ï¼Œåˆ†åˆ«ç”¨äºé˜²æ­¢æç¤ºæ³¨å…¥ã€ä»£ç†å¯¹é½é—®é¢˜å’Œä»£ç†ç”Ÿæˆçš„ä¸å®‰å…¨æˆ–å±é™©ä»£ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åŒ…å«äº†æ˜“äºä½¿ç”¨çš„å¯å®šåˆ¶æ‰«æå™¨ï¼Œä½¿å¾—ä»»ä½•èƒ½å¤Ÿç¼–å†™æ­£åˆ™è¡¨è¾¾å¼æˆ–LLMæç¤ºçš„å¼€å‘è€…éƒ½èƒ½å¿«é€Ÿæ›´æ–°ä»£ç†çš„å®‰å…¨é˜²æŠ¤æ ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMså·²ä»ç®€å•èŠå¤©æœºå™¨äººè¿›åŒ–ä¸ºèƒ½æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„è‡ªä¸»ä»£ç†ã€‚</li>
<li>LLMsçš„æ–°åŠŸèƒ½å¼•å…¥äº†æœªè¢«ç°æœ‰å®‰å…¨æªæ–½å……åˆ†è§£å†³çš„å®‰å…¨é£é™©ã€‚</li>
<li>æ€¥éœ€ä¸€ç§å®æ—¶é˜²æŠ¤æ ç›‘æ§å™¨ä½œä¸ºå¯¹æŠ—ä¸AIä»£ç†ç›¸å…³çš„å®‰å…¨é£é™©çš„æœ€åé˜²çº¿ã€‚</li>
<li>LlamaFirewallæ˜¯ä¸€ä¸ªä¸“æ³¨äºå®‰å…¨çš„é˜²æŠ¤æ æ¡†æ¶ï¼Œæ—¨åœ¨æœåŠ¡è¿™ä¸€éœ€æ±‚ã€‚</li>
<li>LlamaFirewallåŒ…å«ä¸‰ä¸ªå¼ºå¤§çš„é˜²æŠ¤æ ï¼šPromptGuard 2ã€Agent Alignment Checkså’ŒCodeShieldã€‚</li>
<li>PromptGuard 2å¯é˜²æ­¢æç¤ºæ³¨å…¥ç­‰å®‰å…¨é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1dc39404c4e8a720c267f5458f44985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-156c7718dce7387712e4f14c983cbbf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a195dd7c7d2f6ddc6d07c7e9c1ce9550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d67a66ce9ca5da3ff6a46d14bb38015c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Survey-of-Large-AI-Models-for-Future-Communications-Foundations-Applications-and-Challenges"><a href="#A-Comprehensive-Survey-of-Large-AI-Models-for-Future-Communications-Foundations-Applications-and-Challenges" class="headerlink" title="A Comprehensive Survey of Large AI Models for Future Communications:   Foundations, Applications and Challenges"></a>A Comprehensive Survey of Large AI Models for Future Communications:   Foundations, Applications and Challenges</h2><p><strong>Authors:Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Merouane Debbah, Dusit Niyato, Zhu Han</strong></p>
<p>The 6G wireless communications aim to establish an intelligent world of ubiquitous connectivity, providing an unprecedented communication experience. Large artificial intelligence models (LAMs) are characterized by significantly larger scales (e.g., billions or trillions of parameters) compared to typical artificial intelligence (AI) models. LAMs exhibit outstanding cognitive abilities, including strong generalization capabilities for fine-tuning to downstream tasks, and emergent capabilities to handle tasks unseen during training. Therefore, LAMs efficiently provide AI services for diverse communication applications, making them crucial tools for addressing complex challenges in future wireless communication systems. This study provides a comprehensive review of the foundations, applications, and challenges of LAMs in communication. First, we introduce the current state of AI-based communication systems, emphasizing the motivation behind integrating LAMs into communications and summarizing the key contributions. We then present an overview of the essential concepts of LAMs in communication. This includes an introduction to the main architectures of LAMs, such as transformer, diffusion models, and mamba. We also explore the classification of LAMs, including large language models (LLMs), large vision models (LVMs), large multimodal models (LMMs), and world models, and examine their potential applications in communication. Additionally, we cover the training methods and evaluation techniques for LAMs in communication systems. Lastly, we introduce optimization strategies such as chain of thought (CoT), retrieval augmented generation (RAG), and agentic systems. Following this, we discuss the research advancements of LAMs across various communication scenarios. Finally, we analyze the challenges in the current research and provide insights into potential future research directions. </p>
<blockquote>
<p>6Gæ— çº¿é€šä¿¡æ—¨åœ¨å»ºç«‹ä¸€ä¸ªæ— å¤„ä¸åœ¨çš„æ™ºèƒ½äº’è”ä¸–ç•Œï¼Œæä¾›å‰æ‰€æœªæœ‰çš„é€šä¿¡ä½“éªŒã€‚å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆLAMsï¼‰ä¸å…¸å‹çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰æ˜¾è‘—æ›´å¤§çš„è§„æ¨¡ï¼ˆä¾‹å¦‚ï¼Œæ•°åäº¿æˆ–æ•°ä¸‡äº¿å‚æ•°ï¼‰ã€‚LAMsè¡¨ç°å‡ºå“è¶Šçš„è®¤çŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒï¼Œä»¥åŠå¤„ç†è®­ç»ƒæœŸé—´æœªè§ä»»åŠ¡çš„çªå‘èƒ½åŠ›ã€‚å› æ­¤ï¼ŒLAMsä¸ºå„ç§é€šä¿¡åº”ç”¨ç¨‹åºæœ‰æ•ˆåœ°æä¾›AIæœåŠ¡ï¼Œæˆä¸ºè§£å†³æœªæ¥æ— çº¿é€šä¿¡ç³»ç»Ÿä¸­å¤æ‚æŒ‘æˆ˜çš„å…³é”®å·¥å…·ã€‚æœ¬ç ”ç©¶å…¨é¢å›é¡¾äº†é€šä¿¡ä¸­LAMsçš„åŸºç¡€ã€åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºäººå·¥æ™ºèƒ½çš„é€šä¿¡ç³»ç»Ÿçš„å½“å‰çŠ¶æ€ï¼Œå¼ºè°ƒäº†å°†LAMsé›†æˆåˆ°é€šä¿¡ä¸­çš„åŠ¨æœºï¼Œå¹¶æ€»ç»“äº†å…³é”®è´¡çŒ®ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†é€šä¿¡ä¸­LAMsçš„åŸºæœ¬æ¦‚å¿µã€‚è¿™åŒ…æ‹¬ä»‹ç»LAMsçš„ä¸»è¦æ¶æ„ï¼Œå¦‚å˜å‹å™¨ã€æ‰©æ•£æ¨¡å‹å’Œç›å§†å·´ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†LAMsçš„åˆ†ç±»ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰ã€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å’Œä¸–ç•Œæ¨¡å‹ï¼Œå¹¶ç ”ç©¶å®ƒä»¬åœ¨é€šä¿¡ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†é€šä¿¡ç³»ç»Ÿä¸­LAMsçš„è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ã€‚ä¹‹åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¼˜åŒ–ç­–ç•¥ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä»£ç†ç³»ç»Ÿã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†LAMsåœ¨å„ç§é€šä¿¡åœºæ™¯ä¸­çš„ç ”ç©¶è¿›å±•ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†å½“å‰ç ”ç©¶çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†å¯¹æœªæ¥ç ”ç©¶æ–¹å‘çš„æ´å¯Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03556v1">PDF</a> </p>
<p><strong>Summary</strong><br>     6Gæ— çº¿é€šä¿¡è¿½æ±‚å»ºç«‹æ™®éäº’è”çš„æ™ºèƒ½ä¸–ç•Œï¼Œæä¾›å‰æ‰€æœªæœ‰çš„é€šä¿¡ä½“éªŒã€‚å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆLAMsï¼‰å…·æœ‰çªå‡ºçš„è®¤çŸ¥èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå¤„ç†æœªè§ä»»åŠ¡çš„æ¶Œç°èƒ½åŠ›ï¼Œä¸ºå„ç§é€šä¿¡åº”ç”¨æä¾›é«˜æ•ˆçš„äººå·¥æ™ºèƒ½æœåŠ¡ï¼Œå¯¹è§£å†³æœªæ¥æ— çº¿é€šä¿¡ç³»ç»Ÿçš„å¤æ‚æŒ‘æˆ˜è‡³å…³é‡è¦ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†LAMsåœ¨é€šä¿¡é¢†åŸŸçš„åŸºç¡€ã€åº”ç”¨å’ŒæŒ‘æˆ˜ã€‚ä»‹ç»äº†äººå·¥æ™ºèƒ½é€šä¿¡ç³»ç»Ÿçš„å½“å‰çŠ¶æ€ï¼Œæ¦‚è¿°äº†å°†LAMsé›†æˆåˆ°é€šä¿¡ä¸­çš„åŠ¨æœºå’Œå…³é”®è´¡çŒ®ã€‚æ¢è®¨äº†LAMsçš„ä¸»è¦æ¶æ„ã€åˆ†ç±»ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ï¼Œå¹¶è®¨è®ºäº†å…¶åœ¨å„ç§é€šä¿¡åœºæ™¯ä¸­çš„ç ”ç©¶è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>6Gæ— çº¿é€šä¿¡è¿½æ±‚å»ºç«‹æ™®éäº’è”çš„æ™ºèƒ½ä¸–ç•Œï¼Œæä¾›å‰æ‰€æœªæœ‰çš„é€šä¿¡ä½“éªŒã€‚</li>
<li>å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆLAMsï¼‰å…·æœ‰æ˜¾è‘—æ›´å¤§çš„è§„æ¨¡ï¼Œå±•ç°å‡ºå¼ºå¤§çš„è®¤çŸ¥èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>LAMsåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰ã€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å’Œä¸–ç•Œæ¨¡å‹ç­‰åˆ†ç±»ã€‚</li>
<li>LAMsåœ¨é€šä¿¡ç³»ç»Ÿä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¦‚æ™ºèƒ½é€šä¿¡ã€æ•°æ®å¤„ç†å’Œè‡ªåŠ¨åŒ–æ§åˆ¶ç­‰ã€‚</li>
<li>å½“å‰çš„ç ”ç©¶è¿›å±•æ¶µç›–äº†LAMsçš„æ¶æ„ã€åˆ†ç±»ã€è®­ç»ƒæ–¹æ³•å’Œè¯„ä¼°æŠ€æœ¯ç­‰æ–¹é¢ã€‚</li>
<li>æŒ‘æˆ˜åœ¨äºè§£å†³LAMsçš„å¤æ‚æ€§ã€è®¡ç®—èµ„æºå’Œéšç§ä¿æŠ¤ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03556">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-988e81f0cc36ea172bd4d25eedd70f12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ad643684ad0491167f67e3ea3112b1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8f83c20b7a4814f85d72d9081654cb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94d1238b27946cb452f34d6a891e0d47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d19d492314d4a00518b235f8e46c2b54.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="am-ELO-A-Stable-Framework-for-Arena-based-LLM-Evaluation"><a href="#am-ELO-A-Stable-Framework-for-Arena-based-LLM-Evaluation" class="headerlink" title="am-ELO: A Stable Framework for Arena-based LLM Evaluation"></a>am-ELO: A Stable Framework for Arena-based LLM Evaluation</h2><p><strong>Authors:Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, Shijin Wang</strong></p>
<p>Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Ratingâ€™s probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs. </p>
<blockquote>
<p>åŸºäºæˆ˜åœºçš„è¯„ä¼°æ˜¯ç°ä»£äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬ä¸”é‡è¦çš„è¯„ä¼°èŒƒå¼ã€‚åŸºäºELOè¯„çº§ç³»ç»Ÿçš„ç°æœ‰æ¡†æ¶ç”±äºæ’åä¸ä¸€è‡´å’Œå¿½è§†è¯„ä¼°è€…èƒ½åŠ›å·®å¼‚è€Œé¢ä¸´ä¸å¯é¿å…çš„ç¨³å®šæ€§é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¢å¼ºELOè¯„çº§ç³»ç»Ÿï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„ç¨³å®šæˆ˜åœºæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰æ–¹æ³•æ›¿æ¢è¿­ä»£æ›´æ–°æ–¹æ³•ï¼Œç§°ä¸ºm-ELOï¼Œå¹¶ä¸ºæ¨¡å‹æ’åçš„MLEæ–¹æ³•çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§æä¾›äº†ç†è®ºè¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†èå…¥è¯„ä¼°è€…èƒ½åŠ›çš„am-ELOï¼Œèƒ½å¤ŸåŒæ—¶ä¼°è®¡æ¨¡å‹å¾—åˆ†å’Œè¯„ä¼°è€…å¯é æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç¡®ä¿äº†ç¨³å®šæ€§ï¼Œè¯æ˜è¯¥æ¡†æ¶ä¸ºLLMæä¾›äº†æ›´ç¨³å¥ã€å‡†ç¡®å’Œç¨³å®šçš„è¯„ä¼°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03475v1">PDF</a> ICML2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç«æŠ€åœºçš„è¯„ä¼°æ˜¯ç°ä»£äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå°¤å…¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬è€Œé‡è¦çš„è¯„ä¼°èŒƒå¼ã€‚ç°æœ‰åŸºäºELOè¯„åˆ†ç³»ç»Ÿçš„æ¡†æ¶å­˜åœ¨æ’åä¸ä¸€è‡´å’Œå¿½è§†è¯„ä¼°è€…èƒ½åŠ›å·®å¼‚ç­‰ä¸ç¨³å®šé—®é¢˜ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹ç¨³å®šçš„ç«æŠ€åœºæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œé€šè¿‡å¢å¼ºELOè¯„åˆ†ç³»ç»Ÿï¼Œé‡‡ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰æ–¹æ³•æ›¿ä»£è¿­ä»£æ›´æ–°æ–¹æ³•ï¼Œæå‡ºm-ELOï¼Œå¹¶ä»ç†è®ºä¸Šè¯æ˜äº†MLEæ–¹æ³•åœ¨æ¨¡å‹æ’åä¸­çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ç»“åˆè¯„ä¼°è€…èƒ½åŠ›çš„am-ELOï¼Œå¯ä»¥åŒæ—¶ä¼°è®¡æ¨¡å‹å¾—åˆ†å’Œè¯„ä¼°è€…å¯é æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•å…·æœ‰ç¨³å®šæ€§ï¼Œè¯æ˜è¯¥æ¡†æ¶ä¸ºLLMæä¾›äº†æ›´ç¨³å¥ã€å‡†ç¡®å’Œç¨³å®šçš„è¯„ä¼°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«æŠ€åœºè¯„ä¼°æ˜¯ç°ä»£AIæ¨¡å‹çš„é‡è¦è¯„ä¼°æ–¹å¼ï¼Œå°¤å…¶æ˜¯LLMã€‚</li>
<li>åŸºäºELOè¯„åˆ†ç³»ç»Ÿçš„ç°æœ‰æ¡†æ¶å­˜åœ¨ä¸ç¨³å®šé—®é¢˜ï¼Œå¦‚æ’åä¸ä¸€è‡´å’Œå¿½è§†è¯„ä¼°è€…èƒ½åŠ›å·®å¼‚ã€‚</li>
<li>æ–°å‹ç¨³å®šç«æŠ€åœºæ¡†æ¶é€šè¿‡å¢å¼ºELOè¯„åˆ†ç³»ç»Ÿæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰æ–¹æ³•æ›¿ä»£è¿­ä»£æ›´æ–°æ–¹æ³•ï¼Œæå‡ºm-ELOã€‚</li>
<li>m-ELOå…·æœ‰ä¸€è‡´æ€§å’Œç¨³å®šæ€§ï¼Œé€‚ç”¨äºæ¨¡å‹æ’åã€‚</li>
<li>am-ELOç»“åˆè¯„ä¼°è€…èƒ½åŠ›ï¼Œèƒ½åŒæ—¶ä¼°è®¡æ¨¡å‹å¾—åˆ†å’Œè¯„ä¼°è€…å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c71c6a1eb935542c9f2061a36ba70f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1d8837563a0bd72a4d69406d9866719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a8c3bdb4d4c23aa5c1d3964c4611fa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f63d7c79ac5e50d3a0c31c1873187d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5c2a3b8103d7f0b1f39de2a4de6911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bae94a2b1253494d95aa6192bda73a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82d4830ef07e7c29ba3c65f3408ba2c3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-LLMs-on-Long-tail-Entity-Linking-in-Historical-Documents"><a href="#Evaluation-of-LLMs-on-Long-tail-Entity-Linking-in-Historical-Documents" class="headerlink" title="Evaluation of LLMs on Long-tail Entity Linking in Historical Documents"></a>Evaluation of LLMs on Long-tail Entity Linking in Historical Documents</h2><p><strong>Authors:Marta Boscariol, Luana Bulla, Lia Draetta, Beatrice FiumanÃ², Emanuele Lenzi, Leonardo Piano</strong></p>
<p>Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP) applications, enabling the disambiguation of entity mentions by linking them to their corresponding entries in a reference knowledge base (KB). Thanks to their deep contextual understanding capabilities, LLMs offer a new perspective to tackle EL, promising better results than traditional methods. Despite the impressive generalization capabilities of LLMs, linking less popular, long-tail entities remains challenging as these entities are often underrepresented in training data and knowledge bases. Furthermore, the long-tail EL task is an understudied problem, and limited studies address it with LLMs. In the present work, we assess the performance of two popular LLMs, GPT and LLama3, in a long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated benchmark of sentences from domain-specific historical texts, we quantitatively compare the performance of LLMs in identifying and linking entities to their corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity Linking and Relation Extraction framework. Our preliminary experiments reveal that LLMs perform encouragingly well in long-tail EL, indicating that this technology can be a valuable adjunct in filling the gap between head and long-tail EL. </p>
<blockquote>
<p>å®ä½“é“¾æ¥ï¼ˆELï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒé€šè¿‡å°†åœ¨æ–‡æœ¬ä¸­æåŠçš„å®ä½“é“¾æ¥åˆ°å‚è€ƒçŸ¥è¯†åº“ï¼ˆKBï¼‰ä¸­çš„ç›¸åº”æ¡ç›®æ¥å®ç°å®ä½“æ¶ˆæ­§ã€‚ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡æ·±åšçš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œå®ƒä»¬ä¸ºå¤„ç†ELæä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶æœ‰æœ›å¸¦æ¥æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„ç»“æœã€‚å°½ç®¡LLMçš„æ³›åŒ–èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†é“¾æ¥ä¸å¤ªæµè¡Œã€é•¿å°¾å®ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›å®ä½“åœ¨è®­ç»ƒæ•°æ®å’ŒçŸ¥è¯†åº“ä¸­å¾€å¾€è¡¨ç¤ºä¸è¶³ã€‚æ­¤å¤–ï¼Œé•¿å°¾ELä»»åŠ¡æ˜¯ä¸€ä¸ªå°šæœªè¢«æ·±å…¥ç ”ç©¶çš„é—®é¢˜ï¼Œåªæœ‰æœ‰é™çš„ç ”ç©¶ä½¿ç”¨LLMæ¥è§£å†³ã€‚åœ¨ç›®å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹GPTå’ŒLLama3åœ¨é•¿å°¾å®ä½“é“¾æ¥åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨MHERCL v0.1ä½œä¸ºåŸºå‡†æµ‹è¯•é›†ï¼Œè¯¥æµ‹è¯•é›†åŒ…å«æ¥è‡ªç‰¹å®šé¢†åŸŸå†å²æ–‡æœ¬çš„å¥å­è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨ã€‚æˆ‘ä»¬å®šé‡æ¯”è¾ƒäº†LLMåœ¨è¯†åˆ«å®ä½“å¹¶å°†å…¶é“¾æ¥åˆ°ç›¸åº”Wikidataæ¡ç›®çš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„å…³ç³»æŠ½å–æ¡†æ¶ReLiKçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿å°¾ELæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œè¿™è¡¨æ˜è¿™é¡¹æŠ€æœ¯å¯ä»¥å¾ˆå¥½åœ°å¡«è¡¥å¤´éƒ¨å’Œé•¿å°¾ELä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03473v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­ï¼Œå®ä½“é“¾æ¥ï¼ˆELï¼‰èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é€šè¿‡å°†æåŠçš„å®ä½“é“¾æ¥åˆ°å‚è€ƒçŸ¥è¯†åº“ä¸­çš„ç›¸åº”æ¡ç›®æ¥å®ç°æ­§ä¹‰æ¶ˆè§£ã€‚LLMå‡­å€Ÿå…¶å¯¹ä¸Šä¸‹æ–‡çš„æ·±å…¥ç†è§£å±•ç°äº†å…¶åœ¨è§£å†³ELé—®é¢˜ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹é•¿å°¾å®ä½“çš„é“¾æ¥ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨è®­ç»ƒæ•°æ®å’ŒçŸ¥è¯†åº“ä¸­å¸¸å¸¸è¢«ä½ä¼°ã€‚æœ¬æ–‡è¯„ä¼°äº†GPTå’ŒLLama3ä¸¤ç§æµè¡ŒLLMåœ¨å¤„ç†é•¿å°¾å®ä½“é“¾æ¥åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚ä½¿ç”¨æ¥è‡ªç‰¹å®šé¢†åŸŸå†å²æ–‡æœ¬çš„MHERCL v0.1æ‰‹åŠ¨æ³¨é‡ŠåŸºå‡†æ•°æ®é›†è¿›è¡Œå®šé‡æ¯”è¾ƒï¼Œå‘ç°LLMåœ¨è¯†åˆ«å¹¶å°†å®ä½“é“¾æ¥åˆ°å…¶ç›¸åº”çš„Wikidataæ¡ç›®æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œè¡¨ç°å‡ºè¿™ä¸€æŠ€æœ¯åœ¨ç¼©å°å¤´éƒ¨å’Œé•¿å°¾ELå·®è·ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä½“é“¾æ¥ä»»åŠ¡ä¸Šå±•ç°å‡ºè¶…è¶Šä¼ ç»Ÿæ–¹æ³•çš„æ½œåŠ›ã€‚</li>
<li>LLMèƒ½å¤Ÿç†è§£è¯­å¢ƒæ·±å¤„çš„å«ä¹‰ï¼Œä»è€Œæé«˜å®ä½“é“¾æ¥çš„å‡†ç¡®æ€§ã€‚</li>
<li>é•¿å°¾å®ä½“é“¾æ¥ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™äº›å®ä½“åœ¨è®­ç»ƒæ•°æ®å’ŒçŸ¥è¯†åº“ä¸­å¸¸å¸¸è¢«ä½ä¼°æˆ–ç¼ºå¤±ã€‚</li>
<li>ä½¿ç”¨MHERCL v0.1æ•°æ®é›†è¯„ä¼°äº†GPTå’ŒLLama3åœ¨é•¿å°¾å®ä½“é“¾æ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>LLMåœ¨å¤„ç†é•¿å°¾å®ä½“é“¾æ¥æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œæœ‰æ½œåŠ›ç¼©å°å¤´éƒ¨å®ä½“ä¸é•¿å°¾å®ä½“ä¹‹é—´çš„é“¾æ¥å·®è·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-623343854b70e5c18e70994a26aeaf38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-888ffa6a651e38678700f3e2d7e81658.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8fa388e81cef13d0b60b1894d880d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1952c1f7a72ba076f70bf164450572a6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey"><a href="#Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey" class="headerlink" title="Image Recognition with Online Lightweight Vision Transformer: A Survey"></a>Image Recognition with Online Lightweight Vision Transformer: A Survey</h2><p><strong>Authors:Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu</strong></p>
<p>The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a> </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œè¿™ä¿ƒä½¿å…¶é€‚åº”è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚ä¸å·ç§¯ç¥ç»ç½‘ç»œä¸åŒï¼Œè§†è§‰Transformerå¤©ç”Ÿå°±èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶å®ç°å¹¶è¡Œå¤„ç†ï¼Œä½†ç¼ºä¹å½’çº³åè§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œé¢ä¸´ç€è®¡ç®—å’Œå†…å­˜æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡è°ƒæŸ¥äº†åœ¨çº¿ç”Ÿæˆè½»é‡çº§è§†è§‰Transformerè¿›è¡Œå›¾åƒè¯†åˆ«çš„å„ç§ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®é¢†åŸŸï¼šé«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šå¯¹æ¯ä¸ªä¸»é¢˜è¿›è¡Œäº†ç›¸å…³æ¢ç´¢ï¼Œåˆ†æäº†ç²¾åº¦ã€å‚æ•°ã€ååé‡ç­‰ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥çªå‡ºå„è‡ªçš„ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œçµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§è§†è§‰Transformerçš„æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¿€å‘è¿›ä¸€æ­¥æ¢ç´¢ï¼Œä¸ºç¤¾åŒºæä¾›å®ç”¨æŒ‡å¯¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03113v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†å…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« ä¸»è¦ä»‹ç»äº†é’ˆå¯¹å›¾åƒè¯†åˆ«çš„è½»é‡åŒ–æ„¿æ™¯è½¬æ¢å™¨ç”Ÿæˆçš„å„ç§åœ¨çº¿ç­–ç•¥ï¼Œé‡ç‚¹ä»‹ç»äº†é«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®é¢†åŸŸã€‚åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†ç›¸å…³æ¢ç´¢çš„ä¼˜ç¼ºç‚¹å’Œçµæ´»æ€§ã€‚æ–‡ç« æ—¨åœ¨ä¸ºç¤¾åŒºæä¾›è¿›ä¸€æ­¥çš„æ¢ç´¢çµæ„Ÿå’Œå®è·µæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå…·æœ‰æ•è·é•¿è·ç¦»ä¾èµ–æ€§å’Œå¹¶è¡Œå¤„ç†çš„èƒ½åŠ›ã€‚</li>
<li>ä¸å·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œæ„¿æ™¯è½¬æ¢å™¨ç¼ºä¹å½’çº³åè§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œé¢ä¸´è®¡ç®—å’Œå†…å­˜æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†é’ˆå¯¹å›¾åƒè¯†åˆ«çš„è½»é‡åŒ–æ„¿æ™¯è½¬æ¢å™¨çš„åœ¨çº¿ç­–ç•¥ï¼ŒåŒ…æ‹¬é«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®é¢†åŸŸã€‚</li>
<li>æ–‡ç« è¯„ä¼°äº†è¿™äº›ç­–ç•¥åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç²¾åº¦ã€å‚æ•°ã€ååé‡å’Œæƒè¡¡æ–¹é¢çš„ä¼˜ç¼ºç‚¹å’Œçµæ´»æ€§ã€‚</li>
<li>é€šè¿‡è¯„ä¼°å„ç§ç­–ç•¥ï¼Œæ–‡ç« æ—¨åœ¨æä¾›å¯¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è½»é‡åŒ–æ„¿æ™¯è½¬æ¢å™¨å®ç”¨æ€§çš„æ·±å…¥ç†è§£ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨ä¸ºç¤¾åŒºæä¾›è¿›ä¸€æ­¥çš„æ¢ç´¢çµæ„Ÿå’Œå®è·µæŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5071a7b1d1751f65ba993b66908a604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ca69cc9de51d3773f2d7bb86baa851.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f25925fd90604d7668099a19f0559e08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef94e7aeac77304479c3539d58d6a41a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b28395b9db504dbc2e491947606c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c30addd4b8633cdb8cd2b81038bc9114.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Art-of-Repair-Optimizing-Iterative-Program-Repair-with-Instruction-Tuned-Models"><a href="#The-Art-of-Repair-Optimizing-Iterative-Program-Repair-with-Instruction-Tuned-Models" class="headerlink" title="The Art of Repair: Optimizing Iterative Program Repair with   Instruction-Tuned Models"></a>The Art of Repair: Optimizing Iterative Program Repair with   Instruction-Tuned Models</h2><p><strong>Authors:Fernando Vallecillos Ruiz, Max Hort, Leon Moonen</strong></p>
<p>Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.   We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.   Our results show that by using only a fraction (&lt;1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement. </p>
<blockquote>
<p>è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨å‡å°‘è¯†åˆ«å’Œä¿®å¤æºä»£ç ä¸­çš„é”™è¯¯æ‰€éœ€çš„äººå·¥æ“ä½œã€‚åœ¨åŸºäºLLMçš„ä»£ç†å‡ºç°ä¹‹å‰ï¼Œä¸€ç§å¸¸è§ç­–ç•¥æ˜¯é€šè¿‡ç”Ÿæˆå¤§é‡çš„è¡¥ä¸ï¼ˆæœ‰æ—¶é«˜è¾¾æ•°åƒä¸ªï¼‰æ¥æé«˜åŸºå‡†æµ‹è¯•ä¸­çš„ä¿®å¤æ•ˆæœã€‚æœ€è¿‘ï¼Œè‡ªæˆ‘è¿­ä»£èƒ½åŠ›ä½¿LLMèƒ½å¤Ÿåœ¨åé¦ˆçš„æŒ‡å¯¼ä¸‹é€šè¿‡å¤šè½®è¿­ä»£æ¥å®Œå–„è¡¥ä¸ã€‚ç„¶è€Œï¼Œæ–‡çŒ®é€šå¸¸å…³æ³¨å¤šæ¬¡è¿­ä»£è€Œå¿½ç•¥äº†ä¸åŒæ•°é‡çš„è¾“å‡ºã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§å¹³è¡¡è¿™ä¸¤ç§æ–¹æ³•çš„APRç®¡é“ï¼Œå³ç”Ÿæˆå¤šä¸ªè¾“å‡ºå’Œå¤šæ¬¡è¿­ä»£ï¼ŒåŒæ—¶é™åˆ¶æ¯ä¸ªbugçš„æ€»è¡¥ä¸æ•°é‡ä¸º10ã€‚æˆ‘ä»¬å°†ä¸‰ä¸ªæœ€å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´LLMâ€”â€”DeepSeekCoder-Instructã€Codellama-Instructã€Llama3.1-Instructâ€”â€”åº”ç”¨äºAPRä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œäº†APRæ•°æ®é›†çš„ä¸‰ç§è§„æ¨¡ï¼ˆ1Kã€30Kã€65Kï¼‰ä»¥åŠä¸¤ç§æŠ€æœ¯ï¼ˆå®Œå…¨å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰å’ŒLoRAï¼‰çš„å¾®è°ƒï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸¤ä¸ªAPRåŸºå‡†æµ‹è¯•ï¼ˆHumanEval-Javaå’ŒDefects4Jï¼‰ä¸Šè¯„ä¼°å®ƒä»¬çš„ä¿®å¤èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨ä¸€å°éƒ¨åˆ†ï¼ˆ&lt;1%ï¼‰çš„å¾®è°ƒæ•°æ®é›†ï¼Œæˆ‘ä»¬å°±å¯ä»¥æé«˜ç”Ÿæˆåˆç†è¡¥ä¸çš„æ•°é‡ï¼Œè¾¾åˆ°é«˜è¾¾78%ï¼Œè¿™æŒ‘æˆ˜äº†å…ˆå‰æŠ¥å‘Šä½¿ç”¨Full Fine-Tuningæ”¶ç›Šæœ‰é™çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¶…è¿‡æŸä¸ªé˜ˆå€¼ä¼šå¯¼è‡´ç»“æœå‡å°‘ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¿‡åº¦æ‹Ÿåˆå¯¼è‡´çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜åŸºç¡€æ¨¡å‹åœ¨é‡‡ç”¨è¿­ä»£æ–¹å¼åˆ›å»ºè¡¥ä¸æ—¶å—ç›ŠåŒªæµ…ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è¡¥ä¸ã€‚è€Œä¸”ï¼Œåœ¨å¤æ‚çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿­ä»£ç­–ç•¥çš„ä¼˜åŠ¿å˜å¾—æ›´åŠ çªå‡ºã€‚è™½ç„¶ç»è¿‡è°ƒæ•´çš„æ¨¡å‹ä»è¿­ä»£ä¸­è·ç›Šè¾ƒå°‘ï¼Œä½†ä»å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„åŸºå‡†æµ‹è¯•ä¸­ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†éœ€è¦å¹³è¡¡çš„APRç­–ç•¥ï¼Œå°†å¤šè¾“å‡ºç”Ÿæˆå’Œè¿­ä»£æ”¹è¿›ç»“åˆèµ·æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02931v1">PDF</a> Accepted for publication in the research track of the 29th   International Conference on Evaluation and Assessment in Software Engineering   (EASE), 17-20 June 2025, Istanbul, T&quot;urkiye</p>
<p><strong>æ‘˜è¦</strong><br>è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨å‡å°‘æ‰‹åŠ¨è¯†åˆ«å’Œä¿®å¤æºä»£ç ä¸­é”™è¯¯çš„æ‰€éœ€åŠªåŠ›ã€‚éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å´›èµ·ï¼Œè‡ªæˆ‘è¿­ä»£èƒ½åŠ›ä½¿LLMèƒ½å¤Ÿåœ¨åé¦ˆçš„æŒ‡å¯¼ä¸‹å¯¹è¡¥ä¸è¿›è¡Œå¤šæ¬¡è¿­ä»£ä¼˜åŒ–ã€‚æœ¬æ–‡ç ”ç©¶äº†å¹³è¡¡ç”Ÿæˆå¤šä¸ªè¾“å‡ºå’Œå¤šæ¬¡è¿­ä»£çš„APRç®¡é“ï¼ŒåŒæ—¶é™åˆ¶æ¯ä¸ªé”™è¯¯çš„è¡¥ä¸æ€»æ•°ä¸º10ä¸ªã€‚æœ¬æ–‡åº”ç”¨äº†ä¸‰ç§å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒLLMâ€”â€”DeepSeekCoder-Instructã€Codellama-Instructå’ŒLlama3.1-Instructæ¥å®ŒæˆAPRä»»åŠ¡ã€‚è¿›ä¸€æ­¥å¯¹æ¯ç§æ¨¡å‹è¿›è¡ŒAPRæ•°æ®é›†çš„ä¸åŒè§„æ¨¡ï¼ˆ1Kã€30Kã€65Kï¼‰å’Œä¸¤ç§æŠ€æœ¯ï¼ˆå…¨å¾®è°ƒLoRAï¼‰çš„å¾®è°ƒï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸¤ä¸ªAPRåŸºå‡†æµ‹è¯•å¥—ä»¶HumanEval-Javaå’ŒDefects4Jä¸Šè¯„ä¼°å…¶ä¿®å¤èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œä»…ä½¿ç”¨ä¸€å°éƒ¨åˆ†ï¼ˆ&lt;1%ï¼‰çš„å¾®è°ƒæ•°æ®é›†ï¼Œæˆ‘ä»¬å°±å¯ä»¥æé«˜é«˜è¾¾78%çš„å¯è¡Œè¡¥ä¸ç”Ÿæˆæ•°é‡ï¼ŒæŒ‘æˆ˜äº†å…ˆå‰æŠ¥å‘Šä½¿ç”¨å…¨å¾®è°ƒæ”¶ç›Šæœ‰é™çš„ç ”ç©¶ã€‚ç„¶è€Œæˆ‘ä»¬å‘ç°è¶…å‡ºä¸€å®šçš„é˜ˆå€¼ä¼šå¯¼è‡´æ”¶ç›Šé€’å‡ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¿‡åº¦æ‹Ÿåˆé€ æˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜åŸºç¡€æ¨¡å‹é€šè¿‡è¿­ä»£åˆ›å»ºè¡¥ä¸çš„æ–¹å¼æ¯”ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è¡¥ä¸çš„æ–¹å¼å—ç›ŠåŒªæµ…ã€‚å³ä½¿åœ¨å¤æ‚çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿­ä»£ç­–ç•¥çš„ä¼˜åŠ¿ä¹Ÿå˜å¾—æ›´åŠ çªå‡ºã€‚è™½ç„¶å¾®è°ƒæ¨¡å‹ä»è¿­ä»£ä¸­è·ç›Šè¾ƒå°‘ï¼Œä½†ä»å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„åŸºå‡†æµ‹è¯•ä¸­ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†éœ€è¦å¹³è¡¡çš„APRç­–ç•¥ï¼Œç»“åˆå¤šè¾“å‡ºç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªåŠ¨ç¨‹åºä¿®å¤æ—¨åœ¨å‡å°‘ä¿®å¤æºä»£ç é”™è¯¯çš„æ‰‹åŠ¨åŠªåŠ›ã€‚</li>
<li>LLMçš„è‡ªæˆ‘è¿­ä»£èƒ½åŠ›ä½¿è¡¥ä¸ä¼˜åŒ–æˆä¸ºå¯èƒ½ã€‚</li>
<li>ç ”ç©¶äº†ç»“åˆç”Ÿæˆå¤šä¸ªè¾“å‡ºå’Œå¤šæ¬¡è¿­ä»£çš„APRç­–ç•¥ï¼Œé™åˆ¶æ¯ä¸ªé”™è¯¯çš„è¡¥ä¸æ€»æ•°ä¸º10ä¸ªã€‚</li>
<li>ä½¿ç”¨å…ˆè¿›çš„æŒ‡ä»¤å¾®è°ƒLLMå®ŒæˆAPRä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨è¾ƒå°çš„å¾®è°ƒæ•°æ®é›†å³å¯æ˜¾è‘—æé«˜è¡¥ä¸ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>è¿­ä»£ç­–ç•¥å¯¹åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹éƒ½æœ‰ç›Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åŸºå‡†æµ‹è¯•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-482e52f1c11938b4d51e6d6a0fb10d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcdf3f1d21734986b20f7b270b28018c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49b54f2ddecff367b33ea557fc3eed15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91db44d4999816e5db123a234f653ec7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Snakemaker-Seamlessly-transforming-ad-hoc-analyses-into-sustainable-Snakemake-workflows-with-generative-AI"><a href="#Snakemaker-Seamlessly-transforming-ad-hoc-analyses-into-sustainable-Snakemake-workflows-with-generative-AI" class="headerlink" title="Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable   Snakemake workflows with generative AI"></a>Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable   Snakemake workflows with generative AI</h2><p><strong>Authors:Marco Masera, Alessandro Leone, Johannes KÃ¶ster, Ivan Molineris</strong></p>
<p>Reproducibility and sustainability present significant challenges in bioinformatics software development, where rapidly evolving tools and complex workflows often result in short-lived or difficult-to-adapt pipelines. This paper introduces Snakemaker, a tool that leverages generative AI to facilitate researchers build sustainable data analysis pipelines by converting unstructured code into well-defined Snakemake workflows. Snakemaker non-invasively tracks the work performed in the terminal by the researcher, analyzes execution patterns, and generates Snakemake workflows that can be integrated into existing pipelines. Snakemaker also supports the transformation of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the global state of the notebook into discrete, file-based interactions between rules. An integrated chat assistant provides users with fine-grained control through natural language instructions. Snakemaker generates high-quality Snakemake workflows by adhering to the best practices, including Conda environment tracking, generic rule generation and loop unrolling. By lowering the barrier between prototype and production-quality code, Snakemaker addresses a critical gap in computational reproducibility for bioinformatics research. </p>
<blockquote>
<p>åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦è½¯ä»¶å¼€å‘ä¸­ï¼Œå†ç°æ€§å’Œå¯æŒç»­æ€§æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é‡Œï¼Œå¿«é€Ÿæ¼”å˜çš„å·¥å…·å’Œå¤æ‚çš„å·¥ä½œæµç¨‹ç»å¸¸å¯¼è‡´çŸ­æœŸå­˜åœ¨æˆ–éš¾ä»¥é€‚åº”çš„ç®¡é“ã€‚æœ¬æ–‡ä»‹ç»äº†Snakemakerï¼Œå®ƒæ˜¯ä¸€æ¬¾åˆ©ç”¨ç”Ÿæˆäººå·¥æ™ºèƒ½çš„å·¥å…·ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜é€šè¿‡å°†ä»ç»“æ„åŒ–çš„ä»£ç ä¸­ç”Ÿæˆæ˜ç¡®çš„Snakemakeå·¥ä½œæµï¼Œå»ºç«‹å¯æŒç»­çš„æ•°æ®åˆ†æç®¡é“ã€‚Snakemakeréä¾µå…¥æ€§åœ°è¿½è¸ªç ”ç©¶äººå‘˜åœ¨ç»ˆç«¯æ‰§è¡Œçš„å·¥ä½œï¼Œåˆ†ææ‰§è¡Œæ¨¡å¼ï¼Œå¹¶ç”Ÿæˆå¯ä»¥é›†æˆåˆ°ç°æœ‰ç®¡é“ä¸­çš„Snakemakeå·¥ä½œæµã€‚Snakemakerè¿˜æ”¯æŒå°†åºå¤§çš„Ipythonç¬”è®°æœ¬è½¬æ¢ä¸ºæ¨¡å—åŒ–çš„Snakemakeç®¡é“ï¼Œè§£å†³ç¬”è®°æœ¬å…¨å±€çŠ¶æ€çš„é—®é¢˜ï¼Œå°†å…¶è½¬å˜ä¸ºè§„åˆ™ä¹‹é—´ç¦»æ•£ã€åŸºäºæ–‡ä»¶çš„äº¤äº’ã€‚ä¸€ä¸ªé›†æˆçš„èŠå¤©åŠ©æ‰‹é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸ºç”¨æˆ·æä¾›ç²¾ç»†ç²’åº¦çš„æ§åˆ¶ã€‚Snakemakeré€šè¿‡éµå¾ªæœ€ä½³å®è·µç”Ÿæˆé«˜è´¨é‡çš„Snakemakeå·¥ä½œæµï¼ŒåŒ…æ‹¬Condaç¯å¢ƒè·Ÿè¸ªã€é€šç”¨è§„åˆ™ç”Ÿæˆå’Œå¾ªç¯å±•å¼€ã€‚é€šè¿‡é™ä½åŸå‹å’Œç”Ÿäº§è´¨é‡ä»£ç ä¹‹é—´çš„éšœç¢ï¼ŒSnakemakerè§£å†³äº†ç”Ÿç‰©ä¿¡æ¯å­¦ç ”ç©¶ä¸­è®¡ç®—å†ç°æ€§çš„å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02841v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è›‡å½¢æµæ°´çº¿å·¥å…·ï¼ˆSnakemakerï¼‰å¼•å…¥äº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ–¹æ³•ï¼Œé€šè¿‡æŠŠéç»“æ„åŒ–çš„ä»£ç è½¬åŒ–ä¸ºå®šä¹‰çš„è›‡å½¢æµæ°´çº¿å·¥ä½œæµæ¥æå‡æ•°æ®ç®¡é“ç ”ç©¶çš„å¯æŒç»­æ€§ã€‚è¯¥å·¥å…·ä»¥éä¾µå…¥çš„æ–¹å¼è·Ÿè¸ªç ”ç©¶äººå‘˜åœ¨ç»ˆç«¯ä¸­çš„å·¥ä½œè¡¨ç°ï¼Œåˆ†ææ‰§è¡Œæ¨¡å¼å¹¶ç”Ÿæˆå¯ä»¥é›†æˆåˆ°ç°æœ‰æµæ°´çº¿ä¸­çš„è›‡å½¢æµæ°´çº¿å·¥ä½œæµã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒå°†å¤§å‹Ipythonç¬”è®°æœ¬è½¬åŒ–ä¸ºæ¨¡å—åŒ–çš„è›‡å½¢æµæ°´çº¿ï¼Œå¹¶é€šè¿‡ç²¾ç»†ç²’åº¦çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ§åˆ¶ç”¨æˆ·äº¤äº’ã€‚Snakemakeréµå¾ªæœ€ä½³å®è·µç”Ÿæˆé«˜è´¨é‡çš„è›‡å½¢æµæ°´çº¿ï¼ŒåŒ…æ‹¬Condaç¯å¢ƒè·Ÿè¸ªã€é€šç”¨è§„åˆ™ç”Ÿæˆå’Œå¾ªç¯å±•å¼€ç­‰ã€‚è¯¥å·¥å…·è§£å†³äº†ä»åŸå‹åˆ°ç”Ÿäº§è´¨é‡ä»£ç çš„éšœç¢ï¼Œå¡«è¡¥äº†ç”Ÿç‰©ä¿¡æ¯å­¦ç ”ç©¶ä¸­çš„è®¡ç®—å¯é‡å¤æ€§çš„å…³é”®ç©ºç™½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Snakemakeråˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å°†éç»“æ„åŒ–ä»£ç è½¬åŒ–ä¸ºè›‡å½¢æµæ°´çº¿å·¥ä½œæµã€‚</li>
<li>Snakemakerä»¥éä¾µå…¥æ–¹å¼è·Ÿè¸ªç»ˆç«¯å·¥ä½œè¡¨ç°å¹¶åˆ†ææ‰§è¡Œæ¨¡å¼ã€‚</li>
<li>Snakemakeræ”¯æŒå°†å¤§å‹Ipythonç¬”è®°æœ¬è½¬åŒ–ä¸ºæ¨¡å—åŒ–çš„è›‡å½¢æµæ°´çº¿ã€‚</li>
<li>Snakemakerè§£å†³äº†ç”Ÿç‰©ä¿¡æ¯å­¦è½¯ä»¶å¼€å‘çš„å¯æŒç»­æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤æä¾›ç²¾ç»†ç²’åº¦çš„æ§åˆ¶ã€‚</li>
<li>Snakemakeréµå¾ªæœ€ä½³å®è·µç”Ÿæˆé«˜è´¨é‡çš„è›‡å½¢æµæ°´çº¿å·¥ä½œæµã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4df6e52f7ae4d332c6f0a293b21621b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e402509b478344f60970e60a7b4e979.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1212fec1d436f99a0c2e7e36e2d0ba28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7e5fbb8150f072cc3afe89a2a24acfe.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReplaceMe-Network-Simplification-via-Layer-Pruning-and-Linear-Transformations"><a href="#ReplaceMe-Network-Simplification-via-Layer-Pruning-and-Linear-Transformations" class="headerlink" title="ReplaceMe: Network Simplification via Layer Pruning and Linear   Transformations"></a>ReplaceMe: Network Simplification via Layer Pruning and Linear   Transformations</h2><p><strong>Authors:Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko</strong></p>
<p>We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining&#x2F;fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original modelâ€™s performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ReplaceMeï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ— è®­ç»ƒæ·±åº¦å‰ªææ–¹æ³•ï¼Œå®ƒå¯ä»¥é€šè¿‡çº¿æ€§è¿ç®—æœ‰æ•ˆåœ°æ›¿æ¢transformerå—ï¼ŒåŒæ—¶åœ¨ä½å‹ç¼©æ¯”çš„æƒ…å†µä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒçš„å‰ªææ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€ä¸ªå°å‹æ ¡å‡†æ•°æ®é›†ï¼Œç”¨äºä¼°è®¡çº¿æ€§å˜æ¢ä»¥è¿‘ä¼¼å‰ªæå—ã€‚ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥æ— ç¼åœ°ä¸å…¶ä»–transformerå—åˆå¹¶ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„ç½‘ç»œå‚æ•°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReplaceMeå§‹ç»ˆä¼˜äºå…¶ä»–æ— è®­ç»ƒæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠå¤§é‡é‡æ–°è®­ç»ƒ&#x2F;å¾®è°ƒå’Œç»“æ„ä¿®æ”¹çš„å…ˆè¿›å‰ªææ–¹æ³•ä¸­ä¿æŒé«˜åº¦ç«äº‰åŠ›ã€‚åº”ç”¨äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼ŒReplaceMeå®ç°äº†é«˜è¾¾25%çš„å‰ªæç‡ï¼ŒåŒæ—¶åœ¨å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸­ä¿ç•™äº†åŸå§‹æ¨¡å‹çº¦90%çš„æ€§èƒ½â€”â€”æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€æœ€å°ï¼ˆè§å›¾1ï¼‰ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¼€æºåº“ï¼Œå®ç°äº†ReplaceMeä»¥åŠå‡ ç§å…ˆè¿›çš„æ·±åº¦å‰ªææŠ€æœ¯ï¼Œå¯åœ¨è¯¥ä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02819v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ReplaceMeæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œå¯åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°ä½å‹ç¼©æ¯”çš„è½¬æ¢æ“ä½œã€‚ä¸å…¶ä»–éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒçš„ä¼ ç»Ÿå‰ªææ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä»…éœ€è¦ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§è½¬æ¢ä»¥è¿‘ä¼¼å‰ªæå—ã€‚ä¼°è®¡çš„çº¿æ€§æ˜ å°„å¯ä»¥æ— ç¼åœ°ä¸å…¶ä»–è½¬æ¢å™¨å—åˆå¹¶ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„ç½‘ç»œå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ— éœ€è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤å³å¯å®ç°é«˜è¾¾25%çš„å‰ªæï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æ¨¡å‹åœ¨å¼€æ”¾åŸºå‡†æµ‹è¯•ä¸Šçº¦90%çš„æ€§èƒ½ï¼Œè®¡ç®—å¼€é”€æœ€å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReplaceMeæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ·±åº¦å‰ªææ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°ä½å‹ç¼©æ¯”è½¬æ¢æ“ä½œã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªå°çš„æ ¡å‡†æ•°æ®é›†æ¥ä¼°è®¡çº¿æ€§è½¬æ¢ä»¥è¿‘ä¼¼å‰ªæå—ï¼Œæ— éœ€é¢å¤–çš„ç½‘ç»œå‚æ•°ã€‚</li>
<li>ReplaceMeèƒ½å¤Ÿæ— ç¼åœ°ä¸å…¶ä»–è½¬æ¢å™¨å—åˆå¹¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒReplaceMeåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¯å®ç°é«˜è¾¾25%çš„å‰ªæã€‚</li>
<li>ReplaceMeå¯ä»¥åœ¨ä¸æŸå¤±å¤ªå¤šæ€§èƒ½çš„æƒ…å†µä¸‹å®ç°é«˜å‹ç¼©æ¯”ï¼Œä¸å…¶ä»–å…ˆè¿›çš„å‰ªææ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä»»ä½•è®­ç»ƒæˆ–ä¿®å¤æ­¥éª¤å³å¯å®ç°é«˜æ€§èƒ½å‰ªæï¼Œä»è€Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2492f682c8d6095a4fab9b97c2f9dc72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98d9c0b650bedd3eb3804aab8f3d8180.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a601ec0368bd50049922e2a2ab8cf411.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-777f47b7959aca280b98262d32174e81.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SEFE-Superficial-and-Essential-Forgetting-Eliminator-for-Multimodal-Continual-Instruction-Tuning"><a href="#SEFE-Superficial-and-Essential-Forgetting-Eliminator-for-Multimodal-Continual-Instruction-Tuning" class="headerlink" title="SEFE: Superficial and Essential Forgetting Eliminator for Multimodal   Continual Instruction Tuning"></a>SEFE: Superficial and Essential Forgetting Eliminator for Multimodal   Continual Instruction Tuning</h2><p><strong>Authors:Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ho Shing Ip, Sam Kwong</strong></p>
<p>Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the modelâ€™s knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasksâ€™ answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the modelâ€™s knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance. </p>
<blockquote>
<p>å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ï¼ˆMCITï¼‰æ—¨åœ¨ä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰èƒ½å¤Ÿé€æ­¥å­¦ä¹ æ–°ä»»åŠ¡è€Œæ— éœ€é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åœ¨æ­¤èƒŒæ™¯ä¸‹çš„é—å¿˜é—®é¢˜ï¼Œå°†å…¶åˆ†ä¸ºæµ…è¡¨æ€§é—å¿˜å’Œæœ¬è´¨æ€§é—å¿˜ã€‚æµ…è¡¨æ€§é—å¿˜æ˜¯æŒ‡æ¨¡å‹çš„çŸ¥è¯†å¯èƒ½å¹¶æœªçœŸæ­£ä¸¢å¤±ï¼Œä½†ç”±äºåç»­ä»»åŠ¡çš„ç­”æ¡ˆé£æ ¼å½±å“ï¼Œå…¶å¯¹å…ˆå‰ä»»åŠ¡çš„å›ç­”åç¦»äº†é¢„æœŸæ ¼å¼ï¼Œå¯¼è‡´ç»“æœæ— æ³•ä½¿ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ¬è´¨æ€§é—å¿˜æ˜¯æŒ‡æ¨¡å‹æä¾›æ ¼å¼æ­£ç¡®ä½†äº‹å®é”™è¯¯çš„ç­”æ¡ˆï¼Œè¿™è¡¨æ˜çŸ¥è¯†ç¡®å®ä¸¢å¤±äº†ã€‚è¯„ä¼°æœ¬è´¨æ€§é—å¿˜éœ€è¦å…ˆè§£å†³æµ…è¡¨æ€§é—å¿˜é—®é¢˜ï¼Œå› ä¸ºä¸¥é‡çš„æµ…è¡¨æ€§é—å¿˜å¯èƒ½ä¼šæ©ç›–æ¨¡å‹çš„çŸ¥è¯†çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ç­”æ¡ˆé£æ ¼å¤šæ ·åŒ–ï¼ˆASDï¼‰èŒƒå¼ï¼Œè¯¥èŒƒå¼å®šä¹‰äº†ä¸€ä¸ªæ ‡å‡†åŒ–æµç¨‹æ¥è½¬æ¢ä¸åŒä»»åŠ¡çš„æ•°æ®é£æ ¼ï¼Œå°†å…¶è®­ç»ƒé›†ç»Ÿä¸€ä¸ºç±»ä¼¼å¤šæ ·åŒ–çš„é£æ ¼ï¼Œä»¥é˜²æ­¢å› é£æ ¼å˜åŒ–å¯¼è‡´çš„æµ…è¡¨æ€§é—å¿˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†RegLoRAæ¥ç¼“è§£æœ¬è´¨æ€§é—å¿˜ã€‚RegLoRAé€šè¿‡åº”ç”¨æ­£åˆ™åŒ–æ¥ç¨³å®šå…³é”®å‚æ•°ï¼Œå…¶ä¸­ä¸»è¦å­˜å‚¨å…ˆéªŒçŸ¥è¯†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¿ç•™ç°æœ‰æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•´ä½“æ–¹æ³•SEFEè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02486v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æŒç»­æŒ‡ä»¤è°ƒä¼˜ï¼ˆMCITï¼‰çš„ç›®æ ‡ï¼Œæ—¨åœ¨ä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿé€æ­¥å­¦ä¹ æ–°ä»»åŠ¡è€Œä¸å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚æ–‡ç« æ¢è®¨äº†è¯¥èƒŒæ™¯ä¸‹çš„é—å¿˜é—®é¢˜ï¼Œå°†å…¶åˆ†ä¸ºè¡¨é¢æ€§é—å¿˜å’Œæœ¬è´¨æ€§é—å¿˜ã€‚é€šè¿‡å¼•å…¥ç­”æ¡ˆé£æ ¼å¤šæ ·åŒ–ï¼ˆASDï¼‰èŒƒå¼å’ŒRegLoRAæ–¹æ³•ï¼Œæœ‰æ•ˆé˜²æ­¢å› é£æ ¼å˜åŒ–å¯¼è‡´çš„è¡¨é¢æ€§é—å¿˜å¹¶å‡è½»æœ¬è´¨æ€§é—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ•´ä½“æ–¹æ³•SEFEè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MCITçš„ç›®æ ‡æ˜¯ä½¿MLLMsèƒ½å¤Ÿé€æ­¥å­¦ä¹ æ–°ä»»åŠ¡è€Œä¸å¿˜è®°æ—§çŸ¥è¯†ã€‚</li>
<li>é—å¿˜åœ¨MLLMsä¸­åˆ†ä¸ºè¡¨é¢æ€§é—å¿˜å’Œæœ¬è´¨æ€§é—å¿˜ã€‚</li>
<li>è¡¨é¢æ€§é—å¿˜æŒ‡çš„æ˜¯æ¨¡å‹çŸ¥è¯†å¹¶æœªçœŸæ­£ä¸¢å¤±ï¼Œä½†ç”±äºåç»­ä»»åŠ¡ç­”æ¡ˆé£æ ¼çš„å½±å“ï¼Œå¯¹ä»»åŠ¡çš„å›åº”åç¦»äº†é¢„æœŸæ ¼å¼ã€‚</li>
<li>æœ¬è´¨æ€§é—å¿˜æ˜¯æ¨¡å‹çœŸæ­£ä¸§å¤±äº†çŸ¥è¯†ï¼Œè¡¨ç°ä¸ºå›ç­”æ ¼å¼æ­£ç¡®ä½†å†…å®¹å¤±å®ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°æœ¬è´¨æ€§é—å¿˜ï¼Œå¿…é¡»å…ˆè§£å†³è¡¨é¢æ€§é—å¿˜ï¼Œå› ä¸ºä¸¥é‡çš„è¡¨é¢æ€§é—å¿˜å¯èƒ½ä¼šæ©ç›–æ¨¡å‹çš„çŸ¥è¯†çŠ¶æ€ã€‚</li>
<li>å¼•å…¥ASDèŒƒå¼æ¥æ ‡å‡†åŒ–ä¸åŒä»»åŠ¡çš„æ•°æ®æ ·å¼è½¬æ¢ï¼Œé˜²æ­¢å› é£æ ¼å˜åŒ–å¯¼è‡´çš„è¡¨é¢æ€§é—å¿˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23d7b7b11ecddbe1d4a498151157bcec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39dc404e14854b92733685722b4aa148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fa8a2306aa6d982959afeb25eb06a8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c393e08092ded38557706db09f91c70f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Colombian-Waitresses-y-Jueces-canadienses-Gender-and-Country-Biases-in-Occupation-Recommendations-from-LLMs"><a href="#Colombian-Waitresses-y-Jueces-canadienses-Gender-and-Country-Biases-in-Occupation-Recommendations-from-LLMs" class="headerlink" title="Colombian Waitresses y Jueces canadienses: Gender and Country Biases in   Occupation Recommendations from LLMs"></a>Colombian Waitresses y Jueces canadienses: Gender and Country Biases in   Occupation Recommendations from LLMs</h2><p><strong>Authors:Elisa Forcada RodrÃ­guez, Olatz Perez-de-ViÃ±aspre, Jon Ander Campos, Dietrich Klakow, Vagrant Gautam</strong></p>
<p>One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å…¬å¹³æ€§ç ”ç©¶çš„ç›®æ ‡ä¹‹ä¸€æ˜¯è¡¡é‡å’Œå‡è½»ç”±NLPç³»ç»Ÿä¼ æ’­çš„åˆ»æ¿åè§ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„å·¥ä½œå¾€å¾€é›†ä¸­åœ¨å•ä¸€åè§è½´ï¼ˆæœ€å¸¸è§çš„æ˜¯æ€§åˆ«ï¼‰å’Œè‹±è¯­è¯­è¨€ä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç¬¬ä¸€é¡¹å¤šè¯­è¨€äº¤å‰å›½å®¶å’Œæ€§åˆ«åè§çš„ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨ç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„èŒä¸šæ¨èã€‚æˆ‘ä»¬åœ¨è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­ä¸­æ„å»ºäº†æç¤ºåŸºå‡†æµ‹è¯•ï¼Œç³»ç»Ÿåœ°æ”¹å˜å›½å®¶å’Œæ€§åˆ«ï¼Œä½¿ç”¨25ä¸ªå›½å®¶å’Œå››ç»„ä»£è¯ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨è¿™ä¸€åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†5ä¸ªåŸºäºLlamaçš„æ¨¡å‹ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åŒ…å«äº†é‡å¤§çš„æ€§åˆ«å’Œå›½å®¶åè§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ¨¡å‹åœ¨æ€§åˆ«æˆ–å›½å®¶æ–¹é¢è¡¨ç°å‡ºå•ä¸€å…¬å¹³æ€§ï¼ŒåŸºäºå›½å®¶å’Œæ€§åˆ«çš„äº¤å‰èŒä¸šåè§ä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæç¤ºè¯­è¨€å¯¹åè§æœ‰å¾ˆå¤§å½±å“ï¼Œè€ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹å§‹ç»ˆè¡¨ç°å‡ºæœ€ä½å’Œæœ€ç¨³å®šçš„åè§æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œå…¬å¹³ç ”ç©¶äººå‘˜éœ€è¦åœ¨å…¶å·¥ä½œä¸­ä½¿ç”¨äº¤å‰å’Œå¤šè¯­è¨€çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02456v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨NLPçš„å…¬å¹³æ€§ç ”ç©¶ä¸­ï¼Œä¸€ä¸ªç›®æ ‡æ˜¯è¡¡é‡å’Œå‡è½»ç”±NLPç³»ç»Ÿä¼ æ’­çš„åˆ»æ¿åè§ã€‚ç„¶è€Œï¼Œæ­¤ç±»å·¥ä½œå¾€å¾€ä¾§é‡äºå•ä¸€åè§è½´ï¼ˆé€šå¸¸æ˜¯æ€§åˆ«ï¼‰å’Œè‹±è¯­ã€‚æœ¬ç ”ç©¶å¼¥è¡¥äº†è¿™äº›ä¸è¶³ï¼Œé¦–æ¬¡ç ”ç©¶å¤šè¯­è¨€å›½å®¶ä¸æ€§åˆ«åè§äº¤é›†ï¼Œé‡ç‚¹å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿçš„èŒä¸šæ¨èã€‚æˆ‘ä»¬åœ¨è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­ä¸­æ„å»ºäº†åŸºå‡†æµ‹è¯•é›†ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜å›½å®¶å’Œæ€§åˆ«ï¼ˆä½¿ç”¨25ä¸ªå›½å®¶å’Œå››ä¸ªä»£è¯é›†ï¼‰ï¼Œè¯„ä¼°äº†åŸºäºLlamaçš„äº”ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°LLMè•´å«æ˜¾è‘—çš„æ€§åˆ«å’Œå›½å®¶åè§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ¨¡å‹å•ç‹¬å¯¹æ€§åˆ«æˆ–å›½å®¶æ˜¾ç¤ºå…¬å¹³æ€§ï¼ŒåŸºäºä¸¤è€…ç»“åˆçš„äº¤å‰èŒä¸šåè§ä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬è¿˜å‘ç°æç¤ºè¯­è¨€æ˜¾è‘—å½±å“åè§ï¼ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹è¡¨ç°å‡ºæœ€ä½ä¸”æœ€ç¨³å®šçš„åè§æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å…¬å¹³ç ”ç©¶äººå‘˜éœ€è¦åœ¨å·¥ä½œä¸­ä½¿ç”¨äº¤å‰å’Œå¤šè¯­è¨€è§†è§’çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„å›½å®¶ä¸æ€§åˆ«åè§äº¤é›†é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„èŒä¸šæ¨èæ–¹é¢ã€‚</li>
<li>æ„å»ºäº†é’ˆå¯¹è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>ç³»ç»Ÿåœ°ç ”ç©¶ä¸åŒå›½å®¶å’Œæ€§åˆ«å¯¹èŒä¸šæ¨èçš„å½±å“ï¼Œä½¿ç”¨25ä¸ªå›½å®¶å’Œå››ç§ä»£è¯é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„æ€§åˆ«å’Œå›½å®¶åè§ã€‚</li>
<li>å³ä½¿æ¨¡å‹åœ¨å•ä¸€ç»´åº¦ï¼ˆæ€§åˆ«æˆ–å›½å®¶ï¼‰ä¸Šè¡¨ç°å…¬å¹³ï¼Œäº¤å‰èŒä¸šåè§ä»ç„¶å­˜åœ¨ã€‚</li>
<li>æç¤ºè¯­è¨€å¯¹åè§æœ‰æ˜¾è‘—å½±å“ï¼ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹è¡¨ç°å‡ºè¾ƒä½ä¸”ç¨³å®šçš„åè§æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8781c5fc6e05d61c4291f7b0ae81f4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-942c1328b4a39e8630018cdfa9809896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-405066f4418fc649e64d7ba27754e9d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0191101c64436030a912094e9ac35033.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Intra-Layer-Recurrence-in-Transformers-for-Language-Modeling"><a href="#Intra-Layer-Recurrence-in-Transformers-for-Language-Modeling" class="headerlink" title="Intra-Layer Recurrence in Transformers for Language Modeling"></a>Intra-Layer Recurrence in Transformers for Language Modeling</h2><p><strong>Authors:Anthony Nguyen, Wenjun Lin</strong></p>
<p>Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures. </p>
<blockquote>
<p>Transformeræ¨¡å‹å·²åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œä½†å…¶æ·±åº¦å¢åŠ å¯¼è‡´å‚æ•°æ•°é‡å¤§å¹…å¢é•¿ã€‚ç°æœ‰çš„å¾ªç¯Transformeræ–¹æ³•é€šè¿‡å¤šæ¬¡é‡æ–°å¤„ç†å±‚æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨æ•´ä¸ªå±‚å—ä¸­éšæ„åº”ç”¨å¾ªç¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é€å±‚é€’å½’ï¼ˆILRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ›´æœ‰é’ˆå¯¹æ€§çš„æ–¹æ³•ï¼Œå¯ä»¥ä¸€æ¬¡å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­æœ‰é€‰æ‹©åœ°å¯¹ä¸ªåˆ«å±‚åº”ç”¨é€’å½’ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¯¹æ—©æœŸå±‚åˆ†é…æ›´å¤šè¿­ä»£æ¬¡æ•°å¯ä»¥è·å¾—æœ€ä½³ç»“æœã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒILRä¸ºä¼˜åŒ–Transformeræ¶æ„ä¸­çš„å¾ªç¯ç»“æ„æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01855v1">PDF</a> Accepted at Canadian AI 2025. Code available at   <a target="_blank" rel="noopener" href="https://github.com/ant-8/Layer-Recurrent-Transformers">https://github.com/ant-8/Layer-Recurrent-Transformers</a></p>
<p><strong>Summary</strong><br>     å˜æ¢å™¨æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæ ‘ç«‹äº†æ–°çš„åŸºå‡†ï¼Œä½†å…¶æ·±åº¦å¢åŠ å¯¼è‡´å‚æ•°æ•°é‡å¤§å¹…å¢é•¿ã€‚ç°æœ‰å¾ªç¯å˜æ¢å™¨æ–¹æ³•é€šè¿‡å¤šæ¬¡é‡æ–°å¤„ç†å±‚æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬å¸¸å¸¸åœ¨æ•´ä¸ªå±‚å—ä¸Šç›²ç›®åº”ç”¨å¾ªç¯ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å•å‘å‰ä¼ ä¸­çš„é€‰æ‹©æ€§å±‚å†…å¾ªç¯ï¼ˆILRï¼‰ï¼Œæ›´æœ‰é’ˆå¯¹æ€§åœ°åº”ç”¨äºå•ä¸ªå±‚ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ºæ—©æœŸå±‚åˆ†é…æ›´å¤šè¿­ä»£æ¬¡æ•°å¯è·å¾—æœ€ä½³ç»“æœã€‚è¿™è¡¨æ˜ILRä¸ºä¼˜åŒ–å˜æ¢å™¨æ¶æ„ä¸­çš„å¾ªç¯ç»“æ„æä¾›äº†æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜æ¢å™¨æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†æ·±åº¦å¢åŠ å¯¼è‡´å‚æ•°å¢å¤šã€‚</li>
<li>ç°æœ‰å¾ªç¯å˜æ¢å™¨æ–¹æ³•é€šå¸¸åœ¨æ•´ä¸ªå±‚å—ä¸Šç›²ç›®åº”ç”¨å¾ªç¯ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºæ›´æœ‰é’ˆå¯¹æ€§çš„å±‚å†…å¾ªç¯ï¼ˆILRï¼‰æ–¹æ³•ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå¯¹æ—©æœŸå±‚åˆ†é…æ›´å¤šè¿­ä»£æ¬¡æ•°å¯å¾—åˆ°æœ€ä½³ç»“æœã€‚</li>
<li>ILRæ–¹æ³•ä¸ºä¼˜åŒ–å˜æ¢å™¨æ¶æ„ä¸­çš„å¾ªç¯ç»“æ„æä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>ILRèƒ½å¤Ÿé€‰æ‹©æ€§åº”ç”¨ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66bea8df76784c4d61ecda0c035b04bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec57c9faa6aac953b6e7ef0c7649927b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-712f801516d65720187f81fb6a22004f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a0a06a22aa92bf534f6781ffa6467fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5afaa400c9c157917fa704eb19732d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07be0c23b7fac6f4ca15a78037644159.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee43400fb252248b25e62b52da98d41c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Harnessing-the-Power-of-LLMs-Informers-and-Decision-Transformers-for-Intent-driven-RAN-Management-in-6G"><a href="#Harnessing-the-Power-of-LLMs-Informers-and-Decision-Transformers-for-Intent-driven-RAN-Management-in-6G" class="headerlink" title="Harnessing the Power of LLMs, Informers and Decision Transformers for   Intent-driven RAN Management in 6G"></a>Harnessing the Power of LLMs, Informers and Decision Transformers for   Intent-driven RAN Management in 6G</h2><p><strong>Authors:Md Arafat Habib, Pedro Enrique Iturria Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Melike Erol-Kantarci</strong></p>
<p>Intent-driven network management is critical for managing the complexity of 5G and 6G networks. It enables adaptive, on-demand management of the network based on the objectives of the network operators. In this paper, we propose an innovative three-step framework for intent-driven network management based on Generative AI (GenAI) algorithms. First, we fine-tune a Large Language Model (LLM) on a custom dataset using a Quantized Low-Rank Adapter (QLoRA) to enable memory-efficient intent processing within limited computational resources. A Retrieval Augmented Generation (RAG) module is included to support dynamic decision-making. Second, we utilize a transformer architecture for time series forecasting to predict key parameters, such as power consumption, traffic load, and packet drop rate, to facilitate intent validation proactively. Lastly, we introduce a Hierarchical Decision Transformer with Goal Awareness (HDTGA) to optimize the selection and orchestration of network applications and hence, optimize the network. Our intent guidance and processing approach improves BERTScore by 6% and the semantic similarity score by 9% compared to the base LLM model. Again, the proposed predictive intent validation approach can successfully rule out the performance-degrading intents with an average of 88% accuracy. Finally, compared to the baselines, the proposed HDTGA algorithm increases throughput at least by 19.3%, reduces delay by 48.5%, and boosts energy efficiency by 54.9%. </p>
<blockquote>
<p>æ„å›¾é©±åŠ¨çš„ç½‘ç»œç®¡ç†å¯¹äºç®¡ç†5Gå’Œ6Gç½‘ç»œçš„å¤æ‚æ€§è‡³å…³é‡è¦ã€‚å®ƒæ ¹æ®ç½‘ç»œè¿è¥å•†çš„ç›®æ ‡ï¼Œå®ç°äº†ç½‘ç»œçš„è‡ªé€‚åº”æŒ‰éœ€ç®¡ç†ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ç®—æ³•çš„åˆ›æ–°ä¸‰æ­¥æ„å›¾é©±åŠ¨ç½‘ç»œç®¡ç†æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é‡åŒ–ä½ç§©é€‚é…å™¨ï¼ˆQLoRAï¼‰åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸­å®ç°é«˜æ•ˆçš„æ„å›¾å¤„ç†ã€‚åŒ…å«æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—ï¼Œä»¥æ”¯æŒåŠ¨æ€å†³ç­–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å˜å‹å™¨æ¶æ„è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼Œé¢„æµ‹åŠŸè€—ã€äº¤é€šè´Ÿè½½å’ŒåŒ…ä¸¢å¤±ç‡ç­‰å…³é”®å‚æ•°ï¼Œä»¥ä¸»åŠ¨éªŒè¯æ„å›¾ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥å…·æœ‰ç›®æ ‡æ„è¯†åˆ†å±‚å†³ç­–å˜å‹å™¨ï¼ˆHDTGAï¼‰ï¼Œä¼˜åŒ–ç½‘ç»œåº”ç”¨ç¨‹åºçš„é€‰æ‹©å’Œåè°ƒï¼Œä»è€Œä¼˜åŒ–ç½‘ç»œã€‚ä¸åŸºç¡€LLMæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ„å›¾æŒ‡å¯¼å’Œå¤„ç†æ–¹æ³•æé«˜äº†BERTScore 6%å’Œè¯­ä¹‰ç›¸ä¼¼æ€§å¾—åˆ†9%ã€‚åŒæ ·ï¼Œæ‰€æå‡ºçš„é¢„æµ‹æ„å›¾éªŒè¯æ–¹æ³•å¯ä»¥æˆåŠŸæ’é™¤æ€§èƒ½ä¸‹é™çš„æ„å›¾ï¼Œå¹³å‡å‡†ç¡®ç‡ä¸º88%ã€‚æœ€åï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„HDTGAç®—æ³•è‡³å°‘æé«˜ååé‡19.3%ï¼Œé™ä½å»¶è¿Ÿ48.5%ï¼Œæé«˜èƒ½æºæ•ˆç‡54.9%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01841v1">PDF</a> Currently under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ç®—æ³•çš„åˆ›æ–°ä¸‰æ­¥æ¡†æ¶ï¼Œç”¨äºæ„å›¾é©±åŠ¨çš„ç½‘ç»œç®¡ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåˆ©ç”¨é‡åŒ–ä½ç§©é€‚é…å™¨ï¼ˆQLoRAï¼‰åœ¨æœ‰é™è®¡ç®—èµ„æºå†…å®ç°é«˜æ•ˆçš„æ„å›¾å¤„ç†ã€‚åŒæ—¶ï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—æ”¯æŒåŠ¨æ€å†³ç­–åˆ¶å®šã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è½¬æ¢å™¨æ¶æ„è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ï¼Œä»¥é¢„æµ‹å…³é”®å‚æ•°ï¼Œå¦‚åŠŸç‡æ¶ˆè€—ã€äº¤é€šè´Ÿè½½å’ŒåŒ…ä¸¢å¤±ç‡ï¼Œä»¥ä¸»åŠ¨éªŒè¯æ„å›¾ã€‚æœ€åï¼Œå¼•å…¥å…·æœ‰ç›®æ ‡æ„è¯†åˆ†å±‚å†³ç­–è½¬æ¢å™¨ï¼ˆHDTGAï¼‰ä¼˜åŒ–ç½‘ç»œåº”ç”¨å’Œç½‘ç»œçš„é€‰å‹ç¼–æ’ã€‚è¯¥æ¡†æ¶æé«˜äº†ç½‘ç»œç®¡ç†çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å›¾é©±åŠ¨çš„ç½‘ç»œç®¡ç†å¯¹äºç®¡ç†5Gå’Œ6Gç½‘ç»œçš„å¤æ‚æ€§è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ç®—æ³•çš„åˆ›æ–°ä¸‰æ­¥æ¡†æ¶ï¼ŒåŒ…æ‹¬å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€é‡åŒ–ä½ç§©é€‚é…å™¨ï¼ˆQLoRAï¼‰ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å—å’Œæ”¯æŒåŠ¨æ€å†³ç­–çš„æ—¶é—´åºåˆ—é¢„æµ‹è½¬æ¢å™¨æ¶æ„ã€‚</li>
<li>é€šè¿‡å¾®è°ƒLLMå’ŒQLoRAï¼Œå¯åœ¨æœ‰é™çš„è®¡ç®—èµ„æºå†…å®ç°é«˜æ•ˆçš„æ„å›¾å¤„ç†ã€‚</li>
<li>å¼•å…¥å…·æœ‰ç›®æ ‡æ„è¯†çš„åˆ†å±‚å†³ç­–è½¬æ¢å™¨ï¼ˆHDTGAï¼‰ä»¥ä¼˜åŒ–ç½‘ç»œé€‰æ‹©å’Œç¼–æ’ï¼Œæé«˜äº†ç½‘ç»œæ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æ„å›¾æŒ‡å¯¼å’Œå¤„ç†æé«˜äº†BERTScoreå’Œè¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æ•°ã€‚</li>
<li>é¢„æµ‹æ„å›¾éªŒè¯æ–¹æ³•å¯ä»¥æˆåŠŸæ’é™¤æ€§èƒ½ä¸‹é™çš„æ„å›¾ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-079c5b2b007f13f283435d54c5077eb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0c2dd024ba75b8a92dc5ecdaa62aff7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c0520f53a5bd3187e51680bcf44b21e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="How-Transformers-Learn-Regular-Language-Recognition-A-Theoretical-Study-on-Training-Dynamics-and-Implicit-Bias"><a href="#How-Transformers-Learn-Regular-Language-Recognition-A-Theoretical-Study-on-Training-Dynamics-and-Implicit-Bias" class="headerlink" title="How Transformers Learn Regular Language Recognition: A Theoretical Study   on Training Dynamics and Implicit Bias"></a>How Transformers Learn Regular Language Recognition: A Theoretical Study   on Training Dynamics and Implicit Bias</h2><p><strong>Authors:Ruiquan Huang, Yingbin Liang, Jing Yang</strong></p>
<p>Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as <code>even pairs&#39; and </code>parity checkâ€™, the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1&#x2F;t)$. Our experiments validate those theoretical results. </p>
<blockquote>
<p>è¯­è¨€è¯†åˆ«ä»»åŠ¡æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„æ ¹æœ¬ä»»åŠ¡ï¼Œå·²è¢«å¹¿æ³›åº”ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚è¿™äº›ä»»åŠ¡åœ¨è§£é‡Šå˜å‹å™¨çš„å·¥ä½œæœºåˆ¶æ–¹é¢ä¹Ÿèµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¸¸è§„è¯­è¨€è¯†åˆ«ç±»åˆ«ä¸­çš„ä¸¤ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼Œå³â€œå¶æ•°å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€ï¼Œå…¶ç›®çš„æ˜¯ç¡®å®šç»™å®šåºåˆ—ä¸­æŸäº›å­åºåˆ—çš„å‡ºç°æ˜¯å¦ä¸ºå¶æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¢ç´¢ç”±æ³¨æ„åŠ›å±‚åŠ çº¿æ€§å±‚ç»„æˆçš„ä¸€å±‚å˜å‹å™¨å¦‚ä½•å­¦ä¹ è§£å†³è¿™äº›ä»»åŠ¡ï¼Œé€šè¿‡ç†è®ºåˆ†æå…¶åœ¨æ¢¯åº¦ä¸‹é™ä¸‹çš„è®­ç»ƒåŠ¨æ€ã€‚è™½ç„¶â€œå¶æ•°å¯¹â€å¯ä»¥ç›´æ¥ç”±ä¸€å±‚å˜å‹å™¨è§£å†³ï¼Œâ€œå¥‡å¶æ ¡éªŒâ€éœ€è¦é€šè¿‡å°†æ€ç»´é“¾ï¼ˆCoTï¼‰é›†æˆåˆ°ä¸ºâ€œå¶æ•°å¯¹â€ä»»åŠ¡è®­ç»ƒè‰¯å¥½çš„å˜å‹å™¨çš„æ¨ç†é˜¶æ®µæˆ–ä¸€å±‚å˜å‹å™¨çš„è®­ç»ƒä¸­æ¥è§£å†³ã€‚å¯¹äºè¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚çš„è”åˆè®­ç»ƒè¡¨ç°å‡ºä¸¤ä¸ªæ˜æ˜¾çš„é˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæ³¨æ„åŠ›å±‚è¿…é€Ÿå¢é•¿ï¼Œå°†æ•°æ®åºåˆ—æ˜ å°„ä¸ºå¯åˆ†ç¦»çš„å‘é‡ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ³¨æ„åŠ›å±‚å˜å¾—ç¨³å®šï¼Œè€Œçº¿æ€§å±‚çš„å¢é•¿å‘ˆå¯¹æ•°è¶‹åŠ¿ï¼Œå¹¶æœå‘ä¸€ä¸ªæœ€å¤§é—´éš”è¶…å¹³é¢ï¼Œè¯¥è¶…å¹³é¢èƒ½å°†æ³¨æ„åŠ›å±‚çš„è¾“å‡ºæ­£ç¡®åœ°åˆ†ä¸ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼ŒæŸå¤±ä»¥O(1&#x2F;t)çš„é€Ÿåº¦å‡å°ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00926v2">PDF</a> accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¯­è¨€è¯†åˆ«ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹â€œå¶æ•°å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€è¿™ä¸¤ä¸ªä»£è¡¨æ€§ä»»åŠ¡çš„ç†è®ºåˆ†æã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶äº†ä¸€ä¸ªç”±æ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚ç»„æˆçš„ä¸€å±‚å˜å‹å™¨å¦‚ä½•é€šè¿‡æ¢¯åº¦ä¸‹é™è§£å†³è¿™äº›ä»»åŠ¡ã€‚ç ”ç©¶å‘ç°ï¼Œè”åˆè®­ç»ƒæ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ³¨æ„åŠ›å±‚è¿…é€Ÿå¢é•¿ï¼Œå°†æ•°æ®åºåˆ—æ˜ å°„ä¸ºå¯åˆ†å‘é‡ï¼›ç¬¬äºŒé˜¶æ®µæ³¨æ„åŠ›å±‚ç¨³å®šï¼Œçº¿æ€§å±‚ä»¥å¯¹æ•°æ–¹å¼å¢é•¿å¹¶é€æ¸æ¥è¿‘æœ€å¤§é—´éš”è¶…å¹³é¢ï¼Œå°†æ³¨æ„åŠ›å±‚è¾“å‡ºåˆ†ä¸ºæ­£è´Ÿæ ·æœ¬ã€‚å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­è¨€è¯†åˆ«ä»»åŠ¡æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>â€œå¶æ•°å¯¹â€å’Œâ€œå¥‡å¶æ ¡éªŒâ€æ˜¯è¯­è¨€è¯†åˆ«ä¸­çš„ä»£è¡¨æ€§ä»»åŠ¡ï¼Œæ—¨åœ¨ç¡®å®šç»™å®šåºåˆ—ä¸­æŸäº›å­åºåˆ—çš„å‡ºç°æ˜¯å¦ä¸ºå¶æ•°ã€‚</li>
<li>ä¸€å±‚å˜å‹å™¨ç”±æ³¨æ„åŠ›å±‚å’Œçº¿æ€§å±‚ç»„æˆï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™è§£å†³è¿™äº›ä»»åŠ¡çš„ç†è®ºåˆ†ææ˜¯æœ¬ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-60041ab444b877bfdd24cd723725800f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HAIR-Hardness-Aware-Inverse-Reinforcement-Learning-with-Introspective-Reasoning-for-LLM-Alignment"><a href="#HAIR-Hardness-Aware-Inverse-Reinforcement-Learning-with-Introspective-Reasoning-for-LLM-Alignment" class="headerlink" title="HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective   Reasoning for LLM Alignment"></a>HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective   Reasoning for LLM Alignment</h2><p><strong>Authors:Ruoxi Cheng, Haoxuan Ma, Weixin Wang</strong></p>
<p>The alignment of large language models (LLMs) with human values remains critical yet hindered by four key challenges: (1) scarcity of balanced safety datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to shallow alignment, and (4) inability to dynamically adapt rewards according to task difficulty. To address these limitations, we introduce HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a novel alignment approach inspired by shadow models in membership inference attacks. Our approach consists of two main components: (1) construction of a balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using structured prompts that leverage the introspective reasoning capabilities of LLMs; and (2) training of category-specific reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization to task difficulty at both the data and model levels. Comprehensive experiments across four harmlessness and four usefulness benchmarks demonstrate that HAIR achieves state-of-the-art performance, outperforming all baseline methods in safety while maintaining high levels of usefulness. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¥‘åˆä»ç„¶è‡³å…³é‡è¦ï¼Œä½†å—åˆ°å››ä¸ªå…³é”®æŒ‘æˆ˜çš„å½±å“ï¼šï¼ˆ1ï¼‰å¹³è¡¡å®‰å…¨æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼Œï¼ˆ2ï¼‰å¯¹é½ç¨ï¼Œï¼ˆ3ï¼‰ç”±äºæµ…å±‚å¯¹é½è€Œå®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œï¼ˆ4ï¼‰æ— æ³•æ ¹æ®ä»»åŠ¡éš¾åº¦åŠ¨æ€è°ƒæ•´å¥–åŠ±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†HAIRï¼ˆåŸºäºå†…çœæ¨ç†çš„ç¡¬åº¦æ„ŸçŸ¥é€†å‘å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å—æˆå‘˜æ¨ç†æ”»å‡»ä¸­çš„å½±å­æ¨¡å‹å¯å‘çš„æ–°å‹å¯¹é½æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç»“æ„åŒ–æç¤ºï¼Œæ„å»ºç”¨äºä¸ƒä¸ªæœ‰å®³ç±»åˆ«çš„å¹³è¡¡å®‰å…¨â€œè‰æ¡ˆé“¾â€ï¼ˆCoDï¼‰æ•°æ®é›†ï¼Œåˆ©ç”¨LLMçš„å†…çœæ¨ç†èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è®­ç»ƒç‰¹å®šç±»åˆ«çš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨æ•°æ®å’Œæ¨¡å‹å±‚é¢æ ¹æ®ä»»åŠ¡çš„éš¾åº¦åŠ¨æ€è°ƒæ•´ä¼˜åŒ–ã€‚åœ¨å››ä¸ªæ— å®³æ€§å’Œå››ä¸ªæœ‰ç”¨æ€§åŸºå‡†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHAIRå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å®‰å…¨æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºå‡†æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18991v2">PDF</a> The three authors contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¥‘åˆè‡³å…³é‡è¦ï¼Œä½†é¢ä¸´å››å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯¹é½æ–¹æ³•HAIRï¼Œé€šè¿‡æ„å»ºå¹³è¡¡çš„å®‰å…¨Chain-of-Draftæ•°æ®é›†å’Œè®­ç»ƒç‰¹å®šç±»åˆ«çš„å¥–åŠ±æ¨¡å‹ï¼Œå®ç°åŠ¨æ€é€‚åº”ä»»åŠ¡éš¾åº¦çš„ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒHAIRåœ¨å®‰å…¨æ€§å’Œå®ç”¨æ€§æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å¥‘åˆè‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨å››å¤§æŒ‘æˆ˜ã€‚</li>
<li>HAIRæ˜¯ä¸€ç§æ–°çš„LLMå¯¹é½æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŒ‘æˆ˜ã€‚</li>
<li>HAIRé€šè¿‡æ„å»ºå¹³è¡¡çš„å®‰å…¨Chain-of-Draftï¼ˆCoDï¼‰æ•°æ®é›†æ¥æé«˜LLMçš„å®‰å…¨æ€§ã€‚</li>
<li>CoDæ•°æ®é›†åˆ©ç”¨ç»“æ„åŒ–æç¤ºå’ŒLLMçš„ç›´è§‰æ¨ç†èƒ½åŠ›ï¼Œé’ˆå¯¹ä¸ƒç§æœ‰å®³ç±»åˆ«è¿›è¡Œæ„å»ºã€‚</li>
<li>HAIRé€šè¿‡è®­ç»ƒç‰¹å®šç±»åˆ«çš„å¥–åŠ±æ¨¡å‹ï¼Œå®ç°åŠ¨æ€é€‚åº”ä»»åŠ¡éš¾åº¦çš„ä¼˜åŒ–ã€‚</li>
<li>Group Relative Policy Optimizationï¼ˆGRPOï¼‰æ˜¯HAIRä¸­çš„å…³é”®ç»„ä»¶ï¼Œç”¨äºåœ¨æ•°æ®å’Œæ¨¡å‹å±‚é¢è¿›è¡ŒåŠ¨æ€ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18991">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-68a6b7ebba83fb2eb0b3433b17b89ea8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f623c6569f3856aa402517ce429bcaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ed5d2d3ee8206eecd3314e332ba0ebc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-199d7f8f9b972eebe5f412b1925453c8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CASE-â€“-Condition-Aware-Sentence-Embeddings-for-Conditional-Semantic-Textual-Similarity-Measurement"><a href="#CASE-â€“-Condition-Aware-Sentence-Embeddings-for-Conditional-Semantic-Textual-Similarity-Measurement" class="headerlink" title="CASE â€“ Condition-Aware Sentence Embeddings for Conditional Semantic   Textual Similarity Measurement"></a>CASE â€“ Condition-Aware Sentence Embeddings for Conditional Semantic   Textual Similarity Measurement</h2><p><strong>Authors:Gaifan Zhang, Yi Zhou, Danushka Bollegala</strong></p>
<p>The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM), where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised nonlinear projection is learned to reduce the dimensionality of the LLM-based text embeddings. We show that CASE significantly outperforms previously proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing standard benchmark dataset. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings. Moreover, we propose a supervised dimensionality reduction method that not only reduces the dimensionality of LLM-based embeddings but also significantly improves their performance. </p>
<blockquote>
<p>å¥å­æ‰€ä¼ è¾¾çš„æ„ä¹‰å¾€å¾€å–å†³äºå…¶å‡ºç°çš„ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚å°½ç®¡å¥å­åµŒå…¥æ–¹æ³•å·²ç»å–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†å¦‚ä½•æ ¹æ®ä¸Šä¸‹æ–‡ç¯å¢ƒå¯¹å¥å­åµŒå…¥è¿›è¡Œæœ€ä½³ä¿®æ”¹ä»ç„¶ä¸æ˜ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¡ä»¶æ„ŸçŸ¥å¥å­åµŒå…¥ï¼ˆCASEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„æ–¹æ³•ï¼Œç”¨äºåœ¨ç»™å®šæ¡ä»¶ä¸‹ä¸ºå¥å­åˆ›å»ºåµŒå…¥ã€‚é¦–å…ˆï¼ŒCASEä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¡ä»¶åˆ›å»ºåµŒå…¥ï¼Œå…¶ä¸­å¥å­ä¼šå½±å“æ¡ä»¶èšåˆè¿‡ç¨‹ä¸­è®¡ç®—çš„æ ‡è®°çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚æ¥ä¸‹æ¥ï¼Œå­¦ä¹ ç›‘ç£éçº¿æ€§æŠ•å½±ä»¥é™ä½åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥çš„ç»´åº¦ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒCASEåœ¨ç°æœ‰çš„æ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰æå‡ºçš„æ¡ä»¶è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆC-STSï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œå‡å»æ¡ä»¶åµŒå…¥å¯ä»¥æŒç»­æé«˜åŸºäºLLMçš„æ–‡æœ¬åµŒå…¥çš„C-STSæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›‘ç£é™ç»´æ–¹æ³•ï¼Œä¸ä»…é™ä½äº†åŸºäºLLMçš„åµŒå…¥çš„ç»´åº¦ï¼Œè¿˜æ˜¾è‘—æé«˜äº†å…¶æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17279v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¡ä»¶æ„ŸçŸ¥å¥å­åµŒå…¥ï¼ˆCASEï¼‰æ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„æ–¹æ³•ï¼Œç”¨äºåœ¨ç»™å®šçš„æ¡ä»¶ä¸‹ä¸ºå¥å­åˆ›å»ºåµŒå…¥ã€‚å®ƒé¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¡ä»¶åˆ›å»ºåµŒå…¥ï¼Œç„¶åé€šè¿‡ç›‘ç£éçº¿æ€§æŠ•å½±é™ä½æ–‡æœ¬åµŒå…¥çš„ç»´åº¦ã€‚CASEæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¡ä»¶è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆC-STSï¼‰æ–¹æ³•ï¼Œå¹¶å‘ç°å‡å»æ¡ä»¶åµŒå…¥å’Œé‡‡ç”¨ç›‘ç£é™ç»´æ–¹æ³•èƒ½æ”¹å–„LLMåŸºäºçš„æ–‡æœ¬åµŒå…¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¡ä»¶æ„ŸçŸ¥å¥å­åµŒå…¥ï¼ˆCASEï¼‰æ˜¯ä¸ºäº†è§£å†³å¥å­åµŒå…¥å¦‚ä½•æ ¹æ®ä¸Šä¸‹æ–‡è¿›è¡Œä¿®æ”¹çš„é—®é¢˜è€Œæå‡ºçš„æ–¹æ³•ã€‚</li>
<li>CASEä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¡ä»¶åˆ›å»ºåµŒå…¥ï¼Œå¥å­ä¼šå½±å“æ¡ä»¶æ± åŒ–æ—¶çš„æ³¨æ„åŠ›å¾—åˆ†è®¡ç®—ã€‚</li>
<li>CASEé‡‡ç”¨ç›‘ç£éçº¿æ€§æŠ•å½±æ¥é™ä½æ–‡æœ¬åµŒå…¥çš„ç»´åº¦ã€‚</li>
<li>CASEæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¡ä»¶è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆC-STSï¼‰æ–¹æ³•ã€‚</li>
<li>å‡å»æ¡ä»¶åµŒå…¥å¯æ”¹å–„LLMåŸºäºçš„æ–‡æœ¬åµŒå…¥åœ¨C-STSä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„ç›‘ç£é™ç»´æ–¹æ³•ä¸ä»…é™ä½äº†LLMåŸºäºåµŒå…¥çš„ç»´åº¦ï¼Œè¿˜æ˜¾è‘—æé«˜äº†å…¶æ€§èƒ½ã€‚</li>
<li>æ­¤æ–¹æ³•å¯¹äºç†è§£å’Œç”Ÿæˆä¸ç‰¹å®šä¸Šä¸‹æ–‡ç›¸å…³çš„å¥å­åµŒå…¥å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-345aba277e6d6c97d335d64ab6a46938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa21f07e2470c685dbc411ec68cd263c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BIG-Bench-Extra-Hard"><a href="#BIG-Bench-Extra-Hard" class="headerlink" title="BIG-Bench Extra Hard"></a>BIG-Bench Extra Hard</h2><p><strong>Authors:Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat</strong></p>
<p>Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8% for the best general-purpose model and 44.8% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/bbeh">https://github.com/google-deepmind/bbeh</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„éƒ¨ç½²æ—¥ç›Šå¢å¤šï¼Œè¦æ±‚å…·å¤‡ç¨³å¥çš„é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¤šæ ·åŒ–çš„æ¨ç†æŠ€èƒ½é›†ã€‚ç„¶è€Œï¼Œå½“å‰çš„LLMæ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç èƒ½åŠ›ä¸Šï¼Œåœ¨è¯„ä¼°æ›´å¹¿æ³›çš„æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å·®è·ã€‚ä¸€ä¸ªç‰¹åˆ«çš„ä¾‹å¤–æ˜¯BIG-Benchæ•°æ®é›†ï¼Œå®ƒå·²æˆä¸ºè¯„ä¼°LLMé€šç”¨æ¨ç†èƒ½åŠ›çš„é‡è¦åŸºå‡†ï¼Œå¾—ç›Šäºå…¶å¤šæ ·åŒ–çš„æŒ‘æˆ˜ä»»åŠ¡é›†ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¡†æ¶å†…å…¨é¢è¯„ä¼°è·¨å„ç§æŠ€èƒ½çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMçš„æœ€æ–°è¿›å±•å¯¼è‡´åœ¨BIG-BenchåŠå…¶æ›´é«˜çº§ç‰ˆæœ¬BIG-Bench Hard (BBH)ä¸Šçš„é¥±å’Œã€‚æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨BBHçš„è®¸å¤šä»»åŠ¡ä¸Šå–å¾—äº†è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œä»è€Œé™ä½äº†å…¶å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BIG-Bench Extra Hardï¼ˆBBEHï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨çªç ´LLMæ¨ç†è¯„ä¼°çš„ç•Œé™ã€‚BBEHç”¨æ–°å‹ä»»åŠ¡æ›¿æ¢BBHä¸­çš„æ¯ä¸ªä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ¢æŸ¥ç›¸ä¼¼çš„æ¨ç†èƒ½åŠ›ï¼Œä½†éš¾åº¦æ˜¾è‘—å¢åŠ ã€‚æˆ‘ä»¬åœ¨BBEHä¸Šè¯„ä¼°äº†å„ç§æ¨¡å‹ï¼Œè§‚å¯Ÿåˆ°æœ€ä½³é€šç”¨æ¨¡å‹çš„ï¼ˆè°ƒå’Œï¼‰å¹³å‡å‡†ç¡®ç‡ä¸º9.8%ï¼Œæœ€ä½³æ¨ç†ä¸“ç”¨æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ä¸º44.8%ï¼Œè¿™è¡¨æ˜æœ‰å·¨å¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå¹¶çªå‡ºäº†åœ¨LLMä¸­å®ç°ç¨³å¥é€šç”¨æ¨ç†çš„å½“å‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†BBEHï¼š<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/bbeh%E3%80%82">https://github.com/google-deepmind/bbehã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19187v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­è¶Šæ¥è¶Šå¹¿æ³›ï¼Œéœ€è¦å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›å’Œå¤šæ ·åŒ–çš„æŠ€èƒ½é›†ã€‚å½“å‰LLMæ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç èƒ½åŠ›ä¸Šï¼Œç¼ºä¹å¯¹æ›´å¹¿æ³›æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚BIG-Benchæ•°æ®é›†æ˜¯ä¸€ä¸ªé‡è¦çš„ä¾‹å¤–ï¼Œå®ƒå·²æˆä¸ºè¯„ä¼°LLMé€šç”¨æ¨ç†èƒ½åŠ›çš„é‡è¦åŸºå‡†ï¼ŒåŒ…å«å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°å„ç§æŠ€èƒ½çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€LLMçš„è¿‘æœŸè¿›å±•ï¼ŒBIG-BenchåŠå…¶æ›´éš¾ç‰ˆæœ¬BBHå·²è¶‹äºé¥±å’Œã€‚æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨BBHçš„è®¸å¤šä»»åŠ¡ä¸Šå–å¾—äº†è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œé™ä½äº†å…¶æ•ˆç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BIG-Bench Extra Hardï¼ˆBBEHï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨åŠ¨LLMæ¨ç†è¯„ä¼°çš„ç•Œé™ã€‚BBEHç”¨ç±»ä¼¼ä½†éš¾åº¦æ›´å¤§çš„æ–°ä»»åŠ¡æ›¿æ¢äº†BBHä¸­çš„æ¯ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æœ€å¥½çš„é€šç”¨æ¨¡å‹çš„å¹³å‡å‡†ç¡®åº¦ä¸º9.8ï¼…ï¼Œæœ€å¥½çš„æ¨ç†ä¸“ä¸šæ¨¡å‹çš„å¹³å‡å‡†ç¡®åº¦ä¸º44.8ï¼…ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ”¹è¿›ç©ºé—´å¹¶å‡¸æ˜¾äº†å®ç°ç¨³å¥çš„é€šç”¨æ¨ç†åœ¨LLMä¸­çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†BBEHï¼š<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/bbeh%E3%80%82">https://github.com/google-deepmind/bbehã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨æ—¥å¸¸åº”ç”¨ä¸­éœ€æ±‚å¼ºå¤§å’Œå¤šæ ·åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰LLMæ¨ç†åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ•°å­¦å’Œç¼–ç èƒ½åŠ›ä¸Šï¼Œéœ€è¦æ›´å…¨é¢çš„è¯„ä¼°ã€‚</li>
<li>BIG-Benchå·²æˆä¸ºè¯„ä¼°LLMé€šç”¨æ¨ç†èƒ½åŠ›çš„é‡è¦åŸºå‡†ã€‚</li>
<li>éšç€LLMçš„è¿›å±•ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¦‚BIG-Benchå’ŒBBHå·²è¶‹äºé¥±å’Œã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è¿‘ä¹å®Œç¾ï¼Œå‡¸æ˜¾äº†æ–°æŒ‘æˆ˜çš„å¿…è¦æ€§ã€‚</li>
<li>æ¨å‡ºæ–°çš„åŸºå‡†æµ‹è¯•BBEHï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e571303080952684f3d005ae5d8a432c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6680889353916fcbdaffa8beff0608cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef566b0d4946994dad0aa6f3e2b63a89.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-245e2d5d70913e83da6eecd733b7086f.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Multi-Agent System for Comprehensive Soccer Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-48da9301a867503c4e0f83f23e839187.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Rational Retrieval Acts Leveraging Pragmatic Reasoning to Improve   Sparse Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
