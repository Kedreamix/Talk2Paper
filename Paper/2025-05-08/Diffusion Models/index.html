<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Distribution-Conditional Generation From Class Distribution to Creative   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6322316e6e30e464c5f9d1d1bd99ea61.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="Distribution-Conditional-Generation-From-Class-Distribution-to-Creative-Generation"><a href="#Distribution-Conditional-Generation-From-Class-Distribution-to-Creative-Generation" class="headerlink" title="Distribution-Conditional Generation: From Class Distribution to Creative   Generation"></a>Distribution-Conditional Generation: From Class Distribution to Creative   Generation</h2><p><strong>Authors:Fu Feng, Yucheng Xie, Xu Yang, Jing Wang, Xin Geng</strong></p>
<p>Text-to-image (T2I) diffusion models are effective at producing semantically aligned images, but their reliance on training data distributions limits their ability to synthesize truly novel, out-of-distribution concepts. Existing methods typically enhance creativity by combining pairs of known concepts, yielding compositions that, while out-of-distribution, remain linguistically describable and bounded within the existing semantic space. Inspired by the soft probabilistic outputs of classifiers on ambiguous inputs, we propose Distribution-Conditional Generation, a novel formulation that models creativity as image synthesis conditioned on class distributions, enabling semantically unconstrained creative generation. Building on this, we propose DisTok, an encoder-decoder framework that maps class distributions into a latent space and decodes them into tokens of creative concept. DisTok maintains a dynamic concept pool and iteratively sampling and fusing concept pairs, enabling the generation of tokens aligned with increasingly complex class distributions. To enforce distributional consistency, latent vectors sampled from a Gaussian prior are decoded into tokens and rendered into images, whose class distributions-predicted by a vision-language model-supervise the alignment between input distributions and the visual semantics of generated tokens. The resulting tokens are added to the concept pool for subsequent composition. Extensive experiments demonstrate that DisTok, by unifying distribution-conditioned fusion and sampling-based synthesis, enables efficient and flexible token-level generation, achieving state-of-the-art performance with superior text-image alignment and human preference scores. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¯­ä¹‰å¯¹é½çš„å›¾åƒæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ä¾èµ–é™åˆ¶äº†å…¶åˆæˆçœŸæ­£æ–°é¢–ã€è¶…å‡ºåˆ†å¸ƒæ¦‚å¿µçš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡ç»“åˆå·²çŸ¥æ¦‚å¿µå¯¹æ¥å¢å¼ºåˆ›é€ åŠ›ï¼Œä»è€Œäº§ç”Ÿè¶…å‡ºåˆ†å¸ƒä½†ä»ç„¶æ˜¯è¯­è¨€å¯æè¿°ä¸”å­˜åœ¨äºç°æœ‰è¯­ä¹‰ç©ºé—´å†…çš„ç»„åˆã€‚æˆ‘ä»¬å—åˆ°åˆ†ç±»å™¨åœ¨æ¨¡ç³Šè¾“å…¥ä¸Šçš„è½¯æ¦‚ç‡è¾“å‡ºçš„å¯å‘ï¼Œæå‡ºäº†åˆ†å¸ƒæ¡ä»¶ç”Ÿæˆï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¨¡å‹åˆ›é€ åŠ›çš„æ–¹æ³•ï¼Œå³å°†å›¾åƒåˆæˆæ¡ä»¶åŒ–ä¸ºç±»åˆ†å¸ƒï¼Œä»è€Œå®ç°è¯­ä¹‰ä¸Šæ— çº¦æŸçš„åˆ›é€ æ€§ç”Ÿæˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†DisTokï¼Œè¿™æ˜¯ä¸€ç§ç¼–ç å™¨è§£ç å™¨æ¡†æ¶ï¼Œå®ƒå°†ç±»åˆ†å¸ƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´å¹¶è§£ç ä¸ºåˆ›é€ æ€§æ¦‚å¿µçš„æ ‡è®°ã€‚DisTokç»´æŠ¤ä¸€ä¸ªåŠ¨æ€æ¦‚å¿µæ± ï¼Œå¹¶è¿­ä»£é‡‡æ ·å’Œèåˆæ¦‚å¿µå¯¹ï¼Œä»¥ç”Ÿæˆä¸æ—¥ç›Šå¤æ‚çš„ç±»åˆ†å¸ƒå¯¹é½çš„æ ‡è®°ã€‚ä¸ºäº†å¼ºåˆ¶å®æ–½åˆ†å¸ƒä¸€è‡´æ€§ï¼Œä»é«˜æ–¯å…ˆéªŒä¸­é‡‡æ ·çš„æ½œåœ¨å‘é‡è¢«è§£ç ä¸ºæ ‡è®°å¹¶å‘ˆç°ä¸ºå›¾åƒï¼Œå…¶ç±»åˆ†å¸ƒç”±è§†è§‰è¯­è¨€æ¨¡å‹é¢„æµ‹ï¼Œç›‘ç£è¾“å…¥åˆ†å¸ƒä¸ç”Ÿæˆæ ‡è®°çš„è§†è§‰è¯­ä¹‰ä¹‹é—´çš„å¯¹é½ã€‚ç”Ÿæˆçš„æ ‡è®°è¢«æ·»åŠ åˆ°æ¦‚å¿µæ± ä¸­ä»¥ä¾›åç»­ç»„åˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDisToké€šè¿‡ç»Ÿä¸€åˆ†å¸ƒæ¡ä»¶ä¸‹çš„èåˆå’ŒåŸºäºé‡‡æ ·çš„åˆæˆï¼Œå®ç°äº†é«˜æ•ˆçµæ´»çš„æ ‡è®°çº§åˆ«ç”Ÿæˆï¼Œä»¥å“è¶Šçš„æ€§èƒ½å®ç°äº†æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„å¯¹é½å’Œäººç±»åå¥½å¾—åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¯­ä¹‰å¯¹é½çš„å›¾åƒæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å…¶å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ä¾èµ–é™åˆ¶äº†å…¶åˆæˆçœŸæ­£æ–°é¢–ã€è¶…å‡ºåˆ†å¸ƒæ¦‚å¿µçš„èƒ½åŠ›ã€‚æœ¬æ–‡å—åˆ†ç±»å™¨å¯¹æ¨¡ç³Šè¾“å…¥è½¯æ¦‚ç‡è¾“å‡ºçš„å¯å‘ï¼Œæå‡ºäº†åˆ†å¸ƒæ¡ä»¶ç”Ÿæˆè¿™ä¸€æ–°æ¨¡å‹ï¼Œå°†åˆ›é€ åŠ›è§†ä¸ºåŸºäºç±»åˆ«åˆ†å¸ƒçš„å›¾åƒåˆæˆæ¡ä»¶ï¼Œä»è€Œå®ç°è¯­ä¹‰ä¸Šæ— çº¦æŸçš„åˆ›é€ æ€§ç”Ÿæˆã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº†DisTokï¼Œä¸€ç§ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œå°†ç±»åˆ«åˆ†å¸ƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´å¹¶è§£ç ä¸ºåˆ›é€ æ€§æ¦‚å¿µçš„æ ‡è®°ã€‚DisTokç»´æŒä¸€ä¸ªåŠ¨æ€æ¦‚å¿µæ± ï¼Œé€šè¿‡è¿­ä»£é‡‡æ ·å’Œèåˆæ¦‚å¿µå¯¹ï¼Œç”Ÿæˆä¸å¤æ‚ç±»åˆ«åˆ†å¸ƒå¯¹é½çš„æ ‡è®°ã€‚é€šè¿‡ä»é«˜æ–¯å…ˆéªŒä¸­é‡‡æ ·æ½œåœ¨å‘é‡ï¼Œå¹¶å°†å…¶è§£ç ä¸ºæ ‡è®°å’Œå›¾åƒï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹é¢„æµ‹çš„ç±»åˆ«åˆ†å¸ƒæ¥ç›‘ç£è¾“å…¥åˆ†å¸ƒä¸ç”Ÿæˆæ ‡è®°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ç”Ÿæˆçš„æ ‡è®°è¢«æ·»åŠ åˆ°æ¦‚å¿µæ± ä¸­ç”¨äºåç»­ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼ŒDisToké€šè¿‡ç»Ÿä¸€åˆ†å¸ƒæ¡ä»¶ä¸‹çš„èåˆå’ŒåŸºäºé‡‡æ ·çš„åˆæˆï¼Œå®ç°äº†é«˜æ•ˆä¸”çµæ´»çš„æ ‡è®°çº§åˆ«ç”Ÿæˆï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å‡ºè‰²çš„æ–‡æœ¬-å›¾åƒå¯¹é½å’Œæ›´é«˜çš„äººç±»åå¥½è¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹è™½èƒ½æœ‰æ•ˆç”Ÿæˆè¯­ä¹‰å¯¹é½å›¾åƒï¼Œä½†å—é™äºè®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œéš¾ä»¥åˆæˆçœŸæ­£æ–°é¢–ã€è¶…å‡ºåˆ†å¸ƒçš„æ¦‚å¿µã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åˆ†å¸ƒæ¡ä»¶ç”Ÿæˆçš„æ–°æ¨¡å‹ï¼Œä»¥ç±»åˆ«åˆ†å¸ƒä¸ºæ¡ä»¶è¿›è¡Œå›¾åƒåˆæˆï¼Œæ¨åŠ¨è¯­ä¹‰æ— çº¦æŸçš„åˆ›é€ æ€§ç”Ÿæˆã€‚</li>
<li>DisTokä½œä¸ºä¸€ç§ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œèƒ½å°†ç±»åˆ«åˆ†å¸ƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´å¹¶è§£ç ä¸ºåˆ›é€ æ€§æ¦‚å¿µçš„æ ‡è®°ã€‚</li>
<li>DisToké€šè¿‡åŠ¨æ€æ¦‚å¿µæ± å®ç°è¿­ä»£é‡‡æ ·å’Œæ¦‚å¿µå¯¹èåˆï¼Œç”Ÿæˆä¸å¤æ‚ç±»åˆ«åˆ†å¸ƒå¯¹é½çš„æ ‡è®°ã€‚</li>
<li>é€šè¿‡ä»é«˜æ–¯å…ˆéªŒé‡‡æ ·æ½œåœ¨å‘é‡å¹¶è§£ç ä¸ºæ ‡è®°å’Œå›¾åƒï¼ŒDisTokåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»åˆ«åˆ†å¸ƒé¢„æµ‹æ¥ç›‘ç£è¾“å…¥åˆ†å¸ƒä¸ç”Ÿæˆæ ‡è®°ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>å®éªŒè¡¨æ˜DisTokåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ce0c648165bac304f7a83ec2f9ad33f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-702822e90d86e8ea313bccb8dee8981e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3e22280b2ffdb25e22012b4e21c85eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-549eb8fb30c208268badb22791177b12.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Phenotype-Guided-Generative-Model-for-High-Fidelity-Cardiac-MRI-Synthesis-Advancing-Pretraining-and-Clinical-Applications"><a href="#Phenotype-Guided-Generative-Model-for-High-Fidelity-Cardiac-MRI-Synthesis-Advancing-Pretraining-and-Clinical-Applications" class="headerlink" title="Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI   Synthesis: Advancing Pretraining and Clinical Applications"></a>Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI   Synthesis: Advancing Pretraining and Clinical Applications</h2><p><strong>Authors:Ziyu Li, Yujian Hu, Zhengyao Ding, Yiheng Mao, Haitao Li, Fan Yi, Hongkun Zhang, Zhengxing Huang</strong></p>
<p>Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. However, the limited availability of large-scale, high-quality CMR datasets poses a major challenge to the effective application of artificial intelligence (AI) in this domain. Even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of AI models on downstream tasks. In this study, we present Cardiac Phenotype-Guided CMR Generation (CPGG), a novel approach for generating diverse CMR data that covers a wide spectrum of cardiac health status. The CPGG framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from CMR data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity CMR cine sequences that capture both structural and functional features of the heart in a fine-grained manner. We synthesized a massive amount of CMR to expand the pretraining data. Experimental results show that CPGG generates high-quality synthetic CMR data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. These gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. Code is availabel at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CPGG">https://anonymous.4open.science/r/CPGG</a>. </p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒæ˜¯ä¸€ç§ç”¨äºè¯Šæ–­å¿ƒè„ç—…å’Œè¯„ä¼°å¿ƒè„å¥åº·çš„é‡è¦æ— åˆ›å·¥å…·ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡é«˜è´¨é‡CMRæ•°æ®é›†çš„æœ‰é™å¯ç”¨æ€§ç»™äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨è¯¥é¢†åŸŸçš„æœ‰æ•ˆåº”ç”¨å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å³ä½¿æ˜¯æ— æ ‡ç­¾æ•°æ®çš„æ•°é‡å’Œå®ƒæ‰€æ¶µç›–çš„å¥åº·çŠ¶å†µä¹Ÿéš¾ä»¥æ»¡è¶³æ¨¡å‹é¢„è®­ç»ƒçš„éœ€æ±‚ï¼Œè¿™é˜»ç¢äº†AIæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Cardiac Phenotype-Guided CMR Generationï¼ˆCPGGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¤šæ ·åŒ–CMRæ•°æ®çš„æ–°æ–¹æ³•ï¼Œæ¶µç›–äº†å¹¿æ³›çš„å¿ƒè„å¥åº·çŠ¶å†µã€‚CPGGæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨ä»CMRæ•°æ®ä¸­å¾—å‡ºçš„å¿ƒè„è¡¨å‹æ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºè¿™äº›è¡¨å‹çš„æ©ç è‡ªå›å½’æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„CMRç”µå½±åºåˆ—ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼æ•æ‰å¿ƒè„çš„ç»“æ„å’ŒåŠŸèƒ½ç‰¹å¾ã€‚æˆ‘ä»¬åˆæˆäº†å¤§é‡çš„CMRæ•°æ®æ¥æ‰©å±•é¢„è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCPGGç”Ÿæˆçš„é«˜è´¨é‡åˆæˆCMRæ•°æ®æ˜¾è‘—æé«˜äº†å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¯Šæ–­å’Œå¿ƒè„è¡¨å‹é¢„æµ‹ï¼‰çš„æ€§èƒ½ã€‚è¿™äº›æ”¶ç›Šåœ¨å…¬å…±å’Œç§æœ‰æ•°æ®é›†ä¸Šéƒ½å¾—åˆ°äº†è¯æ˜ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CPGG%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/CPGGè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03426v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€é¡¹é‡è¦ç ”ç©¶ï¼Œè¯¥ç ”ç©¶è§£å†³äº†å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆCMRï¼‰æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé™åˆ¶äº†äººå·¥æ™ºèƒ½åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ã€‚è¯¥ç ”ç©¶æå‡ºäº†å¿ƒè„è¡¨å‹å¼•å¯¼CMRç”Ÿæˆï¼ˆCPGGï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæ¶µç›–å¹¿æ³›å¿ƒè„å¥åº·çŠ¶æ€çš„å¤šæ ·çš„CMRæ•°æ®æ¥æ‰©å±•é¢„è®­ç»ƒæ•°æ®ã€‚CPGGæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆåˆ©ç”¨ä»CMRæ•°æ®ä¸­å¾—å‡ºçš„å¿ƒè„è¡¨å‹è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œç„¶åä½¿ç”¨æ©æ¨¡è‡ªå›å½’æ‰©æ•£æ¨¡å‹åŸºäºè¿™äº›è¡¨å‹ç”Ÿæˆé«˜è´¨é‡çš„CMRç”µå½±åºåˆ—ã€‚æ­¤æ–¹æ³•èƒ½ç²¾ç»†æ•æ‰å¿ƒè„çš„ç»“æ„å’ŒåŠŸèƒ½ç‰¹å¾ï¼Œå¹¶åˆæˆå¤§é‡CMRæ•°æ®ä»¥æ‰©å±•é¢„è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCPGGç”Ÿæˆçš„åˆæˆCMRæ•°æ®è´¨é‡é«˜ï¼Œèƒ½æ˜¾è‘—æé«˜è¯Šæ–­å’Œå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¿ƒè„è¡¨å‹é¢„æµ‹ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMRæˆåƒåœ¨å¿ƒè„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è¯„ä¼°ä¸­è‡³å…³é‡è¦ï¼Œä½†å¤§è§„æ¨¡é«˜è´¨é‡CMRæ•°æ®çš„æœ‰é™å¯ç”¨æ€§é™åˆ¶äº†äººå·¥æ™ºèƒ½åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¿ƒè„è¡¨å‹å¼•å¯¼CMRç”Ÿæˆï¼ˆCPGGï¼‰ï¼Œä»¥ç”Ÿæˆæ¶µç›–å¹¿æ³›å¿ƒè„å¥åº·çŠ¶æ€çš„å¤šæ ·åŒ–CMRæ•°æ®ã€‚</li>
<li>CPGGæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨å¿ƒè„è¡¨å‹è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œç„¶åä½¿ç”¨æ©æ¨¡è‡ªå›å½’æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„CMRç”µå½±åºåˆ—ã€‚</li>
<li>åˆæˆå¤§é‡CMRæ•°æ®ä»¥æ‰©å±•é¢„è®­ç»ƒæ•°æ®ï¼Œæ˜¾è‘—æé«˜è¯Šæ–­å’Œå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¿ƒè„è¡¨å‹é¢„æµ‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCPGGç”Ÿæˆçš„åˆæˆCMRæ•°æ®è´¨é‡é«˜ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½æ•æ‰å¿ƒè„çš„ç»“æ„å’ŒåŠŸèƒ½ç‰¹å¾çš„ç»†èŠ‚ï¼Œè¿™æ˜¯å…¶æˆåŠŸçš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33ac1644980055748e38f46c471d59c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0eb2d8a78915db4f531e0996286aed89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11f07c2324d6ce19686326ef4baf0d30.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="FLUX-Text-A-Simple-and-Advanced-Diffusion-Transformer-Baseline-for-Scene-Text-Editing"><a href="#FLUX-Text-A-Simple-and-Advanced-Diffusion-Transformer-Baseline-for-Scene-Text-Editing" class="headerlink" title="FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for   Scene Text Editing"></a>FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for   Scene Text Editing</h2><p><strong>Authors:Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Lei Sun, Xiangxiang Chu</strong></p>
<p>The task of scene text editing is to modify or add texts on images while maintaining the fidelity of newly generated text and visual coherence with the background. Recent works based on latent diffusion models (LDM) show improved text editing results, yet still face challenges and often generate inaccurate or unrecognizable characters, especially for non-Latin ones (\eg, Chinese), which have complex glyph structures. To address these issues, we present FLUX-Text, a simple and advanced multilingual scene text editing framework based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning, considering both visual and textual modalities. To retain the original generative capabilities of FLUX-Fill while enhancing its understanding and generation of glyphs, we propose lightweight glyph and text embedding modules. Owning to the lightweight design, FLUX-Text is trained only with $100K$ training examples compared to current popular methods trained with 2.9M ones. With no bells and whistles, our method achieves state-of-the-art performance on text editing tasks. Qualitative and quantitative experiments on the public datasets demonstrate that our method surpasses previous works in text fidelity. </p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘çš„ä»»åŠ¡æ˜¯åœ¨ä¿æŒæ–°ç”Ÿæˆæ–‡æœ¬çš„ä¿çœŸåº¦å’Œä¸èƒŒæ™¯è§†è§‰è¿è´¯æ€§çš„åŒæ—¶ï¼Œå¯¹å›¾åƒä¸Šçš„æ–‡æœ¬è¿›è¡Œä¿®æ”¹æˆ–æ·»åŠ ã€‚åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„è¿‘æœŸå·¥ä½œåœ¨æ–‡æœ¬ç¼–è¾‘ç»“æœæ–¹é¢æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”ç»å¸¸ç”Ÿæˆä¸å‡†ç¡®æˆ–ä¸å¯è¯†åˆ«çš„å­—ç¬¦ï¼Œç‰¹åˆ«æ˜¯éæ‹‰ä¸å­—ç¬¦ï¼ˆä¾‹å¦‚ä¸­æ–‡ï¼‰ï¼Œè¿™æ˜¯ç”±äºä¸­æ–‡ç­‰å­—ç¬¦å…·æœ‰å¤æ‚çš„å­—å½¢ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºFLUX-Fillçš„ç®€æ´å…ˆè¿›çš„å¤šè¯­è¨€åœºæ™¯æ–‡æœ¬ç¼–è¾‘æ¡†æ¶FLUX-Textã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»”ç»†ç ”ç©¶äº†å­—å½¢æ¡ä»¶ï¼Œè€ƒè™‘äº†è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ã€‚ä¸ºäº†ä¿ç•™FLUX-Fillçš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶æé«˜å…¶å­—å½¢ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§å­—å½¢å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—ã€‚ç”±äºè®¾è®¡è½»é‡çº§ï¼ŒFLUX-Textä»…ä½¿ç”¨100Kè®­ç»ƒæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè€Œå½“å‰æµè¡Œçš„æ–¹æ³•åˆ™ä½¿ç”¨290ä¸‡è®­ç»ƒæ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ²¡æœ‰é¢å¤–ä¿®é¥°çš„æƒ…å†µä¸‹ï¼Œåœ¨æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–‡æœ¬ä¿çœŸåº¦æ–¹é¢è¶…è¶Šäº†ä»¥å‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03329v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>     åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„FLUX-Textåœºæ™¯æ–‡æœ¬ç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡è½»é‡çº§å­—å½¢å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—ï¼Œæé«˜äº†å¯¹éæ‹‰ä¸å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰çš„è¯†åˆ«ä¸ç”Ÿæˆèƒ½åŠ›ï¼Œè®­ç»ƒæ ·æœ¬æ•°é‡å¤§å¹…å‡å°‘è‡³ä»…10ä¸‡ï¼Œå®ç°äº†é«˜æ•ˆçš„å¤šè¯­è¨€æ–‡æœ¬ç¼–è¾‘æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è¿›è¡Œåœºæ™¯æ–‡æœ¬ç¼–è¾‘ã€‚</li>
<li>é¢ä¸´å¯¹éæ‹‰ä¸å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰çš„å¤æ‚å­—å½¢ç»“æ„å¤„ç†æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºFLUX-Textæ¡†æ¶ï¼Œç»“åˆFLUX-FillæŠ€æœ¯å¢å¼ºå­—å½¢å¤„ç†å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§å­—å½¢å’Œæ–‡æœ¬åµŒå…¥æ¨¡å—æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ä»…ç”¨10ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œç›¸è¾ƒäºå½“å‰æµè¡Œæ–¹æ³•å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFLUX-Textåœ¨æ–‡æœ¬ä¿çœŸåº¦ä¸Šè¶…è¶Šå…ˆå‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9629f378d674181adbaa65c4a576989.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1e98f52303edeb8ccba4e46ff8fb7b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f355f7aaac3cd13e5ec2d694a4dbd0cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad63bdeab48fcd0860410353ee292bca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbd1a330b7e4a8dd2992b9ad05efe16b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mamba-Diffusion-Model-with-Learnable-Wavelet-for-Controllable-Symbolic-Music-Generation"><a href="#Mamba-Diffusion-Model-with-Learnable-Wavelet-for-Controllable-Symbolic-Music-Generation" class="headerlink" title="Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic   Music Generation"></a>Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic   Music Generation</h2><p><strong>Authors:Jincheng Zhang, GyÃ¶rgy Fazekas, Charalampos Saitis</strong></p>
<p>The recent surge in the popularity of diffusion models for image synthesis has attracted new attention to their potential for generation tasks in other domains. However, their applications to symbolic music generation remain largely under-explored because symbolic music is typically represented as sequences of discrete events and standard diffusion models are not well-suited for discrete data. We represent symbolic music as image-like pianorolls, facilitating the use of diffusion models for the generation of symbolic music. Moreover, this study introduces a novel diffusion model that incorporates our proposed Transformer-Mamba block and learnable wavelet transform. Classifier-free guidance is utilised to generate symbolic music with target chords. Our evaluation shows that our method achieves compelling results in terms of music quality and controllability, outperforming the strong baseline in pianoroll generation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jinchengzhanggg/proffusion">https://github.com/jinchengzhanggg/proffusion</a>. </p>
<blockquote>
<p>æœ€è¿‘æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­çš„æµè¡Œå¼•èµ·äº†äººä»¬å¯¹è¿™äº›æ¨¡å‹åœ¨å…¶ä»–é¢†åŸŸç”Ÿæˆä»»åŠ¡æ½œåŠ›çš„æ–°å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç¬¦å·éŸ³ä¹ç”Ÿæˆä¸­çš„åº”ç”¨ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†ï¼Œå› ä¸ºç¬¦å·éŸ³ä¹é€šå¸¸è¢«è¡¨ç¤ºä¸ºç¦»æ•£äº‹ä»¶çš„åºåˆ—ï¼Œè€Œæ ‡å‡†æ‰©æ•£æ¨¡å‹å¹¶ä¸é€‚åˆå¤„ç†ç¦»æ•£æ•°æ®ã€‚æˆ‘ä»¬å°†ç¬¦å·éŸ³ä¹è¡¨ç¤ºä¸ºç±»ä¼¼å›¾åƒçš„é’¢ç´å·ï¼Œä¾¿äºä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦å·éŸ³ä¹ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èå…¥äº†æˆ‘ä»¬æ‰€æå‡ºçš„Transformer-Mambaæ¨¡å—å’Œå¯å­¦ä¹ çš„å°æ³¢å˜æ¢ã€‚ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼æ¥ç”Ÿæˆå…·æœ‰ç›®æ ‡å’Œå¼¦çš„ç¬¦å·éŸ³ä¹ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³è´¨å’Œå¯æ§æ€§æ–¹é¢å–å¾—äº†ä»¤äººä¿¡æœçš„ç»“æœï¼Œåœ¨é’¢ç´å·ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jinchengzhanggg/proffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jinchengzhanggg/proffusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03314v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸçš„æ™®åŠå¼•èµ·äº†äººä»¬å¯¹å…¶åœ¨å…¶ä»–é¢†åŸŸç”Ÿæˆä»»åŠ¡çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºç¬¦å·éŸ³ä¹é€šå¸¸è¡¨ç¤ºä¸ºç¦»æ•£äº‹ä»¶çš„åºåˆ—ï¼Œæ ‡å‡†æ‰©æ•£æ¨¡å‹ä¸é€‚ç”¨äºç¦»æ•£æ•°æ®ï¼Œå› æ­¤ç¬¦å·éŸ³ä¹çš„ç”Ÿæˆåº”ç”¨ä»ç„¶è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶å°†ç¬¦å·éŸ³ä¹è¡¨ç¤ºä¸ºå›¾åƒå¼é’¢ç´å·ï¼ˆpianorollsï¼‰ï¼Œä¿ƒè¿›äº†æ‰©æ•£æ¨¡å‹åœ¨ç¬¦å·éŸ³ä¹ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬æå‡ºçš„Transformer-Mambaæ¨¡å—å’Œå­¦ä¹ å°æ³¢å˜æ¢ã€‚é€šè¿‡æ— åˆ†ç±»å™¨å¼•å¯¼ç”Ÿæˆç›®æ ‡å’Œå¼¦çš„ç¬¦å·éŸ³ä¹ã€‚è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨éŸ³ä¹è´¨é‡å’Œå¯æ§æ€§æ–¹é¢å–å¾—äº†ä»¤äººä¿¡æœçš„ç»“æœï¼Œåœ¨é’¢ç´å·ç”Ÿæˆæ–¹é¢ä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸçš„æ™®åŠä¿ƒä½¿äººä»¬å…³æ³¨å…¶åœ¨å…¶ä»–é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>ç¬¦å·éŸ³ä¹ç”Ÿæˆçš„åº”ç”¨ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä»ç„¶æœ‰é™ï¼Œå› ä¸ºç¬¦å·éŸ³ä¹è¡¨ç¤ºä¸ºç¦»æ•£äº‹ä»¶åºåˆ—ï¼Œæ ‡å‡†æ‰©æ•£æ¨¡å‹ä¸é€‚ç”¨ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å°†ç¬¦å·éŸ³ä¹è¡¨ç¤ºä¸ºå›¾åƒå¼é’¢ç´å·ï¼ˆpianorollsï¼‰ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨ç¬¦å·éŸ³ä¹ç”Ÿæˆä¸­çš„åº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒ…å«Transformer-Mambaæ¨¡å—å’Œå­¦ä¹ å°æ³¢å˜æ¢ã€‚</li>
<li>ä½¿ç”¨æ— åˆ†ç±»å™¨å¼•å¯¼ç”Ÿæˆç›®æ ‡å’Œå¼¦çš„ç¬¦å·éŸ³ä¹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨éŸ³ä¹è´¨é‡å’Œå¯æ§æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-05c75419e798148f5ca950721b4dbd42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a0db92b2911a7b5c7b0e5d7d6d8bb7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-482f5d15ff8238280fa4d4b7d9b707aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-883b9abf6d8bbc4274fb112215aa952e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0394375d1a4866f9a6e234f5cbf611f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cff41ae3b12291a97654e1de1a5faf9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-634f45ab9db9772c49cb4d4ad401186e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281db20c02e26372c53911755de13fcd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Not-All-Parameters-Matter-Masking-Diffusion-Models-for-Enhancing-Generation-Ability"><a href="#Not-All-Parameters-Matter-Masking-Diffusion-Models-for-Enhancing-Generation-Ability" class="headerlink" title="Not All Parameters Matter: Masking Diffusion Models for Enhancing   Generation Ability"></a>Not All Parameters Matter: Masking Diffusion Models for Enhancing   Generation Ability</h2><p><strong>Authors:Lei Wang, Senmao Li, Fei Yang, Jianye Wang, Ziheng Zhang, Yuhan Liu, Yaxing Wang, Jian Yang</strong></p>
<p>The diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages. Thus the same network layers are forced to learn both structural and textural information simultaneously, significantly differing from the traditional deep learning architectures (e.g., ResNet or GANs) which captures or generates the image semantic information at different layers. This difference inspires us to explore the time-wise diffusion models. We initially investigate the key contributions of the U-Net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. Capitalizing on this discovery, we propose a simple yet effective method-termed &#96;&#96;MaskUNetâ€™â€™- that enhances generation quality with negligible parameter numbers. Our method fully leverages timestep- and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions. In zero-shot inference on the COCO dataset, MaskUNet achieves the best FID score and further demonstrates its effectiveness in downstream task evaluations. Project page: <a target="_blank" rel="noopener" href="https://gudaochangsheng.github.io/MaskUnet-Page/">https://gudaochangsheng.github.io/MaskUnet-Page/</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ—©æœŸé˜¶æ®µä¸»è¦å…³æ³¨æ„å»ºåŸºæœ¬å›¾åƒç»“æ„ï¼Œè€Œåœ¨åæœŸé˜¶æ®µåˆ™ç”Ÿæˆç²¾ç»†çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å±€éƒ¨ç‰¹å¾å’Œçº¹ç†ã€‚å› æ­¤ï¼Œç›¸åŒçš„ç½‘ç»œå±‚è¢«è¿«åŒæ—¶å­¦ä¹ ç»“æ„å’Œçº¹ç†ä¿¡æ¯ï¼Œè¿™ä¸ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼ˆä¾‹å¦‚ResNetæˆ–GANsï¼‰æœ‰å¾ˆå¤§ä¸åŒï¼Œåè€…åœ¨ä¸åŒçš„å±‚æ•è·æˆ–ç”Ÿæˆå›¾åƒè¯­ä¹‰ä¿¡æ¯ã€‚è¿™ç§å·®å¼‚æ¿€å‘æˆ‘ä»¬æ¢ç´¢æ—¶é—´æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬æœ€åˆç ”ç©¶äº†U-Netå‚æ•°å¯¹å»å™ªè¿‡ç¨‹çš„å…³é”®ä½œç”¨ï¼Œå¹¶ç¡®å®šé€‚å½“åœ°å°†æŸäº›å‚æ•°ï¼ˆåŒ…æ‹¬å¤§å‚æ•°ï¼‰ç½®é›¶æœ‰åŠ©äºå»å™ªï¼Œä»è€Œå¤§å¤§æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚åˆ©ç”¨è¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºâ€œMaskUNetâ€ï¼Œå®ƒåœ¨å‚æ•°æ•°é‡å¾ˆå°‘çš„æƒ…å†µä¸‹æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å……åˆ†åˆ©ç”¨äº†æ—¶é—´æ­¥é•¿å’Œæ ·æœ¬ç›¸å…³çš„æœ‰æ•ˆU-Netå‚æ•°ã€‚ä¸ºäº†ä¼˜åŒ–MaskUNetï¼Œæˆ‘ä»¬æä¾›äº†ä¸¤ç§å¾®è°ƒç­–ç•¥ï¼šåŸºäºè®­ç»ƒçš„æ–¹æ³•å’Œæ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å®šåˆ¶çš„ç½‘ç»œå’Œä¼˜åŒ–å‡½æ•°ã€‚åœ¨COCOæ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬æ¨ç†æ—¶ï¼ŒMaskUNetè·å¾—äº†æœ€ä½³çš„FIDåˆ†æ•°ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ä¸­è¿›ä¸€æ­¥è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gudaochangsheng.github.io/MaskUnet-Page/">https://gudaochangsheng.github.io/MaskUnet-Page/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03097v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„å·¥ä½œæœºåˆ¶ï¼Œè¯¥æ¨¡å‹åœ¨åˆæœŸå…³æ³¨æ„å»ºåŸºæœ¬å›¾åƒç»“æ„ï¼ŒåæœŸç”Ÿæˆç²¾ç»†ç»†èŠ‚ï¼Œå¦‚å±€éƒ¨ç‰¹å¾å’Œçº¹ç†ã€‚ä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¶æ„ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹åœ¨åŒä¸€ç½‘ç»œå±‚ä¸­åŒæ—¶å­¦ä¹ ç»“æ„å’Œçº¹ç†ä¿¡æ¯ã€‚ç ”ç©¶å›¢é˜Ÿæ¢ç´¢äº†U-Netå‚æ•°å¯¹å»å™ªè¿‡ç¨‹çš„è´¡çŒ®ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œMaskUNetâ€çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æ—¶é—´æ­¥é•¿å’Œæ ·æœ¬ç›¸å…³çš„æœ‰æ•ˆU-Netå‚æ•°ï¼Œåœ¨ä¿æŒå‚æ•°æ•°é‡å¾ˆå°‘çš„åŒæ—¶æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œå›¢é˜Ÿè¿˜æä¾›äº†ä¸¤ç§å¾®è°ƒç­–ç•¥æ¥ä¼˜åŒ–MaskUNetçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åˆ†é˜¶æ®µæ„å»ºå›¾åƒç»“æ„å’Œçº¹ç†ä¿¡æ¯ï¼Œä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¶æ„ä¸åŒã€‚</li>
<li>MaskUNetæ–¹æ³•é€šè¿‡æœ‰æ•ˆåˆ©ç”¨æ—¶é—´æ­¥é•¿å’Œæ ·æœ¬ç›¸å…³çš„æœ‰æ•ˆU-Netå‚æ•°ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œæ­£ç¡®åœ°å°†æŸäº›å‚æ•°ï¼ˆåŒ…æ‹¬å¤§å‚æ•°ï¼‰ç½®é›¶æœ‰åŠ©äºå»å™ªã€‚</li>
<li>MaskUNetå®ç°äº†æœ€ä½³FIDåˆ†æ•°ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>MaskUNetæ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œå‚æ•°æ•°é‡å°‘ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæä¾›äº†ä¸¤ç§ä¼˜åŒ–MaskUNetçš„ç­–ç•¥ï¼šåŸºäºè®­ç»ƒçš„æ–¹æ³•å’Œæ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ecd55fca739cedff1cbeac50fa7a6e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc06c06f0f8cd606d44e2608c74c4ec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5031a75e9e6730fb0b96d5ff3388017.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c949ed1cc375e93ce531923a6faf5651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-277759e29441462deab25e1658836663.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Advancing-Generalizable-Tumor-Segmentation-with-Anomaly-Aware-Open-Vocabulary-Attention-Maps-and-Frozen-Foundation-Diffusion-Models"><a href="#Advancing-Generalizable-Tumor-Segmentation-with-Anomaly-Aware-Open-Vocabulary-Attention-Maps-and-Frozen-Foundation-Diffusion-Models" class="headerlink" title="Advancing Generalizable Tumor Segmentation with Anomaly-Aware   Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models"></a>Advancing Generalizable Tumor Segmentation with Anomaly-Aware   Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models</h2><p><strong>Authors:Yankai Jiang, Peng Zhang, Donglin Yang, Yuan Tian, Hai Lin, Xiaosong Wang</strong></p>
<p>We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/Yankai96/DiffuGTS">https://github.com/Yankai96/DiffuGTS</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†é€šç”¨è‚¿ç˜¤åˆ†å‰²æŠ€æœ¯ï¼Œæ—¨åœ¨é’ˆå¯¹å¤šä¸ªè§£å‰–å­¦åŒºåŸŸè®­ç»ƒå•ä¸€æ¨¡å‹æ¥å®ç°é›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸åˆ†å‰²è´¨é‡ã€å¯æ‰©å±•æ€§å’Œé€‚ç”¨çš„æˆåƒæ¨¡å¼èŒƒå›´ç›¸å…³çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥åä¸ºDiffuGTSçš„æ–°å‹æ¡†æ¶ï¼Œæ­ç¤ºäº†å†»ç»“åŒ»ç–—åŸºç¡€æ‰©æ•£æ¨¡å‹å†…éƒ¨è¡¨ç¤ºä½œä¸ºè‚¿ç˜¤åˆ†å‰²çš„é«˜æ•ˆé›¶æ ·æœ¬å­¦ä¹ è€…çš„æ½œåŠ›ã€‚DiffuGTSåŸºäºæ–‡æœ¬æç¤ºåˆ›å»ºå¼‚å¸¸æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œä»¥å®ç°å¯é€šç”¨çš„å¼‚å¸¸åˆ†å‰²ï¼Œè€Œä¸å—é¢„è®¾è®­ç»ƒç±»åˆ«åˆ—è¡¨çš„é™åˆ¶ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å’Œå®Œå–„å¼‚å¸¸åˆ†å‰²æ©è†œï¼ŒDiffuGTSåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´è¡¥å…¨æŠ€æœ¯å°†ç—…ç†åŒºåŸŸè½¬æ¢ä¸ºé«˜è´¨é‡ä¼ªå¥åº·å¯¹åº”ç‰©ï¼Œå¹¶åº”ç”¨æ–°é¢–çš„åƒç´ çº§å’Œç‰¹å¾çº§æ®‹å·®å­¦ä¹ æ–¹æ³•ï¼Œä»è€Œå¾—åˆ°è´¨é‡æ˜¾è‘—æé«˜ä¸”æ›´å…·é€šç”¨æ€§çš„åˆ†å‰²æ©è†œã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªè‚¿ç˜¤ç±»åˆ«ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½å“è¶Šï¼Œåœ¨å¤šä¸ªé›¶æ ·æœ¬è®¾ç½®ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yankai96/DiffuGTS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yankai96/DiffuGTSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02753v1">PDF</a> This paper is accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é€šç”¨è‚¿ç˜¤åˆ†å‰²æŠ€æœ¯ï¼Œæ—¨åœ¨ä½¿ç”¨å•ä¸€æ¨¡å‹å®ç°è·¨ä¸åŒè§£å‰–åŒºåŸŸçš„é›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶DiffuGTSï¼Œåˆ©ç”¨å†»ç»“çš„åŒ»ç–—åŸºç¡€æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºä½œä¸ºé›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²çš„é«˜æ•ˆå­¦ä¹ è€…ã€‚DiffuGTSåˆ›å»ºåŸºäºæ–‡æœ¬æç¤ºçš„å¼‚å¸¸æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œä»¥å®ç°å¯¹è‚¿ç˜¤çš„ä¸€èˆ¬æ€§åˆ†å‰²ï¼Œè€Œä¸å—é¢„è®¾è®­ç»ƒç±»åˆ«åˆ—è¡¨çš„é™åˆ¶ã€‚ä¸ºè¿›ä¸€æ­¥æ”¹å–„å’Œç»†åŒ–å¼‚å¸¸åˆ†å‰²æ©è†œï¼ŒDiffuGTSåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†ç—…ç†åŒºåŸŸè½¬åŒ–ä¸ºé«˜è´¨é‡çš„ä¼ªå¥åº·å¯¹åº”ç‰©ï¼Œå¹¶é€šè¿‡åƒç´ çº§å’Œç‰¹å¾çº§æ®‹å·®å­¦ä¹ çš„æ–¹æ³•åº”ç”¨ï¼Œå¾—åˆ°è´¨é‡æ›´é«˜ã€æ³›åŒ–æ€§æ›´å¼ºçš„åˆ†å‰²æ©è†œã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªè‚¿ç˜¤ç±»åˆ«çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œåœ¨å¤šç§é›¶æ ·æœ¬è®¾ç½®ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ç ”ç©¶äº†é€šç”¨è‚¿ç˜¤åˆ†å‰²æŠ€æœ¯ï¼Œç›®æ ‡æ˜¯ä½¿ç”¨å•ä¸€æ¨¡å‹åœ¨ä¸åŒè§£å‰–åŒºåŸŸè¿›è¡Œé›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶DiffuGTSï¼Œåˆ©ç”¨å†»ç»“çš„åŒ»ç–—åŸºç¡€æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜æ•ˆé›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²å­¦ä¹ ã€‚</li>
<li>DiffuGTSé€šè¿‡åˆ›å»ºåŸºäºæ–‡æœ¬æç¤ºçš„å¼‚å¸¸æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œå®ç°è‚¿ç˜¤çš„ä¸€èˆ¬æ€§åˆ†å‰²ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†ç—…ç†åŒºåŸŸè½¬åŒ–ä¸ºé«˜è´¨é‡çš„ä¼ªå¥åº·å¯¹åº”ç‰©ï¼Œæé«˜äº†å¼‚å¸¸åˆ†å‰²æ©è†œçš„è´¨é‡å’Œæ³›åŒ–æ€§ã€‚</li>
<li>é‡‡ç”¨äº†åƒç´ çº§å’Œç‰¹å¾çº§çš„æ®‹å·®å­¦ä¹ æ–¹æ³•æ¥è¿›ä¸€æ­¥ä¼˜åŒ–åˆ†å‰²æ•ˆæœã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†å’Œè‚¿ç˜¤ç±»åˆ«ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffuGTSæ€§èƒ½ä¼˜è¶Šï¼Œæ˜¾è‘—è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53269bd49807988eaca1480fa1faa914.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bc142f57862bad438e25cb705152d88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfb51bbacab5c7e7c1e78741e825adfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d40b0c0863e63104ef6fa38465efe4d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a279ad190cf1e00c4bc731284e52e69d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ming-Lite-Uni-Advancements-in-Unified-Architecture-for-Natural-Multimodal-Interaction"><a href="#Ming-Lite-Uni-Advancements-in-Unified-Architecture-for-Natural-Multimodal-Interaction" class="headerlink" title="Ming-Lite-Uni: Advancements in Unified Architecture for Natural   Multimodal Interaction"></a>Ming-Lite-Uni: Advancements in Unified Architecture for Natural   Multimodal Interaction</h2><p><strong>Authors:Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang</strong></p>
<p>We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»Ming-Lite-Uniï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œå…·æœ‰æ–°è®¾è®¡çš„ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨å’Œé’ˆå¯¹è§†è§‰å’Œè¯­è¨€çš„èåˆä»»åŠ¡é‡èº«å®šåˆ¶çš„æœ¬åœ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥é¡¹ç›®æä¾›äº†é›†æˆMetaQuerieså’ŒM2-omniæ¡†æ¶çš„å¼€æºå®ç°ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°å‹çš„å¤šå°ºåº¦å¯å­¦ä¹ ä»¤ç‰Œå’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨å›ºå®šçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼ŒMing-Lite-Uniä½¿æœ¬åœ°å¤šæ¨¡å¼ARæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œæ‰©å±•äº†å…¶è¶…è¶Šçº¯è§†è§‰ç†è§£çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯æ˜äº†Ming-Lite-Uniçš„å¼ºå¤§æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶äº¤äº’è¿‡ç¨‹çš„æµç•…æ€§ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡éƒ½å·²å¼€æºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™é¡¹å·¥ä½œä¸åŒæœŸå‘å¸ƒçš„å¤šæ¨¡å¼äººå·¥æ™ºèƒ½é‡Œç¨‹ç¢‘äº‹ä»¶ç›¸ä¸€è‡´ï¼Œå¦‚äºä¸‰æœˆäºŒåäº”æ—¥æ›´æ–°çš„å…·æœ‰æœ¬åœ°å›¾åƒç”Ÿæˆçš„ChatGPT-4oç­‰ï¼Œçªæ˜¾å‡ºåƒMing-Lite-Uniè¿™æ ·çš„ç»Ÿä¸€æ¨¡å‹åœ¨é€šå¾€äººå·¥æ™ºèƒ½é€šç”¨åŒ–é“è·¯ä¸Šçš„é‡è¦æ€§ã€‚Ming-Lite-Uniç›®å‰å¤„äºAlphaé˜¶æ®µï¼Œå¹¶å³å°†è¿›è¡Œè¿›ä¸€æ­¥çš„å®Œå–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02471v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/Ming/tree/main/Ming-unify">https://github.com/inclusionAI/Ming/tree/main/Ming-unify</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Ming-Lite-Uniè¿™ä¸€å¼€æºå¤šæ¨¡æ€æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹ä¸ºå…¨æ–°è®¾è®¡çš„ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨ä»¥åŠé’ˆå¯¹è§†è§‰å’Œè¯­è¨€ç»Ÿä¸€åŒ–çš„æœ¬åœ°å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ã€‚è¯¥é¡¹ç›®å®ç°äº†é›†æˆMetaQuerieså’ŒM2-omniæ¡†æ¶çš„å¼€æºå®ç°ï¼Œå¹¶å¼•å…¥æ–°å‹å¤šå°ºåº¦å¯å­¦ä¹ ä»¤ç‰Œå’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚Ming-Lite-Uniåˆ©ç”¨å›ºå®šçš„MLLMå’Œå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿æœ¬åœ°å¤šæ¨¡æ€ARæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œè¶…è¶Šäº†çº¯è§†è§‰ç†è§£çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºå…¶å¼ºå¤§æ€§èƒ½ï¼Œå¹¶ä¸”äº¤äº’è¿‡ç¨‹æµç•…ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡å‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚è¯¥å·¥ä½œä¸å½“å‰çš„å¤šåª’ä½“äººå·¥æ™ºèƒ½é‡Œç¨‹ç¢‘ï¼ˆå¦‚ChatGPT-4oï¼‰ä¿æŒä¸€è‡´ï¼Œæ˜¾ç¤ºå‡ºç»Ÿä¸€æ¨¡å‹åœ¨è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚ç›®å‰Ming-Lite-Uniä»å¤„äºAlphaé˜¶æ®µï¼Œæœªæ¥ä¼šè¿›ä¸€æ­¥è¿›è¡Œä¼˜åŒ–æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ming-Lite-Uniæ˜¯ä¸€ä¸ªå¼€æºå¤šæ¨¡æ€æ¡†æ¶ï¼Œé›†æˆäº†è§†è§‰å’Œè¯­è¨€å¤„ç†ã€‚</li>
<li>å®ƒåŒ…å«ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨å’Œæœ¬åœ°å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>è¯¥é¡¹ç›®å®ç°äº†MetaQuerieså’ŒM2-omniæ¡†æ¶çš„é›†æˆã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦å¯å­¦ä¹ ä»¤ç‰Œå’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ä¸ºæ–°é¢–ç‰¹ç‚¹ã€‚</li>
<li>Ming-Lite-Unièƒ½æ‰§è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶ä¸”äº¤äº’è¿‡ç¨‹éå¸¸æµç•…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea5b47a911d63af96e2bf282bc2e51ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-161e89071d69ff9ea19ef921d96c9afe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4250c4cfea42b43946ce25636b1e44a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a12669519da0fb4b4b9ba0b26683f33d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29c7288bc0de2e28b2c88cd12d144e21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d019031236945e165e3541d7f4f6b1a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d2bba3d34902481f35e1b20940b3e5c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Regression-is-all-you-need-for-medical-image-translation"><a href="#Regression-is-all-you-need-for-medical-image-translation" class="headerlink" title="Regression is all you need for medical image translation"></a>Regression is all you need for medical image translation</h2><p><strong>Authors:Sebastian Rassmann, David KÃ¼gler, Christian Ewert, Martin Reuter</strong></p>
<p>The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging. </p>
<blockquote>
<p>åœ¨åŒ»ç–—æˆåƒä¸­ï¼Œåœ¨æœ‰é™çš„æ—¶é—´é¢„ç®—å†…è·å–ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒè‡³å…³é‡è¦ã€‚åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆMITï¼‰å¯ä»¥é€šè¿‡ä»è·å–çš„æ•°æ®ç”Ÿæˆåˆæˆå›¾åƒæ¥å¸®åŠ©å¢å¼ºå’Œè¡¥å……ç°æœ‰æ•°æ®é›†ã€‚è™½ç„¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨è‡ªç„¶å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå®ƒä»¬åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„ç›Šå¤„â€”â€”åˆ›é€ æ€§å’Œå›¾åƒé€¼çœŸæ€§â€”â€”å¹¶ä¸ä¸€å®šä¼šè½¬ç§»åˆ°éœ€è¦é«˜åº¦å‡†ç¡®çš„è§£å‰–ä¿¡æ¯çš„åº”ç”¨åœºæ™¯ã€‚äº‹å®ä¸Šï¼Œæ¨¡ä»¿è·å–å™ªå£°æˆ–å†…å®¹å¹»è§‰ä¼šé˜»ç¢å…¶åœ¨ä¸´åºŠä¸Šçš„å®ç”¨æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†YODAï¼ˆYou Only Denoise once - or Averageï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä½“ç§¯çš„æ–°å‹2.5Dæ‰©æ•£æ¨¡å‹æ¡†æ¶ç”¨äºåŒ»å­¦å›¾åƒç¿»è¯‘ã€‚YODAç»“åˆäº†æ‰©æ•£å’Œå›å½’èŒƒå¼æ¥äº§ç”Ÿé€¼çœŸçš„æˆ–æ— å™ªå£°çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æœŸæœ›è¿‘ä¼¼ï¼ˆExpAï¼‰DMé‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—åˆ°MRIä¿¡å·å¹³å‡çš„å¯å‘ã€‚ExpAé‡‡æ ·æŠ‘åˆ¶ç”Ÿæˆçš„å™ªå£°ï¼Œä»è€Œæ¶ˆé™¤å™ªå£°å¯¹å›¾åƒè´¨é‡è¯„ä¼°çš„åè§ã€‚é€šè¿‡åœ¨å››ä¸ªä¸åŒçš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒâ€”â€”åŒ…æ‹¬å¤šå¯¹æ¯”åº¦è„‘éƒ¨MRIå’Œç›†è…”MRI-CTâ€”â€”æˆ‘ä»¬è¯æ˜æ‰©æ•£é‡‡æ ·å’Œå›å½’é‡‡æ ·åœ¨å®è·µä¸­å¯ä»¥å¾—åˆ°ç±»ä¼¼çš„ç»“æœã€‚å› æ­¤ï¼Œæ‰©æ•£é‡‡æ ·åœ¨åŒ»å­¦ç¿»è¯‘ä¸­çš„è®¡ç®—å¼€é”€å¹¶æ²¡æœ‰æä¾›ç³»ç»Ÿæ€§çš„å¥½å¤„ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬è¯æ˜YODAä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„GANå’ŒDMæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒYODAç”Ÿæˆçš„å›¾åƒè¢«è¯æ˜å¯ä»¥ä¸å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­çš„ç‰©ç†é‡‡é›†äº’æ¢ä½¿ç”¨ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä¼˜è¶Šã€‚æˆ‘ä»¬çš„ç ”ç©¶æŒ‘æˆ˜äº†DMåœ¨MITä¸­çš„é¢„è®¾ä¼˜åŠ¿ï¼Œå¹¶ä¸ºMITåœ¨åŒ»ç–—æˆåƒä¸­çš„å®é™…åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02048v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œä¿¡æ¯ä¸°å¯Œçš„å›¾åƒåœ¨æœ‰é™æ—¶é—´é¢„ç®—å†…çš„è·å–è‡³å…³é‡è¦ã€‚åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆMITï¼‰å¯ä»¥é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºå’Œè¡¥å……ç°æœ‰æ•°æ®é›†ã€‚è™½ç„¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨è‡ªç„¶å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶åœ¨åŒ»å­¦åº”ç”¨ä¸­å¹¶ä¸ä¸€å®šèƒ½å‘æŒ¥å‡ºåˆ›é€ æ€§å’Œå›¾åƒçœŸå®æ€§çš„ä¼˜åŠ¿ï¼Œå› ä¸ºåŒ»å­¦åº”ç”¨éœ€è¦é«˜åº¦å‡†ç¡®çš„è§£å‰–ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„2.5Dæ‰©æ•£åŸºäºä½“ç§¯çš„åŒ»å­¦å›¾åƒç¿»è¯‘æ¡†æ¶YODAï¼Œå®ƒå°†æ‰©æ•£å’Œå›å½’èŒƒå¼ç»“åˆèµ·æ¥ï¼Œäº§ç”Ÿé€¼çœŸçš„æˆ–æ— å™ªå£°çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å—MRIä¿¡å·å¹³å‡å¯å‘çš„Expectation-Approximationï¼ˆExpAï¼‰DMé‡‡æ ·ï¼Œç”¨äºæŠ‘åˆ¶ç”Ÿæˆçš„å™ªå£°ï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡è¯„ä¼°çš„å®¢è§‚æ€§ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼ŒYODAåœ¨å¤šç§æ¨¡æ€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¸€äº›æœ€å…ˆè¿›çš„GANå’ŒDMæ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒç”šè‡³å¯ä¸çœŸå®ç‰©ç†é‡‡é›†çš„å›¾åƒäº’æ¢æˆ–æ›´ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒä¸­ï¼Œåœ¨æœ‰é™æ—¶é—´é¢„ç®—å†…è·å–ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒè‡³å…³é‡è¦ã€‚</li>
<li>åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆMITï¼‰èƒ½é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºå’Œè¡¥å……ç°æœ‰æ•°æ®é›†ã€‚</li>
<li>è™½ç„¶GANså’ŒDMsåœ¨è‡ªç„¶å›¾åƒç”Ÿæˆä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ»å­¦åº”ç”¨ä¸­ä¸ä¸€å®šèƒ½å‘æŒ¥å‡ºå…¶åˆ›é€ æ€§å’Œå›¾åƒçœŸå®æ€§çš„ä¼˜åŠ¿ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒç¿»è¯‘æ¡†æ¶YODAï¼Œç»“åˆäº†æ‰©æ•£å’Œå›å½’èŒƒå¼ï¼Œèƒ½ç”Ÿæˆé€¼çœŸçš„æˆ–æ— å™ªå£°çš„å›¾åƒã€‚</li>
<li>YODAé‡‡ç”¨äº†å—MRIä¿¡å·å¹³å‡å¯å‘çš„Expectation-Approximationï¼ˆExpAï¼‰DMé‡‡æ ·ï¼Œä»¥æé«˜å›¾åƒè´¨é‡è¯„ä¼°çš„å®¢è§‚æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒYODAåœ¨å¤šç§æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc818b766647a276f2f1efb52443784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b16f1076d38362ca15bb7c57631877.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2c6add45f6ea928640fb40a54ae5a99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c30b220f82e9fd04a8b2200abd58ad6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e88204effc0f36fcc485522b9d2b47e7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Quaternion-Wavelet-Conditioned-Diffusion-Models-for-Image-Super-Resolution"><a href="#Quaternion-Wavelet-Conditioned-Diffusion-Models-for-Image-Super-Resolution" class="headerlink" title="Quaternion Wavelet-Conditioned Diffusion Models for Image   Super-Resolution"></a>Quaternion Wavelet-Conditioned Diffusion Models for Image   Super-Resolution</h2><p><strong>Authors:Luigi Sigillo, Christian Bianchi, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>Image Super-Resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. The ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. While deep learning has significantly advanced SR, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. Recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. In this work, we introduce ResQu a novel SR framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. Unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. Furthermore, we also leverage the generative priors of foundation models such as Stable Diffusion. Extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding SR results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. The code will be available after the revision process. </p>
<blockquote>
<p>å›¾åƒè¶…åˆ†è¾¨ç‡æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼Œå…¶åº”ç”¨é¢†åŸŸå¹¿æ³›ï¼Œä»åŒ»å­¦å½±åƒåˆ°å«æ˜Ÿåˆ†æéƒ½æœ‰æ¶‰åŠã€‚ä»ä½åˆ†è¾¨ç‡è¾“å…¥é‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒçš„èƒ½åŠ›å¯¹äºæé«˜ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ï¼‰è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»å¤§å¤§æ¨åŠ¨äº†è¶…åˆ†è¾¨ç‡æŠ€æœ¯çš„å‘å±•ï¼Œä½†å®ç°å…·æœ‰ç²¾ç»†ç»†èŠ‚å’Œé€¼çœŸçº¹ç†çš„é«˜è´¨é‡é‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜æ”¾å¤§å€æ•°æ—¶ã€‚æœ€è¿‘åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥åœ¨æ„ŸçŸ¥è´¨é‡å’Œç»“æ„ä¿çœŸåº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ResQuï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œå®ƒå°†å››å…ƒå°æ³¢é¢„å¤„ç†æ¡†æ¶ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å››å…ƒå°æ³¢å’Œæ—¶é—´æ„ŸçŸ¥ç¼–ç å™¨ã€‚ä¸åŒäºé‚£äº›åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç®€å•åº”ç”¨å°æ³¢å˜æ¢çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨å››å…ƒå°æ³¢åµŒå…¥æ¥å¢å¼ºæ¡ä»¶è¿‡ç¨‹ï¼Œè¿™äº›åµŒå…¥åœ¨ä¸åŒçš„å»å™ªé˜¶æ®µè¢«åŠ¨æ€é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨äº†åŸºç¡€æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒçŸ¥è¯†ï¼Œå¦‚Stable Diffusionã€‚åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å‡ºè‰²çš„è¶…åˆ†è¾¨ç‡ç»“æœï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œæ ‡å‡†è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚ä»£ç å°†åœ¨ä¿®è®¢åæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00334v2">PDF</a> Accepted for presentation at IJCNN 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºæ¡†æ¶ResQuï¼Œç»“åˆäº†å››å…ƒå°æ³¢é¢„å¤„ç†æ¡†æ¶å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨å››å…ƒå°æ³¢åµŒå…¥å¹¶åŠ¨æ€é›†æˆäºå»å™ªçš„ä¸åŒé˜¶æ®µï¼Œæé«˜äº†æ¡ä»¶è¿‡ç¨‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒï¼Œå¦‚Stable Diffusionã€‚åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡å’Œæ ‡å‡†è¯„ä¼°æŒ‡æ ‡æ–¹é¢å–å¾—äº†å‡ºè‰²çš„è¶…åˆ†è¾¨ç‡é‡å»ºç»“æœï¼Œå¹¶åœ¨è®¸å¤šæƒ…å†µä¸‹ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè¶…åˆ†è¾¨ç‡æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬é—®é¢˜ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨é¢†åŸŸï¼Œå¦‚åŒ»å­¦å½±åƒå’Œå«æ˜Ÿåˆ†æã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨SRä¸­æœ‰å¾ˆå¤§è¿›å±•ï¼Œä½†åœ¨å®ç°é«˜è´¨é‡é‡å»ºæ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ”¾å¤§å€æ•°æ—¶ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é‡å»ºä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½å‰æ™¯ï¼Œä½†éœ€è¦åœ¨æ„ŸçŸ¥è´¨é‡å’Œç»“æ„ä¿çœŸä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„ResQuæ¡†æ¶ç»“åˆäº†å››å…ƒå°æ³¢é¢„å¤„ç†æ¡†æ¶å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†æ¡ä»¶è¿‡ç¨‹çš„æ€§èƒ½ã€‚</li>
<li>ResQué€šè¿‡åœ¨ä¸åŒé˜¶æ®µçš„å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€åˆ©ç”¨å››å…ƒå°æ³¢åµŒå…¥ï¼Œæé«˜äº†å›¾åƒé‡å»ºçš„ç²¾ç»†åº¦å’Œçº¹ç†çœŸå®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è¿˜åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒï¼Œå¦‚Stable Diffusionï¼Œæ¥æé«˜è¶…åˆ†è¾¨ç‡é‡å»ºçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2c5ad12c7ddc2293a9dab427b450419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22139fd6e235722344e839c075e500ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e997294b6225fd0025ae25444812ab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7e98ec900a2eb753fa88940d3893200.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e3f14e5d52c10733ea6512439a526c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References"><a href="#Cobra-Efficient-Line-Art-COlorization-with-BRoAder-References" class="headerlink" title="Cobra: Efficient Line Art COlorization with BRoAder References"></a>Cobra: Efficient Line Art COlorization with BRoAder References</h2><p><strong>Authors:Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan</strong></p>
<p>The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a>. </p>
<blockquote>
<p>æ¼«ç”»åˆ¶ä½œè¡Œä¸šéœ€è¦åŸºäºå‚è€ƒçš„é«˜ç²¾åº¦ã€é«˜æ•ˆç‡ã€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œçµæ´»æ§åˆ¶çš„çº¿è‰ºå½©è‰²åŒ–ã€‚æ¼«ç”»é¡µé¢é€šå¸¸æ¶‰åŠå¤šç§è§’è‰²ã€ç‰©ä½“å’ŒèƒŒæ™¯ï¼Œè¿™ä½¿ç€è‰²è¿‡ç¨‹å¤æ‚åŒ–ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨çº¿è‰ºå½©è‰²åŒ–çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œé¢ä¸´ç€å¤„ç†å¤§é‡å‚è€ƒå›¾åƒã€è€—æ—¶æ¨ç†å’Œçµæ´»æ§åˆ¶ç­‰æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§é‡ä¸Šä¸‹æ–‡å›¾åƒæŒ‡å¯¼å¯¹çº¿è‰ºå½©è‰²åŒ–è´¨é‡çš„å¿…è¦æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Cobraï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„æ–¹æ³•ï¼Œæ”¯æŒé¢œè‰²æç¤ºï¼ŒåŒæ—¶ä½¿ç”¨è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿã€‚Cobraçš„æ ¸å¿ƒæ˜¯å› æœç¨€ç–DiTæ¶æ„ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨è®¾è®¡çš„ä½ç½®ç¼–ç ã€å› æœç¨€ç–æ³¨æ„åŠ›å’Œé”®å€¼ç¼“å­˜æ¥æœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å¼•ç”¨å¹¶ç¡®ä¿é¢œè‰²èº«ä»½ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒCobraé€šè¿‡å¤§é‡çš„ä¸Šä¸‹æ–‡å‚è€ƒå®ç°äº†å‡†ç¡®çš„çº¿è‰ºå½©è‰²åŒ–ï¼Œå¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦å’Œäº¤äº’æ€§ï¼Œä»è€Œæ»¡è¶³äº†è¡Œä¸šçš„å…³é”®éœ€æ±‚ã€‚æˆ‘ä»¬å·²åœ¨é¡¹ç›®é¡µé¢å‘å¸ƒä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra%E3%80%82">https://zhuang2002.github.io/Cobra/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12240v3">PDF</a> Project page with code: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/Cobra/">https://zhuang2002.github.io/Cobra/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æ¼«ç”»åˆ¶ä½œè¡Œä¸šå¯¹åŸºäºå‚è€ƒçš„çº¿è‰ºæœ¯è‰²å½©åŒ–çš„é«˜å‡†ç¡®æ€§ã€é«˜æ•ˆç‡ã€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œçµæ´»æ§åˆ¶çš„éœ€æ±‚ã€‚æ–‡ä¸­æŒ‡å‡ºï¼Œå°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨çº¿è‰ºæœ¯è‰²å½©åŒ–æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ï¼Œé¢ä¸´å¤„ç†å¤§é‡å‚è€ƒå›¾åƒã€æ¨ç†è€—æ—¶ä»¥åŠçµæ´»æ§åˆ¶ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†Cobraæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ”¯æŒé¢œè‰²æç¤ºå¹¶åˆ©ç”¨è¶…è¿‡200å¼ å‚è€ƒå›¾åƒï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿã€‚Cobraçš„æ ¸å¿ƒæ˜¯ä¸€ç§å› æœç¨€ç–DiTæ¶æ„ï¼Œå®ƒé€šè¿‡ä¸“é—¨è®¾è®¡çš„ä½ç½®ç¼–ç ã€å› æœç¨€ç–æ³¨æ„åŠ›æœºåˆ¶å’Œé”®å€¼ç¼“å­˜æ¥æœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒå¹¶ç¡®ä¿é¢œè‰²ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCobraé€šè¿‡å¹¿æ³›çš„ä¸Šä¸‹æ–‡å‚è€ƒå®ç°äº†å‡†ç¡®çš„çº¿è‰ºæœ¯è‰²å½©åŒ–ï¼Œå¤§å¤§æé«˜äº†æ¨ç†é€Ÿåº¦å’Œäº¤äº’æ€§ï¼Œä»è€Œæ»¡è¶³äº†è¡Œä¸šçš„å…³é”®éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¼«ç”»åˆ¶ä½œéœ€è¦é«˜å‡†ç¡®æ€§ã€é«˜æ•ˆç‡ã€ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å’Œçµæ´»æ§åˆ¶çš„çº¿è‰ºæœ¯è‰²å½©åŒ–ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„åº”ç”¨åœ¨çº¿è‰ºæœ¯è‰²å½©åŒ–æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Cobraæ˜¯ä¸€ç§æ”¯æŒé¢œè‰²æç¤ºå¹¶åˆ©ç”¨å¤§é‡å‚è€ƒå›¾åƒçš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒä½å»¶è¿Ÿã€‚</li>
<li>Cobraçš„æ ¸å¿ƒæ˜¯å› æœç¨€ç–DiTæ¶æ„ï¼Œèƒ½æœ‰æ•ˆç®¡ç†é•¿ä¸Šä¸‹æ–‡å‚è€ƒå’Œç¡®ä¿é¢œè‰²ä¸€è‡´æ€§ã€‚</li>
<li>Cobraå®ç°äº†å‡†ç¡®çš„çº¿è‰ºæœ¯è‰²å½©åŒ–ï¼Œé€šè¿‡å¹¿æ³›çš„ä¸Šä¸‹æ–‡å‚è€ƒã€‚</li>
<li>Cobraæé«˜äº†æ¨ç†é€Ÿåº¦å’Œäº¤äº’æ€§ï¼Œæ»¡è¶³äº†è¡Œä¸šå…³é”®éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå·²åœ¨å…¶é¡¹ç›®é¡µé¢ä¸Šå‘å¸ƒäº†ä»£ç å’Œæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fce05bfb573f21fd4632d84bcaa20360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b803b0b75a21c85a3e4dee2c5bf909.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-388cdcb9f6bffb3f664ebb598607d1e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a4a2ec13ffa912e2b0bca4c9ad5f41e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3274a7d3eefdf6540f238dc922f9c6fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc6d0dc693a42a72772dab7fca8bc73.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models"><a href="#ID-Booth-Identity-consistent-Face-Generation-with-Diffusion-Models" class="headerlink" title="ID-Booth: Identity-consistent Face Generation with Diffusion Models"></a>ID-Booth: Identity-consistent Face Generation with Diffusion Models</h2><p><strong>Authors:Darian TomaÅ¡eviÄ‡, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Å truc, Peter Peer</strong></p>
<p>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at <a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth">https://github.com/dariant/ID-Booth</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå»ºæ¨¡æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆé€‚ç”¨äºå¤šç§é¢†åŸŸçš„é«˜è´¨é‡åˆæˆæ•°æ®ï¼Œå…¶ä¸­åŒ…æ‹¬äººè„¸è¯†åˆ«ã€‚åœ¨æ­¤æƒ…å†µä¸‹ï¼Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºå¼ºå¤§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶å’Œå¾®è°ƒï¼Œä»¥ä¿ƒè¿›æ‰€éœ€èº«ä»½çš„é€¼çœŸå›¾åƒåˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€ä¸è€ƒè™‘ä¸»ä½“çš„èº«ä»½ï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒä¸é¢„æœŸèº«ä»½ä¹‹é—´çš„ä¸€è‡´æ€§è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‡‡ç”¨åŸºäºèº«ä»½è®­ç»ƒç›®æ ‡çš„æ–¹æ³•å¾€å¾€ä¼šåœ¨èº«ä»½çš„å„ä¸ªæ–¹é¢çš„æ‹Ÿåˆè¿‡åº¦ï¼Œä»è€Œé™ä½äº†å¯ä»¥ç”Ÿæˆçš„å›¾åƒå¤šæ ·æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºID-Boothã€‚ID-BoothåŒ…æ‹¬ä¸€ä¸ªç”¨äºæ•°æ®ç”Ÿæˆçš„å»å™ªç½‘ç»œã€ä¸€ä¸ªç”¨äºå°†å›¾åƒæ˜ å°„åˆ°å’Œä»ä½ç»´æ½œåœ¨ç©ºé—´çš„å˜è‡ªåŠ¨ç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯ä»¥é€šè¿‡æç¤ºæ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ–°å‹çš„ä¸‰é‡èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¤šç§æç¤ºè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ç«äº‰æ–¹æ³•å…·æœ‰æ›´å¥½çš„èº«ä»½å†…éƒ¨ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†ç¦»æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚åè¿‡æ¥ï¼Œæ‰€ç”Ÿæˆçš„æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºå°è§„æ¨¡æ•°æ®é›†å¹¶ç”¨äºè®­ç»ƒæ€§èƒ½æ›´å¥½çš„è¯†åˆ«æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŠ¤éšç§ã€‚ID-Boothæ¡†æ¶çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/dariant/ID-Boothä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07392v4">PDF</a> IEEE International Conference on Automatic Face and Gesture   Recognition (FG) 2025, 14 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹ç”Ÿæˆæ¨¡å‹ID-Boothï¼Œè¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨é¢éƒ¨è¯†åˆ«ç­‰é¢†åŸŸåˆæˆå›¾åƒæ—¶é¢ä¸´çš„èº«ä»½ä¸ä¸€è‡´å’Œå›¾åƒå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚ID-BoothåŒ…æ‹¬ä¸€ä¸ªç”¨äºæ•°æ®ç”Ÿæˆçš„é™å™ªç½‘ç»œã€ä¸€ä¸ªç”¨äºå°†å›¾åƒæ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´çš„å˜åˆ†è‡ªç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œä»¥å®ç°åŸºäºæç¤ºçš„ç”Ÿæˆè¿‡ç¨‹æ§åˆ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°å‹çš„ä¸‰å…ƒç»„èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„åˆæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°èº«ä»½ä¸€è‡´çš„å›¾åƒç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒID-Boothåœ¨èº«ä»½å†…ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†æ€§æ–¹é¢è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„å›¾åƒå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„æ•°æ®èƒ½å¤Ÿæœ‰æ•ˆå¢å¼ºå°è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶å¯ç”¨äºè®­ç»ƒæ€§èƒ½æ›´ä½³çš„éšç§ä¿æŠ¤è¯†åˆ«æ¨¡å‹ã€‚ID-Boothæ¡†æ¶çš„æºä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/dariant/ID-Booth%E3%80%82">https://github.com/dariant/ID-Boothã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–°å‹ç”Ÿæˆæ¨¡å‹ID-BoothåŸºäºæ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†åˆæˆå›¾åƒæ—¶èº«ä»½ä¸ä¸€è‡´å’Œå›¾åƒå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ID-BoothåŒ…æ‹¬é™å™ªç½‘ç»œã€å˜åˆ†è‡ªç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå¯å®ç°åŸºäºæç¤ºçš„ç”Ÿæˆè¿‡ç¨‹æ§åˆ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰å…ƒç»„èº«ä»½è®­ç»ƒç›®æ ‡ï¼Œæé«˜äº†èº«ä»½ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†å›¾åƒå¤šæ ·æ€§ã€‚</li>
<li>ID-Boothåœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„èº«ä»½å†…ä¸€è‡´æ€§å’Œèº«ä»½é—´å¯åˆ†æ€§ã€‚</li>
<li>ID-Boothèƒ½å¤Ÿå¢å¼ºå°è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶å¯ç”¨äºè®­ç»ƒæ€§èƒ½æ›´ä½³çš„éšç§ä¿æŠ¤è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>ID-Boothæ¡†æ¶çš„æºä»£ç å·²å…¬å¼€ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
<li>è¯¥æ¡†æ¶çš„æå‡ºä¸ºé¢éƒ¨è¯†åˆ«ç­‰é¢†åŸŸçš„æ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec5b386ba622866281c393e7204b3822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36b6fbb4710dc0e6e31707697f352eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58dd7e90096aaf170d08c2367e80dc22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdf6838033e20f908230d128d17e2b9b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Dynamic-Importance-in-Diffusion-U-Net-for-Enhanced-Image-Synthesis"><a href="#Dynamic-Importance-in-Diffusion-U-Net-for-Enhanced-Image-Synthesis" class="headerlink" title="Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis"></a>Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis</h2><p><strong>Authors:Xi Wang, Ziqi He, Yang Zhou</strong></p>
<p>Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a â€œfree lunchâ€ for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: <a target="_blank" rel="noopener" href="https://github.com/Hytidel/UNetReweighting">https://github.com/Hytidel/UNetReweighting</a> </p>
<blockquote>
<p>ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹é€šå¸¸é‡‡ç”¨U-Netæ¶æ„ã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»æ­ç¤ºäº†æ³¨æ„åŠ›å—åœ¨U-Netä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†æ¨ç†è¿‡ç¨‹ä¸­æ³¨æ„åŠ›å—é‡è¦æ€§åŠ¨æ€å˜åŒ–çš„é‡è¦æ€§ï¼Œè¿™é˜»ç¢äº†å…¶è¿›ä¸€æ­¥å¼€å‘ä»¥æé«˜å›¾åƒåº”ç”¨çš„æ•ˆæœã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šè¯æ˜äº†é‡æ–°è°ƒæ•´U-Netä¸­Transformerå—çš„è¾“å‡ºå¯ä»¥æé«˜é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡å™ªæ¯”ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†é‡è¦æ€§æ¢é’ˆï¼ˆImportance Probeï¼‰ï¼Œä»¥æ­ç¤ºå’Œé‡åŒ–é™å™ªè¿‡ç¨‹ä¸­Transformerå—é‡è¦æ€§çš„åŠ¨æ€å˜åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºè‡ªé€‚åº”é‡è¦æ€§çš„é‡æ–°è°ƒæ•´æƒé‡è°ƒåº¦ç­–ç•¥ï¼Œå¯é’ˆå¯¹ç‰¹å®šçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡è¿›è¡Œå®šåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†è¿‡ç¨‹çš„æ•ˆç‡ï¼Œå¹¶æé«˜äº†æ ·æœ¬çš„ç¾å­¦è´¨é‡å’Œä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºäºU-Netçš„æ¶æ„ä¸­ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Hytidel/UNetReweighting">https://github.com/Hytidel/UNetReweighting</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03471v2">PDF</a> Accepted to ICME 2025. Appendix &amp; Code:   <a target="_blank" rel="noopener" href="https://github.com/Hytidel/UNetReweighting">https://github.com/Hytidel/UNetReweighting</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸­çš„U-Netæ¶æ„ï¼Œå¹¶æŒ‡å‡ºä»¥å¾€ç ”ç©¶å¿½è§†äº†æ³¨æ„åŠ›å—åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„åŠ¨æ€é‡è¦æ€§å˜åŒ–ã€‚æœ¬ç ”ç©¶é€šè¿‡ç†è®ºè¯æ˜äº†é‡æ–°åŠ æƒU-Netä¸­Transformerå—çš„è¾“å‡ºå¯ä»¥æé«˜é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡å·å™ªå£°æ¯”ã€‚åŒæ—¶ï¼Œæå‡ºäº†é‡è¦æ€§æ¢é’ˆï¼ˆImportance Probeï¼‰æ¥æ­ç¤ºå’Œé‡åŒ–å»å™ªè¿‡ç¨‹ä¸­Transformerå—é‡è¦æ€§çš„åŠ¨æ€å˜åŒ–ã€‚æœ€ç»ˆè®¾è®¡äº†ä¸€ç§åŸºäºè‡ªé€‚åº”é‡è¦æ€§çš„é‡æ–°åŠ æƒè°ƒåº¦ï¼Œé€‚ç”¨äºç‰¹å®šçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œå¹¶æé«˜äº†æ ·æœ¬çš„ç¾å­¦è´¨é‡å’Œèº«ä»½ä¸€è‡´æ€§ã€‚æ­¤æ–¹æ³•å¯æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºäºU-Netçš„æ¶æ„ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹é‡‡ç”¨U-Netæ¶æ„ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶æœªå……åˆ†å…³æ³¨æ³¨æ„åŠ›å—åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„åŠ¨æ€é‡è¦æ€§å˜åŒ–ã€‚</li>
<li>é‡æ–°åŠ æƒU-Netä¸­Transformerå—çš„è¾“å‡ºå¯ä»¥æé«˜é‡‡æ ·è¿‡ç¨‹ä¸­çš„ä¿¡å·å™ªå£°æ¯”ã€‚</li>
<li>æå‡ºäº†é‡è¦æ€§æ¢é’ˆï¼ˆImportance Probeï¼‰æ¥é‡åŒ–å»å™ªè¿‡ç¨‹ä¸­Transformerå—çš„é‡è¦æ€§å˜åŒ–ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºè‡ªé€‚åº”é‡è¦æ€§çš„é‡æ–°åŠ æƒè°ƒåº¦ï¼Œé€‚ç”¨äºå›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œå¢å¼ºäº†æ ·æœ¬çš„ç¾å­¦è´¨é‡å’Œèº«ä»½ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºäºU-Netçš„æ¶æ„ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9bcbe26adb81151768fb1ecbad2585f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e34a6d8318fc376b9d724af08cbf3b71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41e1a901a3024111ae1b95393e3c2a50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861e26b90aa8bb65a61070a19f3f4c76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc3418610d20a05e157793dfca67901.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations"><a href="#Video-Prediction-Policy-A-Generalist-Robot-Policy-with-Predictive-Visual-Representations" class="headerlink" title="Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations"></a>Video Prediction Policy: A Generalist Robot Policy with Predictive   Visual Representations</h2><p><strong>Authors:Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen</strong></p>
<p>Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6% increase in success rates for complex real-world dexterous manipulation tasks. Project page at <a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io/">https://video-prediction-policy.github.io</a> </p>
<blockquote>
<p>è§†è§‰è¡¨ç¤ºåœ¨å¼€å‘é€šç”¨æœºå™¨äººç­–ç•¥ä¸­å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä»¥å‰çš„è§†è§‰ç¼–ç å™¨é€šå¸¸ä½¿ç”¨å•å›¾åƒé‡å»ºæˆ–åŒå›¾åƒå¯¹æ¯”å­¦ä¹ è¿›è¡Œé¢„è®­ç»ƒï¼Œæ›´åå‘äºæ•æ‰é™æ€ä¿¡æ¯ï¼Œå¾€å¾€å¿½ç•¥äº†å¯¹äºå®ä½“ä»»åŠ¡è‡³å…³é‡è¦çš„åŠ¨æ€æ–¹é¢ã€‚æœ€è¿‘ï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰å±•ç¤ºäº†é¢„æµ‹æœªæ¥å¸§çš„èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†å¯¹ç‰©ç†ä¸–ç•Œçš„æ·±åˆ»ç†è§£ã€‚æˆ‘ä»¬å‡è®¾VDMsä¼šç”ŸæˆåŒ…å«å½“å‰é™æ€ä¿¡æ¯å’Œé¢„æµ‹çš„æœªæ¥åŠ¨æ€ä¿¡æ¯çš„è§†è§‰è¡¨ç¤ºï¼Œä»è€Œä¸ºæœºå™¨äººåŠ¨ä½œå­¦ä¹ æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºäº†è§†é¢‘é¢„æµ‹ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œå®ƒå­¦ä¹ åŸºäºVDMså†…éƒ¨é¢„æµ‹çš„æœªæ¥è¡¨ç¤ºçš„éšå¼é€†åŠ¨åŠ›å­¦æ¨¡å‹ã€‚ä¸ºäº†é¢„æµ‹æ›´ç²¾ç¡®çš„æœªæ¥ï¼Œæˆ‘ä»¬åœ¨æœºå™¨äººæ•°æ®é›†ä»¥åŠäº’è”ç½‘äººç±»æ“ä½œæ•°æ®ä¸Šå¯¹é¢„è®­ç»ƒçš„è§†é¢‘åŸºç¡€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚åœ¨å®éªŒä¸­ï¼ŒVPPåœ¨Calvin ABC-Dæ³›åŒ–åŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹äºä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³å®ç°äº†18.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œç²¾ç»†æ“ä½œä»»åŠ¡ä¸­æˆåŠŸç‡æé«˜äº†31.6%ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://video-prediction-policy.github.io./">https://video-prediction-policy.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14803v2">PDF</a> ICML 2025 Spotlight Paper. The first two authors contribute equally</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¡¨ç¤ºåœ¨å¼€å‘é€šç”¨æœºå™¨äººç­–ç•¥ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚ä¼ ç»Ÿçš„è§†è§‰ç¼–ç å™¨ä¸»è¦æ•æ‰é™æ€ä¿¡æ¯ï¼Œå¿½ç•¥äº†åŠ¨æ€æ–¹é¢ï¼Œè¿™å¯¹äºå®ä½“ä»»åŠ¡è‡³å…³é‡è¦ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½å¤Ÿé¢„æµ‹æœªæ¥å¸§ï¼Œå¹¶å±•ç¤ºå¯¹ç‰©ç†ä¸–ç•Œçš„æ·±åˆ»ç†è§£ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å‡è®¾VDMsäº§ç”Ÿçš„è§†è§‰è¡¨ç¤ºåŒ…å«å½“å‰é™æ€ä¿¡æ¯å’Œé¢„æµ‹çš„æœªæ¥åŠ¨æ€ï¼Œä¸ºæœºå™¨äººåŠ¨ä½œå­¦ä¹ æä¾›å®è´µæŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºè§†é¢‘é¢„æµ‹çš„ç­–ç•¥ï¼ˆVPPï¼‰ï¼Œå®ƒå­¦ä¹ åŸºäºVDMså†…é¢„æµ‹æœªæ¥è¡¨ç¤ºçš„éšé€†åŠ¨åŠ›å­¦æ¨¡å‹ã€‚åœ¨å®éªŒä¸­ï¼ŒVPPåœ¨Calvin ABC-Dé€šç”¨åŒ–åŸºå‡†æµ‹è¯•ä¸Šç›¸å¯¹äºä¹‹å‰çš„æœ€ä¼˜ç­–ç•¥å®ç°äº†18.6%çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨å¤æ‚çš„çœŸå®ä¸–ç•Œç²¾ç»†æ“ä½œä»»åŠ¡ä¸ŠæˆåŠŸç‡æé«˜äº†31.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMsï¼‰èƒ½é¢„æµ‹æœªæ¥å¸§ï¼Œå±•ç°å¯¹ç‰©ç†ä¸–ç•Œçš„æ·±åˆ»ç†è§£ã€‚</li>
<li>VDMsäº§ç”Ÿçš„è§†è§‰è¡¨ç¤ºåŒ…å«å½“å‰é™æ€ä¿¡æ¯å’Œé¢„æµ‹çš„æœªæ¥åŠ¨æ€ã€‚</li>
<li>åŸºäºè§†é¢‘é¢„æµ‹çš„ç­–ç•¥ï¼ˆVPPï¼‰ç»“åˆäº†VDMså’Œæœºå™¨äººåŠ¨ä½œå­¦ä¹ ã€‚</li>
<li>VPPé€šè¿‡éšé€†åŠ¨åŠ›å­¦æ¨¡å‹é¢„æµ‹æœªæ¥è¡¨ç°ï¼ŒåŸºäºé¢„æµ‹æœªæ¥è¡¨ç¤ºã€‚</li>
<li>å¯¹é¢„è®­ç»ƒçš„è§†é¢‘åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å¯¹æœªæ¥é¢„æµ‹çš„ç²¾ç¡®åº¦ã€‚</li>
<li>VPPåœ¨Calvin ABC-DåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>VPPåœ¨çœŸå®ä¸–ç•Œçš„ç²¾ç»†æ“ä½œä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-28eed16301fb31ab198f640220812916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bafd87c9205eff72bbcf5ca44016234f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-638c74e920c73ec3c3ff707756825979.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ColorFlow-Retrieval-Augmented-Image-Sequence-Colorization"><a href="#ColorFlow-Retrieval-Augmented-Image-Sequence-Colorization" class="headerlink" title="ColorFlow: Retrieval-Augmented Image Sequence Colorization"></a>ColorFlow: Retrieval-Augmented Image Sequence Colorization</h2><p><strong>Authors:Junhao Zhuang, Xuan Ju, Zhaoyang Zhang, Yong Liu, Shiyi Zhang, Chun Yuan, Ying Shan</strong></p>
<p>Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/ColorFlow/">https://zhuang2002.github.io/ColorFlow/</a>. </p>
<blockquote>
<p>è‡ªåŠ¨é»‘ç™½å›¾åƒåºåˆ—ä¸Šè‰²ï¼ŒåŒæ—¶ä¿ç•™äººç‰©å’Œç‰©ä½“èº«ä»½ï¼ˆIDï¼‰ï¼Œæ˜¯ä¸€é¡¹å…·æœ‰æ˜¾è‘—å¸‚åœºéœ€æ±‚çš„å¤æ‚ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨å¡é€šæˆ–æ¼«ç”»ç³»åˆ—çš„ä¸Šè‰²ä¸­ã€‚å°½ç®¡ä½¿ç”¨å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰çš„è§†è§‰ä¸Šè‰²æŠ€æœ¯æœ‰æ‰€è¿›æ­¥ï¼Œä½†å¯æ§æ€§å’Œèº«ä»½ä¸€è‡´æ€§çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œä½¿å¾—å½“å‰è§£å†³æ–¹æ¡ˆä¸é€‚åˆå·¥ä¸šåº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ColorFlowï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå·¥ä¸šåº”ç”¨å›¾åƒåºåˆ—ä¸Šè‰²çš„ä¸‰é˜¶æ®µæ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ä¸éœ€è¦é’ˆå¯¹æ¯ä¸ªIDè¿›è¡Œå¾®è°ƒæˆ–æ˜¾å¼IDåµŒå…¥æå–ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ã€ç¨³å¥å’Œé€šç”¨çš„æ£€ç´¢å¢å¼ºä¸Šè‰²ç®¡é“ï¼Œç”¨äºä»¥ä¸Šè‰²ç›¸å…³çš„é¢œè‰²å‚è€ƒå›¾åƒã€‚æˆ‘ä»¬çš„ç®¡é“è¿˜é‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ï¼šä¸€ä¸ªåˆ†æ”¯ç”¨äºé¢œè‰²èº«ä»½æå–ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯ç”¨äºä¸Šè‰²ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ å’Œé¢œè‰²èº«ä»½åŒ¹é…ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ColorFlow-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå‚è€ƒçš„ä¸Šè‰²ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒColorFlowåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ï¼Œä¸ºåºåˆ—å›¾åƒä¸Šè‰²è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå¹¶æœ‰æœ›æƒ åŠè‰ºæœ¯äº§ä¸šã€‚æˆ‘ä»¬å°†åœ¨é¡¹ç›®é¡µé¢å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://zhuang2002.github.io/ColorFlow/">https://zhuang2002.github.io/ColorFlow/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11815v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://zhuang2002.github.io/ColorFlow/">https://zhuang2002.github.io/ColorFlow/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºColorFlowçš„ä¸‰é˜¶æ®µæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºå·¥ä¸šåº”ç”¨ä¸­å›¾åƒåºåˆ—çš„è‡ªåŠ¨é»‘ç™½å›¾åƒè‰²å½©åŒ–ï¼ŒåŒæ—¶ä¿ç•™è§’è‰²å’Œç‰©ä½“èº«ä»½ã€‚è¯¥æ¡†æ¶å…·æœ‰å¼ºå¤§çš„æ£€ç´¢å¢å¼ºè‰²å½©åŒ–ç®¡é“ï¼Œå¯åŸºäºç›¸å…³é¢œè‰²å‚è€ƒè¿›è¡Œè‰²å½©åŒ–ï¼Œæ— éœ€å¯¹æ¯ä¸ªèº«ä»½è¿›è¡Œå¾®è°ƒæˆ–æ˜¾å¼èº«ä»½åµŒå…¥æå–ã€‚é€šè¿‡åŒåˆ†æ”¯è®¾è®¡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°å¼ºä¸Šä¸‹æ–‡å­¦ä¹ å’Œé¢œè‰²èº«ä»½åŒ¹é…ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ColorFlow-Benchç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å‚è€ƒåŸºç¡€ä¸Šçš„è‰²å½©åŒ–æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒColorFlowåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œä¸ºè‰ºæœ¯äº§ä¸šå¸¦æ¥äº†æ½œåœ¨å¥½å¤„ã€‚æœ‰å…³ä»£ç å’Œæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ColorFlowæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå›¾åƒåºåˆ—çš„è‰²å½©åŒ–ï¼Œé€‚ç”¨äºå·¥ä¸šåº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¼ºå¤§çš„æ£€ç´¢å¢å¼ºè‰²å½©åŒ–ç®¡é“ï¼Œæ— éœ€é’ˆå¯¹æ¯ä¸ªèº«ä»½è¿›è¡Œå¾®è°ƒæˆ–æå–æ˜¾å¼èº«ä»½åµŒå…¥ã€‚</li>
<li>é‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ï¼Œä¸€ä¸ªåˆ†æ”¯ç”¨äºé¢œè‰²èº«ä»½æå–ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯ç”¨äºè‰²å½©åŒ–ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>å¼•å…¥ColorFlow-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨åŸºäºå‚è€ƒçš„è‰²å½©åŒ–æ€§èƒ½ã€‚</li>
<li>ColorFlowåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè‰ºæœ¯äº§ä¸šå¸¦æ¥æ½œåœ¨å¥½å¤„ï¼Œå¹¶èƒ½ä¸ºé»‘ç™½å›¾åƒï¼ˆå¦‚å¡é€šæˆ–æ¼«ç”»ç³»åˆ—ï¼‰å¸¦æ¥è‰²å½©åŒ–åº”ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2a0f42b18ae703c41c91a2441686d13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a138cec6f81a298ca7560eb825d162c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5f0c80ad786051130932a1fc1e6dfbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c4e51000ac02749489c9b69424a48eb.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. In response to this challenge, we take inspiration from recent successes in generative flow networks (GFlowNets) and propose a reinforcement learning method for diffusion model finetuning, dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), that leverages the rich signal in reward gradients for probabilistic diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>é€šå¸¸äººä»¬é€šè¿‡æ”¶é›†ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é›†æ¥è®­ç»ƒå¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œä½†æœ‰æ—¶äººä»¬å¸Œæœ›ä½¿ç”¨ä¸“å®¶è®¾è®¡çš„å¥–åŠ±å‡½æ•°æˆ–ä¸ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ åˆ°çš„å¥–åŠ±å‡½æ•°å¯¹é½å¹¶å¾®è°ƒé¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚ç°æœ‰çš„ç”¨äºæ‰©æ•£æ¨¡å‹å¥–åŠ±å¾®è°ƒçš„åè®­ç»ƒæ–¹æ³•é€šå¸¸é¢ä¸´ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒæ”¶æ•›ç¼“æ…¢ç­‰é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰çš„æœ€æ–°æˆåŠŸä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹å¾®è°ƒçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œåä¸ºNabla-GFlowNetï¼ˆç®€ç§°$\nabla$-GFlowNetï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸­çš„ä¸°å¯Œä¿¡å·è¿›è¡Œæ¦‚ç‡æ‰©æ•£å¾®è°ƒã€‚æˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„ç°å®å¥–åŠ±å‡½æ•°ä¸Šå¿«é€Ÿã€å¤šæ ·åŒ–å’Œä¿ç•™å…ˆéªŒåœ°å¯¹å¤§å‹æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹Stable Diffusionè¿›è¡Œå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v5">PDF</a> Technical Report (37 pages, 31 figures), Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºå¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç§°ä¸ºNabla-GFlowNetã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸­çš„ä¸°å¯Œä¿¡å·è¿›è¡Œæ¦‚ç‡æ‰©æ•£å¾®è°ƒï¼Œè§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹å¥–åŠ±å¾®è°ƒæ–¹æ³•çš„æ ·æœ¬å¤šæ ·æ€§ä¸è¶³ã€å…ˆéªŒçŸ¥è¯†ä¸¢å¤±ä»¥åŠå¾®è°ƒæ”¶æ•›æ…¢çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Stable Diffusionæ¨¡å‹ä¸Šå®ç°äº†å¿«é€Ÿã€å¤šæ ·æ€§å’Œä¿æŒå…ˆéªŒçŸ¥è¯†çš„å¾®è°ƒï¼Œé€‚ç”¨äºä¸åŒçš„çœŸå®å¥–åŠ±å‡½æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ åœ¨æ‰©æ•£æ¨¡å‹å¾®è°ƒä¸­çš„åº”ç”¨ï¼Œæå‡ºNabla-GFlowNetæ–¹æ³•ã€‚</li>
<li>Nabla-GFlowNetåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸­çš„ä¸°å¯Œä¿¡å·è¿›è¡Œæ¦‚ç‡æ‰©æ•£å¾®è°ƒã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹å¥–åŠ±å¾®è°ƒæ–¹æ³•å­˜åœ¨æ ·æœ¬å¤šæ ·æ€§ä¸è¶³ã€å…ˆéªŒçŸ¥è¯†ä¸¢å¤±å’Œå¾®è°ƒæ”¶æ•›æ…¢çš„é—®é¢˜ã€‚</li>
<li>Nabla-GFlowNetåœ¨Stable Diffusionæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¿«é€Ÿã€å¤šæ ·æ€§å’Œä¿æŒå…ˆéªŒçŸ¥è¯†çš„å¾®è°ƒã€‚</li>
<li>Nabla-GFlowNeté€‚ç”¨äºä¸åŒçš„çœŸå®å¥–åŠ±å‡½æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-996773d3d3466e9aef67aea08916df76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21950de2b5b2e07066fd3abf8159ca34.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="OSMamba-Omnidirectional-Spectral-Mamba-with-Dual-Domain-Prior-Generator-for-Exposure-Correction"><a href="#OSMamba-Omnidirectional-Spectral-Mamba-with-Dual-Domain-Prior-Generator-for-Exposure-Correction" class="headerlink" title="OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator   for Exposure Correction"></a>OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator   for Exposure Correction</h2><p><strong>Authors:Gehui Li, Bin Chen, Chen Zhao, Lei Zhang, Jian Zhang</strong></p>
<p>Exposure correction is a fundamental problem in computer vision and image processing. Recently, frequency domain-based methods have achieved impressive improvement, yet they still struggle with complex real-world scenarios under extreme exposure conditions. This is due to the local convolutional receptive fields failing to model long-range dependencies in the spectrum, and the non-generative learning paradigm being inadequate for retrieving lost details from severely degraded regions. In this paper, we propose Omnidirectional Spectral Mamba (OSMamba), a novel exposure correction network that incorporates the advantages of state space models and generative diffusion models to address these limitations. Specifically, OSMamba introduces an omnidirectional spectral scanning mechanism that adapts Mamba to the frequency domain to capture comprehensive long-range dependencies in both the amplitude and phase spectra of deep image features, hence enhancing illumination correction and structure recovery. Furthermore, we develop a dual-domain prior generator that learns from well-exposed images to generate a degradation-free diffusion prior containing correct information about severely under- and over-exposed regions for better detail restoration. Extensive experiments on multiple-exposure and mixed-exposure datasets demonstrate that the proposed OSMamba achieves state-of-the-art performance both quantitatively and qualitatively. </p>
<blockquote>
<p>æ›å…‰æ ¡æ­£ï¼ˆExposure Correctionï¼‰æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚è™½ç„¶åŸºäºé¢‘åŸŸçš„æ–¹æ³•è¿‘æœŸå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ”¹è¿›ï¼Œä½†åœ¨æç«¯æ›å…‰æ¡ä»¶ä¸‹çš„å¤æ‚ç°å®åœºæ™¯ä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™æ˜¯å› ä¸ºå±€éƒ¨å·ç§¯æ„Ÿå—é‡æ— æ³•å¯¹å…‰è°±ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä¸”éç”Ÿæˆæ€§å­¦ä¹ èŒƒå¼æ— æ³•ä»ä¸¥é‡é€€åŒ–çš„åŒºåŸŸä¸­æ£€ç´¢ä¸¢å¤±çš„ç»†èŠ‚ã€‚é’ˆå¯¹è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†å…¨æ–¹ä½å…‰è°±Mambaï¼ˆOSMambaï¼‰ç½‘ç»œã€‚å®ƒç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼ŒOSMambaå¼•å…¥äº†ä¸€ç§å…¨æ–¹ä½é¢‘è°±æ‰«ææœºåˆ¶ï¼Œä½¿Mambaèƒ½å¤Ÿé€‚åº”é¢‘åŸŸï¼Œæ•è·æ·±åº¦å›¾åƒç‰¹å¾çš„å¹…åº¦å’Œç›¸ä½è°±ä¸­çš„å…¨é¢é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä»è€Œå¢å¼ºç…§æ˜æ ¡æ­£å’Œç»“æ„æ¢å¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒåŸŸå…ˆéªŒç”Ÿæˆå™¨ï¼Œå®ƒä»æ›å…‰è‰¯å¥½çš„å›¾åƒä¸­å­¦ä¹ ï¼Œç”Ÿæˆä¸€ä¸ªæ— é€€åŒ–æ‰©æ•£å…ˆéªŒï¼Œå…¶ä¸­åŒ…å«å…³äºä¸¥é‡æ¬ æ›å…‰å’Œè¿‡æ›å…‰åŒºåŸŸçš„æ­£ç¡®ä¿¡æ¯ï¼Œä»¥æ›´å¥½åœ°æ¢å¤ç»†èŠ‚ã€‚åœ¨å¤šç§æ›å…‰å’Œæ··åˆæ›å…‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„OSMambaåœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15255v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºOmnidirectional Spectral Mambaï¼ˆOSMambaï¼‰çš„æ–°å‹æ›å…‰æ ¡æ­£ç½‘ç»œï¼Œç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè§£å†³äº†ç°æœ‰é¢‘ç‡åŸŸæ–¹æ³•åœ¨å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯ä¸‹çš„å±€é™æ€§ã€‚OSMambaé€šè¿‡å¼•å…¥å…¨æ–¹å‘é¢‘è°±æ‰«ææœºåˆ¶ï¼Œé€‚åº”Mambaåˆ°é¢‘ç‡åŸŸï¼Œæ•æ‰æ·±åº¦å›¾åƒç‰¹å¾çš„å¹…åº¦å’Œç›¸ä½é¢‘è°±ä¸­çš„å…¨é¢é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜ç…§æ˜æ ¡æ­£å’Œç»“æ„æ¢å¤èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§åŒåŸŸå…ˆéªŒç”Ÿæˆå™¨ï¼Œä»æ›å…‰è‰¯å¥½çš„å›¾åƒä¸­å­¦ä¹ ï¼Œç”Ÿæˆä¸å«é™è´¨çš„æ‰©æ•£å…ˆéªŒï¼ŒåŒ…å«å…³äºä¸¥é‡æ¬ æ›å’Œè¿‡æ›åŒºåŸŸçš„æ­£ç¡®ä¿¡æ¯ï¼Œä»¥æ›´å¥½åœ°æ¢å¤ç»†èŠ‚ã€‚åœ¨å¤šä¸ªæ›å…‰å’Œæ··åˆæ›å…‰æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„OSMambaåœ¨é‡å’Œè´¨ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ›å…‰æ ¡æ­£æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†ä¸­çš„åŸºç¡€é—®é¢˜ï¼Œè¿‘æœŸé¢‘ç‡åŸŸæ–¹æ³•è™½ç„¶æœ‰æ‰€æ”¹è¿›ï¼Œä½†åœ¨æç«¯æ›å…‰æ¡ä»¶ä¸‹ä»é¢ä¸´å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› å±€éƒ¨å·ç§¯æ„Ÿå—é‡æ— æ³•å»ºæ¨¡é•¿ç¨‹ä¾èµ–å…³ç³»å’Œéç”Ÿæˆå­¦ä¹ èŒƒå¼ä¸è¶³ï¼Œéš¾ä»¥ä»ä¸¥é‡é€€åŒ–åŒºåŸŸä¸­æ£€ç´¢ä¸¢å¤±çš„ç»†èŠ‚ã€‚</li>
<li>æå‡ºçš„OSMambaç½‘ç»œç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>OSMambaé€šè¿‡å…¨æ–¹å‘é¢‘è°±æ‰«ææœºåˆ¶é€‚åº”Mambaåˆ°é¢‘ç‡åŸŸï¼Œåœ¨æ·±åº¦å›¾åƒç‰¹å¾çš„å¹…åº¦å’Œç›¸ä½é¢‘è°±ä¸­æ•æ‰å…¨é¢çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åŒåŸŸå…ˆéªŒç”Ÿæˆå™¨ï¼Œå¯ä»¥ä»è‰¯å¥½æ›å…‰çš„å›¾åƒä¸­å­¦ä¹ ï¼Œç”Ÿæˆæ‰©æ•£å…ˆéªŒä»¥æ›´å¥½åœ°æ¢å¤ç»†èŠ‚ã€‚</li>
<li>OSMambaåœ¨å¤šä¸ªæ›å…‰å’Œæ··åˆæ›å…‰æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe3fa6cafb6e6386369f2d6b055cb1b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79341293cb046cb98778324e732d6989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e724a35aec37f626cd1686757981b5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a2841e6e4407f82146592b1609ee31.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CleAR-Robust-Context-Guided-Generative-Lighting-Estimation-for-Mobile-Augmented-Reality"><a href="#CleAR-Robust-Context-Guided-Generative-Lighting-Estimation-for-Mobile-Augmented-Reality" class="headerlink" title="CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile   Augmented Reality"></a>CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile   Augmented Reality</h2><p><strong>Authors:Yiqin Zhao, Mallesham Dasari, Tian Guo</strong></p>
<p>High-quality environment lighting is essential for creating immersive mobile augmented reality (AR) experiences. However, achieving visually coherent estimation for mobile AR is challenging due to several key limitations in AR device sensing capabilities, including low camera FoV and limited pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address two key limitations of content quality and slow inference. In this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality, diverse environment maps in the format of 360{\deg} HDR images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the output aligns with the physical environmentâ€™s visual context and color appearance. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. Through a combination of quantitative and qualitative evaluations, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy, latency, and robustness, and is rated by 31 participants as producing better renderings for most virtual objects. For example, CleAR achieves 51% to 56% accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties. CleAR produces lighting estimates of comparable or better quality in just 3.2 seconds â€“ over 110X faster than state-of-the-art methods. </p>
<blockquote>
<p>é«˜è´¨é‡çš„ç¯å¢ƒç…§æ˜å¯¹äºåˆ›å»ºæ²‰æµ¸å¼çš„ç§»åŠ¨å¢å¼ºç°å®ï¼ˆARï¼‰ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºARè®¾å¤‡æ„ŸçŸ¥èƒ½åŠ›çš„å‡ ä¸ªå…³é”®é™åˆ¶ï¼Œå¦‚ç›¸æœºè§†é‡è¾ƒå°å’Œåƒç´ åŠ¨æ€èŒƒå›´æœ‰é™ï¼Œå®ç°ç§»åŠ¨ARçš„è§†è§‰ä¸€è‡´æ€§ä¼°è®¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å‘å±•å¯ä»¥ä»ä¸åŒç±»å‹çš„æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒï¼Œè¿™ä¸ºé«˜è´¨é‡ç…§æ˜ä¼°è®¡æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¸ºäº†æœ‰æ•ˆåœ°ä½¿ç”¨ç”Ÿæˆå¼å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³å†…å®¹è´¨é‡å’Œæ¨ç†é€Ÿåº¦æ…¢çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶å®ç°äº†ä¸€ä¸ªåä¸ºCleARçš„ç”Ÿæˆå¼ç…§æ˜ä¼°è®¡ç³»ç»Ÿï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„ç¯å¢ƒåœ°å›¾ï¼Œä»¥360Â°HDRå›¾åƒæ ¼å¼å‘ˆç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç”±ARç¯å¢ƒä¸Šä¸‹æ–‡æ•°æ®å¼•å¯¼çš„ä¸¤æ­¥ç”Ÿæˆç®¡é“ï¼Œä»¥ç¡®ä¿è¾“å‡ºä¸ç‰©ç†ç¯å¢ƒçš„è§†è§‰ä¸Šä¸‹æ–‡å’Œé¢œè‰²å¤–è§‚ä¿æŒä¸€è‡´ã€‚ä¸ºäº†æé«˜ä¸åŒç…§æ˜æ¡ä»¶ä¸‹çš„ä¼°è®¡ç¨³å¥æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®æ—¶ä¿®æ­£ç»„ä»¶æ¥è°ƒæ•´ARè®¾å¤‡ä¸Šçš„ç…§æ˜ä¼°è®¡ç»“æœã€‚é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°çš„ç»“åˆï¼Œæˆ‘ä»¬è¯æ˜äº†CleARåœ¨ä¼°è®¡ç²¾åº¦ã€å»¶è¿Ÿå’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºæœ€æ–°çš„ç…§æ˜ä¼°è®¡æ–¹æ³•ï¼Œå¹¶è¢«31åå‚ä¸è€…è¯„ä»·ä¸ºå¯¹å¤§å¤šæ•°è™šæ‹Ÿå¯¹è±¡äº§ç”Ÿäº†æ›´å¥½çš„æ¸²æŸ“æ•ˆæœã€‚ä¾‹å¦‚ï¼ŒCleARåœ¨ä¸‰ç§ä¸åŒæè´¨å’Œåå°„å±æ€§çš„å¯¹è±¡ä¸Šå®ç°äº†è™šæ‹Ÿå¯¹è±¡æ¸²æŸ“çš„51%è‡³56%çš„å‡†ç¡®åº¦æå‡ã€‚CleARä»…éœ€3.2ç§’å³å¯ç”Ÿæˆç›¸å½“æˆ–æ›´é«˜è´¨é‡çš„ç…§æ˜ä¼°è®¡â€”â€”æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å¿«è¶…è¿‡110å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02179v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é«˜è´¨é‡çš„ç¯å¢ƒå…‰ç…§å¯¹äºåˆ›å»ºæ²‰æµ¸å¼çš„ç§»åŠ¨å¢å¼ºç°å®ï¼ˆARï¼‰ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºARè®¾å¤‡æ„ŸçŸ¥èƒ½åŠ›çš„å…³é”®é™åˆ¶ï¼Œå¦‚ä½ç›¸æœºè§†é‡å’Œæœ‰é™çš„åƒç´ åŠ¨æ€èŒƒå›´ï¼Œå®ç°ç§»åŠ¨ARçš„è§†è§‰è¿è´¯æ€§ä¼°è®¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘ç”Ÿæˆå¼AIçš„è¿›å±•ï¼Œå¯ä»¥ä»ä¸åŒç±»å‹çš„æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä¸ºè§£å†³é«˜è´¨é‡å…‰ç…§ä¼°è®¡é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¸ºäº†æœ‰æ•ˆåœ°ä½¿ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»è§£å†³å†…å®¹è´¨é‡å’Œæ¨ç†é€Ÿåº¦æ…¢çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ã€‚æœ¬ç ”ç©¶è®¾è®¡å¹¶å®æ–½äº†ä¸€ä¸ªåä¸ºCleARçš„ç”Ÿæˆå¼å…‰ç…§ä¼°è®¡ç³»ç»Ÿï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„ç¯å¢ƒåœ°å›¾ï¼Œä»¥360Â°HDRå›¾åƒæ ¼å¼å‘ˆç°ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»“åˆå®šé‡å’Œå®šæ€§è¯„ä¼°ï¼ŒéªŒè¯äº†CleARåœ¨ä¼°è®¡ç²¾åº¦ã€å»¶è¿Ÿå’Œç¨³å¥æ€§æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºå½“å‰å…ˆè¿›çš„å…‰ç…§ä¼°è®¡æ–¹æ³•ï¼Œå¹¶è¢«31ä½å‚ä¸è€…è¯„ä»·ä¸ºå¯¹å¤§å¤šæ•°è™šæ‹Ÿç‰©ä½“çš„æ¸²æŸ“æ•ˆæœæ›´å¥½ã€‚ä¾‹å¦‚ï¼ŒCleARåœ¨ä¸‰ç§ä¸åŒæè´¨å’Œåå°„å±æ€§çš„ç‰©ä½“ä¸Šå®ç°äº†å¯¹è™šæ‹Ÿç‰©ä½“æ¸²æŸ“çš„51%è‡³56%çš„å‡†ç¡®æ€§æ”¹è¿›ï¼Œä¸”åœ¨ä»…3.2ç§’å†…ç”Ÿæˆäº†è´¨é‡ç›¸å½“æˆ–æ›´å¥½çš„å…‰ç…§ä¼°è®¡â€”â€”æ¯”å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•å¿«è¶…è¿‡110å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡ç¯å¢ƒå…‰ç…§å¯¹ç§»åŠ¨ARä½“éªŒè‡³å…³é‡è¦ï¼Œä½†å®ç°è§†è§‰è¿è´¯æ€§ä¼°è®¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç”Ÿæˆå¼AIä¸ºè§£å†³é«˜è´¨é‡å…‰ç…§ä¼°è®¡é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶è®¾è®¡å¹¶å®æ–½äº†ä¸€ä¸ªåä¸ºCleARçš„ç”Ÿæˆå¼å…‰ç…§ä¼°è®¡ç³»ç»Ÿï¼Œå¯ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡ç¯å¢ƒåœ°å›¾ã€‚</li>
<li>CleARåœ¨ä¼°è®¡ç²¾åº¦ã€å»¶è¿Ÿå’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CleARèƒ½å¤Ÿæœ‰æ•ˆæé«˜è™šæ‹Ÿç‰©ä½“æ¸²æŸ“çš„å‡†ç¡®åº¦ï¼Œå°¤å…¶æ˜¯å¯¹ä¸‰ç§ä¸åŒæè´¨å’Œåå°„å±æ€§çš„ç‰©ä½“ã€‚</li>
<li>CleARåœ¨å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å…‰ç…§ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…éœ€è¦3.2ç§’ï¼Œæ¯”ç°æœ‰æ–¹æ³•å¿«è¶…è¿‡110å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fd18996e15566ec16e03094d90ced5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a033d50e3cb156268a2e89cecb7ef38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4374dada49c60dd1f4754a9a1d224215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84a9b95d86ca92d5dd0da1f6cceaf0f1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Predicting-Temporal-Changes-in-a-Patientâ€™s-Chest-X-ray-Images-based-on-Electronic-Health-Records"><a href="#Towards-Predicting-Temporal-Changes-in-a-Patientâ€™s-Chest-X-ray-Images-based-on-Electronic-Health-Records" class="headerlink" title="Towards Predicting Temporal Changes in a Patientâ€™s Chest X-ray Images   based on Electronic Health Records"></a>Towards Predicting Temporal Changes in a Patientâ€™s Chest X-ray Images   based on Electronic Health Records</h2><p><strong>Authors:Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi</strong></p>
<p>Chest X-ray (CXR) is an important diagnostic tool widely used in hospitals to assess patient conditions and monitor changes over time. Recently, generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic CXRs. However, these models mainly focus on conditional generation using single-time-point data, i.e., generating CXRs conditioned on their corresponding reports from a specific time. This limits their clinical utility, particularly for capturing temporal changes. To address this limitation, we propose a novel framework, EHRXDiff, which predicts future CXR images by integrating previous CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc. Our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous CXR image and a history of medical events. We comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. Results show that our framework generates high-quality, realistic future images that effectively capture potential temporal changes. This suggests that our framework could be further developed to support clinical decision-making and provide valuable insights for patient monitoring and treatment planning in the medical field. The code is available at <a target="_blank" rel="noopener" href="https://github.com/dek924/EHRXDiff">https://github.com/dek924/EHRXDiff</a>. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æ˜¯åŒ»é™¢ä¸­å¹¿æ³›ä½¿ç”¨çš„é‡è¦è¯Šæ–­å·¥å…·ï¼Œç”¨äºè¯„ä¼°æ‚£è€…çŠ¶å†µå’Œç›‘æµ‹éšæ—¶é—´çš„å˜åŒ–ã€‚æœ€è¿‘ï¼Œç”Ÿæˆå¼æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆé€¼çœŸçš„åˆæˆCXRæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨å•æ—¶é—´ç‚¹æ•°æ®çš„æ¡ä»¶ç”Ÿæˆä¸Šï¼Œå³æ ¹æ®ç‰¹å®šæ—¶é—´çš„ç›¸åº”æŠ¥å‘Šç”ŸæˆCXRã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠä¸Šçš„å®ç”¨æ€§ï¼Œå°¤å…¶æ˜¯ç”¨äºæ•æ‰æ—¶é—´å˜åŒ–çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶EHRXDiffï¼Œå®ƒé€šè¿‡æ•´åˆå…ˆå‰çš„CXRå’Œéšåçš„åŒ»ç–—äº‹ä»¶ï¼ˆå¦‚å¤„æ–¹ã€å®éªŒå®¤æµ‹é‡ç­‰ï¼‰æ¥é¢„æµ‹æœªæ¥çš„CXRå›¾åƒã€‚æˆ‘ä»¬çš„æ¡†æ¶åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹åŠ¨æ€è·Ÿè¸ªå’Œé¢„æµ‹ç–¾ç—…è¿›å±•ï¼Œä»¥å…ˆå‰çš„CXRå›¾åƒå’ŒåŒ»ç–—äº‹ä»¶å†å²ä¸ºæ¡ä»¶ã€‚æˆ‘ä»¬ä»ä¸´åºŠä¸€è‡´æ€§ã€äººå£ç»Ÿè®¡å­¦ä¸€è‡´æ€§å’Œè§†è§‰çœŸå®æ€§ä¸‰ä¸ªæ–¹é¢å…¨é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç”Ÿæˆçš„é«˜è´¨é‡ã€é€¼çœŸçš„æœªæ¥å›¾åƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰æ½œåœ¨çš„ä¸´æ—¶å˜åŒ–ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥è¿›ä¸€æ­¥å‘å±•ï¼Œä»¥æ”¯æŒä¸´åºŠå†³ç­–åˆ¶å®šï¼Œä¸ºåŒ»å­¦é¢†åŸŸçš„æ‚£è€…ç›‘æµ‹å’Œæ²»ç–—è®¡åˆ’æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dek924/EHRXDiff%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dek924/EHRXDiffæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07012v2">PDF</a> Accepted at Proc. of Conference on Health, Inference, and Learning   (CHIL) 2025 (10 pages for main text, 3 pages for references, 8 pages for   supplementary materials)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶EHRXDiffï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹é¢„æµ‹æœªæ¥çš„èƒ¸éƒ¨Xå…‰å›¾åƒã€‚å®ƒä¸ä»…åŸºäºå…ˆå‰çš„CXRå›¾åƒï¼Œè¿˜ç»“åˆäº†åç»­åŒ»ç–—äº‹ä»¶ï¼Œå¦‚å¤„æ–¹ã€å®éªŒå®¤æµ‹é‡ç­‰ï¼Œä»¥åŠ¨æ€è¿½è¸ªå’Œé¢„æµ‹ç–¾ç—…è¿›å±•ã€‚è¯¥æ¡†æ¶åœ¨ä¸´åºŠä¸€è‡´æ€§ã€äººå£ç»Ÿè®¡ä¸€è‡´æ€§å’Œè§†è§‰é€¼çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„æœªæ¥å›¾åƒé«˜è´¨é‡ä¸”çœŸå®ï¼Œèƒ½æœ‰æ•ˆæ•æ‰æ½œåœ¨çš„æš‚æ—¶å˜åŒ–ã€‚è¿™æœ‰åŠ©äºä¸´åºŠå†³ç­–åˆ¶å®šã€æ‚£è€…ç›‘æµ‹å’Œæ²»ç–—è®¡åˆ’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆçœŸå®çš„åˆæˆèƒ¸éƒ¨Xå…‰å›¾åƒæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>å½“å‰æ¨¡å‹ä¸»è¦å…³æ³¨åŸºäºå•æ—¶é—´ç‚¹æ•°æ®çš„æ¡ä»¶ç”Ÿæˆï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„ç”¨é€”ã€‚</li>
<li>EHRXDiffæ¡†æ¶é€šè¿‡ç»“åˆå…ˆå‰çš„CXRå›¾åƒå’Œåç»­åŒ»ç–—äº‹ä»¶ï¼Œå¦‚å¤„æ–¹å’Œå®éªŒå®¤æµ‹é‡ï¼Œæ¥é¢„æµ‹æœªæ¥çš„CXRå›¾åƒã€‚</li>
<li>EHRXDiffæ¡†æ¶åˆ©ç”¨æ½œä¼æ‰©æ•£æ¨¡å‹åŠ¨æ€è¿½è¸ªå’Œé¢„æµ‹ç–¾ç—…è¿›å±•ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¸´åºŠä¸€è‡´æ€§ã€äººå£ç»Ÿè®¡ä¸€è‡´æ€§å’Œè§†è§‰é€¼çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>EHRXDiffç”Ÿæˆçš„æœªæ¥å›¾åƒé«˜è´¨é‡ä¸”çœŸå®ï¼Œèƒ½æœ‰æ•ˆæ•æ‰æ½œåœ¨çš„ä¸´æ—¶å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6322316e6e30e464c5f9d1d1bd99ea61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78253ae18b3a429e50cd0a209ca3d0af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15dc9c2f797598b2384fcc859a3d9697.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a69d013bc596200f99b1d23237f8be5c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Paragraph-to-Image-Generation-with-Information-Enriched-Diffusion-Model"><a href="#Paragraph-to-Image-Generation-with-Information-Enriched-Diffusion-Model" class="headerlink" title="Paragraph-to-Image Generation with Information-Enriched Diffusion Model"></a>Paragraph-to-Image Generation with Information-Enriched Diffusion Model</h2><p><strong>Authors:Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, Di Zhang</strong></p>
<p>Text-to-image (T2I) models have recently experienced rapid development, achieving astonishing performance in terms of fidelity and textual alignment capabilities. However, given a long paragraph (up to 512 words), these generation models still struggle to achieve strong alignment and are unable to generate images depicting complex scenes. In this paper, we introduce an information-enriched diffusion model for paragraph-to-image generation task, termed ParaDiffusion, which delves into the transference of the extensive semantic comprehension capabilities of large language models to the task of image generation. At its core is using a large language model (e.g., Llama V2) to encode long-form text, followed by fine-tuning with LORA to alignthe text-image feature spaces in the generation task. To facilitate the training of long-text semantic alignment, we also curated a high-quality paragraph-image pair dataset, namely ParaImage. This dataset contains a small amount of high-quality, meticulously annotated data, and a large-scale synthetic dataset with long text descriptions being generated using a vision-language model. Experiments demonstrate that ParaDiffusion outperforms state-of-the-art models (SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45% human voting rate improvements for visual appeal and text faithfulness, respectively. The code and dataset will be released to foster community research on long-text alignment. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹æœ€è¿‘ç»å†äº†å¿«é€Ÿå‘å±•ï¼Œåœ¨ä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½èƒ½åŠ›æ–¹é¢å–å¾—äº†æƒŠäººçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºé•¿è¾¾512å­—çš„æ®µè½ï¼Œè¿™äº›ç”Ÿæˆæ¨¡å‹ä»ç„¶éš¾ä»¥å®ç°å¯¹é½ï¼Œå¹¶ä¸”æ— æ³•ç”Ÿæˆæè¿°å¤æ‚åœºæ™¯çš„å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºæ®µè½åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡çš„ä¸°å¯Œä¿¡æ¯æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸ºParaDiffusionï¼Œå®ƒæ·±å…¥ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯­ä¹‰ç†è§£èƒ½åŠ›çš„è½¬ç§»åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚å…¶æ ¸å¿ƒæ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚Llama V2ï¼‰å¯¹é•¿æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œç„¶åä½¿ç”¨LORAè¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å¯¹é½æ–‡æœ¬å›¾åƒç‰¹å¾ç©ºé—´ã€‚ä¸ºäº†è®­ç»ƒé•¿æ–‡æœ¬è¯­ä¹‰å¯¹é½ï¼Œæˆ‘ä»¬è¿˜ç²¾å¿ƒåˆ¶ä½œäº†ä¸€ä¸ªé«˜è´¨é‡æ®µè½å›¾åƒå¯¹æ•°æ®é›†ï¼Œå³ParaImageã€‚è¯¥æ•°æ®é›†åŒ…å«å°‘é‡é«˜è´¨é‡ã€ç²¾å¿ƒæ³¨é‡Šçš„æ•°æ®ï¼Œä»¥åŠä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆå…·æœ‰é•¿æ–‡æœ¬æè¿°çš„æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒParaDiffusionåœ¨ViLG-300å’ŒParaPromptsä¸Šä¼˜äºç°æœ‰æ¨¡å‹ï¼ˆSD XLã€DeepFloyd IFï¼‰ï¼Œåœ¨è§†è§‰å¸å¼•åŠ›å’Œæ–‡æœ¬å¿ å®åº¦æ–¹é¢åˆ†åˆ«æé«˜äº†é«˜è¾¾15%å’Œ45%çš„äººç±»æŠ•ç¥¨ç‡ã€‚ä»£ç å’Œæ•°æ®é›†å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå¯¹é•¿æ–‡æœ¬å¯¹é½çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.14284v3">PDF</a> The project website is at:   <a target="_blank" rel="noopener" href="https://weijiawu.github.io/ParaDiffusionPage/">https://weijiawu.github.io/ParaDiffusionPage/</a>. Code:   <a target="_blank" rel="noopener" href="https://github.com/weijiawu/ParaDiffusion">https://github.com/weijiawu/ParaDiffusion</a></p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿¡æ¯ä¸°å¯Œçš„æ‰©æ•£æ¨¡å‹â€”â€”ParaDiffusionï¼Œç”¨äºæ®µè½åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama V2ï¼‰å¯¹é•¿æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶ä½¿ç”¨LORAå¾®è°ƒä»¥åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å¯¹é½æ–‡æœ¬å›¾åƒç‰¹å¾ç©ºé—´ã€‚ä¸ºè®­ç»ƒé•¿æ–‡æœ¬è¯­ä¹‰å¯¹é½ï¼Œè¿˜æ¨å‡ºäº†é«˜è´¨é‡çš„æ®µè½å›¾åƒé…å¯¹æ•°æ®é›†ParaImageã€‚å®éªŒè¡¨æ˜ï¼ŒParaDiffusionåœ¨ViLG-300å’ŒParaPromptsä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼ˆSD XLã€DeepFloyd IFï¼‰ï¼Œè§†è§‰å¸å¼•åŠ›å’Œæ–‡æœ¬å¿ å®åº¦åˆ†åˆ«æé«˜äº†é«˜è¾¾15%å’Œ45%ã€‚ä»£ç å’Œæ•°æ®é›†å°†å‘å¸ƒï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå¯¹é•¿æ–‡æœ¬å¯¹é½çš„ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>T2Iæ¨¡å‹åœ¨æ®µè½ï¼ˆé•¿è¾¾512å­—ï¼‰ç”Ÿæˆå›¾åƒæ—¶é¢ä¸´å¯¹é½å›°éš¾ï¼Œæ— æ³•ç”Ÿæˆæè¿°å¤æ‚åœºæ™¯çš„å›¾åƒã€‚</li>
<li>æå‡ºäº†ä¿¡æ¯ä¸°å¯Œçš„æ‰©æ•£æ¨¡å‹ParaDiffusionï¼Œç”¨äºæ®µè½åˆ°å›¾åƒçš„ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ParaDiffusionåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llama V2ï¼‰å¯¹é•¿æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚</li>
<li>ä½¿ç”¨LORAå¾®è°ƒä»¥åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å¯¹é½æ–‡æœ¬å›¾åƒç‰¹å¾ç©ºé—´ã€‚</li>
<li>æ¨å‡ºäº†é«˜è´¨é‡çš„æ®µè½å›¾åƒé…å¯¹æ•°æ®é›†ParaImageï¼ŒåŒ…å«å°‘é‡é«˜è´¨é‡ã€ç²¾å¿ƒæ³¨é‡Šçš„æ•°æ®å’Œå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒParaDiffusionåœ¨è§†è§‰å¸å¼•åŠ›å’Œæ–‡æœ¬å¿ å®åº¦æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ä»£ç å’Œæ•°æ®é›†çš„å‘å¸ƒå°†ä¿ƒè¿›ç¤¾åŒºå¯¹é•¿æ–‡æœ¬å¯¹é½çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.14284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a35b449fcb9f0a8db558372b77e74adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-823313dc20af68b741c289f39cad3941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758dd5753b3502e6d456cb465add1753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b294b4ab889900ffde12b74dec81bc13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d357dd489ccdd5bbe71244ca38f7742b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8eafafb74494b75b1ab78a586a76cc1a.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  A Robust Monotonic Single-Index Model for Skewed and Heavy-Tailed Data   A Deep Neural Network Approach Applied to Periodontal Studies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5cbe5993b0e82f86f21ec2593ea7f2ee.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  HandOcc NeRF-based Hand Rendering with Occupancy Networks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
