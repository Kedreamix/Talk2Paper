<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-08  Reinforced Correlation Between Vision and Language for Precise Medical   AI Assistant">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-4da8151e6c828851e11c0a4b21947103.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-08-更新"><a href="#2025-05-08-更新" class="headerlink" title="2025-05-08 更新"></a>2025-05-08 更新</h1><h2 id="Reinforced-Correlation-Between-Vision-and-Language-for-Precise-Medical-AI-Assistant"><a href="#Reinforced-Correlation-Between-Vision-and-Language-for-Precise-Medical-AI-Assistant" class="headerlink" title="Reinforced Correlation Between Vision and Language for Precise Medical   AI Assistant"></a>Reinforced Correlation Between Vision and Language for Precise Medical   AI Assistant</h2><p><strong>Authors:Haonan Wang, Jiaji Mao, Lehan Wang, Qixiang Zhang, Marawan Elbatel, Yi Qin, Huijun Hu, Baoxun Li, Wenhui Deng, Weifeng Qin, Hongrui Li, Jialin Liang, Jun Shen, Xiaomeng Li</strong></p>
<p>Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation. However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. We propose RCMed, a full-stack AI assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. A self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. This correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. Trained on 20 million image-mask-description triplets, RCMed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. RCMed’s strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare. </p>
<blockquote>
<p>医疗人工智能助手支持医生进行疾病诊断、医学图像分析和报告生成。然而，他们在临床使用中还面临诸多挑战，包括多模态内容精度有限和真实世界环境中的验证不足。我们提出了RCMed，这是一个端到端的人工智能助手，它改进了输入和输出的多模态对齐，能够通过分层视觉语言定位实现精确解剖划分、准确定位和可靠诊断。自我加强的关联机制允许视觉特征为语言上下文提供信息，而语言语义引导像素级注意力，形成一个闭环，精炼了两种模式。这种关联通过颜色区域描述策略得到加强，将解剖结构转化为语义丰富的文本，学习跨尺度的形状-位置-文本关系。在2000万个图像-掩膜-描述三元组上进行训练，RCMed在定位不规则病变和微妙解剖边界方面达到了最先进的精度，在9种模式的165个临床任务中表现出色。与先前的方法相比，它在显微镜图像的细胞分割方面实现了相对23.5%的改进。RCMed强大的视觉语言对齐功能实现了出色的泛化能力，在20种具有临床意义的癌症类型的外部验证中达到了最先进的性能，包括新任务。这项工作展示了集成多模态模型如何捕捉精细模式，在复杂场景中实现人类水平的解释能力，并推动以人为本的人工智能医疗保健的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03380v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>医疗人工智能助手在疾病诊断、医学图像分析和报告生成方面支持医生工作。然而，在实际应用中仍面临多模态内容准确性有限和真实世界环境下验证不足等挑战。本文提出一种全栈式人工智能助手RCMed，通过改进多模态输入输出的对齐方式，实现精确解剖界定、准确定位和可靠诊断。其采用层次化的视觉语言定位技术，通过自我加强的关联机制，实现视觉特征与语言语境的信息交流，提高两种模态的精度。在200万张图像、遮罩和描述三元组训练数据支持下，RCMed在定位不规则病变和微妙解剖边界方面达到最新精准度水平，并在165项临床任务中表现出卓越性能。此外，RCMed在显微镜图像细胞分割方面较前期方法实现了23.5%的相对改进。其强大的视觉语言对齐能力使RCMed在外部验证中表现出卓越泛化能力，涵盖20种临床重要的癌症类型，包括新型任务。本研究展示了集成多模态模型如何捕捉细微模式，实现复杂场景下的人类水平解读，推动以人为中心的人工智能医疗健康发展。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>医疗AI助手在临床应用中面临多模态内容准确性和真实世界验证挑战。</li>
<li>RCMed是一种全栈式AI助手，改进了多模态输入输出的对齐方式。</li>
<li>RCMed实现了视觉特征与语言语境的信息交流，提高诊断精确性。</li>
<li>RCMed在多种临床任务中表现优异，达到最新精准度水平。</li>
<li>RCMed在显微镜图像细胞分割方面较前期方法有显著改进。</li>
<li>RCMed具有强大的视觉语言对齐能力，广泛适用于不同类型的癌症诊断。</li>
<li>集成多模态模型有助于捕捉细微模式，推动医疗AI的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf44027f6506612edfcc9161751ee351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d54dd1a7739034f7a54716a72217468.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c99896f4e1a482c2338943875b5b4f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Seeing-the-Abstract-Translating-the-Abstract-Language-for-Vision-Language-Models"><a href="#Seeing-the-Abstract-Translating-the-Abstract-Language-for-Vision-Language-Models" class="headerlink" title="Seeing the Abstract: Translating the Abstract Language for Vision   Language Models"></a>Seeing the Abstract: Translating the Abstract Language for Vision   Language Models</h2><p><strong>Authors:Davide Talon, Federico Girella, Ziyue Liu, Marco Cristani, Yiming Wang</strong></p>
<p>Natural language goes beyond dryly describing visual content. It contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. Yet, current research in Vision Language Models (VLMs) has not shed light on abstract-oriented language. Our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. Particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. By analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. However, a critical challenge emerges: current general-purpose or fashion-specific VLMs are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. We propose a training-free and model-agnostic method, Abstract-to-Concrete Translator (ACT), to shift abstract representations towards well-represented concrete ones in the VLM latent space, using pre-trained models and existing multimodal databases. On the text-to-image retrieval task, despite being training-free, ACT outperforms the fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. Moreover, the improvement introduced by ACT is consistent with various VLMs, making it a plug-and-play solution. </p>
<blockquote>
<p>自然语言不仅仅是枯燥地描述视觉内容。它包含丰富的抽象概念，用于表达情感、创造力和无法直接感知的属性。然而，目前关于视觉语言模型（VLMs）的研究并没有关注抽象导向的语言。我们的研究通过深入的分析，首次揭示了其在多个领域中的广泛存在和低估的价值。特别是，我们将调查重点放在时尚领域，这是一个具有抽象表达的代表性领域。通过分析最新的大规模多模式时尚数据集，我们发现抽象术语的存在占据主导地位，与具体术语相当，提供新颖的信息，并在检索任务中很有用。但是，出现了一个关键挑战：当前的通用或特定于时尚的VLMs都是使用其文本语料库中缺乏足够抽象词的数据库进行预训练的，这阻碍了它们有效地表示抽象导向语言的能力。我们提出了一种无需训练和模型特定的方法，即抽象到具体翻译器（ACT），利用预训练模型和现有多模式数据库，将抽象表示转向在VLM潜在空间中的良好表示的具体表示。在文本到图像检索任务上，尽管ACT是无需训练的，但在同一数据集和跨数据集设置中，它的性能都优于经过微调的VLMs，显示出其强大的通用能力和有效性。此外，ACT所带来的改进与各种VLMs都是一致的，使其成为即插即用的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03242v1">PDF</a> Accepted to CVPR25. Project page:   <a target="_blank" rel="noopener" href="https://davidetalon.github.io/fashionact-page/">https://davidetalon.github.io/fashionact-page/</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了自然语言在描述视觉内容方面的丰富性，尤其是针对时尚领域中的抽象表达。研究发现在大规模时尚数据集中抽象术语普遍存在并具重要价值。然而，现有的视觉语言模型在表示抽象语言方面存在不足。为此，本文提出了一种无需训练的、模型通用的方法——抽象到具体翻译器（ACT），利用预训练模型和现有多模式数据库，将抽象表示转向VLM潜在空间中的良好表示的具体表示。在文本到图像的检索任务上，ACT表现优异，具有良好的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自然语言包含丰富的抽象概念，能够表达感觉、创造力和无法直接感知的属性。</li>
<li>在时尚领域，抽象表达具有代表性，并普遍存在。</li>
<li>大规模时尚数据集中的抽象术语与具体术语相当，为信息检索提供了新颖视角。</li>
<li>现有的视觉语言模型（VLMs）在表示抽象语言方面存在不足，因为它们通常是在缺乏足够抽象词汇的数据库中进行预训练的。</li>
<li>提出了一种无需训练的模型通用方法——抽象到具体翻译器（ACT），以改善VLMs在表示抽象语言方面的不足。</li>
<li>ACT方法在文本到图像检索任务上表现优异，超越了精细调整的VLMs，在同数据集和跨数据集设置中都展现了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03242">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14eda4be0caca2ebd21911a5ede67c28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5cc52ea096e22d2b5139e317c0bfe97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-920151333beada8d72e120f36325d4da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d05986ff567f8687c5e72f180e2a65b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48db6dca3923465f8eb2402b48b48051.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Path-and-Bone-Contour-Regularized-Unpaired-MRI-to-CT-Translation"><a href="#Path-and-Bone-Contour-Regularized-Unpaired-MRI-to-CT-Translation" class="headerlink" title="Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation"></a>Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation</h2><p><strong>Authors:Teng Zhou, Jax Luo, Yuping Sun, Yiheng Tan, Shun Yao, Nazim Haouchine, Scott Raymond</strong></p>
<p>Accurate MRI-to-CT translation promises the integration of complementary imaging information without the need for additional imaging sessions. Given the practical challenges associated with acquiring paired MRI and CT scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on CT but less distinguishable on MRI, such as bone structures. This limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. To address this challenge, we propose a path- and bone-contour regularized approach for unpaired MRI-to-CT translation. In our method, MRI and CT images are projected to a shared latent space, where the MRI-to-CT mapping is modeled as a continuous flow governed by neural ordinary differential equations. The optimal mapping is obtained by minimizing the transition path length of the flow. To enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from MRI and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. Evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired MRI-to-CT translation approaches, achieving lower overall error rates. Moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/kennysyp/PaBoT">https://github.com/kennysyp/PaBoT</a>. </p>
<blockquote>
<p>准确的MRI到CT转换技术能够在不需要额外的成像会话的情况下，实现互补成像信息的集成。考虑到获取配对MRI和CT扫描的实际挑战，开发能够利用未配对数据集的方法对于推进MRI到CT转换技术的发展至关重要。当前主要依赖于循环一致性和对比学习框架的非配对MRI到CT转换方法，在将MRI上不太可分辨但在CT上可清晰识别的解剖特征进行准确转换时经常面临挑战，例如骨骼结构。这一局限性使得这些方法在放射治疗中的应用不太适用，因为精确骨骼表示对于准确的治疗计划至关重要。为了应对这一挑战，我们提出了一种用于非配对MRI到CT转换的路径和骨骼轮廓正则化方法。在我们的方法中，MRI和CT图像被投影到一个共享潜在空间，其中MRI到CT的映射被建模为由神经常微分方程控制的连续流。通过最小化流的过渡路径长度来获得最佳映射。为了提高翻译后骨骼结构的准确性，我们引入了一个可训练的神经网络从MRI生成骨骼轮廓，并实施了直接和间接鼓励模型关注骨骼轮廓及其相邻区域的机制。在三个数据集上的评估表明，我们的方法优于现有的非配对MRI到CT转换方法，实现了更低的总体错误率。此外，在下游的骨骼分割任务中，我们的方法在保持骨骼结构保真度方面表现出卓越的性能。我们的代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/kennysyp/PaBoT">https://github.com/kennysyp/PaBoT</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对非配对MRI-CT转换的新方法，解决了准确翻译在CT上可分辨但在MRI上难以区分的解剖特征（如骨骼结构）的挑战。新方法通过路径和骨骼轮廓正则化进行MRI-CT转换，利用神经常微分方程建模映射关系，并引入可训练的神经网络生成骨骼轮廓以提高翻译准确性。评估结果表明，该方法在三个数据集上的性能优于现有非配对MRI-CT转换方法，并在下游骨骼分割任务中表现出优越的性能。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>新方法解决了MRI-CT转换中准确翻译特定解剖特征的挑战，特别是骨骼结构。</li>
<li>方法通过路径和骨骼轮廓正则化进行MRI-CT转换，映射关系通过神经常微分方程建模。</li>
<li>利用可训练的神经网络生成骨骼轮廓，提高翻译的准确性。</li>
<li>在三个数据集上的评估表明，新方法性能优于现有非配对MRI-CT转换方法。</li>
<li>下游骨骼分割任务中表现出优越性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b15de95c70ecdc90297e186e0845b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d51dc7a9d901808408691949e2b8a01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c996d07367d272d0ad006f15eeb063d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2df5b895ef32a5d3eaa6d2449d39147.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4da8151e6c828851e11c0a4b21947103.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Sim2Real-in-endoscopy-segmentation-with-a-novel-structure-aware-image-translation"><a href="#Sim2Real-in-endoscopy-segmentation-with-a-novel-structure-aware-image-translation" class="headerlink" title="Sim2Real in endoscopy segmentation with a novel structure aware image   translation"></a>Sim2Real in endoscopy segmentation with a novel structure aware image   translation</h2><p><strong>Authors:Clara Tomasini, Luis Riazuelo, Ana C. Murillo</strong></p>
<p>Automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. However, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. While ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. Generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. The main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. Our approach produces realistic images in different endoscopy scenarios. We demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. In particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. Folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. Our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. We run experiments both on a novel simulated dataset for fold segmentation, and real data from the EndoMapper (EM) dataset. All our new generated data and new EM metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation. </p>
<blockquote>
<p>自动分割内窥镜图像中的解剖标志可以为医生、外科医生提供诊断、治疗或医学训练方面的帮助。然而，获取训练常用监督学习方法所需的注释是一项乏味且困难的任务，尤其是对于真实图像。虽然对于合成数据而言，更容易获得真实情况的注释，但以这些数据训练的模型往往对真实数据的泛化能力不佳。生成方法可以在其上增加逼真的纹理，但在保持原始场景结构方面面临困难。这项工作的主要贡献是一种新型图像翻译模型，该模型可为模拟的内窥镜图像添加逼真的纹理，同时保留关键场景布局信息。我们的方法能够在不同的内窥镜场景中生成逼真的图像。我们证明这些图像可以有效地用于成功训练一个用于具有挑战性的终端任务的模型，无需任何真实标记数据。特别是，我们针对结肠镜图像中的折叠分割任务展示了我们的方法。折叠是关键的解剖标志，可能遮挡结肠粘膜和部分可能的息肉。我们的方法在图像风格翻译后生成了保持原始折叠形状和位置的逼真图像，优于现有方法。我们在针对折叠分割任务的新型模拟数据集和来自EndoMapper（EM）的真实数据集上进行了实验。我们发布所有新生成的数据和新的EM元数据，以促进进一步的研究，因为目前尚无针对折叠分割任务的公开基准测试集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02654v1">PDF</a> </p>
<p><strong>Summary</strong>：本研究开发了一种新的图像翻译模型，该模型能为模拟的内窥镜图像添加逼真的纹理，同时保留关键场景布局信息。模型生成的图像可用于训练无需真实标记数据的模型，对于结肠镜检查图像的折叠分割等任务具有良好的效果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自动分割内窥镜图像中的解剖标志可以为医生、外科医生提供诊断、治疗或医学培训的帮助。</li>
<li>获得通常用于监督学习方法的注释是非常繁琐和困难的任务，特别是对于真实图像。</li>
<li>虽然真实数据的标记注释更容易获得，但基于合成数据训练的模型通常不能很好地推广到真实数据。</li>
<li>本研究提出了一种新的图像翻译模型，该模型能够在模拟内窥镜图像上添加逼真的纹理并保持关键场景布局信息。</li>
<li>该方法生成的图像在不同内窥镜场景中都很逼真。</li>
<li>模型对于折叠分割等任务效果良好，可成功训练模型并用于执行具有挑战性的任务，无需任何真实标记数据。所生成的折叠分割数据集和新数据集公开提供，以推动相关研究的发展。对于当前的折叠分割任务尚无公开基准数据集的问题有所改善。该数据集还发布了一个附加数据集以便于进一步的探索研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-82ded6af77d250456597de57da6025fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-033b5af5b28090ac04ab32fa9eaa7782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5876339af4360564c47ebf798da9dc5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-072ae2ffc85451d78c694b8e8c1b7912.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Regression-is-all-you-need-for-medical-image-translation"><a href="#Regression-is-all-you-need-for-medical-image-translation" class="headerlink" title="Regression is all you need for medical image translation"></a>Regression is all you need for medical image translation</h2><p><strong>Authors:Sebastian Rassmann, David Kügler, Christian Ewert, Martin Reuter</strong></p>
<p>The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging. </p>
<blockquote>
<p>在医疗成像中，在有限的时间预算内获取信息丰富的图像至关重要。医学图像翻译（MIT）可以通过从获取的数据生成合成图像来帮助增强和补充现有数据集。虽然生成对抗网络（GANs）和扩散模型（DMs）在自然图像生成方面取得了显著的成功，它们在医疗应用中的优势——创造力和图像逼真度——并不一定会带来所需的高度准确的解剖信息。事实上，对获取噪声的模仿或内容幻觉会阻碍其在临床上的实用性。在这里，我们介绍了YODA（You Only Denoise once - or Average，你只去噪一次或平均），这是一个基于体积的新型2.5D扩散式MIT框架。YODA结合了扩散和回归范式来产生逼真的或无噪声的输出。此外，我们提出了受MRI信号平均启发的期望近似（ExpA）DM采样。ExpA采样抑制生成的噪声，从而消除噪声对图像质量评估的偏见。我们在四个不同的多模式数据集上进行了大量实验，包括多对比度脑部MRI和盆腔MRI-CT，我们证明扩散和回归采样在实践中产生了类似的结果。因此，扩散采样的计算开销在医学信息翻译中并没有提供系统的优势。基于这些见解，我们证明了YODA优于几种最先进的GAN和DM方法。值得注意的是，YODA生成的图像被证明可以与多个下游任务中的实际采集图像互换，甚至在某些情况下表现更优秀。我们的研究挑战了DM在MIT中的假设优势，为MIT在医疗成像中的实际应用铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02048v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在医疗成像领域中，信息丰富图像的获取对时间预算的限制至关重要。医疗图像翻译（MIT）可以通过生成合成图像来增强和补充现有数据集。尽管生成对抗网络（GANs）和扩散模型（DMs）在自然图像生成方面取得了显著的成功，但它们在医疗应用中的优势——创造性和图像真实性——并不一定能够发挥，因为医疗应用需要高度准确的解剖信息。本文提出了一种新型的2.5D扩散基于体积的MIT框架YODA，它将扩散和回归范式结合起来产生现实或去噪输出。此外，还提出了期望近似（ExpA）DM采样，它受到MRI信号平均的启发，能够抑制生成的噪声，从而消除噪声对图像质量评估的偏见。实验表明，YODA在某些任务上表现出超越先进GAN和DM方法的效果，生成的图像可与物理采集互换甚至更胜一筹。本文的发现挑战了DM在MIT中的假设优势，为MIT在医疗成像中的实际应用铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像翻译（MIT）对于增强和补充现有医疗数据集具有重要意义。</li>
<li>尽管GANs和DMs在自然图像生成方面表现出色，但它们不一定适用于需要高度准确解剖信息的医疗应用。</li>
<li>新提出的YODA框架结合了扩散和回归范式，用于生成真实的或去噪的医疗图像。</li>
<li>ExpA-sampling能够抑制生成的噪声，提高图像质量评估的准确性。</li>
<li>YODA在某些任务上的表现超越了现有的GAN和DM方法。</li>
<li>YODA生成的图像可以与物理采集的图像互换，甚至在某些任务上表现更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7dc818b766647a276f2f1efb52443784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b16f1076d38362ca15bb7c57631877.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2c6add45f6ea928640fb40a54ae5a99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c30b220f82e9fd04a8b2200abd58ad6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e88204effc0f36fcc485522b9d2b47e7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind2Matter-Creating-3D-Models-from-EEG-Signals"><a href="#Mind2Matter-Creating-3D-Models-from-EEG-Signals" class="headerlink" title="Mind2Matter: Creating 3D Models from EEG Signals"></a>Mind2Matter: Creating 3D Models from EEG Signals</h2><p><strong>Authors:Xia Deng, Shen Chen, Jiale Zhou, Lei Li</strong></p>
<p>The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a>. </p>
<blockquote>
<p>大脑信号重建三维物体在脑机接口（BCI）研究中受到广泛关注。目前的研究主要利用功能性磁共振成像（fMRI）进行三维重建任务，因为其具有出色的空间分辨率。然而，fMRI的临床应用受限于其高昂的成本和不支持实时操作。相比之下，脑电图（EEG）作为实时脑机交互系统的经济、无创、可移动解决方案，具有明显优势。虽然深度学习领域的最新进展在神经数据生成图像方面取得了显著进展，但将EEG信号解码为结构化三维表示仍待探索。在本文中，我们提出了一种新型框架，利用神经解码技术和生成模型将EEG记录转化为三维物体重建。我们的方法包括训练EEG编码器以提取时空视觉特征，微调大型语言模型以将这些特征解释为描述性多模式输出，并利用布局指导控制的生成三维高斯分布来合成最终的三维结构。实验表明，我们的模型捕捉了显著的几何和语义特征，为脑机接口（BCI）、虚拟现实和神经仿生器件的应用开辟了道路。我们的代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11936v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于脑电信号进行三维物体重建的研究在脑机接口领域受到广泛关注。当前研究主要利用功能磁共振成像进行三维重建，但其高昂成本和无法实现实时操作限制了临床应用。相较之下，脑电图具有经济、无创、适用于实时脑机交互系统的优势。本文提出一种利用神经解码技术和生成模型将脑电图转化为三维物体重建的新框架，包括训练脑电图编码器提取时空视觉特征、微调大型语言模型以解释这些特征并生成多模式输出，以及利用布局引导控制的生成三维高斯合成最终三维结构。实验证明，该模型能够捕捉显著的几何和语义特征，为脑机接口、虚拟现实和神经仿生学等领域的应用开辟了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>脑电信号用于重建三维物体在脑机接口领域受到关注。</li>
<li>当前主要使用功能磁共振成像进行三维重建，但其成本高且无法实现实时操作。</li>
<li>脑电图作为一种经济、无创、适用于实时脑机交互系统的技术具有优势。</li>
<li>本文提出一种基于神经解码技术和生成模型的框架，将脑电图转化为三维物体重建。</li>
<li>该框架包括训练脑电图编码器、微调语言模型以及利用生成模型合成三维结构。</li>
<li>实验证明该模型能够捕捉显著的几何和语义特征。</li>
<li>该技术为脑机接口、虚拟现实和神经仿生学等领域的应用提供了新思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ac6bb56ca31d13cf649fafa84d818c7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca1f712f6ee96a259fc29c0453b3053f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ba79f5a837f15bdb2207ff4617883b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e223852dc3312651eaa87167b1aefe61.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30f67bacaae88a35a3b6af1b8c63a0c8.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-05-08  RAVU Retrieval Augmented Video Understanding with Compositional   Reasoning over Graph
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-de7f6c38b37cbb82fa68cf62304b2b86.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-08  Towards Smart Point-and-Shoot Photography
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
