<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-05-08  Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using   Only Real Face Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-000bfbfd225f980d772736ec03d6d8a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-08-更新"><a href="#2025-05-08-更新" class="headerlink" title="2025-05-08 更新"></a>2025-05-08 更新</h1><h2 id="Learning-Unknown-Spoof-Prompts-for-Generalized-Face-Anti-Spoofing-Using-Only-Real-Face-Images"><a href="#Learning-Unknown-Spoof-Prompts-for-Generalized-Face-Anti-Spoofing-Using-Only-Real-Face-Images" class="headerlink" title="Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using   Only Real Face Images"></a>Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using   Only Real Face Images</h2><p><strong>Authors:Fangling Jiang, Qi Li, Weining Wang, Wei Shen, Bing Liu, Zhenan Sun</strong></p>
<p>Face anti-spoofing is a critical technology for ensuring the security of face recognition systems. However, its ability to generalize across diverse scenarios remains a significant challenge. In this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. To address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. Our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the model’s ability to generalize to unseen target domains. Specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. This framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. Moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. Experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images. </p>
<blockquote>
<p>人脸识别反欺骗技术是保证人脸识别系统安全的关键技术。然而，其在不同场景下的泛化能力仍然是一个巨大的挑战。在本文中，我们将有限的泛化能力归因于两个关键因素：由外部数据采集变化引起的协变量偏移，以及由新兴攻击类型中的巨大差异引起的语义偏移。为了解决这两个挑战，我们提出了一种应对未知欺骗提示的新型学习方法，该方法仅依赖于单一源域的真实人脸图像。我们的方法通过利用视觉语言模型中的通用知识，为真实人脸和潜在的未知欺骗攻击生成文本提示，从而提高模型对未见目标域的泛化能力。具体来说，我们引入了一个多样化的欺骗提示优化框架来学习有效的提示。该框架在放宽的先验知识空间内约束未知的欺骗提示，同时最大化其与真实人脸图像之间的距离。此外，它强制执行不同欺骗提示之间的语义独立性，以捕获广泛的欺骗模式。在九个数据集上的实验结果表明，学习到的提示有效地转移了视觉语言模型的知识，实现了对未见目标域中多种未知攻击类型的最先进的泛化能力，且无需使用任何欺骗人脸图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03611v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个基于真实人脸图像生成文本提示的方法，用于应对未知攻击类型的面部抗欺骗问题。通过利用视觉语言模型中的通用知识，该方法能够生成针对真实人脸和潜在未知欺骗攻击的提示，从而提高模型对未见目标域的泛化能力。引入了一个多样化的欺骗提示优化框架来学习有效的提示，该框架在放松的先验知识空间中约束未知欺骗提示，同时最大化其与真实人脸图像的距离。此外，它还强制不同欺骗提示之间的语义独立性，以捕捉广泛的欺骗模式。实验结果表明，所学习的提示有效地转移了视觉语言模型的知识，在未见目标域上实现了对多种未知攻击类型的最先进的泛化能力，且无需使用任何欺骗人脸图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面部抗欺骗技术是确保人脸识别系统安全的关键技术。</li>
<li>当前技术面临的主要挑战是其在不同场景下的泛化能力有限。</li>
<li>文中将泛化能力受限归因于两个关键因素：源自外部数据采集变化的协变量偏移和新兴攻击类型间的语义偏移。</li>
<li>提出了一种基于真实人脸图像生成文本提示的新方法，以应对未知欺骗攻击。</li>
<li>该方法利用视觉语言模型中的通用知识，增强了模型对未见目标域的泛化能力。</li>
<li>引入了一个多样化的欺骗提示优化框架，以学习有效的提示，该框架能够在放松的先验知识空间内约束未知欺骗提示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a89a3b9cf0c667faf88e23d1b19e4c59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7c7f365849f6849281023f1d38513d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-787d9f74a6794b1be17e15e8810ed8bd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-Knowledge-based-Prompts-for-Robust-3D-Mask-Presentation-Attack-Detection"><a href="#Learning-Knowledge-based-Prompts-for-Robust-3D-Mask-Presentation-Attack-Detection" class="headerlink" title="Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack   Detection"></a>Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack   Detection</h2><p><strong>Authors:Fangling Jiang, Qi Li, Bing Liu, Weining Wang, Caifeng Shan, Zhenan Sun, Ming-Hsuan Yang</strong></p>
<p>3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets. </p>
<blockquote>
<p>三维掩膜展示攻击检测对于保护人脸识别系统免受日益严重的三维掩膜攻击威胁至关重要。虽然大多数现有方法利用多模式特征或远程光体积描记术（rPPG）信号来区分真实面孔和三维掩膜，但它们面临巨大的挑战，例如多模式传感器的高成本和有限的泛化能力。检测相关的文本描述提供了简洁且普遍的信息，并且成本低廉、易于获取。然而，利用视觉语言多模式特征进行三维掩膜展示攻击检测潜力尚未得到探索。在本文中，我们提出了一种基于知识提示学习框架来探索预训练视觉语言模型对三维掩膜展示攻击检测的强泛化能力。具体来说，我们的方法将知识图谱中的实体和三元组融入提示学习过程，生成精细粒度的、任务特定的明确提示，有效利用嵌入在预训练视觉语言模型中的知识。此外，考虑到不同的输入图像可能会强调不同的知识图谱元素，我们引入了一种基于注意力机制的视觉特定知识过滤器，以根据视觉上下文精炼相关元素。我们还利用因果图理论的见解来促进提示学习过程，进一步增强了方法的泛化能力。在训练过程中，采用了一种消除偶然性关联的范式，该范式利用基于知识的文本特征来去除与类别无关的局部图像斑块，促进学习符合类别相关局部斑块的通用因果提示。实验结果表明，该方法在基准数据集上实现了先进的内部和跨场景检测性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03610v1">PDF</a> </p>
<p><strong>Summary</strong><br>在人脸识别系统中，保护系统免受日益增长的3D面具攻击至关重要。现有的方法主要使用多模态特征或远程光体积法（rPPG）信号来区分真实人脸和3D面具，但它们面临高成本、泛化能力有限的挑战。文本描述检测相关文本提供简洁、通用信息且成本效益高。本文探索了基于知识的提示学习框架在利用视觉语言模型进行3D面具攻击检测方面的强大泛化能力。通过融入知识图谱中的实体和三元组，生成精细的任务特定显式提示，有效挖掘预训练的视觉语言模型中的知识。考虑到不同输入图像可能强调不同的知识图谱元素，我们引入了基于注意力机制的视觉特定知识过滤器来根据视觉上下文调整相关元素。此外，我们利用因果图论的见解来促进提示学习过程，增强方法的泛化能力。训练期间采用消除偶然性相关性的模式，通过利用基于知识的文本特征来移除类别不相关的局部图像补丁，促使学习符合类别相关的局部补丁的通用因果提示。实验结果表明，该方法在基准数据集上实现了先进的内部和跨场景检测性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D面具攻击对人脸识别系统构成威胁，需要有效的检测方法来保护系统。</li>
<li>现有方法主要使用多模态特征和远程光体积法信号检测3D面具，但存在高成本和泛化能力有限的问题。</li>
<li>本文首次探索了视觉语言模型在3D面具攻击检测中的潜力，利用知识图谱中的实体和三元组生成任务特定提示。</li>
<li>引入视觉特定知识过滤器，根据输入图像的不同内容调整相关知识图谱元素的重要性。</li>
<li>结合因果图理论来提升方法的泛化能力，通过消除偶然相关性来提高检测性能。</li>
<li>实验结果表明，该方法在基准数据集上实现了先进的检测性能，适用于不同场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03610">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-14b9923cbd5ca342a5b46689dd633194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20baced0c30e456758b471e665cba05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eca16fe279e5bcbda65e7bb3bdba2c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24439f44e8634c4242c722686f171eb9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey"><a href="#Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey" class="headerlink" title="Image Recognition with Online Lightweight Vision Transformer: A Survey"></a>Image Recognition with Online Lightweight Vision Transformer: A Survey</h2><p><strong>Authors:Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu</strong></p>
<p>The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a> </p>
<blockquote>
<p>Transformer架构在自然语言处理领域取得了巨大成功，这激发了将其适应计算机视觉任务的动机。与卷积神经网络不同，视觉Transformer天生就能捕捉长距离依赖关系并实现并行处理，但缺乏归纳偏见和效率优势，面临计算量和内存方面的挑战，这些挑战限制了其在现实世界中的应用。本文调查了在线生成轻量级视觉Transformer用于图像识别的各种策略，重点关注三个关键领域：高效组件设计、动态网络和知识蒸馏。我们在ImageNet-1K基准测试上评估了每个主题的相关探索，分析了精确度、参数、吞吐量等之间的权衡，以突出各自的优势、劣势和灵活性。最后，我们提出了轻量级视觉Transformer的未来研究方向和潜在挑战，旨在激发进一步探索，为社区提供实用指导。项目页面：<a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03113v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文探讨了将Transformer架构应用于计算机视觉任务的策略，特别是针对图像识别的轻量级视觉Transformer的研究。文章聚焦于三个关键领域：高效组件设计、动态网络和知识蒸馏。文章评估了每个主题在ImageNet-1K基准测试上的相关研究，并分析了精度、参数、吞吐量和灵活性之间的权衡，提出了未来研究的方向和挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer架构在计算机视觉任务中的适用性及其与卷积神经网络的区别。</li>
<li>长程依赖性的捕捉和并行处理在视觉Transformer中的优势。</li>
<li>视觉Transformer面临的主要挑战，包括缺乏归纳偏见和效率优势。</li>
<li>文中介绍的轻量级视觉Transformer的在线策略，包括高效组件设计、动态网络和知识蒸馏。</li>
<li>在ImageNet-1K基准测试上对各种策略的评估结果。</li>
<li>精度、参数、吞吐量和灵活性之间的权衡在视觉Transformer轻量化设计中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5071a7b1d1751f65ba993b66908a604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ca69cc9de51d3773f2d7bb86baa851.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f25925fd90604d7668099a19f0559e08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef94e7aeac77304479c3539d58d6a41a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90b28395b9db504dbc2e491947606c55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c30addd4b8633cdb8cd2b81038bc9114.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deep-learning-of-personalized-priors-from-past-MRI-scans-enables-fast-quality-enhanced-point-of-care-MRI-with-low-cost-systems"><a href="#Deep-learning-of-personalized-priors-from-past-MRI-scans-enables-fast-quality-enhanced-point-of-care-MRI-with-low-cost-systems" class="headerlink" title="Deep learning of personalized priors from past MRI scans enables fast,   quality-enhanced point-of-care MRI with low-cost systems"></a>Deep learning of personalized priors from past MRI scans enables fast,   quality-enhanced point-of-care MRI with low-cost systems</h2><p><strong>Authors:Tal Oved, Beatrice Lena, Chloé F. Najac, Sheng Shen, Matthew S. Rosen, Andrew Webb, Efrat Shimron</strong></p>
<p>Magnetic resonance imaging (MRI) offers superb-quality images, but its accessibility is limited by high costs, posing challenges for patients requiring longitudinal care. Low-field MRI provides affordable imaging with low-cost devices but is hindered by long scans and degraded image quality, including low signal-to-noise ratio (SNR) and tissue contrast. We propose a novel healthcare paradigm: using deep learning to extract personalized features from past standard high-field MRI scans and harnessing them to enable accelerated, enhanced-quality follow-up scans with low-cost systems. To overcome the SNR and contrast differences, we introduce ViT-Fuser, a feature-fusion vision transformer that learns features from past scans, e.g. those stored in standard DICOM CDs. We show that \textit{a single prior scan is sufficient}, and this scan can come from various MRI vendors, field strengths, and pulse sequences. Experiments with four datasets, including glioblastoma data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality images from accelerated low-field scans, with robustness to out-of-distribution data. Our freely available framework thus enables rapid, diagnostic-quality, low-cost imaging for wide healthcare applications. </p>
<blockquote>
<p>磁共振成像（MRI）提供了高质量的图像，但由于成本高昂，其普及程度受到限制，这对于需要长期护理的患者来说具有挑战性。低场MRI通过使用低成本设备提供可负担的成像，但受到扫描时间长和图像质量下降（包括低信噪比（SNR）和组织对比度）的阻碍。我们提出了一种新的医疗范式：使用深度学习从过去的标准高场MRI扫描中提取个性化特征，并利用这些特征实现低成本系统的加速、高质量随访扫描。为了克服信噪比和对比度差异，我们引入了ViT-Fuser，这是一种特征融合视觉变压器，可以从过去的扫描中学习特征，例如存储在标准DICOM光盘中的扫描。我们证明<em>只需一次先验扫描</em>，这次扫描可以来自不同的MRI供应商、磁场强度和脉冲序列。使用包括胶质母细胞瘤数据、低场（50mT）和超低场（6.5mT）数据的四个数据集进行的实验表明，ViT-Fuser优于最先进的方法，能够提供加速低场扫描的高质量图像，并对异常数据具有稳健性。我们提供的免费框架因此能够实现快速、诊断质量、低成本成像，广泛应用于医疗保健领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02470v1">PDF</a> </p>
<p><strong>Summary</strong>：利用深度学习从过去的高场MRI扫描中提取个性化特征，并利用这些特征实现低成本系统的加速、高质量随访扫描。提出ViT-Fuser愿景转换器，学习过去扫描的特征，如存储在标准DICOM CD中的扫描。实验证明，单次先验扫描即可，且该扫描可来自不同的MRI供应商、场强和脉冲序列。ViT-Fuser在加速低场扫描中提供高质量图像，具有对离群数据的稳健性，为广泛的应用提供了快速、诊断性、低成本的成像。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>深度学习用于从过去的高场MRI扫描中提取个性化特征。</li>
<li>使用这些特征可实现低成本系统的加速、高质量随访扫描。</li>
<li>引入ViT-Fuser愿景转换器，学习过去扫描的特征。</li>
<li>单次先验扫描即可满足需求，且来源广泛。</li>
<li>ViT-Fuser在多种数据集上的实验表现优异，包括胶质母细胞瘤数据、低场和超低场数据。</li>
<li>ViT-Fuser能提供加速低场扫描的高质量图像，并具有对离群数据的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-059fd6b96e4a330cdb360ffbd3a4e5bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d8a434c2685a026a0d67544fe0e3cd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79b6b1e8b1ae69d139c12ef53bca8389.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Token-Coordinated-Prompt-Attention-is-Needed-for-Visual-Prompting"><a href="#Token-Coordinated-Prompt-Attention-is-Needed-for-Visual-Prompting" class="headerlink" title="Token Coordinated Prompt Attention is Needed for Visual Prompting"></a>Token Coordinated Prompt Attention is Needed for Visual Prompting</h2><p><strong>Authors:Zichen Liu, Xu Zou, Gang Hua, Jiahuan Zhou</strong></p>
<p>Visual prompting techniques are widely used to efficiently fine-tune pretrained Vision Transformers (ViT) by learning a small set of shared prompts for all tokens. However, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of ViT. This often leads to indistinguishable and biased prompt-extracted features, hindering performance. To address this issue, we propose a plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. Firstly, recognizing the distinct functions of CLS and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into CLS Prompts and Image Prompts, which interact exclusively with CLS tokens and image tokens through attention mechanisms. This enhances their respective discriminative abilities. Furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. This enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. Extensive experiments across various benchmarks demonstrate that TCPA significantly enhances the diversity and discriminative power of the extracted features. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/ICML2025-TCPA">https://github.com/zhoujiahuan1991/ICML2025-TCPA</a>. </p>
<blockquote>
<p>视觉提示技术被广泛用于通过学习一组共享提示来微调预训练的视觉转换器（ViT）。然而，现有方法忽视了不同标记在传递判别信息方面的独特作用，并使用相同的提示与所有标记进行交互，从而限制了ViT的表征容量。这通常会导致难以区分和偏向提示提取的特征，阻碍性能。为了解决这一问题，我们提出了一种即插即用的标记协调提示注意力（TCPA）模块，该模块为不同的标记分配特定的协调提示，以进行基于注意力的交互。首先，我们认识到CLS和图像标记的不同功能——全局信息聚合和局部特征提取，我们将提示分解为CLS提示和图像提示，它们通过注意力机制与CLS标记和图像标记进行独家交互，增强了各自的判别能力。此外，由于不同的图像标记对应于不同的图像块并包含各种信息，我们采用匹配函数来自动为单个标记分配协调提示。这能够实现更精确的关注力交互，提高提取特征的多样性和表征容量。在各种基准测试上的大量实验表明，TCPA显著增强了提取特征的多样性和判别力。代码可在<a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/ICML2025-TCPA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhoujiahuan1991/ICML2025-TCPA上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02406v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视觉提示技术广泛应用于微调预训练的Vision Transformers（ViT），通过学习一小套共享提示来完成所有令牌的效率优化。然而，现有方法忽略了不同令牌在传递鉴别信息方面的独特作用，并使用相同的提示与所有令牌进行交互，从而限制了ViT的代表性容量。为解决此问题，我们提出了即插即用的令牌协调提示注意力（TCPA）模块，该模块为不同的令牌分配特定的协调提示进行基于注意力的交互。我们认识到CLS和图像令牌的不同功能——全局信息聚合和局部特征提取，将提示分为CLS提示和图像提示，它们通过注意力机制与CLS令牌和图像令牌进行独家互动，增强了各自的鉴别能力。此外，由于不同的图像令牌对应于不同的图像补丁并包含各种信息，我们采用匹配功能自动为个别令牌分配协调提示。这实现了更精确的关注力交互，提高了提取特征的多样性和代表性容量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉提示技术用于微调预训练的Vision Transformers（ViT）。</li>
<li>现有方法忽略不同令牌在传递鉴别信息中的独特作用。</li>
<li>提出了Token Coordinated Prompt Attention（TCPA）模块，为不同令牌分配特定的协调提示。</li>
<li>TCPA模块增强CLS令牌和图像令牌各自的鉴别能力。</li>
<li>通过注意力机制，TCPA模块实现CLS提示和图像提示与相应令牌的独家互动。</li>
<li>采用匹配功能自动为图像令牌分配协调提示，提高特征提取的多样性和代表性容量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02406">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-928fc0d77c232f23a2dc6ca6d7fcae7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea3b003a2c89728c6f7026077358f547.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7449976ffc933175a1c9d3dbbf9848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7547718f44cf4b2d8b4b9b7bd94e3617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5026d2f5e1dda8954a428a6a292d1dec.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OASIS-Optimized-Lightweight-Autoencoder-System-for-Distributed-In-Sensor-computing"><a href="#OASIS-Optimized-Lightweight-Autoencoder-System-for-Distributed-In-Sensor-computing" class="headerlink" title="OASIS: Optimized Lightweight Autoencoder System for Distributed   In-Sensor computing"></a>OASIS: Optimized Lightweight Autoencoder System for Distributed   In-Sensor computing</h2><p><strong>Authors:Chengwei Zhou, Sreetama Sarkar, Yuming Li, Arnab Sanyal, Gourav Datta</strong></p>
<p>In-sensor computing, which integrates computation directly within the sensor, has emerged as a promising paradigm for machine vision applications such as AR&#x2F;VR and smart home systems. By processing data on-chip before transmission, it alleviates the bandwidth bottleneck caused by high-resolution, high-frame-rate image transmission, particularly in video applications. We envision a system architecture that integrates a CMOS image sensor (CIS) with a logic chip via advanced packaging, where the logic chip processes early-stage deep neural network (DNN) layers. However, its limited compute and memory make deploying advanced DNNs challenging. A simple solution is to split the model, executing the first part on the logic chip and the rest off-chip. However, modern DNNs require multiple layers before dimensionality reduction, limiting their ability to achieve the primary goal of in-sensor computing: minimizing data bandwidth. To address this, we propose a dual-branch autoencoder-based vision architecture that deploys a lightweight encoder on the logic chip while the task-specific network runs off-chip. The encoder is trained using a triple loss function: (1) task-specific loss to optimize accuracy, (2) entropy loss to enforce compact and compressible representations, and (3) reconstruction loss (mean-square error) to preserve essential visual information. This design enables a four-order-of-magnitude reduction in output activation dimensionality compared to input images, resulting in a $2{-}4.5\times$ decrease in energy consumption, as validated by our hardware-backed semi-analytical energy models. We evaluate our approach on CNN and ViT-based models across applications in smart home and augmented reality domains, achieving state-of-the-art accuracy with energy efficiency of up to 22.7 TOPS&#x2F;W. </p>
<blockquote>
<p>将计算直接集成到传感器中的感器计算（In-sensor computing）已成为AR&#x2F;VR和智能家居系统等机器视觉应用的前景看好的范式。它在传输前在芯片上处理数据，从而减轻了由高分辨率、高帧率图像传输引起的带宽瓶颈，特别是在视频应用中。我们设想了一种通过先进封装技术将CMOS图像传感器（CIS）与逻辑芯片集成的系统架构，逻辑芯片处理早期深度神经网络（DNN）层。然而，其有限的计算和内存使部署先进的DNN面临挑战。一种简单的解决方案是将模型拆分，在逻辑芯片上执行第一部分，其余部分在芯片外执行。然而，现代DNN需要在降维之前进行多层计算，这限制了其实现感器计算主要目标的能力，即最小化数据带宽。为解决这一问题，我们提出了一种基于双分支自动编码器的视觉架构，该架构在逻辑芯片上部署轻量级编码器，而任务特定网络在芯片外运行。编码器使用三重损失函数进行训练：（1）任务特定损失以优化准确性，（2）熵损失以实施紧凑且可压缩的表示形式，（3）重建损失（均方误差）以保留必要的视觉信息。这一设计实现了输出激活维度相对于输入图像的四个数量级的降低，从而实现了能量消耗的减少（减少幅度为2-4.5倍），这已由我们的硬件支持半分析能量模型验证。我们的方法在智能家居和增强现实领域的应用中，对基于CNN和ViT的模型进行了评估，在具有高达22.7 TOPS&#x2F;W的能量效率的同时实现了最先进的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02256v1">PDF</a> Under review; 8 pages, 5 figures</p>
<p><strong>Summary</strong><br>     该文本介绍了一种在传感器内部进行计算的新型计算模式——在传感器计算。该模式对于机器视觉应用如AR&#x2F;VR和智能家居系统具有前景。通过直接在芯片上处理数据，它解决了高分辨率、高帧率图像传输引起的带宽瓶颈问题。作者提出了一种结合CMOS图像传感器和逻辑芯片的系统架构，通过先进封装技术实现。为了解决在逻辑芯片上部署高级深度神经网络（DNN）的挑战，提出了基于自编码机的双分支视觉架构。编码器使用三重损失函数进行训练，以实现优化精度、紧凑和可压缩的表示以及关键视觉信息的保留。这种设计实现了输出激活维度相对于输入图像的四个数量级的降低，减少了能源消耗。该方法的能量效率和准确率在智能家庭和增强现实领域均达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在传感器计算是一种新兴的计算模式，它在机器视觉应用中具有前景，特别是在AR&#x2F;VR和智能家居系统中。</li>
<li>在传感器计算模式通过在传感器内部直接处理数据来解决带宽瓶颈问题。</li>
<li>一种系统架构结合了CMOS图像传感器和逻辑芯片，通过先进封装技术实现集成。</li>
<li>在部署高级DNN时，受到计算和内存限制的挑战，提出了一种基于自编码机的双分支视觉架构解决方案。</li>
<li>编码器使用三重损失函数进行训练，以优化精度、实现紧凑和可压缩的表示，并保留关键视觉信息。</li>
<li>该设计实现了输出激活维度相对于输入图像的显著降低，降低了能源消耗。</li>
<li>该方法在智能家庭和增强现实领域的能量效率和准确率均达到领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-44501ba9d0a140fe87fcf8fe298883d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c72dafe60e7bc2d12988e1ff0a4100.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-329e4e7734c67ba801dd44cf6338fc5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ea5e90e6861ef096a3f492462953e39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e21691b08120585c4e73bc0972cc1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09e8273f17c04424bdeab5f247ce3647.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CSASN-A-Multitask-Attention-Based-Framework-for-Heterogeneous-Thyroid-Carcinoma-Classification-in-Ultrasound-Images"><a href="#CSASN-A-Multitask-Attention-Based-Framework-for-Heterogeneous-Thyroid-Carcinoma-Classification-in-Ultrasound-Images" class="headerlink" title="CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid   Carcinoma Classification in Ultrasound Images"></a>CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid   Carcinoma Classification in Ultrasound Images</h2><p><strong>Authors:Peiqi Li, Yincheng Gao, Renxing Li, Haojie Yang, Yunyun Liu, Boji Liu, Jiahui Ni, Ying Zhang, Yulu Wu, Xiaowei Fang, Lehang Guo, Liping Sun, Jiangang Chen</strong></p>
<p>Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis. </p>
<blockquote>
<p>在超声成像中，异质形态特征和数据不平衡给罕见甲状腺癌的分类带来了重大挑战。为了解决这个问题，我们提出了一种新型的多任务学习框架——通道空间注意力协同网络（CSASN）。该框架整合了一个双分支特征提取器，结合了EfficientNet进行局部空间编码和ViT进行全局语义建模，并配备了一个级联的通道空间注意力优化模块。残差多尺度分类器和动态加权损失函数进一步提高了分类的稳定性和准确性。我们的框架是在一个包含超过2000名患者的多中心数据集上训练的，该数据集来自四个临床机构。通过广泛的消融研究证明，每个模块都对模型性能做出了重大贡献，特别是在识别罕见亚型如FTC和MTC癌方面。实验结果表明，CSASN优于现有的单流CNN或基于Transformer的模型，在类别不平衡的条件下，在精度和召回率之间取得了优越的平衡。该框架为人工智能辅助甲状腺癌诊断提供了有前景的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02211v1">PDF</a> 18 pages, 10 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>一篇关于使用超声成像对稀有甲状腺癌进行分类的研究，提出了一个新型多任务学习框架CSASN，通过结合EfficientNet进行局部空间编码和ViT进行全局语义建模，解决异质形态特征和不平衡数据带来的挑战。该框架采用级联通道空间注意力优化模块，并通过残差多尺度分类器和动态加权损失函数提高分类的稳定性和准确性。经过超过两千名患者的多中心数据集训练，该框架能够有效识别罕见亚型如FTC和MTC癌。研究结果表明，CSASN优于现有的单流CNN或Transformer模型，在类别不平衡条件下实现了精度和召回率的平衡，为AI辅助甲状腺癌诊断提供了有力支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面对超声成像中罕见的甲状腺癌分类的挑战，提出了CSASN多任务学习框架来解决异质形态特征和不平衡数据的问题。</li>
<li>CSASN结合了EfficientNet和ViT两种技术，分别进行局部空间编码和全局语义建模。</li>
<li>通过级联通道空间注意力优化模块来提高模型的性能。</li>
<li>采用残差多尺度分类器和动态加权损失函数，增强了分类的稳定性和准确性。</li>
<li>该框架在多中心数据集上进行了训练，包括超过两千名患者，能有效识别罕见癌症状如FTC和MTC。</li>
<li>CSASN在类别不平衡条件下表现出优越的性能，实现了精度和召回率的平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02211">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-79b322867e6e456af0f27c4dc8478b04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b96ca88b764e1cd2c12d99b1c973c357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce18ccfe9cdbe4a91eaf4d424ea0664.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Heart-Failure-Prediction-using-Modal-Decomposition-and-Masked-Autoencoders-for-Scarce-Echocardiography-Databases"><a href="#Heart-Failure-Prediction-using-Modal-Decomposition-and-Masked-Autoencoders-for-Scarce-Echocardiography-Databases" class="headerlink" title="Heart Failure Prediction using Modal Decomposition and Masked   Autoencoders for Scarce Echocardiography Databases"></a>Heart Failure Prediction using Modal Decomposition and Masked   Autoencoders for Scarce Echocardiography Databases</h2><p><strong>Authors:Andrés Bell-Navas, María Villalba-Orero, Enrique Lara-Pezzi, Jesús Garicano-Mena, Soledad Le Clainche</strong></p>
<p>Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid, and effective prediction. This work presents an automatic system based on a novel deep learning framework which analyses in real-time echocardiography video sequences for the challenging and more specific task of heart failure time prediction. This system works in two stages. The first one transforms the data from a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any machine learning-based framework, including a deep learning-based one. This stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage builds and trains a Vision Transformer (ViT). Self-supervised learning (SSL) methods, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the ViT from scratch, even with scarce databases. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures. The source code will be incorporated into the next version release of the ModelFLOWs-app software (<a target="_blank" rel="noopener" href="https://github.com/modelflows/ModelFLOWs-app">https://github.com/modelflows/ModelFLOWs-app</a>). </p>
<blockquote>
<p>心脏病是国际人类功能障碍的主要诱因。据世界卫生组织（WHO）统计，每年约有1800万人因心脏病而死亡。特别是，心力衰竭（HF）迫使医疗行业需要开发系统以进行早期、快速和有效的预测。本文介绍了一个基于新型深度学习框架的自动系统，该系统可实时分析超声心动图视频序列，以进行更具挑战性的心力衰竭时间预测任务。该系统分为两个阶段。第一阶段将超声心动图视频序列数据库的数据转换为机器学习兼容的注释图像集合，可用于任何基于机器学习（包括深度学习）的框架的训练阶段。这一阶段包括使用高阶动态模式分解（HODMD）算法进行数据增强和特征提取。第二阶段构建并训练Vision Transformer（ViT）。采用迄今为止在心力衰竭预测文献中很少探索的自我监督学习（SSL）方法来有效地从头开始训练ViT，即使数据库稀缺也是如此。所设计的神经网络分析超声心动图序列的图像，以估计心力衰竭发生的时间。获得的结果显示了HODMD算法的有效性以及所提出的系统在多个已建立的ViT和卷积神经网络（CNN）架构方面的优越性。源代码将包含在ModelFLOWs-app软件的下一个版本发布中（<a target="_blank" rel="noopener" href="https://github.com/modelflows/ModelFLOWs-app%EF%BC%89%E3%80%82">https://github.com/modelflows/ModelFLOWs-app）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07606v2">PDF</a> 39 pages, 7 figures. arXiv admin note: substantial text overlap with   arXiv:2404.19579</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于深度学习的自动系统，用于实时分析超声心动图视频序列，预测心脏衰竭的时间。系统分为两个阶段，第一阶段将数据转换为机器学习兼容的标注图像集合，第二阶段构建并训练Vision Transformer（ViT）。采用自监督学习方法有效训练ViT，即使数据库稀缺也能从头开始训练。所设计的神经网络通过分析超声心动图序列的图像来估计心脏衰竭发生的时间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出一种自动系统，基于深度学习的框架，用于实时分析超声心动图视频序列，挑战性地预测心脏衰竭的时间。</li>
<li>系统包括两个主要阶段：数据转换和ViT的构建与训练。</li>
<li>使用高阶动态模式分解（HODMD）算法进行数据增强和特征提取。</li>
<li>首次在心脏衰竭预测领域采用自监督学习方法训练ViT。</li>
<li>设计的神经网络能够通过分析超声心动图序列的图像来预测心脏衰竭发生的时间。</li>
<li>实验结果证明了HODMD算法的有效性以及该系统相较于其他ViT和CNN架构的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-000bfbfd225f980d772736ec03d6d8a9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning"><a href="#Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning" class="headerlink" title="Visual Adaptive Prompting for Compositional Zero-Shot Learning"></a>Visual Adaptive Prompting for Compositional Zero-Shot Learning</h2><p><strong>Authors:Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh</strong></p>
<p>Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results. </p>
<blockquote>
<p>视觉语言模型（VLMs）在视觉和文本数据的联合表示学习方面表现出了令人印象深刻的能力，使其成为用于组合零射击学习（CZSL）等任务的强大工具。CZSL要求模型能够推广到训练期间未明确遇到的新组合的视觉元素，如属性和对象。最近关于CZSL提示的工作主要集中在修改文本编码器的输入，通常使用在多种视觉上下文中不会改变的静态提示。然而，这些方法在捕捉多变的视觉上下文时遇到困难，因为它们专注于文本适应，而不是利用视觉特征进行组合推理。为了解决这一问题，我们提出了视觉自适应提示系统（VAPS），该系统利用可学习的视觉提示存储库和基于相似性的检索机制，在视觉语言模型的框架内缩小语义和视觉特征之间的差距。我们的方法引入了一个动态视觉提示存储库机制，该机制根据图像的视觉特征选择最相关的属性和对象提示。我们提出的系统包括一个视觉提示适配器，鼓励模型学习一个更具泛化能力的嵌入空间。在三种CZSL基准测试上的实验，无论是在封闭世界还是开放世界场景中，都取得了最新技术成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20292v4">PDF</a> </p>
<p><strong>Summary</strong><br>视觉语言模型（VLMs）在联合表示视觉和文本数据方面表现出强大的能力，适用于组合零射学习（CZSL）等任务。为应对CZSL中对于视觉上下文的捕捉难题，提出了一种视觉自适应提示系统（VAPS），该系统利用可学习的视觉提示库和基于相似性的检索机制，缩小语义和视觉特征之间的差距。通过动态视觉提示库机制，系统根据图像视觉特征选择最相关的属性和对象提示。实验证明，该系统在三种CZSL基准测试中均达到最佳状态。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs具备强大的联合表示视觉和文本数据的能力。</li>
<li>CZSL任务需要模型对未在训练中明确遇到的新组合的视觉原始数据进行泛化。</li>
<li>现有提示方法主要侧重于文本适应，难以充分利用视觉特征进行组合推理。</li>
<li>VAPS通过利用可学习的视觉提示库和基于相似性的检索机制来缩小语义和视觉特征之间的差距。</li>
<li>VAPS包括一个动态视觉提示库机制，能够根据图像选择相关提示。</li>
<li>VAPS包括一个视觉提示适配器，鼓励模型学习更具泛化能力的嵌入空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20292">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-911fb6369694972f864d97d3df6e74ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fef2eab66368550060014f7cb7d0a735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-766cdad7fa24ba448d036b48ce9790df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4c706091bcfe9ac755cc2ca42bbcb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f0371debde75c6ece6f5931410997d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Underwater-Image-Enhancement-via-Dehazing-and-Color-Restoration"><a href="#Underwater-Image-Enhancement-via-Dehazing-and-Color-Restoration" class="headerlink" title="Underwater Image Enhancement via Dehazing and Color Restoration"></a>Underwater Image Enhancement via Dehazing and Color Restoration</h2><p><strong>Authors:Chengqin Wu, Shuai Yu, Tuyan Luo, Qiuhua Rao, Qingson Hu, Jingxiang Xu, Lijun Zhang</strong></p>
<p>Underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. To overcome this limitation, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) that dynamically integrates these decoupled features to achieve comprehensive enhancement. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to respectively preserve color fidelity and enhance structural details during network training. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images. </p>
<blockquote>
<p>水下视觉成像对于海洋工程至关重要，但它受到低对比度、模糊和颜色退化等因素的影响，阻碍了后续分析。现有的水下图像增强方法通常将雾霾和色彩投射视为统一的退化过程，忽略了它们内在的独立性，同时忽视了它们之间的协同关系。为了克服这一局限性，我们提出了一种基于Vision Transformer（ViT）的网络（称为WaterFormer）来改善水下图像质量。WaterFormer包含三个主要组件：去雾块（DehazeFormer Block）用于捕获自相关的雾特征并提取深层特征，色彩恢复块（CRB）用于捕获自相关的色彩投射特征，以及通道融合块（CFB）动态融合这些解耦特征以实现全面增强。为了保证真实性，还加入了一个基于水下成像物理模型的软重建层。此外，还设计了色度一致性损失和Sobel色彩损失，以在网络训练过程中分别保持颜色保真度和增强结构细节。大量的实验结果证明，WaterFormer在增强水下图像方面优于其他最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Vision Transformer（ViT）的水下图像增强网络（WaterFormer），旨在解决水下视觉成像中的低对比度和色彩失真问题。该网络包含三个主要组件：去雾块（DehazeFormer Block）、色彩恢复块（CRB）和通道融合块（CFB）。去雾块用于捕捉雾气特征并提取深层特征，色彩恢复块则负责捕捉颜色偏差特征。通道融合块则动态结合了这些独立特征以实现全面增强。此外，网络还包含基于水下成像物理模型的软重建层，并设计了色度一致性损失和Sobel色彩损失，以在训练过程中保持色彩保真度和增强结构细节。实验结果表明，WaterFormer在增强水下图像方面优于其他先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下视觉成像面临低对比度和色彩失真问题，影响下游分析。</li>
<li>现有水下图像增强方法往往将雾气和色彩偏移视为统一退化过程，忽略了它们的独立性和相互作用关系。</li>
<li>提出一种基于Vision Transformer（ViT）的水下图像增强网络（WaterFormer）来解决这一问题。</li>
<li>WaterFormer包含三个主要组件：去雾块、色彩恢复块和通道融合块，分别负责捕捉雾气特征、颜色偏差特征，并动态结合这些特征以实现增强。</li>
<li>网络包含基于水下成像物理模型的软重建层，确保增强的真实性。</li>
<li>设计了色度一致性损失和Sobel色彩损失，以在训练过程中保持色彩保真度和增强结构细节。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09779">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-048819f126a5c0c7b870bf01766fefc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-510c0d7d4b0fd6552bcfe0af3d9bf70b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f17d4345fd7bdfa62572c525f102b960.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb35c4fbe458a718ea9f99b99a06bfe8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MFCLIP-Multi-modal-Fine-grained-CLIP-for-Generalizable-Diffusion-Face-Forgery-Detection"><a href="#MFCLIP-Multi-modal-Fine-grained-CLIP-for-Generalizable-Diffusion-Face-Forgery-Detection" class="headerlink" title="MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face   Forgery Detection"></a>MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face   Forgery Detection</h2><p><strong>Authors:Yaning Zhang, Tianyi Wang, Zitong Yu, Zan Gao, Linlin Shen, Shengyong Chen</strong></p>
<p>The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. </p>
<blockquote>
<p>快速逼真面部生成方法的开发引起了社会和学术界的广泛关注，这凸显了对稳健且通用的面部伪造检测（FFD）技术的迫切需求。尽管现有方法主要使用图像模式捕捉面部伪造模式，但其他模式（如细微噪声和文本）尚未得到完全探索，这限制了模型的泛化能力。此外，大多数FFD方法倾向于识别由GAN生成的面部图像，但难以检测未见过的扩散合成图像。为了解决这些限制，我们旨在利用最前沿的基础模型——对比语言图像预训练（CLIP），实现可通用的扩散面部伪造检测（DFFD）。在本文中，我们提出了一种新的多模式精细CLIP（MFCLIP）模型，它通过语言引导的面部伪造表示学习，挖掘图像噪声模式之间的全面和精细的伪造痕迹，以促进DFFD的发展。具体来说，我们设计了一种精细的语言编码器（FLE），它从分层的文本提示中提取精细的全局语言特征。我们设计了一个多模式视觉编码器（MVE），以捕获全局图像伪造嵌入以及从最丰富的补丁中提取的精细噪声伪造模式，并将它们整合起来挖掘通用的视觉伪造痕迹。此外，我们建立了一种创新的可插拔式样本配对注意力（SPA）方法，以强调相关的负样本对并抑制不相关的样本对，使跨模态样本对能够进行更灵活的对齐。广泛的实验和可视化结果表明，我们的模型在不同的设置（如跨生成器、跨伪造和跨数据集评估）上均超过了现有技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09724v2">PDF</a> </p>
<p><strong>Summary</strong><br>该方法旨在解决社会与学术界对于逼真的面部生成技术快速发展所引发的担忧，特别是对面部伪造检测技术的迫切需求。现有方法主要依赖图像模态捕捉面部伪造模式，但忽略了其他模态如精细噪声和文本的重要性，限制了模型的泛化能力。本研究旨在利用前沿的CLIP模型，实现可泛化的扩散面部伪造检测（DFFD）。为此，提出了一种新颖的多模态精细CLIP（MFCLIP）模型，通过语言引导的面伪造表示学习，挖掘图像噪声模态的全面精细伪造痕迹。设计精细语言编码器（FLE）和多模态视觉编码器（MVE），整合全局图像伪造嵌入和从丰富区域提取的精细噪声伪造模式，挖掘通用视觉伪造痕迹。此外，开发了创新的即插即用样本配对注意力（SPA）方法，强调相关的负样本对并抑制无关的样本对，实现跨模态样本对更灵活的配对。实验和可视化显示，该模型在不同设置上均超越了现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有面部伪造检测技术主要关注图像模态，忽视了其他如精细噪声和文本等模态的重要性。</li>
<li>提出利用CLIP模型实现可泛化的扩散面部伪造检测（DFFD）。</li>
<li>引入多模态精细CLIP（MFCLIP）模型，通过语言引导学习挖掘全面的精细伪造痕迹。</li>
<li>设计了精细语言编码器（FLE）和多模态视觉编码器（MVE），用于提取全局和精细的伪造特征。</li>
<li>创新提出即插即用样本配对注意力（SPA）方法，强化相关样本对的配对。</li>
<li>该模型在跨生成器、跨伪造和跨数据集评估等不同设置上均表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09724">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9f1e9101a2350a2d8754d51298d99117.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8600bd37993aaec2a54c71284c4a2751.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a3fc6d63d8fa60b48020b0626eb817.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84e125868fb2f991cd22f88c6b4b529f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2dd6d1836834b963757968136cea426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbe7bf60be33c6b1d16b37603bf696a2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d7b7a8b9efd58ba54027b747401b9c90.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-08  3D Can Be Explored In 2D Pseudo-Label Generation for LiDAR Point Clouds   Using Sensor-Intensity-Based 2D Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30f67bacaae88a35a3b6af1b8c63a0c8.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-05-08  RAVU Retrieval Augmented Video Understanding with Compositional   Reasoning over Graph
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
