<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using   Only Real Face Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-000bfbfd225f980d772736ec03d6d8a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="Learning-Unknown-Spoof-Prompts-for-Generalized-Face-Anti-Spoofing-Using-Only-Real-Face-Images"><a href="#Learning-Unknown-Spoof-Prompts-for-Generalized-Face-Anti-Spoofing-Using-Only-Real-Face-Images" class="headerlink" title="Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using   Only Real Face Images"></a>Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using   Only Real Face Images</h2><p><strong>Authors:Fangling Jiang, Qi Li, Weining Wang, Wei Shen, Bing Liu, Zhenan Sun</strong></p>
<p>Face anti-spoofing is a critical technology for ensuring the security of face recognition systems. However, its ability to generalize across diverse scenarios remains a significant challenge. In this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. To address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. Our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the modelâ€™s ability to generalize to unseen target domains. Specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. This framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. Moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. Experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«åæ¬ºéª—æŠ€æœ¯æ˜¯ä¿è¯äººè„¸è¯†åˆ«ç³»ç»Ÿå®‰å…¨çš„å…³é”®æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æœ‰é™çš„æ³›åŒ–èƒ½åŠ›å½’å› äºä¸¤ä¸ªå…³é”®å› ç´ ï¼šç”±å¤–éƒ¨æ•°æ®é‡‡é›†å˜åŒ–å¼•èµ·çš„åå˜é‡åç§»ï¼Œä»¥åŠç”±æ–°å…´æ”»å‡»ç±»å‹ä¸­çš„å·¨å¤§å·®å¼‚å¼•èµ·çš„è¯­ä¹‰åç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åº”å¯¹æœªçŸ¥æ¬ºéª—æç¤ºçš„æ–°å‹å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä¾èµ–äºå•ä¸€æºåŸŸçš„çœŸå®äººè„¸å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é€šç”¨çŸ¥è¯†ï¼Œä¸ºçœŸå®äººè„¸å’Œæ½œåœ¨çš„æœªçŸ¥æ¬ºéª—æ”»å‡»ç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹æœªè§ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ¬ºéª—æç¤ºä¼˜åŒ–æ¡†æ¶æ¥å­¦ä¹ æœ‰æ•ˆçš„æç¤ºã€‚è¯¥æ¡†æ¶åœ¨æ”¾å®½çš„å…ˆéªŒçŸ¥è¯†ç©ºé—´å†…çº¦æŸæœªçŸ¥çš„æ¬ºéª—æç¤ºï¼ŒåŒæ—¶æœ€å¤§åŒ–å…¶ä¸çœŸå®äººè„¸å›¾åƒä¹‹é—´çš„è·ç¦»ã€‚æ­¤å¤–ï¼Œå®ƒå¼ºåˆ¶æ‰§è¡Œä¸åŒæ¬ºéª—æç¤ºä¹‹é—´çš„è¯­ä¹‰ç‹¬ç«‹æ€§ï¼Œä»¥æ•è·å¹¿æ³›çš„æ¬ºéª—æ¨¡å¼ã€‚åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå­¦ä¹ åˆ°çš„æç¤ºæœ‰æ•ˆåœ°è½¬ç§»äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ï¼Œå®ç°äº†å¯¹æœªè§ç›®æ ‡åŸŸä¸­å¤šç§æœªçŸ¥æ”»å‡»ç±»å‹çš„æœ€å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€ä½¿ç”¨ä»»ä½•æ¬ºéª—äººè„¸å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03611v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºçœŸå®äººè„¸å›¾åƒç”Ÿæˆæ–‡æœ¬æç¤ºçš„æ–¹æ³•ï¼Œç”¨äºåº”å¯¹æœªçŸ¥æ”»å‡»ç±»å‹çš„é¢éƒ¨æŠ—æ¬ºéª—é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é€šç”¨çŸ¥è¯†ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹çœŸå®äººè„¸å’Œæ½œåœ¨æœªçŸ¥æ¬ºéª—æ”»å‡»çš„æç¤ºï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹æœªè§ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚å¼•å…¥äº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ¬ºéª—æç¤ºä¼˜åŒ–æ¡†æ¶æ¥å­¦ä¹ æœ‰æ•ˆçš„æç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ”¾æ¾çš„å…ˆéªŒçŸ¥è¯†ç©ºé—´ä¸­çº¦æŸæœªçŸ¥æ¬ºéª—æç¤ºï¼ŒåŒæ—¶æœ€å¤§åŒ–å…¶ä¸çœŸå®äººè„¸å›¾åƒçš„è·ç¦»ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼ºåˆ¶ä¸åŒæ¬ºéª—æç¤ºä¹‹é—´çš„è¯­ä¹‰ç‹¬ç«‹æ€§ï¼Œä»¥æ•æ‰å¹¿æ³›çš„æ¬ºéª—æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€å­¦ä¹ çš„æç¤ºæœ‰æ•ˆåœ°è½¬ç§»äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ï¼Œåœ¨æœªè§ç›®æ ‡åŸŸä¸Šå®ç°äº†å¯¹å¤šç§æœªçŸ¥æ”»å‡»ç±»å‹çš„æœ€å…ˆè¿›çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€ä½¿ç”¨ä»»ä½•æ¬ºéª—äººè„¸å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨æŠ—æ¬ºéª—æŠ€æœ¯æ˜¯ç¡®ä¿äººè„¸è¯†åˆ«ç³»ç»Ÿå®‰å…¨çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>å½“å‰æŠ€æœ¯é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æ–‡ä¸­å°†æ³›åŒ–èƒ½åŠ›å—é™å½’å› äºä¸¤ä¸ªå…³é”®å› ç´ ï¼šæºè‡ªå¤–éƒ¨æ•°æ®é‡‡é›†å˜åŒ–çš„åå˜é‡åç§»å’Œæ–°å…´æ”»å‡»ç±»å‹é—´çš„è¯­ä¹‰åç§»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºçœŸå®äººè„¸å›¾åƒç”Ÿæˆæ–‡æœ¬æç¤ºçš„æ–°æ–¹æ³•ï¼Œä»¥åº”å¯¹æœªçŸ¥æ¬ºéª—æ”»å‡»ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é€šç”¨çŸ¥è¯†ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹æœªè§ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå¤šæ ·åŒ–çš„æ¬ºéª—æç¤ºä¼˜åŒ–æ¡†æ¶ï¼Œä»¥å­¦ä¹ æœ‰æ•ˆçš„æç¤ºï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ”¾æ¾çš„å…ˆéªŒçŸ¥è¯†ç©ºé—´å†…çº¦æŸæœªçŸ¥æ¬ºéª—æç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a89a3b9cf0c667faf88e23d1b19e4c59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7c7f365849f6849281023f1d38513d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-787d9f74a6794b1be17e15e8810ed8bd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-Knowledge-based-Prompts-for-Robust-3D-Mask-Presentation-Attack-Detection"><a href="#Learning-Knowledge-based-Prompts-for-Robust-3D-Mask-Presentation-Attack-Detection" class="headerlink" title="Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack   Detection"></a>Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack   Detection</h2><p><strong>Authors:Fangling Jiang, Qi Li, Bing Liu, Weining Wang, Caifeng Shan, Zhenan Sun, Ming-Hsuan Yang</strong></p>
<p>3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets. </p>
<blockquote>
<p>ä¸‰ç»´æ©è†œå±•ç¤ºæ”»å‡»æ£€æµ‹å¯¹äºä¿æŠ¤äººè„¸è¯†åˆ«ç³»ç»Ÿå…å—æ—¥ç›Šä¸¥é‡çš„ä¸‰ç»´æ©è†œæ”»å‡»å¨èƒè‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤šæ¨¡å¼ç‰¹å¾æˆ–è¿œç¨‹å…‰ä½“ç§¯æè®°æœ¯ï¼ˆrPPGï¼‰ä¿¡å·æ¥åŒºåˆ†çœŸå®é¢å­”å’Œä¸‰ç»´æ©è†œï¼Œä½†å®ƒä»¬é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å¤šæ¨¡å¼ä¼ æ„Ÿå™¨çš„é«˜æˆæœ¬å’Œæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ã€‚æ£€æµ‹ç›¸å…³çš„æ–‡æœ¬æè¿°æä¾›äº†ç®€æ´ä¸”æ™®éçš„ä¿¡æ¯ï¼Œå¹¶ä¸”æˆæœ¬ä½å»‰ã€æ˜“äºè·å–ã€‚ç„¶è€Œï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€å¤šæ¨¡å¼ç‰¹å¾è¿›è¡Œä¸‰ç»´æ©è†œå±•ç¤ºæ”»å‡»æ£€æµ‹æ½œåŠ›å°šæœªå¾—åˆ°æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†æç¤ºå­¦ä¹ æ¡†æ¶æ¥æ¢ç´¢é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å¯¹ä¸‰ç»´æ©è†œå±•ç¤ºæ”»å‡»æ£€æµ‹çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¸‰å…ƒç»„èå…¥æç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç”Ÿæˆç²¾ç»†ç²’åº¦çš„ã€ä»»åŠ¡ç‰¹å®šçš„æ˜ç¡®æç¤ºï¼Œæœ‰æ•ˆåˆ©ç”¨åµŒå…¥åœ¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°ä¸åŒçš„è¾“å…¥å›¾åƒå¯èƒ½ä¼šå¼ºè°ƒä¸åŒçš„çŸ¥è¯†å›¾è°±å…ƒç´ ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰ç‰¹å®šçŸ¥è¯†è¿‡æ»¤å™¨ï¼Œä»¥æ ¹æ®è§†è§‰ä¸Šä¸‹æ–‡ç²¾ç‚¼ç›¸å…³å…ƒç´ ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨å› æœå›¾ç†è®ºçš„è§è§£æ¥ä¿ƒè¿›æç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨äº†ä¸€ç§æ¶ˆé™¤å¶ç„¶æ€§å…³è”çš„èŒƒå¼ï¼Œè¯¥èŒƒå¼åˆ©ç”¨åŸºäºçŸ¥è¯†çš„æ–‡æœ¬ç‰¹å¾æ¥å»é™¤ä¸ç±»åˆ«æ— å…³çš„å±€éƒ¨å›¾åƒæ–‘å—ï¼Œä¿ƒè¿›å­¦ä¹ ç¬¦åˆç±»åˆ«ç›¸å…³å±€éƒ¨æ–‘å—çš„é€šç”¨å› æœæç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„å†…éƒ¨å’Œè·¨åœºæ™¯æ£€æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03610v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨äººè„¸è¯†åˆ«ç³»ç»Ÿä¸­ï¼Œä¿æŠ¤ç³»ç»Ÿå…å—æ—¥ç›Šå¢é•¿çš„3Dé¢å…·æ”»å‡»è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨å¤šæ¨¡æ€ç‰¹å¾æˆ–è¿œç¨‹å…‰ä½“ç§¯æ³•ï¼ˆrPPGï¼‰ä¿¡å·æ¥åŒºåˆ†çœŸå®äººè„¸å’Œ3Dé¢å…·ï¼Œä½†å®ƒä»¬é¢ä¸´é«˜æˆæœ¬ã€æ³›åŒ–èƒ½åŠ›æœ‰é™çš„æŒ‘æˆ˜ã€‚æ–‡æœ¬æè¿°æ£€æµ‹ç›¸å…³æ–‡æœ¬æä¾›ç®€æ´ã€é€šç”¨ä¿¡æ¯ä¸”æˆæœ¬æ•ˆç›Šé«˜ã€‚æœ¬æ–‡æ¢ç´¢äº†åŸºäºçŸ¥è¯†çš„æç¤ºå­¦ä¹ æ¡†æ¶åœ¨åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œ3Dé¢å…·æ”»å‡»æ£€æµ‹æ–¹é¢çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡èå…¥çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¸‰å…ƒç»„ï¼Œç”Ÿæˆç²¾ç»†çš„ä»»åŠ¡ç‰¹å®šæ˜¾å¼æç¤ºï¼Œæœ‰æ•ˆæŒ–æ˜é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†ã€‚è€ƒè™‘åˆ°ä¸åŒè¾“å…¥å›¾åƒå¯èƒ½å¼ºè°ƒä¸åŒçš„çŸ¥è¯†å›¾è°±å…ƒç´ ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è§†è§‰ç‰¹å®šçŸ¥è¯†è¿‡æ»¤å™¨æ¥æ ¹æ®è§†è§‰ä¸Šä¸‹æ–‡è°ƒæ•´ç›¸å…³å…ƒç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å› æœå›¾è®ºçš„è§è§£æ¥ä¿ƒè¿›æç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œå¢å¼ºæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ã€‚è®­ç»ƒæœŸé—´é‡‡ç”¨æ¶ˆé™¤å¶ç„¶æ€§ç›¸å…³æ€§çš„æ¨¡å¼ï¼Œé€šè¿‡åˆ©ç”¨åŸºäºçŸ¥è¯†çš„æ–‡æœ¬ç‰¹å¾æ¥ç§»é™¤ç±»åˆ«ä¸ç›¸å…³çš„å±€éƒ¨å›¾åƒè¡¥ä¸ï¼Œä¿ƒä½¿å­¦ä¹ ç¬¦åˆç±»åˆ«ç›¸å…³çš„å±€éƒ¨è¡¥ä¸çš„é€šç”¨å› æœæç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„å†…éƒ¨å’Œè·¨åœºæ™¯æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dé¢å…·æ”»å‡»å¯¹äººè„¸è¯†åˆ«ç³»ç»Ÿæ„æˆå¨èƒï¼Œéœ€è¦æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•æ¥ä¿æŠ¤ç³»ç»Ÿã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨å¤šæ¨¡æ€ç‰¹å¾å’Œè¿œç¨‹å…‰ä½“ç§¯æ³•ä¿¡å·æ£€æµ‹3Dé¢å…·ï¼Œä½†å­˜åœ¨é«˜æˆæœ¬å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨3Dé¢å…·æ”»å‡»æ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“å’Œä¸‰å…ƒç»„ç”Ÿæˆä»»åŠ¡ç‰¹å®šæç¤ºã€‚</li>
<li>å¼•å…¥è§†è§‰ç‰¹å®šçŸ¥è¯†è¿‡æ»¤å™¨ï¼Œæ ¹æ®è¾“å…¥å›¾åƒçš„ä¸åŒå†…å®¹è°ƒæ•´ç›¸å…³çŸ¥è¯†å›¾è°±å…ƒç´ çš„é‡è¦æ€§ã€‚</li>
<li>ç»“åˆå› æœå›¾ç†è®ºæ¥æå‡æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æ¶ˆé™¤å¶ç„¶ç›¸å…³æ€§æ¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„æ£€æµ‹æ€§èƒ½ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14b9923cbd5ca342a5b46689dd633194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20baced0c30e456758b471e665cba05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eca16fe279e5bcbda65e7bb3bdba2c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24439f44e8634c4242c722686f171eb9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey"><a href="#Image-Recognition-with-Online-Lightweight-Vision-Transformer-A-Survey" class="headerlink" title="Image Recognition with Online Lightweight Vision Transformer: A Survey"></a>Image Recognition with Online Lightweight Vision Transformer: A Survey</h2><p><strong>Authors:Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu</strong></p>
<p>The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a> </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œè¿™æ¿€å‘äº†å°†å…¶é€‚åº”è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„åŠ¨æœºã€‚ä¸å·ç§¯ç¥ç»ç½‘ç»œä¸åŒï¼Œè§†è§‰Transformerå¤©ç”Ÿå°±èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶å®ç°å¹¶è¡Œå¤„ç†ï¼Œä½†ç¼ºä¹å½’çº³åè§å’Œæ•ˆç‡ä¼˜åŠ¿ï¼Œé¢ä¸´è®¡ç®—é‡å’Œå†…å­˜æ–¹é¢çš„æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡è°ƒæŸ¥äº†åœ¨çº¿ç”Ÿæˆè½»é‡çº§è§†è§‰Transformerç”¨äºå›¾åƒè¯†åˆ«çš„å„ç§ç­–ç•¥ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªå…³é”®é¢†åŸŸï¼šé«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æ¯ä¸ªä¸»é¢˜çš„ç›¸å…³æ¢ç´¢ï¼Œåˆ†æäº†ç²¾ç¡®åº¦ã€å‚æ•°ã€ååé‡ç­‰ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥çªå‡ºå„è‡ªçš„ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œçµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§è§†è§‰Transformerçš„æœªæ¥ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¿€å‘è¿›ä¸€æ­¥æ¢ç´¢ï¼Œä¸ºç¤¾åŒºæä¾›å®ç”¨æŒ‡å¯¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ajxklo/Lightweight-VIT">https://github.com/ajxklo/Lightweight-VIT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03113v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å°†Transformeræ¶æ„åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å›¾åƒè¯†åˆ«çš„è½»é‡çº§è§†è§‰Transformerçš„ç ”ç©¶ã€‚æ–‡ç« èšç„¦äºä¸‰ä¸ªå…³é”®é¢†åŸŸï¼šé«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚æ–‡ç« è¯„ä¼°äº†æ¯ä¸ªä¸»é¢˜åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šçš„ç›¸å…³ç ”ç©¶ï¼Œå¹¶åˆ†æäº†ç²¾åº¦ã€å‚æ•°ã€ååé‡å’Œçµæ´»æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œæå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§åŠå…¶ä¸å·ç§¯ç¥ç»ç½‘ç»œçš„åŒºåˆ«ã€‚</li>
<li>é•¿ç¨‹ä¾èµ–æ€§çš„æ•æ‰å’Œå¹¶è¡Œå¤„ç†åœ¨è§†è§‰Transformerä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>è§†è§‰Transformeré¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºä¹å½’çº³åè§å’Œæ•ˆç‡ä¼˜åŠ¿ã€‚</li>
<li>æ–‡ä¸­ä»‹ç»çš„è½»é‡çº§è§†è§‰Transformerçš„åœ¨çº¿ç­–ç•¥ï¼ŒåŒ…æ‹¬é«˜æ•ˆç»„ä»¶è®¾è®¡ã€åŠ¨æ€ç½‘ç»œå’ŒçŸ¥è¯†è’¸é¦ã€‚</li>
<li>åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šå¯¹å„ç§ç­–ç•¥çš„è¯„ä¼°ç»“æœã€‚</li>
<li>ç²¾åº¦ã€å‚æ•°ã€ååé‡å’Œçµæ´»æ€§ä¹‹é—´çš„æƒè¡¡åœ¨è§†è§‰Transformerè½»é‡åŒ–è®¾è®¡ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5071a7b1d1751f65ba993b66908a604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ca69cc9de51d3773f2d7bb86baa851.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f25925fd90604d7668099a19f0559e08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef94e7aeac77304479c3539d58d6a41a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90b28395b9db504dbc2e491947606c55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c30addd4b8633cdb8cd2b81038bc9114.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deep-learning-of-personalized-priors-from-past-MRI-scans-enables-fast-quality-enhanced-point-of-care-MRI-with-low-cost-systems"><a href="#Deep-learning-of-personalized-priors-from-past-MRI-scans-enables-fast-quality-enhanced-point-of-care-MRI-with-low-cost-systems" class="headerlink" title="Deep learning of personalized priors from past MRI scans enables fast,   quality-enhanced point-of-care MRI with low-cost systems"></a>Deep learning of personalized priors from past MRI scans enables fast,   quality-enhanced point-of-care MRI with low-cost systems</h2><p><strong>Authors:Tal Oved, Beatrice Lena, ChloÃ© F. Najac, Sheng Shen, Matthew S. Rosen, Andrew Webb, Efrat Shimron</strong></p>
<p>Magnetic resonance imaging (MRI) offers superb-quality images, but its accessibility is limited by high costs, posing challenges for patients requiring longitudinal care. Low-field MRI provides affordable imaging with low-cost devices but is hindered by long scans and degraded image quality, including low signal-to-noise ratio (SNR) and tissue contrast. We propose a novel healthcare paradigm: using deep learning to extract personalized features from past standard high-field MRI scans and harnessing them to enable accelerated, enhanced-quality follow-up scans with low-cost systems. To overcome the SNR and contrast differences, we introduce ViT-Fuser, a feature-fusion vision transformer that learns features from past scans, e.g. those stored in standard DICOM CDs. We show that \textit{a single prior scan is sufficient}, and this scan can come from various MRI vendors, field strengths, and pulse sequences. Experiments with four datasets, including glioblastoma data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality images from accelerated low-field scans, with robustness to out-of-distribution data. Our freely available framework thus enables rapid, diagnostic-quality, low-cost imaging for wide healthcare applications. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æä¾›äº†é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ç”±äºæˆæœ¬é«˜æ˜‚ï¼Œå…¶æ™®åŠç¨‹åº¦å—åˆ°é™åˆ¶ï¼Œè¿™å¯¹äºéœ€è¦é•¿æœŸæŠ¤ç†çš„æ‚£è€…æ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä½åœºMRIé€šè¿‡ä½¿ç”¨ä½æˆæœ¬è®¾å¤‡æä¾›å¯è´Ÿæ‹…çš„æˆåƒï¼Œä½†å—åˆ°æ‰«ææ—¶é—´é•¿å’Œå›¾åƒè´¨é‡ä¸‹é™ï¼ˆåŒ…æ‹¬ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰å’Œç»„ç»‡å¯¹æ¯”åº¦ï¼‰çš„é˜»ç¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŒ»ç–—èŒƒå¼ï¼šä½¿ç”¨æ·±åº¦å­¦ä¹ ä»è¿‡å»çš„æ ‡å‡†é«˜åœºMRIæ‰«æä¸­æå–ä¸ªæ€§åŒ–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç‰¹å¾å®ç°ä½æˆæœ¬ç³»ç»Ÿçš„åŠ é€Ÿã€é«˜è´¨é‡éšè®¿æ‰«æã€‚ä¸ºäº†å…‹æœä¿¡å™ªæ¯”å’Œå¯¹æ¯”åº¦å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViT-Fuserï¼Œè¿™æ˜¯ä¸€ç§ç‰¹å¾èåˆè§†è§‰å˜å‹å™¨ï¼Œå¯ä»¥ä»è¿‡å»çš„æ‰«æä¸­å­¦ä¹ ç‰¹å¾ï¼Œä¾‹å¦‚å­˜å‚¨åœ¨æ ‡å‡†DICOMå…‰ç›˜ä¸­çš„æ‰«æã€‚æˆ‘ä»¬è¯æ˜<em>åªéœ€ä¸€æ¬¡å…ˆéªŒæ‰«æ</em>ï¼Œè¿™æ¬¡æ‰«æå¯ä»¥æ¥è‡ªä¸åŒçš„MRIä¾›åº”å•†ã€ç£åœºå¼ºåº¦å’Œè„‰å†²åºåˆ—ã€‚ä½¿ç”¨åŒ…æ‹¬èƒ¶è´¨æ¯ç»†èƒç˜¤æ•°æ®ã€ä½åœºï¼ˆ50mTï¼‰å’Œè¶…ä½åœºï¼ˆ6.5mTï¼‰æ•°æ®çš„å››ä¸ªæ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒViT-Fuserä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæä¾›åŠ é€Ÿä½åœºæ‰«æçš„é«˜è´¨é‡å›¾åƒï¼Œå¹¶å¯¹å¼‚å¸¸æ•°æ®å…·æœ‰ç¨³å¥æ€§ã€‚æˆ‘ä»¬æä¾›çš„å…è´¹æ¡†æ¶å› æ­¤èƒ½å¤Ÿå®ç°å¿«é€Ÿã€è¯Šæ–­è´¨é‡ã€ä½æˆæœ¬æˆåƒï¼Œå¹¿æ³›åº”ç”¨äºåŒ»ç–—ä¿å¥é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02470v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨æ·±åº¦å­¦ä¹ ä»è¿‡å»çš„é«˜åœºMRIæ‰«æä¸­æå–ä¸ªæ€§åŒ–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç‰¹å¾å®ç°ä½æˆæœ¬ç³»ç»Ÿçš„åŠ é€Ÿã€é«˜è´¨é‡éšè®¿æ‰«æã€‚æå‡ºViT-Fuseræ„¿æ™¯è½¬æ¢å™¨ï¼Œå­¦ä¹ è¿‡å»æ‰«æçš„ç‰¹å¾ï¼Œå¦‚å­˜å‚¨åœ¨æ ‡å‡†DICOM CDä¸­çš„æ‰«æã€‚å®éªŒè¯æ˜ï¼Œå•æ¬¡å…ˆéªŒæ‰«æå³å¯ï¼Œä¸”è¯¥æ‰«æå¯æ¥è‡ªä¸åŒçš„MRIä¾›åº”å•†ã€åœºå¼ºå’Œè„‰å†²åºåˆ—ã€‚ViT-Fuseråœ¨åŠ é€Ÿä½åœºæ‰«æä¸­æä¾›é«˜è´¨é‡å›¾åƒï¼Œå…·æœ‰å¯¹ç¦»ç¾¤æ•°æ®çš„ç¨³å¥æ€§ï¼Œä¸ºå¹¿æ³›çš„åº”ç”¨æä¾›äº†å¿«é€Ÿã€è¯Šæ–­æ€§ã€ä½æˆæœ¬çš„æˆåƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ·±åº¦å­¦ä¹ ç”¨äºä»è¿‡å»çš„é«˜åœºMRIæ‰«æä¸­æå–ä¸ªæ€§åŒ–ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨è¿™äº›ç‰¹å¾å¯å®ç°ä½æˆæœ¬ç³»ç»Ÿçš„åŠ é€Ÿã€é«˜è´¨é‡éšè®¿æ‰«æã€‚</li>
<li>å¼•å…¥ViT-Fuseræ„¿æ™¯è½¬æ¢å™¨ï¼Œå­¦ä¹ è¿‡å»æ‰«æçš„ç‰¹å¾ã€‚</li>
<li>å•æ¬¡å…ˆéªŒæ‰«æå³å¯æ»¡è¶³éœ€æ±‚ï¼Œä¸”æ¥æºå¹¿æ³›ã€‚</li>
<li>ViT-Fuseråœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬èƒ¶è´¨æ¯ç»†èƒç˜¤æ•°æ®ã€ä½åœºå’Œè¶…ä½åœºæ•°æ®ã€‚</li>
<li>ViT-Fuserèƒ½æä¾›åŠ é€Ÿä½åœºæ‰«æçš„é«˜è´¨é‡å›¾åƒï¼Œå¹¶å…·æœ‰å¯¹ç¦»ç¾¤æ•°æ®çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-059fd6b96e4a330cdb360ffbd3a4e5bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d8a434c2685a026a0d67544fe0e3cd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79b6b1e8b1ae69d139c12ef53bca8389.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Token-Coordinated-Prompt-Attention-is-Needed-for-Visual-Prompting"><a href="#Token-Coordinated-Prompt-Attention-is-Needed-for-Visual-Prompting" class="headerlink" title="Token Coordinated Prompt Attention is Needed for Visual Prompting"></a>Token Coordinated Prompt Attention is Needed for Visual Prompting</h2><p><strong>Authors:Zichen Liu, Xu Zou, Gang Hua, Jiahuan Zhou</strong></p>
<p>Visual prompting techniques are widely used to efficiently fine-tune pretrained Vision Transformers (ViT) by learning a small set of shared prompts for all tokens. However, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of ViT. This often leads to indistinguishable and biased prompt-extracted features, hindering performance. To address this issue, we propose a plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. Firstly, recognizing the distinct functions of CLS and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into CLS Prompts and Image Prompts, which interact exclusively with CLS tokens and image tokens through attention mechanisms. This enhances their respective discriminative abilities. Furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. This enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. Extensive experiments across various benchmarks demonstrate that TCPA significantly enhances the diversity and discriminative power of the extracted features. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/ICML2025-TCPA">https://github.com/zhoujiahuan1991/ICML2025-TCPA</a>. </p>
<blockquote>
<p>è§†è§‰æç¤ºæŠ€æœ¯è¢«å¹¿æ³›ç”¨äºé€šè¿‡å­¦ä¹ ä¸€ç»„å…±äº«æç¤ºæ¥å¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸åŒæ ‡è®°åœ¨ä¼ é€’åˆ¤åˆ«ä¿¡æ¯æ–¹é¢çš„ç‹¬ç‰¹ä½œç”¨ï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„æç¤ºä¸æ‰€æœ‰æ ‡è®°è¿›è¡Œäº¤äº’ï¼Œä»è€Œé™åˆ¶äº†ViTçš„è¡¨å¾å®¹é‡ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´éš¾ä»¥åŒºåˆ†å’Œåå‘æç¤ºæå–çš„ç‰¹å¾ï¼Œé˜»ç¢æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„æ ‡è®°åè°ƒæç¤ºæ³¨æ„åŠ›ï¼ˆTCPAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä¸ºä¸åŒçš„æ ‡è®°åˆ†é…ç‰¹å®šçš„åè°ƒæç¤ºï¼Œä»¥è¿›è¡ŒåŸºäºæ³¨æ„åŠ›çš„äº¤äº’ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¤è¯†åˆ°CLSå’Œå›¾åƒæ ‡è®°çš„ä¸åŒåŠŸèƒ½â€”â€”å…¨å±€ä¿¡æ¯èšåˆå’Œå±€éƒ¨ç‰¹å¾æå–ï¼Œæˆ‘ä»¬å°†æç¤ºåˆ†è§£ä¸ºCLSæç¤ºå’Œå›¾åƒæç¤ºï¼Œå®ƒä»¬é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¸CLSæ ‡è®°å’Œå›¾åƒæ ‡è®°è¿›è¡Œç‹¬å®¶äº¤äº’ï¼Œå¢å¼ºäº†å„è‡ªçš„åˆ¤åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç”±äºä¸åŒçš„å›¾åƒæ ‡è®°å¯¹åº”äºä¸åŒçš„å›¾åƒå—å¹¶åŒ…å«å„ç§ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨åŒ¹é…å‡½æ•°æ¥è‡ªåŠ¨ä¸ºå•ä¸ªæ ‡è®°åˆ†é…åè°ƒæç¤ºã€‚è¿™èƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®çš„å…³æ³¨åŠ›äº¤äº’ï¼Œæé«˜æå–ç‰¹å¾çš„å¤šæ ·æ€§å’Œè¡¨å¾å®¹é‡ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTCPAæ˜¾è‘—å¢å¼ºäº†æå–ç‰¹å¾çš„å¤šæ ·æ€§å’Œåˆ¤åˆ«åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhoujiahuan1991/ICML2025-TCPA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhoujiahuan1991/ICML2025-TCPAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02406v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æç¤ºæŠ€æœ¯å¹¿æ³›åº”ç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„Vision Transformersï¼ˆViTï¼‰ï¼Œé€šè¿‡å­¦ä¹ ä¸€å°å¥—å…±äº«æç¤ºæ¥å®Œæˆæ‰€æœ‰ä»¤ç‰Œçš„æ•ˆç‡ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä¸åŒä»¤ç‰Œåœ¨ä¼ é€’é‰´åˆ«ä¿¡æ¯æ–¹é¢çš„ç‹¬ç‰¹ä½œç”¨ï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„æç¤ºä¸æ‰€æœ‰ä»¤ç‰Œè¿›è¡Œäº¤äº’ï¼Œä»è€Œé™åˆ¶äº†ViTçš„ä»£è¡¨æ€§å®¹é‡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å³æ’å³ç”¨çš„ä»¤ç‰Œåè°ƒæç¤ºæ³¨æ„åŠ›ï¼ˆTCPAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä¸ºä¸åŒçš„ä»¤ç‰Œåˆ†é…ç‰¹å®šçš„åè°ƒæç¤ºè¿›è¡ŒåŸºäºæ³¨æ„åŠ›çš„äº¤äº’ã€‚æˆ‘ä»¬è®¤è¯†åˆ°CLSå’Œå›¾åƒä»¤ç‰Œçš„ä¸åŒåŠŸèƒ½â€”â€”å…¨å±€ä¿¡æ¯èšåˆå’Œå±€éƒ¨ç‰¹å¾æå–ï¼Œå°†æç¤ºåˆ†ä¸ºCLSæç¤ºå’Œå›¾åƒæç¤ºï¼Œå®ƒä»¬é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ä¸CLSä»¤ç‰Œå’Œå›¾åƒä»¤ç‰Œè¿›è¡Œç‹¬å®¶äº’åŠ¨ï¼Œå¢å¼ºäº†å„è‡ªçš„é‰´åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç”±äºä¸åŒçš„å›¾åƒä»¤ç‰Œå¯¹åº”äºä¸åŒçš„å›¾åƒè¡¥ä¸å¹¶åŒ…å«å„ç§ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨åŒ¹é…åŠŸèƒ½è‡ªåŠ¨ä¸ºä¸ªåˆ«ä»¤ç‰Œåˆ†é…åè°ƒæç¤ºã€‚è¿™å®ç°äº†æ›´ç²¾ç¡®çš„å…³æ³¨åŠ›äº¤äº’ï¼Œæé«˜äº†æå–ç‰¹å¾çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§å®¹é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æç¤ºæŠ€æœ¯ç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„Vision Transformersï¼ˆViTï¼‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥ä¸åŒä»¤ç‰Œåœ¨ä¼ é€’é‰´åˆ«ä¿¡æ¯ä¸­çš„ç‹¬ç‰¹ä½œç”¨ã€‚</li>
<li>æå‡ºäº†Token Coordinated Prompt Attentionï¼ˆTCPAï¼‰æ¨¡å—ï¼Œä¸ºä¸åŒä»¤ç‰Œåˆ†é…ç‰¹å®šçš„åè°ƒæç¤ºã€‚</li>
<li>TCPAæ¨¡å—å¢å¼ºCLSä»¤ç‰Œå’Œå›¾åƒä»¤ç‰Œå„è‡ªçš„é‰´åˆ«èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ï¼ŒTCPAæ¨¡å—å®ç°CLSæç¤ºå’Œå›¾åƒæç¤ºä¸ç›¸åº”ä»¤ç‰Œçš„ç‹¬å®¶äº’åŠ¨ã€‚</li>
<li>é‡‡ç”¨åŒ¹é…åŠŸèƒ½è‡ªåŠ¨ä¸ºå›¾åƒä»¤ç‰Œåˆ†é…åè°ƒæç¤ºï¼Œæé«˜ç‰¹å¾æå–çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§å®¹é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-928fc0d77c232f23a2dc6ca6d7fcae7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea3b003a2c89728c6f7026077358f547.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c7449976ffc933175a1c9d3dbbf9848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7547718f44cf4b2d8b4b9b7bd94e3617.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5026d2f5e1dda8954a428a6a292d1dec.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OASIS-Optimized-Lightweight-Autoencoder-System-for-Distributed-In-Sensor-computing"><a href="#OASIS-Optimized-Lightweight-Autoencoder-System-for-Distributed-In-Sensor-computing" class="headerlink" title="OASIS: Optimized Lightweight Autoencoder System for Distributed   In-Sensor computing"></a>OASIS: Optimized Lightweight Autoencoder System for Distributed   In-Sensor computing</h2><p><strong>Authors:Chengwei Zhou, Sreetama Sarkar, Yuming Li, Arnab Sanyal, Gourav Datta</strong></p>
<p>In-sensor computing, which integrates computation directly within the sensor, has emerged as a promising paradigm for machine vision applications such as AR&#x2F;VR and smart home systems. By processing data on-chip before transmission, it alleviates the bandwidth bottleneck caused by high-resolution, high-frame-rate image transmission, particularly in video applications. We envision a system architecture that integrates a CMOS image sensor (CIS) with a logic chip via advanced packaging, where the logic chip processes early-stage deep neural network (DNN) layers. However, its limited compute and memory make deploying advanced DNNs challenging. A simple solution is to split the model, executing the first part on the logic chip and the rest off-chip. However, modern DNNs require multiple layers before dimensionality reduction, limiting their ability to achieve the primary goal of in-sensor computing: minimizing data bandwidth. To address this, we propose a dual-branch autoencoder-based vision architecture that deploys a lightweight encoder on the logic chip while the task-specific network runs off-chip. The encoder is trained using a triple loss function: (1) task-specific loss to optimize accuracy, (2) entropy loss to enforce compact and compressible representations, and (3) reconstruction loss (mean-square error) to preserve essential visual information. This design enables a four-order-of-magnitude reduction in output activation dimensionality compared to input images, resulting in a $2{-}4.5\times$ decrease in energy consumption, as validated by our hardware-backed semi-analytical energy models. We evaluate our approach on CNN and ViT-based models across applications in smart home and augmented reality domains, achieving state-of-the-art accuracy with energy efficiency of up to 22.7 TOPS&#x2F;W. </p>
<blockquote>
<p>å°†è®¡ç®—ç›´æ¥é›†æˆåˆ°ä¼ æ„Ÿå™¨ä¸­çš„æ„Ÿå™¨è®¡ç®—ï¼ˆIn-sensor computingï¼‰å·²æˆä¸ºAR&#x2F;VRå’Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿç­‰æœºå™¨è§†è§‰åº”ç”¨çš„å‰æ™¯çœ‹å¥½çš„èŒƒå¼ã€‚å®ƒåœ¨ä¼ è¾“å‰åœ¨èŠ¯ç‰‡ä¸Šå¤„ç†æ•°æ®ï¼Œä»è€Œå‡è½»äº†ç”±é«˜åˆ†è¾¨ç‡ã€é«˜å¸§ç‡å›¾åƒä¼ è¾“å¼•èµ·çš„å¸¦å®½ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘åº”ç”¨ä¸­ã€‚æˆ‘ä»¬è®¾æƒ³äº†ä¸€ç§é€šè¿‡å…ˆè¿›å°è£…æŠ€æœ¯å°†CMOSå›¾åƒä¼ æ„Ÿå™¨ï¼ˆCISï¼‰ä¸é€»è¾‘èŠ¯ç‰‡é›†æˆçš„ç³»ç»Ÿæ¶æ„ï¼Œé€»è¾‘èŠ¯ç‰‡å¤„ç†æ—©æœŸæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å±‚ã€‚ç„¶è€Œï¼Œå…¶æœ‰é™çš„è®¡ç®—å’Œå†…å­˜ä½¿éƒ¨ç½²å…ˆè¿›çš„DNNé¢ä¸´æŒ‘æˆ˜ã€‚ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯å°†æ¨¡å‹æ‹†åˆ†ï¼Œåœ¨é€»è¾‘èŠ¯ç‰‡ä¸Šæ‰§è¡Œç¬¬ä¸€éƒ¨åˆ†ï¼Œå…¶ä½™éƒ¨åˆ†åœ¨èŠ¯ç‰‡å¤–æ‰§è¡Œã€‚ç„¶è€Œï¼Œç°ä»£DNNéœ€è¦åœ¨é™ç»´ä¹‹å‰è¿›è¡Œå¤šå±‚è®¡ç®—ï¼Œè¿™é™åˆ¶äº†å…¶å®ç°æ„Ÿå™¨è®¡ç®—ä¸»è¦ç›®æ ‡çš„èƒ½åŠ›ï¼Œå³æœ€å°åŒ–æ•°æ®å¸¦å®½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŒåˆ†æ”¯è‡ªåŠ¨ç¼–ç å™¨çš„è§†è§‰æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨é€»è¾‘èŠ¯ç‰‡ä¸Šéƒ¨ç½²è½»é‡çº§ç¼–ç å™¨ï¼Œè€Œä»»åŠ¡ç‰¹å®šç½‘ç»œåœ¨èŠ¯ç‰‡å¤–è¿è¡Œã€‚ç¼–ç å™¨ä½¿ç”¨ä¸‰é‡æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼šï¼ˆ1ï¼‰ä»»åŠ¡ç‰¹å®šæŸå¤±ä»¥ä¼˜åŒ–å‡†ç¡®æ€§ï¼Œï¼ˆ2ï¼‰ç†µæŸå¤±ä»¥å®æ–½ç´§å‡‘ä¸”å¯å‹ç¼©çš„è¡¨ç¤ºå½¢å¼ï¼Œï¼ˆ3ï¼‰é‡å»ºæŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰ä»¥ä¿ç•™å¿…è¦çš„è§†è§‰ä¿¡æ¯ã€‚è¿™ä¸€è®¾è®¡å®ç°äº†è¾“å‡ºæ¿€æ´»ç»´åº¦ç›¸å¯¹äºè¾“å…¥å›¾åƒçš„å››ä¸ªæ•°é‡çº§çš„é™ä½ï¼Œä»è€Œå®ç°äº†èƒ½é‡æ¶ˆè€—çš„å‡å°‘ï¼ˆå‡å°‘å¹…åº¦ä¸º2-4.5å€ï¼‰ï¼Œè¿™å·²ç”±æˆ‘ä»¬çš„ç¡¬ä»¶æ”¯æŒåŠåˆ†æèƒ½é‡æ¨¡å‹éªŒè¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ™ºèƒ½å®¶å±…å’Œå¢å¼ºç°å®é¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œå¯¹åŸºäºCNNå’ŒViTçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å…·æœ‰é«˜è¾¾22.7 TOPS&#x2F;Wçš„èƒ½é‡æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02256v1">PDF</a> Under review; 8 pages, 5 figures</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åœ¨ä¼ æ„Ÿå™¨å†…éƒ¨è¿›è¡Œè®¡ç®—çš„æ–°å‹è®¡ç®—æ¨¡å¼â€”â€”åœ¨ä¼ æ„Ÿå™¨è®¡ç®—ã€‚è¯¥æ¨¡å¼å¯¹äºæœºå™¨è§†è§‰åº”ç”¨å¦‚AR&#x2F;VRå’Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿå…·æœ‰å‰æ™¯ã€‚é€šè¿‡ç›´æ¥åœ¨èŠ¯ç‰‡ä¸Šå¤„ç†æ•°æ®ï¼Œå®ƒè§£å†³äº†é«˜åˆ†è¾¨ç‡ã€é«˜å¸§ç‡å›¾åƒä¼ è¾“å¼•èµ·çš„å¸¦å®½ç“¶é¢ˆé—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç»“åˆCMOSå›¾åƒä¼ æ„Ÿå™¨å’Œé€»è¾‘èŠ¯ç‰‡çš„ç³»ç»Ÿæ¶æ„ï¼Œé€šè¿‡å…ˆè¿›å°è£…æŠ€æœ¯å®ç°ã€‚ä¸ºäº†è§£å†³åœ¨é€»è¾‘èŠ¯ç‰‡ä¸Šéƒ¨ç½²é«˜çº§æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºè‡ªç¼–ç æœºçš„åŒåˆ†æ”¯è§†è§‰æ¶æ„ã€‚ç¼–ç å™¨ä½¿ç”¨ä¸‰é‡æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°ä¼˜åŒ–ç²¾åº¦ã€ç´§å‡‘å’Œå¯å‹ç¼©çš„è¡¨ç¤ºä»¥åŠå…³é”®è§†è§‰ä¿¡æ¯çš„ä¿ç•™ã€‚è¿™ç§è®¾è®¡å®ç°äº†è¾“å‡ºæ¿€æ´»ç»´åº¦ç›¸å¯¹äºè¾“å…¥å›¾åƒçš„å››ä¸ªæ•°é‡çº§çš„é™ä½ï¼Œå‡å°‘äº†èƒ½æºæ¶ˆè€—ã€‚è¯¥æ–¹æ³•çš„èƒ½é‡æ•ˆç‡å’Œå‡†ç¡®ç‡åœ¨æ™ºèƒ½å®¶åº­å’Œå¢å¼ºç°å®é¢†åŸŸå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä¼ æ„Ÿå™¨è®¡ç®—æ˜¯ä¸€ç§æ–°å…´çš„è®¡ç®—æ¨¡å¼ï¼Œå®ƒåœ¨æœºå™¨è§†è§‰åº”ç”¨ä¸­å…·æœ‰å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨AR&#x2F;VRå’Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿä¸­ã€‚</li>
<li>åœ¨ä¼ æ„Ÿå™¨è®¡ç®—æ¨¡å¼é€šè¿‡åœ¨ä¼ æ„Ÿå™¨å†…éƒ¨ç›´æ¥å¤„ç†æ•°æ®æ¥è§£å†³å¸¦å®½ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>ä¸€ç§ç³»ç»Ÿæ¶æ„ç»“åˆäº†CMOSå›¾åƒä¼ æ„Ÿå™¨å’Œé€»è¾‘èŠ¯ç‰‡ï¼Œé€šè¿‡å…ˆè¿›å°è£…æŠ€æœ¯å®ç°é›†æˆã€‚</li>
<li>åœ¨éƒ¨ç½²é«˜çº§DNNæ—¶ï¼Œå—åˆ°è®¡ç®—å’Œå†…å­˜é™åˆ¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªç¼–ç æœºçš„åŒåˆ†æ”¯è§†è§‰æ¶æ„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç¼–ç å™¨ä½¿ç”¨ä¸‰é‡æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¼˜åŒ–ç²¾åº¦ã€å®ç°ç´§å‡‘å’Œå¯å‹ç¼©çš„è¡¨ç¤ºï¼Œå¹¶ä¿ç•™å…³é”®è§†è§‰ä¿¡æ¯ã€‚</li>
<li>è¯¥è®¾è®¡å®ç°äº†è¾“å‡ºæ¿€æ´»ç»´åº¦ç›¸å¯¹äºè¾“å…¥å›¾åƒçš„æ˜¾è‘—é™ä½ï¼Œé™ä½äº†èƒ½æºæ¶ˆè€—ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ™ºèƒ½å®¶åº­å’Œå¢å¼ºç°å®é¢†åŸŸçš„èƒ½é‡æ•ˆç‡å’Œå‡†ç¡®ç‡å‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44501ba9d0a140fe87fcf8fe298883d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c72dafe60e7bc2d12988e1ff0a4100.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-329e4e7734c67ba801dd44cf6338fc5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ea5e90e6861ef096a3f492462953e39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e21691b08120585c4e73bc0972cc1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09e8273f17c04424bdeab5f247ce3647.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CSASN-A-Multitask-Attention-Based-Framework-for-Heterogeneous-Thyroid-Carcinoma-Classification-in-Ultrasound-Images"><a href="#CSASN-A-Multitask-Attention-Based-Framework-for-Heterogeneous-Thyroid-Carcinoma-Classification-in-Ultrasound-Images" class="headerlink" title="CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid   Carcinoma Classification in Ultrasound Images"></a>CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid   Carcinoma Classification in Ultrasound Images</h2><p><strong>Authors:Peiqi Li, Yincheng Gao, Renxing Li, Haojie Yang, Yunyun Liu, Boji Liu, Jiahui Ni, Ying Zhang, Yulu Wu, Xiaowei Fang, Lehang Guo, Liping Sun, Jiangang Chen</strong></p>
<p>Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis. </p>
<blockquote>
<p>åœ¨è¶…å£°æˆåƒä¸­ï¼Œå¼‚è´¨å½¢æ€ç‰¹å¾å’Œæ•°æ®ä¸å¹³è¡¡ç»™ç½•è§ç”²çŠ¶è…ºç™Œçš„åˆ†ç±»å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶â€”â€”é€šé“ç©ºé—´æ³¨æ„åŠ›ååŒç½‘ç»œï¼ˆCSASNï¼‰ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸€ä¸ªåŒåˆ†æ”¯ç‰¹å¾æå–å™¨ï¼Œç»“åˆäº†EfficientNetè¿›è¡Œå±€éƒ¨ç©ºé—´ç¼–ç å’ŒViTè¿›è¡Œå…¨å±€è¯­ä¹‰å»ºæ¨¡ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªçº§è”çš„é€šé“ç©ºé—´æ³¨æ„åŠ›ä¼˜åŒ–æ¨¡å—ã€‚æ®‹å·®å¤šå°ºåº¦åˆ†ç±»å™¨å’ŒåŠ¨æ€åŠ æƒæŸå¤±å‡½æ•°è¿›ä¸€æ­¥æé«˜äº†åˆ†ç±»çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜¯åœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡2000åæ‚£è€…çš„å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªå››ä¸ªä¸´åºŠæœºæ„ã€‚é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶è¯æ˜ï¼Œæ¯ä¸ªæ¨¡å—éƒ½å¯¹æ¨¡å‹æ€§èƒ½åšå‡ºäº†é‡å¤§è´¡çŒ®ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«ç½•è§äºšå‹å¦‚FTCå’ŒMTCç™Œæ–¹é¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCSASNä¼˜äºç°æœ‰çš„å•æµCNNæˆ–åŸºäºTransformerçš„æ¨¡å‹ï¼Œåœ¨ç±»åˆ«ä¸å¹³è¡¡çš„æ¡ä»¶ä¸‹ï¼Œåœ¨ç²¾åº¦å’Œå¬å›ç‡ä¹‹é—´å–å¾—äº†ä¼˜è¶Šçš„å¹³è¡¡ã€‚è¯¥æ¡†æ¶ä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©ç”²çŠ¶è…ºç™Œè¯Šæ–­æä¾›äº†æœ‰å‰æ™¯çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02211v1">PDF</a> 18 pages, 10 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç¯‡å…³äºä½¿ç”¨è¶…å£°æˆåƒå¯¹ç¨€æœ‰ç”²çŠ¶è…ºç™Œè¿›è¡Œåˆ†ç±»çš„ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°å‹å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶CSASNï¼Œé€šè¿‡ç»“åˆEfficientNetè¿›è¡Œå±€éƒ¨ç©ºé—´ç¼–ç å’ŒViTè¿›è¡Œå…¨å±€è¯­ä¹‰å»ºæ¨¡ï¼Œè§£å†³å¼‚è´¨å½¢æ€ç‰¹å¾å’Œä¸å¹³è¡¡æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨çº§è”é€šé“ç©ºé—´æ³¨æ„åŠ›ä¼˜åŒ–æ¨¡å—ï¼Œå¹¶é€šè¿‡æ®‹å·®å¤šå°ºåº¦åˆ†ç±»å™¨å’ŒåŠ¨æ€åŠ æƒæŸå¤±å‡½æ•°æé«˜åˆ†ç±»çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚ç»è¿‡è¶…è¿‡ä¸¤åƒåæ‚£è€…çš„å¤šä¸­å¿ƒæ•°æ®é›†è®­ç»ƒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«ç½•è§äºšå‹å¦‚FTCå’ŒMTCç™Œã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCSASNä¼˜äºç°æœ‰çš„å•æµCNNæˆ–Transformeræ¨¡å‹ï¼Œåœ¨ç±»åˆ«ä¸å¹³è¡¡æ¡ä»¶ä¸‹å®ç°äº†ç²¾åº¦å’Œå¬å›ç‡çš„å¹³è¡¡ï¼Œä¸ºAIè¾…åŠ©ç”²çŠ¶è…ºç™Œè¯Šæ–­æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹è¶…å£°æˆåƒä¸­ç½•è§çš„ç”²çŠ¶è…ºç™Œåˆ†ç±»çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†CSASNå¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶æ¥è§£å†³å¼‚è´¨å½¢æ€ç‰¹å¾å’Œä¸å¹³è¡¡æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>CSASNç»“åˆäº†EfficientNetå’ŒViTä¸¤ç§æŠ€æœ¯ï¼Œåˆ†åˆ«è¿›è¡Œå±€éƒ¨ç©ºé—´ç¼–ç å’Œå…¨å±€è¯­ä¹‰å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡çº§è”é€šé“ç©ºé—´æ³¨æ„åŠ›ä¼˜åŒ–æ¨¡å—æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨æ®‹å·®å¤šå°ºåº¦åˆ†ç±»å™¨å’ŒåŠ¨æ€åŠ æƒæŸå¤±å‡½æ•°ï¼Œå¢å¼ºäº†åˆ†ç±»çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä¸­å¿ƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼ŒåŒ…æ‹¬è¶…è¿‡ä¸¤åƒåæ‚£è€…ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«ç½•è§ç™Œç—‡çŠ¶å¦‚FTCå’ŒMTCã€‚</li>
<li>CSASNåœ¨ç±»åˆ«ä¸å¹³è¡¡æ¡ä»¶ä¸‹è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œå®ç°äº†ç²¾åº¦å’Œå¬å›ç‡çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79b322867e6e456af0f27c4dc8478b04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b96ca88b764e1cd2c12d99b1c973c357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce18ccfe9cdbe4a91eaf4d424ea0664.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Heart-Failure-Prediction-using-Modal-Decomposition-and-Masked-Autoencoders-for-Scarce-Echocardiography-Databases"><a href="#Heart-Failure-Prediction-using-Modal-Decomposition-and-Masked-Autoencoders-for-Scarce-Echocardiography-Databases" class="headerlink" title="Heart Failure Prediction using Modal Decomposition and Masked   Autoencoders for Scarce Echocardiography Databases"></a>Heart Failure Prediction using Modal Decomposition and Masked   Autoencoders for Scarce Echocardiography Databases</h2><p><strong>Authors:AndrÃ©s Bell-Navas, MarÃ­a Villalba-Orero, Enrique Lara-Pezzi, JesÃºs Garicano-Mena, Soledad Le Clainche</strong></p>
<p>Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid, and effective prediction. This work presents an automatic system based on a novel deep learning framework which analyses in real-time echocardiography video sequences for the challenging and more specific task of heart failure time prediction. This system works in two stages. The first one transforms the data from a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any machine learning-based framework, including a deep learning-based one. This stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage builds and trains a Vision Transformer (ViT). Self-supervised learning (SSL) methods, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the ViT from scratch, even with scarce databases. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures. The source code will be incorporated into the next version release of the ModelFLOWs-app software (<a target="_blank" rel="noopener" href="https://github.com/modelflows/ModelFLOWs-app">https://github.com/modelflows/ModelFLOWs-app</a>). </p>
<blockquote>
<p>å¿ƒè„ç—…æ˜¯å›½é™…äººç±»åŠŸèƒ½éšœç¢çš„ä¸»è¦è¯±å› ã€‚æ®ä¸–ç•Œå«ç”Ÿç»„ç»‡ï¼ˆWHOï¼‰ç»Ÿè®¡ï¼Œæ¯å¹´çº¦æœ‰1800ä¸‡äººå› å¿ƒè„ç—…è€Œæ­»äº¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¿ƒåŠ›è¡°ç«­ï¼ˆHFï¼‰è¿«ä½¿åŒ»ç–—è¡Œä¸šéœ€è¦å¼€å‘ç³»ç»Ÿä»¥è¿›è¡Œæ—©æœŸã€å¿«é€Ÿå’Œæœ‰æ•ˆçš„é¢„æµ‹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºæ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶çš„è‡ªåŠ¨ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯å®æ—¶åˆ†æè¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åºåˆ—ï¼Œä»¥è¿›è¡Œæ›´å…·æŒ‘æˆ˜æ€§çš„å¿ƒåŠ›è¡°ç«­æ—¶é—´é¢„æµ‹ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µå°†è¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åºåˆ—æ•°æ®åº“çš„æ•°æ®è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ å…¼å®¹çš„æ³¨é‡Šå›¾åƒé›†åˆï¼Œå¯ç”¨äºä»»ä½•åŸºäºæœºå™¨å­¦ä¹ ï¼ˆåŒ…æ‹¬æ·±åº¦å­¦ä¹ ï¼‰çš„æ¡†æ¶çš„è®­ç»ƒé˜¶æ®µã€‚è¿™ä¸€é˜¶æ®µåŒ…æ‹¬ä½¿ç”¨é«˜é˜¶åŠ¨æ€æ¨¡å¼åˆ†è§£ï¼ˆHODMDï¼‰ç®—æ³•è¿›è¡Œæ•°æ®å¢å¼ºå’Œç‰¹å¾æå–ã€‚ç¬¬äºŒé˜¶æ®µæ„å»ºå¹¶è®­ç»ƒVision Transformerï¼ˆViTï¼‰ã€‚é‡‡ç”¨è¿„ä»Šä¸ºæ­¢åœ¨å¿ƒåŠ›è¡°ç«­é¢„æµ‹æ–‡çŒ®ä¸­å¾ˆå°‘æ¢ç´¢çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•æ¥æœ‰æ•ˆåœ°ä»å¤´å¼€å§‹è®­ç»ƒViTï¼Œå³ä½¿æ•°æ®åº“ç¨€ç¼ºä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ‰€è®¾è®¡çš„ç¥ç»ç½‘ç»œåˆ†æè¶…å£°å¿ƒåŠ¨å›¾åºåˆ—çš„å›¾åƒï¼Œä»¥ä¼°è®¡å¿ƒåŠ›è¡°ç«­å‘ç”Ÿçš„æ—¶é—´ã€‚è·å¾—çš„ç»“æœæ˜¾ç¤ºäº†HODMDç®—æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠæ‰€æå‡ºçš„ç³»ç»Ÿåœ¨å¤šä¸ªå·²å»ºç«‹çš„ViTå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æºä»£ç å°†åŒ…å«åœ¨ModelFLOWs-appè½¯ä»¶çš„ä¸‹ä¸€ä¸ªç‰ˆæœ¬å‘å¸ƒä¸­ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/modelflows/ModelFLOWs-app%EF%BC%89%E3%80%82">https://github.com/modelflows/ModelFLOWs-appï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07606v2">PDF</a> 39 pages, 7 figures. arXiv admin note: substantial text overlap with   arXiv:2404.19579</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨ç³»ç»Ÿï¼Œç”¨äºå®æ—¶åˆ†æè¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åºåˆ—ï¼Œé¢„æµ‹å¿ƒè„è¡°ç«­çš„æ—¶é—´ã€‚ç³»ç»Ÿåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µå°†æ•°æ®è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ å…¼å®¹çš„æ ‡æ³¨å›¾åƒé›†åˆï¼Œç¬¬äºŒé˜¶æ®µæ„å»ºå¹¶è®­ç»ƒVision Transformerï¼ˆViTï¼‰ã€‚é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æ•ˆè®­ç»ƒViTï¼Œå³ä½¿æ•°æ®åº“ç¨€ç¼ºä¹Ÿèƒ½ä»å¤´å¼€å§‹è®­ç»ƒã€‚æ‰€è®¾è®¡çš„ç¥ç»ç½‘ç»œé€šè¿‡åˆ†æè¶…å£°å¿ƒåŠ¨å›¾åºåˆ—çš„å›¾åƒæ¥ä¼°è®¡å¿ƒè„è¡°ç«­å‘ç”Ÿçš„æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºä¸€ç§è‡ªåŠ¨ç³»ç»Ÿï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œç”¨äºå®æ—¶åˆ†æè¶…å£°å¿ƒåŠ¨å›¾è§†é¢‘åºåˆ—ï¼ŒæŒ‘æˆ˜æ€§åœ°é¢„æµ‹å¿ƒè„è¡°ç«­çš„æ—¶é—´ã€‚</li>
<li>ç³»ç»ŸåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šæ•°æ®è½¬æ¢å’ŒViTçš„æ„å»ºä¸è®­ç»ƒã€‚</li>
<li>ä½¿ç”¨é«˜é˜¶åŠ¨æ€æ¨¡å¼åˆ†è§£ï¼ˆHODMDï¼‰ç®—æ³•è¿›è¡Œæ•°æ®å¢å¼ºå’Œç‰¹å¾æå–ã€‚</li>
<li>é¦–æ¬¡åœ¨å¿ƒè„è¡°ç«­é¢„æµ‹é¢†åŸŸé‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è®­ç»ƒViTã€‚</li>
<li>è®¾è®¡çš„ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€šè¿‡åˆ†æè¶…å£°å¿ƒåŠ¨å›¾åºåˆ—çš„å›¾åƒæ¥é¢„æµ‹å¿ƒè„è¡°ç«­å‘ç”Ÿçš„æ—¶é—´ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†HODMDç®—æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠè¯¥ç³»ç»Ÿç›¸è¾ƒäºå…¶ä»–ViTå’ŒCNNæ¶æ„çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-000bfbfd225f980d772736ec03d6d8a9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning"><a href="#Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning" class="headerlink" title="Visual Adaptive Prompting for Compositional Zero-Shot Learning"></a>Visual Adaptive Prompting for Compositional Zero-Shot Learning</h2><p><strong>Authors:Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh</strong></p>
<p>Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„è”åˆè¡¨ç¤ºå­¦ä¹ æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºç”¨äºç»„åˆé›¶å°„å‡»å­¦ä¹ ï¼ˆCZSLï¼‰ç­‰ä»»åŠ¡çš„å¼ºå¤§å·¥å…·ã€‚CZSLè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°è®­ç»ƒæœŸé—´æœªæ˜ç¡®é‡åˆ°çš„æ–°ç»„åˆçš„è§†è§‰å…ƒç´ ï¼Œå¦‚å±æ€§å’Œå¯¹è±¡ã€‚æœ€è¿‘å…³äºCZSLæç¤ºçš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥ï¼Œé€šå¸¸ä½¿ç”¨åœ¨å¤šç§è§†è§‰ä¸Šä¸‹æ–‡ä¸­ä¸ä¼šæ”¹å˜çš„é™æ€æç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ•æ‰å¤šå˜çš„è§†è§‰ä¸Šä¸‹æ–‡æ—¶é‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¸“æ³¨äºæ–‡æœ¬é€‚åº”ï¼Œè€Œä¸æ˜¯åˆ©ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œç»„åˆæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºå­˜å‚¨åº“å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¡†æ¶å†…ç¼©å°è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€è§†è§‰æç¤ºå­˜å‚¨åº“æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®å›¾åƒçš„è§†è§‰ç‰¹å¾é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å’Œå¯¹è±¡æç¤ºã€‚æˆ‘ä»¬æå‡ºçš„ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸€ä¸ªæ›´å…·æ³›åŒ–èƒ½åŠ›çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ä¸‰ç§CZSLåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œæ— è®ºæ˜¯åœ¨å°é—­ä¸–ç•Œè¿˜æ˜¯å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ï¼Œéƒ½å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20292v4">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è”åˆè¡¨ç¤ºè§†è§‰å’Œæ–‡æœ¬æ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œé€‚ç”¨äºç»„åˆé›¶å°„å­¦ä¹ ï¼ˆCZSLï¼‰ç­‰ä»»åŠ¡ã€‚ä¸ºåº”å¯¹CZSLä¸­å¯¹äºè§†è§‰ä¸Šä¸‹æ–‡çš„æ•æ‰éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§è§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºåº“å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶ï¼Œç¼©å°è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡åŠ¨æ€è§†è§‰æç¤ºåº“æœºåˆ¶ï¼Œç³»ç»Ÿæ ¹æ®å›¾åƒè§†è§‰ç‰¹å¾é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å’Œå¯¹è±¡æç¤ºã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¸‰ç§CZSLåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMså…·å¤‡å¼ºå¤§çš„è”åˆè¡¨ç¤ºè§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>CZSLä»»åŠ¡éœ€è¦æ¨¡å‹å¯¹æœªåœ¨è®­ç»ƒä¸­æ˜ç¡®é‡åˆ°çš„æ–°ç»„åˆçš„è§†è§‰åŸå§‹æ•°æ®è¿›è¡Œæ³›åŒ–ã€‚</li>
<li>ç°æœ‰æç¤ºæ–¹æ³•ä¸»è¦ä¾§é‡äºæ–‡æœ¬é€‚åº”ï¼Œéš¾ä»¥å……åˆ†åˆ©ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œç»„åˆæ¨ç†ã€‚</li>
<li>VAPSé€šè¿‡åˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºåº“å’ŒåŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶æ¥ç¼©å°è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚</li>
<li>VAPSåŒ…æ‹¬ä¸€ä¸ªåŠ¨æ€è§†è§‰æç¤ºåº“æœºåˆ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å›¾åƒé€‰æ‹©ç›¸å…³æç¤ºã€‚</li>
<li>VAPSåŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´å…·æ³›åŒ–èƒ½åŠ›çš„åµŒå…¥ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-911fb6369694972f864d97d3df6e74ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fef2eab66368550060014f7cb7d0a735.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-766cdad7fa24ba448d036b48ce9790df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4c706091bcfe9ac755cc2ca42bbcb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f0371debde75c6ece6f5931410997d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Underwater-Image-Enhancement-via-Dehazing-and-Color-Restoration"><a href="#Underwater-Image-Enhancement-via-Dehazing-and-Color-Restoration" class="headerlink" title="Underwater Image Enhancement via Dehazing and Color Restoration"></a>Underwater Image Enhancement via Dehazing and Color Restoration</h2><p><strong>Authors:Chengqin Wu, Shuai Yu, Tuyan Luo, Qiuhua Rao, Qingson Hu, Jingxiang Xu, Lijun Zhang</strong></p>
<p>Underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. To overcome this limitation, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) that dynamically integrates these decoupled features to achieve comprehensive enhancement. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to respectively preserve color fidelity and enhance structural details during network training. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images. </p>
<blockquote>
<p>æ°´ä¸‹è§†è§‰æˆåƒå¯¹äºæµ·æ´‹å·¥ç¨‹è‡³å…³é‡è¦ï¼Œä½†å®ƒå—åˆ°ä½å¯¹æ¯”åº¦ã€æ¨¡ç³Šå’Œé¢œè‰²é€€åŒ–ç­‰å› ç´ çš„å½±å“ï¼Œé˜»ç¢äº†åç»­åˆ†æã€‚ç°æœ‰çš„æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•é€šå¸¸å°†é›¾éœ¾å’Œè‰²å½©æŠ•å°„è§†ä¸ºç»Ÿä¸€çš„é€€åŒ–è¿‡ç¨‹ï¼Œå¿½ç•¥äº†å®ƒä»¬å†…åœ¨çš„ç‹¬ç«‹æ€§ï¼ŒåŒæ—¶å¿½è§†äº†å®ƒä»¬ä¹‹é—´çš„ååŒå…³ç³»ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„ç½‘ç»œï¼ˆç§°ä¸ºWaterFormerï¼‰æ¥æ”¹å–„æ°´ä¸‹å›¾åƒè´¨é‡ã€‚WaterFormeråŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šå»é›¾å—ï¼ˆDehazeFormer Blockï¼‰ç”¨äºæ•è·è‡ªç›¸å…³çš„é›¾ç‰¹å¾å¹¶æå–æ·±å±‚ç‰¹å¾ï¼Œè‰²å½©æ¢å¤å—ï¼ˆCRBï¼‰ç”¨äºæ•è·è‡ªç›¸å…³çš„è‰²å½©æŠ•å°„ç‰¹å¾ï¼Œä»¥åŠé€šé“èåˆå—ï¼ˆCFBï¼‰åŠ¨æ€èåˆè¿™äº›è§£è€¦ç‰¹å¾ä»¥å®ç°å…¨é¢å¢å¼ºã€‚ä¸ºäº†ä¿è¯çœŸå®æ€§ï¼Œè¿˜åŠ å…¥äº†ä¸€ä¸ªåŸºäºæ°´ä¸‹æˆåƒç‰©ç†æ¨¡å‹çš„è½¯é‡å»ºå±‚ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†è‰²åº¦ä¸€è‡´æ€§æŸå¤±å’ŒSobelè‰²å½©æŸå¤±ï¼Œä»¥åœ¨ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­åˆ†åˆ«ä¿æŒé¢œè‰²ä¿çœŸåº¦å’Œå¢å¼ºç»“æ„ç»†èŠ‚ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼ŒWaterFormeråœ¨å¢å¼ºæ°´ä¸‹å›¾åƒæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09779v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ°´ä¸‹å›¾åƒå¢å¼ºç½‘ç»œï¼ˆWaterFormerï¼‰ï¼Œæ—¨åœ¨è§£å†³æ°´ä¸‹è§†è§‰æˆåƒä¸­çš„ä½å¯¹æ¯”åº¦å’Œè‰²å½©å¤±çœŸé—®é¢˜ã€‚è¯¥ç½‘ç»œåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šå»é›¾å—ï¼ˆDehazeFormer Blockï¼‰ã€è‰²å½©æ¢å¤å—ï¼ˆCRBï¼‰å’Œé€šé“èåˆå—ï¼ˆCFBï¼‰ã€‚å»é›¾å—ç”¨äºæ•æ‰é›¾æ°”ç‰¹å¾å¹¶æå–æ·±å±‚ç‰¹å¾ï¼Œè‰²å½©æ¢å¤å—åˆ™è´Ÿè´£æ•æ‰é¢œè‰²åå·®ç‰¹å¾ã€‚é€šé“èåˆå—åˆ™åŠ¨æ€ç»“åˆäº†è¿™äº›ç‹¬ç«‹ç‰¹å¾ä»¥å®ç°å…¨é¢å¢å¼ºã€‚æ­¤å¤–ï¼Œç½‘ç»œè¿˜åŒ…å«åŸºäºæ°´ä¸‹æˆåƒç‰©ç†æ¨¡å‹çš„è½¯é‡å»ºå±‚ï¼Œå¹¶è®¾è®¡äº†è‰²åº¦ä¸€è‡´æ€§æŸå¤±å’ŒSobelè‰²å½©æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒè‰²å½©ä¿çœŸåº¦å’Œå¢å¼ºç»“æ„ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWaterFormeråœ¨å¢å¼ºæ°´ä¸‹å›¾åƒæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹è§†è§‰æˆåƒé¢ä¸´ä½å¯¹æ¯”åº¦å’Œè‰²å½©å¤±çœŸé—®é¢˜ï¼Œå½±å“ä¸‹æ¸¸åˆ†æã€‚</li>
<li>ç°æœ‰æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•å¾€å¾€å°†é›¾æ°”å’Œè‰²å½©åç§»è§†ä¸ºç»Ÿä¸€é€€åŒ–è¿‡ç¨‹ï¼Œå¿½ç•¥äº†å®ƒä»¬çš„ç‹¬ç«‹æ€§å’Œç›¸äº’ä½œç”¨å…³ç³»ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ°´ä¸‹å›¾åƒå¢å¼ºç½‘ç»œï¼ˆWaterFormerï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>WaterFormeråŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šå»é›¾å—ã€è‰²å½©æ¢å¤å—å’Œé€šé“èåˆå—ï¼Œåˆ†åˆ«è´Ÿè´£æ•æ‰é›¾æ°”ç‰¹å¾ã€é¢œè‰²åå·®ç‰¹å¾ï¼Œå¹¶åŠ¨æ€ç»“åˆè¿™äº›ç‰¹å¾ä»¥å®ç°å¢å¼ºã€‚</li>
<li>ç½‘ç»œåŒ…å«åŸºäºæ°´ä¸‹æˆåƒç‰©ç†æ¨¡å‹çš„è½¯é‡å»ºå±‚ï¼Œç¡®ä¿å¢å¼ºçš„çœŸå®æ€§ã€‚</li>
<li>è®¾è®¡äº†è‰²åº¦ä¸€è‡´æ€§æŸå¤±å’ŒSobelè‰²å½©æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒè‰²å½©ä¿çœŸåº¦å’Œå¢å¼ºç»“æ„ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-048819f126a5c0c7b870bf01766fefc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-510c0d7d4b0fd6552bcfe0af3d9bf70b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f17d4345fd7bdfa62572c525f102b960.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb35c4fbe458a718ea9f99b99a06bfe8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MFCLIP-Multi-modal-Fine-grained-CLIP-for-Generalizable-Diffusion-Face-Forgery-Detection"><a href="#MFCLIP-Multi-modal-Fine-grained-CLIP-for-Generalizable-Diffusion-Face-Forgery-Detection" class="headerlink" title="MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face   Forgery Detection"></a>MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face   Forgery Detection</h2><p><strong>Authors:Yaning Zhang, Tianyi Wang, Zitong Yu, Zan Gao, Linlin Shen, Shengyong Chen</strong></p>
<p>The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. </p>
<blockquote>
<p>å¿«é€Ÿé€¼çœŸé¢éƒ¨ç”Ÿæˆæ–¹æ³•çš„å¼€å‘å¼•èµ·äº†ç¤¾ä¼šå’Œå­¦æœ¯ç•Œçš„å¹¿æ³›å…³æ³¨ï¼Œè¿™å‡¸æ˜¾äº†å¯¹ç¨³å¥ä¸”é€šç”¨çš„é¢éƒ¨ä¼ªé€ æ£€æµ‹ï¼ˆFFDï¼‰æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨å›¾åƒæ¨¡å¼æ•æ‰é¢éƒ¨ä¼ªé€ æ¨¡å¼ï¼Œä½†å…¶ä»–æ¨¡å¼ï¼ˆå¦‚ç»†å¾®å™ªå£°å’Œæ–‡æœ¬ï¼‰å°šæœªå¾—åˆ°å®Œå…¨æ¢ç´¢ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°FFDæ–¹æ³•å€¾å‘äºè¯†åˆ«ç”±GANç”Ÿæˆçš„é¢éƒ¨å›¾åƒï¼Œä½†éš¾ä»¥æ£€æµ‹æœªè§è¿‡çš„æ‰©æ•£åˆæˆå›¾åƒã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨æœ€å‰æ²¿çš„åŸºç¡€æ¨¡å‹â€”â€”å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ï¼Œå®ç°å¯é€šç”¨çš„æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ï¼ˆDFFDï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡å¼ç²¾ç»†CLIPï¼ˆMFCLIPï¼‰æ¨¡å‹ï¼Œå®ƒé€šè¿‡è¯­è¨€å¼•å¯¼çš„é¢éƒ¨ä¼ªé€ è¡¨ç¤ºå­¦ä¹ ï¼ŒæŒ–æ˜å›¾åƒå™ªå£°æ¨¡å¼ä¹‹é—´çš„å…¨é¢å’Œç²¾ç»†çš„ä¼ªé€ ç—•è¿¹ï¼Œä»¥ä¿ƒè¿›DFFDçš„å‘å±•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç²¾ç»†çš„è¯­è¨€ç¼–ç å™¨ï¼ˆFLEï¼‰ï¼Œå®ƒä»åˆ†å±‚çš„æ–‡æœ¬æç¤ºä¸­æå–ç²¾ç»†çš„å…¨å±€è¯­è¨€ç‰¹å¾ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæ¨¡å¼è§†è§‰ç¼–ç å™¨ï¼ˆMVEï¼‰ï¼Œä»¥æ•è·å…¨å±€å›¾åƒä¼ªé€ åµŒå…¥ä»¥åŠä»æœ€ä¸°å¯Œçš„è¡¥ä¸ä¸­æå–çš„ç²¾ç»†å™ªå£°ä¼ªé€ æ¨¡å¼ï¼Œå¹¶å°†å®ƒä»¬æ•´åˆèµ·æ¥æŒ–æ˜é€šç”¨çš„è§†è§‰ä¼ªé€ ç—•è¿¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ç§åˆ›æ–°çš„å¯æ’æ‹”å¼æ ·æœ¬é…å¯¹æ³¨æ„åŠ›ï¼ˆSPAï¼‰æ–¹æ³•ï¼Œä»¥å¼ºè°ƒç›¸å…³çš„è´Ÿæ ·æœ¬å¯¹å¹¶æŠ‘åˆ¶ä¸ç›¸å…³çš„æ ·æœ¬å¯¹ï¼Œä½¿è·¨æ¨¡æ€æ ·æœ¬å¯¹èƒ½å¤Ÿè¿›è¡Œæ›´çµæ´»çš„å¯¹é½ã€‚å¹¿æ³›çš„å®éªŒå’Œå¯è§†åŒ–ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒçš„è®¾ç½®ï¼ˆå¦‚è·¨ç”Ÿæˆå™¨ã€è·¨ä¼ªé€ å’Œè·¨æ•°æ®é›†è¯„ä¼°ï¼‰ä¸Šå‡è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09724v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥æ–¹æ³•æ—¨åœ¨è§£å†³ç¤¾ä¼šä¸å­¦æœ¯ç•Œå¯¹äºé€¼çœŸçš„é¢éƒ¨ç”ŸæˆæŠ€æœ¯å¿«é€Ÿå‘å±•æ‰€å¼•å‘çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯å¯¹é¢éƒ¨ä¼ªé€ æ£€æµ‹æŠ€æœ¯çš„è¿«åˆ‡éœ€æ±‚ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒæ¨¡æ€æ•æ‰é¢éƒ¨ä¼ªé€ æ¨¡å¼ï¼Œä½†å¿½ç•¥äº†å…¶ä»–æ¨¡æ€å¦‚ç²¾ç»†å™ªå£°å’Œæ–‡æœ¬çš„é‡è¦æ€§ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨å‰æ²¿çš„CLIPæ¨¡å‹ï¼Œå®ç°å¯æ³›åŒ–çš„æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ï¼ˆDFFDï¼‰ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€ç²¾ç»†CLIPï¼ˆMFCLIPï¼‰æ¨¡å‹ï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼çš„é¢ä¼ªé€ è¡¨ç¤ºå­¦ä¹ ï¼ŒæŒ–æ˜å›¾åƒå™ªå£°æ¨¡æ€çš„å…¨é¢ç²¾ç»†ä¼ªé€ ç—•è¿¹ã€‚è®¾è®¡ç²¾ç»†è¯­è¨€ç¼–ç å™¨ï¼ˆFLEï¼‰å’Œå¤šæ¨¡æ€è§†è§‰ç¼–ç å™¨ï¼ˆMVEï¼‰ï¼Œæ•´åˆå…¨å±€å›¾åƒä¼ªé€ åµŒå…¥å’Œä»ä¸°å¯ŒåŒºåŸŸæå–çš„ç²¾ç»†å™ªå£°ä¼ªé€ æ¨¡å¼ï¼ŒæŒ–æ˜é€šç”¨è§†è§‰ä¼ªé€ ç—•è¿¹ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†åˆ›æ–°çš„å³æ’å³ç”¨æ ·æœ¬é…å¯¹æ³¨æ„åŠ›ï¼ˆSPAï¼‰æ–¹æ³•ï¼Œå¼ºè°ƒç›¸å…³çš„è´Ÿæ ·æœ¬å¯¹å¹¶æŠ‘åˆ¶æ— å…³çš„æ ·æœ¬å¯¹ï¼Œå®ç°è·¨æ¨¡æ€æ ·æœ¬å¯¹æ›´çµæ´»çš„é…å¯¹ã€‚å®éªŒå’Œå¯è§†åŒ–æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒè®¾ç½®ä¸Šå‡è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰é¢éƒ¨ä¼ªé€ æ£€æµ‹æŠ€æœ¯ä¸»è¦å…³æ³¨å›¾åƒæ¨¡æ€ï¼Œå¿½è§†äº†å…¶ä»–å¦‚ç²¾ç»†å™ªå£°å’Œæ–‡æœ¬ç­‰æ¨¡æ€çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºåˆ©ç”¨CLIPæ¨¡å‹å®ç°å¯æ³›åŒ–çš„æ‰©æ•£é¢éƒ¨ä¼ªé€ æ£€æµ‹ï¼ˆDFFDï¼‰ã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€ç²¾ç»†CLIPï¼ˆMFCLIPï¼‰æ¨¡å‹ï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼å­¦ä¹ æŒ–æ˜å…¨é¢çš„ç²¾ç»†ä¼ªé€ ç—•è¿¹ã€‚</li>
<li>è®¾è®¡äº†ç²¾ç»†è¯­è¨€ç¼–ç å™¨ï¼ˆFLEï¼‰å’Œå¤šæ¨¡æ€è§†è§‰ç¼–ç å™¨ï¼ˆMVEï¼‰ï¼Œç”¨äºæå–å…¨å±€å’Œç²¾ç»†çš„ä¼ªé€ ç‰¹å¾ã€‚</li>
<li>åˆ›æ–°æå‡ºå³æ’å³ç”¨æ ·æœ¬é…å¯¹æ³¨æ„åŠ›ï¼ˆSPAï¼‰æ–¹æ³•ï¼Œå¼ºåŒ–ç›¸å…³æ ·æœ¬å¯¹çš„é…å¯¹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨è·¨ç”Ÿæˆå™¨ã€è·¨ä¼ªé€ å’Œè·¨æ•°æ®é›†è¯„ä¼°ç­‰ä¸åŒè®¾ç½®ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f1e9101a2350a2d8754d51298d99117.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8600bd37993aaec2a54c71284c4a2751.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a3fc6d63d8fa60b48020b0626eb817.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84e125868fb2f991cd22f88c6b4b529f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2dd6d1836834b963757968136cea426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbe7bf60be33c6b1d16b37603bf696a2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d7b7a8b9efd58ba54027b747401b9c90.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  3D Can Be Explored In 2D Pseudo-Label Generation for LiDAR Point Clouds   Using Sensor-Intensity-Based 2D Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30f67bacaae88a35a3b6af1b8c63a0c8.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  RAVU Retrieval Augmented Video Understanding with Compositional   Reasoning over Graph
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
