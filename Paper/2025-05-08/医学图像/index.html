<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Accelerating the development of oxynitride thin films A combinatorial   investigation of the Al-Si-O-N system">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1ee5ff0a96963288a072db879a6e7eee.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="Accelerating-the-development-of-oxynitride-thin-films-A-combinatorial-investigation-of-the-Al-Si-O-N-system"><a href="#Accelerating-the-development-of-oxynitride-thin-films-A-combinatorial-investigation-of-the-Al-Si-O-N-system" class="headerlink" title="Accelerating the development of oxynitride thin films: A combinatorial   investigation of the Al-Si-O-N system"></a>Accelerating the development of oxynitride thin films: A combinatorial   investigation of the Al-Si-O-N system</h2><p><strong>Authors:Stefanie Frick, Oleksandr Pshyk, Arnold MÃ¼ller, Alexander Wieczorek, Kerstin Thorwarth, Sebastian Siol</strong></p>
<p>Oxynitrides are used in a variety of applications including photocatalysts, high-k dielectrics or wear-resistant coatings and often show intriguing multi-functionality. To accelerate the co-optimization of the relevant material properties of these compositionally complex oxynitride systems, high-throughput synthesis and characterization methods are desirable. In the present work, three approaches were investigated to obtain orthogonal anion and cation gradients on the same substrate by magnetron sputtering. The different approaches included varying positions of the local reactive gas inlets and different combinations of target materials. The best performing approach was applied to screen a large two-dimensional area of the quaternary phase space within the Al-Si-O-N system. This material system is a promising candidate for transparent protective coatings with variable refractive indices. With only five depositions of combinatorial libraries, an anion composition range of 2-46% O&#x2F;(N+O) and a cation composition range of 4-44% Si&#x2F;(Al+Si) is covered. For lower oxygen and silicon contents, a region with hardness of up to 25 GPa is observed, where the material exhibits either wurtzite AlN or a composite microstructure. By increasing the deposition temperature to 400 {\deg}C, an extension of this region can be achieved. At higher oxygen and silicon contents, the structure of the samples is X-ray amorphous. In this structural region, an intimate correlation between hardness and refractive index is confirmed. The results of this study introduce a practical approach to perform high-throughput development of mixed anion materials, which is transferable to many materials systems and applications. </p>
<blockquote>
<p>æ°§æ°®åŒ–ç‰©è¢«å¹¿æ³›åº”ç”¨äºå…‰å‚¬åŒ–å‰‚ã€é«˜ä»‹ç”µå¸¸æ•°ä»‹è´¨æˆ–è€ç£¨æ¶‚å±‚ç­‰å¤šä¸ªé¢†åŸŸï¼Œå¹¶å±•ç°å‡ºä»¤äººç€è¿·çš„å¤šåŠŸèƒ½æ€§ã€‚ä¸ºäº†åŠ é€Ÿè¿™äº›æˆåˆ†å¤æ‚çš„æ°§æ°®åŒ–ç‰©ç³»ç»Ÿçš„ç›¸å…³ææ–™å±æ€§çš„ååŒä¼˜åŒ–ï¼Œéœ€è¦é‡‡ç”¨é«˜é€šé‡åˆæˆå’Œè¡¨å¾æ–¹æ³•ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸‰ç§æ–¹æ³•ï¼Œé€šè¿‡ç£æ§æº…å°„åœ¨åŒä¸€åŸºæ¿ä¸Šè·å¾—æ­£äº¤çš„é˜´ç¦»å­å’Œé˜³ç¦»å­æ¢¯åº¦ã€‚è¿™ä¸‰ç§æ–¹æ³•åŒ…æ‹¬æ”¹å˜å±€éƒ¨ååº”æ°”ä½“å…¥å£çš„ä½ç½®ä»¥åŠé¶ææ–™çš„ä¸åŒç»„åˆã€‚æœ€ä½³æ–¹æ³•è¢«åº”ç”¨äºåœ¨Al-Si-O-Nç³»ç»Ÿå†…ç­›é€‰ä¸€ä¸ªå¤§çš„äºŒç»´åŒºåŸŸã€‚è¯¥ææ–™ç³»ç»Ÿæ˜¯ä¸€ä¸ªæœ‰æœ›ç”¨äºå…·æœ‰å¯å˜æŠ˜å°„ç‡çš„é€æ˜ä¿æŠ¤æ¶‚å±‚çš„å€™é€‰ææ–™ã€‚é€šè¿‡ä»…æ²‰ç§¯äº”æ¬¡ç»„åˆåº“ï¼Œå°±è¦†ç›–äº†é˜´ç¦»å­ç»„æˆèŒƒå›´åœ¨2-46%çš„O&#x2F;(N+O)ï¼Œä»¥åŠé˜³ç¦»å­ç»„æˆèŒƒå›´åœ¨4-44%çš„Si&#x2F;(Al+Si)ã€‚å¯¹äºè¾ƒä½çš„æ°§å’Œç¡…å«é‡ï¼Œè§‚å¯Ÿåˆ°ç¡¬åº¦é«˜è¾¾25 GPaçš„åŒºåŸŸï¼Œè¯¥ææ–™å‘ˆç°å‡ºçº¤é”ŒçŸ¿ç»“æ„AlNæˆ–å¤åˆå¾®è§‚ç»“æ„ã€‚é€šè¿‡å°†æ²‰ç§¯æ¸©åº¦æé«˜åˆ°400Â°Cï¼Œå¯ä»¥å®ç°æ­¤åŒºåŸŸçš„æ‰©å±•ã€‚åœ¨è¾ƒé«˜çš„æ°§å’Œç¡…å«é‡ä¸‹ï¼Œæ ·å“çš„ç»“æ„ä¸ºXå°„çº¿éæ™¶æ€ã€‚åœ¨æ­¤ç»“æ„åŒºåŸŸä¸­ï¼Œè¯å®äº†ç¡¬åº¦ä¸æŠ˜å°„ç‡ä¹‹é—´çš„å¯†åˆ‡è”ç³»ã€‚æœ¬ç ”ç©¶çš„ç»“æœä»‹ç»äº†ä¸€ç§å®é™…å¯è¡Œçš„æ–¹æ³•æ¥æ‰§è¡Œæ··åˆé˜´ç¦»å­ææ–™çš„é«˜é€šé‡å¼€å‘ï¼Œè¿™ç§æ–¹æ³•å¯åº”ç”¨äºå¤šç§ææ–™ç³»ç»Ÿå’Œåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03635v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹æ°§æ°®åŒ–åˆç‰©åœ¨å¤šé¢†åŸŸåº”ç”¨ä¸­çš„ææ–™æ€§èƒ½ä¼˜åŒ–é—®é¢˜ï¼Œæ¢ç´¢äº†é«˜é€šé‡åˆæˆåŠè¡¨å¾æ–¹æ³•ã€‚ç ”ç©¶é‡‡ç”¨ç£æ§æº…å°„æŠ€æœ¯ï¼Œåœ¨åŒä¸€åŸºåº•ä¸Šå®ç°é˜´ç¦»å­å’Œé˜³ç¦»å­çš„æ­£äº¤æ¢¯åº¦ã€‚æœ€ä½³æ–¹æ¡ˆåœ¨Al-Si-O-Nç³»ç»Ÿå†…çš„å¤§èŒƒå›´äºŒç»´åŒºåŸŸè¿›è¡Œäº†ç­›é€‰ï¼Œè¯¥ç³»ç»Ÿåœ¨å¯å˜æŠ˜å°„ç‡çš„é€æ˜é˜²æŠ¤æ¶‚å±‚ä¸­å…·æœ‰åº”ç”¨å‰æ™¯ã€‚ä»…é€šè¿‡äº”æ¬¡æ²‰ç§¯ç»„åˆåº“ï¼Œå°±è¦†ç›–äº†æ°§é˜´ç¦»å­2%~46%åŠç¡…é˜³ç¦»å­4%~44%çš„èŒƒå›´ã€‚åœ¨è¾ƒä½æ°§å’Œç¡…å«é‡åŒºåŸŸï¼Œè§‚å¯Ÿåˆ°ç¡¬åº¦è¾¾25 GPaçš„ææ–™ï¼Œè¡¨ç°ä¸ºçº¤é”ŒçŸ¿ç»“æ„æ°®åŒ–é“æˆ–å¤åˆå¾®è§‚ç»“æ„ã€‚æé«˜æ²‰ç§¯æ¸©åº¦è‡³400Â°Cå¯æ‰©å¤§æ­¤åŒºåŸŸã€‚åœ¨è¾ƒé«˜æ°§å’Œç¡…å«é‡åŒºåŸŸï¼Œæ ·å“ç»“æ„å‘ˆXå°„çº¿éæ™¶æ€ï¼Œç¡¬åº¦ä¸æŠ˜å°„ç‡ä¹‹é—´å­˜åœ¨å¯†åˆ‡å…³è”ã€‚æœ¬ç ”ç©¶ä¸ºæ··åˆé˜´ç¦»å­ææ–™çš„é«˜é€šé‡å¼€å‘æä¾›äº†å®ç”¨æ–¹æ³•ï¼Œå¯å¹¿æ³›åº”ç”¨äºå…¶ä»–ææ–™ä½“ç³»å’Œåº”ç”¨é¢†åŸŸã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ°§æ°®åŒ–åˆç‰©åœ¨å¤šç§åº”ç”¨ä¸­è¡¨ç°å‡ºå¤šåŠŸèƒ½æ€§ï¼Œå¦‚å…‰å‚¬åŒ–å‰‚ã€é«˜ä»‹ç”µå¸¸æ•°ä»‹è´¨å’Œè€ç£¨æ¶‚å±‚ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç£æ§æº…å°„æŠ€æœ¯ï¼Œæ¢ç´¢äº†åœ¨åŒä¸€åŸºåº•ä¸Šå®ç°é˜´ç¦»å­å’Œé˜³ç¦»å­æ¢¯åº¦çš„ä¸‰ç§æ–¹æ³•ã€‚</li>
<li>æœ€ä½³æ€§èƒ½æ–¹æ³•ç”¨äºç­›é€‰Al-Si-O-Nç³»ç»Ÿå†…çš„å¹¿é˜”äºŒç»´åŒºåŸŸï¼Œè¯¥ç³»ç»Ÿå¯¹äºé€æ˜é˜²æŠ¤æ¶‚å±‚å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>é€šè¿‡æ²‰ç§¯ç»„åˆåº“ï¼Œå®ç°äº†æ°§é˜´ç¦»å­å’Œç¡…é˜³ç¦»å­çš„å¹¿æ³›ç»„æˆèŒƒå›´ã€‚</li>
<li>åœ¨ç‰¹å®šæˆåˆ†åŒºåŸŸï¼Œææ–™è¡¨ç°å‡ºé«˜è¾¾25 GPaçš„ç¡¬åº¦ï¼Œå¹¶è§‚å¯Ÿåˆ°çº¤é”ŒçŸ¿ç»“æ„æ°®åŒ–é“æˆ–å¤åˆå¾®è§‚ç»“æ„ã€‚</li>
<li>æé«˜æ²‰ç§¯æ¸©åº¦å¯ä»¥æ‰©å¤§æŸäº›æˆåˆ†åŒºåŸŸçš„ææ–™æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9266bc37f687a955af0d35b13434deca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-434bf3f71c17dad92e1f1b83f57a1bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3f80c164bd6db2a184f35f66a5e82aa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Crystal-structural-evolution-of-Ru-3-Sn-7-under-pressure-and-its-implication-on-possible-electronic-changes"><a href="#Crystal-structural-evolution-of-Ru-3-Sn-7-under-pressure-and-its-implication-on-possible-electronic-changes" class="headerlink" title="Crystal structural evolution of Ru$_3$Sn$_7$ under pressure and its   implication on possible electronic changes"></a>Crystal structural evolution of Ru$_3$Sn$_7$ under pressure and its   implication on possible electronic changes</h2><p><strong>Authors:K. A. Irshad, P. Anees, Hrudananda Jena, Boby Joseph</strong></p>
<p>Ru$_3$Sn$_7$, an intermetallic compound with advanced catalytic properties, exhibits a complex crystal structure and intriguing electronic properties, making it an attractive candidate for investigations under high-pressure (HP). The structural, vibrational and electronic band structure of this compound were investigated at HP up to ~ 20 GPa using synchrotron x-ray powder diffraction, micro-Raman, and density functional theory (DFT), respectively. Despite the local structural changes implied by a discernible reduction in the compressibility and distinct slope changes in the pressure evolution of the symmetric stretching vibrations of the Ru and Sn atoms around 8 GPa, the cubic structure is found to be stable throughout the pressure range. In support, our calculated phonon dispersion relation confirmed the stability of the cubic phase till the highest pressures. A comprehensive analysis of the Raman spectrum reveals the signatures of the pressure induced sudden strengthening of electron-phonon coupling as early as 3 GPa which is backed by a bounce in the phonon and electron density of states (DoS). Electronic structure calculations demonstrate that the metallic nature of Ru$_3$Sn$_7$ is preserved in the studied pressure range with a minor redistribution of electronic DoS across the Fermi level (EF). The band structure calculations predict intriguing changes in the electronic structure, revealing the pressure induced dp hybridization through the high symmetry point of the Brillouin zone which is largely responsible for the observed reduction in the compressibility and enhancement of the electron-phonon coupling in Ru$_3$Sn$_7$. </p>
<blockquote>
<p>Ru$_3$Sn$_7$æ˜¯ä¸€ç§å…·æœ‰å…ˆè¿›å‚¬åŒ–æ€§èƒ½çš„é‡‘å±é—´åŒ–åˆç‰©ï¼Œå…¶æ™¶ä½“ç»“æ„å¤æ‚ä¸”ç”µå­ç‰¹æ€§æœ‰è¶£ï¼Œä½¿å…¶æˆä¸ºé«˜å‹ï¼ˆHPï¼‰ç ”ç©¶ä¸­çš„ç†æƒ³å€™é€‰ç‰©ã€‚è¯¥åŒ–åˆç‰©çš„ç»“æ„ã€æŒ¯åŠ¨å’Œç”µå­å¸¦ç»“æ„åœ¨é«˜å‹ä¸‹è‡³çº¦20 GPaä½¿ç”¨åŒæ­¥åŠ é€Ÿå™¨Xå°„çº¿ç²‰æœ«è¡å°„ã€å¾®æ‹‰æ›¼å…‰è°±å’Œå¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è¿›è¡Œäº†ç ”ç©¶ã€‚å°½ç®¡åœ¨å‹åŠ›æ¼”åŒ–ä¸­è§‚å¯Ÿåˆ°å¯¹ç§°ä¼¸ç¼©æŒ¯åŠ¨çš„å‹ç¼©æ€§æ˜æ˜¾é™ä½å’Œæ–œç‡æ˜æ˜¾å˜åŒ–ï¼Œä¸”å±€éƒ¨ç»“æ„å‘ç”Ÿå˜åŒ–ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§çº¦8 GPaæ—¶Ruå’ŒSnåŸå­çš„å‘¨å›´ï¼Œä½†ç«‹æ–¹ç»“æ„åœ¨æ•´ä¸ªå‹åŠ›èŒƒå›´å†…è¢«è¯æ˜æ˜¯ç¨³å®šçš„ã€‚æˆ‘ä»¬çš„è®¡ç®—å£°å­è‰²æ•£å…³ç³»æ”¯æŒäº†è¿™ä¸€å‘ç°ï¼Œè¯å®äº†ç«‹æ–¹ç›¸åœ¨æœ€é«˜å‹åŠ›ä¸‹ä¹Ÿæ˜¯ç¨³å®šçš„ã€‚å¯¹æ‹‰æ›¼å…‰è°±çš„ç»¼åˆåˆ†ææ­ç¤ºäº†å‹åŠ›è¯±å¯¼çš„ç”µå­-å£°å­è€¦åˆçªç„¶å¢å¼ºçš„è¿¹è±¡ï¼Œæ—©åœ¨3 GPaæ—¶å°±å¼€å§‹äº†ï¼Œè¿™å¾—åˆ°äº†å£°å­å’Œç”µå­æ€å¯†åº¦ï¼ˆDoSï¼‰è·³è·ƒçš„æ”¯æŒã€‚ç”µå­ç»“æ„è®¡ç®—è¡¨æ˜ï¼Œåœ¨ç ”ç©¶çš„å‹åŠ›èŒƒå›´å†…ä¿æŒäº†Ru$_3$Sn$_7$çš„é‡‘å±æ€§è´¨ï¼Œè´¹ç±³èƒ½çº§ï¼ˆEFï¼‰é™„è¿‘çš„ç”µå­æ€å¯†åº¦ç•¥æœ‰é‡æ–°åˆ†å¸ƒã€‚å¸¦ç»“æ„è®¡ç®—é¢„æµ‹ç”µå­ç»“æ„ä¼šå‘ç”Ÿæœ‰è¶£çš„å˜åŒ–ï¼Œæ˜¾ç¤ºå‡ºå‹åŠ›è¯±å¯¼çš„dpæ‚åŒ–é€šè¿‡å¸ƒé‡Œæ¸ŠåŒºçš„é«˜å¯¹ç§°ç‚¹ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¯¼è‡´è§‚å¯Ÿåˆ°çš„å‹ç¼©æ€§é™ä½å’Œç”µå­-å£°å­è€¦åˆå¢å¼ºåœ¨Ru$_3$Sn$_7$ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03571v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Ru$_3$Sn$_7$åœ¨é«˜å‹ä¸‹çš„æ€§è´¨ç ”ç©¶ã€‚é€šè¿‡åŒæ­¥è¾å°„Xå°„çº¿ç²‰æœ«è¡å°„ã€å¾®æ‹‰æ›¼å…‰è°±å’Œå¯†åº¦æ³›å‡½ç†è®ºç­‰æ–¹æ³•ï¼Œå‘ç°å…¶åœ¨é«˜å‹ä¸‹çš„æ™¶ä½“ç»“æ„ã€æŒ¯åŠ¨å’Œç”µå­ç»“æ„å‘ç”Ÿå˜åŒ–ï¼Œä½†ç«‹æ–¹ç»“æ„åœ¨æµ‹è¯•å‹åŠ›èŒƒå›´å†…ä¿æŒç¨³å®šã€‚å‹åŠ›è¯±å¯¼ä¸‹ï¼Œç”µå­-å£°å­è€¦åˆä½œç”¨å¢å¼ºï¼Œç”µå­ç»“æ„å‘ç”Ÿæœ‰è¶£å˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ru$_3$Sn$_7$æ˜¯ä¸€ç§å…·æœ‰å…ˆè¿›å‚¬åŒ–æ€§èƒ½çš„é‡‘å±é—´åŒ–åˆç‰©ï¼Œå…¶æ™¶ä½“ç»“æ„å¤æ‚ä¸”ç”µå­ç‰¹æ€§å¼•äººå…¥èƒœã€‚</li>
<li>åœ¨é«˜å‹ä¸‹ï¼ˆHPï¼‰å¯¹å…¶ç»“æ„ã€æŒ¯åŠ¨å’Œç”µå­ç»“æ„è¿›è¡Œäº†ç ”ç©¶ï¼Œå‹åŠ›èŒƒå›´é«˜è¾¾çº¦20 GPaã€‚</li>
<li>å°½ç®¡åœ¨å¤§çº¦8 GPaçš„å‹åŠ›ä¸‹ï¼ŒRuå’ŒSnåŸå­çš„å¯¹ç§°ä¼¸ç¼©æŒ¯åŠ¨çš„å‹ç¼©æ€§å’Œæ–œç‡å‘ç”Ÿå˜åŒ–ï¼Œä½†ç«‹æ–¹ç»“æ„åœ¨æ•´ä¸ªå‹åŠ›èŒƒå›´å†…ä¿æŒç¨³å®šã€‚</li>
<li>æ‹‰æ›¼å…‰è°±åˆ†ææ˜¾ç¤ºï¼Œå‹åŠ›è¯±å¯¼çš„ç”µå­-å£°å­è€¦åˆä½œç”¨åœ¨3 GPaæ—¶çªç„¶å¢å¼ºã€‚</li>
<li>ç”µå­ç»“æ„è®¡ç®—è¡¨æ˜ï¼ŒRu$_3$Sn$_7$çš„é‡‘å±æ€§è´¨åœ¨ç ”ç©¶çš„å‹åŠ›èŒƒå›´å†…å¾—ä»¥ä¿æŒï¼Œè´¹ç±³èƒ½çº§é™„è¿‘çš„ç”µå­æ€å¯†åº¦æœ‰æ‰€å˜åŒ–ã€‚</li>
<li>å¸¦ç»“æ„è®¡ç®—é¢„æµ‹äº†ç”µå­ç»“æ„çš„æœ‰è¶£å˜åŒ–ï¼ŒåŒ…æ‹¬å‹åŠ›å¼•èµ·çš„dpæ‚åŒ–ï¼Œè¿™ä¸»è¦å¯¼è‡´äº†è§‚å¯Ÿåˆ°çš„å‹ç¼©æ€§é™ä½å’Œç”µå­-å£°å­è€¦åˆçš„å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdc79556f2b4779191e8a8bb94f81430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77e7eaccd822c5126abd75c05d40876c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Artificial-Protozoa-Optimizer-APO-A-novel-bio-inspired-metaheuristic-algorithm-for-engineering-optimization"><a href="#Artificial-Protozoa-Optimizer-APO-A-novel-bio-inspired-metaheuristic-algorithm-for-engineering-optimization" class="headerlink" title="Artificial Protozoa Optimizer (APO): A novel bio-inspired metaheuristic   algorithm for engineering optimization"></a>Artificial Protozoa Optimizer (APO): A novel bio-inspired metaheuristic   algorithm for engineering optimization</h2><p><strong>Authors:Xiaopeng Wang, Vaclav Snasel, Seyedali Mirjalili, Jeng-Shyang Pan, Lingping Kong, Hisham A. Shehadeh</strong></p>
<p>This study proposes a novel artificial protozoa optimizer (APO) that is inspired by protozoa in nature. The APO mimics the survival mechanisms of protozoa by simulating their foraging, dormancy, and reproductive behaviors. The APO was mathematically modeled and implemented to perform the optimization processes of metaheuristic algorithms. The performance of the APO was verified via experimental simulations and compared with 32 state-of-the-art algorithms. Wilcoxon signed-rank test was performed for pairwise comparisons of the proposed APO with the state-of-the-art algorithms, and Friedman test was used for multiple comparisons. First, the APO was tested using 12 functions of the 2022 IEEE Congress on Evolutionary Computation benchmark. Considering practicality, the proposed APO was used to solve five popular engineering design problems in a continuous space with constraints. Moreover, the APO was applied to solve a multilevel image segmentation task in a discrete space with constraints. The experiments confirmed that the APO could provide highly competitive results for optimization problems. The source codes of Artificial Protozoa Optimizer are publicly available at <a target="_blank" rel="noopener" href="https://seyedalimirjalili.com/projects">https://seyedalimirjalili.com/projects</a> and <a target="_blank" rel="noopener" href="https://ww2.mathworks.cn/matlabcentral/fileexchange/162656-artificial-protozoa-optimizer">https://ww2.mathworks.cn/matlabcentral/fileexchange/162656-artificial-protozoa-optimizer</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å—è‡ªç„¶ç•ŒåŸç”ŸåŠ¨ç‰©å¯å‘çš„æ–°å‹äººå·¥åŸç”ŸåŠ¨ç‰©ä¼˜åŒ–å™¨ï¼ˆAPOï¼‰ã€‚APOé€šè¿‡æ¨¡æ‹ŸåŸç”ŸåŠ¨ç‰©çš„è§…é£Ÿã€ä¼‘çœ å’Œç¹æ®–è¡Œä¸ºæ¥æ¨¡ä»¿å…¶ç”Ÿå­˜æœºåˆ¶ã€‚APOåœ¨æ•°å­¦ä¸Šè¢«å»ºæ¨¡å’Œå®ç°ï¼Œä»¥æ‰§è¡Œå…ƒå¯å‘å¼ç®—æ³•çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚é€šè¿‡ä»¿çœŸå®éªŒéªŒè¯äº†APOçš„æ€§èƒ½ï¼Œå¹¶ä¸32ç§æœ€æ–°ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚é‡‡ç”¨Wilcoxonç¬¦å·ç§©æ£€éªŒå¯¹æå‡º APOä¸æœ€æ–°ç®—æ³•è¿›è¡Œé…å¯¹æ¯”è¾ƒï¼Œå¹¶é‡‡ç”¨Friedmanæ£€éªŒè¿›è¡Œå¤šé‡æ¯”è¾ƒã€‚é¦–å…ˆï¼Œä½¿ç”¨2022å¹´IEEEè¿›åŒ–è®¡ç®—å¤§ä¼šåŸºå‡†æµ‹è¯•çš„12ä¸ªå‡½æ•°å¯¹APOè¿›è¡Œäº†æµ‹è¯•ã€‚è€ƒè™‘åˆ°å®ç”¨æ€§ï¼Œæ‰€æå‡ºçš„APOè¢«ç”¨äºè§£å†³è¿ç»­ç©ºé—´ä¸­å…·æœ‰çº¦æŸçš„äº”ä¸ªæµè¡Œçš„å·¥ç¨‹è®¾è®¡é—®é¢˜ã€‚æ­¤å¤–ï¼ŒAPOè¿˜è¢«åº”ç”¨äºè§£å†³ç¦»æ•£ç©ºé—´ä¸­å…·æœ‰çº¦æŸçš„å¤šçº§å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚å®éªŒè¯å®ï¼ŒAPOåœ¨ä¼˜åŒ–é—®é¢˜å¤„ç†æ–¹é¢èƒ½æä¾›æå…·ç«äº‰åŠ›çš„ç»“æœã€‚äººå·¥åŸç”ŸåŠ¨ç‰©ä¼˜åŒ–å™¨çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://seyedalimirjalili.com/projects%E5%92%8Chttps://ww2.mathworks.cn/matlabcentral/fileexchange/162656-%E4%BA%BA%E5%B7%A5%E5%8E%9F%E7%94%9F%E5%8A%B3%E5%AE%9A%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%82">https://seyedalimirjalili.com/projectså’Œhttps://ww2.mathworks.cn/matlabcentral/fileexchange/162656-äººå·¥åŸç”ŸåŠ¨ç‰©ä¼˜åŒ–å™¨å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03512v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶ç•ŒåŸç”ŸåŠ¨ç‰©ç”Ÿå­˜ç­–ç•¥å¯å‘çš„æ–°å‹äººå·¥åŸç”ŸåŠ¨ç‰©ä¼˜åŒ–å™¨ï¼ˆAPOï¼‰è¢«æå‡ºã€‚APOæ¨¡æ‹ŸåŸç”ŸåŠ¨ç‰©çš„è§…é£Ÿã€ä¼‘çœ å’Œç¹æ®–è¡Œä¸ºï¼Œé€šè¿‡æ•°å­¦å»ºæ¨¡å®ç°å…ƒå¯å‘å¼ç®—æ³•çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚ç»å®éªŒä»¿çœŸéªŒè¯ï¼ŒAPOä¸32ç§æœ€æ–°ç®—æ³•ç›¸æ¯”è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªå·¥ç¨‹è®¾è®¡å’Œå›¾åƒåˆ†å‰²é—®é¢˜ä¸­å±•ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APOåŸºäºè‡ªç„¶åŸç”ŸåŠ¨ç‰©çš„ç”Ÿå­˜ç­–ç•¥è®¾è®¡ï¼Œæ¨¡æ‹Ÿå…¶è§…é£Ÿã€ä¼‘çœ å’Œç¹æ®–è¡Œä¸ºã€‚</li>
<li>APOé€šè¿‡æ•°å­¦å»ºæ¨¡å®ç°å…ƒå¯å‘å¼ç®—æ³•çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚</li>
<li>ç»å®éªŒä»¿çœŸéªŒè¯ï¼ŒAPOåœ¨è§£å†³ä¼˜åŒ–é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>APOåœ¨è§£å†³å¤šä¸ªå·¥ç¨‹è®¾è®¡å’Œå›¾åƒåˆ†å‰²é—®é¢˜ä¸­å±•ç°ç«äº‰åŠ›ã€‚</li>
<li>APOèƒ½å¤Ÿå¤„ç†ä¸åŒç©ºé—´ç±»å‹çš„çº¦æŸé—®é¢˜ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›çš„ç®—æ³•ç›¸æ¯”ï¼ŒAPOåœ¨æ€§èƒ½å’Œç«äº‰åŠ›æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b3ed4057dbc548532c001d89a0bf9ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2cd291d3364339cf6fda4e439770b4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85acb18ef2f13cb0e29d2a9d0bed0f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91b8be697f9d78aef68b1a1622cd74fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2de2b9a60ae1657347013f83fd8e99d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MRI-motion-correction-via-efficient-residual-guided-denoising-diffusion-probabilistic-models"><a href="#MRI-motion-correction-via-efficient-residual-guided-denoising-diffusion-probabilistic-models" class="headerlink" title="MRI motion correction via efficient residual-guided denoising diffusion   probabilistic models"></a>MRI motion correction via efficient residual-guided denoising diffusion   probabilistic models</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Qiang Li, Zach Eidex, Richard L. J. Qiu, Chih-Wei Chang, Hui Mao, Xiaofeng Yang</strong></p>
<p>Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly degrade image quality and impair quantitative analysis. Conventional mitigation strategies, such as repeated acquisitions or motion tracking, are costly and workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising diffusion probabilistic model tailored for MRI motion artifact correction. Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in the forward diffusion process, aligning the noise distribution with motion-corrupted data and enabling an efficient four-step reverse diffusion. A U-net backbone enhanced with Swin-Transformer blocks conventional attention layers, improving adaptability across resolutions. Training employs a combined l1+l2 loss, which promotes image sharpness and reduces pixel-level errors. Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic motion simulation framework and on an in-vivo dataset. Comparative analyses were conducted against established methods, including CycleGAN, Pix2pix, and MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and normalized mean squared error (NMSE). Results: The proposed method demonstrated superior performance in removing motion artifacts across all motion severity levels. Res-MoCoDiff consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches. </p>
<blockquote>
<p>ç›®çš„ï¼šç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ä¼ªå½±ä¼šæ˜¾è‘—é™ä½å›¾åƒè´¨é‡å¹¶å½±å“å®šé‡åˆ†æã€‚ä¼ ç»Ÿçš„ç¼“è§£ç­–ç•¥ï¼Œå¦‚é‡å¤é‡‡é›†æˆ–è¿åŠ¨è·Ÿè¸ªï¼Œæˆæœ¬é«˜æ˜‚ä¸”å·¥ä½œæµç¨‹å¤æ‚ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†Res-MoCoDiffï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹MRIè¿åŠ¨ä¼ªå½±æ ¡æ­£çš„é«˜æ•ˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚æ–¹æ³•ï¼šRes-MoCoDiffåœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­èå…¥äº†ä¸€ç§æ–°é¢–çš„æ®‹å·®è¯¯å·®è½¬ç§»æœºåˆ¶ï¼Œä½¿å™ªå£°åˆ†å¸ƒä¸è¿åŠ¨å¤±çœŸæ•°æ®ç›¸åŒ¹é…ï¼Œå¹¶å®ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„å››æ­¥åå‘æ‰©æ•£ã€‚ä½¿ç”¨U-netä¸»å¹²å¹¶ç»“åˆSwin-Transformerå—æ›¿ä»£ä¼ ç»Ÿæ³¨æ„åŠ›å±‚ï¼Œæé«˜äº†è·¨åˆ†è¾¨ç‡çš„é€‚åº”æ€§ã€‚è®­ç»ƒé‡‡ç”¨l1+l2æŸå¤±ç»„åˆï¼Œè¿™ä¿ƒè¿›äº†å›¾åƒæ¸…æ™°åº¦å¹¶å‡å°‘äº†åƒç´ çº§è¯¯å·®ã€‚Res-MoCoDiffåœ¨åˆ©ç”¨ç°å®è¿åŠ¨æ¨¡æ‹Ÿæ¡†æ¶ç”Ÿæˆçš„ç»¼åˆæ•°æ®é›†å’Œä½“å†…æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ä¸åŒ…æ‹¬CycleGANã€Pix2pixå’ŒMT-DDPMç­‰æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒæ€§åˆ†æï¼Œé‡‡ç”¨äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’Œå½’ä¸€åŒ–å‡æ–¹è¯¯å·®ï¼ˆNMSEï¼‰ç­‰å®šé‡æŒ‡æ ‡ã€‚ç»“æœï¼šæ‰€ææ–¹æ³•åœ¨æ‰€æœ‰è¿åŠ¨ä¸¥é‡ç¨‹åº¦çº§åˆ«ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„å»è¿åŠ¨ä¼ªå½±æ€§èƒ½ã€‚Res-MoCoDiffæŒç»­è·å¾—æœ€é«˜çš„SSIMå’Œæœ€ä½çš„NMSEå€¼ï¼Œå¯¹äºè½»å¾®å¤±çœŸï¼ŒPSNRé«˜è¾¾41.91Â±2.94 dBã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¹³å‡é‡‡æ ·æ—¶é—´å‡å°‘åˆ°æ¯æ‰¹ä¸¤ä¸ªå›¾åƒåˆ‡ç‰‡0.37ç§’ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•éœ€è¦101.74ç§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03498v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿åŠ¨ä¼ªå½±æ ¡æ­£çš„é«˜æ•ˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹â€”â€”Res-MoCoDiffã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥æ–°å‹æ®‹å·®è¯¯å·®è½¬ç§»æœºåˆ¶ï¼Œä¸è¿åŠ¨å¤±çœŸæ•°æ®å¯¹é½å™ªå£°åˆ†å¸ƒï¼Œå¹¶å®ç°å››æ­¥åå‘æ‰©æ•£ï¼Œæé«˜å›¾åƒè´¨é‡å’Œå®šé‡åˆ†ææ•ˆç‡ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRes-MoCoDiffåœ¨å»é™¤è¿åŠ¨ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡é‡‡æ ·æ—¶é—´å¤§å¹…å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Res-MoCoDiffæ˜¯ä¸€ç§é’ˆå¯¹MRIè¿åŠ¨ä¼ªå½±æ ¡æ­£çš„å…ˆè¿›å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ä¸€ç§æ–°å‹çš„æ®‹å·®è¯¯å·®è½¬ç§»æœºåˆ¶ï¼Œæœ‰æ•ˆå¯¹é½å™ªå£°åˆ†å¸ƒä¸è¿åŠ¨å¤±çœŸæ•°æ®ã€‚</li>
<li>Res-MoCoDiffå®ç°äº†é«˜æ•ˆå››æ­¥åå‘æ‰©æ•£ï¼Œæé«˜äº†å›¾åƒè´¨é‡å’Œå®šé‡åˆ†ææ•ˆç‡ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRes-MoCoDiffæ€§èƒ½å“è¶Šï¼Œé€‚ç”¨äºä¸åŒè¿åŠ¨ä¸¥é‡ç¨‹åº¦çº§åˆ«çš„ä¼ªå½±å»é™¤ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒRes-MoCoDiffçš„é‡‡æ ·æ—¶é—´å¤§å¹…å‡å°‘ï¼Œæé«˜äº†å·¥ä½œæ•ˆç‡ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†ç»“åˆl1+l2æŸå¤±çš„è®­ç»ƒæ–¹æ³•ï¼Œä¿ƒè¿›äº†å›¾åƒæ¸…æ™°åº¦çš„æå‡å¹¶å‡å°‘äº†åƒç´ çº§è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ef7a2e75981272e7ed933324b217c2e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-440fd2b6bdc84fc3b1a1c966827b73ea.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UPMAD-Net-A-Brain-Tumor-Segmentation-Network-with-Uncertainty-Guidance-and-Adaptive-Multimodal-Feature-Fusion"><a href="#UPMAD-Net-A-Brain-Tumor-Segmentation-Network-with-Uncertainty-Guidance-and-Adaptive-Multimodal-Feature-Fusion" class="headerlink" title="UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance   and Adaptive Multimodal Feature Fusion"></a>UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance   and Adaptive Multimodal Feature Fusion</h2><p><strong>Authors:Zhanyuan Jia, Ni Yao, Danyang Sun, Chuang Han, Yanting Li, Jiaofen Nan, Fubao Zhu, Chen Zhao, Weihua Zhou</strong></p>
<p>Background: Brain tumor segmentation has a significant impact on the diagnosis and treatment of brain tumors. Accurate brain tumor segmentation remains challenging due to their irregular shapes, vague boundaries, and high variability. Objective: We propose a brain tumor segmentation method that combines deep learning with prior knowledge derived from a region-growing algorithm. Methods: The proposed method utilizes a multi-scale feature fusion (MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale features and capture global contextual information. To enhance the modelâ€™s robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout) strategy is employed for uncertainty estimation. Results: Extensive experiments demonstrate that the proposed method achieves superior performance on Brain Tumor Segmentation (BraTS) datasets, significantly outperforming various state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are 89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT) segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019 validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for ET, WT, and TC segmentation, respectively. Ablation studies further confirmed the contribution of each module to segmentation accuracy, indicating that each component played a vital role in overall performance improvement. Conclusion: This study proposed a novel 3D brain tumor segmentation network based on the U-Net architecture. By incorporating the prior knowledge and employing the uncertainty estimation method, the robustness and performance were improved. The code for the proposed method is available at <a target="_blank" rel="noopener" href="https://github.com/chenzhao2023/UPMAD_Net_BrainSeg">https://github.com/chenzhao2023/UPMAD_Net_BrainSeg</a>. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šè„‘è‚¿ç˜¤åˆ†å‰²å¯¹è„‘è‚¿ç˜¤çš„è¯Šæ–­å’Œæ²»ç–—å…·æœ‰é‡è¦å½±å“ã€‚ç”±äºè„‘è‚¿ç˜¤çš„å½¢æ€ä¸è§„åˆ™ã€è¾¹ç•Œæ¨¡ç³Šä»¥åŠé«˜åº¦å¼‚è´¨æ€§ï¼Œå‡†ç¡®çš„è„‘è‚¿ç˜¤åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
</blockquote>
<p>ç›®æ ‡ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ ä¸åŒºåŸŸå¢é•¿ç®—æ³•æ‰€æ´¾ç”Ÿå…ˆéªŒçŸ¥è¯†çš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ã€‚</p>
<p>æ–¹æ³•ï¼šæ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆï¼ˆMSFFï¼‰æ¨¡å—å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAAMï¼‰æ¥æå–å¤šå°ºåº¦ç‰¹å¾å’Œæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†æé«˜æ¨¡å‹åœ¨ä½ç½®ä¿¡åº¦åŒºåŸŸçš„ç¨³å¥æ€§ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›Dropoutï¼ˆMC Dropoutï¼‰ç­–ç•¥è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
<p>ç»“æœï¼šå¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—ä¼˜äºå„ç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨BraTS2021æ•°æ®é›†ä¸Šï¼Œå¢å¼ºè‚¿ç˜¤ï¼ˆETï¼‰åˆ†å‰²çš„Diceå¾—åˆ†ä¸º89.18%ï¼Œæ•´ä¸ªè‚¿ç˜¤ï¼ˆWTï¼‰åˆ†å‰²çš„Diceå¾—åˆ†ä¸º93.67%ï¼Œè‚¿ç˜¤æ ¸å¿ƒï¼ˆTCï¼‰åˆ†å‰²çš„Diceå¾—åˆ†ä¸º91.23%ã€‚åœ¨BraTS2019éªŒè¯é›†ä¸Šï¼ŒETã€WTå’ŒTCåˆ†å‰²çš„éªŒè¯Diceå¾—åˆ†åˆ†åˆ«ä¸º87.43%ã€90.92%å’Œ90.40%ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ¯ä¸ªæ¨¡å—å¯¹åˆ†å‰²ç²¾åº¦çš„è´¡çŒ®ï¼Œè¡¨æ˜æ¯ä¸ªç»„ä»¶åœ¨æ€»ä½“æ€§èƒ½æå‡ä¸­éƒ½èµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03494v1">PDF</a> 21 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ ä¸åŒºåŸŸå¢é•¿ç®—æ³•å…ˆéªŒçŸ¥è¯†çš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶æ¥æå–å¤šå°ºåº¦ç‰¹å¾å¹¶æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åŒæ—¶ï¼Œé‡‡ç”¨è’™ç‰¹å¡æ´›Dropoutç­–ç•¥è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä½ç½®ä¿¡åŒºåŸŸçš„ç¨³å¥æ€§ã€‚åœ¨Brain Tumor Segmentationï¼ˆBraTSï¼‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½å“è¶Šï¼Œæ˜¾è‘—ä¼˜äºå¤šç§æœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤åˆ†å‰²åœ¨è¯Šæ–­ä¸æ²»ç–—ä¸­æœ‰é‡è¦å½±å“ï¼Œä½†ç”±äºè‚¿ç˜¤å½¢çŠ¶ä¸è§„åˆ™ã€è¾¹ç•Œæ¨¡ç³Šå’Œé«˜å˜å¼‚æ€§ï¼Œå‡†ç¡®åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ ä¸åŒºåŸŸå¢é•¿ç®—æ³•å…ˆéªŒçŸ¥è¯†çš„è„‘è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å—å’Œè‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶æ¥æå–å¤šå°ºåº¦ç‰¹å¾å’Œæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨è’™ç‰¹å¡æ´›Dropoutç­–ç•¥è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæé«˜æ¨¡å‹åœ¨ä½ç½®ä¿¡åŒºåŸŸçš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨BraTSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨BraTS2021å’ŒBraTS2019æ•°æ®é›†ä¸Šã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªæ¨¡å—å¯¹åˆ†å‰²ç²¾åº¦çš„è´¡çŒ®ï¼Œè¡¨æ˜æ¯ä¸ªç»„ä»¶åœ¨æ•´ä½“æ€§èƒ½æå‡ä¸­éƒ½èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b55796377ac2d479adde8f136544dcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d2b84dcd2855b0575e45af7461774b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35d96f8775b9cbd44ad9177426d3e415.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Phenotype-Guided-Generative-Model-for-High-Fidelity-Cardiac-MRI-Synthesis-Advancing-Pretraining-and-Clinical-Applications"><a href="#Phenotype-Guided-Generative-Model-for-High-Fidelity-Cardiac-MRI-Synthesis-Advancing-Pretraining-and-Clinical-Applications" class="headerlink" title="Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI   Synthesis: Advancing Pretraining and Clinical Applications"></a>Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI   Synthesis: Advancing Pretraining and Clinical Applications</h2><p><strong>Authors:Ziyu Li, Yujian Hu, Zhengyao Ding, Yiheng Mao, Haitao Li, Fan Yi, Hongkun Zhang, Zhengxing Huang</strong></p>
<p>Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. However, the limited availability of large-scale, high-quality CMR datasets poses a major challenge to the effective application of artificial intelligence (AI) in this domain. Even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of AI models on downstream tasks. In this study, we present Cardiac Phenotype-Guided CMR Generation (CPGG), a novel approach for generating diverse CMR data that covers a wide spectrum of cardiac health status. The CPGG framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from CMR data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity CMR cine sequences that capture both structural and functional features of the heart in a fine-grained manner. We synthesized a massive amount of CMR to expand the pretraining data. Experimental results show that CPGG generates high-quality synthetic CMR data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. These gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. Code is availabel at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CPGG">https://anonymous.4open.science/r/CPGG</a>. </p>
<blockquote>
<p>å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒæ˜¯ä¸€ç§éå¸¸é‡è¦çš„æ— åˆ›å·¥å…·ï¼Œå¯ç”¨äºè¯Šæ–­å¿ƒè„ç—…å¹¶è¯„ä¼°å¿ƒè„å¥åº·ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡é«˜è´¨é‡CMRæ•°æ®é›†çš„æœ‰é™å¯ç”¨æ€§ç»™äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ­¤é¢†åŸŸçš„æœ‰æ•ˆåº”ç”¨å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å³ä½¿æ˜¯æ— æ ‡ç­¾æ•°æ®åŠå…¶æ‰€æ¶µç›–çš„å¥åº·çŠ¶å†µä¹Ÿéš¾ä»¥æ»¡è¶³æ¨¡å‹é¢„è®­ç»ƒçš„éœ€æ±‚ï¼Œè¿™é˜»ç¢äº†AIæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¿ƒè„è¡¨å‹å¼•å¯¼CMRç”Ÿæˆï¼ˆCPGGï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆå¤šæ ·åŒ–CMRæ•°æ®çš„æ–°æ–¹æ³•ï¼Œæ¶µç›–äº†å¹¿æ³›çš„å¿ƒè„å¥åº·çŠ¶å†µã€‚CPGGæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨ä»CMRæ•°æ®ä¸­å¾—å‡ºçš„å¿ƒè„è¡¨å‹æ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒåŸºäºè¿™äº›è¡¨å‹ï¼Œä½¿ç”¨å¸¦æ©ç çš„è‡ªåŠ¨å›å½’æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸCMRç”µå½±åºåˆ—ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼æ•æ‰å¿ƒè„çš„ç»“æ„å’ŒåŠŸèƒ½ç‰¹å¾ã€‚æˆ‘ä»¬åˆæˆäº†å¤§é‡CMRæ•°æ®ä»¥æ‰©å……é¢„è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCPGGç”Ÿæˆçš„é«˜è´¨é‡åˆæˆCMRæ•°æ®æ˜¾è‘—æé«˜äº†å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¯Šæ–­å’Œå¿ƒè„è¡¨å‹é¢„æµ‹ï¼‰çš„æ€§èƒ½ã€‚è¿™äº›æ”¶ç›Šåœ¨å…¬å…±å’Œç§æœ‰æ•°æ®é›†ä¸Šéƒ½å¾—åˆ°äº†è¯æ˜ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CPGG%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/CPGGè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03426v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¿ƒè„ç£å…±æŒ¯ï¼ˆCMRï¼‰æˆåƒåœ¨è¯Šæ–­å¿ƒè„ç—…å’Œè¯„ä¼°å¿ƒè„å¥åº·æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡é«˜è´¨é‡CMRæ•°æ®é›†æœ‰é™ï¼Œç»™äººå·¥æ™ºèƒ½åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹æ–¹æ³•â€”â€”å¿ƒè„è¡¨å‹å¼•å¯¼CMRç”Ÿæˆï¼ˆCPGGï¼‰ï¼Œæ—¨åœ¨ç”Ÿæˆè¦†ç›–å¹¿æ³›å¿ƒè„å¥åº·çŠ¶æ€çš„å¤šæ ·åŒ–CMRæ•°æ®ã€‚CPGGæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ¥æºäºCMRæ•°æ®çš„å¿ƒè„è¡¨å‹è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨æ¡ä»¶åŒ–çš„é®ç½©è‡ªå›å½’æ‰©æ•£æ¨¡å‹ç”Ÿæˆç²¾ç»†æ•æ‰å¿ƒè„ç»“æ„å’ŒåŠŸèƒ½ç‰¹å¾çš„é«˜ä¿çœŸåº¦CMRç”µå½±åºåˆ—ã€‚åˆæˆå¤§é‡CMRæ•°æ®ä»¥æ‰©å……é¢„è®­ç»ƒæ•°æ®ï¼Œå®éªŒç»“æœæ˜¾ç¤ºCPGGç”Ÿæˆçš„åˆæˆCMRæ•°æ®è´¨é‡é«˜ï¼Œæ˜¾è‘—æé«˜è¯Šæ–­å’Œå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¿ƒè„è¡¨å‹é¢„æµ‹ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CPGG%EF%BC%89%E3%80%82">https://anonymous.4open.science/r/CPGGï¼‰ã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMRæˆåƒåœ¨å¿ƒè„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>å¤§è§„æ¨¡é«˜è´¨é‡CMRæ•°æ®é›†çš„æœ‰é™æ€§é™åˆ¶äº†äººå·¥æ™ºèƒ½åœ¨å¿ƒè„å¥åº·é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹æ–¹æ³•CPGGï¼Œæ—¨åœ¨ç”Ÿæˆè¦†ç›–å¹¿æ³›å¿ƒè„å¥åº·çŠ¶æ€çš„å¤šæ ·åŒ–CMRæ•°æ®ã€‚</li>
<li>CPGGæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šè®­ç»ƒç”Ÿæˆæ¨¡å‹å’Œä½¿ç”¨æ¡ä»¶åŒ–çš„é®ç½©è‡ªå›å½’æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸåº¦CMRç”µå½±åºåˆ—ã€‚</li>
<li>åˆæˆå¤§é‡CMRæ•°æ®ç”¨äºæ‰©å……é¢„è®­ç»ƒæ•°æ®ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºCPGGç”Ÿæˆçš„åˆæˆCMRæ•°æ®è´¨é‡é«˜ï¼Œèƒ½æé«˜è¯Šæ–­å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33ac1644980055748e38f46c471d59c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0eb2d8a78915db4f531e0996286aed89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11f07c2324d6ce19686326ef4baf0d30.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CXR-AD-Component-X-ray-Image-Dataset-for-Industrial-Anomaly-Detection"><a href="#CXR-AD-Component-X-ray-Image-Dataset-for-Industrial-Anomaly-Detection" class="headerlink" title="CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection"></a>CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection</h2><p><strong>Authors:Haoyu Bai, Jie Wang, Gaomin Li, Xuan Li, Xiaohu Zhang, Xia Yang</strong></p>
<p>Internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. However, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available X-ray datasets targeting internal defects in components. To address this gap, we construct the first publicly accessible component X-ray anomaly detection (CXR-AD) dataset, comprising real-world X-ray images. The dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. We systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in X-ray imaging, and (3) significant variations in defect scales and morphologies. To evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). Experimental results demonstrate a 29.78% average performance degradation on CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms in handling internal defect detection tasks. To the best of our knowledge, CXR-AD represents the first publicly available X-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies. </p>
<blockquote>
<p>å†…éƒ¨ç¼ºé™·æ£€æµ‹æ˜¯ç¡®ä¿ç»„ä»¶è´¨é‡çš„å…³é”®è¿‡ç¨‹ï¼Œå¼‚å¸¸æ£€æµ‹æ˜¯æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼‚å¸¸æ£€æµ‹æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å¯è§å…‰å›¾åƒçš„è¡¨é¢ç¼ºé™·ä¸Šï¼Œç¼ºä¹é’ˆå¯¹ç»„ä»¶å†…éƒ¨ç¼ºé™·çš„å…¬å¼€Xå°„çº¿æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªå¯å…¬å¼€è®¿é—®çš„ç»„ä»¶Xå°„çº¿å¼‚å¸¸æ£€æµ‹ï¼ˆCXR-ADï¼‰æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œçš„Xå°„çº¿å›¾åƒã€‚è¯¥æ•°æ®é›†æ¶µç›–äº”ä¸ªå·¥ä¸šç»„ä»¶ç±»åˆ«ï¼ŒåŒ…æ‹¬653ä¸ªæ­£å¸¸æ ·æœ¬å’Œ561ä¸ªå¸¦æœ‰ç²¾ç¡®åƒç´ çº§æ©è†œæ³¨é‡Šçš„ç¼ºé™·æ ·æœ¬ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†æ•°æ®é›†çš„ç‰¹ç‚¹ï¼Œå¹¶è¯†åˆ«å‡ºä¸‰ä¸ªä¸»è¦æŠ€æœ¯æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¤æ‚å†…éƒ¨ç»“æ„åŒºåŸŸä¸ç¼ºé™·åŒºåŸŸä¹‹é—´çš„å¼ºè€¦åˆï¼Œï¼ˆ2ï¼‰Xå°„çº¿æˆåƒä¸­å›ºæœ‰çš„ä½å¯¹æ¯”åº¦å’Œé«˜å™ªå£°å¹²æ‰°ï¼Œï¼ˆ3ï¼‰ç¼ºé™·è§„æ¨¡å’Œå½¢æ€çš„æ˜¾è‘—å˜åŒ–ã€‚ä¸ºäº†è¯„ä¼°æ•°æ®é›†çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬å¯¹ä¸‰ç§æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼ˆåŸºäºç‰¹å¾ã€åŸºäºé‡å»ºå’Œé›¶æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸MVTec ADç›¸æ¯”ï¼ŒCXR-ADä¸Šçš„å¹³å‡æ€§èƒ½ä¸‹é™29.78%ï¼Œè¿™çªæ˜¾äº†å½“å‰ç®—æ³•åœ¨å¤„ç†å†…éƒ¨ç¼ºé™·æ£€æµ‹ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCXR-ADæ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€çš„ç”¨äºç»„ä»¶å¼‚å¸¸æ£€æµ‹çš„Xå°„çº¿æ•°æ®é›†ï¼Œå®ƒä¸ºç®—æ³•çš„å‘å±•æä¾›äº†ä¸€ä¸ªçœŸå®çš„å·¥ä¸šåŸºå‡†ï¼Œæé«˜äº†å†…éƒ¨ç¼ºé™·æ£€æµ‹æŠ€æœ¯çš„ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03412v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªé¦–ä¸ªå…¬å¼€å¯ç”¨çš„ç»„ä»¶Xå°„çº¿å¼‚å¸¸æ£€æµ‹ï¼ˆCXR-ADï¼‰æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œçš„Xå°„çº¿å›¾åƒï¼Œæ¶µç›–äº”ç§å·¥ä¸šéƒ¨ä»¶ç±»åˆ«ï¼ŒåŒ…æ‹¬æ­£å¸¸æ ·æœ¬å’Œå¸¦ç²¾ç¡®åƒç´ çº§æ©è†œæ ‡æ³¨çš„ç¼ºé™·æ ·æœ¬ã€‚æ–‡ç« åˆ†æäº†æ•°æ®é›†ç‰¹æ€§ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨å¤æ‚å†…éƒ¨ç»“æ„ã€ä½å¯¹æ¯”åº¦å’Œé«˜å™ªå£°å¹²æ‰°ã€ç¼ºé™·å°ºåº¦å’Œå½¢æ€å¤šæ ·ç­‰ä¸‰ä¸ªæ–¹é¢çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼Œå½“å‰å¼‚å¸¸æ£€æµ‹ç®—æ³•åœ¨CXR-ADä¸Šçš„æ€§èƒ½å¹³å‡ä¸‹é™29.78%ï¼Œå‡¸æ˜¾äº†ç°æœ‰ç®—æ³•åœ¨å¤„ç†å†…éƒ¨ç¼ºé™·æ£€æµ‹ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚CXR-ADä¸ºç»„ä»¶å¼‚å¸¸æ£€æµ‹æä¾›äº†é¦–ä¸ªå…¬å¼€çš„Xå°„çº¿æ•°æ®é›†ï¼Œä¸ºç®—æ³•å‘å±•å’Œå†…éƒ¨ç¼ºé™·æ£€æµ‹æŠ€æœ¯çš„ç²¾ç¡®æ€§æå‡æä¾›äº†å·¥ä¸šåŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†æ„å»ºé¦–ä¸ªå…¬å¼€å¯ç”¨çš„ç»„ä»¶Xå°„çº¿å¼‚å¸¸æ£€æµ‹ï¼ˆCXR-ADï¼‰æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–äº”ç§å·¥ä¸šéƒ¨ä»¶ç±»åˆ«ï¼ŒåŒ…æ‹¬æ­£å¸¸å’Œç¼ºé™·æ ·æœ¬ï¼Œä¸”å¸¦æœ‰ç²¾ç¡®åƒç´ çº§æ ‡æ³¨ã€‚</li>
<li>åˆ†æäº†æ•°æ®é›†ç‰¹æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å…¶åœ¨å†…éƒ¨ç»“æ„å¤æ‚æ€§ã€Xå°„çº¿æˆåƒçš„ä½å¯¹æ¯”åº¦å’Œé«˜å™ªå£°å¹²æ‰°ä»¥åŠç¼ºé™·å°ºåº¦å’Œå½¢æ€çš„å¤šæ ·æ€§ç­‰æ–¹é¢çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>å¯¹æ¯”è¯„ä¼°äº†ä¸‰ç§å…ˆè¿›å¼‚å¸¸æ£€æµ‹ç®—æ³•åœ¨CXR-ADä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œå‘ç°å¹³å‡æ€§èƒ½ä¸‹é™29.78%ã€‚</li>
<li>CXR-ADæ•°æ®é›†æ˜¯ç›®å‰é¦–ä¸ªå…¬å¼€çš„ç”¨äºç»„ä»¶å¼‚å¸¸æ£€æµ‹çš„Xå°„çº¿æ•°æ®é›†ã€‚</li>
<li>CXR-ADæ•°æ®é›†ä¸ºç®—æ³•å‘å±•æä¾›å·¥ä¸šåŸºå‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨å†…éƒ¨ç¼ºé™·æ£€æµ‹æŠ€æœ¯çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abe05670ada8703d183ebbb963cd18c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e14bf29a277319e79a812c6ab55d01a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6e87d9c41295063cea9dcab449bebe4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b8e1f2abd9505877dbd4149129376bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d64d4c1d556141582038c86fa9f124.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Tensor-Core-Beamformer-A-High-Speed-Signal-Processing-Library-for-Multidisciplinary-Use"><a href="#The-Tensor-Core-Beamformer-A-High-Speed-Signal-Processing-Library-for-Multidisciplinary-Use" class="headerlink" title="The Tensor-Core Beamformer: A High-Speed Signal-Processing Library for   Multidisciplinary Use"></a>The Tensor-Core Beamformer: A High-Speed Signal-Processing Library for   Multidisciplinary Use</h2><p><strong>Authors:Leon Oostrum, Bram Veenboer, Ronald Rook, Michael Brown, Pieter Kruizinga, John W. Romein</strong></p>
<p>Beamforming is a well-known technique to combine signals from multiple sensors. It has a wide range of application domains. This paper introduces the Tensor-Core Beamformer: a generic, optimized beamformer library that harnesses the computational power of GPU tensor cores to accelerate beamforming computations. The library hides the complexity of tensor cores from the user, and supports 16-bit and 1-bit precision. An extensive performance evaluation on NVIDIA and AMD GPUs shows that the library outperforms traditional beamforming on regular GPU cores by a wide margin, at much higher energy efficiency. In the 16-bit mode, it achieves over 600 TeraOps&#x2F;s on an AMD MI300X GPU, while approaching 1 TeraOp&#x2F;J. In the 1-bit mode, it breaks the 3 PetaOps&#x2F;s barrier and achieves over 10 TeraOps&#x2F;J on an NVIDIA A100 GPU. The beamforming library can be easily integrated into existing pipelines. We demonstrate its use for medical ultrasound and radio-astronomical instruments. </p>
<blockquote>
<p>æ³¢æŸå½¢æˆæ˜¯ä¸€ç§ä¼—æ‰€å‘¨çŸ¥çš„ä»å¤šä¸ªä¼ æ„Ÿå™¨ç»„åˆä¿¡å·çš„æŠ€æœ¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨é¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†Tensor-Core Beamformerï¼šä¸€ä¸ªé€šç”¨ã€ä¼˜åŒ–çš„æ³¢æŸå½¢æˆåº“ï¼Œå®ƒåˆ©ç”¨GPUå¼ é‡æ ¸å¿ƒçš„è®¡ç®—èƒ½åŠ›æ¥åŠ é€Ÿæ³¢æŸå½¢æˆçš„è®¡ç®—ã€‚è¯¥åº“å‘ç”¨æˆ·éšè—äº†å¼ é‡æ ¸å¿ƒçš„å¤æ‚æ€§ï¼Œå¹¶æ”¯æŒ16ä½å’Œ1ä½ç²¾åº¦ã€‚åœ¨NVIDIAå’ŒAMD GPUä¸Šè¿›è¡Œçš„å¤§è§„æ¨¡æ€§èƒ½è¯„ä¼°è¡¨æ˜ï¼Œè¯¥åº“åœ¨å¸¸è§„GPUæ ¸å¿ƒä¸Šå¤§å¹…è¶…è¶Šäº†ä¼ ç»Ÿæ³¢æŸå½¢æˆï¼Œä¸”èƒ½æ•ˆæ›´é«˜ã€‚åœ¨16ä½æ¨¡å¼ä¸‹ï¼Œå®ƒåœ¨AMD MI300X GPUä¸Šå®ç°äº†è¶…è¿‡600 TeraOps&#x2F;sçš„æ€§èƒ½ï¼Œæ¥è¿‘1 TeraOp&#x2F;Jã€‚åœ¨1ä½æ¨¡å¼ä¸‹ï¼Œå®ƒçªç ´äº†3 PetaOps&#x2F;sçš„éšœç¢ï¼Œå¹¶åœ¨NVIDIA A100 GPUä¸Šå®ç°äº†è¶…è¿‡10 TeraOps&#x2F;Jçš„æ€§èƒ½ã€‚æ³¢æŸå½¢æˆåº“å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰ç®¡é“ä¸­ã€‚æˆ‘ä»¬å±•ç¤ºäº†å…¶åœ¨åŒ»ç–—è¶…å£°å’Œå°„ç”µå¤©æ–‡ä»ªå™¨ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03269v1">PDF</a> 11 pages, 7 figures, accepted at the IEEE International Parallel &amp;   Distributed Processing Symposium (IPDPS) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTensor-Core Beamformerçš„é€šç”¨ä¼˜åŒ–æ³¢æŸå½¢æˆåº“ï¼Œå®ƒåˆ©ç”¨GPUå¼ é‡æ ¸å¿ƒçš„è®¡ç®—èƒ½åŠ›æ¥åŠ é€Ÿæ³¢æŸå½¢æˆè®¡ç®—ã€‚è¯¥åº“æ”¯æŒ16ä½å’Œ1ä½ç²¾åº¦ï¼Œå¹¶ä¸”æ˜“äºé›†æˆåˆ°ç°æœ‰ç®¡é“ä¸­ã€‚åœ¨AMDå’ŒNVIDIA GPUä¸Šçš„æ€§èƒ½è¯„ä¼°è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ³¢æŸå½¢æˆç›¸æ¯”ï¼Œè¯¥åº“å…·æœ‰æ›´é«˜çš„èƒ½æ•ˆå’Œè®¡ç®—æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tensor-Core Beamformeræ˜¯ä¸€ä¸ªåˆ©ç”¨GPUå¼ é‡æ ¸å¿ƒåŠ é€Ÿæ³¢æŸå½¢æˆè®¡ç®—çš„é€šç”¨ä¼˜åŒ–åº“ã€‚</li>
<li>è¯¥åº“æ”¯æŒ16ä½å’Œ1ä½ç²¾åº¦è®¡ç®—ï¼Œæä¾›äº†æ›´é«˜çš„è®¡ç®—æ€§èƒ½å’Œèƒ½æ•ˆã€‚</li>
<li>åœ¨AMD MI300X GPUä¸Šï¼ŒTensor-Core Beamformerçš„16ä½æ¨¡å¼å®ç°äº†è¶…è¿‡600 TeraOps&#x2F;sçš„æ€§èƒ½ï¼Œæ¥è¿‘1 TeraOp&#x2F;Jçš„èƒ½æ•ˆã€‚</li>
<li>åœ¨NVIDIA A100 GPUä¸Šï¼Œè¯¥åº“çš„1ä½æ¨¡å¼çªç ´äº†3 PetaOps&#x2F;sçš„æ€§èƒ½éšœç¢ï¼Œå®ç°äº†è¶…è¿‡10 TeraOps&#x2F;Jçš„èƒ½æ•ˆã€‚</li>
<li>Tensor-Core Beamformeråº“å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„ç®¡é“ä¸­ã€‚</li>
<li>è¯¥åº“åœ¨åŒ»ç–—è¶…å£°å’Œå°„ç”µå¤©æ–‡ä»ªå™¨ç­‰åº”ç”¨ä¸­å¾—åˆ°äº†æ¼”ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9221cb359512b7448f96bdfcca36228.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-455ae7db7501fe3502ac08db984d9b43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c213dd28557f79868971810c2cbb28d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78c9ef065ba2de9d0ec277ad9ecbcdcd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="STG-Spatiotemporal-Graph-Neural-Network-with-Fusion-and-Spatiotemporal-Decoupling-Learning-for-Prognostic-Prediction-of-Colorectal-Cancer-Liver-Metastasis"><a href="#STG-Spatiotemporal-Graph-Neural-Network-with-Fusion-and-Spatiotemporal-Decoupling-Learning-for-Prognostic-Prediction-of-Colorectal-Cancer-Liver-Metastasis" class="headerlink" title="STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal   Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver   Metastasis"></a>STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal   Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver   Metastasis</h2><p><strong>Authors:Yiran Zhu, Wei Yang, Yan su, Zesheng Li, Chengchang Pan, Honggang Qi</strong></p>
<p>We propose a multimodal spatiotemporal graph neural network (STG) framework to predict colorectal cancer liver metastasis (CRLM) progression. Current clinical models do not effectively integrate the tumorâ€™s spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. Our STG framework combines preoperative CT imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the modelâ€™s ability to capture temporal features and improve robustness. A lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. The model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. The innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œï¼ˆSTGï¼‰æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹ç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰çš„è¿›å±•ã€‚å½“å‰çš„ä¸´åºŠæ¨¡å‹ä¸èƒ½æœ‰æ•ˆåœ°æ•´åˆè‚¿ç˜¤çš„ç©ºé—´å¼‚è´¨æ€§ã€åŠ¨æ€æ¼”å˜å’Œå¤æ‚çš„å¤šæ¨¡æ€æ•°æ®å…³ç³»ï¼Œä»è€Œé™åˆ¶äº†å…¶é¢„æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬çš„STGæ¡†æ¶å°†æœ¯å‰CTå½±åƒå’Œä¸´åºŠæ•°æ®æ•´åˆåˆ°å¼‚è´¨å›¾ç»“æ„ä¸­ï¼Œé€šè¿‡ç©ºé—´æ‹“æ‰‘å’Œè·¨æ¨¡æ€è¾¹ç¼˜ï¼Œå®ç°å¯¹è‚¿ç˜¤åˆ†å¸ƒå’ŒåŠ¨æ€æ¼”å˜çš„è”åˆå»ºæ¨¡ã€‚è¯¥æ¡†æ¶ä½¿ç”¨GraphSAGEèšåˆæ—¶ç©ºé‚»åŸŸä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æœ‰ç›‘ç£å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹æ•æ‰æ—¶é—´ç‰¹å¾çš„èƒ½åŠ›ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æ¨¡å‹çš„è½»é‡çº§ç‰ˆæœ¬é€šè¿‡å‡å°‘78.55%çš„å‚æ•°æ•°é‡ï¼Œç»´æŒäº†è¿‘ä¹æœ€å‰æ²¿çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹è”åˆä¼˜åŒ–äº†å¤å‘é£é™©å›å½’å’Œç”Ÿå­˜åˆ†æä»»åŠ¡ï¼Œå¯¹æ¯”æŸå¤±æé«˜äº†ç‰¹å¾è¡¨ç¤ºçš„è¾¨åˆ«åŠ›å’Œè·¨æ¨¡æ€çš„ä¸€è‡´æ€§ã€‚åœ¨MSKCC CRLMæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—¶é—´ç›¸é‚»å‡†ç¡®ç‡ä¸º85%ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º1.1005ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åˆ›æ–°çš„å¼‚è´¨å›¾æ„å»ºå’Œæ—¶ç©ºè§£è€¦æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°æ­ç¤ºäº†åŠ¨æ€è‚¿ç˜¤å¾®ç¯å¢ƒå˜åŒ–å’Œé¢„åä¹‹é—´çš„å…³è”ï¼Œä¸ºä¸ªæ€§åŒ–çš„æ²»ç–—å†³ç­–æä¾›äº†å¯é çš„å®šé‡æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03123v1">PDF</a> 9 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong><br>     æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œï¼ˆSTGï¼‰æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹ç»“è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰çš„è¿›å±•ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æœ¯å‰CTå½±åƒå’Œä¸´åºŠæ•°æ®ï¼Œåˆ›æ–°åœ°ä½¿ç”¨å¼‚è´¨å›¾ç»“æ„è¿›è¡Œè‚¿ç˜¤åˆ†å¸ƒå’ŒåŠ¨æ€æ¼”å˜çš„è”åˆå»ºæ¨¡ã€‚é€šè¿‡æ—¶ç©ºæ‹“æ‰‘å’Œè·¨æ¨¡æ€è¾¹ç¼˜ï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆæ•æ‰è‚¿ç˜¤çš„ç©ºé—´å¼‚è´¨æ€§å’ŒåŠ¨æ€æ¼”å˜ã€‚é‡‡ç”¨GraphSAGEèšåˆæ—¶ç©ºé‚»åŸŸä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨ç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥æé«˜æ¨¡å‹æ•æ‰æ—¶åºç‰¹å¾çš„èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨MSKCC CRLMæ•°æ®é›†ä¸Šçš„æ—¶é—´é‚»è¿‘å‡†ç¡®ç‡ä¸º85%ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º1.1005ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œï¼ˆSTGï¼‰æ¡†æ¶ï¼Œç”¨äºé¢„æµ‹ç»“è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰çš„è¿›å±•ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†æœ¯å‰CTå½±åƒå’Œä¸´åºŠæ•°æ®ï¼Œå½¢æˆå¼‚è´¨å›¾ç»“æ„ï¼Œä»¥è”åˆå»ºæ¨¡è‚¿ç˜¤åˆ†å¸ƒå’ŒåŠ¨æ€æ¼”å˜ã€‚</li>
<li>é€šè¿‡æ—¶ç©ºæ‹“æ‰‘å’Œè·¨æ¨¡æ€è¾¹ç¼˜ï¼Œæœ‰æ•ˆæ•æ‰è‚¿ç˜¤çš„ç©ºé—´å¼‚è´¨æ€§å’ŒåŠ¨æ€æ¼”å˜ã€‚</li>
<li>ä½¿ç”¨GraphSAGEèšåˆæ—¶ç©ºé‚»åŸŸä¿¡æ¯ï¼Œå¢å¼ºæ¨¡å‹æ•æ‰æ—¶åºç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨äº†ç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨MSKCC CRLMæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ—¶é—´é‚»è¿‘å‡†ç¡®ç‡é«˜ï¼Œå¹³å‡ç»å¯¹è¯¯å·®å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f70c15b14f5bcc93bc3fc678252472ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2351609c605fe729594a7a0ca631e1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c28d88295cf113973a47fb025548d4ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ee5ff0a96963288a072db879a6e7eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0490b224baf64e9bcdd5db45998873b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62002329e9395e19548e0ee82fe5270d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Path-and-Bone-Contour-Regularized-Unpaired-MRI-to-CT-Translation"><a href="#Path-and-Bone-Contour-Regularized-Unpaired-MRI-to-CT-Translation" class="headerlink" title="Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation"></a>Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation</h2><p><strong>Authors:Teng Zhou, Jax Luo, Yuping Sun, Yiheng Tan, Shun Yao, Nazim Haouchine, Scott Raymond</strong></p>
<p>Accurate MRI-to-CT translation promises the integration of complementary imaging information without the need for additional imaging sessions. Given the practical challenges associated with acquiring paired MRI and CT scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on CT but less distinguishable on MRI, such as bone structures. This limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. To address this challenge, we propose a path- and bone-contour regularized approach for unpaired MRI-to-CT translation. In our method, MRI and CT images are projected to a shared latent space, where the MRI-to-CT mapping is modeled as a continuous flow governed by neural ordinary differential equations. The optimal mapping is obtained by minimizing the transition path length of the flow. To enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from MRI and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. Evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired MRI-to-CT translation approaches, achieving lower overall error rates. Moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/kennysyp/PaBoT">https://github.com/kennysyp/PaBoT</a>. </p>
<blockquote>
<p>å‡†ç¡®çš„MRIåˆ°CTè½¬æ¢æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–çš„æˆåƒæ—¶é—´çš„æƒ…å†µä¸‹ï¼Œæ•´åˆäº’è¡¥çš„æˆåƒä¿¡æ¯ã€‚è€ƒè™‘åˆ°è·å–é…å¯¹MRIå’ŒCTæ‰«æçš„å®é™…æŒ‘æˆ˜ï¼Œå¼€å‘èƒ½å¤Ÿåˆ©ç”¨æœªé…å¯¹æ•°æ®é›†çš„æ–¹æ³•å¯¹äºæ¨åŠ¨MRIåˆ°CTè½¬æ¢æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ã€‚å½“å‰çš„æœªé…å¯¹MRIåˆ°CTè½¬æ¢æ–¹æ³•ä¸»è¦ä¾èµ–äºå¾ªç¯ä¸€è‡´æ€§å’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œä½†åœ¨å‡†ç¡®è½¬æ¢é‚£äº›åœ¨CTä¸Šæ¸…æ™°å¯è§ä½†åœ¨MRIä¸Šè¾ƒéš¾åŒºåˆ†çš„è§£å‰–ç‰¹å¾æ—¶ï¼Œç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚éª¨éª¼ç»“æ„ã€‚è¿™ä¸€å±€é™æ€§ä½¿å¾—è¿™äº›æ–¹æ³•ä¸å¤ªé€‚åˆæ”¾ç–—åº”ç”¨ï¼Œå› ä¸ºåœ¨æ”¾ç–—è®¡åˆ’ä¸­ç²¾ç¡®çš„éª¨éª¼è¡¨ç¤ºè‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæœªé…å¯¹MRIåˆ°CTè½¬æ¢çš„è·¯å¾„å’Œéª¨éª¼è½®å»“æ­£åˆ™åŒ–æ–¹æ³•ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼ŒMRIå’ŒCTå›¾åƒè¢«æŠ•å°„åˆ°ä¸€ä¸ªå…±äº«æ½œåœ¨ç©ºé—´ï¼Œå…¶ä¸­MRIåˆ°CTçš„æ˜ å°„è¢«å»ºæ¨¡ä¸ºå—ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹æ§åˆ¶çš„è¿ç»­æµã€‚æœ€ä½³æ˜ å°„æ˜¯é€šè¿‡æœ€å°åŒ–æµçš„è¿‡æ¸¡è·¯å¾„é•¿åº¦æ¥è·å¾—çš„ã€‚ä¸ºäº†æé«˜ç¿»è¯‘åçš„éª¨éª¼ç»“æ„çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¥ä»MRIç”Ÿæˆéª¨éª¼è½®å»“ï¼Œå¹¶å®æ–½äº†ç›´æ¥å’Œé—´æ¥é¼“åŠ±æ¨¡å‹å…³æ³¨éª¨éª¼è½®å»“åŠå…¶é‚»è¿‘åŒºåŸŸçš„æœºåˆ¶ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœªé…å¯¹MRIåˆ°CTè½¬æ¢æ–¹æ³•ï¼Œæ€»ä½“è¯¯å·®ç‡æ›´ä½ã€‚æ­¤å¤–ï¼Œåœ¨ä¸‹æ¸¸çš„éª¨éª¼åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒéª¨éª¼ç»“æ„ä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/kennysyp/PaBoT%E3%80%82">https://github.com/kennysyp/PaBoTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§£å†³MRIä¸CTæˆåƒä¿¡æ¯èåˆéš¾é¢˜çš„æ–¹æ³•ã€‚é‰´äºè·å¾—é…å¯¹MRIå’ŒCTæ‰«æçš„å®é™…æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåŸºäºè·¯å¾„å’Œéª¨è½®å»“æ­£åˆ™åŒ–çš„æ— é…å¯¹MRI-CTè½¬æ¢æ–¹æ³•ã€‚é€šè¿‡å»ºæ¨¡MRIåˆ°CTçš„è¿ç»­æ˜ å°„ï¼Œå¹¶åˆ©ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ä¼˜åŒ–æ˜ å°„è·¯å¾„ï¼Œæé«˜äº†éª¨ç»“æ„çš„ç¿»è¯‘å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸éª¨åˆ†å‰²ä»»åŠ¡ä¸­å±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è§£å†³äº†MRIä¸CTæˆåƒèåˆä¸­çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹éª¨éª¼ç»“æ„çš„å‡†ç¡®ç¿»è¯‘é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åŸºäºè·¯å¾„å’Œéª¨è½®å»“æ­£åˆ™åŒ–çš„æ— é…å¯¹MRI-CTè½¬æ¢æ–¹æ³•ï¼Œé€‚ç”¨äºæ”¾å°„æ²»ç–—ç­‰åº”ç”¨åœºæ™¯ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å°†MRIå’ŒCTå›¾åƒæŠ•å½±åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œå¹¶åˆ©ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹å»ºæ¨¡è¿ç»­æ˜ å°„ï¼Œä¼˜åŒ–äº†æ˜ å°„è·¯å¾„é•¿åº¦ã€‚</li>
<li>å¼•å…¥å¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œç”Ÿæˆéª¨è½®å»“ï¼Œæé«˜äº†ç¿»è¯‘å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ä¸‹æ¸¸éª¨åˆ†å‰²ä»»åŠ¡ä¸­å±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b15de95c70ecdc90297e186e0845b04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d51dc7a9d901808408691949e2b8a01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c996d07367d272d0ad006f15eeb063d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2df5b895ef32a5d3eaa6d2449d39147.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4da8151e6c828851e11c0a4b21947103.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lesion-Aware-Generative-Artificial-Intelligence-for-Virtual-Contrast-Enhanced-Mammography-in-Breast-Cancer"><a href="#Lesion-Aware-Generative-Artificial-Intelligence-for-Virtual-Contrast-Enhanced-Mammography-in-Breast-Cancer" class="headerlink" title="Lesion-Aware Generative Artificial Intelligence for Virtual   Contrast-Enhanced Mammography in Breast Cancer"></a>Lesion-Aware Generative Artificial Intelligence for Virtual   Contrast-Enhanced Mammography in Breast Cancer</h2><p><strong>Authors:Aurora Rofena, Arianna Manchia, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi</strong></p>
<p>Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. It acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. While CESM offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. To address these limitations, we propose Seg-CycleGAN, a generative deep learning framework for Virtual Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. Building upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. Experiments on the CESM@UCBM dataset demonstrate that Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while maintaining competitive MSE and VIF. Qualitative evaluations further confirm improved lesion fidelity in the generated images. These results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free CESM alternatives. </p>
<blockquote>
<p>å¯¹æ¯”å¢å¼ºå…‰è°±ä¹³è…ºæ‘„å½±ï¼ˆCESMï¼‰æ˜¯ä¸€ç§åŒèƒ½ä¹³è…ºæ‘„å½±æŠ€æœ¯ï¼Œé€šè¿‡æ³¨å°„ç¢˜åŒ–é€ å½±å‰‚æé«˜ç—…ç¶çš„å¯è§æ€§ã€‚å®ƒè·å–ä¸æ ‡å‡†ä¹³è…ºæ‘„å½±ç›¸å½“çš„ä½èƒ½å›¾åƒå’Œé«˜èƒ½å›¾åƒï¼Œç„¶åå°†å®ƒä»¬ç»„åˆä»¥äº§ç”Ÿçªå‡ºç—…ç¶å¯¹æ¯”å¢å¼ºçš„åŒèƒ½å‡å½±å›¾åƒã€‚è™½ç„¶CESMåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæ ‡å‡†ä¹³è…ºæ‘„å½±ï¼Œä½†å…¶ä½¿ç”¨ä¼šå¸¦æ¥æ›´é«˜çš„è¾å°„æš´éœ²å’Œä¸é€ å½±å‰‚ç›¸å…³çš„æ½œåœ¨å‰¯ä½œç”¨ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Seg-CycleGANï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºCESMè™šæ‹Ÿå¯¹æ¯”å¢å¼ºçš„ç”Ÿæˆæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¨¡å‹ä»ä½èƒ½å›¾åƒä¸­åˆæˆé«˜ä¿çœŸåŒèƒ½å‡å½±å›¾åƒï¼Œåˆ©ç”¨ç—…ç¶åˆ†å‰²å›¾æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹å¹¶æé«˜ç—…ç¶é‡å»ºã€‚åŸºäºæ ‡å‡†CycleGANæ¶æ„ï¼ŒSeg-CycleGANå¼•å…¥äº†å±€éƒ¨æŸå¤±é¡¹ï¼Œé‡ç‚¹å…³æ³¨ç—…ç¶åŒºåŸŸï¼Œæé«˜äº†è¯Šæ–­ç›¸å…³åŒºåŸŸçš„åˆæˆæ•ˆæœã€‚åœ¨CESM@UCBMæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSeg-CycleGANåœ¨PSNRå’ŒSSIMæ–¹é¢ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶ä¿æŒç«äº‰çš„MSEå’ŒVIFã€‚å®šæ€§è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆå›¾åƒä¸­ç—…ç¶çš„ä¿çœŸåº¦æœ‰æ‰€æé«˜ã€‚è¿™äº›ç»“æœæç¤ºï¼Œåˆ†å‰²æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹ä¸ºæ— å¯¹æ¯”å‰‚çš„CESMæ›¿ä»£å“æä¾›äº†å¯è¡Œçš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¯¹æ¯”å¢å¼ºå…‰è°±æˆåƒï¼ˆCESMï¼‰æ˜¯ä¸€ç§åŒèƒ½ä¹³è…ºXçº¿æ‘„å½±æŠ€æœ¯ï¼Œé€šè¿‡æ³¨å°„ç¢˜å¯¹æ¯”å‰‚æé«˜ç—…ç¶çš„å¯è§æ€§ã€‚å®ƒè·å–ä½èƒ½é‡å›¾åƒï¼ˆä¸æ ‡å‡†ä¹³è…ºXçº¿æ‘„å½±ç›¸å½“ï¼‰å’Œé«˜èƒ½é‡å›¾åƒï¼Œç„¶åç»“åˆäº§ç”ŸåŒèƒ½é‡å‡å»å›¾åƒï¼Œçªå‡ºç—…ç¶çš„å¯¹æ¯”åº¦å¢å¼ºã€‚è™½ç„¶CESMç›¸æ¯”æ ‡å‡†ä¹³è…ºXçº¿æ‘„å½±å…·æœ‰æ›´é«˜çš„è¯Šæ–­å‡†ç¡®æ€§ï¼Œä½†å…¶ä½¿ç”¨ä¼šå¸¦æ¥æ›´é«˜çš„è¾å°„æš´éœ²å’Œä¸å¯¹æ¯”å‰‚ç›¸å…³çš„æ½œåœ¨å‰¯ä½œç”¨ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºSeg-CycleGANï¼Œä¸€ç§ç”¨äºCESMè™šæ‹Ÿå¯¹æ¯”åº¦å¢å¼ºçš„ç”Ÿæˆæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¨¡å‹ä»ä½èƒ½é‡å›¾åƒä¸­åˆæˆé«˜ä¿çœŸåŒèƒ½é‡å‡å»å›¾åƒï¼Œåˆ©ç”¨ç—…ç¶åˆ†å‰²å›¾å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜ç—…ç¶é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒSeg-CycleGANåœ¨PSNRå’ŒSSIMæ–¹é¢ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„MSEå’ŒVIFã€‚å®šæ€§è¯„ä¼°è¿›ä¸€æ­¥è¯å®ç”Ÿæˆå›¾åƒä¸­ç—…ç¶çš„ä¿çœŸåº¦æœ‰æ‰€æé«˜ã€‚è¿™äº›ç»“æœæç¤ºï¼Œåˆ†å‰²æ„ŸçŸ¥ç”Ÿæˆæ¨¡å‹ä¸ºæ— å¯¹æ¯”å‰‚CESMæ›¿ä»£å“æä¾›äº†å¯è¡Œçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å¢å¼ºå…‰è°±æˆåƒï¼ˆCESMï¼‰æ˜¯ä¸€ç§åŒèƒ½ä¹³è…ºXçº¿æ‘„å½±æŠ€æœ¯ï¼Œèƒ½æé«˜ç—…ç¶çš„å¯è§æ€§ã€‚</li>
<li>CESMé€šè¿‡è·å–ä½èƒ½é‡å’Œé«˜èƒ½é‡å›¾åƒï¼Œç„¶åç»“åˆäº§ç”ŸåŒèƒ½é‡å‡å»å›¾åƒæ¥çªå‡ºç—…ç¶ã€‚</li>
<li>CESMç›¸è¾ƒäºæ ‡å‡†ä¹³è…ºXçº¿æ‘„å½±æœ‰æ›´é«˜çš„è¯Šæ–­å‡†ç¡®æ€§ï¼Œä½†ä¼´éšæ›´é«˜çš„è¾å°„æš´éœ²å’Œå¯¹æ¯”å‰‚æ½œåœ¨å‰¯ä½œç”¨ã€‚</li>
<li>Seg-CycleGANæ˜¯ä¸€ç§ç”Ÿæˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºCESMçš„è™šæ‹Ÿå¯¹æ¯”åº¦å¢å¼ºã€‚</li>
<li>Seg-CycleGANèƒ½ä»ä½èƒ½é‡å›¾åƒä¸­åˆæˆé«˜ä¿çœŸåŒèƒ½é‡å‡å»å›¾åƒï¼Œåˆ©ç”¨ç—…ç¶åˆ†å‰²å›¾æ”¹å–„ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSeg-CycleGANåœ¨å¤šç§è¯„ä¼°æ ‡å‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåˆæˆå›¾åƒçš„ç—…ç¶ä¿çœŸåº¦æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b32a307ba568dc60c5df4100f3374c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c396666e240f1e129041d30fee3a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6610fd8782e61cf06655cf374dbbb441.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e25290edc6de098be5e335cf768fe3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc7d449eb860782a36b159d0d2e97f6a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Adversarial-Robustness-Analysis-of-Vision-Language-Models-in-Medical-Image-Segmentation"><a href="#Adversarial-Robustness-Analysis-of-Vision-Language-Models-in-Medical-Image-Segmentation" class="headerlink" title="Adversarial Robustness Analysis of Vision-Language Models in Medical   Image Segmentation"></a>Adversarial Robustness Analysis of Vision-Language Models in Medical   Image Segmentation</h2><p><strong>Authors:Anjila Budathoki, Manish Dhakal</strong></p>
<p>Adversarial attacks have been fairly explored for computer vision and vision-language models. However, the avenue of adversarial attack for the vision language segmentation models (VLSMs) is still under-explored, especially for medical image analysis.   Thus, we have investigated the robustness of VLSMs against adversarial attacks for 2D medical images with different modalities with radiology, photography, and endoscopy. The main idea of this project was to assess the robustness of the fine-tuned VLSMs specially in the medical domain setting to address the high risk scenario.   First, we have fine-tuned pre-trained VLSMs for medical image segmentation with adapters.   Then, we have employed adversarial attacks â€“ projected gradient descent (PGD) and fast gradient sign method (FGSM) â€“ on that fine-tuned model to determine its robustness against adversaries.   We have reported modelsâ€™ performance decline to analyze the adversariesâ€™ impact.   The results exhibit significant drops in the DSC and IoU scores after the introduction of these adversaries. Furthermore, we also explored universal perturbation but were not able to find for the medical images.   \footnote{<a target="_blank" rel="noopener" href="https://github.com/anjilab/secure-private-ai%7D">https://github.com/anjilab/secure-private-ai}</a> </p>
<blockquote>
<p>å¯¹æŠ—æ€§æ”»å‡»åœ¨è®¡ç®—æœºè§†è§‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ–¹é¢å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œé’ˆå¯¹è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹ï¼ˆVLSMsï¼‰çš„å¯¹æŠ—æ€§æ”»å‡»é€”å¾„ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢ï¼Œä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶äº†VLSMså¯¹ä¸åŒæ¨¡æ€çš„äºŒç»´åŒ»å­¦å›¾åƒï¼ˆå¦‚æ”¾å°„å­¦ã€æ‘„å½±å’Œå†…é•œï¼‰çš„å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§ã€‚æ­¤é¡¹ç›®çš„ä¸»è¦ç›®çš„æ˜¯è¯„ä¼°ç»è¿‡ç²¾ç»†è°ƒæ•´çš„VLSMsåœ¨åŒ»å­¦é¢†åŸŸè®¾ç½®ä¸­çš„ç¨³å¥æ€§ï¼Œä»¥è§£å†³é«˜é£é™©åœºæ™¯é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é€‚é…å™¨å¯¹é¢„è®­ç»ƒçš„VLSMsè¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²çš„å¾®è°ƒã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å¾®è°ƒæ¨¡å‹é‡‡ç”¨å¯¹æŠ—æ€§æ”»å‡»â€”â€”æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰å’Œå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•ï¼ˆFGSMï¼‰ï¼Œä»¥ç¡®å®šå…¶å¯¹å¯¹æ‰‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†æ¨¡å‹æ€§èƒ½ä¸‹é™çš„æƒ…å†µï¼Œä»¥åˆ†æå¯¹æ‰‹çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¼•å…¥è¿™äº›å¯¹æ‰‹åï¼ŒDSCå’ŒIoUåˆ†æ•°å‡ºç°äº†æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†é€šç”¨æ‰°åŠ¨ï¼Œä½†æœªèƒ½é’ˆå¯¹åŒ»å­¦å›¾åƒæ‰¾åˆ°é€‚ç”¨çš„æ–¹æ³•ã€‚\footnote{<a target="_blank" rel="noopener" href="https://github.com/anjilab/secure-private-ai%7D">https://github.com/anjilab/secure-private-ai}</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02971v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢ç´¢äº†é’ˆå¯¹äºŒç»´åŒ»å­¦å›¾åƒçš„è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹ï¼ˆVLSMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„å¯¹æŠ—æ”»å‡»ç ”ç©¶ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒçš„VLSMsè¿›è¡Œå¾®è°ƒå¹¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æŠ•å½±æ³•å’Œå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•è¿›è¡Œå¯¹æŠ—æ”»å‡»æµ‹è¯•ï¼Œå‘ç°æ¨¡å‹æ€§èƒ½åœ¨å¼•å…¥å¯¹æŠ—æ ·æœ¬åå‡ºç°æ˜¾è‘—ä¸‹é™ã€‚å°šæœªå®ç°é’ˆå¯¹åŒ»å­¦å›¾åƒçš„å…¨å±€æ‰°åŠ¨ç ”ç©¶ã€‚å¦‚éœ€äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œå¯è®¿é—®é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/anjilab/secure-private-ai%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/anjilab/secure-private-aiè¿›è¡ŒæŸ¥çœ‹ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ”»å‡»åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹ï¼ˆVLSMsï¼‰ä¸Šå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„VLSMsæ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œå¯¹æŠ—æ”»å‡»æµ‹è¯•ï¼Œå‘ç°æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>å¯¹æŠ—æ”»å‡»æ–¹æ³•åŒ…æ‹¬æ¢¯åº¦ä¸‹é™æŠ•å½±æ³•ï¼ˆPGDï¼‰å’Œå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•ï¼ˆFGSMï¼‰ã€‚</li>
<li>åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå¼•å…¥å¯¹æŠ—æ ·æœ¬åï¼Œæ¨¡å‹çš„DSCå’ŒIoUå¾—åˆ†æ˜æ˜¾ä¸‹é™ã€‚</li>
<li>ç›®å‰å°šæœªå®ç°é’ˆå¯¹åŒ»å­¦å›¾åƒçš„å…¨å±€æ‰°åŠ¨ç ”ç©¶ã€‚</li>
<li>é¡¹ç›®æ—¨åœ¨è¯„ä¼°ç²¾ç»†è°ƒæ•´çš„VLSMsåœ¨åŒ»å­¦é¢†åŸŸè®¾ç½®ä¸­çš„ç¨³å¥æ€§ï¼Œä»¥åº”å¯¹é«˜é£é™©åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8de2cb12a60c62b503c070e9818b2abc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95cbd622cef73f8bc06b95f2faca7672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0e35007a5f7516a7bc335d2d8e59992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ad6ae087f2b344cfd64c5f9669647f8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Deep-Learning-based-Keypoint-Localization-Estimating-Descriptor-Matching-Performance"><a href="#Unsupervised-Deep-Learning-based-Keypoint-Localization-Estimating-Descriptor-Matching-Performance" class="headerlink" title="Unsupervised Deep Learning-based Keypoint Localization Estimating   Descriptor Matching Performance"></a>Unsupervised Deep Learning-based Keypoint Localization Estimating   Descriptor Matching Performance</h2><p><strong>Authors:David Rivas-Villar, Ãlvaro S. Hervella, JosÃ© Rouco, Jorge Novo</strong></p>
<p>Retinal image registration, particularly for color fundus images, is a challenging yet essential task with diverse clinical applications. Existing registration methods for color fundus images typically rely on keypoints and descriptors for alignment; however, a significant limitation is their reliance on labeled data, which is particularly scarce in the medical domain.   In this work, we present a novel unsupervised registration pipeline that entirely eliminates the need for labeled data. Our approach is based on the principle that locations with distinctive descriptors constitute reliable keypoints. This fully inverts the conventional state-of-the-art approach, conditioning the detector on the descriptor rather than the opposite.   First, we propose an innovative descriptor learning method that operates without keypoint detection or any labels, generating descriptors for arbitrary locations in retinal images. Next, we introduce a novel, label-free keypoint detector network which works by estimating descriptor performance directly from the input image.   We validate our method through a comprehensive evaluation on four hold-out datasets, demonstrating that our unsupervised descriptor outperforms state-of-the-art supervised descriptors and that our unsupervised detector significantly outperforms existing unsupervised detection methods. Finally, our full registration pipeline achieves performance comparable to the leading supervised methods, while not employing any labeled data. Additionally, the label-free nature and design of our method enable direct adaptation to other domains and modalities. </p>
<blockquote>
<p>è§†ç½‘è†œå›¾åƒé…å‡†ï¼Œå°¤å…¶æ˜¯å¯¹äºå½©è‰²çœ¼åº•å›¾åƒæ¥è¯´ï¼Œæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½†è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œåœ¨ä¸´åºŠåº”ç”¨ä¸­å…·æœ‰å¤šç§åº”ç”¨ã€‚ç°æœ‰çš„å½©è‰²çœ¼åº•å›¾åƒé…å‡†æ–¹æ³•é€šå¸¸ä¾èµ–äºå…³é”®ç‚¹åŠå…¶æè¿°ç¬¦æ¥è¿›è¡Œå¯¹é½ï¼›ç„¶è€Œï¼Œå®ƒä»¬çš„ä¸€ä¸ªé‡å¤§å±€é™æ€§æ˜¯ä¾èµ–äºæ ‡è®°æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦é¢†åŸŸå°¤ä¸ºç¨€ç¼ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£é…å‡†æµç¨‹ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹æ ‡è®°æ•°æ®çš„éœ€è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè¿™æ ·ä¸€ä¸ªåŸåˆ™ï¼šå…·æœ‰ç‹¬ç‰¹æè¿°ç¬¦çš„åœ°ç‚¹æ„æˆå¯é çš„å…³é”®ç‚¹ã€‚è¿™å®Œå…¨é¢ å€’äº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°†æ¢æµ‹å™¨ç½®äºæè¿°ç¬¦ä¹‹ä¸Šè€Œä¸æ˜¯ç›¸åã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æè¿°ç¬¦å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨æ²¡æœ‰å…³é”®ç‚¹æ£€æµ‹æˆ–ä»»ä½•æ ‡ç­¾çš„æƒ…å†µä¸‹è¿è¡Œï¼Œä¸ºè§†ç½‘è†œå›¾åƒä¸­çš„ä»»æ„ä½ç½®ç”Ÿæˆæè¿°ç¬¦ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ— æ ‡ç­¾å…³é”®ç‚¹æ£€æµ‹ç½‘ç»œï¼Œå®ƒé€šè¿‡ç›´æ¥ä»è¾“å…¥å›¾åƒä¼°è®¡æè¿°ç¬¦æ€§èƒ½æ¥å·¥ä½œã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªç‹¬ç«‹æ•°æ®é›†çš„ç»¼åˆè¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ— ç›‘ç£æè¿°ç¬¦ä¼˜äºæœ€å…ˆè¿›çš„ç›‘ç£æè¿°ç¬¦ï¼Œæˆ‘ä»¬çš„æ— ç›‘ç£æ£€æµ‹å™¨ä¹Ÿæ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— ç›‘ç£æ£€æµ‹æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬çš„å®Œæ•´é…å‡†æµç¨‹åœ¨ä¸ä½¿ç”¨ä»»ä½•æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†ä¸é¢†å…ˆçš„æœ‰ç›‘ç£æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ–¹æ³•çš„æ— æ ‡ç­¾ç‰¹æ€§å’Œè®¾è®¡ä½¿å…¶èƒ½å¤Ÿç›´æ¥é€‚åº”å…¶ä»–é¢†åŸŸå’Œæ¨¡æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02779v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€æ ‡æ³¨æ•°æ®çš„æ–°å‹è§†ç½‘è†œå›¾åƒæ³¨å†Œæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºå…·æœ‰ç‹¬ç‰¹æè¿°ç¬¦çš„åœ°ç‚¹æ„æˆå¯é å…³é”®ç‚¹çš„åŸç†ï¼Œå®Œå…¨é¢ è¦†äº†ä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡åˆ›æ–°çš„æ— ç›‘ç£æè¿°ç¬¦å­¦ä¹ æ–¹æ³•å’Œæ ‡ç­¾æ— å…³çš„å…³é”®ç‚¹æ£€æµ‹ç½‘ç»œï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„è¡¨ç°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤æ–¹æ³•æ— éœ€ä»»ä½•æ ‡æ³¨æ•°æ®ï¼Œæ€§èƒ½ä¸é¢†å…ˆçš„ç›‘ç£æ–¹æ³•ç›¸å½“ï¼Œå¹¶å¯ç›´æ¥é€‚åº”åˆ°å…¶ä»–é¢†åŸŸå’Œæ¨¡æ€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œå›¾åƒæ³¨å†Œæ˜¯é¢œè‰²çœ¼åº•å›¾åƒçš„é‡è¦ä»»åŠ¡ï¼Œå…·æœ‰å¤šç§ä¸´åºŠåº”ç”¨ã€‚</li>
<li>ä¼ ç»Ÿæ³¨å†Œæ–¹æ³•ä¾èµ–äºå…³é”®ç‚¹å’Œæè¿°ç¬¦è¿›è¡Œå¯¹é½ï¼Œä½†ä¾èµ–æ ‡æ³¨æ•°æ®å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ— ç›‘ç£æ³¨å†Œæ–¹æ³•ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºå…·æœ‰ç‹¬ç‰¹æè¿°ç¬¦çš„åœ°ç‚¹æ„æˆå¯é å…³é”®ç‚¹çš„åŸç†ã€‚</li>
<li>åˆ›æ–°çš„æ— ç›‘ç£æè¿°ç¬¦å­¦ä¹ æ–¹æ³•å’Œæ ‡ç­¾æ— å…³çš„å…³é”®ç‚¹æ£€æµ‹ç½‘ç»œè¢«æå‡ºã€‚</li>
<li>æ–¹æ³•åœ¨å››ä¸ªç‹¬ç«‹æ•°æ®é›†ä¸Šçš„è¡¨ç°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43a1308b03f8d9684152c3577bae7eff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b2bf366bbdcf1ab37aa9988bab11c05.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Advancing-Generalizable-Tumor-Segmentation-with-Anomaly-Aware-Open-Vocabulary-Attention-Maps-and-Frozen-Foundation-Diffusion-Models"><a href="#Advancing-Generalizable-Tumor-Segmentation-with-Anomaly-Aware-Open-Vocabulary-Attention-Maps-and-Frozen-Foundation-Diffusion-Models" class="headerlink" title="Advancing Generalizable Tumor Segmentation with Anomaly-Aware   Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models"></a>Advancing Generalizable Tumor Segmentation with Anomaly-Aware   Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models</h2><p><strong>Authors:Yankai Jiang, Peng Zhang, Donglin Yang, Yuan Tian, Hai Lin, Xiaosong Wang</strong></p>
<p>We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/Yankai96/DiffuGTS">https://github.com/Yankai96/DiffuGTS</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†å¯æ¨å¹¿çš„è‚¿ç˜¤åˆ†å‰²æŠ€æœ¯ï¼Œæ—¨åœ¨é’ˆå¯¹å¤šä¸ªè§£å‰–åŒºåŸŸè¿›è¡Œé›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²è®­ç»ƒå•ä¸ªæ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸åˆ†å‰²è´¨é‡ã€å¯æ‰©å±•æ€§å’Œé€‚ç”¨æˆåƒæ¨¡å¼èŒƒå›´ç›¸å…³çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„æ¡†æ¶DiffuGTSï¼Œå‘ç°äº†å†»ç»“åŒ»ç–—åŸºç¡€æ‰©æ•£æ¨¡å‹å†…éƒ¨è¡¨ç¤ºåœ¨è‚¿ç˜¤åˆ†å‰²ä¸­çš„æ½œåŠ›ï¼Œä½œä¸ºä¸€ç§é«˜æ•ˆçš„é›¶æ ·æœ¬å­¦ä¹ è€…ã€‚DiffuGTSåŸºäºæ–‡æœ¬æç¤ºåˆ›å»ºå¼‚å¸¸æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œå®ç°äº†å¯æ¨å¹¿çš„å¼‚å¸¸åˆ†å‰²ï¼Œä¸å†å±€é™äºé¢„å®šä¹‰çš„è®­ç»ƒç±»åˆ«åˆ—è¡¨ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å’Œæ”¹è¿›å¼‚å¸¸åˆ†å‰²æ©è†œï¼ŒDiffuGTSåˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´ä¿®å¤å°†ç—…ç†åŒºåŸŸè½¬æ¢ä¸ºé«˜è´¨é‡ä¼ªå¥åº·å¯¹åº”ç‰©ï¼Œå¹¶åº”ç”¨æ–°çš„åƒç´ çº§å’Œç‰¹å¾çº§æ®‹å·®å­¦ä¹ æ–¹æ³•ï¼Œå¾—åˆ°è´¨é‡æ˜¾è‘—æé«˜å’Œæ³›åŒ–èƒ½åŠ›å¢å¼ºçš„åˆ†å‰²æ©è†œã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªè‚¿ç˜¤ç±»åˆ«çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œåœ¨å¤šä¸ªé›¶æ ·æœ¬è®¾ç½®ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yankai96/DiffuGTS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yankai96/DiffuGTSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02753v1">PDF</a> This paper is accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é€šç”¨è‚¿ç˜¤åˆ†å‰²æŠ€æœ¯ï¼Œæ—¨åœ¨é’ˆå¯¹å¤šç§è§£å‰–åŒºåŸŸè¿›è¡Œé›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²çš„å•æ¨¡å‹è®­ç»ƒã€‚æ–‡ç« å¼•å…¥äº†ä¸€ä¸ªåä¸ºDiffuGTSçš„æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨å†»ç»“çš„åŒ»ç–—åŸºç¡€æ‰©æ•£æ¨¡å‹å†…çš„å†…éƒ¨è¡¨å¾ä½œä¸ºé›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²çš„é«˜æ•ˆå­¦ä¹ è€…ã€‚DiffuGTSåˆ›å»ºåŸºäºæ–‡æœ¬æç¤ºçš„å¼‚å¸¸æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œä»¥å®ç°å¯æ¨å¹¿çš„å¼‚å¸¸åˆ†å‰²ï¼Œè€Œä¸å—é¢„è®¾è®­ç»ƒç±»åˆ«åˆ—è¡¨çš„é™åˆ¶ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹æ”¹è¿›å’Œç»†åŒ–å¼‚å¸¸åˆ†å‰²æ©è†œï¼Œé€šè¿‡å°†ç—…ç†åŒºåŸŸè½¬åŒ–ä¸ºé«˜è´¨é‡çš„ä¼ªå¥åº·å¯¹åº”ç‰©ï¼Œå¹¶åœ¨åƒç´ çº§åˆ«å’Œç‰¹å¾çº§åˆ«åº”ç”¨æ–°å‹çš„æ®‹å·®å­¦ä¹ æ–¹æ³•ï¼Œè·å¾—è´¨é‡æ›´é«˜ã€æ³›åŒ–æ€§æ›´å¼ºçš„åˆ†å‰²æ©è†œã€‚åœ¨å››ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªè‚¿ç˜¤ç±»åˆ«ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ€§èƒ½å“è¶Šï¼Œåœ¨å¤šé›¶æ ·æœ¬è®¾ç½®ä¸­è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®æ ‡æ˜¯å®ç°é€šç”¨è‚¿ç˜¤åˆ†å‰²ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªå•ä¸€æ¨¡å‹æ¥é€‚åº”å¤šç§è§£å‰–åŒºåŸŸçš„é›¶æ ·æœ¬è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>å¼•å…¥æ–°å‹æ¡†æ¶DiffuGTSï¼Œåˆ©ç”¨å†»ç»“çš„åŒ»ç–—åŸºç¡€æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨è¡¨å¾è¿›è¡Œé›¶æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>DiffuGTSé€šè¿‡åˆ›å»ºå¼‚å¸¸æ„ŸçŸ¥å¼€æ”¾è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œå®ç°å¯æ¨å¹¿çš„å¼‚å¸¸åˆ†å‰²ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºæ”¹è¿›å’Œç»†åŒ–å¼‚å¸¸åˆ†å‰²æ©è†œï¼Œé€šè¿‡è½¬åŒ–ç—…ç†åŒºåŸŸå¹¶åº”ç”¨æ–°å‹çš„åƒç´ çº§å’Œç‰¹å¾çº§æ®‹å·®å­¦ä¹ æ–¹æ³•æé«˜åˆ†å‰²è´¨é‡ã€‚</li>
<li>æ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªè‚¿ç˜¤ç±»åˆ«ä¸Šçš„å®éªŒè¡¨ç°å“è¶Šï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æä¾›çš„ä»£ç å¯åœ¨å…¬å¼€ä»£ç åº“ä¸­æ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Yankai96/DiffuGTS%EF%BC%89%E3%80%82">https://github.com/Yankai96/DiffuGTSï¼‰ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53269bd49807988eaca1480fa1faa914.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bc142f57862bad438e25cb705152d88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfb51bbacab5c7e7c1e78741e825adfe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d40b0c0863e63104ef6fa38465efe4d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a279ad190cf1e00c4bc731284e52e69d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Timing-Is-Everything-Finding-the-Optimal-Fusion-Points-in-Multimodal-Medical-Imaging"><a href="#Timing-Is-Everything-Finding-the-Optimal-Fusion-Points-in-Multimodal-Medical-Imaging" class="headerlink" title="Timing Is Everything: Finding the Optimal Fusion Points in Multimodal   Medical Imaging"></a>Timing Is Everything: Finding the Optimal Fusion Points in Multimodal   Medical Imaging</h2><p><strong>Authors:Valerio Guarrasi, Klara Mogensen, Sara Tassinari, Sara Qvarlander, Paolo Soda</strong></p>
<p>Multimodal deep learning harnesses diverse imaging modalities, such as MRI sequences, to enhance diagnostic accuracy in medical imaging. A key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. Current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. We propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. At each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. This process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. The approach is validated on two multimodal MRI datasets, each addressing different classification tasks. Our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. These architectures demonstrated superior accuracy, F-score, and specificity while maintaining competitive or improved AUC values. Furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. By systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. It provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical AI applications. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åˆ©ç”¨å¤šç§æˆåƒæ¨¡æ€ï¼ˆå¦‚MRIåºåˆ—ï¼‰æ¥æé«˜åŒ»å­¦æˆåƒçš„è¯Šæ–­å‡†ç¡®æ€§ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ç¡®å®šæ•´åˆè¿™äº›æ¨¡æ€çš„æœ€ä½³æ—¶æœºï¼Œç‰¹åˆ«æ˜¯ç¡®å®šåº”æ’å…¥èåˆæ¨¡å—çš„ç½‘ç»œå±‚ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨è°ƒæ•´æˆ–ç©·ä¸¾æœç´¢ï¼Œè¿™ä¸ä»…è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œä¸”æ— æ³•ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜ç»“æœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é¡ºåºå‰å‘æœç´¢ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨å¤šæ¨¡æ€ç½‘ç»œçš„ä¸åŒå±‚çº§é€æ­¥æ¿€æ´»å¹¶è¯„ä¼°å€™é€‰èåˆæ¨¡å—ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œè¯¥ç®—æ³•éƒ½ä¼šæ ¹æ®å…ˆå‰å­¦ä¹ çš„æƒé‡è¿›è¡Œå†è®­ç»ƒï¼Œå¹¶é€šè¿‡æ¯”è¾ƒéªŒè¯æŸå¤±æ¥è¯†åˆ«è¡¨ç°æœ€ä½³çš„é…ç½®ã€‚è¿™ä¸ªè¿‡ç¨‹ç³»ç»Ÿåœ°å‡å°‘äº†æœç´¢ç©ºé—´ï¼Œèƒ½å¤Ÿé«˜æ•ˆç¡®å®šæœ€ä¼˜èåˆæ—¶é—´ï¼Œè€Œæ— éœ€å¯¹æ‰€æœ‰å¯èƒ½çš„æ¨¡å—æ”¾ç½®è¿›è¡Œå…¨é¢æµ‹è¯•ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªå¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½é’ˆå¯¹ä¸åŒçš„åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç®—æ³•å§‹ç»ˆèƒ½å¤Ÿè¯†åˆ«å‡ºä¼˜äºå•æ¨¡æ€åŸºå‡†ã€åæœŸèåˆä»¥åŠæ‰€æœ‰æ½œåœ¨èåˆæ”¾ç½®çš„æš´åŠ›ç»„åˆçš„é…ç½®ã€‚è¿™äº›æ¶æ„åœ¨ä¿æŒç«äº‰åŠ›æˆ–æé«˜AUCå€¼çš„åŒæ—¶ï¼Œå±•ç¤ºäº†æ›´é«˜çš„å‡†ç¡®æ€§ã€Fåˆ†æ•°å’Œç‰¹å¼‚æ€§ã€‚æ­¤å¤–ï¼Œæœç´¢çš„é¡ºåºæ€§æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œä½¿ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ å®ç”¨ã€‚é€šè¿‡ç³»ç»Ÿç¡®å®šèåˆæˆåƒæ¨¡æ€çš„æœ€ä½³æ—¶æœºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºåŒ»å­¦æˆåƒä¸­çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æä¾›äº†å…ˆè¿›çš„ä¼˜åŒ–æ¡†æ¶ã€‚è¿™ä¸ºæ”¹è¿›ä¸´åºŠå†³ç­–åˆ¶å®šå’ŒåŒ»å­¦äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„æ›´çµæ´»ã€å¯æ‰©å±•æ¶æ„é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ï¼Œé€šè¿‡èåˆå¤šç§æˆåƒæ¨¡æ€ï¼ˆå¦‚MRIåºåˆ—ï¼‰æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚é’ˆå¯¹å¦‚ä½•ç¡®å®šèåˆæ¨¡æ€çš„æœ€ä¼˜æ—¶æœºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåºè´¯å‰å‘æœç´¢ç®—æ³•çš„å¤šæ¨¡æ€èåˆä¼˜åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒå±‚çš„å¤šæ¨¡æ€ç½‘ç»œä¸­é€æ­¥æ¿€æ´»å’Œè¯„ä¼°å€™é€‰èåˆæ¨¡å—ï¼Œé€šè¿‡æ¯”è¾ƒéªŒè¯æŸå¤±æ¥è¯†åˆ«æœ€ä½³é…ç½®ï¼Œæœ‰æ•ˆå‡å°‘æœç´¢ç©ºé—´ï¼Œé¿å…æ˜‚è´µçš„è®¡ç®—æˆæœ¬ã€‚åœ¨ä¸¤ç§å¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å‡ºä¼˜äºå•æ¨¡æ€åŸºçº¿ã€åæœŸèåˆåŠæ‰€æœ‰æ½œåœ¨èåˆæ”¾ç½®çš„æš´åŠ›ç»„åˆçš„é…ç½®ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€Få€¼å’Œç‰¹å¼‚æ€§ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›æˆ–æé«˜AUCå€¼ã€‚è¯¥æ–¹æ³•çš„ä¼˜åŒ–è¿‡ç¨‹æ›´ä¸ºå®ç”¨å’Œé«˜æ•ˆï¼Œä¸ºä¸´åºŠå†³ç­–åˆ¶å®šæä¾›äº†å…ˆè¿›çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¿ƒè¿›äº†åŒ»å­¦äººå·¥æ™ºèƒ½åº”ç”¨çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ é€šè¿‡èåˆå¤šç§æˆåƒæ¨¡æ€æé«˜åŒ»å­¦æˆåƒè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç¡®å®šèåˆæ¨¡æ€çš„æœ€ä¼˜æ—¶æœºæ˜¯é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ‰‹åŠ¨è°ƒæ•´æˆ–ç©·ä¸¾æœç´¢ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”æ— æ³•ä¿è¯æœ€ä¼˜ç»“æœã€‚</li>
<li>æå‡ºäº†åŸºäºåºè´¯å‰å‘æœç´¢ç®—æ³•çš„å¤šæ¨¡æ€èåˆä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘æœç´¢ç©ºé—´ï¼Œé«˜æ•ˆç¡®å®šæœ€ä¼˜èåˆæ—¶æœºã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§é…ç½®ä¸‹ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€Få€¼å’Œç‰¹å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e29d15f26a3bab015bc18de01cf6d5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9b8edb9e1ddb514cf0a2a5ee3af7d3d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Sampling-Kantorovich-operators-for-speckle-noise-reduction-using-a-Down-Up-scaling-approach-and-gap-filling-in-remote-sensing-images"><a href="#Sampling-Kantorovich-operators-for-speckle-noise-reduction-using-a-Down-Up-scaling-approach-and-gap-filling-in-remote-sensing-images" class="headerlink" title="Sampling Kantorovich operators for speckle noise reduction using a   Down-Up scaling approach and gap filling in remote sensing images"></a>Sampling Kantorovich operators for speckle noise reduction using a   Down-Up scaling approach and gap filling in remote sensing images</h2><p><strong>Authors:Danilo Costarelli, Mariarosaria Natale</strong></p>
<p>In the literature, several approaches have been proposed for restoring and enhancing remote sensing images, including methods based on interpolation, filtering, and deep learning. In this paper, we investigate the application of multivariate sampling Kantorovich (SK) operators for image reconstruction, with a particular focus on gap filling and speckle noise reduction. To understand the accuracy performances of the proposed algorithms, we first derive a quantitative estimate in $C(\R^n)$ for the error of approximation using the Euler-Maclaurin summation formula, which provides sharper error bounds under minimal regularity conditions. We also establish a convergence result and a quantitative estimate with respect to the dissimilarity index measured by the continuous SSIM for functions in Lebesgue spaces. Additionally, we prove a multidimensional linear prediction result, which is used to design a new SK-based reconstruction algorithm to handle missing data, that we call LP-SK algorithm. To address speckle noise, we integrate SK operators into a newly proposed Down-Up scaling approach. Numerical tests are presented on synthetic and real SAR images to validate the proposed methods. Performance is assessed using similarity metrics such as SSIM and PSNR, along with speckle-specific indexes. Comparative analysis with state-of-the-art techniques highlights the effectiveness of the proposed approaches. </p>
<blockquote>
<p>åœ¨æ–‡çŒ®ä¸­ï¼Œå·²ç»æå‡ºäº†å¤šç§æ¢å¤å’Œå¢å¼ºé¥æ„Ÿå›¾åƒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ’å€¼ã€æ»¤æ³¢å’Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤šå…ƒé‡‡æ ·Kantorovichï¼ˆSKï¼‰ç®—å­åœ¨å›¾åƒé‡å»ºä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«å…³æ³¨é—´éš™å¡«å……å’Œæ–‘ç‚¹å™ªå£°å‡å°‘ã€‚ä¸ºäº†äº†è§£æ‰€æå‡ºç®—æ³•çš„å‡†ç¡®æ€§èƒ½ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨Euler-Maclaurinæ±‚å’Œå…¬å¼å¯¹è¯¯å·®è¿›è¡Œå®šé‡ä¼°è®¡ï¼Œåœ¨æœ€å°çš„æ­£è§„æ€§æ¡ä»¶ä¸‹æä¾›æ›´ç²¾ç¡®çš„è¯¯å·®ç•Œé™ã€‚æˆ‘ä»¬è¿˜é’ˆå¯¹é»æ›¼ç©ºé—´ä¸­çš„å‡½æ•°å»ºç«‹äº†å…³äºè¿ç»­ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡çš„ä¸ç›¸ä¼¼æ€§æŒ‡æ•°çš„æ”¶æ•›æ€§å’Œå®šé‡ä¼°è®¡ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†å¤šç»´çº¿æ€§é¢„æµ‹ç»“æœï¼Œç”¨äºè®¾è®¡ä¸€ç§æ–°å‹çš„åŸºäºSKçš„é‡å»ºç®—æ³•æ¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºLP-SKç®—æ³•ã€‚ä¸ºäº†è§£å†³æ–‘ç‚¹å™ªå£°é—®é¢˜ï¼Œæˆ‘ä»¬å°†SKç®—å­é›†æˆåˆ°ä¸€ç§æ–°æå‡ºçš„è‡ªä¸‹è€Œä¸Šç¼©æ”¾æ–¹æ³•ä¸­ã€‚å¯¹åˆæˆå’ŒçœŸå®SARå›¾åƒè¿›è¡Œäº†æ•°å€¼æµ‹è¯•ï¼Œä»¥éªŒè¯æ‰€æå‡ºçš„æ–¹æ³•ã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä»¥åŠæ–‘ç‚¹ç‰¹å®šæŒ‡æ ‡ã€‚ä¸æœ€æ–°æŠ€æœ¯çš„å¯¹æ¯”åˆ†æçªæ˜¾äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02422v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†å¤šå…ƒé‡‡æ ·Kantorovichç®—å­åœ¨é¥æ„Ÿå›¾åƒé‡å»ºä¸­çš„åº”ç”¨ï¼Œé‡ç‚¹å…³æ³¨äº†æ•°æ®å¡«å……å’Œæ–‘ç‚¹å™ªå£°é™ä½ã€‚ç ”ç©¶å†…å®¹åŒ…æ‹¬è¯¯å·®ä¼°è®¡ã€æ”¶æ•›æ€§ç»“æœã€åŸºäºè¿ç»­SSIMçš„å‹’è´æ ¼ç©ºé—´ä¸­çš„å®šé‡è¯„ä¼°ï¼Œä»¥åŠç”¨äºå¤„ç†ç¼ºå¤±æ•°æ®çš„æ–°SKé‡å»ºç®—æ³•è®¾è®¡ã€‚åŒæ—¶ï¼Œç ”ç©¶å°†SKç®—å­ä¸æ–°çš„Down-Upç¼©æ”¾æ–¹æ³•ç›¸ç»“åˆä»¥è§£å†³æ–‘ç‚¹å™ªå£°é—®é¢˜ã€‚é€šè¿‡åˆæˆå’ŒçœŸå®SARå›¾åƒçš„æ•°å€¼æµ‹è¯•éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸æœ€æ–°æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºä½¿ç”¨å¤šå…ƒé‡‡æ ·Kantorovichï¼ˆSKï¼‰ç®—å­è¿›è¡Œé¥æ„Ÿå›¾åƒé‡å»ºã€‚</li>
<li>é‡ç‚¹å…³æ³¨æ•°æ®å¡«å……å’Œæ–‘ç‚¹å™ªå£°é™ä½é—®é¢˜ã€‚</li>
<li>é€šè¿‡Euler-Maclaurinæ±‚å’Œå…¬å¼è¿›è¡Œè¯¯å·®ä¼°è®¡ï¼Œå¹¶æä¾›äº†æ›´ç²¾ç¡®çš„è¯¯å·®è¾¹ç•Œã€‚</li>
<li>å»ºç«‹äº†å…³äºè¿ç»­SSIMçš„æ”¶æ•›æ€§å’Œå®šé‡è¯„ä¼°ç»“æœã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§æ–°çš„åŸºäºSKç®—å­çš„é‡å»ºç®—æ³•ï¼ˆLP-SKç®—æ³•ï¼‰æ¥å¤„ç†ç¼ºå¤±æ•°æ®ã€‚</li>
<li>å°†SKç®—å­ä¸Down-Upç¼©æ”¾æ–¹æ³•ç»“åˆï¼Œä»¥è§£å†³æ–‘ç‚¹å™ªå£°é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0ceef143a9fd6e24e03f3fb09c8d6e3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Regression-is-all-you-need-for-medical-image-translation"><a href="#Regression-is-all-you-need-for-medical-image-translation" class="headerlink" title="Regression is all you need for medical image translation"></a>Regression is all you need for medical image translation</h2><p><strong>Authors:Sebastian Rassmann, David KÃ¼gler, Christian Ewert, Martin Reuter</strong></p>
<p>The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œåœ¨æœ‰é™çš„æ—¶é—´é¢„ç®—å†…è·å–ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒè‡³å…³é‡è¦ã€‚åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆMITï¼‰å¯ä»¥é€šè¿‡ä»è·å–çš„æ•°æ®ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºå’Œè¡¥å……ç°æœ‰çš„æ•°æ®é›†ã€‚è™½ç„¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨è‡ªç„¶å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå®ƒä»¬åœ¨åŒ»å­¦åº”ç”¨ä¸­å¹¶ä¸ä¸€å®šèƒ½å‘æŒ¥å‡ºåˆ›é€ æ€§å’Œå›¾åƒçœŸå®æ€§çš„ä¼˜åŠ¿ï¼Œå› ä¸ºåŒ»å­¦åº”ç”¨éœ€è¦é«˜åº¦å‡†ç¡®çš„è§£å‰–ä¿¡æ¯ã€‚äº‹å®ä¸Šï¼Œæ¨¡ä»¿è·å–å™ªå£°æˆ–å†…å®¹å¹»è§‰ä¼šé˜»ç¢å…¶åœ¨ä¸´åºŠä¸Šçš„å®ç”¨æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†YODAï¼ˆYou Only Denoise once - or Averageï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„åŸºäº2.5Dæ‰©æ•£çš„ä½“ç§¯åŒ»å­¦å›¾åƒç¿»è¯‘æ¡†æ¶ã€‚YODAç»“åˆäº†æ‰©æ•£å’Œå›å½’èŒƒå¼ï¼Œä»¥äº§ç”Ÿé€¼çœŸçš„æˆ–æ— å™ªå£°çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æœŸæœ›è¿‘ä¼¼ï¼ˆExpAï¼‰DMé‡‡æ ·ï¼Œå®ƒå—åˆ°MRIä¿¡å·å¹³å‡çš„å¯å‘ã€‚ExpAé‡‡æ ·æŠ‘åˆ¶ç”Ÿæˆçš„å™ªå£°ï¼Œä»è€Œæ¶ˆé™¤å™ªå£°å¯¹å›¾åƒè´¨é‡è¯„ä¼°çš„åè§ã€‚é€šè¿‡åœ¨å››ä¸ªä¸åŒçš„å¤šæ¨¡å¼æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒï¼ˆåŒ…æ‹¬å¤šå¯¹æ¯”åº¦è„‘éƒ¨MRIå’Œç›†è…”MRI-CTï¼‰ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰©æ•£é‡‡æ ·å’Œå›å½’é‡‡æ ·åœ¨å®è·µä¸­å¯ä»¥å¾—åˆ°ç±»ä¼¼çš„ç»“æœã€‚å› æ­¤ï¼Œæ‰©æ•£é‡‡æ ·åœ¨è®¡ç®—ä¸Šçš„å¼€é”€å¹¶æ²¡æœ‰åœ¨åŒ»å­¦ä¿¡æ¯ç¿»è¯‘ä¸­æä¾›ç³»ç»Ÿæ€§çš„ä¼˜åŠ¿ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬è¯æ˜äº†YODAä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„GANå’ŒDMæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒYODAç”Ÿæˆçš„å›¾åƒè¢«è¯æ˜å¯ä»¥æ›¿æ¢æˆ–ç”šè‡³ä¼˜äºæŸäº›ä¸‹æ¸¸ä»»åŠ¡çš„ç‰©ç†é‡‡é›†ã€‚æˆ‘ä»¬çš„ç ”ç©¶æŒ‘æˆ˜äº†DMåœ¨MITä¸­çš„æ—¢å®šä¼˜åŠ¿ï¼Œä¸ºåŒ»å­¦æˆåƒä¸­MITçš„å®é™…åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02048v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦ä¿¡æ¯æˆåƒä¸­ï¼Œåœ¨æœ‰é™çš„æ—¶é—´é¢„ç®—å†…è·å–ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒè‡³å…³é‡è¦ã€‚åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆMITï¼‰å¯ä»¥é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºå’Œè¡¥å……ç°æœ‰æ•°æ®é›†ã€‚è™½ç„¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨è‡ªç„¶å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶åœ¨åŒ»å­¦åº”ç”¨ä¸­å´å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºåŒ»å­¦åº”ç”¨è¦æ±‚é«˜åº¦çš„è§£å‰–å‡†ç¡®æ€§ã€‚äº‹å®ä¸Šï¼Œå¯¹é‡‡é›†å™ªå£°çš„æ¨¡ä»¿æˆ–å†…å®¹å¹»è§‰é˜»ç¢äº†å…¶åœ¨ä¸´åºŠä¸Šçš„æ•ˆç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„2.5Dæ‰©æ•£å¼åŒ»å­¦å›¾åƒç¿»è¯‘æ¡†æ¶YODAï¼ˆYou Only Denoise once - or Averageï¼‰ã€‚YODAç»“åˆäº†æ‰©æ•£å’Œå›å½’èŒƒå¼ï¼Œç”Ÿæˆé€¼çœŸçš„æˆ–æ— å™ªå£°çš„è¾“å‡ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æœŸæœ›è¿‘ä¼¼ï¼ˆExpAï¼‰DMé‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»MRIä¿¡å·å¹³å‡ä¸­è·å¾—çµæ„Ÿã€‚ExpAé‡‡æ ·æŠ‘åˆ¶ç”Ÿæˆå™ªå£°ï¼Œä»è€Œæ¶ˆé™¤å™ªå£°å¯¹å›¾åƒè´¨é‡è¯„ä¼°çš„åè§ã€‚åœ¨åŒ…å«å¤šå¯¹æ¯”åº¦è„‘éƒ¨MRIå’Œç›†è…”MRI-CTçš„å››ä¸ªä¸åŒå¤šæ¨¡å¼æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£é‡‡æ ·å’Œå›å½’é‡‡æ ·åœ¨å®è·µä¸­äº§ç”Ÿç›¸ä¼¼ç»“æœã€‚å› æ­¤ï¼Œæ‰©æ•£é‡‡æ ·åœ¨åŒ»å­¦ä¿¡æ¯ç¿»è¯‘ä¸­çš„è®¡ç®—å¼€é”€å¹¶æœªå¸¦æ¥ç³»ç»Ÿæ€§ä¼˜åŠ¿ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬è¯æ˜YODAä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„GANå’ŒDMæ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒYODAç”Ÿæˆçš„å›¾åƒè¢«è¯æ˜å¯ä¸ç‰©ç†é‡‡é›†äº’æ¢ï¼Œç”šè‡³åœ¨æŸäº›ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†DMåœ¨MITä¸­çš„å‡è®¾ä¼˜åŠ¿ï¼Œä¸ºåŒ»å­¦æˆåƒä¸­çš„MITå®é™…åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒä¸­ï¼Œæœ‰é™æ—¶é—´å†…è·å–ä¸°å¯Œä¿¡æ¯çš„å›¾åƒè‡³å…³é‡è¦ã€‚</li>
<li>åŒ»å­¦å›¾åƒç¿»è¯‘ï¼ˆMITï¼‰å¯ä»¥é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒå¢å¼ºå’Œè¡¥å……ç°æœ‰æ•°æ®é›†ã€‚</li>
<li>è™½ç„¶GANså’ŒDMsåœ¨è‡ªç„¶å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦åº”ç”¨ä¸­å¹¶ä¸æ€»èƒ½ä¿æŒåŒæ ·çš„ä¼˜åŠ¿ï¼Œå› ä¸ºåŒ»å­¦åº”ç”¨è¦æ±‚å›¾åƒçš„è§£å‰–ä¿¡æ¯é«˜åº¦å‡†ç¡®ã€‚</li>
<li>æå‡ºçš„YODAæ¡†æ¶ç»“åˆäº†æ‰©æ•£å’Œå›å½’èŒƒå¼ï¼Œç”Ÿæˆé€¼çœŸçš„æˆ–æ— å™ªå£°çš„è¾“å‡ºã€‚</li>
<li>ExpA-samplingæ–¹æ³•èƒ½ä»MRIä¿¡å·å¹³å‡ä¸­è·å¾—çµæ„Ÿï¼Œæœ‰æ•ˆæŠ‘åˆ¶ç”Ÿæˆå™ªå£°ã€‚</li>
<li>æ‰©æ•£é‡‡æ ·ä¸å›å½’é‡‡æ ·åœ¨å®è·µä¸­æ•ˆæœç›¸ä¼¼ï¼Œè®¡ç®—å¼€é”€æœªå¸¦æ¥æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc818b766647a276f2f1efb52443784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b16f1076d38362ca15bb7c57631877.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2c6add45f6ea928640fb40a54ae5a99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c30b220f82e9fd04a8b2200abd58ad6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e88204effc0f36fcc485522b9d2b47e7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Quantum-Enhanced-Classification-of-Brain-Tumors-Using-DNA-Microarray-Gene-Expression-Profiles"><a href="#Quantum-Enhanced-Classification-of-Brain-Tumors-Using-DNA-Microarray-Gene-Expression-Profiles" class="headerlink" title="Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray   Gene Expression Profiles"></a>Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray   Gene Expression Profiles</h2><p><strong>Authors:Emine Akpinar, Batuhan Hangun, Murat Oduncuoglu, Oguz Altun, Onder Eyecioglu, Zeynel Yalcin</strong></p>
<p>DNA microarray technology enables the simultaneous measurement of expression levels of thousands of genes, thereby facilitating the understanding of the molecular mechanisms underlying complex diseases such as brain tumors and the identification of diagnostic genetic signatures. To derive meaningful biological insights from the high-dimensional and complex gene features obtained through this technology and to analyze gene properties in detail, classical AI-based approaches such as machine learning and deep learning are widely employed. However, these methods face various limitations in managing high-dimensional vector spaces and modeling the intricate relationships among genes. In particular, challenges such as hyperparameter tuning, computational costs, and high processing power requirements can hinder their efficiency. To overcome these limitations, quantum computing and quantum AI approaches are gaining increasing attention. Leveraging quantum properties such as superposition and entanglement, quantum methods enable more efficient parallel processing of high-dimensional data and offer faster and more effective solutions to problems that are computationally demanding for classical methods. In this study, a novel model called â€œDeep VQCâ€ is proposed, based on the Variational Quantum Classifier approach. Developed using microarray data containing 54,676 gene features, the model successfully classified four different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and pilocytic astrocytoma-alongside healthy samples with high accuracy. Furthermore, compared to classical ML algorithms, our model demonstrated either superior or comparable classification performance. These results highlight the potential of quantum AI methods as an effective and promising approach for the analysis and classification of complex structures such as brain tumors based on gene expression features. </p>
<blockquote>
<p>DNAå¾®é˜µåˆ—æŠ€æœ¯å¯ä»¥åŒæ—¶æµ‹é‡æ•°åƒä¸ªåŸºå› çš„è¡¨è¾¾æ°´å¹³ï¼Œä»è€Œæœ‰åŠ©äºäº†è§£è„‘è‚¿ç˜¤ç­‰å¤æ‚ç–¾ç—…çš„åˆ†å­æœºåˆ¶ï¼Œå¹¶ç¡®å®šè¯Šæ–­åŸºå› ç‰¹å¾ã€‚ä¸ºäº†ä»è¯¥æŠ€æœ¯è·å¾—çš„é«˜ç»´å’Œå¤æ‚çš„åŸºå› ç‰¹å¾ä¸­å¾—å‡ºæœ‰æ„ä¹‰çš„ç”Ÿç‰©å­¦è§è§£ï¼Œå¹¶è¯¦ç»†åˆ†æåŸºå› å±æ€§ï¼Œå¹¿æ³›é‡‡ç”¨äº†åŸºäºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰ç»å…¸çš„äººå·¥æ™ºèƒ½æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†é«˜ç»´å‘é‡ç©ºé—´ä»¥åŠå»ºæ¨¡åŸºå› ä¹‹é—´å¤æ‚å…³ç³»æ–¹é¢å­˜åœ¨å„ç§å±€é™æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¶…å‚æ•°è°ƒæ•´ã€è®¡ç®—æˆæœ¬å’Œé«˜åº¦å¤„ç†åŠŸç‡è¦æ±‚ç­‰æŒ‘æˆ˜å¯èƒ½ä¼šé˜»ç¢å…¶æ•ˆç‡ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œé‡å­è®¡ç®—å’Œé‡å­äººå·¥æ™ºèƒ½æ–¹æ³•è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚åˆ©ç”¨é‡å­å åŠ å’Œçº ç¼ ç­‰é‡å­å±æ€§ï¼Œé‡å­æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¹¶è¡Œå¤„ç†é«˜ç»´æ•°æ®ï¼Œå¹¶ä¸ºç»å…¸æ–¹æ³•è®¡ç®—ä¸Šéš¾ä»¥å¤„ç†çš„é—®é¢˜æä¾›æ›´å¿«é€Ÿå’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†é‡å­åˆ†ç±»å™¨æ–¹æ³•çš„æ–°å‹æ¨¡å‹ï¼Œåä¸ºâ€œDeep VQCâ€ã€‚è¯¥æ¨¡å‹ä½¿ç”¨åŒ…å«54676ä¸ªåŸºå› ç‰¹å¾çš„å¾®é˜µåˆ—æ•°æ®å¼€å‘è€Œæˆï¼ŒæˆåŠŸåœ°å°†å››ç§ä¸åŒç±»å‹çš„è„‘è‚¿ç˜¤ï¼ˆå®¤ç®¡è†œç˜¤ã€èƒ¶è´¨æ¯ç»†èƒç˜¤ã€é«“æ¯ç»†èƒç˜¤å’Œæ˜Ÿå½¢ç»†èƒç˜¤ï¼‰ä»¥åŠå¥åº·æ ·æœ¬è¿›è¡Œäº†é«˜å‡†ç¡®åº¦çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œä¸ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºä¼˜è¶Šæˆ–ç›¸å½“çš„åˆ†ç±»æ€§èƒ½ã€‚è¿™äº›ç»“æœçªå‡ºäº†é‡å­äººå·¥æ™ºèƒ½æ–¹æ³•åœ¨è„‘è‚¿ç˜¤ç­‰å¤æ‚ç»“æ„çš„åˆ†æå’Œåˆ†ç±»ä¸­çš„æ½œåŠ›å’Œå‰æ™¯ï¼Œæœ‰æœ›æˆä¸ºä¸€ç§åŸºäºåŸºå› è¡¨è¾¾ç‰¹å¾çš„æœ‰æ•ˆä¸”å‰æ™¯å¹¿é˜”çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02033v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DNAå¾®é˜µåˆ—æŠ€æœ¯åœ¨åŸºå› è¡¨è¾¾æ°´å¹³æ£€æµ‹æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨ç†è§£å¤æ‚ç–¾ç—…å¦‚è„‘è‚¿ç˜¤åˆ†å­æœºåˆ¶å’Œè¯Šæ–­é—ä¼ ç‰¹å¾æ–¹é¢çš„ä½œç”¨ã€‚ä¸ºäº†ä»è¯¥æŠ€æœ¯è·å¾—çš„é«˜ç»´å¤æ‚åŸºå› ç‰¹å¾ä¸­è·å–æœ‰æ„ä¹‰çš„ç”Ÿç‰©å­¦è§è§£å¹¶è¯¦ç»†åˆ†æåŸºå› å±æ€§ï¼Œæ™®éé‡‡ç”¨åŸºäºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰å¤å…¸äººå·¥æ™ºèƒ½æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨é«˜ç»´å‘é‡ç©ºé—´ç®¡ç†å’ŒåŸºå› é—´å¤æ‚å…³ç³»å»ºæ¨¡æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºæ­¤ï¼Œé‡å­è®¡ç®—å’Œé‡å­äººå·¥æ™ºèƒ½æ–¹æ³•æ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†é‡å­åˆ†ç±»å™¨çš„æ–°å‹æ¨¡å‹â€œDeep VQCâ€ï¼Œè¯¥æ¨¡å‹æˆåŠŸåˆ©ç”¨å¾®é˜µåˆ—æ•°æ®å¯¹å››ç§ä¸åŒç±»å‹çš„è„‘è‚¿ç˜¤è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¡¨ç°å‡ºä¼˜è¶Šæˆ–ç›¸å½“çš„æ€§èƒ½ã€‚è¿™çªæ˜¾äº†é‡å­äººå·¥æ™ºèƒ½æ–¹æ³•åœ¨è„‘è‚¿ç˜¤åˆ†æå’Œåˆ†ç±»ä¸­çš„æ½œåŠ›å’Œå‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DNAå¾®é˜µåˆ—æŠ€æœ¯èƒ½åŒæ—¶æ£€æµ‹æ•°åƒä¸ªåŸºå› çš„è¡¨è¾¾æ°´å¹³ï¼Œæœ‰åŠ©äºç†è§£å¤æ‚ç–¾ç—…çš„åˆ†å­æœºåˆ¶å’Œè¯Šæ–­é—ä¼ ç‰¹å¾ã€‚</li>
<li>æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç­‰å¤å…¸äººå·¥æ™ºèƒ½æ–¹æ³•å¹¿æ³›åº”ç”¨äºåˆ†æåŸºå› å±æ€§ï¼Œä½†åœ¨å¤„ç†é«˜ç»´æ•°æ®å’Œå»ºæ¨¡å¤æ‚å…³ç³»æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>é‡å­è®¡ç®—å’Œé‡å­äººå·¥æ™ºèƒ½æ–¹æ³•å…·å¤‡å¤„ç†é«˜ç»´æ•°æ®çš„æ½œåŠ›ï¼Œå¹¶èƒ½åœ¨è§£å†³è®¡ç®—å¯†é›†å‹é—®é¢˜ä¸Šæä¾›æ›´é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>â€œDeep VQCâ€æ¨¡å‹æˆåŠŸåˆ©ç”¨å¾®é˜µåˆ—æ•°æ®å¯¹è„‘è‚¿ç˜¤è¿›è¡Œåˆ†ç±»ï¼Œè¡¨ç°å‡ºä¼˜è¶Šæˆ–ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>æ­¤ç ”ç©¶çªæ˜¾äº†é‡å­äººå·¥æ™ºèƒ½åœ¨åˆ†æå’Œåˆ†ç±»å¤æ‚ç»“æ„å¦‚è„‘è‚¿ç˜¤æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec4b9ca72efd4d7df2d516f477b8e204.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18722b6c6fdd8abff567e9871019a72a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05390939ea888cf824e225c4164863a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83ba9d8b143bb083fa9d327585520fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6d107991f506c60b514d125021597d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0a53b99204090feb8cdffe90cf48f51.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Lifelong-Whole-Slide-Image-Analysis-Online-Vision-Language-Adaptation-and-Past-to-Present-Gradient-Distillation"><a href="#Lifelong-Whole-Slide-Image-Analysis-Online-Vision-Language-Adaptation-and-Past-to-Present-Gradient-Distillation" class="headerlink" title="Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation   and Past-to-Present Gradient Distillation"></a>Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation   and Past-to-Present Gradient Distillation</h2><p><strong>Authors:Doanh C. Bui, Hoai Luan Pham, Vu Trung Duong Le, Tuan Hai Vu, Van Duy Tran, Khang Nguyen, Yasuhiko Nakashima</strong></p>
<p>Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. However, the rapid growth of computational tasks involving WSIs poses significant challenges. Given that WSIs are gigapixels in size, they present difficulties in terms of storage, processing, and model training. Therefore, it is essential to develop lifelong learning approaches for WSI analysis. In scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. In this study, we introduce ADaFGrad, a method designed to enhance lifelong learning for whole-slide image (WSI) analysis. First, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slideâ€™s regional tissue features and a predefined text-based prototype buffer. Additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. We construct a sequence of six TCGA datasets for training and evaluation. Experimental results show that ADaFGrad outperforms both state-of-the-art WSI-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules. </p>
<blockquote>
<p>å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWhole Slide Imagesï¼ŒWSIsï¼‰åœ¨å‡†ç¡®çš„ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæä¾›ç»†èƒå±‚é¢çš„ç»„ç»‡ç»†èŠ‚ã€‚ç„¶è€Œï¼Œæ¶‰åŠWSIsçš„è®¡ç®—ä»»åŠ¡çš„å¿«é€Ÿå¢é•¿å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚é‰´äºWSIsçš„åƒå…†åƒç´ å¤§å°ï¼Œå®ƒä»¬åœ¨å­˜å‚¨ã€å¤„ç†å’Œæ¨¡å‹è®­ç»ƒæ–¹é¢å­˜åœ¨å›°éš¾ã€‚å› æ­¤ï¼Œå¼€å‘ç”¨äºWSIåˆ†æçš„ç»ˆèº«å­¦ä¹ æ–¹æ³•è‡³å…³é‡è¦ã€‚åœ¨å¹»ç¯ç‰‡ï¼ˆslidesï¼‰åˆ†å¸ƒåœ¨å¤šä¸ªæœºæ„çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ©ç”¨å®ƒä»¬æ¥å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„åœ¨çº¿æ¨¡å‹ï¼Œä½œä¸ºä¸´åºŠå’ŒåŒ»é™¢ç¯å¢ƒä¸­ç™Œç—‡è¯Šæ–­çš„è®¡ç®—å·¥å…·ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ADaFGradæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨æé«˜å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æçš„ç»ˆèº«å­¦ä¹ æ•ˆèƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨ç—…ç†è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹æ¥æ„å»ºä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å®ç°å¯¹å¹»ç¯ç‰‡åŒºåŸŸç»„ç»‡ç‰¹å¾ä¸åŸºäºæ–‡æœ¬çš„é¢„å…ˆå®šä¹‰åŸå‹ç¼“å†²åŒºä¹‹é—´çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¢¯åº¦è’¸é¦æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ¨¡ä»¿äº†è¿ç»­å­¦ä¹ ç¯å¢ƒä¸­è¿‡å»è¿­ä»£å’Œå½“å‰è¿­ä»£ä¹‹é—´çš„åˆ†ç±»å¤´å‚æ•°çš„æ¢¯åº¦å¯¹æ•°ä¼¼ç„¶åº¦ã€‚æˆ‘ä»¬æ„å»ºäº†å…­ä¸ªTCGAæ•°æ®é›†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»…ç»è¿‡å‡ ä¸ªè®­ç»ƒå‘¨æœŸåï¼ŒADaFGradåœ¨ç±»å¢é‡å­¦ä¹ åœºæ™¯ä¸­è¶…è¶Šäº†æœ€å…ˆè¿›çš„WSIç‰¹å®šæ–¹æ³•å’Œä¼ ç»Ÿçš„æŒç»­å­¦ä¹ æ–¹æ³•ï¼Œé«˜å‡º+5.068%ï¼ŒåŒæ—¶å±•ç°å‡ºæœ€å°‘çš„é—å¿˜ï¼ˆå³ä¿ç•™æœ€å¤šçš„å…ˆå‰ä»»åŠ¡çŸ¥è¯†ï¼‰ã€‚æ­¤å¤–ï¼ŒADaFGradçš„å‡†ç¡®ç‡æ¯”åŸºçº¿é«˜å‡º+40.084%ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†æ‰€æå‡ºæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01984v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…¨æ»‘å›¾åƒï¼ˆWSIsï¼‰åœ¨ç™Œç—‡è¯Šæ–­å’Œé¢„åä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œå®ƒä»¬ä»¥ç»†èƒå±‚é¢æä¾›ç»„ç»‡ç»†èŠ‚ã€‚ç„¶è€Œï¼Œæ¶‰åŠWSIsçš„è®¡ç®—ä»»åŠ¡çš„å¿«é€Ÿå¢é•¿å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚WSIsçš„åƒå…†åƒç´ å¤§å°å¯¼è‡´å­˜å‚¨ã€å¤„ç†å’Œæ¨¡å‹è®­ç»ƒæ–¹é¢çš„å›°éš¾ã€‚å› æ­¤ï¼Œå¼€å‘WSIåˆ†æçš„ç»ˆèº«å­¦ä¹ æ–¹æ³•æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨å¹»ç¯ç‰‡åˆ†å¸ƒåœ¨å¤šä¸ªæœºæ„çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨å®ƒä»¬å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„åœ¨çº¿æ¨¡å‹ï¼Œä½œä¸ºä¸´åºŠå’ŒåŒ»é™¢ç¯å¢ƒä¸­ç™Œç—‡è¯Šæ–­çš„è®¡ç®—å·¥å…·ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ADaFGradæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºå…¨æ»‘å›¾åƒï¼ˆWSIï¼‰åˆ†æçš„ç»ˆèº«å­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨ç—…ç†å­¦è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œå»ºç«‹ä¸€ä¸ªæ¡†æ¶ï¼Œä½¿å¹»ç¯ç‰‡çš„åŒºåŸŸç»„ç»‡ç‰¹å¾ä¸åŸºäºæ–‡æœ¬çš„åŸå‹ç¼“å†²åŒºèƒ½å¤Ÿè¿›è¡Œäº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¢¯åº¦è’¸é¦æœºåˆ¶ï¼Œå®ƒæ¨¡ä»¿äº†è¿ç»­å­¦ä¹ ç¯å¢ƒä¸­è¿‡å»è¿­ä»£å’Œå½“å‰è¿­ä»£åˆ†ç±»å¤´å‚æ•°æ¢¯åº¦çš„å¯¹æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨å…­ä¸ªTCGAæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒADaFGradåœ¨ä»…ç»è¿‡å‡ ä¸ªè®­ç»ƒå‘¨æœŸåï¼Œåœ¨ç±»å¢é‡å­¦ä¹ åœºæ™¯ä¸­ä¼˜äºæœ€æ–°çš„WSIç‰¹å®šå’Œä¼ ç»ŸæŒç»­å­¦ä¹ æ–¹æ³•ï¼Œé«˜å‡ºæœ€å¤š+5.068%ï¼Œå¹¶ä¸”é—å¿˜æœ€å°‘ï¼ˆå³ä¿ç•™æœ€å¤šçš„å…ˆå‰ä»»åŠ¡çŸ¥è¯†ï¼‰ã€‚æ­¤å¤–ï¼ŒADaFGradçš„å‡†ç¡®ç‡æ¯”åŸºçº¿é«˜å‡º+40.084%ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†æ‰€æå‡ºæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…¨æ»‘å›¾åƒï¼ˆWSIsï¼‰åœ¨ç™Œç—‡è¯Šæ–­å’Œé¢„åä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¯¹è®¡ç®—ä»»åŠ¡æå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>WSIsçš„åƒå…†åƒç´ å¤§å°å¸¦æ¥äº†å­˜å‚¨ã€å¤„ç†å’Œæ¨¡å‹è®­ç»ƒçš„éš¾é¢˜ã€‚</li>
<li>å¼€å‘äº†ADaFGradæ–¹æ³•ï¼Œä»¥å¢å¼ºWSIåˆ†æçš„ç»ˆèº«å­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨ç—…ç†å­¦è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹å»ºç«‹æ¡†æ¶ï¼Œå®ç°å¹»ç¯ç‰‡åŒºåŸŸç»„ç»‡ç‰¹å¾ä¸æ–‡æœ¬åŸå‹ç¼“å†²åŒºçš„äº¤äº’ã€‚</li>
<li>æ¢¯åº¦è’¸é¦æœºåˆ¶æ¨¡ä»¿è¿ç»­å­¦ä¹ ç¯å¢ƒä¸­æ¢¯åº¦çš„å˜åŒ–ã€‚</li>
<li>ADaFGradåœ¨ç±»å¢é‡å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72c449c6cb389897c9d533a9d6d0629a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a5646a4fb028bccff04e6f8c2a5b122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d8428131ebb19d8aa831b8475673750.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b815509f8a715fb42096ac363091b9d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c545e512d4eb459afbb10625f1ad2cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2ad5ba337234ff2e94d85ed3596abe9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UNet-3D-with-Adaptive-TverskyCE-Loss-for-Pancreas-Medical-Image-Segmentation"><a href="#UNet-3D-with-Adaptive-TverskyCE-Loss-for-Pancreas-Medical-Image-Segmentation" class="headerlink" title="UNet-3D with Adaptive TverskyCE Loss for Pancreas Medical Image   Segmentation"></a>UNet-3D with Adaptive TverskyCE Loss for Pancreas Medical Image   Segmentation</h2><p><strong>Authors:Xubei Zhang, Mikhail Y. Shalaginov, Tingying Helen Zeng</strong></p>
<p>Pancreatic cancer, which has a low survival rate, is the most intractable one among all cancers. Most diagnoses of this cancer heavily depend on abdominal computed tomography (CT) scans. Therefore, pancreas segmentation is crucial but challenging. Because of the obscure position of the pancreas, surrounded by other large organs, and its small area, the pancreas has often been impeded and difficult to detect. With these challenges , the segmentation results based on Deep Learning (DL) models still need to be improved. In this research, we propose a novel adaptive TverskyCE loss for DL model training, which combines Tversky loss with cross-entropy loss using learnable weights. Our method enables the model to adjust the loss contribution automatically and find the best objective function during training. All experiments were conducted on the National Institutes of Health (NIH) Pancreas-CT dataset. We evaluated the adaptive TverskyCE loss on the UNet-3D and Dilated UNet-3D, and our method achieved a Dice Similarity Coefficient (DSC) of 85.59%, with peak performance up to 95.24%, and the score of 85.14%. DSC and the score were improved by 9.47% and 8.98% respectively compared with the baseline UNet-3D with Tversky loss for pancreas segmentation.   Keywords: Pancreas segmentation, Tversky loss, Cross-entropy loss, UNet-3D, Dilated UNet-3D </p>
<blockquote>
<p>èƒ°è…ºç™Œæ˜¯æ¶æ€§ç¨‹åº¦æé«˜çš„ä¸€ç§ç™Œç—‡ï¼Œåœ¨æ‰€æœ‰ç™Œç—‡ä¸­æœ€ä¸ºæ£˜æ‰‹ã€‚å…¶è¯Šæ–­å¤§å¤šä¾èµ–äºè…¹éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTæ‰«æï¼‰ã€‚å› æ­¤ï¼Œèƒ°è…ºåˆ†å‰²è‡³å…³é‡è¦ä¸”æå…·æŒ‘æˆ˜æ€§ã€‚èƒ°è…ºä½ç½®éšè”½ï¼Œè¢«å…¶ä»–å¤§å‹å™¨å®˜åŒ…å›´ï¼Œä¸”é¢ç§¯è¾ƒå°ï¼Œå› æ­¤ç»å¸¸å—åˆ°é˜»ç¢ï¼Œéš¾ä»¥æ£€æµ‹ã€‚é¢å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²ç»“æœä»éœ€æ”¹è¿›ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªé€‚åº”TverskyCEæŸå¤±ï¼Œç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼Œå®ƒå°†TverskyæŸå¤±ä¸äº¤å‰ç†µæŸå¤±ç›¸ç»“åˆï¼Œä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´æŸå¤±çš„è´¡çŒ®ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰¾åˆ°æœ€ä½³ç›®æ ‡å‡½æ•°ã€‚æ‰€æœ‰å®éªŒå‡åœ¨å›½ç«‹å«ç”Ÿç ”ç©¶é™¢çš„èƒ°è…ºCTæ•°æ®é›†ä¸Šè¿›è¡Œã€‚æˆ‘ä»¬å¯¹UNet-3Då’ŒDilated UNet-3Dæ¨¡å‹è¯„ä¼°äº†è‡ªé€‚åº”TverskyCEæŸå¤±ï¼Œè¯¥æ–¹æ³•å®ç°äº†85.59%çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ï¼Œæœ€é«˜æ€§èƒ½è¾¾åˆ°95.24%ï¼Œè¯„åˆ†ä¸º85.14%ã€‚ä¸åŸºçº¿UNet-3Dæ¨¡å‹ä½¿ç”¨TverskyæŸå¤±è¿›è¡Œèƒ°è…ºåˆ†å‰²ç›¸æ¯”ï¼ŒDSCå’Œè¯„åˆ†åˆ†åˆ«æé«˜äº†9.47%å’Œ8.98%ã€‚å…³é”®è¯ï¼šèƒ°è…ºåˆ†å‰²ã€TverskyæŸå¤±ã€äº¤å‰ç†µæŸå¤±ã€UNet-3Dã€è†¨èƒ€UNet-3Dã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01951v1">PDF</a> 6 pages and 3 figures</p>
<p><strong>Summary</strong><br>èƒ°è…ºç™Œæ˜¯ä¸€ç§éš¾ä»¥æ²»æ„ˆä¸”ç”Ÿå­˜ç‡è¾ƒä½çš„ç™Œç—‡ï¼Œå…¶è¯Šæ–­é«˜åº¦ä¾èµ–äºè…¹éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTæ‰«æï¼‰ã€‚èƒ°è…ºåˆ†å‰²è‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå…¶ä½ç½®éšè”½ä¸”é¢ç§¯å°ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹è‡ªé€‚åº”TverskyCEæŸå¤±å‡½æ•°ï¼Œç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼Œç»“åˆäº†TverskyæŸå¤±å’Œäº¤å‰ç†µæŸå¤±ï¼Œå¹¶è‡ªåŠ¨è°ƒæ•´æŸå¤±è´¡çŒ®ï¼Œæ‰¾åˆ°æœ€ä½³ç›®æ ‡å‡½æ•°ã€‚åœ¨NIHèƒ°è…ºCTæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œä½¿ç”¨UNet-3Då’Œè†¨èƒ€UNet-3Dè¯„ä¼°è‡ªé€‚åº”TverskyCEæŸå¤±ï¼Œå®ç°äº†85.59%çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ï¼Œæœ€é«˜æ€§èƒ½è¾¾åˆ°95.24%ï¼Œè¾ƒåŸºçº¿UNet-3Dä½¿ç”¨TverskyæŸå¤±è¿›è¡Œèƒ°è…ºåˆ†å‰²çš„æ€§èƒ½æå‡åˆ†åˆ«ä¸ºDSCå’Œå¾—åˆ†æé«˜9.47%å’Œæé«˜äº†æ•´ä½“çš„æ¨¡å‹è¡¨ç°ã€‚è‡ªé€‚åº”TverskyCEæŸå¤±æœ‰åŠ©äºæ”¹è¿›èƒ°è…ºåˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ°è…ºç™Œè¯Šæ–­é«˜åº¦ä¾èµ–è…¹éƒ¨CTæ‰«æï¼Œèƒ°è…ºåˆ†å‰²æ˜¯é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>èƒ°è…ºä½ç½®éšè”½ä¸”é¢ç§¯å°ï¼Œå¯¼è‡´åˆ†å‰²å›°éš¾ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹è‡ªé€‚åº”TverskyCEæŸå¤±å‡½æ•°ï¼Œç»“åˆTverskyæŸå¤±å’Œäº¤å‰ç†µæŸå¤±ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨è°ƒæ•´æŸå¤±è´¡çŒ®å¹¶æ‰¾åˆ°æœ€ä½³ç›®æ ‡å‡½æ•°ã€‚</li>
<li>åœ¨NIHèƒ°è…ºCTæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨è‡ªé€‚åº”TverskyCEæŸå¤±çš„UNet-3Då’Œè†¨èƒ€UNet-3Dæ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºDiceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰è¾¾åˆ°85.59%ï¼Œæœ€é«˜æ€§èƒ½ä¸º95.24%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5b05f8c58c2da1c1a34f28acae90df3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bcb15eb4d2274347588b0e955ec6713.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23628bad1dde77cfcf7b72d075b7e834.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66728c26c7a3f9e957bd0267a4299eae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aee7696fba98da0f86e1c42aef633f97.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c8a03ce8cd6e88a4e48df47b0b892da5.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  VITA-Audio Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8eafafb74494b75b1ab78a586a76cc1a.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  A Robust Monotonic Single-Index Model for Skewed and Heavy-Tailed Data   A Deep Neural Network Approach Applied to Periodontal Studies
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
