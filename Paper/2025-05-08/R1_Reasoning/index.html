<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Rational Retrieval Acts Leveraging Pragmatic Reasoning to Improve   Sparse Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-48da9301a867503c4e0f83f23e839187.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="Rational-Retrieval-Acts-Leveraging-Pragmatic-Reasoning-to-Improve-Sparse-Retrieval"><a href="#Rational-Retrieval-Acts-Leveraging-Pragmatic-Reasoning-to-Improve-Sparse-Retrieval" class="headerlink" title="Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve   Sparse Retrieval"></a>Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve   Sparse Retrieval</h2><p><strong>Authors:Arthur Satouf, Gabriel Ben Zenou, Benjamin Piwowarski, Habiboulaye Amadou Boubacar, Pablo Piantanida</strong></p>
<p>Current sparse neural information retrieval (IR) methods, and to a lesser extent more traditional models such as BM25, do not take into account the document collection and the complex interplay between different term weights when representing a single document. In this paper, we show how the Rational Speech Acts (RSA), a linguistics framework used to minimize the number of features to be communicated when identifying an object in a set, can be adapted to the IR case â€“ and in particular to the high number of potential features (here, tokens). RSA dynamically modulates token-document interactions by considering the influence of other documents in the dataset, better contrasting document representations. Experiments show that incorporating RSA consistently improves multiple sparse retrieval models and achieves state-of-the-art performance on out-of-domain datasets from the BEIR benchmark. <a target="_blank" rel="noopener" href="https://github.com/arthur-75/Rational-Retrieval-Acts">https://github.com/arthur-75/Rational-Retrieval-Acts</a> </p>
<blockquote>
<p>å½“å‰ç¨€ç–ç¥ç»ç½‘ç»œä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•ï¼Œä»¥åŠè¾ƒå°‘ç¨‹åº¦çš„æ›´ä¼ ç»Ÿçš„æ¨¡å‹ï¼ˆå¦‚BM25ï¼‰ï¼Œåœ¨è¡¨ç¤ºå•ä¸ªæ–‡æ¡£æ—¶å¹¶æœªè€ƒè™‘åˆ°æ–‡æ¡£é›†åˆå’Œä¸åŒæœ¯è¯­æƒé‡ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†ç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰è¿™ä¸€è¯­è¨€å­¦æ¡†æ¶é€‚åº”äºä¿¡æ¯æ£€ç´¢æ¡ˆä¾‹â€”â€”å°¤å…¶æ˜¯é€‚åº”äºå¤§é‡æ½œåœ¨ç‰¹å¾ï¼ˆåœ¨è¿™é‡Œä¸ºæ ‡è®°ï¼‰ã€‚RSAé€šè¿‡è€ƒè™‘æ•°æ®é›†ä¸­å…¶ä»–æ–‡æ¡£çš„å½±å“æ¥åŠ¨æ€è°ƒèŠ‚æ ‡è®°-æ–‡æ¡£äº¤äº’ï¼Œæ›´å¥½åœ°å¯¹æ¯”æ–‡æ¡£è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œèå…¥RSAèƒ½å¤ŸæŒç»­æ”¹å–„å¤šç§ç¨€ç–æ£€ç´¢æ¨¡å‹ï¼Œå¹¶åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸­çš„è·¨åŸŸæ•°æ®é›†ä¸Šå®ç°æœ€æ–°æ€§èƒ½ã€‚è¯¦æƒ…è¯·å‚è§<a target="_blank" rel="noopener" href="https://github.com/arthur-75/Rational-Retrieval-Acts%E3%80%82">https://github.com/arthur-75/Rational-Retrieval-Actsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03676v1">PDF</a> 6 pages - 2 figures - conference: accepted at SIGIR 2025</p>
<p><strong>Summary</strong>ï¼šå½“å‰ç¨€ç–ç¥ç»ç½‘ç»œä¿¡æ¯æ£€ç´¢æ–¹æ³•ä¸ä¼ ç»Ÿæ¨¡å‹å¦‚BM25ç­‰ï¼Œåœ¨è¡¨ç¤ºå•ä¸ªæ–‡æ¡£æ—¶æœªè€ƒè™‘æ–‡æ¡£é›†åˆå’Œä¸åŒæœ¯è¯­æƒé‡ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•å°†ç”¨äºè¯†åˆ«é›†åˆä¸­å¯¹è±¡æ—¶çš„è¯­è¨€æ¡†æ¶ç†æ€§è¨€è¯­è¡Œä¸ºï¼ˆRSAï¼‰é€‚åº”äºä¿¡æ¯æ£€ç´¢åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¤§é‡æ½œåœ¨ç‰¹å¾ï¼ˆå³ä»¤ç‰Œï¼‰ã€‚RSAé€šè¿‡è€ƒè™‘æ•°æ®é›†ä¸­å…¶ä»–æ–‡æ¡£çš„å½±å“æ¥åŠ¨æ€è°ƒèŠ‚ä»¤ç‰Œä¸æ–‡æ¡£ä¹‹é—´çš„äº¤äº’ï¼Œæ›´å¥½åœ°å¯¹æ¯”æ–‡æ¡£è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œå¼•å…¥RSAèƒ½å¤ŸæŒç»­æ”¹è¿›å¤šç§ç¨€ç–æ£€ç´¢æ¨¡å‹ï¼Œå¹¶åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸­çš„åŸŸå¤–æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰ä¿¡æ¯æ£€ç´¢æ–¹æ³•åœ¨å¤„ç†ç¨€ç–ç¥ç»ç½‘ç»œæ—¶æœªå……åˆ†è€ƒè™‘æ–‡æ¡£é›†åˆå’Œä¸åŒæœ¯è¯­æƒé‡é—´çš„äº¤äº’ã€‚</li>
<li>Rational Speech Acts (RSA)æ¡†æ¶å¯ä»¥è¢«é€‚åº”äºä¿¡æ¯æ£€ç´¢ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤§é‡æ½œåœ¨ç‰¹å¾ï¼ˆä»¤ç‰Œï¼‰ã€‚</li>
<li>RSAé€šè¿‡è€ƒè™‘æ•°æ®é›†ä¸­å…¶ä»–æ–‡æ¡£çš„å½±å“ï¼ŒåŠ¨æ€è°ƒæ•´ä»¤ç‰Œä¸æ–‡æ¡£çš„äº¤äº’ã€‚</li>
<li>RSAèƒ½å¤Ÿæ”¹è¿›å¤šç§ç¨€ç–æ£€ç´¢æ¨¡å‹ã€‚</li>
<li>RSAåœ¨BEIRåŸºå‡†æµ‹è¯•ä¸­çš„åŸŸå¤–æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>RSAå¯¹äºæé«˜ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c88c91dca4033833ad14a131384b2e89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c44d2bcbdcfa5ee6c4c1bd49eed12bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a54cee32db1ccf5040dafb7f1d1369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91751de793a1ab7d83c27dd59bd867ca.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ReGraP-LLaVA-Reasoning-enabled-Graph-based-Personalized-Large-Language-and-Vision-Assistant"><a href="#ReGraP-LLaVA-Reasoning-enabled-Graph-based-Personalized-Large-Language-and-Vision-Assistant" class="headerlink" title="ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language   and Vision Assistant"></a>ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language   and Vision Assistant</h2><p><strong>Authors:Yifan Xiang, Zhenxi Zhang, Bin Li, Yixuan Weng, Shoujun Zhou, Yangfan He, Keqin Li</strong></p>
<p>Recent advances in personalized MLLMs enable effective capture of user-specific concepts, supporting both recognition of personalized concepts and contextual captioning. However, humans typically explore and reason over relations among objects and individuals, transcending surface-level information to achieve more personalized and contextual understanding. To this end, existing methods may face three main limitations: Their training data lacks multi-object sets in which relations among objects are learnable. Building on the limited training data, their models overlook the relations between different personalized concepts and fail to reason over them. Their experiments mainly focus on a single personalized concept, where evaluations are limited to recognition and captioning tasks. To address the limitations, we present a new dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard graph prompting methods are designed to align KGs within the modelâ€™s semantic space. We establish the ReGraP Benchmark, which contains diverse task types: multiple-choice, fill-in-the-blank, True&#x2F;False, and descriptive questions in both open- and closed-ended settings. The proposed benchmark is designed to evaluate the relational reasoning and knowledge-connection capability of personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and other competitive MLLMs. Results show that the proposed model not only learns personalized knowledge but also performs relational reasoning in responses, achieving the SoTA performance compared with the competitive methods. All the codes and datasets are released at: <a target="_blank" rel="noopener" href="https://github.com/xyfyyds/ReGraP">https://github.com/xyfyyds/ReGraP</a>. </p>
<blockquote>
<p>æœ€æ–°çš„ä¸ªæ€§åŒ–MLLMæŠ€æœ¯çš„è¿›æ­¥èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ç”¨æˆ·ç‰¹å®šæ¦‚å¿µï¼Œæ”¯æŒä¸ªæ€§åŒ–æ¦‚å¿µçš„è¯†åˆ«å’Œä¸Šä¸‹æ–‡æè¿°ã€‚ç„¶è€Œï¼Œäººç±»é€šå¸¸æ¢ç´¢å’Œæ¨ç†ç‰©ä½“å’Œä¸ªä½“ä¹‹é—´çš„å…³ç³»ï¼Œè¶…è¶Šè¡¨é¢ä¿¡æ¯ä»¥å®ç°æ›´ä¸ªæ€§åŒ–å’Œä¸Šä¸‹æ–‡çš„è®¤çŸ¥ã€‚ä¸ºæ­¤ï¼Œç°æœ‰æ–¹æ³•å¯èƒ½é¢ä¸´ä¸‰ä¸ªä¸»è¦å±€é™ï¼šä»–ä»¬çš„è®­ç»ƒæ•°æ®ç¼ºä¹å¯å­¦ä¹ ç‰©ä½“ä¹‹é—´å…³ç³»çš„å¤šç›®æ ‡é›†ã€‚åŸºäºæœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œä»–ä»¬çš„æ¨¡å‹å¿½ç•¥äº†ä¸åŒä¸ªæ€§åŒ–æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼Œæ— æ³•å¯¹å®ƒä»¬è¿›è¡Œæ¨ç†ã€‚ä»–ä»¬çš„å®éªŒä¸»è¦é›†ä¸­åœ¨å•ä¸ªä¸ªæ€§åŒ–æ¦‚å¿µä¸Šï¼Œè¯„ä¼°ä»…é™äºè¯†åˆ«å’Œæè¿°ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†ï¼Œåä¸ºReGraPï¼Œç”±åŒ…å«ä¸ªäººçŸ¥è¯†ç»„æˆçš„å›¾åƒã€çŸ¥è¯†å›¾è°±å’ŒåŸºäºçŸ¥è¯†å›¾è°±çš„CoTé—®ç­”å¯¹ç»„æˆï¼Œå…±æœ‰åŒ…å«ä¸ªæ€§åŒ–çŸ¥è¯†çš„å›¾åƒé›†å…±120ç»„ã€‚æˆ‘ä»¬æå‡ºäº†ReGraP-LLaVæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ç›¸åº”çš„çŸ¥è¯†å›¾è°±å’ŒCoTé—®ç­”å¯¹è¿›è¡Œè®­ç»ƒï¼Œè®¾è®¡è½¯ã€ç¡¬å›¾æç¤ºæ–¹æ³•å°†çŸ¥è¯†å›¾è°±çº³å…¥æ¨¡å‹çš„è¯­ä¹‰ç©ºé—´å†…ã€‚æˆ‘ä»¬å»ºç«‹äº†ReGraPåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ï¼šåŒ…æ‹¬é€‰æ‹©é¢˜ã€å¡«ç©ºã€åˆ¤æ–­é¢˜å’Œæè¿°æ€§é—®é¢˜ç­‰å¼€æ”¾å’Œå°é—­ç¯å¢ƒä¸‹çš„é¢˜å‹ã€‚è¯¥åŸºå‡†æµ‹è¯•é›†æ—¨åœ¨è¯„ä¼°ä¸ªæ€§åŒ–MLLMçš„å…³ç³»æ¨ç†å’ŒçŸ¥è¯†è¿æ¥èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹ReGraP-LLaVæ¨¡å‹å’Œæå‡ºäº†çš„å…¶ä»–ç«äº‰åŠ›è¾ƒå¼ºçš„MLLMè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¨¡å‹ä¸ä»…å­¦ä¹ äº†ä¸ªæ€§åŒ–çŸ¥è¯†ï¼Œè¿˜åœ¨å“åº”ä¸­è¿›è¡Œäº†å…³ç³»æ¨ç†ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å‡å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/xyfyyds/ReGraP%E3%80%82">https://github.com/xyfyyds/ReGraPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03654v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸ªæ€§åŒ–MLLMçš„æœ€æ–°è¿›å±•ï¼ŒåŠå…¶åœ¨å®ç°ç”¨æˆ·ç‰¹å®šæ¦‚å¿µæ•æ‰å’Œä¸Šä¸‹æ–‡æ ‡æ³¨æ–¹é¢çš„ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºæ–°çš„æ•°æ®é›†ReGraPåŠå¯¹åº”çš„MLLMæ¨¡å‹ReGraP-LLaVAã€‚é€šè¿‡è½¯ã€ç¡¬å›¾æç¤ºæ–¹æ³•å°†çŸ¥è¯†å›¾è°±ä¸æ¨¡å‹çš„è¯­ä¹‰ç©ºé—´å¯¹é½ã€‚å»ºç«‹ReGraPåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ï¼Œæ—¨åœ¨è¯„ä¼°ä¸ªæ€§åŒ–MLLMçš„å…³ç³»æ¨ç†å’ŒçŸ¥è¯†è¿æ¥èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReGraP-LLaVAæ¨¡å‹ä¸ä»…å­¦ä¹ ä¸ªæ€§åŒ–çŸ¥è¯†ï¼Œè€Œä¸”åœ¨å“åº”ä¸­è¿›è¡Œå…³ç³»æ¨ç†ï¼Œè¾¾åˆ°ä¸ç«äº‰æ–¹æ³•ç›¸æ¯”çš„é¡¶å°–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–MLLMså·²èƒ½æ•æ‰ç”¨æˆ·ç‰¹å®šæ¦‚å¿µå’Œä¸Šä¸‹æ–‡æ ‡æ³¨ï¼Œä½†ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ä¸‰å¤§å±€é™ï¼šç¼ºä¹å¤šå¯¹è±¡å…³ç³»çš„è®­ç»ƒæ•°æ®ã€å¿½ç•¥ä¸åŒä¸ªæ€§åŒ–æ¦‚å¿µé—´çš„å…³ç³»ã€å®éªŒä¸»è¦é›†ä¸­åœ¨å•ä¸€ä¸ªæ€§åŒ–æ¦‚å¿µä¸Šã€‚</li>
<li>å¼•å…¥æ–°æ•°æ®é›†ReGraPï¼ŒåŒ…å«å›¾åƒã€çŸ¥è¯†å›¾è°±å’ŒåŸºäºçŸ¥è¯†å›¾è°±çš„CoTé—®ç­”å¯¹ï¼Œä»¥æ”¯æŒæ›´é«˜çº§å’Œå¤æ‚çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>æå‡ºReGraP-LLaVAæ¨¡å‹ï¼Œé€šè¿‡è½¯å’Œç¡¬å›¾æç¤ºæ–¹æ³•ä¸çŸ¥è¯†å›¾è°±å¯¹é½ã€‚</li>
<li>å»ºç«‹ReGraPåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ï¼Œæ—¨åœ¨è¯„ä¼°ä¸ªæ€§åŒ–MLLMçš„å…³ç³»æ¨ç†å’ŒçŸ¥è¯†è¿æ¥èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ReGraP-LLaVAæ¨¡å‹ä¸ä»…å­¦ä¹ ä¸ªæ€§åŒ–çŸ¥è¯†ï¼Œè¿˜è¿›è¡Œå…³ç³»æ¨ç†ï¼Œè¾¾åˆ°é¡¶å°–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5780422f8af5980417a7523a11e2fa2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee32945cf5860bd77b357629f84bc00c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c51e71fdd3433c21f8d57a533f7128f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91fe9d35f1c7f60e33bb807f2c51ebc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a767c757b0c726dc602755a251a1a124.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ALMA-Aggregated-Lipschitz-Maximization-Attack-on-Auto-encoders"><a href="#ALMA-Aggregated-Lipschitz-Maximization-Attack-on-Auto-encoders" class="headerlink" title="ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders"></a>ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders</h2><p><strong>Authors:Chethan Krishnamurthy Ramanaik, Arjun Roy, Eirini Ntoutsi</strong></p>
<p>Despite the extensive use of deep autoencoders (AEs) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. AE robustness is characterized by the Lipschitz bounds of its components. Existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in AEs. In the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. To address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local Lipschitz bounds by enhancing loss gradient information propagation during attack optimization. We demonstrate through extensive experiments on state-of-the-art AEs that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. As a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦è‡ªç¼–ç å™¨ï¼ˆAEsï¼‰åœ¨å…³é”®åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†ä¸åˆ†ç±»æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä»¬å¯¹æŠ—æ€§æ”»å‡»çš„é²æ£’æ€§ä»ç„¶ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢ã€‚è‡ªç¼–ç å™¨çš„é²æ£’æ€§ç‰¹å¾ä½“ç°åœ¨å…¶ç»„ä»¶çš„Lipschitzç•Œé™ä¸Šã€‚åŸºäºç™½ç›’æ”»å‡»çš„ç°æœ‰é²æ£’æ€§è¯„ä¼°æ¡†æ¶å¹¶æœªå®Œå…¨æŒ–æ˜è‡ªç¼–ç å™¨ä¸­ä¸­é—´ç—…æ€å±‚å­˜åœ¨çš„æ¼æ´ã€‚åœ¨ä¼˜åŒ–ä¸å¯å¯Ÿè§‰çš„èŒƒæ•°æœ‰ç•Œé™„åŠ æ‰°åŠ¨ä»¥æœ€å¤§åŒ–è¾“å‡ºæŸå®³çš„èƒŒæ™¯ä¸‹ï¼Œç°æœ‰æ–¹æ³•åœ¨æœ‰æ•ˆä¼ æ’­å¯¹æŠ—æ€§æŸå¤±æ¢¯åº¦æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¾€å¾€æ”¶æ•›äºæ•ˆæœè¾ƒå·®çš„æ‰°åŠ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå±‚æ¡ä»¶çš„æ–°å‹å¯¹æŠ—æ€§ä¼˜åŒ–ç›®æ ‡ï¼Œé€šè¿‡å¢å¼ºæ”»å‡»ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æŸå¤±æ¢¯åº¦ä¿¡æ¯ä¼ æ’­ï¼Œæœ‰æ•ˆåœ°å¼•å¯¼å¯¹æŠ—å›¾æœå‘å±€éƒ¨Lipschitzç•Œé™åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡é’ˆå¯¹æœ€å…ˆè¿›çš„è‡ªç¼–ç å™¨çš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„å¯¹æŠ—ç›®æ ‡èƒ½å¤Ÿäº§ç”Ÿæ›´å¼ºçš„æ”»å‡»ï¼Œåœ¨é€šç”¨å’Œæ ·æœ¬ç‰¹å®šåœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä½œä¸ºä¸€ç§é’ˆå¯¹è¿™ç§æ”»å‡»çš„é˜²å¾¡æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åœ¨æ¨ç†æ—¶é—´å¯¹æŠ—è®­ç»ƒçš„é˜²å¾¡æ’ä»¶ï¼Œä»¥å‡è½»å¯¹æŠ—æ ·æœ¬çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03646v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦è‡ªç¼–ç å™¨ï¼ˆAEsï¼‰åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§é—®é¢˜ã€‚ç°æœ‰çš„åŸºäºç™½ç›’æ”»å‡»çš„ç¨³å¥æ€§è¯„ä»·æ¡†æ¶æœªèƒ½å……åˆ†åˆ©ç”¨AEä¸­é—´å±‚çš„è„†å¼±æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå±‚æ¡ä»¶å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæŒ‡å¯¼å¯¹æŠ—æ˜ å°„å‘å±€éƒ¨Lipschitzè¾¹ç•ŒåŒºåŸŸï¼Œå¢å¼ºæ”»å‡»ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„æŸå¤±æ¢¯åº¦ä¿¡æ¯ä¼ æ’­ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯¹æœ€å…ˆè¿›çš„AEsè¿›è¡Œæ”»å‡»æ—¶è¡¨ç°æ›´å¼ºå¤§ï¼Œåœ¨é€šç”¨å’Œæ ·æœ¬ç‰¹å®šåœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å¹¶æå‡ºäº†ä¸€ç§é˜²å¾¡æ’ä»¶ï¼Œèƒ½åœ¨æ¨ç†æ—¶é—´å¯¹æŠ—æ”»å‡»çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦è‡ªç¼–ç å™¨ï¼ˆAEsï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„å¯¹æŠ—ç¨³å¥æ€§è¢«è¾ƒå°‘ç ”ç©¶ã€‚</li>
<li>AEçš„ç¨³å¥æ€§å¯é€šè¿‡å…¶ç»„ä»¶çš„Lipschitzè¾¹ç•Œæ¥è¡¨å¾ã€‚</li>
<li>åŸºäºç™½ç›’æ”»å‡»çš„ç°æœ‰ç¨³å¥æ€§è¯„ä»·æ¡†æ¶æœªå……åˆ†å‘æ˜AEä¸­é—´å±‚çš„è„†å¼±æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå±‚æ¡ä»¶çš„å¯¹æŠ—ä¼˜åŒ–ç›®æ ‡ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼å¯¹æŠ—æ”»å‡»ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¢å¼ºæŸå¤±æ¢¯åº¦ä¿¡æ¯çš„ä¼ æ’­ï¼Œä½¿æ”»å‡»æ˜ å°„æœå‘å±€éƒ¨Lipschitzè¾¹ç•ŒåŒºåŸŸã€‚</li>
<li>åœ¨å¯¹æœ€å…ˆè¿›çš„AEsè¿›è¡Œæ”»å‡»æ—¶ï¼Œè¯¥æ–¹æ³•è¡¨ç°æ›´ä¼˜ç§€ï¼Œä¼˜äºç°æœ‰æ–¹æ³•åœ¨é€šç”¨å’Œæ ·æœ¬ç‰¹å®šåœºæ™¯ä¸‹çš„æ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebbb6e6441d5bdd3ca6ae4e0b7b74825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b2578580b06af3cc52d1758c4dfd27e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97975dd6640f24b1be67c969159ee2ce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DyGEnc-Encoding-a-Sequence-of-Textual-Scene-Graphs-to-Reason-and-Answer-Questions-in-Dynamic-Scenes"><a href="#DyGEnc-Encoding-a-Sequence-of-Textual-Scene-Graphs-to-Reason-and-Answer-Questions-in-Dynamic-Scenes" class="headerlink" title="DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer   Questions in Dynamic Scenes"></a>DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer   Questions in Dynamic Scenes</h2><p><strong>Authors:Sergey Linok, Vadim Semenov, Anastasia Trunova, Oleg Bulichev, Dmitry Yudin</strong></p>
<p>The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com&#x2F;linukc&#x2F;DyGEnc. </p>
<blockquote>
<p>äº‹ä»¶åˆ†æåœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¹å‘å±•èƒ½ä¸äººç±»äº¤äº’çš„æ™ºèƒ½ä»£ç†å’Œæœºå™¨äººæå‡ºäº†æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨è§†è§‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä»å›¾åƒä¸­éšå¼æ•è·ä¿¡æ¯ï¼Œç¼ºä¹å¯è§£é‡Šçš„ç©ºé—´æ—¶é—´å¯¹è±¡è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DyGEncâ€”â€”ä¸€ç§æ–°å‹åŠ¨æ€å›¾ç¼–ç æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å‹ç¼©çš„ç©ºé—´æ—¶é—´ç»“æ„è§‚å¯Ÿè¡¨ç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ç›¸ç»“åˆã€‚è¿™ç§ç»“åˆçš„ç›®çš„æ˜¯æ ¹æ®ä¸€ç³»åˆ—æ–‡æœ¬åœºæ™¯å›¾è¿›è¡Œé«˜çº§é—®ç­”ã€‚åœ¨STARå’ŒAGQAæ•°æ®é›†ä¸Šçš„æ‰©å±•è¯„ä¼°è¡¨æ˜ï¼ŒDyGEncåœ¨è§£å†³æœ‰å…³äººä¸å¯¹è±¡äº¤äº’å†å²çš„é—®é¢˜æ—¶ï¼Œè¾ƒç°æœ‰è§†è§‰æ–¹æ³•çš„æ€§èƒ½æé«˜äº†15-25%ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ— ç¼æ‰©å±•åˆ°å¤„ç†åŸå§‹è¾“å…¥å›¾åƒï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹æå–æ˜ç¡®çš„æ–‡æœ¬åœºæ™¯å›¾ï¼Œæ­£å¦‚åœ¨è½®å¼æ“çºµå™¨å¹³å°ä¸Šè¿›è¡Œçš„æœºå™¨äººå®éªŒçš„ç»“æœæ‰€ç¤ºã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›å‘ç°å°†æœ‰åŠ©äºå®ç°ç”¨äºé•¿æœŸæ¨ç†çš„ç¨³å¥å’Œå‹ç¼©çš„å›¾åŸºæœºå™¨äººè®°å¿†ã€‚ä»£ç å¯åœ¨github.com&#x2F;linukc&#x2F;DyGEncæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03581v1">PDF</a> 8 pages, 5 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€é¡¹åä¸ºDyGEncçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€ç¯å¢ƒä¸­æ™ºèƒ½ä»£ç†å’Œæœºå™¨äººä¸äººç±»äº¤äº’çš„å‘å±•æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å‹ç¼©çš„æ—¶ç©ºç»“æ„è§‚æµ‹è¡¨ç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ï¼Œä»¥å®ç°å¯¹ä¸€ç³»åˆ—æ–‡æœ¬åœºæ™¯å›¾çš„å…ˆè¿›é—®ç­”åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDyGEncåœ¨è§£å†³æœ‰å…³äººä¸ç‰©ä½“äº¤äº’å†å²çš„é—®é¢˜æ—¶ï¼Œåœ¨STARå’ŒAGQAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰è§†è§‰æ–¹æ³•ï¼Œå·®è·è¾¾15-25%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯æ— ç¼æ‰©å±•åˆ°å¤„ç†åŸå§‹è¾“å…¥å›¾åƒï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹æå–æ˜ç¡®çš„æ–‡æœ¬åœºæ™¯å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DyGEncæ˜¯ä¸€ç§è§£å†³æ™ºèƒ½ä»£ç†å’Œæœºå™¨äººåœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¸äººç±»äº¤äº’æŒ‘æˆ˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å‹ç¼©çš„æ—¶ç©ºç»“æ„è§‚æµ‹è¡¨ç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥èƒ½åŠ›ã€‚</li>
<li>DyGEncå®ç°äº†åŸºäºä¸€ç³»åˆ—æ–‡æœ¬åœºæ™¯å›¾çš„å…ˆè¿›é—®ç­”åŠŸèƒ½ã€‚</li>
<li>åœ¨STARå’ŒAGQAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDyGEncåœ¨è§£å†³äººä¸ç‰©ä½“äº¤äº’å†å²çš„é—®é¢˜æ—¶ä¼˜äºç°æœ‰è§†è§‰æ–¹æ³•ã€‚</li>
<li>DyGEncçš„æ€§èƒ½æå‡å¹…åº¦ä¸º15-25%ã€‚</li>
<li>DyGEncæ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å¤„ç†åŸå§‹è¾“å…¥å›¾åƒï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹æå–æ˜ç¡®çš„æ–‡æœ¬åœºæ™¯å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4530b5597b7c5e33abb342ca38e70132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ece9bafcfda99f067d67f68b72a0e52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dd396978241690f41e7c67dfa3192dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-629913bcab73f75dfcf4133406cddd23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea2718b7e47358de4c056eaefa3037ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-430caa78c16fd570c9373362a1c2020d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad1765da23aaedf3d3969b5efe429a9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43b0ef76f6b8043b099b679e29e30a69.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Uncovering-the-Limitations-of-Model-Inversion-Evaluation-Benchmarks-and-Connection-to-Type-I-Adversarial-Attacks"><a href="#Uncovering-the-Limitations-of-Model-Inversion-Evaluation-Benchmarks-and-Connection-to-Type-I-Adversarial-Attacks" class="headerlink" title="Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and   Connection to Type-I Adversarial Attacks"></a>Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and   Connection to Type-I Adversarial Attacks</h2><p><strong>Authors:Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, Ngai-Man Cheung</strong></p>
<p>Model Inversion (MI) attacks aim to reconstruct information of private training data by exploiting access to machine learning models. The most common evaluation framework for MI attacks&#x2F;defenses relies on an evaluation model that has been utilized to assess progress across almost all MI attacks and defenses proposed in recent years. In this paper, for the first time, we present an in-depth study of MI evaluation. Firstly, we construct the first comprehensive human-annotated dataset of MI attack samples, based on 28 setups of different MI attacks, defenses, private and public datasets. Secondly, using our dataset, we examine the accuracy of the MI evaluation framework and reveal that it suffers from a significant number of false positives. These findings raise questions about the previously reported success rates of SOTA MI attacks. Thirdly, we analyze the causes of these false positives, design controlled experiments, and discover the surprising effect of Type I adversarial features on MI evaluation, as well as adversarial transferability, highlighting a relationship between two previously distinct research areas. Our findings suggest that the performance of SOTA MI attacks has been overestimated, with the actual privacy leakage being significantly less than previously reported. In conclusion, we highlight critical limitations in the widely used MI evaluation framework and present our methods to mitigate false positive rates. We remark that prior research has shown that Type I adversarial attacks are very challenging, with no existing solution. Therefore, we urge to consider human evaluation as a primary MI evaluation framework rather than merely a supplement as in previous MI research. We also encourage further work on developing more robust and reliable automatic evaluation frameworks. </p>
<blockquote>
<p>æ¨¡å‹åæ¼”ï¼ˆMIï¼‰æ”»å‡»æ—¨åœ¨é€šè¿‡è®¿é—®æœºå™¨å­¦ä¹ æ¨¡å‹æ¥é‡å»ºç§æœ‰è®­ç»ƒæ•°æ®çš„ä¿¡æ¯ã€‚æ¨¡å‹åæ¼”æ”»å‡»&#x2F;é˜²å¾¡çš„æœ€å¸¸è§è¯„ä¼°æ¡†æ¶ä¾èµ–äºä¸€ä¸ªè¯„ä¼°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¿‘å¹´æ¥å‡ ä¹è¢«ç”¨äºè¯„ä¼°æ‰€æœ‰æå‡ºçš„MIæ”»å‡»å’Œé˜²å¾¡æ–¹æ³•çš„è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹MIè¯„ä¼°è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªå…¨é¢çš„åŸºäºäººå·¥æ ‡æ³¨çš„MIæ”»å‡»æ ·æœ¬æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŸºäºä¸åŒçš„MIæ”»å‡»ã€é˜²å¾¡æªæ–½ã€ç§æœ‰å’Œå…¬å…±æ•°æ®é›†çš„28ç§è®¾ç½®ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ç ”ç©¶äº†MIè¯„ä¼°æ¡†æ¶çš„å‡†ç¡®æ€§ï¼Œå¹¶å‘ç°å®ƒå­˜åœ¨å¤§é‡çš„å‡é˜³æ€§ç»“æœã€‚è¿™äº›å‘ç°å¯¹å…ˆå‰æŠ¥å‘Šçš„å…ˆè¿›MIæ”»å‡»çš„æˆåŠŸç‡æå‡ºäº†è´¨ç–‘ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›å‡é˜³æ€§çš„åŸå› ï¼Œè®¾è®¡äº†å—æ§å®éªŒï¼Œå¹¶å‘ç°äº†ç±»å‹ä¸€å¯¹æŠ—ç‰¹å¾å¯¹MIè¯„ä¼°çš„æƒŠäººå½±å“ï¼Œä»¥åŠå¯¹å¯¹æŠ—è¿ç§»æ€§çš„å…³æ³¨ï¼Œçªå‡ºäº†ä¸¤ä¸ªå…ˆå‰ç‹¬ç«‹ç ”ç©¶é¢†åŸŸä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ˆè¿›MIæ”»å‡»çš„æ€§èƒ½è¢«é«˜ä¼°äº†ï¼Œå®é™…çš„éšç§æ³„éœ²ç¨‹åº¦è¿œä½äºå…ˆå‰æŠ¥å‘Šçš„ã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†å¹¿æ³›ä½¿ç”¨çš„MIè¯„ä¼°æ¡†æ¶çš„å…³é”®å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•æ¥é™ä½è¯¯æŠ¥ç‡ã€‚æˆ‘ä»¬æ³¨æ„åˆ°å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œç¬¬ä¸€ç±»å¯¹æŠ—æ€§æ”»å‡»éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°šæ— ç°æœ‰è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ•¦ä¿ƒå°†äººå·¥è¯„ä¼°ä½œä¸ºä¸»è¦çš„MIè¯„ä¼°æ¡†æ¶ï¼Œè€Œä¸æ˜¯åƒä»¥å‰çš„MIç ”ç©¶é‚£æ ·ä»…ä»…ä½œä¸ºè¡¥å……ã€‚æˆ‘ä»¬ä¹Ÿé¼“åŠ±è¿›ä¸€æ­¥å¼€å‘æ›´ç¨³å¥å’Œå¯é çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03519v1">PDF</a> Our dataset and code are available in the Supp</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ¨¡å‹åæ¼”ï¼ˆMIï¼‰æ”»å‡»æ—¨åœ¨é€šè¿‡è®¿é—®æœºå™¨å­¦ä¹ æ¨¡å‹æ¥é‡å»ºç§æœ‰è®­ç»ƒæ•°æ®çš„ä¿¡æ¯ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹MIè¯„ä¼°è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†ç¬¬ä¸€ä¸ªå…¨é¢çš„MIæ”»å‡»æ ·æœ¬äººç±»æ³¨é‡Šæ•°æ®é›†ï¼ŒåŸºäºä¸åŒçš„MIæ”»å‡»å’Œé˜²å¾¡æ–¹æ¡ˆçš„è®¾ç½®ï¼Œæ¶µç›–å…¬å…±å’Œç§æœ‰æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ç ”ç©¶äº†MIè¯„ä¼°æ¡†æ¶çš„å‡†ç¡®æ€§ï¼Œå¹¶å‘ç°å­˜åœ¨å¤§é‡è¯¯æŠ¥ã€‚è¿™äº›å‘ç°å¯¹å…ˆå‰æŠ¥å‘Šçš„æœ€æ–°MIæ”»å‡»çš„æˆåŠŸç‡æå‡ºäº†è´¨ç–‘ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†è¿™äº›è¯¯æŠ¥çš„åŸå› ï¼Œè®¾è®¡äº†å—æ§å®éªŒï¼Œå¹¶å‘ç°äº†ç¬¬ä¸€å‹å¯¹æŠ—ç‰¹å¾å¯¹MIè¯„ä¼°çš„æ„å¤–å½±å“ä»¥åŠå¯¹æŠ—è¿ç§»æ€§ï¼Œçªå‡ºäº†ä¸¤ä¸ªå…ˆå‰æˆªç„¶ä¸åŒçš„ç ”ç©¶é¢†åŸŸä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹æœ€æ–°MIæ”»å‡»çš„æ€§èƒ½è¿›è¡Œäº†é«˜ä¼°ï¼Œå®é™…éšç§æ³„éœ²è¿œä½äºå…ˆå‰çš„æŠ¥å‘Šã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†å¹¿æ³›ä½¿ç”¨çš„MIè¯„ä¼°æ¡†æ¶çš„å…³é”®å±€é™æ€§ï¼Œå¹¶æå‡ºäº†é™ä½è¯¯æŠ¥ç‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬å»ºè®®å°†äººç±»è¯„ä¼°ä½œä¸ºä¸»è¦çš„MIè¯„ä¼°æ¡†æ¶è¿›è¡Œè€ƒè™‘ï¼Œè€Œä¸æ˜¯åƒä»¥å‰çš„MIç ”ç©¶ä¸­é‚£æ ·ä»…ä»…ä½œä¸ºè¡¥å……ã€‚æˆ‘ä»¬ä¹Ÿé¼“åŠ±è¿›ä¸€æ­¥å¼€å‘æ›´ç¨³å¥å’Œå¯é çš„è‡ªåŠ¨è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ„å»ºäº†é¦–ä¸ªå…¨é¢çš„åŸºäºäººç±»æ³¨é‡Šçš„MIæ”»å‡»æ ·æœ¬æ•°æ®é›†ï¼Œæ¶µç›–ä¸åŒè®¾ç½®ä¸‹çš„æ”»å‡»å’Œé˜²å¾¡æ–¹æ¡ˆã€‚</li>
<li>å‘ç°å½“å‰å¹¿æ³›ä½¿ç”¨çš„MIè¯„ä¼°æ¡†æ¶å­˜åœ¨å¤§é‡è¯¯æŠ¥ã€‚</li>
<li>åˆ†æäº†è¯¯æŠ¥çš„åŸå› ï¼Œå¹¶æ­ç¤ºäº†ç¬¬ä¸€å‹å¯¹æŠ—ç‰¹å¾å¯¹MIè¯„ä¼°çš„å½±å“åŠå¯¹æŠ—è¿ç§»æ€§ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œå¯¹æœ€æ–°MIæ”»å‡»çš„æ€§èƒ½å¯èƒ½é«˜äºå®é™…è¡¨ç°ï¼Œéšç§æ³„éœ²æƒ…å†µè¿œä½äºå…ˆå‰æŠ¥å‘Šã€‚</li>
<li>æŒ‡å‡ºå¹¿æ³›ä½¿ç”¨çš„MIè¯„ä¼°æ¡†æ¶å­˜åœ¨å…³é”®å±€é™æ€§ã€‚</li>
<li>æå‡ºé™ä½è¯¯æŠ¥ç‡çš„æ–¹æ³•ï¼Œå¹¶å»ºè®®å°†äººç±»è¯„ä¼°ä½œä¸ºä¸»è¦çš„MIè¯„ä¼°æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5df194a6da04c3348f9584f4545f0778.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a8efba22c18d66a2ac4b7564a67077a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-423489fa9feead36ad682895b0b29b07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbef995f3e4374d373b099dc055172d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-Steganographic-Potentials-of-Language-Models"><a href="#The-Steganographic-Potentials-of-Language-Models" class="headerlink" title="The Steganographic Potentials of Language Models"></a>The Steganographic Potentials of Language Models</h2><p><strong>Authors:Artem Karpov, Tinuade Adeleke, Seong Hah Cho, Natalia Perez-Campanero</strong></p>
<p>The potential for large language models (LLMs) to hide messages within plain text (steganography) poses a challenge to detection and thwarting of unaligned AI agents, and undermines faithfulness of LLMs reasoning. We explore the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL) to: (1) develop covert encoding schemes, (2) engage in steganography when prompted, and (3) utilize steganography in realistic scenarios where hidden reasoning is likely, but not prompted. In these scenarios, we detect the intention of LLMs to hide their reasoning as well as their steganography performance. Our findings in the fine-tuning experiments as well as in behavioral non fine-tuning evaluations reveal that while current models exhibit rudimentary steganographic abilities in terms of security and capacity, explicit algorithmic guidance markedly enhances their capacity for information concealment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™®é€šæ–‡æœ¬ä¸­éšè—æ¶ˆæ¯ï¼ˆéšå†™æœ¯ï¼‰çš„æ½œåŠ›ï¼Œç»™æ£€æµ‹å’Œé˜»æ­¢æœªå¯¹é½çš„AIä»£ç†å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¹¶ç ´åäº†LLMæ¨ç†çš„å¿ å®æ€§ã€‚æˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„éšå†™èƒ½åŠ›ï¼Œä»¥ï¼ˆ1ï¼‰å¼€å‘éšè”½ç¼–ç æ–¹æ¡ˆï¼Œï¼ˆ2ï¼‰åœ¨æç¤ºæ—¶è¿›è¡Œéšå†™ï¼Œï¼ˆ3ï¼‰åœ¨å¯èƒ½å‡ºç°éšè”½æ¨ç†çš„ç°å®åœºæ™¯ä¸­åˆ©ç”¨éšå†™ï¼Œå³ä½¿è¿™äº›åœºæ™¯æ²¡æœ‰è¢«æç¤ºã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬æ£€æµ‹LLMéšè—å…¶æ¨ç†çš„æ„å›¾ä»¥åŠå®ƒä»¬çš„éšå†™è¡¨ç°ã€‚æˆ‘ä»¬åœ¨å¾®è°ƒå®éªŒä»¥åŠéè¡Œä¸ºå¾®è°ƒè¯„ä¼°ä¸­çš„å‘ç°è¡¨æ˜ï¼Œè™½ç„¶å½“å‰æ¨¡å‹åœ¨å®‰å…¨å’Œèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºåŸºæœ¬çš„éšå†™èƒ½åŠ›ï¼Œä½†æ˜ç¡®çš„ç®—æ³•æŒ‡å¯¼æ˜¾è‘—æé«˜äº†å®ƒä»¬éšè—ä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03439v1">PDF</a> Published at Building Trust Workshop at ICLR 2025</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™®é€šæ–‡æœ¬ä¸­éšè—ä¿¡æ¯ï¼ˆéšå†™æœ¯ï¼‰çš„æ½œåŠ›ï¼Œå¯¹æ£€æµ‹ä¸é˜»æ­¢ä¸ç¬¦åˆé¢„æœŸçš„AIä»£ç†æ„æˆæŒ‘æˆ˜ï¼Œå¹¶å½±å“LLMæ¨ç†çš„å¿ å®æ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒåçš„LLMçš„éšå†™èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¼€å‘éšè”½ç¼–ç æ–¹æ¡ˆã€åœ¨æç¤ºä¸‹è¿›è¡Œéšå†™ä»¥åŠåœ¨å¯èƒ½å‡ºç°éšè—æ¨ç†ä½†æœªæç¤ºçš„ç°å®åœºæ™¯ä¸­åº”ç”¨éšå†™ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶å½“å‰æ¨¡å‹çš„éšå†™èƒ½åŠ›å’Œå®‰å…¨æ€§ä»…å¤„äºåˆçº§é˜¶æ®µï¼Œä½†æ˜ç¡®çš„ç®—æ³•æŒ‡å¯¼èƒ½æ˜¾è‘—æé«˜å®ƒä»¬éšè—ä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡åœ¨æ™®é€šæ–‡æœ¬ä¸­éšè—ä¿¡æ¯çš„èƒ½åŠ›ï¼Œè¿™æ„æˆå¯¹æ£€æµ‹ä¸é˜»æ­¢ä¸ç¬¦åˆé¢„æœŸçš„AIä»£ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¾®è°ƒLLMï¼Œå¯æå‡å…¶éšå†™èƒ½åŠ›ã€‚</li>
<li>LLMèƒ½åœ¨æç¤ºä¸‹è¿›è¡Œéšå†™ï¼Œå¹¶åº”ç”¨äºç°å®åœºæ™¯ä¸­çš„éšè—æ¨ç†ã€‚</li>
<li>å½“å‰çš„LLMåœ¨éšå†™çš„å®‰å…¨æ€§å’Œå®¹é‡æ–¹é¢ä»…å¤„äºåˆçº§é˜¶æ®µã€‚</li>
<li>æ˜ç¡®çš„ç®—æ³•æŒ‡å¯¼èƒ½æ˜¾è‘—æé«˜LLMéšè—ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>LLMçš„éšå†™èƒ½åŠ›å¯èƒ½ä¼šå½±å“å…¶æ¨ç†çš„å¿ å®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57d5298e56bf993aa80bc432395689f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f1004186282a279ac402de035501347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59b96fda2d6c7b323a7a33b6fe642349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a57ceef32f2ba1b44d430165a0474a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e0f4bca4ab5a3a0f0f85549a3b57fb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d85d5c98e7413a55cda184e57974274.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Absolute-Zero-Reinforced-Self-play-Reasoning-with-Zero-Data"><a href="#Absolute-Zero-Reinforced-Self-play-Reasoning-with-Zero-Data" class="headerlink" title="Absolute Zero: Reinforced Self-play Reasoning with Zero Data"></a>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</h2><p><strong>Authors:Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ é€šè¿‡éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å±•ç°å‡ºä»ç»“æœå¯¼å‘çš„å¥–åŠ±ä¸­æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚è¿‘æœŸåœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹è¿ä½œçš„RLVRç ”ç©¶é¿å…äº†ç›‘ç£æ¨ç†è¿‡ç¨‹çš„æ ‡ç­¾ï¼Œä½†ä»ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚é«˜è´¨é‡çš„äººé€ ç¤ºä¾‹çš„ç¨€ç¼ºå¼•å‘äº†äººä»¬å¯¹äºé•¿æœŸä¾èµ–äººç±»ç›‘ç£çš„å¯æ‰©å±•æ€§çš„æ‹…å¿§ï¼Œè¿™åœ¨è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒé¢†åŸŸå·²åˆè§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåœ¨ä¸€ä¸ªAIè¶…è¶Šäººç±»æ™ºåŠ›çš„å‡è®¾æœªæ¥ä¸­ï¼Œäººç±»æä¾›çš„ä»»åŠ¡å¯èƒ½ä¸ºè¶…æ™ºèƒ½ç³»ç»Ÿæä¾›æœ‰é™çš„å­¦ä¹ æ½œåŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æ‹…å¿§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼â€”â€”ç»å¯¹é›¶åº¦ï¼ˆAbsolute Zeroï¼‰ï¼Œå…¶ä¸­å•ä¸€æ¨¡å‹å­¦ä¼šæå‡ºèƒ½æœ€å¤§åŒ–å…¶è‡ªèº«å­¦ä¹ è¿›åº¦çš„ä»»åŠ¡ï¼Œå¹¶é€šè¿‡è§£å†³è¿™äº›ä»»åŠ¡æ¥æå‡æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚åœ¨è¿™ä¸€èŒƒå¼ä¸‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç»å¯¹é›¶åº¦æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œä¸€ä¸ªé€šè¿‡ä»£ç æ‰§è¡Œå™¨éªŒè¯æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆçš„ç³»ç»Ÿï¼Œä½œä¸ºéªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºï¼Œå¼•å¯¼å¼€æ”¾å¼ä½†åŸºäºå®é™…çš„å­¦ä¹ ã€‚å°½ç®¡å®Œå…¨æœªç»å¤–éƒ¨æ•°æ®è®­ç»ƒï¼ŒAZRåœ¨ç¼–ç¨‹å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ€»ä½“æœ€ä½³æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¾èµ–æ•°ä¸‡ä¸ªäººå·¥ç²¾é€‰çš„èŒƒä¾‹çš„ç°æœ‰é›¶æ ·æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†AZRå¯ä»¥æˆåŠŸåº”ç”¨äºä¸åŒçš„æ¨¡å‹è§„æ¨¡ï¼Œä¸”ä¸å„ç§æ¨¡å‹ç±»åˆ«å…¼å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03335v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡ç›´æ¥ä»ç»“æœå¯¼å‘çš„å¥–åŠ±ä¸­å­¦ä¹ ã€‚æœ€æ–°RLVRå·¥ä½œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹é¿å…äº†ç›‘ç£æ¨ç†è¿‡ç¨‹çš„æ ‡ç­¾åŒ–ï¼Œä½†ä»ä¾èµ–äºæ‰‹åŠ¨æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé›†åˆè¿›è¡Œè®­ç»ƒã€‚è€ƒè™‘åˆ°é«˜è´¨é‡äººç±»ç”Ÿæˆä¾‹å­çš„ç¨€ç¼ºæ€§ï¼Œä»¥åŠå¯¹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒé¢†åŸŸä¾èµ–äººç±»ç›‘ç£çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„RLVRèŒƒå¼â€”â€”ç»å¯¹é›¶ï¼Œä¸€ä¸ªå•ä¸€æ¨¡å‹åœ¨å…¶ä¸­å­¦ä¹ æå‡ºä»»åŠ¡ä»¥æœ€å¤§åŒ–è‡ªèº«å­¦ä¹ è¿›åº¦å¹¶è§£å†³é—®é¢˜æé«˜æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨æ•°æ®ã€‚åŸºäºæ­¤èŒƒå¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»å¯¹é›¶æ¨ç†å™¨ï¼ˆAZRï¼‰ï¼Œä¸€ä¸ªé€šè¿‡ä»£ç æ‰§è¡Œè€…éªŒè¯æå‡ºçš„ä»£ç æ¨ç†ä»»åŠ¡å’Œç­”æ¡ˆçš„ç³»ç»Ÿï¼Œä½œä¸ºå¯éªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºï¼Œä»¥æŒ‡å¯¼å¼€æ”¾å¼ä½†åŸºäºç°å®çš„å­¦ä¹ ã€‚å°½ç®¡å®Œå…¨åœ¨å¤–éƒ¨æ•°æ®ä¹‹å¤–è¿›è¡Œè®­ç»ƒï¼ŒAZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ€»ä½“æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºä¾èµ–æ•°ä¸‡é¢†åŸŸå†…çš„æ‰‹å·¥æ•´ç†æ ·æœ¬çš„é›¶æ ·æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†AZRå¯ä»¥è·¨ä¸åŒæ¨¡å‹è§„æ¨¡æœ‰æ•ˆåº”ç”¨ï¼Œä¸”é€‚ç”¨äºå„ç§æ¨¡å‹ç±»åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RLVRé€šè¿‡ç›´æ¥å­¦ä¹ ä»ç»“æœå¯¼å‘çš„å¥–åŠ±å¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æœ€æ–°RLVRç ”ç©¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å·¥ä½œï¼Œé¿å…ç›‘ç£æ¨ç†è¿‡ç¨‹æ ‡ç­¾åŒ–ä½†ä¾èµ–æ‰‹å·¥æ•´ç†çš„é—®é¢˜ç­”æ¡ˆé›†ã€‚</li>
<li>äººç±»ç”Ÿæˆé«˜è´¨é‡ä¾‹å­çš„ç¨€ç¼ºæ€§å’Œå¯¹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒé¢†åŸŸä¾èµ–äººç±»ç›‘ç£çš„æŒ‘æˆ˜ä¿ƒä½¿æå‡ºæ–°çš„RLVRèŒƒå¼â€”â€”ç»å¯¹é›¶ã€‚</li>
<li>åœ¨ç»å¯¹é›¶èŒƒå¼ä¸‹ï¼ŒAZRè‡ªæˆ‘è¿›åŒ–å…¶è®­ç»ƒè¯¾ç¨‹å’Œæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ä»£ç æ‰§è¡Œè€…éªŒè¯ä»»åŠ¡å’Œç­”æ¡ˆä½œä¸ºå¯éªŒè¯å¥–åŠ±çš„ç»Ÿä¸€æ¥æºã€‚</li>
<li>AZRåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°æ€»ä½“æœ€ä½³æ€§èƒ½ï¼Œä¸”æ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®ã€‚</li>
<li>AZRå¯ä»¥è·¨ä¸åŒæ¨¡å‹è§„æ¨¡æœ‰æ•ˆåº”ç”¨ï¼Œè¯æ˜äº†å…¶è·¨æ¨¡å‹å…¼å®¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa2316950fb03a344e91511feaceb4d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-100471ed028d8370f5d09c8ba954c734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53990f3c8272ae038fb94dbd3dafdfd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea3ba310819fd4aff8e3d53a3e56fb34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746ff970b2e408d3c9774f657c02f292.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unified-Multimodal-Chain-of-Thought-Reward-Model-through-Reinforcement-Fine-Tuning"><a href="#Unified-Multimodal-Chain-of-Thought-Reward-Model-through-Reinforcement-Fine-Tuning" class="headerlink" title="Unified Multimodal Chain-of-Thought Reward Model through Reinforcement   Fine-Tuning"></a>Unified Multimodal Chain-of-Thought Reward Model through Reinforcement   Fine-Tuning</h2><p><strong>Authors:Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang</strong></p>
<p>Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the modelâ€™s latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the modelâ€™s cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the modelâ€™s prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the modelâ€™s reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºåœ¨å‘è§†è§‰æ¨¡å‹æä¾›ä¸äººç±»åå¥½å¯¹é½çš„å¥–åŠ±ä¿¡å·æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„RMsä¸€èˆ¬ä»…é™äºæä¾›ç›´æ¥å“åº”æˆ–è¿›è¡Œæ·±åº¦æœ‰é™çš„æµ…å±‚æ¨ç†è¿‡ç¨‹ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸å‡†ç¡®ã€‚æˆ‘ä»¬è®¤ä¸ºå°†æ˜ç¡®çš„é•¿æœŸæ€ç»´é“¾ï¼ˆCoTï¼‰èå…¥å¥–åŠ±æ¨ç†è¿‡ç¨‹å¯ä»¥æ˜¾è‘—æé«˜å®ƒä»¬çš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç›¸ä¿¡ä¸€æ—¦RMså†…åŒ–CoTæ¨ç†ï¼Œå…¶é€šè¿‡éšæ€§æ¨ç†èƒ½åŠ›æé«˜ç›´æ¥å“åº”çš„å‡†ç¡®æ€§ä¹Ÿæ˜¯å¯è¡Œçš„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UnifiedReward-Thinkï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€CoTçš„å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œé’ˆå¯¹è§†è§‰ç†è§£å’Œç”Ÿæˆå¥–åŠ±ä»»åŠ¡çš„å¤šç»´åº¦ã€é€æ­¥é•¿æœŸæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä»¥æ¢ç´¢ä¸ºé©±åŠ¨çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•æ¥æ¿€å‘å’Œæ¿€åŠ±æ¨¡å‹çš„æ½œåœ¨å¤æ‚æ¨ç†èƒ½åŠ›ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å°‘é‡çš„å›¾åƒç”Ÿæˆåå¥½æ•°æ®æ¥æç‚¼GPT-4oçš„æ¨ç†è¿‡ç¨‹ï¼Œç”¨äºæ¨¡å‹çš„å†·å¯åŠ¨æ¥å­¦ä¹ CoTæ¨ç†çš„æ ¼å¼å’Œç»“æ„ã€‚ï¼ˆ2ï¼‰éšåï¼Œæˆ‘ä»¬åˆ©ç”¨æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå‡†å¤‡å¤§è§„æ¨¡çš„ç»Ÿä¸€å¤šæ¨¡æ€åå¥½æ•°æ®ï¼Œä»¥æ¿€å‘æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæ­£ç¡®çš„æ¨ç†è¾“å‡ºè¢«ä¿ç•™ç”¨äºæ‹’ç»é‡‡æ ·ä»¥æ”¹è¿›æ¨¡å‹ï¼ˆ3ï¼‰ï¼Œè€Œé”™è¯¯çš„é¢„æµ‹æ ·æœ¬æœ€ç»ˆè¢«ç”¨äºåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å¾®è°ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢ä¸åŒçš„æ¨ç†è·¯å¾„å¹¶ä¼˜åŒ–æ­£ç¡®çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚åœ¨å„ç§è§†è§‰å¥–åŠ±ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03318v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://codegoat24.github.io/UnifiedReward/think">https://codegoat24.github.io/UnifiedReward/think</a></p>
<p><strong>æ‘˜è¦</strong><br>å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºä¸ºè§†è§‰æ¨¡å‹ä¸äººç±»åå¥½æä¾›å¥–åŠ±ä¿¡å·çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰RMsé€šå¸¸ä»…é™äºæä¾›ç›´æ¥å“åº”æˆ–è¿›è¡Œæµ…å±‚æ¬¡çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸å‡†ç¡®ã€‚æœ¬æ–‡æå‡ºå°†æ˜ç¡®çš„é•¿æœŸæ€ç»´é“¾ï¼ˆCoTï¼‰èå…¥å¥–åŠ±æ¨ç†è¿‡ç¨‹ï¼Œèƒ½æ˜¾è‘—æå‡RMsçš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†UnifiedReward-Thinkï¼Œé¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€CoTåŸºäºå¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå¤šç»´åº¦ã€é€æ­¥çš„é•¿æœŸæ€ç»´é“¾æ¨ç†ï¼Œé€‚ç”¨äºè§†è§‰ç†è§£å’Œç”Ÿæˆå¥–åŠ±ä»»åŠ¡ã€‚é€šè¿‡é‡‡ç”¨æ¢ç´¢é©±åŠ¨çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œæ¿€å‘å’Œæ¿€åŠ±æ¨¡å‹çš„æ½œåœ¨å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¹¶å……åˆ†åˆ©ç”¨æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå‡†å¤‡å¤§è§„æ¨¡çš„ç»Ÿä¸€å¤šæ¨¡å¼åå¥½æ•°æ®ï¼Œä»¥æ¿€å‘æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡æ‹’ç»é‡‡æ ·ä¿ç•™æ­£ç¡®çš„æ¨ç†è¾“å‡ºï¼Œå¯¹æ¨¡å‹è¿›è¡Œç²¾ç‚¼å’Œä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§è§†è§‰å¥–åŠ±ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰å­˜åœ¨ç›´æ¥å“åº”å’Œæµ…å±‚æ¬¡æ¨ç†çš„å±€é™æ€§ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·ä¸å‡†ç¡®ã€‚</li>
<li>èå…¥é•¿æœŸæ€ç»´é“¾ï¼ˆCoTï¼‰åˆ°å¥–åŠ±æ¨ç†è¿‡ç¨‹ä¸­èƒ½æ˜¾è‘—æå‡RMsçš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>UnifiedReward-Thinkæ˜¯é¦–ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€CoTåŸºäºå¥–åŠ±æ¨¡å‹ï¼Œæ”¯æŒå¤šç»´åº¦ã€é€æ­¥çš„é•¿æœŸæ€ç»´é“¾æ¨ç†ã€‚</li>
<li>é‡‡ç”¨æ¢ç´¢é©±åŠ¨çš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œæ¿€å‘æ¨¡å‹çš„æ½œåœ¨å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨GPT-4oçš„æ¨ç†è¿‡ç¨‹è¿›è¡Œå†·å¯åŠ¨å­¦ä¹ ï¼Œäº†è§£æ¨¡å‹å’Œä»»åŠ¡çš„æ ¼å¼å’Œç»“æ„ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡ç»Ÿä¸€å¤šæ¨¡å¼åå¥½æ•°æ®ï¼Œæ¿€å‘æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4171ca90214e76708e170b687e00fa11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06bdc9df3ba444aa18f1a82769ae87a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a619c23945fc6a0e92e7c1f735fc44b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e976774fa9d6e50fce6ebc4916b70e2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Soft-Reasoning-Paths-for-Knowledge-Graph-Completion"><a href="#Soft-Reasoning-Paths-for-Knowledge-Graph-Completion" class="headerlink" title="Soft Reasoning Paths for Knowledge Graph Completion"></a>Soft Reasoning Paths for Knowledge Graph Completion</h2><p><strong>Authors:Yanning Hou, Sihang Zhou, Ke Liang, Lingyuan Meng, Xiaoshu Chen, Ke Xu, Siwei Wang, Xinwang Liu, Jian Huang</strong></p>
<p>Reasoning paths are reliable information in knowledge graph completion (KGC) in which algorithms can find strong clues of the actual relation between entities. However, in real-world applications, it is difficult to guarantee that computationally affordable paths exist toward all candidate entities. According to our observation, the prediction accuracy drops significantly when paths are absent. To make the proposed algorithm more stable against the missing path circumstances, we introduce soft reasoning paths. Concretely, a specific learnable latent path embedding is concatenated to each relation to help better model the characteristics of the corresponding paths. The combination of the relation and the corresponding learnable embedding is termed a soft path in our paper. By aligning the soft paths with the reasoning paths, a learnable embedding is guided to learn a generalized path representation of the corresponding relation. In addition, we introduce a hierarchical ranking strategy to make full use of information about the entity, relation, path, and soft path to help improve both the efficiency and accuracy of the model. Extensive experimental results illustrate that our algorithm outperforms the compared state-of-the-art algorithms by a notable margin. The code will be made publicly available after the paper is officially accepted. </p>
<blockquote>
<p>åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰ä¸­ï¼Œæ¨ç†è·¯å¾„æ˜¯å¯é çš„ä¿¡æ¯æ¥æºï¼Œç®—æ³•å¯ä»¥åœ¨å…¶ä¸­æ‰¾åˆ°å®ä½“ä¹‹é—´å®é™…å…³ç³»çš„å¼ºçƒˆçº¿ç´¢ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¾ˆéš¾ä¿è¯å¯¹æ‰€æœ‰å€™é€‰å®ä½“éƒ½å­˜åœ¨è®¡ç®—ä¸Šå¯è¡Œçš„è·¯å¾„ã€‚æ ¹æ®æˆ‘ä»¬çš„è§‚å¯Ÿï¼Œå½“è·¯å¾„ç¼ºå¤±æ—¶ï¼Œé¢„æµ‹ç²¾åº¦ä¼šå¤§å¹…ä¸‹é™ã€‚ä¸ºäº†ä½¿æ‰€æå‡ºç®—æ³•åœ¨ç¼ºå¤±è·¯å¾„çš„æƒ…å†µä¸‹æ›´åŠ ç¨³å®šï¼Œæˆ‘ä»¬å¼•å…¥äº†è½¯æ¨ç†è·¯å¾„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå…³ç³»è¿æ¥äº†ä¸€ä¸ªç‰¹å®šçš„å¯å­¦ä¹ æ½œåœ¨è·¯å¾„åµŒå…¥ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡ç›¸åº”è·¯å¾„çš„ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œå°†å…³ç³»å’Œç›¸åº”çš„å¯å­¦ä¹ åµŒå…¥çš„ç»“åˆç§°ä¸ºè½¯è·¯å¾„ã€‚é€šè¿‡å¯¹è½¯è·¯å¾„ä¸æ¨ç†è·¯å¾„è¿›è¡Œå¯¹é½ï¼Œå¯å¼•å¯¼å¯å­¦ä¹ åµŒå…¥å­¦ä¹ ç›¸åº”å…³ç³»çš„é€šç”¨è·¯å¾„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ’åºç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨å…³äºå®ä½“ã€å…³ç³»ã€è·¯å¾„å’Œè½¯è·¯å¾„çš„ä¿¡æ¯ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„ç®—æ³•ã€‚è®ºæ–‡è¢«æ­£å¼æ¥å—åï¼Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03285v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰ä¸­ï¼Œæ¨ç†è·¯å¾„æ˜¯å¯é çš„ä¿¡æ¯æ¥æºï¼Œç®—æ³•èƒ½å¤Ÿä¾é è¿™äº›è·¯å¾„æ‰¾åˆ°å®ä½“é—´å®é™…å…³ç³»çš„å¼ºçº¿ç´¢ã€‚ä½†åœ¨å®é™…åœºæ™¯ä¸­ï¼Œå¹¶éæ‰€æœ‰å€™é€‰å®ä½“éƒ½å­˜åœ¨è®¡ç®—å¯è¡Œçš„è·¯å¾„ï¼Œè¿™å¯¼è‡´é¢„æµ‹å‡†ç¡®åº¦å¤§å¹…ä¸‹é™ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è½¯æ¨ç†è·¯å¾„ã€‚å…·ä½“åšæ³•æ˜¯ä¸ºæ¯ç§å…³ç³»æ·»åŠ ä¸€ä¸ªç‰¹å®šçš„å¯å­¦ä¹ æ½œåœ¨è·¯å¾„åµŒå…¥ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿå¯¹åº”è·¯å¾„çš„ç‰¹æ€§ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºè½¯è·¯å¾„ã€‚é€šè¿‡è°ƒæ•´è½¯è·¯å¾„å’Œæ¨ç†è·¯å¾„çš„å¯¹é½ï¼Œå¯å¼•å¯¼å­¦ä¹ åµŒå…¥å­¦ä¹ å¯¹åº”å…³ç³»çš„å¹¿ä¹‰è·¯å¾„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨åˆ†å±‚æ’åºç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨å®ä½“ã€å…³ç³»ã€è·¯å¾„å’Œè½¯è·¯å¾„çš„ä¿¡æ¯ï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ˜¾è‘—ä¼˜äºç°æœ‰å…ˆè¿›ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†è·¯å¾„åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ä¸­æ˜¯é‡è¦ä¿¡æ¯æ¥æºï¼Œèƒ½å¸®åŠ©ç®—æ³•æ‰¾åˆ°å®ä½“é—´çš„å®é™…å…³ç³»çº¿ç´¢ã€‚</li>
<li>åœ¨å®é™…åœºæ™¯ä¸­ï¼Œå¹¶éæ‰€æœ‰å€™é€‰å®ä½“éƒ½å­˜åœ¨è®¡ç®—å¯è¡Œçš„æ¨ç†è·¯å¾„ï¼Œå¯¼è‡´é¢„æµ‹éš¾åº¦å¢åŠ ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†è½¯æ¨ç†è·¯å¾„ï¼Œé€šè¿‡æ·»åŠ ç‰¹å®šçš„å¯å­¦ä¹ æ½œåœ¨è·¯å¾„åµŒå…¥æ¥å¢å¼ºæ¨¡å‹å¯¹è·¯å¾„ç¼ºå¤±æƒ…å†µçš„ç¨³å®šæ€§ã€‚</li>
<li>è½¯è·¯å¾„ç»“åˆäº†å…³ç³»å’Œå¯¹åº”çš„å¯å­¦ä¹ åµŒå…¥ï¼Œé€šè¿‡è°ƒæ•´å¯¹é½æ–¹å¼å¼•å¯¼å­¦ä¹ åµŒå…¥å­¦ä¹ å¯¹åº”å…³ç³»çš„å¹¿ä¹‰è·¯å¾„è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚æ’åºç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨å„ç±»ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„æ•ˆç‡ä¸å‡†ç¡®åº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°çš„ç®—æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å…ˆè¿›ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03285">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-68aefcf80d09fb41dc5e0b263358d73b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-982e92b722a8e17221e01d4aab8cb624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa57f3414947c05f24ca684c1a93825d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d6c5e2848eef80d985b417ad72f346.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89604c8860ecf8d7e850157d77f76c3e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DYSTIL-Dynamic-Strategy-Induction-with-Large-Language-Models-for-Reinforcement-Learning"><a href="#DYSTIL-Dynamic-Strategy-Induction-with-Large-Language-Models-for-Reinforcement-Learning" class="headerlink" title="DYSTIL: Dynamic Strategy Induction with Large Language Models for   Reinforcement Learning"></a>DYSTIL: Dynamic Strategy Induction with Large Language Models for   Reinforcement Learning</h2><p><strong>Authors:Borui Wang, Kathleen McKeown, Rex Ying</strong></p>
<p>Reinforcement learning from expert demonstrations has long remained a challenging research problem, and existing state-of-the-art methods using behavioral cloning plus further RL training often suffer from poor generalization, low sample efficiency, and poor model interpretability. Inspired by the strong reasoning abilities of large language models (LLMs), we propose a novel strategy-based reinforcement learning framework integrated with LLMs called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL) to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, and gradually internalizes induced strategies into the RL agent through policy optimization to improve its performance through boosting policy generalization and enhancing sample efficiency. It also provides a direct textual channel to observe and interpret the evolution of the policyâ€™s underlying strategies during training. We test DYSTIL over challenging RL environments from Minigrid and BabyAI, and empirically demonstrate that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate while also enjoying higher sample efficiency during the learning process. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»ä¸“å®¶æ¼”ç¤ºä¸­ä¸€ç›´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ï¼Œè€Œç°æœ‰çš„æœ€æ–°æ–¹æ³•ä½¿ç”¨è¡Œä¸ºå…‹éš†åŠ ä¸Šè¿›ä¸€æ­¥çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¸¸å¸¸é¢ä¸´æ³›åŒ–èƒ½åŠ›å·®ã€æ ·æœ¬æ•ˆç‡ä½å’Œæ¨¡å‹è§£é‡Šæ€§ä¸å¼ºçš„é—®é¢˜ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å‹ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºåŸºäºLLMçš„åŠ¨æ€ç­–ç•¥å½’çº³å¼ºåŒ–å­¦ä¹ ï¼ˆDYSTILï¼‰ï¼Œä»¥å…‹æœè¿™äº›å±€é™æ€§ã€‚DYSTILåŠ¨æ€æŸ¥è¯¢ç­–ç•¥ç”Ÿæˆå‹LLMï¼ŒåŸºäºä¼˜åŠ¿è¯„ä¼°å’Œä¸“å®¶æ¼”ç¤ºæ¥å¼•å¯¼æ–‡æœ¬ç­–ç•¥ï¼Œå¹¶é€šè¿‡ç­–ç•¥ä¼˜åŒ–é€æ¸å°†å¼•å¯¼çš„ç­–ç•¥å†…åŒ–åˆ°å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­ï¼Œä»¥æé«˜å…¶æ€§èƒ½ï¼Œé€šè¿‡æé«˜ç­–ç•¥æ³›åŒ–èƒ½åŠ›å’Œæé«˜æ ·æœ¬æ•ˆç‡æ¥ä¿ƒè¿›æ€§èƒ½æå‡ã€‚å®ƒè¿˜æä¾›äº†ä¸€ä¸ªç›´æ¥çš„æ–‡æœ¬é€šé“æ¥è§‚å¯Ÿå’Œè§£é‡Šè®­ç»ƒè¿‡ç¨‹ä¸­ç­–ç•¥å†…åœ¨å˜åŒ–çš„æ¼”åŒ–ã€‚æˆ‘ä»¬åœ¨Minigridå’ŒBabyAIç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­å¯¹DYSTILè¿›è¡Œäº†æµ‹è¯•ï¼Œç»éªŒè¡¨æ˜ï¼ŒDYSTILåœ¨å¹³å‡æˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ï¼Œæé«˜äº†17.75%ï¼ŒåŒæ—¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ ·æœ¬æ•ˆç‡ä¹Ÿæ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03209v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä»ä¸“å®¶æ¼”ç¤ºä¸­ä¸€ç›´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ³›åŒ–æ€§èƒ½å·®ã€æ ·æœ¬æ•ˆç‡ä½å’Œæ¨¡å‹è§£é‡Šæ€§å·®çš„é—®é¢˜ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºæ¨ç†èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶DYSTILã€‚DYSTILé€šè¿‡ä¼˜åŠ¿ä¼°è®¡å’Œä¸“å®¶æ¼”ç¤ºåŠ¨æ€æŸ¥è¯¢ç­–ç•¥ç”Ÿæˆå‹LLMï¼Œä»¥è¯±å¯¼æ–‡æœ¬ç­–ç•¥ï¼Œå¹¶é€šè¿‡ç­–ç•¥ä¼˜åŒ–é€æ¸å°†è¯±å¯¼ç­–ç•¥å†…åŒ–åˆ°å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­ï¼Œæé«˜äº†ç­–ç•¥æ³›åŒ–å’Œæ ·æœ¬æ•ˆç‡ã€‚å®ƒè¿˜æä¾›äº†ä¸€ä¸ªç›´æ¥çš„æ–‡æœ¬é€šé“æ¥è§‚å¯Ÿå’Œè§£é‡Šè®­ç»ƒè¿‡ç¨‹ä¸­ç­–ç•¥åº•å±‚ç­–ç•¥çš„æ¼”å˜ã€‚åœ¨Minigridå’ŒBabyAIç­‰æŒ‘æˆ˜æ€§çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼ŒDYSTILåœ¨å¹³å‡æˆåŠŸç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œæé«˜äº†17.75%ï¼ŒåŒæ—¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä¹Ÿå…·æœ‰è¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆä¸“å®¶æ¼”ç¤ºä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨æ³›åŒ–ã€æ ·æœ¬æ•ˆç‡å’Œè§£é‡Šæ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶DYSTILï¼Œç»“åˆLLMè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DYSTILé€šè¿‡ä¼˜åŠ¿ä¼°è®¡å’Œä¸“å®¶æ¼”ç¤ºåŠ¨æ€æŸ¥è¯¢LLMä»¥è¯±å¯¼æ–‡æœ¬ç­–ç•¥ã€‚</li>
<li>DYSTILé€šè¿‡ç­–ç•¥ä¼˜åŒ–å°†è¯±å¯¼ç­–ç•¥é€æ¸å†…åŒ–åˆ°å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­ã€‚</li>
<li>DYSTILæé«˜äº†ç­–ç•¥æ³›åŒ–å’Œæ ·æœ¬æ•ˆç‡ã€‚</li>
<li>DYSTILæä¾›äº†ä¸€ä¸ªç›´æ¥çš„æ–‡æœ¬é€šé“æ¥è§‚å¯Ÿå¹¶è§£é‡Šç­–ç•¥æ¼”å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-629709ef090efd27df53ba27b029e5c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48da9301a867503c4e0f83f23e839187.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0862e6b4d850f53b5b933175173dd37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48030ec10bbfdf4d5eb21b2c6637687f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VLM-Q-Learning-Aligning-Vision-Language-Models-for-Interactive-Decision-Making"><a href="#VLM-Q-Learning-Aligning-Vision-Language-Models-for-Interactive-Decision-Making" class="headerlink" title="VLM Q-Learning: Aligning Vision-Language Models for Interactive   Decision-Making"></a>VLM Q-Learning: Aligning Vision-Language Models for Interactive   Decision-Making</h2><p><strong>Authors:Jake Grigsby, Yuke Zhu, Michael Ryoo, Juan Carlos Niebles</strong></p>
<p>Recent research looks to harness the general knowledge and reasoning of large language models (LLMs) into agents that accomplish user-specified goals in interactive environments. Vision-language models (VLMs) extend LLMs to multi-modal data and provide agents with the visual reasoning necessary for new applications in areas such as computer automation. However, agent tasks emphasize skills where accessible open-weight VLMs lag behind their LLM equivalents. For example, VLMs are less capable of following an environmentâ€™s strict output syntax requirements and are more focused on open-ended question answering. Overcoming these limitations requires supervised fine-tuning (SFT) on task-specific expert demonstrations. Our work approaches these challenges from an offline-to-online reinforcement learning (RL) perspective. RL lets us fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of our own model or more capable (larger) models. We explore an off-policy RL solution that retains the stability and simplicity of the widely used SFT workflow while allowing our agent to self-improve and learn from low-quality datasets. We demonstrate this technique with two open-weight VLMs across three multi-modal agent domains. </p>
<blockquote>
<p>æœ€æ–°çš„ç ”ç©¶æ—¨åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€šç”¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›è½¬åŒ–ä¸ºèƒ½å¤Ÿåœ¨äº¤äº’å¼ç¯å¢ƒä¸­å®Œæˆç”¨æˆ·æŒ‡å®šç›®æ ‡çš„ä»£ç†ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°†LLMæ‰©å±•åˆ°å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶ä¸ºä»£ç†æä¾›è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œå¯¹äºè®¡ç®—æœºè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸçš„æ–°åº”ç”¨æ¥è¯´ï¼Œè¿™æ˜¯å¿…è¦çš„ã€‚ç„¶è€Œï¼Œä»£ç†ä»»åŠ¡å¼ºè°ƒçš„æ˜¯å¯ç”¨å¼€æ”¾æƒé‡VLMè½åäºå…¶LLMç­‰æ•ˆé¡¹çš„æŠ€èƒ½ã€‚ä¾‹å¦‚ï¼ŒVLMåœ¨éµå¾ªç¯å¢ƒçš„ä¸¥æ ¼è¾“å‡ºè¯­æ³•è¦æ±‚æ–¹é¢èƒ½åŠ›è¾ƒå·®ï¼Œæ›´ä¾§é‡äºå¼€æ”¾å¼é—®ç­”ã€‚è¦å…‹æœè¿™äº›é™åˆ¶ï¼Œéœ€è¦å¯¹ç‰¹å®šä»»åŠ¡çš„ä¸“å®¶æ¼”ç¤ºè¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œä»ç¦»çº¿åˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§’åº¦æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ è®©æˆ‘ä»¬èƒ½å¤Ÿåœ¨éµå¾ªå¹¿æ³›ä½¿ç”¨çš„SFTå·¥ä½œæµç¨‹çš„ç¨³å®šæ€§å’Œç®€å•æ€§çš„åŒæ—¶ï¼Œå¯¹VLMè¿›è¡Œå¾®è°ƒä»¥æ‰§è¡Œä»£ç†ä»»åŠ¡ï¼Œå¹¶ä»æˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹æˆ–æ›´å¼ºå¤§çš„æ¨¡å‹çš„å¤±è´¥å†³ç­–ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§ç¦»çº¿ç­–ç•¥RLè§£å†³æ–¹æ¡ˆï¼Œå…è®¸æˆ‘ä»¬çš„ä»£ç†è‡ªæˆ‘æ”¹è¿›å¹¶ä»ä½è´¨é‡æ•°æ®é›†ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€ä»£ç†é¢†åŸŸä½¿ç”¨ä¸¤ä¸ªå¼€æ”¾æƒé‡VLMå¯¹æ­¤æŠ€æœ¯è¿›è¡Œäº†æ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03181v1">PDF</a> SSI-FM Workshop ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€šç”¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›è¢«ç”¨äºæ„å»ºèƒ½åœ¨äº’åŠ¨ç¯å¢ƒä¸­å®Œæˆç”¨æˆ·æŒ‡å®šä»»åŠ¡çš„æ™ºèƒ½ä»£ç†ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰©å±•äº†LLMsï¼Œä½¿å…¶èƒ½å¤„ç†å¤šæ¨¡å¼æ•°æ®ï¼Œå¹¶ä¸ºä»£ç†æä¾›è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä»è€Œé€‚ç”¨äºè®¡ç®—æœºè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸçš„æ–°åº”ç”¨ã€‚ç„¶è€Œï¼ŒVLMsåœ¨æŸäº›æŠ€èƒ½æ–¹é¢è½åäºLLMsï¼Œå¦‚éµå¾ªç¯å¢ƒçš„ä¸¥æ ¼è¾“å‡ºè¯­æ³•è¦æ±‚å’Œå¤„ç†å¼€æ”¾å¼é—®ç­”ç­‰ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œéœ€è¦åœ¨ä»»åŠ¡ç‰¹å®šçš„ä¸“å®¶æ¼”ç¤ºä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬ç ”ç©¶ä»çº¿ä¸‹åˆ°çº¿ä¸Šçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§’åº¦åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚RLå…è®¸æˆ‘ä»¬å¾®è°ƒVLMsä»¥å®Œæˆä»£ç†ä»»åŠ¡ï¼ŒåŒæ—¶ä»æˆ‘ä»¬è‡ªå·±æ¨¡å‹çš„å¤±è´¥å†³ç­–æˆ–æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§ç¦»ç­–ç•¥RLè§£å†³æ–¹æ¡ˆï¼Œå®ƒä¿ç•™äº†å¹¿æ³›ä½¿ç”¨çš„SFTå·¥ä½œæµçš„ç¨³å®šæ€§å’Œç®€å•æ€§ï¼ŒåŒæ—¶å…è®¸æˆ‘ä»¬çš„ä»£ç†è‡ªæˆ‘æ”¹è¿›å¹¶ä»ä½è´¨é‡æ•°æ®é›†ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¤šæ¨¡å¼ä»£ç†é¢†åŸŸä½¿ç”¨ä¸¤ä¸ªå¼€æºVLMsæ¼”ç¤ºäº†è¿™é¡¹æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡é€šç”¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œè¢«ç”¨äºæ„å»ºæ™ºèƒ½ä»£ç†ä»¥å®Œæˆç”¨æˆ·æŒ‡å®šä»»åŠ¡ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰©å±•äº†LLMsï¼Œå¤„ç†å¤šæ¨¡å¼æ•°æ®å¹¶å…·å¤‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºæ–°åº”ç”¨é¢†åŸŸã€‚</li>
<li>VLMsåœ¨æŸäº›æŠ€èƒ½æ–¹é¢ç›¸å¯¹è½åäºLLMsï¼Œå¦‚ä¸¥æ ¼è¾“å‡ºè¯­æ³•è¦æ±‚çš„éµå¾ªå’Œå¼€æ”¾å¼é—®ç­”å¤„ç†ã€‚</li>
<li>ä¸ºäº†å…‹æœVLMsçš„é™åˆ¶ï¼Œéœ€åœ¨ä»»åŠ¡ç‰¹å®šçš„ä¸“å®¶æ¼”ç¤ºä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å…è®¸å¯¹VLMsè¿›è¡Œå¾®è°ƒï¼Œä»¥å®Œæˆä»£ç†ä»»åŠ¡ï¼Œå¹¶ä»æ¨¡å‹è‡ªèº«çš„å¤±è´¥å†³ç­–æˆ–æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­å­¦ä¹ ã€‚</li>
<li>æå‡ºçš„ç¦»ç­–ç•¥RLè§£å†³æ–¹æ¡ˆä¿ç•™äº†SFTçš„ç¨³å®šæ€§ã€ç®€å•æ€§ï¼Œå¹¶å…è®¸ä»£ç†è‡ªæˆ‘æ”¹è¿›ï¼Œä»ä½è´¨é‡æ•°æ®é›†ä¸­å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03181">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1fa3fb345b560e51d6c17b4f1aa0ead6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2eb6dd1d3d1ce6e2fa822a8e096dadd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdaddd3a978fedbcb05cd9f876fd4aa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b525326a3590a892bd7b111cc0420821.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RAVU-Retrieval-Augmented-Video-Understanding-with-Compositional-Reasoning-over-Graph"><a href="#RAVU-Retrieval-Augmented-Video-Understanding-with-Compositional-Reasoning-over-Graph" class="headerlink" title="RAVU: Retrieval Augmented Video Understanding with Compositional   Reasoning over Graph"></a>RAVU: Retrieval Augmented Video Understanding with Compositional   Reasoning over Graph</h2><p><strong>Authors:Sameer Malik, Moyuru Yamada, Ayush Singh, Dishank Aggarwal</strong></p>
<p>Comprehending long videos remains a significant challenge for Large Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. To address this limitation, we propose RAVU (Retrieval Augmented Video Understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. We construct a graph representation of the video, capturing both spatial and temporal relationships between entities. This graph serves as a long-term memory, allowing us to track objects and their actions across time. To answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. Our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. Our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other SOTA methods and baselines on two major video QA datasets, NExT-QA and EgoSchema. </p>
<blockquote>
<p>ç†è§£é•¿è§†é¢‘å¯¹äºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç”±äºç¼ºå°‘æ˜ç¡®çš„è®°å¿†å’Œæ£€ç´¢æœºåˆ¶ï¼Œå½“å‰çš„LMMsåœ¨å¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RAVUï¼ˆåŸºäºæ£€ç´¢å¢å¼ºçš„è§†é¢‘ç†è§£ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ—¶ç©ºå›¾ä¸Šçš„ç»„åˆæ¨ç†è¿›è¡Œæ£€ç´¢å¢å¼ºè§†é¢‘ç†è§£çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬æ„å»ºäº†è§†é¢‘çš„å›¾è¡¨ç¤ºï¼Œæ•æ‰å®ä½“ä¹‹é—´çš„ç©ºé—´å’Œæ—¶é—´å…³ç³»ã€‚è¿™ä¸ªå›¾ä½œä¸ºä¸€ä¸ªé•¿æœŸè®°å¿†ï¼Œå…è®¸æˆ‘ä»¬è·Ÿè¸ªç‰©ä½“åŠå…¶éšæ—¶é—´å˜åŒ–çš„è¡Œä¸ºã€‚ä¸ºäº†å›ç­”å¤æ‚çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬å°†æŸ¥è¯¢åˆ†è§£æˆä¸€ç³»åˆ—çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶åœ¨å›¾ä¸Šæ‰§è¡Œè¿™äº›æ­¥éª¤ï¼Œæ£€ç´¢ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—å¯¹é•¿è§†é¢‘çš„å‡†ç¡®ç†è§£æ›´åŠ å¯èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéœ€è¦å¤šè·³æ¨ç†å’Œè·¨å¸§è·Ÿè¸ªç‰©ä½“çš„æŸ¥è¯¢ã€‚åœ¨ä¸¤ä¸ªä¸»è¦çš„è§†é¢‘é—®ç­”æ•°æ®é›†NExT-QAå’ŒEgoSchemaä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰é™çš„æ£€ç´¢å¸§ï¼ˆ5-10å¸§ï¼‰å†…è¡¨ç°å‡ºäº†ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’ŒåŸºå‡†çº¿çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03173v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»¥ç®€æ´æ˜äº†çš„æ–¹å¼æè¿°äº†è§£å†³å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç†è§£é•¿è§†é¢‘æ—¶æ‰€é¢ä¸´çš„å›°å¢ƒçš„æœ€æ–°è¿›å±•ã€‚é¢å¯¹é•¿è§†é¢‘çš„é•¿æ—¶é—´å¤„ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRAVUçš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ—¶ç©ºå›¾ä¸Šçš„ç»„åˆæ¨ç†å¢å¼ºæ£€ç´¢èƒ½åŠ›ã€‚è¯¥æ¡†æ¶æ„å»ºè§†é¢‘çš„å›¾è¡¨ç¤ºï¼Œæ•æ‰å®ä½“ä¹‹é—´çš„ç©ºé—´å’Œæ—¶é—´å…³ç³»ï¼Œä½œä¸ºé•¿æœŸè®°å¿†è·Ÿè¸ªç‰©ä½“å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ†è§£æŸ¥è¯¢å¹¶æ‰§è¡Œå›¾å½¢ä¸Šçš„æ¨ç†æ­¥éª¤æ¥å›ç­”å¤æ‚çš„æŸ¥è¯¢ï¼Œæé«˜äº†å¯¹é•¿è§†é¢‘çš„ç†è§£å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè·³æ¨ç†å’Œè·¨å¸§è·Ÿè¸ªçš„æŸ¥è¯¢ä¸Šã€‚åœ¨ä¸¤ä¸ªä¸»è¦è§†é¢‘é—®ç­”æ•°æ®é›†ä¸Šç›¸æ¯”å…¶ä»–å‰æ²¿æ–¹æ³•å’ŒåŸºçº¿è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ‘˜è¦æ¥æºäºä¸Šè¿°æ–‡æœ¬å¹¶æ§åˆ¶åœ¨äº†ä¸€ç™¾å­—ä»¥å†…ã€‚ </p>
<p><strong>Key Takeaways</strong>ï¼šä»æ–‡æœ¬ä¸­æŠ½å–å‡ºçš„å…³é”®ä¿¡æ¯å¯ä»¥æ•´ç†æˆä»¥ä¸‹å‡ ç‚¹ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87273acb1ad537fef59e3e4bf70b9f09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7635a278516035d45b2cf4f811da0842.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4feace886da341646c410829db2b8e0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-973b74e8b6355fd5bbb1cc26b4583c2c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-affdaabd79f3ecb0e6e6963c123542ce.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CombiBench-Benchmarking-LLM-Capability-for-Combinatorial-Mathematics"><a href="#CombiBench-Benchmarking-LLM-Capability-for-Combinatorial-Mathematics" class="headerlink" title="CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics"></a>CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics</h2><p><strong>Authors:Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibei Xiong, Zhengfeng Yang, Jujian Zhang, Lihong Zhi, Jia Li, Zhengying Liu</strong></p>
<p>Neurosymbolic approaches integrating large language models with formal reasoning have recently achieved human-level performance on mathematics competition problems in algebra, geometry and number theory. In comparison, combinatorics remains a challenging domain, characterized by a lack of appropriate benchmarks and theorem libraries. To address this gap, we introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean~4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level, and span over ten combinatorial topics. CombiBench is suitable for testing IMO solving capabilities since it includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its statement contain an images). Furthermore, we provide a comprehensive and standardized evaluation framework, dubbed Fine-Eval (for $\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for formal mathematics. It accommodates not only proof-based problems but also, for the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval as the evaluation method and Kimina Lean Server as the backend, we benchmark several LLMs on CombiBench and observe that their capabilities for formally solving combinatorial problems remain limited. Among all models tested (none of which has been trained for this particular task), Kimina-Prover attains the best results, solving 7 problems (out of 100) under both <code>with solution&#39;&#39; and </code>without solutionâ€™â€™ scenarios. We open source the benchmark dataset alongside with the code of the proposed evaluation method at <a target="_blank" rel="noopener" href="https://github.com/MoonshotAI/CombiBench/">https://github.com/MoonshotAI/CombiBench/</a>. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å½¢å¼æ¨ç†ç›¸ç»“åˆçš„ç¥ç»è¥ç•¥æ–¹æ³•ï¼Œæœ€è¿‘å·²åœ¨ä»£æ•°ã€å‡ ä½•å’Œæ•°è®ºçš„æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¾¾åˆ°äº†äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»„åˆå­¦ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œå…¶ç‰¹ç‚¹æ˜¯ç¼ºä¹é€‚å½“çš„åŸºå‡†æµ‹è¯•å’Œå®šç†åº“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†CombiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªç»„åˆé—®é¢˜çš„å…¨é¢åŸºå‡†æµ‹è¯•é›†ï¼Œæ¯ä¸ªé—®é¢˜éƒ½åœ¨Lean~4ä¸­å½¢å¼åŒ–ï¼Œå¹¶é…æœ‰å…¶ç›¸åº”çš„éæ­£å¼é™ˆè¿°ã€‚è¯¥é—®é¢˜é›†æ¶µç›–äº†ä»ä¸­å­¦åˆ°IMOå’Œå¤§å­¦æ°´å¹³çš„å¹¿æ³›éš¾åº¦çº§åˆ«ï¼Œå¹¶æ¶µç›–äº†åå¤šä¸ªç»„åˆä¸»é¢˜ã€‚CombiBenché€‚åˆç”¨äºæµ‹è¯•IMOè§£é¢˜èƒ½åŠ›ï¼Œå› ä¸ºå®ƒåŒ…å«äº†è‡ª2000å¹´ä»¥æ¥æ‰€æœ‰çš„IMOç»„åˆé—®é¢˜ï¼ˆé™¤IMO 2004 P3å› å…¶é™ˆè¿°ä¸­åŒ…å«å›¾åƒè€Œé™¤å¤–ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œç§°ä¸ºFine-Evalï¼ˆç”¨äº$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluationï¼‰ï¼Œç”¨äºå½¢å¼æ•°å­¦ã€‚å®ƒä¸ä»…é€‚ç”¨äºåŸºäºè¯æ˜çš„é—®é¢˜ï¼Œè€Œä¸”é¦–æ¬¡å®ç°äº†å¡«å……ç©ºç™½é—®é¢˜çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨Fine-Evalä½œä¸ºè¯„ä¼°æ–¹æ³•ï¼ŒKiminna Lean Serverä½œä¸ºåç«¯ï¼Œåœ¨CombiBenchä¸Šå¯¹å‡ ä¸ªLLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è§‚å¯Ÿåˆ°å®ƒä»¬åœ¨æ­£å¼è§£å†³ç»„åˆé—®é¢˜ä¸Šèƒ½åŠ›æœ‰é™ã€‚åœ¨æµ‹è¯•çš„æ‰€æœ‰æ¨¡å‹ä¸­ï¼ˆæ²¡æœ‰ä»»ä½•æ¨¡å‹æ¥å—è¿‡æ­¤é¡¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼‰ï¼ŒKiminna Proverå–å¾—äº†æœ€å¥½çš„ç»“æœï¼Œåœ¨â€œæœ‰è§£å†³æ–¹æ¡ˆâ€å’Œâ€œæ— è§£å†³æ–¹æ¡ˆâ€çš„åœºæ™¯ä¸‹å‡è§£å†³äº†7ä¸ªé—®é¢˜ï¼ˆæ€»å…±100ä¸ªé—®é¢˜ï¼‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/MoonshotAI/CombiBench/">https://github.com/MoonshotAI/CombiBench/</a>ä¸Šå…¬å¼€åŸºå‡†æµ‹è¯•æ•°æ®é›†ä»¥åŠæ‰€æå‡ºè¯„ä¼°æ–¹æ³•çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03171v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹ç»„åˆæ•°å­¦é¢†åŸŸçš„ä¸€ä¸ªæ–°åŸºå‡†æµ‹è¯•å¥—ä»¶CombiBenchã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«äº†å¤šç§éš¾åº¦çº§åˆ«çš„ç»„åˆé—®é¢˜ï¼Œå¹¶ä¸”é‡‡ç”¨äº†å…¨æ–°çš„è¯„ä¼°æ¡†æ¶Fine-Evalã€‚æ–‡ç« æŒ‡å‡ºå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŸäº›æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¾¾åˆ°äº†äººç±»æ°´å¹³çš„è¡¨ç°ï¼Œä½†åœ¨ç»„åˆæ•°å­¦é¢†åŸŸçš„æ€§èƒ½ä»ç„¶æœ‰é™ã€‚ç›®å‰æœ€å¥½çš„æ¨¡å‹Kiminar-Proveråœ¨åŸºå‡†æµ‹è¯•ä¸­ä»…è§£å†³äº†å…¶ä¸­çš„ä¸ƒä¸ªé—®é¢˜ã€‚ç›¸å…³èµ„æºå·²å¼€æºä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CombiBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„ç»„åˆæ•°å­¦åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«ä»ä¸­å­¦è‡³å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆIMOï¼‰åŠå¤§å­¦çº§åˆ«çš„å¹¿æ³›ç»„åˆé—®é¢˜ã€‚</li>
<li>Fine-Evalæ˜¯ä¸€ä¸ªå…¨æ–°çš„æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå¯ç”¨äºå½¢å¼æ•°å­¦çš„è¯„ä¼°ï¼Œæ”¯æŒè¯æ˜é¢˜åŠå¡«ç©ºé¢˜çš„è¯„ä»·ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»„åˆæ•°å­¦é¢†åŸŸçš„æ€§èƒ½ä»ç„¶æœ‰é™ï¼Œå³ä½¿åœ¨æŸäº›æ•°å­¦ç«èµ›é—®é¢˜ä¸Šè¡¨ç°å‡ºæ¥è¿‘äººç±»æ°´å¹³çš„èƒ½åŠ›ã€‚</li>
<li>Kimina Lean Serverä½œä¸ºåç«¯å·¥å…·ï¼Œç”¨äºåŸºå‡†æµ‹è¯•ã€‚</li>
<li>CombiBenchä¸­æœ€ä¼˜ç§€çš„æ¨¡å‹æ˜¯Kiminar-Proverï¼Œåœ¨æµ‹è¯•ä¸­è§£å†³äº†ä¸ƒä¸ªé—®é¢˜ã€‚</li>
<li>CombiBenchåŠç›¸å…³èµ„æºå·²å¼€æºï¼Œå¯ä¾›å…¬ä¼—ä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-36143a1c4d00c1d84691bce1f1970609.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be0e7d7a82bcdee66619104590cc61d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ba04d8097495a6fd148da666bc95c96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a74ed58c3cdf9e9fc832cab1b555e2c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MORE-Mobile-Manipulation-Rearrangement-Through-Grounded-Language-Reasoning"><a href="#MORE-Mobile-Manipulation-Rearrangement-Through-Grounded-Language-Reasoning" class="headerlink" title="MORE: Mobile Manipulation Rearrangement Through Grounded Language   Reasoning"></a>MORE: Mobile Manipulation Rearrangement Through Grounded Language   Reasoning</h2><p><strong>Authors:Mohammad Mohammadi, Daniel Honerkamp, Martin BÃ¼chner, Matteo Cassinelli, Tim Welschehold, Fabien Despinoy, Igor Gilitschenski, Abhinav Valada</strong></p>
<p>Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at <a target="_blank" rel="noopener" href="https://more-model.cs.uni-freiburg.de/">https://more-model.cs.uni-freiburg.de</a>. </p>
<blockquote>
<p>è‡ªä¸»é•¿æœŸç§»åŠ¨æ“ä½œæ¶µç›–äº†ä¼—å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœºæ™¯åŠ¨æ€å˜åŒ–ã€æœªçŸ¥åŒºåŸŸå’Œé”™è¯¯æ¢å¤ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œåœºæ™¯çº§åˆ«çš„æœºå™¨äººæ¨ç†å’Œè§„åˆ’ã€‚ç„¶è€Œï¼Œå½“å¤„ç†å¤§é‡ç‰©ä½“å’Œå¤§è§„æ¨¡ç¯å¢ƒæ—¶ï¼Œè¿™äº›æ–¹æ³•çš„æ€§èƒ½ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MOREï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºè¯­è¨€æ¨¡å‹è§£å†³èƒ½åŠ›çš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³é›¶ç§»åŠ¨æ“ä½œè§„åˆ’ä»»åŠ¡ä¸­çš„é‡ç»„ä»»åŠ¡ã€‚MOREåˆ©ç”¨åœºæ™¯å›¾æ¥è¡¨ç¤ºç¯å¢ƒï¼Œç»“åˆå®ä¾‹å·®å¼‚ï¼Œå¹¶å¼•å…¥ä¸»åŠ¨è¿‡æ»¤æ–¹æ¡ˆï¼Œæå–ä»»åŠ¡ç›¸å…³çš„å­å›¾å¯¹è±¡å’ŒåŒºåŸŸå®ä¾‹ã€‚è¿™äº›æ­¥éª¤äº§ç”Ÿäº†æœ‰ç•Œè§„åˆ’é—®é¢˜ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰ç°è±¡ï¼Œæé«˜äº†å¯é æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€äº›æ”¹è¿›åŠŸèƒ½ï¼Œä½¿è§„åˆ’èƒ½å¤Ÿåœ¨å®¤å†…å’Œå®¤å¤–ç¯å¢ƒä¸­è¿›è¡Œã€‚æˆ‘ä»¬åœ¨BEHAVIOR-1KåŸºå‡†æµ‹è¯•ä¸Šçš„81é¡¹å¤šæ ·åŒ–çš„é‡ç»„ä»»åŠ¡ä¸­è¯„ä¼°äº†MOREçš„æ€§èƒ½ï¼Œå®ƒæˆä¸ºç¬¬ä¸€ä¸ªæˆåŠŸè§£å†³è¯¥åŸºå‡†æµ‹è¯•ä¸­å¤§éƒ¨åˆ†ä»»åŠ¡çš„æ–¹æ³•ï¼Œè¶…è¶Šäº†è¿‘æœŸåŸºäºåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨æ¨¡ä»¿æ—¥å¸¸æ´»åŠ¨çš„å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://more-model.cs.uni-freiburg.de./">https://more-model.cs.uni-freiburg.deã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03035v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è‡ªä¸»é•¿è¿œè§†è§’ç§»åŠ¨æ“æ§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åœºæ™¯åŠ¨æ€ã€æœªçŸ¥åŒºåŸŸå’Œé”™è¯¯æ¢å¤ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMOREçš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹è§£å†³é›¶ç§»åŠ¨æ“æ§è§„åˆ’ä»»åŠ¡çš„èƒ½åŠ›ã€‚MOREåˆ©ç”¨åœºæ™¯å›¾è¡¨ç¤ºç¯å¢ƒï¼Œå®ç°å®ä¾‹åŒºåˆ†ï¼Œå¹¶å¼•å…¥ä¸»åŠ¨è¿‡æ»¤æ–¹æ¡ˆï¼Œæå–ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚è¿™äº›æ­¥éª¤è§£å†³äº†è§„åˆ’é—®é¢˜ï¼Œæé«˜äº†å¯é æ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åœ¨å®¤å†…å’Œå®¤å¤–ç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’çš„å‡ é¡¹æ”¹è¿›ã€‚åœ¨BEHAVIOR-1KåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMOREæˆåŠŸè§£å†³äº†å¤§é‡ä»»åŠ¡ï¼Œä¼˜äºè¿‘æœŸåŸºäºåŸºç¡€æ¨¡å‹çš„æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€åœ¨[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»é•¿è¿œè§†è§’ç§»åŠ¨æ“æ§é¢ä¸´åœºæ™¯åŠ¨æ€ã€æœªçŸ¥åŒºåŸŸå’Œé”™è¯¯æ¢å¤ç­‰æŒ‘æˆ˜ã€‚</li>
<li>è¿‘æœŸæ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œåœºæ™¯çº§æœºå™¨äººæ¨ç†å’Œè§„åˆ’ï¼Œä½†åœ¨å¤„ç†å¤§é‡ç‰©ä½“å’Œå¤§è§„æ¨¡ç¯å¢ƒæ—¶æ€§èƒ½ä¸‹é™ã€‚</li>
<li>MOREæ–¹æ³•æ—¨åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹è§£å†³é›¶ç§»åŠ¨æ“æ§è§„åˆ’ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>MOREåˆ©ç”¨åœºæ™¯å›¾è¡¨ç¤ºç¯å¢ƒï¼Œå®ç°å®ä¾‹åŒºåˆ†å’Œä¸»åŠ¨è¿‡æ»¤æ–¹æ¡ˆæå–ä»»åŠ¡ç›¸å…³å­å›¾ã€‚</li>
<li>MOREæ–¹æ³•æœ‰æ•ˆè§£å†³äº†è§„åˆ’é—®é¢˜ï¼Œæé«˜äº†å¯é æ€§å’Œæ•ˆç‡ã€‚</li>
<li>MOREæ–¹æ³•åœ¨BEHAVIOR-1KåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸè§£å†³å¤§é‡ä»»åŠ¡ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97fe9ffe61e1325dc5816c48afb7249f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7bed3d0878b786cb730b283893a94d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4c953956e9382885412debdc8b46c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cd19b5f1e86938a797ebfe39bed4be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c205869e53d64e37643445ab301589.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="R1-Reward-Training-Multimodal-Reward-Model-Through-Stable-Reinforcement-Learning"><a href="#R1-Reward-Training-Multimodal-Reward-Model-Through-Stable-Reinforcement-Learning" class="headerlink" title="R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement   Learning"></a>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement   Learning</h2><p><strong>Authors:Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang</strong></p>
<p>Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4%$ improvement on the VL Reward-Bench and a $14.3%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Rewardâ€™s performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ€§èƒ½ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›MRMçš„æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ•°æ®ä¸Šï¼Œä½†å¯¹äºå¥–åŠ±æ¨¡å‹çš„é•¿è¿œæ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§ä»¥åŠå¦‚ä½•æ¿€æ´»è¿™äº›èƒ½åŠ›åœ¨MRMä¸­çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¦‚ä½•ç”¨äºæé«˜å¥–åŠ±å»ºæ¨¡çš„æ•ˆæœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å¥–åŠ±å»ºæ¨¡é—®é¢˜é‡æ–°åˆ¶å®šä¸ºä¸€ä¸ªåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç›´æ¥åº”ç”¨ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚Reinforce++ï¼‰è¿›è¡Œå¥–åŠ±å»ºæ¨¡å¾€å¾€ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šç”šè‡³å´©æºƒï¼Œè¿™æ˜¯ç”±äºè¿™äº›ç®—æ³•æœ¬èº«çš„å±€é™æ€§æ‰€è‡´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StableReinforceç®—æ³•ï¼Œè¯¥ç®—æ³•å¯¹ç°æœ‰çš„RLæ–¹æ³•çš„è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡è¿›è¡Œäº†æ”¹è¿›ã€‚è¿™äº›æ”¹è¿›å¸¦æ¥äº†æ›´ç¨³å®šçš„è®­ç»ƒåŠ¨åŠ›å’Œæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä¸ºäº†ä¿ƒè¿›MRMçš„è®­ç»ƒï¼Œæˆ‘ä»¬ä»å„ç§æ•°æ®é›†ä¸­æ”¶é›†äº†20ä¸‡æ¡åå¥½æ•°æ®ã€‚æˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹R1-Rewardï¼Œä½¿ç”¨StableReinforceç®—æ³•åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤šåª’ä½“å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚ä¸ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ç›¸æ¯”ï¼ŒR1-Rewardåœ¨VL Reward-Benchä¸Šå®ç°äº†8.4%çš„æ”¹è¿›ï¼Œåœ¨å¤šåª’ä½“å¥–åŠ±åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†14.3%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œéšç€æ›´å¤šçš„æ¨ç†è®¡ç®—ï¼ŒR1-Rewardçš„æ€§èƒ½å¾—åˆ°äº†è¿›ä¸€æ­¥æé«˜ï¼Œè¿™çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¼˜åŒ–MRMæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02835v1">PDF</a> Home page: <a target="_blank" rel="noopener" href="https://github.com/yfzhang114/r1_reward">https://github.com/yfzhang114/r1_reward</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰åœ¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„é‡è¦æ€§åŠå…¶ä¼˜åŒ–æ–¹æ³•ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†StableReinforceç®—æ³•æ¥è§£å†³ç°æœ‰RLç®—æ³•åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚é€šè¿‡æ”¹è¿›è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡ï¼ŒStableReinforceç®—æ³•ä½¿è®­ç»ƒè¿‡ç¨‹æ›´åŠ ç¨³å®šï¼Œæ€§èƒ½æ›´åŠ ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŸºäºè¯¥ç®—æ³•è®­ç»ƒçš„R1-Rewardæ¨¡å‹åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œç›¸è¾ƒäºå…ˆå‰æœ€å…ˆè¿›æ¨¡å‹æœ‰æ˜æ˜¾æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆMRMsï¼‰å¯¹å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„åº”ç”¨æœ‰æœ›æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>StableReinforceç®—æ³•è§£å†³äº†ç°æœ‰RLç®—æ³•åœ¨å¥–åŠ±å»ºæ¨¡ä¸­çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>R1-Rewardæ¨¡å‹é€šè¿‡æ”¹è¿›è®­ç»ƒæŸå¤±ã€ä¼˜åŠ¿ä¼°è®¡ç­–ç•¥å’Œå¥–åŠ±è®¾è®¡ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
<li>R1-Rewardæ¨¡å‹åœ¨å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰æœ€å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>R1-Rewardæ¨¡å‹åœ¨å¢åŠ æ¨ç†è®¡ç®—çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a7cbcc4ab4fb4fb5912664463d86fc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b6b63dd86c9800b9990b0d787f46454.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FormalMATH-Benchmarking-Formal-Mathematical-Reasoning-of-Large-Language-Models"><a href="#FormalMATH-Benchmarking-Formal-Mathematical-Reasoning-of-Large-Language-Models" class="headerlink" title="FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language   Models"></a>FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language   Models</h2><p><strong>Authors:Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu</strong></p>
<p>Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning. </p>
<blockquote>
<p>æ•°å­¦é€»è¾‘æ¨ç†ä»æ˜¯äººå·¥æ™ºèƒ½é¢ä¸´çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•çš„èŒƒå›´å’Œè§„æ¨¡å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FormalMATHï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡Lean4åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«5560ä¸ªç»è¿‡æ­£å¼éªŒè¯çš„é—®é¢˜ï¼Œæ¶µç›–ä»é«˜ä¸­å¥¥æ—åŒ¹å…‹æŒ‘æˆ˜åˆ°ä¸åŒé¢†åŸŸï¼ˆå¦‚ä»£æ•°ã€åº”ç”¨æ•°å­¦ã€å¾®ç§¯åˆ†ã€æ•°è®ºå’Œç¦»æ•£æ•°å­¦ï¼‰çš„æœ¬ç§‘å®šç†ã€‚ä¸ºäº†ç¼“è§£æ‰‹åŠ¨å½¢å¼åŒ–çš„ä½æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹äººæœºç»“åˆè‡ªåŠ¨å½¢å¼åŒ–ç®¡é“ï¼Œé›†æˆäº†ï¼šï¼ˆ1ï¼‰ç”¨äºè¯­å¥è‡ªåŠ¨å½¢å¼åŒ–çš„ä¸“ä¸šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œï¼ˆ2ï¼‰å¤šLLMè¯­ä¹‰éªŒè¯ï¼Œä»¥åŠï¼ˆ3ï¼‰ä½¿ç”¨ç°æˆçš„LLMè¯æ˜ä¹¦çš„å¦å®šè¯æ˜è¿‡æ»¤ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¿ç•™72.09%çš„è¯­å¥åœ¨æ‰‹åŠ¨éªŒè¯ä¹‹å‰ç¡®ä¿äº†ä¸åŸå§‹è‡ªç„¶è¯­è¨€é—®é¢˜çš„å¿ å®åº¦ï¼Œé™ä½äº†ä¸“å®¶æ³¨é‡Šæˆæœ¬ã€‚æˆ‘ä»¬å¯¹æœ€æ–°çš„åŸºäºLLMçš„å®šç†è¯æ˜å™¨çš„è¯„ä¼°è¡¨æ˜å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šå³ä½¿åœ¨å®ç”¨çš„é‡‡æ ·é¢„ç®—ä¸‹ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹æˆåŠŸç‡ä¹Ÿåªæœ‰16.46%ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„é¢†åŸŸåè§ï¼ˆä¾‹å¦‚åœ¨ä»£æ•°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¾®ç§¯åˆ†æ–¹é¢å¤±è´¥ï¼‰å’Œå¯¹ç®€åŒ–è‡ªåŠ¨åŒ–ç­–ç•¥çš„è¿‡åº¦ä¾èµ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°è‡ªç„¶è¯­è¨€è§£å†³æ–¹æ¡ˆæŒ‡å¯¼ä¸è¯æ˜æˆåŠŸä¹‹é—´å­˜åœ¨ä¸€ç§åç›´è§‰çš„é€†å‘å…³ç³»ï¼Œè¿™è¡¨æ˜äººç±»ç¼–å†™çš„éæ­£å¼æ¨ç†åœ¨æ­£å¼æ¨ç†åœºæ™¯ä¸­å¼•å…¥å™ªéŸ³è€Œéæ¸…æ™°åº¦ã€‚æˆ‘ä»¬ç›¸ä¿¡FormalMATHä¸ºåŸºå‡†æµ‹è¯•æ•°å­¦é€»è¾‘æ¨ç†æä¾›äº†ä¸€ä¸ªç¨³å¥çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02735v1">PDF</a> Technical Report v1 (33 pages, 8 figures, project page:   <a target="_blank" rel="noopener" href="https://sphere-ai-lab.github.io/FormalMATH/">https://sphere-ai-lab.github.io/FormalMATH/</a>)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººå·¥æ™ºèƒ½åœ¨å¤„ç†å½¢å¼åŒ–æ•°å­¦æ¨ç†æ—¶æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FormalMATHè¿™ä¸€å¤§è§„æ¨¡Lean4åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°æ¶µç›–äº†ä»é«˜ä¸­å¥¥èµ›æŒ‘æˆ˜åˆ°æœ¬ç§‘çº§åˆ«å®šç†è¯æ˜çš„æ­£å¼éªŒè¯é—®é¢˜å…±è®¡5,560ä¸ªã€‚ä¸ºäº†å‡è½»æ‰‹åŠ¨å½¢å¼åŒ–çš„æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹äººæœºç»“åˆè‡ªåŠ¨å½¢å¼åŒ–ç®¡é“ï¼Œé›†æˆäº†è¯­è¨€æ¨¡å‹è‡ªåŠ¨å½¢å¼åŒ–å£°æ˜ã€å¤šè¯­è¨€æ¨¡å‹è¯­ä¹‰éªŒè¯ä»¥åŠåŸºäºå¦å®šçš„åè¯è¿‡æ»¤ç­–ç•¥ã€‚å°½ç®¡æœ‰è‡ªåŠ¨éªŒè¯ç³»ç»Ÿçš„è¾…åŠ©ï¼Œä¸“å®¶æ ‡æ³¨æˆæœ¬ä»ç„¶è¾ƒé«˜ï¼Œä½†è¯¥ç³»ç»Ÿç¡®ä¿äº†åŸå§‹è‡ªç„¶è¯­è¨€é—®é¢˜çš„å¿ å®æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†ç›®å‰æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹å®šç†è¯æ˜å™¨ï¼Œå‘ç°å®ƒä»¬å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šå³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹åœ¨å®é™…é‡‡æ ·é¢„ç®—ä¸‹æˆåŠŸç‡ä¹Ÿåªæœ‰16.46%ï¼Œæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„é¢†åŸŸåè§å’Œè¿‡äºä¾èµ–ç®€åŒ–è‡ªåŠ¨åŒ–æˆ˜æœ¯çš„é—®é¢˜ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œåœ¨æ€ç»´é“¾æ¨ç†åœºæ™¯ä¸­ï¼Œè‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„è§£ç­”ä¸è¯æ˜æˆåŠŸä¹‹é—´å­˜åœ¨ä¸€ç§åå‘å…³ç³»ã€‚æˆ‘ä»¬ç›¸ä¿¡FormalMATHæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ•°å­¦å½¢å¼æ¨ç†èƒ½åŠ›çš„ç¨³å¥åŸºå‡†æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FormalMATHæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«æ¶µç›–å¤šä¸ªé¢†åŸŸçš„æ­£å¼éªŒè¯é—®é¢˜ã€‚</li>
<li>æ¨å‡ºäº†äººæœºç»“åˆè‡ªåŠ¨å½¢å¼åŒ–ç®¡é“ï¼Œä»¥æé«˜æ•ˆç‡å¹¶å‡å°‘äººå·¥æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>è‡ªåŠ¨éªŒè¯ç³»ç»Ÿç¡®ä¿äº†åŸå§‹è‡ªç„¶è¯­è¨€é—®é¢˜çš„å¿ å®æ€§ã€‚</li>
<li>æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹å®šç†è¯æ˜å™¨å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¦‚ä½æˆåŠŸç‡å’Œé¢†åŸŸåè§é—®é¢˜ã€‚</li>
<li>è‡ªç„¶è¯­è¨€æŒ‡å¯¼çš„è§£ç­”ä¸è¯æ˜æˆåŠŸä¹‹é—´å­˜åœ¨ä¸€ç§åå‘å…³ç³»ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨å½¢å¼åŒ–æ¨ç†ä¸­è¿‡äºä¾èµ–ç®€åŒ–è‡ªåŠ¨åŒ–æˆ˜æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9edec3e4795fa0d4d23c93b90145fcec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c67104da2130a29c266835a9514dea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a69c3cdfd796a1554c1142bbcd3149b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47bbb210ea29b8afdf253ac4c64af834.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a3415c4fde2414cc4010744149663f4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Sailing-AI-by-the-Stars-A-Survey-of-Learning-from-Rewards-in-Post-Training-and-Test-Time-Scaling-of-Large-Language-Models"><a href="#Sailing-AI-by-the-Stars-A-Survey-of-Learning-from-Rewards-in-Post-Training-and-Test-Time-Scaling-of-Large-Language-Models" class="headerlink" title="Sailing AI by the Stars: A Survey of Learning from Rewards in   Post-Training and Test-Time Scaling of Large Language Models"></a>Sailing AI by the Stars: A Survey of Learning from Rewards in   Post-Training and Test-Time Scaling of Large Language Models</h2><p><strong>Authors:Xiaobao Wu</strong></p>
<p>Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at <a target="_blank" rel="noopener" href="https://github.com/bobxwu/learning-from-rewards-llm-papers">https://github.com/bobxwu/learning-from-rewards-llm-papers</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•å·²ç»ä»é¢„è®­ç»ƒè§„æ¨¡åŒ–è½¬å‘åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è§„æ¨¡åŒ–ã€‚åœ¨è¿™äº›å‘å±•ä¸­ï¼Œå‡ºç°äº†ä¸€ä¸ªå…³é”®çš„ç»Ÿä¸€èŒƒå¼ï¼šä»å¥–åŠ±ä¸­å­¦ä¹ ï¼Œå…¶ä¸­å¥–åŠ±ä¿¡å·ä½œä¸ºå¼•å¯¼æ˜Ÿæ¥å¼•å¯¼LLMçš„è¡Œä¸ºã€‚å®ƒå·²ç»æ”¯æ’‘äº†ä¸€ç³»åˆ—æµè¡Œçš„æŠ€æœ¯ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆåœ¨RLHFã€DPOå’ŒGRPOä¸­ï¼‰ã€å¥–åŠ±å¼•å¯¼è§£ç å’Œäº‹åæ ¡æ­£ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ä¸€èŒƒå¼ä½¿LLMèƒ½å¤Ÿä»è¢«åŠ¨å­¦ä¹ é™æ€æ•°æ®è¿‡æ¸¡åˆ°ä¸»åŠ¨å­¦ä¹ åŠ¨æ€åé¦ˆã€‚è¿™ä½¿å¾—LLMå…·æœ‰ä¸€è‡´çš„åå¥½å’Œæ·±åšçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å¯¹ä»å¥–åŠ±ä¸­å­¦ä¹ çš„èŒƒå¼è¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ã€‚æˆ‘ä»¬æŒ‰è®­ç»ƒã€æ¨ç†å’Œæ¨ç†åé˜¶æ®µåˆ†ç±»å’Œåˆ†æè¯¥èŒƒå¼ä¸‹çš„ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è®¨è®ºäº†å¥–åŠ±æ¨¡å‹çš„åŸºå‡†æµ‹è¯•å’Œä¸»è¦åº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚ç›¸å…³è®ºæ–‡é›†åˆè¯·è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/bobxwu/learning-from-rewards-llm-papers%E3%80%82">https://github.com/bobxwu/learning-from-rewards-llm-papersã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02686v1">PDF</a> 35 Pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•å·²ä»é¢„è®­ç»ƒæ‰©å±•è½¬å‘åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ‰©å±•ã€‚åœ¨è¿™äº›å‘å±•ä¸­ï¼Œä¸€ä¸ªå…³é”®ç»Ÿä¸€èŒƒå¼åº”è¿è€Œç”Ÿï¼šä»å¥–åŠ±ä¸­å­¦ä¹ ã€‚å¥–åŠ±ä¿¡å·ä½œä¸ºæŒ‡å¯¼LLMè¡Œä¸ºçš„æŒ‡å—æ˜Ÿã€‚å®ƒå·²æˆä¸ºå¤šç§æµè¡ŒæŠ€æœ¯çš„åŸºçŸ³ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆåœ¨RLHFã€DPOå’ŒGRPOä¸­ï¼‰ã€å¥–åŠ±å¼•å¯¼è§£ç å’Œäº‹åæ ¡æ­£ã€‚è¿™ä¸ªèŒƒå¼ä½¿LLMä»è¢«åŠ¨å­¦ä¹ é™æ€æ•°æ®è½¬å˜ä¸ºä»åŠ¨æ€åé¦ˆä¸­å­¦ä¹ ï¼Œèµ‹äºˆLLMå¯¹é½çš„åå¥½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†ä»å¥–åŠ±ä¸­å­¦ä¹ çš„èŒƒå¼ï¼Œåˆ†æå’Œåˆ†ç±»äº†è¯¥èŒƒå¼ä¸‹çš„ç­–ç•¥åœ¨è®­ç»ƒã€æ¨ç†å’Œæ¨ç†åçš„é˜¶æ®µã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†å¥–åŠ±æ¨¡å‹çš„æ ‡å‡†å’Œä¸»è¦åº”ç”¨ï¼Œå¹¶å¼ºè°ƒäº†æŒ‘æˆ˜å’Œæœªæ¥æ–¹å‘ã€‚æˆ‘ä»¬æä¾›è®ºæ–‡é›†åˆé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/bobxwu/learning-from-rewards-llm-papers">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•å·²ä»é¢„è®­ç»ƒæ‰©å±•è½¬å‘åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´æ‰©å±•ã€‚</li>
<li>ä»å¥–åŠ±ä¸­å­¦ä¹ å·²æˆä¸ºLLMçš„ä¸€ä¸ªé‡è¦å‘å±•èŒƒå¼ï¼Œæ¶µç›–å¤šç§æŠ€æœ¯å¦‚å¼ºåŒ–å­¦ä¹ ã€å¥–åŠ±å¼•å¯¼è§£ç å’Œäº‹åæ ¡æ­£ã€‚</li>
<li>è¯¥èŒƒå¼ä½¿LLMä»è¢«åŠ¨å­¦ä¹ é™æ€æ•°æ®è½¬å˜ä¸ºä»åŠ¨æ€åé¦ˆä¸­å­¦ä¹ ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>ä»å¥–åŠ±ä¸­å­¦ä¹ ä½¿LLMå…·å¤‡å¯¹é½çš„åå¥½å’Œæ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†æ¨¡å‹çš„æ™ºèƒ½æ°´å¹³ã€‚</li>
<li>è¯¥è®ºæ–‡å…¨é¢æ¦‚è¿°äº†ä»å¥–åŠ±ä¸­å­¦ä¹ çš„ç­–ç•¥åœ¨è®­ç»ƒã€æ¨ç†å’Œæ¨ç†åçš„é˜¶æ®µï¼Œå¹¶æä¾›äº†ç›¸å…³çš„è®ºæ–‡é›†åˆé“¾æ¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c43fdf4478b3994ee277f8b6d63db448.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3aee7ea5029546bda09a14c2bc99d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f5d62071760171c5b5b07c0c3a7a700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5554c662147411834494e8063e0c5aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8e93cd62b1b964c7330abc1c0bbc06.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Recursive-Decomposition-with-Dependencies-for-Generic-Divide-and-Conquer-Reasoning"><a href="#Recursive-Decomposition-with-Dependencies-for-Generic-Divide-and-Conquer-Reasoning" class="headerlink" title="Recursive Decomposition with Dependencies for Generic Divide-and-Conquer   Reasoning"></a>Recursive Decomposition with Dependencies for Generic Divide-and-Conquer   Reasoning</h2><p><strong>Authors:Sergio HernÃ¡ndez-GutiÃ©rrez, Minttu Alakuijala, Alexander V. Nikitin, Pekka Marttinen</strong></p>
<p>Reasoning tasks are crucial in many domains, especially in science and engineering. Although large language models (LLMs) have made progress in reasoning tasks using techniques such as chain-of-thought and least-to-most prompting, these approaches still do not effectively scale to complex problems in either their performance or execution time. Moreover, they often require additional supervision for each new task, such as in-context examples. In this work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable divide-and-conquer method for solving reasoning problems that requires less supervision than prior approaches. Our method can be directly applied to a new problem class even in the absence of any task-specific guidance. Furthermore, RDD supports sub-task dependencies, allowing for ordered execution of sub-tasks, as well as an error recovery mechanism that can correct mistakes made in previous steps. We evaluate our approach on two benchmarks with six difficulty levels each and in two in-context settings: one with task-specific examples and one without. Our results demonstrate that RDD outperforms other methods in a compute-matched setting as task complexity increases, while also being more computationally efficient. </p>
<blockquote>
<p>æ¨ç†ä»»åŠ¡åœ¨è®¸å¤šé¢†åŸŸéƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç§‘å­¦å’Œå·¥ä¸šé¢†åŸŸã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œé‡‡ç”¨äº†å¦‚æ€ç»´é“¾å’Œæœ€å°‘åˆ°æœ€å¤šçš„æç¤ºç­‰æŠ€æœ¯ï¼Œä½†è¿™äº›æ–¹æ³•ä»æ— æ³•åœ¨æ€§èƒ½æˆ–æ‰§è¡Œæ—¶é—´ä¸Šæœ‰æ•ˆåœ°æ‰©å±•åˆ°å¤æ‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹æ¯ä¸ªæ–°ä»»åŠ¡è¿›è¡Œé¢å¤–çš„ç›‘ç£ï¼Œä¾‹å¦‚ä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¾èµ–çš„é€’å½’åˆ†è§£ï¼ˆRDDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³æ¨ç†é—®é¢˜çš„å¯æ‰©å±•çš„åˆ†è€Œæ²»ä¹‹æ–¹æ³•ï¼Œå®ƒéœ€è¦çš„ç›‘ç£æ¯”å…ˆå‰çš„æ–¹æ³•å°‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç›´æ¥åº”ç”¨äºæ–°çš„é—®é¢˜ç±»åˆ«ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•ç‰¹å®šä»»åŠ¡æŒ‡å¯¼çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼ŒRDDæ”¯æŒå­ä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œå…è®¸æŒ‰é¡ºåºæ‰§è¡Œå­ä»»åŠ¡ï¼Œä»¥åŠä¸€ç§é”™è¯¯æ¢å¤æœºåˆ¶ï¼Œå¯ä»¥çº æ­£ä¹‹å‰æ­¥éª¤ä¸­çŠ¯ä¸‹çš„é”™è¯¯ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹ä¸¤ç§æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¯ä¸ªåŸºå‡†æµ‹è¯•åŒ…æ‹¬å…­ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¹¶åœ¨ä¸¤ç§ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼šä¸€ä¸ªæœ‰ç‰¹å®šä»»åŠ¡ç¤ºä¾‹çš„ä¸Šä¸‹æ–‡ç¯å¢ƒå’Œä¸€ä¸ªæ²¡æœ‰ç‰¹å®šä»»åŠ¡ç¤ºä¾‹çš„ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œåœ¨ç®—åŠ›åŒ¹é…çš„ç¯å¢ƒä¸­ï¼ŒRDDçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒæ—¶å…¶è®¡ç®—æ•ˆç‡ä¹Ÿæ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02576v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åœ¨ç§‘æŠ€å’Œå·¥ç¨‹ç­‰é¢†åŸŸï¼Œæ¨ç†ä»»åŠ¡è‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸Šæœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬å¯¹äºå¤æ‚é—®é¢˜çš„æ€§èƒ½å’Œæ‰§è¡Œæ—¶é—´ä»æ— æ³•æœ‰æ•ˆæ‰©å±•ã€‚æœ¬ç ”ç©¶æå‡ºäº†é€’å½’åˆ†è§£ä¸ä¾èµ–å…³ç³»ï¼ˆRDDï¼‰æ–¹æ³•ï¼Œä¸€ç§è§£å†³æ¨ç†é—®é¢˜çš„å¯ä¼¸ç¼©çš„åˆ†è€Œæ²»ä¹‹æ–¹æ³•ï¼Œå®ƒå‡å°‘äº†ç›‘ç£è¦æ±‚ï¼Œèƒ½å¤Ÿç›´æ¥åº”ç”¨äºæ–°é—®é¢˜ç±»åˆ«è€Œæ— éœ€ä»»ä½•ä»»åŠ¡ç‰¹å®šæŒ‡å¯¼ã€‚RDDæ”¯æŒå­ä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œå…è®¸å­ä»»åŠ¡çš„é¡ºåºæ‰§è¡Œä»¥åŠé”™è¯¯æ¢å¤æœºåˆ¶ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼ŒRDDåœ¨ç¯å¢ƒåŒ¹é…è®¾ç½®ä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ–¹æ³•çš„è¡¨ç°ï¼Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¨ç†ä»»åŠ¡åœ¨è®¸å¤šé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸï¼Œå…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³å¤æ‚æ¨ç†é—®é¢˜æ—¶å­˜åœ¨æ€§èƒ½å’Œæ‰§è¡Œæ—¶é—´çš„å±€é™æ€§ã€‚</li>
<li>é€’å½’åˆ†è§£ä¸ä¾èµ–å…³ç³»ï¼ˆRDDï¼‰æ˜¯ä¸€ç§æ–°çš„è§£å†³æ¨ç†é—®é¢˜çš„åˆ†è€Œæ²»ä¹‹æ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å¹¶å‡å°‘äº†ç›‘ç£éœ€æ±‚ã€‚</li>
<li>RDDå¯ç›´æ¥åº”ç”¨äºæ–°é—®é¢˜ç±»åˆ«ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡æŒ‡å¯¼ã€‚</li>
<li>RDDæ”¯æŒå­ä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œå…è®¸æœ‰åºæ‰§è¡Œå’Œé”™è¯¯æ¢å¤æœºåˆ¶ã€‚</li>
<li>RDDåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡å¤æ‚æ€§å¢åŠ æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-073d88b223b54a191e0b47808764be24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c49d600b7e8b018cee50b7db9cae452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7f78892c5d7f671cbffb8c5ba90cc80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c194848b221dc20249d438d1bea27ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ba588811e0c3ebb6bfb16d921eaa317.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Optimizing-Chain-of-Thought-Reasoners-via-Gradient-Variance-Minimization-in-Rejection-Sampling-and-RL"><a href="#Optimizing-Chain-of-Thought-Reasoners-via-Gradient-Variance-Minimization-in-Rejection-Sampling-and-RL" class="headerlink" title="Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization   in Rejection Sampling and RL"></a>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization   in Rejection Sampling and RL</h2><p><strong>Authors:Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, Tong Zhang</strong></p>
<p>Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/RLHFlow/GVM">https://github.com/RLHFlow/GVM</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¯ä»¥è¢«å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ½œåœ¨å˜é‡é—®é¢˜ï¼Œå…¶ä¸­æ¨¡å‹éœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚è™½ç„¶å…ˆå‰çš„æ–¹æ³•ï¼ˆå¦‚åŸºäºå¥–åŠ±æ’åçš„å¾®è°ƒï¼ˆRAFTï¼‰ï¼‰å·²ç»ä¾èµ–äºè¿™ç§è¡¨è¿°ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨æç¤ºä¹‹é—´åº”ç”¨ç»Ÿä¸€æ¨ç†é¢„ç®—ï¼Œè¿™å¿½ç•¥äº†ä¸åŒéš¾åº¦å’Œæ”¶æ•›è¡Œä¸ºçš„å˜åŒ–æ€§ã€‚æœ¬ç ”ç©¶ç¡®å®šäº†CoTè®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆæ˜¯ç”±äºé™æ€é‡‡æ ·ç­–ç•¥å¯¼è‡´çš„ä¸é«˜æ•ˆçš„éšæœºæ¢¯åº¦ä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºäº†GVM-RAFTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æç¤ºçš„ç‰¹å®šåŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œæ—¨åœ¨åœ¨è®¡ç®—é¢„ç®—çº¦æŸä¸‹æœ€å°åŒ–éšæœºæ¢¯åº¦æ–¹å·®ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›‘æ§æç¤ºæ¥å—ç‡å’Œéšæœºæ¢¯åº¦èŒƒæ•°æ¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œç¡®ä¿äº§ç”Ÿçš„æ¢¯åº¦æ–¹å·®æœ€å°åŒ–ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œåœ¨åˆé€‚çš„æ¡ä»¶ä¸‹ï¼Œæ‰€æå‡ºçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥å¯ä»¥åŠ é€Ÿæ”¶æ•›ä¿è¯ã€‚åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼Œç›¸å¯¹äºåŸå§‹çš„RAFTï¼ŒGVM-RAFTå®ç°äº†2-4å€çš„é€Ÿåº¦æå‡å’Œæ˜¾è‘—çš„å‡†ç¡®æ€§æ”¹è¿›ã€‚æ‰€æå‡ºçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥å¹¶å…¥å…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚GRPOï¼‰ï¼Œåœ¨æ”¶æ•›æ€§å’Œæµ‹è¯•ç²¾åº¦ä¸Šå®ç°ç±»ä¼¼çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç åœ¨<a target="_blank" rel="noopener" href="https://github.com/RLHFlow/GVM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/RLHFlow/GVMä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02391v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å¯å½¢å¼åŒ–ä¸ºä¸€ä¸ªæ½œåœ¨å˜é‡é—®é¢˜ï¼Œéœ€è¦ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤ã€‚æœ¬æ–‡è¯†åˆ«å‡ºCoTè®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆåœ¨äºç”±äºé™æ€é‡‡æ ·ç­–ç•¥å¯¼è‡´çš„æ— æ•ˆéšæœºæ¢¯åº¦ä¼°è®¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„ç‰¹å®šåŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼ˆGVM-RAFTï¼‰ï¼Œæ—¨åœ¨å‡å°‘è®¡ç®—é¢„ç®—çº¦æŸä¸‹çš„éšæœºæ¢¯åº¦æ–¹å·®ã€‚é€šè¿‡ç›‘æµ‹æç¤ºæ¥å—ç‡å’Œéšæœºæ¢¯åº¦èŒƒæ•°æ¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œç¡®ä¿æ¢¯åº¦æ–¹å·®æœ€å°åŒ–ã€‚ç†è®ºåˆ†æå’Œæ•°å­¦æ¨ç†å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥åœ¨é€‚å½“æ¡ä»¶ä¸‹å¯åŠ é€Ÿæ”¶æ•›ï¼Œä¸æ ‡å‡†RAFTç›¸æ¯”ï¼ŒGVM-RAFTå®ç°äº†2-4å€çš„é€Ÿåº¦æå‡å’Œæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¦‚GRPOï¼Œä»¥æé«˜æ”¶æ•›æ€§å’Œæµ‹è¯•ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯å½¢å¼åŒ–ä¸ºæ½œåœ¨å˜é‡é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚RAFTåœ¨æ¨ç†è®­ç»ƒä¸­åº”ç”¨ç»Ÿä¸€æ¨ç†é¢„ç®—ï¼Œå¿½ç•¥äº†ä¸åŒæç¤ºçš„éš¾åº¦å’Œæ”¶æ•›è¡Œä¸ºçš„å·®å¼‚ã€‚</li>
<li>æœ¬æ–‡è¯†åˆ«å‡ºCoTè®­ç»ƒä¸­çš„ç“¶é¢ˆåœ¨äºé™æ€é‡‡æ ·ç­–ç•¥å¯¼è‡´çš„æ— æ•ˆéšæœºæ¢¯åº¦ä¼°è®¡ã€‚</li>
<li>æå‡ºäº†GVM-RAFTï¼Œä¸€ç§åŸºäºæç¤ºçš„ç‰¹å®šåŠ¨æ€æ ·æœ¬åˆ†é…ç­–ç•¥ï¼Œä»¥å‡å°‘éšæœºæ¢¯åº¦æ–¹å·®ã€‚</li>
<li>GVM-RAFTé€šè¿‡ç›‘æµ‹æç¤ºæ¥å—ç‡å’Œéšæœºæ¢¯åº¦èŒƒæ•°æ¥åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>ç†è®ºåˆ†æå’Œå®éªŒè¡¨æ˜ï¼ŒGVM-RAFTå¯åŠ é€Ÿæ”¶æ•›ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>GVM-RAFTç­–ç•¥å¯åº”ç”¨äºå…¶ä»–å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥æé«˜æ”¶æ•›æ€§å’Œæµ‹è¯•ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76f27a70a76faf30a74755b0096ce06f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a28faacb809298b93b1b0602dd0f0ff6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Quantitative-Analysis-of-Performance-Drop-in-DeepSeek-Model-Quantization"><a href="#Quantitative-Analysis-of-Performance-Drop-in-DeepSeek-Model-Quantization" class="headerlink" title="Quantitative Analysis of Performance Drop in DeepSeek Model Quantization"></a>Quantitative Analysis of Performance Drop in DeepSeek Model Quantization</h2><p><strong>Authors:Enbo Zhao, Yi Shen, Shuming Shi, Jieyun Huang, Zhihao Chen, Ning Wang, Siqi Xiao, Jian Zhang, Kai Wang, Shiguo Lian</strong></p>
<p>Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the modelsâ€™ 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100&#x2F;A100 and Huawei 910B. Our implementation of DQ3_K_M is released at <a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-Eval">https://github.com/UnicomAI/DeepSeek-Eval</a>, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¯¹DeepSeek-R1å’ŒV3çš„æœ¬åœ°éƒ¨ç½²éœ€æ±‚å¾ˆé«˜ï¼Œå¯èƒ½æ˜¯å› ä¸ºå®˜æ–¹æœåŠ¡ç»å¸¸å¾ˆç¹å¿™ï¼Œè€Œä¸”ä¸€äº›ç»„ç»‡å¯¹æ•°æ®éšç§å­˜åœ¨æ‹…å¿§ã€‚è™½ç„¶å•æœºéƒ¨ç½²å¯ä»¥æä¾›åŸºç¡€è®¾æ–½çš„ç®€å•æ€§ï¼Œä½†æ¨¡å‹çš„671B FP8å‚æ•°é…ç½®è¶…å‡ºäº†æ ‡å‡†8 GPUæœºå™¨çš„å®é™…å†…å­˜é™åˆ¶ã€‚é‡åŒ–æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œæœ‰åŠ©äºå‡å°‘æ¨¡å‹å†…å­˜æ¶ˆè€—ã€‚ç„¶è€Œï¼Œé‡åŒ–åDeepSeek-R1å’ŒV3çš„æ€§èƒ½å°šä¸æ¸…æ¥šã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šé¦–æ¬¡å¯¹DeepSeekæ¨¡å‹è°±è¿›è¡Œå…¨é¢çš„å¤šä½å®½é‡åŒ–å®šé‡è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œä¸FP8ç›¸æ¯”ï¼Œ4ä½é‡åŒ–å‡ ä¹æ²¡æœ‰æ€§èƒ½ä¸‹é™ï¼ŒåŒæ—¶èƒ½å¤Ÿåœ¨æ ‡å‡†NVIDIA GPUè®¾å¤‡ä¸Šå®ç°å•æœºéƒ¨ç½²ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†DQ3_K_Mçš„åŠ¨æ€3ä½é‡åŒ–æ–¹æ³•ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„Q3_K_Må˜ä½“ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­ä¸4ä½é‡åŒ–ï¼ˆQ4_K_Mï¼‰æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼ŒDQ3_K_Mæ”¯æŒNVIDIA H100&#x2F;A100å’Œåä¸º910Bçš„å•æœºéƒ¨ç½²é…ç½®ã€‚æˆ‘ä»¬çš„DQ3_K_Må®ç°å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-Eval%E4%B8%8A%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%8C%85%E6%8B%ACDeepSeek-R1%E5%92%8CDeepSeek-V3%E7%9A%84%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%843%E4%BD%8D%E9%87%8F%E5%8C%96%E7%89%88%E6%9C%AC%E3%80%82">https://github.com/UnicomAI/DeepSeek-Evalä¸Šï¼Œå…¶ä¸­åŒ…æ‹¬DeepSeek-R1å’ŒDeepSeek-V3çš„ä¼˜åŒ–åçš„3ä½é‡åŒ–ç‰ˆæœ¬ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02390v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¯¹DeepSeek-R1å’ŒV3çš„æœ¬åœ°éƒ¨ç½²éœ€æ±‚å¤§å¢ï¼Œå› å®˜æ–¹æœåŠ¡ç¹å¿™åŠç»„ç»‡å¯¹æ•°æ®éšç§çš„æ‹…å¿§ã€‚å°½ç®¡å•æœºéƒ¨ç½²å¯ç®€åŒ–åŸºç¡€è®¾æ–½ï¼Œä½†æ¨¡å‹é…ç½®çš„å‚æ•°è¶…è¿‡æ ‡å‡†8 GPUæœºå™¨çš„å®é™…å†…å­˜é™åˆ¶ã€‚æœ¬æŠ¥å‘Šé¦–æ¬¡å¯¹DeepSeekæ¨¡å‹è¿›è¡Œå¤šä½å®½é‡åŒ–è¯„ä¼°ï¼Œå‘ç°4ä½é‡åŒ–ä¸FP8ç›¸æ¯”æ€§èƒ½æŸå¤±æå°ï¼Œå¯å®ç°æ ‡å‡†NVIDIA GPUè®¾å¤‡çš„å•æœºéƒ¨ç½²ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€3ä½é‡åŒ–æ–¹æ³•DQ3_K_Mï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸQ3_K_Mæ–¹æ³•ï¼Œä¸”ä¸å¤§å¤šæ•°ä»»åŠ¡ä¸­çš„4ä½é‡åŒ–æ–¹æ³•ç›¸è¿‘ã€‚DQ3_K_Mæ”¯æŒNVIDIA H100&#x2F;A100å’Œåä¸º910Bçš„å•æœºéƒ¨ç½²é…ç½®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1å’ŒV3çš„æœ¬åœ°éƒ¨ç½²éœ€æ±‚å¢åŠ ï¼Œå› ä¸ºå®˜æ–¹æœåŠ¡ç¹å¿™åŠéšç§æ‹…å¿§ã€‚</li>
<li>å•æœºéƒ¨ç½²è™½ç®€åŒ–äº†åŸºç¡€è®¾æ–½ï¼Œä½†æ¨¡å‹å‚æ•°è¶…å‡ºäº†æ ‡å‡†8 GPUæœºå™¨çš„å†…å­˜é™åˆ¶ã€‚</li>
<li>æŠ¥å‘Šé¦–æ¬¡å…¨é¢è¯„ä¼°äº†DeepSeekæ¨¡å‹çš„å¤šä½å®½é‡åŒ–ã€‚</li>
<li>4ä½é‡åŒ–ä¸FP8ç›¸æ¯”æ€§èƒ½æŸå¤±å°ï¼Œå¯å®ç°æ ‡å‡†NVIDIA GPUè®¾å¤‡çš„å•æœºéƒ¨ç½²ã€‚</li>
<li>æå‡ºçš„DQ3_K_MåŠ¨æ€é‡åŒ–æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>DQ3_K_Mä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”æœ‰æ˜¾è‘—æ”¹å–„ï¼Œä¸”åœ¨å¤šæ•°ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘4ä½é‡åŒ–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bc21910e3a5bcc029c102e2e94c8bc0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-323b4ff5e09b6ce896398e7b1883cca1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb46badd3eff23059414f6d4eb8eaf91.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-07be0c23b7fac6f4ca15a78037644159.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  VITA-Audio Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-06/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-06e6530a21dbebc8afb9a187390137e8.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-06  TSTMotion Training-free Scene-awarenText-to-motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
