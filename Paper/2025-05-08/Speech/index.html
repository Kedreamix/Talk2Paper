<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  VITA-Audio Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0501a1e3d1c2026ad6aa448f20bdf51e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="VITA-Audio-Fast-Interleaved-Cross-Modal-Token-Generation-for-Efficient-Large-Speech-Language-Model"><a href="#VITA-Audio-Fast-Interleaved-Cross-Modal-Token-Generation-for-Efficient-Large-Speech-Language-Model" class="headerlink" title="VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model"></a>VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient   Large Speech-Language Model</h2><p><strong>Authors:Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, Xing Sun</strong></p>
<p>With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks. </p>
<blockquote>
<p>éšç€è‡ªç„¶äººæœºäº¤äº’éœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼ŒåŸºäºè¯­éŸ³çš„ç³»ç»Ÿè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå› ä¸ºè¯­éŸ³æ˜¯æ—¥å¸¸æ²Ÿé€šä¸­æœ€å¸¸è§çš„å½¢å¼ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯­éŸ³æ¨¡å‹åœ¨æµæ¨¡å¼ä¸‹ç”Ÿæˆç¬¬ä¸€ä¸ªéŸ³é¢‘ä»¤ç‰Œæ—¶ä»ç„¶é¢ä¸´è¾ƒé«˜çš„å»¶è¿Ÿï¼Œè¿™ä¸ºå…¶éƒ¨ç½²å¸¦æ¥äº†æ˜¾è‘—çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VITA-Audioï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤§å‹è¯­éŸ³æ¨¡å‹ï¼Œå…·æœ‰å¿«é€Ÿçš„éŸ³é¢‘æ–‡æœ¬ä»¤ç‰Œç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€ä»¤ç‰Œé¢„æµ‹ï¼ˆMCTPï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥åœ¨å•ä¸ªæ¨¡å‹å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°ç”Ÿæˆå¤šä¸ªéŸ³é¢‘ä»¤ç‰Œï¼Œè¿™ä¸ä»…å¯ä»¥åŠ é€Ÿæ¨ç†ï¼Œè€Œä¸”å¯ä»¥æ˜¾è‘—é™ä½æµåœºæ™¯ä¸­ç”Ÿæˆç¬¬ä¸€ä¸ªéŸ³é¢‘çš„å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œè¿˜æ¢ç´¢äº†ä¸€ç§å››é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œä»¥åœ¨å°½å¯èƒ½ä¸æŸå¤±è¯­éŸ³è´¨é‡çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹åŠ é€Ÿã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒVITA-Audioæ˜¯æœ‰èƒ½åŠ›åœ¨ç¬¬ä¸€æ¬¡å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­ç”ŸæˆéŸ³é¢‘è¾“å‡ºçš„é¦–ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯å®ç°å®æ—¶çš„ä½å»¶è¿Ÿå¯¹è¯åŠŸèƒ½ã€‚VITA-Audioå®Œå…¨å¯å¤ç°ï¼Œä»…ä½¿ç”¨å¼€æºæ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨7Bå‚æ•°è§„æ¨¡ä¸Šå®ç°äº†3~5å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å’Œè¯­éŸ³é—®ç­”ï¼ˆSQAï¼‰ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03739v1">PDF</a> Training and Inference Codes: <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA-Audio">https://github.com/VITA-MLLM/VITA-Audio</a></p>
<p><strong>Summary</strong></p>
<p>éšç€äººç±»å¯¹è‡ªç„¶äººæœºäº¤äº’çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œè¯­éŸ³ä½œä¸ºæ—¥å¸¸æ²Ÿé€šçš„æœ€å¸¸è§å½¢å¼ä¹‹ä¸€ï¼ŒåŸºäºè¯­éŸ³çš„ç³»ç»Ÿå—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚é’ˆå¯¹ç°æœ‰è¯­éŸ³æ¨¡å‹åœ¨æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­ç”Ÿæˆé¦–ä¸ªéŸ³é¢‘ä»¤ç‰Œæ—¶å­˜åœ¨çš„é«˜å»¶è¿Ÿé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VITA-Audioï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤§å‹è¯­éŸ³æ¨¡å‹ï¼Œå…·æœ‰å¿«é€ŸéŸ³é¢‘æ–‡æœ¬ä»¤ç‰Œç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è½»é‡çº§çš„è·¨æ¨¡æ€ä»¤ç‰Œé¢„æµ‹æ¨¡å—å’Œé‡‡ç”¨å››é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼ŒVITA-Audioä¸ä»…åŠ é€Ÿäº†æ¨ç†ï¼Œè€Œä¸”æ˜¾è‘—é™ä½äº†ç”Ÿæˆé¦–ä¸ªéŸ³é¢‘æ—¶çš„å»¶è¿Ÿã€‚æ­¤å¤–ï¼ŒVITA-Audioæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨é¦–æ¬¡å‰å‘ä¼ é€’ä¸­äº§ç”ŸéŸ³é¢‘è¾“å‡ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å®æ—¶å¯¹è¯èƒ½åŠ›ä¸”å»¶è¿Ÿæä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨7Bå‚æ•°è§„æ¨¡ä¸Šå®ç°äº†3~5å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­éŸ³é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºåŒç±»è§„æ¨¡çš„å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VITA-Audioæ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³æ¨¡å‹çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­ç”Ÿæˆé¦–ä¸ªéŸ³é¢‘ä»¤ç‰Œæ—¶çš„é«˜å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>VITA-Audioé€šè¿‡å¼•å…¥MCTPæ¨¡å—å®ç°å¿«é€ŸéŸ³é¢‘æ–‡æœ¬ä»¤ç‰Œç”Ÿæˆï¼Œé™ä½ç”Ÿæˆé¦–ä¸ªéŸ³é¢‘æ—¶çš„å»¶è¿Ÿã€‚</li>
<li>VITA-Audioé‡‡ç”¨å››é˜¶æ®µæ¸è¿›è®­ç»ƒç­–ç•¥ï¼Œå®ç°æ¨¡å‹åŠ é€Ÿå¹¶æœ€å°åŒ–è¯­éŸ³è´¨é‡çš„æŸå¤±ã€‚</li>
<li>VITA-Audioæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨é¦–æ¬¡å‰å‘ä¼ é€’ä¸­äº§ç”ŸéŸ³é¢‘è¾“å‡ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡å®æ—¶å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒVITA-Audioåœ¨æ¨ç†é€Ÿåº¦ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè¯­éŸ³ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>VITA-Audioæ¨¡å‹å¯å®Œå…¨å¤ç°ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å¼€æºæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>VITA-Audioçš„æå‡ºè¿›ä¸€æ­¥æ¨åŠ¨äº†è‡ªç„¶äººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5670c48015b6d8a3c54331fb3b46f911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947cc875237490afd6f0bc1a990d6e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8a03ce8cd6e88a4e48df47b0b892da5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50d29299695b980d334c08c8692f90a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a843f2703e4d040a0a8962f1f74950d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1bfe1c238d94c64994a433eeb7803c3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PAHA-Parts-Aware-Audio-Driven-Human-Animation-with-Diffusion-Model"><a href="#PAHA-Parts-Aware-Audio-Driven-Human-Animation-with-Diffusion-Model" class="headerlink" title="PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model"></a>PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model</h2><p><strong>Authors:Y. B. Wang, S. Z. Zhou, J. F. Wu, T. Hu, J. N. Zhang, Y. Liu</strong></p>
<p>Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„äººå½¢åŠ¨ç”»æŠ€æœ¯å¹¿æ³›åº”ç”¨äºäººæœºäº¤äº’é¢†åŸŸï¼Œæ‰©æ•£æ¨¡å‹çš„å…´èµ·è¿›ä¸€æ­¥æ¨åŠ¨äº†å…¶å‘å±•ã€‚ç›®å‰ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºå¤šé˜¶æ®µç”Ÿæˆå’Œä¸­é—´è¡¨ç¤ºï¼Œå¯¼è‡´æ¨ç†æ—¶é—´é•¿ï¼Œç‰¹å®šå‰æ™¯åŒºåŸŸç”Ÿæˆè´¨é‡å’ŒéŸ³é¢‘è¿åŠ¨ä¸€è‡´æ€§å­˜åœ¨é—®é¢˜ã€‚è¿™äº›ç¼ºç‚¹ä¸»è¦æ˜¯ç”±äºç¼ºä¹å±€éƒ¨ç²¾ç»†ç›‘ç£æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PAHAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„ç«¯åˆ°ç«¯éŸ³é¢‘é©±åŠ¨äººä½“ä¸ŠåŠèº«åŠ¨ç”»æ¡†æ¶ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§å…³é”®æ–¹æ³•ï¼šéƒ¨ä»¶æ„ŸçŸ¥é‡åŠ æƒï¼ˆPARï¼‰å’Œéƒ¨ä»¶ä¸€è‡´æ€§å¢å¼ºï¼ˆPCEï¼‰ã€‚PARæ ¹æ®å§¿æ€ç½®ä¿¡åº¦åˆ†æ•°åŠ¨æ€è°ƒæ•´åŒºåŸŸè®­ç»ƒæŸå¤±æƒé‡ï¼Œæœ‰æ•ˆæé«˜è§†è§‰æ•ˆæœã€‚PCEæ„å»ºå¹¶è®­ç»ƒåŸºäºæ‰©æ•£çš„åŒºåŸŸéŸ³é¢‘è§†è§‰åˆ†ç±»å™¨ï¼Œæé«˜è¿åŠ¨ä¸€è‡´æ€§å’Œè¯­éŸ³åŒæ­¥æ€§ã€‚éšåï¼Œæˆ‘ä»¬ä¸ºä¸Šè¿°åˆ†ç±»å™¨è®¾è®¡äº†ä¸¤ç§æ–°é¢–æ¨ç†æŒ‡å¯¼æ–¹æ³•ï¼šåºåˆ—æŒ‡å¯¼ï¼ˆSGï¼‰å’Œå·®åˆ†æŒ‡å¯¼ï¼ˆDGï¼‰ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†CNASï¼Œé¦–ä¸ªå…¬å¼€çš„ä¸­æ–‡æ–°é—»ä¸»æ’­è¯­éŸ³æ•°æ®é›†ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å’ŒéªŒè¯ã€‚å¤§é‡çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPAHAåœ¨éŸ³é¢‘è¿åŠ¨å¯¹é½å’Œè§†é¢‘ç›¸å…³è¯„ä¼°æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’ŒCNASæ•°æ®é›†å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03603v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨äººä½“åŠ¨ç”»æŠ€æœ¯å–å¾—æ–°è¿›å±•ã€‚é’ˆå¯¹å½“å‰æ–¹æ³•å­˜åœ¨çš„å¤šé˜¶æ®µç”Ÿæˆã€æ¨ç†æ—¶é—´é•¿ä»¥åŠç‰¹å®šå‰æ™¯åŒºåŸŸç”Ÿæˆè´¨é‡å’ŒéŸ³é¢‘è¿åŠ¨ä¸€è‡´æ€§ç­‰é—®é¢˜ï¼Œæå‡ºPAHAæ¡†æ¶åŠPARå’ŒPCEä¸¤å¤§æ–¹æ³•ã€‚PARé€šè¿‡å§¿æ€ç½®ä¿¡åº¦åˆ†æ•°åŠ¨æ€è°ƒæ•´åŒºåŸŸè®­ç»ƒæŸå¤±æƒé‡ï¼Œæå‡è§†è§‰æ•ˆæœï¼›PCEæ„å»ºåŸºäºæ‰©æ•£æ¨¡å‹çš„åŒºåŸŸéŸ³é¢‘è§†è§‰åˆ†ç±»å™¨ï¼Œå¢å¼ºè¿åŠ¨ä¸€è‡´æ€§ã€‚è®¾è®¡ä¸¤ç§æ–°å‹æ¨ç†å¼•å¯¼æ–¹æ³•SGå’ŒDGï¼Œå®ç°æ•ˆç‡å’Œè´¨é‡çš„å¹³è¡¡ã€‚åŒæ—¶ï¼Œå»ºç«‹é¦–ä¸ªä¸­æ–‡æ–°é—»ä¸»æ’­è¯­éŸ³æ•°æ®é›†CNASï¼Œæ¨åŠ¨ç›¸å…³é¢†åŸŸç ”ç©¶éªŒè¯ã€‚å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜PAHAåœ¨éŸ³é¢‘è¿åŠ¨å¯¹é½å’Œè§†é¢‘ç›¸å…³è¯„ä¼°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨äººä½“åŠ¨ç”»æŠ€æœ¯åœ¨äººæœºäº¤äº’ä¸­å¹¿æ³›åº”ç”¨ï¼Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°è¿›ä¸€æ­¥æ¨åŠ¨äº†å…¶å‘å±•ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨å¤šé˜¶æ®µç”Ÿæˆã€æ¨ç†æ—¶é—´é•¿çš„é—®é¢˜ã€‚</li>
<li>å­˜åœ¨ç‰¹å®šå‰æ™¯åŒºåŸŸç”Ÿæˆè´¨é‡å’ŒéŸ³é¢‘è¿åŠ¨ä¸€è‡´æ€§é—®é¢˜ï¼Œä¸»è¦ç”±äºç¼ºä¹å±€éƒ¨ç²¾ç»†ç›‘ç£æŒ‡å¯¼ã€‚</li>
<li>PAHAæ¡†æ¶é€šè¿‡PARå’ŒPCEä¸¤å¤§æ–¹æ³•è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæé«˜è§†è§‰æ•ˆæœå’Œè¿åŠ¨ä¸€è‡´æ€§ã€‚</li>
<li>PARæ–¹æ³•é€šè¿‡å§¿æ€ç½®ä¿¡åº¦åˆ†æ•°åŠ¨æ€è°ƒæ•´åŒºåŸŸè®­ç»ƒæŸå¤±æƒé‡ã€‚</li>
<li>PCEæ„å»ºåŸºäºæ‰©æ•£æ¨¡å‹çš„åŒºåŸŸéŸ³é¢‘è§†è§‰åˆ†ç±»å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a1f0483af65ce182988a529130b9e39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cece2a16e20fe094b0a091a8a52a52b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd3cf934758a0353aaad6592019986e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c931602be0ec9ec5124b597ff58d187.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-726420598d3c08505ffdccdde0a5f202.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SepALM-Audio-Language-Models-Are-Error-Correctors-for-Robust-Speech-Separation"><a href="#SepALM-Audio-Language-Models-Are-Error-Correctors-for-Robust-Speech-Separation" class="headerlink" title="SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation"></a>SepALM: Audio Language Models Are Error Correctors for Robust Speech   Separation</h2><p><strong>Authors:Zhaoxi Mu, Xinyu Yang, Gang Wang</strong></p>
<p>While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments. </p>
<blockquote>
<p>è™½ç„¶ç›®å‰çš„è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åœ¨å¤„ç†å†—é•¿çš„æ··åˆéŸ³é¢‘æ³¢å½¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å®é™…ç¯å¢ƒä¸­çš„å¤æ‚æ€§æ–¹é¢ç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å˜ˆæ‚å’Œå›å£°ç¯å¢ƒï¼Œè¿™å¯èƒ½å¯¼è‡´åˆ†ç¦»åçš„è¯­éŸ³å‡ºç°ä¼ªåƒæˆ–å¤±çœŸã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†SepALMï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰åœ¨åˆæ­¥åˆ†ç¦»åçº æ­£å’Œé‡æ–°åˆæˆæ–‡æœ¬åŸŸå†…çš„è¯­éŸ³çš„å¼€åˆ›æ€§æ–¹æ³•ã€‚SepALMåŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåˆ†ç¦»å™¨ã€æ ¡æ­£å™¨ã€åˆæˆå™¨å’Œå¯¹é½å™¨ã€‚é€šè¿‡é›†æˆåŸºäºALMçš„ç«¯åˆ°ç«¯é”™è¯¯æ ¡æ­£æœºåˆ¶ï¼Œæˆ‘ä»¬é™ä½äº†è¯¯å·®ç§¯ç´¯çš„é£é™©ï¼Œå¹¶ç»•è¿‡äº†ä¼ ç»Ÿæ–¹æ³•ä¸­é‡åˆ°çš„ä¼˜åŒ–éšœç¢ï¼Œè¿™äº›æ–¹æ³•å°†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†é“¾æ€ç»´ï¼ˆCoTï¼‰æç¤ºå’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›ALMçš„æ¨ç†å’Œè®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒè¯å®ï¼ŒSepALMä¸ä»…æé«˜äº†è¯­éŸ³åˆ†ç¦»çš„ç²¾åº¦ï¼Œè¿˜æ˜¾è‘—æé«˜äº†å¯¹æ–°ç¯å¢ƒçš„é€‚åº”èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03273v1">PDF</a> Appears in IJCAI 2025</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ç°æœ‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åœ¨å¤„ç†å¤æ‚ç°å®ç¯å¢ƒï¼ˆå¦‚å™ªå£°å’Œå›å£°ç¯å¢ƒï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSepALMçš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰åœ¨æ–‡æœ¬åŸŸå†…å¯¹åˆæ­¥åˆ†ç¦»çš„è¯­éŸ³è¿›è¡Œä¿®æ­£å’Œé‡æ–°åˆæˆã€‚é€šè¿‡æ•´åˆALMç«¯åˆ°ç«¯çš„é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œè¯¥æ–¹æ³•é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ä¸­è¯­éŸ³è¯†åˆ«ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆæ—¶çš„ä¼˜åŒ–éš¾é¢˜ï¼Œæé«˜äº†è¯­éŸ³åˆ†ç¦»çš„å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå’ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥ä¿ƒè¿›äº†æ¨¡å‹çš„æ¨ç†å’Œè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SepALMæ—¨åœ¨è§£å†³ç°æœ‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åœ¨ç°å®ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>å®ƒé€šè¿‡éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰åœ¨æ–‡æœ¬åŸŸå†…è¿›è¡Œè¯­éŸ³ä¿®æ­£å’Œé‡æ–°åˆæˆã€‚</li>
<li>SepALMé€šè¿‡æ•´åˆç«¯åˆ°ç«¯çš„é”™è¯¯ä¿®æ­£æœºåˆ¶ï¼Œå‡å°‘äº†é”™è¯¯ç´¯ç§¯çš„é£é™©ã€‚</li>
<li>è¯¥æ–¹æ³•é¿å…äº†ä¼ ç»Ÿæ–¹æ³•ç»“åˆè¯­éŸ³è¯†åˆ«ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„ä¼˜åŒ–éš¾é¢˜ã€‚</li>
<li>SepALMæé«˜äº†è¯­éŸ³åˆ†ç¦»çš„ç²¾ç¡®åº¦å¹¶å¢å¼ºäº†åœ¨æ–°å£°ç¯å¢ƒä¸­çš„é€‚åº”æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºï¼Œä¿ƒè¿›äº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cea2c2de07e9f76d5d287e52a2260884.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bd4438546dbe67bd97e63c99d89b160.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9a8df5c6ef19db4c4e1db45d4e293b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1aa4cfbfb4479b1589955bea09bb0a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CoGenAV-Versatile-Audio-Visual-Representation-Learning-via-Contrastive-Generative-Synchronization"><a href="#CoGenAV-Versatile-Audio-Visual-Representation-Learning-via-Contrastive-Generative-Synchronization" class="headerlink" title="CoGenAV: Versatile Audio-Visual Representation Learning via   Contrastive-Generative Synchronization"></a>CoGenAV: Versatile Audio-Visual Representation Learning via   Contrastive-Generative Synchronization</h2><p><strong>Authors:Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo</strong></p>
<p>The inherent synchronization between a speakerâ€™s lip movements, voice, and the underlying linguistic content offers a rich source of information for improving speech processing tasks, especially in challenging conditions where traditional audio-only systems falter. We introduce CoGenAV, a powerful and data-efficient model designed to learn versatile audio-visual representations applicable across a wide range of speech and audio-visual tasks. CoGenAV is trained by optimizing a dual objective derived from natural audio-visual synchrony, contrastive feature alignment and generative text prediction, using only 223 hours of labeled data from the LRS2 dataset. This contrastive-generative synchronization strategy effectively captures fundamental cross-modal correlations. We showcase the effectiveness and versatility of the learned CoGenAV representations on multiple benchmarks. When utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these representations contribute to achieving a state-of-the-art Word Error Rate (WER) of 1.27. They also enable strong performance in Visual Speech Recognition (VSR) with a WER of 22.0 on LRS2, and significantly improve performance in noisy environments by over 70%. Furthermore, CoGenAV representations benefit speech reconstruction tasks, boosting performance in Speech Enhancement and Separation, and achieve competitive results in audio-visual synchronization tasks like Active Speaker Detection (ASD). Our model will be open-sourced to facilitate further development and collaboration within both academia and industry. </p>
<blockquote>
<p>è¯´è¯è€…çš„å˜´å”‡åŠ¨ä½œã€å£°éŸ³å’Œåº•å±‚è¯­è¨€å†…å®¹ä¹‹é—´çš„å›ºæœ‰åŒæ­¥æ€§ï¼Œä¸ºæ”¹è¿›è¯­éŸ³è¯†åˆ«ä»»åŠ¡æä¾›äº†ä¸°å¯Œçš„ä¿¡æ¯æ¥æºï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ ç»Ÿä»…ä¾èµ–éŸ³é¢‘çš„ç³»ç»Ÿè¡¨ç°ä¸ä½³çš„å›°éš¾æ¡ä»¶ä¸‹ã€‚æˆ‘ä»¬æ¨å‡ºäº†CoGenAVï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”æ•°æ®é«˜æ•ˆæ¨¡å‹ï¼Œæ—¨åœ¨å­¦ä¹ é€‚ç”¨äºå¹¿æ³›è¯­éŸ³å’Œè§†å¬ä»»åŠ¡çš„é€šç”¨è§†å¬è¡¨ç¤ºã€‚CoGenAVé€šè¿‡ä¼˜åŒ–ä»è‡ªç„¶è§†å¬åŒæ­¥ä¸­å¾—å‡ºçš„åŒé‡ç›®æ ‡ã€å¯¹æ¯”ç‰¹å¾å¯¹é½å’Œæ–‡æœ¬ç”Ÿæˆé¢„æµ‹æ¥è¿›è¡Œè®­ç»ƒï¼Œä»…ä½¿ç”¨LRS2æ•°æ®é›†çš„223å°æ—¶æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¿™ç§å¯¹æ¯”ç”ŸæˆåŒæ­¥ç­–ç•¥æœ‰æ•ˆåœ°æ•æ‰äº†è·¨æ¨¡æ€çš„åŸºæœ¬ç›¸å…³æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å­¦ä¹ åˆ°çš„CoGenAVè¡¨ç¤ºçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚åœ¨LRS2ä¸Šçš„è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ä¸­åˆ©ç”¨è¿™äº›è¡¨ç¤ºï¼Œå®ç°äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¾¾åˆ°å…ˆè¿›æ°´å¹³çš„1.27%ã€‚å®ƒä»¬è¿˜åœ¨LRS2ä¸Šçš„è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰ä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œè¯é”™è¯¯ç‡ä¸º22.0ï¼Œå¹¶ä¸”åœ¨å™ªå£°ç¯å¢ƒä¸­çš„æ€§èƒ½æé«˜äº†70%ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒCoGenAVè¡¨ç¤ºè¿˜å—ç›Šäºè¯­éŸ³é‡å»ºä»»åŠ¡ï¼Œæé«˜äº†è¯­éŸ³å¢å¼ºå’Œåˆ†ç¦»çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¦‚ä¸»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰ç­‰è§†å¬åŒæ­¥ä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†å¼€æºï¼Œä»¥ä¿ƒè¿›å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„è¿›ä¸€æ­¥å‘å±•å’Œåˆä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03186v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CoGenAVæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¼˜åŒ–è‡ªç„¶è§†å¬åŒæ­¥ã€å¯¹æ¯”ç‰¹å¾å¯¹é½å’Œç”Ÿæˆæ–‡æœ¬é¢„æµ‹çš„åŒç›®æ ‡ï¼Œåˆ©ç”¨LRS2æ•°æ®é›†ä»…223å°æ—¶çš„æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹çš„å¯¹æ¯”ç”ŸæˆåŒæ­¥ç­–ç•¥æœ‰æ•ˆåœ°æ•æ‰äº†è·¨æ¨¡æ€çš„åŸºæœ¬å…³è”ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCoGenAVè¡¨ç¤ºçš„æœ‰æ•ˆæ€§åŠé€šç”¨æ€§å¾—åˆ°äº†å±•ç¤ºã€‚åœ¨LRS2ä¸Šçš„éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ä¸­ï¼Œå®ƒè¾¾åˆ°äº†å…ˆè¿›çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰1.27ã€‚åœ¨LRS2ä¸Šçš„è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰ä¸­ï¼Œå®ƒçš„æ€§èƒ½ä¹Ÿå¾ˆå¼ºï¼ŒWERä¸º22.0ã€‚æ­¤å¤–ï¼ŒCoGenAVè¡¨ç¤ºè¿˜èƒ½æé«˜è¯­éŸ³é‡å»ºä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœ‰å™ªéŸ³çš„ç¯å¢ƒä¸­æé«˜70%ä»¥ä¸Šçš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹è¿˜å°†å…¬å¼€æºä»£ç ï¼Œä»¥ä¾¿å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„è¿›ä¸€æ­¥å¼€å‘å’Œåä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoGenAVæ¨¡å‹ç»“åˆäº†è¯´è¯è€…çš„å˜´å”‡åŠ¨ä½œã€å£°éŸ³å’Œåº•å±‚è¯­è¨€å†…å®¹ï¼Œä¸ºæ”¹è¿›è¯­éŸ³å¤„ç†ä»»åŠ¡æä¾›äº†ä¸°å¯Œä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ä¼˜åŒ–è‡ªç„¶è§†å¬åŒæ­¥ã€å¯¹æ¯”ç‰¹å¾å¯¹é½å’Œç”Ÿæˆæ–‡æœ¬é¢„æµ‹çš„åŒç›®æ ‡è¿›è¡Œè®¾è®¡ã€‚</li>
<li>ä½¿ç”¨ä»…223å°æ—¶çš„LRS2æ•°æ®é›†æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ˜¾ç¤ºå‡ºæ•°æ®æ•ˆç‡ã€‚</li>
<li>å¯¹æ¯”ç”ŸæˆåŒæ­¥ç­–ç•¥æœ‰æ•ˆæ•æ‰è·¨æ¨¡æ€å…³è”ã€‚</li>
<li>CoGenAVåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰çš„è¯é”™è¯¯ç‡è¾¾åˆ°1.27ã€‚</li>
<li>æ¨¡å‹åœ¨è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨å˜ˆæ‚ç¯å¢ƒä¸­æ€§èƒ½æ˜¾è‘—æå‡è¶…è¿‡70%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d6740e367867df44326dbecc37869e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04dcf30c82012b2e831ae11cf0dcf2b4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="fastabx-A-library-for-efficient-computation-of-ABX-discriminability"><a href="#fastabx-A-library-for-efficient-computation-of-ABX-discriminability" class="headerlink" title="fastabx: A library for efficient computation of ABX discriminability"></a>fastabx: A library for efficient computation of ABX discriminability</h2><p><strong>Authors:Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux</strong></p>
<p>We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/bootphon/fastabx">https://github.com/bootphon/fastabx</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»fastabxï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºABXè¾¨åˆ«ä»»åŠ¡çš„é«˜æ€§èƒ½Pythonåº“ã€‚ABXæ˜¯è¡¡é‡é€šç”¨ç±»åˆ«ä¹‹é—´å·®å¼‚çš„ä¸€ç§åº¦é‡ã€‚å®ƒå·²è¢«å¹¿æ³›ç”¨äºè¯„ä¼°è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºä¸­çš„è¯­éŸ³è¾¨åˆ«åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹è¶³å¤Ÿçš„å·¥å…·ï¼Œå…¶æ›´å¹¿æ³›çš„åº”ç”¨å—åˆ°äº†é™åˆ¶ã€‚fastabxé€šè¿‡æä¾›ä¸€ä¸ªèƒ½å¤Ÿæ„å»ºä»»ä½•ç±»å‹çš„ABXä»»åŠ¡åŒæ—¶æä¾›å¿«é€Ÿå¼€å‘å‘¨æœŸæ‰€éœ€çš„æ•ˆç‡çš„æ¡†æ¶æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œæ— è®ºæ˜¯åœ¨ä»»åŠ¡åˆ›å»ºè¿˜æ˜¯åœ¨è®¡ç®—è¡¨ç¤ºä¹‹é—´çš„è·ç¦»æ–¹é¢ã€‚æˆ‘ä»¬ç›¸ä¿¡fastabxå°†ä¸ºæ›´å¹¿æ³›çš„è¡¨ç¤ºå­¦ä¹ ç¤¾åŒºæä¾›å®è´µçš„èµ„æºï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿç³»ç»Ÿåœ°ç ”ç©¶ä»å¤šä¸ªé¢†åŸŸï¼ˆåŒ…æ‹¬è¯­éŸ³å¤„ç†é¢†åŸŸä¹‹å¤–ï¼‰çš„å·²å­¦ä¹ è¡¨ç¤ºä¸­å¯ä»¥ç›´æ¥æå–å“ªäº›ä¿¡æ¯ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bootphon/fastabx%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bootphon/fastabxæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02692v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>fastabxæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„Pythonåº“ï¼Œç”¨äºæ„å»ºABXè¾¨åˆ«ä»»åŠ¡ã€‚å®ƒæ—¨åœ¨è§£å†³åœ¨è‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºä¸­è¯„ä¼°éŸ³ä½åŒºåˆ†èƒ½åŠ›çš„é—®é¢˜ã€‚è¯¥åº“æä¾›äº†å¹¿æ³›çš„æ¡†æ¶ï¼Œèƒ½æ„å»ºå„ç§ç±»å‹çš„ABXä»»åŠ¡å¹¶åœ¨ä»»åŠ¡åˆ›å»ºå’Œè®¡ç®—è¡¨ç¤ºä¹‹é—´çš„è·ç¦»æ—¶å®ç°é«˜æ•ˆç‡ï¼Œæ»¡è¶³å¿«é€Ÿå¼€å‘å‘¨æœŸçš„éœ€æ±‚ã€‚fastabxå°†æœåŠ¡äºæ›´å¹¿æ³›çš„è¡¨ç¤ºå­¦ä¹ ç¤¾åŒºï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿç³»ç»Ÿåœ°ç ”ç©¶ä»å¤šä¸ªé¢†åŸŸï¼ˆåŒ…æ‹¬è¯­éŸ³å¤„ç†é¢†åŸŸï¼‰çš„å·²å­¦è¡¨ç¤ºä¸­ç›´æ¥æå–çš„ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>fastabxæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºABXè¾¨åˆ«ä»»åŠ¡çš„Pythonåº“ã€‚</li>
<li>ABXç”¨äºè¯„ä¼°éŸ³ä½åŒºåˆ†èƒ½åŠ›çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºä¸­ã€‚</li>
<li>fastabxæä¾›äº†æ„å»ºå„ç§ABXä»»åŠ¡çš„æ¡†æ¶ã€‚</li>
<li>fastabxå®ç°äº†é«˜æ•ˆçš„ä»»åŠ¡åˆ›å»ºå’Œè®¡ç®—è¡¨ç¤ºä¹‹é—´çš„è·ç¦»ã€‚</li>
<li>fastabxå¡«è¡¥äº†ç°æœ‰å·¥å…·çš„ç©ºç™½ï¼Œæ”¯æŒå¿«é€Ÿå¼€å‘å‘¨æœŸã€‚</li>
<li>fastabxå¯¹æ›´å¹¿æ³›çš„è¡¨ç¤ºå­¦ä¹ ç¤¾åŒºæœ‰ä»·å€¼ï¼Œå› ä¸ºå®ƒèƒ½ç³»ç»Ÿåœ°ç ”ç©¶ä»å¤šä¸ªé¢†åŸŸçš„å·²å­¦è¡¨ç¤ºä¸­æå–çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b6cffe2440818cf70aeca750e0a972b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e733de321d7978a32b2815d4e031db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-101f69cb1e08ac2ce706c23b9650f3dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e8a002edd596e2a8f3c6dddc0fbfe44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5d457969d89961efe4f6bb6e025d57c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15c960a614ce2b6ec17e46248f066956.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3272a3008675fb471851ffaf276d857.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Co-3-Gesture-Towards-Coherent-Concurrent-Co-speech-3D-Gesture-Generation-with-Interactive-Diffusion"><a href="#Co-3-Gesture-Towards-Coherent-Concurrent-Co-speech-3D-Gesture-Generation-with-Interactive-Diffusion" class="headerlink" title="Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture   Generation with Interactive Diffusion"></a>Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture   Generation with Interactive Diffusion</h2><p><strong>Authors:Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo</strong></p>
<p>Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakersâ€™ gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \href{<a target="_blank" rel="noopener" href="https://mattie-e.github.io/Co3/%7D%7B/textit%7Bhttps://mattie-e.github.io/Co3/%7D%7D">https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}</a>. </p>
<blockquote>
<p>ä»äººç±»è¯­éŸ³ç”ŸæˆåŠ¨ä½œåœ¨è™šæ‹Ÿè§’è‰²åŠ¨ç”»æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•èƒ½å¤Ÿåˆæˆä¸ªäººè‡ªè¨€è‡ªè¯­æ—¶çš„åŠ¨ä½œï¼Œä½†å®ƒä»¬å¿½ç•¥äº†åŒäººäº’åŠ¨å¯¹è¯æ—¶çš„åŒæ­¥åŠ¨ä½œå»ºæ¨¡çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œç¼ºä¹é«˜è´¨é‡çš„åŒæ­¥è¯­éŸ³åŠ¨ä½œæ•°æ®é›†ä¹Ÿé™åˆ¶äº†å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŒæ­¥è¯­éŸ³åŠ¨ä½œæ•°æ®é›†GES-Interï¼Œå…¶ä¸­åŒ…å«è¶…è¿‡700ä¸‡å¸§çš„å¤šæ ·åŒ–åŒäººäº’åŠ¨å§¿åŠ¿åºåˆ—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†Co$^3$Gestureè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°åŒ…æ‹¬åŒäººäº’åŠ¨åœ¨å†…çš„è¿è´¯åŒæ­¥è¯­éŸ³åŠ¨ä½œåˆæˆã€‚è€ƒè™‘åˆ°ä¸¤ä½å‘è¨€äººçš„ä¸å¯¹ç§°èº«ä½“åŠ¨æ€ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨ä¸¤ä¸ªåŸºäºä¸åŒå‘è¨€äººéŸ³é¢‘çš„åˆä½œç”Ÿæˆåˆ†æ”¯ä¹‹ä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¢å¼ºäººç±»å§¿åŠ¿ä¸ç›¸åº”è¯­éŸ³çš„åè°ƒæ€§ï¼ŒåŒæ—¶ä¸å¯¹è¯ä¼™ä¼´è¿›è¡Œäº’åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶åºäº¤äº’æ¨¡å—ï¼ˆTIMï¼‰ã€‚TIMå¯ä»¥æœ‰æ•ˆåœ°å¯¹ä¸¤ä½å‘è¨€äººçš„åŠ¨ä½œåºåˆ—ä¹‹é—´çš„æ—¶é—´å…³è”è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ï¼Œå°†å…¶ä½œä¸ºäº¤äº’æŒ‡å—å¹¶èåˆåˆ°åŒæ­¥åŠ¨ä½œç”Ÿæˆä¸­ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç›¸äº’æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿›ä¸€æ­¥å…¨é¢æå‡äº¤äº’åŒæ­¥åŠ¨ä½œçš„å­¦ä¹ ä¾èµ–å…³ç³»ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆç”ŸåŠ¨è¿è´¯çš„åŠ¨ä½œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æˆ‘ä»¬çš„æ–°æ”¶é›†çš„GES-Interæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æ¨¡å‹ã€‚æ•°æ®é›†å’Œæºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://mattie-e.github.io/Co3/]%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://mattie-e.github.io/Co3/]ä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01746v1">PDF</a> Accepted as ICLR 2025 (Spotlight)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨ä»äººç±»è¯­éŸ³ç”ŸæˆåŠ¨ä½œå§¿æ€åœ¨è™šæ‹Ÿè§’è‰²åŠ¨ç”»é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•äººè‡ªæˆ‘å¯¹è¯çš„æ‰‹åŠ¿åˆæˆä¸Šï¼Œä½†åœ¨ä¸¤äººäº’åŠ¨å¯¹è¯ä¸­åŒæ­¥çš„æ‰‹åŠ¿å»ºæ¨¡å®è·µä»å­˜åœ¨å±€é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸¤äººäº’åŠ¨è¯­éŸ³åŒæ­¥æ‰‹åŠ¿æ•°æ®é›†GES-Interï¼ŒåŒ…å«è¶…è¿‡7ç™¾ä¸‡å¸§æ•°æ®ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCo$^3$Gestureçš„æ–°æ¡†æ¶ï¼Œç”¨äºå®ç°åŒ…æ‹¬ä¸¤äººäº’åŠ¨åŠ¨ä½œåœ¨å†…çš„è¿è´¯åŒæ­¥è¯­éŸ³æ‰‹åŠ¿åˆæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªåŸºäºä¸åŒè¯´è¯äººéŸ³é¢‘çš„åˆä½œç”Ÿæˆåˆ†æ”¯æ„å»ºï¼Œè€ƒè™‘äº†è¯´è¯äººçš„ä¸å¯¹ç§°åŠ¨æ€ç‰¹æ€§ã€‚ä¸ºå¢å¼ºäº¤äº’å¯¹è¯ä¸­å§¿æ€ä¸éŸ³é¢‘çš„åè°ƒæ€§ï¼Œå¼•å…¥äº†æ—¶åºäº¤äº’æ¨¡å—ï¼ˆTIMï¼‰ï¼Œæœ‰æ•ˆå»ºæ¨¡ä¸¤ä½è¯´è¯äººæ‰‹åŠ¿åºåˆ—é—´çš„æ—¶åºå…³è”è¡¨ç¤ºå¹¶å°†å…¶èå…¥åŒæ­¥æ‰‹åŠ¿ç”Ÿæˆã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾è®¡ç›¸äº’æ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥å¼ºåŒ–äº†äº’åŠ¨åŠ¨ä½œä¹‹é—´çš„å­¦ä¹ ä¾èµ–æ€§ï¼Œä½¿å¾—ç”Ÿæˆçš„åŠ¨ä½œæ›´åŠ ç”ŸåŠ¨è¿è´¯ã€‚åœ¨æ”¶é›†çš„æ–°æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æ•°æ®é›†å’Œæºä»£ç å·²å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ‰‹åŠ¿åˆæˆæ–¹æ³•ä¸»è¦å…³æ³¨å•äººè‡ªæˆ‘å¯¹è¯åœºæ™¯ï¼Œç¼ºä¹å¤„ç†ä¸¤äººäº’åŠ¨å¯¹è¯çš„å®ç”¨æ€§ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸¤äººäº’åŠ¨è¯­éŸ³åŒæ­¥æ‰‹åŠ¿æ•°æ®é›†ï¼ˆGES-Interï¼‰ï¼ŒåŒ…å«ä¸°å¯Œçš„åŠ¨æ€åœºæ™¯æ•°æ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶Co$^3$Gestureï¼Œç”¨äºè¿è´¯çš„åŒæ­¥è¯­éŸ³æ‰‹åŠ¿åˆæˆï¼Œæ”¯æŒä¸¤äººäº’åŠ¨åŠ¨ä½œã€‚</li>
<li>æ¡†æ¶é€šè¿‡ä¸¤ä¸ªåŸºäºä¸åŒè¯´è¯äººéŸ³é¢‘çš„åˆä½œç”Ÿæˆåˆ†æ”¯æ„å»ºï¼Œè€ƒè™‘äº†è¯´è¯äººçš„ä¸å¯¹ç§°åŠ¨æ€ç‰¹æ€§ã€‚</li>
<li>å¼•å…¥æ—¶åºäº¤äº’æ¨¡å—ï¼ˆTIMï¼‰ï¼Œå®ç°äº†äº¤äº’åŠ¨ä½œä¸è¯­éŸ³ä¹‹é—´çš„åè°ƒå»ºæ¨¡ã€‚</li>
<li>é‡‡ç”¨ç›¸äº’æ³¨æ„åŠ›æœºåˆ¶å¼ºåŒ–äº’åŠ¨åŠ¨ä½œä¹‹é—´çš„å­¦ä¹ ä¾èµ–æ€§ï¼Œæå‡åŠ¨ä½œç”Ÿæˆçš„ç”ŸåŠ¨æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-acc634d0afe57c9cc8b96fb27ba41cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0501a1e3d1c2026ad6aa448f20bdf51e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea6f14c6fac0c485116ddfedeb961373.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VANPY-Voice-Analysis-Framework"><a href="#VANPY-Voice-Analysis-Framework" class="headerlink" title="VANPY: Voice Analysis Framework"></a>VANPY: Voice Analysis Framework</h2><p><strong>Authors:Gregory Koushnir, Michael Fire, Galit Fuhrmann Alpert, Dima Kagan</strong></p>
<p>Voice data is increasingly being used in modern digital communications, yet there is still a lack of comprehensive tools for automated voice analysis and characterization. To this end, we developed the VANPY (Voice Analysis in Python) framework for automated pre-processing, feature extraction, and classification of voice data. The VANPY is an open-source end-to-end comprehensive framework that was developed for the purpose of speaker characterization from voice data. The framework is designed with extensibility in mind, allowing for easy integration of new components and adaptation to various voice analysis applications. It currently incorporates over fifteen voice analysis components - including music&#x2F;speech separation, voice activity detection, speaker embedding, vocal feature extraction, and various classification models.   Four of the VANPYâ€™s components were developed in-house and integrated into the framework to extend its speaker characterization capabilities: gender classification, emotion classification, age regression, and height regression. The models demonstrate robust performance across various datasets, although not surpassing state-of-the-art performance.   As a proof of concept, we demonstrate the frameworkâ€™s ability to extract speaker characteristics on a use-case challenge of analyzing character voices from the movie â€œPulp Fiction.â€ The results illustrate the frameworkâ€™s capability to extract multiple speaker characteristics, including gender, age, height, emotion type, and emotion intensity measured across three dimensions: arousal, dominance, and valence. </p>
<blockquote>
<p>è¯­éŸ³æ•°æ®åœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†ä»ç¼ºä¹å…¨é¢çš„è‡ªåŠ¨åŒ–è¯­éŸ³åˆ†æå’Œè¡¨å¾å·¥å…·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ç”¨äºè‡ªåŠ¨åŒ–é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œè¯­éŸ³åˆ†ç±»çš„VANPYï¼ˆPythonè¯­éŸ³åˆ†æï¼‰æ¡†æ¶ã€‚VANPYæ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨ä»è¯­éŸ³æ•°æ®ä¸­å®ç°è¯´è¯äººè¡¨å¾ã€‚è¯¥æ¡†æ¶åœ¨è®¾è®¡æ—¶è€ƒè™‘äº†å¯æ‰©å±•æ€§ï¼Œä¾¿äºé›†æˆæ–°ç»„ä»¶å¹¶é€‚åº”å„ç§è¯­éŸ³åˆ†æåº”ç”¨ã€‚å®ƒç›®å‰é›†æˆäº†è¶…è¿‡åäº”ç§è¯­éŸ³åˆ†æç»„ä»¶ï¼ŒåŒ…æ‹¬éŸ³ä¹&#x2F;è¯­éŸ³åˆ†ç¦»ã€è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€è¯´è¯äººåµŒå…¥ã€è¯­éŸ³ç‰¹å¾æå–å’Œå„ç§åˆ†ç±»æ¨¡å‹ã€‚ä¸ºäº†æ‰©å±•å…¶è¯´è¯äººè¡¨å¾èƒ½åŠ›ï¼Œæˆ‘ä»¬å†…éƒ¨å¼€å‘äº†å››ä¸ªç»„ä»¶å¹¶å°†å…¶é›†æˆåˆ°æ¡†æ¶ä¸­ï¼šæ€§åˆ«åˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ã€å¹´é¾„å›å½’å’Œèº«é«˜å›å½’ã€‚æ¨¡å‹åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œè™½ç„¶æ²¡æœ‰è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨åˆ†æç”µå½±ã€Šä½ä¿—å°è¯´ã€‹ä¸­çš„è§’è‰²å£°éŸ³çš„åº”ç”¨èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæå–å¤šä¸ªè¯´è¯äººçš„ç‰¹å¾ï¼ŒåŒ…æ‹¬æ€§åˆ«ã€å¹´é¾„ã€èº«é«˜ã€æƒ…æ„Ÿç±»å‹å’Œæƒ…æ„Ÿå¼ºåº¦ï¼Œè¿™äº›æƒ…æ„Ÿå¼ºåº¦æ˜¯åœ¨ä¸‰ä¸ªç»´åº¦ä¸Šæµ‹é‡çš„ï¼šå…´å¥‹åº¦ã€æ”¯é…åŠ›å’Œä»·å€¼åˆ¤æ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17579v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€ç°ä»£æ•°å­—é€šä¿¡ä¸­è¯­éŸ³æ•°æ®çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œç¼ºä¹å…¨é¢çš„è‡ªåŠ¨åŒ–è¯­éŸ³åˆ†æå’Œè¡¨å¾å·¥å…·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†VANPYï¼ˆPythonä¸­çš„è¯­éŸ³åˆ†æï¼‰æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–é¢„å¤„ç†ã€ç‰¹å¾æå–å’Œè¯­éŸ³æ•°æ®çš„åˆ†ç±»ã€‚å®ƒæ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨ä»è¯­éŸ³æ•°æ®ä¸­å®ç°è¯´è¯äººè¡¨å¾ã€‚æ¡†æ¶è®¾è®¡å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯è½»æ¾é›†æˆæ–°ç»„ä»¶å¹¶é€‚åº”å„ç§è¯­éŸ³åˆ†æåº”ç”¨ç¨‹åºã€‚ç›®å‰ï¼Œå®ƒå·²é›†æˆäº†è¶…è¿‡åäº”ç§è¯­éŸ³åˆ†æç»„ä»¶ï¼ŒåŒ…æ‹¬éŸ³ä¹&#x2F;è¯­éŸ³åˆ†ç¦»ã€è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€è¯´è¯äººåµŒå…¥ã€è¯­éŸ³ç‰¹å¾æå–å’Œå„ç§åˆ†ç±»æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†å››ä¸ªå†…éƒ¨ç»„ä»¶å¹¶å°†å…¶é›†æˆåˆ°æ¡†æ¶ä¸­ï¼Œä»¥æ‰©å±•å…¶è¯´è¯äººè¡¨å¾åŠŸèƒ½ï¼šæ€§åˆ«åˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ã€å¹´é¾„å›å½’å’Œèº«é«˜å›å½’ã€‚æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå°½ç®¡æ²¡æœ‰è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨åˆ†æç”µå½±â€œä½ä¿—å°è¯´â€ä¸­çš„è§’è‰²å£°éŸ³æ–¹é¢çš„èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿæå–å¤šä¸ªè¯´è¯äººçš„ç‰¹å¾ï¼ŒåŒ…æ‹¬æ€§åˆ«ã€å¹´é¾„ã€èº«é«˜ã€æƒ…æ„Ÿç±»å‹å’Œæƒ…æ„Ÿå¼ºåº¦ã€‚è¿™ä¸‰ä¸ªç»´åº¦åŒ…æ‹¬å…´å¥‹åº¦ã€æ”¯é…åŠ›å’Œä»·å€¼æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç¼ºä¹å…¨é¢å·¥å…·è¿›è¡Œè‡ªåŠ¨åŒ–è¯­éŸ³åˆ†æå’Œè¡¨å¾ï¼Œå› æ­¤éœ€è¦å¼€å‘VANPYæ¡†æ¶ä»¥å¤„ç†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>VANPYæ˜¯ä¸€ä¸ªå¼€æºçš„ç«¯åˆ°ç«¯ç»¼åˆæ¡†æ¶ï¼Œç”¨äºä»è¯­éŸ³æ•°æ®ä¸­å®ç°è¯´è¯äººè¡¨å¾ã€‚</li>
<li>è¯¥æ¡†æ¶è®¾è®¡å…·æœ‰å¯æ‰©å±•æ€§ï¼Œæ˜“äºé›†æˆæ–°ç»„ä»¶å¹¶é€‚åº”å„ç§è¯­éŸ³åˆ†æåº”ç”¨ã€‚</li>
<li>ç›®å‰å·²é›†æˆè¶…è¿‡åäº”ç§è¯­éŸ³åˆ†æç»„ä»¶ï¼ŒåŒ…æ‹¬éŸ³ä¹&#x2F;è¯­éŸ³åˆ†ç¦»ç­‰ã€‚</li>
<li>å¼€å‘å››ä¸ªå†…éƒ¨ç»„ä»¶ä»¥æ‰©å±•å…¶è¯´è¯äººè¡¨å¾åŠŸèƒ½ï¼šæ€§åˆ«åˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ç­‰ã€‚è¿™äº›ç»„ä»¶å¯åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œæ€§èƒ½è¯„ä¼°ã€‚</li>
<li>æ¨¡å‹è™½ç„¶ç¨³å¥ä½†åœ¨æ€§èƒ½ä¸Šå¹¶æœªè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä½†ä»å¯ä½œä¸ºå¼ºæœ‰åŠ›çš„å·¥å…·ç”¨äºå®é™…åœºæ™¯åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ac625ecedefe227d7fb21956074c1e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d9643ada26b159396eef07ec5610fee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c0959617171a2938791b7df8ebeda2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69d6fb5e5da91e6f88b4ee2b2b7a3a8e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HAINAN-Fast-and-Accurate-Transducer-for-Hybrid-Autoregressive-ASR"><a href="#HAINAN-Fast-and-Accurate-Transducer-for-Hybrid-Autoregressive-ASR" class="headerlink" title="HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR"></a>HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR</h2><p><strong>Authors:Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg</strong></p>
<p>We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive inference further enhances the modelâ€™s accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAINANâ€™s flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world speech recognition applications. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†æ··åˆè‡ªå›å½’æ¨ç†è½¬æ¢å™¨ï¼ˆHybrid-Autoregressive INference TrANsducersï¼Œç®€ç§°HAINANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯­éŸ³è¯†åˆ«çš„æ–°å‹æ¶æ„ï¼Œå®ƒæ‰©å±•äº†Token-and-Duration Transducerï¼ˆTDTï¼‰æ¨¡å‹ã€‚é€šè¿‡è®­ç»ƒå¸¦æœ‰éšæœºæ©ç çš„é¢„æµ‹å™¨ç½‘ç»œè¾“å‡ºï¼ŒHAINANæ”¯æŒåŒ…å«æ‰€æœ‰ç½‘ç»œç»„ä»¶çš„è‡ªå›å½’æ¨ç†å’Œéè‡ªå›å½’æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠè‡ªå›å½’æ¨ç†èŒƒå¼ï¼Œé¦–å…ˆä½¿ç”¨éè‡ªå›å½’æ¨ç†ç”Ÿæˆåˆå§‹å‡è®¾ï¼Œç„¶åè¿›è¡Œç»†åŒ–æ­¥éª¤ï¼Œå…¶ä¸­æ¯ä¸ªä»¤ç‰Œé¢„æµ‹éƒ½ä¼šåŸºäºåˆå§‹å‡è®¾ä½¿ç”¨å¹¶è¡Œè‡ªå›å½’è¿›è¡Œé‡å»ºã€‚åœ¨å¤šè¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHAINANåœ¨éè‡ªå›å½’æ¨¡å¼ä¸‹ä¸CTCçš„æ•ˆç‡ç›¸å½“ï¼Œåœ¨è‡ªå›å½’æ¨¡å¼ä¸‹ä¸TDTçš„æ•ˆç‡ç›¸å½“ã€‚åœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œè‡ªå›å½’çš„HAINANä¼˜äºTDTå’ŒRNN-Tï¼Œè€Œé‡‡ç”¨éè‡ªå›å½’æ¨ç†çš„HAINANåˆ™æ˜¾è‘—ä¼˜äºCTCã€‚åŠè‡ªå›å½’æ¨ç†èƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€è¾ƒå°ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†TDTçš„ç»“æœã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†HAINANåœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œé€Ÿåº¦æ–¹é¢çš„çµæ´»æ€§ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œè¯­éŸ³è¯†åˆ«åº”ç”¨çš„å¼ºå¤§å€™é€‰è€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02597v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Hybrid-Autoregressive INference TrANsducersï¼ˆHAINANï¼‰è¿™ä¸€æ–°å‹è¯­éŸ³è¯†åˆ«æ¶æ„ã€‚HAINANæ‰©å±•äº†Token-and-Duration Transducerï¼ˆTDTï¼‰æ¨¡å‹ï¼Œé€šè¿‡éšæœºæ©ç›–é¢„æµ‹ç½‘ç»œè¾“å‡ºæ¥è¿›è¡Œè®­ç»ƒã€‚å®ƒæ”¯æŒè‡ªå›å½’æ¨ç†å’Œéè‡ªå›å½’æ¨ç†ã€‚HAINANåœ¨å¤šç§è¯­è¨€çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨éè‡ªå›å½’æ¨¡å¼ä¸‹ä¸CTCæ•ˆç‡ç›¸å½“ï¼Œåœ¨è‡ªå›å½’æ¨¡å¼ä¸‹ä¸TDTæ•ˆç‡ç›¸å½“ã€‚åœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œè‡ªå›å½’çš„HAINANä¼˜äºTDTå’ŒRNN-Tï¼Œéè‡ªå›å½’çš„HAINANåˆ™æ˜¾è‘—ä¼˜äºCTCã€‚åŠè‡ªå›å½’æ¨ç†è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—å¼€é”€è¾ƒå°ï¼Œæœ‰æ—¶ç”šè‡³å¯ä»¥è¶…è¶ŠTDTçš„è¡¨ç°ã€‚è¿™è¡¨æ˜HAINANåœ¨å¹³è¡¡å‡†ç¡®æ€§ä¸é€Ÿåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¯ç°å®ä¸–ç•Œè¯­éŸ³è¯†åˆ«åº”ç”¨çš„æœ‰åŠ›å€™é€‰è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HAINANæ˜¯ä¸€ç§æ–°å‹çš„è¯­éŸ³è¯†åˆ«æ¶æ„ï¼Œæ‰©å±•äº†TDTæ¨¡å‹ã€‚</li>
<li>HAINANæ”¯æŒè‡ªå›å½’å’Œéè‡ªå›å½’ä¸¤ç§æ¨ç†æ¨¡å¼ã€‚</li>
<li>HAINANé€šè¿‡éšæœºæ©ç›–é¢„æµ‹ç½‘ç»œè¾“å‡ºæ¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¤šç§è¯­è¨€çš„æ•°æ®é›†ä¸Šï¼ŒHAINANåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è‡ªå›å½’çš„HAINANåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºTDTå’ŒRNN-Tã€‚</li>
<li>éè‡ªå›å½’çš„HAINANæ˜¾è‘—ä¼˜äºCTCã€‚</li>
<li>åŠè‡ªå›å½’æ¨ç†æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸”è®¡ç®—å¼€é”€è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ae7a2582d8438bb76c2fcc366054bcf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-577fbd2ac9729abdec8417d49706cd43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0eba685dcfd49c0ff99d4c900c71cefa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bbcf0b3470dc249090ed21b92d400f1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tailored-Design-of-Audio-Visual-Speech-Recognition-Models-using-Branchformers"><a href="#Tailored-Design-of-Audio-Visual-Speech-Recognition-Models-using-Branchformers" class="headerlink" title="Tailored Design of Audio-Visual Speech Recognition Models using   Branchformers"></a>Tailored Design of Audio-Visual Speech Recognition Models using   Branchformers</h2><p><strong>Authors:David Gimeno-GÃ³mez, Carlos-D. MartÃ­nez-Hinarejos</strong></p>
<p>Recent advances in Audio-Visual Speech Recognition (AVSR) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. In most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. However, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. Furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. In this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. Our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the Branchformer, in the design of parameter-efficient AVSR systems. To be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. Extensive experiments on English and Spanish AVSR benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. Even when trained on a moderate scale of data, our models achieve competitive word error rates (WER) of approximately 2.5% for English and surpass existing approaches for Spanish, establishing a new benchmark with an average WER of around 9.1%. These results reflect how our tailored AVSR system is able to reach state-of-the-art recognition rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. Code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/david-gimeno/tailored-avsr">https://github.com/david-gimeno/tailored-avsr</a>. </p>
<blockquote>
<p>è¿‘æœŸåœ¨è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ–¹é¢çš„è¿›å±•ä¸ºè¯¥é¢†åŸŸå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æˆå°±ï¼Œæé«˜äº†æ­¤ç±»ç³»ç»Ÿåœ¨æ¶åŠ£ã€å˜ˆæ‚ç¯å¢ƒä¸­çš„ç¨³å¥æ€§ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯é€šè¿‡è®¾è®¡ç”±ä¸¤ä¸ªç‹¬ç«‹ç¼–ç å™¨ç»„æˆçš„æ¨¡å‹æ¥è§£å†³çš„ï¼Œæ¯ä¸ªç¼–ç å™¨éƒ½ä¸“æ³¨äºä¸€ç§ç‰¹å®šçš„æ¨¡æ€ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿‘æœŸçš„å·¥ä½œå·²ç»æ¢ç´¢äº†ç»Ÿä¸€çš„è§†å¬ç¼–ç å™¨ï¼Œä½†ç¡®å®šæœ€ä½³çš„è·¨æ¨¡æ€æ¶æ„ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºåŒ…å«å¤§é‡å‚æ•°çš„æ¨¡å‹å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„è§†å¬æ¡†æ¶æ¥ç¼©å°è¿™ä¸€ç ”ç©¶å·®è·ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•é¦–æ¬¡åˆ©ç”¨äº†ç¼–ç å™¨æ¶æ„çš„çµæ´»æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¾‹å¦‚Branchformerï¼Œåœ¨è®¾è®¡å‚æ•°æœ‰æ•ˆçš„AVSRç³»ç»Ÿæ—¶ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆä¼°è®¡ä»…éŸ³é¢‘å’Œä»…è§†é¢‘çš„ç³»ç»Ÿï¼Œç„¶ååŸºäºæ¨¡æ€ç‰¹å®šæ¨¡å‹æä¾›çš„å±‚çº§åˆ†æ”¯åˆ†æ•°è®¾è®¡å®šåˆ¶çš„è§†å¬ç»Ÿä¸€ç¼–ç å™¨ã€‚åœ¨æ¶µç›–å¤šç§æ•°æ®æ¡ä»¶å’Œåœºæ™¯çš„è‹±è¯­å’Œè¥¿ç­ç‰™è¯­AVSRåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å³ä½¿åœ¨ä¸­ç­‰è§„æ¨¡çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿå®ç°äº†çº¦2.5%çš„è‹±è¯­å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„è¥¿ç­ç‰™è¯­æ–¹æ³•ï¼Œå»ºç«‹äº†å¹³å‡WERçº¦ä¸º9.1%çš„æ–°åŸºå‡†ã€‚è¿™äº›ç»“æœåæ˜ äº†æˆ‘ä»¬çš„å®šåˆ¶AVSRç³»ç»Ÿå¦‚ä½•èƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½æ¨¡å‹å¤æ‚åº¦çš„åŒæ—¶ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„è¯†åˆ«ç‡ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/david-gimeno/tailored-avsr%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/david-gimeno/tailored-avsrä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.06606v4">PDF</a> Accepted in Computer Speech &amp; Language journal of Elsevier</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸéŸ³è§†è¯­éŸ³è¯†åˆ«çš„è¿›æ­¥æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„çªç ´æ€§è¿›å±•ï¼Œå°¤å…¶åœ¨æ¶åŠ£å™ªå£°ç¯å¢ƒä¸‹æå‡äº†ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡è®¾è®¡åŒ…å«ä¸¤ä¸ªç‹¬ç«‹ç¼–ç å™¨çš„æ¨¡å‹æ¥è§£å†³æ­¤é—®é¢˜ï¼Œæ¯ä¸ªç¼–ç å™¨ä¸“æ³¨äºä¸€ç§æ¨¡æ€ã€‚å°½ç®¡è¿‘æœŸå·¥ä½œå¼€å§‹æ¢ç´¢ç»Ÿä¸€çš„è§†å¬ç¼–ç å™¨ï¼Œä½†ç¡®å®šæœ€ä½³çš„è·¨æ¨¡æ€æ¶æ„ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸ä¾èµ–äºåºå¤§çš„å‚æ•°å’Œæ˜‚è´µçš„è®¡ç®—æˆæœ¬è®­ç»ƒè¿‡ç¨‹ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œé€šè¿‡å¼•å…¥ä¸€ç§æ–°é¢–çš„è§†å¬æ¡†æ¶ï¼Œç»“åˆçµæ´»æ€§å’Œå¯è§£é‡Šæ€§çš„ç¼–ç å™¨æ¶æ„ï¼ˆå¦‚Branchformerï¼‰ï¼Œè®¾è®¡å‚æ•°é«˜æ•ˆçš„AVSRç³»ç»Ÿã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®æ¡ä»¶å’Œåœºæ™¯çš„è‹±è¯­å’Œè¥¿ç­ç‰™è¯­AVSRåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚å³ä½¿åœ¨ä¸­ç­‰è§„æ¨¡æ•°æ®è®­ç»ƒä¸‹ï¼Œæ¨¡å‹çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¹Ÿè¾¾åˆ°çº¦2.5%çš„è‹±è¯­ç«äº‰æ°´å¹³ï¼Œå¹¶è¶…è¶Šç°æœ‰æ–¹æ³•å¯¹è¥¿ç­ç‰™è¯­çš„è¯†åˆ«ï¼Œå¹³å‡WERçº¦ä¸º9.1%ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚è¯¥ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å®šåˆ¶AVSRç³»ç»Ÿèƒ½å¤Ÿåœ¨æ˜¾è‘—é™ä½æ¨¡å‹å¤æ‚æ€§çš„åŒæ—¶è¾¾åˆ°æœ€å…ˆè¿›çš„è¯†åˆ«ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°ç¯å¢ƒä¸‹ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–åŒ…å«ç‹¬ç«‹ç¼–ç å™¨çš„æ¨¡å‹ï¼Œæ¯ä¸ªç¼–ç å™¨é’ˆå¯¹ä¸€ç§æ¨¡æ€ã€‚</li>
<li>ç¡®å®šæœ€ä½³çš„è·¨æ¨¡æ€æ¶æ„æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸”ç°æœ‰æ–¹æ³•æ¨¡å‹å‚æ•°åºå¤§ã€è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„è§†å¬æ¡†æ¶ï¼Œç»“åˆç¼–ç å™¨æ¶æ„çš„çµæ´»æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡ä¸¤æ­¥è®¾è®¡å®šåˆ¶AVSRç³»ç»Ÿï¼šé¦–å…ˆä¼°è®¡éŸ³é¢‘å’Œè§†é¢‘ç³»ç»Ÿï¼Œç„¶ååŸºäºæ¨¡æ€ç‰¹å®šæ¨¡å‹çš„å±‚çº§åˆ†æ”¯åˆ†æ•°è®¾è®¡ç»Ÿä¸€çš„è§†å¬ç¼–ç å™¨ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„AVSRã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.06606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43d287d0a11bedb5b375249136ec29ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c23d75b74e729a3c38b6a0e96290e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed61a68d1970791ba7ce4426da421596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3b41211a61ef015c0b503eb18156abe.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-6c30b220f82e9fd04a8b2200abd58ad6.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Not All Parameters Matter Masking Diffusion Models for Enhancing   Generation Ability
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8bd4b02c25a30c3cc5a38c94f547b27c.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Lesion-Aware Generative Artificial Intelligence for Virtual   Contrast-Enhanced Mammography in Breast Cancer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
