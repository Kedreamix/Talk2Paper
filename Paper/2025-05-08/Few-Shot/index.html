<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Towards Smart Point-and-Shoot Photography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-de7f6c38b37cbb82fa68cf62304b2b86.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    60 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-08-æ›´æ–°"><a href="#2025-05-08-æ›´æ–°" class="headerlink" title="2025-05-08 æ›´æ–°"></a>2025-05-08 æ›´æ–°</h1><h2 id="Towards-Smart-Point-and-Shoot-Photography"><a href="#Towards-Smart-Point-and-Shoot-Photography" class="headerlink" title="Towards Smart Point-and-Shoot Photography"></a>Towards Smart Point-and-Shoot Photography</h2><p><strong>Authors:Jiawan Li, Fei Zhou, Zhipeng Zhong, Jiongzhi Lin, Guoping Qiu</strong></p>
<p>Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing 320K images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets. </p>
<blockquote>
<p>æ•°ç™¾ä¸‡äººç»å¸¸ä½¿ç”¨æ™ºèƒ½æ‰‹æœºä½œä¸ºå³æ‹å³æ‘„ï¼ˆPASï¼‰ç›¸æœºæ‹ç…§ï¼Œç„¶è€Œå¾ˆå°‘æœ‰äººå…·å¤‡æ‹æ‘„å¥½åœºæ™¯ç…§ç‰‡çš„æ‘„å½±æŠ€å·§ã€‚è™½ç„¶ä¼ ç»Ÿçš„PASç›¸æœºå†…ç½®åŠŸèƒ½å¯ä»¥ç¡®ä¿ç…§ç‰‡å¯¹ç„¦æ¸…æ™°ã€äº®åº¦åˆé€‚ï¼Œä½†å®ƒä»¬æ— æ³•å‘Šè¯‰ç”¨æˆ·å¦‚ä½•æ‹æ‘„åœºæ™¯çš„æœ€ä½³é•œå¤´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§æ™ºèƒ½å³æ‹å³æ‘„ï¼ˆSPASï¼‰ç³»ç»Ÿï¼Œä»¥å¸®åŠ©ç”¨æˆ·æ‹æ‘„ä¼˜è´¨ç…§ç‰‡ã€‚æˆ‘ä»¬çš„SPASç³»ç»Ÿé€šè¿‡è‡ªåŠ¨æŒ‡å¯¼ç”¨æˆ·å®æ—¶è°ƒæ•´ç›¸æœºå§¿åŠ¿æ¥å¸®åŠ©ç”¨æˆ·æ‹æ‘„å¥½åœºæ™¯çš„ç…§ç‰‡ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª4000ä¸ªåœºæ™¯çš„320Kå¼ å¸¦æœ‰ç›¸æœºå§¿åŠ¿ä¿¡æ¯çš„å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºCLIPçš„åˆ›æ–°æ€§æ„å›¾è´¨é‡è¯„ä¼°ï¼ˆCCQAï¼‰æ¨¡å‹ï¼Œä¸ºè¿™äº›å›¾åƒåˆ†é…ä¼ªæ ‡ç­¾ã€‚CCQAå¼•å…¥äº†ä¸€ç§ç‹¬ç‰¹çš„å¯å­¦ä¹ æ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œå­¦ä¹ è¿ç»­çš„å•è¯åµŒå…¥ï¼Œèƒ½å¤Ÿè¾¨åˆ«äº”ä¸ªè´¨é‡æè¿°è¯æ‰€æ¶µç›–èŒƒå›´å†…çš„ç»†å¾®è§†è§‰è´¨é‡å·®å¼‚{å·®ã€è¾ƒå·®ã€ä¸€èˆ¬ã€å¥½ã€å®Œç¾}ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç›¸æœºå§¿åŠ¿è°ƒæ•´æ¨¡å‹ï¼ˆCPAMï¼‰ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç¡®å®šå½“å‰è§†å›¾æ˜¯å¦å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›ï¼Œå¦‚æœå¯ä»¥ï¼Œåˆ™è¾“å‡ºä»¥ä¸¤ä¸ªç›¸æœºå§¿åŠ¿è°ƒæ•´è§’åº¦å½¢å¼çš„è°ƒæ•´å»ºè®®ã€‚CPAMçš„ä¸¤ä¸ªä»»åŠ¡æ˜¯æŒ‰é¡ºåºè¿›è¡Œçš„ï¼Œæ¯ä¸ªä»»åŠ¡æ¶‰åŠä¸åŒçš„è®­ç»ƒæ ·æœ¬é›†ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¸¦æœ‰é—¨æ§æŸå¤±å‡½æ•°çš„æ··åˆä¸“å®¶æ¨¡å‹ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒCPAMã€‚æˆ‘ä»¬å°†é€šè¿‡ä½¿ç”¨å…¬å¼€çš„å›¾åƒæ„å›¾æ•°æ®é›†æ¥å…¨é¢å±•ç¤ºSPASç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03638v1">PDF</a> CVPR2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ™ºèƒ½ç‚¹æ‹ç³»ç»Ÿï¼ˆSPASï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ‹æ‘„é«˜è´¨é‡çš„ç…§ç‰‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªåŠ¨å¼•å¯¼ç”¨æˆ·è°ƒæ•´ç›¸æœºå§¿æ€æ¥å®ç°ã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªå¤§å‹å›¾åƒæ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨CLIPæŠ€æœ¯çš„ç»„åˆè´¨é‡è¯„ä¼°æ¨¡å‹å¯¹å›¾åƒè¿›è¡Œä¼ªæ ‡ç­¾æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å¼€å‘äº†ä¸€ç§ç›¸æœºå§¿æ€è°ƒæ•´æ¨¡å‹ï¼ˆCPAMï¼‰ï¼Œå¯åˆ¤æ–­å½“å‰è§†è§’æ˜¯å¦éœ€è¦æ”¹è¿›ï¼Œå¹¶ç»™å‡ºè°ƒæ•´å»ºè®®ã€‚é€šè¿‡å…¬å¼€å¯ç”¨çš„å›¾åƒç»„åˆæ•°æ®é›†ï¼Œå±•ç¤ºäº†SPASç³»ç»Ÿçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPASç³»ç»Ÿæ—¨åœ¨å¸®åŠ©ç”¨æˆ·æ‹æ‘„é«˜è´¨é‡ç…§ç‰‡ï¼Œé€šè¿‡è‡ªåŠ¨å¼•å¯¼è°ƒæ•´ç›¸æœºå§¿æ€å®ç°ã€‚</li>
<li>ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŒ…å«320Kå¼ å›¾ç‰‡çš„å¤§å‹æ•°æ®é›†ï¼Œæ¯å¼ å›¾ç‰‡éƒ½æœ‰ç›¸æœºå§¿æ€ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨äº†åŸºäºCLIPçš„ç»„åˆè´¨é‡è¯„ä¼°æ¨¡å‹ï¼ˆCCQAï¼‰ï¼Œä¸ºå›¾åƒèµ‹äºˆä¼ªæ ‡ç­¾ï¼Œèƒ½è¾¨åˆ«ç»†å¾®çš„è§†è§‰è´¨é‡å·®å¼‚ã€‚</li>
<li>CCQAæ¨¡å‹é‡‡ç”¨è¿ç»­è¯åµŒå…¥æŠ€æœ¯ï¼Œè¦†ç›–â€œåã€å·®ã€ä¸€èˆ¬ã€å¥½ã€å®Œç¾â€äº”ä¸ªè´¨é‡æè¿°ç­‰çº§ã€‚</li>
<li>å¼€å‘äº†ç›¸æœºå§¿æ€è°ƒæ•´æ¨¡å‹ï¼ˆCPAMï¼‰ï¼Œèƒ½åˆ¤æ–­å½“å‰è§†è§’æ˜¯å¦éœ€è¦æ”¹è¿›ï¼Œå¹¶æä¾›è°ƒæ•´å»ºè®®ã€‚</li>
<li>CPAMé‡‡ç”¨æ··åˆä¸“å®¶æ¨¡å‹å’Œé—¨æ§æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒï¼Œå®ç°ç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-34d9408f5f199f1da10b2e981a5ae3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cc0afd3e5d044a877ef03bebf284c56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-736a2bf242408748109a7c72496b415b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c12935a49cf52cd6839e21a5ec63de5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0515d69ec1512fdde307fdb5fb712e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757377b1a89558ed3b43ac258df7110d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Unreasonable-Effectiveness-of-Discrete-Time-Gaussian-Process-Mixtures-for-Robot-Policy-Learning"><a href="#The-Unreasonable-Effectiveness-of-Discrete-Time-Gaussian-Process-Mixtures-for-Robot-Policy-Learning" class="headerlink" title="The Unreasonable Effectiveness of Discrete-Time Gaussian Process   Mixtures for Robot Policy Learning"></a>The Unreasonable Effectiveness of Discrete-Time Gaussian Process   Mixtures for Robot Policy Learning</h2><p><strong>Authors:Jan Ole von Hartz, Adrian RÃ¶fer, Joschka Boedecker, Abhinav Valada</strong></p>
<p>We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel approach for flexible policy representation and imitation learning in robot manipulation. MiDiGap enables learning from as few as five demonstrations using only camera observations and generalizes across a wide range of challenging tasks. It excels at long-horizon behaviors such as making coffee, highly constrained motions such as opening doors, dynamic actions such as scooping with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns these tasks on a CPU in less than a minute and scales linearly to large datasets. We also develop a rich suite of tools for inference-time steering using evidence such as collision signals and robot kinematic constraints. This steering enables novel generalization capabilities, including obstacle avoidance and cross-embodiment policy transfer. MiDiGap achieves state-of-the-art performance on diverse few-shot manipulation benchmarks. On constrained RLBench tasks, it improves policy success by 76 percentage points and reduces trajectory cost by 67%. On multimodal tasks, it improves policy success by 48 percentage points and increases sample efficiency by a factor of 20. In cross-embodiment transfer, it more than doubles policy success. We make the code publicly available at <a target="_blank" rel="noopener" href="https://midigap.cs.uni-freiburg.de/">https://midigap.cs.uni-freiburg.de</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ç¦»æ•£æ—¶é—´é«˜æ–¯è¿‡ç¨‹æ··åˆï¼ˆMiDiGapï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæœºå™¨äººæ“æ§ä¸­çš„çµæ´»ç­–ç•¥è¡¨ç¤ºå’Œæ¨¡ä»¿å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚MiDiGapèƒ½å¤Ÿä»ä»…æœ‰çš„äº”ä¸ªæ¼”ç¤ºä¸­å­¦ä¹ ï¼Œä»…ä½¿ç”¨ç›¸æœºè§‚å¯Ÿï¼Œå¹¶èƒ½å¹¿æ³›é€‚åº”å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å®ƒæ“…é•¿é•¿è¿œè¡Œä¸ºï¼Œå¦‚åˆ¶ä½œå’–å•¡ã€é«˜åº¦çº¦æŸçš„è¿åŠ¨ï¼Œå¦‚å¼€é—¨ã€åŠ¨æ€åŠ¨ä½œï¼Œå¦‚ç”¨é“²å­é“²ä¸œè¥¿ä»¥åŠå¤šæ¨¡å¼ä»»åŠ¡ï¼Œå¦‚æŒ‚æ¯å­ã€‚MiDiGapå¯ä»¥åœ¨CPUä¸Šå­¦ä¹ è¿™äº›ä»»åŠ¡ä¸åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´ï¼Œå¹¶ä¸”çº¿æ€§æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€å¥—ä¸°å¯Œçš„å·¥å…·ï¼Œç”¨äºåœ¨æ¨ç†æ—¶ä½¿ç”¨è¯æ®è¿›è¡Œå¼•å¯¼ï¼Œå¦‚ç¢°æ’ä¿¡å·å’Œæœºå™¨äººè¿åŠ¨å­¦çº¦æŸã€‚è¿™ç§å¼•å¯¼ä½¿æ–°çš„æ³›åŒ–èƒ½åŠ›æˆä¸ºå¯èƒ½ï¼ŒåŒ…æ‹¬é¿éšœå’Œè·¨ä½“æ€ç­–ç•¥è½¬ç§»ã€‚MiDiGapåœ¨å¤šæ ·åŒ–çš„å°‘æ ·æœ¬æ“ä½œåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å—çº¦æŸçš„RLBenchä»»åŠ¡ä¸­ï¼Œå®ƒå°†ç­–ç•¥æˆåŠŸç‡æé«˜äº†76ä¸ªç™¾åˆ†ç‚¹ï¼Œè½¨è¿¹æˆæœ¬é™ä½äº†67%ã€‚åœ¨å¤šæ¨¡å¼ä»»åŠ¡ä¸­ï¼Œå®ƒå°†ç­–ç•¥æˆåŠŸç‡æé«˜äº†48ä¸ªç™¾åˆ†ç‚¹ï¼Œæé«˜äº†æ ·æœ¬æ•ˆç‡çš„20å€ã€‚åœ¨è·¨ä½“æ€è½¬ç§»æ–¹é¢ï¼Œå®ƒä½¿ç­–ç•¥æˆåŠŸç‡å¢åŠ äº†ä¸€å€ä»¥ä¸Šã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://midigap.cs.uni-freiburg.de./">https://midigap.cs.uni-freiburg.deã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03296v1">PDF</a> Submitted for publication to IEEE Transaction on Robotics</p>
<p><strong>Summary</strong></p>
<p>MiDiGapæ˜¯ä¸€ç§ç”¨äºæœºå™¨äººæ“ä½œä¸­çš„çµæ´»ç­–ç•¥è¡¨ç¤ºå’Œæ¨¡ä»¿å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚å®ƒèƒ½å¤Ÿä»ä»…æœ‰çš„äº”ä¸ªæ¼”ç¤ºä¸­å­¦ä¹ ï¼Œä»…ä½¿ç”¨æ‘„åƒå¤´è§‚å¯Ÿï¼Œå¹¶å¯ä»¥å¹¿æ³›é€‚ç”¨äºå„ç§æŒ‘æˆ˜ä»»åŠ¡ã€‚MiDiGapæ“…é•¿å¤„ç†é•¿æœŸè¡Œä¸ºã€é«˜åº¦çº¦æŸçš„è¿åŠ¨ã€åŠ¨æ€åŠ¨ä½œå’Œå¤šæ¨¡å¼ä»»åŠ¡ã€‚å…¶åœ¨CPUä¸Šçš„å­¦ä¹ æ—¶é—´çŸ­äºä¸€åˆ†é’Ÿï¼Œå¹¶å¯çº¿æ€§æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ã€‚æ­¤å¤–ï¼ŒMiDiGapè¿˜å¼€å‘äº†ä¸€å¥—ä¸°å¯Œçš„å·¥å…·ï¼Œç”¨äºåŸºäºè¯æ®è¿›è¡Œæ¨ç†æ—¶é—´æ§åˆ¶ï¼Œå¦‚ç¢°æ’ä¿¡å·å’Œæœºå™¨äººè¿åŠ¨å­¦çº¦æŸã€‚è¿™äº›æ§åˆ¶åŠŸèƒ½å¸¦æ¥äº†æ–°é¢–çš„ä¸€èˆ¬åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬éšœç¢é¿å…å’Œè·¨ä½“æ€ç­–ç•¥è½¬ç§»ã€‚MiDiGapåœ¨å¤šæ ·çš„å°‘æ ·æœ¬æ“ä½œåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiDiGapæ˜¯ä¸€ç§ç”¨äºæœºå™¨äººæ“ä½œçš„æ–°å‹ç­–ç•¥è¡¨ç¤ºå’Œæ¨¡ä»¿å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>å®ƒèƒ½å¤Ÿä»ä»…æœ‰çš„äº”ä¸ªæ¼”ç¤ºä¸­å­¦ä¹ ï¼Œä»…ä½¿ç”¨æ‘„åƒå¤´è§‚å¯Ÿã€‚</li>
<li>MiDiGapé€‚ç”¨äºå„ç§æŒ‘æˆ˜ä»»åŠ¡ï¼ŒåŒ…æ‹¬é•¿æœŸè¡Œä¸ºã€é«˜åº¦çº¦æŸçš„è¿åŠ¨ã€åŠ¨æ€åŠ¨ä½œå’Œå¤šæ¨¡å¼ä»»åŠ¡ã€‚</li>
<li>MiDiGapåœ¨CPUä¸Šçš„å­¦ä¹ æ—¶é—´çŸ­ï¼Œå¹¶å¯å¿«é€Ÿæ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ã€‚</li>
<li>MiDiGapæä¾›ä¸°å¯Œçš„å·¥å…·è¿›è¡Œæ¨ç†æ—¶é—´æ§åˆ¶ï¼ŒåŒ…æ‹¬åŸºäºè¯æ®çš„æ§åˆ¶åŠŸèƒ½ã€‚</li>
<li>è¿™äº›æ§åˆ¶åŠŸèƒ½å¸¦æ¥äº†æ–°é¢–çš„ä¸€èˆ¬åŒ–èƒ½åŠ›ï¼Œå¦‚éšœç¢é¿å…å’Œè·¨ä½“æ€ç­–ç•¥è½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc9d8792e83fea75c11380e14b76cc60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fb53b3ee1a077e5e38a78307794328f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cb1821a4d8c8adfe845c6f874d890d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79a5f0302cdfc448c20ab9e60c5ea5af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b5b1ac447c1d6c32007483d233db6f3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Knowledge-Graphs-for-Enhancing-Large-Language-Models-in-Entity-Disambiguation"><a href="#Knowledge-Graphs-for-Enhancing-Large-Language-Models-in-Entity-Disambiguation" class="headerlink" title="Knowledge Graphs for Enhancing Large Language Models in Entity   Disambiguation"></a>Knowledge Graphs for Enhancing Large Language Models in Entity   Disambiguation</h2><p><strong>Authors:Gerard Pons, Besim Bilalli, Anna Queralt</strong></p>
<p>Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entitiesâ€™ classes in a KG to gradually prune the candidate space as well as the entitiesâ€™ descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KGâ€™s semantic expressivity on the ED performance. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å®ƒä»¬æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„é‡è¦è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒä»¬å¯ä»¥ä»¥é›¶æ¬¡æˆ–å¤šæ¬¡å°‘é‡çš„æ–¹å¼è§£å†³é—®é¢˜ï¼Œä»è€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ã€‚ç„¶è€Œï¼ŒLLMé¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è™šæ„å’Œè®­ç»ƒæ•°æ®ä¸­ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†è¿‡æ—¶æˆ–ç¼ºå¤±ä¿¡æ¯ç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜æ— æ³•é€šè¿‡ç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹æ¥è½»æ¾è§£å†³ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªè€—æ—¶ä¸”æ˜‚è´µçš„è¿‡ç¨‹ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼ŒçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å·²è¢«æè®®ä½œä¸ºç»“æ„åŒ–çš„å¤–éƒ¨ä¿¡æ¯æºæ¥ä¸°å¯ŒLLMã€‚åŸºäºè¿™ä¸€ç†å¿µï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ¥è¯†å›¾è°±æ¥å¢å¼ºLLMçš„é›¶æ¬¡å®ä½“æ¶ˆæ­§ï¼ˆEDï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å€ŸåŠ©çŸ¥è¯†å›¾è°±ä¸­å®ä½“ç±»åˆ«çš„å±‚æ¬¡è¡¨ç¤ºæ¥é€æ­¥ç¼©å°å€™é€‰ç©ºé—´ï¼Œå¹¶åˆ©ç”¨å®ä½“çš„æè¿°æ¥ä¸°å¯Œè¾“å…¥æç¤ºä»¥è·å–é¢å¤–çš„äº‹å®çŸ¥è¯†ã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„EDæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºéå¢å¼ºå‹å’Œä»…æè¿°å¢å¼ºçš„LLMï¼Œå¹¶ä¸”æ¯”ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹å…·æœ‰æ›´é«˜çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†é”™è¯¯åˆ†æï¼Œå¹¶è®¨è®ºäº†æ‰€åˆ©ç”¨çš„çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰è¡¨è¾¾åŠ›å¯¹EDæ€§èƒ½çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02737v2">PDF</a> Pre-print submitted to ISWC 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°çªå‡ºã€‚ç„¶è€Œï¼ŒLLMé¢ä¸´çŸ¥è¯†è¿‡æ—¶ã€ç¼ºå¤±æˆ–å¹»è§‰ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä½œä¸ºç»“æ„åŒ–å¤–éƒ¨ä¿¡æ¯æºæ¥ä¸°å¯ŒLLMæˆä¸ºä¸€é¡¹è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶é‡‡ç”¨çŸ¥è¯†å›¾è°±å¢å¼ºLLMè¿›è¡Œé›¶æ ·æœ¬å®ä½“æ¶ˆæ­§ï¼ˆEDï¼‰ï¼Œé€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±ä¸­å®ä½“ç±»åˆ«çš„å±‚æ¬¡è¡¨ç¤ºï¼Œé€æ­¥ç¼©å°å€™é€‰ç©ºé—´ï¼Œå¹¶å€ŸåŠ©å®ä½“æè¿°æ¥ä¸°å¯Œè¾“å…¥æç¤ºä¸­çš„äº‹å®çŸ¥è¯†ã€‚åœ¨æµè¡ŒEDæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºéå¢å¼ºå’Œä»…æè¿°å¢å¼ºçš„LLMï¼Œä¸”å…·æœ‰æ¯”ç‰¹å®šä»»åŠ¡æ¨¡å‹æ›´é«˜çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†è¯¯å·®åˆ†æï¼Œå¹¶è®¨è®ºäº†æ‰€ç”¨çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›å¯¹EDæ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æƒ…å¢ƒä¸‹ã€‚</li>
<li>LLMé¢ä¸´çŸ¥è¯†è¿‡æ—¶ã€ç¼ºå¤±å’Œå¹»è§‰ç­‰é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¯ä½œä¸ºç»“æ„åŒ–å¤–éƒ¨ä¿¡æ¯æºï¼Œç”¨äºå¢å¼ºLLMçš„æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨çŸ¥è¯†å›¾è°±è¿›è¡Œå®ä½“æ¶ˆæ­§ï¼ˆEDï¼‰ï¼Œç»“åˆå®ä½“ç±»åˆ«å±‚æ¬¡è¡¨ç¤ºå’Œå®ä½“æè¿°æ¥ä¸°å¯Œè¾“å…¥æç¤ºã€‚</li>
<li>åœ¨æµè¡ŒEDæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–LLMï¼Œä¸”å…·æœ‰é«˜é€‚åº”æ€§ã€‚</li>
<li>çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰è¡¨è¾¾èƒ½åŠ›å¯¹å®ä½“æ¶ˆæ­§æ€§èƒ½æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e081a5072f6d4ed09e243512cf7c25fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eff3b9d587bbdcf1c4b25116ba9da99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a64b83d8a562a85e9df0a86141e19f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de7f6c38b37cbb82fa68cf62304b2b86.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization"><a href="#Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization" class="headerlink" title="Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization"></a>Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization</h2><p><strong>Authors:Chuck Arvin</strong></p>
<p>As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate &#96;&#96;scaling effectsâ€™â€™ - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä¸æ–­æå‡ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å‘ˆç°äº†ä¸€ç³»åˆ—å®éªŒï¼Œæ—¨åœ¨è¯„ä¼°ç°ä»£LLMï¼ˆå‚æ•°èŒƒå›´ä»3Båˆ°90B+ï¼‰åœ¨CaseHOLDè¿™ä¸€æ³•å¾‹åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯¥æ•°æ®é›†ç”¨äºè¯†åˆ«åˆ¤ä¾‹æ³•æ‘˜è¦ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†â€œè§„æ¨¡æ•ˆåº”â€â€”â€”åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼šéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§è€Œæé«˜ï¼Œæ›´å¼ºå¤§çš„æ¨¡å‹å¦‚GPT4oå’ŒAmazonNovaProåˆ†åˆ«å–å¾—äº†å®è§‚F1åˆ†æ•°ä¸º0.744å’Œ0.720ã€‚è¿™äº›åˆ†æ•°ä¸è¯¥æ•°æ®é›†ä¸Šå·²å‘å¸ƒçš„æœ€ä½³ç»“æœå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•æŠ€æœ¯å¤æ‚æ¨¡å‹çš„è®­ç»ƒã€å¾®è°ƒæˆ–å°‘æ ·æœ¬æç¤ºã€‚ä¸ºç¡®ä¿è¿™äº›å¼ºæœ‰åŠ›çš„ç»“æœå¹¶éç”±äºè®­ç»ƒæ•°æ®ä¸­å¸æ³•æ„è§çš„åˆ»æ¿è®°å¿†æ‰€è‡´ï¼Œæˆ‘ä»¬å¼€å‘å¹¶é‡‡ç”¨äº†æ–°å‹çš„å¼•ç”¨åŒ¿åæµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä¿ç•™äº†è¯­ä¹‰å«ä¹‰ï¼ŒåŒæ—¶ç¡®ä¿äº†æ¡ˆä¾‹åç§°å’Œå¼•ç”¨çš„è™šæ„æ€§ã€‚åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹ä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼ˆå®è§‚F1ä¸º0.728ï¼‰ï¼Œè¿™è¡¨æ˜å…¶è¡¨ç°å¹¶éç”±äºæ­»è®°ç¡¬èƒŒã€‚è¿™äº›å‘ç°å±•ç¤ºäº†LLMåœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ½œåŠ›ä»¥åŠå½“å‰å­˜åœ¨çš„å±€é™æ€§ï¼Œå¯¹äºè‡ªåŠ¨åŒ–æ³•å¾‹åˆ†æå’Œæ³•å¾‹åŸºå‡†çš„å¼€å‘ä¸è¡¡é‡å…·æœ‰é‡è¦çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02172v1">PDF</a> Presented as a short paper at International Conference on Artificial   Intelligence and Law 2025 (Chicago, IL)</p>
<p><strong>Summary</strong></p>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œè¯„ä¼°äº†ä¸åŒè§„æ¨¡LLMåœ¨CaseHOLDæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹æ€§èƒ½éšè§„æ¨¡æå‡è€Œæ”¹å–„ï¼ŒGPT4oå’ŒAmazonNovaProç­‰å¤§å‹æ¨¡å‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœå…·æœ‰ç«äº‰åŠ›ï¼Œä¸”æ— éœ€å¤æ‚æ¨¡å‹è®­ç»ƒã€å¾®è°ƒæˆ–å°‘æ ·æœ¬æç¤ºã€‚é€šè¿‡æ–°å‹å¼•ç”¨åŒ¿ååŒ–æµ‹è¯•ï¼Œç¡®ä¿æ¨¡å‹æ€§èƒ½å¹¶éä»…å› è®°å¿†è®­ç»ƒæ•°æ®ä¸­çš„å¸æ³•æ„è§è€Œè¾¾æˆã€‚è¿™ä¸€ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–æ³•å¾‹åˆ†æå’Œæ³•å¾‹åŸºå‡†çš„å¼€å‘å’Œè¯„ä¼°æä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ³•å¾‹ä»»åŠ¡æ€§èƒ½è¯„ä¼°ä¸­è¡¨ç°å‡ºè§„æ¨¡æ•ˆç›Šï¼Œæ€§èƒ½éšæ¨¡å‹è§„æ¨¡æå‡è€Œæ”¹å–„ã€‚</li>
<li>GPT4oå’ŒAmazonNovaProç­‰æ¨¡å‹åœ¨CaseHOLDæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œå®è§‚F1åˆ†æ•°é«˜ä¸”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æœ€ä½³æ¨¡å‹æ€§èƒ½ä¸éœ€è¦å¤æ‚çš„æ¨¡å‹è®­ç»ƒã€å¾®è°ƒæˆ–å°‘æ ·æœ¬æç¤ºã€‚</li>
<li>é€šè¿‡å¼•ç”¨åŒ¿ååŒ–æµ‹è¯•éªŒè¯äº†æ¨¡å‹æ€§èƒ½å¹¶éä»…åŸºäºè®­ç»ƒæ•°æ®çš„è®°å¿†ã€‚</li>
<li>æ¨¡å‹åœ¨æ³•å¾‹ä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾ç¤ºè‡ªåŠ¨åŒ–æ³•å¾‹åˆ†æçš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹å¼€å‘å’Œåº”ç”¨æ³•å¾‹åŸºå‡†çš„æœªæ¥å‘å±•æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7b9b0d76b2e2812d643cddfb5378b1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22dee1fee3bb963cc63fbd728b2ba911.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-974f29b3d5ba445aeffde7558d8e6144.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9adc73211c22c0b0ecb6cc0984cd1fe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b51ceda711f76de4442f49ea93c0e28.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Retrieval-augmented-in-context-learning-for-multimodal-large-language-models-in-disease-classification"><a href="#Retrieval-augmented-in-context-learning-for-multimodal-large-language-models-in-disease-classification" class="headerlink" title="Retrieval-augmented in-context learning for multimodal large language   models in disease classification"></a>Retrieval-augmented in-context learning for multimodal large language   models in disease classification</h2><p><strong>Authors:Zaifu Zhan, Shuang Zhou, Xiaoshan Zhou, Yongkang Xiao, Jun Wang, Jiawen Deng, He Zhu, Yu Hou, Rui Zhang</strong></p>
<p>Objectives: We aim to dynamically retrieve informative demonstrations, enhancing in-context learning in multimodal large language models (MLLMs) for disease classification.   Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL) framework, which integrates retrieval-augmented generation (RAG) and in-context learning (ICL) to adaptively select demonstrations with similar disease patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines embeddings from diverse encoders, including ResNet, BERT, BioBERT, and ClinicalBERT, to retrieve appropriate demonstrations, and constructs conversational prompts optimized for ICL. We evaluated the framework on two real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies, similarity metrics, and varying numbers of demonstrations.   Results: RAICL consistently improved classification performance. Accuracy increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs being stronger than images alone. The richness of information embedded in each modality will determine which embedding model can be used to get better results. Few-shot experiments showed that increasing the number of retrieved examples further enhanced performance. Across different similarity metrics, Euclidean distance achieved the highest accuracy while cosine similarity yielded better macro-F1 scores. RAICL demonstrated consistent improvements across various MLLMs, confirming its robustness and versatility.   Conclusions: RAICL provides an efficient and scalable approach to enhance in-context learning in MLLMs for multimodal disease classification. </p>
<blockquote>
<p>ç›®æ ‡ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŠ¨æ€æ£€ç´¢ä¿¡æ¯ä¸°å¯Œçš„æ¼”ç¤ºå†…å®¹ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œç”¨äºç–¾ç—…åˆ†ç±»ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºRAICLï¼ˆæ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰çš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œä»¥è‡ªé€‚åº”åœ°é€‰æ‹©å…·æœ‰ç›¸ä¼¼ç–¾ç—…æ¨¡å¼çš„æ¼”ç¤ºå†…å®¹ï¼Œä½¿MLLMsä¸­çš„ICLæ›´åŠ æœ‰æ•ˆã€‚å…·ä½“æ¥è¯´ï¼ŒRAICLä¼šæ£€æŸ¥æ¥è‡ªä¸åŒç¼–ç å™¨çš„åµŒå…¥ï¼ŒåŒ…æ‹¬ResNetã€BERTã€BioBERTå’ŒClinicalBERTï¼Œä»¥æ£€ç´¢é€‚å½“çš„æ¼”ç¤ºå†…å®¹ï¼Œå¹¶æ„å»ºé’ˆå¯¹ICLä¼˜åŒ–çš„å¯¹è¯æç¤ºã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆTCGAå’ŒIU Chest Xå°„çº¿ï¼‰ä¸Šè¯„ä¼°äº†æ¡†æ¶çš„æ€§èƒ½ï¼Œè¯„ä¼°å…¶åœ¨å¤šä¸ªMLLMsï¼ˆQwenã€Llavaã€Gemmaï¼‰ã€åµŒå…¥ç­–ç•¥ã€ç›¸ä¼¼åº¦æŒ‡æ ‡å’Œä¸åŒæ•°é‡çš„æ¼”ç¤ºå†…å®¹æ–¹é¢çš„è¡¨ç°ã€‚ç»“æœï¼šRAICLæŒç»­æé«˜äº†åˆ†ç±»æ€§èƒ½ã€‚TCGAä¸Šçš„å‡†ç¡®ç‡ä»0.7854æé«˜åˆ°0.8368ï¼ŒIU Chest Xå°„çº¿ä¸Šçš„å‡†ç¡®ç‡ä»0.7924æé«˜åˆ°0.8658ã€‚å¤šæ¨¡æ€è¾“å…¥ä¼˜äºå•æ¨¡æ€è¾“å…¥ï¼Œæ–‡æœ¬è¾“å…¥çš„æ•ˆèƒ½å¼ºäºå›¾åƒã€‚æ¯ç§æ¨¡æ€ä¸­åµŒå…¥çš„ä¿¡æ¯ä¸°å¯Œç¨‹åº¦å°†å†³å®šä½¿ç”¨å“ªç§åµŒå…¥æ¨¡å‹å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚å°‘é‡æ ·æœ¬çš„å®éªŒè¡¨æ˜ï¼Œå¢åŠ æ£€ç´¢åˆ°çš„ç¤ºä¾‹æ•°é‡å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚åœ¨ä¸åŒçš„ç›¸ä¼¼åº¦æŒ‡æ ‡ä¸­ï¼Œæ¬§å‡ é‡Œå¾—è·ç¦»å®ç°äº†æœ€é«˜çš„å‡†ç¡®ç‡ï¼Œè€Œä½™å¼¦ç›¸ä¼¼åº¦è·å¾—äº†æ›´å¥½çš„å®è§‚F1åˆ†æ•°ã€‚RAICLåœ¨å„ç§MLLMsä¸­è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æ”¹è¿›ï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ç»“è®ºï¼šRAICLä¸ºå¤šæ¨¡æ€ç–¾ç—…åˆ†ç±»åœ¨MLLMsä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02087v1">PDF</a> 17 Pages, 1 figure, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é€šè¿‡åŠ¨æ€æ£€ç´¢æœ‰ä¿¡æ¯é‡çš„æ¼”ç¤ºç¤ºä¾‹ï¼Œæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç–¾ç—…åˆ†ç±»æ–¹é¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æå‡ºäº†ä¸€ç§åä¸ºRAICLçš„æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œå¯è‡ªé€‚åº”åœ°é€‰æ‹©å…·æœ‰ç›¸ä¼¼ç–¾ç—…æ¨¡å¼çš„æ¼”ç¤ºç¤ºä¾‹ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€æ•°æ®é›†TCGAå’ŒIU Chest X-rayä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAICLæé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€åµŒå…¥ç­–ç•¥ã€ç›¸ä¼¼åº¦åº¦é‡ä»¥åŠä¸åŒæ•°é‡çš„æ¼”ç¤ºç¤ºä¾‹ä¸­éƒ½è¡¨ç°å‡ºäº†ä¼˜è¶Šæ€§ã€‚è¿™è¡¨æ˜RAICLæ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç–¾ç—…åˆ†ç±»æ–¹é¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆRAICLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç–¾ç—…åˆ†ç±»æ–¹é¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©æ¼”ç¤ºç¤ºä¾‹æ¥å¢å¼ºå­¦ä¹ ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„å¤šæ¨¡æ€æ•°æ®é›†TCGAå’ŒIU Chest X-rayä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAICLèƒ½æé«˜ç–¾ç—…åˆ†ç±»çš„æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€è¾“å…¥ä¼˜äºå•æ¨¡æ€è¾“å…¥ï¼Œæ–‡æœ¬è¾“å…¥æ¯”å›¾åƒè¾“å…¥æ›´æœ‰æ•ˆã€‚</li>
<li>ä¸åŒåµŒå…¥æ¨¡å‹çš„æ•ˆæœå–å†³äºå…¶æ‰€åµŒå…¥ä¿¡æ¯çš„ä¸°å¯Œç¨‹åº¦ã€‚</li>
<li>å¢åŠ æ£€ç´¢åˆ°çš„ç¤ºä¾‹æ•°é‡èƒ½è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e0224b6b81335fcfbc47b32893dd613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3efc261caf0037b03082634723a8dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9ad7b09574640219beea0bc2b57ed11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee88f5fe94813d5385ad2043fcc8afe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction"><a href="#ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction" class="headerlink" title="ReLI: A Language-Agnostic Approach to Human-Robot Interaction"></a>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</h2><p><strong>Authors:Linus Nwankwo, Bjoern Ellensohn, Ozan Ã–zdenizci, Elmar Rueckert</strong></p>
<p>Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instructionâ€™s linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLIâ€™s robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLIâ€™s potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at <a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/">https://linusnep.github.io/ReLI/</a>. </p>
<blockquote>
<p>ç›®å‰ï¼Œå°†è‡ªä¸»æ™ºèƒ½ä½“é€‚åº”äºå·¥ä¸šã€å®¶å±…å’Œå…¶ä»–æ—¥å¸¸ä»»åŠ¡æ­£åœ¨è·å¾—åŠ¨åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å…¨çƒæˆ–è·¨è¯­è¨€çš„åº”ç”¨åœºæ™¯ä¸­ï¼Œç¡®ä¿ä¸ç¯å¢ƒçš„æœ‰æ•ˆäº’åŠ¨å¹¶æ‰§è¡Œå¤šæ ·è¯­è¨€çš„éé™åˆ¶æ€§äººç±»ä»»åŠ¡æŒ‡å®šæŒ‡ä»¤ä»æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReLIï¼Œè¿™æ˜¯ä¸€ä¸ªä¸è¯­è¨€æ— å…³çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿è‡ªä¸»æ™ºèƒ½ä½“èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶å¯¹è¯ã€å¯¹å‘¨å›´ç¯å¢ƒè¿›è¡Œè¯­ä¹‰æ¨ç†å¹¶æ‰§è¡Œä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œæ— è®ºä»»åŠ¡æŒ‡ä»¤çš„è¯­è¨€èµ·æºå¦‚ä½•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œå°†å…¶è½¬åŒ–ä¸ºè¯­è¨€åˆ°è¡ŒåŠ¨æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥é€šè¿‡è‡ªç„¶ã€æµç•…çš„äººæœºå¯¹è¯äº’åŠ¨ï¼Œç›´æ¥æä¾›å¸¸è¯†æ¨ç†å’Œé«˜çº§æœºå™¨äººæ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è·¨è¯­è¨€æ¨¡å‹æ¥åœ°ï¼Œä»¥ç¡®ä¿ReLIåœ¨å…¨çƒèŒƒå›´å†…æ¨å¹¿å„ç§è¯­è¨€ã€‚ä¸ºäº†è¯æ˜ReLIçš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬åœ¨å„ç§çŸ­æœŸå’Œé•¿æœŸä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„æ¨¡æ‹Ÿå’ŒçœŸå®å®éªŒï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„ç©ºé—´å¯¼èˆªã€åœºæ™¯ä¿¡æ¯æ£€ç´¢å’ŒæŸ¥è¯¢å¯¼å‘ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠè¶…è¿‡7ä¸‡å¤šæ¬¡å¯¹è¯çš„140ç§è¯­è¨€ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å¹³å‡è€Œè¨€ï¼ŒReLIåœ¨è·¨è¯­è¨€æŒ‡ä»¤è§£æå’Œä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡æ–¹é¢è¾¾åˆ°äº†è¶…è¿‡90%Â±0.2çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¯æ˜äº†ReLIåœ¨å®é™…ä¸–ç•Œä¸­å¢å¼ºè‡ªç„¶äººæœºäº¤äº’çš„æ½œåŠ›ï¼ŒåŒæ—¶æ”¯æŒè¯­è¨€å¤šæ ·æ€§ã€‚æ¼”ç¤ºå’Œèµ„æºå°†åœ¨<a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://linusnep.github.io/ReLI/ä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºReLIçš„è¯­è¨€é€šç”¨æ¡†æ¶ï¼Œå®ƒèƒ½å®ç°è‡ªä¸»ä»£ç†åœ¨è‡ªç„¶ç¯å¢ƒä¸‹è¿›è¡Œäº¤æµã€ç†è§£ç¯å¢ƒå¹¶è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡æ“ä½œï¼Œè€Œæ— éœ€å—åˆ°ä»»åŠ¡æŒ‡ä»¤è¯­è¨€æ¥æºçš„é™åˆ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤§å‹é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å»ºç«‹è¯­è¨€åˆ°è¡ŒåŠ¨æ¨¡å‹ï¼Œæ”¯æŒè‡ªç”±æµå¼çš„äººæœºå¯¹è¯äº’åŠ¨å’Œæœºå™¨äººæ§åˆ¶ã€‚ç»è¿‡è·¨è¯­è¨€è®­ç»ƒï¼ŒReLIå¯æ¨å¹¿åˆ°å…¨çƒå¤šç§è¯­è¨€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReLIåœ¨å¤šç§çŸ­æœŸå’Œé•¿æœŸä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼ŒæˆåŠŸæ‰§è¡Œè·¨è¯­è¨€æŒ‡ä»¤çš„å‡†ç¡®ç‡è¾¾åˆ°90%ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReLIæ˜¯ä¸€ä¸ªè¯­è¨€é€šç”¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿è‡ªä¸»ä»£ç†èƒ½å¤Ÿåœ¨å„ç§ç¯å¢ƒä¸‹è‡ªç„¶åœ°äº¤æµã€ç†è§£ç¯å¢ƒå¹¶æ‰§è¡Œä»»åŠ¡ï¼Œä¸å—ä»»åŠ¡æŒ‡ä»¤è¯­è¨€æ¥æºçš„é™åˆ¶ã€‚</li>
<li>ReLIé€šè¿‡å»ºç«‹è¯­è¨€åˆ°è¡ŒåŠ¨æ¨¡å‹ï¼Œæ”¯æŒè‡ªç”±æµå¼çš„äººæœºå¯¹è¯äº’åŠ¨å’Œæœºå™¨äººæ§åˆ¶ã€‚</li>
<li>ReLIé€šè¿‡å¤§å‹é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹å®ç°ï¼Œå…·å¤‡é€šç”¨æ€§å’Œå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>ReLIç»è¿‡è·¨è¯­è¨€è®­ç»ƒï¼Œå¯ä»¥æ¨å¹¿åˆ°å…¨çƒå¤šç§è¯­è¨€ï¼Œå¢å¼ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®é€‚æ€§ã€‚</li>
<li>ReLIåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶åŸºç¡€å’Œå°‘é‡æ ·æœ¬çš„ç©ºé—´å¯¼èˆªã€åœºæ™¯ä¿¡æ¯æ£€ç´¢å’ŒæŸ¥è¯¢å¯¼å‘ä»»åŠ¡ä¸­ã€‚</li>
<li>ReLIåœ¨è·¨è¯­è¨€æŒ‡ä»¤è§£æå’Œä»»åŠ¡æ‰§è¡Œæ–¹é¢çš„å‡†ç¡®ç‡è¶…è¿‡90%ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d10faf471c1e3b33652793c06cfbbb34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45284bfa77d4deac8023ed8fd8a1e0a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729cc5c73428c130c3f9f924b3a5982d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d54418794f8851404b96e7859613097c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes"><a href="#Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes" class="headerlink" title="Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes"></a>Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes</h2><p><strong>Authors:Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves</strong></p>
<p>Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the modelâ€™s ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations. </p>
<blockquote>
<p>äº¤äº’å¼3Dåˆ†å‰²æŠ€æœ¯é€šè¿‡ç»“åˆç”¨æˆ·æä¾›çš„ç‚¹å‡»ï¼Œåœ¨å¤æ‚çš„3Dåœºæ™¯ä¸­ç”Ÿæˆç²¾ç¡®çš„å¯¹è±¡æ©æ¨¡æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ä¸¤å¤§æŒ‘æˆ˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼šï¼ˆ1ï¼‰å¦‚ä½•ä»ç¨€ç–çš„ç”¨æˆ·ç‚¹å‡»ä¸­æœ‰æ•ˆæ³›åŒ–ä»¥äº§ç”Ÿç²¾ç¡®çš„åˆ†å‰²ï¼›ï¼ˆ2ï¼‰é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œä»¥å¸®åŠ©ç”¨æˆ·è¯†åˆ«ä¸å¯é çš„åŒºåŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NPISeg3Dï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¦‚ç‡æ¡†æ¶ï¼Œå®ƒåŸºäºç¥ç»è¿‡ç¨‹ï¼ˆNPsï¼‰æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼ŒNPISeg3Då¼•å…¥äº†ä¸€ç§åˆ†å±‚æ½œåœ¨å˜é‡ç»“æ„ï¼ŒåŒ…æ‹¬åœºæ™¯ç‰¹å®šå’Œå¯¹è±¡ç‰¹å®šçš„æ½œåœ¨å˜é‡ï¼Œé€šè¿‡æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œå¯¹è±¡ç‰¹å®šç‰¹å¾ï¼Œå¢å¼ºå°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¦‚ç‡åŸå‹è°ƒåˆ¶å™¨ï¼Œå®ƒè‡ªé€‚åº”åœ°åˆ©ç”¨å¯¹è±¡ç‰¹å®šçš„æ½œåœ¨å˜é‡è°ƒåˆ¶ç‚¹å‡»åŸå‹ï¼Œæé«˜äº†æ¨¡å‹æ•æ‰å¯¹è±¡æ„ŸçŸ¥ä¸Šä¸‹æ–‡å’Œé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åœ¨å››ä¸ª3Dç‚¹äº‘æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNPISeg3Dåœ¨ç‚¹å‡»æ¬¡æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹å®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01726v1">PDF</a> ICML 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·ç‚¹å‡»çš„äº¤äº’å¼ä¸‰ç»´åˆ†å‰²æŠ€æœ¯ä¸ºå¤æ‚ä¸‰ç»´åœºæ™¯ä¸­çš„ç²¾ç¡®å¯¹è±¡æ©æ¨¡ç”Ÿæˆæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¸¤ä¸ªå…³é”®é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼šï¼ˆ1ï¼‰å¦‚ä½•ä»ç¨€ç–çš„ç”¨æˆ·ç‚¹å‡»ä¸­æœ‰æ•ˆæ³›åŒ–ä»¥äº§ç”Ÿç²¾ç¡®çš„åˆ†å‰²ï¼›ï¼ˆ2ï¼‰é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ä»¥å¸®åŠ©ç”¨æˆ·è¯†åˆ«ä¸å¯é åŒºåŸŸã€‚æœ¬ç ”ç©¶æå‡ºäº†NPISeg3Dï¼Œä¸€ä¸ªåŸºäºç¥ç»è¿‡ç¨‹ï¼ˆNPsï¼‰çš„æ–°å‹æ¦‚ç‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚NPISeg3Då¼•å…¥äº†ä¸€ç§åˆ†å±‚æ½œåœ¨å˜é‡ç»“æ„ï¼Œé€šè¿‡æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œå¯¹è±¡ç‰¹å®šç‰¹å¾æ¥å¢å¼ºå°æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§æ¦‚ç‡åŸå‹è°ƒåˆ¶å™¨ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´ç‚¹å‡»åŸå‹ä¸å¯¹è±¡ç‰¹å®šçš„æ½œåœ¨å˜é‡ï¼Œæé«˜äº†æ¨¡å‹æ•æ‰å¯¹è±¡æ„ŸçŸ¥ä¸Šä¸‹æ–‡å’Œé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚åœ¨å››ä¸ªä¸‰ç»´ç‚¹äº‘æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNPISeg3Dåœ¨ç‚¹å‡»æ¬¡æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹å®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤äº’å¼ä¸‰ç»´åˆ†å‰²æŠ€æœ¯é€šè¿‡ç»“åˆç”¨æˆ·ç‚¹å‡»ï¼Œä¸ºå¤æ‚ä¸‰ç»´åœºæ™¯ä¸­çš„ç²¾ç¡®å¯¹è±¡æ©æ¨¡ç”Ÿæˆæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç›®å‰è¯¥æŠ€æœ¯é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šä»ç¨€ç–çš„ç”¨æˆ·ç‚¹å‡»ä¸­æœ‰æ•ˆæ³›åŒ–ä»¥åŠé‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚</li>
<li>NPISeg3Dæ˜¯ä¸€ä¸ªåŸºäºç¥ç»è¿‡ç¨‹çš„æ¦‚ç‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>NPISeg3Dé€šè¿‡å¼•å…¥åˆ†å±‚æ½œåœ¨å˜é‡ç»“æ„ï¼Œå¢å¼ºäº†å°æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œæ•æ‰å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚</li>
<li>NPISeg3Dè®¾è®¡çš„æ¦‚ç‡åŸå‹è°ƒåˆ¶å™¨èƒ½è‡ªé€‚åº”è°ƒæ•´ç‚¹å‡»åŸå‹ä¸å¯¹è±¡ç‰¹å®šæ½œåœ¨å˜é‡ï¼Œæé«˜å¯¹è±¡æ„ŸçŸ¥ä¸Šä¸‹æ–‡çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜NPISeg3Dåœ¨ä¸‰ç»´ç‚¹äº‘æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šæ€§èƒ½ï¼Œç‚¹å‡»æ¬¡æ•°å°‘ä¸”æä¾›å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>NPISeg3Dä¸ºç”¨æˆ·ä¸ä¸‰ç»´åœºæ™¯çš„äº¤äº’æä¾›äº†æ–°çš„è§†è§’å’Œå·¥å…·ï¼Œæœ‰æœ›æ¨åŠ¨ä¸‰ç»´åˆ†å‰²æŠ€æœ¯çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c45f6d3228c6a92194314008e4afe5ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f18f3de93ac36caae89259f46990cef4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99a957d11ccce3378a22fe7509e8f476.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22b60cffef3c5a932ce15c72520d8c45.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-brain-inspired-generative-model-for-EEG-based-cognitive-state-identification"><a href="#A-brain-inspired-generative-model-for-EEG-based-cognitive-state-identification" class="headerlink" title="A brain-inspired generative model for EEG-based cognitive state   identification"></a>A brain-inspired generative model for EEG-based cognitive state   identification</h2><p><strong>Authors:Bin Hu, Zhi-Hong Guan</strong></p>
<p>This article proposes a brain-inspired generative (BIG) model that merges an impulsive-attention neural network and a variational autoencoder (VAE) for identifying cognitive states based on electroencephalography (EEG) data. A hybrid learning method is presented for training the model by integrating gradient-based learning and heteroassociative memory. The BIG model is capable of achieving multi-task objectives: classification, generating new EEG, and brain network interpretation, alleviating the limitations of excessive data training and high computational cost in conventional approaches. Experimental results on two public EEG datasets demonstrate that the BIG model achieves a classification accuracy above 89%, comparable with state-of-the-art methods, while reducing computational cost by nearly 11% over the baseline EEGNet. Incorporating the generated EEG data for training, the BIG model exhibits comparative performance in a few-shot pattern.Ablation studies justify the poised brain-inspired characteristic regarding the impulsive-attention module and the hybrid learning method. Thanks to the performance advantages with interpretable outputs, this BIG model has application potential for building digital twins of the brain. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å—å¤§è„‘å¯å‘çš„ç”Ÿæˆï¼ˆBIGï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å†²åŠ¨æ€§æ³¨æ„ç¥ç»ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå¯æ ¹æ®è„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®è¯†åˆ«è®¤çŸ¥çŠ¶æ€ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ··åˆå­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ï¼Œå°†åŸºäºæ¢¯åº¦çš„å­¦ä¹ ä¸å¼‚è”æƒ³è®°å¿†ç›¸ç»“åˆã€‚BIGæ¨¡å‹èƒ½å¤Ÿå®ç°å¤šä»»åŠ¡ç›®æ ‡ï¼šåˆ†ç±»ã€ç”Ÿæˆæ–°çš„EEGæ•°æ®å’Œè§£é‡Šè„‘ç½‘ç»œï¼Œä»è€Œç¼“è§£ä¼ ç»Ÿæ–¹æ³•ä¸­è¿‡åº¦æ•°æ®è®­ç»ƒå’Œè®¡ç®—æˆæœ¬è¿‡é«˜çš„å±€é™æ€§ã€‚åœ¨ä¸¤ä¸ªå…¬å…±EEGæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBIGæ¨¡å‹çš„åˆ†ç±»å‡†ç¡®ç‡é«˜äº89%ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶è¾ƒåŸºçº¿EEGNeté™ä½äº†è¿‘11%çš„è®¡ç®—æˆæœ¬ã€‚é€šè¿‡åˆ©ç”¨ç”Ÿæˆçš„EEGæ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒBIGæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½ã€‚é€šè¿‡æ¶ˆèç ”ç©¶éªŒè¯äº†å†²åŠ¨æ€§æ³¨æ„æ¨¡å—å’Œæ··åˆå­¦ä¹ æ–¹æ³•åœ¨å¤§è„‘å¯å‘ç‰¹æ€§æ–¹é¢çš„åˆç†æ€§ã€‚ç”±äºå…·æœ‰å¯è§£é‡Šçš„è¾“å‡ºå’Œæ€§èƒ½ä¼˜åŠ¿ï¼Œå› æ­¤è¯¥BIGæ¨¡å‹åœ¨æ„å»ºå¤§è„‘çš„æ•°å­—åŒ–åŒèƒèƒæ–¹é¢å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01685v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å—å¤§è„‘å¯å‘çš„ç”Ÿæˆï¼ˆBIGï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å†²åŠ¨æ³¨æ„åŠ›ç¥ç»ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œç”¨äºåŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®è¯†åˆ«è®¤çŸ¥çŠ¶æ€ã€‚æ–‡ç« å±•ç¤ºäº†é€šè¿‡æ¢¯åº¦å­¦ä¹ å’Œå¼‚è”æƒ³è®°å¿†é›†æˆè¿›è¡Œæ¨¡å‹è®­ç»ƒçš„æ··åˆå­¦ä¹ æ–¹æ³•ã€‚BIGæ¨¡å‹èƒ½å¤Ÿå®Œæˆå¤šä»»åŠ¡ç›®æ ‡ï¼šåˆ†ç±»ã€ç”Ÿæˆæ–°çš„EEGæ•°æ®å’Œè§£é‡Šè„‘ç½‘ç»œï¼Œç¼“è§£ä¼ ç»Ÿæ–¹æ³•ä¸­è¿‡åº¦æ•°æ®è®­ç»ƒå’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„é™åˆ¶ã€‚åœ¨å…¬å…±EEGæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBIGæ¨¡å‹çš„åˆ†ç±»å‡†ç¡®ç‡é«˜äº89%ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼Œä¸”è¾ƒåŸºçº¿EEGNeté™ä½äº†è¿‘11%çš„è®¡ç®—æˆæœ¬ã€‚ä½¿ç”¨ç”Ÿæˆçš„EEGæ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒBIGæ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬æ¨¡å¼ä¸­ä¹Ÿè¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†å†²åŠ¨æ³¨æ„åŠ›æ¨¡å—å’Œæ··åˆå­¦ä¹ æ–¹æ³•çš„ç¨³å¥æ€§ã€‚ç”±äºå…·æœ‰å¯è§£é‡Šçš„è¾“å‡ºå’Œæ€§èƒ½ä¼˜åŠ¿ï¼ŒBIGæ¨¡å‹åœ¨æ„å»ºå¤§è„‘çš„æ•°å­—åŒ–åŒèƒèƒæ–¹é¢å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å—å¤§è„‘å¯å‘çš„ç”Ÿæˆï¼ˆBIGï¼‰æ¨¡å‹ï¼Œèåˆäº†å†²åŠ¨æ³¨æ„åŠ›ç¥ç»ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚</li>
<li>BIGæ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€ç”Ÿæˆæ–°çš„EEGæ•°æ®å’Œè§£é‡Šè„‘ç½‘ç»œã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ··åˆå­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†æ¢¯åº¦å­¦ä¹ å’Œå¼‚è”æƒ³è®°å¿†ã€‚</li>
<li>åœ¨å…¬å…±EEGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBIGæ¨¡å‹åˆ†ç±»å‡†ç¡®ç‡é«˜äº89%ï¼Œå¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>ä½¿ç”¨ç”Ÿæˆçš„EEGæ•°æ®è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒBIGæ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬æƒ…å†µä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¶ˆèç ”ç©¶éªŒè¯äº†å†²åŠ¨æ³¨æ„åŠ›æ¨¡å—å’Œæ··åˆå­¦ä¹ æ–¹æ³•çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-627fc2e66a8bb362615d19ae4182a3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3504830cb40297a67a2ed13f5fd863dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d47297884bf5b4fbd6d697b004e7470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87951d93fff3a2199d579fa0817b3e70.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PIN-WM-Learning-Physics-INformed-World-Models-for-Non-Prehensile-Manipulation"><a href="#PIN-WM-Learning-Physics-INformed-World-Models-for-Non-Prehensile-Manipulation" class="headerlink" title="PIN-WM: Learning Physics-INformed World Models for Non-Prehensile   Manipulation"></a>PIN-WM: Learning Physics-INformed World Models for Non-Prehensile   Manipulation</h2><p><strong>Authors:Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu</strong></p>
<p>While non-prehensile manipulation (e.g., controlled pushing&#x2F;poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts. </p>
<blockquote>
<p>éé¢„æ¡å¼æ“ä½œï¼ˆä¾‹å¦‚å—æ§çš„æ¨&#x2F;æˆ³ï¼‰æ„æˆäº†åŸºç¡€æœºå™¨äººæŠ€èƒ½çš„ä¸€éƒ¨åˆ†ï¼Œä½†ç”±äºå…¶å¯¹æ¶‰åŠæ‘©æ“¦å’Œæ¢å¤åŠ›çš„å¤æ‚ç‰©ç†äº¤äº’çš„é«˜åº¦æ•æ„Ÿæ€§ï¼Œå…¶å­¦ä¹ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å®ç°ç¨³å¥çš„æ”¿ç­–å­¦ä¹ å’Œæ³›åŒ–ï¼Œæˆ‘ä»¬é€‰æ‹©å­¦ä¹ ä¸éé¢„æ¡æ“ä½œç›¸å…³çš„3Dåˆšä½“åŠ¨åŠ›å­¦ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºåŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†PIN-WMï¼Œè¿™æ˜¯ä¸€ç§ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ä»è§†è§‰è§‚å¯Ÿä¸­è¿›è¡Œç«¯åˆ°ç«¯çš„3Dåˆšä½“åŠ¨åŠ›å­¦ç³»ç»Ÿè¯†åˆ«ã€‚é€šè¿‡é‡‡ç”¨å¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿï¼ŒPIN-WMä»…é€šè¿‡å°‘é‡ä»»åŠ¡æ— å…³çš„ç‰©ç†äº¤äº’è½¨è¿¹å³å¯å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒPIN-WMé€šè¿‡é«˜æ–¯æ··åˆï¼ˆGaussian Splattingï¼‰äº§ç”Ÿçš„è§‚å¯ŸæŸå¤±è¿›è¡Œå­¦ä¹ ï¼Œæ— éœ€è¿›è¡ŒçŠ¶æ€ä¼°è®¡ã€‚ä¸ºäº†å¼¥æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬é€šè¿‡ç‰©ç†æ„ŸçŸ¥éšæœºåŒ–å°†å­¦åˆ°çš„PIN-WMå˜æˆä¸€ç»„â€œæ•°å­—åˆ†èº«â€ï¼Œé€šè¿‡æ‰°åŠ¨ç‰©ç†å’Œæ¸²æŸ“å‚æ•°æ¥ç”Ÿæˆå¤šæ ·åŒ–å’Œæœ‰æ„ä¹‰çš„PIN-WMå˜åŒ–ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œé€šè¿‡ç‰©ç†æ„ŸçŸ¥çš„æ•°å­—åˆ†èº«å¢å¼ºçš„PIN-WMï¼Œæœ‰åŠ©äºå­¦ä¹ å…·æœ‰æ¨¡æ‹Ÿåˆ°ç°å®è¿ç§»èƒ½åŠ›çš„ç¨³å¥çš„éé¢„æ¡æ“ä½œæŠ€èƒ½ï¼Œè¶…è¶Šäº†æœ€æ–°çš„Real2Sim2RealæŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16693v2">PDF</a> Robotics: Science and Systems 2025</p>
<p><strong>Summary</strong></p>
<p>éæŠ“å–æ“ä½œï¼ˆå¦‚æ§åˆ¶æ¨åŠ¨&#x2F;æˆ³åˆºï¼‰æ˜¯æœºå™¨äººæŠ€æœ¯ä¸­çš„åŸºç¡€æŠ€èƒ½ï¼Œä½†ç”±äºæ¶‰åŠåˆ°æ‘©æ“¦å’Œæ¢å¤ç­‰å¤æ‚ç‰©ç†äº¤äº’ï¼Œå…¶å­¦ä¹ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºå®ç°ç¨³å¥çš„ç­–ç•¥å­¦ä¹ å’Œæ³›åŒ–ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å­¦ä¹ éæŠ“å–æ“ä½œæ‰€æ¶‰åŠçš„3Dåˆšä½“åŠ¨åŠ›å­¦ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶ç”¨äºåŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†PIN-WMï¼Œä¸€ç§ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ä»è§†è§‰è§‚å¯Ÿä¸­è¿›è¡Œç«¯åˆ°ç«¯çš„3Dåˆšä½“åŠ¨åŠ›å­¦ç³»ç»Ÿè¯†åˆ«ã€‚é‡‡ç”¨å¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿï¼ŒPIN-WMä»…é€šè¿‡å°‘é‡ä»»åŠ¡æ— å…³çš„ç‰©ç†äº¤äº’è½¨è¿¹å³å¯å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒPIN-WMé€šè¿‡é«˜æ–¯æ•£æ–‘è¯±å¯¼çš„è§‚å¯ŸæŸå¤±è¿›è¡Œå­¦ä¹ ï¼Œæ— éœ€çŠ¶æ€ä¼°è®¡ã€‚ä¸ºå¼¥ä»¿çœŸä¸çœŸå®ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å°†å­¦åˆ°çš„PIN-WMè½¬åŒ–ä¸ºä¸€ç³»åˆ—ç‰©ç†æ„ŸçŸ¥éšæœºåŒ–çš„æ•°å­—åˆ†èº«ï¼Œé€šè¿‡æ‰°åŠ¨ç‰©ç†å’Œæ¸²æŸ“å‚æ•°ç”Ÿæˆå¤šæ ·ä¸”å…·æ„ä¹‰çš„PIN-WMå˜åŒ–ã€‚åœ¨ä»¿çœŸå’ŒçœŸå®ä¸–ç•Œæµ‹è¯•ä¸­çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¾…ä»¥ç‰©ç†æ„ŸçŸ¥æ•°å­—åˆ†èº«çš„PIN-WMï¼Œèƒ½å¤Ÿä¿ƒè¿›å…·æœ‰Sim2Realè¿ç§»èƒ½åŠ›çš„ç¨³å¥éæŠ“å–æ“ä½œæŠ€èƒ½çš„å­¦ä¹ ï¼Œè¶…è¶ŠReal2Sim2Realå½“å‰æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éæŠ“å–æ“ä½œçš„æœºå™¨äººæŠ€æœ¯å­¦ä¹ é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ¶‰åŠå¤æ‚ç‰©ç†äº¤äº’ã€‚</li>
<li>æå‡ºäº†PIN-WMæ¨¡å‹ï¼Œé€šè¿‡ç‰©ç†ä¿¡æ¯ä¸–ç•Œæ¨¡å‹è¿›è¡Œé«˜æ•ˆç«¯åˆ°ç«¯è¯†åˆ«ã€‚</li>
<li>ä»…é€šè¿‡å°‘é‡ä»»åŠ¡æ— å…³çš„ç‰©ç†äº¤äº’è½¨è¿¹å³å¯å­¦ä¹ PIN-WMæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨é«˜æ–¯æ•£æ–‘è¯±å¯¼çš„è§‚å¯ŸæŸå¤±å­¦ä¹ PIN-WMæ¨¡å‹ï¼Œæ— éœ€çŠ¶æ€ä¼°è®¡ã€‚</li>
<li>é€šè¿‡è½¬åŒ–ä¸ºæ•°å­—åˆ†èº«æ¥ç¼©å°ä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ã€‚</li>
<li>PIN-WMé…åˆæ•°å­—åˆ†èº«ä¿ƒè¿›ç¨³å¥çš„éæŠ“å–æ“ä½œæŠ€èƒ½å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6edd20c06c48ef422b8968ef067637e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-650dc498b4b6715cc538deb1c022f2e5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the modelâ€™s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æ—¶é—´åŠ¨ä½œå®šä½ï¼ˆTALï¼‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„è¯¦ç»†æ ‡æ³¨æ•°æ®ï¼Œè€Œå°‘æ ·æœ¬TALé€šè¿‡ä»…ä½¿ç”¨å°‘é‡çš„è®­ç»ƒæ ·æœ¬æ¥è¯†åˆ«æœªè§è¿‡çš„åŠ¨ä½œç±»åˆ«ï¼Œå‡å°‘äº†è¿™ç§ä¾èµ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å°‘æ ·æœ¬TALæ–¹æ³•é€šå¸¸åªå…³æ³¨è§†é¢‘çº§åˆ«çš„ä¿¡æ¯ï¼Œå¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯ï¼Œæ–‡æœ¬ä¿¡æ¯å¯ä»¥ä¸ºå®šä½ä»»åŠ¡æä¾›æœ‰ä»·å€¼çš„è¯­ä¹‰æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬æ—¶é—´åŠ¨ä½œå®šä½æ–¹æ³•ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ–‡æœ¬æ¨ç†æ¥æé«˜å®šä½æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æ¥æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬-è§†è§‰å¯¹é½æ¨¡å—ï¼Œæ—¨åœ¨ä»¥ä¸åŒçº§åˆ«å¯¹é½æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°è¡¨è¾¾æ–‡æœ¬å±‚é¢åŠ¨ä½œçš„æ—¶ç©ºä¾èµ–å…³ç³»å’Œå› æœå…³ç³»ï¼Œè¾…åŠ©åŠ¨ä½œå®šä½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç±»ä¼¼é“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„æ¨ç†æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆç±»ä¼¼CoTçš„æ–‡æœ¬æè¿°è§†é¢‘ã€‚ç”Ÿæˆçš„æ–‡æœ¬å¯ä»¥æ•æ‰æ¯”è§†è§‰ç‰¹å¾æ›´å¤šçš„åŠ¨ä½œå˜åŒ–ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„ActivityNet1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æˆ‘ä»¬å¼•å…¥äº†åä¸ºHuman-related Anomaly Localizationçš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶æ¢ç´¢äº†TALä»»åŠ¡åœ¨äººç±»å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†è¯­ä¹‰æ„ŸçŸ¥æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œä»¥åœ¨æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘çš„ä¸åŒå±‚æ¬¡ä¸Šè¿›è¡Œå¯¹é½ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°è¡¨è¾¾æ–‡æœ¬å±‚é¢åŠ¨ä½œçš„æ—¶ç©ºä¾èµ–å’Œå› æœå…³ç³»ï¼Œè¾…åŠ©åŠ¨ä½œå®šä½ï¼Œè¯¥æ–¹æ³•è¿˜è®¾è®¡äº†ç±»ä¼¼Chain of Thoughtçš„æ¨ç†æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½ï¼ˆFew-Shot TALï¼‰æ–¹æ³•å¯ä»¥å‡å°‘å¯¹å¤§é‡è¯¦ç»†æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬å³å¯è¯†åˆ«æœªè§è¿‡çš„åŠ¨ä½œç±»åˆ«ã€‚</li>
<li>ç°æœ‰å°‘æ ·æœ¬TALæ–¹æ³•ä¸»è¦å…³æ³¨è§†é¢‘çº§åˆ«ä¿¡æ¯ï¼Œå¿½ç•¥äº†æ–‡æœ¬ä¿¡æ¯ï¼Œè¯¥æ–‡æœ¬æå‡ºåˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ–‡ä¸­è®¾è®¡äº†ä¸€ä¸ªæ–°å‹çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«è¯­ä¹‰æ„ŸçŸ¥æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œæ—¨åœ¨åœ¨ä¸åŒå±‚æ¬¡ä¸Šå¯¹é½æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘ã€‚</li>
<li>å¼•å…¥äº†ç±»ä¼¼Chain of Thoughtçš„æ¨ç†æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬å±‚é¢æ›´å¥½åœ°è¡¨è¾¾åŠ¨ä½œçš„æ—¶ç©ºä¾èµ–å’Œå› æœå…³ç³»ï¼Œè¾…åŠ©åŠ¨ä½œå®šä½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ActivityNet1.3å’ŒTHUMOS14å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶æ¢ç´¢äº†äººç±»å¼‚å¸¸æ£€æµ‹ä¸­çš„TALä»»åŠ¡åº”ç”¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c28b4d56e719be49e7b23beb9e51bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a64c1df0179a3d48520ff1d1aed0ddc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dc5ab6a2751031974916ba1a71c7ffd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning"><a href="#Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning" class="headerlink" title="Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning"></a>Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning</h2><p><strong>Authors:Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</strong></p>
<p>Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods. </p>
<blockquote>
<p>åœ¨äººå·¥æ™ºèƒ½ä¸­ï¼Œæ•´åˆæ–°ç±»åˆ«ä¿¡æ¯è€Œä¸ä¸¢å¤±å…ˆå‰è·å¾—çš„çŸ¥è¯†ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œé€šå¸¸è¢«ç§°ä¸ºç¾éš¾æ€§é—å¿˜ã€‚å°‘æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é€šè¿‡é¦–å…ˆåœ¨ä¸€ä¸ªç¨³å¥çš„åŸºç¡€ç±»åˆ«æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨ä¸€ç³»åˆ—è¿ç»­çš„ä¼šè¯ä¸­ä»…ä½¿ç”¨æ¯ä¸ªæ–°ç±»åˆ«çš„å°‘é‡æ ‡è®°ç¤ºä¾‹æ¥é€æ­¥é€‚åº”æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å®¹æ˜“åœ¨æ–°çš„æœ‰é™æ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼Œè¿™å¯èƒ½ä¼šæŸå®³æ•´ä½“æ€§èƒ½å¹¶åŠ å‰§é—å¿˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„FSCILæ–°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å†»ç»“çš„Vision Transformerï¼ˆViTï¼‰ä¸»å¹²å¹¶é€šè¿‡å‚æ•°é«˜æ•ˆçš„åŠ æ³•æ›´æ–°è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å†»ç»“äº†é¢„è®­ç»ƒçš„ViTå‚æ•°ï¼Œå¹¶é€šè¿‡åŠ æ³•æ›´æ–°æœºåˆ¶æœ‰é€‰æ‹©åœ°å°†å¯è®­ç»ƒæƒé‡æ³¨å…¥è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚è¿™ç§è®¾è®¡åªæ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ä»¥é€‚åº”æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¸ä¼šç‰ºç‰²åŸºç¡€ä¼šè¯æœŸé—´å­¦åˆ°çš„è¡¨ç¤ºã€‚é€šè¿‡å¾®è°ƒæœ‰é™æ•°é‡çš„å‚æ•°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†å†»ç»“ViTä¸­çš„å¯æ³›åŒ–ç‰¹å¾ï¼ŒåŒæ—¶é™ä½äº†è¿‡æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼Œç”±äºå¤§å¤šæ•°å‚æ•°ä¿æŒä¸å˜ï¼Œå½“å¼•å…¥å°‘é‡æ–°æ•°æ®æ‰¹æ¬¡æ—¶ï¼Œæ¨¡å‹é¿å…äº†è¦†ç›–å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿FSCILæ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08982v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„Few-Shot Class Incremental Learningï¼ˆFSCILï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å†»ç»“çš„Vision Transformerï¼ˆViTï¼‰ä¸»å¹²å¹¶é€šè¿‡å‚æ•°æœ‰æ•ˆçš„æ›´æ–°æœºåˆ¶è¿›è¡Œå¢å¼ºã€‚é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„ViTå‚æ•°ï¼Œå¹¶åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©æ€§æ³¨å…¥å¯è®­ç»ƒæƒé‡ï¼Œè¯¥æ–¹æ³•åœ¨é€‚åº”æ–°ç±»åˆ«æ—¶ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€ä¼šè¯ä¸­å­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚é€šè¿‡å¾®è°ƒæœ‰é™çš„å‚æ•°ï¼Œè¯¥æ–¹æ³•åœ¨å†»ç»“çš„ViTä¸­ä¿ç•™äº†å¯æ³›åŒ–çš„ç‰¹å¾ï¼Œå¹¶é™ä½äº†è¿‡æ‹Ÿåˆçš„é£é™©ã€‚æ­¤å¤–ï¼Œç”±äºå¤§éƒ¨åˆ†å‚æ•°ä¿æŒä¸å˜ï¼Œæ¨¡å‹åœ¨å¼•å…¥å°‘é‡æ–°æ•°æ®æ‰¹æ¬¡æ—¶é¿å…äº†è¦†ç›–å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Class Incremental Learning (FSCIL) æ—¨åœ¨è§£å†³äººå·¥æ™ºèƒ½ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå³åœ¨ä¸å¤±å»å…ˆå‰å­¦ä¹ çŸ¥è¯†çš„æƒ…å†µä¸‹æ•´åˆæ–°ç±»åˆ«ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„FSCILæ¡†æ¶åˆ©ç”¨å†»ç»“çš„Vision Transformer (ViT) ä¸»å¹²ï¼Œé€šè¿‡å‚æ•°æœ‰æ•ˆçš„æ›´æ–°æœºåˆ¶è¿›è¡Œå¢å¼ºã€‚</li>
<li>æ¡†æ¶é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„ViTå‚æ•°å¹¶ä»…åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­æ³¨å…¥å¯è®­ç»ƒæƒé‡æ¥å·¥ä½œï¼Œä»¥é€‚åº”æ–°ç±»åˆ«ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…æ›´æ–°ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä»¥ä¿ç•™åŸºç¡€ä¼šè¯ä¸­å­¦ä¹ åˆ°çš„è¡¨ç¤ºï¼Œå¹¶åœ¨å†»ç»“çš„ViTä¸­ä¿ç•™å¯æ³›åŒ–çš„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¾®è°ƒæœ‰é™çš„å‚æ•°ï¼Œè¯¥æ–¹æ³•é™ä½äº†è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå¹¶åœ¨å¼•å…¥æ–°çš„å°‘é‡æ•°æ®æ‰¹æ¬¡æ—¶é¿å…äº†è¦†ç›–å…ˆå‰å­¦ä¹ çš„çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–FSCILæ–¹æ³•ï¼Œè¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79a7a0018cff974282588ca6395ba110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97db49e7a982ebece3df7365ad3d4ac9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbf051031222750908d09c09d552775a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4bedfa457b37aa884645f3f627a78ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9fbbb4029c9d2a9adef6e5f9682398a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c297fb926cd4685cfbab548e234eccca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Frankenstein-Optimizer-Harnessing-the-Potential-by-Revisiting-Optimization-Tricks"><a href="#Frankenstein-Optimizer-Harnessing-the-Potential-by-Revisiting-Optimization-Tricks" class="headerlink" title="Frankenstein Optimizer: Harnessing the Potential by Revisiting   Optimization Tricks"></a>Frankenstein Optimizer: Harnessing the Potential by Revisiting   Optimization Tricks</h2><p><strong>Authors:Chia-Wei Hsu, Nien-Ti Tsou, Yu-Cheng Chen, Yang Jeong Park, Ju Li</strong></p>
<p>Gradient-based optimization drives the unprecedented performance of modern deep neural network models across diverse applications. Adaptive algorithms have accelerated neural network training due to their rapid convergence rates; however, they struggle to find &#96;&#96;flat minimaâ€ reliably, resulting in suboptimal generalization compared to stochastic gradient descent (SGD). By revisiting various adaptive algorithmsâ€™ mechanisms, we propose the Frankenstein optimizer, which combines their advantages. The proposed Frankenstein dynamically adjusts first- and second-momentum coefficients according to the optimizerâ€™s current state to directly maintain consistent learning dynamics and immediately reflect sudden gradient changes. Extensive experiments across several research domains such as computer vision, natural language processing, few-shot learning, and scientific simulations show that Frankenstein surpasses existing adaptive algorithms and SGD empirically regarding convergence speed and generalization performance. Furthermore, this research deepens our understanding of adaptive algorithms through centered kernel alignment analysis and loss landscape visualization during the learning process. Code is available at <a target="_blank" rel="noopener" href="https://github.com/acctouhou/Frankenstein_optimizer">https://github.com/acctouhou/Frankenstein_optimizer</a> </p>
<blockquote>
<p>åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ¨åŠ¨äº†ç°ä»£æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨å¤šç§åº”ç”¨ä¸­çš„å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚è‡ªé€‚åº”ç®—æ³•ç”±äºå¿«é€Ÿçš„æ”¶æ•›ç‡è€ŒåŠ é€Ÿäº†ç¥ç»ç½‘ç»œè®­ç»ƒï¼›ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¯é åœ°æ‰¾åˆ°â€œå¹³å¦æœ€å°å€¼â€æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´ä¸éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç›¸æ¯”æ¬¡ä¼˜çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å„ç§è‡ªé€‚åº”ç®—æ³•çš„æœºåˆ¶è¿›è¡Œé‡æ–°å®¡è§†ï¼Œæå‡ºäº†ç»“åˆäº†å®ƒä»¬ä¼˜åŠ¿çš„Frankensteinä¼˜åŒ–å™¨ã€‚æ‰€æå‡ºçš„Frankensteinä¼šæ ¹æ®ä¼˜åŒ–å™¨çš„å½“å‰çŠ¶æ€åŠ¨æ€è°ƒæ•´ç¬¬ä¸€å’Œç¬¬äºŒåŠ¨é‡ç³»æ•°ï¼Œä»¥ç›´æ¥ä¿æŒä¸€è‡´çš„å­¦ä¹ åŠ¨åŠ›å¹¶ç«‹å³åæ˜ çªç„¶çš„æ¢¯åº¦å˜åŒ–ã€‚åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å°æ ·æœ¬å­¦ä¹ å’Œç§‘å­¦æ¨¡æ‹Ÿç­‰å¤šä¸ªç ”ç©¶é¢†åŸŸçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFrankensteinåœ¨æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„è‡ªé€‚åº”ç®—æ³•å’ŒSGDã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶é€šè¿‡ä¸­å¿ƒæ ¸å¯¹é½åˆ†æå’Œå­¦ä¹ è¿‡ç¨‹ä¸­çš„æŸå¤±æ™¯è§‚å¯è§†åŒ–ï¼Œæ·±åŒ–äº†æˆ‘ä»¬å¯¹è‡ªé€‚åº”ç®—æ³•çš„ç†è§£ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/acctouhou/Frankenstein_optimizer">https://github.com/acctouhou/Frankenstein_optimizer</a> ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02147v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç°ä»£æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„å“è¶Šæ€§èƒ½å¾—ç›Šäºæ¢¯åº¦ä¼˜åŒ–æŠ€æœ¯çš„æ¨åŠ¨ã€‚è™½ç„¶è‡ªé€‚åº”ç®—æ³•å› å…¶å¿«é€Ÿæ”¶æ•›ç‡åŠ é€Ÿäº†ç¥ç»ç½‘ç»œè®­ç»ƒï¼Œä½†åœ¨å¯»æ‰¾â€œå¹³å¦æœ€å°å€¼â€æ–¹é¢å­˜åœ¨å¯é æ€§é—®é¢˜ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸åŠéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ã€‚ç ”ç©¶è€…æå‡ºç»“åˆå„ç§è‡ªé€‚åº”ç®—æ³•ä¼˜åŠ¿çš„Frankensteinä¼˜åŒ–å™¨ï¼Œå¯æ ¹æ®å½“å‰çŠ¶æ€åŠ¨æ€è°ƒæ•´ä¸€ã€äºŒé˜¶åŠ¨é‡ç³»æ•°ï¼Œç»´æŒç¨³å®šå­¦ä¹ åŠ¨æ€å¹¶å¿«é€Ÿå“åº”æ¢¯åº¦çªå˜ã€‚è·¨å¤šä¸ªé¢†åŸŸå¦‚è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å°æ ·æœ¬å­¦ä¹ å’Œç§‘å­¦æ¨¡æ‹Ÿçš„å®éªŒæ˜¾ç¤ºï¼ŒFrankensteinåœ¨æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰è‡ªé€‚åº”ç®—æ³•å’ŒSGDã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡ä¸­å¿ƒæ ¸å¯¹é½åˆ†æå’Œå­¦ä¹ è¿‡ç¨‹æŸå¤±æ™¯è§‚å¯è§†åŒ–æ·±åŒ–äº†å¯¹è‡ªé€‚åº”ç®—æ³•çš„ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢¯åº¦ä¼˜åŒ–æ˜¯æ¨åŠ¨ç°ä»£æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹é«˜æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>è‡ªé€‚åº”ç®—æ³•è™½åŠ é€Ÿç¥ç»ç½‘ç»œè®­ç»ƒï¼Œä½†åœ¨å¯»æ‰¾å¹³å¦æœ€å°å€¼æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>Frankensteinä¼˜åŒ–å™¨ç»“åˆå¤šç§è‡ªé€‚åº”ç®—æ³•çš„ä¼˜åŠ¿ï¼ŒåŠ¨æ€è°ƒæ•´åŠ¨é‡ç³»æ•°ä»¥ç»´æŒç¨³å®šå­¦ä¹ å¹¶å“åº”æ¢¯åº¦å˜åŒ–ã€‚</li>
<li>Frankensteinä¼˜åŒ–å™¨åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰ç®—æ³•ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸­å¿ƒæ ¸å¯¹é½åˆ†æå’ŒæŸå¤±æ™¯è§‚å¯è§†åŒ–æ·±åŒ–äº†å¯¹è‡ªé€‚åº”ç®—æ³•çš„ç†è§£ã€‚</li>
<li>Frankensteinä¼˜åŒ–å™¨é€‚ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œå¦‚è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å°æ ·æœ¬å­¦ä¹ å’Œç§‘å­¦æ¨¡æ‹Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9e9ab5a904aa0007fedd2e14ec278e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f7c204e8ed8a5540f1e19ad2da3e981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb4d75cf1c71f30d0be9b3c01bf832bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-130f6f1de819ebc8d78b2e84f39fcecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf29d02b99f37d815ff57863a802421f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46d9d5228003adf902685949489009b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f427b253f6ed3fce11eba9bc6ca843de.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Predicting-potentially-abusive-clauses-in-Chilean-terms-of-services-with-natural-language-processing"><a href="#Predicting-potentially-abusive-clauses-in-Chilean-terms-of-services-with-natural-language-processing" class="headerlink" title="Predicting potentially abusive clauses in Chilean terms of services with   natural language processing"></a>Predicting potentially abusive clauses in Chilean terms of services with   natural language processing</h2><p><strong>Authors:Christoffer Loeffler, Andrea MartÃ­nez Freile, TomÃ¡s Rey Pizarro</strong></p>
<p>This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and&#x2F;or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å…³æ³¨æ¶ˆè´¹è€…åˆåŒä¸­ä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜çš„æ—¥ç›ŠåŠ å‰§ï¼Œè¿™ä¸€é—®é¢˜å› åœ¨çº¿æœåŠ¡çš„æ™®åŠåŠå…¶å¤æ‚çš„æœåŠ¡æ¡æ¬¾ï¼ˆå¾ˆå°‘è¢«é˜…è¯»ï¼‰è€Œæ„ˆå‘ä¸¥é‡ã€‚å°½ç®¡å·²ç»å¼€å±•äº†å…³äºè‡ªåŠ¨åˆ†ææ–¹æ³•çš„ç ”ç©¶ï¼Œä½†é—®é¢˜æ˜¯ç”±äºç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æœºå™¨å­¦ä¹ æ–¹æ³•ä»¥åŠæ¬§æ´²è”ç›Ÿç­‰ä¸»è¦å¸æ³•ç®¡è¾–åŒºè€ŒåŠ å‰§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•å’Œå¤§é‡çš„æ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«å››ä¸ªç±»åˆ«å’Œæ€»å…±20ä¸ªå­ç±»åˆ«çš„å…¨æ–°æ³¨é‡Šæ–¹æ¡ˆï¼Œå¹¶å¯¹å…¶åº”ç”¨äºæ™ºåˆ©ä½¿ç”¨çš„50é¡¹åœ¨çº¿æœåŠ¡æ¡æ¬¾ã€‚æˆ‘ä»¬å¯¹åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¼ºè°ƒäº†è¯­è¨€ç‰¹å®šå’Œ&#x2F;æˆ–é¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒã€å°‘é‡æ ·æœ¬å¤§å°å’Œæ¨¡å‹æ¶æ„ç­‰å› ç´ å¦‚ä½•å½±å“æ½œåœ¨æ»¥ç”¨æ¡æ¬¾çš„æ£€æµ‹å’Œåˆ†ç±»ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸åŒä»»åŠ¡å’Œæ¨¡å‹çš„æ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œæ£€æµ‹ä»»åŠ¡çš„æœ€é«˜å®F1åˆ†æ•°èŒƒå›´ä»79%åˆ°89%ï¼Œå¾®è§‚F1åˆ†æ•°é«˜è¾¾96%ï¼Œè€Œåˆ†ç±»ä»»åŠ¡çš„å®F1åˆ†æ•°èŒƒå›´ä»60%åˆ°70%ï¼Œå¾®è§‚F1åˆ†æ•°ä»64%åˆ°80%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ³•å¾‹æ¡æ¬¾çš„è¥¿ç­ç‰™è¯­å¤šæ ‡ç­¾åˆ†ç±»æ•°æ®é›†ï¼Œå®ƒé€‚ç”¨äºæ™ºåˆ©æ³•å¾‹ï¼Œå¹¶å¯¹è¥¿ç­ç‰™è¯­æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥çš„æ–¹æ³•å¼€å‘ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œè¯¥ç ”ç©¶å¯¹äºå¾ˆå°‘è€ƒè™‘çš„æ³•å¾‹åˆ†æå…·æœ‰å®é™…æ„ä¹‰ï¼Œå¹¶æœ‰æœ›ä¸ºæ¶ˆè´¹è€…åœ¨æ™ºåˆ©ä¹ƒè‡³æ•´ä¸ªæ‹‰ä¸ç¾æ´²æä¾›æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00865v2">PDF</a> 39 pages, 2 figures, 8 tables, accepted for publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ¶ˆè´¹è€…åˆåŒä¸­ä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜çš„æ—¥ç›Šä¸¥é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¿æœåŠ¡åè®®çš„æ™®åŠåŠ å‰§äº†è¿™ä¸€é—®é¢˜ã€‚ç ”ç©¶ä»‹ç»äº†æ–°çš„æ–¹æ³•è®ºå’Œå¤§é‡æ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œå¹¶æå‡ºäº†æ–°çš„æ ‡æ³¨æ–¹æ¡ˆå’Œå››ä¸ªç±»åˆ«ä¸‹çš„äºŒåä¸ªå­ç±»åˆ«ã€‚é€šè¿‡å¯¹æ™ºåˆ©ä½¿ç”¨çš„äº”åé¡¹åœ¨çº¿æœåŠ¡åè®®çš„åº”ç”¨è¯„ä¼°ï¼Œå‘ç°è¯­è¨€ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒã€å°‘é‡æ ·æœ¬å¤§å°å’Œæ¨¡å‹æ¶æ„ç­‰å› ç´ ä¼šå½±å“æ½œåœ¨æ»¥ç”¨æ¡æ¬¾çš„æ£€æµ‹å’Œåˆ†ç±»ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒä»»åŠ¡å’Œæ¨¡å‹çš„æ€§èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œæ£€æµ‹ä»»åŠ¡çš„æœ€é«˜å®è§‚F1åˆ†æ•°åœ¨79%åˆ°89%ä¹‹é—´ï¼Œå¾®è§‚F1åˆ†æ•°é«˜è¾¾96%ï¼Œè€Œåˆ†ç±»ä»»åŠ¡çš„å®è§‚F1åˆ†æ•°åœ¨60%åˆ°70%ä¹‹é—´ï¼Œå¾®è§‚F1åˆ†æ•°åœ¨64%åˆ°80%ä¹‹é—´ã€‚è¿™ä¸ºæœªæ¥æ³•å¾‹åˆ†ææ–¹æ³•çš„å‘å±•æä¾›äº†åŸºç¡€ï¼Œå¹¶æœ‰æœ›æ”¯æŒæ™ºåˆ©å’Œæ‹‰ä¸ç¾æ´²çš„æ¶ˆè´¹è€…å®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨æ¶ˆè´¹è€…åˆåŒä¸­ä¿¡æ¯ä¸å¯¹ç§°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤æ‚çš„åœ¨çº¿æœåŠ¡åè®®ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ–¹æ³•è®ºå’Œæ ‡æ³¨æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ•°æ®é›†æ¶µç›–æ™ºåˆ©ä½¿ç”¨çš„åœ¨çº¿æœåŠ¡åè®®ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œå‘ç°è¯­è¨€ç‰¹å®šå’Œé¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒå¯¹æ£€æµ‹æ½œåœ¨æ»¥ç”¨æ¡æ¬¾è‡³å…³é‡è¦ã€‚</li>
<li>ä¸åŒä»»åŠ¡å’Œæ¨¡å‹çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œæ£€æµ‹ä»»åŠ¡çš„F1åˆ†æ•°è¾ƒé«˜ã€‚</li>
<li>è¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ³•å¾‹æ¡æ¬¾çš„è¥¿ç­ç‰™è¯­å¤šæ ‡ç­¾åˆ†ç±»æ•°æ®é›†ï¼Œåº”ç”¨æ™ºåˆ©æ³•å¾‹å¹¶å…¨é¢è¯„ä¼°è¥¿ç­ç‰™è¯­æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶ä¸ºæœªæ¥æ³•å¾‹åˆ†ææ–¹æ³•çš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b71b19a998e618d4537842c8733ba790.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unlocking-Transfer-Learning-for-Open-World-Few-Shot-Recognition"><a href="#Unlocking-Transfer-Learning-for-Open-World-Few-Shot-Recognition" class="headerlink" title="Unlocking Transfer Learning for Open-World Few-Shot Recognition"></a>Unlocking Transfer Learning for Open-World Few-Shot Recognition</h2><p><strong>Authors:Byeonggeun Kim, Juntae Lee, Kyuhong Shim, Simyung Chang</strong></p>
<p>Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR. </p>
<blockquote>
<p>å°‘æ ·æœ¬å¼€æ”¾é›†è¯†åˆ«ï¼ˆFSOSRï¼‰æ—¨åœ¨åº”å¯¹ç°å®ä¸–ç•Œä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œæ—¨åœ¨å°†è¾“å…¥åˆ†ç±»ä¸ºå·²çŸ¥ç±»åˆ«ï¼ˆç§°ä¸ºå°é—­é›†ç±»åˆ«ï¼‰ï¼ŒåŒæ—¶è¯†åˆ«ä¸å±äºè¿™äº›ç±»åˆ«çš„å¼€æ”¾é›†è¾“å…¥ã€‚è™½ç„¶è¿ç§»å­¦ä¹ å·²æˆä¸ºå°é—­ä¸–ç•Œä¸­ä¸€ç§çªå‡ºçš„èŒƒå¼ï¼Œå…¶ä¸­æ¨¡å‹è¢«è°ƒæ•´åˆ°ç»™å®šçš„å°‘æ ·æœ¬ä»»åŠ¡ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒæ— æ³•æ‰©å±•åˆ°å¼€æ”¾ä¸–ç•Œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¼€æ”¾é›†æ„ŸçŸ¥å…ƒå­¦ä¹ ä¸å¼€æ”¾é›†è‡ªç”±è¿ç§»å­¦ä¹ ã€‚åœ¨å¼€æ”¾é›†æ„ŸçŸ¥å…ƒå­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥å»ºç«‹åº¦é‡ç©ºé—´ï¼Œä½œä¸ºåç»­é˜¶æ®µçš„æœ‰ç›Šèµ·ç‚¹ã€‚åœ¨å¼€æ”¾é›†è‡ªç”±è¿ç§»å­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è¿ç§»å­¦ä¹ è¿›ä¸€æ­¥é€‚åº”ç‰¹å®šçš„ç›®æ ‡ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§é€šè¿‡ä¿®æ”¹è®­ç»ƒæ•°æ®é›†æˆ–ç”Ÿæˆä¼ªå¼€æ”¾é›†ç¤ºä¾‹æ¥æ¨¡æ‹Ÿå¼€æ”¾é›†ç¤ºä¾‹çš„ç­–ç•¥ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å¹¿æ³›è®¤å¯çš„miniImageNetå’ŒtieredImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œä»…å¢åŠ äº†1.5%çš„è®­ç»ƒåŠªåŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜äº†è¿ç§»å­¦ä¹ åœ¨FSOSRä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09986v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Few-Shot Open-Set Recognitionï¼ˆFSOSRï¼‰çš„ç›®æ ‡ä¸æŒ‘æˆ˜ã€‚å®ƒæ—¨åœ¨å°†è¾“å…¥åˆ†ç±»ä¸ºå·²çŸ¥ç±»åˆ«ï¼ˆç§°ä¸ºå°é—­é›†ç±»åˆ«ï¼‰ï¼ŒåŒæ—¶è¯†åˆ«ä¸å±äºè¿™äº›ç±»åˆ«çš„å¼€æ”¾é›†è¾“å…¥ã€‚è™½ç„¶è¿ç§»å­¦ä¹ å·²æˆä¸ºå°é—­ä¸–ç•Œä¸­çš„ä¸»æµèŒƒå¼ï¼Œä½†åœ¨å¼€æ”¾ä¸–ç•Œä¸­å´å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¼€æ”¾é›†æ„ŸçŸ¥å…ƒå­¦ä¹ ä¸å¼€æ”¾é›†è‡ªç”±è¿ç§»å­¦ä¹ ã€‚åœ¨å¼€æ”¾é›†æ„ŸçŸ¥å…ƒå­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹è¢«è®­ç»ƒä»¥å»ºç«‹åº¦é‡ç©ºé—´ï¼Œä½œä¸ºåç»­é˜¶æ®µçš„èµ·ç‚¹ã€‚åœ¨å¼€æ”¾é›†è‡ªç”±è¿ç§»å­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡è¿ç§»å­¦ä¹ è¿›ä¸€æ­¥é€‚åº”ç‰¹å®šç›®æ ‡ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¿®æ”¹è®­ç»ƒæ•°æ®é›†æˆ–ç”Ÿæˆä¼ªå¼€æ”¾é›†ç¤ºä¾‹æ¥æ¨¡æ‹Ÿå¼€æ”¾é›†ç¤ºä¾‹çš„ç­–ç•¥ä¹Ÿè¢«å¼•å…¥ã€‚è¯¥æ–¹æ³•åœ¨miniImageNetå’ŒtieredImageNetä¸¤ä¸ªå¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä»…å¢åŠ äº†1.5%çš„è®­ç»ƒåŠªåŠ›ã€‚ç ”ç©¶è¯æ˜äº†è¿ç§»å­¦ä¹ åœ¨FSOSRä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Open-Set Recognition (FSOSR) æ—¨åœ¨è§£å†³å°†è¾“å…¥åˆ†ç±»ä¸ºå·²çŸ¥ç±»åˆ«å¹¶è¯†åˆ«å¼€æ”¾é›†è¾“å…¥çš„é—®é¢˜ã€‚</li>
<li>è¿ç§»å­¦ä¹ åœ¨å°é—­ä¸–ç•Œä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†åœ¨å¼€æ”¾ä¸–ç•Œä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µæ–¹æ³•åŒ…æ‹¬å¼€æ”¾é›†æ„ŸçŸ¥å…ƒå­¦ä¹ å’Œå¼€æ”¾é›†è‡ªç”±è¿ç§»å­¦ä¹ ã€‚</li>
<li>åœ¨å¼€æ”¾é›†æ„ŸçŸ¥å…ƒå­¦ä¹ é˜¶æ®µï¼Œæ¨¡å‹å»ºç«‹åº¦é‡ç©ºé—´ä½œä¸ºåç»­é˜¶æ®µçš„èµ·ç‚¹ã€‚</li>
<li>å¼€æ”¾é›†è‡ªç”±è¿ç§»å­¦ä¹ é˜¶æ®µä½¿æ¨¡å‹é€‚åº”ç‰¹å®šç›®æ ‡ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå¼€æ”¾é›†ç¤ºä¾‹çš„ç­–ç•¥æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04d22293ff9aea26d02ac9843d9ab84c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9b8876c163a6403e1b09941030239e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f254cd586bc625ab4300056c60a0892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f34076d4f58c14d0eb1c4f4399c1a0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4da8151e6c828851e11c0a4b21947103.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Reinforced Correlation Between Vision and Language for Precise Medical   AI Assistant
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-245e2d5d70913e83da6eecd733b7086f.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-08  Multi-Agent System for Comprehensive Soccer Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
