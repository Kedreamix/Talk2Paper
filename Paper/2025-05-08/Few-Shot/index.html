<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-08  Towards Smart Point-and-Shoot Photography">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-de7f6c38b37cbb82fa68cf62304b2b86.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    60 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-08-更新"><a href="#2025-05-08-更新" class="headerlink" title="2025-05-08 更新"></a>2025-05-08 更新</h1><h2 id="Towards-Smart-Point-and-Shoot-Photography"><a href="#Towards-Smart-Point-and-Shoot-Photography" class="headerlink" title="Towards Smart Point-and-Shoot Photography"></a>Towards Smart Point-and-Shoot Photography</h2><p><strong>Authors:Jiawan Li, Fei Zhou, Zhipeng Zhong, Jiongzhi Lin, Guoping Qiu</strong></p>
<p>Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing 320K images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets. </p>
<blockquote>
<p>数百万人经常使用智能手机作为即拍即摄（PAS）相机拍照，然而很少有人具备拍摄好场景照片的摄影技巧。虽然传统的PAS相机内置功能可以确保照片对焦清晰、亮度合适，但它们无法告诉用户如何拍摄场景的最佳镜头。在本文中，我们首次提出了一种智能即拍即摄（SPAS）系统，以帮助用户拍摄优质照片。我们的SPAS系统通过自动指导用户实时调整相机姿势来帮助用户拍摄好场景的照片。我们首先构建了一个大型数据集，包含来自4000个场景的320K张带有相机姿势信息的图像。然后，我们开发了一种基于CLIP的创新性构图质量评估（CCQA）模型，为这些图像分配伪标签。CCQA引入了一种独特的可学习文本嵌入技术，学习连续的单词嵌入，能够辨别五个质量描述词所涵盖范围内的细微视觉质量差异{差、较差、一般、好、完美}。最后，我们开发了一个相机姿势调整模型（CPAM），该模型首先确定当前视图是否可以进一步改进，如果可以，则输出以两个相机姿势调整角度形式的调整建议。CPAM的两个任务是按顺序进行的，每个任务涉及不同的训练样本集，因此我们采用了一种带有门控损失函数的混合专家模型，以端到端的方式训练CPAM。我们将通过使用公开的图像构图数据集来全面展示SPAS系统的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03638v1">PDF</a> CVPR2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的智能点拍系统（SPAS），旨在帮助用户拍摄高质量的照片。该系统通过自动引导用户调整相机姿态来实现。研究者构建了一个大型图像数据集，并利用CLIP技术的组合质量评估模型对图像进行伪标签标注。此外，他们还开发了一种相机姿态调整模型（CPAM），可判断当前视角是否需要改进，并给出调整建议。通过公开可用的图像组合数据集，展示了SPAS系统的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPAS系统旨在帮助用户拍摄高质量照片，通过自动引导调整相机姿态实现。</li>
<li>研究者构建了一个包含320K张图片的大型数据集，每张图片都有相机姿态信息。</li>
<li>采用了基于CLIP的组合质量评估模型（CCQA），为图像赋予伪标签，能辨别细微的视觉质量差异。</li>
<li>CCQA模型采用连续词嵌入技术，覆盖“坏、差、一般、好、完美”五个质量描述等级。</li>
<li>开发了相机姿态调整模型（CPAM），能判断当前视角是否需要改进，并提供调整建议。</li>
<li>CPAM采用混合专家模型和门控损失函数进行训练，实现端到端的训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03638">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-34d9408f5f199f1da10b2e981a5ae3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cc0afd3e5d044a877ef03bebf284c56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-736a2bf242408748109a7c72496b415b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c12935a49cf52cd6839e21a5ec63de5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0515d69ec1512fdde307fdb5fb712e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757377b1a89558ed3b43ac258df7110d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Unreasonable-Effectiveness-of-Discrete-Time-Gaussian-Process-Mixtures-for-Robot-Policy-Learning"><a href="#The-Unreasonable-Effectiveness-of-Discrete-Time-Gaussian-Process-Mixtures-for-Robot-Policy-Learning" class="headerlink" title="The Unreasonable Effectiveness of Discrete-Time Gaussian Process   Mixtures for Robot Policy Learning"></a>The Unreasonable Effectiveness of Discrete-Time Gaussian Process   Mixtures for Robot Policy Learning</h2><p><strong>Authors:Jan Ole von Hartz, Adrian Röfer, Joschka Boedecker, Abhinav Valada</strong></p>
<p>We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel approach for flexible policy representation and imitation learning in robot manipulation. MiDiGap enables learning from as few as five demonstrations using only camera observations and generalizes across a wide range of challenging tasks. It excels at long-horizon behaviors such as making coffee, highly constrained motions such as opening doors, dynamic actions such as scooping with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns these tasks on a CPU in less than a minute and scales linearly to large datasets. We also develop a rich suite of tools for inference-time steering using evidence such as collision signals and robot kinematic constraints. This steering enables novel generalization capabilities, including obstacle avoidance and cross-embodiment policy transfer. MiDiGap achieves state-of-the-art performance on diverse few-shot manipulation benchmarks. On constrained RLBench tasks, it improves policy success by 76 percentage points and reduces trajectory cost by 67%. On multimodal tasks, it improves policy success by 48 percentage points and increases sample efficiency by a factor of 20. In cross-embodiment transfer, it more than doubles policy success. We make the code publicly available at <a target="_blank" rel="noopener" href="https://midigap.cs.uni-freiburg.de/">https://midigap.cs.uni-freiburg.de</a>. </p>
<blockquote>
<p>我们提出了离散时间高斯过程混合（MiDiGap）方法，这是一种用于机器人操控中的灵活策略表示和模仿学习的新方法。MiDiGap能够从仅有的五个演示中学习，仅使用相机观察，并能广泛适应各种具有挑战性的任务。它擅长长远行为，如制作咖啡、高度约束的运动，如开门、动态动作，如用铲子铲东西以及多模式任务，如挂杯子。MiDiGap可以在CPU上学习这些任务不到一分钟的时间，并且线性扩展到大型数据集。我们还开发了一套丰富的工具，用于在推理时使用证据进行引导，如碰撞信号和机器人运动学约束。这种引导使新的泛化能力成为可能，包括避障和跨体态策略转移。MiDiGap在多样化的少样本操作基准测试中达到了最先进的性能。在受约束的RLBench任务中，它将策略成功率提高了76个百分点，轨迹成本降低了67%。在多模式任务中，它将策略成功率提高了48个百分点，提高了样本效率的20倍。在跨体态转移方面，它使策略成功率增加了一倍以上。我们的代码公开在<a target="_blank" rel="noopener" href="https://midigap.cs.uni-freiburg.de./">https://midigap.cs.uni-freiburg.de。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.03296v1">PDF</a> Submitted for publication to IEEE Transaction on Robotics</p>
<p><strong>Summary</strong></p>
<p>MiDiGap是一种用于机器人操作中的灵活策略表示和模仿学习的新方法。它能够从仅有的五个演示中学习，仅使用摄像头观察，并可以广泛适用于各种挑战任务。MiDiGap擅长处理长期行为、高度约束的运动、动态动作和多模式任务。其在CPU上的学习时间短于一分钟，并可线性扩展到大型数据集。此外，MiDiGap还开发了一套丰富的工具，用于基于证据进行推理时间控制，如碰撞信号和机器人运动学约束。这些控制功能带来了新颖的一般化能力，包括障碍避免和跨体态策略转移。MiDiGap在多样的少样本操作基准测试中达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MiDiGap是一种用于机器人操作的新型策略表示和模仿学习方法。</li>
<li>它能够从仅有的五个演示中学习，仅使用摄像头观察。</li>
<li>MiDiGap适用于各种挑战任务，包括长期行为、高度约束的运动、动态动作和多模式任务。</li>
<li>MiDiGap在CPU上的学习时间短，并可快速扩展到大型数据集。</li>
<li>MiDiGap提供丰富的工具进行推理时间控制，包括基于证据的控制功能。</li>
<li>这些控制功能带来了新颖的一般化能力，如障碍避免和跨体态策略转移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.03296">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fc9d8792e83fea75c11380e14b76cc60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fb53b3ee1a077e5e38a78307794328f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cb1821a4d8c8adfe845c6f874d890d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79a5f0302cdfc448c20ab9e60c5ea5af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b5b1ac447c1d6c32007483d233db6f3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Knowledge-Graphs-for-Enhancing-Large-Language-Models-in-Entity-Disambiguation"><a href="#Knowledge-Graphs-for-Enhancing-Large-Language-Models-in-Entity-Disambiguation" class="headerlink" title="Knowledge Graphs for Enhancing Large Language Models in Entity   Disambiguation"></a>Knowledge Graphs for Enhancing Large Language Models in Entity   Disambiguation</h2><p><strong>Authors:Gerard Pons, Besim Bilalli, Anna Queralt</strong></p>
<p>Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entities’ classes in a KG to gradually prune the candidate space as well as the entities’ descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KG’s semantic expressivity on the ED performance. </p>
<blockquote>
<p>最近的大型语言模型（LLM）的进步使它们成为自然语言处理任务的重要解决方案。值得注意的是，它们可以以零次或多次少量的方式解决问题，从而无需针对特定任务进行训练或微调模型。然而，LLM面临一些挑战，包括虚构和训练数据中特定领域的知识过时或缺失信息等问题。这些问题无法通过用新数据重新训练模型来轻松解决，因为这是一个耗时且昂贵的过程。为了缓解这些问题，知识图谱（KG）已被提议作为结构化的外部信息源来丰富LLM。基于这一理念，在这项工作中，我们使用知识图谱来增强LLM的零次实体消歧（ED）。为此，我们借助知识图谱中实体类别的层次表示来逐步缩小候选空间，并利用实体的描述来丰富输入提示以获取额外的事实知识。我们在流行的ED数据集上的评估表明，所提出的方法优于非增强型和仅描述增强的LLM，并且比特定任务的模型具有更高的适应性。此外，我们进行了错误分析，并讨论了所利用的知识图谱的语义表达力对ED性能的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02737v2">PDF</a> Pre-print submitted to ISWC 2024</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自然语言处理任务中表现出显著的优势，尤其在零样本或少样本场景下表现突出。然而，LLM面临知识过时、缺失或幻觉等问题。为解决这些问题，利用知识图谱（KG）作为结构化外部信息源来丰富LLM成为一项解决方案。本研究采用知识图谱增强LLM进行零样本实体消歧（ED），通过利用知识图谱中实体类别的层次表示，逐步缩小候选空间，并借助实体描述来丰富输入提示中的事实知识。在流行ED数据集上的评估显示，该方法优于非增强和仅描述增强的LLM，且具有比特定任务模型更高的适应性。此外，还进行了误差分析，并讨论了所用知识图谱的语义表达能力对ED性能的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）可在自然语言处理任务中表现优异，尤其在零样本或少样本情境下。</li>
<li>LLM面临知识过时、缺失和幻觉等问题。</li>
<li>知识图谱（KG）可作为结构化外部信息源，用于增强LLM的性能。</li>
<li>本研究利用知识图谱进行实体消歧（ED），结合实体类别层次表示和实体描述来丰富输入提示。</li>
<li>在流行ED数据集上的评估显示，该方法优于其他LLM，且具有高适应性。</li>
<li>知识图谱的语义表达能力对实体消歧性能有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e081a5072f6d4ed09e243512cf7c25fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eff3b9d587bbdcf1c4b25116ba9da99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a64b83d8a562a85e9df0a86141e19f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de7f6c38b37cbb82fa68cf62304b2b86.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization"><a href="#Identifying-Legal-Holdings-with-LLMs-A-Systematic-Study-of-Performance-Scale-and-Memorization" class="headerlink" title="Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization"></a>Identifying Legal Holdings with LLMs: A Systematic Study of Performance,   Scale, and Memorization</h2><p><strong>Authors:Chuck Arvin</strong></p>
<p>As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate &#96;&#96;scaling effects’’ - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks. </p>
<blockquote>
<p>随着大型语言模型（LLM）的能力不断提升，评估它们在现有基准测试上的表现变得至关重要。本研究呈现了一系列实验，旨在评估现代LLM（参数范围从3B到90B+）在CaseHOLD这一法律基准数据集上的表现，该数据集用于识别判例法摘要。我们的实验展示了“规模效应”——在此任务上的表现会随着模型规模的扩大而提高，更强大的模型如GPT4o和AmazonNovaPro分别取得了宏观F1分数为0.744和0.720。这些分数与该数据集上已发布的最佳结果具有竞争力，并且不需要任何技术复杂模型的训练、微调或少样本提示。为确保这些强有力的结果并非由于训练数据中司法意见的刻板记忆所致，我们开发并采用了新型的引用匿名测试，该测试保留了语义含义，同时确保了案例名称和引用的虚构性。在这些条件下，模型保持了强大的性能（宏观F1为0.728），这表明其表现并非由于死记硬背。这些发现展示了LLM在法律任务上的潜力以及当前存在的局限性，对于自动化法律分析和法律基准的开发与衡量具有重要的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02172v1">PDF</a> Presented as a short paper at International Conference on Artificial   Intelligence and Law 2025 (Chicago, IL)</p>
<p><strong>Summary</strong></p>
<p>现代大型语言模型（LLM）在法律任务上的性能评估至关重要。本研究通过一系列实验，评估了不同规模LLM在CaseHOLD数据集上的表现。实验结果显示模型性能随规模提升而改善，GPT4o和AmazonNovaPro等大型模型表现出优异的性能。这些结果具有竞争力，且无需复杂模型训练、微调或少样本提示。通过新型引用匿名化测试，确保模型性能并非仅因记忆训练数据中的司法意见而达成。这一研究为自动化法律分析和法律基准的开发和评估提供了重要启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在法律任务性能评估中表现出规模效益，性能随模型规模提升而改善。</li>
<li>GPT4o和AmazonNovaPro等模型在CaseHOLD数据集上的表现优异，宏观F1分数高且具有竞争力。</li>
<li>最佳模型性能不需要复杂的模型训练、微调或少样本提示。</li>
<li>通过引用匿名化测试验证了模型性能并非仅基于训练数据的记忆。</li>
<li>模型在法律任务上的表现显示自动化法律分析的潜力和当前局限性。</li>
<li>该研究对开发和应用法律基准的未来发展有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d7b9b0d76b2e2812d643cddfb5378b1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22dee1fee3bb963cc63fbd728b2ba911.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-974f29b3d5ba445aeffde7558d8e6144.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9adc73211c22c0b0ecb6cc0984cd1fe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b51ceda711f76de4442f49ea93c0e28.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Retrieval-augmented-in-context-learning-for-multimodal-large-language-models-in-disease-classification"><a href="#Retrieval-augmented-in-context-learning-for-multimodal-large-language-models-in-disease-classification" class="headerlink" title="Retrieval-augmented in-context learning for multimodal large language   models in disease classification"></a>Retrieval-augmented in-context learning for multimodal large language   models in disease classification</h2><p><strong>Authors:Zaifu Zhan, Shuang Zhou, Xiaoshan Zhou, Yongkang Xiao, Jun Wang, Jiawen Deng, He Zhu, Yu Hou, Rui Zhang</strong></p>
<p>Objectives: We aim to dynamically retrieve informative demonstrations, enhancing in-context learning in multimodal large language models (MLLMs) for disease classification.   Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL) framework, which integrates retrieval-augmented generation (RAG) and in-context learning (ICL) to adaptively select demonstrations with similar disease patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines embeddings from diverse encoders, including ResNet, BERT, BioBERT, and ClinicalBERT, to retrieve appropriate demonstrations, and constructs conversational prompts optimized for ICL. We evaluated the framework on two real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies, similarity metrics, and varying numbers of demonstrations.   Results: RAICL consistently improved classification performance. Accuracy increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs being stronger than images alone. The richness of information embedded in each modality will determine which embedding model can be used to get better results. Few-shot experiments showed that increasing the number of retrieved examples further enhanced performance. Across different similarity metrics, Euclidean distance achieved the highest accuracy while cosine similarity yielded better macro-F1 scores. RAICL demonstrated consistent improvements across various MLLMs, confirming its robustness and versatility.   Conclusions: RAICL provides an efficient and scalable approach to enhance in-context learning in MLLMs for multimodal disease classification. </p>
<blockquote>
<p>目标：我们的目标是动态检索信息丰富的演示内容，以增强多模态大型语言模型（MLLMs）的上下文学习，用于疾病分类。方法：我们提出了一个名为RAICL（检索增强上下文学习）的框架，它结合了检索增强生成（RAG）和上下文学习（ICL），以自适应地选择具有相似疾病模式的演示内容，使MLLMs中的ICL更加有效。具体来说，RAICL会检查来自不同编码器的嵌入，包括ResNet、BERT、BioBERT和ClinicalBERT，以检索适当的演示内容，并构建针对ICL优化的对话提示。我们在两个真实世界的多模态数据集（TCGA和IU Chest X射线）上评估了框架的性能，评估其在多个MLLMs（Qwen、Llava、Gemma）、嵌入策略、相似度指标和不同数量的演示内容方面的表现。结果：RAICL持续提高了分类性能。TCGA上的准确率从0.7854提高到0.8368，IU Chest X射线上的准确率从0.7924提高到0.8658。多模态输入优于单模态输入，文本输入的效能强于图像。每种模态中嵌入的信息丰富程度将决定使用哪种嵌入模型可以获得更好的结果。少量样本的实验表明，增加检索到的示例数量可以进一步提高性能。在不同的相似度指标中，欧几里得距离实现了最高的准确率，而余弦相似度获得了更好的宏观F1分数。RAICL在各种MLLMs中表现出持续的性能改进，证明了其稳健性和通用性。结论：RAICL为多模态疾病分类在MLLMs中的上下文学习提供了一种高效且可扩展的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02087v1">PDF</a> 17 Pages, 1 figure, 7 tables</p>
<p><strong>Summary</strong></p>
<p>本文旨在通过动态检索有信息量的演示示例，提高多模态大型语言模型（MLLMs）在疾病分类方面的上下文学习能力。提出了一种名为RAICL的检索增强上下文学习框架，该框架结合了检索增强生成（RAG）和上下文学习（ICL），可自适应地选择具有相似疾病模式的演示示例，从而更有效地在多模态大型语言模型中进行上下文学习。在真实世界的多模态数据集TCGA和IU Chest X-ray上的实验表明，RAICL提高了分类性能，并且在不同的多模态大型语言模型、嵌入策略、相似度度量以及不同数量的演示示例中都表现出了优越性。这表明RAICL是一个有效且可扩展的方法，可提高多模态大型语言模型在疾病分类方面的上下文学习能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的检索增强上下文学习（RAICL）框架，旨在提高多模态大型语言模型（MLLMs）在疾病分类方面的上下文学习能力。</li>
<li>结合了检索增强生成（RAG）和上下文学习（ICL），通过自适应选择演示示例来增强学习。</li>
<li>在真实世界的多模态数据集TCGA和IU Chest X-ray上的实验表明，RAICL能提高疾病分类的性能。</li>
<li>多模态输入优于单模态输入，文本输入比图像输入更有效。</li>
<li>不同嵌入模型的效果取决于其所嵌入信息的丰富程度。</li>
<li>增加检索到的示例数量能进一步提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02087">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4e0224b6b81335fcfbc47b32893dd613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3efc261caf0037b03082634723a8dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9ad7b09574640219beea0bc2b57ed11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ee88f5fe94813d5385ad2043fcc8afe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction"><a href="#ReLI-A-Language-Agnostic-Approach-to-Human-Robot-Interaction" class="headerlink" title="ReLI: A Language-Agnostic Approach to Human-Robot Interaction"></a>ReLI: A Language-Agnostic Approach to Human-Robot Interaction</h2><p><strong>Authors:Linus Nwankwo, Bjoern Ellensohn, Ozan Özdenizci, Elmar Rueckert</strong></p>
<p>Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction’s linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI’s robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI’s potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at <a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/">https://linusnep.github.io/ReLI/</a>. </p>
<blockquote>
<p>目前，将自主智能体适应于工业、家居和其他日常任务正在获得动力。然而，在全球或跨语言的应用场景中，确保与环境的有效互动并执行多样语言的非限制性人类任务指定指令仍是一个未解决的问题。为了应对这一挑战，我们提出了ReLI，这是一个与语言无关的框架，旨在使自主智能体能够进行自然对话、对周围环境进行语义推理并执行下游任务，而无论任务指令的语言起源如何。首先，我们基于大规模预训练基础模型，将其转化为语言到行动模型，这些模型可以通过自然、流畅的人机对话互动，直接提供常识推理和高级机器人控制。此外，我们进行了跨语言模型接地，以确保ReLI在全球范围内推广各种语言。为了证明ReLI的稳健性，我们在各种短期和长期任务上进行了广泛的模拟和真实实验，包括零样本和少量样本的空间导航、场景信息检索和查询导向任务。我们在涉及超过7万多次对话的140种语言上进行了基准测试。平均而言，ReLI在跨语言指令解析和任务执行成功率方面达到了超过90%±0.2的准确率。这些结果证明了ReLI在实际世界中增强自然人机交互的潜力，同时支持语言多样性。演示和资源将在<a target="_blank" rel="noopener" href="https://linusnep.github.io/ReLI/%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://linusnep.github.io/ReLI/上公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01862v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为ReLI的语言通用框架，它能实现自主代理在自然环境下进行交流、理解环境并进行下游任务操作，而无需受到任务指令语言来源的限制。该框架通过大型预训练基础模型建立语言到行动模型，支持自由流式的人机对话互动和机器人控制。经过跨语言训练，ReLI可推广到全球多种语言。实验结果显示，ReLI在多种短期和长期任务上表现出强大的稳健性，成功执行跨语言指令的准确率达到90%以上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReLI是一个语言通用的框架，旨在使自主代理能够在各种环境下自然地交流、理解环境并执行任务，不受任务指令语言来源的限制。</li>
<li>ReLI通过建立语言到行动模型，支持自由流式的人机对话互动和机器人控制。</li>
<li>ReLI通过大型预训练基础模型实现，具备通用性和强大的学习能力。</li>
<li>ReLI经过跨语言训练，可以推广到全球多种语言，增强了其在实际应用中的普适性。</li>
<li>ReLI在模拟和真实世界的实验中表现出强大的稳健性，特别是在零基础和少量样本的空间导航、场景信息检索和查询导向任务中。</li>
<li>ReLI在跨语言指令解析和任务执行方面的准确率超过90%，证明了其有效性和实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d10faf471c1e3b33652793c06cfbbb34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45284bfa77d4deac8023ed8fd8a1e0a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-729cc5c73428c130c3f9f924b3a5982d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d54418794f8851404b96e7859613097c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes"><a href="#Probabilistic-Interactive-3D-Segmentation-with-Hierarchical-Neural-Processes" class="headerlink" title="Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes"></a>Probabilistic Interactive 3D Segmentation with Hierarchical Neural   Processes</h2><p><strong>Authors:Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves</strong></p>
<p>Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model’s ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations. </p>
<blockquote>
<p>交互式3D分割技术通过结合用户提供的点击，在复杂的3D场景中生成精确的对象掩模方面展现出了巨大的潜力。然而，还有两大挑战尚未得到充分探索：（1）如何从稀疏的用户点击中有效泛化以产生精确的分割；（2）量化预测不确定性，以帮助用户识别不可靠的区域。在这项工作中，我们提出了NPISeg3D，这是一个新的概率框架，它基于神经过程（NPs）来解决这些挑战。具体来说，NPISeg3D引入了一种分层潜在变量结构，包括场景特定和对象特定的潜在变量，通过捕捉全局上下文和对象特定特征，增强少样本泛化能力。此外，我们设计了一个概率原型调制器，它自适应地利用对象特定的潜在变量调制点击原型，提高了模型捕捉对象感知上下文和量化预测不确定性的能力。在四个3D点云数据集上的实验表明，NPISeg3D在点击次数较少的情况下实现了优越的分割性能，同时提供了可靠的不确定性估计。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01726v1">PDF</a> ICML 2025 Proceedings</p>
<p><strong>Summary</strong></p>
<p>基于用户点击的交互式三维分割技术为复杂三维场景中的精确对象掩模生成提供了有前景的解决方案。然而，两个关键问题尚未得到充分研究：（1）如何从稀疏的用户点击中有效泛化以产生精确的分割；（2）量化预测不确定性以帮助用户识别不可靠区域。本研究提出了NPISeg3D，一个基于神经过程（NPs）的新型概率框架，旨在解决这些问题。NPISeg3D引入了一种分层潜在变量结构，通过捕捉全局上下文和对象特定特征来增强小样本泛化能力。此外，设计了一种概率原型调制器，能够自适应地调整点击原型与对象特定的潜在变量，提高了模型捕捉对象感知上下文和量化预测不确定性的能力。在四个三维点云数据集上的实验表明，NPISeg3D在点击次数较少的情况下实现了优越的分割性能，同时提供了可靠的不确定性估计。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>交互式三维分割技术通过结合用户点击，为复杂三维场景中的精确对象掩模生成提供了有效方法。</li>
<li>目前该技术面临两个挑战：从稀疏的用户点击中有效泛化以及量化预测不确定性。</li>
<li>NPISeg3D是一个基于神经过程的概率框架，旨在解决上述挑战。</li>
<li>NPISeg3D通过引入分层潜在变量结构，增强了小样本泛化能力，捕捉全局和局部特征。</li>
<li>NPISeg3D设计的概率原型调制器能自适应调整点击原型与对象特定潜在变量，提高对象感知上下文的捕捉能力。</li>
<li>实验证明NPISeg3D在三维点云数据集上实现了优越性能，点击次数少且提供可靠的不确定性估计。</li>
<li>NPISeg3D为用户与三维场景的交互提供了新的视角和工具，有望推动三维分割技术的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01726">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c45f6d3228c6a92194314008e4afe5ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f18f3de93ac36caae89259f46990cef4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99a957d11ccce3378a22fe7509e8f476.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22b60cffef3c5a932ce15c72520d8c45.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-brain-inspired-generative-model-for-EEG-based-cognitive-state-identification"><a href="#A-brain-inspired-generative-model-for-EEG-based-cognitive-state-identification" class="headerlink" title="A brain-inspired generative model for EEG-based cognitive state   identification"></a>A brain-inspired generative model for EEG-based cognitive state   identification</h2><p><strong>Authors:Bin Hu, Zhi-Hong Guan</strong></p>
<p>This article proposes a brain-inspired generative (BIG) model that merges an impulsive-attention neural network and a variational autoencoder (VAE) for identifying cognitive states based on electroencephalography (EEG) data. A hybrid learning method is presented for training the model by integrating gradient-based learning and heteroassociative memory. The BIG model is capable of achieving multi-task objectives: classification, generating new EEG, and brain network interpretation, alleviating the limitations of excessive data training and high computational cost in conventional approaches. Experimental results on two public EEG datasets demonstrate that the BIG model achieves a classification accuracy above 89%, comparable with state-of-the-art methods, while reducing computational cost by nearly 11% over the baseline EEGNet. Incorporating the generated EEG data for training, the BIG model exhibits comparative performance in a few-shot pattern.Ablation studies justify the poised brain-inspired characteristic regarding the impulsive-attention module and the hybrid learning method. Thanks to the performance advantages with interpretable outputs, this BIG model has application potential for building digital twins of the brain. </p>
<blockquote>
<p>本文提出了一种受大脑启发的生成（BIG）模型，该模型结合了冲动性注意神经网络和变分自编码器（VAE），可根据脑电图（EEG）数据识别认知状态。文章提出了一种混合学习方法来训练模型，将基于梯度的学习与异联想记忆相结合。BIG模型能够实现多任务目标：分类、生成新的EEG数据和解释脑网络，从而缓解传统方法中过度数据训练和计算成本过高的局限性。在两个公共EEG数据集上的实验结果表明，BIG模型的分类准确率高于89%，与最新方法相当，同时较基线EEGNet降低了近11%的计算成本。通过利用生成的EEG数据进行训练，BIG模型在少量样本情况下表现出相当的性能。通过消融研究验证了冲动性注意模块和混合学习方法在大脑启发特性方面的合理性。由于具有可解释的输出和性能优势，因此该BIG模型在构建大脑的数字化双胞胎方面具有应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01685v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种受大脑启发的生成（BIG）模型，该模型结合了冲动注意力神经网络和变分自编码器（VAE），用于基于脑电图（EEG）数据识别认知状态。文章展示了通过梯度学习和异联想记忆集成进行模型训练的混合学习方法。BIG模型能够完成多任务目标：分类、生成新的EEG数据和解释脑网络，缓解传统方法中过度数据训练和计算成本高昂的限制。在公共EEG数据集上的实验结果表明，BIG模型的分类准确率高于89%，与最新技术相当，且较基线EEGNet降低了近11%的计算成本。使用生成的EEG数据进行训练，BIG模型在少数样本模式中也表现出相当的性能。消融研究验证了冲动注意力模块和混合学习方法的稳健性。由于具有可解释的输出和性能优势，BIG模型在构建大脑的数字化双胞胎方面具有应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章提出了一种新的受大脑启发的生成（BIG）模型，融合了冲动注意力神经网络和变分自编码器（VAE）。</li>
<li>BIG模型能够处理多任务，包括分类、生成新的EEG数据和解释脑网络。</li>
<li>模型通过混合学习方法进行训练，结合了梯度学习和异联想记忆。</li>
<li>在公共EEG数据集上的实验表明，BIG模型分类准确率高于89%，并降低了计算成本。</li>
<li>使用生成的EEG数据进行训练时，BIG模型在少数样本情况下表现出良好的性能。</li>
<li>消融研究验证了冲动注意力模块和混合学习方法的必要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-627fc2e66a8bb362615d19ae4182a3aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3504830cb40297a67a2ed13f5fd863dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d47297884bf5b4fbd6d697b004e7470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87951d93fff3a2199d579fa0817b3e70.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PIN-WM-Learning-Physics-INformed-World-Models-for-Non-Prehensile-Manipulation"><a href="#PIN-WM-Learning-Physics-INformed-World-Models-for-Non-Prehensile-Manipulation" class="headerlink" title="PIN-WM: Learning Physics-INformed World Models for Non-Prehensile   Manipulation"></a>PIN-WM: Learning Physics-INformed World Models for Non-Prehensile   Manipulation</h2><p><strong>Authors:Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu</strong></p>
<p>While non-prehensile manipulation (e.g., controlled pushing&#x2F;poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts. </p>
<blockquote>
<p>非预握式操作（例如受控的推&#x2F;戳）构成了基础机器人技能的一部分，但由于其对涉及摩擦和恢复力的复杂物理交互的高度敏感性，其学习仍然具有挑战性。为了实现稳健的政策学习和泛化，我们选择学习与非预握操作相关的3D刚体动力学世界模型，并将其用于基于模型的强化学习。我们提出了PIN-WM，这是一种物理信息世界模型，能够高效地从视觉观察中进行端到端的3D刚体动力学系统识别。通过采用可微分物理模拟，PIN-WM仅通过少量任务无关的物理交互轨迹即可学习。此外，PIN-WM通过高斯混合（Gaussian Splatting）产生的观察损失进行学习，无需进行状态估计。为了弥模拟与真实之间的差异，我们通过物理感知随机化将学到的PIN-WM变成一组“数字分身”，通过扰动物理和渲染参数来生成多样化和有意义的PIN-WM变化。在模拟和真实世界测试上的广泛评估表明，通过物理感知的数字分身增强的PIN-WM，有助于学习具有模拟到现实迁移能力的稳健的非预握操作技能，超越了最新的Real2Sim2Real技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16693v2">PDF</a> Robotics: Science and Systems 2025</p>
<p><strong>Summary</strong></p>
<p>非抓取操作（如控制推动&#x2F;戳刺）是机器人技术中的基础技能，但由于涉及到摩擦和恢复等复杂物理交互，其学习仍然具有挑战性。为实现稳健的策略学习和泛化，我们选择了学习非抓取操作所涉及的3D刚体动力学世界模型，并用于基于模型强化学习。我们提出了PIN-WM，一种物理信息世界模型，能够高效地从视觉观察中进行端到端的3D刚体动力学系统识别。采用可微分物理模拟，PIN-WM仅通过少量任务无关的物理交互轨迹即可学习。此外，PIN-WM通过高斯散斑诱导的观察损失进行学习，无需状态估计。为弥仿真与真实之间的差距，我们将学到的PIN-WM转化为一系列物理感知随机化的数字分身，通过扰动物理和渲染参数生成多样且具意义的PIN-WM变化。在仿真和真实世界测试中的广泛评估表明，辅以物理感知数字分身的PIN-WM，能够促进具有Sim2Real迁移能力的稳健非抓取操作技能的学习，超越Real2Sim2Real当前最佳水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非抓取操作的机器人技术学习面临挑战，主要由于涉及复杂物理交互。</li>
<li>提出了PIN-WM模型，通过物理信息世界模型进行高效端到端识别。</li>
<li>仅通过少量任务无关的物理交互轨迹即可学习PIN-WM模型。</li>
<li>利用高斯散斑诱导的观察损失学习PIN-WM模型，无需状态估计。</li>
<li>通过转化为数字分身来缩小仿真与现实之间的差距。</li>
<li>PIN-WM配合数字分身促进稳健的非抓取操作技能学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6edd20c06c48ef422b8968ef067637e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-650dc498b4b6715cc538deb1c022f2e5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model’s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>传统的时间动作定位（TAL）方法依赖于大量的详细标注数据，而少样本TAL通过仅使用少量的训练样本来识别未见过的动作类别，减少了这种依赖。然而，现有的少样本TAL方法通常只关注视频级别的信息，忽略了文本信息，文本信息可以为定位任务提供有价值的语义支持。因此，我们提出了一种新的少样本时间动作定位方法，通过链式思维文本推理来提高定位性能。具体来说，我们设计了一种新的少样本学习框架，利用文本语义信息来提高模型捕捉动作共性和变化的能力，其中包括一个语义感知的文本-视觉对齐模块，旨在以不同级别对齐查询和支持视频。同时，为了更好地表达文本层面动作的时空依赖关系和因果关系，辅助动作定位，我们设计了类似链式思维（CoT）的推理方法，逐步引导视觉语言模型（VLM）和大型语言模型（LLM）生成类似CoT的文本描述视频。生成的文本可以捕捉比视觉特征更多的动作变化。我们在公开可用的ActivityNet1.3和THUMOS14数据集上进行了大量实验。我们引入了名为Human-related Anomaly Localization的第一个数据集，并探索了TAL任务在人类异常检测中的应用。实验结果表明，我们的方法在单实例和多实例场景中显著优于现有方法。我们将公开我们的代码、数据和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种基于Chain-of-Thought文本推理的少样本时序动作定位方法。该方法利用文本语义信息提高模型捕捉动作共性和变化的能力，并设计了语义感知文本视觉对齐模块，以在查询和支持视频的不同层次上进行对齐。同时，为了更好地表达文本层面动作的时空依赖和因果关系，辅助动作定位，该方法还设计了类似Chain of Thought的推理方法。实验结果表明，该方法在单实例和多实例场景下均显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本时序动作定位（Few-Shot TAL）方法可以减少对大量详细标注数据的依赖，仅使用少量训练样本即可识别未见过的动作类别。</li>
<li>现有少样本TAL方法主要关注视频级别信息，忽略了文本信息，该文本提出利用文本语义信息提高模型性能。</li>
<li>文中设计了一个新型的少样本学习框架，包含语义感知文本视觉对齐模块，旨在在不同层次上对齐查询和支持视频。</li>
<li>引入了类似Chain of Thought的推理方法，在文本层面更好地表达动作的时空依赖和因果关系，辅助动作定位。</li>
<li>该方法在ActivityNet1.3和THUMOS14公开数据集上进行了广泛实验，并探索了人类异常检测中的TAL任务应用。</li>
<li>实验结果表明，该方法在单实例和多实例场景下均显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c28b4d56e719be49e7b23beb9e51bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a64c1df0179a3d48520ff1d1aed0ddc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dc5ab6a2751031974916ba1a71c7ffd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning"><a href="#Adaptive-Additive-Parameter-Updates-of-Vision-Transformers-for-Few-Shot-Continual-Learning" class="headerlink" title="Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning"></a>Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot   Continual Learning</h2><p><strong>Authors:Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</strong></p>
<p>Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods. </p>
<blockquote>
<p>在人工智能中，整合新类别信息而不丢失先前获得的知识仍然是一个核心挑战，通常被称为灾难性遗忘。少样本类别增量学习（FSCIL）通过首先在一个稳健的基础类别数据集上训练模型，然后在一系列连续的会话中仅使用每个新类别的少量标记示例来逐步适应模型来解决这个问题。然而，这种方法容易在新的有限数据上过拟合，这可能会损害整体性能并加剧遗忘。在这项工作中，我们提出了一种简单有效的FSCIL新框架，它利用冻结的Vision Transformer（ViT）主干并通过参数高效的加法更新进行增强。我们的方法冻结了预训练的ViT参数，并通过加法更新机制有选择地将可训练权重注入自注意力模块。这种设计只更新一小部分参数以适应新类别，同时不会牺牲基础会话期间学到的表示。通过微调有限数量的参数，我们的方法保留了冻结ViT中的可泛化特征，同时降低了过拟合的风险。此外，由于大多数参数保持不变，当引入少量新数据批次时，模型避免了覆盖先前学习的知识。在基准数据集上的大量实验表明，我们的方法相较于基线FSCIL方法达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08982v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个简单有效的Few-Shot Class Incremental Learning（FSCIL）框架，该框架利用冻结的Vision Transformer（ViT）主干并通过参数有效的更新机制进行增强。通过冻结预训练的ViT参数，并在自注意力模块中选择性注入可训练权重，该方法在适应新类别时仅更新一小部分参数，同时保留基础会话中学习到的表示。通过微调有限的参数，该方法在冻结的ViT中保留了可泛化的特征，并降低了过拟合的风险。此外，由于大部分参数保持不变，模型在引入少量新数据批次时避免了覆盖先前学习的知识。实验表明，该方法在基准数据集上的性能达到了先进水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Class Incremental Learning (FSCIL) 旨在解决人工智能中的灾难性遗忘问题，即在不失去先前学习知识的情况下整合新类别信息。</li>
<li>提出的FSCIL框架利用冻结的Vision Transformer (ViT) 主干，通过参数有效的更新机制进行增强。</li>
<li>框架通过冻结预训练的ViT参数并仅在自注意力模块中注入可训练权重来工作，以适应新类别。</li>
<li>该方法仅更新一小部分参数，以保留基础会话中学习到的表示，并在冻结的ViT中保留可泛化的特征。</li>
<li>通过微调有限的参数，该方法降低了过拟合的风险，并在引入新的少量数据批次时避免了覆盖先前学习的知识。</li>
<li>实验表明，该框架在基准数据集上的性能优于其他FSCIL方法，达到了先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-79a7a0018cff974282588ca6395ba110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97db49e7a982ebece3df7365ad3d4ac9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbf051031222750908d09c09d552775a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4bedfa457b37aa884645f3f627a78ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9fbbb4029c9d2a9adef6e5f9682398a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c297fb926cd4685cfbab548e234eccca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Frankenstein-Optimizer-Harnessing-the-Potential-by-Revisiting-Optimization-Tricks"><a href="#Frankenstein-Optimizer-Harnessing-the-Potential-by-Revisiting-Optimization-Tricks" class="headerlink" title="Frankenstein Optimizer: Harnessing the Potential by Revisiting   Optimization Tricks"></a>Frankenstein Optimizer: Harnessing the Potential by Revisiting   Optimization Tricks</h2><p><strong>Authors:Chia-Wei Hsu, Nien-Ti Tsou, Yu-Cheng Chen, Yang Jeong Park, Ju Li</strong></p>
<p>Gradient-based optimization drives the unprecedented performance of modern deep neural network models across diverse applications. Adaptive algorithms have accelerated neural network training due to their rapid convergence rates; however, they struggle to find &#96;&#96;flat minima” reliably, resulting in suboptimal generalization compared to stochastic gradient descent (SGD). By revisiting various adaptive algorithms’ mechanisms, we propose the Frankenstein optimizer, which combines their advantages. The proposed Frankenstein dynamically adjusts first- and second-momentum coefficients according to the optimizer’s current state to directly maintain consistent learning dynamics and immediately reflect sudden gradient changes. Extensive experiments across several research domains such as computer vision, natural language processing, few-shot learning, and scientific simulations show that Frankenstein surpasses existing adaptive algorithms and SGD empirically regarding convergence speed and generalization performance. Furthermore, this research deepens our understanding of adaptive algorithms through centered kernel alignment analysis and loss landscape visualization during the learning process. Code is available at <a target="_blank" rel="noopener" href="https://github.com/acctouhou/Frankenstein_optimizer">https://github.com/acctouhou/Frankenstein_optimizer</a> </p>
<blockquote>
<p>基于梯度的优化推动了现代深度神经网络模型在多种应用中的前所未有的性能。自适应算法由于快速的收敛率而加速了神经网络训练；然而，它们在可靠地找到“平坦最小值”方面遇到困难，导致与随机梯度下降（SGD）相比次优的泛化能力。我们通过对各种自适应算法的机制进行重新审视，提出了结合了它们优势的Frankenstein优化器。所提出的Frankenstein会根据优化器的当前状态动态调整第一和第二动量系数，以直接保持一致的学习动力并立即反映突然的梯度变化。在计算机视觉、自然语言处理、小样本学习和科学模拟等多个研究领域的广泛实验表明，Frankenstein在收敛速度和泛化性能上超越了现有的自适应算法和SGD。此外，本研究通过中心核对齐分析和学习过程中的损失景观可视化，深化了我们对自适应算法的理解。代码可在<a target="_blank" rel="noopener" href="https://github.com/acctouhou/Frankenstein_optimizer">https://github.com/acctouhou/Frankenstein_optimizer</a> 中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02147v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>现代深度神经网络模型的卓越性能得益于梯度优化技术的推动。虽然自适应算法因其快速收敛率加速了神经网络训练，但在寻找“平坦最小值”方面存在可靠性问题，导致泛化性能不及随机梯度下降（SGD）。研究者提出结合各种自适应算法优势的Frankenstein优化器，可根据当前状态动态调整一、二阶动量系数，维持稳定学习动态并快速响应梯度突变。跨多个领域如计算机视觉、自然语言处理、小样本学习和科学模拟的实验显示，Frankenstein在收敛速度和泛化性能上超越现有自适应算法和SGD。此外，该研究还通过中心核对齐分析和学习过程损失景观可视化深化了对自适应算法的理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>梯度优化是推动现代深度神经网络模型高性能的关键。</li>
<li>自适应算法虽加速神经网络训练，但在寻找平坦最小值方面存在挑战，影响泛化性能。</li>
<li>Frankenstein优化器结合多种自适应算法的优势，动态调整动量系数以维持稳定学习并响应梯度变化。</li>
<li>Frankenstein优化器在收敛速度和泛化性能上超越现有算法。</li>
<li>研究通过中心核对齐分析和损失景观可视化深化了对自适应算法的理解。</li>
<li>Frankenstein优化器适用于多个领域，如计算机视觉、自然语言处理、小样本学习和科学模拟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b9e9ab5a904aa0007fedd2e14ec278e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f7c204e8ed8a5540f1e19ad2da3e981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb4d75cf1c71f30d0be9b3c01bf832bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-130f6f1de819ebc8d78b2e84f39fcecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf29d02b99f37d815ff57863a802421f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46d9d5228003adf902685949489009b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f427b253f6ed3fce11eba9bc6ca843de.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Predicting-potentially-abusive-clauses-in-Chilean-terms-of-services-with-natural-language-processing"><a href="#Predicting-potentially-abusive-clauses-in-Chilean-terms-of-services-with-natural-language-processing" class="headerlink" title="Predicting potentially abusive clauses in Chilean terms of services with   natural language processing"></a>Predicting potentially abusive clauses in Chilean terms of services with   natural language processing</h2><p><strong>Authors:Christoffer Loeffler, Andrea Martínez Freile, Tomás Rey Pizarro</strong></p>
<p>This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and&#x2F;or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole. </p>
<blockquote>
<p>本研究关注消费者合同中信息不对称问题的日益加剧，这一问题因在线服务的普及及其复杂的服务条款（很少被阅读）而愈发严重。尽管已经开展了关于自动分析方法的研究，但问题是由于研究主要集中在英语机器学习方法以及欧洲联盟等主要司法管辖区而加剧。我们提出了一种新的方法和大量的数据集来解决这一差距。我们提出了一个包含四个类别和总共20个子类别的全新注释方案，并对其应用于智利使用的50项在线服务条款。我们对基于转换器的模型进行了评估，强调了语言特定和&#x2F;或领域特定的预训练、少量样本大小和模型架构等因素如何影响潜在滥用条款的检测和分类。结果显示，不同任务和模型的性能存在很大差异，检测任务的最高宏F1分数范围从79%到89%，微观F1分数高达96%，而分类任务的宏F1分数范围从60%到70%，微观F1分数从64%到80%。值得注意的是，这是第一个用于法律条款的西班牙语多标签分类数据集，它适用于智利法律，并对西班牙语模型在法律领域进行了全面评估。我们的工作为未来的方法开发研究奠定了基础，该研究对于很少考虑的法律分析具有实际意义，并有望为消费者在智利乃至整个拉丁美洲提供支持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.00865v2">PDF</a> 39 pages, 2 figures, 8 tables, accepted for publication</p>
<p><strong>Summary</strong></p>
<p>本文关注消费者合同中信息不对称问题的日益严重，特别是在线服务协议的普及加剧了这一问题。研究介绍了新的方法论和大量数据集来解决这一差距，并提出了新的标注方案和四个类别下的二十个子类别。通过对智利使用的五十项在线服务协议的应用评估，发现语言特定和领域特定的预训练、少量样本大小和模型架构等因素会影响潜在滥用条款的检测和分类。研究结果显示，不同任务和模型的性能存在很大差异，检测任务的最高宏观F1分数在79%到89%之间，微观F1分数高达96%，而分类任务的宏观F1分数在60%到70%之间，微观F1分数在64%到80%之间。这为未来法律分析方法的发展提供了基础，并有望支持智利和拉丁美洲的消费者实际应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究关注消费者合同中信息不对称问题，特别是复杂的在线服务协议。</li>
<li>提出了新的方法论和标注方案来解决这一问题，数据集涵盖智利使用的在线服务协议。</li>
<li>研究评估了基于转换器的模型，发现语言特定和领域特定的预训练对检测潜在滥用条款至关重要。</li>
<li>不同任务和模型的性能差异显著，检测任务的F1分数较高。</li>
<li>这是首个针对法律条款的西班牙语多标签分类数据集，应用智利法律并全面评估西班牙语模型在法律领域的表现。</li>
<li>研究为未来法律分析方法的发展奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.00865">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b71b19a998e618d4537842c8733ba790.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Unlocking-Transfer-Learning-for-Open-World-Few-Shot-Recognition"><a href="#Unlocking-Transfer-Learning-for-Open-World-Few-Shot-Recognition" class="headerlink" title="Unlocking Transfer Learning for Open-World Few-Shot Recognition"></a>Unlocking Transfer Learning for Open-World Few-Shot Recognition</h2><p><strong>Authors:Byeonggeun Kim, Juntae Lee, Kyuhong Shim, Simyung Chang</strong></p>
<p>Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR. </p>
<blockquote>
<p>少样本开放集识别（FSOSR）旨在应对现实世界中的一项关键挑战，旨在将输入分类为已知类别（称为封闭集类别），同时识别不属于这些类别的开放集输入。虽然迁移学习已成为封闭世界中一种突出的范式，其中模型被调整到给定的少样本任务，但我们发现它无法扩展到开放世界。为了应对这一挑战，我们提出了一种两阶段的方法，包括开放集感知元学习与开放集自由迁移学习。在开放集感知元学习阶段，模型被训练以建立度量空间，作为后续阶段的有益起点。在开放集自由迁移学习阶段，模型通过迁移学习进一步适应特定的目标任务。此外，我们还介绍了一种通过修改训练数据集或生成伪开放集示例来模拟开放集示例的策略。所提出的方法在广泛认可的miniImageNet和tieredImageNet基准测试上实现了最新性能，仅增加了1.5%的训练努力。我们的工作证明了迁移学习在FSOSR中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09986v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Few-Shot Open-Set Recognition（FSOSR）的目标与挑战。它旨在将输入分类为已知类别（称为封闭集类别），同时识别不属于这些类别的开放集输入。虽然迁移学习已成为封闭世界中的主流范式，但在开放世界中却存在局限性。为此，研究者提出了一种两阶段的方法，包括开放集感知元学习与开放集自由迁移学习。在开放集感知元学习阶段，模型被训练以建立度量空间，作为后续阶段的起点。在开放集自由迁移学习阶段，模型通过迁移学习进一步适应特定目标任务。此外，通过修改训练数据集或生成伪开放集示例来模拟开放集示例的策略也被引入。该方法在miniImageNet和tieredImageNet两个广泛认可的基准测试上实现了最佳性能，仅增加了1.5%的训练努力。研究证明了迁移学习在FSOSR中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Open-Set Recognition (FSOSR) 旨在解决将输入分类为已知类别并识别开放集输入的问题。</li>
<li>迁移学习在封闭世界中很受欢迎，但在开放世界中存在局限性。</li>
<li>提出的两阶段方法包括开放集感知元学习和开放集自由迁移学习。</li>
<li>在开放集感知元学习阶段，模型建立度量空间作为后续阶段的起点。</li>
<li>开放集自由迁移学习阶段使模型适应特定目标任务。</li>
<li>通过模拟开放集示例的策略来提高模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09986">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-04d22293ff9aea26d02ac9843d9ab84c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9b8876c163a6403e1b09941030239e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f254cd586bc625ab4300056c60a0892.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80f34076d4f58c14d0eb1c4f4399c1a0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4da8151e6c828851e11c0a4b21947103.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-08  Reinforced Correlation Between Vision and Language for Precise Medical   AI Assistant
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-245e2d5d70913e83da6eecd733b7086f.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-05-08  Multi-Agent System for Comprehensive Soccer Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
