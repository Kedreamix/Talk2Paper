<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-11  Audio-Sync Video Generation with Multi-Stream Temporal Control">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-74a4a1d64c3d2d0240ec91bd5e838ab6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    56 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-11-更新"><a href="#2025-06-11-更新" class="headerlink" title="2025-06-11 更新"></a>2025-06-11 更新</h1><h2 id="Audio-Sync-Video-Generation-with-Multi-Stream-Temporal-Control"><a href="#Audio-Sync-Video-Generation-with-Multi-Stream-Temporal-Control" class="headerlink" title="Audio-Sync Video Generation with Multi-Stream Temporal Control"></a>Audio-Sync Video Generation with Multi-Stream Temporal Control</h2><p><strong>Authors:Shuchen Weng, Haojie Zheng, Zheng Chang, Si Li, Boxin Shi, Xinlong Wang</strong></p>
<p>Audio is inherently temporal and closely synchronized with the visual world, making it a naturally aligned and expressive control signal for controllable video generation (e.g., movies). Beyond control, directly translating audio into video is essential for understanding and visualizing rich audio narratives (e.g., Podcasts or historical recordings). However, existing approaches fall short in generating high-quality videos with precise audio-visual synchronization, especially across diverse and complex audio types. In this work, we introduce MTV, a versatile framework for audio-sync video generation. MTV explicitly separates audios into speech, effects, and music tracks, enabling disentangled control over lip motion, event timing, and visual mood, respectively – resulting in fine-grained and semantically aligned video generation. To support the framework, we additionally present DEMIX, a dataset comprising high-quality cinematic videos and demixed audio tracks. DEMIX is structured into five overlapped subsets, enabling scalable multi-stage training for diverse generation scenarios. Extensive experiments demonstrate that MTV achieves state-of-the-art performance across six standard metrics spanning video quality, text-video consistency, and audio-video alignment. Project page: <a target="_blank" rel="noopener" href="https://hjzheng.net/projects/MTV/">https://hjzheng.net/projects/MTV/</a>. </p>
<blockquote>
<p>音频本质上具有时间性，并与视觉世界紧密同步，使其成为可控视频生成（例如电影）的自然对齐和表达控制信号。除了控制之外，直接将音频翻译成视频对于理解和可视化丰富的音频叙事（例如播客或历史记录）至关重要。然而，现有方法在生成具有精确音视频同步的高质量视频方面存在不足，尤其是在跨越多样和复杂的音频类型时。在这项工作中，我们介绍了MTV，这是一个用于音频同步视频生成的通用框架。MTV明确地将音频分离为语音、音效和音乐轨道，实现对嘴唇运动、事件时间和视觉情绪的分离控制，从而实现了精细且语义对齐的视频生成。为了支持该框架，我们还推出了DEMIX，这是一个包含高质量电影视频和混音音频轨道的数据集。DEMIX被结构化分为五个重叠的子集，能够实现可扩展的多阶段训练，以适应多种生成场景。大量实验表明，MTV在六项标准指标上达到了卓越的性能，涵盖了视频质量、文本与视频的一致性、音频与视频的对齐。项目页面：<a target="_blank" rel="noopener" href="https://hjzheng.net/projects/MTV/%E3%80%82">https://hjzheng.net/projects/MTV/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08003v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>音频与视频生成之间的紧密关联表现在其同步性及时效性上，MTV框架可对音频进行细致的分类转化生成高质量视频，并保证精准的音视频同步。通过分离音频的语音、音效和音乐轨迹，MTV可实现精细控制，如唇部动作、事件时序和视觉氛围等。此外，为支持MTV框架，我们推出了DEMIX数据集，包含高质量电影视频和混合音频轨迹。DEMIX数据集的五个重叠子集为各种生成场景提供了可扩展的多阶段训练功能。大量实验证明，MTV在视频质量、文本视频一致性以及音视频对齐等方面均达到业界领先水平。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>音频和视频天然关联且需要保持同步性，表现在通过音频生成高质量视频的紧密联系。</li>
<li>MTV框架可实现高质量的音视频同步生成，提升音视频质量和用户体验。</li>
<li>MTV通过分离音频轨迹实现对唇部动作、事件时序和视觉氛围的精细控制。</li>
<li>DEMIX数据集支持MTV框架，包含高质量电影视频和混合音频轨迹。</li>
<li>DEMIX数据集具有五个重叠子集，为不同生成场景提供灵活多变的数据支持。</li>
<li>MTV在视频质量、文本视频一致性以及音视频对齐等方面表现优异，达到业界领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-79707e07d8adbb38786e4cdbfdef263d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da5064913d5a52f676e29d52c7f6b253.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-666adefbb36a56a4b37ca19fe3079633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6961f05c5ed9d7a69cb306fc07add964.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transcript-Prompted-Whisper-with-Dictionary-Enhanced-Decoding-for-Japanese-Speech-Annotation"><a href="#Transcript-Prompted-Whisper-with-Dictionary-Enhanced-Decoding-for-Japanese-Speech-Annotation" class="headerlink" title="Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation"></a>Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation</h2><p><strong>Authors:Rui Hu, Xiaolong Lin, Jiawang Liu, Shixi Huang, Zhenpeng Zhan</strong></p>
<p>In this paper, we propose a method for annotating phonemic and prosodic labels on a given audio-transcript pair, aimed at constructing Japanese text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale pre-trained automatic speech recognition (ASR) model, conditioned on ground truth transcripts, to simultaneously output phrase-level graphemes and annotation labels. To further correct errors in phonemic labeling, we employ a decoding strategy that utilizes dictionary prior knowledge. The objective evaluation results demonstrate that our proposed method outperforms previous approaches relying solely on text or audio. The subjective evaluation results indicate that the naturalness of speech synthesized by the TTS model, trained with labels annotated using our method, is comparable to that of a model trained with manual annotations. </p>
<blockquote>
<p>在这篇论文中，我们提出了一种对给定音频转录对进行音素和语调标签标注的方法，旨在构建日本语音文本（TTS）数据集。我们的方法涉及对大规模预训练自动语音识别（ASR）模型进行微调，该模型基于真实转录本，以同时输出短语级别的字母和注释标签。为了进一步纠正音素标注中的错误，我们采用了一种利用词典先验知识的解码策略。目标评价结果证明，我们的方法优于仅依赖文本或音频的以前的方法。主观评价结果表明，使用我们的方法进行标注训练的TTS模型合成的语音自然度与手动标注训练的模型相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07646v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种对给定音频-文本对进行音素和韵律标签标注的方法，旨在构建日语文本转语音（TTS）数据集。通过微调大规模预训练自动语音识别（ASR）模型，结合真实文本转录本同时输出词组层面的字元和注释标签。为修正音素标注中的错误，我们采用了一种利用词典先验知识的解码策略。客观评估结果表明，我们的方法优于仅依赖文本或音频的先前方法。主观评估结果表明，使用我们的方法进行标签注释训练的TTS模型合成的语音自然度与手动注释训练的模型相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个为日语文本转语音（TTS）数据集标注音素和韵律标签的方法。</li>
<li>通过微调大规模预训练的自动语音识别（ASR）模型来实现标注。</li>
<li>结合真实文本转录本同时输出词组层面的字元和注释标签。</li>
<li>采用利用词典先验知识的解码策略来修正音素标注中的错误。</li>
<li>客观评估显示，该方法优于其他仅依赖文本或音频的方法。</li>
<li>主观评估显示，使用此方法标注训练的TTS模型合成的语音自然度与手动注释训练的模型相似。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-455272088a4ad9824a3d088068471bfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b099872c6cb285ec27fe64a2d312d47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-676e46a26e8ac244d8e02ac8c012653f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c44e77a2bd8effc16cc374eee9236a66.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Speaker-Distinguishable-CTC-Learning-Speaker-Distinction-Using-CTC-for-Multi-Talker-Speech-Recognition"><a href="#Speaker-Distinguishable-CTC-Learning-Speaker-Distinction-Using-CTC-for-Multi-Talker-Speech-Recognition" class="headerlink" title="Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for   Multi-Talker Speech Recognition"></a>Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for   Multi-Talker Speech Recognition</h2><p><strong>Authors:Asahi Sakuma, Hiroaki Sato, Ryuga Sugano, Tadashi Kumano, Yoshihiko Kawai, Tetsuji Ogawa</strong></p>
<p>This paper presents a novel framework for multi-talker automatic speech recognition without the need for auxiliary information. Serialized Output Training (SOT), a widely used approach, suffers from recognition errors due to speaker assignment failures. Although incorporating auxiliary information, such as token-level timestamps, can improve recognition accuracy, extracting such information from natural conversational speech remains challenging. To address this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension of CTC that jointly assigns a token and its corresponding speaker label to each frame. We further integrate SD-CTC into the SOT framework, enabling the SOT model to learn speaker distinction using only overlapping speech and transcriptions. Experimental comparisons show that multi-task learning with SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves performance comparable to state-of-the-art methods relying on auxiliary information. </p>
<blockquote>
<p>本文提出了一种无需辅助信息的多说话者自动语音识别的新框架。序列输出训练（SOT）是一种广泛使用的方法，但由于说话人分配失败而导致识别错误。虽然结合辅助信息（如令牌级时间戳）可以提高识别精度，但从自然对话语音中提取此类信息仍然具有挑战性。为了解决这一局限性，我们提出了Speaker-Distinguishable CTC（SD-CTC），它是CTC的一种扩展，可以联合为每一帧分配令牌及其相应的说话人标签。我们将SD-CTC进一步集成到SOT框架中，使SOT模型仅使用重叠语音和转录来学习说话人区分。实验比较表明，使用SD-CTC和SOT进行多任务学习降低了SOT模型的错误率26%，并实现了与依赖辅助信息的最新技术相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07515v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文提出了一种无需辅助信息的多说话人自动语音识别的新框架。针对序列化输出训练（SOT）中因说话人分配失败导致的识别错误问题，提出了Speaker-Distinguishable CTC（SD-CTC）方法。该方法将每个帧的令牌及其对应的说话人标签一起分配，并集成到SOT框架中，使SOT模型能够仅利用重叠语音和转录来学习说话人区分。实验比较显示，多任务学习与SD-CTC和SOT相结合，将SOT模型的错误率降低了26%，并实现了与依赖辅助信息的最新方法相当的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了无需辅助信息的多说话人自动语音识别新框架。</li>
<li>序列化输出训练（SOT）存在说话人分配失败的识别错误问题。</li>
<li>Speaker-Distinguishable CTC（SD-CTC）方法被提出以解决这个问题，可以每个帧分配令牌及其对应的说话人标签。</li>
<li>SD-CTC被集成到SOT框架中，使模型能利用重叠语音和转录来学习说话人区分。</li>
<li>实验显示多任务学习与SD-CTC和SOT结合能显著降低SOT模型的错误率。</li>
<li>该方法实现了与依赖辅助信息的最新方法相当的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07515">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ca6ac77ab5a5d4c080cce26a98eb8ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc36dbe0b5d2dc9a338c7b7e7d6c4082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8c9896e50efeb74f6cbea353fa0e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83cf4064530fdeca549dc2fb98f908e6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeRAGEC-Denoising-Named-Entity-Candidates-with-Synthetic-Rationale-for-ASR-Error-Correction"><a href="#DeRAGEC-Denoising-Named-Entity-Candidates-with-Synthetic-Rationale-for-ASR-Error-Correction" class="headerlink" title="DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for   ASR Error Correction"></a>DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for   ASR Error Correction</h2><p><strong>Authors:Solee Im, Wonjun Lee, Jinmyeong An, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee</strong></p>
<p>We present DeRAGEC, a method for improving Named Entity (NE) correction in Automatic Speech Recognition (ASR) systems. By extending the Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC employs synthetic denoising rationales to filter out noisy NE candidates before correction. By leveraging phonetic similarity and augmented definitions, it refines noisy retrieved NEs using in-context learning, requiring no additional training. Experimental results on CommonVoice and STOP datasets show significant improvements in Word Error Rate (WER) and NE hit ratio, outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28% relative reduction in WER compared to ASR without postprocessing. Our source code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/solee0022/deragec">https://github.com/solee0022/deragec</a> </p>
<blockquote>
<p>我们提出了DeRAGEC方法，用于改进自动语音识别（ASR）系统中的命名实体（NE）校正。通过扩展基于检索的增强生成式错误校正（RAGEC）框架，DeRAGEC采用合成降噪理由来过滤校正前的嘈杂NE候选词。通过利用语音相似性并增强定义，它利用上下文学习对嘈杂的检索到的NE进行改进，无需额外训练。在CommonVoice和STOP数据集上的实验结果显示，词错误率（WER）和NE命中率都有显著提高，超过了基线ASR和RAGEC方法。具体来说，与没有后处理的ASR相比，我们实现了相对减少28%的WER。我们的源代码公开在：<a target="_blank" rel="noopener" href="https://github.com/solee0022/deragec">https://github.com/solee0022/deragec</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07510v1">PDF</a> ACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DeRAGEC方法，该方法用于提高自动语音识别（ASR）系统中的命名实体（NE）校正效果。它通过扩展基于检索的辅助生成式错误校正（RAGEC）框架，采用合成降噪理由来过滤出命名实体识别中的噪声候选词，再进行校正。利用语音相似性并扩充定义，该方法在不进行额外训练的情况下，通过上下文学习对噪声检索的命名实体进行精炼。在CommonVoice和STOP数据集上的实验结果显示，该方法在单词错误率（WER）和命名实体命中率方面有了显著提高，超越了基线ASR和RAGEC方法。具体来说，与未进行后处理的ASR相比，我们实现了相对降低28%的WER。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeRAGEC方法扩展了RAGEC框架以提高ASR系统中的命名实体识别效果。</li>
<li>通过合成降噪理由，DeRAGEC能够过滤出命名实体识别中的噪声候选词。</li>
<li>利用语音相似性和扩充定义，DeRAGEC能够精炼噪声检索的命名实体。</li>
<li>该方法在不进行额外训练的情况下，通过上下文学习提高命名实体的识别准确性。</li>
<li>在CommonVoice和STOP数据集上的实验结果显示，DeRAGEC显著提高了单词错误率（WER）和命名实体命中率。</li>
<li>DeRAGEC相对于基线ASR和RAGEC方法表现出更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-363d69e1a117067bf71b39eb14f09ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e47785556229af4a90776fa0519f1e81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae0e454dab8fcebb110d07206c0865ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91c2566a4604e61cf0db7382669e9082.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09df6138c267854f4aabc6aea0061bda.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hybrid-Vision-Transformer-Mamba-Framework-for-Autism-Diagnosis-via-Eye-Tracking-Analysis"><a href="#Hybrid-Vision-Transformer-Mamba-Framework-for-Autism-Diagnosis-via-Eye-Tracking-Analysis" class="headerlink" title="Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via   Eye-Tracking Analysis"></a>Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via   Eye-Tracking Analysis</h2><p><strong>Authors:Wafaa Kasri, Yassine Himeur, Abigail Copiaco, Wathiq Mansoor, Ammar Albanna, Valsamma Eapen</strong></p>
<p>Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early intervention. This study presents a hybrid deep learning framework combining Vision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking data. The model uses attention-based fusion to integrate visual, speech, and facial cues, capturing both spatial and temporal dynamics. Unlike traditional handcrafted methods, it applies state-of-the-art deep learning and explainable AI techniques to enhance diagnostic accuracy and transparency. Tested on the Saliency4ASD dataset, the proposed ViT-Mamba model outperformed existing methods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94 specificity. These findings show the model’s promise for scalable, interpretable ASD screening, especially in resource-constrained or remote clinical settings where access to expert diagnosis is limited. </p>
<blockquote>
<p>准确诊断自闭症谱系障碍（ASD）对于早期干预至关重要。本研究提出了一种结合视觉Transformer（ViT）和视觉加速器（Vision Mamba）的混合深度学习框架，用于使用眼动数据检测ASD。该模型使用基于注意力的融合技术，整合视觉、语音和面部线索，捕捉空间和时间的动态。与传统的手工方法不同，它采用最先进的深度学习和可解释的AI技术，以提高诊断的准确性和透明度。在Saliency4ASD数据集上进行的测试表明，所提出的ViT-Mamba模型优于现有方法，实现了0.96的准确率、0.95的F1分数、0.97的灵敏度和0.94的特异性。这些发现表明该模型在可扩展和可解释的ASD筛查方面显示出巨大潜力，特别是在资源受限或远程临床环境中，专业诊断的获取受到限制的情况下。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06886v1">PDF</a> 7 pages, 4 figures and 2 tables</p>
<p><strong>Summary</strong>：<br>本研究提出一种结合Vision Transformers（ViT）和Vision Mamba的深度学习框架，利用眼动数据对自闭症谱系障碍（ASD）进行准确诊断。该模型通过注意力融合机制整合视觉、语音和面部线索，捕捉空间和时间的动态变化。相比传统的手工方法，它采用先进的深度学习和可解释的AI技术提高诊断的准确性和透明度。在Saliency4ASD数据集上的测试表明，所提出的ViT-Mamba模型优于现有方法，准确率、F1分数、灵敏度和特异性分别达到了0.96、0.95、0.97和0.94。这一模型在可扩展性和可解释性方面展现出潜力，尤其是在资源有限或远程临床环境中，可为缺乏专家诊断的地区提供便利。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>本研究利用深度学习框架结合Vision Transformers（ViT）和Vision Mamba技术，旨在提高自闭症谱系障碍（ASD）诊断的准确性。</li>
<li>模型通过注意力融合机制整合视觉、语音和面部线索，以捕捉空间和时间动态。</li>
<li>相比传统的手工方法，该模型采用先进的深度学习和可解释的AI技术，增强了诊断的透明度和准确性。</li>
<li>在Saliency4ASD数据集上的测试表明，ViT-Mamba模型表现优异，具有较高的准确率、F1分数、灵敏度和特异性。</li>
<li>此模型对于资源有限的地区或远程临床环境特别有用，可作为一种可扩展且可解释性强的小儿自闭症筛查工具。</li>
<li>此模型在自闭症诊断中的应用前景广阔，有望推动相关领域的进一步发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0231dd14650aeca284645478f560245b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6a4540b58d9eab87b6bf707ab66d3b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bf196fe44404486570b10899589ebd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf01a37d7d1f15f8dd71e735cbd37913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74a4a1d64c3d2d0240ec91bd5e838ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-575c54e7f14e1066b72fb90f511400fa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="They-want-to-pretend-not-to-understand-The-Limits-of-Current-LLMs-in-Interpreting-Implicit-Content-of-Political-Discourse"><a href="#They-want-to-pretend-not-to-understand-The-Limits-of-Current-LLMs-in-Interpreting-Implicit-Content-of-Political-Discourse" class="headerlink" title="They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse"></a>They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse</h2><p><strong>Authors:Walter Paci, Alessandro Panunzi, Sandro Pezzelle</strong></p>
<p>Implicit content plays a crucial role in political discourse, where speakers systematically employ pragmatic strategies such as implicatures and presuppositions to influence their audiences. Large Language Models (LLMs) have demonstrated strong performance in tasks requiring complex semantic and pragmatic understanding, highlighting their potential for detecting and explaining the meaning of implicit content. However, their ability to do this within political discourse remains largely underexplored. Leveraging, for the first time, the large IMPAQTS corpus, which comprises Italian political speeches with the annotation of manipulative implicit content, we propose methods to test the effectiveness of LLMs in this challenging problem. Through a multiple-choice task and an open-ended generation task, we demonstrate that all tested models struggle to interpret presuppositions and implicatures. We conclude that current LLMs lack the key pragmatic capabilities necessary for accurately interpreting highly implicit language, such as that found in political discourse. At the same time, we highlight promising trends and future directions for enhancing model performance. We release our data and code at <a target="_blank" rel="noopener" href="https://github.com/WalterPaci/IMPAQTS-PID">https://github.com/WalterPaci/IMPAQTS-PID</a> </p>
<blockquote>
<p>隐晦内容在政治话语中发挥着至关重要的作用，演讲者系统地采用语用策略，如隐含意义和预设，来影响他们的听众。大型语言模型（LLM）在需要复杂语义和语用理解的任务中表现出了强大的性能，突显了它们在检测和解释隐晦内容意义方面的潜力。然而，它们在政治话语中执行此任务的能力在很大程度上尚未被探索。我们首次利用大型的IMPAQTS语料库，该语料库包含带有操纵性隐晦内容的意大利政治演讲注释，我们提出了测试LLM在此难题中的有效性的方法。通过多项选择题和开放式生成任务，我们证明所有测试过的模型在解释预设和隐含意义方面都存在困难。我们得出结论，当前的大型语言模型缺乏准确解释高度隐晦语言（如政治话语中的语言）所需的关键语用能力。同时，我们强调了提高模型性能的可行趋势和未来方向。我们在<a target="_blank" rel="noopener" href="https://github.com/WalterPaci/IMPAQTS-PID">https://github.com/WalterPaci/IMPAQTS-PID</a>上发布了我们的数据和代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06775v1">PDF</a> Accepted to the ACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>文本指出隐式内容在政治话语中起到关键作用，政治家们使用诸如隐含义和预设等语用策略来影响听众。大型语言模型在需要复杂语义和语用理解的任务中表现出强大性能，具有检测和解释隐式内容含义的潜力。然而，其在政治话语中的能力尚未得到充分探索。研究首次使用包含意大利政治演讲和操纵性隐式内容注解的IMPAQTS语料库，测试了大型语言模型在处理这一难题时的有效性。通过多项选择题和开放式生成任务发现所有测试模型在解释预设和隐含义方面存在困难。研究认为当前的大型语言模型缺乏准确解释政治话语中高度隐晦语言的关键语用能力。同时，该研究还指出了提升模型性能的潜在趋势和未来方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>隐式内容在政治话语中起关键作用，政治家利用隐含义和预设等策略影响听众。</li>
<li>大型语言模型在复杂的语义和语用理解任务中表现出强大的性能。</li>
<li>大型语言模型具有检测和解释隐式内容含义的潜力，但在政治话语中的能力尚未充分探索。</li>
<li>利用IMPAQTS语料库对大型语言模型处理政治话语中的隐式内容进行了测试。</li>
<li>在多项选择题和开放式生成任务中发现，现有模型在解释政治话语中的预设和隐含义方面存在困难。</li>
<li>当前的大型语言模型缺乏准确解释高度隐晦语言的语用能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9f99dd89d35b55c6358aacede1676926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fffa1afea654369ed02047598737791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dc26dd2911398e6befc68b406480bb9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LitMAS-A-Lightweight-and-Generalized-Multi-Modal-Anti-Spoofing-Framework-for-Biometric-Security"><a href="#LitMAS-A-Lightweight-and-Generalized-Multi-Modal-Anti-Spoofing-Framework-for-Biometric-Security" class="headerlink" title="LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing   Framework for Biometric Security"></a>LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing   Framework for Biometric Security</h2><p><strong>Authors:Nidheesh Gorthi, Kartik Thakral, Rishabh Ranjan, Richa Singh, Mayank Vatsa</strong></p>
<p>Biometric authentication systems are increasingly being deployed in critical applications, but they remain susceptible to spoofing. Since most of the research efforts focus on modality-specific anti-spoofing techniques, building a unified, resource-efficient solution across multiple biometric modalities remains a challenge. To address this, we propose LitMAS, a $\textbf{Li}$gh$\textbf{t}$ weight and generalizable $\textbf{M}$ulti-modal $\textbf{A}$nti-$\textbf{S}$poofing framework designed to detect spoofing attacks in speech, face, iris, and fingerprint-based biometric systems. At the core of LitMAS is a Modality-Aligned Concentration Loss, which enhances inter-class separability while preserving cross-modal consistency and enabling robust spoof detection across diverse biometric traits. With just 6M parameters, LitMAS surpasses state-of-the-art methods by $1.36%$ in average EER across seven datasets, demonstrating high efficiency, strong generalizability, and suitability for edge deployment. Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/IAB-IITJ/LitMAS">https://github.com/IAB-IITJ/LitMAS</a>. </p>
<blockquote>
<p>生物识别认证系统正在越来越多地应用于关键应用中，但它们仍然容易受到欺骗攻击。由于大多数研究工作都集中在针对特定模态的反欺骗技术上，因此在多个生物识别模态上构建统一、资源高效的解决方案仍然是一个挑战。为了解决这个问题，我们提出了LitMAS，这是一个轻量级且可推广的多模态反欺骗框架，旨在检测语音、面部、虹膜和指纹等基于生物识别的系统中的欺骗攻击。LitMAS的核心是模态对齐浓度损失，这增强了类间可分性，同时保持了跨模态的一致性，并实现了各种生物特征上的稳健欺骗检测。LitMAS仅有6M参数，在七个数据集上的平均EER超出最新方法1.36%，证明了其高效性、强通用性和边缘部署的适用性。代码和训练模型可在<a target="_blank" rel="noopener" href="https://github.com/IAB-IITJ/LitMAS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IAB-IITJ/LitMAS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06759v1">PDF</a> Accepted in Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种轻量级、多模态的反欺骗框架LitMAS，用于检测语音、面部、虹膜和指纹等生物识别系统中的欺骗攻击。该框架通过模态对齐浓度损失技术，提高了不同生物特征之间的跨模态一致性，实现了高效、通用性强和适合边缘部署的反欺骗检测。在七个数据集上的平均错误接受率比现有技术高出1.36%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LitMAS是一个轻量级、多模态的反欺骗框架，旨在检测多种生物识别系统中的欺骗攻击。</li>
<li>该框架结合了模态对齐浓度损失技术，增强了不同生物特征之间的跨模态一致性。</li>
<li>LitMAS框架在七个数据集上的平均错误接受率比现有技术高出1.36%，显示出其高效性和优越性。</li>
<li>该框架具有强大的通用性，适用于各种生物识别模态，包括语音、面部、虹膜和指纹等。</li>
<li>LitMAS框架的代码和训练模型已经公开可用，方便其他研究者使用和进一步开发。</li>
<li>该框架特别适合在边缘设备进行部署，具有较低的参数需求和资源占用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4008e2f4bbf972601fa5f20548b051bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-453d6d5df3e04d204388a74dc050871b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bae5a4c219d50dc5001ab46f1603c02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372417e9f2dc526b0b94fc48563e4418.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55cca83a17b8587972d8c9d16def798f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Length-Generalization-For-Transformer-based-Speech-Enhancement"><a href="#Exploring-Length-Generalization-For-Transformer-based-Speech-Enhancement" class="headerlink" title="Exploring Length Generalization For Transformer-based Speech Enhancement"></a>Exploring Length Generalization For Transformer-based Speech Enhancement</h2><p><strong>Authors:Qiquan Zhang, Hongxu Zhu, Xinyuan Qian, Eliathamby Ambikairajah, Haizhou Li</strong></p>
<p>Transformer network architecture has proven effective in speech enhancement. However, as its core module, self-attention suffers from quadratic complexity, making it infeasible for training on long speech utterances. In practical scenarios, speech enhancement models are often required to perform on noisy speech at run-time that is substantially longer than the training utterances. It remains a challenge how a Transformer-based speech enhancement model can generalize to long speech utterances. In this paper, extensive empirical studies are conducted to explore the model’s length generalization ability. In particular, we conduct speech enhancement experiments on four training objectives and evaluate with five metrics. Our studies establish that positional encoding is an effective instrument to dampen the effect of utterance length on speech enhancement. We first explore several existing positional encoding methods, and the results show that relative positional encoding methods exhibit a better length generalization property than absolute positional encoding methods. Additionally, we also explore a simpler and more effective positional encoding scheme, i.e. LearnLin, that uses only one trainable parameter for each attention head to scale the real relative position between time frames, which learns the different preferences on short- or long-term dependencies of these heads. The results demonstrate that our proposal exhibits excellent length generalization ability with comparable or superior performance than other state-of-the-art positional encoding strategies. </p>
<blockquote>
<p>Transformer网络架构在语音增强方面已经证明是有效的。然而，作为其核心模块，自注意力机制存在二次复杂性，使得它对长语音片段的训练变得不可行。在实际场景中，语音增强模型往往需要在运行时对噪声语音进行增强处理，这些语音明显长于训练时的片段。基于Transformer的语音增强模型如何推广到长语音片段仍然是一个挑战。本文进行了大量的实证研究，以探索模型对长语音片段的泛化能力。特别是，我们在四个训练目标上进行了语音增强实验，并用五个指标进行了评估。我们的研究表明，位置编码是减弱语音片段长度对语音增强效果影响的有效工具。我们首先探索了现有的几种位置编码方法，结果表明相对位置编码方法具有更好的长度泛化属性。此外，我们还探索了一种更简单有效的位置编码方案，即LearnLin。它为每个注意力头只使用一个可训练参数来缩放时间帧之间的真实相对位置，学习这些头对短期或长期依赖的不同偏好。结果表明，我们的方案具有出色的长度泛化能力，与其他先进的位置编码策略相比，具有可比或更优越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06697v1">PDF</a> 14 pages; Accepted by TASLP</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Transformer网络架构在语音增强中的有效性，但其核心模块自注意力的二次复杂性限制了其在长语音片段上的应用。为应对此挑战，本文进行了大量实证研究，探索了模型的长度泛化能力。研究结果显示，位置编码有助于降低语音长度对增强效果的影响。本文探讨了多种位置编码方法，发现相对位置编码展现出更好的泛化性能。此外，还提出了一种更简单有效的位置编码方案LearnLin，仅使用一个可训练参数，即可实现对时间帧间真实相对位置的缩放，并展现出卓越的长度泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer网络在语音增强中表现出良好的效果。</li>
<li>自注意力的二次复杂性限制了其在长语音片段的应用。</li>
<li>位置编码有助于提升模型的长度泛化能力。</li>
<li>相对位置编码方法相较于绝对位置编码方法展现出更好的泛化性能。</li>
<li>LearnLin是一种简单有效的位置编码方案，能实现对时间帧间真实相对位置的缩放。</li>
<li>LearnLin与其他先进的位置编码策略相比，展现出卓越的长度泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06697">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1a36520be644d7423eb795ba771c18a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-026aa6408de56c0679c70b9fbc7821b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faa82c923a2b526cec88f45cf53b6570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05ec20d30caef9809a0e87022b6082ff.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Fast-and-Lightweight-Model-for-Causal-Audio-Visual-Speech-Separation"><a href="#A-Fast-and-Lightweight-Model-for-Causal-Audio-Visual-Speech-Separation" class="headerlink" title="A Fast and Lightweight Model for Causal Audio-Visual Speech Separation"></a>A Fast and Lightweight Model for Causal Audio-Visual Speech Separation</h2><p><strong>Authors:Wendi Sang, Kai Li, Runxuan Yang, Jianqiang Huang, Xiaolin Hu</strong></p>
<p>Audio-visual speech separation (AVSS) aims to extract a target speech signal from a mixed signal by leveraging both auditory and visual (lip movement) cues. However, most existing AVSS methods exhibit complex architectures and rely on future context, operating offline, which renders them unsuitable for real-time applications. Inspired by the pipeline of RTFSNet, we propose a novel streaming AVSS model, named Swift-Net, which enhances the causal processing capabilities required for real-time applications. Swift-Net adopts a lightweight visual feature extraction module and an efficient fusion module for audio-visual integration. Additionally, Swift-Net employs Grouped SRUs to integrate historical information across different feature spaces, thereby improving the utilization efficiency of historical information. We further propose a causal transformation template to facilitate the conversion of non-causal AVSS models into causal counterparts. Experiments on three standard benchmark datasets (LRS2, LRS3, and VoxCeleb2) demonstrated that under causal conditions, our proposed Swift-Net exhibited outstanding performance, highlighting the potential of this method for processing speech in complex environments. </p>
<blockquote>
<p>视听语音分离（AVSS）旨在利用听觉和视觉（唇部动作）线索从混合信号中提取目标语音信号。然而，大多数现有的AVSS方法具有复杂的架构，并且依赖于未来上下文进行离线操作，这使得它们不适合实时应用。受RTFSNet流程的启发，我们提出了一种新型的流式AVSS模型，名为Swift-Net，它增强了实时应用所需的因果处理能力。Swift-Net采用轻量级的视觉特征提取模块和高效的视听融合模块。此外，Swift-Net采用分组 SRU来整合不同特征空间的历史信息，从而提高历史信息的利用效率。我们进一步提出了因果转换模板，以促进非因果AVSS模型向因果模型的转化。在三个标准基准数据集（LRS2、LRS3和VoxCeleb2）上的实验表明，在因果条件下，我们提出的Swift-Net表现出卓越的性能，突显了该方法在复杂环境中处理语音的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06689v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的实时音频视觉语音分离模型Swift-Net，该模型能够从混合信号中提取目标语音信号，利用听觉和视觉线索（如嘴唇动作）。Swift-Net具有轻量级的视觉特征提取模块和高效的音频视觉融合模块，并采用Grouped SRU技术整合历史信息，提高历史信息的利用效率。此外，我们还提出了一种因果转换模板，可将非因果AVSS模型转换为因果模型。实验结果表明，在因果条件下，Swift-Net在复杂环境中处理语音的潜力巨大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Swift-Net是一种新型的音频视觉语音分离模型，旨在从混合信号中提取目标语音信号。</li>
<li>Swift-Net利用听觉和视觉线索（如嘴唇动作）进行语音分离。</li>
<li>Swift-Net具有轻量级的视觉特征提取模块和高效的融合模块。</li>
<li>Grouped SRU技术用于整合历史信息，提高历史信息的利用效率。</li>
<li>提出了一种因果转换模板，可将非因果AVSS模型转换为因果模型。</li>
<li>实验结果表明，Swift-Net在因果条件下表现出卓越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06689">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d0301e51c1ce025302aa2f6d51486d9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ce124eb4911c14d79c3f7092132d4bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a307986eb87e5ba9bc32456ca12703d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9f5fc7e30f8973a5619722d2cad0e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd15825ce12ddcdad5d62cc89c65f8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74aa001209dc0869fbfddeaa7ebfc9e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62c9c975003e39b0e7f171750ff1897e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AS-ASR-A-Lightweight-Framework-for-Aphasia-Specific-Automatic-Speech-Recognition"><a href="#AS-ASR-A-Lightweight-Framework-for-Aphasia-Specific-Automatic-Speech-Recognition" class="headerlink" title="AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech   Recognition"></a>AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech   Recognition</h2><p><strong>Authors:Chen Bao, Chuanbing Huo, Qinyu Chen, Chang Gao</strong></p>
<p>This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition. </p>
<blockquote>
<p>本文提出了AS-ASR，这是一个基于Whisper-tiny的轻量级失语症专用语音识别框架，适用于边缘设备的低资源部署。我们的方法引入了一种混合训练策略，该策略以不同的比例系统地结合了标准和失语语音，实现了稳健的泛化能力，以及基于GPT-4的参考增强方法，该方法可以优化嘈杂的失语症转录，提高监督质量。我们在多个数据混合配置和评估设置上进行了广泛的实验。结果表明，我们微调后的模型显著优于零样本基线，在失语语音上的WER降低了超过30%，同时保持了标准语音的性能。所提出的框架为真实世界中的失语语音识别提供了可扩展且高效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06566v1">PDF</a> Under review</p>
<p><strong>Summary</strong>：</p>
<p>本文提出了一个针对失语症患者的轻量级语音识别框架AS-ASR，基于Whisper-tiny构建，适用于边缘设备的低资源部署。该框架采用混合训练策略，结合标准语音和失语症语音的不同比例数据，提高了模型的稳健性。同时，利用GPT-4技术，对嘈杂的失语症语音转录进行修正，提升了监督学习的质量。实验结果显示，该模型在多个数据混合配置和评估环境下表现优异，相较于零基准模型显著降低了词错误率（WER），在失语症语音识别方面降低了超过30%，同时保持了标准语音识别的性能。此框架为真实世界的失语症语音识别提供了可扩展和高效的解决方案。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AS-ASR是一个针对失语症患者的轻量级语音识别框架，适用于边缘设备。</li>
<li>该框架结合标准语音和失语症语音数据，采用混合训练策略。</li>
<li>引入GPT-4技术，对嘈杂的失语症语音转录进行修正，提升监督质量。</li>
<li>模型在多个数据混合配置和评估环境下表现优异。</li>
<li>相较于零基准模型，新词错误率（WER）在失语症语音识别方面降低了超过30%。</li>
<li>模型在保持标准语音识别性能的同时，优化了失语症语音识别的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4e4ac1a620dac5920c0630e75c8c5a95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd57461f6dcc29eaada13736e27ce3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1e9eedb4c63f8e557004a1a959a061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15a558707a9d0225d8501646e1116639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-147d1d3d03445b42d77a494309d10421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de923e8f54595b4b5a750385227e796e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds"><a href="#DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds" class="headerlink" title="DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds"></a>DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds</h2><p><strong>Authors:Takuya Hasumi, Yusuke Fujita</strong></p>
<p>We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio. Our dataset is available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a>. </p>
<blockquote>
<p>我们为电影音频源分离（CASS）提出一个新的数据集，该数据集处理非语言声音。现有的CASS数据集仅包含阅读风格的音频作为语音主干。这些数据集与实际的电影音频不同，后者更可能包含演绎出来的声音。因此，在常规数据集上训练的模型往往会出现问题，即在情绪高涨的声音（如笑声和尖叫声）更容易被分离出来作为效果，而非语音。为了解决这个问题，我们构建了新的数据集DnR-nonverbal。该数据集包含语音主干中的非语言声音，如笑声和尖叫声等。通过实验，我们揭示了当前CASS模型在非语言声音提取方面的问题，并表明我们的数据集可以在合成和实际的电影音频中有效地解决这一问题。我们的数据集可在<a target="_blank" rel="noopener" href="https://zenodo.org/records/1547064%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://zenodo.org/records/15470640上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02499v2">PDF</a> Accepted to Interspeech 2025, 5 pages, 3 figures, dataset is   available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a></p>
<p><strong>总结</strong><br>    针对电影音频源分离（CASS）提出新的数据集，涵盖非语言声音。现有CASS数据集仅包含阅读式声音作为语音素材，与真实电影音频不同，后者更可能包含表演出的声音。因此，在常规数据集上训练的模型在处理情绪高涨的声音（如笑声和尖叫）时，更容易将其分离为效果而非语音。为解决此问题，我们构建了新的数据集DnR-nonverbal，其中包含笑声和尖叫等非语言声音在语音素材中。实验表明，当前CASS模型存在非语言声音提取问题，我们的数据集可在合成和真实电影音频中有效解决此问题。数据集可在<a target="_blank" rel="noopener" href="https://zenodo.org/records/1547064%E5%8F%9B%E8%99%95%E3%80%82">https://zenodo.org/records/15470640获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>现有CASS数据集主要关注阅读式声音，与真实电影音频存在差异。</li>
<li>电影音频包含更多表演出的声音，如情绪高涨的声音（如笑声和尖叫）。</li>
<li>在常规数据集上训练的模型在处理非语言声音时存在困难，易将其错误地分离为效果而非语音。</li>
<li>为解决此问题，提出新的数据集DnR-nonverbal，包含非语言声音（如笑声和尖叫）。</li>
<li>DnR-nonverbal数据集能有效解决当前CASS模型在非语言声音提取方面的问题。</li>
<li>实验表明，该数据集在合成和真实电影音频中都表现出良好的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5f6b1f300ba7aa7ca06e6621874d55ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc541b87a835e2a690582d46f4581a58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb12411b02ac22ae5770a36ce8351313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8b2402a95e6e80f9d24ff4b2eb17df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe6a8144804e1c29ece79ed2031fee6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals"><a href="#A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals" class="headerlink" title="A Hypernetwork-Based Approach to KAN Representation of Audio Signals"></a>A Hypernetwork-Based Approach to KAN Representation of Audio Signals</h2><p><strong>Authors:Patryk Marszałek, Maciej Rut, Piotr Kawa, Przemysław Spurek, Piotr Syga</strong></p>
<p>Implicit neural representations (INR) have gained prominence for efficiently encoding multimedia data, yet their applications in audio signals remain limited. This study introduces the Kolmogorov-Arnold Network (KAN), a novel architecture using learnable activation functions, as an effective INR model for audio representation. KAN demonstrates superior perceptual performance over previous INRs, achieving the lowest Log-SpectralDistance of 1.29 and the highest Perceptual Evaluation of Speech Quality of 3.57 for 1.5 s audio. To extend KAN’s utility, we propose FewSound, a hypernetwork-based architecture that enhances INR parameter updates. FewSound outperforms the state-of-the-art HyperSound, with a 33.3% improvement in MSE and 60.87% in SI-SNR. These results show KAN as a robust and adaptable audio representation with the potential for scalability and integration into various hypernetwork frameworks. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git">https://github.com/gmum/fewsound.git</a>. </p>
<blockquote>
<p>隐式神经网络表示（INR）在高效编码多媒体数据方面已受到广泛关注，但其在音频信号中的应用仍然有限。本研究引入了Kolmogorov-Arnold网络（KAN），这是一种使用可学习激活函数的新型架构，作为音频表示的有效INR模型。KAN在感知性能上优于先前的INR，在1.5秒音频上实现了最低的Log-SpectralDistance为1.29和最高的语音质量感知评价为3.57。为了扩展KAN的实用性，我们提出了基于超网络的FewSound架构，用于增强INR参数更新。FewSound的性能优于最新的HyperSound，在MSE上提高了33.3%，在SI-SNR上提高了60.87%。这些结果证明了KAN是一个稳健且适应性强的音频表示方法，具有可扩展性和集成到各种超网络框架的潜力。源代码可访问于 <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git%E3%80%82">https://github.com/gmum/fewsound.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02585v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文引入Kolmogorov-Arnold网络（KAN），它是一种使用可学习激活函数的新型架构，作为一种有效的隐神经表示（INR）模型用于音频表示。KAN在音频信号上的表现优于先前的INR模型，实现了最低的Log-SpectralDistance和最高的语音质量感知评价。为扩展KAN的实用性，提出了基于超网络的FewSound架构，能够增强INR参数更新。FewSound优于当前最先进的HyperSound，在MSE和SI-SNR方面分别提高了33.3%和60.87%。研究结果表明，KAN是一种稳健且可适应的音频表示方法，具有可扩展性和集成到各种超网络框架的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了Kolmogorov-Arnold网络（KAN）作为隐神经表示（INR）模型的新架构，专门用于音频表示。</li>
<li>KAN通过使用可学习激活函数提高了音频表示的感知性能。</li>
<li>KAN在音频信号上的表现优于先前的INR模型，实现了较低的Log-SpectralDistance和较高的语音质量感知评价。</li>
<li>提出了基于超网络的FewSound架构，以增强INR模型的参数更新。</li>
<li>FewSound在性能上超越了现有的HyperSound，在MSE和SI-SNR方面有明显的改进。</li>
<li>KAN的音频表示方法具有稳健性和适应性，有望扩展到更多应用场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02585">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8a2d45823a3a19b9af6bf8d34fef74c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d66decd69f9cb4471f6f979dacdeb64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcfba1594c7efd2bfc4dcea05d811d00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-575341573b922c275b961ac081aba667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be934bc7cbedaae63f4effb10e94f3e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-619bdfeee4b4ab257ad336173b5e1101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3623bae21d9b5681c9e561211c78652e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e564406de7e20a3290295beda4a6a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement"><a href="#LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement" class="headerlink" title="LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement"></a>LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement</h2><p><strong>Authors:Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie</strong></p>
<p>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area. </p>
<blockquote>
<p>近期语言模型（LM）的进展在语义理解和上下文建模方面展示了强大的能力，这在生成性语音增强（SE）中尤其显著。然而，许多基于LM的SE方法主要关注语义信息，往往忽视了声音信息的关键作用，这导致增强后的语音在声音上存在不一致性，并且在不同的SE任务中的泛化能力有限。在本文中，我们介绍了LLaSE-G1，这是一个基于LLaMA的语言模型，旨在激励其在语音增强方面的泛化能力。LLaSE-G1的主要贡献如下：首先，为了减轻声音不一致的问题，LLaSE-G1采用WavLM的连续表示作为输入，并使用X-Codec2预测语音令牌，以最大限度地保留声音。其次，为了提升泛化能力，LLaSE-G1引入了双通道输入和输出，能够统一多个SE任务而无需特定任务标识。最后，LLaSE-G1超越了先前的特定任务的判别性和生成性SE模型，在测试时展示了规模效应，并对未见过的SE任务展现了新兴能力。此外，我们公开了代码和模型，以支持该领域的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00493v3">PDF</a> ACL2025 main, Codes available at   <a target="_blank" rel="noopener" href="https://github.com/Kevin-naticl/LLaSE-G1">https://github.com/Kevin-naticl/LLaSE-G1</a></p>
<p><strong>摘要</strong></p>
<p>近期语言模型（LM）的进步在语义理解和上下文建模方面展现出强大的能力，尤其在生成性语音增强（SE）领域尤为突出。然而，许多基于LM的SE方法主要关注语义信息，忽视了声学信息的关键作用，导致增强后的语音出现声学不一致性，并且在不同的SE任务中的泛化能力有限。本文介绍了LLaSE-G1，一种基于LLaMA的语言模型，旨在提高语音增强的泛化能力。LLaSE-G1的主要贡献如下：首先，为了缓解声学不一致性，LLaSE-G1采用WavLM的连续表示作为输入，并从X-Codec2预测语音标记，以最大程度地保留声学特征。其次，为了提升泛化能力，LLaSE-G1引入了双通道输入和输出，统一了多个SE任务，无需特定任务标识。最后，LLaSE-G1在测试时表现出超越先前任务特定的判别性和生成性SE模型的优势，并展现出对未见过的SE任务的适应能力。此外，我们公开了代码和模型，以支持该领域的进一步研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLaSE-G1采用WavLM的连续表示和X-Codec2的语音标记预测，以缓解语音增强中的声学不一致性。</li>
<li>通过引入双通道输入和输出，LLaSE-G1统一了多个语音增强任务，提升了模型的泛化能力。</li>
<li>LLaSE-G1在测试时表现出优势，超越了任务特定的判别性和生成性SE模型。</li>
<li>LLaSE-G1对未见的SE任务具有适应能力。</li>
<li>LLaSE-G1模型能够最大化地保留声学特征。</li>
<li>公开的代码和模型有助于支持该领域的进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2d980168bf2154bb32011e6a510c1745.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b0519543f682a16f37db49b59d4486d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Long-duration-Talking-Video-Synthesis-with-Linear-Diffusion-Transformer-under-Multimodal-Guidance"><a href="#Efficient-Long-duration-Talking-Video-Synthesis-with-Linear-Diffusion-Transformer-under-Multimodal-Guidance" class="headerlink" title="Efficient Long-duration Talking Video Synthesis with Linear Diffusion   Transformer under Multimodal Guidance"></a>Efficient Long-duration Talking Video Synthesis with Linear Diffusion   Transformer under Multimodal Guidance</h2><p><strong>Authors:Haojie Zhang, Zhihao Liang, Ruibo Fu, Bingyan Liu, Zhengqi Wen, Xuefei Liu, Chenxing Li, Yaling Liang</strong></p>
<p>Portrait image animation using audio has rapidly advanced, but challenges remain in efficiently fusing multimodal inputs while ensuring temporal and portrait consistency with minimal computational cost. To address this, we present LetsTalk, a LinEar diffusion TranSformer for Talking video synthesis. LetsTalk incorporates a deep compression autoencoder to obtain efficient latent representations, and a spatio-temporal-aware transformer with efficient linear attention to effectively fuse multimodal information and enhance spatio-temporal consistency. We systematically explore and summarize three fusion schemes, ranging from shallow to deep fusion. We thoroughly analyze their characteristics, applicability, and trade-offs, thereby bridging critical gaps in multimodal conditional guidance. Based on modality differences of image, audio, and video generation, we adopt deep (Symbiotic Fusion) for portrait to ensure consistency, and shallow (Direct Fusion) for audio to align animation with speech while preserving motion diversity. To maintain temporal consistency in long-duration video generation, we propose a memory bank mechanism that preserves inter-clip dependencies, effectively preventing degradation across extended sequences. Furthermore, we develop a noise-regularized training strategy that explicitly compensates for DDPM sampling artifacts, significantly improving the model’s robustness in continuous generation scenarios.Our extensive experiments demonstrate that our approach achieves state-of-the-art generation quality, producing temporally coherent and realistic videos with enhanced diversity and liveliness, while maintaining remarkable efficiency through its optimized model design with 8$\times$ fewer parameters. </p>
<blockquote>
<p>基于音频的肖像图像动画技术已经迅速发展，但在确保时间和肖像连贯性的同时，高效融合多模式输入仍存在挑战，且计算成本较低。为了解决这一问题，我们推出了LetsTalk，这是一款用于视频合成的线性扩散Transformer。LetsTalk结合深度压缩自编码器以获得有效的潜在表示，以及具有高效线性注意力的时空感知转换器，以有效融合多模式信息并增强时空连贯性。我们系统地探索和总结了从浅到深融合的三种融合方案。我们彻底分析了它们的特性、适用性和权衡，从而填补了多模式条件指导的关键空白。基于图像、音频和视频生成的模态差异，我们采用深度（共生融合）融合肖像以确保连贯性，以及浅层（直接融合）融合音频，以使动画与语音对齐，同时保持运动多样性。为了在长期视频生成中保持时间连贯性，我们提出了一种内存银行机制，保留剪辑间的依赖性，有效防止扩展序列中的退化。此外，我们开发了一种噪声正则化训练策略，明确补偿DDPM采样伪影，大大提高了模型在连续生成场景中的稳健性。我们的广泛实验表明，我们的方法达到了最先进的生成质量，产生了时间连贯且逼真的视频，具有增强的多样性和生动性，同时通过其优化后的模型设计保持了卓越的效率，参数减少了8倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16748v2">PDF</a> 16 pages, 13 figures</p>
<p><strong>Summary</strong><br>     本文提出一种名为LetsTalk的线性扩散Transformer语音动画生成模型，用于合成语音视频。该模型通过深度压缩自编码器获得高效潜在表示，并利用时空感知的Transformer和高效线性注意力机制有效融合多模态信息，提高时空一致性。同时探讨了三种融合方案，采用深度融合确保肖像一致性，浅层融合使动画与语音对齐并保持运动多样性。为维持长视频生成的时序一致性，提出记忆库机制，通过保存剪辑间依赖关系防止序列扩展时的性能下降。此外，开发噪声正则化训练策略，明确补偿DDPM采样伪影，提高连续生成场景下的模型稳健性。实验表明，该方法生成质量达到领先水平，能生成时序连贯、逼真的视频，具有增强多样性和生动性，且通过优化模型设计实现高效率，参数减少8倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出名为LetsTalk的线性扩散Transformer模型用于语音驱动的肖像动画生成。</li>
<li>引入深度压缩自编码器以实现高效潜在表示。</li>
<li>利用时空感知的Transformer和线性注意力机制进行多模态信息融合。</li>
<li>探讨了三种融合方案，并根据图像、音频和视频生成的特点采用不同的融合策略。</li>
<li>为保持长视频生成的时序一致性，引入记忆库机制保存剪辑间依赖关系。</li>
<li>开发噪声正则化训练策略以改善模型的连续生成性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16748">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8469b9c6d8ec7622e84aef78bf26ea09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6c01db42660c949271c61b08375b387.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79c59191fa802b8919da776ea2178282.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede0c2e0d671797785e41ae0ccd023b1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dynamic-SUPERB-Phase-2-A-Collaboratively-Expanding-Benchmark-for-Measuring-the-Capabilities-of-Spoken-Language-Models-with-180-Tasks"><a href="#Dynamic-SUPERB-Phase-2-A-Collaboratively-Expanding-Benchmark-for-Measuring-the-Capabilities-of-Spoken-Language-Models-with-180-Tasks" class="headerlink" title="Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for   Measuring the Capabilities of Spoken Language Models with 180 Tasks"></a>Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for   Measuring the Capabilities of Spoken Language Models with 180 Tasks</h2><p><strong>Authors:Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Chih-Kai Yang, Wenze Ren, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Fabian Ritter-Gutierrez, Kuan-Po Huang, Siddhant Arora, You-Kuan Lin, Ming To Chuang, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, Jun-You Wang, Cheng-Hsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor R. Guimarães, Jionghao Han, Tzu-Quan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, Yu-Hua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, Kuan-Yu Fang Chiang, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, Hsi-Che Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Jui-Chiang Wei, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Vahid Noroozi, Zhehuai Chen, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, Hung-yi Lee</strong></p>
<p>Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at <a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb">https://github.com/dynamic-superb/dynamic-superb</a>. </p>
<blockquote>
<p>多模态基础模型，如Gemini和ChatGPT，通过无缝集成各种形式的数据，已经彻底改变了人机交互。开发一种能够理解广泛自然语言指令的通用口语模型，对于弥合沟通差距和促进更直观的人机交互至关重要。然而，缺乏全面的评估基准构成了巨大的挑战。我们提出了Dynamic-SUPERB Phase-2，这是一个开放且不断发展的基准测试，用于全面评估基于指令的通用语音模型。基于第一代的基础上，第二版纳入了全球研究社区合作贡献的12 5项新任务，使基准测试的任务总数扩大到1 8 0项，成为语音和音频评估领域最大的基准测试。虽然Dynamic-SUPERB第一代仅限于分类任务，但Dynamic-SUPERB Phase-2通过引入一系列新颖且多样化的任务来拓宽其评估能力，包括回归和序列生成，涵盖语音、音乐和环保音频。评估结果表明，没有模型能够普遍表现出良好的性能。SALMONN-13B在英语语音识别方面表现出色，Qwen2-Audio-7B-Instruct在情绪识别方面表现出高准确率，但当前模型仍需要进一步创新才能处理更广泛的范围的任务。我们在<a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb%E5%BC%80%E6%BA%90%E4%BA%86%E6%89%80%E6%9C%89%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%92%8C%E8%AF%84%E4%BC%B0%E7%AE%A1%E9%81%93%E3%80%82">https://github.com/dynamic-superb/dynamic-superb开源了所有任务数据和评估管道。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05361v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     多模态基础模型，如Gemini和ChatGPT，通过无缝集成各种形式的数据，已经革新了人机互动方式。开发一种通用口语模型，理解广泛的自然语言指令，对于弥合沟通鸿沟和促进更直观互动至关重要。然而，缺乏全面的评估基准构成重大挑战。我们推出Dynamic-SUPERB Phase-2，这是一个开放和不断发展的基准测试，用于全面评估指令型通用语音模型。相较于第一代，第二代新增了全球研究社区共同贡献的125个新任务，使基准测试任务总数扩充至180个，成为语音和音频评估领域最大的基准测试。Dynamic-SUPERB Phase-2不仅拓宽了评估能力，还引入了各种新颖且多样化的任务，包括回归和序列生成，涵盖语音、音乐和环保音频。评估结果显示，没有模型能够普遍表现优异。SALMONN-1_在英文语音识别方面表现出色，而Qwen2 Audio则能精准识别情绪。当前模型仍需要进一步创新处理更多样化的任务。我们已在开源社区公开所有任务数据和评估流程管道（<a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb%EF%BC%89%E3%80%82">https://github.com/dynamic-superb/dynamic-superb）。</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态基础模型如Gemini和ChatGPT革新了人机互动方式。</li>
<li>开发通用口语模型对于弥合沟通鸿沟和促进更直观互动至关重要。</li>
<li>缺乏全面的评估基准是对语音模型发展的重大挑战。</li>
<li>Dynamic-SUPERB Phase-2是开放和不断发展的基准测试，用于全面评估指令型通用语音模型。</li>
<li>Dynamic-SUPERB Phase-2共包含180个任务，涵盖语音、音乐和环保音频等多个领域的新颖多样化任务。</li>
<li>目前没有模型在所有任务中表现优异，需要进一步的创新和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05361">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-24a22bab331141b95aecc9a9d5a663fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5582bb894c0a4334c08b88fce86e5ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec906cde41f3c346593d2a24f85cf424.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-613aa4ad09b6348e5a5671af6e7d713b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae59827483b9627327bd3e4196668041.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-06-11  GHOST 2.0 generative high-fidelity one shot transfer of heads
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-826ae48f553e7d49fa66e17bb9a5eae4.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-11  Variational Supervised Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
