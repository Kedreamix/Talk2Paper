<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Audio-Sync Video Generation with Multi-Stream Temporal Control">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-74a4a1d64c3d2d0240ec91bd5e838ab6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="Audio-Sync-Video-Generation-with-Multi-Stream-Temporal-Control"><a href="#Audio-Sync-Video-Generation-with-Multi-Stream-Temporal-Control" class="headerlink" title="Audio-Sync Video Generation with Multi-Stream Temporal Control"></a>Audio-Sync Video Generation with Multi-Stream Temporal Control</h2><p><strong>Authors:Shuchen Weng, Haojie Zheng, Zheng Chang, Si Li, Boxin Shi, Xinlong Wang</strong></p>
<p>Audio is inherently temporal and closely synchronized with the visual world, making it a naturally aligned and expressive control signal for controllable video generation (e.g., movies). Beyond control, directly translating audio into video is essential for understanding and visualizing rich audio narratives (e.g., Podcasts or historical recordings). However, existing approaches fall short in generating high-quality videos with precise audio-visual synchronization, especially across diverse and complex audio types. In this work, we introduce MTV, a versatile framework for audio-sync video generation. MTV explicitly separates audios into speech, effects, and music tracks, enabling disentangled control over lip motion, event timing, and visual mood, respectively â€“ resulting in fine-grained and semantically aligned video generation. To support the framework, we additionally present DEMIX, a dataset comprising high-quality cinematic videos and demixed audio tracks. DEMIX is structured into five overlapped subsets, enabling scalable multi-stage training for diverse generation scenarios. Extensive experiments demonstrate that MTV achieves state-of-the-art performance across six standard metrics spanning video quality, text-video consistency, and audio-video alignment. Project page: <a target="_blank" rel="noopener" href="https://hjzheng.net/projects/MTV/">https://hjzheng.net/projects/MTV/</a>. </p>
<blockquote>
<p>éŸ³é¢‘æœ¬è´¨ä¸Šå…·æœ‰æ—¶é—´æ€§ï¼Œå¹¶ä¸è§†è§‰ä¸–ç•Œç´§å¯†åŒæ­¥ï¼Œä½¿å…¶æˆä¸ºå¯æ§è§†é¢‘ç”Ÿæˆï¼ˆä¾‹å¦‚ç”µå½±ï¼‰çš„è‡ªç„¶å¯¹é½å’Œè¡¨è¾¾æ§åˆ¶ä¿¡å·ã€‚é™¤äº†æ§åˆ¶ä¹‹å¤–ï¼Œç›´æ¥å°†éŸ³é¢‘ç¿»è¯‘æˆè§†é¢‘å¯¹äºç†è§£å’Œå¯è§†åŒ–ä¸°å¯Œçš„éŸ³é¢‘å™äº‹ï¼ˆä¾‹å¦‚æ’­å®¢æˆ–å†å²è®°å½•ï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰ç²¾ç¡®éŸ³è§†é¢‘åŒæ­¥çš„é«˜è´¨é‡è§†é¢‘æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨è·¨è¶Šå¤šæ ·å’Œå¤æ‚çš„éŸ³é¢‘ç±»å‹æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MTVï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºéŸ³é¢‘åŒæ­¥è§†é¢‘ç”Ÿæˆçš„é€šç”¨æ¡†æ¶ã€‚MTVæ˜ç¡®åœ°å°†éŸ³é¢‘åˆ†ç¦»ä¸ºè¯­éŸ³ã€éŸ³æ•ˆå’ŒéŸ³ä¹è½¨é“ï¼Œå®ç°å¯¹å˜´å”‡è¿åŠ¨ã€äº‹ä»¶æ—¶é—´å’Œè§†è§‰æƒ…ç»ªçš„åˆ†ç¦»æ§åˆ¶ï¼Œä»è€Œå®ç°äº†ç²¾ç»†ä¸”è¯­ä¹‰å¯¹é½çš„è§†é¢‘ç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†DEMIXï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«é«˜è´¨é‡ç”µå½±è§†é¢‘å’Œæ··éŸ³éŸ³é¢‘è½¨é“çš„æ•°æ®é›†ã€‚DEMIXè¢«ç»“æ„åŒ–åˆ†ä¸ºäº”ä¸ªé‡å çš„å­é›†ï¼Œèƒ½å¤Ÿå®ç°å¯æ‰©å±•çš„å¤šé˜¶æ®µè®­ç»ƒï¼Œä»¥é€‚åº”å¤šç§ç”Ÿæˆåœºæ™¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMTVåœ¨å…­é¡¹æ ‡å‡†æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ¶µç›–äº†è§†é¢‘è´¨é‡ã€æ–‡æœ¬ä¸è§†é¢‘çš„ä¸€è‡´æ€§ã€éŸ³é¢‘ä¸è§†é¢‘çš„å¯¹é½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://hjzheng.net/projects/MTV/%E3%80%82">https://hjzheng.net/projects/MTV/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08003v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>éŸ³é¢‘ä¸è§†é¢‘ç”Ÿæˆä¹‹é—´çš„ç´§å¯†å…³è”è¡¨ç°åœ¨å…¶åŒæ­¥æ€§åŠæ—¶æ•ˆæ€§ä¸Šï¼ŒMTVæ¡†æ¶å¯å¯¹éŸ³é¢‘è¿›è¡Œç»†è‡´çš„åˆ†ç±»è½¬åŒ–ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œå¹¶ä¿è¯ç²¾å‡†çš„éŸ³è§†é¢‘åŒæ­¥ã€‚é€šè¿‡åˆ†ç¦»éŸ³é¢‘çš„è¯­éŸ³ã€éŸ³æ•ˆå’ŒéŸ³ä¹è½¨è¿¹ï¼ŒMTVå¯å®ç°ç²¾ç»†æ§åˆ¶ï¼Œå¦‚å”‡éƒ¨åŠ¨ä½œã€äº‹ä»¶æ—¶åºå’Œè§†è§‰æ°›å›´ç­‰ã€‚æ­¤å¤–ï¼Œä¸ºæ”¯æŒMTVæ¡†æ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DEMIXæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡ç”µå½±è§†é¢‘å’Œæ··åˆéŸ³é¢‘è½¨è¿¹ã€‚DEMIXæ•°æ®é›†çš„äº”ä¸ªé‡å å­é›†ä¸ºå„ç§ç”Ÿæˆåœºæ™¯æä¾›äº†å¯æ‰©å±•çš„å¤šé˜¶æ®µè®­ç»ƒåŠŸèƒ½ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒMTVåœ¨è§†é¢‘è´¨é‡ã€æ–‡æœ¬è§†é¢‘ä¸€è‡´æ€§ä»¥åŠéŸ³è§†é¢‘å¯¹é½ç­‰æ–¹é¢å‡è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>éŸ³é¢‘å’Œè§†é¢‘å¤©ç„¶å…³è”ä¸”éœ€è¦ä¿æŒåŒæ­¥æ€§ï¼Œè¡¨ç°åœ¨é€šè¿‡éŸ³é¢‘ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„ç´§å¯†è”ç³»ã€‚</li>
<li>MTVæ¡†æ¶å¯å®ç°é«˜è´¨é‡çš„éŸ³è§†é¢‘åŒæ­¥ç”Ÿæˆï¼Œæå‡éŸ³è§†é¢‘è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚</li>
<li>MTVé€šè¿‡åˆ†ç¦»éŸ³é¢‘è½¨è¿¹å®ç°å¯¹å”‡éƒ¨åŠ¨ä½œã€äº‹ä»¶æ—¶åºå’Œè§†è§‰æ°›å›´çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>DEMIXæ•°æ®é›†æ”¯æŒMTVæ¡†æ¶ï¼ŒåŒ…å«é«˜è´¨é‡ç”µå½±è§†é¢‘å’Œæ··åˆéŸ³é¢‘è½¨è¿¹ã€‚</li>
<li>DEMIXæ•°æ®é›†å…·æœ‰äº”ä¸ªé‡å å­é›†ï¼Œä¸ºä¸åŒç”Ÿæˆåœºæ™¯æä¾›çµæ´»å¤šå˜çš„æ•°æ®æ”¯æŒã€‚</li>
<li>MTVåœ¨è§†é¢‘è´¨é‡ã€æ–‡æœ¬è§†é¢‘ä¸€è‡´æ€§ä»¥åŠéŸ³è§†é¢‘å¯¹é½ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79707e07d8adbb38786e4cdbfdef263d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da5064913d5a52f676e29d52c7f6b253.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-666adefbb36a56a4b37ca19fe3079633.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6961f05c5ed9d7a69cb306fc07add964.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Transcript-Prompted-Whisper-with-Dictionary-Enhanced-Decoding-for-Japanese-Speech-Annotation"><a href="#Transcript-Prompted-Whisper-with-Dictionary-Enhanced-Decoding-for-Japanese-Speech-Annotation" class="headerlink" title="Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation"></a>Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation</h2><p><strong>Authors:Rui Hu, Xiaolong Lin, Jiawang Liu, Shixi Huang, Zhenpeng Zhan</strong></p>
<p>In this paper, we propose a method for annotating phonemic and prosodic labels on a given audio-transcript pair, aimed at constructing Japanese text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale pre-trained automatic speech recognition (ASR) model, conditioned on ground truth transcripts, to simultaneously output phrase-level graphemes and annotation labels. To further correct errors in phonemic labeling, we employ a decoding strategy that utilizes dictionary prior knowledge. The objective evaluation results demonstrate that our proposed method outperforms previous approaches relying solely on text or audio. The subjective evaluation results indicate that the naturalness of speech synthesized by the TTS model, trained with labels annotated using our method, is comparable to that of a model trained with manual annotations. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹ç»™å®šéŸ³é¢‘è½¬å½•å¯¹è¿›è¡ŒéŸ³ç´ å’Œè¯­è°ƒæ ‡ç­¾æ ‡æ³¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ„å»ºæ—¥æœ¬è¯­éŸ³æ–‡æœ¬ï¼ˆTTSï¼‰æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠå¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹åŸºäºçœŸå®è½¬å½•æœ¬ï¼Œä»¥åŒæ—¶è¾“å‡ºçŸ­è¯­çº§åˆ«çš„å­—æ¯å’Œæ³¨é‡Šæ ‡ç­¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥çº æ­£éŸ³ç´ æ ‡æ³¨ä¸­çš„é”™è¯¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ©ç”¨è¯å…¸å…ˆéªŒçŸ¥è¯†çš„è§£ç ç­–ç•¥ã€‚ç›®æ ‡è¯„ä»·ç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä»…ä¾èµ–æ–‡æœ¬æˆ–éŸ³é¢‘çš„ä»¥å‰çš„æ–¹æ³•ã€‚ä¸»è§‚è¯„ä»·ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œæ ‡æ³¨è®­ç»ƒçš„TTSæ¨¡å‹åˆæˆçš„è¯­éŸ³è‡ªç„¶åº¦ä¸æ‰‹åŠ¨æ ‡æ³¨è®­ç»ƒçš„æ¨¡å‹ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07646v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹ç»™å®šéŸ³é¢‘-æ–‡æœ¬å¯¹è¿›è¡ŒéŸ³ç´ å’ŒéŸµå¾‹æ ‡ç­¾æ ‡æ³¨çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ„å»ºæ—¥è¯­æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ•°æ®é›†ã€‚é€šè¿‡å¾®è°ƒå¤§è§„æ¨¡é¢„è®­ç»ƒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œç»“åˆçœŸå®æ–‡æœ¬è½¬å½•æœ¬åŒæ—¶è¾“å‡ºè¯ç»„å±‚é¢çš„å­—å…ƒå’Œæ³¨é‡Šæ ‡ç­¾ã€‚ä¸ºä¿®æ­£éŸ³ç´ æ ‡æ³¨ä¸­çš„é”™è¯¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ©ç”¨è¯å…¸å…ˆéªŒçŸ¥è¯†çš„è§£ç ç­–ç•¥ã€‚å®¢è§‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä»…ä¾èµ–æ–‡æœ¬æˆ–éŸ³é¢‘çš„å…ˆå‰æ–¹æ³•ã€‚ä¸»è§‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œæ ‡ç­¾æ³¨é‡Šè®­ç»ƒçš„TTSæ¨¡å‹åˆæˆçš„è¯­éŸ³è‡ªç„¶åº¦ä¸æ‰‹åŠ¨æ³¨é‡Šè®­ç»ƒçš„æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªä¸ºæ—¥è¯­æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ•°æ®é›†æ ‡æ³¨éŸ³ç´ å’ŒéŸµå¾‹æ ‡ç­¾çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå¤§è§„æ¨¡é¢„è®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ¥å®ç°æ ‡æ³¨ã€‚</li>
<li>ç»“åˆçœŸå®æ–‡æœ¬è½¬å½•æœ¬åŒæ—¶è¾“å‡ºè¯ç»„å±‚é¢çš„å­—å…ƒå’Œæ³¨é‡Šæ ‡ç­¾ã€‚</li>
<li>é‡‡ç”¨åˆ©ç”¨è¯å…¸å…ˆéªŒçŸ¥è¯†çš„è§£ç ç­–ç•¥æ¥ä¿®æ­£éŸ³ç´ æ ‡æ³¨ä¸­çš„é”™è¯¯ã€‚</li>
<li>å®¢è§‚è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–ä»…ä¾èµ–æ–‡æœ¬æˆ–éŸ³é¢‘çš„æ–¹æ³•ã€‚</li>
<li>ä¸»è§‚è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨æ­¤æ–¹æ³•æ ‡æ³¨è®­ç»ƒçš„TTSæ¨¡å‹åˆæˆçš„è¯­éŸ³è‡ªç„¶åº¦ä¸æ‰‹åŠ¨æ³¨é‡Šè®­ç»ƒçš„æ¨¡å‹ç›¸ä¼¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-455272088a4ad9824a3d088068471bfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b099872c6cb285ec27fe64a2d312d47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-676e46a26e8ac244d8e02ac8c012653f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c44e77a2bd8effc16cc374eee9236a66.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Speaker-Distinguishable-CTC-Learning-Speaker-Distinction-Using-CTC-for-Multi-Talker-Speech-Recognition"><a href="#Speaker-Distinguishable-CTC-Learning-Speaker-Distinction-Using-CTC-for-Multi-Talker-Speech-Recognition" class="headerlink" title="Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for   Multi-Talker Speech Recognition"></a>Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for   Multi-Talker Speech Recognition</h2><p><strong>Authors:Asahi Sakuma, Hiroaki Sato, Ryuga Sugano, Tadashi Kumano, Yoshihiko Kawai, Tetsuji Ogawa</strong></p>
<p>This paper presents a novel framework for multi-talker automatic speech recognition without the need for auxiliary information. Serialized Output Training (SOT), a widely used approach, suffers from recognition errors due to speaker assignment failures. Although incorporating auxiliary information, such as token-level timestamps, can improve recognition accuracy, extracting such information from natural conversational speech remains challenging. To address this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension of CTC that jointly assigns a token and its corresponding speaker label to each frame. We further integrate SD-CTC into the SOT framework, enabling the SOT model to learn speaker distinction using only overlapping speech and transcriptions. Experimental comparisons show that multi-task learning with SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves performance comparable to state-of-the-art methods relying on auxiliary information. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è¾…åŠ©ä¿¡æ¯çš„å¤šè¯´è¯è€…è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„æ–°æ¡†æ¶ã€‚åºåˆ—è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•ï¼Œä½†ç”±äºè¯´è¯äººåˆ†é…å¤±è´¥è€Œå¯¼è‡´è¯†åˆ«é”™è¯¯ã€‚è™½ç„¶ç»“åˆè¾…åŠ©ä¿¡æ¯ï¼ˆå¦‚ä»¤ç‰Œçº§æ—¶é—´æˆ³ï¼‰å¯ä»¥æé«˜è¯†åˆ«ç²¾åº¦ï¼Œä½†ä»è‡ªç„¶å¯¹è¯è¯­éŸ³ä¸­æå–æ­¤ç±»ä¿¡æ¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Speaker-Distinguishable CTCï¼ˆSD-CTCï¼‰ï¼Œå®ƒæ˜¯CTCçš„ä¸€ç§æ‰©å±•ï¼Œå¯ä»¥è”åˆä¸ºæ¯ä¸€å¸§åˆ†é…ä»¤ç‰ŒåŠå…¶ç›¸åº”çš„è¯´è¯äººæ ‡ç­¾ã€‚æˆ‘ä»¬å°†SD-CTCè¿›ä¸€æ­¥é›†æˆåˆ°SOTæ¡†æ¶ä¸­ï¼Œä½¿SOTæ¨¡å‹ä»…ä½¿ç”¨é‡å è¯­éŸ³å’Œè½¬å½•æ¥å­¦ä¹ è¯´è¯äººåŒºåˆ†ã€‚å®éªŒæ¯”è¾ƒè¡¨æ˜ï¼Œä½¿ç”¨SD-CTCå’ŒSOTè¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ é™ä½äº†SOTæ¨¡å‹çš„é”™è¯¯ç‡26%ï¼Œå¹¶å®ç°äº†ä¸ä¾èµ–è¾…åŠ©ä¿¡æ¯çš„æœ€æ–°æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07515v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è¾…åŠ©ä¿¡æ¯çš„å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„æ–°æ¡†æ¶ã€‚é’ˆå¯¹åºåˆ—åŒ–è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰ä¸­å› è¯´è¯äººåˆ†é…å¤±è´¥å¯¼è‡´çš„è¯†åˆ«é”™è¯¯é—®é¢˜ï¼Œæå‡ºäº†Speaker-Distinguishable CTCï¼ˆSD-CTCï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†æ¯ä¸ªå¸§çš„ä»¤ç‰ŒåŠå…¶å¯¹åº”çš„è¯´è¯äººæ ‡ç­¾ä¸€èµ·åˆ†é…ï¼Œå¹¶é›†æˆåˆ°SOTæ¡†æ¶ä¸­ï¼Œä½¿SOTæ¨¡å‹èƒ½å¤Ÿä»…åˆ©ç”¨é‡å è¯­éŸ³å’Œè½¬å½•æ¥å­¦ä¹ è¯´è¯äººåŒºåˆ†ã€‚å®éªŒæ¯”è¾ƒæ˜¾ç¤ºï¼Œå¤šä»»åŠ¡å­¦ä¹ ä¸SD-CTCå’ŒSOTç›¸ç»“åˆï¼Œå°†SOTæ¨¡å‹çš„é”™è¯¯ç‡é™ä½äº†26%ï¼Œå¹¶å®ç°äº†ä¸ä¾èµ–è¾…åŠ©ä¿¡æ¯çš„æœ€æ–°æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†æ— éœ€è¾…åŠ©ä¿¡æ¯çš„å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–°æ¡†æ¶ã€‚</li>
<li>åºåˆ—åŒ–è¾“å‡ºè®­ç»ƒï¼ˆSOTï¼‰å­˜åœ¨è¯´è¯äººåˆ†é…å¤±è´¥çš„è¯†åˆ«é”™è¯¯é—®é¢˜ã€‚</li>
<li>Speaker-Distinguishable CTCï¼ˆSD-CTCï¼‰æ–¹æ³•è¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æ¯ä¸ªå¸§åˆ†é…ä»¤ç‰ŒåŠå…¶å¯¹åº”çš„è¯´è¯äººæ ‡ç­¾ã€‚</li>
<li>SD-CTCè¢«é›†æˆåˆ°SOTæ¡†æ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½åˆ©ç”¨é‡å è¯­éŸ³å’Œè½¬å½•æ¥å­¦ä¹ è¯´è¯äººåŒºåˆ†ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºå¤šä»»åŠ¡å­¦ä¹ ä¸SD-CTCå’ŒSOTç»“åˆèƒ½æ˜¾è‘—é™ä½SOTæ¨¡å‹çš„é”™è¯¯ç‡ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ä¸ä¾èµ–è¾…åŠ©ä¿¡æ¯çš„æœ€æ–°æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca6ac77ab5a5d4c080cce26a98eb8ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc36dbe0b5d2dc9a338c7b7e7d6c4082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8c9896e50efeb74f6cbea353fa0e1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83cf4064530fdeca549dc2fb98f908e6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeRAGEC-Denoising-Named-Entity-Candidates-with-Synthetic-Rationale-for-ASR-Error-Correction"><a href="#DeRAGEC-Denoising-Named-Entity-Candidates-with-Synthetic-Rationale-for-ASR-Error-Correction" class="headerlink" title="DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for   ASR Error Correction"></a>DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for   ASR Error Correction</h2><p><strong>Authors:Solee Im, Wonjun Lee, Jinmyeong An, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee</strong></p>
<p>We present DeRAGEC, a method for improving Named Entity (NE) correction in Automatic Speech Recognition (ASR) systems. By extending the Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC employs synthetic denoising rationales to filter out noisy NE candidates before correction. By leveraging phonetic similarity and augmented definitions, it refines noisy retrieved NEs using in-context learning, requiring no additional training. Experimental results on CommonVoice and STOP datasets show significant improvements in Word Error Rate (WER) and NE hit ratio, outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28% relative reduction in WER compared to ASR without postprocessing. Our source code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/solee0022/deragec">https://github.com/solee0022/deragec</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†DeRAGECæ–¹æ³•ï¼Œç”¨äºæ”¹è¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„å‘½åå®ä½“ï¼ˆNEï¼‰æ ¡æ­£ã€‚é€šè¿‡æ‰©å±•åŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆå¼é”™è¯¯æ ¡æ­£ï¼ˆRAGECï¼‰æ¡†æ¶ï¼ŒDeRAGECé‡‡ç”¨åˆæˆé™å™ªç†ç”±æ¥è¿‡æ»¤æ ¡æ­£å‰çš„å˜ˆæ‚NEå€™é€‰è¯ã€‚é€šè¿‡åˆ©ç”¨è¯­éŸ³ç›¸ä¼¼æ€§å¹¶å¢å¼ºå®šä¹‰ï¼Œå®ƒåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å¯¹å˜ˆæ‚çš„æ£€ç´¢åˆ°çš„NEè¿›è¡Œæ”¹è¿›ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚åœ¨CommonVoiceå’ŒSTOPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’ŒNEå‘½ä¸­ç‡éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œè¶…è¿‡äº†åŸºçº¿ASRå’ŒRAGECæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æ²¡æœ‰åå¤„ç†çš„ASRç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†ç›¸å¯¹å‡å°‘28%çš„WERã€‚æˆ‘ä»¬çš„æºä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/solee0022/deragec">https://github.com/solee0022/deragec</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07510v1">PDF</a> ACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DeRAGECæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨äºæé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿä¸­çš„å‘½åå®ä½“ï¼ˆNEï¼‰æ ¡æ­£æ•ˆæœã€‚å®ƒé€šè¿‡æ‰©å±•åŸºäºæ£€ç´¢çš„è¾…åŠ©ç”Ÿæˆå¼é”™è¯¯æ ¡æ­£ï¼ˆRAGECï¼‰æ¡†æ¶ï¼Œé‡‡ç”¨åˆæˆé™å™ªç†ç”±æ¥è¿‡æ»¤å‡ºå‘½åå®ä½“è¯†åˆ«ä¸­çš„å™ªå£°å€™é€‰è¯ï¼Œå†è¿›è¡Œæ ¡æ­£ã€‚åˆ©ç”¨è¯­éŸ³ç›¸ä¼¼æ€§å¹¶æ‰©å……å®šä¹‰ï¼Œè¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å¯¹å™ªå£°æ£€ç´¢çš„å‘½åå®ä½“è¿›è¡Œç²¾ç‚¼ã€‚åœ¨CommonVoiceå’ŒSTOPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå‘½åå®ä½“å‘½ä¸­ç‡æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ï¼Œè¶…è¶Šäº†åŸºçº¿ASRå’ŒRAGECæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œä¸æœªè¿›è¡Œåå¤„ç†çš„ASRç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†ç›¸å¯¹é™ä½28%çš„WERã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeRAGECæ–¹æ³•æ‰©å±•äº†RAGECæ¡†æ¶ä»¥æé«˜ASRç³»ç»Ÿä¸­çš„å‘½åå®ä½“è¯†åˆ«æ•ˆæœã€‚</li>
<li>é€šè¿‡åˆæˆé™å™ªç†ç”±ï¼ŒDeRAGECèƒ½å¤Ÿè¿‡æ»¤å‡ºå‘½åå®ä½“è¯†åˆ«ä¸­çš„å™ªå£°å€™é€‰è¯ã€‚</li>
<li>åˆ©ç”¨è¯­éŸ³ç›¸ä¼¼æ€§å’Œæ‰©å……å®šä¹‰ï¼ŒDeRAGECèƒ½å¤Ÿç²¾ç‚¼å™ªå£°æ£€ç´¢çš„å‘½åå®ä½“ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ æé«˜å‘½åå®ä½“çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨CommonVoiceå’ŒSTOPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeRAGECæ˜¾è‘—æé«˜äº†å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå‘½åå®ä½“å‘½ä¸­ç‡ã€‚</li>
<li>DeRAGECç›¸å¯¹äºåŸºçº¿ASRå’ŒRAGECæ–¹æ³•è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-363d69e1a117067bf71b39eb14f09ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e47785556229af4a90776fa0519f1e81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae0e454dab8fcebb110d07206c0865ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91c2566a4604e61cf0db7382669e9082.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09df6138c267854f4aabc6aea0061bda.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hybrid-Vision-Transformer-Mamba-Framework-for-Autism-Diagnosis-via-Eye-Tracking-Analysis"><a href="#Hybrid-Vision-Transformer-Mamba-Framework-for-Autism-Diagnosis-via-Eye-Tracking-Analysis" class="headerlink" title="Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via   Eye-Tracking Analysis"></a>Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via   Eye-Tracking Analysis</h2><p><strong>Authors:Wafaa Kasri, Yassine Himeur, Abigail Copiaco, Wathiq Mansoor, Ammar Albanna, Valsamma Eapen</strong></p>
<p>Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early intervention. This study presents a hybrid deep learning framework combining Vision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking data. The model uses attention-based fusion to integrate visual, speech, and facial cues, capturing both spatial and temporal dynamics. Unlike traditional handcrafted methods, it applies state-of-the-art deep learning and explainable AI techniques to enhance diagnostic accuracy and transparency. Tested on the Saliency4ASD dataset, the proposed ViT-Mamba model outperformed existing methods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94 specificity. These findings show the modelâ€™s promise for scalable, interpretable ASD screening, especially in resource-constrained or remote clinical settings where access to expert diagnosis is limited. </p>
<blockquote>
<p>å‡†ç¡®è¯Šæ–­è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰å¯¹äºæ—©æœŸå¹²é¢„è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰Transformerï¼ˆViTï¼‰å’Œè§†è§‰åŠ é€Ÿå™¨ï¼ˆVision Mambaï¼‰çš„æ··åˆæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä½¿ç”¨çœ¼åŠ¨æ•°æ®æ£€æµ‹ASDã€‚è¯¥æ¨¡å‹ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„èåˆæŠ€æœ¯ï¼Œæ•´åˆè§†è§‰ã€è¯­éŸ³å’Œé¢éƒ¨çº¿ç´¢ï¼Œæ•æ‰ç©ºé—´å’Œæ—¶é—´çš„åŠ¨æ€ã€‚ä¸ä¼ ç»Ÿçš„æ‰‹å·¥æ–¹æ³•ä¸åŒï¼Œå®ƒé‡‡ç”¨æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œå¯è§£é‡Šçš„AIæŠ€æœ¯ï¼Œä»¥æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚åœ¨Saliency4ASDæ•°æ®é›†ä¸Šè¿›è¡Œçš„æµ‹è¯•è¡¨æ˜ï¼Œæ‰€æå‡ºçš„ViT-Mambaæ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†0.96çš„å‡†ç¡®ç‡ã€0.95çš„F1åˆ†æ•°ã€0.97çš„çµæ•åº¦å’Œ0.94çš„ç‰¹å¼‚æ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜è¯¥æ¨¡å‹åœ¨å¯æ‰©å±•å’Œå¯è§£é‡Šçš„ASDç­›æŸ¥æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™æˆ–è¿œç¨‹ä¸´åºŠç¯å¢ƒä¸­ï¼Œä¸“ä¸šè¯Šæ–­çš„è·å–å—åˆ°é™åˆ¶çš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06886v1">PDF</a> 7 pages, 4 figures and 2 tables</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»“åˆVision Transformersï¼ˆViTï¼‰å’ŒVision Mambaçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨çœ¼åŠ¨æ•°æ®å¯¹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰è¿›è¡Œå‡†ç¡®è¯Šæ–­ã€‚è¯¥æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›èåˆæœºåˆ¶æ•´åˆè§†è§‰ã€è¯­éŸ³å’Œé¢éƒ¨çº¿ç´¢ï¼Œæ•æ‰ç©ºé—´å’Œæ—¶é—´çš„åŠ¨æ€å˜åŒ–ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„æ‰‹å·¥æ–¹æ³•ï¼Œå®ƒé‡‡ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œå¯è§£é‡Šçš„AIæŠ€æœ¯æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚åœ¨Saliency4ASDæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œæ‰€æå‡ºçš„ViT-Mambaæ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®ç‡ã€F1åˆ†æ•°ã€çµæ•åº¦å’Œç‰¹å¼‚æ€§åˆ†åˆ«è¾¾åˆ°äº†0.96ã€0.95ã€0.97å’Œ0.94ã€‚è¿™ä¸€æ¨¡å‹åœ¨å¯æ‰©å±•æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™æˆ–è¿œç¨‹ä¸´åºŠç¯å¢ƒä¸­ï¼Œå¯ä¸ºç¼ºä¹ä¸“å®¶è¯Šæ–­çš„åœ°åŒºæä¾›ä¾¿åˆ©ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ç»“åˆVision Transformersï¼ˆViTï¼‰å’ŒVision MambaæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ³¨æ„åŠ›èåˆæœºåˆ¶æ•´åˆè§†è§‰ã€è¯­éŸ³å’Œé¢éƒ¨çº¿ç´¢ï¼Œä»¥æ•æ‰ç©ºé—´å’Œæ—¶é—´åŠ¨æ€ã€‚</li>
<li>ç›¸æ¯”ä¼ ç»Ÿçš„æ‰‹å·¥æ–¹æ³•ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ å’Œå¯è§£é‡Šçš„AIæŠ€æœ¯ï¼Œå¢å¼ºäº†è¯Šæ–­çš„é€æ˜åº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨Saliency4ASDæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒViT-Mambaæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€çµæ•åº¦å’Œç‰¹å¼‚æ€§ã€‚</li>
<li>æ­¤æ¨¡å‹å¯¹äºèµ„æºæœ‰é™çš„åœ°åŒºæˆ–è¿œç¨‹ä¸´åºŠç¯å¢ƒç‰¹åˆ«æœ‰ç”¨ï¼Œå¯ä½œä¸ºä¸€ç§å¯æ‰©å±•ä¸”å¯è§£é‡Šæ€§å¼ºçš„å°å„¿è‡ªé—­ç—‡ç­›æŸ¥å·¥å…·ã€‚</li>
<li>æ­¤æ¨¡å‹åœ¨è‡ªé—­ç—‡è¯Šæ–­ä¸­çš„åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œæœ‰æœ›æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0231dd14650aeca284645478f560245b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6a4540b58d9eab87b6bf707ab66d3b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bf196fe44404486570b10899589ebd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf01a37d7d1f15f8dd71e735cbd37913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74a4a1d64c3d2d0240ec91bd5e838ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-575c54e7f14e1066b72fb90f511400fa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="They-want-to-pretend-not-to-understand-The-Limits-of-Current-LLMs-in-Interpreting-Implicit-Content-of-Political-Discourse"><a href="#They-want-to-pretend-not-to-understand-The-Limits-of-Current-LLMs-in-Interpreting-Implicit-Content-of-Political-Discourse" class="headerlink" title="They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse"></a>They want to pretend not to understand: The Limits of Current LLMs in   Interpreting Implicit Content of Political Discourse</h2><p><strong>Authors:Walter Paci, Alessandro Panunzi, Sandro Pezzelle</strong></p>
<p>Implicit content plays a crucial role in political discourse, where speakers systematically employ pragmatic strategies such as implicatures and presuppositions to influence their audiences. Large Language Models (LLMs) have demonstrated strong performance in tasks requiring complex semantic and pragmatic understanding, highlighting their potential for detecting and explaining the meaning of implicit content. However, their ability to do this within political discourse remains largely underexplored. Leveraging, for the first time, the large IMPAQTS corpus, which comprises Italian political speeches with the annotation of manipulative implicit content, we propose methods to test the effectiveness of LLMs in this challenging problem. Through a multiple-choice task and an open-ended generation task, we demonstrate that all tested models struggle to interpret presuppositions and implicatures. We conclude that current LLMs lack the key pragmatic capabilities necessary for accurately interpreting highly implicit language, such as that found in political discourse. At the same time, we highlight promising trends and future directions for enhancing model performance. We release our data and code at <a target="_blank" rel="noopener" href="https://github.com/WalterPaci/IMPAQTS-PID">https://github.com/WalterPaci/IMPAQTS-PID</a> </p>
<blockquote>
<p>éšæ™¦å†…å®¹åœ¨æ”¿æ²»è¯è¯­ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæ¼”è®²è€…ç³»ç»Ÿåœ°é‡‡ç”¨è¯­ç”¨ç­–ç•¥ï¼Œå¦‚éšå«æ„ä¹‰å’Œé¢„è®¾ï¼Œæ¥å½±å“ä»–ä»¬çš„å¬ä¼—ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦å¤æ‚è¯­ä¹‰å’Œè¯­ç”¨ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æ£€æµ‹å’Œè§£é‡Šéšæ™¦å†…å®¹æ„ä¹‰æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ”¿æ²»è¯è¯­ä¸­æ‰§è¡Œæ­¤ä»»åŠ¡çš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬é¦–æ¬¡åˆ©ç”¨å¤§å‹çš„IMPAQTSè¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“åŒ…å«å¸¦æœ‰æ“çºµæ€§éšæ™¦å†…å®¹çš„æ„å¤§åˆ©æ”¿æ²»æ¼”è®²æ³¨é‡Šï¼Œæˆ‘ä»¬æå‡ºäº†æµ‹è¯•LLMåœ¨æ­¤éš¾é¢˜ä¸­çš„æœ‰æ•ˆæ€§çš„æ–¹æ³•ã€‚é€šè¿‡å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬è¯æ˜æ‰€æœ‰æµ‹è¯•è¿‡çš„æ¨¡å‹åœ¨è§£é‡Šé¢„è®¾å’Œéšå«æ„ä¹‰æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹å‡†ç¡®è§£é‡Šé«˜åº¦éšæ™¦è¯­è¨€ï¼ˆå¦‚æ”¿æ²»è¯è¯­ä¸­çš„è¯­è¨€ï¼‰æ‰€éœ€çš„å…³é”®è¯­ç”¨èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æé«˜æ¨¡å‹æ€§èƒ½çš„å¯è¡Œè¶‹åŠ¿å’Œæœªæ¥æ–¹å‘ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/WalterPaci/IMPAQTS-PID">https://github.com/WalterPaci/IMPAQTS-PID</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06775v1">PDF</a> Accepted to the ACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æŒ‡å‡ºéšå¼å†…å®¹åœ¨æ”¿æ²»è¯è¯­ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œæ”¿æ²»å®¶ä»¬ä½¿ç”¨è¯¸å¦‚éšå«ä¹‰å’Œé¢„è®¾ç­‰è¯­ç”¨ç­–ç•¥æ¥å½±å“å¬ä¼—ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦å¤æ‚è¯­ä¹‰å’Œè¯­ç”¨ç†è§£çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œå…·æœ‰æ£€æµ‹å’Œè§£é‡Šéšå¼å†…å®¹å«ä¹‰çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶åœ¨æ”¿æ²»è¯è¯­ä¸­çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç ”ç©¶é¦–æ¬¡ä½¿ç”¨åŒ…å«æ„å¤§åˆ©æ”¿æ²»æ¼”è®²å’Œæ“çºµæ€§éšå¼å†…å®¹æ³¨è§£çš„IMPAQTSè¯­æ–™åº“ï¼Œæµ‹è¯•äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™ä¸€éš¾é¢˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡å‘ç°æ‰€æœ‰æµ‹è¯•æ¨¡å‹åœ¨è§£é‡Šé¢„è®¾å’Œéšå«ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ç ”ç©¶è®¤ä¸ºå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹å‡†ç¡®è§£é‡Šæ”¿æ²»è¯è¯­ä¸­é«˜åº¦éšæ™¦è¯­è¨€çš„å…³é”®è¯­ç”¨èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜æŒ‡å‡ºäº†æå‡æ¨¡å‹æ€§èƒ½çš„æ½œåœ¨è¶‹åŠ¿å’Œæœªæ¥æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšå¼å†…å®¹åœ¨æ”¿æ²»è¯è¯­ä¸­èµ·å…³é”®ä½œç”¨ï¼Œæ”¿æ²»å®¶åˆ©ç”¨éšå«ä¹‰å’Œé¢„è®¾ç­‰ç­–ç•¥å½±å“å¬ä¼—ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„è¯­ä¹‰å’Œè¯­ç”¨ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰æ£€æµ‹å’Œè§£é‡Šéšå¼å†…å®¹å«ä¹‰çš„æ½œåŠ›ï¼Œä½†åœ¨æ”¿æ²»è¯è¯­ä¸­çš„èƒ½åŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>åˆ©ç”¨IMPAQTSè¯­æ–™åº“å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†æ”¿æ²»è¯è¯­ä¸­çš„éšå¼å†…å®¹è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>åœ¨å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨è§£é‡Šæ”¿æ²»è¯è¯­ä¸­çš„é¢„è®¾å’Œéšå«ä¹‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹å‡†ç¡®è§£é‡Šé«˜åº¦éšæ™¦è¯­è¨€çš„è¯­ç”¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9f99dd89d35b55c6358aacede1676926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fffa1afea654369ed02047598737791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dc26dd2911398e6befc68b406480bb9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LitMAS-A-Lightweight-and-Generalized-Multi-Modal-Anti-Spoofing-Framework-for-Biometric-Security"><a href="#LitMAS-A-Lightweight-and-Generalized-Multi-Modal-Anti-Spoofing-Framework-for-Biometric-Security" class="headerlink" title="LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing   Framework for Biometric Security"></a>LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing   Framework for Biometric Security</h2><p><strong>Authors:Nidheesh Gorthi, Kartik Thakral, Rishabh Ranjan, Richa Singh, Mayank Vatsa</strong></p>
<p>Biometric authentication systems are increasingly being deployed in critical applications, but they remain susceptible to spoofing. Since most of the research efforts focus on modality-specific anti-spoofing techniques, building a unified, resource-efficient solution across multiple biometric modalities remains a challenge. To address this, we propose LitMAS, a $\textbf{Li}$gh$\textbf{t}$ weight and generalizable $\textbf{M}$ulti-modal $\textbf{A}$nti-$\textbf{S}$poofing framework designed to detect spoofing attacks in speech, face, iris, and fingerprint-based biometric systems. At the core of LitMAS is a Modality-Aligned Concentration Loss, which enhances inter-class separability while preserving cross-modal consistency and enabling robust spoof detection across diverse biometric traits. With just 6M parameters, LitMAS surpasses state-of-the-art methods by $1.36%$ in average EER across seven datasets, demonstrating high efficiency, strong generalizability, and suitability for edge deployment. Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/IAB-IITJ/LitMAS">https://github.com/IAB-IITJ/LitMAS</a>. </p>
<blockquote>
<p>ç”Ÿç‰©è¯†åˆ«è®¤è¯ç³»ç»Ÿæ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°åº”ç”¨äºå…³é”®åº”ç”¨ä¸­ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°æ¬ºéª—æ”»å‡»ã€‚ç”±äºå¤§å¤šæ•°ç ”ç©¶å·¥ä½œéƒ½é›†ä¸­åœ¨é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„åæ¬ºéª—æŠ€æœ¯ä¸Šï¼Œå› æ­¤åœ¨å¤šä¸ªç”Ÿç‰©è¯†åˆ«æ¨¡æ€ä¸Šæ„å»ºç»Ÿä¸€ã€èµ„æºé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LitMASï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”å¯æ¨å¹¿çš„å¤šæ¨¡æ€åæ¬ºéª—æ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹è¯­éŸ³ã€é¢éƒ¨ã€è™¹è†œå’ŒæŒ‡çº¹ç­‰åŸºäºç”Ÿç‰©è¯†åˆ«çš„ç³»ç»Ÿä¸­çš„æ¬ºéª—æ”»å‡»ã€‚LitMASçš„æ ¸å¿ƒæ˜¯æ¨¡æ€å¯¹é½æµ“åº¦æŸå¤±ï¼Œè¿™å¢å¼ºäº†ç±»é—´å¯åˆ†æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è·¨æ¨¡æ€çš„ä¸€è‡´æ€§ï¼Œå¹¶å®ç°äº†å„ç§ç”Ÿç‰©ç‰¹å¾ä¸Šçš„ç¨³å¥æ¬ºéª—æ£€æµ‹ã€‚LitMASä»…æœ‰6Må‚æ•°ï¼Œåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡EERè¶…å‡ºæœ€æ–°æ–¹æ³•1.36%ï¼Œè¯æ˜äº†å…¶é«˜æ•ˆæ€§ã€å¼ºé€šç”¨æ€§å’Œè¾¹ç¼˜éƒ¨ç½²çš„é€‚ç”¨æ€§ã€‚ä»£ç å’Œè®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IAB-IITJ/LitMAS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IAB-IITJ/LitMASæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06759v1">PDF</a> Accepted in Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§ã€å¤šæ¨¡æ€çš„åæ¬ºéª—æ¡†æ¶LitMASï¼Œç”¨äºæ£€æµ‹è¯­éŸ³ã€é¢éƒ¨ã€è™¹è†œå’ŒæŒ‡çº¹ç­‰ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­çš„æ¬ºéª—æ”»å‡»ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ€å¯¹é½æµ“åº¦æŸå¤±æŠ€æœ¯ï¼Œæé«˜äº†ä¸åŒç”Ÿç‰©ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼Œå®ç°äº†é«˜æ•ˆã€é€šç”¨æ€§å¼ºå’Œé€‚åˆè¾¹ç¼˜éƒ¨ç½²çš„åæ¬ºéª—æ£€æµ‹ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡é”™è¯¯æ¥å—ç‡æ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º1.36%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LitMASæ˜¯ä¸€ä¸ªè½»é‡çº§ã€å¤šæ¨¡æ€çš„åæ¬ºéª—æ¡†æ¶ï¼Œæ—¨åœ¨æ£€æµ‹å¤šç§ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­çš„æ¬ºéª—æ”»å‡»ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†æ¨¡æ€å¯¹é½æµ“åº¦æŸå¤±æŠ€æœ¯ï¼Œå¢å¼ºäº†ä¸åŒç”Ÿç‰©ç‰¹å¾ä¹‹é—´çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>LitMASæ¡†æ¶åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡é”™è¯¯æ¥å—ç‡æ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º1.36%ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å¼ºå¤§çš„é€šç”¨æ€§ï¼Œé€‚ç”¨äºå„ç§ç”Ÿç‰©è¯†åˆ«æ¨¡æ€ï¼ŒåŒ…æ‹¬è¯­éŸ³ã€é¢éƒ¨ã€è™¹è†œå’ŒæŒ‡çº¹ç­‰ã€‚</li>
<li>LitMASæ¡†æ¶çš„ä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²ç»å…¬å¼€å¯ç”¨ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
<li>è¯¥æ¡†æ¶ç‰¹åˆ«é€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡è¿›è¡Œéƒ¨ç½²ï¼Œå…·æœ‰è¾ƒä½çš„å‚æ•°éœ€æ±‚å’Œèµ„æºå ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4008e2f4bbf972601fa5f20548b051bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-453d6d5df3e04d204388a74dc050871b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bae5a4c219d50dc5001ab46f1603c02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372417e9f2dc526b0b94fc48563e4418.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55cca83a17b8587972d8c9d16def798f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Length-Generalization-For-Transformer-based-Speech-Enhancement"><a href="#Exploring-Length-Generalization-For-Transformer-based-Speech-Enhancement" class="headerlink" title="Exploring Length Generalization For Transformer-based Speech Enhancement"></a>Exploring Length Generalization For Transformer-based Speech Enhancement</h2><p><strong>Authors:Qiquan Zhang, Hongxu Zhu, Xinyuan Qian, Eliathamby Ambikairajah, Haizhou Li</strong></p>
<p>Transformer network architecture has proven effective in speech enhancement. However, as its core module, self-attention suffers from quadratic complexity, making it infeasible for training on long speech utterances. In practical scenarios, speech enhancement models are often required to perform on noisy speech at run-time that is substantially longer than the training utterances. It remains a challenge how a Transformer-based speech enhancement model can generalize to long speech utterances. In this paper, extensive empirical studies are conducted to explore the modelâ€™s length generalization ability. In particular, we conduct speech enhancement experiments on four training objectives and evaluate with five metrics. Our studies establish that positional encoding is an effective instrument to dampen the effect of utterance length on speech enhancement. We first explore several existing positional encoding methods, and the results show that relative positional encoding methods exhibit a better length generalization property than absolute positional encoding methods. Additionally, we also explore a simpler and more effective positional encoding scheme, i.e. LearnLin, that uses only one trainable parameter for each attention head to scale the real relative position between time frames, which learns the different preferences on short- or long-term dependencies of these heads. The results demonstrate that our proposal exhibits excellent length generalization ability with comparable or superior performance than other state-of-the-art positional encoding strategies. </p>
<blockquote>
<p>Transformerç½‘ç»œæ¶æ„åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢å·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œä½œä¸ºå…¶æ ¸å¿ƒæ¨¡å—ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶å­˜åœ¨äºŒæ¬¡å¤æ‚æ€§ï¼Œä½¿å¾—å®ƒå¯¹é•¿è¯­éŸ³ç‰‡æ®µçš„è®­ç»ƒå˜å¾—ä¸å¯è¡Œã€‚åœ¨å®é™…åœºæ™¯ä¸­ï¼Œè¯­éŸ³å¢å¼ºæ¨¡å‹å¾€å¾€éœ€è¦åœ¨è¿è¡Œæ—¶å¯¹å™ªå£°è¯­éŸ³è¿›è¡Œå¢å¼ºå¤„ç†ï¼Œè¿™äº›è¯­éŸ³æ˜æ˜¾é•¿äºè®­ç»ƒæ—¶çš„ç‰‡æ®µã€‚åŸºäºTransformerçš„è¯­éŸ³å¢å¼ºæ¨¡å‹å¦‚ä½•æ¨å¹¿åˆ°é•¿è¯­éŸ³ç‰‡æ®µä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡è¿›è¡Œäº†å¤§é‡çš„å®è¯ç ”ç©¶ï¼Œä»¥æ¢ç´¢æ¨¡å‹å¯¹é•¿è¯­éŸ³ç‰‡æ®µçš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨å››ä¸ªè®­ç»ƒç›®æ ‡ä¸Šè¿›è¡Œäº†è¯­éŸ³å¢å¼ºå®éªŒï¼Œå¹¶ç”¨äº”ä¸ªæŒ‡æ ‡è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä½ç½®ç¼–ç æ˜¯å‡å¼±è¯­éŸ³ç‰‡æ®µé•¿åº¦å¯¹è¯­éŸ³å¢å¼ºæ•ˆæœå½±å“çš„æœ‰æ•ˆå·¥å…·ã€‚æˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†ç°æœ‰çš„å‡ ç§ä½ç½®ç¼–ç æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ³•å…·æœ‰æ›´å¥½çš„é•¿åº¦æ³›åŒ–å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸€ç§æ›´ç®€å•æœ‰æ•ˆçš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå³LearnLinã€‚å®ƒä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´åªä½¿ç”¨ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°æ¥ç¼©æ”¾æ—¶é—´å¸§ä¹‹é—´çš„çœŸå®ç›¸å¯¹ä½ç½®ï¼Œå­¦ä¹ è¿™äº›å¤´å¯¹çŸ­æœŸæˆ–é•¿æœŸä¾èµ–çš„ä¸åŒåå¥½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆå…·æœ‰å‡ºè‰²çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ï¼Œä¸å…¶ä»–å…ˆè¿›çš„ä½ç½®ç¼–ç ç­–ç•¥ç›¸æ¯”ï¼Œå…·æœ‰å¯æ¯”æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06697v1">PDF</a> 14 pages; Accepted by TASLP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformerç½‘ç»œæ¶æ„åœ¨è¯­éŸ³å¢å¼ºä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½†å…¶æ ¸å¿ƒæ¨¡å—è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨é•¿è¯­éŸ³ç‰‡æ®µä¸Šçš„åº”ç”¨ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡è¿›è¡Œäº†å¤§é‡å®è¯ç ”ç©¶ï¼Œæ¢ç´¢äº†æ¨¡å‹çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½ç½®ç¼–ç æœ‰åŠ©äºé™ä½è¯­éŸ³é•¿åº¦å¯¹å¢å¼ºæ•ˆæœçš„å½±å“ã€‚æœ¬æ–‡æ¢è®¨äº†å¤šç§ä½ç½®ç¼–ç æ–¹æ³•ï¼Œå‘ç°ç›¸å¯¹ä½ç½®ç¼–ç å±•ç°å‡ºæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ›´ç®€å•æœ‰æ•ˆçš„ä½ç½®ç¼–ç æ–¹æ¡ˆLearnLinï¼Œä»…ä½¿ç”¨ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œå³å¯å®ç°å¯¹æ—¶é—´å¸§é—´çœŸå®ç›¸å¯¹ä½ç½®çš„ç¼©æ”¾ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformerç½‘ç»œåœ¨è¯­éŸ³å¢å¼ºä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
<li>è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨é•¿è¯­éŸ³ç‰‡æ®µçš„åº”ç”¨ã€‚</li>
<li>ä½ç½®ç¼–ç æœ‰åŠ©äºæå‡æ¨¡å‹çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ³•ç›¸è¾ƒäºç»å¯¹ä½ç½®ç¼–ç æ–¹æ³•å±•ç°å‡ºæ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>LearnLinæ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œèƒ½å®ç°å¯¹æ—¶é—´å¸§é—´çœŸå®ç›¸å¯¹ä½ç½®çš„ç¼©æ”¾ã€‚</li>
<li>LearnLinä¸å…¶ä»–å…ˆè¿›çš„ä½ç½®ç¼–ç ç­–ç•¥ç›¸æ¯”ï¼Œå±•ç°å‡ºå“è¶Šçš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a36520be644d7423eb795ba771c18a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-026aa6408de56c0679c70b9fbc7821b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faa82c923a2b526cec88f45cf53b6570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05ec20d30caef9809a0e87022b6082ff.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Fast-and-Lightweight-Model-for-Causal-Audio-Visual-Speech-Separation"><a href="#A-Fast-and-Lightweight-Model-for-Causal-Audio-Visual-Speech-Separation" class="headerlink" title="A Fast and Lightweight Model for Causal Audio-Visual Speech Separation"></a>A Fast and Lightweight Model for Causal Audio-Visual Speech Separation</h2><p><strong>Authors:Wendi Sang, Kai Li, Runxuan Yang, Jianqiang Huang, Xiaolin Hu</strong></p>
<p>Audio-visual speech separation (AVSS) aims to extract a target speech signal from a mixed signal by leveraging both auditory and visual (lip movement) cues. However, most existing AVSS methods exhibit complex architectures and rely on future context, operating offline, which renders them unsuitable for real-time applications. Inspired by the pipeline of RTFSNet, we propose a novel streaming AVSS model, named Swift-Net, which enhances the causal processing capabilities required for real-time applications. Swift-Net adopts a lightweight visual feature extraction module and an efficient fusion module for audio-visual integration. Additionally, Swift-Net employs Grouped SRUs to integrate historical information across different feature spaces, thereby improving the utilization efficiency of historical information. We further propose a causal transformation template to facilitate the conversion of non-causal AVSS models into causal counterparts. Experiments on three standard benchmark datasets (LRS2, LRS3, and VoxCeleb2) demonstrated that under causal conditions, our proposed Swift-Net exhibited outstanding performance, highlighting the potential of this method for processing speech in complex environments. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³åˆ†ç¦»ï¼ˆAVSSï¼‰æ—¨åœ¨åˆ©ç”¨å¬è§‰å’Œè§†è§‰ï¼ˆå”‡éƒ¨åŠ¨ä½œï¼‰çº¿ç´¢ä»æ··åˆä¿¡å·ä¸­æå–ç›®æ ‡è¯­éŸ³ä¿¡å·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„AVSSæ–¹æ³•å…·æœ‰å¤æ‚çš„æ¶æ„ï¼Œå¹¶ä¸”ä¾èµ–äºæœªæ¥ä¸Šä¸‹æ–‡è¿›è¡Œç¦»çº¿æ“ä½œï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚å—RTFSNetæµç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æµå¼AVSSæ¨¡å‹ï¼Œåä¸ºSwift-Netï¼Œå®ƒå¢å¼ºäº†å®æ—¶åº”ç”¨æ‰€éœ€çš„å› æœå¤„ç†èƒ½åŠ›ã€‚Swift-Neté‡‡ç”¨è½»é‡çº§çš„è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œé«˜æ•ˆçš„è§†å¬èåˆæ¨¡å—ã€‚æ­¤å¤–ï¼ŒSwift-Neté‡‡ç”¨åˆ†ç»„ SRUæ¥æ•´åˆä¸åŒç‰¹å¾ç©ºé—´çš„å†å²ä¿¡æ¯ï¼Œä»è€Œæé«˜å†å²ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å› æœè½¬æ¢æ¨¡æ¿ï¼Œä»¥ä¿ƒè¿›éå› æœAVSSæ¨¡å‹å‘å› æœæ¨¡å‹çš„è½¬åŒ–ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æ•°æ®é›†ï¼ˆLRS2ã€LRS3å’ŒVoxCeleb2ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å› æœæ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„Swift-Netè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†è¯¥æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­å¤„ç†è¯­éŸ³çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06689v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å®æ—¶éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æ¨¡å‹Swift-Netï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»æ··åˆä¿¡å·ä¸­æå–ç›®æ ‡è¯­éŸ³ä¿¡å·ï¼Œåˆ©ç”¨å¬è§‰å’Œè§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œï¼‰ã€‚Swift-Netå…·æœ‰è½»é‡çº§çš„è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œé«˜æ•ˆçš„éŸ³é¢‘è§†è§‰èåˆæ¨¡å—ï¼Œå¹¶é‡‡ç”¨Grouped SRUæŠ€æœ¯æ•´åˆå†å²ä¿¡æ¯ï¼Œæé«˜å†å²ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å› æœè½¬æ¢æ¨¡æ¿ï¼Œå¯å°†éå› æœAVSSæ¨¡å‹è½¬æ¢ä¸ºå› æœæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å› æœæ¡ä»¶ä¸‹ï¼ŒSwift-Netåœ¨å¤æ‚ç¯å¢ƒä¸­å¤„ç†è¯­éŸ³çš„æ½œåŠ›å·¨å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Swift-Netæ˜¯ä¸€ç§æ–°å‹çš„éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œæ—¨åœ¨ä»æ··åˆä¿¡å·ä¸­æå–ç›®æ ‡è¯­éŸ³ä¿¡å·ã€‚</li>
<li>Swift-Netåˆ©ç”¨å¬è§‰å’Œè§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œï¼‰è¿›è¡Œè¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>Swift-Netå…·æœ‰è½»é‡çº§çš„è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œé«˜æ•ˆçš„èåˆæ¨¡å—ã€‚</li>
<li>Grouped SRUæŠ€æœ¯ç”¨äºæ•´åˆå†å²ä¿¡æ¯ï¼Œæé«˜å†å²ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å› æœè½¬æ¢æ¨¡æ¿ï¼Œå¯å°†éå› æœAVSSæ¨¡å‹è½¬æ¢ä¸ºå› æœæ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSwift-Netåœ¨å› æœæ¡ä»¶ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d0301e51c1ce025302aa2f6d51486d9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ce124eb4911c14d79c3f7092132d4bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a307986eb87e5ba9bc32456ca12703d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9f5fc7e30f8973a5619722d2cad0e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd15825ce12ddcdad5d62cc89c65f8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74aa001209dc0869fbfddeaa7ebfc9e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62c9c975003e39b0e7f171750ff1897e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AS-ASR-A-Lightweight-Framework-for-Aphasia-Specific-Automatic-Speech-Recognition"><a href="#AS-ASR-A-Lightweight-Framework-for-Aphasia-Specific-Automatic-Speech-Recognition" class="headerlink" title="AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech   Recognition"></a>AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech   Recognition</h2><p><strong>Authors:Chen Bao, Chuanbing Huo, Qinyu Chen, Chang Gao</strong></p>
<p>This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny, tailored for low-resource deployment on edge devices. Our approach introduces a hybrid training strategy that systematically combines standard and aphasic speech at varying ratios, enabling robust generalization, and a GPT-4-based reference enhancement method that refines noisy aphasic transcripts, improving supervision quality. We conduct extensive experiments across multiple data mixing configurations and evaluation settings. Results show that our fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30% while preserving performance on standard speech. The proposed framework offers a scalable, efficient solution for real-world disordered speech recognition. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†AS-ASRï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºWhisper-tinyçš„è½»é‡çº§å¤±è¯­ç—‡ä¸“ç”¨è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡çš„ä½èµ„æºéƒ¨ç½²ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ··åˆè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»¥ä¸åŒçš„æ¯”ä¾‹ç³»ç»Ÿåœ°ç»“åˆäº†æ ‡å‡†å’Œå¤±è¯­è¯­éŸ³ï¼Œå®ç°äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠåŸºäºGPT-4çš„å‚è€ƒå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¼˜åŒ–å˜ˆæ‚çš„å¤±è¯­ç—‡è½¬å½•ï¼Œæé«˜ç›‘ç£è´¨é‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®æ··åˆé…ç½®å’Œè¯„ä¼°è®¾ç½®ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œåœ¨å¤±è¯­è¯­éŸ³ä¸Šçš„WERé™ä½äº†è¶…è¿‡30%ï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†è¯­éŸ³çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ºçœŸå®ä¸–ç•Œä¸­çš„å¤±è¯­è¯­éŸ³è¯†åˆ«æä¾›äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06566v1">PDF</a> Under review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤±è¯­ç—‡æ‚£è€…çš„è½»é‡çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶AS-ASRï¼ŒåŸºäºWhisper-tinyæ„å»ºï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡çš„ä½èµ„æºéƒ¨ç½²ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆè®­ç»ƒç­–ç•¥ï¼Œç»“åˆæ ‡å‡†è¯­éŸ³å’Œå¤±è¯­ç—‡è¯­éŸ³çš„ä¸åŒæ¯”ä¾‹æ•°æ®ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œåˆ©ç”¨GPT-4æŠ€æœ¯ï¼Œå¯¹å˜ˆæ‚çš„å¤±è¯­ç—‡è¯­éŸ³è½¬å½•è¿›è¡Œä¿®æ­£ï¼Œæå‡äº†ç›‘ç£å­¦ä¹ çš„è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®æ··åˆé…ç½®å’Œè¯„ä¼°ç¯å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºé›¶åŸºå‡†æ¨¡å‹æ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨å¤±è¯­ç—‡è¯­éŸ³è¯†åˆ«æ–¹é¢é™ä½äº†è¶…è¿‡30%ï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½ã€‚æ­¤æ¡†æ¶ä¸ºçœŸå®ä¸–ç•Œçš„å¤±è¯­ç—‡è¯­éŸ³è¯†åˆ«æä¾›äº†å¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AS-ASRæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤±è¯­ç—‡æ‚£è€…çš„è½»é‡çº§è¯­éŸ³è¯†åˆ«æ¡†æ¶ï¼Œé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆæ ‡å‡†è¯­éŸ³å’Œå¤±è¯­ç—‡è¯­éŸ³æ•°æ®ï¼Œé‡‡ç”¨æ··åˆè®­ç»ƒç­–ç•¥ã€‚</li>
<li>å¼•å…¥GPT-4æŠ€æœ¯ï¼Œå¯¹å˜ˆæ‚çš„å¤±è¯­ç—‡è¯­éŸ³è½¬å½•è¿›è¡Œä¿®æ­£ï¼Œæå‡ç›‘ç£è´¨é‡ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®æ··åˆé…ç½®å’Œè¯„ä¼°ç¯å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç›¸è¾ƒäºé›¶åŸºå‡†æ¨¡å‹ï¼Œæ–°è¯é”™è¯¯ç‡ï¼ˆWERï¼‰åœ¨å¤±è¯­ç—‡è¯­éŸ³è¯†åˆ«æ–¹é¢é™ä½äº†è¶…è¿‡30%ã€‚</li>
<li>æ¨¡å‹åœ¨ä¿æŒæ ‡å‡†è¯­éŸ³è¯†åˆ«æ€§èƒ½çš„åŒæ—¶ï¼Œä¼˜åŒ–äº†å¤±è¯­ç—‡è¯­éŸ³è¯†åˆ«çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e4ac1a620dac5920c0630e75c8c5a95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd57461f6dcc29eaada13736e27ce3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1e9eedb4c63f8e557004a1a959a061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15a558707a9d0225d8501646e1116639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-147d1d3d03445b42d77a494309d10421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de923e8f54595b4b5a750385227e796e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds"><a href="#DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds" class="headerlink" title="DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds"></a>DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds</h2><p><strong>Authors:Takuya Hasumi, Yusuke Fujita</strong></p>
<p>We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio. Our dataset is available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºç”µå½±éŸ³é¢‘æºåˆ†ç¦»ï¼ˆCASSï¼‰æå‡ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¤„ç†éè¯­è¨€å£°éŸ³ã€‚ç°æœ‰çš„CASSæ•°æ®é›†ä»…åŒ…å«é˜…è¯»é£æ ¼çš„éŸ³é¢‘ä½œä¸ºè¯­éŸ³ä¸»å¹²ã€‚è¿™äº›æ•°æ®é›†ä¸å®é™…çš„ç”µå½±éŸ³é¢‘ä¸åŒï¼Œåè€…æ›´å¯èƒ½åŒ…å«æ¼”ç»å‡ºæ¥çš„å£°éŸ³ã€‚å› æ­¤ï¼Œåœ¨å¸¸è§„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šå‡ºç°é—®é¢˜ï¼Œå³åœ¨æƒ…ç»ªé«˜æ¶¨çš„å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«å£°ï¼‰æ›´å®¹æ˜“è¢«åˆ†ç¦»å‡ºæ¥ä½œä¸ºæ•ˆæœï¼Œè€Œéè¯­éŸ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°çš„æ•°æ®é›†DnR-nonverbalã€‚è¯¥æ•°æ®é›†åŒ…å«è¯­éŸ³ä¸»å¹²ä¸­çš„éè¯­è¨€å£°éŸ³ï¼Œå¦‚ç¬‘å£°å’Œå°–å«å£°ç­‰ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰CASSæ¨¡å‹åœ¨éè¯­è¨€å£°éŸ³æå–æ–¹é¢çš„é—®é¢˜ï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥åœ¨åˆæˆå’Œå®é™…çš„ç”µå½±éŸ³é¢‘ä¸­æœ‰æ•ˆåœ°è§£å†³è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://zenodo.org/records/1547064%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://zenodo.org/records/15470640ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02499v2">PDF</a> Accepted to Interspeech 2025, 5 pages, 3 figures, dataset is   available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a></p>
<p><strong>æ€»ç»“</strong><br>    é’ˆå¯¹ç”µå½±éŸ³é¢‘æºåˆ†ç¦»ï¼ˆCASSï¼‰æå‡ºæ–°çš„æ•°æ®é›†ï¼Œæ¶µç›–éè¯­è¨€å£°éŸ³ã€‚ç°æœ‰CASSæ•°æ®é›†ä»…åŒ…å«é˜…è¯»å¼å£°éŸ³ä½œä¸ºè¯­éŸ³ç´ æï¼Œä¸çœŸå®ç”µå½±éŸ³é¢‘ä¸åŒï¼Œåè€…æ›´å¯èƒ½åŒ…å«è¡¨æ¼”å‡ºçš„å£°éŸ³ã€‚å› æ­¤ï¼Œåœ¨å¸¸è§„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¤„ç†æƒ…ç»ªé«˜æ¶¨çš„å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«ï¼‰æ—¶ï¼Œæ›´å®¹æ˜“å°†å…¶åˆ†ç¦»ä¸ºæ•ˆæœè€Œéè¯­éŸ³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°çš„æ•°æ®é›†DnR-nonverbalï¼Œå…¶ä¸­åŒ…å«ç¬‘å£°å’Œå°–å«ç­‰éè¯­è¨€å£°éŸ³åœ¨è¯­éŸ³ç´ æä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰CASSæ¨¡å‹å­˜åœ¨éè¯­è¨€å£°éŸ³æå–é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨åˆæˆå’ŒçœŸå®ç”µå½±éŸ³é¢‘ä¸­æœ‰æ•ˆè§£å†³æ­¤é—®é¢˜ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://zenodo.org/records/1547064%E5%8F%9B%E8%99%95%E3%80%82">https://zenodo.org/records/15470640è·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰CASSæ•°æ®é›†ä¸»è¦å…³æ³¨é˜…è¯»å¼å£°éŸ³ï¼Œä¸çœŸå®ç”µå½±éŸ³é¢‘å­˜åœ¨å·®å¼‚ã€‚</li>
<li>ç”µå½±éŸ³é¢‘åŒ…å«æ›´å¤šè¡¨æ¼”å‡ºçš„å£°éŸ³ï¼Œå¦‚æƒ…ç»ªé«˜æ¶¨çš„å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«ï¼‰ã€‚</li>
<li>åœ¨å¸¸è§„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¤„ç†éè¯­è¨€å£°éŸ³æ—¶å­˜åœ¨å›°éš¾ï¼Œæ˜“å°†å…¶é”™è¯¯åœ°åˆ†ç¦»ä¸ºæ•ˆæœè€Œéè¯­éŸ³ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºæ–°çš„æ•°æ®é›†DnR-nonverbalï¼ŒåŒ…å«éè¯­è¨€å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«ï¼‰ã€‚</li>
<li>DnR-nonverbalæ•°æ®é›†èƒ½æœ‰æ•ˆè§£å†³å½“å‰CASSæ¨¡å‹åœ¨éè¯­è¨€å£°éŸ³æå–æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†åœ¨åˆæˆå’ŒçœŸå®ç”µå½±éŸ³é¢‘ä¸­éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f6b1f300ba7aa7ca06e6621874d55ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc541b87a835e2a690582d46f4581a58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb12411b02ac22ae5770a36ce8351313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8b2402a95e6e80f9d24ff4b2eb17df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe6a8144804e1c29ece79ed2031fee6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals"><a href="#A-Hypernetwork-Based-Approach-to-KAN-Representation-of-Audio-Signals" class="headerlink" title="A Hypernetwork-Based Approach to KAN Representation of Audio Signals"></a>A Hypernetwork-Based Approach to KAN Representation of Audio Signals</h2><p><strong>Authors:Patryk MarszaÅ‚ek, Maciej Rut, Piotr Kawa, PrzemysÅ‚aw Spurek, Piotr Syga</strong></p>
<p>Implicit neural representations (INR) have gained prominence for efficiently encoding multimedia data, yet their applications in audio signals remain limited. This study introduces the Kolmogorov-Arnold Network (KAN), a novel architecture using learnable activation functions, as an effective INR model for audio representation. KAN demonstrates superior perceptual performance over previous INRs, achieving the lowest Log-SpectralDistance of 1.29 and the highest Perceptual Evaluation of Speech Quality of 3.57 for 1.5 s audio. To extend KANâ€™s utility, we propose FewSound, a hypernetwork-based architecture that enhances INR parameter updates. FewSound outperforms the state-of-the-art HyperSound, with a 33.3% improvement in MSE and 60.87% in SI-SNR. These results show KAN as a robust and adaptable audio representation with the potential for scalability and integration into various hypernetwork frameworks. The source code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git">https://github.com/gmum/fewsound.git</a>. </p>
<blockquote>
<p>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰åœ¨é«˜æ•ˆç¼–ç å¤šåª’ä½“æ•°æ®æ–¹é¢å·²å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å…¶åœ¨éŸ³é¢‘ä¿¡å·ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ–°å‹æ¶æ„ï¼Œä½œä¸ºéŸ³é¢‘è¡¨ç¤ºçš„æœ‰æ•ˆINRæ¨¡å‹ã€‚KANåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šä¼˜äºå…ˆå‰çš„INRï¼Œåœ¨1.5ç§’éŸ³é¢‘ä¸Šå®ç°äº†æœ€ä½çš„Log-SpectralDistanceä¸º1.29å’Œæœ€é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ä¸º3.57ã€‚ä¸ºäº†æ‰©å±•KANçš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œç”¨äºå¢å¼ºINRå‚æ•°æ›´æ–°ã€‚FewSoundçš„æ€§èƒ½ä¼˜äºæœ€æ–°çš„HyperSoundï¼Œåœ¨MSEä¸Šæé«˜äº†33.3%ï¼Œåœ¨SI-SNRä¸Šæé«˜äº†60.87%ã€‚è¿™äº›ç»“æœè¯æ˜äº†KANæ˜¯ä¸€ä¸ªç¨³å¥ä¸”é€‚åº”æ€§å¼ºçš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œé›†æˆåˆ°å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚æºä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/gmum/fewsound.git%E3%80%82">https://github.com/gmum/fewsound.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02585v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼•å…¥Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ï¼Œå®ƒæ˜¯ä¸€ç§ä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°çš„æ–°å‹æ¶æ„ï¼Œä½œä¸ºä¸€ç§æœ‰æ•ˆçš„éšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¨¡å‹ç”¨äºéŸ³é¢‘è¡¨ç¤ºã€‚KANåœ¨éŸ³é¢‘ä¿¡å·ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„INRæ¨¡å‹ï¼Œå®ç°äº†æœ€ä½çš„Log-SpectralDistanceå’Œæœ€é«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ã€‚ä¸ºæ‰©å±•KANçš„å®ç”¨æ€§ï¼Œæå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œèƒ½å¤Ÿå¢å¼ºINRå‚æ•°æ›´æ–°ã€‚FewSoundä¼˜äºå½“å‰æœ€å…ˆè¿›çš„HyperSoundï¼Œåœ¨MSEå’ŒSI-SNRæ–¹é¢åˆ†åˆ«æé«˜äº†33.3%å’Œ60.87%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒKANæ˜¯ä¸€ç§ç¨³å¥ä¸”å¯é€‚åº”çš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œé›†æˆåˆ°å„ç§è¶…ç½‘ç»œæ¡†æ¶çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä½œä¸ºéšç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¨¡å‹çš„æ–°æ¶æ„ï¼Œä¸“é—¨ç”¨äºéŸ³é¢‘è¡¨ç¤ºã€‚</li>
<li>KANé€šè¿‡ä½¿ç”¨å¯å­¦ä¹ æ¿€æ´»å‡½æ•°æé«˜äº†éŸ³é¢‘è¡¨ç¤ºçš„æ„ŸçŸ¥æ€§èƒ½ã€‚</li>
<li>KANåœ¨éŸ³é¢‘ä¿¡å·ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„INRæ¨¡å‹ï¼Œå®ç°äº†è¾ƒä½çš„Log-SpectralDistanceå’Œè¾ƒé«˜çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ã€‚</li>
<li>æå‡ºäº†åŸºäºè¶…ç½‘ç»œçš„FewSoundæ¶æ„ï¼Œä»¥å¢å¼ºINRæ¨¡å‹çš„å‚æ•°æ›´æ–°ã€‚</li>
<li>FewSoundåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„HyperSoundï¼Œåœ¨MSEå’ŒSI-SNRæ–¹é¢æœ‰æ˜æ˜¾çš„æ”¹è¿›ã€‚</li>
<li>KANçš„éŸ³é¢‘è¡¨ç¤ºæ–¹æ³•å…·æœ‰ç¨³å¥æ€§å’Œé€‚åº”æ€§ï¼Œæœ‰æœ›æ‰©å±•åˆ°æ›´å¤šåº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8a2d45823a3a19b9af6bf8d34fef74c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d66decd69f9cb4471f6f979dacdeb64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcfba1594c7efd2bfc4dcea05d811d00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-575341573b922c275b961ac081aba667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be934bc7cbedaae63f4effb10e94f3e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-619bdfeee4b4ab257ad336173b5e1101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3623bae21d9b5681c9e561211c78652e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14e564406de7e20a3290295beda4a6a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement"><a href="#LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement" class="headerlink" title="LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement"></a>LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement</h2><p><strong>Authors:Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie</strong></p>
<p>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area. </p>
<blockquote>
<p>è¿‘æœŸè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„è¿›å±•åœ¨è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢å±•ç¤ºäº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œè¿™åœ¨ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä¸­å°¤å…¶æ˜¾è‘—ã€‚ç„¶è€Œï¼Œè®¸å¤šåŸºäºLMçš„SEæ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¾€å¾€å¿½è§†äº†å£°éŸ³ä¿¡æ¯çš„å…³é”®ä½œç”¨ï¼Œè¿™å¯¼è‡´å¢å¼ºåçš„è¯­éŸ³åœ¨å£°éŸ³ä¸Šå­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„SEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaSE-G1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ¿€åŠ±å…¶åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚LLaSE-G1çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œä¸ºäº†å‡è½»å£°éŸ³ä¸ä¸€è‡´çš„é—®é¢˜ï¼ŒLLaSE-G1é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨X-Codec2é¢„æµ‹è¯­éŸ³ä»¤ç‰Œï¼Œä»¥æœ€å¤§é™åº¦åœ°ä¿ç•™å£°éŸ³ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æå‡æ³›åŒ–èƒ½åŠ›ï¼ŒLLaSE-G1å¼•å…¥äº†åŒé€šé“è¾“å…¥å’Œè¾“å‡ºï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤šä¸ªSEä»»åŠ¡è€Œæ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚æœ€åï¼ŒLLaSE-G1è¶…è¶Šäº†å…ˆå‰çš„ç‰¹å®šä»»åŠ¡çš„åˆ¤åˆ«æ€§å’Œç”Ÿæˆæ€§SEæ¨¡å‹ï¼Œåœ¨æµ‹è¯•æ—¶å±•ç¤ºäº†è§„æ¨¡æ•ˆåº”ï¼Œå¹¶å¯¹æœªè§è¿‡çš„SEä»»åŠ¡å±•ç°äº†æ–°å…´èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00493v3">PDF</a> ACL2025 main, Codes available at   <a target="_blank" rel="noopener" href="https://github.com/Kevin-naticl/LLaSE-G1">https://github.com/Kevin-naticl/LLaSE-G1</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„è¿›æ­¥åœ¨è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰é¢†åŸŸå°¤ä¸ºçªå‡ºã€‚ç„¶è€Œï¼Œè®¸å¤šåŸºäºLMçš„SEæ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¿½è§†äº†å£°å­¦ä¿¡æ¯çš„å…³é”®ä½œç”¨ï¼Œå¯¼è‡´å¢å¼ºåçš„è¯­éŸ³å‡ºç°å£°å­¦ä¸ä¸€è‡´æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„SEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†LLaSE-G1ï¼Œä¸€ç§åŸºäºLLaMAçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è¯­éŸ³å¢å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚LLaSE-G1çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œä¸ºäº†ç¼“è§£å£°å­¦ä¸ä¸€è‡´æ€§ï¼ŒLLaSE-G1é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»X-Codec2é¢„æµ‹è¯­éŸ³æ ‡è®°ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°ä¿ç•™å£°å­¦ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æå‡æ³›åŒ–èƒ½åŠ›ï¼ŒLLaSE-G1å¼•å…¥äº†åŒé€šé“è¾“å…¥å’Œè¾“å‡ºï¼Œç»Ÿä¸€äº†å¤šä¸ªSEä»»åŠ¡ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚æœ€åï¼ŒLLaSE-G1åœ¨æµ‹è¯•æ—¶è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰ä»»åŠ¡ç‰¹å®šçš„åˆ¤åˆ«æ€§å’Œç”Ÿæˆæ€§SEæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå¹¶å±•ç°å‡ºå¯¹æœªè§è¿‡çš„SEä»»åŠ¡çš„é€‚åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLaSE-G1é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºå’ŒX-Codec2çš„è¯­éŸ³æ ‡è®°é¢„æµ‹ï¼Œä»¥ç¼“è§£è¯­éŸ³å¢å¼ºä¸­çš„å£°å­¦ä¸ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒé€šé“è¾“å…¥å’Œè¾“å‡ºï¼ŒLLaSE-G1ç»Ÿä¸€äº†å¤šä¸ªè¯­éŸ³å¢å¼ºä»»åŠ¡ï¼Œæå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LLaSE-G1åœ¨æµ‹è¯•æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œè¶…è¶Šäº†ä»»åŠ¡ç‰¹å®šçš„åˆ¤åˆ«æ€§å’Œç”Ÿæˆæ€§SEæ¨¡å‹ã€‚</li>
<li>LLaSE-G1å¯¹æœªè§çš„SEä»»åŠ¡å…·æœ‰é€‚åº”èƒ½åŠ›ã€‚</li>
<li>LLaSE-G1æ¨¡å‹èƒ½å¤Ÿæœ€å¤§åŒ–åœ°ä¿ç•™å£°å­¦ç‰¹å¾ã€‚</li>
<li>å…¬å¼€çš„ä»£ç å’Œæ¨¡å‹æœ‰åŠ©äºæ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d980168bf2154bb32011e6a510c1745.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b0519543f682a16f37db49b59d4486d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Long-duration-Talking-Video-Synthesis-with-Linear-Diffusion-Transformer-under-Multimodal-Guidance"><a href="#Efficient-Long-duration-Talking-Video-Synthesis-with-Linear-Diffusion-Transformer-under-Multimodal-Guidance" class="headerlink" title="Efficient Long-duration Talking Video Synthesis with Linear Diffusion   Transformer under Multimodal Guidance"></a>Efficient Long-duration Talking Video Synthesis with Linear Diffusion   Transformer under Multimodal Guidance</h2><p><strong>Authors:Haojie Zhang, Zhihao Liang, Ruibo Fu, Bingyan Liu, Zhengqi Wen, Xuefei Liu, Chenxing Li, Yaling Liang</strong></p>
<p>Portrait image animation using audio has rapidly advanced, but challenges remain in efficiently fusing multimodal inputs while ensuring temporal and portrait consistency with minimal computational cost. To address this, we present LetsTalk, a LinEar diffusion TranSformer for Talking video synthesis. LetsTalk incorporates a deep compression autoencoder to obtain efficient latent representations, and a spatio-temporal-aware transformer with efficient linear attention to effectively fuse multimodal information and enhance spatio-temporal consistency. We systematically explore and summarize three fusion schemes, ranging from shallow to deep fusion. We thoroughly analyze their characteristics, applicability, and trade-offs, thereby bridging critical gaps in multimodal conditional guidance. Based on modality differences of image, audio, and video generation, we adopt deep (Symbiotic Fusion) for portrait to ensure consistency, and shallow (Direct Fusion) for audio to align animation with speech while preserving motion diversity. To maintain temporal consistency in long-duration video generation, we propose a memory bank mechanism that preserves inter-clip dependencies, effectively preventing degradation across extended sequences. Furthermore, we develop a noise-regularized training strategy that explicitly compensates for DDPM sampling artifacts, significantly improving the modelâ€™s robustness in continuous generation scenarios.Our extensive experiments demonstrate that our approach achieves state-of-the-art generation quality, producing temporally coherent and realistic videos with enhanced diversity and liveliness, while maintaining remarkable efficiency through its optimized model design with 8$\times$ fewer parameters. </p>
<blockquote>
<p>åŸºäºéŸ³é¢‘çš„è‚–åƒå›¾åƒåŠ¨ç”»æŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†åœ¨ç¡®ä¿æ—¶é—´å’Œè‚–åƒè¿è´¯æ€§çš„åŒæ—¶ï¼Œé«˜æ•ˆèåˆå¤šæ¨¡å¼è¾“å…¥ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LetsTalkï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºè§†é¢‘åˆæˆçš„çº¿æ€§æ‰©æ•£Transformerã€‚LetsTalkç»“åˆæ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ä»¥è·å¾—æœ‰æ•ˆçš„æ½œåœ¨è¡¨ç¤ºï¼Œä»¥åŠå…·æœ‰é«˜æ•ˆçº¿æ€§æ³¨æ„åŠ›çš„æ—¶ç©ºæ„ŸçŸ¥è½¬æ¢å™¨ï¼Œä»¥æœ‰æ•ˆèåˆå¤šæ¨¡å¼ä¿¡æ¯å¹¶å¢å¼ºæ—¶ç©ºè¿è´¯æ€§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢å’Œæ€»ç»“äº†ä»æµ…åˆ°æ·±èåˆçš„ä¸‰ç§èåˆæ–¹æ¡ˆã€‚æˆ‘ä»¬å½»åº•åˆ†æäº†å®ƒä»¬çš„ç‰¹æ€§ã€é€‚ç”¨æ€§å’Œæƒè¡¡ï¼Œä»è€Œå¡«è¡¥äº†å¤šæ¨¡å¼æ¡ä»¶æŒ‡å¯¼çš„å…³é”®ç©ºç™½ã€‚åŸºäºå›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆçš„æ¨¡æ€å·®å¼‚ï¼Œæˆ‘ä»¬é‡‡ç”¨æ·±åº¦ï¼ˆå…±ç”Ÿèåˆï¼‰èåˆè‚–åƒä»¥ç¡®ä¿è¿è´¯æ€§ï¼Œä»¥åŠæµ…å±‚ï¼ˆç›´æ¥èåˆï¼‰èåˆéŸ³é¢‘ï¼Œä»¥ä½¿åŠ¨ç”»ä¸è¯­éŸ³å¯¹é½ï¼ŒåŒæ—¶ä¿æŒè¿åŠ¨å¤šæ ·æ€§ã€‚ä¸ºäº†åœ¨é•¿æœŸè§†é¢‘ç”Ÿæˆä¸­ä¿æŒæ—¶é—´è¿è´¯æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…å­˜é“¶è¡Œæœºåˆ¶ï¼Œä¿ç•™å‰ªè¾‘é—´çš„ä¾èµ–æ€§ï¼Œæœ‰æ•ˆé˜²æ­¢æ‰©å±•åºåˆ—ä¸­çš„é€€åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å™ªå£°æ­£åˆ™åŒ–è®­ç»ƒç­–ç•¥ï¼Œæ˜ç¡®è¡¥å¿DDPMé‡‡æ ·ä¼ªå½±ï¼Œå¤§å¤§æé«˜äº†æ¨¡å‹åœ¨è¿ç»­ç”Ÿæˆåœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç”Ÿæˆè´¨é‡ï¼Œäº§ç”Ÿäº†æ—¶é—´è¿è´¯ä¸”é€¼çœŸçš„è§†é¢‘ï¼Œå…·æœ‰å¢å¼ºçš„å¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ï¼ŒåŒæ—¶é€šè¿‡å…¶ä¼˜åŒ–åçš„æ¨¡å‹è®¾è®¡ä¿æŒäº†å“è¶Šçš„æ•ˆç‡ï¼Œå‚æ•°å‡å°‘äº†8å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16748v2">PDF</a> 16 pages, 13 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºLetsTalkçš„çº¿æ€§æ‰©æ•£Transformerè¯­éŸ³åŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºåˆæˆè¯­éŸ³è§†é¢‘ã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨è·å¾—é«˜æ•ˆæ½œåœ¨è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨æ—¶ç©ºæ„ŸçŸ¥çš„Transformerå’Œé«˜æ•ˆçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œæé«˜æ—¶ç©ºä¸€è‡´æ€§ã€‚åŒæ—¶æ¢è®¨äº†ä¸‰ç§èåˆæ–¹æ¡ˆï¼Œé‡‡ç”¨æ·±åº¦èåˆç¡®ä¿è‚–åƒä¸€è‡´æ€§ï¼Œæµ…å±‚èåˆä½¿åŠ¨ç”»ä¸è¯­éŸ³å¯¹é½å¹¶ä¿æŒè¿åŠ¨å¤šæ ·æ€§ã€‚ä¸ºç»´æŒé•¿è§†é¢‘ç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§ï¼Œæå‡ºè®°å¿†åº“æœºåˆ¶ï¼Œé€šè¿‡ä¿å­˜å‰ªè¾‘é—´ä¾èµ–å…³ç³»é˜²æ­¢åºåˆ—æ‰©å±•æ—¶çš„æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œå¼€å‘å™ªå£°æ­£åˆ™åŒ–è®­ç»ƒç­–ç•¥ï¼Œæ˜ç¡®è¡¥å¿DDPMé‡‡æ ·ä¼ªå½±ï¼Œæé«˜è¿ç»­ç”Ÿæˆåœºæ™¯ä¸‹çš„æ¨¡å‹ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆè´¨é‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œèƒ½ç”Ÿæˆæ—¶åºè¿è´¯ã€é€¼çœŸçš„è§†é¢‘ï¼Œå…·æœ‰å¢å¼ºå¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ï¼Œä¸”é€šè¿‡ä¼˜åŒ–æ¨¡å‹è®¾è®¡å®ç°é«˜æ•ˆç‡ï¼Œå‚æ•°å‡å°‘8å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåä¸ºLetsTalkçš„çº¿æ€§æ‰©æ•£Transformeræ¨¡å‹ç”¨äºè¯­éŸ³é©±åŠ¨çš„è‚–åƒåŠ¨ç”»ç”Ÿæˆã€‚</li>
<li>å¼•å…¥æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨ä»¥å®ç°é«˜æ•ˆæ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨æ—¶ç©ºæ„ŸçŸ¥çš„Transformerå’Œçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤šæ¨¡æ€ä¿¡æ¯èåˆã€‚</li>
<li>æ¢è®¨äº†ä¸‰ç§èåˆæ–¹æ¡ˆï¼Œå¹¶æ ¹æ®å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆçš„ç‰¹ç‚¹é‡‡ç”¨ä¸åŒçš„èåˆç­–ç•¥ã€‚</li>
<li>ä¸ºä¿æŒé•¿è§†é¢‘ç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§ï¼Œå¼•å…¥è®°å¿†åº“æœºåˆ¶ä¿å­˜å‰ªè¾‘é—´ä¾èµ–å…³ç³»ã€‚</li>
<li>å¼€å‘å™ªå£°æ­£åˆ™åŒ–è®­ç»ƒç­–ç•¥ä»¥æ”¹å–„æ¨¡å‹çš„è¿ç»­ç”Ÿæˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8469b9c6d8ec7622e84aef78bf26ea09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6c01db42660c949271c61b08375b387.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79c59191fa802b8919da776ea2178282.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ede0c2e0d671797785e41ae0ccd023b1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dynamic-SUPERB-Phase-2-A-Collaboratively-Expanding-Benchmark-for-Measuring-the-Capabilities-of-Spoken-Language-Models-with-180-Tasks"><a href="#Dynamic-SUPERB-Phase-2-A-Collaboratively-Expanding-Benchmark-for-Measuring-the-Capabilities-of-Spoken-Language-Models-with-180-Tasks" class="headerlink" title="Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for   Measuring the Capabilities of Spoken Language Models with 180 Tasks"></a>Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for   Measuring the Capabilities of Spoken Language Models with 180 Tasks</h2><p><strong>Authors:Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Chih-Kai Yang, Wenze Ren, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Fabian Ritter-Gutierrez, Kuan-Po Huang, Siddhant Arora, You-Kuan Lin, Ming To Chuang, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, Jun-You Wang, Cheng-Hsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor R. GuimarÃ£es, Jionghao Han, Tzu-Quan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, Yu-Hua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, Kuan-Yu Fang Chiang, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, Hsi-Che Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Jui-Chiang Wei, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Vahid Noroozi, Zhehuai Chen, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, Hung-yi Lee</strong></p>
<p>Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at <a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb">https://github.com/dynamic-superb/dynamic-superb</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå¦‚Geminiå’ŒChatGPTï¼Œé€šè¿‡æ— ç¼é›†æˆå„ç§å½¢å¼çš„æ•°æ®ï¼Œå·²ç»å½»åº•æ”¹å˜äº†äººæœºäº¤äº’ã€‚å¼€å‘ä¸€ç§èƒ½å¤Ÿç†è§£å¹¿æ³›è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„é€šç”¨å£è¯­æ¨¡å‹ï¼Œå¯¹äºå¼¥åˆæ²Ÿé€šå·®è·å’Œä¿ƒè¿›æ›´ç›´è§‚çš„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†æ„æˆäº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†Dynamic-SUPERB Phase-2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”ä¸æ–­å‘å±•çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°åŸºäºæŒ‡ä»¤çš„é€šç”¨è¯­éŸ³æ¨¡å‹ã€‚åŸºäºç¬¬ä¸€ä»£çš„åŸºç¡€ä¸Šï¼Œç¬¬äºŒç‰ˆçº³å…¥äº†å…¨çƒç ”ç©¶ç¤¾åŒºåˆä½œè´¡çŒ®çš„12 5é¡¹æ–°ä»»åŠ¡ï¼Œä½¿åŸºå‡†æµ‹è¯•çš„ä»»åŠ¡æ€»æ•°æ‰©å¤§åˆ°1 8 0é¡¹ï¼Œæˆä¸ºè¯­éŸ³å’ŒéŸ³é¢‘è¯„ä¼°é¢†åŸŸæœ€å¤§çš„åŸºå‡†æµ‹è¯•ã€‚è™½ç„¶Dynamic-SUPERBç¬¬ä¸€ä»£ä»…é™äºåˆ†ç±»ä»»åŠ¡ï¼Œä½†Dynamic-SUPERB Phase-2é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—æ–°é¢–ä¸”å¤šæ ·åŒ–çš„ä»»åŠ¡æ¥æ‹“å®½å…¶è¯„ä¼°èƒ½åŠ›ï¼ŒåŒ…æ‹¬å›å½’å’Œåºåˆ—ç”Ÿæˆï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œç¯ä¿éŸ³é¢‘ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰æ¨¡å‹èƒ½å¤Ÿæ™®éè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚SALMONN-13Båœ¨è‹±è¯­è¯­éŸ³è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒQwen2-Audio-7B-Instructåœ¨æƒ…ç»ªè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ï¼Œä½†å½“å‰æ¨¡å‹ä»éœ€è¦è¿›ä¸€æ­¥åˆ›æ–°æ‰èƒ½å¤„ç†æ›´å¹¿æ³›çš„èŒƒå›´çš„ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb%E5%BC%80%E6%BA%90%E4%BA%86%E6%89%80%E6%9C%89%E4%BB%BB%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%92%8C%E8%AF%84%E4%BC%B0%E7%AE%A1%E9%81%93%E3%80%82">https://github.com/dynamic-superb/dynamic-superbå¼€æºäº†æ‰€æœ‰ä»»åŠ¡æ•°æ®å’Œè¯„ä¼°ç®¡é“ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05361v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå¦‚Geminiå’ŒChatGPTï¼Œé€šè¿‡æ— ç¼é›†æˆå„ç§å½¢å¼çš„æ•°æ®ï¼Œå·²ç»é©æ–°äº†äººæœºäº’åŠ¨æ–¹å¼ã€‚å¼€å‘ä¸€ç§é€šç”¨å£è¯­æ¨¡å‹ï¼Œç†è§£å¹¿æ³›çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¯¹äºå¼¥åˆæ²Ÿé€šé¸¿æ²Ÿå’Œä¿ƒè¿›æ›´ç›´è§‚äº’åŠ¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºDynamic-SUPERB Phase-2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾å’Œä¸æ–­å‘å±•çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°æŒ‡ä»¤å‹é€šç”¨è¯­éŸ³æ¨¡å‹ã€‚ç›¸è¾ƒäºç¬¬ä¸€ä»£ï¼Œç¬¬äºŒä»£æ–°å¢äº†å…¨çƒç ”ç©¶ç¤¾åŒºå…±åŒè´¡çŒ®çš„125ä¸ªæ–°ä»»åŠ¡ï¼Œä½¿åŸºå‡†æµ‹è¯•ä»»åŠ¡æ€»æ•°æ‰©å……è‡³180ä¸ªï¼Œæˆä¸ºè¯­éŸ³å’ŒéŸ³é¢‘è¯„ä¼°é¢†åŸŸæœ€å¤§çš„åŸºå‡†æµ‹è¯•ã€‚Dynamic-SUPERB Phase-2ä¸ä»…æ‹“å®½äº†è¯„ä¼°èƒ½åŠ›ï¼Œè¿˜å¼•å…¥äº†å„ç§æ–°é¢–ä¸”å¤šæ ·åŒ–çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›å½’å’Œåºåˆ—ç”Ÿæˆï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œç¯ä¿éŸ³é¢‘ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ²¡æœ‰æ¨¡å‹èƒ½å¤Ÿæ™®éè¡¨ç°ä¼˜å¼‚ã€‚SALMONN-1_åœ¨è‹±æ–‡è¯­éŸ³è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒQwen2 Audioåˆ™èƒ½ç²¾å‡†è¯†åˆ«æƒ…ç»ªã€‚å½“å‰æ¨¡å‹ä»éœ€è¦è¿›ä¸€æ­¥åˆ›æ–°å¤„ç†æ›´å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å·²åœ¨å¼€æºç¤¾åŒºå…¬å¼€æ‰€æœ‰ä»»åŠ¡æ•°æ®å’Œè¯„ä¼°æµç¨‹ç®¡é“ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb%EF%BC%89%E3%80%82">https://github.com/dynamic-superb/dynamic-superbï¼‰ã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¦‚Geminiå’ŒChatGPTé©æ–°äº†äººæœºäº’åŠ¨æ–¹å¼ã€‚</li>
<li>å¼€å‘é€šç”¨å£è¯­æ¨¡å‹å¯¹äºå¼¥åˆæ²Ÿé€šé¸¿æ²Ÿå’Œä¿ƒè¿›æ›´ç›´è§‚äº’åŠ¨è‡³å…³é‡è¦ã€‚</li>
<li>ç¼ºä¹å…¨é¢çš„è¯„ä¼°åŸºå‡†æ˜¯å¯¹è¯­éŸ³æ¨¡å‹å‘å±•çš„é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>Dynamic-SUPERB Phase-2æ˜¯å¼€æ”¾å’Œä¸æ–­å‘å±•çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°æŒ‡ä»¤å‹é€šç”¨è¯­éŸ³æ¨¡å‹ã€‚</li>
<li>Dynamic-SUPERB Phase-2å…±åŒ…å«180ä¸ªä»»åŠ¡ï¼Œæ¶µç›–è¯­éŸ³ã€éŸ³ä¹å’Œç¯ä¿éŸ³é¢‘ç­‰å¤šä¸ªé¢†åŸŸçš„æ–°é¢–å¤šæ ·åŒ–ä»»åŠ¡ã€‚</li>
<li>ç›®å‰æ²¡æœ‰æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„åˆ›æ–°å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24a22bab331141b95aecc9a9d5a663fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5582bb894c0a4334c08b88fce86e5ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec906cde41f3c346593d2a24f85cf424.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-613aa4ad09b6348e5a5671af6e7d713b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae59827483b9627327bd3e4196668041.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  GHOST 2.0 generative high-fidelity one shot transfer of heads
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-826ae48f553e7d49fa66e17bb9a5eae4.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Variational Supervised Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
