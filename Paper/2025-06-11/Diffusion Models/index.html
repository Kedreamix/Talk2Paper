<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  StableMTL Repurposing Latent Diffusion Models for Multi-Task Learning   from Partially Annotated Synthetic Datasets">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ae032f62b8b073c8f4f35b9a54924c4e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="StableMTL-Repurposing-Latent-Diffusion-Models-for-Multi-Task-Learning-from-Partially-Annotated-Synthetic-Datasets"><a href="#StableMTL-Repurposing-Latent-Diffusion-Models-for-Multi-Task-Learning-from-Partially-Annotated-Synthetic-Datasets" class="headerlink" title="StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning   from Partially Annotated Synthetic Datasets"></a>StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning   from Partially Annotated Synthetic Datasets</h2><p><strong>Authors:Anh-Quan Cao, Ivan Lopes, Raoul de Charette</strong></p>
<p>Multi-task learning for dense prediction is limited by the need for extensive annotation for every task, though recent works have explored training with partial task labels. Leveraging the generalization power of diffusion models, we extend the partial learning setup to a zero-shot setting, training a multi-task model on multiple synthetic datasets, each labeled for only a subset of tasks. Our method, StableMTL, repurposes image generators for latent regression. Adapting a denoising framework with task encoding, per-task conditioning and a tailored training scheme. Instead of per-task losses requiring careful balancing, a unified latent loss is adopted, enabling seamless scaling to more tasks. To encourage inter-task synergy, we introduce a multi-stream model with a task-attention mechanism that converts N-to-N task interactions into efficient 1-to-N attention, promoting effective cross-task sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks. </p>
<blockquote>
<p>å¤šä»»åŠ¡å­¦ä¹ åœ¨å¯†é›†é¢„æµ‹ä¸Šçš„åº”ç”¨å—é™äºæ¯ä¸ªä»»åŠ¡éœ€è¦å¤§é‡æ ‡æ³¨çš„éœ€æ±‚ï¼Œå°½ç®¡æœ€è¿‘æœ‰æ¢ç´¢éƒ¨åˆ†ä»»åŠ¡æ ‡ç­¾çš„è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°†éƒ¨åˆ†å­¦ä¹ è®¾ç½®æ‰©å±•åˆ°é›¶æ ·æœ¬è®¾ç½®ï¼Œåœ¨å¤šä¸ªåˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒå¤šä»»åŠ¡æ¨¡å‹ï¼Œæ¯ä¸ªæ•°æ®é›†åªå¯¹éƒ¨åˆ†ä»»åŠ¡è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•StableMTLé‡æ–°å®šä½å›¾åƒç”Ÿæˆå™¨ç”¨äºæ½œåœ¨å›å½’ã€‚é€šè¿‡ä»»åŠ¡ç¼–ç ã€æ¯ä¸ªä»»åŠ¡çš„æ¡ä»¶å’Œä¸€ä¸ªé‡èº«å®šåˆ¶çš„è®­ç»ƒæ–¹æ¡ˆæ¥é€‚åº”å»å™ªæ¡†æ¶ã€‚ä¸å†ä½¿ç”¨éœ€è¦ä»”ç»†å¹³è¡¡çš„æ¯ä¸ªä»»åŠ¡æŸå¤±ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„æ½œåœ¨æŸå¤±ï¼Œå¯ä»¥è½»æ¾æ‰©å±•åˆ°æ›´å¤šä»»åŠ¡ã€‚ä¸ºäº†é¼“åŠ±ä»»åŠ¡é—´çš„ååŒä½œç”¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…·æœ‰ä»»åŠ¡æ³¨æ„åŠ›æœºåˆ¶çš„å¤šæµæ¨¡å‹ï¼Œå°†N-to-Nä»»åŠ¡äº¤äº’è½¬æ¢ä¸ºé«˜æ•ˆçš„1-to-Næ³¨æ„åŠ›ï¼Œä¿ƒè¿›æœ‰æ•ˆçš„è·¨ä»»åŠ¡å…±äº«ã€‚StableMTLåœ¨8ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„7ä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08013v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/astra-vision/StableMTL">https://github.com/astra-vision/StableMTL</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šä»»åŠ¡å­¦ä¹ åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹å¤šä»»åŠ¡å­¦ä¹ éœ€è¦å¤§é‡æ ‡æ³¨çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºStableMTLçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å¯¹å¤šä¸ªåˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªæ•°æ®é›†ä»…é’ˆå¯¹éƒ¨åˆ†ä»»åŠ¡è¿›è¡Œæ ‡æ³¨ã€‚è¯¥æ–¹æ³•é€šè¿‡å›¾åƒç”Ÿæˆå™¨è¿›è¡Œæ½œåœ¨å›å½’ï¼Œé‡‡ç”¨å»å™ªæ¡†æ¶å¹¶ç»“åˆä»»åŠ¡ç¼–ç ã€ä»»åŠ¡æ¡ä»¶åŠå®šåˆ¶è®­ç»ƒæ–¹æ¡ˆã€‚é€šè¿‡ç»Ÿä¸€æ½œåœ¨æŸå¤±æ›¿ä»£äº†éœ€è¦ç²¾ç»†å¹³è¡¡çš„ä»»åŠ¡æŸå¤±ï¼Œå¹¶æ”¯æŒæ— ç¼æ‰©å±•åˆ°æ›´å¤šä»»åŠ¡ã€‚ä¸ºå¢å¼ºä»»åŠ¡é—´çš„ååŒä½œç”¨ï¼Œæ–‡ç« å¼•å…¥äº†å¤šä»»åŠ¡æµæ¨¡å‹ï¼Œé‡‡ç”¨ä»»åŠ¡æ³¨æ„åŠ›æœºåˆ¶å°†N-to-Nä»»åŠ¡äº¤äº’è½¬æ¢ä¸ºé«˜æ•ˆçš„1-to-Næ³¨æ„åŠ›ï¼Œä¿ƒè¿›è·¨ä»»åŠ¡çš„å…±äº«ã€‚StableMTLåœ¨7ä¸ªä»»åŠ¡ã€8ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡è¶…è¿‡åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†StableMTLæ–¹æ³•ï¼Œå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå¤šä»»åŠ¡å­¦ä¹ ä¸­çš„å¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>StableMTLåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è®­ç»ƒï¼Œåˆ©ç”¨åˆæˆæ•°æ®é›†å¹¶ä»…å¯¹éƒ¨åˆ†ä»»åŠ¡è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>é€šè¿‡å›¾åƒç”Ÿæˆå™¨è¿›è¡Œæ½œåœ¨å›å½’ï¼Œé‡‡ç”¨å»å™ªæ¡†æ¶ç»“åˆä»»åŠ¡ç¼–ç ã€æ¡ä»¶åŠå®šåˆ¶è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨äº†ç»Ÿä¸€æ½œåœ¨æŸå¤±ï¼Œæ— éœ€ç²¾ç»†å¹³è¡¡å„ä»»åŠ¡æŸå¤±ï¼Œå¹¶å¯ä»¥æ— ç¼æ‰©å±•åˆ°æ›´å¤šä»»åŠ¡ã€‚</li>
<li>å¼•å…¥å¤šä»»åŠ¡æµæ¨¡å‹åŠä»»åŠ¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°ä»»åŠ¡é—´çš„æœ‰æ•ˆå…±äº«å’ŒååŒã€‚</li>
<li>StableMTLåœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54e9497499da003113051e6dbbf9680c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-129bfe2d37e02a9a9d88227c2ca27829.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a33a5a917f2ce948ec8fc59078684ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77bd604f069560baec6f503465a30dd9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generative-Modeling-of-Weights-Generalization-or-Memorization"><a href="#Generative-Modeling-of-Weights-Generalization-or-Memorization" class="headerlink" title="Generative Modeling of Weights: Generalization or Memorization?"></a>Generative Modeling of Weights: Generalization or Memorization?</h2><p><strong>Authors:Boya Zeng, Yida Yin, Zhiqiu Xu, Zhuang Liu</strong></p>
<p>Generative models, with their success in image and video generation, have recently been explored for synthesizing effective neural network weights. These approaches take trained neural network checkpoints as training data, and aim to generate high-performing neural network weights during inference. In this work, we examine four representative methods on their ability to generate novel model weights, i.e., weights that are different from the checkpoints seen during training. Surprisingly, we find that these methods synthesize weights largely by memorization: they produce either replicas, or at best simple interpolations, of the training checkpoints. Current methods fail to outperform simple baselines, such as adding noise to the weights or taking a simple weight ensemble, in obtaining different and simultaneously high-performing models. We further show that this memorization cannot be effectively mitigated by modifying modeling factors commonly associated with memorization in image diffusion models, or applying data augmentations. Our findings provide a realistic assessment of what types of data current generative models can model, and highlight the need for more careful evaluation of generative models in new domains. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/boyazeng/weight_memorization">https://github.com/boyazeng/weight_memorization</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œæœ€è¿‘è¢«æ¢ç´¢ç”¨äºåˆæˆæœ‰æ•ˆçš„ç¥ç»ç½‘ç»œæƒé‡ã€‚è¿™äº›æ–¹æ³•å°†è®­ç»ƒè¿‡çš„ç¥ç»ç½‘ç»œçš„æ£€æŸ¥ç‚¹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆé«˜æ€§èƒ½çš„ç¥ç»ç½‘ç»œæƒé‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†å››ç§ä»£è¡¨æ€§æ–¹æ³•åœ¨ç”Ÿæˆæ–°å‹æ¨¡å‹æƒé‡æ–¹é¢çš„èƒ½åŠ›ï¼Œå³ä¸åŒäºè®­ç»ƒè¿‡ç¨‹ä¸­æ‰€è§æ£€æŸ¥ç‚¹çš„æƒé‡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ–¹æ³•ä¸»è¦é€šè¿‡è®°å¿†åˆæˆæƒé‡ï¼šå®ƒä»¬äº§ç”Ÿçš„æ˜¯è®­ç»ƒæ£€æŸ¥ç‚¹çš„å¤åˆ¶å“ï¼Œæˆ–è€…æœ€å¤šæ˜¯ç®€å•çš„æ’å€¼ã€‚å½“å‰çš„æ–¹æ³•æœªèƒ½è¶…è¶Šç®€å•çš„åŸºçº¿ï¼Œå¦‚å‘æƒé‡æ·»åŠ å™ªå£°æˆ–é‡‡å–ç®€å•çš„æƒé‡é›†åˆï¼Œæ¥è·å¾—ä¸åŒä¸”åŒæ—¶é«˜æ€§èƒ½çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæ— æ³•é€šè¿‡ä¿®æ”¹ä¸å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­è®°å¿†æœ‰å…³çš„å»ºæ¨¡å› ç´ æˆ–åº”ç”¨æ•°æ®å¢å¼ºæ¥æœ‰æ•ˆç¼“è§£è¿™ç§è®°å¿†é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå½“å‰ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿçš„æ•°æ®ç±»å‹æä¾›äº†ç°å®çš„è¯„ä¼°ï¼Œå¹¶å¼ºè°ƒäº†åœ¨æ–°é¢†åŸŸä¸­å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ›´ä»”ç»†è¯„ä¼°çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/boyazeng/weight_memorization%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/boyazeng/weight_memorizationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07998v1">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://boyazeng.github.io/weight_memorization">https://boyazeng.github.io/weight_memorization</a></p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œè¿‘æœŸè¢«æ¢ç´¢ç”¨äºåˆæˆæœ‰æ•ˆçš„ç¥ç»ç½‘ç»œæƒé‡ã€‚ç„¶è€Œï¼Œæœ¬æ–‡å‘ç°è¿™äº›æ–¹æ³•ä¸»è¦é€šè¿‡è®°å¿†åˆæˆæƒé‡ï¼Œéš¾ä»¥ç”Ÿæˆä¸è®­ç»ƒæ£€æŸ¥ç‚¹ä¸åŒçš„æ–°æ¨¡å‹æƒé‡ã€‚å½“å‰çš„æ–¹æ³•æœªèƒ½è¶…è¶Šç®€å•åŸºçº¿ï¼Œå¦‚æ·»åŠ æƒé‡å™ªå£°æˆ–é‡‡ç”¨ç®€å•æƒé‡é›†æˆæ–¹æ³•ï¼Œæ¥è·å¾—ä¸åŒä¸”é«˜æ€§èƒ½çš„æ¨¡å‹ã€‚æœ¬æ–‡è¿˜å‘ç°ï¼Œæ— æ³•é€šè¿‡ä¿®æ”¹ä¸å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è®°å¿†ç›¸å…³çš„å»ºæ¨¡å› ç´ æˆ–åº”ç”¨æ•°æ®å¢å¼ºæ¥æœ‰æ•ˆç¼“è§£è®°å¿†é—®é¢˜ã€‚æœ¬æ–‡çš„å‘ç°å¯¹è¯„ä¼°å½“å‰ç”Ÿæˆæ¨¡å‹èƒ½å¤„ç†çš„æ•°æ®ç±»å‹æä¾›äº†ç°å®è¯„ä¼°ï¼Œå¹¶å¼ºè°ƒåœ¨æ–°é¢†åŸŸéœ€è¦æ›´ä»”ç»†åœ°è¯„ä¼°ç”Ÿæˆæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹è¢«æ¢ç´¢ç”¨äºåˆæˆç¥ç»ç½‘ç»œæƒé‡ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜æ€§èƒ½çš„ç¥ç»ç½‘ç»œæƒé‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡è®°å¿†åˆæˆæƒé‡ï¼Œéš¾ä»¥ç”Ÿæˆä¸è®­ç»ƒæ£€æŸ¥ç‚¹ä¸åŒçš„æ–°æ¨¡å‹æƒé‡ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•æœªèƒ½è¶…è¶Šç®€å•åŸºçº¿æ–¹æ³•ï¼Œå¦‚æ·»åŠ å™ªå£°æˆ–é‡‡ç”¨æƒé‡é›†æˆã€‚</li>
<li>é€šè¿‡ä¿®æ”¹ä¸è®°å¿†ç›¸å…³çš„å»ºæ¨¡å› ç´ æˆ–åº”ç”¨æ•°æ®å¢å¼ºæ— æ³•æœ‰æ•ˆç¼“è§£è®°å¿†é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æä¾›çš„ä»£ç åº“å¯å…¬å¼€è®¿é—®ï¼Œå±•ç¤ºäº†ç ”ç©¶æˆæœå’Œå®é™…åº”ç”¨ç¤ºä¾‹ã€‚</li>
<li>ç”Ÿæˆçš„æ¨¡å‹æ€§èƒ½å—é™ï¼Œéœ€è¦å¯¹æ–°é¢†åŸŸçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ›´ä»”ç»†çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d358e9f4e53e089002eb68c54b2da99d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c24fa02237c6118c73cd7a3232b05bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecc13797a57e9fd93f3fb769e98f65fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1dcfd07dabeb77eebf8eb0acaebae24b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a86bb6d705e1714a22ab95b15f98e65e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d8e4e89ef8eecf8a4810c4bf67d4de3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ab736f51c656b36706e5500f82e35f8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Rethinking-Cross-Modal-Interaction-in-Multimodal-Diffusion-Transformers"><a href="#Rethinking-Cross-Modal-Interaction-in-Multimodal-Diffusion-Transformers" class="headerlink" title="Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers"></a>Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers</h2><p><strong>Authors:Zhengyao Lv, Tianlin Pan, Chenyang Si, Zhaoxi Chen, Wangmeng Zuo, Ziwei Liu, Kwan-Yee K. Wong</strong></p>
<p>Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose \textbf{Temperature-Adjusted Cross-modal Attention (TACA)}, a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/Vchitect/TACA%7D">https://github.com/Vchitect/TACA}</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTsï¼‰åœ¨æ–‡æœ¬é©±åŠ¨è§†è§‰ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„MM-DiTæ¨¡å‹ï¼Œå¦‚FLUXï¼Œåœ¨å®ç°æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå†…å®¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½æ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬ç¡®å®šäº†MM-DiTæ³¨æ„æœºåˆ¶ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œå³1)ç”±äºè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´ä»¤ç‰Œçš„ä¸å¹³è¡¡æŠ‘åˆ¶äº†è·¨æ¨¡æ€æ³¨æ„ï¼Œä»¥åŠ2)ç¼ºä¹æ—¶é—´æ­¥é•¿æ„ŸçŸ¥çš„æ³¨æ„æƒé‡ï¼Œè¿™é˜»ç¢äº†å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆTACAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å‚æ•°æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ¸©åº¦ç¼©æ”¾å’Œæ—¶é—´æ­¥é•¿ä¾èµ–çš„è°ƒæ•´æ¥åŠ¨æ€åœ°é‡æ–°å¹³è¡¡å¤šæ¨¡æ€äº¤äº’ã€‚ç»“åˆLoRAå¾®è°ƒï¼ŒTACAåœ¨T2I-CompBenchåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬å›¾åƒå¯¹é½ï¼Œä¸”è®¡ç®—å¼€é”€æå°ã€‚æˆ‘ä»¬åœ¨æœ€å…ˆè¿›çš„æ¨¡å‹FLUXå’ŒSD3.5ä¸Šæµ‹è¯•äº†TACAï¼Œè¯æ˜äº†å…¶åœ¨ç‰©ä½“å¤–è§‚ã€å±æ€§ç»‘å®šå’Œç©ºé—´å…³ç³»æ–¹é¢æé«˜å›¾åƒæ–‡æœ¬å¯¹é½çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å¹³è¡¡è·¨æ¨¡æ€æ³¨æ„åŠ›å¯¹äºæé«˜è¯­ä¹‰ä¿çœŸåº¦çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Vchitect/TACA%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Vchitect/TACAå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07986v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰åœ¨æ–‡æœ¬é©±åŠ¨è§†è§‰ç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹å¦‚FLUXåœ¨æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå†…å®¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æ–‡ç« æŒ‡å‡ºMM-DiTæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1ï¼‰è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´ä»¤ç‰Œçš„ä¸å¹³è¡¡æŠ‘åˆ¶äº†è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼›2ï¼‰ç¼ºä¹æ—¶é—´æ­¥é•¿æ„ŸçŸ¥çš„æ³¨æ„åŠ›æƒé‡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºæ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆTACAï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ¸©åº¦ç¼©æ”¾å’Œæ—¶é—´æ­¥é•¿ä¾èµ–è°ƒæ•´åŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€äº¤äº’ã€‚ç»“åˆLoRAå¾®è°ƒï¼ŒTACAåœ¨T2I-CompBenchåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†æ–‡æœ¬å›¾åƒå¯¹é½èƒ½åŠ›ï¼Œè®¡ç®—å¼€é”€å°ã€‚åœ¨FLUXå’ŒSD3.5ç­‰å…ˆè¿›æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯äº†TACAåœ¨æ”¹å–„å›¾åƒæ–‡æœ¬å¯¹é½æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰åœ¨æ–‡æœ¬é©±åŠ¨è§†è§‰ç”Ÿæˆä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨æ–‡æœ¬æç¤ºä¸ç”Ÿæˆå†…å®¹ç²¾ç¡®å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>MM-DiTçš„æ³¨æ„åŠ›æœºåˆ¶ä¸­å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä»¤ç‰Œä¸å¹³è¡¡å’Œç¼ºä¹æ—¶é—´æ­¥é•¿æ„ŸçŸ¥çš„æ³¨æ„åŠ›æƒé‡ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºæ¸©åº¦è°ƒæ•´è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆTACAï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ¸©åº¦ç¼©æ”¾å’Œæ—¶é—´æ­¥é•¿ä¾èµ–è°ƒæ•´æ¥åŠ¨æ€å¹³è¡¡å¤šæ¨¡æ€äº¤äº’ã€‚</li>
<li>TACAç»“åˆLoRAå¾®è°ƒï¼Œæ˜¾è‘—æé«˜æ–‡æœ¬å›¾åƒå¯¹é½èƒ½åŠ›ï¼Œåœ¨T2I-CompBenchåŸºå‡†æµ‹è¯•ä¸Šæœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>TACAæ–¹æ³•å…·æœ‰å‚æ•°æ•ˆç‡é«˜ã€è®¡ç®—å¼€é”€å°çš„ä¼˜ç‚¹ã€‚</li>
<li>åœ¨FLUXå’ŒSD3.5ç­‰å…ˆè¿›æ¨¡å‹ä¸Šçš„å®éªŒéªŒè¯äº†TACAçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09c6525e39a10a79da026db04bb8747f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91fbeb554b91a91d618ea1e1b1991cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff2bc89b360aee1426f7c0c4fc0c90e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d74fca66dbef92b1a3401abbd1be702.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffuse-Everything-Multimodal-Diffusion-Models-on-Arbitrary-State-Spaces"><a href="#Diffuse-Everything-Multimodal-Diffusion-Models-on-Arbitrary-State-Spaces" class="headerlink" title="Diffuse Everything: Multimodal Diffusion Models on Arbitrary State   Spaces"></a>Diffuse Everything: Multimodal Diffusion Models on Arbitrary State   Spaces</h2><p><strong>Authors:Kevin Rojas, Yuchen Zhu, Sichen Zhu, Felix X. -F. Ye, Molei Tao</strong></p>
<p>Diffusion models have demonstrated remarkable performance in generating unimodal data across various tasks, including image, video, and text generation. On the contrary, the joint generation of multimodal data through diffusion models is still in the early stages of exploration. Existing approaches heavily rely on external preprocessing protocols, such as tokenizers and variational autoencoders, to harmonize varied data representations into a unified, unimodal format. This process heavily demands the high accuracy of encoders and decoders, which can be problematic for applications with limited data. To lift this restriction, we propose a novel framework for building multimodal diffusion models on arbitrary state spaces, enabling native generation of coupled data across different modalities. By introducing an innovative decoupled noise schedule for each modality, we enable both unconditional and modality-conditioned generation within a single model simultaneously. We empirically validate our approach for text-image generation and mixed-type tabular data synthesis, demonstrating that it achieves competitive performance. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šç”Ÿæˆå•æ¨¡æ€æ•°æ®è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ç”Ÿæˆã€‚ç›¸åï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„è”åˆç”Ÿæˆä»åœ¨åˆæ­¥æ¢ç´¢é˜¶æ®µã€‚ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤–éƒ¨é¢„å¤„ç†åè®®ï¼Œå¦‚æ ‡è®°å™¨å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼Œä»¥åè°ƒä¸åŒçš„æ•°æ®è¡¨ç¤ºä¸ºä¸€ä¸ªç»Ÿä¸€ã€å•æ¨¡æ€çš„æ ¼å¼ã€‚è¿™ä¸ªè¿‡ç¨‹å¯¹ç¼–ç å™¨å’Œè§£ç å™¨çš„é«˜ç²¾åº¦æå‡ºäº†å¾ˆé«˜çš„è¦æ±‚ï¼Œå¯¹äºæ•°æ®æœ‰é™çš„åº”ç”¨å¯èƒ½ä¼šæˆä¸ºé—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨ä»»æ„çŠ¶æ€ç©ºé—´ä¸Šæ„å»ºå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°ä¸åŒæ¨¡æ€ä¸‹è€¦åˆæ•°æ®çš„åŸç”Ÿç”Ÿæˆã€‚é€šè¿‡å¼•å…¥é’ˆå¯¹æ¯ç§æ¨¡æ€çš„åˆ›æ–°è§£è€¦å™ªå£°æ—¶é—´è¡¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹ä¸­åŒæ—¶å®ç°æ— æ¡ä»¶ç”Ÿæˆå’Œæ¨¡æ€æ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹æ–‡æœ¬å›¾åƒç”Ÿæˆå’Œæ··åˆç±»å‹è¡¨æ ¼æ•°æ®åˆæˆè¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07903v1">PDF</a> Accepted to ICML 2025. Code available at   <a target="_blank" rel="noopener" href="https://github.com/KevinRojas1499/Diffuse-Everything">https://github.com/KevinRojas1499/Diffuse-Everything</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²åœ¨ç”Ÿæˆå•æ¨¡æ€æ•°æ®æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ¶µç›–å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„è”åˆç”Ÿæˆä»å¤„äºæ¢ç´¢çš„æ—©æœŸé˜¶æ®µã€‚ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤–éƒ¨é¢„å¤„ç†åè®®ï¼Œå¦‚ä»¤ç‰ŒåŒ–å™¨å’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œä»¥å°†ä¸åŒçš„æ•°æ®è¡¨ç¤ºç»Ÿä¸€ä¸ºå•ä¸€æ¨¡æ€æ ¼å¼ã€‚è¿™ä¸€è¿‡ç¨‹å¯¹ç¼–ç å™¨å’Œè§£ç å™¨çš„é«˜å‡†ç¡®æ€§æœ‰å¾ˆé«˜çš„è¦æ±‚ï¼Œå¯¹äºæ•°æ®æœ‰é™çš„åº”ç”¨å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨ä»»æ„çŠ¶æ€ç©ºé—´ä¸Šæ„å»ºå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œå®ç°äº†è·¨ä¸åŒæ¨¡æ€çš„è€¦åˆæ•°æ®æœ¬åœ°ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥é’ˆå¯¹æ¯ç§æ¨¡æ€çš„è§£è€¦å™ªå£°è°ƒåº¦ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹ä¸­åŒæ—¶å®ç°æ— æ¡ä»¶ç”Ÿæˆå’Œæ¨¡æ€æ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹æ–‡æœ¬å›¾åƒç”Ÿæˆå’Œæ··åˆç±»å‹è¡¨æ ¼æ•°æ®åˆæˆè¿›è¡Œäº†å®è¯éªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå•æ¨¡æ€æ•°æ®æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>å¤šæ¨¡æ€æ•°æ®çš„è”åˆç”Ÿæˆé€šè¿‡æ‰©æ•£æ¨¡å‹ä»å¤„äºæ—©æœŸæ¢ç´¢é˜¶æ®µã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨é¢„å¤„ç†åè®®æ¥ç»Ÿä¸€ä¸åŒæ•°æ®è¡¨ç¤ºã€‚</li>
<li>å¯¹ç¼–ç å™¨å’Œè§£ç å™¨çš„é«˜å‡†ç¡®æ€§æœ‰è¾ƒé«˜è¦æ±‚ï¼Œè¿™åœ¨æ•°æ®æœ‰é™çš„åº”ç”¨ä¸­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œé€‚ç”¨äºä»»æ„çŠ¶æ€ç©ºé—´ä¸Šçš„æ„å»ºã€‚</li>
<li>é€šè¿‡å¼•å…¥è§£è€¦å™ªå£°è°ƒåº¦ï¼Œå®ç°äº†è·¨ä¸åŒæ¨¡æ€çš„æœ¬åœ°æ•°æ®ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beaf8e818890605907026be79656d0a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-599e2fa0c8c5f383cd1b5b1d24ec44af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7db5478279a8dfb90036db155b33bf6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-677c122ea7f02e7e3367e5ddd8f85e3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56535978981b33db7ff4ba4523181a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-433c8a3164a187fa11bcd8b5c5723d2b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FunDiff-Diffusion-Models-over-Function-Spaces-for-Physics-Informed-Generative-Modeling"><a href="#FunDiff-Diffusion-Models-over-Function-Spaces-for-Physics-Informed-Generative-Modeling" class="headerlink" title="FunDiff: Diffusion Models over Function Spaces for Physics-Informed   Generative Modeling"></a>FunDiff: Diffusion Models over Function Spaces for Physics-Informed   Generative Modeling</h2><p><strong>Authors:Sifan Wang, Zehao Dou, Tong-Rui Liu, Lu Lu</strong></p>
<p>Recent advances in generative modeling â€“ particularly diffusion models and flow matching â€“ have achieved remarkable success in synthesizing discrete data such as images and videos. However, adapting these models to physical applications remains challenging, as the quantities of interest are continuous functions governed by complex physical laws. Here, we introduce $\textbf{FunDiff}$, a novel framework for generative modeling in function spaces. FunDiff combines a latent diffusion process with a function autoencoder architecture to handle input functions with varying discretizations, generate continuous functions evaluable at arbitrary locations, and seamlessly incorporate physical priors. These priors are enforced through architectural constraints or physics-informed loss functions, ensuring that generated samples satisfy fundamental physical laws. We theoretically establish minimax optimality guarantees for density estimation in function spaces, showing that diffusion-based estimators achieve optimal convergence rates under suitable regularity conditions. We demonstrate the practical effectiveness of FunDiff across diverse applications in fluid dynamics and solid mechanics. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy and low-resolution data. Code and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/sifanexisted/fundiff">https://github.com/sifanexisted/fundiff</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹â€”â€”ç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…â€”â€”åœ¨åˆæˆç¦»æ•£æ•°æ®ï¼ˆå¦‚å›¾åƒå’Œè§†é¢‘ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹é€‚åº”äºå®é™…åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ„Ÿå…´è¶£çš„é‡æ˜¯å—å¤æ‚ç‰©ç†å®šå¾‹æ§åˆ¶çš„è¿ç»­å‡½æ•°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†$\textbf{FunDiff}$ï¼Œä¸€ä¸ªå‡½æ•°ç©ºé—´ç”Ÿæˆæ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚FunDiffç»“åˆäº†æ½œåœ¨æ‰©æ•£è¿‡ç¨‹å’Œå‡½æ•°è‡ªç¼–ç å™¨æ¶æ„ï¼Œä»¥å¤„ç†å…·æœ‰ä¸åŒç¦»æ•£åŒ–çš„è¾“å…¥å‡½æ•°ï¼Œç”Ÿæˆå¯åœ¨ä»»æ„ä½ç½®è¯„ä¼°çš„è¿ç»­å‡½æ•°ï¼Œå¹¶æ— ç¼åœ°èå…¥ç‰©ç†å…ˆéªŒã€‚è¿™äº›å…ˆéªŒé€šè¿‡ç»“æ„çº¦æŸæˆ–ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°æ¥å®æ–½ï¼Œç¡®ä¿ç”Ÿæˆçš„æ ·æœ¬æ»¡è¶³åŸºæœ¬ç‰©ç†å®šå¾‹ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šå»ºç«‹äº†å‡½æ•°ç©ºé—´å¯†åº¦ä¼°è®¡çš„æœ€å°æœ€å¤§æœ€ä¼˜æ€§ä¿è¯ï¼Œè¡¨æ˜åœ¨åˆé€‚çš„æ­£åˆ™æ¡ä»¶ä¸‹ï¼ŒåŸºäºæ‰©æ•£çš„ä¼°è®¡å™¨è¾¾åˆ°äº†æœ€ä¼˜æ”¶æ•›é€Ÿåº¦ã€‚æˆ‘ä»¬åœ¨æµä½“åŠ¨åŠ›å­¦å’Œå›ºä½“åŠ›å­¦ç­‰å¤šæ ·åŒ–åº”ç”¨ä¸­å±•ç¤ºäº†FunDiffçš„å®é™…æ•ˆæœã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†ç¬¦åˆç‰©ç†ä¸€è‡´çš„æ ·æœ¬ï¼Œå¯¹ç›®æ ‡åˆ†å¸ƒå…·æœ‰å¾ˆé«˜çš„ä¿çœŸåº¦ï¼Œå¹¶ä¸”å¯¹å™ªå£°å’Œä½åˆ†è¾¨ç‡æ•°æ®è¡¨ç°å‡ºç¨³å¥æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sifanexisted/fundiff%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/sifanexisted/fundiffå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07902v1">PDF</a> 31 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä¸å‡½æ•°è‡ªç¼–ç å™¨çš„ç»“åˆï¼Œå®ç°äº†å‡½æ•°ç©ºé—´çš„ç”Ÿæˆå»ºæ¨¡ã€‚FunDiffæ¡†æ¶èƒ½å¤Ÿå¤„ç†ä¸åŒç¦»æ•£åŒ–çš„è¾“å…¥å‡½æ•°ï¼Œç”Ÿæˆå¯åœ¨ä»»æ„ä½ç½®è¯„ä¼°çš„è¿ç»­å‡½æ•°ï¼Œå¹¶èå…¥ç‰©ç†å…ˆéªŒã€‚è¯¥æ–¹æ³•åœ¨æµä½“åŠ¨åŠ›å­¦å’Œå›ºä½“åŠ›å­¦ç­‰åº”ç”¨ä¸­è¡¨ç°å‡ºå®ç”¨æ€§ï¼Œç”Ÿæˆæ ·æœ¬ç¬¦åˆç‰©ç†è§„å¾‹ï¼Œå¯¹å™ªå£°å’Œä½åˆ†è¾¨ç‡æ•°æ®å…·æœ‰ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FunDiffç»“åˆæ‰©æ•£æ¨¡å‹å’Œå‡½æ•°è‡ªç¼–ç å™¨ï¼Œå®ç°äº†å‡½æ•°ç©ºé—´çš„ç”Ÿæˆå»ºæ¨¡ã€‚</li>
<li>èƒ½å¤Ÿå¤„ç†ä¸åŒç¦»æ•£åŒ–çš„è¾“å…¥å‡½æ•°ï¼Œç”Ÿæˆè¿ç»­å‡½æ•°ã€‚</li>
<li>FunDiffé€šè¿‡æ¶æ„çº¦æŸæˆ–ç‰©ç†ä¿¡æ¯æŸå¤±å‡½æ•°èå…¥ç‰©ç†å…ˆéªŒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å‡½æ•°ç©ºé—´å¯†åº¦ä¼°è®¡ä¸­è¾¾åˆ°æœ€ä¼˜æ”¶æ•›é€Ÿç‡ã€‚</li>
<li>FunDiffåœ¨æµä½“åŠ¨åŠ›å­¦å’Œå›ºä½“åŠ›å­¦ç­‰åº”ç”¨ä¸­å…·æœ‰å®ç”¨æ€§ã€‚</li>
<li>ç”Ÿæˆæ ·æœ¬ç¬¦åˆç‰©ç†è§„å¾‹ï¼Œå…·æœ‰é«˜ä¿çœŸåº¦ã€‚</li>
<li>FunDiffå¯¹å™ªå£°å’Œä½åˆ†è¾¨ç‡æ•°æ®å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f32c865d5703b2ca31582df9e72c2f8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Diffusion-Counterfactual-Generation-with-Semantic-Abduction"><a href="#Diffusion-Counterfactual-Generation-with-Semantic-Abduction" class="headerlink" title="Diffusion Counterfactual Generation with Semantic Abduction"></a>Diffusion Counterfactual Generation with Semantic Abduction</h2><p><strong>Authors:Rajat Rasal, Avinash Kori, Fabio De Sousa Ribeiro, Tian Xia, Ben Glocker</strong></p>
<p>Counterfactual image generation presents significant challenges, including preserving identity, maintaining perceptual quality, and ensuring faithfulness to an underlying causal model. While existing auto-encoding frameworks admit semantic latent spaces which can be manipulated for causal control, they struggle with scalability and fidelity. Advancements in diffusion models present opportunities for improving counterfactual image editing, having demonstrated state-of-the-art visual quality, human-aligned perception and representation learning capabilities. Here, we present a suite of diffusion-based causal mechanisms, introducing the notions of spatial, semantic and dynamic abduction. We propose a general framework that integrates semantic representations into diffusion models through the lens of Pearlian causality to edit images via a counterfactual reasoning process. To our knowledge, this is the first work to consider high-level semantic identity preservation for diffusion counterfactuals and to demonstrate how semantic control enables principled trade-offs between faithful causal control and identity preservation. </p>
<blockquote>
<p>ç”Ÿæˆåäº‹å®å›¾åƒé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿æŒèº«ä»½ã€ç»´æŒæ„ŸçŸ¥è´¨é‡å’Œç¡®ä¿å¿ äºæ½œåœ¨å› æœæ¨¡å‹ã€‚è™½ç„¶ç°æœ‰çš„è‡ªåŠ¨ç¼–ç æ¡†æ¶å…è®¸è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥è¿›è¡Œå› æœæ§åˆ¶æ“ä½œï¼Œä½†å®ƒä»¬é¢ä¸´ç€å¯æ‰©å±•æ€§å’Œä¿çœŸåº¦çš„é—®é¢˜ã€‚æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ä¸ºæ”¹è¿›åäº‹å®å›¾åƒç¼–è¾‘æä¾›äº†æœºä¼šï¼Œå®ƒä»¬å·²æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ã€ä¸äººç±»å¯¹é½çš„æ„ŸçŸ¥èƒ½åŠ›å’Œè¡¨å¾å­¦ä¹ èƒ½åŠ›ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—åŸºäºæ‰©æ•£çš„å› æœæœºåˆ¶ï¼Œå¼•å…¥äº†ç©ºé—´ã€è¯­ä¹‰å’ŒåŠ¨æ€å½’çº³çš„æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šè¿‡çš®å°”å£«å› æœæ€§çš„è§†è§’å°†è¯­ä¹‰è¡¨ç¤ºé›†æˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­çš„é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡åäº‹å®æ¨ç†è¿‡ç¨‹æ¥ç¼–è¾‘å›¾åƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹è€ƒè™‘é«˜çº§è¯­ä¹‰èº«ä»½ä¿ç•™çš„æ‰©æ•£åäº‹å®çš„å·¥ä½œï¼Œå¹¶å±•ç¤ºäº†è¯­ä¹‰æ§åˆ¶å¦‚ä½•å®ç°åœ¨å¿ å®å› æœæ§åˆ¶å’Œèº«ä»½ä¿ç•™ä¹‹é—´çš„åŸåˆ™æ€§æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07883v1">PDF</a> Proceedings of the 42nd International Conference on Machine Learning,   Vancouver, Canada</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­çš„æ½œåŠ›åŠå…¶åœ¨ç”Ÿæˆåäº‹å®å›¾åƒæ—¶çš„æŒ‘æˆ˜ä¸åˆ›æ–°ã€‚è¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æœºåˆ¶çš„å› æœç¼–è¾‘æ¡†æ¶ï¼Œå¼•å…¥ç©ºé—´ã€è¯­ä¹‰å’ŒåŠ¨æ€æ¨æ–­æ¦‚å¿µï¼Œå¹¶é€šè¿‡Pearlå› æœè§†è§’æ•´åˆè¯­ä¹‰è¡¨ç¤ºã€‚è¯¥æ¡†æ¶èƒ½åœ¨åäº‹å®æ¨ç†è¿‡ç¨‹ä¸­ç¼–è¾‘å›¾åƒï¼Œåœ¨ä¿æŒé«˜çº§è¯­ä¹‰èº«ä»½çš„åŒæ—¶å®ç°å› æœæ§åˆ¶ã€‚æ­¤ç ”ç©¶åœ¨æ‰©æ•£æ¨¡å‹çš„åäº‹å®ä¸­è€ƒè™‘äº†é«˜çº§è¯­ä¹‰èº«ä»½çš„ä¿ç•™ï¼Œå±•ç¤ºäº†è¯­ä¹‰æ§åˆ¶å¦‚ä½•å¹³è¡¡å› æœæ§åˆ¶ä¸èº«ä»½ä¿ç•™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨ç”Ÿæˆåäº‹å®å›¾åƒæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æŒ‘æˆ˜åŒ…æ‹¬ä¿æŒèº«ä»½ã€ç»´æŒæ„ŸçŸ¥è´¨é‡å’Œç¡®ä¿å¿ äºæ½œåœ¨å› æœæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æœºåˆ¶çš„å› æœç¼–è¾‘æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´ã€è¯­ä¹‰å’ŒåŠ¨æ€æ¨æ–­æ¦‚å¿µã€‚</li>
<li>é€šè¿‡Pearlå› æœè§†è§’æ•´åˆè¯­ä¹‰è¡¨ç¤ºï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨åäº‹å®æ¨ç†è¿‡ç¨‹ä¸­ç¼–è¾‘å›¾åƒã€‚</li>
<li>è¯¥ç ”ç©¶è€ƒè™‘äº†é«˜çº§è¯­ä¹‰èº«ä»½çš„ä¿ç•™åœ¨æ‰©æ•£æ¨¡å‹çš„åäº‹å®ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è¯­ä¹‰æ§åˆ¶å®ç°å› æœæ§åˆ¶ä¸èº«ä»½ä¿ç•™ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>è¯¥æ¡†æ¶å±•ç¤ºäº†å…ˆè¿›çš„è§†è§‰è´¨é‡ã€äººç±»å¯¹é½æ„ŸçŸ¥å’Œè¡¨ç¤ºå­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b429f772176ec8a8ec9047e54aeb340f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d8cc72a32cd5608c941cd2575d661b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f43089c16e70e20899c0d9679e22abdc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R3D2-Realistic-3D-Asset-Insertion-via-Diffusion-for-Autonomous-Driving-Simulation"><a href="#R3D2-Realistic-3D-Asset-Insertion-via-Diffusion-for-Autonomous-Driving-Simulation" class="headerlink" title="R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving   Simulation"></a>R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving   Simulation</h2><p><strong>Authors:William Ljungbergh, Bernardo Taveira, Wenzhao Zheng, Adam Tonderski, Chensheng Peng, Fredrik Kahl, Christoffer Petersson, Michael Felsberg, Kurt Keutzer, Masayoshi Tomizuka, Wei Zhan</strong></p>
<p>Validating autonomous driving (AD) systems requires diverse and safety-critical testing, making photorealistic virtual environments essential. Traditional simulation platforms, while controllable, are resource-intensive to scale and often suffer from a domain gap with real-world data. In contrast, neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a scalable solution for creating photorealistic digital twins of real-world driving scenes. However, they struggle with dynamic object manipulation and reusability as their per-scene optimization-based methodology tends to result in incomplete object models with integrated illumination effects. This paper introduces R3D2, a lightweight, one-step diffusion model designed to overcome these limitations and enable realistic insertion of complete 3D assets into existing scenes by generating plausible rendering effects-such as shadows and consistent lighting-in real time. This is achieved by training R3D2 on a novel dataset: 3DGS object assets are generated from in-the-wild AD data using an image-conditioned 3D generative model, and then synthetically placed into neural rendering-based virtual environments, allowing R3D2 to learn realistic integration. Quantitative and qualitative evaluations demonstrate that R3D2 significantly enhances the realism of inserted assets, enabling use-cases like text-to-3D asset insertion and cross-scene&#x2F;dataset object transfer, allowing for true scalability in AD validation. To promote further research in scalable and realistic AD simulation, we will release our dataset and code, see <a target="_blank" rel="noopener" href="https://research.zenseact.com/publications/R3D2/">https://research.zenseact.com/publications/R3D2/</a>. </p>
<blockquote>
<p>éªŒè¯è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç³»ç»Ÿéœ€è¦è¿›è¡Œå¤šæ ·åŒ–å’Œå®‰å…¨å…³é”®çš„æµ‹è¯•ï¼Œè¿™ä½¿å¾—é€¼çœŸè™šæ‹Ÿç¯å¢ƒå˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿä»¿çœŸå¹³å°è™½ç„¶å¯æ§ï¼Œä½†è§„æ¨¡æ‰©å±•èµ„æºå¯†é›†ï¼Œä¸”ç»å¸¸ä¸çœŸå®ä¸–ç•Œæ•°æ®å­˜åœ¨é¢†åŸŸå·®è·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¥ç»é‡å»ºæ–¹æ³•ï¼ˆå¦‚3Dé«˜æ–¯å–·æ¶‚æŠ€æœ¯ï¼ˆ3DGSï¼‰ï¼‰æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåˆ›å»ºçœŸå®é©¾é©¶åœºæ™¯çš„é€¼çœŸæ•°å­—åŒèƒèƒã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŠ¨æ€å¯¹è±¡æ“ä½œå’Œé‡ç”¨æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬åŸºäºåœºæ™¯çš„ä¼˜åŒ–çš„æ–¹æ³•å¾€å¾€å¯¼è‡´å…·æœ‰é›†æˆç…§æ˜æ•ˆæœçš„ä¸å®Œæ•´å¯¹è±¡æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†R3D2ï¼Œä¸€ä¸ªè½»é‡çº§çš„ã€ä¸€æ­¥åˆ°ä½çš„æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ï¼Œå¹¶é€šè¿‡ç”Ÿæˆé€¼çœŸçš„æ¸²æŸ“æ•ˆæœï¼ˆå¦‚é˜´å½±å’Œä¸€è‡´ç…§æ˜ï¼‰æ¥å®ç°å°†å®Œæ•´çš„3Dèµ„äº§å®æ—¶æ’å…¥ç°æœ‰åœºæ™¯ã€‚è¿™æ˜¯é€šè¿‡åœ¨æ–°å‹æ•°æ®é›†ä¸Šè®­ç»ƒR3D2å®ç°çš„ï¼šä½¿ç”¨å›¾åƒæ¡ä»¶ä¸‹çš„3Dç”Ÿæˆæ¨¡å‹ä»é‡ç”ŸADæ•°æ®ä¸­ç”Ÿæˆ3DGSå¯¹è±¡èµ„äº§ï¼Œç„¶åå°†å…¶åˆæˆåœ°æ”¾å…¥åŸºäºç¥ç»æ¸²æŸ“çš„è™šæ‹Ÿç¯å¢ƒä¸­ï¼Œä½¿R3D2å­¦ä¹ é€¼çœŸçš„é›†æˆæ–¹å¼ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒR3D2æ˜¾ç€æé«˜äº†æ’å…¥èµ„äº§çš„çœŸå®æ€§ï¼Œèƒ½å¤Ÿå®ç°æ–‡æœ¬åˆ°3Dèµ„äº§æ’å…¥å’Œè·¨åœºæ™¯&#x2F;æ•°æ®é›†å¯¹è±¡è½¬ç§»ç­‰ç”¨ä¾‹ï¼Œä¸ºADéªŒè¯æä¾›äº†çœŸæ­£çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†ä¿ƒè¿›å¯æ‰©å±•å’Œé€¼çœŸçš„ADæ¨¡æ‹Ÿçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç ï¼Œè¯¦è§<a target="_blank" rel="noopener" href="https://research.zenseact.com/publications/R3D2/%E3%80%82">https://research.zenseact.com/publications/R3D2/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07826v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>ç¥ç»ç½‘ç»œé‡å»ºæ–¹æ³•ï¼Œå¦‚ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ï¼Œä¸ºåˆ›å»ºçœŸå®ä¸–ç•Œé©¾é©¶åœºæ™¯çš„å…‰ç…§çº§æ•°å­—åŒèƒèƒæä¾›äº†å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŠ¨æ€ç‰©ä½“æ“ä½œå’Œå†åˆ©ç”¨æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡å¼•å…¥R3D2æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…‹æœäº†è¿™äº›å±€é™æ€§ï¼Œå¹¶å…è®¸å®æ—¶ç”Ÿæˆé€¼çœŸçš„æ¸²æŸ“æ•ˆæœï¼Œä¾‹å¦‚é˜´å½±å’Œä¸€è‡´çš„å…‰ç…§ã€‚R3D2é€šè¿‡åœ¨é‡å¤–é©¾é©¶æ•°æ®ä¸Šç”Ÿæˆä¸‰ç»´ç”Ÿæˆæ¨¡å‹çš„æ–°æ•°æ®é›†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¹¶å°†å…¶åˆæˆåœ°æ”¾ç½®åœ¨åŸºäºç¥ç»æ¸²æŸ“çš„è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ ã€‚å› æ­¤ï¼Œè¯¥æ¨¡å‹å¯ç”¨äºçœŸå®é©¾é©¶åœºæ™¯çš„å¤§è§„æ¨¡ä»¿çœŸå’Œè‡ªä¸»é©¾é©¶ç³»ç»Ÿæµ‹è¯•ã€‚è¯¥è®ºæ–‡è¿˜å…¬å¼€äº†æ•°æ®é›†å’Œä»£ç ä»¥æ¨åŠ¨ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li><p>è‡ªä¸»é©¾é©¶ç³»ç»ŸéªŒè¯éœ€è¦å¤šæ ·åŒ–çš„å®‰å…¨æµ‹è¯•ï¼Œè¿™è¦æ±‚æ¨¡æ‹Ÿé€¼çœŸçš„è™šæ‹Ÿç¯å¢ƒã€‚ä¼ ç»Ÿæ¨¡æ‹Ÿå¹³å°åœ¨æ‰©å¤§è§„æ¨¡æ–¹é¢å­˜åœ¨èµ„æºå¯†é›†çš„é—®é¢˜ï¼Œä¸”ä¸çœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ã€‚æœ¬æ–‡å¼ºè°ƒäº†æ–°å‹ä»¿çœŸæŠ€æœ¯åœ¨åˆ›å»ºå¤§è§„æ¨¡åœºæ™¯å’Œé›†æˆèµ„æºä¸Šçš„éœ€æ±‚é‡è¦æ€§ã€‚åœ¨é«˜åº¦ä»¿çœŸä¸”å¯é çš„å¤§è§„æ¨¡åœºæ™¯ä¸Šå¯ä»¥å¤§å¹…é™ä½åå¤è·¯æµ‹å’Œå·¥ç¨‹æŠ•å…¥çš„æˆæœ¬ä¸ç²¾åŠ›æŸè€—ï¼Œä»è€Œå®ç°è‡ªåŠ¨é©¾é©¶çš„å¤§è§„æ¨¡å•†ä¸šåº”ç”¨å‰æ™¯ã€‚è¿™äº›å…³é”®æŠ€æœ¯ä½¿å¾—å»ºç«‹é«˜æ•ˆçš„è‡ªä¸»é©¾é©¶æµ‹è¯•ä½“ç³»æˆä¸ºå¯èƒ½ã€‚åŸºäºæ­¤å»ºç«‹çš„ä¸€ä½“åŒ–éªŒè¯ç³»ç»Ÿå°†ä¸ºåæœŸåŠ é€ŸæŠ€æœ¯è½åœ°æä¾›äº†å¼ºåŠ›çš„åº•å±‚æ”¯æŒå·¥å…·ï¼›åœ¨å®é™…å¼€æ”¾é“è·¯ä¸Šçš„æµ‹è¯•ç»“æœå·®è·ç›¸æ¯”äºæŠ€æœ¯æ›´æ–°å·®å¼‚æ›´æœ‰æ„ä¹‰å’Œå½±å“æ€§ä»·å€¼ä¼šè·å¾—å¾ˆå¤§æ”¹å–„æå‡æ½œåŠ›ã€‚åŒæ—¶å…¬å¼€çš„æ•°æ®é›†å’Œä»£ç ä¿ƒè¿›äº†è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå‘å±•ã€‚</p>
</li>
<li><p>R3D2æ¨¡å‹æ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œé‡å»ºæ–¹æ³•ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿå®æ—¶ç”Ÿæˆé€¼çœŸçš„æ¸²æŸ“æ•ˆæœï¼Œå¦‚é˜´å½±å’Œä¸€è‡´çš„å…‰ç…§ã€‚å®ƒé€šè¿‡ç”Ÿæˆæ–°çš„æ•°æ®é›†å¹¶åˆæˆåœ°æ”¾ç½®åœ¨åŸºäºç¥ç»æ¸²æŸ“çš„è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¿™ä¸ºè‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„æµ‹è¯•æä¾›äº†æ›´é€¼çœŸçš„æ¨¡æ‹Ÿç¯å¢ƒã€‚åŒæ—¶å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å…è®¸åœ¨ä¸åŒåœºæ™¯é—´è¿›è¡Œçµæ´»åˆ‡æ¢å’Œèµ„äº§è½¬ç§»ä½¿ç”¨åœºæ™¯ä¸°å¯Œå¤šå˜ã€‚é€šè¿‡å®æ—¶æ¸²æŸ“æŠ€æœ¯ï¼Œå®ç°äº†æ›´åŠ é€¼çœŸçš„è™šæ‹Ÿç¯å¢ƒæ¨¡æ‹Ÿï¼Œæé«˜äº†è‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„æµ‹è¯•è´¨é‡å’Œæ•ˆç‡ã€‚è¿™ä¸€åˆ›æ–°æŠ€æœ¯ä¸ºè‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„æµ‹è¯•å’ŒéªŒè¯å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚å¯¹äºè‡ªä¸»é©¾é©¶ç³»ç»Ÿè€Œè¨€ä¸ä»…ä¸ºå…¶åœ¨å¼€å‘å’ŒéªŒè¯è¿‡ç¨‹ä¸­æä¾›æ›´åŠ æ¥è¿‘çœŸå®çš„ä»¿çœŸæµ‹è¯•ç¯å¢ƒä»è€Œæé«˜ç³»ç»Ÿå®‰å…¨æ€§å’Œå¯é æ€§è€Œä¸”ä¹Ÿæ¨åŠ¨äº†ç›¸å…³æŠ€æœ¯äº§ä¸šçš„å‘å±•è¿›æ­¥å’Œè¡Œä¸šæ ‡å‡†çš„æå‡åŠè§„èŒƒè½åœ°é€Ÿåº¦åŠ å¿«ä¹Ÿæ‹“å±•äº†ç›¸å…³çš„å¸‚åœºéœ€æ±‚è¾¹ç•Œå¯¹äºåæœŸè¿è¥å’Œå¸‚åœºæ‹“å±•å¸¦æ¥é‡è¦å½±å“ä»·å€¼å’Œäº§ä¸šè”åŠ¨å‡çº§æ¨åŠ¨åŠ›ä¸ºè‡ªä¸»ç ”å‘åˆ¶é€ çš„æ ¸å¿ƒç«äº‰åŠ›èµ‹èƒ½åŠ æŒã€‚åŒæ—¶å…¬å¼€çš„æ•°æ®é›†å’Œä»£ç ä¿ƒè¿›äº†è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå‘å±•ã€‚è¿™ä¸€æŠ€æœ¯æœ‰æœ›æ¨åŠ¨è‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„æµ‹è¯•å’ŒéªŒè¯è¿›å…¥æ–°çš„é˜¶æ®µå¹¶ä¸ºç›¸å…³äº§ä¸šå¸¦æ¥é©å‘½æ€§çš„å˜é©å’Œå‘å±•æœºé‡ã€‚ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57e3df31715fbe5a1c6ec72bdb75ac8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41145e8c08437307e5db09e0815d0117.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5ecc16eb2ef7e566ab24ff981588137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be7882be64bdf0aabfaf4f2abe4504f6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Self-Cascaded-Diffusion-Models-for-Arbitrary-Scale-Image-Super-Resolution"><a href="#Self-Cascaded-Diffusion-Models-for-Arbitrary-Scale-Image-Super-Resolution" class="headerlink" title="Self-Cascaded Diffusion Models for Arbitrary-Scale Image   Super-Resolution"></a>Self-Cascaded Diffusion Models for Arbitrary-Scale Image   Super-Resolution</h2><p><strong>Authors:Junseo Bang, Joonhee Lee, Kyeonghyun Lee, Haechang Lee, Dong Un Kang, Se Young Chun</strong></p>
<p>Arbitrary-scale image super-resolution aims to upsample images to any desired resolution, offering greater flexibility than traditional fixed-scale super-resolution. Recent approaches in this domain utilize regression-based or generative models, but many of them are a single-stage upsampling process, which may be challenging to learn across a wide, continuous distribution of scaling factors. Progressive upsampling strategies have shown promise in mitigating this issue, yet their integration with diffusion models for flexible upscaling remains underexplored. Here, we present CasArbi, a novel self-cascaded diffusion framework for arbitrary-scale image super-resolution. CasArbi meets the varying scaling demands by breaking them down into smaller sequential factors and progressively enhancing the image resolution at each step with seamless transitions for arbitrary scales. Our novel coordinate-guided residual diffusion model allows for the learning of continuous image representations while enabling efficient diffusion sampling. Extensive experiments demonstrate that our CasArbi outperforms prior arts in both perceptual and distortion performance metrics across diverse arbitrary-scale super-resolution benchmarks. </p>
<blockquote>
<p>ä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç›®æ ‡æ˜¯å°†å›¾åƒæ”¾å¤§åˆ°ä»»ä½•æ‰€éœ€çš„åˆ†è¾¨ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å›ºå®šå°ºåº¦è¶…åˆ†è¾¨ç‡ï¼Œå®ƒæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚è™½ç„¶è¯¥é¢†åŸŸæœ€æ–°çš„æ–¹æ³•é‡‡ç”¨äº†åŸºäºå›å½’æˆ–ç”Ÿæˆæ¨¡å‹ï¼Œä½†å…¶ä¸­è®¸å¤šéƒ½æ˜¯å•é˜¶æ®µä¸Šé‡‡æ ·è¿‡ç¨‹ï¼Œå¯¹äºå¹¿æ³›çš„è¿ç»­å°ºåº¦å› å­åˆ†å¸ƒï¼Œè¿™å¯èƒ½ä¼šå¸¦æ¥æŒ‘æˆ˜ã€‚æ¸è¿›å¼ä¸Šé‡‡æ ·ç­–ç•¥åœ¨ç¼“è§£è¿™ä¸ªé—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬ä¸æ‰©æ•£æ¨¡å‹çš„é›†æˆä»¥å®ç°çµæ´»æ”¾å¤§å´ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†CasArbiï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡çš„æ–°å‹è‡ªçº§è”æ‰©æ•£æ¡†æ¶ã€‚CasArbié€šè¿‡å°†å„ç§ç¼©æ”¾éœ€æ±‚åˆ†è§£ä¸ºè¾ƒå°çš„è¿ç»­å› å­æ¥æ»¡è¶³å®ƒä»¬ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥é€æ­¥å¢å¼ºå›¾åƒåˆ†è¾¨ç‡ï¼Œä»¥å®ç°ä»»æ„å°ºåº¦çš„æ— ç¼è¿‡æ¸¡ã€‚æˆ‘ä»¬åˆ›æ–°æ€§çš„åæ ‡å¼•å¯¼æ®‹å·®æ‰©æ•£æ¨¡å‹å…è®¸å­¦ä¹ è¿ç»­å›¾åƒè¡¨ç¤ºï¼ŒåŒæ—¶å®ç°æœ‰æ•ˆçš„æ‰©æ•£é‡‡æ ·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CasArbiåœ¨æ„ŸçŸ¥å’Œå¤±çœŸæ€§èƒ½æŒ‡æ ‡æ–¹é¢å‡è¶…è¶Šäº†å…ˆå‰æŠ€æœ¯åœ¨å„ç§ä»»æ„å°ºåº¦è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07813v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå…³äºä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç ”ç©¶æ—¨åœ¨å°†å›¾åƒæ”¾å¤§åˆ°ä»»ä½•æ‰€éœ€åˆ†è¾¨ç‡ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå›ºå®šå°ºåº¦çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯æ›´å…·çµæ´»æ€§ã€‚å½“å‰æ–¹æ³•å¤šé‡‡ç”¨å›å½’æˆ–ç”Ÿæˆæ¨¡å‹ï¼Œä½†å•ä¸€é˜¶æ®µçš„æ”¾å¤§è¿‡ç¨‹åœ¨è·¨è¶Šè¿ç»­å°ºåº¦å› å­æ—¶å­¦ä¹ è¾ƒä¸ºå›°éš¾ã€‚æ¸è¿›å¼æ”¾å¤§ç­–ç•¥æœ‰åŠ©äºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ä¸æ‰©æ•£æ¨¡å‹çš„ç»“åˆè¿›è¡Œçµæ´»æ”¾å¤§ä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºCasArbiï¼Œä¸€ç§æ–°å‹è‡ªçº§è”æ‰©æ•£æ¡†æ¶ï¼Œç”¨äºä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚CasArbié€šè¿‡å°†ä¸åŒçš„å°ºåº¦éœ€æ±‚åˆ†è§£ä¸ºè‹¥å¹²è¿ç»­çš„å°å°ºåº¦å› å­ï¼Œå¹¶é€æ­¥å¢å¼ºå›¾åƒåˆ†è¾¨ç‡ï¼Œä»¥å®ç°ä»»æ„å°ºåº¦çš„æ— ç¼è¿‡æ¸¡ã€‚å…¶åˆ›æ–°çš„åæ ‡å¼•å¯¼æ®‹å·®æ‰©æ•£æ¨¡å‹èƒ½å­¦ä¹ è¿ç»­å›¾åƒè¡¨ç¤ºï¼ŒåŒæ—¶å®ç°é«˜æ•ˆçš„æ‰©æ•£é‡‡æ ·ã€‚å®éªŒè¡¨æ˜ï¼ŒCasArbiåœ¨å¤šç§ä»»æ„å°ºåº¦è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ„ŸçŸ¥å’Œå¤±çœŸæ€§èƒ½æŒ‡æ ‡å‡è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»»æ„å°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡æ—¨åœ¨å°†å›¾åƒæ”¾å¤§åˆ°ä»»ä½•æ‰€éœ€åˆ†è¾¨ç‡ï¼Œå…·æœ‰æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´é«˜çš„çµæ´»æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´å­¦ä¹ è¿ç»­å°ºåº¦å› å­çš„æŒ‘æˆ˜ï¼Œå•ä¸€é˜¶æ®µçš„æ”¾å¤§è¿‡ç¨‹å¯èƒ½éš¾ä»¥åº”å¯¹ã€‚</li>
<li>æ¸è¿›å¼æ”¾å¤§ç­–ç•¥æœ‰åŠ©äºç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>CasArbiæ˜¯ä¸€ç§æ–°å‹è‡ªçº§è”æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£å°ºåº¦éœ€æ±‚å¹¶é€æ­¥å¢å¼ºå›¾åƒåˆ†è¾¨ç‡ï¼Œå®ç°ä»»æ„å°ºåº¦çš„æ— ç¼è¿‡æ¸¡ã€‚</li>
<li>CasArbié‡‡ç”¨åˆ›æ–°çš„åæ ‡å¼•å¯¼æ®‹å·®æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å­¦ä¹ è¿ç»­å›¾åƒè¡¨ç¤ºå¹¶å®ç°é«˜æ•ˆæ‰©æ•£é‡‡æ ·ã€‚</li>
<li>å®éªŒè¡¨æ˜CasArbiåœ¨å¤šä¸ªè¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a23890aabad884cd9008bbf16d5d2d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-654abef3e7b676e9529e1c4028ea3d83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01983d411f389e17f7481ed700e4f6bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae032f62b8b073c8f4f35b9a54924c4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aac530c051d18247eb47ef92a047b949.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8e3e45d687cd3894102da7ee42bf6fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e039779d5c865cd7c80d251ebbd4d94f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Difference-Inversion-Interpolate-and-Isolate-the-Difference-with-Token-Consistency-for-Image-Analogy-Generation"><a href="#Difference-Inversion-Interpolate-and-Isolate-the-Difference-with-Token-Consistency-for-Image-Analogy-Generation" class="headerlink" title="Difference Inversion: Interpolate and Isolate the Difference with Token   Consistency for Image Analogy Generation"></a>Difference Inversion: Interpolate and Isolate the Difference with Token   Consistency for Image Analogy Generation</h2><p><strong>Authors:Hyunsoo Kim, Donghyun Kim, Suhyun Kim</strong></p>
<p>How can we generate an image Bâ€™ that satisfies A:Aâ€™::B:Bâ€™, given the input images A,Aâ€™ and B? Recent works have tackled this challenge through approaches like visual in-context learning or visual instruction. However, these methods are typically limited to specific models (e.g. InstructPix2Pix. Inpainting models) rather than general diffusion models (e.g. Stable Diffusion, SDXL). This dependency may lead to inherited biases or lower editing capabilities. In this paper, we propose Difference Inversion, a method that isolates only the difference from A and Aâ€™ and applies it to B to generate a plausible Bâ€™. To address model dependency, it is crucial to structure prompts in the form of a â€œFull Promptâ€ suitable for input to stable diffusion models, rather than using an â€œInstruction Promptâ€. To this end, we accurately extract the Difference between A and Aâ€™ and combine it with the prompt of B, enabling a plug-and-play application of the difference. To extract a precise difference, we first identify it through 1) Delta Interpolation. Additionally, to ensure accurate training, we propose the 2) Token Consistency Loss and 3) Zero Initialization of Token Embeddings. Our extensive experiments demonstrate that Difference Inversion outperforms existing baselines both quantitatively and qualitatively, indicating its ability to generate more feasible Bâ€™ in a model-agnostic manner. </p>
<blockquote>
<p>ç»™å®šè¾“å…¥å›¾åƒAã€Aâ€™å’ŒBï¼Œå¦‚ä½•ç”Ÿæˆæ»¡è¶³A:Aâ€™::B:Bâ€™çš„å›¾åƒBâ€™ï¼Ÿè¿‘æœŸçš„ç ”ç©¶å·¥ä½œé€šè¿‡ä¸Šä¸‹æ–‡è§†è§‰å­¦ä¹ æˆ–è§†è§‰æŒ‡ä»¤ç­‰æ–¹æ³•æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä»…é™äºç‰¹å®šæ¨¡å‹ï¼ˆä¾‹å¦‚InstructPix2Pixã€è¡¥å…¨æ¨¡å‹ï¼‰ï¼Œè€Œéé€šç”¨çš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚Stable Diffusionã€SDXLï¼‰ã€‚è¿™ç§ä¾èµ–æ€§å¯èƒ½å¯¼è‡´ç»§æ‰¿çš„åè§æˆ–è¾ƒä½çš„ç¼–è¾‘èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå·®å¼‚åè½¬çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä»Aå’ŒAâ€™ä¸­åˆ†ç¦»å‡ºå·®å¼‚å¹¶å°†å…¶åº”ç”¨äºBï¼Œä»¥ç”Ÿæˆåˆç†çš„Bâ€™ã€‚ä¸ºäº†è§£å†³æ¨¡å‹ä¾èµ–æ€§ï¼Œå…³é”®åœ¨äºæ„å»ºé€‚åˆè¾“å…¥ç¨³å®šæ‰©æ•£æ¨¡å‹çš„â€œå®Œæ•´æç¤ºâ€ï¼Œè€Œéä½¿ç”¨â€œæŒ‡ä»¤æç¤ºâ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‡†ç¡®åœ°æå–äº†Aå’ŒAâ€™ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†å…¶ä¸Bçš„æç¤ºç›¸ç»“åˆï¼Œå®ç°äº†å·®å¼‚çš„å³æ’å³ç”¨åº”ç”¨ã€‚ä¸ºäº†ç²¾ç¡®æå–å·®å¼‚ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡1ï¼‰Deltaæ’å€¼æ¥è¯†åˆ«å®ƒã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿å‡†ç¡®çš„è®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†2ï¼‰æ ‡è®°ä¸€è‡´æ€§æŸå¤±å’Œ3ï¼‰æ ‡è®°åµŒå…¥çš„é›¶åˆå§‹åŒ–ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå·®å¼‚åè½¬åœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¯æ˜å…¶ä»¥æ¨¡å‹æ— å…³çš„æ–¹å¼ç”Ÿæˆæ›´å¯è¡Œçš„Bâ€™çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07750v1">PDF</a> Published at CVPR 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œå·®å¼‚åè½¬ï¼ˆDifference Inversionï¼‰â€çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæ»¡è¶³ç»™å®šæ¡ä»¶çš„æ–°å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡æå–è¾“å…¥å›¾åƒAå’ŒAâ€™ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å°†å…¶åº”ç”¨äºå›¾åƒBæ¥ç”Ÿæˆåˆç†çš„å›¾åƒBâ€™ã€‚ä¸ºå…‹æœæ¨¡å‹ä¾èµ–æ€§é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†â€œå…¨æç¤ºï¼ˆFull Promptï¼‰â€ç»“æ„ï¼Œä»¥é€‚åº”é€šç”¨æ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionå’ŒSDXLã€‚æ­¤å¤–ï¼Œä¸ºæé«˜å‡†ç¡®æ€§ï¼Œè¿˜ä»‹ç»äº†å·®å¼‚æå–çš„Delta Interpolationæ–¹æ³•ä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­çš„Tokenä¸€è‡´æ€§æŸå¤±å’Œé›¶åˆå§‹åŒ–ä»¤ç‰ŒåµŒå…¥æŠ€æœ¯ã€‚å®éªŒè¯æ˜ï¼Œå·®å¼‚åè½¬æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ä¸Šå‡ä¼˜äºç°æœ‰åŸºçº¿ï¼Œèƒ½ä»¥æ¨¡å‹æ— å…³çš„æ–¹å¼ç”Ÿæˆæ›´å¯è¡Œçš„å›¾åƒBâ€™ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºâ€œå·®å¼‚åè½¬ï¼ˆDifference Inversionï¼‰â€æ–¹æ³•ï¼Œé€šè¿‡æå–è¾“å…¥å›¾åƒçš„å·®å¼‚å¹¶åº”ç”¨äºå¦ä¸€å›¾åƒæ¥ç”Ÿæˆæ–°å›¾åƒã€‚</li>
<li>å…‹æœç‰¹å®šæ¨¡å‹ä¾èµ–ï¼Œé‡‡ç”¨é€šç”¨æ‰©æ•£æ¨¡å‹é€‚åº”ç­–ç•¥ï¼Œå³ä½¿ç”¨â€œå…¨æç¤ºï¼ˆFull Promptï¼‰â€ç»“æ„ã€‚</li>
<li>é‡‡ç”¨Delta Interpolationæ–¹æ³•å‡†ç¡®æå–å›¾åƒå·®å¼‚ã€‚</li>
<li>ä¸ºæé«˜å‡†ç¡®æ€§ï¼Œå¼•å…¥äº†Tokenä¸€è‡´æ€§æŸå¤±å’Œé›¶åˆå§‹åŒ–ä»¤ç‰ŒåµŒå…¥æŠ€æœ¯ã€‚</li>
<li>å·®å¼‚åè½¬æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§å®éªŒä¸­å‡è¡¨ç°å‡ºä¼˜äºç°æœ‰åŸºçº¿çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´åˆç†ä¸”æ¨¡å‹æ— å…³çš„å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07750">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fb55915f271aeca2b8764fbc0282101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdd5d61c40673649ec8e34b53cff97b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533b2dcd503451e345dfe4e20280112b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a23292cdb8692c421aeaf6212401a58a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Consistent-Video-Editing-as-Flow-Driven-Image-to-Video-Generation"><a href="#Consistent-Video-Editing-as-Flow-Driven-Image-to-Video-Generation" class="headerlink" title="Consistent Video Editing as Flow-Driven Image-to-Video Generation"></a>Consistent Video Editing as Flow-Driven Image-to-Video Generation</h2><p><strong>Authors:Ge Wang, Songlin Fan, Hangxu Liu, Quanjian Song, Hewei Wang, Jinfeng Xu</strong></p>
<p>With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method. </p>
<blockquote>
<p>éšç€è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç¹è£ï¼Œè§†é¢‘ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨å¾—åˆ°äº†æ˜¾è‘—æ¨åŠ¨ï¼Œä¸”æ— éœ€æ¶ˆè€—å¤§é‡è®¡ç®—æˆæœ¬ã€‚è¯¥ä»»åŠ¡çš„ä¸€ä¸ªç‰¹æ®ŠæŒ‘æˆ˜åœ¨äºä»æºè§†é¢‘åˆ°ç¼–è¾‘è§†é¢‘çš„åŠ¨æ€è½¬ç§»è¿‡ç¨‹ï¼Œè¿™éœ€è¦è€ƒè™‘ä¸¤è€…ä¹‹é—´çš„å½¢çŠ¶å˜å½¢ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè§†é¢‘åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•å¯¹è§†é¢‘ç¼–è¾‘è¿›è¡Œå¤æ‚è¿åŠ¨æ¨¡å¼çš„å»ºæ¨¡ï¼Œä¸”ä»æ ¹æœ¬ä¸Šä»…é™äºå¯¹è±¡æ›¿æ¢ï¼Œå¯¹äºéåˆšæ€§å¯¹è±¡è¿åŠ¨çš„ä»»åŠ¡ï¼Œå¦‚å¤šå¯¹è±¡å’Œäººåƒç¼–è¾‘ï¼Œå´è¢«å¤§å¤§å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å…‰æµåœ¨å¤æ‚è¿åŠ¨å»ºæ¨¡ä¸­æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†FlowV2Væ¥é‡æ–°ç ”ç©¶ä»¥æµé©±åŠ¨çš„å›¾ç‰‡åˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆä»»åŠ¡çš„è§†é¢‘ç¼–è¾‘ã€‚å…·ä½“æ¥è¯´ï¼ŒFlowV2Vå°†æ•´ä¸ªç®¡é“åˆ†è§£ä¸ºé¦–å¸§ç¼–è¾‘å’Œæ¡ä»¶I2Vç”Ÿæˆï¼Œå¹¶æ¨¡æ‹Ÿä¸å˜å½¢å½¢çŠ¶å¯¹é½çš„ä¼ªæµåºåˆ—ï¼Œä»è€Œç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚åœ¨DAVIS-EDITä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒFlowV2Våœ¨DOVERå’Œwarping errorä¸Šåˆ†åˆ«æé«˜äº†13.67%å’Œ50.66%ï¼Œè¯æ˜äº†å…¶åœ¨æ—¶é—´ä¸€è‡´æ€§å’Œæ ·æœ¬è´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œåˆ†æäº†æ‰€æå‡ºæ–¹æ³•ä¸­é¦–å¸§èŒƒå¼å’Œæµå¯¹é½çš„å†…éƒ¨åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07713v1">PDF</a> 16 pages, 12 figures</p>
<p><strong>Summary</strong><br>è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†è§†é¢‘ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨çš„å‘å±•ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ–‡é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤æ‚è¿åŠ¨æ¨¡å¼å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…‰å­¦æµçš„FlowV2Væ¨¡å‹ï¼Œç”¨äºå®ç°æµé©±åŠ¨å›¾åƒåˆ°è§†é¢‘çš„è½¬æ¢ï¼Œä»è€Œæé«˜è§†é¢‘ç¼–è¾‘çš„è´¨é‡ã€‚è¯¥æ¨¡å‹å°†æµç¨‹åˆ†è§£ä¸ºé¦–å¸§ç¼–è¾‘å’Œæ¡ä»¶I2Vç”Ÿæˆï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿä¸å˜å½¢å½¢çŠ¶å¯¹é½çš„ä¼ªæµåºåˆ—ï¼Œç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowV2Våœ¨DAVIS-EDITæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„æ—¶é—´ä¸€è‡´æ€§å’Œæ ·æœ¬è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ä¿ƒè¿›äº†è§†é¢‘ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨çš„å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚è¿åŠ¨æ¨¡å¼å»ºæ¨¡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éåˆšæ€§ç‰©ä½“è¿åŠ¨çš„ä»»åŠ¡ä¸Šã€‚</li>
<li>FlowV2Væ¨¡å‹åˆ©ç”¨å…‰å­¦æµè¿›è¡Œå¤æ‚è¿åŠ¨å»ºæ¨¡ï¼Œæé«˜äº†è§†é¢‘ç¼–è¾‘çš„è´¨é‡ã€‚</li>
<li>FlowV2Vå°†è§†é¢‘ç¼–è¾‘æµç¨‹åˆ†è§£ä¸ºé¦–å¸§ç¼–è¾‘å’Œæ¡ä»¶I2Vç”Ÿæˆã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿä¸å˜å½¢å½¢çŠ¶å¯¹é½çš„ä¼ªæµåºåˆ—ï¼ŒFlowV2Vç¡®ä¿äº†ç¼–è¾‘è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowV2Våœ¨DAVIS-EDITæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5a595a98afd6dbbc699d53702085a9e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-394d391dfd4939fe5003853a37493ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb97e470f32a8ec5055535fc713607d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Evaluating-Robustness-in-Latent-Diffusion-Models-via-Embedding-Level-Augmentation"><a href="#Evaluating-Robustness-in-Latent-Diffusion-Models-via-Embedding-Level-Augmentation" class="headerlink" title="Evaluating Robustness in Latent Diffusion Models via Embedding Level   Augmentation"></a>Evaluating Robustness in Latent Diffusion Models via Embedding Level   Augmentation</h2><p><strong>Authors:Boris Martirosyan, Alexey Karmanov</strong></p>
<p>Latent diffusion models (LDMs) achieve state-of-the-art performance across various tasks, including image generation and video synthesis. However, they generally lack robustness, a limitation that remains not fully explored in current research. In this paper, we propose several methods to address this gap. First, we hypothesize that the robustness of LDMs primarily should be measured without their text encoder, because if we take and explore the whole architecture, the problems of image generator and text encoders wll be fused. Second, we introduce novel data augmentation techniques designed to reveal robustness shortcomings in LDMs when processing diverse textual prompts. We then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using Dreambooth, incorporating these proposed augmentation methods across multiple tasks. Finally, we propose a novel evaluation pipeline specifically tailored to assess the robustness of LDMs fine-tuned via Dreambooth. </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆå’Œè§†é¢‘åˆæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹ç¨³å¥æ€§ï¼Œè¿™ä¸€å±€é™æ€§åœ¨å½“å‰ç ”ç©¶ä¸­å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ç§è§£å†³è¿™ä¸€å·®è·çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‡è®¾LDMçš„ç¨³å¥æ€§ä¸»è¦åº”åœ¨æ²¡æœ‰æ–‡æœ¬ç¼–ç å™¨çš„æƒ…å†µä¸‹è¿›è¡Œæµ‹é‡ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬æ¥å—å¹¶æ¢ç´¢æ•´ä¸ªæ¶æ„ï¼Œå›¾åƒç”Ÿæˆå™¨å’Œæ–‡æœ¬ç¼–ç å™¨çš„é—®é¢˜å°†ä¼šèåˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨æ­ç¤ºLDMåœ¨å¤„ç†å„ç§æ–‡æœ¬æç¤ºæ—¶çš„ç¨³å¥æ€§ç¼ºé™·ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Dreamboothå¯¹Stable Diffusion 3å’ŒStable Diffusion XLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨å¤šé¡¹ä»»åŠ¡ä¸­èå…¥è¿™äº›å»ºè®®çš„å¢å¼ºæ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸“é—¨è¯„ä¼°é€šè¿‡Dreamboothå¾®è°ƒåçš„LDMç¨³å¥æ€§çš„æ–°å‹è¯„ä¼°æµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07706v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„é²æ£’æ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†å¤šç§è§£å†³æ–¹æ³•ã€‚ç ”ç©¶è®¤ä¸ºï¼ŒLDMçš„é²æ£’æ€§ä¸»è¦åº”é€šè¿‡å»æ‰æ–‡æœ¬ç¼–ç å™¨æ¥è¯„ä¼°ï¼›å¼•å…¥æ–°å‹æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä»¥æ­ç¤ºLDMåœ¨å¤„ç†å¤šæ ·æ–‡æœ¬æç¤ºæ—¶çš„ç¨³å¥æ€§ä¸è¶³ï¼›å¹¶ä½¿ç”¨Dreamboothå¯¹Stable Diffusion 3å’ŒStable Diffusion XLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæå‡ºä¸“é—¨è¯„ä¼°é€šè¿‡Dreamboothå¾®è°ƒåçš„LDMé²æ£’æ€§çš„è¯„ä¼°æµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMsåœ¨å›¾åƒç”Ÿæˆå’Œè§†é¢‘åˆæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶é²æ£’æ€§ä»éœ€æé«˜ã€‚</li>
<li>LDMçš„é²æ£’æ€§ä¸»è¦é€šè¿‡å»æ‰æ–‡æœ¬ç¼–ç å™¨æ¥è¯„ä¼°ï¼Œä»¥é¿å…å›¾åƒç”Ÿæˆå™¨å’Œæ–‡æœ¬ç¼–ç å™¨çš„é—®é¢˜ç›¸äº’å¹²æ‰°ã€‚</li>
<li>å¼•å…¥æ–°å‹æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä»¥æ›´å…¨é¢åœ°æ­ç¤ºLDMåœ¨å¤„ç†å¤šæ ·æ–‡æœ¬æç¤ºæ—¶çš„ç¨³å¥æ€§çŸ­æ¿ã€‚</li>
<li>ä½¿ç”¨Dreamboothå¯¹Stable Diffusion 3å’ŒStable Diffusion XLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å…¶æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸“é—¨çš„è¯„ä¼°æµç¨‹ï¼Œç”¨äºè¯„ä¼°é€šè¿‡Dreamboothå¾®è°ƒåçš„LDMçš„é²æ£’æ€§ã€‚</li>
<li>LDMçš„é²æ£’æ€§æ”¹å–„å¯¹äºè§£å†³å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-253997b39f519891ffd3dcfcfbde81c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-553c2695d9a3431dab74ba189f467f25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348b30711e34df573191341f561dd21e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05074e7e69a8a4b08221d20c12502b0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-430f773637f0d2ca93503c7a429146f7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NOVA3D-Normal-Aligned-Video-Diffusion-Model-for-Single-Image-to-3D-Generation"><a href="#NOVA3D-Normal-Aligned-Video-Diffusion-Model-for-Single-Image-to-3D-Generation" class="headerlink" title="NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D   Generation"></a>NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D   Generation</h2><p><strong>Authors:Yuxiao Yang, Peihao Li, Yuhong Zhang, Junzhe Lu, Xianglong He, Minghan Qin, Weitao Wang, Haoqian Wang</strong></p>
<p>3D AI-generated content (AIGC) has made it increasingly accessible for anyone to become a 3D content creator. While recent methods leverage Score Distillation Sampling to distill 3D objects from pretrained image diffusion models, they often suffer from inadequate 3D priors, leading to insufficient multi-view consistency. In this work, we introduce NOVA3D, an innovative single-image-to-3D generation framework. Our key insight lies in leveraging strong 3D priors from a pretrained video diffusion model and integrating geometric information during multi-view video fine-tuning. To facilitate information exchange between color and geometric domains, we propose the Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving generalization and multi-view consistency. Moreover, we introduce the de-conflict geometry fusion algorithm, which improves texture fidelity by addressing multi-view inaccuracies and resolving discrepancies in pose alignment. Extensive experiments validate the superiority of NOVA3D over existing baselines. </p>
<blockquote>
<p>éšç€ä¸‰ç»´äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰çš„å‘å±•ï¼Œä»»ä½•äººéƒ½èƒ½æ›´å®¹æ˜“åœ°æˆä¸ºä¸‰ç»´å†…å®¹åˆ›ä½œè€…ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨å¾—åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯ä»é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ä¸­è’¸é¦å‡ºä¸‰ç»´ç‰©ä½“ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´å¤šè§†è§’ä¸€è‡´æ€§ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†NOVA3Dï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„å•å›¾åƒåˆ°ä¸‰ç»´ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£åœ¨äºåˆ©ç”¨æ¥è‡ªé¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸‰ç»´å…ˆéªŒçŸ¥è¯†å’Œåœ¨å¤šè§†è§’è§†é¢‘å¾®è°ƒè¿‡ç¨‹ä¸­èå…¥å‡ ä½•ä¿¡æ¯ã€‚ä¸ºäº†ä¿ƒè¿›é¢œè‰²å’Œå‡ ä½•åŸŸä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ä½•æ—¶é—´å¯¹é½ï¼ˆGTAï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›å’Œå¤šè§†è§’ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å»å†²çªå‡ ä½•èåˆç®—æ³•ï¼Œé€šè¿‡è§£å†³å¤šè§†è§’ä¸å‡†ç¡®çš„é—®é¢˜å¹¶è§£å†³å§¿æ€å¯¹é½çš„æ­§ä¹‰ï¼Œæé«˜äº†çº¹ç†ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒéªŒè¯äº†NOVA3Dåœ¨ç°æœ‰åŸºçº¿ä¹‹ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07698v1">PDF</a> 8 pages, 7 figures, accepted by ICME 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå½“å‰æ–¹æ³•é€šè¿‡Score Distillation SamplingæŠ€æœ¯ä»å›¾åƒä¸­æå–3Då¯¹è±¡ï¼Œä½†ç¼ºä¹è¶³å¤Ÿçš„3Då…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´å¤šè§†è§’ä¸€è‡´æ€§ä¸è¶³ã€‚æœ¬ç ”ç©¶å¼•å…¥NOVA3Dï¼Œä¸€ä¸ªåˆ›æ–°çš„å•å›¾åƒåˆ°3Dç”Ÿæˆæ¡†æ¶ã€‚å€ŸåŠ©é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å¼ºå¤§3Då…ˆéªŒçŸ¥è¯†å’Œå‡ ä½•ä¿¡æ¯é›†æˆï¼Œåœ¨å¤šè§†è§’è§†é¢‘å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¿¡æ¯äº¤æ¢ã€‚é€šè¿‡æå‡ºGeometry-Temporal Alignmentï¼ˆGTAï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œå¤šè§†è§’ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥å»å†²çªå‡ ä½•èåˆç®—æ³•ï¼Œè§£å†³å¤šè§†è§’ä¸å‡†ç¡®é—®é¢˜å’Œå§¿æ€å¯¹é½ä¸­çš„å·®å¼‚ï¼Œæé«˜çº¹ç†ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆ3Då†…å®¹ï¼Œä½†å­˜åœ¨å¤šè§†è§’ä¸€è‡´æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>NOVA3Dæ¡†æ¶è¢«å¼•å…¥ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„å¼ºå¤§3Då…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡é›†æˆå‡ ä½•ä¿¡æ¯å¹¶åœ¨å¤šè§†è§’è§†é¢‘å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¿¡æ¯äº¤æ¢ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›å’Œå¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥Geometry-Temporal Alignmentï¼ˆGTAï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¿ƒè¿›é¢œè‰²å’Œå‡ ä½•é¢†åŸŸä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚</li>
<li>å»å†²çªå‡ ä½•èåˆç®—æ³•è¢«å¼€å‘ï¼Œä»¥æé«˜çº¹ç†ä¿çœŸåº¦å¹¶è§£å†³å¤šè§†è§’ä¸å‡†ç¡®é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸º3Då†…å®¹åˆ›ä½œæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œä½¿å¾—æ›´å¤šäººèƒ½å¤Ÿä¾¿æ·åœ°æˆä¸º3Då†…å®¹åˆ›ä½œè€…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9830254d0360b4a818dfccde8576c89c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb634951f3ffe50584227273fc71086.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5137063ba5d66a5d36562055f4a2d13e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eeef672460d46b378ad63dc4d632e90c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3789d2b7d399321e0644073d81aa0f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d9fa1b433cfba384477e76d0487dd77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40983824d21139c0c72751df963b8178.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ec3b163ba3bb3af5242c73a8d966ee.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Explore-the-vulnerability-of-black-box-models-via-diffusion-models"><a href="#Explore-the-vulnerability-of-black-box-models-via-diffusion-models" class="headerlink" title="Explore the vulnerability of black-box models via diffusion models"></a>Explore the vulnerability of black-box models via diffusion models</h2><p><strong>Authors:Jiacheng Shi, Yanfu Zhang, Huajie Shao, Ashley Gao</strong></p>
<p>Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications. However, these models also present security and privacy risks, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data. The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯è¿›æ­¥èƒ½å¤Ÿåœ¨å„ç§åº”ç”¨ä¸­ç”Ÿæˆé«˜ä¿çœŸå’Œé€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¹Ÿå¸¦æ¥äº†å®‰å…¨å’Œéšç§é£é™©ï¼ŒåŒ…æ‹¬ç‰ˆæƒä¾µçŠ¯ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ä»¥åŠå¯èƒ½æ¶æ„åˆ©ç”¨äº§ç”Ÿæœ‰å®³æˆ–å†’çŠ¯æ€§å†…å®¹çš„åˆ›å»ºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ç§æ–°å‹å®‰å…¨å¨èƒï¼Œæ”»å‡»è€…åˆ©ç”¨æ‰©æ•£æ¨¡å‹APIç”Ÿæˆåˆæˆå›¾åƒï¼Œç„¶åç”¨å…¶è®­ç»ƒé«˜æ€§èƒ½æ›¿ä»£æ¨¡å‹ã€‚è¿™ä½¿å¾—æ”»å‡»è€…å¯ä»¥åœ¨æ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»¥æå°‘çš„æŸ¥è¯¢æ¬¡æ•°å¯¹é»‘ç›’åˆ†ç±»æ¨¡å‹æ‰§è¡ŒåŸºäºæ¨¡å‹æå–å’Œè¿ç§»çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚ç”Ÿæˆçš„å›¾åƒåˆ†è¾¨ç‡è¶³å¤Ÿé«˜ä¸”å¤šæ ·åŒ–ï¼Œè¶³ä»¥è®­ç»ƒä¸€ä¸ªè¾“å‡ºä¸ç›®æ ‡æ¨¡å‹ç´§å¯†åŒ¹é…çš„æ›¿ä»£æ¨¡å‹ã€‚åœ¨åŒ…æ‹¬CIFARå’ŒImageNetå­é›†çš„ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨0.01æ¬¡æŸ¥è¯¢é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ”¹è¿›äº†27.37%ï¼Œåœ¨å¯¹ç›®æ ‡æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»ä¸­è¾¾åˆ°äº†98.68%çš„æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07590v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºè·¨å¤šä¸ªåº”ç”¨ç”Ÿæˆé«˜ä¿çœŸå’Œé€¼çœŸçš„å›¾åƒæä¾›äº†å¯èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†å®‰å…¨å’Œéšç§é—®é¢˜ï¼Œå¦‚ç‰ˆæƒä¾µçŠ¯ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ä»¥åŠå¯èƒ½æ¶æ„åˆ©ç”¨äº§ç”Ÿçš„æœ‰å®³æˆ–å†’çŠ¯æ€§å†…å®¹ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†ä¸€ç§æ–°å‹å®‰å…¨å¨èƒï¼Œæ”»å‡»è€…åˆ©ç”¨æ‰©æ•£æ¨¡å‹APIç”Ÿæˆåˆæˆå›¾åƒï¼Œè¿›è€Œè®­ç»ƒé«˜æ€§èƒ½æ›¿ä»£æ¨¡å‹ï¼Œä»è€Œå¯¹é»‘ç®±åˆ†ç±»æ¨¡å‹æ‰§è¡Œæ¨¡å‹æå–å’ŒåŸºäºè½¬ç§»çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œæ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒ…æ‹¬CIFARå’ŒImageNetçš„å­é›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨0.01å€çš„æŸ¥è¯¢é¢„ç®—çš„æƒ…å†µä¸‹ï¼Œå¹³å‡æ”¹è¿›äº†27.37%ï¼Œåœ¨å¯¹ç›®æ ‡æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»ä¸­è¾¾åˆ°äº†98.68%çš„æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†é«˜ä¿çœŸå’Œé€¼çœŸçš„å›¾åƒç”Ÿæˆã€‚</li>
<li>è¿™äº›æ¨¡å‹å­˜åœ¨å®‰å…¨å’Œéšç§é—®é¢˜ï¼Œå¦‚ç‰ˆæƒä¾µçŠ¯å’Œæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚</li>
<li>æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹APIç”Ÿæˆåˆæˆå›¾åƒæ¥è®­ç»ƒé«˜æ€§èƒ½æ›¿ä»£æ¨¡å‹ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•èƒ½å¤Ÿæ‰§è¡Œæ¨¡å‹æå–å’ŒåŸºäºè½¬ç§»çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚</li>
<li>æ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ï¼Œæ”»å‡»æˆåŠŸç‡é«˜ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½å¹³å‡æ”¹è¿›äº†27.37%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-061f35db58bf1b5bac44a8df0b22fc37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc445919f2a4ffb95a5409d07084c8ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1322da6b889f93a18dd0653023aa108e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37468c921a783a2a2d8e078f792f89a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ebbbe4618d6d42880d8199cce47eedf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed672944f113ee92bfc37d454c3fa97f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Guided-Diffusion-for-Image-Restoration-on-Edge-Devices-Toward-Lightweight-Perception-in-Embodied-AI"><a href="#Multi-Step-Guided-Diffusion-for-Image-Restoration-on-Edge-Devices-Toward-Lightweight-Perception-in-Embodied-AI" class="headerlink" title="Multi-Step Guided Diffusion for Image Restoration on Edge Devices:   Toward Lightweight Perception in Embodied AI"></a>Multi-Step Guided Diffusion for Image Restoration on Edge Devices:   Toward Lightweight Perception in Embodied AI</h2><p><strong>Authors:Aditya Chakravarty</strong></p>
<p>Diffusion models have shown remarkable flexibility for solving inverse problems without task-specific retraining. However, existing approaches such as Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update per denoising step, limiting restoration fidelity and robustness, especially in embedded or out-of-distribution settings. In this work, we introduce a multistep optimization strategy within each denoising timestep, significantly enhancing image quality, perceptual accuracy, and generalization. Our experiments on super-resolution and Gaussian deblurring demonstrate that increasing the number of gradient updates per step improves LPIPS and PSNR with minimal latency overhead. Notably, we validate this approach on a Jetson Orin Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally trained on face datasets, generalizes effectively to natural and aerial scenes. Our findings highlight MPGDâ€™s potential as a lightweight, plug-and-play restoration module for real-time visual perception in embodied AI agents such as drones and mobile robots. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³æ— éœ€ç‰¹å®šä»»åŠ¡å†è®­ç»ƒçš„é€†é—®é¢˜æ–¹é¢å±•ç°å‡ºäº†æ˜¾è‘—çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼ˆå¦‚æµå½¢ä¿æŒå¼•å¯¼æ‰©æ•£ï¼ˆMPGDï¼‰ï¼‰åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ä»…åº”ç”¨ä¸€æ¬¡æ¢¯åº¦æ›´æ–°ï¼Œè¿™é™åˆ¶äº†æ¢å¤çš„è´¨é‡å’Œç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åµŒå…¥å¼æˆ–åˆ†å¸ƒå¤–çš„ç¯å¢ƒä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¯ä¸€æ­¥ä¸­çš„å¤šæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ã€æ„ŸçŸ¥å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨è¶…åˆ†è¾¨ç‡å’Œé«˜æ–¯å»æ¨¡ç³Šæ–¹é¢çš„å®éªŒè¡¨æ˜ï¼Œå¢åŠ æ¯æ­¥çš„æ¢¯åº¦æ›´æ–°æ¬¡æ•°å¯ä»¥æé«˜LPIPSå’ŒPSNRæŒ‡æ ‡ï¼ŒåŒæ—¶åªæœ‰å¾ˆå°çš„å»¶è¿Ÿå¼€é”€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨é€€åŒ–ImageNetå’Œæ— äººæœºæ•°æ®é›†çš„Jetson Orin Nanoä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜MPGDåœ¨é¢éƒ¨æ•°æ®é›†ä¸Šçš„è®­ç»ƒå¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°è‡ªç„¶å’Œèˆªç©ºåœºæ™¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†MPGDä½œä¸ºè½»é‡çº§ã€å³æ’å³ç”¨æ¢å¤æ¨¡å—çš„æ½œåŠ›ï¼Œé€‚ç”¨äºæ— äººæœºå’Œç§»åŠ¨æœºå™¨äººç­‰å®ä½“äººå·¥æ™ºèƒ½ä»£ç†ä¸­çš„å®æ—¶è§†è§‰æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07286v1">PDF</a> Accepted in CVPR 2025 Embodied AI Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨è§£å†³é€†é—®é¢˜ä¸­çš„çµæ´»æ€§ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æ–¹æ³•å¦‚æµå½¢ä¿æŒå¼•å¯¼æ‰©æ•£ï¼ˆMPGDï¼‰åœ¨æ¯æ­¥å»å™ªä¸­ä»…åº”ç”¨å•ä¸€æ¢¯åº¦æ›´æ–°ï¼Œé™åˆ¶äº†æ¢å¤è´¨é‡å’Œé²æ£’æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨æ¯ä¸ªå»å™ªæ—¶é—´æ­¥é•¿å†…è¿›è¡Œå¤šæ¬¡æ¢¯åº¦æ›´æ–°ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ã€æ„ŸçŸ¥å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå¢åŠ æ¯æ­¥çš„æ¢¯åº¦æ›´æ–°æ¬¡æ•°èƒ½æé«˜LPIPSå’ŒPSNRæŒ‡æ ‡ï¼Œä¸”å»¶è¿Ÿå¼€é”€è¾ƒå°ã€‚æ­¤å¤–ï¼Œåœ¨Jetson Orin Nanoå¹³å°ä¸Šä½¿ç”¨é€€åŒ–ImageNetå’Œæ— äººæœºæ•°æ®é›†éªŒè¯äº†MPGDæ–¹æ³•å¯¹é¢éƒ¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼Œå±•ç¤ºå…¶åœ¨æ— äººæœºå’Œç§»åŠ¨æœºå™¨äººç­‰æ™ºèƒ½å®ä½“çš„å®æ—¶è§†è§‰æ„ŸçŸ¥ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹å±•ç¤ºäº†è§£å†³é€†é—®é¢˜çš„çµæ´»æ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚MPGDåœ¨è§£å†³é€†é—®é¢˜æ—¶ä»…ä½¿ç”¨å•ä¸€æ¢¯åº¦æ›´æ–°ï¼Œé™åˆ¶äº†æ¢å¤è´¨é‡å’Œé²æ£’æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¤šæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œæé«˜äº†å›¾åƒè´¨é‡ã€æ„ŸçŸ¥å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¢åŠ æ¯æ­¥çš„æ¢¯åº¦æ›´æ–°æ¬¡æ•°èƒ½æé«˜å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡LPIPSå’ŒPSNRã€‚</li>
<li>å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨Jetson Orin Nanoå¹³å°ä¸Šå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æœªå‚ä¸è®­ç»ƒçš„è‡ªç„¶åœºæ™¯å’Œèˆªç©ºå›¾åƒä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰ä½œä¸ºå®æ—¶è§†è§‰æ„ŸçŸ¥ä¸­è½»é‡çº§ã€å³æ’å³ç”¨çš„æ¢å¤æ¨¡å—çš„æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad7663d829b795e7a0aa49ce50a313ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bebdab10aecd645a814c67cea486edc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adc194bc314a0a2db6e64e0cd977a8e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-098d699960f90e114be95c4a6ebb1271.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Continuous-Semi-Implicit-Models"><a href="#Continuous-Semi-Implicit-Models" class="headerlink" title="Continuous Semi-Implicit Models"></a>Continuous Semi-Implicit Models</h2><p><strong>Authors:Longlin Yu, Jiajun Zha, Tong Yang, Tianyu Xie, Xiangyu Zhang, S. -H. Gary Chan, Cheng Zhang</strong></p>
<p>Semi-implicit distributions have shown great promise in variational inference and generative modeling. Hierarchical semi-implicit models, which stack multiple semi-implicit layers, enhance the expressiveness of semi-implicit distributions and can be used to accelerate diffusion models given pretrained score networks. However, their sequential training often suffers from slow convergence. In this paper, we introduce CoSIM, a continuous semi-implicit model that extends hierarchical semi-implicit models into a continuous framework. By incorporating a continuous transition kernel, CoSIM enables efficient, simulation-free training. Furthermore, we show that CoSIM achieves consistency with a carefully designed transition kernel, offering a novel approach for multistep distillation of generative models at the distributional level. Extensive experiments on image generation demonstrate that CoSIM performs on par or better than existing diffusion model acceleration methods, achieving superior performance on FD-DINOv2. </p>
<blockquote>
<p>åŠéšåˆ†å¸ƒï¼ˆSemi-implicit distributionsï¼‰åœ¨å˜åˆ†æ¨æ–­å’Œç”Ÿæˆå»ºæ¨¡æ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚å±‚æ¬¡åŠéšæ¨¡å‹ï¼ˆHierarchical semi-implicit modelsï¼‰é€šè¿‡å †å å¤šä¸ªåŠéšå±‚ï¼Œå¢å¼ºäº†åŠéšåˆ†å¸ƒçš„è¡¨è¾¾åŠ›ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿç»™å®šé¢„è®­ç»ƒåˆ†æ•°ç½‘ç»œçš„æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é¡ºåºè®­ç»ƒé€šå¸¸å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CoSIMï¼Œä¸€ä¸ªè¿ç»­åŠéšæ¨¡å‹ï¼Œå®ƒå°†å±‚æ¬¡åŠéšæ¨¡å‹æ‰©å±•åˆ°ä¸€ä¸ªè¿ç»­æ¡†æ¶ä¸­ã€‚é€šè¿‡å¼•å…¥è¿ç»­è½¬æ¢æ ¸ï¼ˆcontinuous transition kernelï¼‰ï¼ŒCoSIMå®ç°äº†é«˜æ•ˆã€æ— æ¨¡æ‹Ÿçš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è½¬æ¢æ ¸ï¼ŒCoSIMåœ¨åˆ†å¸ƒå±‚é¢å®ç°äº†ä¸€è‡´æ€§ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„å¤šæ­¥è’¸é¦æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoSIMçš„è¡¨ç°ä¸ç°æœ‰æ‰©æ•£æ¨¡å‹åŠ é€Ÿæ–¹æ³•æŒå¹³æˆ–æ›´å¥½ï¼Œåœ¨FD-DINOv2ä¸Šå–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06778v1">PDF</a> 26 pages, 8 figures, ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŠéšåˆ†å¸ƒå·²åœ¨å˜åˆ†æ¨æ–­å’Œç”Ÿæˆå»ºæ¨¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚å±‚æ¬¡åŠéšæ¨¡å‹é€šè¿‡å †å å¤šä¸ªåŠéšå±‚å¢å¼ºäº†åŠéšåˆ†å¸ƒçš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶å¯ç”¨äºåŠ é€ŸåŸºäºé¢„è®­ç»ƒè¯„åˆ†ç½‘ç»œçš„æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶åºè´¯è®­ç»ƒé€šå¸¸å­˜åœ¨æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è¿ç»­åŠéšæ¨¡å‹ï¼ˆCoSIMï¼‰ï¼Œå®ƒå°†å±‚æ¬¡åŠéšæ¨¡å‹æ‰©å±•åˆ°äº†ä¸€ä¸ªè¿ç»­æ¡†æ¶ä¸­ã€‚é€šè¿‡å¼•å…¥è¿ç»­è½¬æ¢æ ¸ï¼ŒCoSIMå®ç°äº†æ— éœ€æ¨¡æ‹Ÿçš„é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†CoSIMé€šè¿‡ç²¾å¿ƒè®¾è®¡è½¬æ¢æ ¸å®ç°äº†ä¸€è‡´æ€§ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„åˆ†å¸ƒçº§å¤šæ­¥è’¸é¦æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ã€‚åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCoSIMçš„è¡¨ç°ä¸ç°æœ‰æ‰©æ•£æ¨¡å‹åŠ é€Ÿæ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ï¼Œåœ¨FD-DINOv2ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠéšåˆ†å¸ƒåœ¨å˜åˆ†æ¨æ–­å’Œç”Ÿæˆå»ºæ¨¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å±‚æ¬¡åŠéšæ¨¡å‹é€šè¿‡å †å å¤šä¸ªåŠéšå±‚å¢å¼ºäº†è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ç°æœ‰å±‚æ¬¡åŠéšæ¨¡å‹çš„åºè´¯è®­ç»ƒå­˜åœ¨æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚</li>
<li>CoSIMæ˜¯ä¸€ç§è¿ç»­åŠéšæ¨¡å‹ï¼Œå°†å±‚æ¬¡åŠéšæ¨¡å‹æ‰©å±•åˆ°äº†è¿ç»­æ¡†æ¶ä¸­ã€‚</li>
<li>CoSIMé€šè¿‡å¼•å…¥è¿ç»­è½¬æ¢æ ¸ï¼Œå®ç°äº†é«˜æ•ˆã€æ— éœ€æ¨¡æ‹Ÿçš„è®­ç»ƒã€‚</li>
<li>CoSIMåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸ç°æœ‰æ‰©æ•£æ¨¡å‹åŠ é€Ÿæ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00803575085f06c5158ae7a83e286e0f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reward-Instruct-A-Reward-Centric-Approach-to-Fast-Photo-Realistic-Image-Generation"><a href="#Reward-Instruct-A-Reward-Centric-Approach-to-Fast-Photo-Realistic-Image-Generation" class="headerlink" title="Reward-Instruct: A Reward-Centric Approach to Fast Photo-Realistic Image   Generation"></a>Reward-Instruct: A Reward-Centric Approach to Fast Photo-Realistic Image   Generation</h2><p><strong>Authors:Yihong Luo, Tianyang Hu, Weijian Luo, Kenji Kawaguchi, Jing Tang</strong></p>
<p>This paper addresses the challenge of achieving high-quality and fast image generation that aligns with complex human preferences. While recent advancements in diffusion models and distillation have enabled rapid generation, the effective integration of reward feedback for improved abilities like controllability and preference alignment remains a key open problem. Existing reward-guided post-training approaches targeting accelerated few-step generation often deem diffusion distillation losses indispensable. However, in this paper, we identify an interesting yet fundamental paradigm shift: as conditions become more specific, well-designed reward functions emerge as the primary driving force in training strong, few-step image generative models. Motivated by this insight, we introduce Reward-Instruct, a novel and surprisingly simple reward-centric approach for converting pre-trained base diffusion models into reward-enhanced few-step generators. Unlike existing methods, Reward-Instruct does not rely on expensive yet tricky diffusion distillation losses. Instead, it iteratively updates the few-step generatorâ€™s parameters by directly sampling from a reward-tilted parameter distribution. Such a training approach entirely bypasses the need for expensive diffusion distillation losses, making it favorable to scale in high image resolutions. Despite its simplicity, Reward-Instruct yields surprisingly strong performance. Our extensive experiments on text-to-image generation have demonstrated that Reward-Instruct achieves state-of-the-art results in visual quality and quantitative metrics compared to distillation-reliant methods, while also exhibiting greater robustness to the choice of reward function. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨åº”å¯¹å®ç°ç¬¦åˆå¤æ‚äººç±»åå¥½çš„é«˜è´¨é‡ä¸”å¿«é€Ÿçš„å›¾åƒç”ŸæˆæŒ‘æˆ˜ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹å’Œè’¸é¦çš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†å¿«é€Ÿç”Ÿæˆï¼Œä½†å¦‚ä½•æœ‰æ•ˆæ•´åˆå¥–åŠ±åé¦ˆä»¥æé«˜å¯æ§æ€§å’Œåå¥½å¯¹é½ç­‰èƒ½åŠ›ä»æ˜¯å…³é”®å¼€æ”¾é—®é¢˜ã€‚é’ˆå¯¹åŠ é€Ÿå°‘æ­¥éª¤ç”Ÿæˆçš„ç›®æ ‡ï¼Œç°æœ‰çš„å¥–åŠ±å¼•å¯¼åè®­ç»ƒæ–¹æ³•é€šå¸¸è®¤ä¸ºæ‰©æ•£è’¸é¦æŸå¤±ä¸å¯æˆ–ç¼ºã€‚ç„¶è€Œï¼Œæœ¬æ–‡è¯†åˆ«å‡ºäº†ä¸€ä¸ªæœ‰è¶£ä¸”åŸºæœ¬çš„èŒƒå¼è½¬å˜ï¼šéšç€æ¡ä»¶å˜å¾—æ›´åŠ å…·ä½“ï¼Œè®¾è®¡ç²¾è‰¯çš„å¥–åŠ±å‡½æ•°æˆä¸ºè®­ç»ƒå¼ºå¤§å°‘æ­¥éª¤å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚å—æ­¤è§è§£çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†Reward-Instructï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”å‡ºäººæ„æ–™çš„ä»¥å¥–åŠ±ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œå¯å°†é¢„è®­ç»ƒçš„åŸºå‡†æ‰©æ•£æ¨¡å‹è½¬æ¢ä¸ºå¥–åŠ±å¢å¼ºçš„å°‘æ­¥éª¤ç”Ÿæˆå™¨ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒReward-Instructä¸ä¾èµ–äºæ˜‚è´µè€Œå¤æ‚çš„æ‰©æ•£è’¸é¦æŸå¤±ã€‚ç›¸åï¼Œå®ƒé€šè¿‡ç›´æ¥ä»å¥–åŠ±å€¾æ–œçš„å‚æ•°åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·æ¥è¿­ä»£æ›´æ–°å°‘æ­¥éª¤ç”Ÿæˆå™¨çš„å‚æ•°ã€‚è¿™ç§è®­ç»ƒæ–¹å¼å®Œå…¨ç»•è¿‡äº†æ˜‚è´µçš„æ‰©æ•£è’¸é¦æŸå¤±çš„éœ€æ±‚ï¼Œä½¿å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æ›´å…·å¯æ‰©å±•æ€§ã€‚å°½ç®¡å…¶ç®€å•æ€§ï¼ŒReward-Instructçš„è¡¨ç°å´å‡ºäººæ„æ–™åœ°å¼ºå¤§ã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ä¾èµ–è’¸é¦çš„æ–¹æ³•ç›¸æ¯”ï¼ŒReward-Instructåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼ŒåŒæ—¶å¯¹å¥–åŠ±å‡½æ•°çš„é€‰æ‹©è¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13070v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œå¥–åŠ±åé¦ˆæ¥å®ç°é«˜è´¨é‡ã€å¿«é€Ÿçš„å›¾åƒç”Ÿæˆï¼Œå¹¶ä¸äººå¤æ‚çš„åå¥½å¯¹é½ã€‚æ–‡ç« æŒ‡å‡ºï¼Œéšç€æ¡ä»¶å˜å¾—æ—¥ç›Šç‰¹å®šåŒ–ï¼Œè®¾è®¡è‰¯å¥½çš„å¥–åŠ±å‡½æ•°æˆä¸ºè®­ç»ƒå¼ºå¤§ã€å°‘æ•°æ­¥éª¤å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºReward-Instructçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥å¥–åŠ±ä¸ºä¸­å¿ƒï¼Œå¯å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºå¥–åŠ±å¢å¼ºçš„å°‘æ•°æ­¥éª¤ç”Ÿæˆå™¨ã€‚ä¸ä¾èµ–æ˜‚è´µçš„æ‰©æ•£è’¸é¦æŸå¤±çš„æ–¹æ³•ä¸åŒï¼ŒReward-Instructé€šè¿‡ç›´æ¥ä»å¥–åŠ±å€¾æ–œçš„å‚æ•°åˆ†å¸ƒä¸­é‡‡æ ·æ¥æ›´æ–°å°‘æ•°æ­¥éª¤ç”Ÿæˆå™¨çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å®Œå…¨é¿å…äº†æ˜‚è´µçš„æ‰©æ•£è’¸é¦æŸå¤±çš„éœ€è¦ï¼Œä½¿å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æ›´å…·å¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒReward-Instructåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶å¯¹å¥–åŠ±å‡½æ•°çš„é€‰æ‹©è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡ç« æŒ‡å‡ºäº†åœ¨æ‰©æ•£æ¨¡å‹å’Œå¥–åŠ±åé¦ˆçš„ç»“åˆä¸‹å®ç°é«˜è´¨é‡å’Œå¿«é€Ÿå›¾åƒç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä¸å¤æ‚äººç±»åå¥½å¯¹é½çš„é—®é¢˜ã€‚</li>
<li>å­˜åœ¨ä¸€ä¸ªå…³é”®çš„è§‚å¿µè½¬å˜ï¼šåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œè‰¯å¥½çš„å¥–åŠ±å‡½æ•°æˆä¸ºè®­ç»ƒå°‘æ•°æ­¥éª¤å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±ä¸ºä¸­å¿ƒçš„æ–¹æ³•Reward-Instructï¼Œç”¨äºå°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºå¥–åŠ±å¢å¼ºçš„å°‘æ•°æ­¥éª¤ç”Ÿæˆå™¨ã€‚</li>
<li>Reward-Instructæ–¹æ³•ä¸ä¾èµ–æ˜‚è´µçš„æ‰©æ•£è’¸é¦æŸå¤±ï¼Œè€Œæ˜¯ç›´æ¥ä»å¥–åŠ±å€¾æ–œçš„å‚æ•°åˆ†å¸ƒä¸­é‡‡æ ·æ›´æ–°å‚æ•°ã€‚</li>
<li>Reward-Instructæ–¹æ³•å¯å®Œå…¨é¿å…æ‰©æ•£è’¸é¦æŸå¤±çš„éœ€è¦ï¼Œä½¿å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­å…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒReward-Instructåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>Reward-Instructå¯¹å¥–åŠ±å‡½æ•°çš„é€‰æ‹©å…·æœ‰è¾ƒå¼ºçš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c60ddf02b7bba5ab073561943bf6fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-337a9c9e22ca026de9b5b7b6dc3c83e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b0b82eca6167b443c8ea11091622536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4628beb503d0d739ed466c08ed8ed52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c325e0ac3e7052ff84aa38cda90b44ef.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Gungnir-Exploiting-Stylistic-Features-in-Images-for-Backdoor-Attacks-on-Diffusion-Models"><a href="#Gungnir-Exploiting-Stylistic-Features-in-Images-for-Backdoor-Attacks-on-Diffusion-Models" class="headerlink" title="Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on   Diffusion Models"></a>Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on   Diffusion Models</h2><p><strong>Authors:Yu Pan, Jiahao Chen, Bingrong Dai, Lin Wang, Yi Du, Jiao Liu</strong></p>
<p>In recent years, Diffusion Models (DMs) have demonstrated significant advances in the field of image generation. However, according to current research, DMs are vulnerable to backdoor attacks, which allow attackers to control the modelâ€™s output by inputting data containing covert triggers, such as a specific visual patch or phrase. Existing defense strategies are well equipped to thwart such attacks through backdoor detection and trigger inversion because previous attack methods are constrained by limited input spaces and low-dimensional triggers. For example, visual triggers are easily observed by defenders, text-based or attention-based triggers are more susceptible to neural network detection. To explore more possibilities of backdoor attack in DMs, we propose Gungnir, a novel method that enables attackers to activate the backdoor in DMs through style triggers within input images. Our approach proposes using stylistic features as triggers for the first time and implements backdoor attacks successfully in image-to-image tasks by introducing Reconstructing-Adversarial Noise (RAN) and Short-Term Timesteps-Retention (STTR). Our technique generates trigger-embedded images that are perceptually indistinguishable from clean images, thus bypassing both manual inspection and automated detection neural networks. Experiments demonstrate that Gungnir can easily bypass existing defense methods. Among existing DM defense frameworks, our approach achieves a 0 backdoor detection rate (BDR). Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/paoche11/Gungnir">https://github.com/paoche11/Gungnir</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ ¹æ®å½“å‰çš„ç ”ç©¶ï¼ŒDMså®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡è¾“å…¥åŒ…å«éšè”½è§¦å‘å™¨çš„æ•°æ®æ¥æ§åˆ¶æ¨¡å‹çš„è¾“å‡ºï¼Œä¾‹å¦‚ç‰¹å®šçš„è§†è§‰æ–‘å—æˆ–çŸ­è¯­ã€‚ç°æœ‰çš„é˜²å¾¡ç­–ç•¥é€šè¿‡åé—¨æ£€æµ‹å’Œè§¦å‘åè½¬æ¥æœ‰æ•ˆåœ°é˜»æ­¢æ­¤ç±»æ”»å‡»ï¼Œå› ä¸ºä»¥å‰çš„æ”»å‡»æ–¹æ³•å—åˆ°æœ‰é™è¾“å…¥ç©ºé—´å’Œä½ç»´è§¦å‘çš„é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œè§†è§‰è§¦å‘å™¨å¾ˆå®¹æ˜“è¢«é˜²å¾¡è€…è§‚å¯Ÿåˆ°ï¼Œè€ŒåŸºäºæ–‡æœ¬æˆ–åŸºäºæ³¨æ„åŠ›çš„è§¦å‘å™¨æ›´å®¹æ˜“å—åˆ°ç¥ç»ç½‘ç»œæ£€æµ‹ã€‚ä¸ºäº†æ¢ç´¢DMä¸­åé—¨æ”»å‡»çš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Gungnirè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿä½¿æ”»å‡»è€…é€šè¿‡è¾“å…¥å›¾åƒä¸­çš„é£æ ¼è§¦å‘å™¨åœ¨DMä¸­æ¿€æ´»åé—¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–æ¬¡æå‡ºä½¿ç”¨é£æ ¼ç‰¹å¾ä½œä¸ºè§¦å‘å™¨ï¼Œå¹¶é€šè¿‡å¼•å…¥é‡å»ºå¯¹æŠ—å™ªå£°ï¼ˆRANï¼‰å’ŒçŸ­æœŸæ—¶é—´æ­¥ä¿ç•™ï¼ˆSTTRï¼‰æˆåŠŸåœ°åœ¨å›¾åƒåˆ°å›¾åƒä»»åŠ¡ä¸­å®ç°åé—¨æ”»å‡»ã€‚æˆ‘ä»¬çš„æŠ€æœ¯ç”Ÿæˆäº†åµŒå…¥è§¦å‘å™¨çš„å›¾åƒï¼Œè¿™äº›å›¾åƒåœ¨æ„ŸçŸ¥ä¸Šä¸å¹²å‡€å›¾åƒæ— æ³•åŒºåˆ†ï¼Œä»è€Œç»•è¿‡äº†æ‰‹åŠ¨æ£€æŸ¥å’Œè‡ªåŠ¨åŒ–æ£€æµ‹ç¥ç»ç½‘ç»œã€‚å®éªŒè¡¨æ˜ï¼ŒGungnirå¯ä»¥è½»æ¾ç»•è¿‡ç°æœ‰é˜²å¾¡æ–¹æ³•ã€‚åœ¨ç°æœ‰çš„DMé˜²å¾¡æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†0åé—¨æ£€æµ‹ç‡ï¼ˆBDRï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/paoche11/Gungnir%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/paoche11/Gungniræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20650v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusion Modelsï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºDMså­˜åœ¨åé—¨æ”»å‡»çš„é£é™©ã€‚ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åä¸ºGungnirçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é£æ ¼ç‰¹å¾ä½œä¸ºè§¦å‘å™¨ï¼Œé€šè¿‡å‘è¾“å…¥å›¾åƒä¸­åµŒå…¥è§¦å‘å› ç´ ï¼Œå®ç°åœ¨å›¾åƒåˆ°å›¾åƒçš„DMä»»åŠ¡ä¸­è¿›è¡Œåé—¨æ”»å‡»ã€‚è¿™ç§æ–¹æ³•ç”Ÿæˆçš„å¯è§¦å‘å›¾åƒä¸å¹²å‡€å›¾åƒåœ¨æ„ŸçŸ¥ä¸Šæ— æ³•åŒºåˆ†ï¼Œä»è€Œç»•è¿‡äº†æ‰‹åŠ¨æ£€æŸ¥å’Œè‡ªåŠ¨åŒ–æ£€æµ‹ç¥ç»ç½‘ç»œã€‚å®éªŒè¡¨æ˜ï¼ŒGungnirå¯ä»¥è½»æ¾ç»•è¿‡ç°æœ‰é˜²å¾¡æ–¹æ³•ï¼Œç°æœ‰DMé˜²å¾¡æ¡†æ¶å¯¹å…¶åé—¨æ£€æµ‹ç‡ä¸ºé›¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Modelsï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨åé—¨æ”»å‡»çš„é£é™©ã€‚</li>
<li>Gungniræ˜¯ä¸€ç§æ–°å‹åé—¨æ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨é£æ ¼ç‰¹å¾ä½œä¸ºè§¦å‘å™¨ï¼Œåœ¨å›¾åƒåˆ°å›¾åƒçš„DMä»»åŠ¡ä¸­å®æ–½æ”»å‡»ã€‚</li>
<li>Gungnirç”Ÿæˆçš„å¯è§¦å‘å›¾åƒä¸å¹²å‡€å›¾åƒåœ¨æ„ŸçŸ¥ä¸Šæ— æ³•åŒºåˆ†ï¼Œå¯ç»•è¿‡æ‰‹åŠ¨æ£€æŸ¥å’Œè‡ªåŠ¨åŒ–æ£€æµ‹ç¥ç»ç½‘ç»œã€‚</li>
<li>ç°æœ‰é˜²å¾¡ç­–ç•¥å¯¹Gungniræ–¹æ³•æ•ˆæœä¸ä½³ï¼Œç°æœ‰DMé˜²å¾¡æ¡†æ¶å¯¹å…¶åé—¨æ£€æµ‹ç‡ä¸ºé›¶ã€‚</li>
<li>Gungniré€šè¿‡å¼•å…¥é‡å»ºå¯¹æŠ—å™ªå£°ï¼ˆRANï¼‰å’ŒçŸ­æœŸæ—¶é—´æ­¥ä¿ç•™ï¼ˆSTTRï¼‰å®ç°äº†åé—¨æ”»å‡»ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-60498088d5e38b5f2825f8389ea22b9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc1a2d8ccf2256a2a2857c9ba590e20d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bab06f9fc547a0b2c40652a0533ac66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a81fca22a2eff8388dc126313d3c966d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aff4cb75641dfb8a32e6796489f219df.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="RestoreGrad-Signal-Restoration-Using-Conditional-Denoising-Diffusion-Models-with-Jointly-Learned-Prior"><a href="#RestoreGrad-Signal-Restoration-Using-Conditional-Denoising-Diffusion-Models-with-Jointly-Learned-Prior" class="headerlink" title="RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion   Models with Jointly Learned Prior"></a>RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion   Models with Jointly Learned Prior</h2><p><strong>Authors:Ching-Hua Lee, Chouchang Yang, Jaejin Cho, Yashas Malur Saidutta, Rakshith Sharma Srinivasa, Yilin Shen, Hongxia Jin</strong></p>
<p>Denoising diffusion probabilistic models (DDPMs) can be utilized to recover a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information when shaping the prior, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder (VAE) framework, taking advantage of the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process. </p>
<blockquote>
<p>é™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰å¯ä»¥é€šè¿‡å¯¹é€€åŒ–ä¿¡å·è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»é€€åŒ–è§‚æµ‹ä¸­æ¢å¤å‡ºå¹²å‡€ä¿¡å·ã€‚é€€åŒ–ä¿¡å·æœ¬èº«æ˜¯å¹²å‡€ä¿¡å·çš„æ±¡æŸ“ç‰ˆæœ¬ï¼›ç”±äºè¿™ç§ç›¸å…³æ€§ï¼Œå®ƒä»¬å¯èƒ½åŒ…å«æœ‰å…³ç›®æ ‡æ¸…æ´æ•°æ®åˆ†å¸ƒçš„ä¸€äº›æœ‰ç”¨ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯é‡‡ç”¨æ ‡å‡†é«˜æ–¯ä½œä¸ºå…ˆéªŒåˆ†å¸ƒï¼Œåœ¨æ„å»ºå…ˆéªŒæ—¶ä¸¢å¼ƒäº†æ­¤ç±»ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆ©ç”¨ä¸æ‰©æ•£æ¨¡å‹è”åˆå­¦ä¹ çš„æ›´å…·ä¿¡æ¯é‡çš„å…ˆéªŒï¼Œæ”¹è¿›ç”¨äºä¿¡å·æ¢å¤çš„æ¡ä»¶DDPMã€‚æ‰€æå‡ºçš„æ¡†æ¶ç§°ä¸ºRestoreGradï¼Œæ— ç¼é›†æˆäº†DDPMsåˆ°å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¡†æ¶ä¸­ï¼Œåˆ©ç”¨é€€åŒ–ä¿¡å·å’Œå¹²å‡€ä¿¡å·ä¹‹é—´çš„ç›¸å…³æ€§æ¥ç¼–ç æ›´å¥½çš„æ‰©æ•£å…ˆéªŒã€‚åœ¨è¯­éŸ³å’Œå›¾åƒæ¢å¤ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†RestoreGradç›¸è¾ƒäºç°æœ‰çš„DDPMåŸºçº¿æ–¹æ³•å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼ˆè®­ç»ƒæ­¥éª¤å‡å°‘äº†5-10å€ï¼‰ï¼Œæ¢å¤çš„ä¿¡å·è´¨é‡æ›´å¥½ï¼Œå¹¶ä¸”åœ¨æ¨ç†æ—¶é—´ä¸­ä½¿ç”¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ï¼ˆå‡å°‘äº†2-2.5å€ï¼‰æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ï¼Œè¿™è¯æ˜äº†åˆ©ç”¨è”åˆå­¦ä¹ çš„å…ˆéªŒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æé«˜æ•ˆç‡çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13574v3">PDF</a> Accepted by ICML 2025 - Camera Ready Version</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰ä»é€€åŒ–è§‚æµ‹ä¸­æ¢å¤æ¸…æ´ä¿¡å·çš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ¡ä»¶DDPMsä¿¡å·æ¢å¤æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨ä¸æ¸…æ´ä¿¡å·ç›¸å…³çš„é€€åŒ–ä¿¡å·ï¼Œé‡‡ç”¨è”åˆå­¦ä¹ çš„æ‰©æ•£å…ˆéªŒï¼Œå°†DDPMsæ— ç¼é›†æˆåˆ°å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ¡†æ¶ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³å’Œå›¾åƒæ¢å¤ä»»åŠ¡ä¸Šå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¾¾åˆ°æ›´é«˜çš„æ¢å¤ä¿¡å·è´¨é‡ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´ä½¿ç”¨æ›´å°‘çš„é‡‡æ ·æ­¥éª¤ï¼Œè¯æ˜äº†è”åˆå­¦ä¹ å…ˆéªŒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ•ˆç‡æ”¹è¿›ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDPMsèƒ½å¤Ÿä»é€€åŒ–è§‚æµ‹ä¸­æ¢å¤æ¸…æ´ä¿¡å·ï¼Œé€šè¿‡æ¡ä»¶æ¨¡å‹å®ç°ã€‚</li>
<li>ç°æœ‰æ ‡å‡†é«˜æ–¯å…ˆéªŒåˆ†å¸ƒä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œå½±å“æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„RestoreGradæ¡†æ¶ç»“åˆäº†DDPMså’ŒVAEï¼Œåˆ©ç”¨é€€åŒ–ä¿¡å·å’Œæ¸…æ´ä¿¡å·ä¹‹é—´çš„ç›¸å…³æ€§æ¥ç¼–ç æ›´å¥½çš„æ‰©æ•£å…ˆéªŒã€‚</li>
<li>RestoreGradåœ¨è¯­éŸ³å’Œå›¾åƒæ¢å¤ä»»åŠ¡ä¸Šå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„æ¢å¤ä¿¡å·è´¨é‡ã€‚</li>
<li>RestoreGradä½¿ç”¨æ›´å°‘çš„é‡‡æ ·æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>è”åˆå­¦ä¹ çš„å…ˆéªŒæœ‰åŠ©äºæé«˜æ‰©æ•£è¿‡ç¨‹çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-67cb6ade28b319dfa1cf9d3ebcced19b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6546be501bde5c89ec9e11877602e6a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fafdbedbb69d5e822451e02239b32ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c28d9cd23588c6dd4b945d9f578b842d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CHATS-Combining-Human-Aligned-Optimization-and-Test-Time-Sampling-for-Text-to-Image-Generation"><a href="#CHATS-Combining-Human-Aligned-Optimization-and-Test-Time-Sampling-for-Text-to-Image-Generation" class="headerlink" title="CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for   Text-to-Image Generation"></a>CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for   Text-to-Image Generation</h2><p><strong>Authors:Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>Diffusion models have emerged as a dominant approach for text-to-image generation. Key components such as the human preference alignment and classifier-free guidance play a crucial role in ensuring generation quality. However, their independent application in current text-to-image models continues to face significant challenges in achieving strong text-image alignment, high generation quality, and consistency with human aesthetic standards. In this work, we for the first time, explore facilitating the collaboration of human performance alignment and test-time sampling to unlock the potential of text-to-image models. Consequently, we introduce CHATS (Combining Human-Aligned optimization and Test-time Sampling), a novel generative framework that separately models the preferred and dispreferred distributions and employs a proxy-prompt-based sampling strategy to utilize the useful information contained in both distributions. We observe that CHATS exhibits exceptional data efficiency, achieving strong performance with only a small, high-quality funetuning dataset. Extensive experiments demonstrate that CHATS surpasses traditional preference alignment methods, setting new state-of-the-art across various standard benchmarks. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»å¯¼æ–¹æ³•ã€‚å…³é”®ç»„ä»¶ï¼Œå¦‚äººç±»åå¥½å¯¹é½å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œåœ¨ç¡®ä¿ç”Ÿæˆè´¨é‡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å½“å‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„ç‹¬ç«‹åº”ç”¨ï¼Œåœ¨å®ç°å¼ºå¤§çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€é«˜ç”Ÿæˆè´¨é‡å’Œä¸äººç±»å®¡ç¾æ ‡å‡†çš„ä¸€è‡´æ€§æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†äººç±»æ€§èƒ½å¯¹é½å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·çš„åä½œï¼Œä»¥è§£é”æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ½œåŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CHATSï¼ˆç»“åˆäººç±»å¯¹é½ä¼˜åŒ–å’Œæµ‹è¯•æ—¶é—´é‡‡æ ·ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œåˆ†åˆ«å»ºæ¨¡é¦–é€‰å’Œä¸å—æ¬¢è¿çš„åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨åŸºäºä»£ç†æç¤ºçš„é‡‡æ ·ç­–ç•¥ï¼Œåˆ©ç”¨è¿™ä¸¤ä¸ªåˆ†å¸ƒä¸­åŒ…å«çš„æœ‰ç”¨ä¿¡æ¯ã€‚æˆ‘ä»¬å‘ç°CHATSå…·æœ‰å‡ºè‰²çš„æ•°æ®æ•ˆç‡ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªå°è€Œé«˜è´¨é‡å¾®è°ƒæ•°æ®é›†å°±èƒ½å®ç°å¼ºåŠ²è¡¨ç°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCHATSè¶…è¶Šäº†ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•ï¼Œåœ¨å¤šç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12579v3">PDF</a> ICML 2025. The code is publicly available at   <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/CHATS">https://github.com/AIDC-AI/CHATS</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸»å¯¼æ–¹æ³•ï¼Œä½†å…¶åœ¨å®ç°é«˜è´¨é‡æ–‡æœ¬å›¾åƒå¯¹é½ã€é«˜ç”Ÿæˆè´¨é‡å’Œç¬¦åˆäººç±»å®¡ç¾æ ‡å‡†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†äººç±»æ€§èƒ½å¯¹é½ä¸æµ‹è¯•æ—¶é‡‡æ ·çš„åä½œæ½œåŠ›ï¼Œå¹¶å¼•å…¥CHATSï¼ˆç»“åˆäººç±»å¯¹é½ä¼˜åŒ–å’Œæµ‹è¯•æ—¶é‡‡æ ·ï¼‰è¿™ä¸€æ–°å‹ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ†åˆ«å»ºæ¨¡åå¥½å’Œéåå¥½åˆ†å¸ƒï¼Œå¹¶é‡‡ç”¨åŸºäºä»£ç†æç¤ºçš„é‡‡æ ·ç­–ç•¥ï¼Œåˆ©ç”¨ä¸¤ç§åˆ†å¸ƒä¸­çš„æœ‰ç”¨ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒCHATSè¡¨ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œåœ¨å°å‹é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†ä¸Šå³å¯å®ç°å‡ºè‰²æ€§èƒ½ï¼Œå¹¶è¶…è¶Šä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸçš„åœ°ä½æ˜¾è‘—ã€‚</li>
<li>äººç±»æ€§èƒ½å¯¹é½å’Œæµ‹è¯•æ—¶é‡‡æ ·åœ¨æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ä¸Šè‡³å…³é‡è¦ã€‚</li>
<li>CHATSæ¡†æ¶é¦–æ¬¡ç»“åˆäº†äººç±»å¯¹é½ä¼˜åŒ–å’Œæµ‹è¯•æ—¶é‡‡æ ·ã€‚</li>
<li>CHATSæ¡†æ¶å®ç°äº†åå¥½å’Œéåå¥½åˆ†å¸ƒçš„ç‹¬ç«‹å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡åŸºäºä»£ç†æç¤ºçš„é‡‡æ ·ç­–ç•¥ï¼ŒCHATSèƒ½å¤Ÿåˆ©ç”¨ä¸¤ç§åˆ†å¸ƒä¸­çš„æœ‰ç”¨ä¿¡æ¯ã€‚</li>
<li>CHATSå±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ï¼Œåœ¨å°å‹é«˜è´¨é‡å¾®è°ƒæ•°æ®é›†ä¸Šå³å¯å®ç°å‡ºè‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ad25987b4a9d3f976678d71c461d360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32edb55e828d3ba59c303be4bfbbbe6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e73e63cca66c0c2e7337d962f3758c0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Hummingbird-High-Fidelity-Image-Generation-via-Multimodal-Context-Alignment"><a href="#Hummingbird-High-Fidelity-Image-Generation-via-Multimodal-Context-Alignment" class="headerlink" title="Hummingbird: High Fidelity Image Generation via Multimodal Context   Alignment"></a>Hummingbird: High Fidelity Image Generation via Multimodal Context   Alignment</h2><p><strong>Authors:Minh-Quan Le, Gaurav Mittal, Tianjian Meng, A S M Iftekhar, Vishwas Suryanarayanan, Barun Patra, Dimitris Samaras, Mei Chen</strong></p>
<p>While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes in generated images consistent with a multimodal context, i.e. a reference image with accompanying text guidance query. To address this, we introduce $\textbf{Hummingbird}$, the first diffusion-based image generator which, given a multimodal context, generates highly diverse images w.r.t. the reference image while ensuring high fidelity by accurately preserving scene attributes, such as object interactions and spatial relationships from the text guidance. Hummingbird employs a novel Multimodal Context Evaluator that simultaneously optimizes our formulated Global Semantic and Fine-grained Consistency Rewards to ensure generated images preserve the scene attributes of reference images in relation to the text guidance while maintaining diversity. As the first model to address the task of maintaining both diversity and fidelity given a multimodal context, we introduce a new benchmark formulation incorporating MME Perception and Bongard HOI datasets. Benchmark experiments show Hummingbird outperforms all existing methods by achieving superior fidelity while maintaining diversity, validating Hummingbirdâ€™s potential as a robust multimodal context-aligned image generator in complex visual tasks. Project page: <a target="_blank" rel="noopener" href="https://roar-ai.github.io/hummingbird">https://roar-ai.github.io/hummingbird</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶åœ¨ä¸ºå¯¹è±¡ä¸­å¿ƒä»»åŠ¡ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„åˆæˆæ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨åœºæ™¯æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œäººæœºäº¤äº’ï¼ˆHOIï¼‰æ¨ç†ï¼‰æ–¹é¢ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€é‡åˆ°å›°éš¾ã€‚åœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¿æŒç”Ÿæˆå›¾åƒçš„åœºæ™¯å±æ€§ä¸å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼ˆå³å¸¦æœ‰æ–‡æœ¬æŒ‡å¯¼æŸ¥è¯¢çš„å‚è€ƒå›¾åƒï¼‰çš„ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>Hummingbird</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨ã€‚ç»™å®šå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼ŒHummingbirdèƒ½å¤Ÿç”Ÿæˆä¸å‚è€ƒå›¾åƒé«˜åº¦ç›¸å…³çš„å¤šæ ·åŒ–å›¾åƒï¼ŒåŒæ—¶ç¡®ä¿é«˜ä¿çœŸåº¦ï¼Œå‡†ç¡®ä¿ç•™åœºæ™¯å±æ€§ï¼Œå¦‚å¯¹è±¡äº¤äº’å’Œæ–‡æœ¬æŒ‡å¯¼ä¸­çš„ç©ºé—´å…³ç³»ã€‚Hummingbirdé‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è¯„ä¼°å™¨ï¼Œè¯¥è¯„ä¼°å™¨åŒæ—¶ä¼˜åŒ–äº†æˆ‘ä»¬åˆ¶å®šçš„å…¨å±€è¯­ä¹‰å’Œç²¾ç»†ç²’åº¦ä¸€è‡´æ€§å¥–åŠ±ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒåœ¨ä¿æŒæ–‡æœ¬æŒ‡å¯¼çš„å‚è€ƒå›¾åƒåœºæ™¯å±æ€§çš„åŒæ—¶ï¼Œä¿æŒå¤šæ ·æ€§ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªè§£å†³åœ¨ç»™å®šå¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶ä¿æŒå¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å½¢å¼ï¼Œç»“åˆäº†MMEæ„ŸçŸ¥å’ŒBongard HOIæ•°æ®é›†ã€‚åŸºå‡†æµ‹è¯•å®éªŒè¡¨æ˜ï¼ŒHummingbirdä¼˜äºæ‰€æœ‰ç°æœ‰æ–¹æ³•ï¼Œåœ¨ä¿æŒå¤šæ ·æ€§çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„ä¿çœŸåº¦ï¼ŒéªŒè¯äº†Hummingbirdä½œä¸ºå¤æ‚è§†è§‰ä»»åŠ¡çš„ç¨³å¥å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å¯¹é½å›¾åƒç”Ÿæˆå™¨çš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://roar-ai.github.io/hummingbird">https://roar-ai.github.io/hummingbird</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05153v2">PDF</a> Accepted to ICLR 2025. Project page with code release:   <a target="_blank" rel="noopener" href="https://roar-ai.github.io/hummingbird">https://roar-ai.github.io/hummingbird</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨åœºæ™¯æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”å’Œäººç±»ç‰©ä½“äº¤äº’æ¨ç†ï¼‰ä¸­çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨â€”â€”Hummingbirdã€‚Hummingbirdèƒ½å¤Ÿç»“åˆå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–çš„å›¾åƒï¼ŒåŒæ—¶ç¡®ä¿å›¾åƒçš„é«˜ä¿çœŸåº¦ï¼Œå‡†ç¡®ä¿ç•™åœºæ™¯å±æ€§ï¼Œå¦‚ç‰©ä½“äº¤äº’å’Œç©ºé—´å…³ç³»ã€‚å®ƒé€šè¿‡æ–°å‹çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è¯„ä¼°å™¨ï¼ŒåŒæ—¶ä¼˜åŒ–å…¨å±€è¯­ä¹‰å’Œç²¾ç»†ä¸€è‡´æ€§å¥–åŠ±ï¼Œä»¥åœ¨ä¿æŒå›¾åƒå¤šæ ·æ€§çš„åŒæ—¶ï¼Œç¡®ä¿ä¸æ–‡æœ¬æŒ‡å¯¼ç›¸å…³çš„åœºæ™¯å±æ€§çš„ä¿ç•™ã€‚ä½œä¸ºé¦–ä¸ªè§£å†³åœ¨ç»™å®šå¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶ä¿æŒå¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æ¨¡å‹ï¼Œæœ¬æ–‡å¼•å…¥äº†ç»“åˆMMEæ„ŸçŸ¥å’ŒBongard HOIæ•°æ®é›†çš„æ–°åŸºå‡†æµ‹è¯•æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒHummingbirdåœ¨ä¿æŒå¤šæ ·æ€§çš„åŒæ—¶å®ç°äº†é«˜ä¿çœŸåº¦ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­ä½œä¸ºç¨³å¥çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å¯¹é½å›¾åƒç”Ÿæˆå™¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨åœºæ™¯æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”å’Œäººç±»ç‰©ä½“äº¤äº’æ¨ç†ï¼‰ä¸­è¡¨ç°ä¸è¶³ã€‚</li>
<li>Hummingbirdæ˜¯åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿç»“åˆå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ç”Ÿæˆå¤šæ ·åŒ–ä¸”é«˜ä¿çœŸåº¦çš„å›¾åƒã€‚</li>
<li>Hummingbirdé€šè¿‡æ–°å‹çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è¯„ä¼°å™¨ï¼ŒåŒæ—¶ä¼˜åŒ–å…¨å±€è¯­ä¹‰å’Œç²¾ç»†ä¸€è‡´æ€§å¥–åŠ±ã€‚</li>
<li>Hummingbirdèƒ½å¤Ÿåœ¨ä¿æŒå¤šæ ·æ€§çš„åŒæ—¶ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸æ–‡æœ¬æŒ‡å¯¼ç›¸å…³çš„åœºæ™¯å±æ€§çš„ä¿ç•™ã€‚</li>
<li>ä½œä¸ºé¦–ä¸ªè§£å†³åœ¨ç»™å®šå¤šæ¨¡æ€ä¸Šä¸‹æ–‡æ—¶ä¿æŒå¤šæ ·æ€§å’Œä¿çœŸåº¦çš„æ¨¡å‹ï¼ŒHummingbirdå…·æœ‰æ½œåŠ›æˆä¸ºå¤æ‚è§†è§‰ä»»åŠ¡ä¸­çš„ç¨³å¥å¤šæ¨¡æ€ä¸Šä¸‹æ–‡å¯¹é½å›¾åƒç”Ÿæˆå™¨ã€‚</li>
<li>å¼•å…¥çš„æ–°åŸºå‡†æµ‹è¯•æ–¹æ³•ç»“åˆäº†MMEæ„ŸçŸ¥å’ŒBongard HOIæ•°æ®é›†ï¼Œä»¥è¯„ä¼°å›¾åƒç”Ÿæˆå™¨çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f45cb568ca443b38e4a0d60e7c0ec40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb5aab0ceba12366141e346d42cd94f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5324cb294e0dbcba95884db08c0b2480.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50e4ed7aa3408254ec81ed2af52b073e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c3005ee20960f03d62fd66584cd43830.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  CXR-LT 2024 A MICCAI challenge on long-tailed, multi-label, and   zero-shot disease classification from chest X-ray
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6b44a90b1b737d8d9dfbec564dbf746a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Genesis Multimodal Driving Scene Generation with Spatio-Temporal and   Cross-Modal Consistency
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
