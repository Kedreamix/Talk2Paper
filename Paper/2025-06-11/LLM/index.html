<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Play to Generalize Learning to Reason Through Game Play">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c26749c0e1e150ac16d161f9e046d29a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="Play-to-Generalize-Learning-to-Reason-Through-Game-Play"><a href="#Play-to-Generalize-Learning-to-Reason-Through-Game-Play" class="headerlink" title="Play to Generalize: Learning to Reason Through Game Play"></a>Play to Generalize: Learning to Reason Through Game Play</h2><p><strong>Authors:Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei</strong></p>
<p>Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base modelâ€™s performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å‘å±•å¯æ¨å¹¿çš„æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å—è®¤çŸ¥ç§‘å­¦æ–‡çŒ®çš„å¯å‘ï¼Œè¯¥æ–‡çŒ®è¡¨æ˜æ¸¸æˆå¯ä»¥ä¿ƒè¿›å¯è¿ç§»çš„è®¤çŸ¥æŠ€èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒèŒƒå¼ï¼Œå³è§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ï¼Œå…¶ä¸­MLLMsé€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆæ¥å‘å±•è·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨ç®€å•çš„ç±»ä¼¼è¡—æœºæ¸¸æˆä¸Šå¯¹å…·æœ‰7Bå‚æ•°çš„MLLMè¿›è¡Œåè®­ç»ƒï¼Œä¾‹å¦‚Snakeæ¸¸æˆï¼Œæ˜¾è‘—æé«˜äº†å…¶åœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVistaï¼‰å’Œå¤šå­¦ç§‘é—®é¢˜ï¼ˆå¦‚MMMUï¼‰ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ²¡æœ‰æ¥è§¦åˆ°ä»»ä½•è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹æŒæ¡äº†å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ã€‚ä»¤äººç©ç›®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¸“é—¨é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†æ•°æ®è°ƒæ•´çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†åŸºç¡€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“å®¶æ¨¡å‹é€šå¸¸æ— æ³•è¾¾åˆ°çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒèŒƒå¼ï¼šåˆæˆã€åŸºäºè§„åˆ™çš„æ¸¸æˆå¯ä»¥ä½œä¸ºå¯æ§ä¸”å¯æ‰©å±•çš„é¢„æ–‡æœ¬ä»»åŠ¡ï¼Œä»¥è§£é”MLLMä¸­çš„å¯æ¨å¹¿å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08011v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yunfeixie233.github.io/ViGaL/">https://yunfeixie233.github.io/ViGaL/</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼€å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œé¢ä¸´ç€æŒ‘æˆ˜ã€‚å—è®¤çŸ¥ç§‘å­¦æ–‡çŒ®çš„å¯å‘ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰çš„æ–°å‹è®­ç»ƒåèŒƒå¼ï¼Œé€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆï¼Œä¿ƒè¿›MLLMså‘å±•è·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¯¹æ‹¥æœ‰7Bå‚æ•°çš„MLLMè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¡—æœºæ¸¸æˆè®­ç»ƒåï¼Œå…¶åœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVistaï¼‰å’Œå¤šå­¦ç§‘é—®é¢˜ï¼ˆå¦‚MMMUï¼‰ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨RLè¿‡ç¨‹ä¸­å¹¶æœªæ¥è§¦ä»»ä½•è§£é¢˜è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ï¼Œè¿™è¡¨æ˜æ¨¡å‹æŒæ¡äº†å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºä¸“é—¨è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†åŸºç¡€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜åŸºäºè§„åˆ™çš„æ¸¸æˆå¯ä»¥ä½œä¸ºå¯æ§ä¸”å¯æ‰©å±•çš„é¢„æ–‡æœ¬ä»»åŠ¡ï¼Œè§£é”MLLMsä¸­çš„é€šç”¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¤çŸ¥ç§‘å­¦æ–‡çŒ®è¡¨æ˜æ¸¸æˆå¯ä»¥ä¿ƒè¿›å¯è¿ç§»çš„è®¤çŸ¥æŠ€èƒ½å‘å±•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è®­ç»ƒåèŒƒå¼â€”â€”è§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ï¼Œé€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆï¼Œä¿ƒè¿›MLLMså‘å±•è·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¯ä»¥æå‡MLLMsåœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•å’Œå¤šå­¦ç§‘é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹åœ¨æŒæ¡æ¨ç†æŠ€èƒ½çš„è¿‡ç¨‹ä¸­ï¼Œå¹¶æœªæ¥è§¦è§£é¢˜è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºä¸“é—¨è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä¿æŒäº†åŸºç¡€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œè§£å†³äº†ä¸“å®¶æ¨¡å‹å¸¸é¢ä¸´çš„éš¾é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f6f2d9ca54356dd054737efcf1d47d6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac153d76d43fe736753dec1f0695097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c32a2e629cb32b21e18efcf432294d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Supporting-Construction-Worker-Well-Being-with-a-Multi-Agent-Conversational-AI-System"><a href="#Supporting-Construction-Worker-Well-Being-with-a-Multi-Agent-Conversational-AI-System" class="headerlink" title="Supporting Construction Worker Well-Being with a Multi-Agent   Conversational AI System"></a>Supporting Construction Worker Well-Being with a Multi-Agent   Conversational AI System</h2><p><strong>Authors:Fan Yang, Yuan Tian, Jiansong Zhang</strong></p>
<p>The construction industry is characterized by both high physical and psychological risks, yet supports of mental health remain limited. While advancements in artificial intelligence (AI), particularly large language models (LLMs), offer promising solutions, their potential in construction remains largely underexplored. To bridge this gap, we developed a conversational multi-agent system that addresses industry-specific challenges through an AI-driven approach integrated with domain knowledge. In parallel, it fulfills construction workersâ€™ basic psychological needs by enabling interactions with multiple agents, each has a distinct persona. This approach ensures that workers receive both practical problem-solving support and social engagement, ultimately contributing to their overall well-being. We evaluate its usability and effectiveness through a within-subjects user study with 12 participants. The results show that our system significantly outperforms the single-agent baseline, achieving improvements of 18% in usability, 40% in self-determination, 60% in social presence, and 60% in trust. These findings highlight the promise of LLM-driven AI systems in providing domain-specific support for construction workers. </p>
<blockquote>
<p>å»ºç­‘ä¸šçš„ç‰¹ç‚¹æ˜¯æ—¢å­˜åœ¨é«˜ç‰©ç†é£é™©åˆå­˜åœ¨å¿ƒç†é£é™©ï¼Œä½†ç²¾ç¥å¥åº·æ”¯æŒä»ç„¶æœ‰é™ã€‚è™½ç„¶äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å–å¾—äº†è¿›å±•ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬åœ¨å»ºç­‘ä¸šä¸­çš„æ½œåŠ›ä»ç„¶è¢«å¤§å¤§ä½ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯¹è¯å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡é›†æˆäººå·¥æ™ºèƒ½å’Œé¢†åŸŸçŸ¥è¯†çš„æ–¹æ³•æ¥è§£å†³è¡Œä¸šç‰¹å®šçš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œå®ƒé€šè¿‡ä½¿å·¥äººèƒ½å¤Ÿä¸å¤šä¸ªæ™ºèƒ½ä½“äº’åŠ¨ï¼Œæ»¡è¶³æ–½å·¥å·¥äººçš„åŸºæœ¬å¿ƒç†éœ€æ±‚ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“éƒ½å…·æœ‰ç‹¬ç‰¹çš„ä¸ªæ€§ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿å·¥äººä¸ä»…è·å¾—å®é™…çš„è§£å†³é—®é¢˜çš„æ”¯æŒï¼Œè¿˜è·å¾—ç¤¾äº¤å‚ä¸ï¼Œæœ€ç»ˆä¸ºä»–ä»¬çš„æ•´ä½“ç¦ç¥‰åšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬é€šè¿‡ä¸€é¡¹æœ‰12åå‚ä¸è€…å†…éƒ¨ç”¨æˆ·ç ”ç©¶æ¥è¯„ä¼°å…¶å¯ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ˜¾è‘—ä¼˜äºå•æ™ºèƒ½ä½“åŸºçº¿ï¼Œåœ¨å¯ç”¨æ€§æ–¹é¢æé«˜äº†18%ï¼Œåœ¨è‡ªä¸»æ€§æ–¹é¢æé«˜äº†40%ï¼Œåœ¨ç¤¾ä¼šå­˜åœ¨æ„Ÿæ–¹é¢æé«˜äº†60%ï¼Œåœ¨ä¿¡ä»»åº¦æ–¹é¢æé«˜äº†60%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LLMé©±åŠ¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨ä¸ºå»ºç­‘å·¥äººæä¾›ç‰¹å®šé¢†åŸŸçš„æ”¯æŒæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07997v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–½å·¥è¡Œä¸šå­˜åœ¨é«˜é£é™©ï¼ŒåŒ…æ‹¬èº«ä½“å’Œå¿ƒç†é£é™©ï¼Œä½†å¿ƒç†å¥åº·æ”¯æŒä»ç„¶æœ‰é™ã€‚äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†äº†è§£ã€‚å¼€å‘äº†ä¸€ç§åŸºäºAIçš„å¤šä»£ç†ç³»ç»Ÿï¼Œç»“åˆè¡Œä¸šçŸ¥è¯†åº”å¯¹è¡Œä¸šæŒ‘æˆ˜å¹¶å®ç°åŸºæœ¬å¿ƒç†éœ€æ±‚ï¼Œå®ç°æ–½å·¥å·¥äººçš„é—®é¢˜è§£å—å¹¶æå‡å…¶å¹¸ç¦æ„Ÿã€‚è¯„ä¼°è¡¨æ˜ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„å¯ç”¨æ€§å¹¶èƒ½æ˜¾è‘—æé«˜å·¥ä½œæ•ˆç‡å’Œç”¨æˆ·ä¿¡ä»»åº¦ã€‚è¿™äº›å‘ç°çªå‡ºäº†LLMåœ¨æ”¯æŒå»ºç­‘è¡Œä¸šä»ä¸šè€…æ–¹é¢æ½œåŠ›å·¨å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–½å·¥è¡Œä¸šå­˜åœ¨é«˜ç‰©ç†å’Œå¿ƒç†é£é™©ï¼Œä½†å¿ƒç†å¥åº·æ”¯æŒæœ‰é™ã€‚</li>
<li>AIå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè¯¥è¡Œä¸šæä¾›æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åŸºäºAIçš„å¤šä»£ç†ç³»ç»Ÿï¼Œæ•´åˆè¡Œä¸šçŸ¥è¯†è§£å†³ç‰¹å®šæŒ‘æˆ˜ã€‚</li>
<li>ç³»ç»Ÿæ”¯æŒæ–½å·¥å·¥äººçš„åŸºæœ¬å¿ƒç†éœ€æ±‚å¹¶å®ç°ç¤¾ä¼šäº¤å¾€ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡ç”¨æˆ·ç ”ç©¶è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶æ˜¾è‘—æé«˜äº†å¯ç”¨æ€§å’Œç”¨æˆ·ä¿¡ä»»åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9cee9bb71bc64a5545ec202509639fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01197be0beef9f6fadc344369dbeffd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71e142c8b5552441f6ff5ec8c5aa3df0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><a href="#HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization" class="headerlink" title="HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in   Combinatorial Optimization"></a>HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in   Combinatorial Optimization</h2><p><strong>Authors:Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</strong></p>
<p>While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’ŒåŸºäºä»£ç†çš„é—®é¢˜è§£å†³æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ–¹æ³•æ— æ³•å……åˆ†è¯„ä¼°å®ƒä»¬çš„èƒ½åŠ›ï¼šç°æœ‰çš„åŸºå‡†æµ‹è¯•è¦ä¹ˆä¾èµ–äºå°é—­å¼é—®é¢˜ï¼Œå®¹æ˜“é¥±å’Œå’Œè®°å¿†ï¼Œè¦ä¹ˆç¼ºä¹ä¸€è‡´æ€§å’Œä¸¥è°¨æ€§çš„ä¸»è§‚æ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HeuriGymï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºLLMç”Ÿæˆçš„å¯å‘å¼ç®—æ³•è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜è€Œè®¾è®¡çš„ä»£ç†æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹æ˜¯ç›®æ ‡æ˜ç¡®ã€è§£å†³æ–¹æ¡ˆç©ºé—´å¹¿é˜”ã€‚HeuriGymä½¿LLMèƒ½å¤Ÿæå‡ºå¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡ä»£ç æ‰§è¡Œæ¥æ”¶è¯„ä¼°åé¦ˆï¼Œå¹¶è¿­ä»£ä¼˜åŒ–å…¶è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨è®¡ç®—æœºç³»ç»Ÿã€ç‰©æµå’Œç”Ÿç‰©å­¦ç­‰é¢†åŸŸçš„ä¹ä¸ªé—®é¢˜ä¸Šè¯„ä¼°äº†ä¹ä¸ªæœ€å…ˆè¿›æ¨¡å‹ï¼Œæš´éœ²äº†å·¥å…·ä½¿ç”¨ã€è§„åˆ’å’Œè‡ªé€‚åº”æ¨ç†æ–¹é¢çš„æŒç»­å±€é™æ€§ã€‚ä¸ºäº†é‡åŒ–æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†è´¨é‡æ”¶ç›ŠæŒ‡æ•°ï¼ˆQYIï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¢èƒ½åæ˜ è§£å†³æ–¹æ¡ˆé€šè¿‡ç‡åˆèƒ½åæ˜ è§£å†³æ–¹æ¡ˆè´¨é‡çš„æŒ‡æ ‡ã€‚å³ä½¿æ˜¯é¡¶å°–æ¨¡å‹ï¼Œå¦‚GPT-o4-mini-highå’ŒGemini-2.5-Proï¼Œå…¶QYIå¾—åˆ†ä¹Ÿåªæœ‰0.6ï¼Œè¿œä½äºä¸“å®¶åŸºå‡†çº¿1ã€‚æˆ‘ä»¬çš„å¼€æºåŸºå‡†æµ‹è¯•æ—¨åœ¨å¼•å¯¼LLMçš„å‘å±•ï¼Œä»¥å®ç°ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸæ›´æœ‰æ•ˆã€æ›´å®é™…çš„é—®é¢˜è§£å†³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07972v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’ŒåŸºäºä»£ç†çš„é—®é¢˜è§£å†³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°æ–¹æ³•æ— æ³•å……åˆ†è¯„ä¼°å…¶èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHeuriGymçš„ä»£ç†æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMç”Ÿæˆçš„ç»„åˆä¼˜åŒ–é—®é¢˜çš„å¯å‘å¼ç®—æ³•ã€‚è¯¥æ¡†æ¶æ˜ç¡®äº†ç›®æ ‡å¹¶æ‰©å¤§äº†è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä½¿LLMèƒ½å¤Ÿæå‡ºå¯å‘å¼ç­–ç•¥ã€é€šè¿‡ä»£ç æ‰§è¡Œæ¥æ”¶è¯„ä¼°åé¦ˆå¹¶è¿­ä»£ä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚åœ¨è·¨è®¡ç®—æœºã€ç‰©æµå’Œç”Ÿç‰©å­¦ç­‰é¢†åŸŸçš„ä¹ä¸ªé—®é¢˜ä¸Šå¯¹ä¹ç§æœ€æ–°æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæš´éœ²å‡ºå·¥å…·ä½¿ç”¨ã€è§„åˆ’å’Œè‡ªé€‚åº”æ¨ç†æ–¹é¢çš„æŒç»­å±€é™æ€§ã€‚ä¸ºäº†é‡åŒ–æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†è´¨é‡æ”¶ç›ŠæŒ‡æ•°ï¼ˆQYIï¼‰ï¼Œè¯¥æŒ‡æ•°åŒæ—¶æ•æ‰è§£å†³æ–¹æ¡ˆçš„é€šè¿‡ç‡å’Œè´¨é‡ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-o4-mini-highå’ŒGemini-2.5-Proçš„QYIå¾—åˆ†ä¹Ÿåªæœ‰0.6ï¼Œè¿œä½äºä¸“å®¶åŸºå‡†çº¿1ã€‚æˆ‘ä»¬çš„å¼€æºåŸºå‡†æµ‹è¯•æ—¨åœ¨å¼•å¯¼LLMçš„å‘å±•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è§£å†³ç§‘å­¦å’Œå·¥ç¨‹é¢†åŸŸçš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ¨ç†å’ŒåŸºäºä»£ç†çš„é—®é¢˜è§£å†³ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°æ–¹æ³•å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ ‡å‡†éš¾ä»¥å‡†ç¡®è¡¡é‡LLMçš„èƒ½åŠ›ï¼Œéœ€å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>HeuriGymæ¡†æ¶è¢«è®¾è®¡ç”¨äºè¯„ä¼°LLMåœ¨ç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šçš„å¯å‘å¼ç®—æ³•æ€§èƒ½ã€‚</li>
<li>HeuriGymæœ‰æ˜ç¡®çš„ç›®æ ‡å’Œå¹¿æ³›çš„è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œæ”¯æŒLLMæå‡ºå¯å‘å¼ç­–ç•¥å¹¶è·å¾—åé¦ˆã€‚</li>
<li>åœ¨å¤šä¸ªé¢†åŸŸçš„é—®é¢˜è¯„ä¼°ä¸­ï¼ŒLLMæ˜¾ç¤ºå‡ºå·¥å…·ä½¿ç”¨ã€è§„åˆ’å’Œè‡ªé€‚åº”æ¨ç†çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºè´¨é‡æ”¶ç›ŠæŒ‡æ•°ï¼ˆQYIï¼‰ä½œä¸ºè¯„ä¼°LLMæ€§èƒ½çš„é‡åŒ–æŒ‡æ ‡ï¼Œæ¶µç›–è§£å†³æ–¹æ¡ˆçš„é€šè¿‡ç‡å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-884141648c7c519cdee0233b1c569f84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2a16d85f5e842a5cc2bbdc43835d6e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fcbc6abe54f0a907ff838c11dd8d6c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb98a06758f560a6550c2d102f31d762.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CyberV-Cybernetics-for-Test-time-Scaling-in-Video-Understanding"><a href="#CyberV-Cybernetics-for-Test-time-Scaling-in-Video-Understanding" class="headerlink" title="CyberV: Cybernetics for Test-time Scaling in Video Understanding"></a>CyberV: Cybernetics for Test-time Scaling in Video Understanding</h2><p><strong>Authors:Jiahao Meng, Shuyang Sun, Yue Tan, Lu Qi, Yunhai Tong, Xiangtai Li, Longyin Wen</strong></p>
<p>Current Multimodal Large Language Models (MLLMs) may struggle with understanding long or complex videos due to computational demands at test time, lack of robustness, and limited accuracy, primarily stemming from their feed-forward processing nature. These limitations could be more severe for models with fewer parameters. To address these limitations, we propose a novel framework inspired by cybernetic principles, redesigning video MLLMs as adaptive systems capable of self-monitoring, self-correction, and dynamic resource allocation during inference. Our approach, CyberV, introduces a cybernetic loop consisting of an MLLM Inference System, a Sensor, and a Controller. Specifically, the sensor monitors forward processes of the MLLM and collects intermediate interpretations, such as attention drift, then the controller determines when and how to trigger self-correction and generate feedback to guide the next round. This test-time adaptive scaling framework enhances frozen MLLMs without requiring retraining or additional components. Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0% improvement, achieving performance even comparable to human experts. Furthermore, our method demonstrates consistent gains on general-purpose benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and generalization capabilities in making MLLMs more robust and accurate for dynamic video understanding. The code is released at <a target="_blank" rel="noopener" href="https://github.com/marinero4972/CyberV">https://github.com/marinero4972/CyberV</a>. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºæµ‹è¯•æ—¶çš„è®¡ç®—éœ€æ±‚ã€ç¼ºä¹ç¨³å¥æ€§ä»¥åŠå‡†ç¡®æ€§æœ‰é™ï¼Œå¯èƒ½éš¾ä»¥ç†è§£å’Œå¤„ç†é•¿æˆ–å¤æ‚çš„è§†é¢‘ï¼Œè¿™äº›é™åˆ¶ä¸»è¦æºäºå…¶å‰é¦ˆå¤„ç†æ€§è´¨ã€‚å¯¹äºå‚æ•°è¾ƒå°‘çš„æ¨¡å‹ï¼Œè¿™äº›é™åˆ¶å¯èƒ½æ›´åŠ ä¸¥é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—æ§åˆ¶è®ºå¯å‘çš„æ–°å‹æ¡†æ¶ï¼Œé‡æ–°è®¾è®¡è§†é¢‘MLLMsä¸ºè‡ªé€‚åº”ç³»ç»Ÿï¼Œå…·å¤‡è‡ªæˆ‘ç›‘æ§ã€è‡ªæˆ‘æ ¡æ­£å’ŒåŠ¨æ€èµ„æºåˆ†é…çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•CyberVå¼•å…¥äº†ä¸€ä¸ªæ§åˆ¶å›è·¯ï¼ŒåŒ…æ‹¬MLLMæ¨ç†ç³»ç»Ÿã€ä¼ æ„Ÿå™¨å’Œæ§åˆ¶å™¨ã€‚å…·ä½“è€Œè¨€ï¼Œä¼ æ„Ÿå™¨ç›‘æ§MLLMçš„å‰å‘è¿‡ç¨‹å¹¶æ”¶é›†ä¸­é—´è§£é‡Šï¼Œå¦‚æ³¨æ„åŠ›æ¼‚ç§»ï¼Œç„¶åæ§åˆ¶å™¨ç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•è§¦å‘è‡ªæˆ‘æ ¡æ­£å¹¶ç”Ÿæˆåé¦ˆæ¥æŒ‡å¯¼ä¸‹ä¸€è½®ã€‚è¿™ç§æµ‹è¯•æ—¶çš„è‡ªé€‚åº”ç¼©æ”¾æ¡†æ¶å¢å¼ºäº†å†»ç»“çš„MLLMsçš„æ€§èƒ½ï¼Œè€Œæ— éœ€è¿›è¡Œå†è®­ç»ƒæˆ–æ·»åŠ é¢å¤–ç»„ä»¶ã€‚å®éªŒè¡¨æ˜æœ‰æ˜¾è‘—æ”¹è¿›ï¼šCyberVåœ¨VideoMMMUä¸Šå°†Qwen2.5-VL-7Bæå‡äº†8.3%ï¼Œå°†InternVL3-8Bæå‡äº†5.5%ï¼Œè¶…è¶Šäº†ç«äº‰æ€§çš„ä¸“æœ‰æ¨¡å‹GPT-4oã€‚å½“åº”ç”¨äºQwen2.5-VL-7Bæ—¶ï¼Œå®ƒå®ç°äº†10.0%çš„æ”¹è¿›ï¼Œæ€§èƒ½ç”šè‡³å¯ä¸äººç±»ä¸“å®¶ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨VideoMMEå’ŒWorldSenseç­‰é€šç”¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æ”¶ç›Šï¼Œè¿™çªå‡ºäº†å…¶åœ¨ä½¿MLLMsæ›´å¥å£®å’Œå‡†ç¡®åœ°è¿›è¡ŒåŠ¨æ€è§†é¢‘ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/marinero4972/CyberV%E3%80%82">https://github.com/marinero4972/CyberVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07971v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æˆ–å¤æ‚è§†é¢‘æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è®¡ç®—éœ€æ±‚å¤§ã€ç¨³å¥æ€§ä¸è¶³å’Œå‡†ç¡®æ€§æœ‰é™ç­‰é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¡†æ¶â€”â€”CyberVã€‚è¯¥æ¡†æ¶å€Ÿé‰´æ§åˆ¶è®ºåŸç†ï¼Œå°†è§†é¢‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°è®¾è®¡ä¸ºè‡ªé€‚åº”ç³»ç»Ÿï¼Œå…·å¤‡è‡ªæˆ‘ç›‘æ§ã€è‡ªæˆ‘æ ¡æ­£å’ŒåŠ¨æ€èµ„æºåˆ†é…çš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªåŒ…å«MLLMæ¨ç†ç³»ç»Ÿã€ä¼ æ„Ÿå™¨å’Œæ§åˆ¶å™¨çš„æ§åˆ¶è®ºå¾ªç¯ï¼Œå®ç°äº†æµ‹è¯•æ—¶çš„è‡ªé€‚åº”ç¼©æ”¾ã€‚å®éªŒè¯æ˜ï¼ŒCyberVåœ¨VideoMMMUç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸”å…·å¤‡ä¸€èˆ¬åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºå¤šä¸ªåŸºå‡†æµ‹è¯•é›†ã€‚ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é•¿æˆ–å¤æ‚è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è®¡ç®—éœ€æ±‚å¤§ã€ç¨³å¥æ€§ä¸è¶³å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>è¿™äº›é—®é¢˜ä¸»è¦ç”±äºæ¨¡å‹çš„å‰é¦ˆå¤„ç†æ€§è´¨ä»¥åŠå‚æ•°æ•°é‡å’Œè®¡ç®—èµ„æºçš„é™åˆ¶ã€‚</li>
<li>æå‡ºçš„CyberVæ¡†æ¶å€Ÿé‰´æ§åˆ¶è®ºåŸç†ï¼Œå°†MLLMsé‡æ–°è®¾è®¡ä¸ºè‡ªé€‚åº”ç³»ç»Ÿã€‚</li>
<li>CyberVæ¡†æ¶åŒ…å«MLLMæ¨ç†ç³»ç»Ÿã€ä¼ æ„Ÿå™¨å’Œæ§åˆ¶å™¨ï¼Œèƒ½å¤Ÿå®ç°è‡ªæˆ‘ç›‘æ§ã€è‡ªæˆ‘æ ¡æ­£å’ŒåŠ¨æ€èµ„æºåˆ†é…ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCyberVæ˜¾è‘—æé«˜äº†MLLMsåœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬VideoMMMUç­‰ç‰¹å®šä»»åŠ¡ã€‚</li>
<li>CyberVè¿˜å±•ç¤ºäº†ä¸€è‡´æ€§çš„å¢ç›Šåœ¨ä¸€èˆ¬ç›®çš„çš„åŸºå‡†æµ‹è¯•é›†ä¸Šï¼Œå¦‚VideoMMEå’ŒWorldSenseã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-001bde8efe450cb7543a85b4dcf329ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff3aef706a8648422dcca30a0faa7ae3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ae2462816659144371bbe7fa70e8027.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SpaCE-10-A-Comprehensive-Benchmark-for-Multimodal-Large-Language-Models-in-Compositional-Spatial-Intelligence"><a href="#SpaCE-10-A-Comprehensive-Benchmark-for-Multimodal-Large-Language-Models-in-Compositional-Spatial-Intelligence" class="headerlink" title="SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models   in Compositional Spatial Intelligence"></a>SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models   in Compositional Spatial Intelligence</h2><p><strong>Authors:Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, Rongrong Ji</strong></p>
<p>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in various multimodal tasks. To pursue higher intelligence in space, MLLMs require integrating multiple atomic spatial capabilities to handle complex and dynamic tasks. However, existing benchmarks struggle to comprehensively evaluate the spatial intelligence of common MLLMs from the atomic level to the compositional level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial capabilities, which are combined to form 8 compositional capabilities. Based on these definitions, we propose a novel hierarchical annotation pipeline to generate high-quality and diverse question-answer (QA) pairs. With over 150+ hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor scenes in SpaCE-10, which covers various evaluation settings like point cloud input and multi-choice QA. We conduct an extensive evaluation of common MLLMs on SpaCE-10 and find that even the most advanced MLLM still lags behind humans by large margins. Through our careful study, we also draw several significant findings that benefit the MLLM community. For example, we reveal that the shortcoming of counting capability greatly limits the compositional spatial capabilities of existing MLLMs. The evaluation code and benchmark datasets are available at <a target="_blank" rel="noopener" href="https://github.com/Cuzyoung/SpaCE-10">https://github.com/Cuzyoung/SpaCE-10</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ä¸ºäº†è¿½æ±‚æ›´é«˜çš„ç©ºé—´æ™ºèƒ½ï¼ŒMLLMséœ€è¦æ•´åˆå¤šç§åŸå­ç©ºé—´èƒ½åŠ›æ¥å¤„ç†å¤æ‚å’ŒåŠ¨æ€çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°å¸¸è§MLLMsçš„ç©ºé—´æ™ºèƒ½æ–¹é¢ï¼Œä»åŸå­å±‚é¢åˆ°ç»„åˆå±‚é¢éƒ½å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SpaCE-10ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»„åˆç©ºé—´è¯„ä¼°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚åœ¨SpaCE-10ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†10ç§åŸå­ç©ºé—´èƒ½åŠ›ï¼Œå®ƒä»¬ç›¸ç»“åˆå½¢æˆäº†8ç§ç»„åˆèƒ½åŠ›ã€‚åŸºäºè¿™äº›å®šä¹‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†å±‚æ³¨é‡Šç®¡é“ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„é—®ç­”å¯¹ã€‚ç»è¿‡è¶…è¿‡150å°æ—¶çš„äººåŠ›ä¸“å®¶åŠªåŠ›ï¼Œæˆ‘ä»¬åœ¨SpaCE-10ä¸­è·å¾—äº†è¶…è¿‡5000ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠ811ä¸ªçœŸå®çš„å®¤å†…åœºæ™¯ï¼Œæ¶µç›–äº†å„ç§è¯„ä¼°è®¾ç½®ï¼Œå¦‚ç‚¹äº‘è¾“å…¥å’Œå¤šé¡¹é€‰æ‹©é—®ç­”ã€‚æˆ‘ä»¬å¯¹å¸¸è§çš„MLLMsåœ¨SpaCE-10ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMä»ç„¶è¿œè¿œè½åäºäººç±»ã€‚é€šè¿‡æˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†å‡ ä¸ªå¯¹MLLMç¤¾åŒºæœ‰ç›Šçš„é‡å¤§å‘ç°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ­ç¤ºè®¡æ•°èƒ½åŠ›çš„ä¸è¶³æå¤§åœ°é™åˆ¶äº†ç°æœ‰MLLMsçš„ç»„åˆç©ºé—´èƒ½åŠ›ã€‚è¯„ä¼°ä»£ç å’ŒåŸºå‡†æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Cuzyoung/SpaCE-10">https://github.com/Cuzyoung/SpaCE-10</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07966v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»éœ€æé«˜ç©ºé—´æ™ºèƒ½ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†SpaCE-10åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…å«10é¡¹åŸå­ç©ºé—´èƒ½åŠ›å’Œ8é¡¹ç»„åˆèƒ½åŠ›ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„ç©ºé—´æ™ºèƒ½ã€‚ç ”ç©¶è€…é€šè¿‡æ–°çš„åˆ†å±‚æ³¨é‡Šç®¡é“ç”Ÿæˆäº†é«˜è´¨é‡çš„å¤šæ ·åŒ–é—®ç­”å¯¹ï¼Œå¹¶åœ¨è¶…è¿‡5kä¸ªQAå¯¹ä¸Šå¯¹å¸¸è§MLLMè¿›è¡Œäº†è¯„ä¼°ã€‚å‘ç°æœ€å…ˆè¿›çš„MLLMä»ç„¶è¿œè¿œè½åäºäººç±»ã€‚åŒæ—¶å‘ç°è®¡æ•°èƒ½åŠ›çš„ä¸è¶³æå¤§åœ°é™åˆ¶äº†ç°æœ‰MLLMçš„ç»„åˆç©ºé—´èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»éœ€æé«˜ç©ºé—´æ™ºèƒ½ã€‚</li>
<li>SpaCE-10æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsç©ºé—´æ™ºèƒ½çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«åŸå­å’Œç»„åˆèƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>SpaCE-10é€šè¿‡æ–°çš„åˆ†å±‚æ³¨é‡Šç®¡é“ç”Ÿæˆäº†é«˜è´¨é‡çš„å¤šæ ·åŒ–é—®ç­”å¯¹ã€‚</li>
<li>æœ€å…ˆè¿›çš„MLLMåœ¨SpaCE-10ä¸Šä»ç„¶è¿œè¿œè½åäºäººç±»è¡¨ç°ã€‚</li>
<li>è®¡æ•°èƒ½åŠ›çš„ä¸è¶³æ˜¯é™åˆ¶ç°æœ‰MLLMç»„åˆç©ºé—´èƒ½åŠ›çš„ä¸»è¦å› ç´ ã€‚</li>
<li>SpaCE-10æä¾›äº†ä¸°å¯Œçš„è¯„ä»·è®¾ç½®ï¼Œå¦‚ç‚¹äº‘è¾“å…¥å’Œå¤šé€‰é—®ç­”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffc4d9f8f04bc4211aa6d0cb1370a184.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36617e0f158e60151f4b25dd02778a8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03f18f4b61a208a028a00c703b775c39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f47373894cb7fe52707b661136e3bf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ProtocolLLM-RTL-Benchmark-for-SystemVerilog-Generation-of-Communication-Protocols"><a href="#ProtocolLLM-RTL-Benchmark-for-SystemVerilog-Generation-of-Communication-Protocols" class="headerlink" title="ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication   Protocols"></a>ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication   Protocols</h2><p><strong>Authors:Arnav Sheth, Ivaxi Sheth, Mario Fritz</strong></p>
<p>Recent advances in Large Language Models (LLMs) have shown promising capabilities in generating code for general-purpose programming languages. In contrast, their applicability for hardware description languages, particularly for generating synthesizable and functionally correct designs, remains significantly underexplored. HDLs such as SystemVerilog are logic-oriented and demand strict adherence to timing semantics, concurrency, and synthesizability constraints. Moreover, HDL-based design flows encompass a broad set of tasks beyond structural code generation, including testbench development, assertion-based verification, timing closure, and protocol-level integration for on-chip communication. The objective of our paper is to analyze the capabilities of state-of-the-art LLMs in generating SystemVerilog implementations of standard communication protocols, a core component of embedded and System-on-Chip (SoC) architectures. This paper introduces the first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and AXI. We define code generation tasks that capture varying levels of design abstraction and prompt specificity. The generated designs are assessed for syntactic correctness, synthesizability, and functional fidelity via waveform simulation and test benches. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ˜¾ç¤ºå‡ºåœ¨ç”Ÿæˆé€šç”¨ç¼–ç¨‹è¯­è¨€ä»£ç æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆç‰¹åˆ«æ˜¯ç”Ÿæˆå¯åˆæˆå’ŒåŠŸèƒ½æ€§æ­£ç¡®çš„è®¾è®¡ï¼‰æ–¹é¢çš„é€‚ç”¨æ€§ä»ç„¶è¿œè¿œæœªè¢«å……åˆ†æ¢ç´¢ã€‚SystemVerilogç­‰ç¡¬ä»¶æè¿°è¯­è¨€é¢å‘é€»è¾‘ï¼Œéœ€è¦ä¸¥æ ¼éµå¾ªæ—¶åºè¯­ä¹‰ã€å¹¶å‘æ€§å’Œå¯åˆæˆæ€§çº¦æŸã€‚æ­¤å¤–ï¼ŒåŸºäºHDLçš„è®¾è®¡æµç¨‹æ¶µç›–äº†ç»“æ„ä»£ç ç”Ÿæˆä¹‹å¤–çš„ä¸€ç³»åˆ—ä»»åŠ¡ï¼ŒåŒ…æ‹¬æµ‹è¯•å¹³å°å¼€å‘ã€åŸºäºæ–­è¨€çš„éªŒè¯ã€æ—¶åºé—­åˆå’ŒèŠ¯ç‰‡ä¸Šé€šä¿¡çš„åè®®çº§é›†æˆã€‚æˆ‘ä»¬è®ºæ–‡çš„ç›®æ ‡æ˜¯åˆ†åˆ†ææœ€æ–°LLMåœ¨ç”ŸæˆSystemVerilogå®ç°æ ‡å‡†é€šä¿¡åè®®æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™æ˜¯åµŒå…¥å¼å’Œç‰‡ä¸Šç³»ç»Ÿï¼ˆSoCï¼‰æ¶æ„çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å››ä¸ªå¹¿æ³›ä½¿ç”¨åè®®çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼šSPIã€I2Cã€UARTå’ŒAXIã€‚æˆ‘ä»¬å®šä¹‰äº†æ•æ‰ä¸åŒè®¾è®¡æŠ½è±¡å±‚æ¬¡å’Œç‰¹å®šæç¤ºçš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡æ³¢å½¢ä»¿çœŸå’Œæµ‹è¯•å¹³å°è¯„ä¼°æ‰€ç”Ÿæˆè®¾è®¡çš„è¯­æ³•æ­£ç¡®æ€§ã€å¯åˆæˆæ€§å’ŒåŠŸèƒ½ä¿çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07945v1">PDF</a> Accepted at MLSysArch@ISCA 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé€šç”¨ç¼–ç¨‹è¯­è¨€ä»£ç æ–¹é¢å±•ç°å‡ºè‰¯å¥½èƒ½åŠ›ï¼Œä½†åœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¯åˆæˆå’ŒåŠŸèƒ½æ€§æ­£ç¡®çš„è®¾è®¡æ–¹é¢ï¼Œå…¶åº”ç”¨ä»æ˜¾è‘—ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨åˆ†ææœ€æ–°LLMåœ¨ç”ŸæˆSystemVerilogå®ç°çš„é€šä¿¡åè®®æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å¼•å…¥é’ˆå¯¹SPIã€I2Cã€UARTå’ŒAXIå››ç§å¹¿æ³›ä½¿ç”¨çš„åè®®çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚é€šè¿‡æ³¢å½¢ä»¿çœŸå’Œæµ‹è¯•å¹³å°ï¼Œè¯„ä¼°ç”Ÿæˆçš„è®¾è®¡çš„è¯­æ³•æ­£ç¡®æ€§ã€å¯åˆæˆæ€§å’ŒåŠŸèƒ½ä¿çœŸæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨ç¡¬ä»¶æè¿°è¯­è¨€ï¼ˆHDLï¼‰å¦‚SystemVerilogçš„ä»£ç ç”Ÿæˆæ–¹é¢åº”ç”¨è¾ƒå°‘ã€‚</li>
<li>HDLséœ€è¦ä¸¥æ ¼éµå¾ªæ—¶åºè¯­ä¹‰ã€å¹¶å‘æ€§å’Œå¯åˆæˆæ€§çº¦æŸã€‚</li>
<li>LLMç”ŸæˆSystemVerilogå®ç°çš„é€šä¿¡åè®®æ˜¯æœ¬æ–‡çš„é‡ç‚¹ã€‚</li>
<li>å¼•å…¥é’ˆå¯¹SPIã€I2Cã€UARTå’ŒAXIå››ç§é€šä¿¡åè®®çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚</li>
<li>è¯„ä¼°ç”Ÿæˆçš„è®¾è®¡çš„è¯­æ³•æ­£ç¡®æ€§ã€å¯åˆæˆæ€§å’ŒåŠŸèƒ½ä¿çœŸæ€§ã€‚</li>
<li>LLMåœ¨ç”Ÿæˆä¸åŒæŠ½è±¡å±‚æ¬¡å’Œå…·ä½“æ€§çš„è®¾è®¡ä»£ç æ–¹é¢æœ‰å¾…è¿›ä¸€æ­¥æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a794b3851556fab3d82451b22eb2e971.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efe25cb6c8d28f3586ec1f94920244ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3438fe8fac7b784783a9e62af7bbc441.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7713fd5b286a50c4f27cb5744d840e1d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Decoupling-the-Image-Perception-and-Multimodal-Reasoning-for-Reasoning-Segmentation-with-Digital-Twin-Representations"><a href="#Decoupling-the-Image-Perception-and-Multimodal-Reasoning-for-Reasoning-Segmentation-with-Digital-Twin-Representations" class="headerlink" title="Decoupling the Image Perception and Multimodal Reasoning for Reasoning   Segmentation with Digital Twin Representations"></a>Decoupling the Image Perception and Multimodal Reasoning for Reasoning   Segmentation with Digital Twin Representations</h2><p><strong>Authors:Yizhen Li, Dell Zhang, Xuelong Li, Yiqing Shen</strong></p>
<p>Reasoning Segmentation (RS) is a multimodal vision-text task that requires segmenting objects based on implicit text queries, demanding both precise visual perception and vision-text reasoning capabilities. Current RS approaches rely on fine-tuning vision-language models (VLMs) for both perception and reasoning, but their tokenization of images fundamentally disrupts continuous spatial relationships between objects. We introduce DTwinSeger, a novel RS approach that leverages Digital Twin (DT) representation as an intermediate layer to decouple perception from reasoning. Innovatively, DTwinSeger reformulates RS as a two-stage process, where the first transforms the image into a structured DT representation that preserves spatial relationships and semantic properties and then employs a Large Language Model (LLM) to perform explicit reasoning over this representation to identify target objects. We propose a supervised fine-tuning method specifically for LLM with DT representation, together with a corresponding fine-tuning dataset Seg-DT, to enhance the LLMâ€™s reasoning capabilities with DT representations. Experiments show that our method can achieve state-of-the-art performance on two image RS benchmarks and three image referring segmentation benchmarks. It yields that DT representation functions as an effective bridge between vision and text, enabling complex multimodal reasoning tasks to be accomplished solely with an LLM. </p>
<blockquote>
<p>æ¨ç†åˆ†å‰²ï¼ˆRSï¼‰æ˜¯ä¸€é¡¹å¤šæ¨¡æ€è§†è§‰æ–‡æœ¬ä»»åŠ¡ï¼Œéœ€è¦åŸºäºéšå¼æ–‡æœ¬æŸ¥è¯¢å¯¹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œè¦æ±‚å…·å¤‡ç²¾ç¡®çš„è§†è§‰æ„ŸçŸ¥å’Œè§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚å½“å‰çš„RSæ–¹æ³•ä¾èµ–äºå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥è¿›è¡Œæ„ŸçŸ¥å’Œæ¨ç†ï¼Œä½†å®ƒä»¬çš„å›¾åƒæ ‡è®°æ–¹å¼ä¼šç ´åå¯¹è±¡ä¹‹é—´è¿ç»­çš„æ—¶ç©ºå…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†DTwinSegerï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„RSæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºä½œä¸ºä¸­é—´å±‚ï¼Œå°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦ã€‚DTwinSegeråˆ›æ–°åœ°å°†RSé‡æ„ä¸ºä¸€ä¸ªä¸¤é˜¶æ®µçš„è¿‡ç¨‹ï¼Œå…¶ä¸­ç¬¬ä¸€é˜¶æ®µå°†å›¾åƒè½¬æ¢ä¸ºç»“æ„åŒ–çš„DTè¡¨ç¤ºï¼Œä¿ç•™æ—¶ç©ºå…³ç³»å’Œè¯­ä¹‰å±æ€§ï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ­¤è¡¨ç¤ºè¿›è¡Œæ˜ç¡®çš„æ¨ç†ï¼Œä»¥è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹LLMçš„å¸¦æœ‰DTè¡¨ç¤ºçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œä»¥åŠç›¸åº”çš„å¾®è°ƒæ•°æ®é›†Seg-DTï¼Œä»¥å¢å¼ºLLMå¯¹DTè¡¨ç¤ºçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤é¡¹å›¾åƒRSåŸºå‡†æµ‹è¯•å’Œä¸‰é¡¹å›¾åƒå¼•ç”¨åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜DTè¡¨ç¤ºä½œä¸ºè§†è§‰å’Œæ–‡æœ¬ä¹‹é—´çš„æœ‰æ•ˆæ¡¥æ¢ï¼Œèƒ½å¤Ÿä»…ä½¿ç”¨LLMå®Œæˆå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07943v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RSï¼ˆReasoning Segmentationï¼‰æ˜¯ä¸€é¡¹å¤šæ¨¡æ€è§†è§‰æ–‡æœ¬ä»»åŠ¡ï¼Œéœ€è¦åŸºäºéšå¼æ–‡æœ¬æŸ¥è¯¢å¯¹ç‰©ä½“è¿›è¡Œåˆ†å‰²ï¼Œè¦æ±‚å…·å¤‡ç²¾ç¡®è§†è§‰æ„ŸçŸ¥å’Œè§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰RSæ–¹æ³•é€šè¿‡å¾®è°ƒè·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®ç°æ„ŸçŸ¥å’Œæ¨ç†ï¼Œä½†å…¶å¯¹å›¾åƒçš„ç¬¦å·åŒ–ä¼šç ´åç‰©ä½“é—´çš„è¿ç»­ç©ºé—´å…³ç³»ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹RSæ–¹æ³•DTwinSegerï¼Œåˆ©ç”¨æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºä½œä¸ºä¸­é—´å±‚ï¼Œå®ç°æ„ŸçŸ¥ä¸æ¨ç†çš„è§£è€¦ã€‚DTwinSegerå°†RSé‡æ–°æ„å»ºä¸ºä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µå°†å›¾åƒè½¬æ¢ä¸ºç»“æ„åŒ–çš„DTè¡¨ç¤ºï¼Œä¿ç•™ç©ºé—´å…³ç³»å’Œè¯­ä¹‰å±æ€§ï¼›ç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤è¡¨ç¤ºä¸Šè¿›è¡Œæ˜¾å¼æ¨ç†ï¼Œä»¥è¯†åˆ«ç›®æ ‡ç‰©ä½“ã€‚æœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹LLMçš„åŸºäºDTè¡¨ç¤ºçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„å¾®è°ƒæ•°æ®é›†Seg-DTï¼Œä»¥å¢å¼ºLLMåœ¨DTè¡¨ç¤ºæ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤é¡¹RSåŸºå‡†æµ‹è¯•å’Œä¸‰é¡¹å›¾åƒå¼•ç”¨åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿™è¡¨æ˜DTè¡¨ç¤ºæ˜¯è¿æ¥è§†è§‰å’Œæ–‡æœ¬çš„æœ‰æ•ˆæ¡¥æ¢ï¼Œèƒ½å¤Ÿä»…å‡­LLMå®Œæˆå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RSä»»åŠ¡éœ€è¦åŸºäºéšå¼æ–‡æœ¬æŸ¥è¯¢è¿›è¡Œç‰©ä½“åˆ†å‰²ï¼Œè¦æ±‚è§†è§‰æ„ŸçŸ¥å’Œè§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›å…¼å¤‡ã€‚</li>
<li>å½“å‰RSæ–¹æ³•é€šè¿‡å¾®è°ƒè·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å®ç°æ„ŸçŸ¥å’Œæ¨ç†ï¼Œä½†å­˜åœ¨å›¾åƒç¬¦å·åŒ–å¯¼è‡´çš„ç‰©ä½“ç©ºé—´å…³ç³»ç ´åé—®é¢˜ã€‚</li>
<li>DTwinSegeræ–¹æ³•åˆ©ç”¨æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºä½œä¸ºä¸­é—´å±‚æ¥è§£è€¦æ„ŸçŸ¥ä¸æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>DTwinSegerå°†RSä»»åŠ¡åˆ’åˆ†ä¸ºä¸¤é˜¶æ®µï¼šå›¾åƒè½¬æ¢è‡³ç»“æ„åŒ–DTè¡¨ç¤ºï¼Œä»¥åŠåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ˜¾å¼æ¨ç†ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹LLMçš„åŸºäºDTè¡¨ç¤ºçš„ç›‘ç£å¾®è°ƒæ–¹æ³•å’Œç›¸åº”çš„å¾®è°ƒæ•°æ®é›†Seg-DTã€‚</li>
<li>å®éªŒè¯æ˜DTwinSegeræ–¹æ³•åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-210e8649a1e65f2d54306c2ba4db24d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7283a9c967630d5c583afbea9ea1b498.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cd91779976a2206cd3d53b8387acaff.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Solving-Inequality-Proofs-with-Large-Language-Models"><a href="#Solving-Inequality-Proofs-with-Large-Language-Models" class="headerlink" title="Solving Inequality Proofs with Large Language Models"></a>Solving Inequality Proofs with Large Language Models</h2><p><strong>Authors:Jiayi Sheng, Luna Lyu, Jikai Jin, Tony Xia, Alex Gu, James Zou, Pan Lu</strong></p>
<p>Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at <a target="_blank" rel="noopener" href="https://ineqmath.github.io/">https://ineqmath.github.io/</a>. </p>
<blockquote>
<p>åœ¨æ¨ªè·¨å¤šä¸ªç§‘å­¦å’Œæ•°å­¦é¢†åŸŸä¸­ï¼Œè¯æ˜ä¸ç­‰å¼å¯¹äºé«˜é˜¶æ¨ç†èƒ½åŠ›æœ‰ç€é‡è¦è¦æ±‚ï¼Œä¾‹å¦‚å‘ç°ç´§å¯†ç•Œé™å’Œç­–ç•¥æ€§åœ°åº”ç”¨å®šç†ã€‚è¿™ä½¿å¾—å®ƒæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªç‹¬ç‰¹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸï¼Œä¸ºä¸€èˆ¬æ•°å­¦é—®é¢˜è§£å†³æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚ç°æœ‰æ•°æ®é›†å¾€å¾€ç¨€ç¼ºã€åˆæˆæˆ–è¿‡äºå½¢å¼åŒ–ï¼Œé˜»ç¢äº†è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§éæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡åˆ¶å®šæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°†ä¸ç­‰å¼è¯æ˜é‡æ–°å®šä½ä¸ºä¸¤ä¸ªå¯è‡ªåŠ¨æ£€æŸ¥çš„ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‘å¸ƒäº†IneqMathæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“å®¶ç­–åˆ’çš„å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ä¸ç­‰å¼æ•°æ®é›†ï¼ŒåŒ…æ‹¬æµ‹è¯•é›†å’Œè®­ç»ƒè¯­æ–™åº“ï¼Œå…¶ä¸­å¯Œå«é€æ­¥è§£å†³æ–¹æ¡ˆå’Œå®šç†æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§æ–°å‹LLMè¯„ä¼°æ¡†æ¶ï¼ˆLLM-as-judgeï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æœ€ç»ˆç­”æ¡ˆåˆ¤æ–­å™¨å’Œå››ä¸ªé€æ­¥åˆ¤æ–­å™¨ï¼Œæ—¨åœ¨æ£€æµ‹å¸¸è§çš„æ¨ç†ç¼ºé™·ã€‚å¯¹IneqMathä¸Š29ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„äº‹å®ï¼šå³ä½¿åœ¨é€æ­¥å®¡æŸ¥ä¸‹ï¼Œå³ä½¿æ˜¯é¡¶çº§æ¨¡å‹å¦‚o1ä¹Ÿä»…è¾¾åˆ°ä¸åˆ°ç™¾åˆ†ä¹‹åçš„æ€»ä½“å‡†ç¡®ç‡ï¼›è¿™ä¸ä»…è€ƒè™‘æœ€ç»ˆç­”æ¡ˆç­‰ä»·çš„å‡†ç¡®ç‡ç›¸æ¯”ä¸‹é™äº†é«˜è¾¾65.5%ã€‚è¿™ç§å·®å¼‚æš´éœ²äº†è„†å¼±çš„æ¼”ç»é“¾ä»¥åŠå½“å‰LLMåœ¨æ‰¾åˆ°ç­”æ¡ˆå’Œæ„å»ºä¸¥æ ¼è¯æ˜ä¹‹é—´çš„å…³é”®å·®è·ã€‚æ‰©å¤§æ¨¡å‹è§„æ¨¡å¹¶å¢åŠ æµ‹è¯•æ—¶é—´çš„è®¡ç®—æ‰€å¸¦æ¥çš„æ•´ä½“è¯æ˜æ­£ç¡®æ€§çš„æ”¶ç›Šæœ‰é™ã€‚ç›¸åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ï¼Œå¦‚å®šç†å¼•å¯¼æ¨ç†å’Œè‡ªæˆ‘å®Œå–„ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://ineqmath.github.io/%E8%8E%B7%E5%BE%97%E3%80%82">https://ineqmath.github.io/è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07927v1">PDF</a> 52 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸å¹³ç­‰è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦å„é¢†åŸŸçš„é‡è¦æ€§ï¼Œå®ƒæµ‹è¯•äº†é«˜çº§æ¨ç†æŠ€èƒ½ï¼Œå¦‚å¯»æ‰¾ç´§å¯†ç•Œé™å’Œæˆ˜ç•¥å®šç†åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†å¸¸å¸¸ç¨€ç¼ºã€åˆæˆæˆ–è¿‡äºå½¢å¼åŒ–ï¼Œé˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§éæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡åˆ¶å®šæ–¹å¼ï¼Œå°†ä¸å¹³ç­‰è¯æ˜è½¬åŒ–ä¸ºä¸¤ä¸ªå¯è‡ªåŠ¨æ£€æŸ¥çš„ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å‘å¸ƒäº†ç”±ä¸“å®¶ç­–åˆ’çš„åŒ…å«å¥¥èµ›çº§ä¸ç­‰å¼çš„IneqMathæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†é€æ­¥è§£å†³æ–¹æ¡ˆå’Œå®šç†æ³¨é‡Šã€‚æ–‡ç« è¿˜å¼€å‘äº†ä¸€ç§æ–°å‹çš„LLMè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆæœ€ç»ˆç­”æ¡ˆè¯„åˆ¤å’Œå››ä¸ªé€æ­¥è¯„åˆ¤ï¼Œæ—¨åœ¨æ£€æµ‹å¸¸è§çš„æ¨ç†é”™è¯¯ã€‚å¯¹29æ¬¾é¢†å…ˆçš„LLMè¿›è¡Œçš„ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨é€æ­¥å®¡æŸ¥ä¸‹ï¼Œé¡¶çº§æ¨¡å‹å¦‚o1çš„æ•´ä½“å‡†ç¡®ç‡ä¹Ÿä¸è¶³10%ï¼Œè¿™ä¸€æ•°å­—è¾ƒä»…è€ƒè™‘æœ€ç»ˆç­”æ¡ˆç­‰ä»·æ—¶çš„å‡†ç¡®ç‡ä¸‹é™äº†é«˜è¾¾65.5%ã€‚è¿™è¡¨æ˜å½“å‰LLMåœ¨æ‰¾åˆ°ç­”æ¡ˆå’Œæ„å»ºä¸¥è°¨è¯æ˜ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å·®è·ã€‚æ¨¡å‹è§„æ¨¡å’Œæµ‹è¯•æ—¶é—´è®¡ç®—çš„å¢åŠ å¯¹æé«˜æ•´ä½“è¯æ˜çš„æ­£ç¡®æ€§è´¡çŒ®æœ‰é™ï¼Œåè€Œçªå‡ºäº†å®šç†å¼•å¯¼æ¨ç†å’Œè‡ªæˆ‘å®Œå–„ç­‰å…·æœ‰æ½œåŠ›çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸å¹³ç­‰è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œæµ‹è¯•é«˜çº§æ¨ç†æŠ€èƒ½ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨ç¨€ç¼ºã€åˆæˆæˆ–è¿‡äºå½¢å¼åŒ–çš„é—®é¢˜ï¼Œå½±å“äº†ä¸å¹³ç­‰è¯æ˜é¢†åŸŸçš„è¿›å±•ã€‚</li>
<li>æå‡ºä¸€ç§éæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡åˆ¶å®šæ–¹å¼ï¼Œå°†ä¸å¹³ç­‰è¯æ˜è½¬åŒ–ä¸ºç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ä¸¤ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>å‘å¸ƒäº†IneqMathæ•°æ®é›†ï¼ŒåŒ…å«å¥¥èµ›çº§ä¸ç­‰å¼ã€é€æ­¥è§£å†³æ–¹æ¡ˆå’Œå®šç†æ³¨é‡Šã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹çš„LLMè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆæœ€ç»ˆç­”æ¡ˆè¯„åˆ¤å’Œé€æ­¥è¯„åˆ¤ï¼Œä»¥æ£€æµ‹æ¨ç†é”™è¯¯ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºé¡¶çº§LLMåœ¨é€æ­¥å®¡æŸ¥ä¸‹çš„æ•´ä½“å‡†ç¡®ç‡è¾ƒä½ï¼Œè¡¨æ˜åœ¨æ„å»ºä¸¥è°¨è¯æ˜æ–¹é¢å­˜åœ¨å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24d6807ad9a7a27be26eabf18149a963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347a41acf9860aecd7604ac85491658b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c84462cb1c208b8cc381d7c4d71a3bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9444336e97b3eac6081cebe0c35264e0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MiniCPM4-Ultra-Efficient-LLMs-on-End-Devices"><a href="#MiniCPM4-Ultra-Efficient-LLMs-on-End-Devices" class="headerlink" title="MiniCPM4: Ultra-Efficient LLMs on End Devices"></a>MiniCPM4: Ultra-Efficient LLMs on End Devices</h2><p><strong>Authors: MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun</strong></p>
<p>This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MiniCPM4ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸ºç»ˆç«¯ä¾§è®¾å¤‡è®¾è®¡çš„é«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªå…³é”®é¢†åŸŸçš„ç³»ç»Ÿæ€§åˆ›æ–°å®ç°äº†è¿™ç§é«˜æ•ˆæ€§ï¼šæ¨¡å‹æ¶æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•å’Œæ¨ç†ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¨¡å‹æ¶æ„æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†InfLLM v2ï¼Œè¿™æ˜¯ä¸€ç§å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡å¤„ç†çš„é¢„å¡«å……å’Œè§£ç é˜¶æ®µã€‚åœ¨è®­ç»ƒæ•°æ®æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†UltraCleanï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤å’Œç”Ÿæˆç­–ç•¥ï¼Œä»¥åŠUltraChat v2ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†ä»…ä½¿ç”¨8ä¸‡äº¿ä¸ªè®­ç»ƒä»¤ç‰Œå°±å®ç°äº†ä»¤äººæ»¡æ„çš„æ¨¡å‹æ€§èƒ½ã€‚åœ¨è®­ç»ƒç®—æ³•æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ModelTunnel v2è¿›è¡Œé«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥æœç´¢ï¼Œå¹¶é€šè¿‡å¼•å…¥åˆ†å—æ»šåŠ¨è´Ÿè½½å¹³è¡¡çš„å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®é«˜æ•ˆä¸‰å…ƒLLM BitCPMæ¥æ”¹è¿›ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•ã€‚åœ¨æ¨ç†ç³»ç»Ÿæ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†CPM.cuï¼Œå®ƒé›†æˆäº†ç¨€ç–æ³¨æ„åŠ›ã€æ¨¡å‹é‡åŒ–å’ŒæŠ•æœºé‡‡æ ·ï¼Œä»¥å®ç°é«˜æ•ˆçš„é¢„å¡«å……å’Œè§£ç ã€‚ä¸ºäº†æ»¡è¶³ä¸åŒçš„è®¾å¤‡éœ€æ±‚ï¼ŒMiniCPM4æä¾›ä¸¤ä¸ªç‰ˆæœ¬ï¼Œåˆ†åˆ«ä¸º0.5Bå’Œ8Bå‚æ•°ã€‚å……è¶³çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMiniCPM4åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç›¸ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼Œå‡¸æ˜¾äº†å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼ŒMiniCPM4-8Bç›¸è¾ƒäºQwen3-8Bè¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„é€‚åº”ï¼ŒMiniCPM4æˆåŠŸåº”ç”¨äºå¤šç§åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬å¯ä¿¡è°ƒæŸ¥ç”Ÿæˆå’Œä½¿ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®çš„å·¥å…·ï¼Œå……åˆ†å±•ç¤ºäº†å…¶å¹¿æ³›çš„å¯ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07900v1">PDF</a> MiniCPM4 Technical Report</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸“ä¸ºç«¯ä¾§è®¾å¤‡è®¾è®¡çš„é«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹MiniCPM4ã€‚é€šè¿‡æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•å’Œæ¨ç†ç³»ç»Ÿå››ä¸ªæ–¹é¢çš„ç³»ç»Ÿæ€§åˆ›æ–°ï¼Œå®ç°äº†é«˜æ•ˆç‡ã€‚æ¨¡å‹æ¶æ„æ–¹é¢ï¼Œæå‡ºäº†InfLLM v2ï¼Œä¸€ç§å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡å¤„ç†çš„é¢„å¡«å……å’Œè§£ç é˜¶æ®µã€‚è®­ç»ƒæ•°æ®æ–¹é¢ï¼Œæå‡ºäº†UltraCleanå’ŒUltraChat v2ï¼Œåˆ†åˆ«ç”¨äºæœ‰æ•ˆçš„é¢„è®­ç»ƒæ•°æ®è¿‡æ»¤ç”Ÿæˆç­–ç•¥å’Œå…¨é¢çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†ã€‚è®­ç»ƒç®—æ³•æ–¹é¢ï¼Œé€šè¿‡ModelTunnel v2è¿›è¡Œé«˜æ•ˆé¢„è®­ç»ƒç­–ç•¥æœç´¢ï¼Œå¼•å…¥åˆ†å—æ»šåŠ¨æ”¹è¿›è´Ÿè½½å¹³è¡¡å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®é«˜æ•ˆä¸‰å…ƒLLM BitCPMã€‚æ¨ç†ç³»ç»Ÿæ–¹é¢ï¼Œæå‡ºæ•´åˆç¨€ç–æ³¨æ„åŠ›ã€æ¨¡å‹é‡åŒ–å’ŒæŠ•æœºé‡‡æ ·çš„CPM.cuï¼Œä»¥å®ç°é«˜æ•ˆé¢„å¡«å……å’Œè§£ç ã€‚MiniCPM4æœ‰ä¸¤ç§ç‰ˆæœ¬ï¼Œå‚æ•°åˆ†åˆ«ä¸º0.5Bå’Œ8Bï¼Œä»¥æ»¡è¶³ä¸åŒçš„è®¾å¤‡éœ€æ±‚ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMiniCPM4åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç›¸ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼Œçªæ˜¾å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯MiniCPM4-8Båœ¨å¤„ç†é•¿åºåˆ—æ—¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„é€Ÿåº¦æ”¹è¿›ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿›ä¸€æ­¥é€‚åº”ï¼ŒMiniCPM4æˆåŠŸåº”ç”¨äºå¯ä¿¡è°ƒæŸ¥ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨ç­‰åº”ç”¨ç¨‹åºï¼Œæ˜¾ç¤ºäº†å…¶å¹¿æ³›çš„å¯ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MiniCPM4æ˜¯ä¸€ä¸ªä¸ºç«¯ä¾§è®¾å¤‡è®¾è®¡çš„é«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é€šè¿‡åœ¨æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒç®—æ³•å’Œæ¨ç†ç³»ç»Ÿå››ä¸ªæ–¹é¢çš„åˆ›æ–°å®ç°é«˜æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†InfLLM v2çš„å¯è®­ç»ƒç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€Ÿé•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚</li>
<li>é€šè¿‡UltraCleanç­–ç•¥è¿›è¡Œé¢„è®­ç»ƒæ•°æ®è¿‡æ»¤å’Œç”Ÿæˆï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>å¼•å…¥ModelTunnel v2è¿›è¡Œé«˜æ•ˆé¢„è®­ç»ƒç­–ç•¥æœç´¢ï¼Œå¹¶æ”¹è¿›äº†è®­ç»ƒç®—æ³•ã€‚</li>
<li>æ¨å‡ºäº†é›†æˆå¤šç§æŠ€æœ¯çš„æ¨ç†ç³»ç»ŸCPM.cuï¼Œå®ç°é«˜æ•ˆé¢„å¡«å……å’Œè§£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3fdcf9af433083144cf658a0c8a1fdf2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MEMOIR-Lifelong-Model-Editing-with-Minimal-Overwrite-and-Informed-Retention-for-LLMs"><a href="#MEMOIR-Lifelong-Model-Editing-with-Minimal-Overwrite-and-Informed-Retention-for-LLMs" class="headerlink" title="MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed   Retention for LLMs"></a>MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed   Retention for LLMs</h2><p><strong>Authors:Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard</strong></p>
<p>Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably - without retraining or forgetting previous information - remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œä¸­éƒ¨ç½²çš„è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦åç»­æ›´æ–°æ¥èå…¥æ–°çš„æˆ–ä¿®æ­£çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œåœ¨ä¸è¿›è¡Œå†è®­ç»ƒæˆ–é—å¿˜å…ˆå‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆä¸”å¯é åœ°ç¼–è¾‘æ­¤ç±»æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ç°æœ‰çš„ç»ˆèº«æ¨¡å‹ç¼–è¾‘æ–¹æ³•è¦ä¹ˆæŸå®³æ³›åŒ–èƒ½åŠ›ï¼Œè¦ä¹ˆå¹²æ‰°è¿‡å»çš„ç¼–è¾‘ï¼Œè¦ä¹ˆæ— æ³•æ‰©å±•åˆ°é•¿çš„ç¼–è¾‘åºåˆ—ã€‚æˆ‘ä»¬æå‡ºäº†MEMOIRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¯æ‰©å±•æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ®‹ç•™è®°å¿†ï¼ˆå³ä¸“ç”¨å‚æ•°æ¨¡å—ï¼‰æ³¨å…¥çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›ã€‚MEMOIRé€šè¿‡æ ·æœ¬ç›¸å…³æ©ç å¯¹è¾“å…¥æ¿€æ´»è¿›è¡Œç¨€ç–åŒ–ï¼Œå°†æ¯ä¸ªç¼–è¾‘é™åˆ¶åœ¨å†…å­˜å‚æ•°çš„ä¸€ä¸ªç‹¬ç‰¹å­é›†ä¸­ï¼Œæœ€å°åŒ–ç¼–è¾‘ä¹‹é—´çš„å¹²æ‰°ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒé€šè¿‡æ¯”è¾ƒæ–°æŸ¥è¯¢çš„ç¨€ç–æ¿€æ´»æ¨¡å¼ä¸ç¼–è¾‘è¿‡ç¨‹ä¸­å­˜å‚¨çš„æ¨¡å¼æ¥è¯†åˆ«ç›¸å…³çš„ç¼–è¾‘ã€‚è¿™å…è®¸å¯¹é‡æ–°è¡¨è¿°çš„æŸ¥è¯¢è¿›è¡Œæ³›åŒ–ï¼Œä»…æ¿€æ´»ç›¸å…³çŸ¥è¯†ï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸å¿…è¦è®°å¿†æ¿€æ´»ä»¥åº”å¯¹ä¸ç›¸å…³çš„æç¤ºã€‚åœ¨LLaMA-3å’ŒMistralä¸Šçš„é—®ç­”ã€å¹»è§‰æ ¡æ­£å’Œè¶…å‡ºåˆ†å¸ƒæ³›åŒ–åŸºå‡†æµ‹è¯•çš„å®éªŒè¡¨æ˜ï¼ŒMEMOIRåœ¨å¯é æ€§ã€æ³›åŒ–å’Œå±€éƒ¨æ€§æŒ‡æ ‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¯æ‰©å±•åˆ°æ•°ä»¥åƒè®¡çš„è¿ç»­ç¼–è¾‘ä¸”å‡ ä¹ä¸ä¼šé—å¿˜ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07899v1">PDF</a> The first two authors contributed equally to this work</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­è¨€æ¨¡å‹åœ¨çœŸå®ç³»ç»Ÿåº”ç”¨ä¸­é¢ä¸´çš„é—®é¢˜ï¼Œå³éœ€è¦äº‹åæ›´æ–°ä»¥èå…¥æ–°çš„æˆ–ä¿®æ­£çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨ä¸é‡æ–°è®­ç»ƒæˆ–é—å¿˜æ—§ä¿¡æ¯çš„æƒ…å†µä¸‹é«˜æ•ˆå¯é åœ°ç¼–è¾‘è¿™äº›æ¨¡å‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯æ‰©å±•æ¡†æ¶MEMOIRï¼Œå®ƒé€šè¿‡æ®‹ä½™å†…å­˜ï¼ˆå³ä¸“ç”¨å‚æ•°æ¨¡å—ï¼‰æ³¨å…¥çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½ã€‚MEMOIRé€šè¿‡æ ·æœ¬ç›¸å…³æ©ç å¯¹è¾“å…¥æ¿€æ´»è¿›è¡Œç¨€ç–åŒ–ï¼Œå°†æ¯æ¬¡ç¼–è¾‘é™åˆ¶åœ¨å†…å­˜å‚æ•°çš„ä¸€ä¸ªç‰¹å®šå­é›†ä¸Šï¼Œä»è€Œæœ€å°åŒ–ç¼–è¾‘ä¹‹é—´çš„å¹²æ‰°ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå®ƒé€šè¿‡æ¯”è¾ƒæ–°æŸ¥è¯¢çš„ç¨€ç–æ¿€æ´»æ¨¡å¼ä¸ç¼–è¾‘æœŸé—´å­˜å‚¨çš„æ¨¡å¼æ¥è¯†åˆ«ç›¸å…³çš„ç¼–è¾‘ã€‚å®éªŒè¡¨æ˜ï¼ŒMEMOIRåœ¨å¯é æ€§ã€æ³›åŒ–å’Œå±€éƒ¨åº¦é‡æ–¹é¢è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œèƒ½å¤Ÿæ‰©å±•åˆ°æ•°åƒä¸ªè¿ç»­ç¼–è¾‘è€Œå‡ ä¹ä¸é—å¿˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹éœ€è¦äº‹åæ›´æ–°ä»¥èå…¥æ–°æˆ–ä¿®æ­£çš„çŸ¥è¯†ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ç¼–è¾‘æ–¹æ³•å­˜åœ¨æ•ˆç‡ã€å¯é æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚</li>
<li>MEMOIRæ¡†æ¶é€šè¿‡æ®‹ä½™å†…å­˜æ³¨å…¥çŸ¥è¯†ï¼Œä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½ã€‚</li>
<li>MEMOIRé€šè¿‡ç¨€ç–åŒ–è¾“å…¥æ¿€æ´»æ¥é™åˆ¶æ¯æ¬¡ç¼–è¾‘çš„èŒƒå›´ï¼Œå‡å°‘ç¼–è¾‘é—´çš„å¹²æ‰°ã€‚</li>
<li>MEMOIRé€šè¿‡æ¯”è¾ƒæŸ¥è¯¢çš„æ¿€æ´»æ¨¡å¼ä¸å­˜å‚¨çš„æ¨¡å¼æ¥è¯†åˆ«ç›¸å…³ç¼–è¾‘ã€‚</li>
<li>MEMOIRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é—®ç­”ã€å¹»è§‰ä¿®æ­£å’Œåˆ†å¸ƒå¤–æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-135dcbcd5b0c472d02b476c88f3e3882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a86ea41e40be35699191f050c81fc9a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a56a6fb3794d1ecfdfb390bab0884201.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-644a28f774a92c3e030ed995b0ba04d1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Synthetic-Visual-Genome"><a href="#Synthetic-Visual-Genome" class="headerlink" title="Synthetic Visual Genome"></a>Synthetic Visual Genome</h2><p><strong>Authors:Jae Sung Park, Zixian Ma, Linjie Li, Chenhao Zheng, Cheng-Yu Hsieh, Ximing Lu, Khyathi Chandu, Quan Kong, Norimasa Kobori, Ali Farhadi, Yejin Choi, Ranjay Krishna</strong></p>
<p>Reasoning over visual relationships-spatial, functional, interactional, social, etc.-is considered to be a fundamental component of human cognition. Yet, despite the major advances in visual comprehension in multimodal language models (MLMs), precise reasoning over relationships and their generations remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely annotated relationships capable of constructing high-quality dense scene graphs at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by completing the missing relations of selected objects in existing scene graphs using a teacher MLM and a carefully designed filtering process to ensure high-quality. To generate more accurate and rich scene graphs at scale for any image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further refines ROBINâ€™s predicted scene graphs by removing unlikely relations and&#x2F;or suggesting relevant ones. In total, our dataset contains 146K images and 5.6M relationships for 2.6M objects. Results show that our ROBIN-3B model, despite being trained on less than 3 million instances, outperforms similar-size models trained on over 300 million instances on relationship understanding benchmarks, and even surpasses larger models up to 13B parameters. Notably, it achieves state-of-the-art performance in referring expression comprehension with a score of 88.9, surpassing the previous best of 87.4. Our results suggest that training on the refined scene graph data is crucial to maintaining high performance across diverse visual reasoning task. </p>
<blockquote>
<p>è§†è§‰å…³ç³»æ¨ç†â€”â€”ç©ºé—´ã€åŠŸèƒ½ã€äº¤äº’ã€ç¤¾ä¼šç­‰å…³ç³»æ¨ç†è¢«è®¤ä¸ºæ˜¯äººç±»è®¤çŸ¥çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å‡†ç¡®åœ°è¿›è¡Œå…³ç³»åŠå…¶ä»£é™…æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†ROBINï¼šä¸€ä¸ªç”¨å¯†é›†æ ‡æ³¨çš„å…³ç³»è¿›è¡Œå¾®è°ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æŒ‡ä»¤ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡æ„å»ºé«˜è´¨é‡å¯†é›†åœºæ™¯å›¾ã€‚ä¸ºäº†è®­ç»ƒROBINï¼Œæˆ‘ä»¬æ•´ç†å‡ºäº†ä¸€ä¸ªåˆæˆåœºæ™¯å›¾æ•°æ®é›†SVGï¼Œé€šè¿‡å®Œæˆç°æœ‰åœºæ™¯å›¾ä¸­é€‰å®šå¯¹è±¡çš„ç¼ºå¤±å…³ç³»ï¼Œå¹¶åˆ©ç”¨æ•™å¸ˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œç²¾å¿ƒè®¾è®¡çš„è¿‡æ»¤è¿‡ç¨‹æ¥ç¡®ä¿é«˜è´¨é‡ã€‚ä¸ºäº†ä¸ºä»»ä½•å›¾åƒç”Ÿæˆæ›´å‡†ç¡®å’Œä¸°å¯Œçš„åœºæ™¯å›¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†SG-EDITï¼šä¸€ä¸ªè‡ªæˆ‘è’¸é¦æ¡†æ¶ï¼ŒGPT-4oè¿›ä¸€æ­¥ç²¾ç‚¼ROBINé¢„æµ‹çš„åœºæ™¯å›¾ï¼Œé€šè¿‡åˆ é™¤ä¸å¤ªå¯èƒ½çš„å…³ç³»å’Œ&#x2F;æˆ–æå‡ºç›¸å…³çš„å…³ç³»ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ€»å…±åŒ…å«14.6ä¸‡å¼ å›¾åƒå’Œ560ä¸‡ä¸ªå…³ç³»ï¼Œæ¶‰åŠ260ä¸‡ä¸ªå¯¹è±¡ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ROBIN-3Bæ¨¡å‹è™½ç„¶åœ¨å°‘äº300ä¸‡ä¸ªå®ä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨å…³ç³»ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†åœ¨è¶…è¿‡3äº¿ä¸ªå®ä¾‹ä¸Šè®­ç»ƒçš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†å‚æ•°é«˜è¾¾13Bçš„å¤§å‹æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨æŒ‡ä»£è¡¨è¾¾å¼ç†è§£æ–¹é¢è¾¾åˆ°äº†88.9åˆ†ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æˆç»©87.4åˆ†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç²¾ç‚¼çš„åœºæ™¯å›¾æ•°æ®è¿›è¡Œè®­ç»ƒå¯¹äºåœ¨ä¸åŒå¤šæ ·çš„è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¿æŒé«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07643v1">PDF</a> CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººç±»è®¤çŸ¥ä¸­è§†è§‰å…³ç³»æ¨ç†æ˜¯ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ¶‰åŠç©ºé—´ã€åŠŸèƒ½ã€äº¤äº’ã€ç¤¾ä¼šç­‰å…³ç³»ã€‚å°½ç®¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç²¾ç¡®æ¨ç†å…³ç³»åŠå…¶ç”Ÿæˆä»æ˜¯æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ROBINæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¯†é›†æ³¨é‡Šçš„å…³ç³»è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡æ„å»ºé«˜è´¨é‡å¯†é›†åœºæ™¯å›¾ã€‚ä¸ºè®­ç»ƒROBINï¼Œæˆ‘ä»¬æ•´ç†äº†SVGæ•°æ®é›†ï¼Œé€šè¿‡æ•™å¸ˆMLMå®Œæˆç°æœ‰åœºæ™¯å›¾ä¸­é€‰å®šå¯¹è±¡çš„ç¼ºå¤±å…³ç³»ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„è¿‡æ»¤è¿‡ç¨‹ç¡®ä¿é«˜è´¨é‡ã€‚ä¸ºå¤§è§„æ¨¡ç”Ÿæˆæ›´å‡†ç¡®å’Œä¸°å¯Œçš„åœºæ™¯å›¾ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SG-EDITæ¡†æ¶ï¼ŒGPT-4oè¿›ä¸€æ­¥ç²¾ç‚¼ROBINé¢„æµ‹çš„åœºæ™¯å›¾ï¼Œç§»é™¤ä¸å¤ªå¯èƒ½çš„å…³ç³»æˆ–æå‡ºç›¸å…³å…³ç³»ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«14.6ä¸‡å¼ å›¾åƒå’Œ560ä¸‡å…³ç³»ï¼Œæ¶‰åŠ260ä¸‡ä¸ªå¯¹è±¡ã€‚å°½ç®¡ROBIN-3Bæ¨¡å‹è®­ç»ƒå®ä¾‹å°‘äº300ä¸‡ï¼Œä½†åœ¨å…³ç³»ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ç±»ä¼¼å¤§å°çš„æ¨¡å‹ï¼ˆè®­ç»ƒå®ä¾‹è¶…è¿‡3äº¿ï¼‰ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§å‚æ•°çš„æ¨¡å‹ã€‚åœ¨æŒ‡ä»£è¡¨è¾¾å¼ç†è§£æ–¹é¢ï¼ŒROBINå–å¾—äº†æœ€é«˜åˆ†88.9åˆ†ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æˆç»©87.4åˆ†ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç²¾ç‚¼çš„åœºæ™¯å›¾æ•°æ®è¿›è¡Œè®­ç»ƒå¯¹ä¿æŒå„ç§è§†è§‰æ¨ç†ä»»åŠ¡çš„é«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰å…³ç³»æ¨ç†æ˜¯äººç±»è®¤çŸ¥çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œæ¶µç›–å¤šç§è§†è§‰å…³ç³»å¦‚ç©ºé—´ã€åŠŸèƒ½ã€äº¤äº’å’Œç¤¾ä¼šå…³ç³»ã€‚</li>
<li>å°½ç®¡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†ç²¾ç¡®æ¨ç†è§†è§‰å…³ç³»ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥ROBINæ¨¡å‹ï¼Œé€šè¿‡å¯†é›†æ³¨é‡Šçš„å…³ç³»è¿›è¡Œè®­ç»ƒï¼Œèƒ½æ„å»ºé«˜è´¨é‡å¯†é›†åœºæ™¯å›¾ã€‚</li>
<li>ä¸ºè®­ç»ƒROBINï¼Œæ•´ç†äº†SVGæ•°æ®é›†ï¼Œé€šè¿‡æ•™å¸ˆMLMå®Œæˆç°æœ‰åœºæ™¯å›¾ä¸­å¯¹è±¡ç¼ºå¤±çš„å…³ç³»ã€‚</li>
<li>æ¨å‡ºSG-EDITæ¡†æ¶ï¼Œè¿›ä¸€æ­¥æé«˜ROBINæ¨¡å‹é¢„æµ‹åœºæ™¯å›¾çš„å‡†ç¡®æ€§ã€‚</li>
<li>ROBIN-3Bæ¨¡å‹åœ¨å…³ç³»ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå³ä½¿è®­ç»ƒå®ä¾‹è¾ƒå°‘ä¹Ÿè¶…è¶Šäº†ç±»ä¼¼å’Œæ›´å¤§çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-924f4293351ff51bb94798301ffe523e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360b760e3e843fcc72df2f251f2e1f16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5570a0daad4790bb2a7b4b9b2f2c0518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abbba258c2e7da9cae69db4a07393801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f666ae0aaa166acb9ccbf70dfc72c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69f4b31120041786c1b6806a233075f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Instructing-Large-Language-Models-for-Low-Resource-Languages-A-Systematic-Study-for-Basque"><a href="#Instructing-Large-Language-Models-for-Low-Resource-Languages-A-Systematic-Study-for-Basque" class="headerlink" title="Instructing Large Language Models for Low-Resource Languages: A   Systematic Study for Basque"></a>Instructing Large Language Models for Low-Resource Languages: A   Systematic Study for Basque</h2><p><strong>Authors:Oscar Sainz, Naiara Perez, Julen Etxaniz, Joseba Fernandez de Landa, Itziar Aldabe, Iker GarcÃ­a-Ferrero, Aimar Zabala, Ekhi Azurmendi, German Rigau, Eneko Agirre, Mikel Artetxe, Aitor Soroa</strong></p>
<p>Instructing language models with user intent requires large instruction datasets, which are only available for a limited set of languages. In this paper, we explore alternatives to conventional instruction adaptation pipelines in low-resource scenarios. We assume a realistic scenario for low-resource languages, where only the following are available: corpora in the target language, existing open-weight multilingual base and instructed backbone LLMs, and synthetically generated instructions sampled from the instructed backbone. We present a comprehensive set of experiments for Basque that systematically study different combinations of these components evaluated on benchmarks and human preferences from 1,680 participants. Our conclusions show that target language corpora are essential, with synthetic instructions yielding robust models, and, most importantly, that using as backbone an instruction-tuned model outperforms using a base non-instructed model, and improved results when scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near frontier models of much larger sizes for Basque, without using any Basque data apart from the 1.2B word corpora. We release code, models, instruction datasets, and human preferences to support full reproducibility in future research on low-resource language adaptation. </p>
<blockquote>
<p>ä½¿ç”¨ç”¨æˆ·æ„å›¾æ¥æŒ‡å¯¼è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡æŒ‡ä»¤æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åªé€‚ç”¨äºæœ‰é™çš„è¯­è¨€é›†åˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½èµ„æºåœºæ™¯ä¸‹ä¼ ç»ŸæŒ‡ä»¤é€‚åº”ç®¡é“çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬å‡è®¾äº†ä¸€ä¸ªä½èµ„æºè¯­è¨€çš„ç°å®åœºæ™¯ï¼Œå…¶ä¸­ä»…æä¾›ä»¥ä¸‹èµ„æºï¼šç›®æ ‡è¯­è¨€è¯­æ–™åº“ã€ç°æœ‰çš„å¼€æ”¾æƒé‡å¤šè¯­è¨€åŸºç¡€å’Œæœ‰æŒ‡ä»¤éª¨æ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠä»æŒ‡ä»¤éª¨æ¶ä¸­é‡‡æ ·å¾—åˆ°çš„åˆæˆæŒ‡ä»¤ã€‚æˆ‘ä»¬å¯¹å·´æ–¯å…‹è¯­è¿›è¡Œäº†ä¸€ç³»åˆ—ç»¼åˆå®éªŒï¼Œè¯¥å®éªŒç³»ç»Ÿåœ°ç ”ç©¶äº†è¿™äº›ç»„ä»¶çš„ä¸åŒç»„åˆï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•å’Œäººç±»åå¥½ï¼ˆæ¥è‡ª1680åå‚ä¸è€…ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“è®ºæ˜¾ç¤ºï¼Œç›®æ ‡è¯­è¨€è¯­æ–™åº“è‡³å…³é‡è¦ï¼ŒåˆæˆæŒ‡ä»¤äº§ç”Ÿäº†ç¨³å¥çš„æ¨¡å‹ï¼Œè€Œä¸”æœ€é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨æŒ‡ä»¤è°ƒæ•´åçš„æ¨¡å‹ä½œä¸ºéª¨æ¶ä¼˜äºä½¿ç”¨åŸºç¡€éæŒ‡ä»¤æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ‰©å¤§è§„æ¨¡æ—¶æé«˜äº†ç»“æœã€‚ä½¿ç”¨Llama 3.1æŒ‡ä»¤70Bä½œä¸ºéª¨æ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸ä½¿ç”¨ä»»ä½•å·´æ–¯å…‹è¯­æ•°æ®çš„æƒ…å†µä¸‹ï¼ˆé™¤äº†12äº¿å•è¯è¯­æ–™åº“å¤–ï¼‰ï¼Œæ¥è¿‘æ›´å¤§è§„æ¨¡çš„å·´æ–¯å…‹è¯­å‰æ²¿æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç ã€æ¨¡å‹ã€æŒ‡ä»¤æ•°æ®é›†å’Œäººç±»åå¥½ï¼Œä»¥æ”¯æŒæœªæ¥ä½èµ„æºè¯­è¨€é€‚åº”ç ”ç©¶ä¸­çš„å®Œå…¨å¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07597v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†åœ¨ç¼ºä¹å¤§é‡æŒ‡ä»¤æ•°æ®é›†çš„ä½èµ„æºåœºæ™¯ä¸‹ï¼Œå¦‚ä½•å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œç”¨æˆ·æ„å›¾æŒ‡å¯¼ã€‚ç ”ç©¶åœ¨å·´æ–¯å…‹è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€çš„ç°å®æƒ…æ™¯ä¸‹å±•å¼€ï¼Œé€šè¿‡ä¸€ç³»åˆ—å®éªŒå’Œè¯„ä¼°ï¼Œè¯å®ç›®æ ‡è¯­è¨€è¯­æ–™åº“çš„é‡è¦æ€§ï¼Œä»¥åŠåˆæˆæŒ‡ä»¤èƒ½ç”Ÿæˆç¨³å¥æ¨¡å‹ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä½œä¸ºä¸»å¹²ç½‘ç»œä¼˜äºä½¿ç”¨åŸºç¡€éæŒ‡ä»¤æ¨¡å‹ï¼Œå¹¶åœ¨æ‰©å¤§è§„æ¨¡æ—¶å–å¾—æ›´å¥½çš„ç»“æœã€‚ä½¿ç”¨Llama 3.1æŒ‡ä»¤70Bä½œä¸ºä¸»å¹²ç½‘ï¼Œæ¨¡å‹æ€§èƒ½æ¥è¿‘é’ˆå¯¹å·´æ–¯å…‹è¯­çš„æ›´å¤§è§„æ¨¡å‰æ²¿æ¨¡å‹ï¼Œä¸”æ— éœ€ä½¿ç”¨é™¤å·´æ–¯å…‹è¯­æ–™åº“ä»¥å¤–çš„ä»»ä½•æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨ä½èµ„æºåœºæ™¯ä¸‹çš„è¯­è¨€æ¨¡å‹ç”¨æˆ·æ„å›¾æŒ‡å¯¼ã€‚</li>
<li>åœ¨å·´æ–¯å…‹è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€çš„ç°å®æƒ…æ™¯ä¸‹å±•å¼€ç ”ç©¶ã€‚</li>
<li>ç›®æ ‡è¯­è¨€è¯­æ–™åº“çš„é‡è¦æ€§å¾—åˆ°è¯å®ã€‚</li>
<li>åˆæˆæŒ‡ä»¤èƒ½æœ‰æ•ˆç”Ÿæˆç¨³å¥çš„æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä½œä¸ºä¸»å¹²ç½‘ç»œè¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æ‰©å¤§è§„æ¨¡æ—¶ï¼Œä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹èƒ½å–å¾—æ›´å¥½çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81c68402faa1c38f1c733cf474d80e27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec78aade1b3d52cfe7e59bfd40e9efde.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f55a3ab3b1b2d10b14dc6212d269a17a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition"><a href="#GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition" class="headerlink" title="GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition"></a>GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition</h2><p><strong>Authors:Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He</strong></p>
<p>Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of \textit{Faithfully Recognize What Youâ€™ve Seen}, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>. </p>
<blockquote>
<p>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯é€šè¿‡å°†åˆ†å­å›¾åƒè½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼æ¥æ•°å­—åŒ–åŒ–å­¦çŸ¥è¯†çš„é‡è¦æŠ€æœ¯ã€‚å°½ç®¡æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬çš„å›¾åƒæè¿°æ–¹æ³•é€šå¸¸éš¾ä»¥å¤„ç†å¤æ‚çš„åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´çš„æ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GTR-Mol-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ä¸¤é¡¹å…³é”®åˆ›æ–°çš„æ–°æ¡†æ¶ï¼šï¼ˆ1ï¼‰é€šè¿‡è¿ç»­åŸå­é”®é¢„æµ‹é€æ­¥è§£æåˆ†å­å›¾çš„â€œå›¾éå†ä½œä¸ºè§†è§‰æ€ç»´é“¾â€æœºåˆ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼›ä»¥åŠï¼ˆ2ï¼‰â€œå¿ å®è¯†åˆ«ä½ æ‰€çœ‹åˆ°çš„â€çš„æ•°æ®ä¸­å¿ƒåŸåˆ™ï¼Œè¯¥åŸåˆ™è§£å†³äº†å›¾åƒä¸­ç¼©å†™ç»“æ„ä¸æ‰©å±•æ³¨é‡Šä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†GTR-CoT-1.3Mï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå…·æœ‰ç²¾å¿ƒæ ¡æ­£çš„æ³¨é‡Šï¼Œå¹¶å¼•å…¥äº†MolRec-Benchï¼Œè¿™æ˜¯ä¸“ä¸ºOCSRä¸­å›¾å½¢è§£æç²¾åº¦çš„ç²¾ç»†è¯„ä¼°è€Œè®¾è®¡çš„ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMç›¸è¾ƒäºä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸçš„VLMä»¥åŠå•†ä¸šé€šç”¨VLMå–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚ç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¸¦æœ‰åŠŸèƒ½å›¢ç¼©å†™çš„åˆ†å­å›¾åƒçš„åœºæ™¯ä¸­ï¼ŒGTR-Mol-VLMåœ¨SMILESåŸºå’Œå›¾å½¢åŸºæŒ‡æ ‡ä¸Šçš„è¡¨ç°éƒ½æ¯”ç¬¬äºŒååŸºå‡†é«˜å‡ºçº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå°†æ¨åŠ¨OCSRæŠ€æœ¯æ›´æœ‰æ•ˆåœ°æ»¡è¶³ç°å®ä¸–ç•Œçš„éœ€æ±‚ï¼Œä»è€Œæ¨åŠ¨åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT%E5%8F%91%E5%B8%83GTR-CoT%E3%80%82">https://github.com/opendatalab/GTR-CoTå‘å¸ƒGTR-CoTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07553v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯æ•°å­—åŒ–åŒ–å­¦çŸ¥è¯†çš„é‡è¦æ­¥éª¤ï¼Œå¯å°†åˆ†å­å›¾åƒè½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´æ³¨é‡Šæ—¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†GTR-Mol-VLMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿäººç±»æ¨ç†çš„â€œå›¾éå†ä½œä¸ºè§†è§‰æ€ç»´é“¾â€æœºåˆ¶å’Œè§£å†³å›¾åƒä¸­çš„ç¼©ç•¥ç»“æ„ä¸æ‰©å±•æ³¨é‡Šä¹‹é—´ä¸åŒ¹é…é—®é¢˜çš„â€œå¿ å®è¯†åˆ«ä½ æ‰€çœ‹åˆ°çš„â€çš„æ•°æ®ä¸­å¿ƒåŸåˆ™ã€‚ä¸ºæ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæ„å»ºäº†å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Må’Œé’ˆå¯¹OCSRçš„å›¾è§£æå‡†ç¡®æ€§çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•MolRec-Benchã€‚å®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMç›¸è¾ƒäºä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸVLMså’Œå•†ä¸šé€šç”¨VLMså–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰åŠŸèƒ½ç»„ç¼©ç•¥çš„åˆ†å­å›¾åƒæ—¶ï¼Œåœ¨SMILESå’Œå›¾å½¢æŒ‡æ ‡ä¸Šå‡ä¼˜äºç¬¬äºŒååŸºçº¿çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>OCSRåœ¨æ•°å­—åŒ–åŒ–å­¦çŸ¥è¯†ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½å°†åˆ†å­å›¾åƒè½¬ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´æ³¨é‡Šæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>GTR-Mol-VLMæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œå›¾éå†è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>GTR-Mol-VLMåŒ…æ‹¬ä¸¤å¤§åˆ›æ–°ï¼šGraph Traversal as Visual Chain of Thoughtå’ŒFaithfully Recognize What Youâ€™ve Seenæœºåˆ¶ã€‚</li>
<li>ä¸ºæ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæ„å»ºäº†å¤§å‹æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Må’Œé¦–ä¸ªåŸºå‡†æµ‹è¯•MolRec-Benchã€‚</li>
<li>GTR-Mol-VLMç›¸è¾ƒäºå…¶ä»–æ¨¡å‹åœ¨å¤„ç†åˆ†å­å›¾åƒæ—¶å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰åŠŸèƒ½ç»„ç¼©ç•¥çš„å›¾åƒæ—¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2948ec9739215f865d5391b777589216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66ff9a7989f7206f8faf4a9bcbc1b83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2356decec6238f37994b975776cad5e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04f3054bcb2af3a39565c202059e7abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c47d29fef216a4706ab4f75e44828a4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Multilingual-Vulnerability-Detection-How-Far-Are-We"><a href="#Large-Language-Models-for-Multilingual-Vulnerability-Detection-How-Far-Are-We" class="headerlink" title="Large Language Models for Multilingual Vulnerability Detection: How Far   Are We?"></a>Large Language Models for Multilingual Vulnerability Detection: How Far   Are We?</h2><p><strong>Authors:Honglin Shu, Michael Fu, Junji Yu, Dong Wang, Chakkrit Tantithamthavorn, Junjie Chen, Yasutaka Kamei</strong></p>
<p>Various deep learning-based approaches utilizing pre-trained language models (PLMs) have been proposed for automated vulnerability detection. With recent advancements in large language models (LLMs), several studies have begun exploring their application to vulnerability detection tasks. However, existing studies primarily focus on specific programming languages (e.g., C&#x2F;C++) and function-level detection, leaving the strengths and weaknesses of PLMs and LLMs in multilingual and multi-granularity scenarios largely unexplored. To bridge this gap, we conduct a comprehensive fine-grained empirical study evaluating the effectiveness of state-of-the-art PLMs and LLMs for multilingual vulnerability detection. Using over 30,000 real-world vulnerability-fixing patches across seven programming languages, we systematically assess model performance at both the function-level and line-level. Our key findings indicate that GPT-4o, enhanced through instruction tuning and few-shot prompting, significantly outperforms all other evaluated models, including CodeT5P. Furthermore, the LLM-based approach demonstrates superior capability in detecting unique multilingual vulnerabilities, particularly excelling in identifying the most dangerous and high-severity vulnerabilities. These results underscore the promising potential of adopting LLMs for multilingual vulnerability detection at function-level and line-level, revealing their complementary strengths and substantial improvements over PLM approaches. This first empirical evaluation of PLMs and LLMs for multilingual vulnerability detection highlights LLMsâ€™ value in addressing real-world software security challenges. </p>
<blockquote>
<p>é’ˆå¯¹è‡ªåŠ¨æ¼æ´æ£€æµ‹ï¼Œå·²ç»æå‡ºäº†åˆ©ç”¨å„ç§åŸºäºæ·±åº¦å­¦ä¹ å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„æ–¹æ³•ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°å‘å±•ï¼Œä¸€äº›ç ”ç©¶å¼€å§‹æ¢ç´¢å…¶åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰å’Œå‡½æ•°çº§åˆ«çš„æ£€æµ‹ä¸Šï¼Œè€Œé’ˆå¯¹å¤šè¯­ç§å’Œå¤šç²’åº¦åœºæ™¯ä¸­çš„PLMå’ŒLLMçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„ç²¾ç»†å®è¯ç ”ç©¶ï¼Œè¯„ä¼°äº†æœ€å…ˆè¿›çš„PLMå’ŒLLMåœ¨å¤šè¯­ç§æ¼æ´æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åˆ©ç”¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸­çš„è¶…è¿‡ä¸‰ä¸‡ä»½çœŸå®ä¸–ç•Œæ¼æ´ä¿®å¤è¡¥ä¸ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹å’Œå‡½æ•°çº§åˆ«ä»¥åŠè¡Œçº§åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°è¡¨æ˜ï¼Œç»è¿‡æŒ‡ä»¤è°ƒæ•´å’Œå°‘é‡æç¤ºå¢å¼ºçš„GPT-4oåœ¨æ‰€æœ‰å…¶ä»–è¯„ä¼°æ¨¡å‹ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼ŒåŒ…æ‹¬CodeT5Pã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨æ£€æµ‹ç‹¬ç‰¹çš„å¤šè¯­ç§æ¼æ´æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ“…é•¿è¯†åˆ«æœ€å±é™©å’Œé«˜ä¸¥é‡ç¨‹åº¦çš„æ¼æ´ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†é‡‡ç”¨LLMè¿›è¡Œå¤šè¯­ç§æ¼æ´æ£€æµ‹åœ¨å‡½æ•°çº§åˆ«å’Œè¡Œçº§åˆ«çš„å·¨å¤§æ½œåŠ›ï¼Œå‡¸æ˜¾äº†å®ƒä»¬åœ¨äº’è¡¥å¼ºåº¦å’Œå®è´¨æ€§æ”¹è¿›æ–¹é¢çš„ä¼˜åŠ¿ã€‚è¿™ä¸€é’ˆå¯¹PLMå’ŒLLMåœ¨å¤šè¯­ç§æ¼æ´æ£€æµ‹æ–¹é¢çš„é¦–æ¬¡å®è¯ç ”ç©¶å‡¸æ˜¾äº†LLMåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶å®‰å…¨æŒ‘æˆ˜æ–¹é¢çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07503v1">PDF</a> 33 pages, 9 figures</p>
<p><strong>Summary</strong><br>åœ¨å„ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹æ–¹æ¡ˆä¸­ï¼Œç ”ç©¶äººå‘˜å·²é‡‡ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°å‘å±•ï¼Œå¤šé¡¹ç ”ç©¶å¼€å§‹æ¢ç´¢å…¶åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰å’ŒåŠŸèƒ½å±‚é¢çš„æ£€æµ‹ä¸Šï¼Œå¯¹PLMså’ŒLLMsåœ¨å¤šè¯­ç§å’Œå¤šç²’åº¦åœºæ™¯ä¸‹çš„ä¼˜åŠ£çŸ¥ä¹‹ç”šå°‘ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„ç²¾ç»†å®è¯ç ”ç©¶ï¼Œè¯„ä¼°äº†æœ€å‰æ²¿çš„PLMså’ŒLLMsåœ¨å¤šè¯­ç§æ¼æ´æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨è¶…è¿‡ä¸‰ä¸‡ä»½çœŸå®ä¸–ç•Œæ¼æ´ä¿®å¤è¡¥ä¸ï¼ˆæ¶µç›–ä¸ƒç§ç¼–ç¨‹è¯­è¨€ï¼‰ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹åœ¨åŠŸèƒ½å±‚é¢å’Œè¡Œçº§åˆ«çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°GPT-4oé€šè¿‡æŒ‡ä»¤å¾®è°ƒå’Œå°æ ·æœ¬æç¤ºæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬CodeT5Pã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨æ£€æµ‹ç‹¬ç‰¹çš„å¤šè¯­ç§æ¼æ´æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ“…é•¿è¯†åˆ«æœ€å±é™©çš„é«˜çº§åˆ«æ¼æ´ã€‚è¿™äº›ç»“æœçªæ˜¾äº†é‡‡ç”¨LLMsè¿›è¡Œå¤šè¯­ç§æ¼æ´æ£€æµ‹çš„æ½œåŠ›ï¼Œæ­ç¤ºäº†å…¶åœ¨åŠŸèƒ½å±‚é¢å’Œè¡Œçº§åˆ«çš„ä¼˜åŠ¿ä»¥åŠå¯¹PLMæ–¹æ³•çš„å®è´¨æ€§æ”¹è¿›ã€‚è¿™æ˜¯é¦–æ¬¡é’ˆå¯¹PLMså’ŒLLMsè¿›è¡Œçš„å¤šè¯­ç§æ¼æ´æ£€æµ‹çš„å®è¯ç ”ç©¶ï¼Œçªæ˜¾äº†LLMsåœ¨åº”å¯¹çœŸå®è½¯ä»¶å®‰å…¨æŒ‘æˆ˜æ–¹é¢çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•å·²åº”ç”¨äºåŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨å¼€å§‹å—åˆ°ç ”ç©¶å…³æ³¨ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨ç‰¹å®šç¼–ç¨‹è¯­è¨€å’ŒåŠŸèƒ½å±‚é¢çš„æ£€æµ‹ï¼Œå¿½ç•¥äº†å¤šè¯­ç§å’Œå¤šç²’åº¦åœºæ™¯ã€‚</li>
<li>GPT-4oåœ¨æ¼æ´æ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒå’Œå°æ ·æœ¬æç¤ºæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>åŸºäºLLMçš„æ–¹æ³•åœ¨æ£€æµ‹å¤šè¯­ç§æ¼æ´æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œå°¤å…¶æ“…é•¿è¯†åˆ«é«˜çº§åˆ«æ¼æ´ã€‚</li>
<li>LLMsåœ¨åŠŸèƒ½å±‚é¢å’Œè¡Œçº§åˆ«çš„æ¼æ´æ£€æµ‹æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œç›¸è¾ƒäºPLMæ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f80769346fd26a09814959eefcf59a3c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Basis-Transformers-for-Multi-Task-Tabular-Regression"><a href="#Basis-Transformers-for-Multi-Task-Tabular-Regression" class="headerlink" title="Basis Transformers for Multi-Task Tabular Regression"></a>Basis Transformers for Multi-Task Tabular Regression</h2><p><strong>Authors:Wei Min Loh, Jiaqi Shang, Pascal Poupart</strong></p>
<p>Dealing with tabular data is challenging due to partial information, noise, and heterogeneous structure. Existing techniques often struggle to simultaneously address key aspects of tabular data such as textual information, a variable number of columns, and unseen data without metadata besides column names. We propose a novel architecture, \textit{basis transformers}, specifically designed to tackle these challenges while respecting inherent invariances in tabular data, including hierarchical structure and the representation of numeric values. We evaluate our design on a multi-task tabular regression benchmark, achieving an improvement of 0.338 in the median $R^2$ score and the lowest standard deviation across 34 tasks from the OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters than the best-performing baseline and surpasses pretrained large language model baselines â€“ even when initialized from randomized weights. </p>
<blockquote>
<p>å¤„ç†è¡¨æ ¼æ•°æ®å› éƒ¨åˆ†ä¿¡æ¯ã€å™ªå£°å’Œå¼‚è´¨ç»“æ„è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æŠ€æœ¯å¾€å¾€éš¾ä»¥åŒæ—¶è§£å†³è¡¨æ ¼æ•°æ®çš„å…³é”®æ–¹é¢ï¼Œå¦‚æ–‡æœ¬ä¿¡æ¯ã€å¯å˜åˆ—æ•°å’Œé™¤åˆ—åå¤–çš„å…ƒæ•°æ®ä¿¡æ¯ä¸­çš„æœªçŸ¥æ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„â€”â€”åŸºç¡€è½¬æ¢å™¨ï¼ˆbasis transformersï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶å°Šé‡è¡¨æ ¼æ•°æ®ä¸­çš„å›ºæœ‰ä¸å˜æ€§ï¼ŒåŒ…æ‹¬å±‚æ¬¡ç»“æ„å’Œæ•°å€¼è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä»»åŠ¡è¡¨æ ¼å›å½’åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„è®¾è®¡ï¼Œåœ¨OpenML-CTR23åŸºå‡†æµ‹è¯•çš„34ä¸ªä»»åŠ¡ä¸­ï¼Œä¸­ä½æ•°RÂ²å¾—åˆ†æé«˜äº†0.338ï¼Œæ ‡å‡†åå·®æœ€ä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å‚æ•°æ•°é‡æ˜¯è¡¨ç°æœ€ä½³åŸºå‡†æ¨¡å‹çš„äº”åˆ†ä¹‹ä¸€ï¼Œç”šè‡³åœ¨éšæœºæƒé‡åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿè¶…è¿‡äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06926v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚éƒ¨åˆ†ä¿¡æ¯ã€å™ªå£°å’Œå¼‚è´¨ç»“æ„ç­‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„â€”â€”åŸºç¡€è½¬æ¢å™¨ã€‚è¯¥æ¶æ„èƒ½åŒæ—¶å¤„ç†è¡¨æ ¼æ•°æ®çš„å…³é”®æ–¹é¢ï¼Œå¦‚æ–‡æœ¬ä¿¡æ¯ã€å¯å˜åˆ—æ•°å’Œé™¤åˆ—åå¤–çš„æ— å…ƒæ•°æ®æœªçŸ¥æ•°æ®ï¼Œå¹¶å°Šé‡è¡¨æ ¼æ•°æ®å›ºæœ‰çš„ä¸å˜æ€§ï¼ŒåŒ…æ‹¬å±‚æ¬¡ç»“æ„å’Œæ•°å€¼è¡¨ç¤ºã€‚åœ¨å¤šé¡¹ä»»åŠ¡è¡¨æ ¼å›å½’åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°è¯¥è®¾è®¡ï¼Œåœ¨OpenML-CTR23åŸºå‡†æµ‹è¯•çš„34ä¸ªä»»åŠ¡ä¸­ï¼Œä¸­ä½æ•°RÂ²å¾—åˆ†æé«˜äº†0.338ï¼Œå¹¶ä¸”æ ‡å‡†åå·®æœ€ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹çš„å‚æ•°æ•°é‡æ˜¯æœ€ä½³åŸºå‡†æµ‹è¯•çš„äº”åˆ†ä¹‹ä¸€ï¼Œå³ä½¿åœ¨éšæœºæƒé‡åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿè¶…è¿‡äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼æ•°æ®å¤„ç†é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éƒ¨åˆ†ä¿¡æ¯ã€å™ªå£°å’Œå¼‚è´¨ç»“æ„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„â€”â€”åŸºç¡€è½¬æ¢å™¨ï¼Œä¸“é—¨è®¾è®¡æ¥å¤„ç†è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åŸºç¡€è½¬æ¢å™¨èƒ½åŒæ—¶å¤„ç†è¡¨æ ¼æ•°æ®çš„å¤šä¸ªå…³é”®æ–¹é¢ï¼Œå¦‚æ–‡æœ¬ä¿¡æ¯å’Œå¯å˜åˆ—æ•°ã€‚</li>
<li>è¯¥æ¶æ„å°Šé‡è¡¨æ ¼æ•°æ®çš„ä¸å˜æ€§ï¼ŒåŒ…æ‹¬å±‚æ¬¡ç»“æ„å’Œæ•°å€¼è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šé¡¹ä»»åŠ¡è¡¨æ ¼å›å½’åŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŸºç¡€è½¬æ¢å™¨å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸­ä½æ•°RÂ²å¾—åˆ†æé«˜äº†0.338ã€‚</li>
<li>åŸºç¡€è½¬æ¢å™¨çš„å‚æ•°æ•°é‡æ˜¯æœ€ä½³åŸºå‡†æµ‹è¯•çš„äº”åˆ†ä¹‹ä¸€ï¼Œæ•ˆç‡æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7752068e3f0478bca081a131a8c62c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea79c65822113505c98d1296f5fd8a89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fab8ae67f5d5254eebb233c665b90e36.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Private-GPTs-for-LLM-driven-testing-in-software-development-and-machine-learning"><a href="#Private-GPTs-for-LLM-driven-testing-in-software-development-and-machine-learning" class="headerlink" title="Private GPTs for LLM-driven testing in software development and machine   learning"></a>Private GPTs for LLM-driven testing in software development and machine   learning</h2><p><strong>Authors:Jakub Jagielski, Markus Abel</strong></p>
<p>In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple â€œHello Worldâ€ program and a digit classification model, showing that structured prompts lead to higher-quality test outputs. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ç§äººGPTæ ¹æ®éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•ä»£ç çš„èƒ½åŠ›ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä½œä¸ºè¾“å…¥é¡¹çš„éªŒæ”¶æ ‡å‡†ï¼Œè¿™äº›æ ‡å‡†è¢«åˆ¶å®šä¸ºå²è¯—æˆ–æ•…äº‹çš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸ç”¨äºç°ä»£å¼€å‘è¿‡ç¨‹ã€‚è¿™ä¸ºäº§å“æ‰€æœ‰è€…æˆ–å•†ä¸šæ™ºèƒ½æä¾›äº†ä¸€ç§é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥äº§ç”Ÿå¯æµ‹è¯•æ ‡å‡†çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§æ–¹å¼æ¢ç´¢æ‰€ç”Ÿæˆæµ‹è¯•çš„è´¨é‡ï¼ši) ç›´æ¥è®©å¤§å‹è¯­è¨€æ¨¡å‹ä»éœ€æ±‚ç”Ÿæˆä»£ç ï¼›ii) é€šè¿‡ä½¿ç”¨Gherkinè¯­æ³•çš„ä¸­é—´æ­¥éª¤ã€‚ç»“æœè¡¨æ˜ï¼Œä¸¤æ­¥ç¨‹åºäº§ç”Ÿæ›´å¥½çš„ç»“æœâ€”â€”æˆ‘ä»¬æ ¹æ®äººç±»å¯è¯»æ€§å’Œæœ€ä½³ç¼–ç å®è·µæ¥å®šä¹‰æ›´å¥½ï¼Œå³ä»£ç è¡Œæ•°ä»¥åŠæµ‹è¯•è¿‡ç¨‹ä¸­é€šå¸¸ä½¿ç”¨çš„é¢å¤–åº“çš„ç”¨é€”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä¸¤ç§åœºæ™¯ä¸‹è¯„ä¼°æç¤ºçš„æœ‰æ•ˆæ€§ï¼šä¸€ä¸ªç®€å•çš„â€œHello Worldâ€ç¨‹åºå’Œæ•°å­—åˆ†ç±»æ¨¡å‹ï¼Œæ˜¾ç¤ºç»“æ„åŒ–æç¤ºä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„æµ‹è¯•è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06509v1">PDF</a> 5 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç§äººGPTæ ¹æ®éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•ä»£ç çš„èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨ä½œä¸ºè¾“å…¥çš„è¦æ±‚æ ‡å‡†ï¼Œè¿™äº›æ ‡å‡†è¢«èå…¥å²è¯—æˆ–æ•…äº‹ä¸­ï¼Œè¿™åœ¨ç°ä»£å¼€å‘è¿‡ç¨‹ä¸­å¹¿æ³›ä½¿ç”¨ã€‚è¿™å…è®¸äº§å“ç»ç†æˆ–å•†ä¸šæ™ºèƒ½äººå‘˜é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥åˆ¶å®šå¯æµ‹è¯•çš„æ ‡å‡†ã€‚æœ¬æ–‡é€šè¿‡ä¸¤ç§æ–¹å¼æ¢è®¨äº†æ‰€ç”Ÿæˆçš„æµ‹è¯•çš„è´¨é‡ï¼šä¸€æ˜¯ç›´æ¥ä»éœ€æ±‚ä¸­è®©å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç ï¼ŒäºŒæ˜¯é€šè¿‡Gherkinè¯­æ³•è¿›è¡Œä¸­é—´æ­¥éª¤ã€‚ç»“æœè¡¨æ˜ï¼Œä¸¤æ­¥è¿‡ç¨‹èƒ½äº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œåœ¨å¯é˜…è¯»æ€§å’Œæœ€ä½³ç¼–ç å®è·µæ–¹é¢è¡¨ç°æ›´å¥½ã€‚é€šè¿‡å¯¹ä¸€ä¸ªç®€å•çš„â€œHello Worldâ€ç¨‹åºå’Œæ•°å­—åˆ†ç±»æ¨¡å‹çš„åœºæ™¯è¯„ä¼°ï¼Œå±•ç¤ºäº†ç»“æ„åŒ–æç¤ºå¯ä»¥äº§ç”Ÿæ›´é«˜è´¨é‡çš„æµ‹è¯•è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨ç§äººGPTè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ä»£ç çš„èƒ½åŠ›ï¼ŒåŸºäºèå…¥ç°ä»£å¼€å‘è¿‡ç¨‹ä¸­çš„å²è¯—æˆ–æ•…äº‹çš„éœ€æ±‚æ ‡å‡†ä½œä¸ºè¾“å…¥ã€‚</li>
<li>é€šè¿‡ä¸¤ç§æ–¹å¼è¯„ä¼°äº†æ‰€ç”Ÿæˆçš„æµ‹è¯•ä»£ç çš„è´¨é‡ï¼šç›´æ¥ä»éœ€æ±‚ç”Ÿæˆä»£ç å’Œä½¿ç”¨Gherkinè¯­æ³•ä½œä¸ºä¸­é—´æ­¥éª¤ã€‚</li>
<li>ä¸¤æ­¥è¿‡ç¨‹èƒ½å¤Ÿæ›´å¥½åœ°ç”Ÿæˆå¯é˜…è¯»çš„ä»£ç å¹¶éµå¾ªæœ€ä½³ç¼–ç å®è·µã€‚</li>
<li>ä½¿ç”¨ç»“æ„åŒ–æç¤ºå¯ä»¥æé«˜ç”Ÿæˆæµ‹è¯•ä»£ç çš„è´¨é‡ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªå®ä¾‹åœºæ™¯ï¼Œâ€œHello Worldâ€ç¨‹åºå’Œæ•°å­—åˆ†ç±»æ¨¡å‹éªŒè¯äº†GPTçš„å®é™…åº”ç”¨èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹èå…¥ä¸šåŠ¡æµç¨‹ä¸­ï¼Œäº§å“æ‰€æœ‰è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°åˆ¶å®šå¯æµ‹è¯•çš„æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a046be968b46553d8b5b6e1753788d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc7af55b530af7ce79310e43f1f8f665.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4318f785b9c10222aae83d9ae558936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb4cd5b7316c05506aacbf5ee53e36f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a0c2a3c1b8324008e5428c27f765ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aad51f4578c591ad99e6b381b1e45fbc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Text-to-LoRA-Instant-Transformer-Adaption"><a href="#Text-to-LoRA-Instant-Transformer-Adaption" class="headerlink" title="Text-to-LoRA: Instant Transformer Adaption"></a>Text-to-LoRA: Instant Transformer Adaption</h2><p><strong>Authors:Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange</strong></p>
<p>While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.   Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SakanaAI/text-to-lora">https://github.com/SakanaAI/text-to-lora</a> </p>
<blockquote>
<p>è™½ç„¶åŸºç¡€æ¨¡å‹ä¸ºå¿«é€Ÿå†…å®¹åˆ›å»ºæä¾›äº†é€šç”¨å·¥å…·ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé€‚åº”ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹æ¶‰åŠæ•°æ®é›†çš„ç²¾å¿ƒç­›é€‰å’Œåº•å±‚æ¨¡å‹çš„åå¤å¾®è°ƒã€‚å¾®è°ƒæŠ€æœ¯ä½¿å®è·µè€…èƒ½å¤Ÿä¸ºè®¸å¤šæ–°åº”ç”¨é€‚åº”åŸºç¡€æ¨¡å‹ï¼Œä½†éœ€è¦æ˜‚è´µä¸”è€—æ—¶çš„è®­ç»ƒï¼ŒåŒæ—¶å¯¹è¶…å‚æ•°é€‰æ‹©éå¸¸æ•æ„Ÿã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Text-to-LoRAï¼ˆT2Lï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå³æ—¶é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»…åŸºäºç›®æ ‡ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚T2Læ˜¯ä¸€ç§è¶…ç½‘ç»œï¼Œç»è¿‡è®­ç»ƒå¯æ„å»ºå•æ¬¡å»‰ä»·å‰å‘ä¼ é€’ä¸­çš„LoRAsã€‚åœ¨ä½¿ç”¨ä¸€å¥—9ä¸ªé¢„è®­ç»ƒLoRAé€‚é…å™¨ï¼ˆGSM8Kã€Arcç­‰ï¼‰å¯¹T2Lè¿›è¡Œè®­ç»ƒåï¼Œæˆ‘ä»¬æ˜¾ç¤ºï¼Œä¸´æ—¶æ„å»ºçš„LoRAå®ä¾‹åœ¨ç›¸åº”æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é€‚é…å™¨ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼ŒT2Lå¯ä»¥å‹ç¼©æ•°ç™¾ä¸ªLoRAå®ä¾‹å¹¶é›¶æ ·æœ¬æ³›åŒ–åˆ°å®Œå…¨æœªè§è¿‡çš„ä»»åŠ¡ã€‚è¿™ä¸€æ–¹æ³•ä¸ºæ°‘ä¸»åŒ–åŸºç¡€æ¨¡å‹çš„ä¸“ä¸šåŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå¹¶å®ç°äº†è¯­è¨€è‡ªé€‚åº”çš„æœ€å°è®¡ç®—éœ€æ±‚ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SakanaAI/text-to-lora%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SakanaAI/text-to-loraæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06105v2">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºç¡€æ¨¡å‹ä¸ºå¿«é€Ÿå†…å®¹åˆ›å»ºæä¾›äº†é€šç”¨å·¥å…·ï¼Œä½†é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé€‚åº”ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™éœ€è¦ç²¾å¿ƒæŒ‘é€‰æ•°æ®é›†å¹¶å¤šæ¬¡å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚è™½ç„¶å¾®è°ƒæŠ€æœ¯ä½¿ä»ä¸šè€…èƒ½å¤Ÿä¸ºè®¸å¤šæ–°åº”ç”¨é€‚åº”åŸºç¡€æ¨¡å‹ï¼Œä½†éœ€è¦æ˜‚è´µä¸”è€—æ—¶çš„è®­ç»ƒï¼Œå¹¶ä¸”å¯¹è¶…å‚æ•°é€‰æ‹©éå¸¸æ•æ„Ÿã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Text-to-LoRAï¼ˆT2Lï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿåœ¨é£è¡Œä¸­é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨¡å‹ï¼Œä»…åŸºäºç›®æ ‡ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°ã€‚T2Læ˜¯ä¸€ä¸ªè¶…ç½‘ç»œï¼Œç»è¿‡è®­ç»ƒå¯æ„é€ å•æ¬¡å»‰ä»·å‰å‘ä¼ é€’ä¸­çš„LoRAsã€‚åœ¨åŸºäºä¸€ç³»åˆ—é¢„è®­ç»ƒçš„LoRAé€‚é…å™¨ï¼ˆGSM8Kã€Arcç­‰ï¼‰è®­ç»ƒT2Låï¼Œæˆ‘ä»¬è¯æ˜ä¸´æ—¶æ„å»ºçš„LoRAå®ä¾‹åœ¨ç›¸åº”æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¸ç‰¹å®šä»»åŠ¡é€‚é…å™¨ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼ŒT2Lå¯ä»¥å‹ç¼©æ•°ç™¾ä¸ªLoRAå®ä¾‹ï¼Œå¹¶é›¶é•œå¤´æ³›åŒ–åˆ°å®Œå…¨æœªè§è¿‡çš„ä»»åŠ¡ã€‚è¿™ä¸€æ­¥éª¤æœç€æ°‘ä¸»åŒ–åŸºç¡€æ¨¡å‹çš„ä¸“é—¨åŒ–è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œå¹¶é€šè¿‡æœ€å°çš„è®¡ç®—éœ€æ±‚å®ç°äº†åŸºäºè¯­è¨€çš„é€‚åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹è™½é€šç”¨ä½†é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé€‚åº”ã€‚</li>
<li>ä¼ ç»Ÿé€‚åº”åŸºç¡€æ¨¡å‹çš„æ–¹æ³•éœ€è¦ç²¾å¿ƒæŒ‘é€‰æ•°æ®é›†å’Œå¤šæ¬¡å¾®è°ƒï¼Œè¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚</li>
<li>Text-to-LoRAï¼ˆT2Lï¼‰æ¨¡å‹èƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æè¿°ç›®æ ‡ä»»åŠ¡æ¥é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>T2Læ˜¯ä¸€ä¸ªè¶…ç½‘ç»œï¼Œå¯ä»¥åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­æ„é€ LoRAsã€‚</li>
<li>T2Lç»è¿‡ä¸€ç³»åˆ—é¢„è®­ç»ƒLoRAé€‚é…å™¨çš„è®­ç»ƒåï¼Œå…¶å®ä¾‹æ€§èƒ½ä¸ç‰¹å®šä»»åŠ¡é€‚é…å™¨ç›¸å½“ã€‚</li>
<li>T2Lèƒ½å‹ç¼©å¤šä¸ªLoRAå®ä¾‹å¹¶æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ã€‚</li>
<li>T2Lçš„å‡ºç°æ˜¯æœç€æ°‘ä¸»åŒ–åŸºç¡€æ¨¡å‹çš„ä¸“é—¨åŒ–è¿ˆè¿›çš„é‡è¦ä¸€æ­¥ï¼Œå¯å®ç°åŸºäºè¯­è¨€çš„å¿«é€Ÿé€‚åº”ä¸”è®¡ç®—éœ€æ±‚å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de23f7a28dc4ef9d973117d58c43c251.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c26749c0e1e150ac16d161f9e046d29a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87bb29df5db6c0b545f1a8a3a780f6bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95418855372cf02eb6118ec5fb173ec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d39e7411399fc346932ad3f15627b358.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping"><a href="#MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping" class="headerlink" title="MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping"></a>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</h2><p><strong>Authors:Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang</strong></p>
<p>Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¿›å±•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›çªç ´ä¸»è¦å¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼ï¼Œå®ƒåˆ©ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾å¤šæ¨¡æ€æ•°æ®ï¼Œç„¶ååœ¨ç²¾é€‰çš„æ ‡è®°æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¹¶ä½¿ç”¨é«˜è´¨é‡æç¤ºã€‚è™½ç„¶äººä»¬å¯¹æ‰©å¤§æŒ‡ä»¤å¾®è°ƒä»¥æ¶µç›–æ•°é‡å’Œè§„æ¨¡æ—¥ç›Šå¢å¤§çš„æ•°æ®é›†è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä»…å¢åŠ æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡çš„æ•°é‡å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ç›¸åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡è·¨æ¨¡æ€çš„é€šç”¨äº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¦‚å‘ç°å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆä½¿ç”¨å…·æœ‰ç‹¬ç‰¹ä¿¡æ¯çš„æ¨¡æ€é€‰æ‹©ï¼Œæˆ–éœ€è¦ååŒèåˆä»¥ä»ä¸¤ç§æ¨¡æ€ä¸­å‘ç°æ–°ä¿¡æ¯ï¼Œè¿™é¼“åŠ±æ¨¡å‹åœ¨ç»„å†…å­¦ä¹ å¯è¿ç§»æŠ€èƒ½ï¼ŒåŒæ—¶æŠ‘åˆ¶äº†ä¸åŒ¹é…ä»»åŠ¡çš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MINTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ç®€å•è€Œæœ‰æ•ˆçš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯¹äºå¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¤§å¤§ä¼˜äºç°æœ‰çš„ä»»åŠ¡åˆ†ç»„åŸºçº¿ï¼Œåœ¨é€šç”¨æ€§å’Œä¸“ä¸šåŒ–ä¹‹é—´è¾¾åˆ°äº†æœ‰æ•ˆå¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02308v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›çªç ´ä¸»è¦å¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼ï¼Œåˆ©ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾å¤šæ¨¡æ€æ•°æ®ï¼Œéšååœ¨ç²¾é€‰çš„æœ‰æ ‡ç­¾æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¹¶ä½¿ç”¨é«˜è´¨é‡æç¤ºã€‚ç ”ç©¶å‘ç°ï¼Œå•çº¯å¢åŠ æŒ‡ä»¤å¾®è°ƒçš„ä»»åŠ¡æ•°é‡å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ç›¸åï¼Œé€šè¿‡å…±åŒäº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¦‚å¯»æ‰¾å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆé€‰æ‹©ä¸ç‹¬ç‰¹ä¿¡æ¯åŒ¹é…çš„æ¨¡æ€æˆ–éœ€è¦ååŒèåˆä»¥å‘ç°ä¸¤ç§æ¨¡æ€ä¸­çš„æ–°ä¿¡æ¯ï¼Œèƒ½é¼“åŠ±æ¨¡å‹å­¦ä¹ ç»„å†…çš„å¯è½¬ç§»æŠ€èƒ½å¹¶å‡å°‘ä¸ç›¸å…³ä»»åŠ¡çš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„MINTä»»åŠ¡åˆ†ç»„ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¤§å¤§è¶…è¿‡äº†ç°æœ‰çš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡åˆ†ç»„åŸºçº¿ï¼Œåœ¨é€šç”¨æ€§å’Œä¸“ä¸šæ€§ä¹‹é—´å–å¾—äº†æœ‰æ•ˆå¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼å’Œå¤§è§„æ¨¡æ•°æ®çš„ä½¿ç”¨ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒæ˜¯æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ç¯èŠ‚ã€‚</li>
<li>å•çº¯å¢åŠ æŒ‡ä»¤å¾®è°ƒçš„ä»»åŠ¡æ•°é‡å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>ä»»åŠ¡åˆ†ç»„æ˜¯é‡è¦çš„ï¼Œåº”è¯¥æ ¹æ®å…±åŒäº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ã€‚</li>
<li>MINTä»»åŠ¡åˆ†ç»„ç­–ç•¥åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹ï¼Œèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>MINTç­–ç•¥åœ¨é€šç”¨æ€§å’Œä¸“ä¸šæ€§ä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa85d1264c0927d35083a71ff7966822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46c8b0d028b1b66475fde8cf7d352404.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36ccf88a5ad33d4372fec785079c6819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05e9fb874ab7c8d07b5597edadef3473.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Diversity-of-Transformer-Layers-One-Aspect-of-Parameter-Scaling-Laws"><a href="#Diversity-of-Transformer-Layers-One-Aspect-of-Parameter-Scaling-Laws" class="headerlink" title="Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws"></a>Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws</h2><p><strong>Authors:Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe</strong></p>
<p>Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layerâ€™s output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layersâ€™ outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study. </p>
<blockquote>
<p>Transformeræ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç°å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»è¦éª¨å¹²æ¶æ„ã€‚æœ€è¿‘å…³äºå‚æ•°ç¼©æ”¾å®šå¾‹çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡å¢åŠ å‚æ•°è§„æ¨¡ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜Transformerçš„ä»»åŠ¡è§£å†³æ€§èƒ½ã€‚å°½ç®¡æœ€è¿‘é€šè¿‡åˆ†ææ®‹å·®æµæ¥æ·±å…¥ç†è§£Transformerå†…éƒ¨è¡Œä¸ºçš„ç ”ç©¶åŠ æ·±äº†æˆ‘ä»¬å¯¹Transformerçš„ç†è§£ï¼Œä½†è¿™äº›å†…éƒ¨æœºåˆ¶ä¸å‚æ•°ç¼©æ”¾å®šå¾‹ä¹‹é—´çš„å…³ç³»ä»ä¸æ¸…æ¥šã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å…³æ³¨å±‚åŠå…¶å¤§å°ï¼Œè¿™ä¸»è¦å†³å®šäº†Transformerçš„å‚æ•°è§„æ¨¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹æ®‹å·®æµä¸­çš„å±‚è¿›è¡Œç†è®ºä¸Šçš„åç½®å¤šæ ·æ€§åˆ†è§£ç ”ç©¶ã€‚åˆ†è§£å°†ï¼ˆiï¼‰åç½®ä½œä¸ºæ¯å±‚è¾“å‡ºä¸åŸºå‡†å€¼çš„è¯¯å·®ï¼Œï¼ˆiiï¼‰å¤šæ ·æ€§ä½œä¸ºæŒ‡ç¤ºæ¯å±‚è¾“å‡ºå½¼æ­¤ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨è¿™ä¸€ç†è®ºä¸‹åˆ†æTransformerè¡¨æ˜ï¼Œå½“å„ä¸ªå±‚åšå‡ºçš„é¢„æµ‹æ¥è¿‘æ­£ç¡®ç­”æ¡ˆå¹¶ä¿æŒç›¸äº’å¤šæ ·æ€§æ—¶ï¼Œæ€§èƒ½ä¼šæé«˜ã€‚å½“å•ä¸ªå±‚çš„è¾“å‡ºè¿œç¦»åŸºå‡†å€¼æ—¶ï¼Œå¤šæ ·æ€§å˜å¾—å°¤ä¸ºå…³é”®ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥ä¿¡æ¯ç†è®ºå¤šæ ·æ€§å¹¶å±•ç¤ºæˆ‘ä»¬çš„ä¸»è¦å‘ç°ï¼šåªæœ‰åœ¨å„å±‚è¡¨ç°ä¸åŒå³å…·æœ‰å¤šæ ·æ€§æ—¶ï¼Œå¢åŠ å±‚æ•°æ‰èƒ½æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†é€šè¿‡å¢åŠ å±‚æ•°æ‰€å®ç°çš„æ€§èƒ½æ”¶ç›Šè¡¨ç°å‡ºäºšæ¨¡æ€§ï¼šéšç€é¢å¤–å±‚çš„å¢åŠ ï¼Œè¾¹é™…æ”¹è¿›é€æ¸å‡å°‘ï¼Œè¿™ä¸å‚æ•°ç¼©æ”¾å®šå¾‹é¢„æµ‹çš„æ—¥å¿—æ”¶æ•›ç›¸ä¸€è‡´ã€‚åœ¨å¤šä¸ªè¯­ä¹‰ç†è§£ä»»åŠ¡ä¸Šå¯¹å„ç§LLMè¿›è¡Œçš„å®éªŒå®è¯è¯å®äº†æœ¬ç ”ç©¶ä¸­å¾—å‡ºçš„ç†è®ºå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24009v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†Transformeråœ¨ä¸åŒä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œä»¥åŠå…¶ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»è¦æ¶æ„çš„ä¼˜åŠ¿ã€‚æ–‡ç« é€šè¿‡å‚æ•°è§„æ¨¡å®šå¾‹çš„ç ”ç©¶å±•ç¤ºäº†å¢åŠ å‚æ•°è§„æ¨¡èƒ½å¤Ÿæé«˜ä»»åŠ¡è§£å†³æ€§èƒ½ã€‚é€šè¿‡å¯¹Transformerå†…éƒ¨æœºåˆ¶çš„è§£è¯»ï¼Œæ–‡ç« åˆ†æäº†å…¶æ®‹å·®æµä¸­çš„å±‚çº§åŠå…¶å¤§å°å¯¹å‚æ•°è§„æ¨¡çš„å½±å“ã€‚é€šè¿‡åå·®å¤šæ ·æ€§å’Œç†è®ºåˆ†è§£çš„æ–¹æ³•ï¼Œç ”ç©¶å‘ç°å½“å„å±‚çº§é¢„æµ‹æ¥è¿‘æ­£ç¡®ç­”æ¡ˆå¹¶ä¿æŒç›¸äº’å¤šæ ·æ€§æ—¶ï¼Œæ€§èƒ½ä¼šæé«˜ã€‚æ­¤å¤–ï¼Œå½“å„å±‚çº§è¾“å‡ºè¿œç¦»çœŸå®ç­”æ¡ˆæ—¶ï¼Œå¤šæ ·æ€§å˜å¾—å°¤ä¸ºé‡è¦ã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä¿¡æ¯ç†è®ºå¤šæ ·æ€§ï¼Œå¹¶å‘ç°å¢åŠ å±‚çº§åªæœ‰åœ¨è¿™äº›å±‚çº§è¡¨ç°ä¸åŒæ—¶æ‰ä¼šæé«˜æ€§èƒ½ï¼Œå¹¶æ­ç¤ºæ€§èƒ½æå‡çš„äºšæ¨¡æ€§ç‰¹å¾ï¼šéšç€é¢å¤–å±‚çº§çš„å¢åŠ ï¼Œè¾¹é™…æ•ˆç›Šé€æ¸å‡å°‘ï¼Œè¿™ä¸å‚æ•°è§„æ¨¡å®šå¾‹é¢„æµ‹çš„æ—¥å¿—æ”¶æ•›ç›¸ç¬¦ã€‚é€šè¿‡å¤šé¡¹è¯­ä¹‰ç†è§£ä»»åŠ¡çš„å®éªŒéªŒè¯æœ¬æ–‡çš„ç†è®ºæˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformeræ¶æ„åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»è¦æ¶æ„ã€‚</li>
<li>é€šè¿‡å‚æ•°è§„æ¨¡å®šå¾‹ç ”ç©¶è¯æ˜äº†å¢åŠ å‚æ•°è§„æ¨¡èƒ½æå‡ä»»åŠ¡è§£å†³æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åå·®å¤šæ ·æ€§å’Œç†è®ºåˆ†è§£æ–¹æ³•åˆ†æäº†Transformerå†…éƒ¨å±‚çº§åŠå…¶å¤§å°å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>å½“å±‚çº§é¢„æµ‹æ¥è¿‘æ­£ç¡®ç­”æ¡ˆå¹¶ä¿æŒç›¸äº’å¤šæ ·æ€§æ—¶ï¼ŒTransformeræ€§èƒ½æœ€ä½³ã€‚</li>
<li>åœ¨å±‚çº§è¾“å‡ºè¿œç¦»çœŸå®ç­”æ¡ˆæ—¶ï¼Œå¤šæ ·æ€§çš„é‡è¦æ€§å°¤ä¸ºçªå‡ºã€‚</li>
<li>ä¿¡æ¯ç†è®ºå¤šæ ·æ€§çš„å¼•å…¥æ­ç¤ºäº†å¢åŠ å±‚çº§åªæœ‰åœ¨å®ƒä»¬è¡¨ç°ä¸åŒæ—¶æ‰èƒ½æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6161acc1e3b5b72e7d08a1afa7217403.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9d6c7942a814d9a52c18e069a6c8bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c85da6469ef4fdd9957ad1f98ef30a5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="When-Two-LLMs-Debate-Both-Think-Theyâ€™ll-Win"><a href="#When-Two-LLMs-Debate-Both-Think-Theyâ€™ll-Win" class="headerlink" title="When Two LLMs Debate, Both Think Theyâ€™ll Win"></a>When Two LLMs Debate, Both Think Theyâ€™ll Win</h2><p><strong>Authors:Pradyumna Shyama Prasad, Minh Nhat Nguyen</strong></p>
<p>Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed &gt;&#x3D;75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: modelsâ€™ private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLMs are now increasingly deployed without careful review in assistant and agentic roles.   Code for our experiments is available at <a target="_blank" rel="noopener" href="https://github.com/pradyuprasad/llms_overconfidence">https://github.com/pradyuprasad/llms_overconfidence</a> </p>
<blockquote>
<p>åœ¨é¢å¯¹åå¯¹æ„è§æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¦å‡†ç¡®åœ°è°ƒæ•´å…¶ä¿¡å¿ƒï¼Ÿæˆ‘ä»¬åœ¨é™æ€åŸºäºäº‹å®çš„é—®ç­”ä»»åŠ¡ä¸­æµ‹é‡äº†æ ¡å‡†çš„åŸºç¡€ä¸Šï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ¨æ€å¯¹æŠ—æ€§è¾©è®ºç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå¹¶ç‹¬ç‰¹åœ°ç»“åˆäº†ä¸¤ç§ç°å®å› ç´ ï¼šï¼ˆaï¼‰å¤šè½®å½¢å¼è¦æ±‚æ¨¡å‹éšç€æ–°ä¿¡æ¯çš„å‡ºç°è€Œæ›´æ–°ä¿¡å¿µï¼›ï¼ˆbï¼‰é›¶å’Œç»“æ„ä»¥æ§åˆ¶ä»»åŠ¡ç›¸å…³çš„ä¸ç¡®å®šæ€§ï¼Œå› ä¸ºç›¸äº’çš„é«˜ä¿¡å¿ƒå£°æ˜æ„å‘³ç€ç³»ç»Ÿæ€§çš„è¿‡åº¦è‡ªä¿¡ã€‚æˆ‘ä»¬ç»„ç»‡äº†åç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´è¿›è¡Œçš„ä¸ºæœŸä¸‰è½®çš„è¾©è®ºèµ›ï¼Œå¹¶è®©æ¨¡å‹åœ¨æ¯æ¬¡è¾©è®ºè½®æ¬¡åç§ä¸‹è¯„ä¼°ä»–ä»¬è·èƒœçš„ä¿¡å¿ƒï¼ˆ0-100%ï¼‰ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°äº”ä¸ªä»¤äººæ‹…å¿§çš„æ¨¡å¼ï¼šï¼ˆ1ï¼‰ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ï¼šæ¨¡å‹è¾©è®ºçš„åˆå§‹å¹³å‡ä¿¡å¿ƒä¸º72.9%ï¼Œè€Œç†æ€§åŸºçº¿ä¸º50%ã€‚ï¼ˆ2ï¼‰ä¿¡å¿ƒå‡çº§ï¼šè¾©è®ºè€…å¹¶æ²¡æœ‰éšç€è¾©è®ºçš„è¿›è¡Œè€Œé™ä½ä¿¡å¿ƒï¼Œåè€Œå¢åŠ äº†è·èƒœçš„æ¦‚ç‡ï¼Œåˆ°æœ€åä¸€è½®å¹³å‡è¾¾åˆ°83%ã€‚ï¼ˆ3ï¼‰ç›¸äº’é«˜ä¼°ï¼šåœ¨61.7%çš„è¾©è®ºä¸­ï¼ŒåŒæ–¹åŒæ—¶å£°ç§°èƒœåˆ©çš„å¯èƒ½æ€§å¤§äºæˆ–ç­‰äº75%ï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„ä¸å¯èƒ½äº‹ä»¶ã€‚ï¼ˆ4ï¼‰æŒç»­çš„è‡ªæˆ‘è¾©è®ºåè§ï¼šè¾©è®ºç›¸åŒå‰¯æœ¬çš„æ¨¡å‹ä¿¡å¿ƒä»64.1%å¢åŠ åˆ°75.2%ï¼›å³ä½¿æ˜ç¡®å‘ŠçŸ¥ä»–ä»¬è·èƒœçš„æœºä¼šæ­£å¥½æ˜¯50%ï¼Œä¿¡å¿ƒä»ç„¶ä¸Šå‡ï¼ˆä»50.0%ä¸Šå‡åˆ°57.1%ï¼‰ã€‚ï¼ˆ5ï¼‰ç§äººçš„æ¨ç†ä¸ä¸€è‡´ï¼šæ¨¡å‹çš„ç§å¯†æ¶‚é¸¦ç¬”è®°æœ‰æ—¶ä¸å…¶å…¬å¼€çš„ä¿¡å¿ƒè¯„çº§ä¸åŒï¼Œè¿™å¼•å‘äº†å…³äºæ€ç»´é“¾æ¨ç†çœŸå®æ€§çš„æ‹…å¿§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMç¼ºä¹åœ¨åŠ¨æ€å¤šè½®ä»»åŠ¡ä¸­å‡†ç¡®è‡ªæˆ‘è¯„ä¼°æˆ–æ›´æ–°å…¶ä¿¡å¿µçš„èƒ½åŠ›ï¼›è¿™æ˜¯ä¸€ä¸ªé‡å¤§æ‹…å¿§ï¼Œå› ä¸ºè¶Šæ¥è¶Šå¤šçš„LLMåœ¨æœªç»è¿‡è°¨æ…å®¡æŸ¥çš„æƒ…å†µä¸‹è¢«éƒ¨ç½²åˆ°åŠ©ç†å’Œä»£ç†è§’è‰²ä¸­ã€‚æˆ‘ä»¬çš„å®éªŒä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pradyuprasad/llms_overconfidence%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/pradyuprasad/llms_overconfidenceæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19184v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŠ¨æ€å¯¹æŠ—æ€§è¾©è®ºç¯å¢ƒä¸­çš„ä¿¡å¿ƒè°ƒæ•´èƒ½åŠ›ç ”ç©¶ã€‚ç ”ç©¶å‘ç°LLMå­˜åœ¨ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ã€ä¿¡å¿ƒé€’å¢ã€ç›¸äº’é«˜ä¼°ã€æŒç»­è‡ªæˆ‘è¾©è®ºåè§å’Œç§äººæ¨ç†å¤±è°ƒç­‰é—®é¢˜ã€‚è¿™äº›é—®é¢˜è¡¨æ˜LLMåœ¨åŠ¨æ€å¤šè½®ä»»åŠ¡ä¸­å‡†ç¡®è‡ªæˆ‘è¯„ä¼°æˆ–æ›´æ–°ä¿¡å¿µçš„èƒ½åŠ›å­˜åœ¨ä¸è¶³ï¼Œå¯¹äºåœ¨åŠ©ç†å’Œä»£ç†è§’è‰²ä¸­æœªç»ä»”ç»†å®¡æŸ¥å³éƒ¨ç½²LLMçš„ä¸»è¦å…³æ³¨ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs åœ¨åŠ¨æ€ã€å¤šå›åˆçš„è¾©è®ºç¯å¢ƒä¸­ï¼Œå­˜åœ¨ç³»ç»Ÿæ€§è¿‡åº¦è‡ªä¿¡ï¼Œåˆå§‹ä¿¡å¿ƒå¹³å‡å€¼ä¸º72.9%ï¼Œè¿œé«˜äºç†æ€§åŸºçº¿50%ã€‚</li>
<li>LLMs çš„ä¿¡å¿ƒä¼šéšç€è¾©è®ºçš„è¿›è¡Œè€Œé€’å¢ï¼Œè€Œéå‡å°‘ï¼Œåœ¨æœ€åä¸€è½®å¹³å‡è¾¾åˆ°83%ã€‚</li>
<li>åœ¨61.7%çš„è¾©è®ºä¸­ï¼ŒåŒæ–¹éƒ½åŒæ—¶å£°ç§°èƒœåˆ©æ¦‚ç‡å¤§äºç­‰äº75%ï¼Œè¿™æ˜¯ä¸€ä¸ªé€»è¾‘ä¸Šçš„ä¸å¯èƒ½æƒ…å†µã€‚</li>
<li>LLMs åœ¨ä¸è‡ªèº«å‰¯æœ¬è¾©è®ºæ—¶ï¼Œä¿¡å¿ƒä»64.1%å¢åŠ åˆ°75.2%ï¼Œå³ä½¿æ˜ç¡®å‘ŠçŸ¥å…¶è·èƒœæœºä¼šä¸º50%ï¼Œä¿¡å¿ƒä»ç„¶ä¸Šå‡ã€‚</li>
<li>LLMs çš„ç§äººæ¨ç†ä¸å…¶å…¬å¼€ä¿¡å¿ƒè¯„çº§æœ‰æ—¶å­˜åœ¨ä¸ä¸€è‡´ï¼Œè¿™å¼•å‘äº†å¯¹å…¶æ€ç»´è¿è´¯æ€§çš„æ‹…å¿§ã€‚</li>
<li>LLMs åœ¨åŠ¨æ€ã€å¤šå›åˆçš„ä»»åŠ¡ä¸­ç¼ºä¹å‡†ç¡®è‡ªæˆ‘è¯„ä¼°å’Œæ›´æ–°ä¿¡å¿µçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84ab06caf01ae6dad32428e976a003dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b48b815213773a3e48fcc18bc0db90bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfa1b70284266a09f09607e876160257.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ebaf1e78042dfde9592658f265d87b81.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Supporting Construction Worker Well-Being with a Multi-Agent   Conversational AI System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1c2b999d7deebf24c2faaf6297258a56.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Play to Generalize Learning to Reason Through Game Play
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
