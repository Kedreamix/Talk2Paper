<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Play to Generalize Learning to Reason Through Game Play">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1c2b999d7deebf24c2faaf6297258a56.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="Play-to-Generalize-Learning-to-Reason-Through-Game-Play"><a href="#Play-to-Generalize-Learning-to-Reason-Through-Game-Play" class="headerlink" title="Play to Generalize: Learning to Reason Through Game Play"></a>Play to Generalize: Learning to Reason Through Game Play</h2><p><strong>Authors:Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei</strong></p>
<p>Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base modelâ€™s performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å‘å±•å¯æ¨å¹¿çš„æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å—è®¤çŸ¥ç§‘å­¦æ–‡çŒ®çš„å¯å‘ï¼Œè¯¥æ–‡çŒ®è¡¨æ˜æ¸¸æˆå¯ä»¥ä¿ƒè¿›å¯è¿ç§»çš„è®¤çŸ¥æŠ€èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒèŒƒå¼ï¼Œå³è§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ã€‚åœ¨æ­¤èŒƒå¼ä¸­ï¼ŒMLLMsé€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆæ¥å‘å±•è·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å…·æœ‰7Bå‚æ•°çš„MLLMè¿›è¡Œåè®­ç»ƒï¼Œä½¿å…¶åœ¨ç®€å•çš„ç±»ä¼¼è¡—æœºæ¸¸æˆï¼ˆå¦‚Snakeï¼‰ä¸Šçš„è¡¨ç°å¾—åˆ°æ˜¾è‘—æå‡ã€‚é‡è¦çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVistaå’Œå¤šå­¦ç§‘é—®é¢˜MMMUï¼‰ä¸Šçš„è¡¨ç°å¾—åˆ°æ˜¾è‘—å¢å¼ºï¼Œä¸”åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­æ— éœ€æŸ¥çœ‹ä»»ä½•è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ï¼Œè¿™è¡¨æ˜è¯¥æ¨¡å‹æŒæ¡äº†å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºä¸“é—¨åœ¨å¤šæ¨¡æ€æ¨ç†æ•°æ®ä¸Šè°ƒæ•´è¿‡çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†åŸºç¡€æ¨¡å‹åœ¨ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚è¿™æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºä¸“ä¸šæ¨¡å‹é€šå¸¸åœ¨è¿™æ–¹é¢è¡¨ç°ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†ä¸€ç§æ–°å‹åè®­ç»ƒèŒƒå¼ï¼šåˆæˆã€åŸºäºè§„åˆ™çš„æ¸¸æˆå¯ä»¥ä½œä¸ºå¯æ§å’Œå¯æ‰©å±•çš„é¢„æ–‡æœ¬ä»»åŠ¡ï¼Œè§£é”MLLMä¸­çš„é€šç”¨å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08011v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yunfeixie233.github.io/ViGaL/">https://yunfeixie233.github.io/ViGaL/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¤çŸ¥ç§‘å­¦æ–‡çŒ®å¯¹æ¸¸æˆå¯ä»¥ä¿ƒè¿›è½¬ç§»è®¤çŸ¥æŠ€èƒ½çš„è§‚ç‚¹ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§åä¸ºViGaLï¼ˆè§†è§‰æ¸¸æˆå­¦ä¹ ï¼‰çš„æ–°å‹è®­ç»ƒåèŒƒå¼ã€‚åœ¨æ­¤èŒƒå¼ä¸‹ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯ä»¥é€šè¿‡ç±»ä¼¼æ¸¸æˆçš„å­¦ä¹ ï¼Œæé«˜éåŸŸå¤–æ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä»¥æ¨¡ä»¿ç”µå­æ¸¸æˆçš„æ¨¡å¼ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å«æœ‰7äº¿å‚æ•°çš„MLLMè¿›è¡Œè®­ç»ƒåå¤„ç†ï¼Œå‘ç°æ¨¡å‹åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æˆç»©ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»ä¼¼äºMathVistaè¿™æ ·çš„æµ‹è¯•æ•°å­¦çŸ¥è¯†å’Œç»¼åˆé¢˜ä¸­å±•ç°çªå‡ºä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå€¼å¾—ä¸€æçš„æ˜¯ï¼Œå³ä¾¿æ²¡æœ‰æ¶‰åŠå…·ä½“è§£é¢˜æŠ€å·§ã€å…¬å¼æˆ–å›¾è¡¨ç­‰ä¸“é¡¹è®­ç»ƒï¼Œæ¨¡å‹ä¹Ÿèƒ½æŒæ¡å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ã€‚è¿™ä¸€å‘ç°ä¸ºæ–°çš„è®­ç»ƒåèŒƒå¼æä¾›äº†å¯ç¤ºï¼šåˆæˆè§„åˆ™çš„æ¸¸æˆå¯ä»¥ä½œä¸ºå¯æ§ä¸”å¯æ‰©å±•çš„é¢„æ–‡æœ¬ä»»åŠ¡ï¼Œè§£é”MLLMä¸­çš„æ³›åŒ–å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡ç»è¿‡ä¸“é¡¹è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºVisual Game Learningï¼ˆViGaLï¼‰èŒƒå¼ç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¯¹MLLMè¿›è¡Œç±»ä¼¼æ¸¸æˆçš„è®­ç»ƒå¤„ç†ï¼Œæ— éœ€ä¸“é—¨é’ˆå¯¹ç‰¹å®šçš„è§£é¢˜æ–¹æ³•æˆ–å­¦ç§‘è¿›è¡ŒæŒ‡å¯¼ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜è®­ç»ƒåçš„æ¨¡å‹åœ¨ç±»ä¼¼äºMathVistaçš„æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè¯´æ˜è½¬ç§»æŠ€èƒ½çš„å½¢æˆæœºåˆ¶æ˜¾è‘—æœ‰æ•ˆã€‚</li>
<li>ViGaLæ–¹æ³•å¯ä»¥å¹¿æ³›åº”ç”¨äºä¸åŒç±»å‹çš„æ¸¸æˆç¯å¢ƒï¼Œå¢å¼ºæ¨¡å‹çš„é€‚åº”æ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>è®­ç»ƒåçš„æ¨¡å‹åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç»è¿‡ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå¯¹ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•çš„åŸæœ‰æ€§èƒ½æ°´å¹³ã€‚</li>
<li>æ¸¸æˆå­¦ä¹ èŒƒå¼ä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’å’Œæ€è·¯ã€‚è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶å¯ç»§ç»­æ¢ç´¢ä¸åŒç±»å‹æ¸¸æˆçš„æ•ˆç”¨ä»¥åŠå…¶ä¸è®¤çŸ¥èƒ½åŠ›çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6f2d9ca54356dd054737efcf1d47d6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac153d76d43fe736753dec1f0695097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c32a2e629cb32b21e18efcf432294d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OneIG-Bench-Omni-dimensional-Nuanced-Evaluation-for-Image-Generation"><a href="#OneIG-Bench-Omni-dimensional-Nuanced-Evaluation-for-Image-Generation" class="headerlink" title="OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation"></a>OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation</h2><p><strong>Authors:Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, Hai-Bao Chen</strong></p>
<p>Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å› èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬æç¤ºå¯¹é½çš„é«˜è´¨é‡å›¾åƒè€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼ŒT2Iæ¨¡å‹çš„å¿«é€Ÿå‘å±•æ˜¾ç¤ºå‡ºæ—©æœŸåŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œç¼ºä¹å…¨é¢è¯„ä¼°ï¼Œä¾‹å¦‚å¯¹æ¨ç†ã€æ–‡æœ¬æ¸²æŸ“å’Œé£æ ¼çš„è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€è¿‘çš„æœ€å…ˆè¿›æ¨¡å‹å‡­å€Ÿå…¶ä¸°å¯Œçš„çŸ¥è¯†å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨éœ€è¦å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å›¾åƒç”Ÿæˆé—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†ç°æœ‰çš„è¯„ä¼°ç³»ç»Ÿå°šæœªå……åˆ†è§£å†³è¿™ä¸€å‰æ²¿é—®é¢˜ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OneIG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ç»¼åˆåŸºå‡†æ¡†æ¶ï¼Œç”¨äºå¯¹T2Iæ¨¡å‹è¿›è¡Œè·¨å¤šä¸ªç»´åº¦çš„ç²¾ç»†è¯„ä¼°ï¼ŒåŒ…æ‹¬æç¤ºå›¾åƒå¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ç²¾åº¦ã€æ¨ç†ç”Ÿæˆå†…å®¹ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ç»“æ„åŒ–è¯„ä¼°ï¼Œæ­¤åŸºå‡†æµ‹è¯•èƒ½å¤Ÿæ·±å…¥åˆ†ææ¨¡å‹æ€§èƒ½ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå®è·µè€…ç¡®å®šå›¾åƒç”Ÿæˆæ•´ä¸ªæµç¨‹ä¸­çš„ä¼˜åŠ¿å’Œç“¶é¢ˆã€‚ç‰¹åˆ«æ˜¯ï¼ŒOneIG-Benché€šè¿‡å…è®¸ç”¨æˆ·å…³æ³¨ç‰¹å®šçš„è¯„ä¼°å­é›†æ¥å®ç°çµæ´»è¯„ä¼°ã€‚ç”¨æˆ·æ— éœ€ä¸ºæ•´ä¸ªæç¤ºé›†ç”Ÿæˆå›¾åƒï¼Œè€Œåªéœ€ä¸ºä¸æ‰€é€‰ç»´åº¦ç›¸å…³çš„æç¤ºç”Ÿæˆå›¾åƒï¼Œå¹¶ç›¸åº”åœ°å®Œæˆè¯„ä¼°ã€‚æˆ‘ä»¬çš„ä»£ç åº“å’Œæ•°æ®é›†ç°å·²å…¬å¼€å¯ç”¨ï¼Œä»¥ä¿ƒè¿›T2Iç ”ç©¶ç¤¾åŒºå†…çš„å¯é‡å¤è¯„ä¼°ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07977v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„é‡è¦æ€§åŠå…¶å±€é™æ€§ï¼Œå¼ºè°ƒäº†ç°æœ‰è¯„ä¼°ç³»ç»Ÿçš„ä¸è¶³ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªåä¸ºOneIG-Benchçš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹T2Iæ¨¡å‹è¿›è¡Œç²¾ç»†è¯„ä¼°ï¼ŒåŒ…æ‹¬æç¤ºå›¾åƒå¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ç²¾åº¦ã€æ¨ç†ç”Ÿæˆå†…å®¹ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ç­‰å¤šä¸ªç»´åº¦ã€‚OneIG-Benchå…è®¸ç”¨æˆ·çµæ´»è¯„ä¼°ï¼Œåªé’ˆå¯¹æ‰€é€‰ç»´åº¦çš„æç¤ºç”Ÿæˆå›¾åƒå¹¶å®Œæˆç›¸åº”è¯„ä¼°ã€‚æ­¤ä»£ç åº“å’Œæ•°æ®é›†ç°å·²å…¬å¼€ï¼Œä»¥æ¨åŠ¨T2Iç ”ç©¶ç¤¾åŒºçš„å¯é‡å¤æ€§è¯„ä»·ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨ç”Ÿæˆä¸æ–‡æœ¬æç¤ºå¯¹é½çš„é«˜è´¨é‡å›¾åƒæ–¹é¢å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ç³»ç»Ÿå¯¹äºT2Iæ¨¡å‹çš„å…¨é¢è¯„ä»·å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ã€æ–‡æœ¬æ¸²æŸ“å’Œé£æ ¼æ–¹é¢çš„è¯„ä»·ã€‚</li>
<li>OneIG-Benchæ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°T2Iæ¨¡å‹çš„æ¡†æ¶ï¼Œæ¶µç›–å¤šä¸ªç»´åº¦ï¼Œå¦‚æç¤ºå›¾åƒå¯¹é½ã€æ–‡æœ¬æ¸²æŸ“ç²¾åº¦ã€æ¨ç†ç”Ÿæˆå†…å®¹ã€é£æ ¼åŒ–å’Œå¤šæ ·æ€§ã€‚</li>
<li>OneIG-Benchå…è®¸ç”¨æˆ·çµæ´»è¯„ä¼°ï¼Œåªé’ˆå¯¹ç‰¹å®šè¯„ä¼°å­é›†è¿›è¡Œå›¾åƒç”Ÿæˆå’Œè¯„ä¼°ã€‚</li>
<li>OneIG-Benchçš„å‡ºç°æ˜¯ä¸ºäº†è§£å†³ç°æœ‰è¯„ä¼°ç³»ç»Ÿçš„ä¸è¶³ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ›´æ·±å…¥åœ°äº†è§£æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>OneIG-Benchçš„å…¬å¼€æ•°æ®é›†å’Œä»£ç åº“ä¿ƒè¿›äº†T2Iç ”ç©¶ç¤¾åŒºçš„å¯é‡å¤æ€§è¯„ä»·ç ”ç©¶å’Œè·¨æ¨¡å‹æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98f0f4c3515f5091d61880c7a5d3aec8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0113c71ca059e049e83bffe5bfab23d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e01425d833bd91bac164d150ac5c289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e10a45bd31d48192069449dd366c0c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Thinking-vs-Doing-Agents-that-Reason-by-Scaling-Test-Time-Interaction"><a href="#Thinking-vs-Doing-Agents-that-Reason-by-Scaling-Test-Time-Interaction" class="headerlink" title="Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction"></a>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</h2><p><strong>Authors:Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar</strong></p>
<p>The current paradigm of test-time scaling relies on generating long reasoning traces (â€œthinkingâ€ more) before producing a response. In agent problems that require interaction, this can be done by generating thinking traces before acting in the world. However, this process does not allow agents to acquire new information from the environment or adapt their behavior over time. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agentâ€™s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout. To demonstrate the promise of this scaling dimension, we study the domain of web agents. We first show that even prompting-based interaction scaling without any training can improve task success on web benchmarks non-trivially. Building on this, we introduce TTI (Test-Time Interaction), a curriculum-based online reinforcement learning (RL) approach that trains agents by adaptively adjusting their rollout lengths. Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data web agents on WebVoyager and WebArena benchmarks. We further show that TTI enables agents to balance exploration and exploitation adaptively. Our results establish interaction scaling as a powerful, complementary axis to scaling per-step compute, offering new avenues for training adaptive agents. </p>
<blockquote>
<p>å½“å‰æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„æ¨¡å¼ä¾èµ–äºåœ¨äº§ç”Ÿå›åº”ä¹‹å‰ç”Ÿæˆé•¿çš„æ¨ç†è½¨è¿¹ï¼ˆå³â€œæ€è€ƒâ€æ›´å¤šï¼‰ã€‚åœ¨éœ€è¦äº¤äº’çš„ä»£ç†é—®é¢˜ä¸­ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨ä¸–ç•Œä¸­è¡ŒåŠ¨ä¹‹å‰ç”Ÿæˆæ€è€ƒè½¨è¿¹æ¥å®Œæˆã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸å…è®¸ä»£ç†ä»ç¯å¢ƒä¸­è·å–æ–°ä¿¡æ¯ï¼Œä¹Ÿä¸èƒ½éšç€æ—¶é—´çš„æ¨ç§»æ”¹å˜å®ƒä»¬çš„è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºæ‰©å¤§æµ‹è¯•æ—¶é—´äº¤äº’ï¼Œè¿™æ˜¯æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„ä¸€ä¸ªå°šæœªå¼€å‘çš„ç»´åº¦ï¼Œå®ƒå°†ä»£ç†çš„äº¤äº’èŒƒå›´æ‰©å¤§åˆ°åœ¨ä¸€æ¬¡è¿è¡Œä¸­æ‰§è¡Œä¸°å¯Œçš„è¡Œä¸ºï¼Œå¦‚æ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€é‡æ–°è§„åˆ’ã€‚ä¸ºäº†è¯æ˜è¿™ä¸€ç¼©æ”¾ç»´åº¦çš„æ½œåŠ›ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç½‘é¡µä»£ç†é¢†åŸŸã€‚æˆ‘ä»¬é¦–å…ˆè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹åŸºäºæç¤ºçš„äº¤äº’ç¼©æ”¾ä¹Ÿå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜ç½‘é¡µåŸºå‡†æµ‹è¯•çš„ä»»åŠ¡æˆåŠŸç‡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†TTIï¼ˆæµ‹è¯•æ—¶é—´äº¤äº’ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¯¾ç¨‹çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´è¿è¡Œé•¿åº¦æ¥è®­ç»ƒä»£ç†ã€‚ä½¿ç”¨Gemma 3 12Bæ¨¡å‹ï¼ŒTTIåœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­äº§ç”Ÿäº†æœ€å…ˆè¿›çš„å¼€æºå¼€æ”¾æ•°æ®ç½‘é¡µä»£ç†ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒTTIä½¿ä»£ç†èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†äº¤äº’ç¼©æ”¾ä½œä¸ºå¼ºå¤§çš„ã€ä¸æ¯æ­¥è®¡ç®—ç¼©æ”¾ç›¸è¾…ç›¸æˆçš„è½´ï¼Œä¸ºè®­ç»ƒè‡ªé€‚åº”ä»£ç†æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong>ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07976v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æµ‹è¯•æ—¶äº¤äº’æ‰©å±•çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­çš„äº’åŠ¨èƒ½åŠ›ï¼Œå®ç°ä¸°å¯Œè¡Œä¸ºå¦‚æ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€è§„åˆ’ç­‰ã€‚é€šè¿‡è°ƒæ•´æ™ºèƒ½ä½“çš„äº¤äº’èŒƒå›´å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†æ™ºèƒ½ä½“åœ¨WebåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•å¯ä½¿æ™ºèƒ½ä½“é€‚åº”ç¯å¢ƒæ–°ä¿¡æ¯ï¼Œå¹¶åœ¨äº’åŠ¨ä¸­å®ç°è‡ªé€‚åº”å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚æ€»ä¹‹ï¼Œäº¤äº’æ‰©å±•å·²æˆä¸ºè®¡ç®—èƒ½åŠ›çš„å¼ºå¤§è¡¥å……ç»´åº¦ï¼Œä¸ºè®­ç»ƒè‡ªé€‚åº”æ™ºèƒ½ä½“æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶äº¤äº’æ‰©å±•å…è®¸æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­å®ç°æ›´ä¸°å¯Œè¡Œä¸ºï¼Œå¦‚æ¢ç´¢ã€å›æº¯å’ŒåŠ¨æ€è§„åˆ’ç­‰ã€‚</li>
<li>é€šè¿‡è°ƒæ•´æ™ºèƒ½ä½“çš„äº¤äº’èŒƒå›´ï¼Œå¯æé«˜å…¶åœ¨WebåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚</li>
<li>TTIï¼ˆæµ‹è¯•æ—¶äº¤äº’ï¼‰æ˜¯ä¸€ç§åŸºäºåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ–¹æ³•ï¼Œå¯é€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ™ºèƒ½ä½“çš„è¯•è¿è¡Œæ—¶é•¿æ¥æé«˜å…¶è¡¨ç°ã€‚</li>
<li>ä½¿ç”¨Gemma 3 12Bæ¨¡å‹çš„TTIæ–¹æ³•åœ¨WebVoyagerå’ŒWebArenaåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°å¼€æºçŠ¶æ€çš„è¡¨ç°ã€‚</li>
<li>TTIä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªé€‚åº”å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚</li>
<li>äº¤äº’æ‰©å±•å·²æˆä¸ºè®¡ç®—èƒ½åŠ›çš„ä¸€ä¸ªå¼ºå¤§è¡¥å……ç»´åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5cab904783bc19f772d7eb766a54ff45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c5a90370d605ca6f1cbe84755e6eb36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3895e122cd82008845075175bef98d46.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Decoupling-the-Image-Perception-and-Multimodal-Reasoning-for-Reasoning-Segmentation-with-Digital-Twin-Representations"><a href="#Decoupling-the-Image-Perception-and-Multimodal-Reasoning-for-Reasoning-Segmentation-with-Digital-Twin-Representations" class="headerlink" title="Decoupling the Image Perception and Multimodal Reasoning for Reasoning   Segmentation with Digital Twin Representations"></a>Decoupling the Image Perception and Multimodal Reasoning for Reasoning   Segmentation with Digital Twin Representations</h2><p><strong>Authors:Yizhen Li, Dell Zhang, Xuelong Li, Yiqing Shen</strong></p>
<p>Reasoning Segmentation (RS) is a multimodal vision-text task that requires segmenting objects based on implicit text queries, demanding both precise visual perception and vision-text reasoning capabilities. Current RS approaches rely on fine-tuning vision-language models (VLMs) for both perception and reasoning, but their tokenization of images fundamentally disrupts continuous spatial relationships between objects. We introduce DTwinSeger, a novel RS approach that leverages Digital Twin (DT) representation as an intermediate layer to decouple perception from reasoning. Innovatively, DTwinSeger reformulates RS as a two-stage process, where the first transforms the image into a structured DT representation that preserves spatial relationships and semantic properties and then employs a Large Language Model (LLM) to perform explicit reasoning over this representation to identify target objects. We propose a supervised fine-tuning method specifically for LLM with DT representation, together with a corresponding fine-tuning dataset Seg-DT, to enhance the LLMâ€™s reasoning capabilities with DT representations. Experiments show that our method can achieve state-of-the-art performance on two image RS benchmarks and three image referring segmentation benchmarks. It yields that DT representation functions as an effective bridge between vision and text, enabling complex multimodal reasoning tasks to be accomplished solely with an LLM. </p>
<blockquote>
<p>æ¨ç†åˆ†å‰²ï¼ˆRSï¼‰æ˜¯ä¸€é¡¹å¤šæ¨¡æ€çš„è§†è§‰æ–‡æœ¬ä»»åŠ¡ï¼Œå®ƒè¦æ±‚æ ¹æ®éšå¼æ–‡æœ¬æŸ¥è¯¢å¯¹å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œéœ€è¦ç²¾ç¡®çš„è§†è§‰æ„ŸçŸ¥å’Œè§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚å½“å‰çš„RSæ–¹æ³•ä¾èµ–äºå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¾®è°ƒæ¥è¿›è¡Œæ„ŸçŸ¥å’Œæ¨ç†ï¼Œä½†å®ƒä»¬çš„å›¾åƒæ ‡è®°æ–¹å¼ä¼šç ´åå¯¹è±¡ä¹‹é—´è¿ç»­çš„ç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬å¼•å…¥äº†DTwinSegerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RSæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºä½œä¸ºä¸­é—´å±‚ï¼Œå°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦ã€‚åˆ›æ–°çš„æ˜¯ï¼ŒDTwinSegerå°†RSé‡æ–°å®šä¹‰ä¸ºä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ï¼Œå…¶ä¸­ç¬¬ä¸€é˜¶æ®µå°†å›¾åƒè½¬æ¢ä¸ºç»“æ„åŒ–çš„DTè¡¨ç¤ºï¼Œä¿ç•™ç©ºé—´å…³ç³»å’Œè¯­ä¹‰å±æ€§ï¼Œç„¶åé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ­¤è¡¨ç¤ºè¿›è¡Œæ˜ç¡®çš„æ¨ç†ï¼Œä»¥è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹LLMçš„DTè¡¨ç¤ºè¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ï¼Œå¹¶é…ä»¥ç›¸åº”çš„å¾®è°ƒæ•°æ®é›†Seg-DTï¼Œä»¥å¢å¼ºLLMä½¿ç”¨DTè¡¨ç¤ºçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤é¡¹å›¾åƒRSåŸºå‡†æµ‹è¯•å’Œä¸‰é¡¹å›¾åƒå¼•ç”¨åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜DTè¡¨ç¤ºä½œä¸ºè§†è§‰å’Œæ–‡æœ¬ä¹‹é—´çš„æœ‰æ•ˆæ¡¥æ¢ï¼Œèƒ½å¤Ÿä»…ä½¿ç”¨LLMå®Œæˆå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07943v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤šæ¨¡æ€è§†è§‰æ–‡æœ¬ä»»åŠ¡ä¸­çš„æ¨ç†åˆ†å‰²ï¼ˆRSï¼‰éœ€è¦åŸºäºéšæ–‡æœ¬æŸ¥è¯¢è¿›è¡Œå¯¹è±¡åˆ†å‰²ï¼Œè¦æ±‚ç²¾ç¡®çš„è§†è§‰æ„ŸçŸ¥å’Œè§†è§‰æ–‡æœ¬æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„RSæ–¹æ³•ä¾èµ–äºå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œå¾®è°ƒä»¥å®ç°æ„ŸçŸ¥å’Œæ¨ç†ï¼Œä½†å…¶å¯¹å›¾åƒçš„ç¬¦å·åŒ–ä¼šç ´åå¯¹è±¡é—´çš„è¿ç»­ç©ºé—´å…³ç³»ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„RSæ–¹æ³•DTwinSegerï¼Œå®ƒåˆ©ç”¨æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºä½œä¸ºä¸­é—´å±‚æ¥è§£è€¦æ„ŸçŸ¥å’Œæ¨ç†ã€‚DTwinSegerå°†RSé‡æ–°æ„å»ºä¸ºä¸€ä¸ªä¸¤é˜¶æ®µçš„è¿‡ç¨‹ï¼Œé¦–å…ˆå°†å›¾åƒè½¬æ¢ä¸ºä¿ç•™ç©ºé—´å…³ç³»å’Œè¯­ä¹‰å±æ€§çš„ç»“æ„åŒ–DTè¡¨ç¤ºï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ­¤è¡¨ç¤ºè¿›è¡Œæ˜ç¡®çš„æ¨ç†ä»¥è¯†åˆ«ç›®æ ‡å¯¹è±¡ã€‚æœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é’ˆå¯¹LLMçš„ç‰¹å®šç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç›¸åº”çš„å¾®è°ƒæ•°æ®é›†Seg-DTæ¥å¢å¼ºLLMåœ¨DTè¡¨ç¤ºæ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒRSåŸºå‡†æµ‹è¯•é›†å’Œå›¾åƒæŒ‡ä»£åˆ†å‰²åŸºå‡†æµ‹è¯•é›†ä¸Šå‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œè¯æ˜äº†DTè¡¨ç¤ºä½œä¸ºè¿æ¥è§†è§‰å’Œæ–‡æœ¬çš„æœ‰æ•ˆæ¡¥æ¢ï¼Œä½¿å¾—å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡èƒ½å¤Ÿä»…é€šè¿‡LLMæ¥å®Œæˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¨ç†åˆ†å‰²ï¼ˆRSï¼‰æ˜¯ç»“åˆè§†è§‰ä¸æ–‡æœ¬çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œéœ€æ ¹æ®éšæ–‡æœ¬æŸ¥è¯¢è¿›è¡Œç‰©ä½“åˆ†å‰²ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä½†æ­¤æ–¹æ³•åœ¨ç¬¦å·åŒ–å›¾åƒæ—¶ä¼šç ´åå¯¹è±¡é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚</li>
<li>DTwinSegeråˆ©ç”¨æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºä½œä¸ºä¸­é—´å±‚ï¼Œå°†æ„ŸçŸ¥ä¸æ¨ç†è§£è€¦ã€‚</li>
<li>DTwinSegerå°†RSè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå›¾åƒè½¬åŒ–ä¸ºç»“æ„åŒ–DTè¡¨ç¤ºï¼Œå†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¨ç†ã€‚</li>
<li>æå‡ºä¸€ç§é’ˆå¯¹LLMçš„ç‰¹å®šç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œç»“åˆå¾®è°ƒæ•°æ®é›†Seg-DTï¼Œå¢å¼ºLLMåœ¨DTè¡¨ç¤ºä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDTwinSegeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-210e8649a1e65f2d54306c2ba4db24d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7283a9c967630d5c583afbea9ea1b498.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cd91779976a2206cd3d53b8387acaff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Solving-Inequality-Proofs-with-Large-Language-Models"><a href="#Solving-Inequality-Proofs-with-Large-Language-Models" class="headerlink" title="Solving Inequality Proofs with Large Language Models"></a>Solving Inequality Proofs with Large Language Models</h2><p><strong>Authors:Jiayi Sheng, Luna Lyu, Jikai Jin, Tony Xia, Alex Gu, James Zou, Pan Lu</strong></p>
<p>Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at <a target="_blank" rel="noopener" href="https://ineqmath.github.io/">https://ineqmath.github.io/</a>. </p>
<blockquote>
<p>ä¸å¹³ç­‰è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦é¢†åŸŸçš„å¤šä¸ªæ–¹é¢éƒ½è‡³å…³é‡è¦ï¼Œå®ƒæµ‹è¯•äº†å‘ç°ç´§å¯†ç•Œé™å’Œæˆ˜ç•¥å®šç†åº”ç”¨ç­‰é«˜çº§æ¨ç†æŠ€èƒ½ã€‚è¿™ä½¿å¾—å®ƒæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªç‹¬ç‰¹ä¸”å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸï¼Œä¸ºä¸€èˆ¬æ•°å­¦é—®é¢˜è§£å†³æä¾›äº†æ·±å…¥è§è§£ã€‚è¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°ç°æœ‰æ•°æ®é›†çš„é˜»ç¢ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸ç¨€ç¼ºã€åˆæˆæˆ–è¿‡äºå½¢å¼åŒ–ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§éæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡åˆ¶å®šæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°†ä¸å¹³ç­‰è¯æ˜é‡æ–°åˆ¶å®šä¸ºä¸¤ä¸ªå¯ä»¥è‡ªåŠ¨æ£€æŸ¥çš„ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¨å‡ºäº†IneqMathæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€å¥—ä¸“å®¶ç­–åˆ’çš„å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ä¸ç­‰å¼æ•°æ®é›†ï¼ŒåŒ…æ‹¬æµ‹è¯•é›†å’Œè®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶è¾…ä»¥é€æ­¥è§£å†³æ–¹æ¡ˆå’Œå®šç†æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§æ–°å‹LLMè¯„ä¼°æ¡†æ¶ï¼ˆLLMä½œä¸ºæ³•å®˜ï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æœ€ç»ˆç­”æ¡ˆæ³•å®˜å’Œå››æ­¥æ³•å®˜çš„è®¾è®¡ï¼Œæ—¨åœ¨æ£€æµ‹å¸¸è§çš„æ¨ç†ç¼ºé™·ã€‚å¯¹IneqMathä¸Š29ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„äº‹å®ï¼šå³ä½¿åœ¨é€æ­¥å®¡æŸ¥ä¸‹ï¼Œå³ä½¿æ˜¯é¡¶çº§æ¨¡å‹å¦‚o1çš„æ•´ä½“å‡†ç¡®ç‡ä¹Ÿä½äº10%ï¼›ä¸ä»…è€ƒè™‘æœ€ç»ˆç­”æ¡ˆç­‰ä»·çš„å‡†ç¡®ç‡ç›¸æ¯”ï¼Œè¿™ä¸€å·®è·é«˜è¾¾é«˜è¾¾65.5%ã€‚è¿™ç§å·®å¼‚æš´éœ²äº†è„†å¼±çš„æ¼”ç»é“¾ä»¥åŠå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰¾åˆ°ç­”æ¡ˆä¸æ„å»ºä¸¥æ ¼è¯æ˜ä¹‹é—´çš„å…³é”®å·®è·ã€‚æ‰©å¤§æ¨¡å‹è§„æ¨¡å¹¶å¢åŠ æµ‹è¯•æ—¶é—´çš„è®¡ç®—å¯¹æé«˜æ•´ä½“è¯æ˜çš„æ­£ç¡®æ€§äº§ç”Ÿæœ‰é™æ”¶ç›Šã€‚ç›¸åï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†å®šç†å¼•å¯¼æ¨ç†å’Œè‡ªæˆ‘å®Œå–„ç­‰å…·æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://ineqmath.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://ineqmath.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07927v1">PDF</a> 52 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸ç­‰å¼è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦é¢†åŸŸçš„é‡è¦æ€§ï¼Œæµ‹è¯•äº†é«˜çº§æ¨ç†æŠ€èƒ½ï¼Œå¦‚å¯»æ‰¾ç´§å¯†ç•Œé™å’Œæˆ˜ç•¥å®šç†åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿™ä¸ªé¢†åŸŸçš„å‘å±•å—åˆ°ç°æœ‰æ•°æ®é›†çš„é˜»ç¢ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸ç¨€ç¼ºã€åˆæˆæˆ–åƒµåŒ–ã€‚å› æ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªéæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡å…¬å¼ï¼Œå°†ä¸ç­‰å¼è¯æ˜é‡æ–°åˆ¶å®šä¸ºä¸¤ä¸ªå¯è‡ªåŠ¨æ£€æŸ¥çš„ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæ–‡ç« å‘å¸ƒäº†ä¸€ä¸ªä¸“å®¶ç¼–åˆ¶çš„ä¸ç­‰å¼è¯æ˜æ•°æ®é›†IneqMathï¼ŒåŒ…æ‹¬å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ä¸ç­‰å¼æµ‹è¯•é›†å’Œä¸°å¯Œçš„é€æ­¥è§£å†³æ–¹æ¡ˆå’Œå®šç†æ³¨é‡Šã€‚æ–‡ç« è¿˜å¼€å‘äº†ä¸€ç§æ–°å‹LLMè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆæœ€ç»ˆç­”æ¡ˆè¯„å®¡å’Œå››ä¸ªé€æ­¥è¯„å®¡ï¼Œæ—¨åœ¨æ£€æµ‹å¸¸è§çš„æ¨ç†ç¼ºé™·ã€‚å¯¹29ç§é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨é€æ­¥å®¡æŸ¥ä¸‹ï¼Œé¡¶çº§æ¨¡å‹çš„å‡†ç¡®ç‡ä¹Ÿä»¤äººæƒŠè®¶åœ°ä½äº10%ã€‚è¿™è¡¨æ˜å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰¾åˆ°ç­”æ¡ˆä¸æ„å»ºä¸¥æ ¼è¯æ˜ä¹‹é—´å­˜åœ¨å…³é”®å·®è·ã€‚æ‰©å¤§æ¨¡å‹è§„æ¨¡å¹¶å¢åŠ æµ‹è¯•æ—¶é—´è®¡ç®—ä»…å¸¦æ¥æœ‰é™çš„æ€»ä½“è¯æ˜æ­£ç¡®æ€§æ”¶ç›Šã€‚ç›¸åï¼Œç ”ç©¶å‘ç°å®šç†å¼•å¯¼æ¨ç†å’Œè‡ªæˆ‘å®Œå–„ç­‰ç ”ç©¶æ–¹å‘å…·æœ‰å¸Œæœ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ç­‰å¼è¯æ˜åœ¨ç§‘å­¦å’Œæ•°å­¦é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œæµ‹è¯•é«˜çº§æ¨ç†æŠ€èƒ½ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³ä¸ç­‰å¼è¯æ˜ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ•°æ®é›†å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªéæ­£å¼ä½†å¯éªŒè¯çš„ä»»åŠ¡å…¬å¼ï¼Œå°†ä¸ç­‰å¼è¯æ˜åˆ†ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç•Œé™ä¼°è®¡å’Œå…³ç³»é¢„æµ‹ã€‚</li>
<li>å‘å¸ƒäº†IneqMathæ•°æ®é›†ï¼ŒåŒ…å«å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ä¸ç­‰å¼é—®é¢˜ï¼Œä»¥åŠé€æ­¥è§£å†³æ–¹æ¡ˆå’Œå®šç†æ³¨é‡Šã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹LLMè¯„ä¼°æ¡†æ¶ï¼Œä»¥æ£€æµ‹æ¨ç†ç¼ºé™·ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨é€æ­¥å®¡æŸ¥ä¸‹ï¼Œé¡¶çº§å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®ç‡ä¹Ÿä½äº10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24d6807ad9a7a27be26eabf18149a963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347a41acf9860aecd7604ac85491658b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c84462cb1c208b8cc381d7c4d71a3bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9444336e97b3eac6081cebe0c35264e0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="HAIBU-ReMUD-Reasoning-Multimodal-Ultrasound-Dataset-and-Model-Bridging-to-General-Specific-Domains"><a href="#HAIBU-ReMUD-Reasoning-Multimodal-Ultrasound-Dataset-and-Model-Bridging-to-General-Specific-Domains" class="headerlink" title="HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging   to General Specific Domains"></a>HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging   to General Specific Domains</h2><p><strong>Authors:Shijie Wang, Yilun Zhang, Zeyu Lai, Dexing Kong</strong></p>
<p>Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at <a target="_blank" rel="noopener" href="https://github.com/ShiDaizi/ReMUD">https://github.com/ShiDaizi/ReMUD</a>, addressing the data shortage issue in specific domain MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨é¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ•°æ®ï¼Œå¦‚å›¾åƒæ–‡æœ¬æ•°æ®æˆ–è§†é¢‘æ–‡æœ¬æ•°æ®ï¼Œåœ¨æŸäº›ç‰¹å®šé¢†åŸŸçš„è¡¨ç°è¾ƒå·®ã€‚åœ¨ä¸€äº›ç‰¹å®šé¢†åŸŸï¼Œè™½ç„¶å­˜åœ¨å¤§é‡çš„å›¾å½¢å’Œæ–‡æœ¬æ•°æ®ï¼Œä½†å®ƒä»¬åˆ†å¸ƒæ•£ä¹±ï¼Œç¼ºä¹æ ‡å‡†åŒ–å®‰æ’ã€‚åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸï¼Œæœ‰è¶…å£°è¯Šæ–­ä¹¦ç±ã€è¶…å£°ä¸´åºŠæŒ‡å—ã€è¶…å£°è¯Šæ–­æŠ¥å‘Šç­‰ç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›è¶…å£°ææ–™é€šå¸¸ä¿å­˜åœ¨PDFã€å›¾åƒç­‰å½¢å¼ä¸‹ï¼Œæ— æ³•ç›´æ¥ç”¨äºMLLMsçš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07837v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å›¾åƒæ–‡æœ¬æ¨ç†ç›‘ç£å¾®è°ƒæ•°æ®ç”Ÿæˆç®¡é“ï¼Œç”¨äºä»ç‰¹å®šé¢†åŸŸçš„ææ–™ä¸­åˆ›å»ºç‰¹å®šé¢†åŸŸçš„å››å…ƒç»„ï¼ˆå›¾åƒã€é—®é¢˜ã€æ€è€ƒè½¨è¿¹å’Œç­”æ¡ˆï¼‰ã€‚å»ºç«‹äº†åŒ»ç–—è¶…å£°é¢†åŸŸçš„ReMUDæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡45ï¼Œ000æ¡æ¨ç†å’Œéæ¨ç†ç›‘ç£å¾®è°ƒé—®ç­”å’Œè§†è§‰é—®ç­”æ•°æ®ã€‚ç»è¿‡Qwen2.5-VL-7B-Instructç²¾ç»†è°ƒæ•´çš„ReMUD-7Bæ¨¡å‹åœ¨åŒ»ç–—è¶…å£°é¢†åŸŸä¼˜äºé€šç”¨é¢†åŸŸçš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚æ•°æ®é›†åŠç›¸å…³èµ„æºå·²å‘å¸ƒåœ¨GitHubä¸Šï¼Œä»¥è§£å†³ç‰¹å®šé¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„å›¾åƒæ–‡æœ¬æ•°æ®æ˜¯ä¸»è¦åŸå› ã€‚</li>
<li>åœ¨åŒ»ç–—è¶…å£°é¢†åŸŸå­˜åœ¨å¤§é‡çš„å›¾å½¢å’Œæ–‡æœ¬æ•°æ®ä½†æ— æ³•ç›´æ¥ä½¿ç”¨äºMLLMè®­ç»ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒæ–‡æœ¬æ¨ç†ç›‘ç£å¾®è°ƒæ•°æ®ç”Ÿæˆç®¡é“ï¼Œåˆ›å»ºäº†ç‰¹å®šçš„åŒ»å­¦è¶…å£°é¢†åŸŸæ•°æ®é›†ReMUDã€‚</li>
<li>ReMUDæ•°æ®é›†åŒ…å«è¶…è¿‡45ï¼Œ000æ¡æ¨ç†å’Œéæ¨ç†ç›‘ç£å¾®è°ƒé—®ç­”å’Œè§†è§‰é—®ç­”æ•°æ®ã€‚</li>
<li>ReMUD-7Bæ¨¡å‹åœ¨åŒ»ç–—è¶…å£°é¢†åŸŸçš„è¡¨ç°ä¼˜äºé€šç”¨é¢†åŸŸçš„MLLMsã€‚</li>
<li>æ•°æ®é›†åŠç›¸å…³èµ„æºå·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šï¼Œä»¥ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fffaf4e2bfecb4101d6bd0638af5fda2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b254b8009f955ae9ecd694e298e51103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a48ee1bca24fa0dcfeb6a2c248c61271.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3faa8cf3c8c2284e4bda9dc6eadb2c6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d927056d36a258b4a2b6901d34d6af13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f74b2dee9afddfa7790d0ca53f99c4c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Guideline-Forest-Experience-Induced-Multi-Guideline-Reasoning-with-Stepwise-Aggregation"><a href="#Guideline-Forest-Experience-Induced-Multi-Guideline-Reasoning-with-Stepwise-Aggregation" class="headerlink" title="Guideline Forest: Experience-Induced Multi-Guideline Reasoning with   Stepwise Aggregation"></a>Guideline Forest: Experience-Induced Multi-Guideline Reasoning with   Stepwise Aggregation</h2><p><strong>Authors:Jiaxiang CHen, Zhuo Wang, Mingxi Zou, Qifan Wang, Zenglin Xu</strong></p>
<p>Human reasoning is flexible, adaptive, and grounded in prior experience-qualities that large language models (LLMs) still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner. We propose Guideline Forest, a framework that enhances LLMs reasoning by inducing structured reasoning strategies-called guidelines-from verified examples and executing them via step-wise aggregation. Unlike test-time search or single-path distillation, our method draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants. Much like human reasoning, these variants reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step-enabling the model to adaptively resolve uncertainty and synthesize robust solutions.We evaluate Guideline Forest on four benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and programmatic reasoning. Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation, underscoring the Guideline Forestâ€™s adaptability and generalization potential. </p>
<blockquote>
<p>äººç±»æ¨ç†å…·æœ‰çµæ´»æ€§ã€é€‚åº”æ€§å’ŒåŸºäºå…ˆå‰ç»éªŒçš„ç‰¹è´¨ï¼Œè¿™äº›ç‰¹è´¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»éš¾ä»¥æ¨¡ä»¿ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆåœ¨æ¨ç†æ—¶æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œè¦ä¹ˆé€šè¿‡æ˜‚è´µçš„æ“ä½œå¯»æ‰¾æœ€ä½³å·¥ä½œæµç¨‹ï¼Œä½†ä¸¤ç§æ–¹æ³•éƒ½æœªèƒ½ä»¥ç»“æ„åŒ–ã€é«˜æ•ˆçš„æ–¹å¼åˆ©ç”¨å¤šç§å¯é‡å¤ä½¿ç”¨çš„ç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†â€œæŒ‡å—æ£®æ—â€æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä»éªŒè¯è¿‡çš„ç¤ºä¾‹ä¸­å¼•å¯¼ç»“æ„åŒ–æ¨ç†ç­–ç•¥ï¼ˆç§°ä¸ºæŒ‡å—ï¼‰å¹¶é€šè¿‡é€æ­¥èšåˆæ¥æ‰§è¡Œï¼Œä»è€Œå¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ã€‚ä¸åŒäºæµ‹è¯•æ—¶çš„æœç´¢æˆ–å•è·¯å¾„è’¸é¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºé€šè¿‡è¯±å¯¼å¯é‡å¤ä½¿ç”¨çš„æŒ‡å—å’Œå°†å…¶æ‰©å±•ä¸ºå¤šç§å˜ä½“æ¥éªŒè¯æ¨ç†ç»éªŒã€‚è¿™äº›å˜ä½“åæ˜ äº†æ›¿ä»£çš„æ€ç»´æ¨¡å¼ï¼Œå®ƒä»¬å¹¶è¡Œæ‰§è¡Œï¼Œé€šè¿‡è‡ªæˆ‘ä¿®æ­£è¿›è¡Œç»†åŒ–ï¼Œå¹¶é€æ­¥èšåˆâ€”â€”ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°è§£å†³ä¸ç¡®å®šæ€§å¹¶åˆæˆç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨æ¶µç›–æ•°å­¦å’Œç¨‹åºæ¨ç†çš„å››ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆGSM8Kã€MATH-500ã€MBPPå’ŒHumanEvalï¼‰ä¸Šè¯„ä¼°äº†â€œæŒ‡å—æ£®æ—â€ã€‚å®ƒæŒç»­ä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬CoTã€ReActã€ToTã€FoTå’ŒAFlowã€‚â€œæŒ‡å—æ£®æ—â€çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§å’Œæ½œåŠ›åœ¨é€æ­¥èšåˆçš„æ¸…é™¤ç ”ç©¶ä¸­å¾—åˆ°äº†è¿›ä¸€æ­¥å¼ºè°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07820v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œæŒ‡å—æ£®æ—â€çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»éªŒè¯å®ä¾‹ä¸­å¼•å¯¼ç»“æ„åŒ–æ¨ç†ç­–ç•¥ï¼ˆç§°ä¸ºæŒ‡å—ï¼‰å¹¶å¯¹å…¶è¿›è¡Œé€æ­¥èšåˆæ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸æµ‹è¯•æ—¶é—´æœç´¢æˆ–å•è·¯å¾„è’¸é¦ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç»è¿‡éªŒè¯çš„æ¨ç†ç»éªŒæ¥å¼•å¯¼å¯é‡å¤ä½¿ç”¨çš„æŒ‡å—ï¼Œå¹¶å°†å…¶æ‰©å±•ä¸ºå„ç§å˜ä½“ã€‚è¿™äº›å˜ä½“åæ˜ äº†æ›¿ä»£æ€ç»´æ¨¡å¼ï¼Œå¯å¹¶è¡Œæ‰§è¡Œã€é€šè¿‡è‡ªæˆ‘ä¿®æ­£è¿›è¡Œæ”¹è¿›ï¼Œå¹¶åˆ†é˜¶æ®µèšåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”è§£å†³ä¸ç¡®å®šæ€§å’Œåˆæˆç¨³å¥è§£å†³æ–¹æ¡ˆã€‚åœ¨æ¶µç›–æ•°å­¦å’Œç¨‹åºæ¨ç†çš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒæŒ‡å—æ£®æ—æŒç»­ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬CoTã€ReActã€ToTã€FoTå’ŒAFlowã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨¡æ‹Ÿäººç±»æ¨ç†çš„çµæ´»æ€§ã€é€‚åº”æ€§å’ŒåŸºäºå…ˆå‰ç»éªŒæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¦ä¹ˆåœ¨æ¨ç†æ—¶é—´æ¢ç´¢å¤šæ ·åŒ–çš„æ¨ç†è·¯å¾„ï¼Œè¦ä¹ˆé€šè¿‡æ˜‚è´µçš„æ“ä½œå¯»æ‰¾æœ€ä½³å·¥ä½œæµç¨‹ï¼Œä½†ä¸¤è€…éƒ½æœªèƒ½ä»¥ç»“æ„åŒ–ã€é«˜æ•ˆçš„æ–¹å¼åˆ©ç”¨å¤šç§å¯é‡å¤ä½¿ç”¨çš„ç­–ç•¥ã€‚</li>
<li>â€œæŒ‡å—æ£®æ—â€æ¡†æ¶é€šè¿‡ä»éªŒè¯å®ä¾‹ä¸­å¼•å¯¼ç»“æ„åŒ–æ¨ç†ç­–ç•¥å¹¶åˆ†é˜¶æ®µèšåˆæ‰§è¡Œæ¥å¢å¼ºLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸æµ‹è¯•æ—¶é—´æœç´¢æˆ–å•è·¯å¾„è’¸é¦ä¸åŒï¼Œå¼ºè°ƒåˆ©ç”¨ç»è¿‡éªŒè¯çš„æ¨ç†ç»éªŒæ¥å¼•å¯¼å¯é‡å¤ä½¿ç”¨çš„æŒ‡å—ï¼Œå¹¶å°†å…¶æ‰©å±•ä¸ºåæ˜ æ›¿ä»£æ€ç»´æ¨¡å¼çš„å˜ä½“ã€‚</li>
<li>è¿™äº›å˜ä½“å¯å¹¶è¡Œæ‰§è¡Œã€é€šè¿‡è‡ªæˆ‘ä¿®æ­£è¿›è¡Œæ”¹è¿›ï¼Œå¹¶åˆ†é˜¶æ®µèšåˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè§£å†³ä¸ç¡®å®šæ€§å’Œåˆæˆç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼Œâ€œæŒ‡å—æ£®æ—â€æŒç»­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–å¼ºå¤§çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4907574598d93ab60a4aab5b44b879bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d93c53d4966b7f6230c1377bca2e0a32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6001b171b845ed59bbae75e9ba8a688a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Looking-Beyond-Visible-Cues-Implicit-Video-Question-Answering-via-Dual-Clue-Reasoning"><a href="#Looking-Beyond-Visible-Cues-Implicit-Video-Question-Answering-via-Dual-Clue-Reasoning" class="headerlink" title="Looking Beyond Visible Cues: Implicit Video Question Answering via   Dual-Clue Reasoning"></a>Looking Beyond Visible Cues: Implicit Video Question Answering via   Dual-Clue Reasoning</h2><p><strong>Authors:Tieyuan Chen, Huabin Liu, Yi Wang, Chaofan Gan, Mingxi Lyu, Gui Zou, Weiyao Lin</strong></p>
<p>Video Question Answering (VideoQA) aims to answer natural language questions based on the given video, with prior work primarily focusing on identifying the duration of relevant segments, referred to as explicit visual evidence. However, explicit visual evidence is not always directly available, particularly when questions target symbolic meanings or deeper intentions, leading to significant performance degradation. To fill this gap, we introduce a novel task and dataset, $\textbf{I}$mplicit $\textbf{V}$ideo $\textbf{Q}$uestion $\textbf{A}$nswering (I-VQA), which focuses on answering questions in scenarios where explicit visual evidence is inaccessible. Given an implicit question and its corresponding video, I-VQA requires answering based on the contextual visual cues present within the video. To tackle I-VQA, we propose a novel reasoning framework, IRM (Implicit Reasoning Model), incorporating dual-stream modeling of contextual actions and intent clues as implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the Visual Enhancement Module (VEM). AIM deduces and preserves question-related dual clues by generating clue candidates and performing relation deduction. VEM enhances contextual visual representation by leveraging key contextual clues. Extensive experiments validate the effectiveness of our IRM in I-VQA tasks, outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76%$, $1.37%$, and $4.87%$, respectively. Additionally, IRM performs SOTA on similar implicit advertisement understanding and future prediction in traffic-VQA. Datasets and codes are available for double-blind review in anonymous repo: <a target="_blank" rel="noopener" href="https://github.com/tychen-SJTU/Implicit-VideoQA">https://github.com/tychen-SJTU/Implicit-VideoQA</a>. </p>
<blockquote>
<p>è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æ—¨åœ¨æ ¹æ®ç»™å®šçš„è§†é¢‘å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œæ—©æœŸçš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨è¯†åˆ«ç›¸å…³ç‰‡æ®µçš„æŒç»­æ—¶é—´ï¼Œç§°ä¸ºæ˜ç¡®è§†è§‰è¯æ®ã€‚ç„¶è€Œï¼Œæ˜ç¡®è§†è§‰è¯æ®å¹¶éæ€»æ˜¯å¯ä»¥ç›´æ¥è·å¾—ï¼Œç‰¹åˆ«æ˜¯å½“é—®é¢˜é’ˆå¯¹è±¡å¾æ„ä¹‰æˆ–æ›´æ·±å±‚æ¬¡çš„æ„å›¾æ—¶ï¼Œä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œå³éšå¼è§†é¢‘é—®ç­”ï¼ˆI-VQAï¼‰ï¼Œä¸“æ³¨äºå›ç­”åœ¨æ˜ç¡®è§†è§‰è¯æ®æ— æ³•è·å–çš„æƒ…å†µä¸‹çš„é—®é¢˜ã€‚ç»™å®šéšå¼é—®é¢˜å’Œå…¶å¯¹åº”çš„è§†é¢‘ï¼ŒI-VQAéœ€è¦åŸºäºè§†é¢‘ä¸­å­˜åœ¨çš„ä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢æ¥å›ç­”ã€‚ä¸ºäº†è§£å†³I-VQAï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œå³IRMï¼ˆéšå¼æ¨ç†æ¨¡å‹ï¼‰ï¼Œå®ƒç»“åˆäº†ä¸Šä¸‹æ–‡åŠ¨ä½œå’Œæ„å›¾çº¿ç´¢çš„åŒé‡æµå»ºæ¨¡ï¼Œä½œä¸ºéšå¼æ¨ç†é“¾ã€‚IRMåŒ…æ‹¬åŠ¨ä½œæ„å›¾æ¨¡å—ï¼ˆAIMï¼‰å’Œè§†è§‰å¢å¼ºæ¨¡å—ï¼ˆVEMï¼‰ã€‚AIMé€šè¿‡ç”Ÿæˆçº¿ç´¢å€™é€‰å¹¶æ‰§è¡Œå…³ç³»æ¨ç†æ¥æ¨æ–­å’Œä¿å­˜ä¸é—®é¢˜ç›¸å…³çš„åŒé‡çº¿ç´¢ã€‚VEMé€šè¿‡åˆ©ç”¨å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢å¢å¼ºä¸Šä¸‹æ–‡è§†è§‰è¡¨ç¤ºã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„IRMåœ¨I-VQAä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç›¸å¯¹äºGPT-4oã€OpenAI-o3å’Œç»è¿‡å¾®è°ƒçš„è§†é¢‘èŠå¤©2åˆ†åˆ«æé«˜äº†0.76%ã€1.37%å’Œ4.87%ã€‚æ­¤å¤–ï¼ŒIRMåœ¨ç±»ä¼¼çš„éšå¼å¹¿å‘Šç†è§£å’Œæœªæ¥äº¤é€šé¢„æµ‹çš„äº¤é€šé—®ç­”ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨åŒ¿åå­˜å‚¨åº“ä¸­ä¾›åŒç›²è¯„å®¡ä½¿ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/tychen-SJTU/Implicit-VideoQA">https://github.com/tychen-SJTU/Implicit-VideoQA</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07811v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>     è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰é€šå¸¸åŸºäºè§†é¢‘å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œä¹‹å‰çš„å·¥ä½œä¸»è¦å…³æ³¨äºè¯†åˆ«ç›¸å…³æ®µè½çš„æŒç»­æ—¶é—´ï¼Œå³æ˜¾å¼è§†è§‰è¯æ®ã€‚ç„¶è€Œï¼Œå½“é—®é¢˜é’ˆå¯¹è±¡å¾æ„ä¹‰æˆ–æ›´æ·±å±‚æ¬¡çš„æ„å›¾æ—¶ï¼Œæ˜¾å¼è§†è§‰è¯æ®å¹¶ä¸æ€»æ˜¯å¯ç›´æ¥è·å¾—ï¼Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æ–°çš„ä»»åŠ¡å’Œæ•°æ®é›†â€”â€”éšå«è§†é¢‘é—®ç­”ï¼ˆI-VQAï¼‰ï¼Œä¸“æ³¨äºå›ç­”éšæ€§é—®é¢˜å’Œå¯¹åº”è§†é¢‘çš„é—®é¢˜ã€‚I-VQAåŸºäºè§†é¢‘ä¸­çš„ä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢æ¥å›ç­”é—®é¢˜ã€‚ä¸ºè§£å†³I-VQAï¼Œæˆ‘ä»¬æå‡ºäº†éšå«æ¨ç†æ¨¡å‹ï¼ˆIRMï¼‰çš„æ–°æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡åŠ¨ä½œå’Œæ„å›¾æš—ç¤ºçš„åŒå‘æµå»ºæ¨¡ä½œä¸ºéšå«æ¨ç†é“¾ã€‚IRMåŒ…æ‹¬åŠ¨ä½œæ„å›¾æ¨¡å—ï¼ˆAIMï¼‰å’Œè§†è§‰å¢å¼ºæ¨¡å—ï¼ˆVEMï¼‰ã€‚AIMé€šè¿‡ç”Ÿæˆçº¿ç´¢å€™é€‰å’Œæ‰§è¡Œå…³ç³»æ¨å¯¼æ¥æ¨æ–­å’Œä¿å­˜ä¸é—®é¢˜ç›¸å…³çš„åŒé‡çº¿ç´¢ã€‚VEMåˆ©ç”¨å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢å¢å¼ºä¸Šä¸‹æ–‡è§†è§‰è¡¨ç¤ºã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„IRMåœ¨I-VQAä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¶…è¶Šäº†GPT-4oã€OpenAI-o3å’Œå¾®è°ƒåçš„VideoChat2ã€‚æ­¤å¤–ï¼ŒIRMåœ¨ç±»ä¼¼çš„éšæ€§å¹¿å‘Šç†è§£å’Œæœªæ¥äº¤é€šé¢„æµ‹-VQAä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç›¸å…³æ•°æ®é›†å’Œä»£ç å¯åœ¨åŒ¿åä»“åº“ä¸­è¿›è¡ŒåŒé‡ç›²å®¡ï¼š<a target="_blank" rel="noopener" href="https://github.com/tychen-SJTU/Implicit-VideoQA%E3%80%82">https://github.com/tychen-SJTU/Implicit-VideoQAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰ä¸»è¦ä¾èµ–æ˜¾å¼è§†è§‰è¯æ®å›ç­”é—®é¢˜ï¼Œä½†å½“é—®é¢˜æ¶‰åŠè±¡å¾æ„ä¹‰æˆ–æ·±å±‚æ„å›¾æ—¶ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¼šå¤±æ•ˆã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†éšå«è§†é¢‘é—®ç­”ï¼ˆI-VQAï¼‰ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä¸“æ³¨äºå›ç­”éšæ€§é—®é¢˜å’Œè§†é¢‘ã€‚</li>
<li>I-VQAè¦æ±‚åŸºäºè§†é¢‘ä¸­çš„ä¸Šä¸‹æ–‡è§†è§‰çº¿ç´¢æ¥å›ç­”é—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ¨ç†æ¡†æ¶â€”â€”éšå«æ¨ç†æ¨¡å‹ï¼ˆIRMï¼‰ï¼ŒåŒ…æ‹¬åŠ¨ä½œæ„å›¾æ¨¡å—ï¼ˆAIMï¼‰å’Œè§†è§‰å¢å¼ºæ¨¡å—ï¼ˆVEMï¼‰ã€‚</li>
<li>AIMé€šè¿‡ç”Ÿæˆçº¿ç´¢å€™é€‰å’Œå…³ç³»æ¨å¯¼æ¥æ¨æ–­å’Œä¿å­˜ä¸é—®é¢˜ç›¸å…³çš„åŒé‡çº¿ç´¢ã€‚</li>
<li>VEMåˆ©ç”¨å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢å¢å¼ºè§†è§‰è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-42c661eba2bcff3b7724ce352bba00e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50651315b9ae235df4d083f77da1b213.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faa1675148372133e4f3a2e5b1c3142a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-738ab69e52c01c64d521722563e7970b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e4839e0662caec85fb9ac32a2d3e5c1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Re-ranking-Reasoning-Context-with-Tree-Search-Makes-Large-Vision-Language-Models-Stronger"><a href="#Re-ranking-Reasoning-Context-with-Tree-Search-Makes-Large-Vision-Language-Models-Stronger" class="headerlink" title="Re-ranking Reasoning Context with Tree Search Makes Large   Vision-Language Models Stronger"></a>Re-ranking Reasoning Context with Tree Search Makes Large   Vision-Language Models Stronger</h2><p><strong>Authors:Qi Yang, Chenghao Zhang, Lubin Fan, Kun Ding, Jieping Ye, Shiming Xiang</strong></p>
<p>Recent advancements in Large Vision Language Models (LVLMs) have significantly improved performance in Visual Question Answering (VQA) tasks through multimodal Retrieval-Augmented Generation (RAG). However, existing methods still face challenges, such as the scarcity of knowledge with reasoning examples and erratic responses from retrieved knowledge. To address these issues, in this study, we propose a multimodal RAG framework, termed RCTS, which enhances LVLMs by constructing a Reasoning Context-enriched knowledge base and a Tree Search re-ranking method. Specifically, we introduce a self-consistent evaluation mechanism to enrich the knowledge base with intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This ensures that LVLMs can leverage high-quality contextual reasoning for better and more consistent responses. Extensive experiments demonstrate that our framework achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods. It highlights the effectiveness of our knowledge base and re-ranking method in improving LVLMs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yannqi/RCTS-RAG">https://github.com/yannqi/RCTS-RAG</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¾è‘—æé«˜äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹æ¨ç†ç¤ºä¾‹å’Œæ¥è‡ªæ£€ç´¢çŸ¥è¯†çš„éšæœºå“åº”ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€RAGæ¡†æ¶ï¼Œç§°ä¸ºRCTSï¼Œé€šè¿‡æ„å»ºä¸°å¯Œçš„æ¨ç†ä¸Šä¸‹æ–‡çŸ¥è¯†åº“å’Œæ ‘æœç´¢æ’åºæ–¹æ³•ï¼Œå¢å¼ºLVLMsçš„åŠŸèƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªæ´½è¯„ä¼°æœºåˆ¶ï¼Œä»¥å†…åœ¨æ¨ç†æ¨¡å¼ä¸°å¯ŒçŸ¥è¯†åº“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ç»“åˆå¯å‘å¼å¥–åŠ±çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTS-HRï¼‰ï¼Œä»¥ä¼˜å…ˆå¤„ç†æœ€ç›¸å…³çš„ç¤ºä¾‹ã€‚è¿™ç¡®ä¿äº†LVLMsèƒ½å¤Ÿåˆ©ç”¨é«˜è´¨é‡çš„ä¸Šæ–‡æ¨ç†ï¼Œä»¥è·å–æ›´å¥½ã€æ›´ä¸€è‡´çš„å“åº”ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šä¸ªVQAæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’ŒVanilla-RAGæ–¹æ³•ã€‚è¿™å‡¸æ˜¾äº†æˆ‘ä»¬çš„çŸ¥è¯†åº“å’Œæ’åºæ–¹æ³•åœ¨æ”¹å–„LVLMsæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yannqi/RCTS-RAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yannqi/RCTS-RAGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07785v1">PDF</a> ICML 2025 Spotlight. 22 pages, 16 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰äº†æ˜¾è‘—æå‡ï¼Œè¿™å¾—ç›Šäºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»é¢ä¸´çŸ¥è¯†æ¨ç†ç¤ºä¾‹åŒ®ä¹å’Œä»æ£€ç´¢çŸ¥è¯†ä¸­äº§ç”Ÿçš„åº”ç­”ä¸ç¨³å®šç­‰æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€RAGæ¡†æ¶ï¼ˆRCTSï¼‰ï¼Œé€šè¿‡æ„å»ºå¯Œå«æ¨ç†ä¸Šä¸‹æ–‡çš„çŸ¥è¯†åº“å’Œæ ‘æœç´¢é‡æ’åºæ–¹æ³•ï¼Œå¢å¼ºLVLMsçš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªVQAæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å’Œå¸¸è§„RAGæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsåœ¨VQAä»»åŠ¡ä¸Šçš„æ€§èƒ½é€šè¿‡RAGæŠ€æœ¯å¾—åˆ°æ˜¾è‘—æå‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´çŸ¥è¯†æ¨ç†ç¤ºä¾‹ä¸è¶³å’Œåº”ç­”ä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„RCTSæ¡†æ¶é€šè¿‡æ„å»ºå¯Œå«æ¨ç†ä¸Šä¸‹æ–‡çš„çŸ¥è¯†åº“å¢å¼ºLVLMsã€‚</li>
<li>RCTSå¼•å…¥è‡ªæˆ‘ä¸€è‡´è¯„ä»·æœºåˆ¶ï¼Œä»¥å†…åœ¨æ¨ç†æ¨¡å¼ä¸°å¯ŒçŸ¥è¯†åº“ã€‚</li>
<li>é‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ä¸å¯å‘å¼å¥–åŠ±ï¼ˆMCTS-HRï¼‰ä»¥ä¼˜å…ˆå¤„ç†æœ€ç›¸å…³çš„ç¤ºä¾‹ã€‚</li>
<li>RCTSæ¡†æ¶åœ¨å¤šä¸ªVQAæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8781519a94d578645927420ee873d8b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1888fa97df1ee97e272b230ca44b1f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d27e5353b9c9f981d40e8889080fe248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c2b999d7deebf24c2faaf6297258a56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73fc3f67eaea8780e847b09a0edafa5f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Design-and-Evaluation-of-Deep-Learning-Based-Dual-Spectrum-Image-Fusion-Methods"><a href="#Design-and-Evaluation-of-Deep-Learning-Based-Dual-Spectrum-Image-Fusion-Methods" class="headerlink" title="Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods"></a>Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods</h2><p><strong>Authors:Beining Xu, Junxian Li</strong></p>
<p>Visible images offer rich texture details, while infrared images emphasize salient targets. Fusing these complementary modalities enhances scene understanding, particularly for advanced vision tasks under challenging conditions. Recently, deep learning-based fusion methods have gained attention, but current evaluations primarily rely on general-purpose metrics without standardized benchmarks or downstream task performance. Additionally, the lack of well-developed dual-spectrum datasets and fair algorithm comparisons hinders progress.   To address these gaps, we construct a high-quality dual-spectrum dataset captured in campus environments, comprising 1,369 well-aligned visible-infrared image pairs across four representative scenarios: daytime, nighttime, smoke occlusion, and underpasses. We also propose a comprehensive and fair evaluation framework that integrates fusion speed, general metrics, and object detection performance using the lang-segment-anything model to ensure fairness in downstream evaluation.   Extensive experiments benchmark several state-of-the-art fusion algorithms under this framework. Results demonstrate that fusion models optimized for downstream tasks achieve superior performance in target detection, especially in low-light and occluded scenes. Notably, some algorithms that perform well on general metrics do not translate to strong downstream performance, highlighting limitations of current evaluation practices and validating the necessity of our proposed framework.   The main contributions of this work are: (1)a campus-oriented dual-spectrum dataset with diverse and challenging scenes; (2) a task-aware, comprehensive evaluation framework; and (3) thorough comparative analysis of leading fusion methods across multiple datasets, offering insights for future development. </p>
<blockquote>
<p>å¯è§å›¾åƒæä¾›äº†ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ï¼Œè€Œçº¢å¤–å›¾åƒåˆ™çªå‡ºæ˜¾ç¤ºæ˜¾è‘—ç›®æ ‡ã€‚èåˆè¿™äº›äº’è¡¥æ¨¡å¼å¢å¼ºäº†åœºæ™¯ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹çš„é«˜çº§è§†è§‰ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„èåˆæ–¹æ³•å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œä½†å½“å‰çš„è¯„ä¼°ä¸»è¦ä¾èµ–äºé€šç”¨æŒ‡æ ‡ï¼Œç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç¼ºä¹å‘è¾¾çš„åŒå…‰è°±æ•°æ®é›†å’Œå…¬æ­£çš„ç®—æ³•æ¯”è¾ƒé˜»ç¢äº†è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬åœ¨æ ¡å›­ç¯å¢ƒä¸­æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åŒå…‰è°±æ•°æ®é›†ï¼ŒåŒ…å«å››ç§ä»£è¡¨æ€§åœºæ™¯ä¸‹çš„1369å¯¹è‰¯å¥½å¯¹é½çš„å¯è§å…‰çº¢å¤–å›¾åƒå¯¹ï¼šç™½å¤©ã€å¤œæ™šã€çƒŸé›¾é®æŒ¡å’Œåœ°é“ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå…¨é¢å…¬æ­£çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†èåˆé€Ÿåº¦ã€é€šç”¨æŒ‡æ ‡å’Œå¯¹è±¡æ£€æµ‹æ€§èƒ½ï¼Œä½¿ç”¨lang-segment-anythingæ¨¡å‹ä»¥ç¡®ä¿ä¸‹æ¸¸è¯„ä¼°çš„å…¬å¹³æ€§ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œå¯¹å‡ ç§æœ€å…ˆè¿›çš„èåˆç®—æ³•è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„èåˆæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å…‰å’Œé®æŒ¡åœºæ™¯ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€äº›åœ¨é€šç”¨æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½çš„ç®—æ³•å¹¶ä¸ç­‰åŒäºåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¼ºåŠ²è¡¨ç°ï¼Œè¿™çªå‡ºäº†å½“å‰è¯„ä¼°å®è·µçš„å±€é™æ€§ï¼Œå¹¶éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºçš„æ¡†æ¶çš„å¿…è¦æ€§ã€‚æœ¬å·¥ä½œçš„ä¸»è¦è´¡çŒ®æ˜¯ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªé¢å‘æ ¡å›­çš„åŒå…‰è°±æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªä»»åŠ¡æ„ŸçŸ¥çš„å…¨é¢è¯„ä¼°æ¡†æ¶ï¼›ï¼ˆ3ï¼‰å¯¹å¤šä¸ªæ•°æ®é›†çš„é¢†å…ˆèåˆæ–¹æ³•çš„å…¨é¢æ¯”è¾ƒåˆ†æï¼Œä¸ºæœªæ¥çš„å‘å±•æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07779v1">PDF</a> 11 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>å¯è§å›¾åƒæä¾›ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ï¼Œè€Œçº¢å¤–å›¾åƒåˆ™çªå‡ºæ˜¾è‘—ç›®æ ‡ã€‚èåˆè¿™äº›äº’è¡¥æ¨¡å¼å¢å¼ºäº†åœºæ™¯ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹çš„é«˜çº§è§†è§‰ä»»åŠ¡ã€‚å°½ç®¡æœ€è¿‘åŸºäºæ·±åº¦å­¦ä¹ çš„èåˆæ–¹æ³•å—åˆ°å…³æ³¨ï¼Œä½†å½“å‰è¯„ä¼°ä¸»è¦ä¾èµ–äºé€šç”¨æŒ‡æ ‡ï¼Œç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è¯„ä¼°ã€‚æ­¤å¤–ï¼Œç¼ºä¹å‘è¾¾çš„åŒå…‰è°±æ•°æ®é›†å’Œå…¬å¹³çš„ç®—æ³•æ¯”è¾ƒé˜»ç¢äº†è¿›å±•ã€‚ä¸ºè§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†é«˜è´¨é‡çš„åŒå…‰è°±æ•°æ®é›†ï¼ŒåŒ…å«å››ç§ä»£è¡¨æ€§åœºæ™¯ä¸‹çš„1369å¯¹å¯è§å…‰çº¢å¤–å›¾åƒå¯¹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå…¨é¢è€Œå…¬å¹³çš„è¯„ä¼°æ¡†æ¶ï¼Œèåˆäº†èåˆé€Ÿåº¦ã€é€šç”¨æŒ‡æ ‡å’Œå¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜¾ç¤ºï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„èåˆæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å…‰å’Œé®æŒ¡åœºæ™¯ä¸­ã€‚ä¸€äº›åœ¨é€šç”¨æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½çš„ç®—æ³•åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œçªæ˜¾äº†å½“å‰è¯„ä¼°å®è·µçš„å±€é™æ€§å¹¶éªŒè¯äº†æ‰€æå‡ºçš„æ¡†æ¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€æ–‡ç« ä»‹ç»äº†å¯è§å›¾åƒä¸çº¢å¤–å›¾åƒçš„ç‰¹ç‚¹å’Œèåˆçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„é«˜çº§è§†è§‰ä»»åŠ¡ä¸­ã€‚<br>äºŒã€å½“å‰èåˆæ–¹æ³•çš„è¯„ä¼°ä¸»è¦ä¾èµ–é€šç”¨æŒ‡æ ‡ï¼Œç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è¯„ä¼°ã€‚<br>ä¸‰ã€ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æ„å»ºäº†ä¸€ä¸ªæ ¡å›­å¯¼å‘çš„åŒå…‰è°±æ•°æ®é›†ï¼ŒåŒ…å«ä¸åŒåœºæ™¯çš„å¯è§å…‰çº¢å¤–å›¾åƒå¯¹ã€‚<br>å››ã€æå‡ºäº†ä¸€ä¸ªå…¨é¢è€Œå…¬å¹³çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†èåˆé€Ÿåº¦ã€é€šç”¨æŒ‡æ ‡å’Œå¯¹è±¡æ£€æµ‹æ€§èƒ½è¯„ä¼°ã€‚<br>äº”ã€å®éªŒç»“æœæ˜¾ç¤ºé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„èåˆæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚<br>å…­ã€æ–‡ç« çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ ¡å›­å¯¼å‘çš„åŒå…‰è°±æ•°æ®é›†ã€ä»»åŠ¡æ„ŸçŸ¥çš„è¯„ä¼°æ¡†æ¶ä»¥åŠå¯¹é¢†å…ˆèåˆæ–¹æ³•çš„æ¯”è¾ƒåˆ†æã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a42d3b98b4bad62567261abeafa05e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d862f02ceb26aa7c7edcf53280dca2ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b3125d13bfee7152fb358b63e7b62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fc63fd10f5ae90b4496a00cd6906747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2f27930105d221a556194831039881b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f633865a6eb2457ad9086d6b8295609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b51f93e5910a58de4b7c61f4a246da8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cfbbaafbe8abff736b2c9c583db251f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="QUITE-A-Query-Rewrite-System-Beyond-Rules-with-LLM-Agents"><a href="#QUITE-A-Query-Rewrite-System-Beyond-Rules-with-LLM-Agents" class="headerlink" title="QUITE: A Query Rewrite System Beyond Rules with LLM Agents"></a>QUITE: A Query Rewrite System Beyond Rules with LLM Agents</h2><p><strong>Authors:Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang</strong></p>
<p>Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle. </p>
<blockquote>
<p>æŸ¥è¯¢é‡å†™å°†SQLæŸ¥è¯¢è½¬æ¢ä¸ºè¯­ä¹‰ä¸Šç­‰æ•ˆä¸”è¿è¡Œæ•ˆç‡æ›´é«˜çš„å½¢å¼ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„é‡å†™è§„åˆ™ï¼Œä½†å®ƒä»¬åªèƒ½å¤„ç†æœ‰é™çš„æŸ¥è¯¢é›†ï¼Œå¹¶å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™ç§å±€é™æ€§æºäºåŸºäºè§„åˆ™çš„æŸ¥è¯¢é‡å†™æ‰€é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼š(1)å‘ç°å’ŒéªŒè¯æ–°è§„åˆ™å¾ˆå›°éš¾ï¼Œ(2)å›ºå®šçš„é‡å†™è§„åˆ™ä¸èƒ½æ¨å¹¿åˆ°æ–°æŸ¥è¯¢æ¨¡å¼ï¼Œ(3)æŸäº›é‡å†™æŠ€æœ¯æ— æ³•è¡¨ç¤ºä¸ºå›ºå®šè§„åˆ™ã€‚å—äººç±»ä¸“å®¶å±•ç°å‡ºæ˜¾è‘—çš„é‡å†™èƒ½åŠ›ä½†é¢ä¸´å¯æ‰©å±•æ€§é—®é¢˜çš„å¯å‘ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¯æ˜äº†æ¥è¿‘äººç±»æ°´å¹³çš„è¯­ä¹‰å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨LLMé‡å†™SQLæŸ¥è¯¢çš„æ–°æ–¹æ³•ã€‚ç”±äºLLMä¸­çš„å¹»è§‰é—®é¢˜ï¼Œç›´æ¥åº”ç”¨LLMé€šå¸¸ä¼šå¯¼è‡´ä¸ç­‰æ•ˆå’Œæ¬¡ä¼˜æŸ¥è¯¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLLMä»£ç†çš„æ— éœ€è®­ç»ƒå’Œåé¦ˆæ„ŸçŸ¥çš„QUERY REWRITEï¼ˆæŸ¥è¯¢é‡å†™ï¼‰ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿå°†SQLæŸ¥è¯¢é‡å†™ä¸ºè¯­ä¹‰ç­‰æ•ˆçš„å½¢å¼ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œä¸åŸºäºè§„åˆ™çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¦†ç›–æ›´å¹¿æ³›çš„æŸ¥è¯¢æ¨¡å¼å’Œé‡å†™ç­–ç•¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç”±æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰æ§åˆ¶çš„å¤šä»£ç†æ¡†æ¶ï¼Œä¸ºLLMé…å¤‡ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®æ—¶æ•°æ®åº“åé¦ˆå¢å¼ºé‡å†™è¿‡ç¨‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé‡å†™ä¸­é—´ä»¶ï¼Œä»¥å¢å¼ºLLMç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ç­‰æ•ˆç‰©çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„æç¤ºæ³¨å…¥æŠ€æœ¯ï¼Œä»¥æ”¹è¿›é‡å†™æŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒQUERY REWRITEå¯ä»¥å‡å°‘é«˜è¾¾35.8%çš„æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ï¼Œå¹¶ä¸”äº§ç”Ÿçš„é‡å†™æ¬¡æ•°æ¯”å…ˆå‰çš„æ–¹æ³•å¤š24.1%ï¼Œè¦†ç›–æ—©æœŸç³»ç»Ÿæ— æ³•å¤„ç†çš„æŸ¥è¯¢æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07675v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæŸ¥è¯¢é‡å†™æŠ€æœ¯çš„SQLæŸ¥è¯¢ä¼˜åŒ–æ–¹æ³•æå‡ºï¼Œæ—¨åœ¨å°†SQLæŸ¥è¯¢è½¬æ¢ä¸ºè¯­ä¹‰ç­‰æ•ˆä¸”è¿è¡Œæ•ˆç‡æ›´é«˜çš„å½¢å¼ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–é¢„è®¾çš„æ”¹å†™è§„åˆ™ï¼Œä½†å¤„ç†æŸ¥è¯¢å­é›†æœ‰é™ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒSQLæŸ¥è¯¢æ”¹å†™çš„æ–°æ–¹æ³•ï¼Œå…‹æœè§„åˆ™æ–¹æ³•çš„å±€é™æ€§ã€‚ç»“åˆäººç±»ä¸“å®¶çš„æ”¹å†™èƒ½åŠ›å’ŒLLMsçš„è¯­ä¹‰ç†è§£ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å…‹æœäº†LLMsçš„å¹»è§‰é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºQUIETï¼ˆæŸ¥è¯¢é‡å†™ï¼‰çš„è®­ç»ƒå¤–å’Œåé¦ˆæ„ŸçŸ¥ç³»ç»Ÿã€‚è¯¥ç³»ç»ŸåŸºäºLLMä»£ç†ï¼Œå¯é‡å†™SQLæŸ¥è¯¢ä¸ºè¯­ä¹‰ç­‰æ•ˆå½¢å¼ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œè¦†ç›–æ›´å¹¿æ³›çš„æŸ¥è¯¢æ¨¡å¼å’Œæ”¹å†™ç­–ç•¥ã€‚é€šè¿‡è®¾è®¡ç”±æœ‰é™çŠ¶æ€æœºæ§åˆ¶çš„å¤šä»£ç†æ¡†æ¶ã€å¼€å‘é‡å†™ä¸­é—´ä»¶å¹¶é‡‡ç”¨æç¤ºæ³¨å…¥æŠ€æœ¯ï¼ŒQUIETå‡å°‘äº†æŸ¥è¯¢æ‰§è¡Œæ—¶é—´å¹¶æé«˜æ”¹å†™æŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒQUIETè¾ƒç°æœ‰æ–¹æ³•å‡å°‘æœ€å¤šè¾¾35.8%çš„æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ï¼Œäº§ç”Ÿæ›´å¤šè¾¾24.1%çš„æ”¹å†™æ¬¡æ•°ï¼Œè¦†ç›–æ—©æœŸç³»ç»Ÿæ— æ³•å¤„ç†çš„æŸ¥è¯¢æ¡ˆä¾‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰æŸ¥è¯¢é‡å†™æ–¹æ³•ä¸»è¦ä¾èµ–é¢„è®¾è§„åˆ™ï¼Œå­˜åœ¨å¤„ç†æŸ¥è¯¢å­é›†æœ‰é™å’Œæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡è¿‘ä¼¼äººç±»æ°´å¹³çš„è¯­ä¹‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯åœ¨SQLæŸ¥è¯¢é‡å†™ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>QUITEç³»ç»Ÿç»“åˆLLMså’Œæœ‰é™çŠ¶æ€æœºæ§åˆ¶çš„å¤šä»£ç†æ¡†æ¶ï¼Œæé«˜SQLæŸ¥è¯¢é‡å†™æ•ˆç‡å’Œè´¨é‡ã€‚</li>
<li>QUITEé€šè¿‡å®æ—¶æ•°æ®åº“åé¦ˆå’Œé‡å†™ä¸­é—´ä»¶å¢å¼ºLLMsç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢ç­‰æ•ˆç‰©çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨æç¤ºæ³¨å…¥æŠ€æœ¯æ”¹å–„é‡å†™æŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºQUIETåœ¨å‡å°‘æŸ¥è¯¢æ‰§è¡Œæ—¶é—´å’Œå¢åŠ æ”¹å†™æ¬¡æ•°æ–¹é¢è¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8dc1801511ffb9ea636011984f350e99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31ab5b312a177d6563552464e41793cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c279035dc05419fab8840d05a33005b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d421738bbf7fba6d3fa988bd78399e72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e38705cc7e4f9d5c3d8adc88e17e17c0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Synthetic-Visual-Genome"><a href="#Synthetic-Visual-Genome" class="headerlink" title="Synthetic Visual Genome"></a>Synthetic Visual Genome</h2><p><strong>Authors:Jae Sung Park, Zixian Ma, Linjie Li, Chenhao Zheng, Cheng-Yu Hsieh, Ximing Lu, Khyathi Chandu, Quan Kong, Norimasa Kobori, Ali Farhadi, Yejin Choi, Ranjay Krishna</strong></p>
<p>Reasoning over visual relationships-spatial, functional, interactional, social, etc.-is considered to be a fundamental component of human cognition. Yet, despite the major advances in visual comprehension in multimodal language models (MLMs), precise reasoning over relationships and their generations remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely annotated relationships capable of constructing high-quality dense scene graphs at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by completing the missing relations of selected objects in existing scene graphs using a teacher MLM and a carefully designed filtering process to ensure high-quality. To generate more accurate and rich scene graphs at scale for any image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further refines ROBINâ€™s predicted scene graphs by removing unlikely relations and&#x2F;or suggesting relevant ones. In total, our dataset contains 146K images and 5.6M relationships for 2.6M objects. Results show that our ROBIN-3B model, despite being trained on less than 3 million instances, outperforms similar-size models trained on over 300 million instances on relationship understanding benchmarks, and even surpasses larger models up to 13B parameters. Notably, it achieves state-of-the-art performance in referring expression comprehension with a score of 88.9, surpassing the previous best of 87.4. Our results suggest that training on the refined scene graph data is crucial to maintaining high performance across diverse visual reasoning task. </p>
<blockquote>
<p>å¯¹äººç±»è®¤çŸ¥æ¥è¯´ï¼Œç†è§£å’Œæ¨ç†è§†è§‰å…³ç³»ï¼ˆå¦‚ç©ºé—´å…³ç³»ã€åŠŸèƒ½å…³ç³»ã€äº’åŠ¨å…³ç³»ã€ç¤¾ä¼šå…³ç³»ç­‰ï¼‰æ˜¯ä¸€ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰çš„è§†è§‰ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¯¹å…³ç³»çš„ç²¾ç¡®æ¨ç†åŠå…¶ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ROBINï¼šä¸€ä¸ªç”¨å¯†é›†æ³¨é‡Šå…³ç³»è°ƒæ•´çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æŒ‡ä»¤ï¼Œèƒ½å¤Ÿå¤§è§„æ¨¡æ„å»ºé«˜è´¨é‡å¯†é›†åœºæ™¯å›¾ã€‚ä¸ºäº†è®­ç»ƒROBINï¼Œæˆ‘ä»¬æ•´ç†äº†SVGï¼Œä¸€ä¸ªåˆæˆåœºæ™¯å›¾æ•°æ®é›†ï¼Œé€šè¿‡å®Œæˆç°æœ‰åœºæ™¯å›¾ä¸­é€‰å®šå¯¹è±¡çš„ç¼ºå¤±å…³ç³»ï¼Œå¹¶åˆ©ç”¨æ•™å¸ˆå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å’Œç²¾å¿ƒè®¾è®¡çš„è¿‡æ»¤è¿‡ç¨‹æ¥ç¡®ä¿é«˜è´¨é‡ã€‚ä¸ºäº†ä¸ºä»»ä½•å›¾åƒç”Ÿæˆæ›´å‡†ç¡®å’Œä¸°å¯Œçš„åœºæ™¯å›¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†SG-EDITï¼šä¸€ä¸ªè‡ªæˆ‘è’¸é¦æ¡†æ¶ï¼ŒGPT-4oè¿›ä¸€æ­¥ç²¾ç‚¼ROBINé¢„æµ‹çš„åœºæ™¯å›¾ï¼Œé€šè¿‡åˆ é™¤ä¸å¤ªå¯èƒ½çš„å…³ç³»å’Œ&#x2F;æˆ–æå‡ºç›¸å…³çš„å…³ç³»ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ€»å…±åŒ…å«14.6ä¸‡å¼ å›¾åƒå’Œ560ä¸‡å…³ç³»ï¼Œæ¶‰åŠå¯¹è±¡è¾¾260ä¸‡ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ROBIN-3Bæ¨¡å‹è™½ç„¶åœ¨å°‘äº300ä¸‡ä¸ªå®ä¾‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨å…³ç³»ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å´è¶…è¿‡äº†åœ¨è¶…è¿‡3äº¿ä¸ªå®ä¾‹ä¸Šè¿›è¡Œè®­ç»ƒçš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†å‚æ•°é«˜è¾¾13Bçš„å¤§å‹æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨æŒ‡ä»£è¡¨è¾¾å¼ç†è§£æ–¹é¢è¾¾åˆ°äº†88.9åˆ†ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æˆç»©87.4åˆ†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ç²¾ç»†åœºæ™¯å›¾æ•°æ®è¿›è¡Œè®­ç»ƒå¯¹äºåœ¨ä¸åŒè§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¿æŒé«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07643v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰å…³ç³»æ¨ç†æ˜¯äººç±»è®¤çŸ¥çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œä½†åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰ä¸­å¯¹å…³ç³»çš„ç²¾ç¡®æ¨ç†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ROBINï¼Œä¸€ç§é€šè¿‡å¯†é›†æ³¨é‡Šå…³ç³»è¿›è¡Œè®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒï¼Œèƒ½å¤Ÿæ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡å¯†é›†åœºæ™¯å›¾ã€‚ä¸ºäº†è®­ç»ƒROBINï¼Œä½œè€…ä½¿ç”¨SVGæ•°æ®é›†ï¼Œé€šè¿‡å®Œæˆç°æœ‰åœºæ™¯å›¾ä¸­é€‰å®šå¯¹è±¡çš„ç¼ºå¤±å…³ç³»è¿›è¡Œæ•™å¸ˆMLMçš„ç²¾å¿ƒè®¾è®¡è¿‡æ»¤è¿‡ç¨‹ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡çš„æ•°æ®ã€‚ä¸ºäº†ç”Ÿæˆä»»ä½•å›¾åƒçš„å¤§è§„æ¨¡å‡†ç¡®ä¸”ä¸°å¯Œçš„åœºæ™¯å›¾ï¼Œä½œè€…å¼•å…¥äº†SG-EDITï¼Œä¸€ä¸ªè‡ªæˆ‘è’¸é¦æ¡†æ¶ï¼Œå…¶ä¸­GPT-4oè¿›ä¸€æ­¥æ”¹è¿›äº†ROBINé¢„æµ‹çš„åœºæ™¯å›¾ã€‚å®éªŒç»“æœæ˜¾è¡¨ï¼ŒROBIN-3Bæ¨¡å‹åœ¨å…³ç³»ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼Œç”šè‡³åœ¨å¼•ç”¨è¡¨è¾¾å¼ç†è§£æ–¹é¢è¾¾åˆ°äº†88.9åˆ†çš„æœ€ä½³è¡¨ç°ã€‚è¿™è¡¨æ˜åœ¨ç²¾ç‚¼çš„åœºæ™¯å›¾æ•°æ®è¿›è¡Œè®­ç»ƒå¯¹äºä¿æŒå¤šæ ·åŒ–çš„è§†è§‰æ¨ç†ä»»åŠ¡çš„é«˜æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å…³ç³»æ¨ç†æ˜¯äººç±»è®¤çŸ¥çš„åŸºç¡€ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLMsï¼‰åœ¨ç²¾ç¡®æ¨ç†è§†è§‰å…³ç³»æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ROBINæ˜¯ä¸€ä¸ªé€šè¿‡å¯†é›†æ³¨é‡Šå…³ç³»è¿›è¡Œè®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡çš„åœºæ™¯å›¾ã€‚</li>
<li>SVGæ•°æ®é›†é€šè¿‡å®Œæˆç°æœ‰åœºæ™¯å›¾ä¸­é€‰å®šå¯¹è±¡çš„ç¼ºå¤±å…³ç³»æ¥è®­ç»ƒROBINã€‚</li>
<li>SG-EDITæ˜¯ä¸€ä¸ªè‡ªæˆ‘è’¸é¦æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆæ›´å‡†ç¡®ä¸”ä¸°å¯Œçš„åœºæ™¯å›¾ã€‚</li>
<li>ROBIN-3Bæ¨¡å‹åœ¨å…³ç³»ç†è§£å’Œå¼•ç”¨è¡¨è¾¾å¼ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-924f4293351ff51bb94798301ffe523e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360b760e3e843fcc72df2f251f2e1f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5570a0daad4790bb2a7b4b9b2f2c0518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abbba258c2e7da9cae69db4a07393801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f666ae0aaa166acb9ccbf70dfc72c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69f4b31120041786c1b6806a233075f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition"><a href="#GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition" class="headerlink" title="GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition"></a>GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition</h2><p><strong>Authors:Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He</strong></p>
<p>Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of \textit{Faithfully Recognize What Youâ€™ve Seen}, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>. </p>
<blockquote>
<p>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯å°†åŒ–å­¦çŸ¥è¯†æ•°å­—åŒ–çš„å…³é”®è¿‡ç¨‹ï¼Œé€šè¿‡å°†åˆ†å­å›¾åƒè½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼æ¥å®ç°ã€‚å°½ç®¡æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬çš„å›¾åƒæè¿°æ–¹æ³•å¾€å¾€éš¾ä»¥å¤„ç†å¤æ‚çš„åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´çš„æ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GTR-Mol-VLMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰â€œå›¾å½¢éå†ä½œä¸ºè§†è§‰æ€ç»´é“¾â€æœºåˆ¶ï¼Œé€šè¿‡è¿ç»­åŸå­é”®é¢„æµ‹é€æ­¥è§£æåˆ†å­å›¾æ¥æ¨¡æ‹Ÿäººç±»æ¨ç†ï¼›ï¼ˆ2ï¼‰â€œçœŸå®è¯†åˆ«ä½ æ‰€çœ‹åˆ°çš„â€çš„æ•°æ®ä¸­å¿ƒåŸåˆ™ï¼Œè§£å†³å›¾åƒä¸­ç¼©ç•¥ç»“æ„ä¸æ‰©å±•æ³¨é‡Šä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Mï¼Œå…¶ä¸­åŒ…å«ä»”ç»†æ ¡æ­£çš„æ³¨é‡Šï¼Œå¹¶å¼•å…¥äº†ä¸“ä¸ºOCSRå›¾ä¸­è§£æå‡†ç¡®æ€§è¿›è¡Œç²¾ç»†è¯„ä¼°è€Œè®¾è®¡çš„ç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•MolRec-Benchã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMä¸ä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸVLMå’Œå•†ä¸šé€šç”¨VLMç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚ç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¸¦æœ‰å®˜èƒ½å›¢ç¼©ç•¥çš„åˆ†å­å›¾åƒçš„åœºæ™¯ä¸­ï¼ŒGTR-Mol-VLMåœ¨SMILESå’ŒåŸºäºå›¾çš„æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºç¬¬äºŒååŸºå‡†çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå°†æ¨åŠ¨OCSRæŠ€æœ¯æ›´æœ‰æ•ˆåœ°æ»¡è¶³ç°å®ä¸–ç•Œçš„éœ€æ±‚ï¼Œä»è€Œæ¨åŠ¨åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>å‘å¸ƒGTR-CoTã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07553v1">PDF</a> </p>
<p><strong>Summary</strong><br>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯å®ç°åŒ–å­¦çŸ¥è¯†æ•°å­—åŒ–çš„å…³é”®ï¼Œèƒ½å¤Ÿå°†åˆ†å­å›¾åƒè½¬åŒ–ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚ä¸ºäº†å…‹æœå½“å‰æ¨¡å‹åœ¨è§£æå¤æ‚åˆ†å­ç»“æ„å’Œå¤„ç†æ ‡æ³¨ä¸ä¸€è‡´ç­‰é—®é¢˜ä¸Šçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GTR-Mol-VLMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼ŒåŒ…æ‹¬æ¨¡æ‹Ÿäººç±»æ¨ç†çš„â€œå›¾éå†ä½œä¸ºè§†è§‰æ€ç»´é“¾â€æœºåˆ¶å’Œè§£å†³å›¾åƒç¼©ç•¥ç»“æ„ä¸æ‰©å±•æ ‡æ³¨ä¸åŒ¹é…é—®é¢˜çš„â€œæ‰€è§å³æ‰€è¯†â€æ•°æ®ä¸­å¿ƒåŸåˆ™ã€‚ä¸ºæ”¯æŒæ¨¡å‹å‘å±•ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Mï¼Œå¹¶å¼•å…¥äº†é’ˆå¯¹å›¾å½¢è§£æå‡†ç¡®æ€§çš„OCSRé¦–æ¦œMolRec-Benchã€‚å®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMç›¸è¾ƒäºä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸVLMå’Œå•†ä¸šé€šç”¨VLMå–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰å®˜èƒ½å›¢ç¼©ç•¥çš„åˆ†å­å›¾åƒåœºæ™¯ä¸­ï¼Œè¾ƒç¬¬äºŒåæ¨¡å‹é«˜å‡ºçº¦14ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™å°†åœ¨åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸæ¨åŠ¨OCSRæŠ€æœ¯æ›´å¥½åœ°æ»¡è¶³ç°å®éœ€æ±‚ã€‚ç›¸å…³æ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/gtr-cot%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/opendatalab/GTR-CoTå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCSRåœ¨åŒ–å­¦çŸ¥è¯†æ•°å­—åŒ–ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèƒ½å°†åˆ†å­å›¾åƒè½¬åŒ–ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚</li>
<li>å½“å‰VLMåœ¨OCSRä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¤æ‚åˆ†å­ç»“æ„è§£æå’Œæ ‡æ³¨ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>GTR-Mol-VLMæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿäººç±»æ¨ç†å’Œå›¾éå†æœºåˆ¶è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>GTR-Mol-VLMåŒ…æ‹¬ä¸¤å¤§åˆ›æ–°ï¼šGraph Traversal as Visual Chain of Thoughtå’ŒFaithfully Recognize What Youâ€™ve SeenåŸåˆ™ã€‚</li>
<li>ä¸ºæ”¯æŒæ¨¡å‹å‘å±•ï¼Œæ„å»ºäº†å¤§å‹æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Må’Œé’ˆå¯¹å›¾å½¢è§£æå‡†ç¡®æ€§çš„MolRec-Benchã€‚</li>
<li>GTR-Mol-VLMåœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰å®˜èƒ½å›¢ç¼©ç•¥çš„åˆ†å­å›¾åƒåœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2948ec9739215f865d5391b777589216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66ff9a7989f7206f8faf4a9bcbc1b83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2356decec6238f37994b975776cad5e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f3054bcb2af3a39565c202059e7abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c47d29fef216a4706ab4f75e44828a4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-What-Reinforcement-Learning-Canâ€™t-Interleaved-Online-Fine-Tuning-for-Hardest-Questions"><a href="#Learning-What-Reinforcement-Learning-Canâ€™t-Interleaved-Online-Fine-Tuning-for-Hardest-Questions" class="headerlink" title="Learning What Reinforcement Learning Canâ€™t: Interleaved Online   Fine-Tuning for Hardest Questions"></a>Learning What Reinforcement Learning Canâ€™t: Interleaved Online   Fine-Tuning for Hardest Questions</h2><p><strong>Authors:Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, Wentao Zhang</strong></p>
<p>Recent advances in large language model (LLM) reasoning have shown that sophisticated behaviors such as planning and self-reflection can emerge through reinforcement learning (RL). However, despite these successes, RL in its current form remains insufficient to induce capabilities that exceed the limitations of the base model, as it is primarily optimized based on existing knowledge of the model rather than facilitating the acquisition of new information. To address this limitation, we employ supervised fine-tuning (SFT) to learn what RL cannot, which enables the incorporation of new knowledge and reasoning patterns by leveraging high-quality demonstration data. We analyze the training dynamics of RL and SFT for LLM reasoning and find that RL excels at maintaining and improving performance on questions within the modelâ€™s original capabilities, while SFT is more effective at enabling progress on questions beyond the current scope of the model. Motivated by the complementary strengths of RL and SFT, we introduce a novel training approach, \textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily trained using RL, but when it encounters challenging questions, high-quality solutions are collected for fine-tuning, and the training process alternates between RL and fine-tuning to enhance the modelâ€™s reasoning abilities. ReLIFT achieves an average improvement of over +5.2 points across five competition-level benchmarks and one out-of-distribution benchmark compared to other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both RL and SFT while using only 13% of the detailed demonstration data, highlighting its scalability. These results provide compelling evidence that ReLIFT overcomes the fundamental limitations of RL and underscores the significant potential. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å‡ºç°è§„åˆ’ã€è‡ªæˆ‘åæ€ç­‰å¤æ‚è¡Œä¸ºã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†è¿™äº›æˆåŠŸï¼Œå½“å‰çš„å¼ºåŒ–å­¦ä¹ å½¢å¼ä»ç„¶ä¸è¶³ä»¥è¯±å¯¼è¶…è¶ŠåŸºç¡€æ¨¡å‹å±€é™çš„èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä¸»è¦æ˜¯åŸºäºæ¨¡å‹ç°æœ‰çŸ¥è¯†çš„ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ä¿ƒè¿›æ–°ä¿¡æ¯çš„è·å–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å­¦ä¹ å¼ºåŒ–å­¦ä¹ æ‰€ä¸èƒ½æ¶‰åŠçš„é¢†åŸŸï¼Œè¿™é€šè¿‡åˆ©ç”¨é«˜è´¨é‡æ¼”ç¤ºæ•°æ®ï¼Œå®ç°äº†æ–°çŸ¥è¯†å’Œæ¨ç†æ¨¡å¼çš„èåˆã€‚æˆ‘ä»¬åˆ†æäº†å¼ºåŒ–å­¦ä¹ å’ŒSFTåœ¨LLMæ¨ç†æ–¹é¢çš„è®­ç»ƒåŠ¨æ€ï¼Œå¹¶å‘ç°å¼ºåŒ–å­¦ä¹ åœ¨ç»´æŒå’Œæ”¹è¿›æ¨¡å‹åŸæœ‰èƒ½åŠ›èŒƒå›´å†…çš„é—®é¢˜æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè€ŒSFTåœ¨è§£å†³è¶…å‡ºæ¨¡å‹å½“å‰èŒƒå›´çš„é—®é¢˜æ—¶æ›´ä¸ºæœ‰æ•ˆã€‚å—å¼ºåŒ–å­¦ä¹ å’ŒSFTäº’è¡¥ä¼˜ç‚¹çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è®­ç»ƒæ–¹æ³•â€”â€”ReLIFTï¼ˆ\textbf{Re}inforcement \textbf{L}earningä¸åœ¨çº¿\textbf{F}ine-\textbf{T}uningäº¤æ›¿è¿›è¡Œçš„\textbf{I}nterleaved with \textbf{T}ask-\textbf{S}pecific Fine-\textbf{T}uningï¼‰ã€‚åœ¨ReLIFTä¸­ï¼Œæ¨¡å‹ä¸»è¦ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä½†å½“é‡åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æ—¶ï¼Œä¼šæ”¶é›†é«˜è´¨é‡è§£å†³æ–¹æ¡ˆè¿›è¡Œå¾®è°ƒï¼Œè®­ç»ƒè¿‡ç¨‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸å¾®è°ƒä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºå…¶ä»–é›¶å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼ŒReLIFTåœ¨äº”ä¸ªç«èµ›åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªè¶…å‡ºåˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†+5.2åˆ†çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä»…ä½¿ç”¨13%çš„è¯¦ç»†æ¼”ç¤ºæ•°æ®çš„æƒ…å†µä¸‹å±•ç¤ºäº†ReLIFTä¼˜äºå¼ºåŒ–å­¦ä¹ å’ŒSFTçš„è¡¨ç°ï¼Œçªæ˜¾äº†å…¶å¯æ‰©å±•æ€§ã€‚è¿™äº›ç»“æœæä¾›äº†æœ‰åŠ›è¯æ®è¡¨æ˜ReLIFTå…‹æœäº†å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶çªå‡ºäº†å…¶å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07527v1">PDF</a> 12 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†é¢†åŸŸçš„è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å±•ç°å‡ºè§„åˆ’ã€è‡ªæˆ‘åæ€ç­‰å¤æ‚è¡Œä¸ºã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰è¿™äº›æˆåŠŸï¼Œå½“å‰çš„RLå½¢å¼ä»ç„¶ä¸è¶³ä»¥è¯±å¯¼è¶…è¿‡åŸºç¡€æ¨¡å‹å±€é™æ€§çš„èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä¸»è¦æ˜¯åŸºäºæ¨¡å‹ç°æœ‰çŸ¥è¯†çš„ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ä¿ƒè¿›æ–°ä¿¡æ¯çš„è·å–ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å­¦ä¹ RLæ‰€ä¸èƒ½çš„ï¼Œé€šè¿‡åˆ©ç”¨é«˜è´¨é‡ç¤ºèŒƒæ•°æ®ï¼Œå®ç°æ–°çŸ¥è¯†å’Œæ¨ç†æ¨¡å¼çš„èå…¥ã€‚åˆ†æRLå’ŒSFTçš„LLMæ¨ç†è®­ç»ƒåŠ¨æ€å‘ç°ï¼ŒRLåœ¨ä¿æŒå’Œæ”¹è¿›æ¨¡å‹åŸæœ‰èƒ½åŠ›èŒƒå›´å†…çš„æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€ŒSFTåœ¨ä½¿æ¨¡å‹è¿›æ­¥äºè¶…å‡ºå½“å‰èŒƒå›´çš„é—®é¢˜ä¸Šæ›´æœ‰æ•ˆã€‚å—RLå’ŒSFTäº’è¡¥ä¼˜åŠ¿çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„LLMè®­ç»ƒæ–¹å¼â€”â€”<strong>ReLIFT</strong>ï¼ˆ<strong>Re</strong>inforcement <strong>L</strong>earning <strong>I</strong>nterleaved with Online <strong>F</strong>ine-\textbf{T}uningï¼‰ã€‚åœ¨ReLIFTä¸­ï¼Œæ¨¡å‹ä¸»è¦ä½¿ç”¨RLè¿›è¡Œè®­ç»ƒï¼Œä½†å½“é‡åˆ°æœ‰æŒ‘æˆ˜çš„é—®é¢˜æ—¶ï¼Œæ”¶é›†é«˜è´¨é‡è§£å†³æ–¹æ¡ˆè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨RLå’Œå¾®è°ƒä¹‹é—´äº¤æ›¿è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºå…¶ä»–é›¶RLæ¨¡å‹ï¼ŒReLIFTåœ¨äº”ä¸ªç«èµ›çº§åŸºå‡†å’Œä¸€ä¸ªç¦»åˆ†å¸ƒåŸºå‡†ä¸Šçš„å¹³å‡æé«˜äº†è¶…è¿‡+5.2ç‚¹ï¼›æ­¤å¤–ï¼ŒReLIFTçš„è¡¨ç°ä¼˜äºRLå’ŒSFTï¼ŒåŒæ—¶ä»…ä½¿ç”¨13%çš„è¯¦ç»†æ¼”ç¤ºæ•°æ®ï¼Œå‡¸æ˜¾äº†å…¶å¯æ‰©å±•æ€§ã€‚è¿™äº›ç»“æœæä¾›äº†æœ‰åŠ›è¯æ®ï¼Œè¯æ˜ReLIFTå…‹æœäº†RLçš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶çªå‡ºäº†å…¶æ˜¾è‘—æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­äº§ç”Ÿå¤æ‚è¡Œä¸ºï¼Œå¦‚è§„åˆ’å’Œè‡ªæˆ‘åæ€ã€‚</li>
<li>å½“å‰RLçš„å±€é™æ€§åœ¨äºå…¶æ— æ³•è¯±å¯¼è¶…è¿‡åŸºç¡€æ¨¡å‹èƒ½åŠ›èŒƒå›´çš„æ–°èƒ½åŠ›æˆ–çŸ¥è¯†ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰èƒ½å¤Ÿæœ‰æ•ˆç»“åˆæ–°çŸ¥è¯†å’Œæ¨ç†æ¨¡å¼ï¼Œå¼¥è¡¥RLçš„ä¸è¶³ã€‚</li>
<li>RLåœ¨ä¿æŒå’Œæ”¹è¿›æ¨¡å‹åŸæœ‰æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€ŒSFTæ›´æ“…é•¿è§£å†³è¶…å‡ºæ¨¡å‹å½“å‰èŒƒå›´çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è®­ç»ƒæ–¹å¼â€”â€”ReLIFTï¼Œç»“åˆäº†RLå’Œåœ¨çº¿å¾®è°ƒçš„ä¼˜åŠ¿ï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReLIFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœå’Œæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1030a8555a2e603f07708543fddd7c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd5f39b5171a51ed4e322ffb02fbaba0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-109cfbc6aa1dc3b166553229923d615d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-414cd195d1582e702a249e4a05b32625.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Boosting-Vulnerability-Detection-of-LLMs-via-Curriculum-Preference-Optimization-with-Synthetic-Reasoning-Data"><a href="#Boosting-Vulnerability-Detection-of-LLMs-via-Curriculum-Preference-Optimization-with-Synthetic-Reasoning-Data" class="headerlink" title="Boosting Vulnerability Detection of LLMs via Curriculum Preference   Optimization with Synthetic Reasoning Data"></a>Boosting Vulnerability Detection of LLMs via Curriculum Preference   Optimization with Synthetic Reasoning Data</h2><p><strong>Authors:Xin-Cheng Wen, Yijun Yang, Cuiyun Gao, Yang Xiao, Deheng Ye</strong></p>
<p>Large language models (LLMs) demonstrate considerable proficiency in numerous coding-related tasks; however, their capabilities in detecting software vulnerabilities remain limited. This limitation primarily stems from two factors: (1) the absence of reasoning data related to vulnerabilities, which hinders the modelsâ€™ ability to capture underlying vulnerability patterns; and (2) their focus on learning semantic representations rather than the reason behind them, thus failing to recognize semantically similar vulnerability samples. Furthermore, the development of LLMs specialized in vulnerability detection is challenging, particularly in environments characterized by the scarcity of high-quality datasets. In this paper, we propose a novel framework ReVD that excels at mining vulnerability patterns through reasoning data synthesizing and vulnerability-specific preference optimization. Specifically, we construct forward and backward reasoning processes for vulnerability and corresponding fixed code, ensuring the synthesis of high-quality reasoning data. Moreover, we design the triplet supervised fine-tuning followed by curriculum online preference optimization for enabling ReVD to better understand vulnerability patterns. The extensive experiments conducted on PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for LLM-based software vulnerability detection, e.g., 12.24%-22.77% improvement in the accuracy. The source code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Xin-Cheng-Wen/PO4Vul">https://github.com/Xin-Cheng-Wen/PO4Vul</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªç¼–ç¨‹ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨æ£€æµ‹è½¯ä»¶æ¼æ´æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚è¿™ç§é™åˆ¶ä¸»è¦æºäºä¸¤ä¸ªå› ç´ ï¼šï¼ˆ1ï¼‰ç¼ºå°‘å…³äºæ¼æ´çš„æ¨ç†æ•°æ®ï¼Œè¿™é˜»ç¢äº†æ¨¡å‹æ•æ‰åº•å±‚æ¼æ´æ¨¡å¼çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰å®ƒä»¬ä¾§é‡äºå­¦ä¹ è¯­ä¹‰è¡¨ç¤ºè€ŒéèƒŒåçš„åŸå› ï¼Œå› æ­¤æ— æ³•è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼çš„æ¼æ´æ ·æœ¬ã€‚æ­¤å¤–ï¼Œå¼€å‘ä¸“é—¨ç”¨äºæ¼æ´æ£€æµ‹çš„LLMé¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜è´¨é‡æ•°æ®é›†ç¨€ç¼ºçš„ç¯å¢ƒä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ReVDï¼Œå®ƒé€šè¿‡åˆæˆæ¨ç†æ•°æ®å’Œä¼˜åŒ–æ¼æ´ç‰¹å®šåå¥½æ¥æŒ–æ˜æ¼æ´æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¼æ´å’Œç›¸åº”çš„å›ºå®šä»£ç æ„å»ºäº†æ­£å‘å’Œåå‘æ¨ç†è¿‡ç¨‹ï¼Œç¡®ä¿åˆæˆé«˜è´¨é‡çš„æ¨ç†æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰å…ƒç»„ç›‘ç£å¾®è°ƒï¼Œéšåæ˜¯è¯¾ç¨‹åœ¨çº¿åå¥½ä¼˜åŒ–ï¼Œä½¿ReVDèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¼æ´æ¨¡å¼ã€‚åœ¨PrimeVulå’ŒSVENæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReVDåœ¨åŸºäºLLMçš„è½¯ä»¶æ¼æ´æ£€æµ‹æ–¹é¢æ ‘ç«‹äº†æ–°çš„ä¸šç•Œæœ€ä½³æ°´å¹³ï¼Œä¾‹å¦‚åœ¨å‡†ç¡®ç‡ä¸Šæé«˜äº†12.24%~22.77%ã€‚æºä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Xin-Cheng-Wen/PO4Vul">https://github.com/Xin-Cheng-Wen/PO4Vul</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07390v1">PDF</a> Accepted by ACL 2025 Findings</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨æ£€æµ‹è½¯ä»¶æ¼æ´æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºReVDçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆæˆæ¼æ´ç›¸å…³æ¨ç†æ•°æ®å’Œä¼˜åŒ–ç‰¹å®šæ¼æ´åå¥½ï¼Œä»¥æŒ–æ˜æ¼æ´æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼ŒReVDåœ¨PrimeVulå’ŒSVENæ•°æ®é›†ä¸Šçš„è½¯ä»¶æ¼æ´æ£€æµ‹è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå‡†ç¡®ç‡æé«˜äº†12.24%\ï½22.77%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†æ£€æµ‹è½¯ä»¶æ¼æ´çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æ¼æ´æ£€æµ‹èƒ½åŠ›å—é™çš„ä¸»è¦åŸå› åŒ…æ‹¬ç¼ºä¹ç›¸å…³æ¨ç†æ•°æ®å’Œä¸“æ³¨äºè¯­ä¹‰è¡¨ç¤ºè€Œéå…¶åŸå› ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºReVDçš„æ–°æ¡†æ¶ï¼Œæ“…é•¿æŒ–æ˜æ¼æ´æ¨¡å¼ï¼Œé€šè¿‡åˆæˆæ¼æ´ç›¸å…³æ¨ç†æ•°æ®å’Œä¼˜åŒ–ç‰¹å®šæ¼æ´åå¥½ã€‚</li>
<li>ReVDæ„å»ºæ­£å‘å’Œé€†å‘æ¨ç†è¿‡ç¨‹ï¼Œä»¥ç¡®ä¿åˆæˆé«˜è´¨é‡çš„æ¨ç†æ•°æ®ã€‚</li>
<li>ReVDé‡‡ç”¨ä¸‰å…ƒç»„ç›‘ç£å¾®è°ƒï¼Œéšåè¿›è¡Œè¯¾ç¨‹åœ¨çº¿åå¥½ä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ¼æ´æ¨¡å¼ã€‚</li>
<li>åœ¨PrimeVulå’ŒSVENæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜ReVDåœ¨è½¯ä»¶æ¼æ´æ£€æµ‹æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3dd7806c5dd8ea098e5fd2190f156c23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94047ebd924a8a160187398bb5dc8ccc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-478618352fb0e6ceffac8df67b93c74b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e494f7920adbda5e6cbed4cd19333804" align="middle">
<img src="https://picx.zhimg.com/v2-b46c171cf535db221556911123c6e693.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SCGAgent-Recreating-the-Benefits-of-Reasoning-Models-for-Secure-Code-Generation-with-Agentic-Workflows"><a href="#SCGAgent-Recreating-the-Benefits-of-Reasoning-Models-for-Secure-Code-Generation-with-Agentic-Workflows" class="headerlink" title="SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code   Generation with Agentic Workflows"></a>SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code   Generation with Agentic Workflows</h2><p><strong>Authors:Rebecca Saul, Hao Wang, Koushik Sen, David Wagner</strong></p>
<p>Large language models (LLMs) have seen widespread success in code generation tasks for different scenarios, both everyday and professional. However current LLMs, despite producing functional code, do not prioritize security and may generate code with exploitable vulnerabilities. In this work, we propose techniques for generating code that is more likely to be secure and introduce SCGAgent, a proactive secure coding agent that implements our techniques. We use security coding guidelines that articulate safe programming practices, combined with LLM-generated unit tests to preserve functional correctness. In our evaluation, we find that SCGAgent is able to preserve nearly 98% of the functionality of the base Sonnet-3.7 LLM while achieving an approximately 25% improvement in security. Moreover, SCGAgent is able to match or best the performance of sophisticated reasoning LLMs using a non-reasoning model and an agentic workflow. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒåœºæ™¯ä¸‹çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å¹¿æ³›åº”ç”¨ï¼Œæ— è®ºæ˜¯æ—¥å¸¸ç”Ÿæ´»è¿˜æ˜¯ä¸“ä¸šé¢†åŸŸã€‚ç„¶è€Œï¼Œå°½ç®¡å½“å‰çš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç”ŸæˆåŠŸèƒ½æ€§çš„ä»£ç ï¼Œä½†å®ƒä»¬å¹¶ä¸ä¼˜å…ˆè€ƒè™‘å®‰å…¨æ€§ï¼Œå¯èƒ½ä¼šç”Ÿæˆå…·æœ‰å¯åˆ©ç”¨æ¼æ´çš„ä»£ç ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”Ÿæˆæ›´å¯èƒ½å®‰å…¨çš„ä»£ç çš„æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†SCGAgentï¼Œè¿™æ˜¯ä¸€ä¸ªç§¯æä¸»åŠ¨çš„å®‰å…¨ç¼–ç ä»£ç†ï¼Œå®ç°äº†æˆ‘ä»¬çš„æŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨å®‰å…¨ç¼–ç æŒ‡å—æ¥æ˜ç¡®å®‰å…¨çš„ç¼–ç¨‹å®è·µï¼Œç»“åˆLLMç”Ÿæˆçš„å•å…ƒæµ‹è¯•æ¥ä¿æŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å‘ç°SCGAgentèƒ½å¤Ÿåœ¨ä¿æŒSonnet-3.7åŸºç¡€LLMè¿‘98%çš„åŠŸèƒ½çš„åŒæ—¶ï¼Œå®ç°äº†å¤§çº¦25%çš„å®‰å…¨æ€§æå‡ã€‚æ­¤å¤–ï¼ŒSCGAgentä½¿ç”¨ä¸€ä¸ªéæ¨ç†æ¨¡å‹å’Œä»£ç†å·¥ä½œæµç¨‹ï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šå¤æ‚æ¨ç†LLMçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç”Ÿæˆçš„ä»£ç å¯èƒ½å­˜åœ¨å®‰å…¨æ¼æ´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›´å®‰å…¨ç”Ÿæˆä»£ç çš„æŠ€æœ¯ï¼Œå¹¶å¼•å…¥äº†SCGAgentï¼ˆä¸€ç§ä¸»åŠ¨å®‰å…¨ç¼–ç ä»£ç†ï¼‰ã€‚è¯¥ä»£ç†ç»“åˆäº†å®‰å…¨ç¼–ç å‡†åˆ™å’ŒLLMç”Ÿæˆçš„å•å…ƒæµ‹è¯•æ¥ä¿æŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒSCGAgentåœ¨ä¿æŒSonnet-3.7 LLMåŸºç¡€åŠŸèƒ½çš„åŒæ—¶ï¼Œå®‰å…¨æ€§æé«˜äº†çº¦25%ï¼Œå¹¶ä¸”å¯ä»¥åŒ¹é…æˆ–è¶…è¶Šé«˜çº§æ¨ç†LLMçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¿æ³›åº”ç”¨äºä»£ç ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>LLMç”Ÿæˆçš„ä»£ç å¯èƒ½å­˜åœ¨å®‰å…¨æ¼æ´ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æ›´å®‰å…¨ç”Ÿæˆä»£ç çš„æŠ€æœ¯ã€‚</li>
<li>SCGAgentæ˜¯ä¸€ç§ä¸»åŠ¨å®‰å…¨ç¼–ç ä»£ç†ï¼Œç»“åˆäº†å®‰å…¨ç¼–ç å‡†åˆ™å’ŒLLMç”Ÿæˆçš„å•å…ƒæµ‹è¯•æ¥ä¿æŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>SCGAgentèƒ½å¤Ÿåœ¨ä¿æŒåŸºç¡€åŠŸèƒ½çš„åŒæ—¶æé«˜ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§ã€‚</li>
<li>SCGAgentçš„æ€§èƒ½ä¸é«˜çº§æ¨ç†LLMç›¸å½“æˆ–æ›´ä¼˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8419ee9ef45e9339b9971f25913eaa81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73a575cb6f6e7a22afd4ed28d5f5461b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c96b2fbec806f10e90fc648a385e4ecd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f858bf3255358780eae45379546aaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9aa62ec1c60ce3e9830d742b3d82a0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b77c26e05f2a8ab5d02e5d4888077b9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="EviNet-Evidential-Reasoning-Network-for-Resilient-Graph-Learning-in-the-Open-and-Noisy-Environments"><a href="#EviNet-Evidential-Reasoning-Network-for-Resilient-Graph-Learning-in-the-Open-and-Noisy-Environments" class="headerlink" title="EviNet: Evidential Reasoning Network for Resilient Graph Learning in the   Open and Noisy Environments"></a>EviNet: Evidential Reasoning Network for Resilient Graph Learning in the   Open and Noisy Environments</h2><p><strong>Authors:Weijie Guan, Haohui Wang, Jian Kang, Lihui Liu, Dawei Zhou</strong></p>
<p>Graph learning has been crucial to many real-world tasks, but they are often studied with a closed-world assumption, with all possible labels of data known a priori. To enable effective graph learning in an open and noisy environment, it is critical to inform the model users when the model makes a wrong prediction to in-distribution data of a known class, i.e., misclassification detection or when the model encounters out-of-distribution from novel classes, i.e., out-of-distribution detection. This paper introduces Evidential Reasoning Network (EVINET), a framework that addresses these two challenges by integrating Beta embedding within a subjective logic framework. EVINET includes two key modules: Dissonance Reasoning for misclassification detection and Vacuity Reasoning for out-of-distribution detection. Extensive experiments demonstrate that EVINET outperforms state-of-the-art methods across multiple metrics in the tasks of in-distribution classification, misclassification detection, and out-of-distribution detection. EVINET demonstrates the necessity of uncertainty estimation and logical reasoning for misclassification detection and out-of-distribution detection and paves the way for open-world graph learning. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/SSSKJ/EviNET">https://github.com/SSSKJ/EviNET</a>. </p>
<blockquote>
<p>å›¾å­¦ä¹ åœ¨å¤šä¸ªç°å®ä»»åŠ¡ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯åœ¨å°é—­ä¸–ç•Œå‡è®¾ä¸‹è¿›è¡Œç ”ç©¶ï¼Œå³æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ ‡ç­¾éƒ½æ˜¯äº‹å…ˆå·²çŸ¥çš„ã€‚ä¸ºäº†åœ¨å¼€æ”¾å’Œå˜ˆæ‚çš„ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆçš„å›¾å­¦ä¹ ï¼Œæ¨¡å‹ç”¨æˆ·å¯¹æ¨¡å‹åšå‡ºçš„é”™è¯¯é¢„æµ‹çš„äº†è§£è‡³å…³é‡è¦ï¼Œæ— è®ºæ˜¯åœ¨å·²çŸ¥ç±»åˆ«çš„å†…éƒ¨æ•°æ®åˆ†å¸ƒä¸­çš„è¯¯åˆ†ç±»æ£€æµ‹ï¼Œè¿˜æ˜¯åœ¨é‡åˆ°æ¥è‡ªæ–°ç±»åˆ«çš„å¤–éƒ¨æ•°æ®åˆ†å¸ƒæ—¶çš„å¤–éƒ¨æ£€æµ‹ã€‚æœ¬æ–‡ä»‹ç»äº†è¯æ®æ¨ç†ç½‘ç»œï¼ˆEVINETï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä¸»è§‚é€»è¾‘æ¡†æ¶ä¸­çš„BetaåµŒå…¥æ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜çš„æ¡†æ¶ã€‚EVINETåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç”¨äºè¯¯åˆ†ç±»æ£€æµ‹çš„ä¸å’Œè°æ¨ç†å’Œç”¨äºå¤–éƒ¨æ£€æµ‹çš„ç©ºç¼ºæ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEVINETåœ¨å†…éƒ¨åˆ†å¸ƒåˆ†ç±»ã€è¯¯åˆ†ç±»æ£€æµ‹å’Œå¤–éƒ¨æ£€æµ‹çš„ä»»åŠ¡ä¸­ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚EVINETè¯æ˜äº†ä¸ç¡®å®šä¼°è®¡å’Œé€»è¾‘æ¨ç†å¯¹äºè¯¯åˆ†ç±»æ£€æµ‹å’Œå¤–éƒ¨æ£€æµ‹çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SSSKJ/EviNET%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SSSKJ/EviNETè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07288v1">PDF</a> KDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Evidential Reasoning Networkï¼ˆEVINETï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡é›†æˆBetaåµŒå…¥å’Œä¸»è§‚é€»è¾‘æ¥è§£å†³å›¾å­¦ä¹ åœ¨å¼€æ”¾å’Œå™ªå£°ç¯å¢ƒä¸­çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šè¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ã€‚EVINETåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç”¨äºè¯¯åˆ†ç±»æ£€æµ‹çš„ç¦»å’Œè°ç†æ¨¡å—å’Œç”¨äºæœªçŸ¥ç±»åˆ«æ£€æµ‹çš„ç©ºç™½åŒºåŸŸæ¨¡å—ã€‚å®éªŒç»“æœè¯æ˜äº†EVINETåœ¨å¤šåº¦é‡æ ‡å‡†ä¸‹çš„ä»»åŠ¡ä¸­ï¼Œå¦‚å†…éƒ¨åˆ†å¸ƒåˆ†ç±»ã€è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚EVINETæ˜¾ç¤ºä¸ç¡®å®šæ€§ä¼°è®¡å’Œé€»è¾‘æ¨ç†åœ¨è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ä¸­çš„å¿…è¦æ€§ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Evidential Reasoning Network (EVINET)æ¡†æ¶è§£å†³äº†å›¾å­¦ä¹ åœ¨å¼€æ”¾å’Œå™ªå£°ç¯å¢ƒä¸­çš„è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹æŒ‘æˆ˜ã€‚</li>
<li>EVINETåŒ…æ‹¬ç”¨äºè¯¯åˆ†ç±»æ£€æµ‹çš„ç¦»å’Œè°ç†æ¨¡å—å’Œç”¨äºæœªçŸ¥ç±»åˆ«æ£€æµ‹çš„ç©ºç™½åŒºåŸŸæ¨¡å—ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†EVINETåœ¨å¤šç§ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬å†…éƒ¨åˆ†å¸ƒåˆ†ç±»ã€è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ã€‚</li>
<li>EVINETå¼ºè°ƒäº†ä¸ç¡®å®šæ€§ä¼°è®¡å’Œé€»è¾‘æ¨ç†åœ¨è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«æ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-52f81e2efc508787762d6ba84e6690b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47896b6335ffc33d7298c904ce42790b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d9437bb8ba1033fd6a02232dfdfcbfc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Visual-Reasoning-with-Visual-Tokens-Scaling-and-Verification"><a href="#Multi-Step-Visual-Reasoning-with-Visual-Tokens-Scaling-and-Verification" class="headerlink" title="Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification"></a>Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification</h2><p><strong>Authors:Tianyi Bai, Zengjie Hu, Fupeng Sun, Jiantao Qiu, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, Wentao Zhang</strong></p>
<p>Multi-modal large language models (MLLMs) have achieved remarkable capabilities by integrating visual perception with language understanding, enabling applications such as image-grounded dialogue, visual question answering, and scientific analysis. However, most MLLMs adopt a static inference paradigm, encoding the entire image into fixed visual tokens upfront, which limits their ability to iteratively refine understanding or adapt to context during inference. This contrasts sharply with human perception, which is dynamic, selective, and feedback-driven. In this work, we introduce a novel framework for inference-time visual token scaling that enables MLLMs to perform iterative, verifier-guided reasoning over visual content. We formulate the problem as a Markov Decision Process, involving a reasoner that proposes visual actions and a verifier, which is trained via multi-step Direct Preference Optimization (DPO), that evaluates these actions and determines when reasoning should terminate. To support this, we present a new dataset, VTS, comprising supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning comparisons (VTS-DPO). Our method significantly outperforms existing approaches across diverse visual reasoning benchmarks, offering not only improved accuracy but also more interpretable and grounded reasoning processes. These results demonstrate the promise of dynamic inference mechanisms for enabling fine-grained, context-aware visual reasoning in next-generation MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ï¼Œå®ç°äº†å›¾åƒåŸºç¡€å¯¹è¯ã€è§†è§‰é—®ç­”å’Œç§‘å­¦ç ”ç©¶åˆ†æç­‰åº”ç”¨çš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MLLMsé‡‡ç”¨é™æ€æ¨ç†æ¨¡å¼ï¼Œé¢„å…ˆå°†æ•´ä¸ªå›¾åƒç¼–ç æˆå›ºå®šçš„è§†è§‰æ ‡è®°ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ä¼˜åŒ–ç†è§£æˆ–é€‚åº”ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚è¿™ä¸äººç±»åŠ¨æ€ã€æœ‰é€‰æ‹©æ€§å’Œåé¦ˆé©±åŠ¨çš„è®¤çŸ¥æ„ŸçŸ¥å½¢æˆé²œæ˜å¯¹æ¯”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ¨ç†æ—¶é—´è§†è§‰æ ‡è®°ç¼©æ”¾æ¡†æ¶ï¼Œä½¿MLLMsèƒ½å¤Ÿå¯¹è§†è§‰å†…å®¹è¿›è¡Œè¿­ä»£ã€éªŒè¯è€…å¼•å¯¼æ¨ç†ã€‚æˆ‘ä»¬å°†è¿™ä¸ªé—®é¢˜è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ¶‰åŠä¸€ä¸ªæå‡ºè§†è§‰è¡Œä¸ºçš„æ¨ç†è€…å’Œä¸€ä¸ªé€šè¿‡å¤šæ­¥éª¤ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œè®­ç»ƒçš„éªŒè¯è€…ï¼Œåè€…è¯„ä¼°è¿™äº›è¡Œä¸ºå¹¶ç¡®å®šä½•æ—¶ç»ˆæ­¢æ¨ç†ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†VTSï¼Œå®ƒç”±ç›‘ç£æ¨ç†è½¨è¿¹ï¼ˆVTS-SFTï¼‰å’Œåå¥½æ ‡è®°æ¨ç†æ¯”è¾ƒï¼ˆVTS-DPOï¼‰ç»„æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”æä¾›äº†æ›´å¯è§£é‡Šå’ŒåŸºäºç°å®çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™äº›ç»“æœè¯æ˜äº†åŠ¨æ€æ¨ç†æœºåˆ¶åœ¨ä¸‹ä¸€ä»£MLLMsä¸­å®ç°ç²¾ç»†ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰æ¨ç†çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ï¼Œå®ç°äº†å›¾åƒæ”¯æ’‘å¯¹è¯ã€è§†è§‰é—®ç­”å’Œç§‘å­¦åˆ†æç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MLLMsé‡‡ç”¨é™æ€æ¨ç†æ¨¡å¼ï¼Œé¢„å…ˆå°†æ•´ä¸ªå›¾åƒç¼–ç æˆå›ºå®šçš„è§†è§‰æ ‡è®°ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿­ä»£ä¼˜åŒ–ç†è§£æˆ–é€‚åº”ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¨ç†æ—¶è§†è§‰æ ‡è®°ç¼©æ”¾æ¡†æ¶ï¼Œä½¿MLLMsèƒ½å¤Ÿåœ¨è§†è§‰å†…å®¹ä¸Šæ‰§è¡Œè¿­ä»£ã€éªŒè¯å™¨å¼•å¯¼æ¨ç†ã€‚æˆ‘ä»¬å°†é—®é¢˜è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ¶‰åŠæå‡ºè§†è§‰åŠ¨ä½œçš„ç†ç”±å’Œé€šè¿‡å¤šæ­¥éª¤ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒçš„éªŒè¯å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”æä¾›äº†æ›´å¯è§£é‡Šå’ŒåŸºäºåœ°é¢çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™è¯æ˜äº†åŠ¨æ€æ¨ç†æœºåˆ¶åœ¨ä¸‹ä¸€ä»£MLLMsä¸­å®ç°ç²¾ç»†ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰æ¨ç†çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMså·²ç»å®ç°äº†å›¾åƒæ”¯æ’‘å¯¹è¯ã€è§†è§‰é—®ç­”å’Œç§‘å­¦åˆ†æç­‰åº”ç”¨çš„æ˜¾è‘—èƒ½åŠ›ã€‚</li>
<li>å¤§å¤šæ•°MLLMsé‡‡ç”¨é™æ€æ¨ç†æ¨¡å¼ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¿­ä»£ç†è§£å’Œä¸Šä¸‹æ–‡é€‚åº”èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ¨ç†æ—¶è§†è§‰æ ‡è®°ç¼©æ”¾æ¡†æ¶ï¼Œä½¿MLLMsèƒ½å¤Ÿæ‰§è¡Œè¿­ä»£ã€éªŒè¯å™¨å¼•å¯¼çš„æ¨ç†ã€‚</li>
<li>é—®é¢˜è¢«è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ¶‰åŠç†ç”±å’ŒéªŒè¯å™¨çš„è§’è‰²ã€‚</li>
<li>éªŒè¯å™¨é€šè¿‡å¤šæ­¥éª¤ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è®­ç»ƒã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæä¾›äº†æ›´å¯è§£é‡Šå’ŒåŸºäºåœ°é¢çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af2ad3babba9dbda60d103d9af992d09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d7a6d668a8f1b3e7fafe586292ea5c5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learn-as-Individuals-Evolve-as-a-Team-Multi-agent-LLMs-Adaptation-in-Embodied-Environments"><a href="#Learn-as-Individuals-Evolve-as-a-Team-Multi-agent-LLMs-Adaptation-in-Embodied-Environments" class="headerlink" title="Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in   Embodied Environments"></a>Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in   Embodied Environments</h2><p><strong>Authors:Xinran Li, Chenjia Bai, Zijian Li, Jiakun Zheng, Ting Xiao, Jun Zhang</strong></p>
<p>Large language models (LLMs) possess extensive knowledge bases and strong reasoning capabilities, making them promising tools for complex, multi-agent planning in embodied environments. However, despite LLMsâ€™ advanced abilities and the sophisticated modular design of agentic methods, existing LLM-based planning algorithms remain limited by weak adaptation capabilities to multi-agent embodied scenarios. We address this limitation by introducing a framework that enables LLM agents to learn and evolve both before and during test time, equipping them with environment-relevant knowledge for better planning and enhanced communication for improved cooperation. Inspired by centralized training with decentralized execution in multi-agent reinforcement learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)} paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents learn a local utility function from exploratory datasets to better comprehend the embodied environment, which is then queried during test time to support informed decision-making. At the team level, LLM agents collaboratively and iteratively maintain and update a shared cooperation knowledge list based on new experiences, using it to guide more effective communication. By combining individual learning with team evolution, LIET enables comprehensive and flexible adaptation for LLM agents. Our experiments on Communicative Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing baselines and exhibits strong cooperative planning abilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„çŸ¥è¯†åº“å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå…·æœ‰ä½“ç¯å¢ƒä¸­çš„å¤æ‚å¤šä¸»ä½“è§„åˆ’çš„æœ‰å‰é€”çš„å·¥å…·ã€‚ç„¶è€Œï¼Œå°½ç®¡LLMå…·æœ‰å…ˆè¿›çš„èƒ½åŠ›å’Œå¤æ‚çš„æ¨¡å—åŒ–è®¾è®¡ï¼Œä½†ç°æœ‰çš„åŸºäºLLMçš„è§„åˆ’ç®—æ³•åœ¨é€‚åº”å¤šä¸»ä½“ç¯å¢ƒçš„åœºæ™¯ä¸­ä»ç„¶å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ä¸ªæ¡†æ¶æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œä½¿LLMä»£ç†èƒ½å¤Ÿåœ¨æµ‹è¯•å‰åå­¦ä¹ å’Œè¿›åŒ–ï¼Œä¸ºå®ƒä»¬é…å¤‡ä¸ç¯å®é™…å¢ƒç›¸å…³çš„çŸ¥è¯†ä»¥æ›´å¥½åœ°è§„åˆ’å’Œå¢å¼ºçš„é€šä¿¡ä»¥æ”¹å–„åˆä½œã€‚å—å¤šä¸»ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é›†ä¸­è®­ç»ƒä¸åˆ†å¸ƒå¼æ‰§è¡Œçš„å¯å‘ï¼Œæˆ‘ä»¬ä¸ºå¤šä¸»ä½“LLMé€‚åº”æ€§æå‡ºäº†ä¸€ä¸ªâ€œä¸ªä½“å­¦ä¹ ï¼Œå›¢é˜Ÿè¿›åŒ–ï¼ˆLIETï¼‰â€çš„æ¨¡å¼ã€‚åœ¨ä¸ªä½“å±‚é¢ï¼ŒLLMä»£ç†ä»æ¢ç´¢æ€§æ•°æ®é›†ä¸­å­¦ä¹ æœ¬åœ°æ•ˆç”¨å‡½æ•°ä»¥æ›´å¥½åœ°äº†è§£ç¯å¢ƒï¼Œç„¶ååœ¨æµ‹è¯•æœŸé—´æŸ¥è¯¢è¯¥å‡½æ•°ä»¥æ”¯æŒå†³ç­–ã€‚åœ¨å›¢é˜Ÿå±‚é¢ï¼ŒLLMä»£ç†åŸºäºæ–°ç»éªŒåä½œå¹¶è¿­ä»£åœ°ç»´æŠ¤å’Œæ›´æ–°å…±äº«çš„åˆä½œçŸ¥è¯†åˆ—è¡¨ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥æŒ‡å¯¼æ›´æœ‰æ•ˆçš„æ²Ÿé€šã€‚é€šè¿‡å°†ä¸ªä½“å­¦ä¹ ä¸å›¢é˜Ÿè¿›åŒ–ç›¸ç»“åˆï¼ŒLIETä½¿LLMä»£ç†å…·æœ‰å…¨é¢çµæ´»çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬åœ¨é€šä¿¡è§‚å¯Ÿä¸å¸®åŠ©å’Œä¸‰ç»´ä¸–ç•Œå¤šä¸»ä½“è¿è¾“åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLIETå®ä¾‹åŒ–çš„LLaMAå’ŒGPT-4oè¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿å¹¶å±•ç°å‡ºå¼ºå¤§çš„åˆä½œè§„åˆ’èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½“ç¯å¢ƒå¤æ‚å¤šä¸»ä½“è§„åˆ’ä¸­å…·æœ‰å¹¿é˜”çš„çŸ¥è¯†åŸºç¡€å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨å¤šä¸»ä½“ä½“ç¯å¢ƒåœºæ™¯ä¸­çš„é€‚åº”æ€§æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œä½¿LLMä¸»ä½“èƒ½å¤Ÿåœ¨æµ‹è¯•å‰åå­¦ä¹ å’Œè¿›åŒ–ï¼Œé…å¤‡ä¸ç¯å¢ƒç›¸å…³çš„çŸ¥è¯†ï¼Œä»¥æ›´å¥½åœ°è§„åˆ’å’Œæ”¹è¿›åˆä½œä¸­çš„æ²Ÿé€šã€‚æˆ‘ä»¬å€Ÿé‰´äº†å¤šä¸»ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é›†ä¸­è®­ç»ƒä¸åˆ†æ•£æ‰§è¡Œç­–ç•¥ï¼Œæå‡ºäº†é¢å‘å¤šä¸»ä½“LLMé€‚åº”çš„â€œä¸ªä½“å­¦ä¹ ã€å›¢é˜Ÿè¿›åŒ–ï¼ˆLIETï¼‰â€èŒƒå¼ã€‚åœ¨ä¸ªä½“å±‚é¢ï¼ŒLLMä¸»ä½“ä»æ¢ç´¢æ•°æ®é›†å­¦ä¹ æœ¬åœ°æ•ˆç”¨å‡½æ•°ä»¥æ›´å¥½åœ°ç†è§£ä½“ç¯å¢ƒï¼Œå¹¶åœ¨æµ‹è¯•æœŸé—´æŸ¥è¯¢ä»¥æ”¯æŒå†³ç­–ã€‚åœ¨å›¢é˜Ÿå±‚é¢ï¼ŒLLMä¸»ä½“åä½œå¹¶è¿­ä»£åœ°ç»´æŠ¤å’Œæ›´æ–°å…±äº«çš„åˆä½œçŸ¥è¯†åˆ—è¡¨ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œæ²Ÿé€šã€‚é€šè¿‡ç»“åˆä¸ªä½“å­¦ä¹ ä¸å›¢é˜Ÿè¿›åŒ–ï¼ŒLIETå®ç°äº†LLMä¸»ä½“çš„å…¨é¢çµæ´»é€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼ŒLIETåœ¨é€šä¿¡è§‚å¯Ÿä¸å¸®åŠ©å’Œä¸‰ç»´ä¸–ç•Œå¤šä¸»ä½“è¿è¾“åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡å¼ºå¤§çš„çŸ¥è¯†åŸºç¡€å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨å¤šä¸»ä½“ä½“ç¯å¢ƒè§„åˆ’ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰LLMè§„åˆ’ç®—æ³•åœ¨å¤šä¸»ä½“ä½“ç¯å¢ƒåœºæ™¯ä¸­çš„é€‚åº”æ€§æœ‰é™ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ¡†æ¶ï¼Œä½¿LLMä¸»ä½“èƒ½åœ¨æµ‹è¯•å‰åå­¦ä¹ å’Œè¿›åŒ–ï¼Œä»¥æ›´å¥½åœ°é€‚åº”ç¯å¢ƒã€‚</li>
<li>æå‡ºäº†â€œä¸ªä½“å­¦ä¹ ã€å›¢é˜Ÿè¿›åŒ–ï¼ˆLIETï¼‰â€èŒƒå¼ï¼Œç»“åˆä¸ªä½“å’Œå›¢é˜Ÿå±‚é¢çš„å­¦ä¹ ä¸è¿›åŒ–ã€‚</li>
<li>åœ¨ä¸ªä½“å±‚é¢ï¼ŒLLMä¸»ä½“ä»æ¢ç´¢æ•°æ®é›†å­¦ä¹ æœ¬åœ°æ•ˆç”¨å‡½æ•°ã€‚</li>
<li>åœ¨å›¢é˜Ÿå±‚é¢ï¼ŒLLMä¸»ä½“åä½œæ›´æ–°å…±äº«çš„åˆä½œçŸ¥è¯†åˆ—è¡¨ä»¥æé«˜æ²Ÿé€šæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7aad24c1e7a87064efdaa86e3232ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b62535f050ffea0748fa89ebf55481.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4aa5c27e8791586fc04c6a2818447ea0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LLM-Enhanced-Rapid-Reflex-Async-Reflect-Embodied-Agent-for-Real-Time-Decision-Making-in-Dynamically-Changing-Environments"><a href="#LLM-Enhanced-Rapid-Reflex-Async-Reflect-Embodied-Agent-for-Real-Time-Decision-Making-in-Dynamically-Changing-Environments" class="headerlink" title="LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time   Decision-Making in Dynamically Changing Environments"></a>LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time   Decision-Making in Dynamically Changing Environments</h2><p><strong>Authors:Yangqing Zheng, Shunqi Mao, Dingxin Zhang, Weidong Cai</strong></p>
<p>In the realm of embodied intelligence, the evolution of large language models (LLMs) has markedly enhanced agent decision making. Consequently, researchers have begun exploring agent performance in dynamically changing high-risk scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under these extreme conditions, the delay in decision making emerges as a crucial yet insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that translates inference delays in decision-making into equivalent simulation frames, thus aligning cognitive and physical costs under a single FPS-based metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a lightweight LLM-guided feedback module with a rule-based agent to enable immediate reactive behaviors and asynchronous reflective refinements in situ. Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios. </p>
<blockquote>
<p>åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¼”å˜æ˜¾è‘—å¢å¼ºäº†ä»£ç†å†³ç­–åˆ¶å®šã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢ä»£ç†åœ¨åŠ¨æ€å˜åŒ–çš„é«˜é£é™©åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨HAZARDåŸºå‡†æµ‹è¯•ä¸­çš„ç«ç¾ã€æ´ªæ°´å’Œé£åŠ›åœºæ™¯ã€‚åœ¨è¿™äº›æç«¯æ¡ä»¶ä¸‹ï¼Œå†³ç­–åˆ¶å®šçš„å»¶è¿Ÿæˆä¸ºä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´è½¬æ¢æœºåˆ¶ï¼ˆTCMï¼‰ï¼Œè¯¥æœºåˆ¶å°†å†³ç­–æ¨ç†å»¶è¿Ÿè½¬åŒ–ä¸ºç­‰æ•ˆçš„æ¨¡æ‹Ÿå¸§ï¼Œä»è€Œåœ¨ä¸€ä¸ªåŸºäºFPSçš„æŒ‡æ ‡ä¸‹å¯¹é½è®¤çŸ¥å’Œèº«ä½“æˆæœ¬ã€‚é€šè¿‡æ‰©å±•HAZARDçš„å“åº”å»¶è¿Ÿï¼ˆRLï¼‰å’Œå»¶è¿Ÿåˆ°åŠ¨ä½œæ¯”ç‡ï¼ˆLARï¼‰ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®Œå…¨å»¶è¿Ÿæ„ŸçŸ¥çš„è¯„ä¼°åè®®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¿«é€Ÿååº”å¼‚æ­¥åå°„ä»£ç†ï¼ˆRRARAï¼‰ï¼Œå®ƒå°†è½»é‡çº§LLMæŒ‡å¯¼çš„åé¦ˆæ¨¡å—ä¸åŸºäºè§„åˆ™çš„ä»£ç†ç›¸ç»“åˆï¼Œä»¥å®ç°åœ¨ç°åœºçš„å³æ—¶ååº”è¡Œä¸ºå’Œå¼‚æ­¥åå°„æ”¹è¿›ã€‚åœ¨HAZARDä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRRARAåœ¨å»¶è¿Ÿæ•æ„Ÿåœºæ™¯ä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07223v1">PDF</a> Accepted by the CVPR 2025 Embodied AI Workshop</p>
<p><strong>Summary</strong></p>
<p>åœ¨å®ä½“æ™ºèƒ½é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¼”è¿›æ˜¾è‘—æå‡äº†ä»£ç†å†³ç­–åˆ¶å®šèƒ½åŠ›ã€‚ç ”ç©¶è€…å¼€å§‹åœ¨åŠ¨æ€å˜åŒ–çš„é«˜é£é™©åœºæ™¯ä¸­æ¢ç´¢ä»£ç†æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨HAZARDåŸºå‡†æµ‹è¯•ä¸­çš„ç«ç¾ã€æ´ªæ°´å’Œé£åŠ›åœºæ™¯ã€‚åœ¨æç«¯æ¡ä»¶ä¸‹ï¼Œå†³ç­–åˆ¶å®šçš„å»¶è¿Ÿæˆä¸ºä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªå……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºæ—¶é—´è½¬æ¢æœºåˆ¶ï¼ˆTCMï¼‰ï¼Œå°†å†³ç­–æ¨ç†å»¶è¿Ÿè½¬åŒ–ä¸ºç­‰æ•ˆæ¨¡æ‹Ÿå¸§ï¼Œä»è€Œåœ¨ä¸€ä¸ªåŸºäºFPSçš„å•ä¸€æŒ‡æ ‡ä¸‹å¯¹é½è®¤çŸ¥å’Œç‰©ç†æˆæœ¬ã€‚é€šè¿‡ä¸ºHAZARDå¢åŠ å“åº”å»¶è¿Ÿï¼ˆRLï¼‰å’Œå»¶è¿Ÿè¡ŒåŠ¨æ¯”ç‡ï¼ˆLARï¼‰ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®Œå…¨å»¶è¿Ÿæ„ŸçŸ¥çš„è¯„ä»·åè®®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä»‹ç»äº†å¿«é€Ÿååº”å¼‚æ­¥åæ€ä»£ç†ï¼ˆRRARAï¼‰ï¼Œè¯¥ä»£ç†ç»“åˆè½»é‡çº§LLMæŒ‡å¯¼çš„åé¦ˆæ¨¡å—å’ŒåŸºäºè§„åˆ™çš„ä»£ç†ï¼Œå¯å®ç°å³æ—¶ååº”è¡Œä¸ºå’Œå¼‚æ­¥åæ€æ”¹è¿›ã€‚åœ¨HAZARDä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRRARAåœ¨å»¶è¿Ÿæ•æ„Ÿåœºæ™¯ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç†å†³ç­–åˆ¶å®šä¸­çš„æ¼”è¿›ï¼Œå¢å¼ºäº†ä»£ç†åœ¨å¤æ‚åœºæ™¯ä¸­çš„å†³ç­–èƒ½åŠ›ã€‚</li>
<li>åœ¨åŠ¨æ€å˜åŒ–çš„é«˜é£é™©åœºæ™¯ä¸­ï¼Œå¦‚ç«ç¾ã€æ´ªæ°´å’Œé£åŠ›ç­‰æç«¯æ¡ä»¶ä¸‹ï¼Œå†³ç­–åˆ¶å®šçš„å»¶è¿Ÿæˆä¸ºå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ—¶é—´è½¬æ¢æœºåˆ¶ï¼ˆTCMï¼‰ï¼Œå°†æ¨ç†å»¶è¿Ÿè½¬åŒ–ä¸ºç­‰æ•ˆæ¨¡æ‹Ÿå¸§ï¼Œç»Ÿä¸€è®¤çŸ¥å’Œç‰©ç†æˆæœ¬åº¦é‡ã€‚</li>
<li>æ‰©å±•äº†HAZARDåŸºå‡†æµ‹è¯•ï¼Œå¢åŠ å“åº”å»¶è¿Ÿï¼ˆRLï¼‰å’Œå»¶è¿Ÿè¡ŒåŠ¨æ¯”ç‡ï¼ˆLARï¼‰ï¼Œä»¥æ”¯æŒå»¶è¿Ÿæ„ŸçŸ¥çš„è¯„ä»·ã€‚</li>
<li>ä»‹ç»äº†å¿«é€Ÿååº”å¼‚æ­¥åæ€ä»£ç†ï¼ˆRRARAï¼‰ï¼Œèåˆäº†LLMåé¦ˆæ¨¡å—å’ŒåŸºäºè§„åˆ™çš„ä»£ç†ï¼Œä»¥æ”¯æŒå³æ—¶ååº”å’Œå¼‚æ­¥åæ€ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒRRARAåœ¨å»¶è¿Ÿæ•æ„Ÿåœºæ™¯ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd7be5dacf53e66bb86fa215b1794137.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd6616196daa08efa4b06c935c7f847.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c26749c0e1e150ac16d161f9e046d29a.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Play to Generalize Learning to Reason Through Game Play
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e0214daec15f8469b37a2ea28f1f0220.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  MOGO Residual Quantized Hierarchical Causal Transformer for   High-Quality and Real-Time 3D Human Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
