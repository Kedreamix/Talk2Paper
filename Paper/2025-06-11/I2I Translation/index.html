<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-11  Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b51f93e5910a58de4b7c61f4a246da8b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-11-更新"><a href="#2025-06-11-更新" class="headerlink" title="2025-06-11 更新"></a>2025-06-11 更新</h1><h2 id="Design-and-Evaluation-of-Deep-Learning-Based-Dual-Spectrum-Image-Fusion-Methods"><a href="#Design-and-Evaluation-of-Deep-Learning-Based-Dual-Spectrum-Image-Fusion-Methods" class="headerlink" title="Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods"></a>Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods</h2><p><strong>Authors:Beining Xu, Junxian Li</strong></p>
<p>Visible images offer rich texture details, while infrared images emphasize salient targets. Fusing these complementary modalities enhances scene understanding, particularly for advanced vision tasks under challenging conditions. Recently, deep learning-based fusion methods have gained attention, but current evaluations primarily rely on general-purpose metrics without standardized benchmarks or downstream task performance. Additionally, the lack of well-developed dual-spectrum datasets and fair algorithm comparisons hinders progress.   To address these gaps, we construct a high-quality dual-spectrum dataset captured in campus environments, comprising 1,369 well-aligned visible-infrared image pairs across four representative scenarios: daytime, nighttime, smoke occlusion, and underpasses. We also propose a comprehensive and fair evaluation framework that integrates fusion speed, general metrics, and object detection performance using the lang-segment-anything model to ensure fairness in downstream evaluation.   Extensive experiments benchmark several state-of-the-art fusion algorithms under this framework. Results demonstrate that fusion models optimized for downstream tasks achieve superior performance in target detection, especially in low-light and occluded scenes. Notably, some algorithms that perform well on general metrics do not translate to strong downstream performance, highlighting limitations of current evaluation practices and validating the necessity of our proposed framework.   The main contributions of this work are: (1)a campus-oriented dual-spectrum dataset with diverse and challenging scenes; (2) a task-aware, comprehensive evaluation framework; and (3) thorough comparative analysis of leading fusion methods across multiple datasets, offering insights for future development. </p>
<blockquote>
<p>可见图像提供了丰富的纹理细节，而红外图像则突出显示显著目标。融合这些互补模式可以增强对场景的理解，特别是对于具有挑战性的条件下的高级视觉任务。最近，基于深度学习的融合方法已经引起了人们的关注，但当前的评价主要依赖于通用指标，缺乏标准基准测试或下游任务性能。此外，缺乏发达的双光谱数据集和公平的算法比较阻碍了进展。为了解决这些差距，我们在校园环境中构建了一个高质量的双光谱数据集，包含四种代表性场景的1369对可见光红外图像对：白天、夜晚、烟雾遮挡和过路隧道。我们还提出了一个全面公平的评估框架，该框架结合了融合速度、通用指标以及使用lang-segment-anything模型的对象检测性能，以确保下游评估的公平性。在该框架下，对几种最先进的融合算法进行了广泛的实验评估。结果表明，针对下游任务优化的融合模型在目标检测方面表现出卓越的性能，尤其在低光照和遮挡场景中。值得注意的是，一些在通用指标上表现良好的算法在下游性能上并不突出，这突出了当前评估实践的局限性，并验证了我们所提出的框架的必要性。这项工作的主要贡献是：（1）以校园为中心的双光谱数据集，具有多样化和挑战性的场景；（2）任务感知的全面评估框架；（3）多个数据集上领先融合方法的比较分析，为未来发展提供见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07779v1">PDF</a> 11 pages, 13 figures</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了可见光和红外图像的融合技术，对于场景理解有重要作用。当前缺乏标准化数据集和评估框架，因此构建了校园环境下的高质量双光谱数据集，并提出了综合评估框架。实验结果显示，针对下游任务优化的融合模型在目标检测上表现优异，特别是在低光和遮挡场景中。本文的主要贡献包括校园导向的双光谱数据集、任务感知的综合评估框架以及对领先融合方法的全面比较分析。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>可见光和红外图像融合技术对于场景理解至关重要。</li>
<li>当前缺乏标准化数据集和评估框架，限制了研究的进展。</li>
<li>构建了一个高质量的双光谱数据集，包括校园环境下的四种代表性场景。</li>
<li>提出了一个综合评估框架，结合了融合速度、通用指标和对象检测性能。</li>
<li>实验结果显示针对下游任务优化的融合模型在目标检测上表现更好。</li>
<li>一些在通用指标上表现良好的算法在下游性能上并不强，凸显了当前评估实践的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07779">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9a42d3b98b4bad62567261abeafa05e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d862f02ceb26aa7c7edcf53280dca2ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51b3125d13bfee7152fb358b63e7b62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fc63fd10f5ae90b4496a00cd6906747.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2f27930105d221a556194831039881b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f633865a6eb2457ad9086d6b8295609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b51f93e5910a58de4b7c61f4a246da8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cfbbaafbe8abff736b2c9c583db251f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Culturally-diverse-Multilingual-Multimodal-Video-Benchmark-Model"><a href="#A-Culturally-diverse-Multilingual-Multimodal-Video-Benchmark-Model" class="headerlink" title="A Culturally-diverse Multilingual Multimodal Video Benchmark &amp; Model"></a>A Culturally-diverse Multilingual Multimodal Video Benchmark &amp; Model</h2><p><strong>Authors:Bhuiyan Sanjid Shafique, Ashmal Vayani, Muhammad Maaz, Hanoona Abdul Rasheed, Dinura Dissanayake, Mohammed Irfan Kurpath, Yahya Hmaiti, Go Inoue, Jean Lahoud, Md. Safirur Rashid, Shadid Intisar Quasem, Maheen Fatima, Franco Vidal, Mykola Maslych, Ketan Pravin More, Sanoojan Baliah, Hasindri Watawana, Yuhao Li, Fabian Farestam, Leon Schaller, Roman Tymtsiv, Simon Weber, Hisham Cholakkal, Ivan Laptev, Shin’ichi Satoh, Michael Felsberg, Mubarak Shah, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Large multimodal models (LMMs) have recently gained attention due to their effectiveness to understand and generate descriptions of visual content. Most existing LMMs are in English language. While few recent works explore multilingual image LMMs, to the best of our knowledge, moving beyond the English language for cultural and linguistic inclusivity is yet to be investigated in the context of video LMMs. In pursuit of more inclusive video LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to evaluate Video LMMs across 14 languages, including both low- and high-resource languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian, Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is designed to rigorously test video LMMs across 15 categories including eight culturally diverse categories, ranging from lifestyles and festivals to foods and rituals and from local landmarks to prominent cultural personalities. ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice questions spanning various video durations (short, medium, and long) with 8k samples that are manually verified by native language speakers. In addition, we also introduce a machine translated multilingual video training set comprising 1.2 million samples and develop a simple multilingual video LMM, named ViMUL, that is shown to provide a better tradeoff between high-and low-resource languages for video understanding. We hope our ViMUL-Bench and multilingual video LMM along with a large-scale multilingual video training set will help ease future research in developing cultural and linguistic inclusive multilingual video LMMs. Our proposed benchmark, video LMM and training data will be publicly released at <a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/ViMUL/">https://mbzuai-oryx.github.io/ViMUL/</a>. </p>
<blockquote>
<p>近期，大型多模态模型（LMM）因其理解和生成视觉内容描述的有效性而受到关注。现有的大多数LMM都是英语。虽然有一些最近的研究开始探索多语言图像LMM，但据我们所知，在视频LMM的情境中，为了文化和语言的包容性转向非英语语言尚未被研究。为了寻求更具包容性的视频LMM，我们引入了一个多语言视频LMM基准测试，名为ViMUL-Bench，旨在评估包括低资源和高资源语言在内的14种语言的视频LMM，这些语言包括英语、中文、西班牙语、法语、德语、印地语、阿拉伯语、俄语、孟加拉语、乌尔都语、僧伽罗语、泰米尔语、瑞典语和日语。我们的ViMUL-Bench被设计为严格测试包括生活方式和节日在内的文化多样性以及当地地标和著名文化人物等类别的视频LMM共十五大类别。ViMUL-Bench包括开放性问题（短形式和长形式）和选择题，涵盖各种视频时长（短、中和长），共包含经过母语者手动验证的八千个样本。此外，我们还引入了包含一百二十万样本的机器翻译多语言视频训练集，并开发了一个简单的多语言视频LMM，名为ViMUL，它在高资源和低资源语言的视频理解方面表现出更好的权衡。我们希望我们的ViMUL-Bench和多语言视频LMM以及大规模多语言视频训练集将帮助推动未来开发具有文化和语言包容性的多语言视频LMM的研究。我们提议的基准测试、视频LMM和训练数据将在<a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/ViMUL/%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://mbzuai-oryx.github.io/ViMUL/上公开发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07032v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一个名为ViMUL-Bench的多语种视频大型模态模型（LMM）基准测试。该基准测试旨在评估包括英语、中文等多达14种语言的视频LMM性能。它涵盖了文化多样性丰富的类别，包括生活方式、节日、食物、仪式以及当地地标和著名文化人物等。此外，还引入了机器翻译的多语种视频训练集，并开发了一个名为ViMUL的简单多语种视频LMM模型。该基准测试和模型旨在促进多语种视频LMM的研究发展，并将在公开网站上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）能有效理解和生成视觉内容描述，目前大部分为英语模型。</li>
<li>为提高文化及语言包容性，需要探索非英语的多语种视频LMM。</li>
<li>提出了名为ViMUL-Bench的多语种视频LMM基准测试，涵盖14种语言，包括低资源和高资源语言。</li>
<li>ViMUL-Bench包含开放性问题（短式和长式）和选择题，涉及不同视频时长和手动验证的8K样本。</li>
<li>引入了一个机器翻译的多语种视频训练集，包含120万样本。</li>
<li>开发了一个名为ViMUL的简单多语种视频LMM模型，能在高资源和低资源语言间提供更好的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-70a9b6ba372ffc82c9a9f966ff8eec0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caed7febf4abe3b84effbd104d50bf6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-626d168696be83ac186743e3e308b5ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec1f48930fcdc807703571ff53db9b78.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimal-Transport-Driven-Asymmetric-Image-to-Image-Translation-for-Nuclei-Segmentation-of-Histological-Images"><a href="#Optimal-Transport-Driven-Asymmetric-Image-to-Image-Translation-for-Nuclei-Segmentation-of-Histological-Images" class="headerlink" title="Optimal Transport Driven Asymmetric Image-to-Image Translation for   Nuclei Segmentation of Histological Images"></a>Optimal Transport Driven Asymmetric Image-to-Image Translation for   Nuclei Segmentation of Histological Images</h2><p><strong>Authors:Suman Mahapatra, Pradipta Maji</strong></p>
<p>Segmentation of nuclei regions from histological images enables morphometric analysis of nuclei structures, which in turn helps in the detection and diagnosis of diseases under consideration. To develop a nuclei segmentation algorithm, applicable to different types of target domain representations, image-to-image translation networks can be considered as they are invariant to target domain image representations. One of the important issues with image-to-image translation models is that they fail miserably when the information content between two image domains are asymmetric in nature. In this regard, the paper introduces a new deep generative model for segmenting nuclei structures from histological images. The proposed model considers an embedding space for handling information-disparity between information-rich histological image space and information-poor segmentation map domain. Integrating judiciously the concepts of optimal transport and measure theory, the model develops an invertible generator, which provides an efficient optimization framework with lower network complexity. The concept of invertible generator automatically eliminates the need of any explicit cycle-consistency loss. The proposed model also introduces a spatially-constrained squeeze operation within the framework of invertible generator to maintain spatial continuity within the image patches. The model provides a better trade-off between network complexity and model performance compared to other existing models having complex network architectures. The performance of the proposed deep generative model, along with a comparison with state-of-the-art nuclei segmentation methods, is demonstrated on publicly available histological image data sets. </p>
<blockquote>
<p>从组织学图像中分割核区域，能够对核结构进行形态计量分析，这进而有助于对所考虑的疾病进行检测和诊断。为了开发适用于不同类型目标域表示的核分割算法，可以考虑图像到图像的翻译网络，因为它们对目标域图像表示具有不变性。图像到图像翻译模型的一个重要问题是，当两个图像域之间的信息内容在本质上是不对称的时候，这些模型就会彻底失败。在这方面，论文介绍了一种用于从组织学图像中分割核结构的新型深度生成模型。所提出模型考虑了一个嵌入空间，以处理信息丰富的组织学图像空间与信息贫乏的分割图域之间的信息差异。通过巧妙地融合最优传输和测度论的概念，该模型开发了一个可逆生成器，提供了一个有效的优化框架，降低了网络复杂性。可逆生成器的概念自动消除了任何显式循环一致性的需要。所提出模型还在可逆生成器的框架内引入了空间约束的挤压操作，以保持图像斑块内的空间连续性。与其他具有复杂网络架构的现有模型相比，该模型在网络复杂度和模型性能之间提供了更好的权衡。所提出深度生成模型的性能，以及与前述最先进的核分割方法的比较，都在公开可用的组织学图像数据集上进行了演示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07023v1">PDF</a> 13 pages, 8 figures</p>
<p><strong>Summary</strong>:<br>核区域分割有助于形态计量分析，有助于检测诊断疾病。针对图像信息内容不对称问题，本文提出一种基于深度生成模型的细胞核分割方法。该模型引入嵌入空间处理图像间信息差异，并采用最优传输和度量理论概念，开发了一个可反转生成器以降低网络复杂度并提供有效优化框架。新模型在保证空间连续性的同时实现了与现有模型的良好权衡，性能已在公开可用的病理图像数据集上进行了展示。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>核区域分割有助于疾病的检测与诊断。</li>
<li>图像分割算法需要适应不同目标域表示类型。</li>
<li>图像到图像的翻译模型在处理信息内容不对称时可能失效。</li>
<li>本文提出一种基于深度生成模型的细胞核分割新方法，解决了信息贫瘠的分割映射域与丰富的组织学图像空间之间的信息差异问题。</li>
<li>模型结合了最优传输和度量理论概念，提出了一个可反转生成器，优化了网络复杂性并消除了对显式循环一致性损失的依赖。</li>
<li>模型引入了空间约束挤压操作来维持图像斑块的空间连续性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07023">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-088e13cb55609716537efa4479ebbd7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81340956c726bb9eafc0635aa417f6a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-239a63974fc624a534b5fc29670d2c4f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPC-to-3D-Novel-View-Synthesis-from-Binary-SPC-via-I2I-translation"><a href="#SPC-to-3D-Novel-View-Synthesis-from-Binary-SPC-via-I2I-translation" class="headerlink" title="SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation"></a>SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation</h2><p><strong>Authors:Sumit Sharma, Gopi Raju Matta, Kaushik Mitra</strong></p>
<p>Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging technology, capable of detecting individual photons with remarkable timing precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable image capture at exceptionally high speeds under both low and high illumination. Enabling 3D reconstruction and radiance field recovery from such SPC data holds significant promise. However, the binary nature of SPC images leads to severe information loss, particularly in texture and color, making traditional 3D synthesis techniques ineffective. To address this challenge, we propose a modular two-stage framework that converts binary SPC images into high-quality colorized novel views. The first stage performs image-to-image (I2I) translation using generative models such as Pix2PixHD, converting binary SPC inputs into plausible RGB representations. The second stage employs 3D scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian Splatting (3DGS) to generate novel views. We validate our two-stage pipeline (Pix2PixHD + Nerf&#x2F;3DGS) through extensive qualitative and quantitative experiments, demonstrating significant improvements in perceptual quality and geometric consistency over the alternative baseline. </p>
<blockquote>
<p>单光子雪崩二极管（SPAD）代表了一种前沿的成像技术，能够以惊人的时间精度检测单个光子。基于此灵敏度，单光子相机（SPC）能够在低光照和高光照条件下以极高的速度捕获图像。从这样的SPC数据中实现3D重建和辐射场恢复具有巨大的潜力。然而，SPC图像的二进制特性导致信息严重丢失，特别是在纹理和颜色方面，使得传统的3D合成技术无效。为了解决这一挑战，我们提出了一种模块化两阶段框架，将二进制SPC图像转化为高质量彩色新颖视图。第一阶段使用生成模型（如Pix2PixHD）进行图像到图像（I2I）翻译，将二进制SPC输入转换为可信的RGB表示。第二阶段采用3D场景重建技术（如神经辐射场（NeRF）或高斯喷绘（3DGS））生成新颖视图。我们通过大量的定性和定量实验验证了我们的两阶段管道（Pix2PixHD + NeRF&#x2F;3DGS），在感知质量和几何一致性方面相对于替代基线表现出了显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06890v1">PDF</a> Accepted for publication at ICIP 2025</p>
<p><strong>Summary</strong><br>     单光子雪崩二极管（SPAD）是前沿成像技术，能检测单个光子并具有出色时间精度。基于此技术的单光子相机（SPC）可在高、低光照下实现超高速成像。从SPC数据中重建3D和恢复辐射场展现巨大潜力。但SPC图像的二进制特性导致纹理和色彩信息大量损失，传统3D合成技术难以应对。为解决此挑战，我们提出模块化两阶段框架，将二进制SPC图像转化为高质量彩色新颖视图。第一阶段使用Pix2PixHD等生成模型进行图像到图像（I2I）翻译，将二进制SPC输入转化为可信的RGB表示。第二阶段采用神经网络辐射场（NeRF）或高斯喷绘（3DGS）等3D场景重建技术生成新颖视图。我们通过大量定性和定量实验验证了Pix2PixHD + Nerf&#x2F;3DGS的两阶段管道的有效性，在感知质量和几何一致性方面相较于其他基线方法表现出显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>单光子雪崩二极管（SPAD）能检测单个光子并具有出色时间精度，为高速成像提供了可能。</li>
<li>单光子相机（SPC）可实现低光照和高光照下的超高速成像。<br>3.SPC数据的3D重建和辐射场恢复具有巨大潜力。<br>4.SPC图像的二进制特性导致信息和色彩纹理大量损失，使得传统3D合成技术难以应用。<br>5.为解决此挑战，提出模块化两阶段框架，第一阶段将SPC图像转化为RGB图像，第二阶段进行3D场景重建生成新颖视图。<br>6.使用Pix2PixHD等生成模型进行图像到图像（I2I）翻译。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c0bfd0e964496670cbdb5b84f471565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b15420e147acc04274beff86e285685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0140776939a5c31ab6cf3a36001ff468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da76a65831c84633b03413fcefc71d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72b74a5bad2eea5ef881605e7e25fca4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LoopDB-A-Loop-Closure-Dataset-for-Large-Scale-Simultaneous-Localization-and-Mapping"><a href="#LoopDB-A-Loop-Closure-Dataset-for-Large-Scale-Simultaneous-Localization-and-Mapping" class="headerlink" title="LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization   and Mapping"></a>LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization   and Mapping</h2><p><strong>Authors:Mohammad-Maher Nakshbandi, Ziad Sharawy, Dorian Cojocaru, Sorin Grigorescu</strong></p>
<p>In this study, we introduce LoopDB, which is a challenging loop closure dataset comprising over 1000 images captured across diverse environments, including parks, indoor scenes, parking spaces, as well as centered around individual objects. Each scene is represented by a sequence of five consecutive images. The dataset was collected using a high resolution camera, providing suitable imagery for benchmarking the accuracy of loop closure algorithms, typically used in simultaneous localization and mapping. As ground truth information, we provide computed rotations and translations between each consecutive images. Additional to its benchmarking goal, the dataset can be used to train and fine-tune loop closure methods based on deep neural networks. LoopDB is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RovisLab/LoopDB">https://github.com/RovisLab/LoopDB</a>. </p>
<blockquote>
<p>本研究中，我们引入了LoopDB，这是一个具有挑战性的闭环数据集，包含1000张以上在不同环境下捕获的图像，包括公园、室内场景、停车位，以及围绕单个物体的场景。每个场景由五张连续图像序列表示。该数据集使用高分辨率相机收集，为评估闭环算法的准确性提供了合适的图像，通常用于同时定位和地图构建。作为地面真实信息，我们提供了每两张连续图像之间的计算旋转和平移信息。除了其基准测试目标之外，该数据集还可用于训练和微调基于深度神经网络的闭环方法。LoopDB可在<a target="_blank" rel="noopener" href="https://github.com/RovisLab/LoopDB%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RovisLab/LoopDB公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06771v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究介绍了LoopDB数据集，它是一个包含超过一千张图像的挑战性闭环数据集，图像拍摄自公园、室内场景、停车场等不同环境，以及围绕单个物体的场景。每个场景由五张连续图像组成。该数据集使用高分辨率相机收集，适合评估闭环算法（常用于同时定位和地图构建）的准确性。作为地面真实信息，我们提供了每两张连续图像之间的计算旋转和平移信息。除了作为基准测试目标外，该数据集也可用于训练和微调基于深度神经网络的闭环方法。LoopDB可在<a target="_blank" rel="noopener" href="https://github.com/RovisLab/LoopDB%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/RovisLab/LoopDB公开访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoopDB是一个包含多样环境的闭环数据集，图像数量超过一千张。</li>
<li>数据集中的每个场景由五张连续图像表示。</li>
<li>数据集使用高分辨率相机收集，适用于评估闭环算法准确性。</li>
<li>作为地面真实信息，提供了每两张连续图像间的旋转和平移数据。</li>
<li>LoopDB不仅用于基准测试，还可用于训练和微调基于深度神经网络的闭环方法。</li>
<li>该数据集公开可用，方便研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06771">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-34d3720aa7c0a917fb22a56e980a8c2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cda4c5185b02a030032d3ee4b0e226d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25ae6d5f45d7aa5527ec56370e21c258.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6a55682198d4c2b37a7b78fc5e1ea32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-680ccb3416222d6d41ac8eb622f65ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8100d3a440c2f1e120c3a78ba9c2e0be.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling"><a href="#GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling" class="headerlink" title="GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling"></a>GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling</h2><p><strong>Authors:Siran Li, Chen Liu, Ruiyang Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang</strong></p>
<p>Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet, a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage, a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment shapes. GarmageNet employs a latent diffusion transformer to synthesize panel-wise geometry images and integrates GarmageJigsaw, a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet, a large-scale dataset comprising over 10,000 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions-laying the foundation for fully automated, production-ready pipelines in digital fashion. Project page: <a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet">https://style3d.github.io/garmagenet</a>. </p>
<blockquote>
<p>真实的数字服装建模仍然是一个劳动密集型的任务，这主要是因为将二维缝纫图案转化为高保真、可用于模拟的三维服装的过程非常复杂。我们引入了GarmageNet，这是一个统一的生成框架，可以自动创建二维缝纫图案、构建缝纫关系，并合成兼容基于物理模拟的三维服装初始化。我们的方法的核心是Garmage，这是一种新的服装表示方法，它将每个面板编码为一个结构化的几何图像，有效地弥合了二维结构图案和三维服装形状之间的语义和几何差距。GarmageNet采用潜在扩散变压器来合成面板式的几何图像，并集成了GarmageJigsaw，这是一个用于预测面板轮廓上点对点缝合连接的神经模块。为了支持和评估，我们构建了GarmageSet，这是一个由超过10000件专业设计的服装组成的大规模数据集，具有详细的结构和风格注释。我们的方法展示了在多个应用场景下的通用性和有效性，包括从多模式设计概念（文本提示、草图、照片）生成可扩展的服装、从原始平面缝纫图案自动建模、从非结构化的点云中恢复图案，以及使用常规指令进行渐进式的服装编辑——这为全自动、生产准备的数字时尚管道奠定了基础。项目页面：<a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet%E3%80%82">https://style3d.github.io/garmagenet。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01483v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种名为GarmageNet的统一生成框架，该框架可自动化创建2D缝纫图案、构建缝纫关系，并合成可用于物理模拟的3D服装初始化。其核心方法是通过一种新的服装表示方式Garmage，将每个面板编码为结构化几何图像，有效桥接了2D结构图案与3D服装形状之间的语义和几何差距。GarmageNet采用潜在扩散变压器合成面板级的几何图像，并集成了GarmageJigsaw，一个用于预测面板轮廓上点对点缝合连接的神经网络模块。此外，为了支持训练和评估，构建了包含超过10,000个专业设计服装的GarmageSet大型数据集，具有详细的结构和风格注释。此方法在多种应用场景中表现出通用性和有效性，包括从多模式设计概念（文本提示、草图、照片）生成服装、从原始平面缝纫图案自动建模、从非结构化的点云恢复图案，以及使用常规指令进行渐进式服装编辑，为数字时尚领域中的全自动生产准备管道奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GarmageNet是一个自动化创建2D缝纫图案、构建缝纫关系并合成3D服装的生成框架。</li>
<li>Garmage是一种新的服装表示方法，将每个面板编码为结构化几何图像，以桥接2D和3D之间的语义和几何差距。</li>
<li>GarmageNet使用潜在扩散变压器和GarmageJigsaw来合成面板级的几何图像并预测点对点的缝合连接。</li>
<li>GarmageSet数据集包含大量专业设计的服装，具有详细的结构和风格注释，用于支持训练和评估。</li>
<li>GarmageNet具有多种应用场景，包括从多模式设计概念生成服装、从原始平面缝纫图案建模、从点云恢复图案以及使用常规指令进行渐进式服装编辑。</li>
<li>GarmageNet方法为数字时尚领域的全自动生产准备管道奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4bcdd8c482c3e442eafc91514b4777fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43dc17277258313a976b120ff7095981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de499678cd256abcaed3dabb844c4d7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85e1a357fc421e3beccafe96dd3ce669.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Manual2Skill-Learning-to-Read-Manuals-and-Acquire-Robotic-Skills-for-Furniture-Assembly-Using-Vision-Language-Models"><a href="#Manual2Skill-Learning-to-Read-Manuals-and-Acquire-Robotic-Skills-for-Furniture-Assembly-Using-Vision-Language-Models" class="headerlink" title="Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for   Furniture Assembly Using Vision-Language Models"></a>Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for   Furniture Assembly Using Vision-Language Models</h2><p><strong>Authors:Chenrui Tie, Shengxiang Sun, Jinxuan Zhu, Yiwei Liu, Jingxiang Guo, Yue Hu, Haonan Chen, Junting Chen, Ruihai Wu, Lin Shao</strong></p>
<p>Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.Project Page: <a target="_blank" rel="noopener" href="https://owensun2004.github.io/Furniture-Assembly-Web/">https://owensun2004.github.io/Furniture-Assembly-Web/</a> </p>
<blockquote>
<p>人类拥有通过解读抽象说明书来理解并执行复杂操作任务的不凡能力。然而，对于机器人来说，这一能力仍是一个巨大挑战，因为它们无法解读抽象指令并将其翻译成可执行动作。在本文中，我们提出了Manual2Skill这一新型框架，使机器人能够在高级手册指令的引导下执行复杂的装配任务。我们的方法利用视觉语言模型（VLM）从指令图像中提取结构化信息，然后使用这些信息构建分层装配图。这些图代表部件、子组件和它们之间的关系。为了促进任务执行，姿态估计模型预测每个装配步骤中组件的相对6D姿态。同时，运动规划模块为真实世界的机器人实现生成可操作序列。我们通过成功组装几个真实世界的宜家家具产品来证明Manual2Skill的有效性。这一应用突出其在效率和精确度方面的能力，能够管理长期操作任务，显著提高了机器人从说明书学习的实用性。这项工作标志着在推进机器人系统方面取得了进展，使其能够以类似于人类的能力理解和执行复杂的操作任务。项目页面：<a target="_blank" rel="noopener" href="https://owensun2004.github.io/Furniture-Assembly-Web/">https://owensun2004.github.io/Furniture-Assembly-Web/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10090v2">PDF</a> </p>
<p><strong>Summary</strong>：人类能解读抽象说明书并执行复杂的操作任务，但机器人在这方面仍面临挑战。本研究提出了Manual2Skill框架，使机器人能够根据高级手册指令执行复杂的装配任务。该框架利用视觉语言模型从指令图像中提取结构化信息，构建层次装配图，并通过姿态估计模型预测每个装配步骤组件的6D姿态，同时运动规划模块生成可用于真实机器人实施的动作序列。成功装配IKEA家具的实践应用展示了其管理长期操作任务的能力和效率，提高了机器人从说明书学习的实用性。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>机器人执行复杂操作任务的能力受限于无法解读抽象指令并转化为可执行动作。</li>
<li>Manual2Skill框架利用视觉语言模型处理指令图像并构建层次装配图，帮助机器人完成复杂装配任务。</li>
<li>姿态估计模型在装配过程中预测组件的6D姿态。</li>
<li>运动规划模块为机器人生成实际执行动作的顺序。</li>
<li>通过成功装配IKEA家具的实践应用，展示了该框架在机器人学习方面的有效性。</li>
<li>Manual2Skill框架提高了机器人处理长期操作任务的能力和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10090">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f5552b327bc86cd53ad5de775f51483a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c39a6382d4f99841375d6771f2527489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806630b42a003c4b152aec433492d706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74674c6d9ad2b4a9bdfeb48d062b52b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction"><a href="#LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction" class="headerlink" title="LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction"></a>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction</h2><p><strong>Authors:Hrishav Bakul Barua, Kalin Stefanov, Lemuel Lai En Che, Abhinav Dhall, KokSheik Wong, Ganesh Krishnasamy</strong></p>
<p>The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: <a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> </p>
<blockquote>
<p>将低动态范围（LDR）图像翻译成高动态范围（HDR）图像是一项重要的计算机视觉任务。有大量研究采用传统的非学习方法以及现代的数据驱动方法，重点使用单曝光和多曝光的LDR进行HDR图像重建。然而，大多数目前最前沿的方法都需要高质量配对的{LDR，HDR}数据集来进行模型训练。此外，关于使用未配对数据集完成该任务的文献很少，也就是说，模型学习域之间的映射，即{LDR，HDR}。本文提出了LLM-HDR方法，它将大型语言模型（LLM）的感知能力集成到一个经过修改的语义和循环一致的对抗性架构中，该架构利用未配对的{LDR，HDR}数据集进行训练。该方法引入了新型伪影和曝光感知生成器来解决视觉伪影去除问题，以及解决语义一致性这一尚未深入探讨的问题的编码器和损失函数。LLM-HDR是第一个在自监督设置中使用LLM来完成{LDR，HDR}翻译任务的方法。该方法在多个基准数据集上实现了最先进的性能，并能重建高质量的HDR图像。该工作的官方网站地址为：<a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM">https://github.com/HrishavBakulBarua/LLM</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v3">PDF</a> </p>
<p><strong>Summary</strong>:<br>此文本主要介绍了从低动态范围（LDR）图像到高动态范围（HDR）图像的翻译在计算机视觉任务中的重要性。目前大多数最先进的方法都需要高质量配对的数据集进行模型训练。本文提出了一种名为LLM-HDR的方法，该方法将大型语言模型（LLM）的感知能力融入到一个修改过的语义和循环一致的对抗性架构中，利用未配对的LDR和HDR数据集进行训练。该方法引入了新的伪影感知生成器和曝光感知生成器来解决视觉伪影去除问题，以及一个编码器和损失来解决语义一致性这一尚未探索的问题。LLM-HDR是第一个在自我监督设置中使用LLM进行LDR到HDR翻译任务的方法。该方法在多个基准数据集上取得了最先进的性能，并能重建高质量HDR图像。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LDR到HDR图像的翻译是计算机视觉领域的一个重要任务。</li>
<li>当前的方法大多需要高质量配对的数据集进行模型训练。</li>
<li>LLM-HDR方法结合了大型语言模型（LLM）的感知能力。</li>
<li>LLM-HDR采用修改过的语义和循环一致的对抗性架构。</li>
<li>该方法利用未配对的LDR和HDR数据集进行训练。</li>
<li>LLM-HDR通过引入新的伪影感知生成器和曝光感知生成器来解决视觉伪影去除问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0604352e626b0cd673be111849895840.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c796c8b662950542046b576ea2142a76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9558764027549d613a2463ed6c1f1f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PID-Physics-Informed-Diffusion-Model-for-Infrared-Image-Generation"><a href="#PID-Physics-Informed-Diffusion-Model-for-Infrared-Image-Generation" class="headerlink" title="PID: Physics-Informed Diffusion Model for Infrared Image Generation"></a>PID: Physics-Informed Diffusion Model for Infrared Image Generation</h2><p><strong>Authors:Fangyuan Mao, Jilin Mei, Shun Lu, Fuyang Liu, Liang Chen, Fangzhou Zhao, Yu Hu</strong></p>
<p>Infrared imaging technology has gained significant attention for its reliable sensing ability in low visibility conditions, prompting many studies to convert the abundant RGB images to infrared images. However, most existing image translation methods treat infrared images as a stylistic variation, neglecting the underlying physical laws, which limits their practical application. To address these issues, we propose a Physics-Informed Diffusion (PID) model for translating RGB images to infrared images that adhere to physical laws. Our method leverages the iterative optimization of the diffusion model and incorporates strong physical constraints based on prior knowledge of infrared laws during training. This approach enhances the similarity between translated infrared images and the real infrared domain without increasing extra training parameters. Experimental results demonstrate that PID significantly outperforms existing state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fangyuanmao/PID">https://github.com/fangyuanmao/PID</a>. </p>
<blockquote>
<p>红外成像技术因其低能见度条件下的可靠感知能力而受到广泛关注，促使许多研究将丰富的RGB图像转换为红外图像。然而，大多数现有的图像翻译方法将红外图像视为风格变化，忽略了其背后的物理定律，这限制了它们的实际应用。为了解决这些问题，我们提出了一种基于物理信息的扩散（PID）模型，用于将RGB图像转换为遵循物理定律的红外图像。我们的方法利用扩散模型的迭代优化，并在训练过程中结合红外定律的先验知识，融入强大的物理约束。这种方法提高了翻译后的红外图像与真实红外域之间的相似性，且没有增加额外的训练参数。实验结果表明，PID显著优于现有最先进的方法。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/fangyuanmao/PID%E5%A4%84%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/fangyuanmao/PID处下载。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09299v2">PDF</a> Accepted by Pattern Recognition</p>
<p><strong>Summary</strong></p>
<p>红外成像技术在低能见度条件下的可靠感知能力引起了广泛关注，促使许多研究将丰富的RGB图像转换为红外图像。然而，现有的图像翻译方法大多将红外图像视为风格变化，忽略了其背后的物理定律，限制了实际应用。为解决这一问题，我们提出了一个结合物理定律的扩散模型（PID），用于将RGB图像翻译为红外图像。该方法通过扩散模型的迭代优化，并在训练过程中融入基于红外定律的强物理约束。实验结果表明，PID显著优于现有最先进的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>红外成像技术在低能见度条件下具有可靠感知能力，引发广泛关注。</li>
<li>现有图像翻译方法多忽略红外图像的物理定律，限制了实际应用。</li>
<li>提出的PID模型结合物理定律进行RGB到红外图像的翻译。</li>
<li>PID模型通过扩散模型的迭代优化，融入强物理约束。</li>
<li>PID模型在翻译红外图像时，增强了与真实红外领域的相似性。</li>
<li>实验结果表明，PID模型显著优于现有最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.09299">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bdf65a0aeb2c6d679ed4ee090b7fcbf6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24d89a8656c23c0a949f3ea4d265383a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce4f80ed901ecd40f40a7a4a34f6e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4ee69c84a617dc87d54a35b18f4e56.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30b5df75b847d650ac3666deb9e58ca7.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-06-11  CyberV Cybernetics for Test-time Scaling in Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f89150e1506b94b2747fd9537302af55.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-11  Vuyko Mistral Adapting LLMs for Low-Resource Dialectal Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
