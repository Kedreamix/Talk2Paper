<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b51f93e5910a58de4b7c61f4a246da8b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="Design-and-Evaluation-of-Deep-Learning-Based-Dual-Spectrum-Image-Fusion-Methods"><a href="#Design-and-Evaluation-of-Deep-Learning-Based-Dual-Spectrum-Image-Fusion-Methods" class="headerlink" title="Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods"></a>Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods</h2><p><strong>Authors:Beining Xu, Junxian Li</strong></p>
<p>Visible images offer rich texture details, while infrared images emphasize salient targets. Fusing these complementary modalities enhances scene understanding, particularly for advanced vision tasks under challenging conditions. Recently, deep learning-based fusion methods have gained attention, but current evaluations primarily rely on general-purpose metrics without standardized benchmarks or downstream task performance. Additionally, the lack of well-developed dual-spectrum datasets and fair algorithm comparisons hinders progress.   To address these gaps, we construct a high-quality dual-spectrum dataset captured in campus environments, comprising 1,369 well-aligned visible-infrared image pairs across four representative scenarios: daytime, nighttime, smoke occlusion, and underpasses. We also propose a comprehensive and fair evaluation framework that integrates fusion speed, general metrics, and object detection performance using the lang-segment-anything model to ensure fairness in downstream evaluation.   Extensive experiments benchmark several state-of-the-art fusion algorithms under this framework. Results demonstrate that fusion models optimized for downstream tasks achieve superior performance in target detection, especially in low-light and occluded scenes. Notably, some algorithms that perform well on general metrics do not translate to strong downstream performance, highlighting limitations of current evaluation practices and validating the necessity of our proposed framework.   The main contributions of this work are: (1)a campus-oriented dual-spectrum dataset with diverse and challenging scenes; (2) a task-aware, comprehensive evaluation framework; and (3) thorough comparative analysis of leading fusion methods across multiple datasets, offering insights for future development. </p>
<blockquote>
<p>å¯è§å›¾åƒæä¾›äº†ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ï¼Œè€Œçº¢å¤–å›¾åƒåˆ™çªå‡ºæ˜¾ç¤ºæ˜¾è‘—ç›®æ ‡ã€‚èåˆè¿™äº›äº’è¡¥æ¨¡å¼å¯ä»¥å¢å¼ºå¯¹åœºæ™¯çš„ç†è§£ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹çš„é«˜çº§è§†è§‰ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„èåˆæ–¹æ³•å·²ç»å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œä½†å½“å‰çš„è¯„ä»·ä¸»è¦ä¾èµ–äºé€šç”¨æŒ‡æ ‡ï¼Œç¼ºä¹æ ‡å‡†åŸºå‡†æµ‹è¯•æˆ–ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç¼ºä¹å‘è¾¾çš„åŒå…‰è°±æ•°æ®é›†å’Œå…¬å¹³çš„ç®—æ³•æ¯”è¾ƒé˜»ç¢äº†è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬åœ¨æ ¡å›­ç¯å¢ƒä¸­æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åŒå…‰è°±æ•°æ®é›†ï¼ŒåŒ…å«å››ç§ä»£è¡¨æ€§åœºæ™¯çš„1369å¯¹å¯è§å…‰çº¢å¤–å›¾åƒå¯¹ï¼šç™½å¤©ã€å¤œæ™šã€çƒŸé›¾é®æŒ¡å’Œè¿‡è·¯éš§é“ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå…¨é¢å…¬å¹³çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†èåˆé€Ÿåº¦ã€é€šç”¨æŒ‡æ ‡ä»¥åŠä½¿ç”¨lang-segment-anythingæ¨¡å‹çš„å¯¹è±¡æ£€æµ‹æ€§èƒ½ï¼Œä»¥ç¡®ä¿ä¸‹æ¸¸è¯„ä¼°çš„å…¬å¹³æ€§ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œå¯¹å‡ ç§æœ€å…ˆè¿›çš„èåˆç®—æ³•è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„èåˆæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ä½å…‰ç…§å’Œé®æŒ¡åœºæ™¯ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€äº›åœ¨é€šç”¨æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½çš„ç®—æ³•åœ¨ä¸‹æ¸¸æ€§èƒ½ä¸Šå¹¶ä¸çªå‡ºï¼Œè¿™çªå‡ºäº†å½“å‰è¯„ä¼°å®è·µçš„å±€é™æ€§ï¼Œå¹¶éªŒè¯äº†æˆ‘ä»¬æ‰€æå‡ºçš„æ¡†æ¶çš„å¿…è¦æ€§ã€‚è¿™é¡¹å·¥ä½œçš„ä¸»è¦è´¡çŒ®æ˜¯ï¼šï¼ˆ1ï¼‰ä»¥æ ¡å›­ä¸ºä¸­å¿ƒçš„åŒå…‰è°±æ•°æ®é›†ï¼Œå…·æœ‰å¤šæ ·åŒ–å’ŒæŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼›ï¼ˆ2ï¼‰ä»»åŠ¡æ„ŸçŸ¥çš„å…¨é¢è¯„ä¼°æ¡†æ¶ï¼›ï¼ˆ3ï¼‰å¤šä¸ªæ•°æ®é›†ä¸Šé¢†å…ˆèåˆæ–¹æ³•çš„æ¯”è¾ƒåˆ†æï¼Œä¸ºæœªæ¥å‘å±•æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07779v1">PDF</a> 11 pages, 13 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯è§å…‰å’Œçº¢å¤–å›¾åƒçš„èåˆæŠ€æœ¯ï¼Œå¯¹äºåœºæ™¯ç†è§£æœ‰é‡è¦ä½œç”¨ã€‚å½“å‰ç¼ºä¹æ ‡å‡†åŒ–æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œå› æ­¤æ„å»ºäº†æ ¡å›­ç¯å¢ƒä¸‹çš„é«˜è´¨é‡åŒå…‰è°±æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ç»¼åˆè¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„èåˆæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å…‰å’Œé®æŒ¡åœºæ™¯ä¸­ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ ¡å›­å¯¼å‘çš„åŒå…‰è°±æ•°æ®é›†ã€ä»»åŠ¡æ„ŸçŸ¥çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ä»¥åŠå¯¹é¢†å…ˆèåˆæ–¹æ³•çš„å…¨é¢æ¯”è¾ƒåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæŠ€æœ¯å¯¹äºåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ç¼ºä¹æ ‡å‡†åŒ–æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œé™åˆ¶äº†ç ”ç©¶çš„è¿›å±•ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„åŒå…‰è°±æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ ¡å›­ç¯å¢ƒä¸‹çš„å››ç§ä»£è¡¨æ€§åœºæ™¯ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†èåˆé€Ÿåº¦ã€é€šç”¨æŒ‡æ ‡å’Œå¯¹è±¡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–çš„èåˆæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
<li>ä¸€äº›åœ¨é€šç”¨æŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½çš„ç®—æ³•åœ¨ä¸‹æ¸¸æ€§èƒ½ä¸Šå¹¶ä¸å¼ºï¼Œå‡¸æ˜¾äº†å½“å‰è¯„ä¼°å®è·µçš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a42d3b98b4bad62567261abeafa05e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d862f02ceb26aa7c7edcf53280dca2ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51b3125d13bfee7152fb358b63e7b62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fc63fd10f5ae90b4496a00cd6906747.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2f27930105d221a556194831039881b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f633865a6eb2457ad9086d6b8295609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b51f93e5910a58de4b7c61f4a246da8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cfbbaafbe8abff736b2c9c583db251f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Culturally-diverse-Multilingual-Multimodal-Video-Benchmark-Model"><a href="#A-Culturally-diverse-Multilingual-Multimodal-Video-Benchmark-Model" class="headerlink" title="A Culturally-diverse Multilingual Multimodal Video Benchmark &amp; Model"></a>A Culturally-diverse Multilingual Multimodal Video Benchmark &amp; Model</h2><p><strong>Authors:Bhuiyan Sanjid Shafique, Ashmal Vayani, Muhammad Maaz, Hanoona Abdul Rasheed, Dinura Dissanayake, Mohammed Irfan Kurpath, Yahya Hmaiti, Go Inoue, Jean Lahoud, Md. Safirur Rashid, Shadid Intisar Quasem, Maheen Fatima, Franco Vidal, Mykola Maslych, Ketan Pravin More, Sanoojan Baliah, Hasindri Watawana, Yuhao Li, Fabian Farestam, Leon Schaller, Roman Tymtsiv, Simon Weber, Hisham Cholakkal, Ivan Laptev, Shinâ€™ichi Satoh, Michael Felsberg, Mubarak Shah, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Large multimodal models (LMMs) have recently gained attention due to their effectiveness to understand and generate descriptions of visual content. Most existing LMMs are in English language. While few recent works explore multilingual image LMMs, to the best of our knowledge, moving beyond the English language for cultural and linguistic inclusivity is yet to be investigated in the context of video LMMs. In pursuit of more inclusive video LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to evaluate Video LMMs across 14 languages, including both low- and high-resource languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian, Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is designed to rigorously test video LMMs across 15 categories including eight culturally diverse categories, ranging from lifestyles and festivals to foods and rituals and from local landmarks to prominent cultural personalities. ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice questions spanning various video durations (short, medium, and long) with 8k samples that are manually verified by native language speakers. In addition, we also introduce a machine translated multilingual video training set comprising 1.2 million samples and develop a simple multilingual video LMM, named ViMUL, that is shown to provide a better tradeoff between high-and low-resource languages for video understanding. We hope our ViMUL-Bench and multilingual video LMM along with a large-scale multilingual video training set will help ease future research in developing cultural and linguistic inclusive multilingual video LMMs. Our proposed benchmark, video LMM and training data will be publicly released at <a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/ViMUL/">https://mbzuai-oryx.github.io/ViMUL/</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰å› å…¶ç†è§£å’Œç”Ÿæˆè§†è§‰å†…å®¹æè¿°çš„æœ‰æ•ˆæ€§è€Œå—åˆ°å…³æ³¨ã€‚ç°æœ‰çš„å¤§å¤šæ•°LMMéƒ½æ˜¯è‹±è¯­ã€‚è™½ç„¶æœ‰ä¸€äº›æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢å¤šè¯­è¨€å›¾åƒLMMï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨è§†é¢‘LMMçš„æƒ…å¢ƒä¸­ï¼Œä¸ºäº†æ–‡åŒ–å’Œè¯­è¨€çš„åŒ…å®¹æ€§è½¬å‘éè‹±è¯­è¯­è¨€å°šæœªè¢«ç ”ç©¶ã€‚ä¸ºäº†å¯»æ±‚æ›´å…·åŒ…å®¹æ€§çš„è§†é¢‘LMMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤šè¯­è¨€è§†é¢‘LMMåŸºå‡†æµ‹è¯•ï¼Œåä¸ºViMUL-Benchï¼Œæ—¨åœ¨è¯„ä¼°åŒ…æ‹¬ä½èµ„æºå’Œé«˜èµ„æºè¯­è¨€åœ¨å†…çš„14ç§è¯­è¨€çš„è§†é¢‘LMMï¼Œè¿™äº›è¯­è¨€åŒ…æ‹¬è‹±è¯­ã€ä¸­æ–‡ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€å¾·è¯­ã€å°åœ°è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€ä¿„è¯­ã€å­ŸåŠ æ‹‰è¯­ã€ä¹Œå°”éƒ½è¯­ã€åƒ§ä¼½ç½—è¯­ã€æ³°ç±³å°”è¯­ã€ç‘å…¸è¯­å’Œæ—¥è¯­ã€‚æˆ‘ä»¬çš„ViMUL-Benchè¢«è®¾è®¡ä¸ºä¸¥æ ¼æµ‹è¯•åŒ…æ‹¬ç”Ÿæ´»æ–¹å¼å’ŒèŠ‚æ—¥åœ¨å†…çš„æ–‡åŒ–å¤šæ ·æ€§ä»¥åŠå½“åœ°åœ°æ ‡å’Œè‘—åæ–‡åŒ–äººç‰©ç­‰ç±»åˆ«çš„è§†é¢‘LMMå…±åäº”å¤§ç±»åˆ«ã€‚ViMUL-BenchåŒ…æ‹¬å¼€æ”¾æ€§é—®é¢˜ï¼ˆçŸ­å½¢å¼å’Œé•¿å½¢å¼ï¼‰å’Œé€‰æ‹©é¢˜ï¼Œæ¶µç›–å„ç§è§†é¢‘æ—¶é•¿ï¼ˆçŸ­ã€ä¸­å’Œé•¿ï¼‰ï¼Œå…±åŒ…å«ç»è¿‡æ¯è¯­è€…æ‰‹åŠ¨éªŒè¯çš„å…«åƒä¸ªæ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŒ…å«ä¸€ç™¾äºŒåä¸‡æ ·æœ¬çš„æœºå™¨ç¿»è¯‘å¤šè¯­è¨€è§†é¢‘è®­ç»ƒé›†ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç®€å•çš„å¤šè¯­è¨€è§†é¢‘LMMï¼Œåä¸ºViMULï¼Œå®ƒåœ¨é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€çš„è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æƒè¡¡ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ViMUL-Benchå’Œå¤šè¯­è¨€è§†é¢‘LMMä»¥åŠå¤§è§„æ¨¡å¤šè¯­è¨€è§†é¢‘è®­ç»ƒé›†å°†å¸®åŠ©æ¨åŠ¨æœªæ¥å¼€å‘å…·æœ‰æ–‡åŒ–å’Œè¯­è¨€åŒ…å®¹æ€§çš„å¤šè¯­è¨€è§†é¢‘LMMçš„ç ”ç©¶ã€‚æˆ‘ä»¬æè®®çš„åŸºå‡†æµ‹è¯•ã€è§†é¢‘LMMå’Œè®­ç»ƒæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/ViMUL/%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://mbzuai-oryx.github.io/ViMUL/ä¸Šå…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07032v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºViMUL-Benchçš„å¤šè¯­ç§è§†é¢‘å¤§å‹æ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°åŒ…æ‹¬è‹±è¯­ã€ä¸­æ–‡ç­‰å¤šè¾¾14ç§è¯­è¨€çš„è§†é¢‘LMMæ€§èƒ½ã€‚å®ƒæ¶µç›–äº†æ–‡åŒ–å¤šæ ·æ€§ä¸°å¯Œçš„ç±»åˆ«ï¼ŒåŒ…æ‹¬ç”Ÿæ´»æ–¹å¼ã€èŠ‚æ—¥ã€é£Ÿç‰©ã€ä»ªå¼ä»¥åŠå½“åœ°åœ°æ ‡å’Œè‘—åæ–‡åŒ–äººç‰©ç­‰ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æœºå™¨ç¿»è¯‘çš„å¤šè¯­ç§è§†é¢‘è®­ç»ƒé›†ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåä¸ºViMULçš„ç®€å•å¤šè¯­ç§è§†é¢‘LMMæ¨¡å‹ã€‚è¯¥åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹æ—¨åœ¨ä¿ƒè¿›å¤šè¯­ç§è§†é¢‘LMMçš„ç ”ç©¶å‘å±•ï¼Œå¹¶å°†åœ¨å…¬å¼€ç½‘ç«™ä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰èƒ½æœ‰æ•ˆç†è§£å’Œç”Ÿæˆè§†è§‰å†…å®¹æè¿°ï¼Œç›®å‰å¤§éƒ¨åˆ†ä¸ºè‹±è¯­æ¨¡å‹ã€‚</li>
<li>ä¸ºæé«˜æ–‡åŒ–åŠè¯­è¨€åŒ…å®¹æ€§ï¼Œéœ€è¦æ¢ç´¢éè‹±è¯­çš„å¤šè¯­ç§è§†é¢‘LMMã€‚</li>
<li>æå‡ºäº†åä¸ºViMUL-Benchçš„å¤šè¯­ç§è§†é¢‘LMMåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–14ç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä½èµ„æºå’Œé«˜èµ„æºè¯­è¨€ã€‚</li>
<li>ViMUL-BenchåŒ…å«å¼€æ”¾æ€§é—®é¢˜ï¼ˆçŸ­å¼å’Œé•¿å¼ï¼‰å’Œé€‰æ‹©é¢˜ï¼Œæ¶‰åŠä¸åŒè§†é¢‘æ—¶é•¿å’Œæ‰‹åŠ¨éªŒè¯çš„8Kæ ·æœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæœºå™¨ç¿»è¯‘çš„å¤šè¯­ç§è§†é¢‘è®­ç»ƒé›†ï¼ŒåŒ…å«120ä¸‡æ ·æœ¬ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåä¸ºViMULçš„ç®€å•å¤šè¯­ç§è§†é¢‘LMMæ¨¡å‹ï¼Œèƒ½åœ¨é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€é—´æä¾›æ›´å¥½çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-70a9b6ba372ffc82c9a9f966ff8eec0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-caed7febf4abe3b84effbd104d50bf6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-626d168696be83ac186743e3e308b5ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec1f48930fcdc807703571ff53db9b78.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimal-Transport-Driven-Asymmetric-Image-to-Image-Translation-for-Nuclei-Segmentation-of-Histological-Images"><a href="#Optimal-Transport-Driven-Asymmetric-Image-to-Image-Translation-for-Nuclei-Segmentation-of-Histological-Images" class="headerlink" title="Optimal Transport Driven Asymmetric Image-to-Image Translation for   Nuclei Segmentation of Histological Images"></a>Optimal Transport Driven Asymmetric Image-to-Image Translation for   Nuclei Segmentation of Histological Images</h2><p><strong>Authors:Suman Mahapatra, Pradipta Maji</strong></p>
<p>Segmentation of nuclei regions from histological images enables morphometric analysis of nuclei structures, which in turn helps in the detection and diagnosis of diseases under consideration. To develop a nuclei segmentation algorithm, applicable to different types of target domain representations, image-to-image translation networks can be considered as they are invariant to target domain image representations. One of the important issues with image-to-image translation models is that they fail miserably when the information content between two image domains are asymmetric in nature. In this regard, the paper introduces a new deep generative model for segmenting nuclei structures from histological images. The proposed model considers an embedding space for handling information-disparity between information-rich histological image space and information-poor segmentation map domain. Integrating judiciously the concepts of optimal transport and measure theory, the model develops an invertible generator, which provides an efficient optimization framework with lower network complexity. The concept of invertible generator automatically eliminates the need of any explicit cycle-consistency loss. The proposed model also introduces a spatially-constrained squeeze operation within the framework of invertible generator to maintain spatial continuity within the image patches. The model provides a better trade-off between network complexity and model performance compared to other existing models having complex network architectures. The performance of the proposed deep generative model, along with a comparison with state-of-the-art nuclei segmentation methods, is demonstrated on publicly available histological image data sets. </p>
<blockquote>
<p>ä»ç»„ç»‡å­¦å›¾åƒä¸­åˆ†å‰²æ ¸åŒºåŸŸï¼Œèƒ½å¤Ÿå¯¹æ ¸ç»“æ„è¿›è¡Œå½¢æ€è®¡é‡åˆ†æï¼Œè¿™è¿›è€Œæœ‰åŠ©äºå¯¹æ‰€è€ƒè™‘çš„ç–¾ç—…è¿›è¡Œæ£€æµ‹å’Œè¯Šæ–­ã€‚ä¸ºäº†å¼€å‘é€‚ç”¨äºä¸åŒç±»å‹ç›®æ ‡åŸŸè¡¨ç¤ºçš„æ ¸åˆ†å‰²ç®—æ³•ï¼Œå¯ä»¥è€ƒè™‘å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ç½‘ç»œï¼Œå› ä¸ºå®ƒä»¬å¯¹ç›®æ ‡åŸŸå›¾åƒè¡¨ç¤ºå…·æœ‰ä¸å˜æ€§ã€‚å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹çš„ä¸€ä¸ªé‡è¦é—®é¢˜æ˜¯ï¼Œå½“ä¸¤ä¸ªå›¾åƒåŸŸä¹‹é—´çš„ä¿¡æ¯å†…å®¹åœ¨æœ¬è´¨ä¸Šæ˜¯ä¸å¯¹ç§°çš„æ—¶å€™ï¼Œè¿™äº›æ¨¡å‹å°±ä¼šå½»åº•å¤±è´¥ã€‚åœ¨è¿™æ–¹é¢ï¼Œè®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºä»ç»„ç»‡å­¦å›¾åƒä¸­åˆ†å‰²æ ¸ç»“æ„çš„æ–°å‹æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€‚æ‰€æå‡ºæ¨¡å‹è€ƒè™‘äº†ä¸€ä¸ªåµŒå…¥ç©ºé—´ï¼Œä»¥å¤„ç†ä¿¡æ¯ä¸°å¯Œçš„ç»„ç»‡å­¦å›¾åƒç©ºé—´ä¸ä¿¡æ¯è´«ä¹çš„åˆ†å‰²å›¾åŸŸä¹‹é—´çš„ä¿¡æ¯å·®å¼‚ã€‚é€šè¿‡å·§å¦™åœ°èåˆæœ€ä¼˜ä¼ è¾“å’Œæµ‹åº¦è®ºçš„æ¦‚å¿µï¼Œè¯¥æ¨¡å‹å¼€å‘äº†ä¸€ä¸ªå¯é€†ç”Ÿæˆå™¨ï¼Œæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¼˜åŒ–æ¡†æ¶ï¼Œé™ä½äº†ç½‘ç»œå¤æ‚æ€§ã€‚å¯é€†ç”Ÿæˆå™¨çš„æ¦‚å¿µè‡ªåŠ¨æ¶ˆé™¤äº†ä»»ä½•æ˜¾å¼å¾ªç¯ä¸€è‡´æ€§çš„éœ€è¦ã€‚æ‰€æå‡ºæ¨¡å‹è¿˜åœ¨å¯é€†ç”Ÿæˆå™¨çš„æ¡†æ¶å†…å¼•å…¥äº†ç©ºé—´çº¦æŸçš„æŒ¤å‹æ“ä½œï¼Œä»¥ä¿æŒå›¾åƒæ–‘å—å†…çš„ç©ºé—´è¿ç»­æ€§ã€‚ä¸å…¶ä»–å…·æœ‰å¤æ‚ç½‘ç»œæ¶æ„çš„ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ç½‘ç»œå¤æ‚åº¦å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ã€‚æ‰€æå‡ºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥åŠä¸å‰è¿°æœ€å…ˆè¿›çš„æ ¸åˆ†å‰²æ–¹æ³•çš„æ¯”è¾ƒï¼Œéƒ½åœ¨å…¬å¼€å¯ç”¨çš„ç»„ç»‡å­¦å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07023v1">PDF</a> 13 pages, 8 figures</p>
<p><strong>Summary</strong>:<br>æ ¸åŒºåŸŸåˆ†å‰²æœ‰åŠ©äºå½¢æ€è®¡é‡åˆ†æï¼Œæœ‰åŠ©äºæ£€æµ‹è¯Šæ–­ç–¾ç—…ã€‚é’ˆå¯¹å›¾åƒä¿¡æ¯å†…å®¹ä¸å¯¹ç§°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ç»†èƒæ ¸åˆ†å‰²æ–¹æ³•ã€‚è¯¥æ¨¡å‹å¼•å…¥åµŒå…¥ç©ºé—´å¤„ç†å›¾åƒé—´ä¿¡æ¯å·®å¼‚ï¼Œå¹¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“å’Œåº¦é‡ç†è®ºæ¦‚å¿µï¼Œå¼€å‘äº†ä¸€ä¸ªå¯åè½¬ç”Ÿæˆå™¨ä»¥é™ä½ç½‘ç»œå¤æ‚åº¦å¹¶æä¾›æœ‰æ•ˆä¼˜åŒ–æ¡†æ¶ã€‚æ–°æ¨¡å‹åœ¨ä¿è¯ç©ºé—´è¿ç»­æ€§çš„åŒæ—¶å®ç°äº†ä¸ç°æœ‰æ¨¡å‹çš„è‰¯å¥½æƒè¡¡ï¼Œæ€§èƒ½å·²åœ¨å…¬å¼€å¯ç”¨çš„ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å±•ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ ¸åŒºåŸŸåˆ†å‰²æœ‰åŠ©äºç–¾ç—…çš„æ£€æµ‹ä¸è¯Šæ–­ã€‚</li>
<li>å›¾åƒåˆ†å‰²ç®—æ³•éœ€è¦é€‚åº”ä¸åŒç›®æ ‡åŸŸè¡¨ç¤ºç±»å‹ã€‚</li>
<li>å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯å†…å®¹ä¸å¯¹ç§°æ—¶å¯èƒ½å¤±æ•ˆã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ç»†èƒæ ¸åˆ†å‰²æ–°æ–¹æ³•ï¼Œè§£å†³äº†ä¿¡æ¯è´«ç˜ çš„åˆ†å‰²æ˜ å°„åŸŸä¸ä¸°å¯Œçš„ç»„ç»‡å­¦å›¾åƒç©ºé—´ä¹‹é—´çš„ä¿¡æ¯å·®å¼‚é—®é¢˜ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†æœ€ä¼˜ä¼ è¾“å’Œåº¦é‡ç†è®ºæ¦‚å¿µï¼Œæå‡ºäº†ä¸€ä¸ªå¯åè½¬ç”Ÿæˆå™¨ï¼Œä¼˜åŒ–äº†ç½‘ç»œå¤æ‚æ€§å¹¶æ¶ˆé™¤äº†å¯¹æ˜¾å¼å¾ªç¯ä¸€è‡´æ€§æŸå¤±çš„ä¾èµ–ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†ç©ºé—´çº¦æŸæŒ¤å‹æ“ä½œæ¥ç»´æŒå›¾åƒæ–‘å—çš„ç©ºé—´è¿ç»­æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-088e13cb55609716537efa4479ebbd7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81340956c726bb9eafc0635aa417f6a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-239a63974fc624a534b5fc29670d2c4f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SPC-to-3D-Novel-View-Synthesis-from-Binary-SPC-via-I2I-translation"><a href="#SPC-to-3D-Novel-View-Synthesis-from-Binary-SPC-via-I2I-translation" class="headerlink" title="SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation"></a>SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation</h2><p><strong>Authors:Sumit Sharma, Gopi Raju Matta, Kaushik Mitra</strong></p>
<p>Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging technology, capable of detecting individual photons with remarkable timing precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable image capture at exceptionally high speeds under both low and high illumination. Enabling 3D reconstruction and radiance field recovery from such SPC data holds significant promise. However, the binary nature of SPC images leads to severe information loss, particularly in texture and color, making traditional 3D synthesis techniques ineffective. To address this challenge, we propose a modular two-stage framework that converts binary SPC images into high-quality colorized novel views. The first stage performs image-to-image (I2I) translation using generative models such as Pix2PixHD, converting binary SPC inputs into plausible RGB representations. The second stage employs 3D scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian Splatting (3DGS) to generate novel views. We validate our two-stage pipeline (Pix2PixHD + Nerf&#x2F;3DGS) through extensive qualitative and quantitative experiments, demonstrating significant improvements in perceptual quality and geometric consistency over the alternative baseline. </p>
<blockquote>
<p>å•å…‰å­é›ªå´©äºŒæç®¡ï¼ˆSPADï¼‰ä»£è¡¨äº†ä¸€ç§å‰æ²¿çš„æˆåƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿä»¥æƒŠäººçš„æ—¶é—´ç²¾åº¦æ£€æµ‹å•ä¸ªå…‰å­ã€‚åŸºäºæ­¤çµæ•åº¦ï¼Œå•å…‰å­ç›¸æœºï¼ˆSPCï¼‰èƒ½å¤Ÿåœ¨ä½å…‰ç…§å’Œé«˜å…‰ç…§æ¡ä»¶ä¸‹ä»¥æé«˜çš„é€Ÿåº¦æ•è·å›¾åƒã€‚ä»è¿™æ ·çš„SPCæ•°æ®ä¸­å®ç°3Dé‡å»ºå’Œè¾å°„åœºæ¢å¤å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒSPCå›¾åƒçš„äºŒè¿›åˆ¶ç‰¹æ€§å¯¼è‡´ä¿¡æ¯ä¸¥é‡ä¸¢å¤±ï¼Œç‰¹åˆ«æ˜¯åœ¨çº¹ç†å’Œé¢œè‰²æ–¹é¢ï¼Œä½¿å¾—ä¼ ç»Ÿçš„3DåˆæˆæŠ€æœ¯æ— æ•ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†äºŒè¿›åˆ¶SPCå›¾åƒè½¬åŒ–ä¸ºé«˜è´¨é‡å½©è‰²æ–°é¢–è§†å›¾ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Pix2PixHDï¼‰è¿›è¡Œå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘ï¼Œå°†äºŒè¿›åˆ¶SPCè¾“å…¥è½¬æ¢ä¸ºå¯ä¿¡çš„RGBè¡¨ç¤ºã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨3Dåœºæ™¯é‡å»ºæŠ€æœ¯ï¼ˆå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆ–é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰ï¼‰ç”Ÿæˆæ–°é¢–è§†å›¾ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ä¸¤é˜¶æ®µç®¡é“ï¼ˆPix2PixHD + NeRF&#x2F;3DGSï¼‰ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢ç›¸å¯¹äºæ›¿ä»£åŸºçº¿è¡¨ç°å‡ºäº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06890v1">PDF</a> Accepted for publication at ICIP 2025</p>
<p><strong>Summary</strong><br>     å•å…‰å­é›ªå´©äºŒæç®¡ï¼ˆSPADï¼‰æ˜¯å‰æ²¿æˆåƒæŠ€æœ¯ï¼Œèƒ½æ£€æµ‹å•ä¸ªå…‰å­å¹¶å…·æœ‰å‡ºè‰²æ—¶é—´ç²¾åº¦ã€‚åŸºäºæ­¤æŠ€æœ¯çš„å•å…‰å­ç›¸æœºï¼ˆSPCï¼‰å¯åœ¨é«˜ã€ä½å…‰ç…§ä¸‹å®ç°è¶…é«˜é€Ÿæˆåƒã€‚ä»SPCæ•°æ®ä¸­é‡å»º3Då’Œæ¢å¤è¾å°„åœºå±•ç°å·¨å¤§æ½œåŠ›ã€‚ä½†SPCå›¾åƒçš„äºŒè¿›åˆ¶ç‰¹æ€§å¯¼è‡´çº¹ç†å’Œè‰²å½©ä¿¡æ¯å¤§é‡æŸå¤±ï¼Œä¼ ç»Ÿ3DåˆæˆæŠ€æœ¯éš¾ä»¥åº”å¯¹ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºæ¨¡å—åŒ–ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå°†äºŒè¿›åˆ¶SPCå›¾åƒè½¬åŒ–ä¸ºé«˜è´¨é‡å½©è‰²æ–°é¢–è§†å›¾ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨Pix2PixHDç­‰ç”Ÿæˆæ¨¡å‹è¿›è¡Œå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘ï¼Œå°†äºŒè¿›åˆ¶SPCè¾“å…¥è½¬åŒ–ä¸ºå¯ä¿¡çš„RGBè¡¨ç¤ºã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æˆ–é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰ç­‰3Dåœºæ™¯é‡å»ºæŠ€æœ¯ç”Ÿæˆæ–°é¢–è§†å›¾ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†Pix2PixHD + Nerf&#x2F;3DGSçš„ä¸¤é˜¶æ®µç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ„ŸçŸ¥è´¨é‡å’Œå‡ ä½•ä¸€è‡´æ€§æ–¹é¢ç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•å…‰å­é›ªå´©äºŒæç®¡ï¼ˆSPADï¼‰èƒ½æ£€æµ‹å•ä¸ªå…‰å­å¹¶å…·æœ‰å‡ºè‰²æ—¶é—´ç²¾åº¦ï¼Œä¸ºé«˜é€Ÿæˆåƒæä¾›äº†å¯èƒ½ã€‚</li>
<li>å•å…‰å­ç›¸æœºï¼ˆSPCï¼‰å¯å®ç°ä½å…‰ç…§å’Œé«˜å…‰ç…§ä¸‹çš„è¶…é«˜é€Ÿæˆåƒã€‚<br>3.SPCæ•°æ®çš„3Dé‡å»ºå’Œè¾å°„åœºæ¢å¤å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚<br>4.SPCå›¾åƒçš„äºŒè¿›åˆ¶ç‰¹æ€§å¯¼è‡´ä¿¡æ¯å’Œè‰²å½©çº¹ç†å¤§é‡æŸå¤±ï¼Œä½¿å¾—ä¼ ç»Ÿ3DåˆæˆæŠ€æœ¯éš¾ä»¥åº”ç”¨ã€‚<br>5.ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºæ¨¡å—åŒ–ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç¬¬ä¸€é˜¶æ®µå°†SPCå›¾åƒè½¬åŒ–ä¸ºRGBå›¾åƒï¼Œç¬¬äºŒé˜¶æ®µè¿›è¡Œ3Dåœºæ™¯é‡å»ºç”Ÿæˆæ–°é¢–è§†å›¾ã€‚<br>6.ä½¿ç”¨Pix2PixHDç­‰ç”Ÿæˆæ¨¡å‹è¿›è¡Œå›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c0bfd0e964496670cbdb5b84f471565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b15420e147acc04274beff86e285685.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0140776939a5c31ab6cf3a36001ff468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da76a65831c84633b03413fcefc71d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72b74a5bad2eea5ef881605e7e25fca4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LoopDB-A-Loop-Closure-Dataset-for-Large-Scale-Simultaneous-Localization-and-Mapping"><a href="#LoopDB-A-Loop-Closure-Dataset-for-Large-Scale-Simultaneous-Localization-and-Mapping" class="headerlink" title="LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization   and Mapping"></a>LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization   and Mapping</h2><p><strong>Authors:Mohammad-Maher Nakshbandi, Ziad Sharawy, Dorian Cojocaru, Sorin Grigorescu</strong></p>
<p>In this study, we introduce LoopDB, which is a challenging loop closure dataset comprising over 1000 images captured across diverse environments, including parks, indoor scenes, parking spaces, as well as centered around individual objects. Each scene is represented by a sequence of five consecutive images. The dataset was collected using a high resolution camera, providing suitable imagery for benchmarking the accuracy of loop closure algorithms, typically used in simultaneous localization and mapping. As ground truth information, we provide computed rotations and translations between each consecutive images. Additional to its benchmarking goal, the dataset can be used to train and fine-tune loop closure methods based on deep neural networks. LoopDB is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RovisLab/LoopDB">https://github.com/RovisLab/LoopDB</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†LoopDBï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—­ç¯æ•°æ®é›†ï¼ŒåŒ…å«1000å¼ ä»¥ä¸Šåœ¨ä¸åŒç¯å¢ƒä¸‹æ•è·çš„å›¾åƒï¼ŒåŒ…æ‹¬å…¬å›­ã€å®¤å†…åœºæ™¯ã€åœè½¦ä½ï¼Œä»¥åŠå›´ç»•å•ä¸ªç‰©ä½“çš„åœºæ™¯ã€‚æ¯ä¸ªåœºæ™¯ç”±äº”å¼ è¿ç»­å›¾åƒåºåˆ—è¡¨ç¤ºã€‚è¯¥æ•°æ®é›†ä½¿ç”¨é«˜åˆ†è¾¨ç‡ç›¸æœºæ”¶é›†ï¼Œä¸ºè¯„ä¼°é—­ç¯ç®—æ³•çš„å‡†ç¡®æ€§æä¾›äº†åˆé€‚çš„å›¾åƒï¼Œé€šå¸¸ç”¨äºåŒæ—¶å®šä½å’Œåœ°å›¾æ„å»ºã€‚ä½œä¸ºåœ°é¢çœŸå®ä¿¡æ¯ï¼Œæˆ‘ä»¬æä¾›äº†æ¯ä¸¤å¼ è¿ç»­å›¾åƒä¹‹é—´çš„è®¡ç®—æ—‹è½¬å’Œå¹³ç§»ä¿¡æ¯ã€‚é™¤äº†å…¶åŸºå‡†æµ‹è¯•ç›®æ ‡ä¹‹å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜å¯ç”¨äºè®­ç»ƒå’Œå¾®è°ƒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„é—­ç¯æ–¹æ³•ã€‚LoopDBå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RovisLab/LoopDB%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RovisLab/LoopDBå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06771v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†LoopDBæ•°æ®é›†ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡ä¸€åƒå¼ å›¾åƒçš„æŒ‘æˆ˜æ€§é—­ç¯æ•°æ®é›†ï¼Œå›¾åƒæ‹æ‘„è‡ªå…¬å›­ã€å®¤å†…åœºæ™¯ã€åœè½¦åœºç­‰ä¸åŒç¯å¢ƒï¼Œä»¥åŠå›´ç»•å•ä¸ªç‰©ä½“çš„åœºæ™¯ã€‚æ¯ä¸ªåœºæ™¯ç”±äº”å¼ è¿ç»­å›¾åƒç»„æˆã€‚è¯¥æ•°æ®é›†ä½¿ç”¨é«˜åˆ†è¾¨ç‡ç›¸æœºæ”¶é›†ï¼Œé€‚åˆè¯„ä¼°é—­ç¯ç®—æ³•ï¼ˆå¸¸ç”¨äºåŒæ—¶å®šä½å’Œåœ°å›¾æ„å»ºï¼‰çš„å‡†ç¡®æ€§ã€‚ä½œä¸ºåœ°é¢çœŸå®ä¿¡æ¯ï¼Œæˆ‘ä»¬æä¾›äº†æ¯ä¸¤å¼ è¿ç»­å›¾åƒä¹‹é—´çš„è®¡ç®—æ—‹è½¬å’Œå¹³ç§»ä¿¡æ¯ã€‚é™¤äº†ä½œä¸ºåŸºå‡†æµ‹è¯•ç›®æ ‡å¤–ï¼Œè¯¥æ•°æ®é›†ä¹Ÿå¯ç”¨äºè®­ç»ƒå’Œå¾®è°ƒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„é—­ç¯æ–¹æ³•ã€‚LoopDBå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RovisLab/LoopDB%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/RovisLab/LoopDBå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoopDBæ˜¯ä¸€ä¸ªåŒ…å«å¤šæ ·ç¯å¢ƒçš„é—­ç¯æ•°æ®é›†ï¼Œå›¾åƒæ•°é‡è¶…è¿‡ä¸€åƒå¼ ã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æ¯ä¸ªåœºæ™¯ç”±äº”å¼ è¿ç»­å›¾åƒè¡¨ç¤ºã€‚</li>
<li>æ•°æ®é›†ä½¿ç”¨é«˜åˆ†è¾¨ç‡ç›¸æœºæ”¶é›†ï¼Œé€‚ç”¨äºè¯„ä¼°é—­ç¯ç®—æ³•å‡†ç¡®æ€§ã€‚</li>
<li>ä½œä¸ºåœ°é¢çœŸå®ä¿¡æ¯ï¼Œæä¾›äº†æ¯ä¸¤å¼ è¿ç»­å›¾åƒé—´çš„æ—‹è½¬å’Œå¹³ç§»æ•°æ®ã€‚</li>
<li>LoopDBä¸ä»…ç”¨äºåŸºå‡†æµ‹è¯•ï¼Œè¿˜å¯ç”¨äºè®­ç»ƒå’Œå¾®è°ƒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„é—­ç¯æ–¹æ³•ã€‚</li>
<li>è¯¥æ•°æ®é›†å…¬å¼€å¯ç”¨ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34d3720aa7c0a917fb22a56e980a8c2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cda4c5185b02a030032d3ee4b0e226d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25ae6d5f45d7aa5527ec56370e21c258.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6a55682198d4c2b37a7b78fc5e1ea32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-680ccb3416222d6d41ac8eb622f65ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8100d3a440c2f1e120c3a78ba9c2e0be.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling"><a href="#GarmageNet-A-Multimodal-Generative-Framework-for-Sewing-Pattern-Design-and-Generic-Garment-Modeling" class="headerlink" title="GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling"></a>GarmageNet: A Multimodal Generative Framework for Sewing Pattern Design   and Generic Garment Modeling</h2><p><strong>Authors:Siran Li, Chen Liu, Ruiyang Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang</strong></p>
<p>Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet, a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage, a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment shapes. GarmageNet employs a latent diffusion transformer to synthesize panel-wise geometry images and integrates GarmageJigsaw, a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet, a large-scale dataset comprising over 10,000 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions-laying the foundation for fully automated, production-ready pipelines in digital fashion. Project page: <a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet">https://style3d.github.io/garmagenet</a>. </p>
<blockquote>
<p>çœŸå®çš„æ•°å­—æœè£…å»ºæ¨¡ä»ç„¶æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†å‹çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå°†äºŒç»´ç¼çº«å›¾æ¡ˆè½¬åŒ–ä¸ºé«˜ä¿çœŸã€å¯ç”¨äºæ¨¡æ‹Ÿçš„ä¸‰ç»´æœè£…çš„è¿‡ç¨‹éå¸¸å¤æ‚ã€‚æˆ‘ä»¬å¼•å…¥äº†GarmageNetï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥è‡ªåŠ¨åˆ›å»ºäºŒç»´ç¼çº«å›¾æ¡ˆã€æ„å»ºç¼çº«å…³ç³»ï¼Œå¹¶åˆæˆå…¼å®¹åŸºäºç‰©ç†æ¨¡æ‹Ÿçš„ä¸‰ç»´æœè£…åˆå§‹åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯Garmageï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æœè£…è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒå°†æ¯ä¸ªé¢æ¿ç¼–ç ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„å‡ ä½•å›¾åƒï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†äºŒç»´ç»“æ„å›¾æ¡ˆå’Œä¸‰ç»´æœè£…å½¢çŠ¶ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚GarmageNeté‡‡ç”¨æ½œåœ¨æ‰©æ•£å˜å‹å™¨æ¥åˆæˆé¢æ¿å¼çš„å‡ ä½•å›¾åƒï¼Œå¹¶é›†æˆäº†GarmageJigsawï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹é¢æ¿è½®å»“ä¸Šç‚¹å¯¹ç‚¹ç¼åˆè¿æ¥çš„ç¥ç»æ¨¡å—ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†GarmageSetï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è¶…è¿‡10000ä»¶ä¸“ä¸šè®¾è®¡çš„æœè£…ç»„æˆçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…·æœ‰è¯¦ç»†çš„ç»“æ„å’Œé£æ ¼æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†åœ¨å¤šä¸ªåº”ç”¨åœºæ™¯ä¸‹çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä»å¤šæ¨¡å¼è®¾è®¡æ¦‚å¿µï¼ˆæ–‡æœ¬æç¤ºã€è‰å›¾ã€ç…§ç‰‡ï¼‰ç”Ÿæˆå¯æ‰©å±•çš„æœè£…ã€ä»åŸå§‹å¹³é¢ç¼çº«å›¾æ¡ˆè‡ªåŠ¨å»ºæ¨¡ã€ä»éç»“æ„åŒ–çš„ç‚¹äº‘ä¸­æ¢å¤å›¾æ¡ˆï¼Œä»¥åŠä½¿ç”¨å¸¸è§„æŒ‡ä»¤è¿›è¡Œæ¸è¿›å¼çš„æœè£…ç¼–è¾‘â€”â€”è¿™ä¸ºå…¨è‡ªåŠ¨ã€ç”Ÿäº§å‡†å¤‡çš„æ•°å­—æ—¶å°šç®¡é“å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://style3d.github.io/garmagenet%E3%80%82">https://style3d.github.io/garmagenetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01483v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºGarmageNetçš„ç»Ÿä¸€ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯è‡ªåŠ¨åŒ–åˆ›å»º2Dç¼çº«å›¾æ¡ˆã€æ„å»ºç¼çº«å…³ç³»ï¼Œå¹¶åˆæˆå¯ç”¨äºç‰©ç†æ¨¡æ‹Ÿçš„3Dæœè£…åˆå§‹åŒ–ã€‚å…¶æ ¸å¿ƒæ–¹æ³•æ˜¯é€šè¿‡ä¸€ç§æ–°çš„æœè£…è¡¨ç¤ºæ–¹å¼Garmageï¼Œå°†æ¯ä¸ªé¢æ¿ç¼–ç ä¸ºç»“æ„åŒ–å‡ ä½•å›¾åƒï¼Œæœ‰æ•ˆæ¡¥æ¥äº†2Dç»“æ„å›¾æ¡ˆä¸3Dæœè£…å½¢çŠ¶ä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚GarmageNeté‡‡ç”¨æ½œåœ¨æ‰©æ•£å˜å‹å™¨åˆæˆé¢æ¿çº§çš„å‡ ä½•å›¾åƒï¼Œå¹¶é›†æˆäº†GarmageJigsawï¼Œä¸€ä¸ªç”¨äºé¢„æµ‹é¢æ¿è½®å»“ä¸Šç‚¹å¯¹ç‚¹ç¼åˆè¿æ¥çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæ„å»ºäº†åŒ…å«è¶…è¿‡10,000ä¸ªä¸“ä¸šè®¾è®¡æœè£…çš„GarmageSetå¤§å‹æ•°æ®é›†ï¼Œå…·æœ‰è¯¦ç»†çš„ç»“æ„å’Œé£æ ¼æ³¨é‡Šã€‚æ­¤æ–¹æ³•åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºé€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä»å¤šæ¨¡å¼è®¾è®¡æ¦‚å¿µï¼ˆæ–‡æœ¬æç¤ºã€è‰å›¾ã€ç…§ç‰‡ï¼‰ç”Ÿæˆæœè£…ã€ä»åŸå§‹å¹³é¢ç¼çº«å›¾æ¡ˆè‡ªåŠ¨å»ºæ¨¡ã€ä»éç»“æ„åŒ–çš„ç‚¹äº‘æ¢å¤å›¾æ¡ˆï¼Œä»¥åŠä½¿ç”¨å¸¸è§„æŒ‡ä»¤è¿›è¡Œæ¸è¿›å¼æœè£…ç¼–è¾‘ï¼Œä¸ºæ•°å­—æ—¶å°šé¢†åŸŸä¸­çš„å…¨è‡ªåŠ¨ç”Ÿäº§å‡†å¤‡ç®¡é“å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GarmageNetæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–åˆ›å»º2Dç¼çº«å›¾æ¡ˆã€æ„å»ºç¼çº«å…³ç³»å¹¶åˆæˆ3Dæœè£…çš„ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>Garmageæ˜¯ä¸€ç§æ–°çš„æœè£…è¡¨ç¤ºæ–¹æ³•ï¼Œå°†æ¯ä¸ªé¢æ¿ç¼–ç ä¸ºç»“æ„åŒ–å‡ ä½•å›¾åƒï¼Œä»¥æ¡¥æ¥2Då’Œ3Dä¹‹é—´çš„è¯­ä¹‰å’Œå‡ ä½•å·®è·ã€‚</li>
<li>GarmageNetä½¿ç”¨æ½œåœ¨æ‰©æ•£å˜å‹å™¨å’ŒGarmageJigsawæ¥åˆæˆé¢æ¿çº§çš„å‡ ä½•å›¾åƒå¹¶é¢„æµ‹ç‚¹å¯¹ç‚¹çš„ç¼åˆè¿æ¥ã€‚</li>
<li>GarmageSetæ•°æ®é›†åŒ…å«å¤§é‡ä¸“ä¸šè®¾è®¡çš„æœè£…ï¼Œå…·æœ‰è¯¦ç»†çš„ç»“æ„å’Œé£æ ¼æ³¨é‡Šï¼Œç”¨äºæ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>GarmageNetå…·æœ‰å¤šç§åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬ä»å¤šæ¨¡å¼è®¾è®¡æ¦‚å¿µç”Ÿæˆæœè£…ã€ä»åŸå§‹å¹³é¢ç¼çº«å›¾æ¡ˆå»ºæ¨¡ã€ä»ç‚¹äº‘æ¢å¤å›¾æ¡ˆä»¥åŠä½¿ç”¨å¸¸è§„æŒ‡ä»¤è¿›è¡Œæ¸è¿›å¼æœè£…ç¼–è¾‘ã€‚</li>
<li>GarmageNetæ–¹æ³•ä¸ºæ•°å­—æ—¶å°šé¢†åŸŸçš„å…¨è‡ªåŠ¨ç”Ÿäº§å‡†å¤‡ç®¡é“å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bcdd8c482c3e442eafc91514b4777fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43dc17277258313a976b120ff7095981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de499678cd256abcaed3dabb844c4d7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85e1a357fc421e3beccafe96dd3ce669.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Manual2Skill-Learning-to-Read-Manuals-and-Acquire-Robotic-Skills-for-Furniture-Assembly-Using-Vision-Language-Models"><a href="#Manual2Skill-Learning-to-Read-Manuals-and-Acquire-Robotic-Skills-for-Furniture-Assembly-Using-Vision-Language-Models" class="headerlink" title="Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for   Furniture Assembly Using Vision-Language Models"></a>Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for   Furniture Assembly Using Vision-Language Models</h2><p><strong>Authors:Chenrui Tie, Shengxiang Sun, Jinxuan Zhu, Yiwei Liu, Jingxiang Guo, Yue Hu, Haonan Chen, Junting Chen, Ruihai Wu, Lin Shao</strong></p>
<p>Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.Project Page: <a target="_blank" rel="noopener" href="https://owensun2004.github.io/Furniture-Assembly-Web/">https://owensun2004.github.io/Furniture-Assembly-Web/</a> </p>
<blockquote>
<p>äººç±»æ‹¥æœ‰é€šè¿‡è§£è¯»æŠ½è±¡è¯´æ˜ä¹¦æ¥ç†è§£å¹¶æ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡çš„ä¸å‡¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºæœºå™¨äººæ¥è¯´ï¼Œè¿™ä¸€èƒ½åŠ›ä»æ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•è§£è¯»æŠ½è±¡æŒ‡ä»¤å¹¶å°†å…¶ç¿»è¯‘æˆå¯æ‰§è¡ŒåŠ¨ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Manual2Skillè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿåœ¨é«˜çº§æ‰‹å†ŒæŒ‡ä»¤çš„å¼•å¯¼ä¸‹æ‰§è¡Œå¤æ‚çš„è£…é…ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»æŒ‡ä»¤å›¾åƒä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨è¿™äº›ä¿¡æ¯æ„å»ºåˆ†å±‚è£…é…å›¾ã€‚è¿™äº›å›¾ä»£è¡¨éƒ¨ä»¶ã€å­ç»„ä»¶å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†ä¿ƒè¿›ä»»åŠ¡æ‰§è¡Œï¼Œå§¿æ€ä¼°è®¡æ¨¡å‹é¢„æµ‹æ¯ä¸ªè£…é…æ­¥éª¤ä¸­ç»„ä»¶çš„ç›¸å¯¹6Då§¿æ€ã€‚åŒæ—¶ï¼Œè¿åŠ¨è§„åˆ’æ¨¡å—ä¸ºçœŸå®ä¸–ç•Œçš„æœºå™¨äººå®ç°ç”Ÿæˆå¯æ“ä½œåºåˆ—ã€‚æˆ‘ä»¬é€šè¿‡æˆåŠŸç»„è£…å‡ ä¸ªçœŸå®ä¸–ç•Œçš„å®œå®¶å®¶å…·äº§å“æ¥è¯æ˜Manual2Skillçš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€åº”ç”¨çªå‡ºå…¶åœ¨æ•ˆç‡å’Œç²¾ç¡®åº¦æ–¹é¢çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿç®¡ç†é•¿æœŸæ“ä½œä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜äº†æœºå™¨äººä»è¯´æ˜ä¹¦å­¦ä¹ çš„å®ç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€åœ¨æ¨è¿›æœºå™¨äººç³»ç»Ÿæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥ç±»ä¼¼äºäººç±»çš„èƒ½åŠ›ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://owensun2004.github.io/Furniture-Assembly-Web/">https://owensun2004.github.io/Furniture-Assembly-Web/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10090v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šäººç±»èƒ½è§£è¯»æŠ½è±¡è¯´æ˜ä¹¦å¹¶æ‰§è¡Œå¤æ‚çš„æ“ä½œä»»åŠ¡ï¼Œä½†æœºå™¨äººåœ¨è¿™æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†Manual2Skillæ¡†æ¶ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ ¹æ®é«˜çº§æ‰‹å†ŒæŒ‡ä»¤æ‰§è¡Œå¤æ‚çš„è£…é…ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä»æŒ‡ä»¤å›¾åƒä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œæ„å»ºå±‚æ¬¡è£…é…å›¾ï¼Œå¹¶é€šè¿‡å§¿æ€ä¼°è®¡æ¨¡å‹é¢„æµ‹æ¯ä¸ªè£…é…æ­¥éª¤ç»„ä»¶çš„6Då§¿æ€ï¼ŒåŒæ—¶è¿åŠ¨è§„åˆ’æ¨¡å—ç”Ÿæˆå¯ç”¨äºçœŸå®æœºå™¨äººå®æ–½çš„åŠ¨ä½œåºåˆ—ã€‚æˆåŠŸè£…é…IKEAå®¶å…·çš„å®è·µåº”ç”¨å±•ç¤ºäº†å…¶ç®¡ç†é•¿æœŸæ“ä½œä»»åŠ¡çš„èƒ½åŠ›å’Œæ•ˆç‡ï¼Œæé«˜äº†æœºå™¨äººä»è¯´æ˜ä¹¦å­¦ä¹ çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœºå™¨äººæ‰§è¡Œå¤æ‚æ“ä½œä»»åŠ¡çš„èƒ½åŠ›å—é™äºæ— æ³•è§£è¯»æŠ½è±¡æŒ‡ä»¤å¹¶è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œã€‚</li>
<li>Manual2Skillæ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†æŒ‡ä»¤å›¾åƒå¹¶æ„å»ºå±‚æ¬¡è£…é…å›¾ï¼Œå¸®åŠ©æœºå™¨äººå®Œæˆå¤æ‚è£…é…ä»»åŠ¡ã€‚</li>
<li>å§¿æ€ä¼°è®¡æ¨¡å‹åœ¨è£…é…è¿‡ç¨‹ä¸­é¢„æµ‹ç»„ä»¶çš„6Då§¿æ€ã€‚</li>
<li>è¿åŠ¨è§„åˆ’æ¨¡å—ä¸ºæœºå™¨äººç”Ÿæˆå®é™…æ‰§è¡ŒåŠ¨ä½œçš„é¡ºåºã€‚</li>
<li>é€šè¿‡æˆåŠŸè£…é…IKEAå®¶å…·çš„å®è·µåº”ç”¨ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶åœ¨æœºå™¨äººå­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Manual2Skillæ¡†æ¶æé«˜äº†æœºå™¨äººå¤„ç†é•¿æœŸæ“ä½œä»»åŠ¡çš„èƒ½åŠ›å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5552b327bc86cd53ad5de775f51483a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c39a6382d4f99841375d6771f2527489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806630b42a003c4b152aec433492d706.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a74674c6d9ad2b4a9bdfeb48d062b52b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction"><a href="#LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction" class="headerlink" title="LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction"></a>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction</h2><p><strong>Authors:Hrishav Bakul Barua, Kalin Stefanov, Lemuel Lai En Che, Abhinav Dhall, KokSheik Wong, Ganesh Krishnasamy</strong></p>
<p>The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: <a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> </p>
<blockquote>
<p>å°†ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒç¿»è¯‘æˆé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒæ˜¯ä¸€é¡¹é‡è¦çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚æœ‰å¤§é‡ç ”ç©¶é‡‡ç”¨ä¼ ç»Ÿçš„éå­¦ä¹ æ–¹æ³•ä»¥åŠç°ä»£çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œé‡ç‚¹ä½¿ç”¨å•æ›å…‰å’Œå¤šæ›å…‰çš„LDRè¿›è¡ŒHDRå›¾åƒé‡å»ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç›®å‰æœ€å‰æ²¿çš„æ–¹æ³•éƒ½éœ€è¦é«˜è´¨é‡é…å¯¹çš„{LDRï¼ŒHDR}æ•°æ®é›†æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œå…³äºä½¿ç”¨æœªé…å¯¹æ•°æ®é›†å®Œæˆè¯¥ä»»åŠ¡çš„æ–‡çŒ®å¾ˆå°‘ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹å­¦ä¹ åŸŸä¹‹é—´çš„æ˜ å°„ï¼Œå³{LDRï¼ŒHDR}ã€‚æœ¬æ–‡æå‡ºäº†LLM-HDRæ–¹æ³•ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›é›†æˆåˆ°ä¸€ä¸ªç»è¿‡ä¿®æ”¹çš„è¯­ä¹‰å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ€§æ¶æ„ä¸­ï¼Œè¯¥æ¶æ„åˆ©ç”¨æœªé…å¯¹çš„{LDRï¼ŒHDR}æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ–°å‹ä¼ªå½±å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ï¼Œä»¥åŠè§£å†³è¯­ä¹‰ä¸€è‡´æ€§è¿™ä¸€å°šæœªæ·±å…¥æ¢è®¨çš„é—®é¢˜çš„ç¼–ç å™¨å’ŒæŸå¤±å‡½æ•°ã€‚LLM-HDRæ˜¯ç¬¬ä¸€ä¸ªåœ¨è‡ªç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨LLMæ¥å®Œæˆ{LDRï¼ŒHDR}ç¿»è¯‘ä»»åŠ¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½é‡å»ºé«˜è´¨é‡çš„HDRå›¾åƒã€‚è¯¥å·¥ä½œçš„å®˜æ–¹ç½‘ç«™åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM">https://github.com/HrishavBakulBarua/LLM</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v3">PDF</a> </p>
<p><strong>Summary</strong>:<br>æ­¤æ–‡æœ¬ä¸»è¦ä»‹ç»äº†ä»ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒåˆ°é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒçš„ç¿»è¯‘åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚ç›®å‰å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•éƒ½éœ€è¦é«˜è´¨é‡é…å¯¹çš„æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLM-HDRçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›èå…¥åˆ°ä¸€ä¸ªä¿®æ”¹è¿‡çš„è¯­ä¹‰å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ€§æ¶æ„ä¸­ï¼Œåˆ©ç”¨æœªé…å¯¹çš„LDRå’ŒHDRæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ–°çš„ä¼ªå½±æ„ŸçŸ¥ç”Ÿæˆå™¨å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªç¼–ç å™¨å’ŒæŸå¤±æ¥è§£å†³è¯­ä¹‰ä¸€è‡´æ€§è¿™ä¸€å°šæœªæ¢ç´¢çš„é—®é¢˜ã€‚LLM-HDRæ˜¯ç¬¬ä¸€ä¸ªåœ¨è‡ªæˆ‘ç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨LLMè¿›è¡ŒLDRåˆ°HDRç¿»è¯‘ä»»åŠ¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½é‡å»ºé«˜è´¨é‡HDRå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LDRåˆ°HDRå›¾åƒçš„ç¿»è¯‘æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å¤§å¤šéœ€è¦é«˜è´¨é‡é…å¯¹çš„æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>LLM-HDRæ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>LLM-HDRé‡‡ç”¨ä¿®æ”¹è¿‡çš„è¯­ä¹‰å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ€§æ¶æ„ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æœªé…å¯¹çš„LDRå’ŒHDRæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>LLM-HDRé€šè¿‡å¼•å…¥æ–°çš„ä¼ªå½±æ„ŸçŸ¥ç”Ÿæˆå™¨å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0604352e626b0cd673be111849895840.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c796c8b662950542046b576ea2142a76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9558764027549d613a2463ed6c1f1f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PID-Physics-Informed-Diffusion-Model-for-Infrared-Image-Generation"><a href="#PID-Physics-Informed-Diffusion-Model-for-Infrared-Image-Generation" class="headerlink" title="PID: Physics-Informed Diffusion Model for Infrared Image Generation"></a>PID: Physics-Informed Diffusion Model for Infrared Image Generation</h2><p><strong>Authors:Fangyuan Mao, Jilin Mei, Shun Lu, Fuyang Liu, Liang Chen, Fangzhou Zhao, Yu Hu</strong></p>
<p>Infrared imaging technology has gained significant attention for its reliable sensing ability in low visibility conditions, prompting many studies to convert the abundant RGB images to infrared images. However, most existing image translation methods treat infrared images as a stylistic variation, neglecting the underlying physical laws, which limits their practical application. To address these issues, we propose a Physics-Informed Diffusion (PID) model for translating RGB images to infrared images that adhere to physical laws. Our method leverages the iterative optimization of the diffusion model and incorporates strong physical constraints based on prior knowledge of infrared laws during training. This approach enhances the similarity between translated infrared images and the real infrared domain without increasing extra training parameters. Experimental results demonstrate that PID significantly outperforms existing state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fangyuanmao/PID">https://github.com/fangyuanmao/PID</a>. </p>
<blockquote>
<p>çº¢å¤–æˆåƒæŠ€æœ¯å› å…¶ä½èƒ½è§åº¦æ¡ä»¶ä¸‹çš„å¯é æ„ŸçŸ¥èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä¿ƒä½¿è®¸å¤šç ”ç©¶å°†ä¸°å¯Œçš„RGBå›¾åƒè½¬æ¢ä¸ºçº¢å¤–å›¾åƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å›¾åƒç¿»è¯‘æ–¹æ³•å°†çº¢å¤–å›¾åƒè§†ä¸ºé£æ ¼å˜åŒ–ï¼Œå¿½ç•¥äº†å…¶èƒŒåçš„ç‰©ç†å®šå¾‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†ä¿¡æ¯çš„æ‰©æ•£ï¼ˆPIDï¼‰æ¨¡å‹ï¼Œç”¨äºå°†RGBå›¾åƒè½¬æ¢ä¸ºéµå¾ªç‰©ç†å®šå¾‹çš„çº¢å¤–å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¿­ä»£ä¼˜åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆçº¢å¤–å®šå¾‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œèå…¥å¼ºå¤§çš„ç‰©ç†çº¦æŸã€‚è¿™ç§æ–¹æ³•æé«˜äº†ç¿»è¯‘åçš„çº¢å¤–å›¾åƒä¸çœŸå®çº¢å¤–åŸŸä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä¸”æ²¡æœ‰å¢åŠ é¢å¤–çš„è®­ç»ƒå‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIDæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fangyuanmao/PID%E5%A4%84%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/fangyuanmao/PIDå¤„ä¸‹è½½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09299v2">PDF</a> Accepted by Pattern Recognition</p>
<p><strong>Summary</strong></p>
<p>çº¢å¤–æˆåƒæŠ€æœ¯åœ¨ä½èƒ½è§åº¦æ¡ä»¶ä¸‹çš„å¯é æ„ŸçŸ¥èƒ½åŠ›å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä¿ƒä½¿è®¸å¤šç ”ç©¶å°†ä¸°å¯Œçš„RGBå›¾åƒè½¬æ¢ä¸ºçº¢å¤–å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒç¿»è¯‘æ–¹æ³•å¤§å¤šå°†çº¢å¤–å›¾åƒè§†ä¸ºé£æ ¼å˜åŒ–ï¼Œå¿½ç•¥äº†å…¶èƒŒåçš„ç‰©ç†å®šå¾‹ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“åˆç‰©ç†å®šå¾‹çš„æ‰©æ•£æ¨¡å‹ï¼ˆPIDï¼‰ï¼Œç”¨äºå°†RGBå›¾åƒç¿»è¯‘ä¸ºçº¢å¤–å›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©æ•£æ¨¡å‹çš„è¿­ä»£ä¼˜åŒ–ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥åŸºäºçº¢å¤–å®šå¾‹çš„å¼ºç‰©ç†çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIDæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¢å¤–æˆåƒæŠ€æœ¯åœ¨ä½èƒ½è§åº¦æ¡ä»¶ä¸‹å…·æœ‰å¯é æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¼•å‘å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰å›¾åƒç¿»è¯‘æ–¹æ³•å¤šå¿½ç•¥çº¢å¤–å›¾åƒçš„ç‰©ç†å®šå¾‹ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>æå‡ºçš„PIDæ¨¡å‹ç»“åˆç‰©ç†å®šå¾‹è¿›è¡ŒRGBåˆ°çº¢å¤–å›¾åƒçš„ç¿»è¯‘ã€‚</li>
<li>PIDæ¨¡å‹é€šè¿‡æ‰©æ•£æ¨¡å‹çš„è¿­ä»£ä¼˜åŒ–ï¼Œèå…¥å¼ºç‰©ç†çº¦æŸã€‚</li>
<li>PIDæ¨¡å‹åœ¨ç¿»è¯‘çº¢å¤–å›¾åƒæ—¶ï¼Œå¢å¼ºäº†ä¸çœŸå®çº¢å¤–é¢†åŸŸçš„ç›¸ä¼¼æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPIDæ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.09299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bdf65a0aeb2c6d679ed4ee090b7fcbf6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24d89a8656c23c0a949f3ea4d265383a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce4f80ed901ecd40f40a7a4a34f6e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e4ee69c84a617dc87d54a35b18f4e56.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30b5df75b847d650ac3666deb9e58ca7.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  CyberV Cybernetics for Test-time Scaling in Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f89150e1506b94b2747fd9537302af55.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Vuyko Mistral Adapting LLMs for Low-Resource Dialectal Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
