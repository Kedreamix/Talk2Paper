<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Vuyko Mistral Adapting LLMs for Low-Resource Dialectal Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f89150e1506b94b2747fd9537302af55.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="Vuyko-Mistral-Adapting-LLMs-for-Low-Resource-Dialectal-Translation"><a href="#Vuyko-Mistral-Adapting-LLMs-for-Low-Resource-Dialectal-Translation" class="headerlink" title="Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation"></a>Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation</h2><p><strong>Authors:Roman Kyslyi, Yuliia Maksymiuk, Ihor Pysmennyi</strong></p>
<p>In this paper we introduce the first effort to adapt large language models (LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and morphologically complex dialect spoken in the Carpathian Highlands. We created a parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a dictionary of 7320 dialectal word mappings. We also addressed data shortage by proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate synthetic parallel translation pairs, expanding the corpus with 52142 examples. We have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a standard-to-dialect translation task, also comparing with few-shot GPT-4o translation. In the absence of human annotators, we adopt a multi-metric evaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment (GPT-4o). The results show that even small(7B) finetuned models outperform zero-shot baselines such as GPT-4o across both automatic and LLM-evaluated metrics. All data, models, and code are publicly released at: <a target="_blank" rel="noopener" href="https://github.com/woters/vuyko-hutsul">https://github.com/woters/vuyko-hutsul</a> </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é¦–æ¬¡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€‚åº”ä¹Œå…‹å…°æ–¹è¨€ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯èµ«æ‰˜å§†è¯­ï¼‰çš„åŠªåŠ›ã€‚è¿™æ˜¯ä¸€ç§èµ„æºè´«ä¹ä¸”å½¢æ€å¤æ‚çš„æ–¹è¨€ï¼Œä½äºå–€å°”å·´é˜¡å±±è„‰é«˜åœ°ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«9852ä¸ªæ–¹è¨€åˆ°æ ‡å‡†ä¹Œå…‹å…°è¯­çš„å¥å­å¯¹çš„å¹³è¡Œè¯­æ–™åº“ï¼Œä»¥åŠåŒ…å«7320ä¸ªæ–¹è¨€è¯æ±‡æ˜ å°„çš„è¯å…¸ã€‚ä¸ºäº†è§£å†³æ•°æ®çŸ­ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“æ¥ç”Ÿæˆåˆæˆå¹³è¡Œç¿»è¯‘å¯¹ï¼Œæ‰©å¤§äº†è¯­æ–™åº“è‡³åŒ…å«52,142ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬ä½¿ç”¨LoRAå¾®è°ƒäº†å¤šä¸ªå¼€æºLLMï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œäº†ä¸€é¡¹æ ‡å‡†åˆ°æ–¹è¨€çš„ç¿»è¯‘ä»»åŠ¡è¯„ä¼°ï¼ŒåŒæ—¶è¿˜ä¸GPT-4oçš„ç¿»è¯‘è¿›è¡Œäº†å°‘æ ·æœ¬æ¯”è¾ƒã€‚åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨å™¨çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¤šæŒ‡æ ‡è¯„ä¼°ç­–ç•¥ï¼Œç»“åˆäº†BLEUã€chrF++ã€TERå’ŒåŸºäºLLMçš„åˆ¤æ–­ï¼ˆGPT-4oï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨è‡ªåŠ¨å’ŒLLMè¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œå¾®è°ƒçš„å°å‹ï¼ˆ7Bï¼‰æ¨¡å‹ä¹Ÿä¼˜äºé›¶æ ·æœ¬åŸºçº¿ï¼Œå¦‚GPT-4oã€‚æ‰€æœ‰æ•°æ®ã€æ¨¡å‹å’Œä»£ç å‡å·²å…¬å¼€å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/woters/vuyko-hutsul">https://github.com/woters/vuyko-hutsul</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07617v1">PDF</a> Preprint. Will be published at Proceedings of the Fourth Ukrainian   Natural Language Processing Workshop (UNLP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”ä¹Œå…‹å…°è¯­æ–¹è¨€ï¼ˆæœ¬ä¾‹ä¸ºèƒ¡èŒ¨è¯­ï¼‰çš„é¦–æ¬¡å°è¯•ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†åŒ…å«9852ä¸ªæ–¹è¨€è‡³æ ‡å‡†ä¹Œå…‹å…°è¯­å¥å­å¯¹çš„å¹³è¡Œè¯­æ–™åº“ï¼Œå¹¶å»ºç«‹äº†åŒ…å«7320ä¸ªæ–¹è¨€è¯æ±‡æ˜ å°„çš„è¯å…¸ã€‚ä¸ºè§£å†³æ•°æ®çŸ­ç¼ºé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†å…ˆè¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ï¼Œç”Ÿæˆåˆæˆå¹³è¡Œç¿»è¯‘å¯¹ï¼Œå¢åŠ äº†52142ä¸ªä¾‹å­ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨LoRAå¾®è°ƒäº†å¤šä¸ªå¼€æºLLMsï¼Œå¹¶åœ¨æ ‡å‡†åˆ°æ–¹è¨€çš„ç¿»è¯‘ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒæ—¶ä¸GPT-4oçš„å°‘é‡ç¿»è¯‘è¿›è¡Œäº†æ¯”è¾ƒã€‚ç”±äºæ²¡æœ‰äººå·¥æ ‡æ³¨è€…ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†å¤šæŒ‡æ ‡è¯„ä¼°ç­–ç•¥ï¼Œç»“åˆäº†BLEUã€chrF++ã€TERå’ŒåŸºäºLLMçš„åˆ¤æ–­ï¼ˆGPT-4oï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¾ƒå°çš„ï¼ˆ7Bï¼‰å¾®è°ƒæ¨¡å‹ä¹Ÿèƒ½åœ¨è‡ªåŠ¨å’ŒLLMè¯„ä¼°æŒ‡æ ‡ä¸Šè¶…è¶Šé›¶æ ·æœ¬åŸºçº¿ï¼Œå¦‚GPT-4oã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿé¦–æ¬¡å°è¯•å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚åº”ä¹Œå…‹å…°çš„èƒ¡èŒ¨æ–¹è¨€ï¼Œè¿™æ˜¯ä¸€ç§èµ„æºè¾ƒå°‘ä¸”å½¢æ€å¤æ‚çš„æ–¹è¨€ã€‚</li>
<li>åˆ›å»ºäº†åŒ…å«9852ä¸ªæ–¹è¨€è‡³æ ‡å‡†ä¹Œå…‹å…°è¯­å¥å­å¯¹çš„å¹³è¡Œè¯­æ–™åº“å’ŒåŒ…å«7320ä¸ªæ–¹è¨€è¯æ±‡æ˜ å°„çš„è¯å…¸ã€‚</li>
<li>æå‡ºå…ˆè¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“æ¥è§£å†³æ•°æ®çŸ­ç¼ºé—®é¢˜ï¼ŒæˆåŠŸç”Ÿæˆäº†åˆæˆå¹³è¡Œç¿»è¯‘å¯¹ï¼Œå¢åŠ äº†52142ä¸ªä¾‹å­ã€‚</li>
<li>ä½¿ç”¨LoRAå¾®è°ƒäº†å¤šä¸ªå¼€æºLLMsï¼Œå¹¶åœ¨æ ‡å‡†è‡³æ–¹è¨€ç¿»è¯‘ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å¯¹æ¯”äº†å¾®è°ƒæ¨¡å‹ä¸GPT-4oåœ¨å°‘é‡ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¤šæŒ‡æ ‡è¯„ä¼°ç­–ç•¥ï¼ŒåŒ…æ‹¬BLEUã€chrF++ã€TERå’ŒåŸºäºLLMçš„åˆ¤æ–­ï¼ˆGPT-4oï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1baf1c6a65cfdc28476438a42e83aa18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86c7f4faae312d1f0ffa207b896b3e30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f622257f17565a798b4ded80dcf5f40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78d52bf07b3288665fe0fc5a447aedc4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Multilingual-Vulnerability-Detection-How-Far-Are-We"><a href="#Large-Language-Models-for-Multilingual-Vulnerability-Detection-How-Far-Are-We" class="headerlink" title="Large Language Models for Multilingual Vulnerability Detection: How Far   Are We?"></a>Large Language Models for Multilingual Vulnerability Detection: How Far   Are We?</h2><p><strong>Authors:Honglin Shu, Michael Fu, Junji Yu, Dong Wang, Chakkrit Tantithamthavorn, Junjie Chen, Yasutaka Kamei</strong></p>
<p>Various deep learning-based approaches utilizing pre-trained language models (PLMs) have been proposed for automated vulnerability detection. With recent advancements in large language models (LLMs), several studies have begun exploring their application to vulnerability detection tasks. However, existing studies primarily focus on specific programming languages (e.g., C&#x2F;C++) and function-level detection, leaving the strengths and weaknesses of PLMs and LLMs in multilingual and multi-granularity scenarios largely unexplored. To bridge this gap, we conduct a comprehensive fine-grained empirical study evaluating the effectiveness of state-of-the-art PLMs and LLMs for multilingual vulnerability detection. Using over 30,000 real-world vulnerability-fixing patches across seven programming languages, we systematically assess model performance at both the function-level and line-level. Our key findings indicate that GPT-4o, enhanced through instruction tuning and few-shot prompting, significantly outperforms all other evaluated models, including CodeT5P. Furthermore, the LLM-based approach demonstrates superior capability in detecting unique multilingual vulnerabilities, particularly excelling in identifying the most dangerous and high-severity vulnerabilities. These results underscore the promising potential of adopting LLMs for multilingual vulnerability detection at function-level and line-level, revealing their complementary strengths and substantial improvements over PLM approaches. This first empirical evaluation of PLMs and LLMs for multilingual vulnerability detection highlights LLMsâ€™ value in addressing real-world software security challenges. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹å·²ç»è¢«æå‡ºã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸå‘å±•ï¼Œä¸€äº›ç ”ç©¶å¼€å§‹æ¢ç´¢å…¶åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰å’Œå‡½æ•°çº§åˆ«çš„æ£€æµ‹ä¸Šï¼Œå¯¹äºå¤šè¯­è¨€å’Œå¤šç²’åº¦åœºæ™¯ä¸‹PLMå’ŒLLMçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹å½“å‰å…ˆè¿›çš„PLMå’ŒLLMåœ¨å¤šè¯­è¨€æ¼æ´æ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†å…¨é¢çš„ç²¾ç»†å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬åˆ©ç”¨è¶…è¿‡ä¸‰ä¸‡ä»½çœŸå®ä¸–ç•Œçš„æ¼æ´ä¿®å¤è¡¥ä¸ï¼Œè·¨è¶Šä¸ƒç§ç¼–ç¨‹è¯­è¨€ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹åœ¨å‡½æ•°çº§åˆ«å’Œè¡Œçº§åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°è¡¨æ˜ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒä¼˜å’Œå°‘é‡æç¤ºå¢å¼ºçš„GPT-4oåœ¨æ‰€æœ‰å…¶ä»–è¯„ä¼°æ¨¡å‹ä¸­è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼ŒåŒ…æ‹¬CodeT5Pã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨æ£€æµ‹ç‹¬ç‰¹çš„å¤šè¯­è¨€æ¼æ´æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ“…é•¿è¯†åˆ«æœ€å±é™©å’Œé«˜ä¸¥é‡ç¨‹åº¦çš„æ¼æ´ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†é‡‡ç”¨LLMè¿›è¡Œå¤šè¯­è¨€æ¼æ´æ£€æµ‹ï¼ˆåœ¨å‡½æ•°çº§åˆ«å’Œè¡Œçº§åˆ«ï¼‰çš„æ½œåŠ›ï¼Œå±•ç°äº†å…¶äº’è¡¥ä¼˜åŠ¿å’Œç›¸å¯¹äºPLMæ–¹æ³•çš„é‡å¤§æ”¹è¿›ã€‚è¿™æ˜¯å¯¹PLMå’ŒLLMåœ¨å¤šè¯­è¨€æ¼æ´æ£€æµ‹æ–¹é¢çš„é¦–æ¬¡å®è¯ç ”ç©¶ï¼Œçªå‡ºäº†LLMåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶å®‰å…¨æŒ‘æˆ˜ä¸­çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07503v1">PDF</a> 33 pages, 9 figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é’ˆå¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€æ¼æ´æ£€æµ‹æ–¹é¢çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢ç»†è‡´çš„å®è¯ç ”ç©¶ã€‚ç ”ç©¶ä½¿ç”¨è¶…è¿‡3ä¸‡ä»½çœŸå®ä¸–ç•Œçš„æ¼æ´ä¿®å¤è¡¥ä¸ï¼Œåœ¨ä¸ƒç§ç¼–ç¨‹è¯­è¨€ä¸‹å¯¹æ¨¡å‹è¿›è¡Œäº†åŠŸèƒ½çº§åˆ«å’Œè¡Œçº§åˆ«çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œå°‘æ ·æœ¬æç¤ºå¢å¼ºçš„GPT-4oåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå°¤å…¶æ˜¯æ£€æµ‹å’Œè¯†åˆ«å¤šè¯­è¨€å’Œé«˜é£é™©æ¼æ´æ–¹é¢çš„èƒ½åŠ›æ›´ä¸ºçªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé¡¹åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²åº”ç”¨äºè‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹ï¼Œå°¤å…¶æ˜¯åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚C&#x2F;C++ï¼‰å’ŒåŠŸèƒ½çº§åˆ«çš„æ£€æµ‹ï¼Œå¯¹äºå¤šè¯­è¨€å’Œå¤šç²’åº¦åœºæ™¯ä¸­çš„æ¨¡å‹å¼ºå¼±å°šå¾…æ¢ç´¢ã€‚</li>
<li>GPT-4oåœ¨æŒ‡ä»¤è°ƒæ•´å’Œå°‘æ ·æœ¬æç¤ºçš„å¢å¼ºä¸‹ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬CodeT5Pã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹ç‹¬ç‰¹çš„å¤šè¯­è¨€æ¼æ´æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå°¤å…¶æ“…é•¿è¯†åˆ«é«˜é£é™©å’Œé«˜ä¸¥é‡æ€§çš„æ¼æ´ã€‚</li>
<li>LLMsåœ¨åŠŸèƒ½çº§åˆ«å’Œè¡Œçº§åˆ«çš„æ¼æ´æ£€æµ‹ä¸­å±•ç°å‡ºäº’è¡¥ä¼˜åŠ¿å’Œå®è´¨æ€§æ”¹è¿›ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å¯¹PLMså’ŒLLMsåœ¨å¤šè¯­è¨€æ¼æ´æ£€æµ‹é¢†åŸŸè¿›è¡Œçš„å®è¯ç ”ç©¶ï¼Œçªå‡ºäº†LLMsåœ¨è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶å®‰å…¨æŒ‘æˆ˜ä¸­çš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f80769346fd26a09814959eefcf59a3c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Prompt-to-Protection-A-Comparative-Study-of-Multimodal-LLMs-in-Construction-Hazard-Recognition"><a href="#Prompt-to-Protection-A-Comparative-Study-of-Multimodal-LLMs-in-Construction-Hazard-Recognition" class="headerlink" title="Prompt to Protection: A Comparative Study of Multimodal LLMs in   Construction Hazard Recognition"></a>Prompt to Protection: A Comparative Study of Multimodal LLMs in   Construction Hazard Recognition</h2><p><strong>Authors:Nishi Chaudhary, S M Jamil Uddin, Sathvik Sharath Chandra, Anto Ovid, Alex Albert</strong></p>
<p>The recent emergence of multimodal large language models (LLMs) has introduced new opportunities for improving visual hazard recognition on construction sites. Unlike traditional computer vision models that rely on domain-specific training and extensive datasets, modern LLMs can interpret and describe complex visual scenes using simple natural language prompts. However, despite growing interest in their applications, there has been limited investigation into how different LLMs perform in safety-critical visual tasks within the construction domain. To address this gap, this study conducts a comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5, GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify potential hazards from real-world construction images. Each model was tested under three prompting strategies: zero-shot, few-shot, and chain-of-thought (CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated basic safety context and a hazard source mnemonic, and CoT provided step-by-step reasoning examples to scaffold model thinking. Quantitative analysis was performed using precision, recall, and F1-score metrics across all conditions. Results reveal that prompting strategy significantly influenced performance, with CoT prompting consistently producing higher accuracy across models. Additionally, LLM performance varied under different conditions, with GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also demonstrate the critical role of prompt design in enhancing the accuracy and consistency of multimodal LLMs for construction safety applications. This study offers actionable insights into the integration of prompt engineering and LLMs for practical hazard recognition, contributing to the development of more reliable AI-assisted safety systems. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºæ”¹å–„æ–½å·¥ç°åœºçš„è§†è§‰å±é™©è¯†åˆ«æä¾›äº†æ–°çš„æœºä¼šã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–ç‰¹å®šé¢†åŸŸè®­ç»ƒå’Œå¤§é‡æ•°æ®é›†çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸åŒï¼Œç°ä»£LLMå¯ä»¥ä½¿ç”¨ç®€å•çš„è‡ªç„¶è¯­è¨€æç¤ºæ¥è§£é‡Šå’Œæè¿°å¤æ‚çš„è§†è§‰åœºæ™¯ã€‚ç„¶è€Œï¼Œå°½ç®¡å¯¹å…¶åº”ç”¨çš„å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œä½†å…³äºå¦‚ä½•åœ¨å»ºç­‘é¢†åŸŸçš„å®‰å…¨å…³é”®è§†è§‰ä»»åŠ¡ä¸­ä¸åŒLLMçš„è¡¨ç°çš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬ç ”ç©¶å¯¹äº”ç§æœ€æ–°LLMè¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ï¼šClaude-3 Opusã€GPT-4.5ã€GPT-4oã€GPT-o3å’ŒGemini 2.0 Proï¼Œä»¥è¯„ä¼°å®ƒä»¬ä»ç°å®ä¸–ç•Œå»ºç­‘å›¾åƒä¸­è¯†åˆ«æ½œåœ¨å±é™©çš„èƒ½åŠ›ã€‚æ¯ç§æ¨¡å‹éƒ½åœ¨ä¸‰ç§æç¤ºç­–ç•¥ä¸‹è¿›è¡Œäº†æµ‹è¯•ï¼šé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚é›¶æ ·æœ¬æç¤ºæ¶‰åŠæœ€å°æŒ‡ä»¤ï¼Œå°‘æ ·æœ¬ç»“åˆäº†åŸºæœ¬çš„å®‰å…¨èƒŒæ™¯å’Œå±é™©æºæç¤ºï¼Œè€Œæ€ç»´é“¾æç¤ºåˆ™æä¾›äº†é€æ­¥æ¨ç†çš„ç¤ºä¾‹æ¥å¼•å¯¼æ¨¡å‹æ€è€ƒã€‚ä½¿ç”¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æŒ‡æ ‡å¯¹æ‰€æœ‰æ¡ä»¶ä¸‹çš„ç»“æœè¿›è¡Œäº†å®šé‡åˆ†æã€‚ç»“æœæ­ç¤ºï¼Œæç¤ºç­–ç•¥å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œæ€ç»´é“¾æç¤ºåœ¨æ¨¡å‹ä¹‹é—´å§‹ç»ˆäº§ç”Ÿæ›´é«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œåœ¨ä¸åŒæ¡ä»¶ä¸‹LLMçš„è¡¨ç°æœ‰æ‰€ä¸åŒï¼ŒGPT-4.5å’ŒGPT-o3åœ¨å¤§å¤šæ•°è®¾ç½®ä¸­éƒ½è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¿˜è¡¨æ˜ï¼Œæç¤ºè®¾è®¡åœ¨å¢å¼ºå¤šæ¨¡æ€LLMåœ¨å»ºç­‘æ–½å·¥å®‰å…¨åº”ç”¨ä¸­çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚æœ¬ç ”ç©¶ä¸ºæ•´åˆæç¤ºå·¥ç¨‹å’ŒLLMè¿›è¡Œå®é™…å±é™©è¯†åˆ«æä¾›äº†åˆ‡å®å¯è¡Œçš„è§è§£ï¼Œæœ‰åŠ©äºå¼€å‘æ›´å¯é çš„AIè¾…åŠ©å®‰å…¨ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07436v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†äº”ç§æœ€å…ˆè¿›çš„æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«å»ºç­‘å·¥åœ°æ½œåœ¨å±é™©æ–¹é¢çš„è¡¨ç°ã€‚é€šè¿‡é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œé“¾å¼æ€ç»´æç¤ºç­–ç•¥è¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°æç¤ºç­–ç•¥å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œé“¾å¼æ€ç»´æç¤ºç­–ç•¥è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œä¸åŒæ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼ŒGPT-4.5å’ŒGPT-o3åœ¨å¤§å¤šæ•°è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæç¤ºè®¾è®¡å¯¹äºæé«˜æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å»ºç­‘æ–½å·¥å®‰å…¨åº”ç”¨ä¸­çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å»ºç­‘å·¥åœ°å±é™©è¯†åˆ«ä¸­å…·æœ‰æ”¹è¿›æ½œåŠ›ã€‚</li>
<li>ä¸åŒçš„è¯­è¨€æ¨¡å‹åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>æç¤ºç­–ç•¥ï¼ˆå¦‚é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œé“¾å¼æ€ç»´ï¼‰å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>é“¾å¼æ€ç»´æç¤ºç­–ç•¥åœ¨æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>GPT-4.5å’ŒGPT-o3åœ¨å¤§å¤šæ•°è®¾ç½®ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>æç¤ºè®¾è®¡åœ¨æé«˜æ¨¡å‹å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ca45b78d5d2d883e28aeb462d140a69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a7895a06ecc4d7c4c838fba452ca0bb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Variational-Supervised-Contrastive-Learning"><a href="#Variational-Supervised-Contrastive-Learning" class="headerlink" title="Variational Supervised Contrastive Learning"></a>Variational Supervised Contrastive Learning</h2><p><strong>Authors:Ziwen Wang, Jiajun Fan, Thao Nguyen, Heng Ji, Ge Liu</strong></p>
<p>Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. </p>
<blockquote>
<p>å¯¹æ¯”å­¦ä¹ å·²åœ¨ä¸åŒæ¨¡æ€çš„å½¢çŠ¶è¡¨ç¤ºç©ºé—´ä¸­å±•ç°å‡ºé«˜æ•ˆå’Œé€‚åº”æ€§å¼ºçš„ç‰¹ç‚¹ï¼Œé€šè¿‡å°†ç›¸ä¼¼æ ·æœ¬æ‹‰åœ¨ä¸€èµ·å¹¶å°†ä¸ç›¸ä¼¼æ ·æœ¬æ¨å¼€ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰è‹¥æ²¡æœ‰å¯¹åµŒå…¥åˆ†å¸ƒè¿›è¡Œæ˜ç¡®è°ƒæ§ï¼Œé™¤éæœ‰äº’è¡¥ä¿¡å·å¼•å¯¼é…å¯¹é€‰æ‹©ï¼Œè¯­ä¹‰ä¸Šç›¸å…³çš„å®ä¾‹å¯èƒ½ä¼šä¸ç»æ„åœ°è¢«æ¨å¼€ï¼›ï¼ˆ2ï¼‰è¿‡åº¦ä¾èµ–å¤§é‡å†…éƒ¨æ‰¹æ¬¡è´Ÿæ ·æœ¬å’Œå®šåˆ¶å¢å¼ºç­–ç•¥ä¼šé˜»ç¢æ³›åŒ–ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºå˜åˆ†ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆVarConï¼‰ï¼Œå®ƒå°†ç›‘ç£å¯¹æ¯”å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºæ½œåœ¨ç±»åˆ«å˜é‡ä¸Šçš„å˜åˆ†æ¨æ–­ï¼Œå¹¶æœ€å¤§åŒ–åéªŒåŠ æƒè¯æ®ä¸‹é™ï¼ˆELBOï¼‰ï¼Œä»¥æ›¿ä»£è¯¦å°½çš„é…å¯¹æ¯”è¾ƒï¼Œå®ç°é«˜æ•ˆçš„ç±»æ„ŸçŸ¥åŒ¹é…ï¼Œå¹¶åœ¨åµŒå…¥ç©ºé—´ä¸­ç²¾ç»†æ§åˆ¶ç±»å†…ç¦»æ•£åº¦ã€‚ä»…åœ¨å›¾åƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬åœ¨CIFAR-10ã€CIFAR-100ã€ImageNet-100å’ŒImageNet-1Kä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVarConï¼ˆ1ï¼‰å®ç°äº†å¯¹æ¯”å­¦ä¹ æ¡†æ¶çš„æœ€æ–°æ€§èƒ½ï¼Œåœ¨ImageNet-1Kä¸Šè¾¾åˆ°79.36%çš„Top-1å‡†ç¡®ç‡ï¼Œåœ¨CIFAR-100ä¸Šä½¿ç”¨ResNet-50ç¼–ç å™¨è¾¾åˆ°78.29%ï¼Œå¹¶åœ¨ä»…200ä¸ªå‘¨æœŸå†…æ”¶æ•›ï¼›ï¼ˆ2ï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­çš„å†³ç­–è¾¹ç•Œå’Œè¯­ä¹‰ç»„ç»‡æ›´åŠ æ¸…æ™°ï¼Œè¿™ç”±KNNåˆ†ç±»ã€å±‚æ¬¡èšç±»ç»“æœå’Œè¿ç§»å­¦ä¹ è¯„ä¼°æ‰€è¯æ˜ï¼›ï¼ˆ3ï¼‰åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°å‡ºä¼˜äºç›‘ç£åŸºå‡†å’Œå¤šç§å¢å¼ºç­–ç•¥çš„ä¼˜è¶Šæ€§èƒ½å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¯¹æ¯”å­¦ä¹ åœ¨æ„å»ºè·¨ä¸åŒæ¨¡æ€çš„è¡¨ç¤ºç©ºé—´æ—¶å±•ç°å‡ºé«˜æ•ˆå’Œé€‚åº”æ€§ï¼Œé€šè¿‡æ‹‰è¿‘ç›¸ä¼¼æ ·æœ¬å¹¶æ¨è¿œä¸ç›¸ä¼¼æ ·æœ¬ã€‚ç„¶è€Œï¼Œå­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ç¼ºä¹åµŒå…¥åˆ†å¸ƒçš„æ˜ç¡®è°ƒæ§ï¼Œé™¤éæœ‰äº’è¡¥ä¿¡å·å¼•å¯¼é…å¯¹é€‰æ‹©ï¼Œå¦åˆ™è¯­ä¹‰ç›¸å…³çš„å®ä¾‹å¯èƒ½ä¼šè¢«æ— æ„ä¸­æ¨å¼€ï¼›äºŒæ˜¯è¿‡åº¦ä¾èµ–å¤§é‡å†…æ‰¹è´Ÿæ ·æœ¬å’Œå®šåˆ¶å¢å¼ºç­–ç•¥ï¼Œé˜»ç¢äº†æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå˜åˆ†ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆVarConï¼‰ï¼Œå°†ç›‘ç£å¯¹æ¯”å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºæ½œåœ¨ç±»åˆ«å˜é‡çš„å˜åˆ†æ¨æ–­ï¼Œå¹¶æœ€å¤§åŒ–åéªŒåŠ æƒè¯æ®ä¸‹é™ï¼ˆELBOï¼‰ï¼Œä»¥æ›¿ä»£è¯¦å°½çš„é…å¯¹æ¯”è¾ƒï¼Œå®ç°é«˜æ•ˆçš„ç±»åˆ«æ„ŸçŸ¥åŒ¹é…ï¼Œå¹¶åœ¨åµŒå…¥ç©ºé—´ä¸­ç²¾ç»†æ§åˆ¶ç±»å†…ç¦»æ•£åº¦ã€‚ä»…å¯¹å›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜VarConåœ¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä½¿ç”¨ResNet-50ç¼–ç å™¨åœ¨ImageNet-1Kä¸Šè¾¾åˆ°79.36%çš„Top-1å‡†ç¡®ç‡å’Œåœ¨CIFAR-100ä¸Šè¾¾åˆ°78.29%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨ä»…200ä¸ªå‘¨æœŸå†…æ”¶æ•›ï¼›å…¶å®éªŒç»“æœè¿˜è¡¨æ˜ï¼ŒVarConåœ¨åµŒå…¥ç©ºé—´ä¸­äº§ç”Ÿäº†æ›´æ¸…æ™°å†³ç­–è¾¹ç•Œå’Œè¯­ä¹‰ç»„ç»‡ï¼Œå¹¶åœ¨KNNåˆ†ç±»ã€å±‚æ¬¡èšç±»ç»“æœå’Œè¿ç§»å­¦ä¹ è¯„ä¼°ä¸­å¾—åˆ°è¯å®ï¼›æ­¤å¤–ï¼ŒVarConåœ¨å°‘é•œå¤´å­¦ä¹ ä¸Šè¡¨ç°å‡ºä¼˜äºç›‘ç£åŸºå‡†çš„ä¼˜è¶Šæ€§èƒ½å’Œåœ¨å„ç§å¢å¼ºç­–ç•¥ä¸­çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ åœ¨ä¸åŒæ¨¡æ€é—´æ„å»ºè¡¨ç¤ºç©ºé—´æ—¶å±•ç°é«˜æ•ˆå’Œé€‚åº”æ€§ã€‚</li>
<li>ç°æœ‰å¯¹æ¯”å­¦ä¹ å­˜åœ¨è¯­ä¹‰ç›¸å…³å®ä¾‹å¯èƒ½è¢«æ¨å¼€ã€è¿‡åº¦ä¾èµ–å†…æ‰¹è´Ÿæ ·æœ¬å’Œå®šåˆ¶å¢å¼ºç­–ç•¥çš„å±€é™ã€‚</li>
<li>å˜åˆ†ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆVarConï¼‰é€šè¿‡å˜åˆ†æ¨æ–­æ”¹è¿›å¯¹æ¯”å­¦ä¹ ï¼Œå®ç°æ›´ç²¾ç»†çš„ç±»åˆ«æ„ŸçŸ¥åŒ¹é…å’Œç±»å†…ç¦»æ•£åº¦æ§åˆ¶ã€‚</li>
<li>VarConåœ¨å›¾åƒæ•°æ®ä¸Šè¾¾åˆ°å¯¹æ¯”å­¦ä¹ æœ€ä½³æ€§èƒ½ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ï¼Œå‡†ç¡®ç‡é«˜ã€‚</li>
<li>VarConäº§ç”Ÿçš„åµŒå…¥ç©ºé—´å…·æœ‰æ›´æ¸…æ™°å†³ç­–è¾¹ç•Œå’Œè¯­ä¹‰ç»„ç»‡ã€‚</li>
<li>VarConåœ¨å°‘é•œå¤´å­¦ä¹ ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºç›‘ç£åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44b6b55071b66f2ad2226c50ec86f558.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb15fab0e13c2d1ff96c357021e20ffc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adapter-Naturally-Serves-as-Decoupler-for-Cross-Domain-Few-Shot-Semantic-Segmentation"><a href="#Adapter-Naturally-Serves-as-Decoupler-for-Cross-Domain-Few-Shot-Semantic-Segmentation" class="headerlink" title="Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic   Segmentation"></a>Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic   Segmentation</h2><p><strong>Authors:Jintao Tong, Ran Ma, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the model on a source-domain dataset with sufficient samples, and then transfer the model to target-domain datasets where only a few samples are available for efficient fine-tuning. There are majorly two challenges in this task: (1) the domain gap and (2) fine-tuning with scarce data. To solve these challenges, we revisit the adapter-based methods, and discover an intriguing insight not explored in previous works: the adapter not only helps the fine-tuning of downstream tasks but also naturally serves as a domain information decoupler. Then, we delve into this finding for an interpretation, and find the modelâ€™s inherent structure could lead to a natural decoupling of domain information. Building upon this insight, we propose the Domain Feature Navigator (DFN), which is a structure-based decoupler instead of loss-based ones like current works, to capture domain-specific information, thereby directing the modelâ€™s attention towards domain-agnostic knowledge. Moreover, to prevent the potential excessive overfitting of DFN during the source-domain training, we further design the SAM-SVN method to constrain DFN from learning sample-specific knowledge. On target domains, we freeze the model and fine-tune the DFN to learn target-specific knowledge specific. Extensive experiments demonstrate that our method surpasses the state-of-the-art method in CD-FSS significantly by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰æ—¨åœ¨å…ˆåœ¨æºåŸŸæ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†æœ‰è¶³å¤Ÿæ ·æœ¬ï¼Œç„¶åå°†æ¨¡å‹è½¬ç§»åˆ°ç›®æ ‡åŸŸæ•°æ®é›†ï¼Œç›®æ ‡åŸŸæ•°æ®é›†åªæœ‰å°‘é‡æ ·æœ¬å¯ç”¨äºæœ‰æ•ˆå¾®è°ƒã€‚æ­¤ä»»åŠ¡ä¸»è¦æœ‰ä¸¤ä¸ªæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰é¢†åŸŸå·®è·å’Œï¼ˆ2ï¼‰ç¨€ç¼ºæ•°æ®çš„å¾®è°ƒã€‚æˆ‘ä»¬é‡æ–°å®¡è§†äº†åŸºäºé€‚é…å™¨çš„æ–¹æ³•ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªä»¥å‰çš„ç ”ç©¶ä¸­æœªæ¢ç´¢çš„æœ‰è¶£è§è§£ï¼šé€‚é…å™¨ä¸ä»…æœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼Œè€Œä¸”è¿˜è‡ªç„¶åœ°ä½œä¸ºé¢†åŸŸä¿¡æ¯è§£è€¦å™¨ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„å†…åœ¨ç»“æ„è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå‘ç°å…¶å¯ä»¥å¯¼è‡´é¢†åŸŸä¿¡æ¯çš„è‡ªç„¶è§£è€¦ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç»“æ„çš„è§£è€¦å™¨Domain Feature Navigatorï¼ˆDFNï¼‰ï¼Œè€Œä¸æ˜¯å½“å‰å·¥ä½œä¸­çš„åŸºäºæŸå¤±çš„æ–¹æ³•ï¼Œä»¥æ•è·ç‰¹å®šäºé¢†åŸŸçš„ç‰¹å¾ä¿¡æ¯ï¼Œä»è€Œå¼•å¯¼æ¨¡å‹çš„æ³¨æ„åŠ›è½¬å‘é¢†åŸŸé€šç”¨çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†é˜²æ­¢DFNåœ¨æºåŸŸè®­ç»ƒæœŸé—´å¯èƒ½å‡ºç°çš„è¿‡åº¦æ‹Ÿåˆï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†SAM-SVNæ–¹æ³•æ¥çº¦æŸDFNå­¦ä¹ ç‰¹å®šæ ·æœ¬çš„çŸ¥è¯†ã€‚åœ¨ç›®æ ‡åŸŸä¸Šï¼Œæˆ‘ä»¬å†»ç»“æ¨¡å‹å¹¶å¾®è°ƒDFNæ¥å­¦ä¹ ç‰¹å®šäºç›®æ ‡çš„çŸ¥è¯†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨åŸŸå°æ ·æœ¬åˆ†å‰²åœºæ™¯ä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å•æ ·æœ¬å’Œäº”æ ·æœ¬åœºæ™¯ä¸­åˆ†åˆ«æé«˜äº†2.69%å’Œ4.68%çš„MIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07376v1">PDF</a> ICML 2025 Spotlight</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹æ¨¡å‹åœ¨æºåŸŸæ•°æ®é›†ä¸Šé¢„è®­ç»ƒåï¼Œè½¬ç§»åˆ°ç›®æ ‡åŸŸæ•°æ®é›†è¿›è¡Œå¾®è°ƒæ—¶é‡åˆ°çš„åŸŸå·®è·å’Œæ ·æœ¬ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºåŸºäºé€‚é…å™¨æ–¹æ³•çš„æ”¹è¿›æ–¹æ¡ˆã€‚æå‡ºäº†ä¸€ä¸ªæ–°çš„è§è§£ï¼Œå³é€‚é…å™¨ä¸ä»…æœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼Œè¿˜èƒ½è‡ªç„¶å……å½“åŸŸä¿¡æ¯è§£è€¦å™¨ã€‚åŸºäºæ­¤ï¼Œæ„å»ºäº†é¢†åŸŸç‰¹å¾å¯¼èˆªå™¨ï¼ˆDFNï¼‰æ¥æ•æ‰ç‰¹å®šé¢†åŸŸçš„åŸŸä¿¡æ¯å¹¶æŒ‡å¯¼æ¨¡å‹å­¦ä¹ é¢†åŸŸé€šç”¨çš„çŸ¥è¯†ã€‚é€šè¿‡è®¾è®¡SAM-SVNæ–¹æ³•é˜²æ­¢DFNåœ¨æºåŸŸè®­ç»ƒä¸­çš„è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶åœ¨ç›®æ ‡åŸŸä¸Šå†»ç»“æ¨¡å‹ï¼Œä»…å¾®è°ƒDFNå­¦ä¹ ç‰¹å®šç›®æ ‡çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CD-FSSä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåŸŸå·®è·å’Œæ ·æœ¬ç¨€ç¼ºã€‚</li>
<li>åŸºäºé€‚é…å™¨æ–¹æ³•çš„æ”¹è¿›æ–¹æ¡ˆè¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>é€‚é…å™¨ä¸ä»…æœ‰åŠ©äºä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒï¼ŒåŒæ—¶ä¹Ÿèƒ½è‡ªç„¶åœ°è¿›è¡ŒåŸŸä¿¡æ¯è§£è€¦ã€‚</li>
<li>é¢†åŸŸç‰¹å¾å¯¼èˆªå™¨ï¼ˆDFNï¼‰è¢«æ„å»ºä¸ºç»“æ„è§£è€¦å™¨æ¥æ•æ‰ç‰¹å®šé¢†åŸŸçš„åŸŸä¿¡æ¯ã€‚</li>
<li>DFNçš„è®¾è®¡èƒ½å¤ŸæŒ‡å¯¼æ¨¡å‹å­¦ä¹ é¢†åŸŸé€šç”¨çš„çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡SAM-SVNæ–¹æ³•é˜²æ­¢DFNåœ¨æºåŸŸè®­ç»ƒä¸­çš„è¿‡åº¦æ‹Ÿåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3b7af01d6247c2b8699182a2272f45f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5039cd02119fda2663f0c69d26d345.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c98f0093c8927dee08e7095b5300aa76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60ff1469ab95edc30ecc19f852a51806.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de79d4f6d4606499570fae042ecf2631.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e8587bb2f8aa4d6c952f3659d6ee049.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Parsing-the-Switch-LLM-Based-UD-Annotation-for-Complex-Code-Switched-and-Low-Resource-Languages"><a href="#Parsing-the-Switch-LLM-Based-UD-Annotation-for-Complex-Code-Switched-and-Low-Resource-Languages" class="headerlink" title="Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched   and Low-Resource Languages"></a>Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched   and Low-Resource Languages</h2><p><strong>Authors:Olga Kellert, Nemika Tyagi, Muhammad Imran, Nelvin Licona-Guevara, Carlos GÃ³mez-RodrÃ­guez</strong></p>
<p>Code-switching presents a complex challenge for syntactic analysis, especially in low-resource language settings where annotated data is scarce. While recent work has explored the use of large language models (LLMs) for sequence-level tagging, few approaches systematically investigate how well these models capture syntactic structure in code-switched contexts. Moreover, existing parsers trained on monolingual treebanks often fail to generalize to multilingual and mixed-language input. To address this gap, we introduce the BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal Dependencies (UD) annotations for code-switched text. First, we develop a prompt-based framework for Spanish-English and Spanish-Guaran&#39;i data, combining few-shot LLM prompting with expert review. Second, we release two annotated datasets, including the first Spanish-Guaran&#39;i UD-parsed corpus. Third, we conduct a detailed syntactic analysis of switch points across language pairs and communicative contexts. Experimental results show that BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly outperforming prior baselines and multilingual parsers. These results show that LLMs, when carefully guided, can serve as practical tools for bootstrapping syntactic resources in under-resourced, code-switched environments. Data and source code are available at <a target="_blank" rel="noopener" href="https://github.com/N3mika/ParsingProject">https://github.com/N3mika/ParsingProject</a> </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢ç»™å¥æ³•åˆ†æå¸¦æ¥äº†å¤æ‚çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„è¯­è¨€ç¯å¢ƒä¸­ï¼Œæ ‡æ³¨æ•°æ®ååˆ†ç¨€ç¼ºã€‚è™½ç„¶è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åºåˆ—çº§æ ‡ç­¾ä¸­çš„åº”ç”¨ï¼Œä½†å¾ˆå°‘æœ‰æ–¹æ³•ç³»ç»Ÿåœ°ç ”ç©¶è¿™äº›æ¨¡å‹åœ¨ä»£ç åˆ‡æ¢ç¯å¢ƒä¸­æ•æ‰å¥æ³•ç»“æ„çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºäºå•è¯­æ ‘åº“çš„è§£æå™¨å¾€å¾€æ— æ³•æ¨å¹¿åˆ°å¤šè¯­è¨€å’Œæ··åˆè¯­è¨€è¾“å…¥ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒè¯­è§£æå™¨ï¼ˆBiLingua Parserï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„æ³¨é‡Šç®¡é“ï¼Œæ—¨åœ¨ç”Ÿæˆä»£ç åˆ‡æ¢æ–‡æœ¬çš„é€šç”¨ä¾èµ–ï¼ˆUDï¼‰æ³¨é‡Šã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºè¥¿ç­ç‰™è¯­-è‹±è¯­å’Œè¥¿ç­ç‰™è¯­-ç“œæ‹‰å°¼è¯­æ•°æ®å¼€å‘äº†ä¸€ä¸ªåŸºäºæç¤ºçš„æ¡†æ¶ï¼Œå°†å‡ æ¬¡LLMæç¤ºä¸ä¸“å®¶è¯„å®¡ç›¸ç»“åˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªæ ‡æ³¨æ•°æ®é›†ï¼ŒåŒ…æ‹¬é¦–ä¸ªè¥¿ç­ç‰™è¯­-ç“œæ‹‰å°¼è¯­UDè§£æè¯­æ–™åº“ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å¯¹è·¨è¯­è¨€å¯¹å’Œäº¤é™…ç¯å¢ƒçš„åˆ‡æ¢ç‚¹è¿›è¡Œäº†è¯¦ç»†çš„å¥æ³•åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸“å®¶è¯„å®¡åï¼ŒåŒè¯­è§£æå™¨è¾¾åˆ°äº†é«˜è¾¾95.29%çš„LASï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡å’ŒFå€¼çš„å¹³å‡å€¼ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºå‡†æ¨¡å‹å’Œè·¨è¯­è¨€è§£æå™¨ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“å—åˆ°è°¨æ…å¼•å¯¼æ—¶ï¼ŒLLMå¯ä»¥ä½œä¸ºåœ¨èµ„æºä¸è¶³ã€ä»£ç åˆ‡æ¢ç¯å¢ƒä¸­å¯åŠ¨å¥æ³•èµ„æºçš„å®ç”¨å·¥å…·ã€‚æ•°æ®å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/N3mika/ParsingProject%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/N3mika/ParsingProjectä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07274v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä»£ç åˆ‡æ¢å¯¹å¥æ³•åˆ†æå¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯èµ„æºåŒ®ä¹çš„è¯­è¨€ç¯å¢ƒä¸‹æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†BiLingua Parserã€‚å®ƒæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ³¨é‡Šç®¡é“ï¼Œç”¨äºç”Ÿæˆä»£ç åˆ‡æ¢æ–‡æœ¬çš„é€šç”¨ä¾èµ–å…³ç³»æ³¨é‡Šã€‚é€šè¿‡ç»“åˆå°‘æ ·æœ¬LLMæç¤ºå’Œä¸“å®¶è¯„å®¡ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºæç¤ºçš„æ¡†æ¶ï¼Œç”¨äºè¥¿ç­ç‰™è¯­-è‹±è¯­å’Œè¥¿ç­ç‰™è¯­-ç“œæ‹‰å°¼è¯­æ•°æ®ã€‚æœ¬æ–‡è¿˜å‘å¸ƒäº†ä¸¤ä¸ªæ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…æ‹¬é¦–ä¸ªè¥¿ç­ç‰™è¯­-ç“œæ‹‰å°¼è¯­UDè§£æè¯­æ–™åº“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸“å®¶è¯„å®¡åï¼ŒBiLingua Parserçš„LASè¾¾åˆ°äº†95.29%ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºå‡†æµ‹è¯•å’Œå¤šè¯­è¨€è§£æå™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢å¯¹å¥æ³•åˆ†ææ„æˆå¤æ‚æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„è¯­è¨€ç¯å¢ƒä¸­ã€‚</li>
<li>BiLingua Parseræ˜¯é¦–ä¸ªé’ˆå¯¹ä»£ç åˆ‡æ¢æ–‡æœ¬çš„é€šç”¨ä¾èµ–å…³ç³»æ³¨é‡Šçš„å¤§å‹è¯­è¨€æ¨¡å‹æ³¨é‡Šç®¡é“ã€‚</li>
<li>é€šè¿‡ç»“åˆå°‘æ ·æœ¬LLMæç¤ºå’Œä¸“å®¶è¯„å®¡ï¼Œä¸ºè¥¿ç­ç‰™è¯­-è‹±è¯­å’Œè¥¿ç­ç‰™è¯­-ç“œæ‹‰å°¼è¯­æ•°æ®å¼€å‘äº†ä¸€ç§åŸºäºæç¤ºçš„æ¡†æ¶ã€‚</li>
<li>å‘å¸ƒäº†ä¸¤ä¸ªæ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…æ‹¬é¦–ä¸ªè¥¿ç­ç‰™è¯­-ç“œæ‹‰å°¼è¯­UDè§£æè¯­æ–™åº“ã€‚</li>
<li>BiLingua Parserçš„LASåœ¨ä¸“å®¶è¯„å®¡åè¾¾åˆ°äº†95.29%ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŸºå‡†æµ‹è¯•å’Œå¤šè¯­è¨€è§£æå™¨ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»”ç»†æŒ‡å¯¼ä¸‹å¯ä»¥åœ¨èµ„æºæœ‰é™ã€ä»£ç åˆ‡æ¢çš„ç¯å¢ƒä¸­ä½œä¸ºå®ç”¨å·¥å…·æ¥å¼•å¯¼å¥æ³•èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07274">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-057001259b841ca6d12b2c27c2bbedb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c053a637890bcc8ef152e550a4fb09e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b406c735f54f28aa6d9bd16c670ab60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e4a90ca98bde005127cc34eac70f5ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55c0d078dbfa7614bf5571a835b8a501.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62e5b47e5da36ee789edcc93a0ad77e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd393e64bc2ca4d22e59454204fe7e28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6c714a7a0f19081d87070577107143.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Distillation-from-Speech-and-Music-Representation-Models"><a href="#Multi-Distillation-from-Speech-and-Music-Representation-Models" class="headerlink" title="Multi-Distillation from Speech and Music Representation Models"></a>Multi-Distillation from Speech and Music Representation Models</h2><p><strong>Authors:Jui-Chiang Wei, Yi-Cheng Lin, Fabian Ritter-Gutierrez, Hung-yi Lee</strong></p>
<p>Real-world audio often mixes speech and music, yet models typically handle only one domain. This paper introduces a multi-teacher distillation framework that unifies speech and music models into a single one while significantly reducing model size. Our approach leverages the strengths of domain-specific teacher models, such as HuBERT for speech and MERT for music, and explores various strategies to balance both domains. Experiments across diverse tasks demonstrate that our model matches the performance of domain-specific models, showing the effectiveness of cross-domain distillation. Additionally, we conduct few-shot learning experiments, highlighting the need for general models in real-world scenarios where labeled data is limited. Our results show that our model not only performs on par with specialized models but also outperforms them in few-shot scenarios, proving that a cross-domain approach is essential and effective for diverse tasks with limited data. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„éŸ³é¢‘ç»å¸¸æ··åˆäº†è¯­éŸ³å’ŒéŸ³ä¹ï¼Œä½†æ¨¡å‹é€šå¸¸åªå¤„ç†ä¸€ä¸ªé¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è¯­éŸ³å’ŒéŸ³ä¹æ¨¡å‹ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œå¹¶æ˜¾è‘—å‡å°äº†æ¨¡å‹å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•™å¸ˆæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¦‚ç”¨äºè¯­éŸ³çš„HuBERTå’Œç”¨äºéŸ³ä¹çš„MERTï¼Œå¹¶æ¢ç´¢äº†å„ç§ç­–ç•¥æ¥å¹³è¡¡è¿™ä¸¤ä¸ªé¢†åŸŸã€‚åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†ç‰¹å®šé¢†åŸŸæ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†è·¨åŸŸè’¸é¦çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å°æ ·æœ¬å­¦ä¹ å®éªŒï¼Œå¼ºè°ƒåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­éœ€è¦é€šç”¨æ¨¡å‹ï¼Œå› ä¸ºæ ‡æ³¨æ•°æ®æœ‰é™ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†ä¸“ä¸šæ¨¡å‹çš„æ°´å¹³ï¼Œè€Œä¸”åœ¨å°æ ·ä¾‹åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ï¼Œè¯æ˜äº†è·¨åŸŸæ–¹æ³•å¯¹äºæœ‰é™æ•°æ®çš„å¤šæ ·ä»»åŠ¡çš„é‡è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07237v1">PDF</a> 8 pages, 1 figures</p>
<p><strong>Summary</strong><br>éŸ³é¢‘é¢†åŸŸä¸­ï¼Œç°å®ä¸­å¸¸å¸¸åŒæ—¶åŒ…å«è¯­éŸ³å’ŒéŸ³ä¹ï¼Œä½†ç°æœ‰æ¨¡å‹é€šå¸¸åªé’ˆå¯¹å•ä¸€é¢†åŸŸè¿›è¡Œå¤„ç†ã€‚æœ¬æ–‡æå‡ºä¸€ç§å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œå°†è¯­éŸ³å’ŒéŸ³ä¹æ¨¡å‹ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹è§„æ¨¡ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å¦‚HuBERTé’ˆå¯¹è¯­éŸ³çš„ç‰¹å®šé¢†åŸŸæ•™å¸ˆæ¨¡å‹å’Œé’ˆå¯¹éŸ³ä¹çš„MERTç­‰ç‰¹å®šé¢†åŸŸçš„æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶æ¢ç´¢äº†å¤šç§ç­–ç•¥ä»¥å¹³è¡¡è¯­éŸ³å’ŒéŸ³ä¹çš„å¤„ç†æ•ˆæœã€‚å®éªŒç»“æœå±•ç¤ºäº†å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ä¸‹ç»Ÿä¸€æ¨¡å‹å¤„ç†è·¨é¢†åŸŸçš„æ€§èƒ½å’Œä¼˜ç‚¹ï¼ŒéªŒè¯äº†å…¶å¯¹å¤šæ ·åŒ–çš„ç°å®åº”ç”¨ä»»åŠ¡çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¿›è¡Œäº†å°æ ·æœ¬å­¦ä¹ å®éªŒï¼Œè¯æ˜äº†åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šç”¨æ¨¡å‹çš„é‡è¦æ€§åŠå…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œç»Ÿä¸€å¤„ç†è¯­éŸ³å’ŒéŸ³ä¹é¢†åŸŸï¼Œæ˜¾è‘—å‡å°æ¨¡å‹è§„æ¨¡ã€‚</li>
<li>åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•™å¸ˆæ¨¡å‹ï¼Œå¦‚é’ˆå¯¹è¯­éŸ³çš„HuBERTå’Œé’ˆå¯¹éŸ³ä¹çš„MERTã€‚</li>
<li>æ¢ç´¢äº†å¤šç§ç­–ç•¥å¹³è¡¡è¯­éŸ³å’ŒéŸ³ä¹çš„å¤„ç†æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶ä¸‹çš„ç»Ÿä¸€æ¨¡å‹æ€§èƒ½ä¸ç‰¹å®šé¢†åŸŸæ¨¡å‹ç›¸å½“ã€‚</li>
<li>åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºä¼˜äºç‰¹å®šé¢†åŸŸæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>éªŒè¯äº†ç»Ÿä¸€æ¨¡å‹çš„é€‚åº”æ€§å’Œå¤šæ ·æ€§ä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-edc249f67c6e74c8571af5c776649f11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ddb37f7c54f40fd5698c1156db8c15f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf99e13d17b5b58b486e00209dde03a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-543992c7a6137dab54c8d60aba78096c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc50ffc3bccd04bd39d56d6a25360b39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9a879b6d376dc913c5a167d7f62742.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ab31b4609537f22bd290559c0a0c8c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-624f4672327fe8a4e8dd919ea52c1e18.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Layered-Self-Supervised-Knowledge-Distillation-Framework-for-Efficient-Multimodal-Learning-on-the-Edge"><a href="#A-Layered-Self-Supervised-Knowledge-Distillation-Framework-for-Efficient-Multimodal-Learning-on-the-Edge" class="headerlink" title="A Layered Self-Supervised Knowledge Distillation Framework for Efficient   Multimodal Learning on the Edge"></a>A Layered Self-Supervised Knowledge Distillation Framework for Efficient   Multimodal Learning on the Edge</h2><p><strong>Authors:Tarique Dahri, Zulfiqar Ali Memon, Zhenyu Yu, Mohd. Yamani Idna Idris, Sheheryar Khan, Sadiq Ahmad, Maged Shoman, Saddam Aziz, Rizwan Qureshi</strong></p>
<p>We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework for training compact deep learning models. Unlike traditional methods that rely on pre-trained teacher networks, our approach appends auxiliary classifiers to intermediate feature maps, generating diverse self-supervised knowledge and enabling one-to-one transfer across different network stages. Our method achieves an average improvement of 4.54% over the state-of-the-art PS-KD method and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on ImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under few-shot learning scenarios also achieve state-of-the-art results. These findings demonstrate the effectiveness of our approach in enhancing model generalization and performance without the need for large over-parameterized teacher networks. Importantly, at the inference stage, all auxiliary classifiers can be removed, yielding no extra computational cost. This makes our model suitable for deploying small language models on affordable low-computing devices. Owing to its lightweight design and adaptability, our framework is particularly suitable for multimodal sensing and cyber-physical environments that require efficient and responsive inference. LSSKD facilitates the development of intelligent agents capable of learning from limited sensory data under weak supervision. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†åˆ†å±‚è‡ªç›‘ç£çŸ¥è¯†è’¸é¦ï¼ˆLSSKDï¼‰æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒç´§å‡‘çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä¸åŒäºä¼ ç»Ÿä¾èµ–é¢„è®­ç»ƒæ•™å¸ˆç½‘ç»œçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è¾…åŠ©åˆ†ç±»å™¨é™„åŠ åˆ°ä¸­é—´ç‰¹å¾å›¾ä¸Šï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„è‡ªç›‘ç£çŸ¥è¯†ï¼Œå®ç°ä¸åŒç½‘ç»œé˜¶æ®µçš„ä¸€å¯¹ä¸€çŸ¥è¯†è¿ç§»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨CIFAR-100æ•°æ®é›†ä¸Šè¾ƒæœ€æ–°çš„PS-KDæ–¹æ³•å¹³å‡æé«˜äº†4.54%ï¼Œè¾ƒSSKDæé«˜äº†1.14%ï¼Œåœ¨ImageNetä¸Šè¾ƒHASSKDæé«˜äº†0.32%ã€‚åœ¨Tiny ImageNetå’ŒCIFAR-100ä¸Šçš„å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯å®éªŒä¹Ÿè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–å¤§å‹è¶…å‚æ•°æ•™å¸ˆç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹çš„æ³›åŒ–å’Œæ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œåœ¨æ¨ç†é˜¶æ®µï¼Œæ‰€æœ‰è¾…åŠ©åˆ†ç±»å™¨éƒ½å¯ä»¥è¢«ç§»é™¤ï¼Œä¸ä¼šå¸¦æ¥é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹é€‚åˆåœ¨è´Ÿæ‹…å¾—èµ·çš„ä½è®¡ç®—è®¾å¤‡ä¸Šéƒ¨ç½²å°å‹è¯­è¨€æ¨¡å‹ã€‚ç”±äºå…¶è½»é‡çº§çš„è®¾è®¡å’Œé€‚åº”æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºéœ€è¦é«˜æ•ˆå’Œå“åº”è¿…é€Ÿçš„å¤šæ¨¡æ€æ„ŸçŸ¥å’Œç½‘ç»œç‰©ç†ç¯å¢ƒã€‚LSSKDä¿ƒè¿›äº†æ™ºèƒ½ä»£ç†çš„å‘å±•ï¼Œä½¿å…¶åœ¨å¼±ç›‘ç£ä¸‹èƒ½ä»æœ‰é™çš„æ„Ÿå®˜æ•°æ®ä¸­å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåˆ†å±‚è‡ªç›‘ç£çŸ¥è¯†è’¸é¦ï¼ˆLSSKDï¼‰æ¡†æ¶è®­ç»ƒç´§å‡‘æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•è¢«æå‡ºã€‚è¯¥æ–¹æ³•é€šè¿‡é™„åŠ è¾…åŠ©åˆ†ç±»å™¨åˆ°ä¸­é—´ç‰¹å¾æ˜ å°„ï¼Œç”Ÿæˆå¤šæ ·çš„è‡ªç›‘ç£çŸ¥è¯†ï¼Œå®ç°äº†åœ¨ä¸åŒç½‘ç»œé˜¶æ®µä¸€å¯¹ä¸€çš„çŸ¥è¯†è½¬ç§»ï¼Œæé«˜äº†æ¨¡å‹åœ¨CIFAR-100å’ŒImageNetæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¾…åŠ©åˆ†ç±»å™¨å¯ä»¥è¢«ç§»é™¤ï¼Œä¸ä¼šå¢åŠ é¢å¤–çš„è®¡ç®—æˆæœ¬ï¼Œé€‚åˆåœ¨ä½è®¡ç®—è®¾å¤‡ä¸Šéƒ¨ç½²å°å‹è¯­è¨€æ¨¡å‹ã€‚LSSKDæ¡†æ¶é€‚ç”¨äºå¤šæ¨¡æ€æ„ŸçŸ¥å’Œç‰©ç†ç¯å¢ƒï¼Œå°¤å…¶é€‚åˆéœ€è¦é«˜æ•ˆå“åº”æ¨ç†çš„åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LSSKDæ¡†æ¶ç”¨äºè®­ç»ƒç´§å‡‘çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>é€šè¿‡é™„åŠ è¾…åŠ©åˆ†ç±»å™¨åˆ°ä¸­é—´ç‰¹å¾æ˜ å°„å®ç°è‡ªç›‘ç£çŸ¥è¯†è’¸é¦ã€‚</li>
<li>åœ¨CIFAR-100å’ŒImageNetæ•°æ®é›†ä¸Šå®ç°äº†å¯¹æœ€æ–°æŠ€æœ¯çš„æ”¹è¿›ã€‚</li>
<li>åœ¨å°‘é•œå¤´å­¦ä¹ åœºæ™¯ä¸‹ä¹Ÿå–å¾—äº†æœ€ä½³ç»“æœã€‚</li>
<li>LSSKDæé«˜äº†æ¨¡å‹çš„æ³›åŒ–å’Œæ€§èƒ½ï¼Œæ— éœ€å¤§å‹è¶…å‚æ•°åŒ–æ•™å¸ˆç½‘ç»œã€‚</li>
<li>æ¨ç†é˜¶æ®µæ— é¢å¤–è®¡ç®—æˆæœ¬ï¼Œé€‚åˆåœ¨ä½è®¡ç®—è®¾å¤‡ä¸Šéƒ¨ç½²å°å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>LSSKDæ¡†æ¶é€‚ç”¨äºå¤šæ¨¡æ€æ„ŸçŸ¥å’Œç‰©ç†ç¯å¢ƒï¼Œå°¤å…¶é€‚åˆéœ€è¦é«˜æ•ˆå“åº”çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-690647c34864f98babd9c65b07e7cd8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e83e6531827acdd521e616217f0d0664.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0229526c5ec8a364f7f0d8229302fec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0553fa262a93fa3fab6bc2cb7ab76808.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42df37a89a1577eca19efd214b1106e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec75ce02c59ff48bc789066d1b47f1c5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Meta-Adaptive-Prompt-Distillation-for-Few-Shot-Visual-Question-Answering"><a href="#Meta-Adaptive-Prompt-Distillation-for-Few-Shot-Visual-Question-Answering" class="headerlink" title="Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering"></a>Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</h2><p><strong>Authors:Akash Gupta, Amos Storkey, Mirella Lapata</strong></p>
<p>Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰é€šå¸¸ä¾èµ–äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä»¥åœ¨æå°‘çš„ç›‘ç£æƒ…å†µä¸‹æ‰§è¡Œæ–°ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒICLçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°çš„LMMä¸­ï¼Œè¡¨ç°å¹¶ä¸ç¨³å®šï¼Œå¹¶ä¸”å¹¶ä¸æ€»æ˜¯éšç€ç¤ºä¾‹æ•°é‡çš„å¢åŠ è€Œå•è°ƒæå‡ã€‚æˆ‘ä»¬å‡è®¾è¿™æ˜¯å› ä¸ºLMMè¢«å›¾åƒåµŒå…¥ä¸­å­˜åœ¨çš„é¢å¤–ä¿¡æ¯æ‰€æ·¹æ²¡ï¼Œè€Œè¿™äº›ä¿¡æ¯å¯¹äºä¸‹æ¸¸ä»»åŠ¡æ¥è¯´æ˜¯ä¸å¿…è¦çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ƒå­¦ä¹ æ–¹æ³•ï¼Œä¸ºåœ¨LMMä¸­åŸ¹å…»å°‘é‡æ ·æœ¬èƒ½åŠ›æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡ä½¿ç”¨ä¸€å¥—å›ºå®šçš„è½¯æç¤ºï¼Œè¿™äº›æç¤ºæ˜¯ä»ä»»åŠ¡ç›¸å…³çš„å›¾åƒç‰¹å¾ä¸­æç‚¼å‡ºæ¥çš„ï¼Œå¹¶ä¸”å¯ä»¥åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å°‘é‡ç¤ºä¾‹è¿›è¡Œé€‚åº”ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ç§æç‚¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ³¨æ„åŠ›æ˜ å°„æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è½»æ¾åœ°ä¸æµè¡Œçš„LLaVA v1.5æ¶æ„é›†æˆï¼Œå¹¶ä¸è½¯æç¤ºè”åˆå­¦ä¹ ï¼Œåªéœ€å‡ æ­¥æ¢¯åº¦æ­¥éª¤ï¼Œä¾¿èƒ½åœ¨ä½æ•°æ®çŠ¶æ€ä¸‹é€‚åº”LMMçš„ä»»åŠ¡ã€‚åœ¨VL-ICL Benchä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºICLå’Œç›¸å…³æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œå³ä½¿åœ¨å›¾åƒæ‰°åŠ¨ä¸‹ä¹Ÿèƒ½æé«˜è§†è§‰é—®ç­”ä»»åŠ¡çš„ä»»åŠ¡å½’çº³å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¾èµ–ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è¿›è¡Œå°‘é‡ç›‘ç£çš„æ–°ä»»åŠ¡ã€‚ä½†ICLæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°çš„LMMsä¸­ï¼Œè¡¨ç°ä¸ç¨³å®šï¼Œå¹¶ééšä¾‹å­å¢åŠ è€Œå•è°ƒæå‡ã€‚æˆ‘ä»¬å‡è®¾è¿™æ˜¯å› ä¸ºLMMè¢«å›¾åƒåµŒå…¥ä¸­çš„é¢å¤–ä¿¡æ¯æ‰€æ·¹æ²¡ï¼Œè€Œè¿™äº›ä¿¡æ¯å¹¶ä¸ä¸ºä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ƒå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ä»ä»»åŠ¡ç›¸å…³å›¾åƒç‰¹å¾ä¸­æç‚¼å‡ºå›ºå®šçš„ä¸€ç»„è½¯æç¤ºæ¥å®ç°LMMsçš„å°‘é‡èƒ½åŠ›ã€‚è¿™äº›è½¯æç¤ºå¯åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å°‘é‡ä¾‹å­è¿›è¡Œé€‚åº”ã€‚ä¸ºä¾¿äºæç‚¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ³¨æ„åŠ›æ˜ å°„æ¨¡å—ï¼Œå¯è½»æ¾é›†æˆåˆ°æµè¡Œçš„LLaVA v1.5æ¶æ„ä¸­ï¼Œå¹¶ä¸è½¯æç¤ºè”åˆå­¦ä¹ ï¼Œåœ¨åªæœ‰å°‘æ•°æ¢¯åº¦æ­¥éª¤çš„æƒ…å†µä¸‹å®ç°LMMsçš„ä»»åŠ¡é€‚åº”æ€§ã€‚åœ¨VL-ICL Benchä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºICLå’Œç›¸å…³æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œå³ä½¿åœ¨å›¾åƒæ‰°åŠ¨ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMsåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­é¢ä¸´æ€§èƒ½ä¸ç¨³å®šé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°å‹æ¨¡å‹ä¸­ã€‚</li>
<li>é¢å¤–ä¿¡æ¯åœ¨å›¾åƒåµŒå…¥ä¸­å¯èƒ½æ·¹æ²¡LMMsï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æå‡ºä¸€ç§å…ƒå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æç‚¼ä»»åŠ¡ç›¸å…³å›¾åƒç‰¹å¾ä¸­çš„è½¯æç¤ºæ¥å¢å¼ºLMMsçš„å°‘é‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>è½¯æç¤ºå¯åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å°‘é‡ä¾‹å­è¿›è¡Œé€‚åº”ã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›æ˜ å°„æ¨¡å—ä»¥ç®€åŒ–é›†æˆåˆ°æµè¡Œçš„LLaVA v1.5æ¶æ„ä¸­ã€‚</li>
<li>å…ƒå­¦ä¹ æ–¹æ³•å’Œæ³¨æ„åŠ›æ˜ å°„æ¨¡å—è”åˆå­¦ä¹ ï¼Œæé«˜ä»»åŠ¡é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f89150e1506b94b2747fd9537302af55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0593c1bbdd98a5b3a74bb630e7c81079.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d825c926c830df2ef7acb01e27bafe2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Quantile-Regression-with-Large-Language-Models-for-Price-Prediction"><a href="#Quantile-Regression-with-Large-Language-Models-for-Price-Prediction" class="headerlink" title="Quantile Regression with Large Language Models for Price Prediction"></a>Quantile Regression with Large Language Models for Price Prediction</h2><p><strong>Authors:Nikhita Vedula, Dushyanta Dhyani, Laleh Jalali, Boris Oreshkin, Mohsen Bayati, Shervin Malmasi</strong></p>
<p>Large Language Models (LLMs) have shown promise in structured prediction tasks, including regression, but existing approaches primarily focus on point estimates and lack systematic comparison across different methods. We investigate probabilistic regression using LLMs for unstructured inputs, addressing challenging text-to-distribution prediction tasks such as price estimation where both nuanced text understanding and uncertainty quantification are critical. We propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. Through extensive experiments across three diverse price prediction datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations, as measured by three established metrics each for prediction accuracy and distributional calibration. Our systematic comparison of LLM approaches, model architectures, training approaches, and data scaling reveals that Mistral-7B consistently outperforms encoder architectures, embedding-based methods, and few-shot learning methods. Our experiments also reveal the effectiveness of LLM-assisted label correction in achieving human-level accuracy without systematic bias. Our curated datasets are made available at <a target="_blank" rel="noopener" href="https://github.com/vnik18/llm-price-quantile-reg/">https://github.com/vnik18/llm-price-quantile-reg/</a> to support future research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»“æ„é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬å›å½’ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç‚¹ä¼°è®¡ä¸Šï¼Œç¼ºä¹ä¸åŒæ–¹æ³•ä¹‹é—´çš„ç³»ç»Ÿæ¯”è¾ƒã€‚æˆ‘ä»¬è°ƒæŸ¥äº†ä½¿ç”¨LLMè¿›è¡Œæ¦‚ç‡å›å½’ä»¥å¤„ç†éç»“æ„åŒ–è¾“å…¥ï¼Œè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–‡æœ¬åˆ°åˆ†å¸ƒé¢„æµ‹ä»»åŠ¡ï¼Œå¦‚ä»·æ ¼é¢„ä¼°ï¼Œå…¶ä¸­å¾®å¦™çš„æ–‡æœ¬ç†è§£å’Œä¸ç¡®å®šæ€§é‡åŒ–éƒ½æ˜¯å…³é”®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é‡åŒ–å›å½’æ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿäº§ç”Ÿå®Œæ•´çš„é¢„æµ‹åˆ†å¸ƒï¼Œæ”¹è¿›äº†ä¼ ç»Ÿçš„ç‚¹ä¼°è®¡ã€‚é€šè¿‡ä¸‰ä¸ªä¸åŒä»·æ ¼é¢„æµ‹æ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç»è¿‡é‡åŒ–å¤´å¾®è°ƒè¿‡çš„Mistral-7Bæ¨¡å‹åœ¨ç‚¹é¢„ä¼°å’Œåˆ†å¸ƒé¢„ä¼°æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªé’ˆå¯¹é¢„æµ‹ç²¾åº¦å’Œåˆ†å¸ƒæ ¡å‡†çš„æ—¢å®šæŒ‡æ ‡æ¥è¡¡é‡ã€‚æˆ‘ä»¬å¯¹LLMæ–¹æ³•ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®è§„æ¨¡çš„ç³»ç»Ÿæ¯”è¾ƒè¡¨æ˜ï¼ŒMistral-7Bå§‹ç»ˆä¼˜äºç¼–ç å™¨æ¶æ„ã€åŸºäºåµŒå…¥çš„æ–¹æ³•å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜æ­ç¤ºäº†LLMè¾…åŠ©æ ‡ç­¾æ ¡æ­£çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ— ç³»ç»Ÿåè§çš„æƒ…å†µä¸‹å®ç°äº†äººç±»çº§åˆ«çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç²¾é€‰æ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/vnik18/llm-price-quantile-reg/%E8%8E%B7%E5%BE%97%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/vnik18/llm-price-quantile-reg/è·å¾—ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06657v1">PDF</a> Accepted to Findings of ACL, 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–é¢„æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼ŒåŒ…æ‹¬å›å½’ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç‚¹ä¼°è®¡ï¼Œç¼ºä¹ä¸åŒæ–¹æ³•ä¹‹é—´çš„ç³»ç»Ÿæ¯”è¾ƒã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨LLMsè¿›è¡Œæ¦‚ç‡å›å½’ï¼Œé’ˆå¯¹æ–‡æœ¬åˆ°åˆ†å¸ƒé¢„æµ‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œå¦‚ä»·æ ¼é¢„ä¼°ç­‰æ—¢éœ€è¦ç²¾ç»†æ–‡æœ¬ç†è§£åˆéœ€è¦ä¸ç¡®å®šæ€§é‡åŒ–çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›å½’åˆ†ä½æ•°æ–¹æ³•ï¼Œä½¿LLMsèƒ½å¤Ÿäº§ç”Ÿå®Œæ•´çš„é¢„æµ‹åˆ†å¸ƒï¼Œæ”¹è¿›äº†ä¼ ç»Ÿçš„ç‚¹ä¼°è®¡ã€‚é€šè¿‡ä¸‰ä¸ªä¸åŒçš„ä»·æ ¼é¢„æµ‹æ•°æ®é›†çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ç»è¿‡åˆ†ä½æ•°å¤´å¾®è°ƒè¿‡çš„Mistral-7Bæ¨¡å‹åœ¨ç‚¹ä¼°è®¡å’Œåˆ†å¸ƒä¼°è®¡æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œé€šè¿‡ä¸‰ç§è¯„ä¼°é¢„æµ‹ç²¾åº¦å’Œåˆ†å¸ƒæ ¡å‡†çš„åº¦é‡æŒ‡æ ‡è¿›è¡Œè¡¡é‡ã€‚æˆ‘ä»¬å¯¹LLMæ–¹æ³•ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œæ•°æ®è§„æ¨¡è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒï¼Œå‘ç°Mistral-7Båœ¨ç¼–ç å™¨æ¶æ„ã€åŸºäºåµŒå…¥çš„æ–¹æ³•å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜æ­ç¤ºäº†LLMè¾…åŠ©æ ‡ç­¾æ ¡æ­£åœ¨å®ç°æ— ç³»ç»Ÿæ€§åå·®çš„äººç±»æ°´å¹³ç²¾åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vnik18/llm-price-quantile-reg/">https://github.com/vnik18/llm-price-quantile-reg/</a>ä¸Šè·å–ï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsåœ¨ç»“æ„åŒ–é¢„æµ‹ä»»åŠ¡ä¸­æœ‰æ½œåŠ›ï¼ŒåŒ…æ‹¬å›å½’ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç‚¹ä¼°è®¡ï¼Œç¼ºä¹ç³»ç»Ÿæ¯”è¾ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›å½’åˆ†ä½æ•°æ–¹æ³•ï¼Œä½¿LLMsèƒ½å¤Ÿäº§ç”Ÿå®Œæ•´çš„é¢„æµ‹åˆ†å¸ƒã€‚</li>
<li>Mistral-7Bæ¨¡å‹åœ¨ç‚¹ä¼°è®¡å’Œåˆ†å¸ƒä¼°è®¡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>Mistral-7Båœ¨å¤šç§æ¶æ„å’Œæ–¹æ³•ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>LLMè¾…åŠ©æ ‡ç­¾æ ¡æ­£æœ‰åŠ©äºå®ç°æ— ç³»ç»Ÿæ€§åå·®çš„äººç±»æ°´å¹³ç²¾åº¦ã€‚</li>
<li>æä¾›äº†æ•°æ®é›†ä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90e6374b2b3fd5a8a11e1bf52aa62c6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf9597d1739093a721d4256173fe695.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5a1bc6c8cede9b9084093562b50946a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c87f50a94310b626b6e91a55c154ffcf.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SafeLawBench-Towards-Safe-Alignment-of-Large-Language-Models"><a href="#SafeLawBench-Towards-Safe-Alignment-of-Large-Language-Models" class="headerlink" title="SafeLawBench: Towards Safe Alignment of Large Language Models"></a>SafeLawBench: Towards Safe Alignment of Large Language Models</h2><p><strong>Authors:Chuxue Cao, Han Zhu, Jiaming Ji, Qichao Sun, Zhenghao Zhu, Yinyu Wu, Juntao Dai, Yaodong Yang, Sirui Han, Yike Guo</strong></p>
<p>With the growing prevalence of large language models (LLMs), the safety of LLMs has raised significant concerns. However, there is still a lack of definitive standards for evaluating their safety due to the subjective nature of current safety benchmarks. To address this gap, we conducted the first exploration of LLMsâ€™ safety evaluation from a legal perspective by proposing the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three levels based on legal standards, providing a systematic and comprehensive framework for evaluation. It comprises 24,860 multi-choice questions and 1,106 open-domain question-answering (QA) tasks. Our evaluation included 2 closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot prompting, highlighting the safety features of each model. We also evaluated the LLMsâ€™ safety-related reasoning stability and refusal behavior. Additionally, we found that a majority voting mechanism can enhance model performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench, while the average accuracy of 20 LLMs remains at 68.8%. We urge the community to prioritize research on the safety of LLMs. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¥ç›Šæ™®åŠï¼ŒLLMçš„å®‰å…¨é—®é¢˜å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºå½“å‰å®‰å…¨åŸºå‡†çš„ä¸»è§‚æ€§ï¼Œå¯¹äºè¯„ä¼°LLMçš„å®‰å…¨æ€§çš„æ˜ç¡®æ ‡å‡†ä»ç„¶ç¼ºä¹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ä»æ³•å¾‹è§’åº¦å¯¹LLMçš„å®‰å…¨è¯„ä¼°è¿›è¡Œäº†é¦–æ¬¡æ¢ç´¢ï¼Œå¹¶æå‡ºäº†SafeLawBenchåŸºå‡†æµ‹è¯•ã€‚SafeLawBenchæ ¹æ®æ³•å¾‹æ ‡å‡†å°†å®‰å…¨é£é™©åˆ†ä¸ºä¸‰ä¸ªçº§åˆ«ï¼Œä¸ºè¯„ä¼°æä¾›äº†ä¸€ä¸ªç³»ç»Ÿå’Œå…¨é¢çš„æ¡†æ¶ã€‚å®ƒåŒ…å«äº†24860ä¸ªé€‰æ‹©é¢˜å’Œ1106ä¸ªå¼€æ”¾åŸŸé—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬2ä¸ªå°é—­æºLLMå’Œ18ä¸ªå¼€æºLLMï¼Œé‡‡ç”¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œçªå‡ºäº†æ¯ä¸ªæ¨¡å‹çš„å®‰å…¨ç‰¹æ€§ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†LLMçš„å®‰å…¨ç›¸å…³æ¨ç†ç¨³å®šæ€§åŠæ‹’ç»è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å¤šæ•°æŠ•ç¥¨æœºåˆ¶å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨SafeLawBenchçš„å¤šé¡¹é€‰æ‹©é¢˜ä¸­ï¼Œé¢†å…ˆçš„SOTAæ¨¡å‹å¦‚Claude-3.5-Sonnetå’ŒGPT-4oçš„å‡†ç¡®ç‡ä¹Ÿæœªè¶…è¿‡80.5%ï¼Œè€Œ20ä¸ªLLMçš„å¹³å‡å‡†ç¡®ç‡ä»…ä¸º68.8%ã€‚æˆ‘ä»¬æ•¦ä¿ƒç¤¾åŒºä¼˜å…ˆç ”ç©¶LLMçš„å®‰å…¨é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06636v1">PDF</a> Accepted to ACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†ç›®å‰ç¼ºä¹æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†ã€‚æœ¬ç ”ç©¶ä»æ³•å¾‹è§†è§’å‡ºå‘ï¼Œæå‡ºäº†SafeLawBenchè¯„ä¼°å·¥å…·ï¼Œå°†LLMsçš„å®‰å…¨é£é™©åˆ†ä¸ºä¸‰çº§ã€‚ç ”ç©¶åŒ…æ‹¬å¤šä¸ªé€‰æ‹©é¢˜å’Œå¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ï¼Œå¯¹LLMsè¿›è¡Œå®‰å…¨è¯„ä¼°ï¼Œå‘ç°å¤šæ•°æŠ•ç¥¨æœºåˆ¶èƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨SafeLawBenchä¸Šçš„å‡†ç¡®ç‡ä»æœ‰å¾…æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®‰å…¨é—®é¢˜äºŸå¾…å…³æ³¨ï¼Œç¼ºä¹ç»Ÿä¸€è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>SafeLawBenchè¯„ä¼°å·¥å…·ä»æ³•å¾‹è§†è§’å‡ºå‘ï¼Œå¯¹LLMsçš„å®‰å…¨é£é™©è¿›è¡Œç³»ç»ŸåŒ–ã€å…¨é¢åŒ–çš„è¯„ä»·ã€‚</li>
<li>SafeLawBenchåŒ…æ‹¬å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ï¼Œç”¨ä»¥è¯„ä¼°LLMsçš„å®‰å…¨æ€§ã€‚</li>
<li>å¤šæ•°æŠ•ç¥¨æœºåˆ¶æœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨SafeLawBenchä¸Šçš„å‡†ç¡®ç‡ä»æœ‰æé«˜ç©ºé—´ï¼Œæœ€é«˜å‡†ç¡®ç‡ä¸è¶…è¿‡80.5%ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠä¸¤ç§ç±»å‹çš„LLMsï¼šå°é—­æºä»£ç å’Œå¼€æ”¾æºä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-939df83a3ba2f3185b7f3c33398ded13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e576dd2f31a7a3d090b44d111abf509f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd0fcf2af0284b09738f980f6a44b7a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c98af944ddd8320f520c3c81aed6330e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4025745073e8e9010c60009f644a91e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7013b3ef3f13ec5518834190feed24eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-473dea748b065192c5bcf65b5a424db6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Four-Eyes-Are-Better-Than-Two-Harnessing-the-Collaborative-Potential-of-Large-Models-via-Differentiated-Thinking-and-Complementary-Ensembles"><a href="#Four-Eyes-Are-Better-Than-Two-Harnessing-the-Collaborative-Potential-of-Large-Models-via-Differentiated-Thinking-and-Complementary-Ensembles" class="headerlink" title="Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of   Large Models via Differentiated Thinking and Complementary Ensembles"></a>Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of   Large Models via Differentiated Thinking and Complementary Ensembles</h2><p><strong>Authors:Jun Xie, Xiongjun Guan, Yingjian Zhu, Zhaoran Zhao, Xinming Wang, Hongzhu Yi, Feng Chen, Zhepeng Wang</strong></p>
<p>In this paper, we present the runner-up solution for the Ego4D EgoSchema Challenge at CVPR 2025 (Confirmed on May 20, 2025). Inspired by the success of large models, we evaluate and leverage leading accessible multimodal large models and adapt them to video understanding tasks via few-shot learning and model ensemble strategies. Specifically, diversified prompt styles and process paradigms are systematically explored and evaluated to effectively guide the attention of large models, fully unleashing their powerful generalization and adaptability abilities. Experimental results demonstrate that, with our carefully designed approach, directly utilizing an individual multimodal model already outperforms the previous state-of-the-art (SOTA) method which includes several additional processes. Besides, an additional stage is further introduced that facilitates the cooperation and ensemble of periodic results, which achieves impressive performance improvements. We hope this work serves as a valuable reference for the practical application of large models and inspires future research in the field. Our Code is available at <a target="_blank" rel="noopener" href="https://github.com/XiongjunGuan/EgoSchema-CVPR25">https://github.com/XiongjunGuan/EgoSchema-CVPR25</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨2025å¹´CVPRä¸Šçš„Ego4D EgoSchemaæŒ‘æˆ˜èµ›ä¸­çš„äºšå†›è§£å†³æ–¹æ¡ˆï¼ˆæˆªè‡³æ—¥æœŸä¸º2025å¹´5æœˆ20æ—¥ï¼‰ã€‚å—å¤§å‹æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬è¯„ä¼°å¹¶é‡‡ç”¨äº†é¢†å…ˆçš„å¯è®¿é—®å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼Œå¹¶é€šè¿‡å°æ ·æœ¬å­¦ä¹ å’Œæ¨¡å‹é›†æˆç­–ç•¥å°†å®ƒä»¬é€‚åº”è§†é¢‘ç†è§£ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œç³»ç»Ÿåœ°æ¢ç´¢å¹¶è¯„ä¼°äº†å¤šæ ·åŒ–çš„æç¤ºé£æ ¼å’Œæµç¨‹èŒƒå¼ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼å¤§å‹æ¨¡å‹çš„æ³¨æ„åŠ›ï¼Œå……åˆ†å‘æŒ¥å®ƒä»¬å¼ºå¤§çš„æ³›åŒ–å’Œé€‚åº”æ€§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡çš„æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨å•ä¸ªå¤šæ¨¡æ€æ¨¡å‹å·²ç»è¶…è¶Šäº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ˆåŒ…æ‹¬å‡ ä¸ªé¢å¤–çš„æµç¨‹ï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†é¢å¤–çš„é˜¶æ®µï¼Œä¿ƒè¿›äº†å‘¨æœŸæ€§ç»“æœçš„åˆä½œå’Œé›†æˆï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºå¤§å‹æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›æœ‰ä»·å€¼çš„å‚è€ƒï¼Œå¹¶æ¿€å‘è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiongjunGuan/EgoSchema-CVPR25">https://github.com/XiongjunGuan/EgoSchema-CVPR25</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16784v2">PDF</a> </p>
<p><strong>Summary</strong><br>è®ºæ–‡ä»‹ç»äº†åœ¨CVPR 2025å¹´ä¸¾åŠçš„EgoSchemaæŒ‘æˆ˜èµ›ä¸­è·å¾—äºšå†›çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨ç°æœ‰æˆç†Ÿçš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œå€ŸåŠ©å¤šæ ·åŒ–çš„æç¤ºæ–¹å¼å’Œæµç¨‹æ¨¡å¼è®¾è®¡æ€æƒ³æ¥è¿›è¡Œå¤šæ¨¡æ€çš„è§†é¢‘å†…å®¹è¯†åˆ«ä»»åŠ¡ã€‚è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ¨¡å‹èåˆç­–ç•¥ï¼Œä½¿å¾—å•ä¸ªæ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†å…ˆå‰çš„æœ€ä¼˜æ°´å¹³ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ˜¯CVPR 2025å¹´EgoSchemaæŒ‘æˆ˜èµ›çš„äºšå†›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶äººå‘˜åˆ©ç”¨å¹¶è¯„ä¼°äº†ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†å°‘æ ·æœ¬å­¦ä¹ ç­–ç•¥è¿›è¡Œé€‚åº”è§†é¢‘ç†è§£ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡å¤šæ ·åŒ–çš„æç¤ºé£æ ¼å’Œæµç¨‹æ¨¡å¼ï¼Œæœ‰æ•ˆåœ°å¼•å¯¼äº†å¤§å‹æ¨¡å‹çš„æ³¨æ„åŠ›ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨å•ä¸€çš„å¤šæ¨¡æ€æ¨¡å‹å·²ç»è¶…è¶Šäº†å…ˆå‰æœ€ä¼˜æ–¹æ³•çš„è¡¨ç°ï¼Œå‡å°‘äº†é¢å¤–çš„å¤„ç†æµç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¿ƒè¿›é˜¶æ®µæ€§ç»“æœçš„åˆä½œä¸èåˆçš„é¢å¤–é˜¶æ®µï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½è¡¨ç°ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¤§å‹æ¨¡å‹çš„å®é™…åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a06561494834f6110a46d479fc265c98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ec9c4f9acd583625c519fa4ead21194.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6905d89fedcb70282befe6a036062e04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c191b6cb32e4e5e0cc531d8887719f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9b9118bad4343419ba8f2f4b2d1f519.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf2aa5fa774159bb7a5bddd2d58ff987.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MutualNeRF-Improve-the-Performance-of-NeRF-under-Limited-Samples-with-Mutual-Information-Theory"><a href="#MutualNeRF-Improve-the-Performance-of-NeRF-under-Limited-Samples-with-Mutual-Information-Theory" class="headerlink" title="MutualNeRF: Improve the Performance of NeRF under Limited Samples with   Mutual Information Theory"></a>MutualNeRF: Improve the Performance of NeRF under Limited Samples with   Mutual Information Theory</h2><p><strong>Authors:Zifan Wang, Jingwei Li, Yitang Li, Yunze Liu</strong></p>
<p>This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field (NeRF) performance under limited samples using Mutual Information Theory. While NeRF excels in 3D scene synthesis, challenges arise with limited data and existing methods that aim to introduce prior knowledge lack theoretical support in a unified framework. We introduce a simple but theoretically robust concept, Mutual Information, as a metric to uniformly measure the correlation between images, considering both macro (semantic) and micro (pixel) levels. For sparse view sampling, we strategically select additional viewpoints containing more non-overlapping scene information by minimizing mutual information without knowing ground truth images beforehand. Our framework employs a greedy algorithm, offering a near-optimal solution. For few-shot view synthesis, we maximize the mutual information between inferred images and ground truth, expecting inferred images to gain more relevant information from known images. This is achieved by incorporating efficient, plug-and-play regularization terms. Experiments under limited samples show consistent improvement over state-of-the-art baselines in different settings, affirming the efficacy of our framework. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MutualNeRFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äº’ä¿¡æ¯ç†è®ºåœ¨æœ‰é™æ ·æœ¬ä¸‹å¢å¼ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ€§èƒ½ã€‚è™½ç„¶NeRFåœ¨3Dåœºæ™¯åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä»ä¼šå‡ºç°æŒ‘æˆ˜ï¼Œç°æœ‰å¼•å…¥å…ˆéªŒçŸ¥è¯†çš„æ–¹æ³•åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­ç¼ºä¹ç†è®ºæ”¯æŒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•ä½†ç†è®ºä¸Šç¨³å¥çš„æ¦‚å¿µâ€”â€”äº’ä¿¡æ¯ï¼Œä½œä¸ºä¸€ä¸ªæŒ‡æ ‡æ¥ç»Ÿä¸€æµ‹é‡å›¾åƒä¹‹é—´çš„å…³è”åº¦ï¼ŒåŒæ—¶è€ƒè™‘å®è§‚ï¼ˆè¯­ä¹‰ï¼‰å’Œå¾®è§‚ï¼ˆåƒç´ ï¼‰ä¸¤ä¸ªå±‚é¢ã€‚å¯¹äºç¨€ç–è§†å›¾é‡‡æ ·ï¼Œæˆ‘ä»¬æˆ˜ç•¥æ€§åœ°é€‰æ‹©åŒ…å«æ›´å¤šéé‡å åœºæ™¯ä¿¡æ¯çš„é¢å¤–è§‚ç‚¹ï¼Œé€šè¿‡æœ€å°åŒ–äº’ä¿¡æ¯æ¥é€‰å–ï¼Œè€Œæ— éœ€äº‹å…ˆäº†è§£çœŸå®å›¾åƒã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨è´ªå¿ƒç®—æ³•ï¼Œæä¾›æ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äºå°‘æ•°è§†å›¾åˆæˆï¼Œæˆ‘ä»¬åœ¨æ¨æ–­å›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´æœ€å¤§åŒ–äº’ä¿¡æ¯ï¼ŒæœŸæœ›æ¨æ–­å›¾åƒä»å·²çŸ¥å›¾åƒä¸­è·å¾—æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚è¿™æ˜¯é€šè¿‡èå…¥é«˜æ•ˆã€å³æ’å³ç”¨çš„æ­£åˆ™åŒ–é¡¹æ¥å®ç°çš„ã€‚åœ¨æœ‰é™æ ·æœ¬ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸åŒè®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11386v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäº’ä¿¡æ¯ç†è®ºæå‡NeRFåœ¨æœ‰é™æ ·æœ¬ä¸‹çš„æ€§èƒ½ã€‚å¼•å…¥äº’ä¿¡æ¯ä½œä¸ºè¡¡é‡å›¾åƒé—´å…³è”æ€§çš„ç»Ÿä¸€åº¦é‡æ ‡å‡†ï¼Œå…¼é¡¾å®è§‚è¯­ä¹‰å’Œå¾®è§‚åƒç´ å±‚é¢ã€‚é€šè¿‡æœ€å°åŒ–äº’ä¿¡æ¯ï¼Œé€‰æ‹©å«æœ‰æ›´å¤šéé‡å åœºæ™¯ä¿¡æ¯çš„è§†è§’è¿›è¡Œç¨€ç–è§†å›¾é‡‡æ ·ã€‚åœ¨å°‘é‡è§†å›¾åˆæˆä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–æ¨æ–­å›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯ï¼Œæé«˜æ¨æ–­å›¾åƒä»å·²çŸ¥å›¾åƒä¸­è·å¾—ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MutualNeRFæ¡†æ¶åŸºäºäº’ä¿¡æ¯ç†è®ºå¢å¼ºNeRFåœ¨æœ‰é™æ ·æœ¬ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>äº’ä¿¡æ¯ä½œä¸ºåº¦é‡å›¾åƒé—´å…³è”æ€§çš„ç»Ÿä¸€æ ‡å‡†ï¼Œå…¼é¡¾å®è§‚å’Œå¾®è§‚å±‚é¢ã€‚</li>
<li>é€šè¿‡æœ€å°åŒ–äº’ä¿¡æ¯ï¼Œè¿›è¡Œç¨€ç–è§†å›¾é‡‡æ ·ï¼Œé€‰æ‹©å«æœ‰æ›´å¤šéé‡å åœºæ™¯ä¿¡æ¯çš„è§†è§’ã€‚</li>
<li>åœ¨å°‘é‡è§†å›¾åˆæˆä¸­ï¼Œæœ€å¤§åŒ–æ¨æ–­å›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨é«˜æ•ˆã€å³æ’å³ç”¨çš„æ­£åˆ™åŒ–é¡¹æ¥å®ç°ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒè®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºå¼•å…¥å…ˆéªŒçŸ¥è¯†æä¾›äº†ç†è®ºæ”¯æŒï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨3Dåœºæ™¯åˆæˆä¸­çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2217ef208eedf994dc76df3cb6051f60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e500e9bab978da8670e50a8436a171e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50706cc105475f1b20c0d0a975b30b2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bda0eab60c436d7c5dd15a22b55a5a3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DINeMo-Learning-Neural-Mesh-Models-with-no-3D-Annotations"><a href="#DINeMo-Learning-Neural-Mesh-Models-with-no-3D-Annotations" class="headerlink" title="DINeMo: Learning Neural Mesh Models with no 3D Annotations"></a>DINeMo: Learning Neural Mesh Models with no 3D Annotations</h2><p><strong>Authors:Weijie Guo, Guofeng Zhang, Wufei Ma, Alan Yuille</strong></p>
<p>Category-level 3D&#x2F;6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at <a target="_blank" rel="noopener" href="https://analysis-by-synthesis.github.io/DINeMo/">https://analysis-by-synthesis.github.io/DINeMo/</a>. </p>
<blockquote>
<p>ç±»åˆ«çº§åˆ«çš„3D&#x2F;6Då§¿æ€ä¼°è®¡æ˜¯å®ç°å…¨é¢3Dåœºæ™¯ç†è§£çš„å…³é”®æ­¥éª¤ï¼Œè¿™å°†ä¸ºæœºå™¨äººæŠ€æœ¯å’ŒåµŒå…¥å¼äººå·¥æ™ºèƒ½çš„å¹¿æ³›åº”ç”¨æä¾›å¯èƒ½ã€‚è¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†ç¥ç»ç½‘æ ¼æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»åˆæˆåˆ†æçš„è§’åº¦æ¥è§£å†³ä¸€ç³»åˆ—2Då’Œ3Dä»»åŠ¡ã€‚å°½ç®¡è¿™äº›æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¢å¼ºäº†å¤„ç†éƒ¨åˆ†é®æŒ¡å’Œé¢†åŸŸåç§»çš„ç¨³å¥æ€§ï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äº3Dæ³¨é‡Šè¿›è¡Œéƒ¨åˆ†å¯¹æ¯”å­¦ä¹ ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨èŒƒå›´å¹¶é˜»ç¢äº†æœ‰æ•ˆæ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DINeMoï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘æ ¼æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä»å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹ä¸­è·å¾—ä¼ªå¯¹åº”æ¥è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä»»ä½•3Dæ³¨é‡Šã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŒå‘ä¼ªå¯¹åº”ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆä¼ªå¯¹åº”ã€‚åœ¨æ±½è½¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DINeMoåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬çš„3Då§¿æ€ä¼°è®¡æ–¹é¢éƒ½å¤§å¤§ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œä¸å®Œå…¨ç›‘ç£çš„æ–¹æ³•çš„å·®è·ç¼©å°äº†67.3%ã€‚æ­¤å¤–ï¼Œå½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ›´å¤šæœªæ ‡è®°å›¾åƒæ—¶ï¼Œæˆ‘ä»¬çš„DINeMoèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œæ‰©å±•ï¼Œè¿™æ˜¾ç¤ºäº†ä¸ä¾èµ–3Dæ³¨é‡Šçš„ç›‘ç£å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://analysis-by-synthesis.github.io/DINeMo/%E6%89%BE%E5%88%B0%E3%80%82">https://analysis-by-synthesis.github.io/DINeMo/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20220v2">PDF</a> Accepted to 3rd Workshop on Compositional 3D Vision at CVPR 2025   (C3DV)</p>
<p><strong>Summary</strong></p>
<p>æ— æ ‡æ³¨çš„3Dåœºæ™¯ç†è§£åœ¨æœºå™¨äººå’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚æœ€è¿‘ç¥ç»ç½‘ç»œæ¨¡å‹çš„ç ”ç©¶ä¸»è¦ä¾§é‡äºåŸºäºåˆæˆåˆ†æçš„æ–¹æ³•æ¥å¤„ç†å¤šç§ä»»åŠ¡ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å¯¹äºéƒ¨åˆ†é®æŒ¡å’Œé¢†åŸŸåç§»æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œä½†å®ƒä»¬ä»å—é™äºéœ€è¦åŸºäºæ ‡æ³¨æ•°æ®çš„éƒ¨åˆ†å¯¹æ¯”å­¦ä¹ ï¼Œå¯¼è‡´ç±»åˆ«æœ‰é™ä¸”éš¾ä»¥è§„æ¨¡åŒ–æ‰©å±•ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDINeMoçš„æ–°å‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡è§†è§‰åŸºç¡€æ¨¡å‹å¾—åˆ°çš„ä¼ªå¯¹åº”å…³ç³»è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æ ‡æ³¨æ•°æ®ã€‚é€šè¿‡é‡‡ç”¨åŒå‘ä¼ªå¯¹åº”å…³ç³»ç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆå±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œäº§ç”Ÿä¼ªå¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è½¦è¾†æ•°æ®é›†ä¸Šï¼ŒDINeMoçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„é›¶æ¬¡å’Œå°‘é‡ä»»åŠ¡ä¸­è®­ç»ƒçš„ä¸‰ç»´å§¿æ€ä¼°è®¡æ¨¡å‹ï¼Œå¹¶ä¸å®Œå…¨ç›‘ç£çš„æ–¹æ³•å·®è·ç¼©å°è‡³ç¼©å°è‡³ç™¾åˆ†ä¹‹å…­ç‚¹ä¸ƒä¸‰ã€‚æ­¤å¤–ï¼ŒDINeMoå¯ä»¥æœ‰æ•ˆåœ°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥æ›´å¤šçš„æ— æ ‡ç­¾å›¾åƒï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç›‘ç£å­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DINeMoæ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºè§£å†³ç±»åˆ«çº§åˆ«çš„ä¸‰ç»´å§¿æ€ä¼°è®¡é—®é¢˜ã€‚</li>
<li>DINeMoåˆ©ç”¨å¤§è§„æ¨¡è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¼ªå¯¹åº”å…³ç³»è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•æ ‡æ³¨æ•°æ®ã€‚å®ƒé€šè¿‡åˆæˆåˆ†ææ–¹æ³•ç»“åˆäº†å±€éƒ¨å¤–è§‚ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆä¼ªå¯¹åº”å…³ç³»ã€‚æ­¤ç‰¹ç‚¹å…‹æœäº†ä»¥å¾€æ–¹æ³•éœ€è¦åŸºäºæ ‡æ³¨æ•°æ®çš„é™åˆ¶ï¼Œæ‰©å¤§äº†åº”ç”¨èŒƒå›´å¹¶æé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37ef849d4669f4d49be2b9a86955eee6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0a99184dbcd0288e307b750540d34ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5beed820aa09625905f40d318c99e2e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0141fadec830fa4147b84d2b48df501c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98aa22db4e404e89f8e41043af3ea838.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SimLTD-Simple-Supervised-and-Semi-Supervised-Long-Tailed-Object-Detection"><a href="#SimLTD-Simple-Supervised-and-Semi-Supervised-Long-Tailed-Object-Detection" class="headerlink" title="SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object   Detection"></a>SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object   Detection</h2><p><strong>Authors:Phi Vu Tran</strong></p>
<p>While modern visual recognition systems have made significant advancements, many continue to struggle with the open problem of learning from few exemplars. This paper focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. Existing methods for long-tailed detection resort to external ImageNet labels to augment the low-shot training instances. However, such dependency on a large labeled database has limited utility in practical scenarios. We propose a versatile and scalable approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations. Our SimLTD framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes. Our approach can be viewed as an improved head-to-tail model transfer paradigm without the added complexities of meta-learning or knowledge distillation, as was required in past research. By harnessing supplementary unlabeled images, without extra image labels, SimLTD establishes new record results on the challenging LVIS v1 benchmark across both supervised and semi-supervised settings. </p>
<blockquote>
<p>å°½ç®¡ç°ä»£è§†è§‰è¯†åˆ«ç³»ç»Ÿå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è®¸å¤šç³»ç»Ÿä»é¢ä¸´ç€ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ è¿™ä¸€å¼€æ”¾æ€§é—®é¢˜ã€‚æœ¬æ–‡å…³æ³¨åœ¨å¯¹è±¡ç±»åˆ«éµå¾ªè‡ªç„¶é•¿å°¾åˆ†å¸ƒçš„æƒ…å¢ƒä¸‹ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚ç°æœ‰çš„é•¿å°¾æ£€æµ‹æ–¹æ³•ä¾èµ–äºå¤–éƒ¨ImageNetæ ‡ç­¾æ¥å¢å¼ºä½æ ·æœ¬è®­ç»ƒå®ä¾‹ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åœºæ™¯ä¸­ï¼Œå¯¹å¤§é‡æœ‰æ ‡ç­¾æ•°æ®åº“çš„ä¾èµ–å…·æœ‰å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¯é€‰çš„æ— æ ‡ç­¾å›¾åƒï¼Œè¿™äº›å›¾åƒæ— éœ€äººå·¥æ³¨é‡Šå³å¯è½»æ¾æ”¶é›†ã€‚æˆ‘ä»¬çš„SimLTDæ¡†æ¶ç®€å•ç›´è§‚ï¼Œåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆ1ï¼‰åœ¨ä¸°å¯Œçš„å¤´éƒ¨ç±»åˆ«ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰åœ¨ç¨€ç¼ºçš„å°¾éƒ¨ç±»åˆ«ä¸Šè¿›è¡Œè¿ç§»å­¦ä¹ ï¼›ï¼ˆ3ï¼‰åœ¨å¤´éƒ¨å’Œå°¾éƒ¨ç±»åˆ«çš„é‡‡æ ·é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¢«è§†ä¸ºä¸€ç§æ”¹è¿›çš„å¤´åˆ°å°¾æ¨¡å‹è¿ç§»èŒƒå¼ï¼Œæ— éœ€å¼•å…¥è¿‡å»ç ”ç©¶ä¸­æ‰€éœ€çš„å…ƒå­¦ä¹ æˆ–çŸ¥è¯†è’¸é¦ç­‰é¢å¤–å¤æ‚æ€§ã€‚é€šè¿‡åˆ©ç”¨é¢å¤–çš„æ— æ ‡ç­¾å›¾åƒï¼Œæ— éœ€é¢å¤–çš„å›¾åƒæ ‡ç­¾ï¼ŒSimLTDåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVIS v1åŸºå‡†æµ‹è¯•ä¸­åˆ›ä¸‹äº†æ–°çš„è®°å½•ç»“æœï¼Œæ¶µç›–äº†ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ä¸¤ç§æƒ…å¢ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20047v3">PDF</a> CVPR 2025. The reference code is available at   <a target="_blank" rel="noopener" href="https://github.com/lexisnexis-risk-open-source/simltd">https://github.com/lexisnexis-risk-open-source/simltd</a></p>
<p><strong>Summary</strong><br>ç°ä»£è§†è§‰è¯†åˆ«ç³»ç»Ÿè™½æœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨å­¦ä¹ å°‘é‡æ ·æœ¬è¿™ä¸€å¼€æ”¾é—®é¢˜ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å…³æ³¨å¯¹è±¡æ£€æµ‹ä¸­çš„é•¿å°¾åˆ†å¸ƒé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•ä¾èµ–ImageNetæ ‡ç­¾æ¥æ‰©å……ä½æ ·æœ¬è®­ç»ƒå®ä¾‹ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æ•ˆç”¨æœ‰é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§çµæ´»å¯æ‰©å±•çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¯é€‰çš„æ— æ ‡ç­¾å›¾åƒï¼Œè¿™äº›å›¾åƒæ— éœ€äººå·¥æ ‡æ³¨å³å¯æ”¶é›†ã€‚SimLTDæ¡†æ¶ç®€å•ç›´è§‚ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šåœ¨ä¸°å¯Œçš„å¤´éƒ¨ç±»åˆ«ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›åœ¨ç¨€ç¼ºçš„å°¾éƒ¨ç±»åˆ«ä¸Šè¿›è¡Œè¿ç§»å­¦ä¹ ï¼›åœ¨å¤´éƒ¨å’Œå°¾éƒ¨ç±»åˆ«çš„é‡‡æ ·é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•å¯è§†ä¸ºæ”¹è¿›çš„å¤´åˆ°å°¾æ¨¡å‹è¿ç§»èŒƒå¼ï¼Œæ— éœ€å…ƒå­¦ä¹ æˆ–çŸ¥è¯†è’¸é¦çš„å¤æ‚è¿‡ç¨‹ã€‚é€šè¿‡åˆ©ç”¨é¢å¤–çš„æ— æ ‡ç­¾å›¾åƒï¼ŒSimLTDåœ¨ä¸ä½¿ç”¨é¢å¤–å›¾åƒæ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVIS v1åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„è®°å½•ç»“æœï¼Œæ¶µç›–äº†ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ä¸¤ç§æƒ…å†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£è§†è§‰è¯†åˆ«ç³»ç»Ÿåœ¨å­¦ä¹ å°‘é‡æ ·æœ¬ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ImageNetæ ‡ç­¾æ‰©å……ä½æ ·æœ¬è®­ç»ƒå®ä¾‹ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æ•ˆæœæœ‰é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨æ— æ ‡ç­¾å›¾åƒçš„æ–¹æ³•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯æ”¶é›†ã€‚</li>
<li>SimLTDæ¡†æ¶åŒ…æ‹¬é¢„è®­ç»ƒã€è¿ç§»å­¦ä¹ å’Œå¾®è°ƒä¸‰ä¸ªæ­¥éª¤ã€‚</li>
<li>SimLTDæ¡†æ¶å®ç°äº†å¤´åˆ°å°¾æ¨¡å‹è¿ç§»çš„æ”¹è¿›èŒƒå¼ï¼Œæ— éœ€å¤æ‚çš„å…ƒå­¦ä¹ æˆ–çŸ¥è¯†è’¸é¦è¿‡ç¨‹ã€‚</li>
<li>åˆ©ç”¨é¢å¤–çš„æ— æ ‡ç­¾å›¾åƒï¼ŒSimLTDåœ¨LVIS v1åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„è®°å½•ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5bc62ce5129eb638d2f95fdd296d2131.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30282318a0024c9a2e68d27f452fafd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9358db3055dd608324c93e8634107194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ee2def362661f2d40f25ed20094a294.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfb59dd606f47f718650502eec5b7403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ad83366254906d9b43c1394a63217cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7512fd2a74fab281f3bf07969d0911d5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Enhancing-Few-Shot-Vision-Language-Classification-with-Large-Multimodal-Model-Features"><a href="#Enhancing-Few-Shot-Vision-Language-Classification-with-Large-Multimodal-Model-Features" class="headerlink" title="Enhancing Few-Shot Vision-Language Classification with Large Multimodal   Model Features"></a>Enhancing Few-Shot Vision-Language Classification with Large Multimodal   Model Features</h2><p><strong>Authors:Chancharik Mitra, Brandon Huang, Tianning Chai, Zhiqiu Lin, Assaf Arbelle, Rogerio Feris, Leonid Karlinsky, Trevor Darrell, Deva Ramanan, Roei Herzig</strong></p>
<p>Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks. Despite strong performance, LMMsâ€™ generative outputs are not specialized for vision-language classification tasks (i.e., tasks with vision-language inputs and discrete labels) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for these tasks is the extraction of useful features from generative LMMs. To overcome this, we propose an approach that leverages multimodal feature extraction from the LMMâ€™s latent space. Toward this end, we present Sparse Attention Vectors (SAVs) â€“ a finetuning-free method that leverages sparse attention head activations (fewer than 5% of the heads) in LMMs as strong feature representations. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of vision-language classification tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations. </p>
<blockquote>
<p>ç”Ÿæˆå¼å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¦‚LLaVAå’ŒQwen-VLåœ¨å¤šç§è§†è§‰è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å°½ç®¡æ€§èƒ½å¼ºå¤§ï¼ŒLMMsçš„ç”Ÿæˆè¾“å‡ºå¹¶ä¸ä¸“é—¨é’ˆå¯¹è§†è§‰è¯­è¨€åˆ†ç±»ä»»åŠ¡ï¼ˆå³å…·æœ‰è§†è§‰è¯­è¨€è¾“å…¥å’Œç¦»æ•£æ ‡ç­¾çš„ä»»åŠ¡ï¼‰ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©è§†è§‰é—®ç­”ã€‚åˆ©ç”¨LMMså®Œæˆè¿™äº›ä»»åŠ¡çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä»ç”Ÿæˆå¼LMMsä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨LMMæ½œåœ¨ç©ºé—´ä¸­çš„å¤šæ¨¡æ€ç‰¹å¾æå–çš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–æ³¨æ„åŠ›å‘é‡ï¼ˆSAVsï¼‰â€”â€”ä¸€ç§æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨LMMä¸­å°‘äº5%çš„æ³¨æ„åŠ›å¤´æ¿€æ´»ä½œä¸ºå¼ºå¤§çš„ç‰¹å¾è¡¨ç¤ºã€‚ä»…é€šè¿‡å°‘é‡æ ·æœ¬ï¼ŒSAVsåœ¨å¤šç§è§†è§‰è¯­è¨€åˆ†ç±»ä»»åŠ¡ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸å„ç§å°æ ·æœ¬å’Œå¾®è°ƒåŸºå‡†ç›¸æ¯”å…·æœ‰é¢†å…ˆæ°´å¹³ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜æš—ç¤ºï¼Œéšç€é¢å¤–æ ·æœ¬çš„å¢åŠ ï¼ŒSAVsçš„æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œå¹¶ä¸”èƒ½æ¨å¹¿åˆ°ç±»ä¼¼ä»»åŠ¡ï¼Œè¿™è¡¨æ˜SAVsæ˜¯æœ‰æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00142v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLaVAå’ŒQwen-VLç­‰å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨LMMçš„æ½œåœ¨ç©ºé—´ä¸­çš„å¤šæ¨¡æ€ç‰¹å¾æå–æ¥è§£å†³å…¶åœ¨è§†è§‰è¯­è¨€åˆ†ç±»ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡Sparse Attention Vectorsï¼ˆSAVsï¼‰å®ç°ï¼Œåˆ©ç”¨LMMä¸­è¾ƒå°‘çš„æ³¨æ„åŠ›å¤´æ¿€æ´»ï¼ˆå°‘äº5%ï¼‰ä½œä¸ºå¼ºå¤§çš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨å¤šä¸ªè§†è§‰è¯­è¨€åˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒSAVsä»…é€šè¿‡å°‘é‡æ ·æœ¬å°±å®ç°äº†ä¸å¤šç§å°‘æ ·æœ¬å¾®è°ƒåŸºçº¿ç›¸æ¯”çš„å“è¶Šæ€§èƒ½ã€‚å®éªŒè¿˜è¡¨æ˜ï¼ŒSAVsçš„æ€§èƒ½å¯éšé¢å¤–æ ·æœ¬çš„å¢åŠ è€Œæå‡ï¼Œå¹¶èƒ½æ³›åŒ–åˆ°ç±»ä¼¼ä»»åŠ¡ï¼Œè¯æ˜å…¶ä½œä¸ºæœ‰æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMså¦‚LLaVAå’ŒQwen-VLåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰è¯­è¨€åˆ†ç±»ä»»åŠ¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨LMMæ½œåœ¨ç©ºé—´ä¸­çš„å¤šæ¨¡æ€ç‰¹å¾æå–çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>SAVsæ˜¯ä¸€ç§æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨LMMä¸­å°‘é‡çš„æ³¨æ„åŠ›å¤´æ¿€æ´»ä½œä¸ºå¼ºå¤§çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>SAVsåœ¨å°‘é‡æ ·æœ¬ä¸‹å°±èƒ½åœ¨å¤šä¸ªè§†è§‰è¯­è¨€åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>SAVsçš„æ€§èƒ½å¯éšé¢å¤–æ ·æœ¬çš„å¢åŠ è€Œæå‡ï¼Œå¹¶å¯ä»¥æ³›åŒ–åˆ°ç±»ä¼¼ä»»åŠ¡ã€‚</li>
<li>SAVsä½œä¸ºæœ‰æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºå…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41b22f2d7c03c37f50f79b8206a3be02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57fcb2cec9f5d5b42a27b4bb8d7d7aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9214f834f820667374f2aec0281905f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-124b3b83c26565c82a8f04c7e8acb0d8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Open-FinLLMs-Open-Multimodal-Large-Language-Models-for-Financial-Applications"><a href="#Open-FinLLMs-Open-Multimodal-Large-Language-Models-for-Financial-Applications" class="headerlink" title="Open-FinLLMs: Open Multimodal Large Language Models for Financial   Applications"></a>Open-FinLLMs: Open Multimodal Large Language Models for Financial   Applications</h2><p><strong>Authors:Jimin Huang, Mengxi Xiao, Dong Li, Zihao Jiang, Yuzhe Yang, Yifei Zhang, Lingfei Qian, Yan Wang, Xueqing Peng, Yang Ren, Ruoyu Xiang, Zhengyu Chen, Xiao Zhang, Yueru He, Weiguang Han, Shunian Chen, Lihang Shen, Daniel Kim, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Guojun Xiong, Zhiwei Liu, Zheheng Luo, Zhiyuan Yao, Ruey-Ling Weng, Meikang Qiu, Kaleb E Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jian-Yun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Qianqian Xie, Sophia Ananiadou, Junichi Tsujii</strong></p>
<p>Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit{Open-FinLLMs}, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE">https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE</a>) and models under OSI-approved licenses. </p>
<blockquote>
<p>é‡‘èLLMï¼ˆå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰åœ¨æ¨è¿›é‡‘èä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸåº”ç”¨æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å—åˆ°æœ‰é™è¯­æ–™åº“ã€å¼±å¤šæ¨¡æ€èƒ½åŠ›å’Œç‹­çª„è¯„ä¼°èŒƒå›´çš„é™åˆ¶ï¼Œä½¿å…¶ä¸å¤ªé€‚åˆçœŸå®ä¸–ç•Œçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†â€œOpen-FinLLMsâ€ï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„å¤šæ¨¡æ€é‡‘èLLMï¼Œæ—¨åœ¨å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—å’Œå›¾è¡¨æ•°æ®ç­‰å„ç§ä»»åŠ¡ï¼Œå¹¶åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥å¥—ä»¶åŒ…æ‹¬FinLLaMAï¼ˆåœ¨åŒ…å«52äº¿æ ‡è®°çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼‰ã€FinLLaMA-Instructï¼ˆä½¿ç”¨573Ké‡‘èæŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼‰å’ŒFinLLaVAï¼ˆé€šè¿‡143ä¸‡å¯¹å¤šæ¨¡æ€è°ƒä¼˜å¢å¼ºè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ï¼‰ã€‚æˆ‘ä»¬å¯¹Open-FinLLMsè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬14ä¸ªé‡‘èä»»åŠ¡ã€30ä¸ªæ•°æ®é›†å’Œ4ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå—ç›‘ç£å¾®è°ƒè®¾ç½®ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ¨å‡ºäº†ä¸¤ä¸ªæ–°çš„å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒOpen-FinLLMsåœ¨é‡‘èNLPã€å†³ç­–å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…ˆè¿›çš„é‡‘èå’Œé€šç”¨LLMï¼Œå¦‚GPT-4ï¼Œå‡¸æ˜¾äº†å®ƒä»¬åœ¨åº”å¯¹ç°å®æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚ä¸ºäº†ä¿ƒè¿›å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„åˆ›æ–°å’Œåˆä½œï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ‰€æœ‰ä»£ç ï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE%EF%BC%89%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E9%81%B5%E5%BE%AAOSI%E6%89%B9%E5%87%86%E8%AE%B8%E5%8F%AF%E8%AF%81%E3%80%82">https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSEï¼‰å’Œæ¨¡å‹ï¼Œå¹¶éµå¾ªOSIæ‰¹å‡†è®¸å¯è¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11878v3">PDF</a> 33 pages, 13 figures</p>
<p><strong>Summary</strong>ï¼šå¼€æ”¾é‡‘èLLMå±•ç°å¤šå…ƒèƒ½åŠ›ã€‚LLMåœ¨æ¨è¿›é‡‘èä»»åŠ¡å’Œç›¸å…³åº”ç”¨æ–¹é¢æ½œåŠ›å·¨å¤§ï¼Œä½†ä»å—é™äºè¯­æ–™åº“ç¨€ç¼ºã€å¤šæ¨¡æ€èƒ½åŠ›è–„å¼±ä»¥åŠè¯„ä»·æ¨¡å¼å±€é™ï¼Œéš¾ä»¥å®ç°ç°å®ä¸–ç•Œçš„åº”ç”¨åœºæ™¯ã€‚é’ˆå¯¹æ­¤æŒ‘æˆ˜ï¼Œå¼€æ”¾é‡‘èLLMæ˜¯é¦–ä¸ªå¼€æºçš„å¤šæ¨¡æ€é‡‘èLLMå·¥å…·ï¼Œå¯å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—å’Œå›¾è¡¨æ•°æ®ç­‰å¤šç§ä»»åŠ¡ï¼Œåœ¨é›¶æ ·æœ¬ã€å°æ ·æœ¬å¾®è°ƒåœºæ™¯ä¸‹è¡¨ç°å“è¶Šã€‚æˆ‘ä»¬æä¾›ä¸‰ä¸ªå¥—ä»¶åŒ…æ‹¬FinLLaMAï¼ˆé¢„è®­ç»ƒè¯­æ–™åº“è¾¾52äº¿è¯ï¼‰ã€FinLLaMA-Instructï¼ˆç»è¿‡57ä¸‡æ¡é‡‘èæŒ‡ä»¤ç²¾ç»†è°ƒæ•´ï¼‰å’ŒFinLLaVAï¼ˆå¢å¼ºå¤šæ¨¡æ€è°ƒä¼˜å¯¹å®ç°å¼ºè·¨æ¨¡æ€æ¨ç†ï¼‰ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¼€æ”¾é‡‘èLLMåœ¨å¤šç§é‡‘èä»»åŠ¡å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šä¼˜äºå…ˆè¿›é‡‘èå’Œé€šç”¨LLMï¼Œå¦‚GPT-4ç­‰ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰ä»£ç å’Œæ¨¡å‹ä»¥æ¨åŠ¨å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„åˆä½œåˆ›æ–°ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¼€æ”¾é‡‘èLLMå…·å¤‡å¤„ç†å¤šç§é‡‘èä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—å’Œå›¾è¡¨æ•°æ®ç­‰ã€‚</li>
<li>è¯¥å·¥å…·èåˆäº†å¤šæ¨¡æ€æŠ€æœ¯ï¼Œæå‡äº†å¤„ç†å¤æ‚é‡‘èæ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>å¼€æ”¾é‡‘èLLMåœ¨é›¶æ ·æœ¬ã€å°æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸‹è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>é€šè¿‡å…¨é¢çš„è¯„ä¼°ï¼Œå¼€æ”¾é‡‘èLLMè¢«è¯å®ä¼˜äºå…¶ä»–å…ˆè¿›çš„é‡‘èå’Œé€šç”¨LLMã€‚</li>
<li>è¯¥å·¥å…·æä¾›äº†ä¸‰ä¸ªå¥—ä»¶ä»¥é€‚åº”ä¸åŒçš„é‡‘èåº”ç”¨åœºæ™¯éœ€æ±‚ã€‚</li>
<li>å¼€æ”¾é‡‘èLLMçš„å‘å¸ƒæ—¨åœ¨ä¿ƒè¿›å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„åˆä½œä¸åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f01d0b5a3e9c2a0614ae27abbac59d07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daefbbfbe3895cc4862a4d1aa661e364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26dbdbe2c679f7d0a5c07d9a77b0c6e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae702bdac4222e9d230c9de8cead75b0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Group-On-Boosting-One-Shot-Segmentation-with-Supportive-Query"><a href="#Group-On-Boosting-One-Shot-Segmentation-with-Supportive-Query" class="headerlink" title="Group-On: Boosting One-Shot Segmentation with Supportive Query"></a>Group-On: Boosting One-Shot Segmentation with Supportive Query</h2><p><strong>Authors:Hanjing Zhou, Mingze Yin, Danny Chen, Jian Wu, JinTai Chen</strong></p>
<p>One-shot semantic segmentation aims to segment query images given only ONE annotated support image of the same class. This task is challenging because target objects in the support and query images can be largely different in appearance and pose (i.e., intra-class variation). Prior works suggested that incorporating more annotated support images in few-shot settings boosts performances but increases costs due to additional manual labeling. In this paper, we propose a novel and effective approach for ONE-shot semantic segmentation, called Group-On, which packs multiple query images in batches for the benefit of mutual knowledge support within the same category. Specifically, after coarse segmentation masks of the batch of queries are predicted, query-mask pairs act as pseudo support data to enhance mask predictions mutually. To effectively steer such process, we construct an innovative MoME module, where a flexible number of mask experts are guided by a scene-driven router and work together to make comprehensive decisions, fully promoting mutual benefits of queries. Comprehensive experiments on three standard benchmarks show that, in the ONE-shot setting, Group-On significantly outperforms previous works by considerable margins. With only one annotated support image, Group-On can be even competitive with the counterparts using 5 annotated images. </p>
<blockquote>
<p>å•æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ä»…ä½¿ç”¨ä¸€ä¸ªåŒç±»åˆ«çš„æ ‡æ³¨æ”¯æŒå›¾åƒå¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚è¿™ä¸€ä»»åŠ¡æ˜¯å……æ»¡æŒ‘æˆ˜çš„ï¼Œå› ä¸ºæ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡åœ¨å¤–è§‚å’Œå§¿æ€ä¸Šå¯èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼ˆå³ç±»å†…å˜åŒ–ï¼‰ã€‚æ—©æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å°‘æ•°æ ·æœ¬è®¾ç½®ä¸­åŠ å…¥æ›´å¤šçš„æ ‡æ³¨æ”¯æŒå›¾åƒå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†å› é¢å¤–æ‰‹åŠ¨æ ‡æ³¨è€Œäº§ç”Ÿçš„æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å•æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æ–°å‹æœ‰æ•ˆæ–¹æ³•ï¼Œç§°ä¸ºGroup-Onã€‚å®ƒå°†å¤šä¸ªæŸ¥è¯¢å›¾åƒåˆ†æ‰¹å¤„ç†ï¼Œä»¥è·å–åŒä¸€ç±»åˆ«å†…çš„ç›¸äº’çŸ¥è¯†æ”¯æŒã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨é¢„æµ‹ä¸€æ‰¹æŸ¥è¯¢å›¾åƒçš„ç²—ç•¥åˆ†å‰²æ©è†œåï¼ŒæŸ¥è¯¢-æ©è†œå¯¹ä½œä¸ºä¼ªæ”¯æŒæ•°æ®ï¼Œç›¸äº’å¢å¼ºæ©è†œé¢„æµ‹ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆ›æ–°å‹çš„MoMEæ¨¡å—ï¼Œå…¶ä¸­ç”±åœºæ™¯é©±åŠ¨è·¯ç”±å™¨æŒ‡å¯¼çš„å¤šä¸ªçµæ´»æ©è†œä¸“å®¶å¯ä»¥å…±åŒå·¥ä½œï¼Œä»¥åšå‡ºå…¨é¢å†³ç­–ï¼Œå……åˆ†å‘æŒ¥æŸ¥è¯¢ä¹‹é—´çš„äº’åˆ©ä¼˜åŠ¿ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨å•æ ·æœ¬è®¾ç½®ä¸‹ï¼ŒGroup-Onæ˜¾è‘—ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œå…·æœ‰ç›¸å½“å¤§çš„ä¼˜åŠ¿ã€‚ä»…ä½¿ç”¨ä¸€ä¸ªæ ‡æ³¨çš„æ”¯æŒå›¾åƒï¼ŒGroup-Onç”šè‡³å¯ä»¥ä¸ä½¿ç”¨5ä¸ªæ ‡æ³¨å›¾åƒçš„åŒç±»æ–¹æ³•ç›¸ç«äº‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.11871v2">PDF</a> </p>
<p><strong>Summary</strong><br>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ä»…ä½¿ç”¨ä¸€å¼ åŒç±»æ ‡æ³¨çš„æ”¯æŒå›¾åƒå¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œåˆ†å‰²ã€‚æ­¤ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡åœ¨å¤–è§‚å’Œå§¿åŠ¿ä¸Šå¯èƒ½å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼ˆå³ç±»å†…å˜åŒ–ï¼‰ã€‚ä»¥å‰çš„å·¥ä½œè¡¨æ˜ï¼Œåœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­åŠ å…¥æ›´å¤šæ ‡æ³¨çš„æ”¯æŒå›¾åƒå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†ä¼šå¢åŠ ç”±äºé¢å¤–æ‰‹åŠ¨æ ‡æ³¨çš„æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æ–°å‹æœ‰æ•ˆæ–¹æ³•ï¼Œç§°ä¸ºGroup-Onï¼Œå®ƒå°†å¤šä¸ªæŸ¥è¯¢å›¾åƒåˆ†æ‰¹æ‰“åŒ…ï¼Œä»¥åœ¨åŒç±»ä¸­äº’ç›¸è·å–çŸ¥è¯†æ”¯æŒã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨é¢„æµ‹äº†æ‰¹æ¬¡æŸ¥è¯¢å›¾åƒçš„ç²—ç•¥åˆ†å‰²æ©è†œåï¼ŒæŸ¥è¯¢æ©è†œå¯¹å……å½“ä¼ªæ”¯æŒæ•°æ®ï¼Œäº’ç›¸å¢å¼ºæ©è†œé¢„æµ‹ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¼•å¯¼è¿™ä¸€è¿‡ç¨‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆ›æ–°çš„MoMEæ¨¡å—ï¼Œå…¶ä¸­ç”±åœºæ™¯é©±åŠ¨çš„è·¯ç”±å™¨æŒ‡å¯¼å¯å˜æ•°é‡çš„æ©è†œä¸“å®¶å…±åŒåšå‡ºå…¨é¢å†³ç­–ï¼Œå……åˆ†ä¿ƒè¿›æŸ¥è¯¢ä¹‹é—´çš„ç›¸äº’åˆ©ç›Šã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œåœ¨å•æ¬¡æ‹æ‘„è®¾ç½®ä¸­ï¼ŒGroup-Onæ˜¾è‘—ä¼˜äºä»¥å‰çš„å·¥ä½œã€‚ä»…ä½¿ç”¨ä¸€å¼ æ ‡æ³¨çš„æ”¯æŒå›¾åƒï¼ŒGroup-Onç”šè‡³å¯ä»¥ä¸ä½¿ç”¨5å¼ æ ‡æ³¨å›¾åƒçš„åŒç±»äº§å“ç›¸ç«äº‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Group-Onæ–¹æ³•é’ˆå¯¹å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡æ‰¹é‡å¤„ç†æŸ¥è¯¢å›¾åƒï¼Œå®ç°äº’ç›¸çš„çŸ¥è¯†æ”¯æŒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æŸ¥è¯¢å›¾åƒä¹‹é—´çš„ç²—ç•¥åˆ†å‰²æ©è†œæ¥äº’ç›¸å¢å¼ºé¢„æµ‹ç»“æœã€‚</li>
<li>åˆ›æ–°æ€§åœ°æå‡ºäº†MoMEæ¨¡å—ï¼Œé€šè¿‡åœºæ™¯é©±åŠ¨çš„è·¯ç”±å™¨å¼•å¯¼å¯å˜æ•°é‡çš„æ©è†œä¸“å®¶å…±åŒåšå‡ºå†³ç­–ã€‚</li>
<li>Group-Onæ–¹æ³•æ˜¾è‘—æé«˜äº†å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»…ä½¿ç”¨ä¸€å¼ æ ‡æ³¨çš„æ”¯æŒå›¾åƒæ—¶ã€‚</li>
<li>Group-Onæ–¹æ³•åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æŸ¥è¯¢å›¾åƒä¹‹é—´çš„äº’åŠ©å…³ç³»ï¼ŒGroup-Onæ–¹æ³•é™ä½äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.11871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0715916b318b06332ff9e1e1763d3f95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e976240e16df17f611395269b892d1c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-683945ec6f1b4c9fd3f8babf0b9904af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd72b8da1eab3f688dfa74d16adea32a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-819f6c4ad6a30ca527ae04a3de1ffc59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2bb115bc51cd387768236aa3716e218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17b8c8600d7587f294922863f9e86944.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc9419c0dc91e0f3c3af650a9f8bb61.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b51f93e5910a58de4b7c61f4a246da8b.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion   Methods
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5a9273df27a9d4d38716f41510ec8c4b.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  A Culturally-diverse Multilingual Multimodal Video Benchmark & Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
