<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  CXR-LT 2024 A MICCAI challenge on long-tailed, multi-label, and   zero-shot disease classification from chest X-ray">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c3005ee20960f03d62fd66584cd43830.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-11-æ›´æ–°"><a href="#2025-06-11-æ›´æ–°" class="headerlink" title="2025-06-11 æ›´æ–°"></a>2025-06-11 æ›´æ–°</h1><h2 id="CXR-LT-2024-A-MICCAI-challenge-on-long-tailed-multi-label-and-zero-shot-disease-classification-from-chest-X-ray"><a href="#CXR-LT-2024-A-MICCAI-challenge-on-long-tailed-multi-label-and-zero-shot-disease-classification-from-chest-X-ray" class="headerlink" title="CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and   zero-shot disease classification from chest X-ray"></a>CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and   zero-shot disease classification from chest X-ray</h2><p><strong>Authors:Mingquan Lin, Gregory Holste, Song Wang, Yiliang Zhou, Yishu Wei, Imon Banerjee, Pengyi Chen, Tianjie Dai, Yuexi Du, Nicha C. Dvornek, Yuyan Ge, Zuowei Guo, Shouhei Hanaoka, Dongkyun Kim, Pablo Messina, Yang Lu, Denis Parra, Donghyun Son, Ãlvaro Soto, Aisha Urooj, RenÃ© Vidal, Yosuke Yamagishi, Zefan Yang, Ruichi Zhang, Yang Zhou, Leo Anthony Celi, Ronald M. Summers, Zhiyong Lu, Hao Chen, Adam Flanders, George Shih, Zhangyang Wang, Yifan Peng</strong></p>
<p>The CXR-LT series is a community-driven initiative designed to enhance lung disease classification using chest X-rays (CXR). It tackles challenges in open long-tailed lung disease classification and enhances the measurability of state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve these goals by providing high-quality benchmark CXR data for model development and conducting comprehensive evaluations to identify ongoing issues impacting lung disease classification performance. Building on the success of CXR-LT 2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45 disease labels, including 19 new rare disease findings. It also introduces a new focus on zero-shot learning to address limitations identified in the previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed classification on a large, noisy test set, (ii) long-tailed classification on a manually annotated â€œgold standardâ€ subset, and (iii) zero-shot generalization to five previously unseen disease findings. This paper provides an overview of CXR-LT 2024, detailing the data curation process and consolidating state-of-the-art solutions, including the use of multimodal models for rare disease detection, advanced generative approaches to handle noisy labels, and zero-shot learning strategies for unseen diseases. Additionally, the expanded dataset enhances disease coverage to better represent real-world clinical settings, offering a valuable resource for future research. By synthesizing the insights and innovations of participating teams, we aim to advance the development of clinically realistic and generalizable diagnostic models for chest radiography. </p>
<blockquote>
<p>CXR-LTç³»åˆ—æ˜¯ä¸€ä¸ªç¤¾åŒºé©±åŠ¨çš„é¡¹ç›®ï¼Œæ—¨åœ¨åˆ©ç”¨èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æŠ€æœ¯æé«˜è‚ºéƒ¨ç–¾ç—…çš„åˆ†ç±»èƒ½åŠ›ã€‚å®ƒè§£å†³äº†å¼€æ”¾é•¿å°¾è‚ºéƒ¨ç–¾ç—…åˆ†ç±»çš„æŒ‘æˆ˜ï¼Œå¹¶æé«˜äº†æœ€å…ˆè¿›æŠ€æœ¯çš„è¡¡é‡æ ‡å‡†ã€‚CXR-LT 2023æ˜¯é¦–æ¬¡æ´»åŠ¨ï¼Œæ—¨åœ¨é€šè¿‡ä¸ºæ¨¡å‹å¼€å‘æä¾›é«˜è´¨é‡åŸºå‡†CXRæ•°æ®ï¼Œå¹¶è¿›è¡Œå…¨é¢è¯„ä¼°ä»¥è¯†åˆ«å½±å“è‚ºéƒ¨ç–¾ç—…åˆ†ç±»æ€§èƒ½çš„é—®é¢˜ï¼Œå®ç°è¿™äº›ç›®æ ‡ã€‚åŸºäºCXR-LT 2023çš„æˆåŠŸï¼ŒCXR-LT 2024å°†æ•°æ®é›†æ‰©å±•åˆ°377,110å¼ èƒ¸éƒ¨Xå…‰ï¼ˆCXRsï¼‰å›¾åƒå’Œ45ç§ç–¾ç—…æ ‡ç­¾ï¼ŒåŒ…æ‹¬19ç§æ–°çš„ç½•è§ç–¾ç—…å‘ç°ã€‚å®ƒè¿˜å¼•å…¥äº†é›¶æ ·æœ¬å­¦ä¹ çš„æ–°é‡ç‚¹ï¼Œä»¥è§£å†³ä»¥å‰æ´»åŠ¨ä¸­è¯†åˆ«å‡ºçš„å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒCXR-LT 2024åŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼šï¼ˆiï¼‰å¤§å‹å˜ˆæ‚æµ‹è¯•é›†ä¸Šçš„é•¿å°¾åˆ†ç±»ï¼Œï¼ˆiiï¼‰åœ¨æ‰‹åŠ¨æ³¨é‡Šçš„â€œé‡‘æ ‡å‡†â€å­é›†ä¸Šçš„é•¿å°¾åˆ†ç±»ï¼Œä»¥åŠï¼ˆiiiï¼‰å¯¹äº”ç§ä¹‹å‰æœªè§è¿‡çš„ç–¾ç—…çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚æœ¬æ–‡æ¦‚è¿°äº†CXR-LT 2024ï¼Œè¯¦ç»†ä»‹ç»äº†æ•°æ®æ•´ç†è¿‡ç¨‹å¹¶å·©å›ºäº†æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç”¨äºç½•è§ç–¾ç—…æ£€æµ‹çš„å¤šæ¨¡å¼æ¨¡å‹ã€å¤„ç†å˜ˆæ‚æ ‡ç­¾çš„é«˜çº§ç”Ÿæˆæ–¹æ³•å’Œç”¨äºæœªè§ç–¾ç—…çš„é›¶æ ·æœ¬å­¦ä¹ ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæ‰©å±•çš„æ•°æ®é›†æé«˜äº†ç–¾ç—…è¦†ç›–ç‡ï¼Œä»¥æ›´å¥½åœ°ä»£è¡¨çœŸå®ä¸–ç•Œä¸´åºŠç¯å¢ƒï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚é€šè¿‡ç»¼åˆå„å‚èµ›é˜Ÿçš„è§è§£å’Œåˆ›æ–°ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¨åŠ¨ä¸´åºŠç°å®å’Œå¯æ¨å¹¿çš„èƒ¸éƒ¨æ”¾å°„å­¦è¯Šæ–­æ¨¡å‹çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07984v1">PDF</a> 17 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¤¾åŒºé©±åŠ¨çš„CXR-LTç³»åˆ—æ—¨åœ¨åˆ©ç”¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å¢å¼ºå¯¹è‚ºéƒ¨ç–¾ç—…çš„åˆ†ç±»èƒ½åŠ›ã€‚å…¶è§£å†³å¼€æ”¾å¼é•¿å°¾è‚ºéƒ¨ç–¾ç—…åˆ†ç±»çš„æŒ‘æˆ˜ï¼Œå¹¶å¢å¼ºæœ€æ–°æŠ€æœ¯çš„å¯è¡¡é‡æ€§ã€‚CXR-LT 2024åœ¨æˆåŠŸçš„åŸºç¡€ä¸Šæ‰©å±•æ•°æ®é›†è‡³æ¶µç›–å¤šç§ç–¾ç—…æ ‡ç­¾ï¼Œå¹¶å¼•å…¥é›¶æ ·æœ¬å­¦ä¹ ä»¥åº”å¯¹å…ˆå‰è¯†åˆ«åˆ°çš„å±€é™æ€§ã€‚æ—¨åœ¨ä¿ƒè¿›ä¸´åºŠè¯Šæ–­æ¨¡å‹çš„å®é™…å’Œæ™®éæ€§å‘å±•ã€‚æ­¤æ´»åŠ¨æœ‰åˆ©äºåŠ å¼ºè¡Œä¸šè¿›æ­¥ä¸äººæ‰åŸ¹å…»èƒ½åŠ›ååŒå¢é•¿æ¨¡å¼å½¢æˆè‰¯æ€§å‘å±•è¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CXR-LTç³»åˆ—æ˜¯ä¸€ä¸ªç¤¾åŒºé©±åŠ¨çš„é¡¹ç›®ï¼Œæ—¨åœ¨é€šè¿‡èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æé«˜è‚ºéƒ¨ç–¾ç—…çš„åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>è¯¥é¡¹ç›®è§£å†³äº†å¼€æ”¾å¼é•¿å°¾è‚ºéƒ¨ç–¾ç—…åˆ†ç±»çš„æŒ‘æˆ˜ï¼Œå¢å¼ºäº†æœ€æ–°æŠ€æœ¯çš„å¯è¡¡é‡æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d64cc589a934d0a0eda6406713deb5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-996266601a982afc4532c39304425068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eeb40b7169db9229503b0be515ff0cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8b42f89e1875f17c82c1a3ac38e573.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa172990ff6476527e13f40b89441e2b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VIVAT-Virtuous-Improving-VAE-Training-through-Artifact-Mitigation"><a href="#VIVAT-Virtuous-Improving-VAE-Training-through-Artifact-Mitigation" class="headerlink" title="VIVAT: Virtuous Improving VAE Training through Artifact Mitigation"></a>VIVAT: Virtuous Improving VAE Training through Artifact Mitigation</h2><p><strong>Authors:Lev Novitskiy, Viacheslav Vasilev, Maria Kovaleva, Vladimir Arkhipkin, Denis Dimitrov</strong></p>
<p>Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training. </p>
<blockquote>
<p>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä»ç„¶æ˜¯ç”Ÿæˆè®¡ç®—æœºè§†è§‰çš„åŸºçŸ³ï¼Œä½†å…¶è®­ç»ƒå¸¸å¸¸å—åˆ°ä¼ªå½±çš„å›°æ‰°ï¼Œè¿™äº›ä¼ªå½±ä¼šé™ä½é‡å»ºå’Œç”Ÿæˆè´¨é‡ã€‚æœ¬æ–‡ä»‹ç»äº†VIVATï¼Œè¿™æ˜¯ä¸€ç§ç¼“è§£KL-VAEè®­ç»ƒä¸­å¸¸è§ä¼ªå½±çš„ç³»ç»Ÿæ€§æ–¹æ³•ï¼Œæ— éœ€è¿›è¡Œæ ¹æœ¬æ€§çš„æ¶æ„æ”¹å˜ã€‚æˆ‘ä»¬å¯¹äº”ç§å¸¸è§çš„ä¼ªå½±è¿›è¡Œäº†è¯¦ç»†çš„åˆ†ç±»ï¼ŒåŒ…æ‹¬è‰²å½©åç§»ã€ç½‘æ ¼æ¨¡å¼ã€æ¨¡ç³Šã€è§’è½å’Œæ¶²æ»´ä¼ªå½±ï¼Œå¹¶åˆ†æäº†å®ƒä»¬çš„æ ¹æœ¬åŸå› ã€‚é€šè¿‡ç®€å•çš„ä¿®æ”¹ï¼ŒåŒ…æ‹¬è°ƒæ•´æŸå¤±æƒé‡ã€å¡«å……ç­–ç•¥ä»¥åŠç©ºé—´æ¡ä»¶å½’ä¸€åŒ–çš„é›†æˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†VAEæ€§èƒ½çš„æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å›¾åƒé‡å»ºæŒ‡æ ‡ï¼ˆPSNRå’ŒSSIMï¼‰çš„æœ€æ–°ç»“æœï¼Œå¹¶æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ï¼Œè¿™ä½“ç°åœ¨æ›´é«˜çš„CLIPåˆ†æ•°ä¸Šã€‚VIVATåœ¨ä¿æŒKL-VAEæ¡†æ¶ç®€æ´æ€§çš„åŒæ—¶ï¼Œè§£å†³äº†å…¶å®è·µæŒ‘æˆ˜ï¼Œä¸ºå¸Œæœ›ä¼˜åŒ–VAEè®­ç»ƒçš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VIVATï¼Œä¸€ç§é’ˆå¯¹KL-VAEè®­ç»ƒä¸­å¸¸è§ä¼ªå½±çš„ç³»ç»Ÿæ€§è§£å†³æ–¹æ³•ï¼Œæ— éœ€è¿›è¡Œæ ¹æœ¬æ€§çš„æ¶æ„æ”¹å˜ã€‚æ–‡ç« è¯¦ç»†åˆ†ç±»äº†äº”ç§å¸¸è§çš„ä¼ªå½±ï¼Œåˆ†æå…¶æ ¹æœ¬åŸå› ï¼Œå¹¶é€šè¿‡ç®€å•çš„ä¿®æ”¹ï¼Œå¦‚è°ƒæ•´æŸå¤±æƒé‡ã€å¡«å……ç­–ç•¥å’Œé›†æˆç©ºé—´æ¡ä»¶å½’ä¸€åŒ–ï¼Œæ˜¾è‘—æé«˜äº†VAEçš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒé‡å»ºæŒ‡æ ‡ï¼ˆPSNRå’ŒSSIMï¼‰ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ã€‚VIVATä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†ä¼˜åŒ–VAEè®­ç»ƒçš„å®ç”¨è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIVATæ˜¯ä¸€ç§é’ˆå¯¹KL-VAEè®­ç»ƒä¸­å¸¸è§ä¼ªå½±çš„è§£å†³æ–¹æ³•ã€‚</li>
<li>æ–‡ç« è¯¦ç»†åˆ†ç±»äº†äº”ç§å¸¸è§ä¼ªå½±ï¼šé¢œè‰²åç§»ã€ç½‘æ ¼æ¨¡å¼ã€æ¨¡ç³Šã€è§’è½å’Œæ¶²æ»´ä¼ªå½±ã€‚</li>
<li>é€šè¿‡è°ƒæ•´æŸå¤±æƒé‡ã€æ”¹è¿›å¡«å……ç­–ç•¥å’Œé›†æˆç©ºé—´æ¡ä»¶å½’ä¸€åŒ–ç­‰ç®€å•ä¿®æ”¹ï¼ŒVAEæ€§èƒ½å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>VIVATæ–¹æ³•åœ¨å›¾åƒé‡å»ºæŒ‡æ ‡ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬PSNRå’ŒSSIMã€‚</li>
<li>VIVATæé«˜äº†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ï¼Œè¡¨ç°ä¸ºæ›´é«˜çš„CLIPè¯„åˆ†ã€‚</li>
<li>VIVATä¿ç•™äº†KL-VAEæ¡†æ¶çš„ç®€æ´æ€§ï¼ŒåŒæ—¶è§£å†³äº†å…¶å®è·µä¸­çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d19e073534ec4cf7c244ee318c161c51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff7d4497e961c7b6ac6d4bdd7817d899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b72ad1a3247c1b966bbda24feb42f81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c44bc95103a2b75b2f242ffab89279c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HAIBU-ReMUD-Reasoning-Multimodal-Ultrasound-Dataset-and-Model-Bridging-to-General-Specific-Domains"><a href="#HAIBU-ReMUD-Reasoning-Multimodal-Ultrasound-Dataset-and-Model-Bridging-to-General-Specific-Domains" class="headerlink" title="HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging   to General Specific Domains"></a>HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging   to General Specific Domains</h2><p><strong>Authors:Shijie Wang, Yilun Zhang, Zeyu Lai, Dexing Kong</strong></p>
<p>Multimodal large language models (MLLMs) have shown great potential in general domains but perform poorly in some specific domains due to a lack of domain-specific data, such as image-text data or vedio-text data. In some specific domains, there is abundant graphic and textual data scattered around, but lacks standardized arrangement. In the field of medical ultrasound, there are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic diagnostic reports, and so on. However, these ultrasonic materials are often saved in the forms of PDF, images, etc., and cannot be directly used for the training of MLLMs. This paper proposes a novel image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, and answer) from domain-specific materials. A medical ultrasound domain dataset ReMUD is established, containing over 45,000 reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound field. To facilitate research, the ReMUD dataset, data generation codebase, and ReMUD-7B parameters will be released at <a target="_blank" rel="noopener" href="https://github.com/ShiDaizi/ReMUD">https://github.com/ShiDaizi/ReMUD</a>, addressing the data shortage issue in specific domain MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹ç‰¹å®šé¢†åŸŸçš„æ•°æ®ï¼Œå¦‚å›¾åƒæ–‡æœ¬æ•°æ®æˆ–è§†é¢‘æ–‡æœ¬æ•°æ®ï¼Œåœ¨ä¸€äº›ç‰¹å®šé¢†åŸŸçš„è¡¨ç°è¾ƒå·®ã€‚åœ¨ä¸€äº›ç‰¹å®šé¢†åŸŸï¼Œè™½ç„¶å­˜åœ¨å¤§é‡çš„å›¾å½¢å’Œæ–‡æœ¬æ•°æ®åˆ†æ•£ï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–å®‰æ’ã€‚åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸï¼Œæœ‰è¶…å£°è¯Šæ–­ä¹¦ç±ã€è¶…å£°ä¸´åºŠæŒ‡å—ã€è¶…å£°è¯Šæ–­æŠ¥å‘Šç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›è¶…å£°ææ–™é€šå¸¸ä¿å­˜åœ¨PDFã€å›¾åƒç­‰å½¢å¼ä¸‹ï¼Œæ— æ³•ç›´æ¥ç”¨äºè®­ç»ƒMLLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07837v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç‰¹å®šé¢†åŸŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ•°æ®çŸ­ç¼ºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒæ–‡æœ¬æ¨ç†ç›‘ç£å¾®è°ƒæ•°æ®ç”Ÿæˆæµç¨‹ã€‚é€šè¿‡åˆ›å»ºç‰¹å®šé¢†åŸŸçš„å››å…ƒç»„ï¼ˆå›¾åƒã€é—®é¢˜ã€æ€è€ƒè½¨è¿¹å’Œç­”æ¡ˆï¼‰ï¼Œä»åŒ»å­¦è¶…å£°é¢†åŸŸçš„ä¸“ä¸šèµ„æ–™ä¸­å»ºç«‹äº†ReMUDæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡45ï¼Œ000ä¸ªç”¨äºç›‘ç£å’Œè§†è§‰é—®ç­”çš„æ¨ç†å’Œéæ¨ç†æ•°æ®ã€‚ç»è¿‡åœ¨Qwen2.5-VL-7B-Instructä¸Šå¾®è°ƒçš„ReMUD-7Bæ¨¡å‹åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸè¡¨ç°å‡ºä¼˜äºé€šç”¨é¢†åŸŸMLLMsçš„æ€§èƒ½ã€‚æ•°æ®å’Œèµ„æºå·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨ä¸€èˆ¬é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œç¼ºä¹ç‰¹å®šé¢†åŸŸçš„å›¾åƒæ–‡æœ¬æˆ–è§†é¢‘æ–‡æœ¬æ•°æ®æ˜¯ä¸»è¦é—®é¢˜ã€‚</li>
<li>åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸï¼Œå­˜åœ¨å¤§é‡åˆ†æ•£çš„å›¾å½¢å’Œæ–‡æœ¬æ•°æ®ï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–å®‰æ’ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›å»ºç‰¹å®šé¢†åŸŸæ•°æ®çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå›¾åƒæ–‡æœ¬å››å…ƒç»„ï¼ˆå›¾åƒã€é—®é¢˜ã€æ€è€ƒè½¨è¿¹å’Œç­”æ¡ˆï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>åˆ›å»ºäº†åŒ»å­¦è¶…å£°é¢†åŸŸçš„ReMUDæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡45ï¼Œ000ä¸ªç”¨äºç›‘ç£å’Œè§†è§‰é—®ç­”çš„æ¨ç†å’Œéæ¨ç†æ•°æ®ã€‚</li>
<li>ReMUD-7Bæ¨¡å‹åœ¨åŒ»å­¦è¶…å£°é¢†åŸŸçš„æ€§èƒ½ä¼˜äºé€šç”¨MLLMsã€‚</li>
<li>ReMUDæ•°æ®é›†ã€æ•°æ®ç”Ÿæˆä»£ç åº“å’ŒReMUD-7Bå‚æ•°å·²å…¬å¼€å‘å¸ƒï¼Œä»¥è§£å†³ç‰¹å®šé¢†åŸŸMLLMsçš„æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fffaf4e2bfecb4101d6bd0638af5fda2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b254b8009f955ae9ecd694e298e51103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a48ee1bca24fa0dcfeb6a2c248c61271.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3faa8cf3c8c2284e4bda9dc6eadb2c6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d927056d36a258b4a2b6901d34d6af13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f74b2dee9afddfa7790d0ca53f99c4c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Blind-Super-Resolution-Network-for-Spatial-Specific-and-Spatial-Agnostic-Degradations"><a href="#Adaptive-Blind-Super-Resolution-Network-for-Spatial-Specific-and-Spatial-Agnostic-Degradations" class="headerlink" title="Adaptive Blind Super-Resolution Network for Spatial-Specific and   Spatial-Agnostic Degradations"></a>Adaptive Blind Super-Resolution Network for Spatial-Specific and   Spatial-Agnostic Degradations</h2><p><strong>Authors:Weilei Wen, Chunle Guo, Wenqi Ren, Hongpeng Wang, Xiuli Shao</strong></p>
<p>Prior methodologies have disregarded the diversities among distinct degradation types during image reconstruction, employing a uniform network model to handle multiple deteriorations. Nevertheless, we discover that prevalent degradation modalities, including sampling, blurring, and noise, can be roughly categorized into two classes. We classify the first class as spatial-agnostic dominant degradations, less affected by regional changes in image space, such as downsampling and noise degradation. The second class degradation type is intimately associated with the spatial position of the image, such as blurring, and we identify them as spatial-specific dominant degradations. We introduce a dynamic filter network integrating global and local branches to address these two degradation types. This network can greatly alleviate the practical degradation problem. Specifically, the global dynamic filtering layer can perceive the spatial-agnostic dominant degradation in different images by applying weights generated by the attention mechanism to multiple parallel standard convolution kernels, enhancing the networkâ€™s representation ability. Meanwhile, the local dynamic filtering layer converts feature maps of the image into a spatially specific dynamic filtering operator, which performs spatially specific convolution operations on the image features to handle spatial-specific dominant degradations. By effectively integrating both global and local dynamic filtering operators, our proposed method outperforms state-of-the-art blind super-resolution algorithms in both synthetic and real image datasets. </p>
<blockquote>
<p>å…ˆå‰çš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­å¿½è§†äº†ä¸åŒç±»å‹é€€åŒ–ä¹‹é—´çš„å¤šæ ·æ€§ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„ç½‘ç»œæ¨¡å‹æ¥å¤„ç†å¤šç§é€€åŒ–é—®é¢˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å¸¸è§çš„é€€åŒ–æ¨¡å¼ï¼ŒåŒ…æ‹¬é‡‡æ ·ã€æ¨¡ç³Šå’Œå™ªå£°ï¼Œå¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ç±»ã€‚æˆ‘ä»¬å°†ç¬¬ä¸€ç±»ç§°ä¸ºç©ºé—´æ— å…³ä¸»å¯¼å‹é€€åŒ–ï¼Œè¾ƒå°‘å—åˆ°å›¾åƒç©ºé—´åŒºåŸŸå˜åŒ–çš„å½±å“ï¼Œå¦‚ä¸‹é‡‡æ ·å’Œå™ªå£°é€€åŒ–ã€‚ç¬¬äºŒç±»é€€åŒ–ç±»å‹ä¸å›¾åƒçš„ç©ºé—´ä½ç½®å¯†åˆ‡ç›¸å…³ï¼Œå¦‚æ¨¡ç³Šï¼Œæˆ‘ä»¬å°†å…¶è¯†åˆ«ä¸ºç©ºé—´ç‰¹å®šä¸»å¯¼å‹é€€åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€æ»¤æ³¢ç½‘ç»œï¼Œèåˆäº†å…¨å±€å’Œå±€éƒ¨åˆ†æ”¯ï¼Œä»¥è§£å†³è¿™ä¸¤ç§é€€åŒ–é—®é¢˜ã€‚è¯¥ç½‘ç»œå¯ä»¥å¤§å¤§ç¼“è§£å®é™…é€€åŒ–é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œå…¨å±€åŠ¨æ€æ»¤æ³¢å±‚é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆçš„æƒé‡å¯¹å¤šä¸ªå¹¶è¡Œæ ‡å‡†å·ç§¯æ ¸è¿›è¡Œæ„ŸçŸ¥ï¼Œæ¥æ„ŸçŸ¥ä¸åŒå›¾åƒä¸­çš„ç©ºé—´æ— å…³ä¸»å¯¼å‹é€€åŒ–ï¼Œå¢å¼ºäº†ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›ã€‚åŒæ—¶ï¼Œå±€éƒ¨åŠ¨æ€æ»¤æ³¢å±‚å°†å›¾åƒçš„ç‰¹å¾å›¾è½¬æ¢ä¸ºç©ºé—´ç‰¹å®šçš„åŠ¨æ€æ»¤æ³¢ç®—å­ï¼Œå¯¹å›¾åƒç‰¹å¾è¿›è¡Œç©ºé—´ç‰¹å®šçš„å·ç§¯æ“ä½œï¼Œä»¥å¤„ç†ç©ºé—´ç‰¹å®šä¸»å¯¼å‹é€€åŒ–ã€‚é€šè¿‡æœ‰æ•ˆåœ°èåˆå…¨å±€å’Œå±€éƒ¨åŠ¨æ€æ»¤æ³¢ç®—å­ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®å›¾åƒæ•°æ®é›†ä¸Šéƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç›²è¶…åˆ†è¾¨ç‡ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07705v1">PDF</a> IEEE TRANSACTIONS ON IMAGE PROCESSING</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨å¤„ç†å›¾åƒé‡å»ºä¸­çš„ä¸åŒé€€åŒ–ç±»å‹æ—¶ï¼Œå…ˆå‰çš„æ–¹æ³•å¿½ç•¥äº†å…¶å¤šæ ·æ€§ï¼Œä½¿ç”¨ç»Ÿä¸€ç½‘ç»œæ¨¡å‹å¤„ç†å¤šç§é€€åŒ–é—®é¢˜ã€‚ç„¶è€Œï¼Œè¯¥æ–‡å°†å¸¸è§çš„é€€åŒ–æ¨¡å¼ï¼ˆå¦‚é‡‡æ ·ã€æ¨¡ç³Šå’Œå™ªå£°ï¼‰åˆ†ä¸ºä¸¤ç±»ï¼šç©ºé—´ä¸å¯çŸ¥å‹é€€åŒ–ä¸ç©ºé—´ç‰¹å®šå‹é€€åŒ–ã€‚é’ˆå¯¹ä¸åŒé€€åŒ–ç±»å‹ï¼Œè¯¥æ–‡å¼•å…¥äº†ä¸€ä¸ªç»“åˆå…¨å±€å’Œå±€éƒ¨åˆ†æ”¯çš„åŠ¨æ€æ»¤æ³¢å™¨ç½‘ç»œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥ç½‘ç»œé€šè¿‡å…¨å±€åŠ¨æ€æ»¤æ³¢å±‚æ„ŸçŸ¥ä¸åŒå›¾åƒä¸­çš„ç©ºé—´ä¸å¯çŸ¥å‹é€€åŒ–ï¼Œé€šè¿‡å±€éƒ¨åŠ¨æ€æ»¤æ³¢å±‚å¤„ç†ç©ºé—´ç‰¹å®šå‹é€€åŒ–ã€‚é€šè¿‡ä¸¤è€…çš„æœ‰æ•ˆç»“åˆï¼Œè¯¥æ–‡æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®å›¾åƒæ•°æ®é›†ä¸Šå‡è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç›²è¶…åˆ†è¾¨ç‡ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­å¯¹å›¾åƒé‡å»ºä¸­çš„é€€åŒ–ç±»å‹è¿›è¡Œäº†åˆ†ç±»ï¼ŒåŒ…æ‹¬ç©ºé—´ä¸å¯çŸ¥å‹é€€åŒ–å’Œç©ºé—´ç‰¹å®šå‹é€€åŒ–ã€‚</li>
<li>ç©ºé—´ä¸å¯çŸ¥å‹é€€åŒ–åŒ…æ‹¬é‡‡æ ·å’Œå™ªå£°é™è§£ç­‰ï¼Œå—å›¾åƒç©ºé—´åŒºåŸŸå˜åŒ–å½±å“è¾ƒå°ã€‚</li>
<li>ç©ºé—´ç‰¹å®šå‹é€€åŒ–å¦‚æ¨¡ç³Šä¸ç©ºé—´ä½ç½®ç´§å¯†ç›¸å…³ã€‚</li>
<li>å¼•å…¥çš„åŠ¨æ€æ»¤æ³¢å™¨ç½‘ç»œç»“åˆäº†å…¨å±€å’Œå±€éƒ¨åˆ†æ”¯æ¥å¤„ç†è¿™ä¸¤ç§é€€åŒ–ç±»å‹ã€‚</li>
<li>å…¨å±€åŠ¨æ€æ»¤æ³¢å±‚é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ç”Ÿæˆçš„æƒé‡åº”ç”¨äºå¤šä¸ªå¹¶è¡Œæ ‡å‡†å·ç§¯æ ¸ï¼Œä»¥æ„ŸçŸ¥ç©ºé—´ä¸å¯çŸ¥å‹é€€åŒ–ã€‚</li>
<li>å±€éƒ¨åŠ¨æ€æ»¤æ³¢å±‚å°†å›¾åƒçš„ç‰¹å¾å›¾è½¬æ¢ä¸ºç©ºé—´ç‰¹å®šçš„åŠ¨æ€æ»¤æ³¢ç®—å­ï¼Œå¤„ç†ç©ºé—´ç‰¹å®šå‹é€€åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dea26ed964f96fb07876b76b7fd882da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdc4c5f24e43657a76fc4828950196f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf6486cb47f2224faacc6c0e6c9e4a62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50a01ba4dbe13a52aba79e8f79a7ac5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FMaMIL-Frequency-Driven-Mamba-Multi-Instance-Learning-for-Weakly-Supervised-Lesion-Segmentation-in-Medical-Images"><a href="#FMaMIL-Frequency-Driven-Mamba-Multi-Instance-Learning-for-Weakly-Supervised-Lesion-Segmentation-in-Medical-Images" class="headerlink" title="FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly   Supervised Lesion Segmentation in Medical Images"></a>FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly   Supervised Lesion Segmentation in Medical Images</h2><p><strong>Authors:Hangbei Cheng, Xiaorong Dong, Xueyu Liu, Jianan Zhang, Xuetao Ma, Mingqiang Wei, Liansheng Wang, Junxin Chen, Yongfei Wu</strong></p>
<p>Accurate lesion segmentation in histopathology images is essential for diagnostic interpretation and quantitative analysis, yet it remains challenging due to the limited availability of costly pixel-level annotations. To address this, we propose FMaMIL, a novel two-stage framework for weakly supervised lesion segmentation based solely on image-level labels. In the first stage, a lightweight Mamba-based encoder is introduced to capture long-range dependencies across image patches under the MIL paradigm. To enhance spatial sensitivity and structural awareness, we design a learnable frequency-domain encoding module that supplements spatial-domain features with spectrum-based information. CAMs generated in this stage are used to guide segmentation training. In the second stage, we refine the initial pseudo labels via a CAM-guided soft-label supervision and a self-correction mechanism, enabling robust training even under label noise. Extensive experiments on both public and private histopathology datasets demonstrate that FMaMIL outperforms state-of-the-art weakly supervised methods without relying on pixel-level annotations, validating its effectiveness and potential for digital pathology applications. </p>
<blockquote>
<p>åœ¨ç—…ç†å›¾åƒä¸­ï¼Œç²¾ç¡®åœ°åˆ†å‰²ç—…ç¶å¯¹äºè¯Šæ–­è§£è¯»å’Œå®šé‡åˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ˜‚è´µçš„åƒç´ çº§æ ‡æ³¨çš„æœ‰é™å¯ç”¨æ€§ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FMaMILï¼Œè¿™æ˜¯ä¸€ç§ä»…åŸºäºå›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£ç—…ç¶åˆ†å‰²çš„ä¸¤é˜¶æ®µæ–°å‹æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºMambaçš„è½»é‡çº§ç¼–ç å™¨ï¼Œåœ¨MILèŒƒå¼ä¸‹æ•è·å›¾åƒè¡¥ä¸ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–æ€§ã€‚ä¸ºäº†æé«˜ç©ºé—´æ•æ„Ÿæ€§å’Œç»“æ„æ„è¯†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„é¢‘åŸŸç¼–ç æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥è¡¥å……ç©ºé—´åŸŸç‰¹å¾ä»¥åŸºäºå…‰è°±çš„ä¿¡æ¯ã€‚æœ¬é˜¶æ®µç”Ÿæˆçš„CAMç”¨äºæŒ‡å¯¼åˆ†å‰²è®­ç»ƒã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡CAMå¼•å¯¼çš„è½¯ä»¶æ ‡ç­¾ç›‘ç£å’Œè‡ªæˆ‘æ ¡æ­£æœºåˆ¶æ¥ä¼˜åŒ–åˆå§‹çš„ä¼ªæ ‡ç­¾ï¼Œå³ä½¿åœ¨æ ‡ç­¾å™ªå£°ä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥çš„è®­ç»ƒã€‚åœ¨å…¬å…±å’Œç§æœ‰ç—…ç†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFMaMILåœ¨ä¸éœ€è¦åƒç´ çº§æ³¨é‡Šçš„æƒ…å†µä¸‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„å¼±ç›‘ç£æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨æ•°å­—ç—…ç†åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07652v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒä¸­çš„ç²¾å‡†ç—…å˜åˆ†å‰²å¯¹äºè¯Šæ–­è§£è¯»å’Œå®šé‡åˆ†æè‡³å…³é‡è¦ï¼Œä½†å—é™äºæ˜‚è´µçš„åƒç´ çº§æ ‡æ³¨çš„å¯ç”¨æ€§ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ä»…åŸºäºå›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£ç—…å˜åˆ†å‰²çš„ä¸¤é˜¶æ®µæ¡†æ¶FMaMILã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥åŸºäºMambaçš„è½»é‡çº§ç¼–ç å™¨ï¼Œåœ¨MILèŒƒå¼ä¸‹æ•è·å›¾åƒè¡¥ä¸é—´çš„é•¿ç¨‹ä¾èµ–æ€§ã€‚ä¸ºæé«˜ç©ºé—´æ•æ„Ÿæ€§å’Œç»“æ„æ„è¯†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯å­¦ä¹ çš„é¢‘åŸŸç¼–ç æ¨¡å—ï¼Œä»¥è¡¥å……åŸºäºç©ºé—´åŸŸçš„ç‰¹å¾å¹¶åŠ å…¥åŸºäºé¢‘è°±çš„ä¿¡æ¯ã€‚æœ¬é˜¶æ®µç”Ÿæˆçš„CAMsç”¨äºæŒ‡å¯¼åˆ†å‰²è®­ç»ƒã€‚ç¬¬äºŒé˜¶æ®µé€šè¿‡CAMå¼•å¯¼çš„è½¯æ ‡ç­¾ç›‘ç£å’Œè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ï¼Œå¯¹åˆå§‹ä¼ªæ ‡ç­¾è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œå³ä½¿åœ¨æ ‡ç­¾å™ªå£°ä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥è®­ç»ƒã€‚åœ¨å…¬å…±å’Œç§æœ‰ç—…ç†å­¦æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFMaMILåœ¨ä¸ä¾èµ–åƒç´ çº§æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œä¼˜äºæœ€å…ˆè¿›å¼±ç›‘ç£æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æ•ˆæœå’Œæ•°å­—åŒ–ç—…ç†å­¦åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­çš„ç—…å˜åˆ†å‰²å¯¹äºè¯Šæ–­è§£è¯»å’Œå®šé‡åˆ†æéå¸¸é‡è¦ã€‚</li>
<li>ç”±äºåƒç´ çº§æ ‡æ³¨æˆæœ¬é«˜æ˜‚ï¼Œç—…å˜åˆ†å‰²ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µå¼±ç›‘ç£ç—…å˜åˆ†å‰²æ¡†æ¶FMaMILï¼Œä»…ä¾èµ–å›¾åƒçº§æ ‡ç­¾ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡Mambaç¼–ç å™¨æ•è·å›¾åƒè¡¥ä¸é—´çš„é•¿ç¨‹ä¾èµ–æ€§ï¼Œå¹¶è®¾è®¡é¢‘åŸŸç¼–ç æ¨¡å—æé«˜ç©ºé—´æ•æ„Ÿæ€§å’Œç»“æ„æ„è¯†ã€‚</li>
<li>CAMsåœ¨ç¬¬ä¸€é˜¶æ®µç”¨äºæŒ‡å¯¼åˆ†å‰²è®­ç»ƒã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡è½¯æ ‡ç­¾ç›‘ç£å’Œè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ç²¾ç»†åŒ–åˆå§‹ä¼ªæ ‡ç­¾ï¼Œå®ç°ç¨³å¥è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb6cc1ff3dc53d917dd9a32aae3ff894.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec451d070566da8032e883a1f78b53be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f523b3d4f35f6700c949454a2b7bee8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ebfc49e088e032c9f34cd1a88dd068d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FAMSeg-Fetal-Femur-and-Cranial-Ultrasound-Segmentation-Using-Feature-Aware-Attention-and-Mamba-Enhancement"><a href="#FAMSeg-Fetal-Femur-and-Cranial-Ultrasound-Segmentation-Using-Feature-Aware-Attention-and-Mamba-Enhancement" class="headerlink" title="FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using   Feature-Aware Attention and Mamba Enhancement"></a>FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using   Feature-Aware Attention and Mamba Enhancement</h2><p><strong>Authors:Jie He, Minglang Chen, Minying Lu, Bocheng Liang, Junming Wei, Guiyan Peng, Jiaxi Chen, Ying Tan</strong></p>
<p>Accurate ultrasound image segmentation is a prerequisite for precise biometrics and accurate assessment. Relying on manual delineation introduces significant errors and is time-consuming. However, existing segmentation models are designed based on objects in natural scenes, making them difficult to adapt to ultrasound objects with high noise and high similarity. This is particularly evident in small object segmentation, where a pronounced jagged effect occurs. Therefore, this paper proposes a fetal femur and cranial ultrasound image segmentation model based on feature perception and Mamba enhancement to address these challenges. Specifically, a longitudinal and transverse independent viewpoint scanning convolution block and a feature perception module were designed to enhance the ability to capture local detail information and improve the fusion of contextual information. Combined with the Mamba-optimized residual structure, this design suppresses the interference of raw noise and enhances local multi-dimensional scanning. The system builds global information and local feature dependencies, and is trained with a combination of different optimizers to achieve the optimal solution. After extensive experimental validation, the FAMSeg network achieved the fastest loss reduction and the best segmentation performance across images of varying sizes and orientations. </p>
<blockquote>
<p>ç²¾ç¡®çš„è¶…å£°å›¾åƒåˆ†å‰²æ˜¯ç²¾ç¡®ç”Ÿç‰©æµ‹å®šå’Œå‡†ç¡®è¯„ä¼°çš„å‰æã€‚ä¾èµ–æ‰‹åŠ¨æç»˜ä¼šäº§ç”Ÿé‡å¤§è¯¯å·®ï¼Œä¸”è€—æ—¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åˆ†å‰²æ¨¡å‹æ˜¯åŸºäºè‡ªç„¶åœºæ™¯ä¸­çš„å¯¹è±¡è®¾è®¡çš„ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥é€‚åº”é«˜å™ªå£°å’Œé«˜ç›¸ä¼¼æ€§çš„è¶…å£°å¯¹è±¡ã€‚è¿™åœ¨å°å¯¹è±¡åˆ†å‰²ä¸­å°¤å…¶æ˜æ˜¾ï¼Œä¼šå‡ºç°æ˜æ˜¾çš„é”¯é½¿æ•ˆåº”ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾æ„ŸçŸ¥å’ŒMambaå¢å¼ºçš„èƒå„¿è‚¡éª¨å’Œé¢…è¶…å£°å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†çºµå‘å’Œæ¨ªå‘ç‹¬ç«‹è§‚ç‚¹æ‰«æå·ç§¯å—å’Œç‰¹å¾æ„ŸçŸ¥æ¨¡å—ï¼Œä»¥å¢å¼ºæ•è·å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶æ”¹å–„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èåˆã€‚ç»“åˆç»è¿‡Mambaä¼˜åŒ–çš„æ®‹å·®ç»“æ„ï¼Œè¿™ç§è®¾è®¡æŠ‘åˆ¶äº†åŸå§‹å™ªå£°çš„å¹²æ‰°ï¼Œå¢å¼ºäº†å±€éƒ¨å¤šç»´æ‰«æã€‚è¯¥ç³»ç»Ÿå»ºç«‹å…¨å±€ä¿¡æ¯å’Œå±€éƒ¨ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆä¸åŒçš„ä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒFAMSegç½‘ç»œå®ç°äº†æœ€å¿«çš„æŸå¤±å‡å°‘å’Œæœ€ä½³çš„åˆ†å‰²æ€§èƒ½ï¼Œé€‚ç”¨äºä¸åŒå¤§å°å’Œæ–¹å‘çš„å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07431v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç‰¹å¾æ„ŸçŸ¥å’ŒMambaå¢å¼ºçš„èƒå„¿è‚¡éª¨å’Œé¢…éª¨è¶…å£°å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹éš¾ä»¥é€‚åº”é«˜å™ªå£°å’Œé«˜ç›¸ä¼¼åº¦çš„è¶…å£°å¯¹è±¡çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹è®¾è®¡äº†ä¸€ä¸ªçºµå‘å’Œæ¨ªå‘ç‹¬ç«‹è§†ç‚¹æ‰«æå·ç§¯å—å’Œç‰¹å¾æ„ŸçŸ¥æ¨¡å—ï¼Œå¢å¼ºäº†æ•æ‰å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä¼˜åŒ–äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èåˆã€‚ç»“åˆMambaä¼˜åŒ–æ®‹å·®ç»“æ„ï¼Œè¯¥è®¾è®¡æŠ‘åˆ¶äº†åŸå§‹å™ªå£°çš„å¹²æ‰°ï¼Œæé«˜äº†å±€éƒ¨å¤šç»´æ‰«æèƒ½åŠ›ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼ŒFAMSegç½‘ç»œåœ¨ä¸åŒå¤§å°å’Œæ–¹å‘çš„å›¾åƒä¸Šå®ç°äº†æœ€å¿«çš„æŸå¤±é™ä½å’Œæœ€ä½³çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å¯¹äºç”Ÿç‰©è®¡é‡å­¦å’Œè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åº”å¯¹é«˜å™ªå£°å’Œé«˜ç›¸ä¼¼åº¦çš„è¶…å£°å¯¹è±¡æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾æ„ŸçŸ¥å’ŒMambaå¢å¼ºçš„èƒå„¿è‚¡éª¨å’Œé¢…éª¨è¶…å£°å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡è®¾è®¡çºµå‘å’Œæ¨ªå‘ç‹¬ç«‹è§†ç‚¹æ‰«æå·ç§¯å—ï¼Œå¢å¼ºäº†æ•æ‰å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>Mambaä¼˜åŒ–æ®‹å·®ç»“æ„æœ‰åŠ©äºæŠ‘åˆ¶åŸå§‹å™ªå£°å¹²æ‰°ï¼Œæé«˜å±€éƒ¨å¤šç»´æ‰«æèƒ½åŠ›ã€‚</li>
<li>FAMSegç½‘ç»œç»è¿‡è®­ç»ƒåï¼Œå®ç°äº†å¿«é€Ÿçš„æŸå¤±é™ä½å’Œå‡ºè‰²çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9a6ef1e1f413a2e0bfefd71c7931b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1c73982632542b13bac20264fd9e87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef98628f170c202ef7a91771694060b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="C3S3-Complementary-Competition-and-Contrastive-Selection-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#C3S3-Complementary-Competition-and-Contrastive-Selection-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="C3S3: Complementary Competition and Contrastive Selection for   Semi-Supervised Medical Image Segmentation"></a>C3S3: Complementary Competition and Contrastive Selection for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Jiaying He, Yitong Lin, Jiahe Chen, Honghui Xu, Jianwei Zheng</strong></p>
<p>For the immanent challenge of insufficiently annotated samples in the medical field, semi-supervised medical image segmentation (SSMIS) offers a promising solution. Despite achieving impressive results in delineating primary target areas, most current methodologies struggle to precisely capture the subtle details of boundaries. This deficiency often leads to significant diagnostic inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised segmentation model that synergistically integrates complementary competition and contrastive selection. This design significantly sharpens boundary delineation and enhances overall precision. Specifically, we develop an $\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining boundary localization. Additionally, we incorporate a $\textit{Dynamic Complementary Competition}$ module that leverages two high-performing sub-networks to generate pseudo-labels, thereby further improving segmentation quality. The proposed C3S3 undergoes rigorous validation on two publicly accessible datasets, encompassing the practices of both MRI and CT scans. The results demonstrate that our method achieves superior performance compared to previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our approach achieves a notable improvement of at least $6%$, highlighting the significant advancements. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Y-TARL/C3S3">https://github.com/Y-TARL/C3S3</a>. </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦é¢†åŸŸæ ‡æ³¨æ ·æœ¬ä¸è¶³è¿™ä¸€è¿«åœ¨çœ‰ç«çš„æŒ‘æˆ˜ï¼ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡åœ¨å‹¾ç”»ä¸»è¦ç›®æ ‡åŒºåŸŸæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å¤§å¤šæ•°å½“å‰çš„æ–¹æ³•åœ¨ç²¾ç¡®æ•æ‰è¾¹ç•Œçš„ç»†å¾®ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™ç§ç¼ºé™·é€šå¸¸ä¼šå¯¼è‡´è¯Šæ–­ä¸Šçš„é‡å¤§è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†C3S3ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŠç›‘ç£åˆ†å‰²æ¨¡å‹ï¼Œå®ƒååŒæ•´åˆäº†äº’è¡¥ç«äº‰å’Œå¯¹æ¯”é€‰æ‹©ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—åœ°æé«˜äº†è¾¹ç•Œçš„å‹¾ç”»ç²¾åº¦ï¼Œå¹¶å¢å¼ºäº†æ•´ä½“çš„ç²¾ç¡®åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸“é—¨çš„â€œç»“æœé©±åŠ¨å¯¹æ¯”å­¦ä¹ â€æ¨¡å—ï¼Œç”¨äºæ”¹è¿›è¾¹ç•Œå®šä½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èå…¥äº†ä¸€ä¸ªâ€œåŠ¨æ€äº’è¡¥ç«äº‰â€æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ä¸¤ä¸ªé«˜æ€§èƒ½å­ç½‘ç»œæ¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜äº†åˆ†å‰²è´¨é‡ã€‚æ‰€æå‡ºçš„C3S3åœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ï¼Œæ¶µç›–äº†MRIå’ŒCTæ‰«æçš„å®è·µã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºå…ˆå‰çš„é¡¶å°–ç«äº‰å¯¹æ‰‹å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨95HDå’ŒASDæŒ‡æ ‡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è‡³å°‘6%çš„æ˜¾è‘—æ”¹è¿›ï¼Œå‡¸æ˜¾äº†é‡å¤§çš„è¿›å±•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Y-TARL/C3S">https://github.com/Y-TARL/C3S</a> è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07368v1">PDF</a> 6 pages, 4 figures, ICME2025</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰å¯¹äºåŒ»å­¦é¢†åŸŸä¸­æ ·æœ¬æ ‡æ³¨ä¸è¶³çš„é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•åœ¨ç²¾ç¡®æ•æ‰è¾¹ç•Œç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´è¯Šæ–­ä¸å‡†ç¡®ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†C3S3æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èåˆäº†äº’è¡¥ç«äº‰å’Œå¯¹æ¯”é€‰æ‹©æœºåˆ¶ï¼Œæé«˜äº†è¾¹ç•Œå‹¾å‹’çš„ç²¾ç¡®åº¦ã€‚é€šè¿‡ç»“æœé©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ æ¨¡å—å’ŒåŠ¨æ€äº’è¡¥ç«äº‰æ¨¡å—ï¼ŒC3S3åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–é¡¶å°–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨95HDå’ŒASDæŒ‡æ ‡ä¸Šï¼Œæ”¹è¿›è‡³å°‘6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ ·æœ¬æ ‡æ³¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ•æ‰åŒ»å­¦å›¾åƒè¾¹ç•Œç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´è¯Šæ–­ä¸å‡†ç¡®ã€‚</li>
<li>C3S3æ¨¡å‹é€šè¿‡èåˆäº’è¡¥ç«äº‰å’Œå¯¹æ¯”é€‰æ‹©æœºåˆ¶æ¥æé«˜è¾¹ç•Œå‹¾å‹’çš„ç²¾ç¡®åº¦ã€‚</li>
<li>C3S3æ¨¡å‹åŒ…æ‹¬ç»“æœé©±åŠ¨çš„å¯¹æ¯”å­¦ä¹ æ¨¡å—å’ŒåŠ¨æ€äº’è¡¥ç«äº‰æ¨¡å—ã€‚</li>
<li>C3S3åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–é¡¶å°–æ–¹æ³•ã€‚</li>
<li>C3S3åœ¨95HDå’ŒASDæŒ‡æ ‡ä¸Šçš„æ”¹è¿›è‡³å°‘ä¸º6%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c3005ee20960f03d62fd66584cd43830.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fef6d69631480ef26f7b36de351b39c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b9c37a653b3601863b124337903c75f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86db6b50f76f6c9a229871490c79d7ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7d7ae36a9e0a0a7745fcce72c182b4a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Advancing-Waterfall-Plots-for-Cancer-Treatment-Response-Assessment-through-Adjustment-of-Incomplete-Follow-Up-Time"><a href="#Advancing-Waterfall-Plots-for-Cancer-Treatment-Response-Assessment-through-Adjustment-of-Incomplete-Follow-Up-Time" class="headerlink" title="Advancing Waterfall Plots for Cancer Treatment Response Assessment   through Adjustment of Incomplete Follow-Up Time"></a>Advancing Waterfall Plots for Cancer Treatment Response Assessment   through Adjustment of Incomplete Follow-Up Time</h2><p><strong>Authors: Zhe,  Wang, Linda Z. Sun, Cong Chen</strong></p>
<p>Waterfall plots are a key tool in early phase oncology clinical studies for visualizing individual patientsâ€™ tumor size changes and provide efficacy assessment. However, comparing waterfall plots from ongoing studies with limited follow-up to those from completed studies with long follow-up is challenging due to underestimation of tumor response in ongoing patients. To address this, we propose a novel adjustment method that projects the waterfall plot of an ongoing study to approximate its appearance with sufficient follow-up. Recognizing that waterfall plots are simply rotated survival functions of best tumor size reduction from the baseline (in percentage), we frame the problem in a survival analysis context and adjust weight of each ongoing patients in an interim look Kaplan-Meier curve by leveraging the probability of potential tumor response improvement (i.e., â€œcensoringâ€). The probability of improvement is quantified through an incomplete multinomial model to estimate the best tumor size change occurrence at each scan time. The adjusted waterfall plots of experimental treatments from ongoing studies are suitable for comparison with historical controls from completed studies, without requiring individual-level data of those controls. A real-data example demonstrates the utility of this method for robust efficacy evaluations. </p>
<blockquote>
<p>ç€‘å¸ƒå›¾ï¼ˆWaterfall plotsï¼‰æ˜¯è‚¿ç˜¤å­¦æ—©æœŸé˜¶æ®µä¸´åºŠç ”ç©¶ä¸­ç”¨äºå¯è§†åŒ–ä¸ªåˆ«æ‚£è€…è‚¿ç˜¤å¤§å°å˜åŒ–å¹¶è¿›è¡Œç–—æ•ˆè¯„ä¼°çš„å…³é”®å·¥å…·ã€‚ç„¶è€Œï¼Œç”±äºæ­£åœ¨è¿›è¡Œçš„æ‚£è€…çš„è‚¿ç˜¤ååº”ä¼°è®¡ä¸è¶³ï¼Œå°†æ­£åœ¨è¿›è¡Œç ”ç©¶çš„ç€‘å¸ƒå›¾ä¸å·²å®Œæˆç ”ç©¶çš„ç€‘å¸ƒå›¾è¿›è¡Œæ¯”è¾ƒæ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è°ƒæ•´æ–¹æ³•ï¼Œå°†æ­£åœ¨è¿›è¡Œçš„ç€‘å¸ƒå›¾æŠ•å½±ä»¥è¿‘ä¼¼å…¶å……åˆ†éšè®¿åçš„å¤–è§‚ã€‚è®¤è¯†åˆ°ç€‘å¸ƒå›¾å®é™…ä¸Šæ˜¯åŸºçº¿æœ€ä½³è‚¿ç˜¤ç¼©å°å°ºå¯¸çš„æ—‹è½¬ç”Ÿå­˜å‡½æ•°ï¼ˆä»¥ç™¾åˆ†æ¯”è¡¨ç¤ºï¼‰ï¼Œæˆ‘ä»¬åœ¨ç”Ÿå­˜åˆ†æçš„èƒŒæ™¯ä¸‹å®šä¹‰é—®é¢˜ï¼Œå¹¶åˆ©ç”¨æ½œåœ¨çš„è‚¿ç˜¤ååº”æ”¹å–„çš„å¯èƒ½æ€§æ¥è°ƒæ•´ä¸´æ—¶æŸ¥çœ‹Kaplan-Meieræ›²çº¿ä¸­æ¯ä½æ‚£è€…çš„æƒé‡ï¼ˆå³â€œå®¡æŸ¥â€ï¼‰ã€‚æ”¹è¿›çš„æ¦‚ç‡æ˜¯é€šè¿‡ä¸å®Œæ•´çš„å¤šé¡¹æ¨¡å‹é‡åŒ–çš„ï¼Œç”¨äºä¼°è®¡æ¯æ¬¡æ‰«ææ—¶é—´æœ€ä½³è‚¿ç˜¤å¤§å°å˜åŒ–çš„å‘ç”Ÿæƒ…å†µã€‚ä½¿ç”¨æ­¤æ–¹æ³•è°ƒæ•´æ­£åœ¨è¿›è¡Œç ”ç©¶çš„å®éªŒæ²»ç–—ç€‘å¸ƒå›¾ï¼Œå¯å°†å…¶ä¸å·²å®Œæˆç ”ç©¶çš„å¯¹ç…§ç»„è¿›è¡Œæ¯”è¾ƒï¼Œæ— éœ€è·å–å¯¹ç…§ç»„çš„ä¸ªä½“çº§æ•°æ®ã€‚ä¸€ä¸ªå®é™…æ•°æ®çš„ä¾‹å­å±•ç¤ºäº†è¿™ç§æ–¹æ³•åœ¨ç¨³å¥ç–—æ•ˆè¯„ä¼°ä¸­çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07365v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä½¿ç”¨ç€‘å¸ƒå›¾ä½œä¸ºæ—©æœŸè‚¿ç˜¤ä¸´åºŠè¯•éªŒä¸­çš„å…³é”®å·¥å…·ï¼Œå±•ç¤ºæ‚£è€…çš„è‚¿ç˜¤å¤§å°å˜åŒ–ä»¥è¯„ä¼°æ²»ç–—æ•ˆæœã€‚å¯¹äºæ­£åœ¨è¿›è¡Œçš„è‚¿ç˜¤ç ”ç©¶ä¸å·²å®Œæˆçš„ç ”ç©¶çš„ç€‘å¸ƒå›¾æ¯”è¾ƒä¸­å­˜åœ¨é—®é¢˜ï¼Œå¯¹æ­¤ä½œè€…æå‡ºä¸€ç§æ–°é¢–çš„æ ¡å‡†æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹æ­£åœ¨è¿›è¡Œçš„ç€‘å¸ƒå›¾ï¼Œä½¿å…¶åœ¨æœªæ¥æœ‰è¶³å¤Ÿçš„è·Ÿè¸ªæ—¶é—´æ—¶æ¥è¿‘å®é™…çŠ¶å†µã€‚åˆ©ç”¨ç”Ÿå­˜åˆ†ææ¡†æ¶å°†é—®é¢˜è°ƒæ•´ç”Ÿå­˜å‡½æ•°çš„æ¦‚ç‡æ¥è®¡ç®—å„ä¸ªç—…æ‚£æœ€ä½³è‚¿ç˜¤å‡å°æ•ˆæœçš„æƒé‡è°ƒæ•´ã€‚æ¨¡å‹å¯¹æ”¹è¿›å¯èƒ½æ€§çš„ä¼°è®¡ä¸»è¦ä¾èµ–ä¸å®Œå…¨å¤šé¡¹æ¨¡å‹ï¼Œå¯¹æ¯æ¬¡æ‰«æçš„æœ€ä½³è‚¿ç˜¤å¤§å°å˜åŒ–è¿›è¡Œä¼°è®¡ã€‚ç»è¿‡æ ¡å‡†çš„æ­£åœ¨è¿›è¡Œçš„è¯•éªŒçš„ç€‘å¸ƒå›¾ä¸å·²å®Œæˆçš„å†å²å¯¹ç…§ç ”ç©¶ç›¸æ¯”ï¼Œæ— éœ€ä¸ªä½“å±‚é¢çš„æ•°æ®ã€‚ä¸€ä¸ªçœŸå®çš„æ•°æ®ç¤ºä¾‹å±•ç¤ºäº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ï¼Œä¸ºç¨³å¥çš„æ•ˆç”¨è¯„ä¼°æä¾›äº†é€”å¾„ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€Waterfall plotsåœ¨æ—©æœŸé˜¶æ®µè‚¿ç˜¤ä¸´åºŠç ”ç©¶ä¸­ç”¨äºå¯è§†åŒ–ä¸ªåˆ«æ‚£è€…çš„è‚¿ç˜¤å¤§å°å˜åŒ–ï¼Œä»¥è¯„ä¼°æ²»ç–—æ•ˆæœã€‚<br>äºŒã€æ¯”è¾ƒä¸åŒè·Ÿè¸ªæ—¶é—´çš„ç€‘å¸ƒå›¾å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºæ­£åœ¨è¿›è¡Œçš„æ‚£è€…çš„è‚¿ç˜¤ååº”å¯èƒ½è¢«ä½ä¼°ã€‚<br>ä¸‰ã€æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è°ƒæ•´æ–¹æ³•ï¼Œé¢„æµ‹æ­£åœ¨è¿›è¡Œçš„ç€‘å¸ƒå›¾ä»¥æ¨¡æ‹Ÿå…¶å……åˆ†è·Ÿè¸ªæ—¶çš„è¡¨ç°ã€‚<br>å››ã€è¯¥é—®é¢˜è¢«ç½®äºç”Ÿå­˜åˆ†æçš„æ¡†æ¶å†…ï¼Œåˆ©ç”¨ç”Ÿå­˜å‡½æ•°çš„æ¦‚ç‡æ¥è°ƒæ•´æ‚£è€…æƒé‡çš„è°ƒæ•´æ–¹æ³•ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-237bc55b70cf1f1bfbe2c9b19ae477e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47d5abbc0afb604a0c18e6cc13917b4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3efdff30b3611b3cd0da6d8c3dc246d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="XRISM-Spectroscopy-of-the-Stellar-Mass-Black-Hole-4U-1630-472-in-Outburst"><a href="#XRISM-Spectroscopy-of-the-Stellar-Mass-Black-Hole-4U-1630-472-in-Outburst" class="headerlink" title="XRISM Spectroscopy of the Stellar-Mass Black Hole 4U 1630-472 in   Outburst"></a>XRISM Spectroscopy of the Stellar-Mass Black Hole 4U 1630-472 in   Outburst</h2><p><strong>Authors:Jon M. Miller, Misaki Mizumoto, Megumi Shidatsu, Ralf Ballhausen, Ehud Behar, Maria Diaz Trigo, Chris Done, Tadayasu Dotani, Javier Garcia, Timothy Kallman, Shogo B. Kobayashi, Aya Kubota, Randall Smith, Hiromitsu Takahashi, Makoto Tashiro, Yoshihiro Ueda, Jacco Vink, Shinya Yamada, Shin Watanabe, Ryo Iizuka, Yukikatsu Terada, Chris Baluta, Yoshiaki Kanemaru, Shoji Ogawa, Tessei Yoshida, Katsuhiro Hayashi</strong></p>
<p>We report on XRISM&#x2F;Resolve spectroscopy of the recurrent transient and well-known black hole candidate 4U 1630$-$472 during its 2024 outburst. The source was captured at the end of a disk-dominated high&#x2F;soft state, at an Eddington fraction of $\lambda_\mathrm{Edd} \sim 0.05~(10 M_{\odot}&#x2F;M_\mathrm{BH})$. A variable absorption spectrum with unprecedented complexity is revealed with the Resolve calorimeter. This marks one of the lowest Eddington fractions at which highly ionized absorption has been detected in an X-ray binary. The strongest lines are fully resolved, with He-like Fe XXV separated into resonance and intercombination components, and H-like Fe XXVI seen as a spin-orbit doublet. The depth of some absorption lines varied by almost an order of magnitude, far more than expected based on a 10% variation in apparent X-ray flux and ionization parameter. The velocity of some absorption components also changed significantly. Jointly modeling two flux segments with a consistent model including four photoionization zones, the spectrum can be described in terms of highly ionized but likely failed winds that sometimes show red-shifts, variable obscuration that may signal asymmetric structures in the middle and outer accretion disk, and a tentative very fast outflow ($v &#x3D; 0.026-0.033c$). We discuss the impact of these findings on our understanding of accretion and winds in stellar-mass black holes, and potential consequences for future studies. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†å…³äºåœ¨2024å¹´çˆ†å‘æœŸé—´å¯¹åå¤ç¬æ€å’Œå·²çŸ¥é»‘æ´å€™é€‰ä½“4U 1630-472è¿›è¡Œçš„XRISM&#x2F;Resolveå…‰è°±çš„ç ”ç©¶ã€‚è¯¥æºæ˜¯åœ¨åœ†ç›˜ä¸»å¯¼çš„é«˜è½¯æ€ç»“æŸæ—¶æ•è·çš„ï¼Œå…¶çˆ±ä¸é¡¿åˆ†æ•°ä¸ºÎ»eddâ‰ˆ0.05ï¼ˆå¯¹äºæ’æ˜Ÿè´¨é‡é»‘æ´ï¼‰ã€‚Resolveçƒ­é‡è®¡æ­ç¤ºäº†ä¸€ä¸ªå…·æœ‰å‰æ‰€æœªæœ‰çš„å¤æ‚æ€§çš„å¯å˜å¸æ”¶å…‰è°±ã€‚è¿™æ˜¯è¿„ä»Šä¸ºæ­¢åœ¨Xå°„çº¿åŒæ˜Ÿä¸­æ£€æµ‹åˆ°çš„çˆ±ä¸é¡¿åˆ†æ•°æœ€ä½çš„å¼ºç¦»å­åŒ–å¸æ”¶ä½“ä¹‹ä¸€ã€‚æœ€å¼ºçš„çº¿å®Œå…¨è§£å†³ï¼ŒHeç±»é“XXVåˆ†ä¸ºå…±æŒ¯å’Œç»„åˆæˆåˆ†ï¼ŒHç±»é“XXVIè¡¨ç°ä¸ºè‡ªæ—‹è½¨é“åŒå³°ã€‚æŸäº›å¸æ”¶çº¿çš„æ·±åº¦å˜åŒ–å‡ ä¹è¾¾åˆ°ä¸€ä¸ªæ•°é‡çº§ï¼Œè¿œå¤§äºæ ¹æ®è§‚å¯Ÿåˆ°çš„Xå°„çº¿æµé‡å’Œç”µç¦»å‚æ•°å˜åŒ–ç™¾åˆ†ä¹‹åæ‰€é¢„æœŸçš„æ·±åº¦å˜åŒ–ã€‚æŸäº›å¸æ”¶æˆåˆ†çš„é€Ÿåº¦ä¹Ÿå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚é€šè¿‡è”åˆå»ºæ¨¡ä¸¤ä¸ªæµé‡æ®µå¹¶åŒ…æ‹¬å››ä¸ªå…‰ç¦»å­åŒ–åŒºåŸŸçš„ä¸€è‡´æ¨¡å‹ï¼Œè¯¥å…‰è°±å¯ä»¥è¢«æè¿°ä¸ºé«˜åº¦ç”µç¦»ä½†å¯èƒ½å¤±è´¥çš„å¶å°”æ˜¾ç¤ºå‡ºçº¢ç§»çš„é£ï¼Œä»¥åŠåœ¨å¯èƒ½ä¸å¯¹ç§°ç»“æ„çš„ä¸­å±‚å’Œå¤–å±‚å¼•èµ·çš„å¯å˜æ€§é®è”½å’Œæ½œåœ¨çš„é«˜é€Ÿæµå‡ºï¼ˆv &#x3D; 0.026-0.033cï¼‰ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™äº›å‘ç°å¯¹æˆ‘ä»¬å¯¹æ’æ˜Ÿè´¨é‡é»‘æ´çš„å¸ç§¯å’Œé£çš„äº†è§£çš„å½±å“ä»¥åŠå¯¹æœªæ¥ç ”ç©¶çš„æ½œåœ¨å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07319v1">PDF</a> Accepted for publication in ApJL</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æŠ¥å‘Šäº†å…³äºæ’æ˜Ÿè´¨é‡é»‘æ´å€™é€‰ä½“4U 1630-472åœ¨2024å¹´çˆ†å‘æœŸé—´çš„XRISM&#x2F;Resolveå…‰è°±åˆ†æã€‚æºå¤„äºä»¥ç›˜ä¸ºä¸»çš„è½¯æ€çš„é«˜æ€æœ«æœŸï¼Œå…¶çˆ±ä¸é¡¿æ¯”çº¦ä¸ºÎ»_Edd â‰ˆ 0.05ã€‚é€šè¿‡Resolveçƒ­é‡è®¡æ­ç¤ºäº†å…·æœ‰å‰æ‰€æœªæœ‰çš„å¤æ‚æ€§çš„å¯å˜å¸æ”¶å…‰è°±ã€‚è¿™åœ¨Xå°„çº¿åŒæ˜Ÿä¸­æ ‡è®°äº†æ£€æµ‹åˆ°é«˜åº¦ç”µç¦»å¸æ”¶çš„æœ€å°çš„çˆ±ä¸é¡¿æ¯”ä¹‹ä¸€ã€‚æœ€å¼ºçš„è°±çº¿å®Œå…¨è§£æï¼ŒHeçŠ¶Fe XXVè¢«åˆ†ç¦»ä¸ºå…±æŒ¯å’Œç»„åˆæˆåˆ†ï¼ŒHçŠ¶Fe XXVIè¡¨ç°ä¸ºè‡ªæ—‹è½¨é“åŒå³°ã€‚æŸäº›å¸æ”¶çº¿çš„æ·±åº¦å˜åŒ–å¹…åº¦æ¥è¿‘ä¸€ä¸ªæ•°é‡çº§ï¼Œè¿œè¶…åŸºäºå¯è§Xå°„çº¿æµé‡å’Œç”µç¦»å‚æ•°å˜åŒ–çš„é¢„æœŸã€‚æŸäº›å¸æ”¶æˆåˆ†çš„é€Ÿåº¦ä¹Ÿå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚é€šè¿‡åŒ…å«å››ä¸ªå…‰ç¦»å­åŒ–åŒºçš„ç»Ÿä¸€æ¨¡å‹å¯¹ä¸¤ä¸ªæµé‡æ®µè¿›è¡Œè”åˆå»ºæ¨¡ï¼Œè¯¥å…‰è°±å¯ä»¥è¢«æè¿°ä¸ºé«˜åº¦ç”µç¦»ä½†å¯èƒ½å¤±è´¥çš„æœ‰æ—¶æ˜¾ç¤ºçº¢ç§»çš„é£ï¼Œä»¥åŠå¯èƒ½è¡¨æ˜ä¸­éƒ¨å’Œå¤–å±‚ç›˜çš„ä¸å¯¹ç§°ç»“æ„çš„å¯å˜é®è”½ï¼Œè¿˜æœ‰ä¸€ä¸ªå‡è®¾çš„å¿«é€Ÿæµå‡ºï¼ˆv &#x3D; 0.026-0.033cï¼‰ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™äº›å‘ç°å¯¹æˆ‘ä»¬å¯¹æ’æ˜Ÿè´¨é‡é»‘æ´çš„å¸ç§¯å’Œé£çš„è®¤çŸ¥çš„å½±å“ä»¥åŠå¯¹æœªæ¥ç ”ç©¶çš„æ½œåœ¨åæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨XRISM&#x2F;Resolveå…‰è°±ä»ªæŠ¥å‘Šäº†æ’æ˜Ÿè´¨é‡é»‘æ´å€™é€‰ä½“4U 1630-472åœ¨2024å¹´çˆ†å‘æœŸé—´çš„å…‰è°±åˆ†æã€‚</li>
<li>æºå¤„äºç›˜ä¸»å¯¼çš„é«˜è½¯æ€æœ«æœŸï¼Œè§‚æµ‹åˆ°çˆ±ä¸é¡¿æ¯”çº¦ä¸ºÎ»_Edd â‰ˆ 0.05çš„å¤æ‚å¯å˜å¸æ”¶å…‰è°±ã€‚</li>
<li>æ­ç¤ºäº†é«˜åº¦ç”µç¦»çš„å¸æ”¶çº¿ï¼Œè¿™åœ¨ä½çˆ±ä¸é¡¿æ¯”ä¸‹æ˜¯ç½•è§çš„ã€‚</li>
<li>æœ€å¼ºè°±çº¿å®Œå…¨è§£æï¼ŒåŒ…æ‹¬HeçŠ¶Fe XXVå’ŒHçŠ¶Fe XXVIçš„ä¸åŒæˆåˆ†ã€‚</li>
<li>ä¸€äº›å¸æ”¶çº¿çš„æ·±åº¦å˜åŒ–å¹…åº¦è¶…è¿‡é¢„æœŸï¼Œæš—ç¤ºäº†ä¸Xå°„çº¿æµé‡å’Œç”µç¦»å‚æ•°å˜åŒ–çš„å¤æ‚å…³ç³»ã€‚</li>
<li>é€šè¿‡è”åˆå»ºæ¨¡æ­ç¤ºäº†å¯èƒ½çš„å¤±è´¥çš„é£ã€ä¸å¯¹ç§°çš„ç›˜ç»“æ„ä»¥åŠå¿«é€Ÿæµå‡ºç­‰ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8543535f83d1c5fddfe40d92618cf86b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Narrative-Review-on-Large-AI-Models-in-Lung-Cancer-Screening-Diagnosis-and-Treatment-Planning"><a href="#A-Narrative-Review-on-Large-AI-Models-in-Lung-Cancer-Screening-Diagnosis-and-Treatment-Planning" class="headerlink" title="A Narrative Review on Large AI Models in Lung Cancer Screening,   Diagnosis, and Treatment Planning"></a>A Narrative Review on Large AI Models in Lung Cancer Screening,   Diagnosis, and Treatment Planning</h2><p><strong>Authors:Jiachen Zhong, Yiting Wang, Di Zhu, Ziwei Wang</strong></p>
<p>Lung cancer remains one of the most prevalent and fatal diseases worldwide, demanding accurate and timely diagnosis and treatment. Recent advancements in large AI models have significantly enhanced medical image understanding and clinical decision-making. This review systematically surveys the state-of-the-art in applying large AI models to lung cancer screening, diagnosis, prognosis, and treatment. We categorize existing models into modality-specific encoders, encoder-decoder frameworks, and joint encoder architectures, highlighting key examples such as CLIP, BLIP, Flamingo, BioViL-T, and GLoRIA. We further examine their performance in multimodal learning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR. Applications span pulmonary nodule detection, gene mutation prediction, multi-omics integration, and personalized treatment planning, with emerging evidence of clinical deployment and validation. Finally, we discuss current limitations in generalizability, interpretability, and regulatory compliance, proposing future directions for building scalable, explainable, and clinically integrated AI systems. Our review underscores the transformative potential of large AI models to personalize and optimize lung cancer care. </p>
<blockquote>
<p>è‚ºç™Œä»ç„¶æ˜¯å…¨çƒæœ€å¸¸è§å’Œè‡´å‘½çš„ç–¾ç—…ä¹‹ä¸€ï¼Œéœ€è¦å‡†ç¡®åŠæ—¶çš„è¯Šæ–­å’Œæ²»ç–—ã€‚æœ€è¿‘ï¼Œå¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°æé«˜äº†åŒ»å­¦å›¾åƒç†è§£å’Œä¸´åºŠå†³ç­–åˆ¶å®šã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„è¿°äº†å°†å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åº”ç”¨äºè‚ºç™Œç­›æŸ¥ã€è¯Šæ–­ã€é¢„åå’Œæ²»ç–—çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å°†ç°æœ‰æ¨¡å‹åˆ†ä¸ºæ¨¡æ€ç‰¹å®šç¼–ç å™¨ã€ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶å’Œè”åˆç¼–ç å™¨æ¶æ„ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†CLIPã€BLIPã€Flamingoã€BioViL-Tå’ŒGLoRIAç­‰å…³é”®ç¤ºä¾‹ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨LIDC-IDRIã€NLSTå’ŒMIMIC-CXRç­‰åŸºå‡†æ•°æ®é›†ï¼Œè¿›ä¸€æ­¥è¯„ä¼°äº†å®ƒä»¬åœ¨å¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚åº”ç”¨åŒ…æ‹¬è‚ºç»“èŠ‚æ£€æµ‹ã€åŸºå› çªå˜é¢„æµ‹ã€å¤šç»„å­¦æ•´åˆå’Œä¸ªæ€§åŒ–æ²»ç–—è®¡åˆ’ï¼Œæœ‰ä¸´åºŠéƒ¨ç½²å’ŒéªŒè¯çš„åˆæ­¥è¯æ®ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†ç›®å‰åœ¨é€šç”¨æ€§ã€å¯è§£é‡Šæ€§å’Œåˆè§„æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æ„å»ºå¯æ‰©å±•ã€å¯è§£é‡Šå’Œä¸´åºŠæ•´åˆçš„AIç³»ç»Ÿçš„æœªæ¥æ–¹å‘ã€‚æˆ‘ä»¬çš„ç»¼è¿°å¼ºè°ƒäº†å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨ä¸ªæ€§åŒ–ä¼˜åŒ–è‚ºç™Œæ²»ç–—æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07236v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†å¤§å‹äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è‚ºç™Œç­›æŸ¥ã€è¯Šæ–­ã€é¢„åå’Œæ²»ç–—ä¸­çš„åº”ç”¨ç°çŠ¶ï¼Œåˆ†ä¸ºæ¨¡æ€ç‰¹å®šç¼–ç å™¨ã€ç¼–ç å™¨è§£ç å™¨æ¡†æ¶å’Œè”åˆç¼–ç å™¨æ¶æ„ç­‰ç±»å‹ã€‚åº”ç”¨æ¶µç›–äº†è‚ºç»“èŠ‚æ£€æµ‹ã€åŸºå› çªå˜é¢„æµ‹ã€å¤šç»„å­¦é›†æˆå’Œä¸ªæ€§åŒ–æ²»ç–—è§„åˆ’ç­‰ã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†ç›®å‰æ¨¡å‹åœ¨æ³›åŒ–æ€§ã€å¯è§£é‡Šæ€§å’Œæ³•è§„åˆè§„æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥æ„å»ºå¯æ‰©å±•ã€å¯è§£é‡Šå’Œä¸´åºŠé›†æˆçš„AIç³»ç»Ÿçš„æ–¹å‘ã€‚å¤§å‹AIæ¨¡å‹åœ¨è‚ºç™Œè¯Šç–—ä¸ªæ€§åŒ–ä¼˜åŒ–æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‚ºç™Œä»ç„¶æ˜¯ä¸€ç§å¸¸è§ä¸”è‡´å‘½çš„ç–¾ç—…ï¼Œéœ€è¦å‡†ç¡®åŠæ—¶çš„è¯Šæ–­å’Œæ²»ç–—ã€‚</li>
<li>å¤§å‹AIæ¨¡å‹åœ¨åŒ»ç–—å›¾åƒç†è§£å’Œä¸´åºŠå†³ç­–åˆ¶å®šæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åˆ†ä¸ºæ¨¡æ€ç‰¹å®šç¼–ç å™¨ã€ç¼–ç å™¨è§£ç å™¨æ¡†æ¶å’Œè”åˆç¼–ç å™¨æ¶æ„ä¸‰ç§ç±»å‹ã€‚</li>
<li>å¤§å‹AIæ¨¡å‹åœ¨è‚ºç™Œç­›æŸ¥ã€è¯Šæ–­ã€é¢„åå’Œæ²»ç–—æ–¹é¢æœ‰ç€å¹¿æ³›åº”ç”¨ã€‚</li>
<li>åº”ç”¨åŒ…æ‹¬è‚ºç»“èŠ‚æ£€æµ‹ã€åŸºå› çªå˜é¢„æµ‹ã€å¤šç»„å­¦é›†æˆå’Œä¸ªæ€§åŒ–æ²»ç–—è§„åˆ’ç­‰ã€‚</li>
<li>ç›®å‰æ¨¡å‹å­˜åœ¨æ³›åŒ–æ€§ã€å¯è§£é‡Šæ€§å’Œæ³•è§„åˆè§„æ€§ç­‰æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cbb9f2bc2de98920a6a0b87b4aa855c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84a2f1f149fa4206dd7c997cbccf77b0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Simultaneous-Segmentation-of-Ventricles-and-Normal-Abnormal-White-Matter-Hyperintensities-in-Clinical-MRI-using-Deep-Learning"><a href="#Simultaneous-Segmentation-of-Ventricles-and-Normal-Abnormal-White-Matter-Hyperintensities-in-Clinical-MRI-using-Deep-Learning" class="headerlink" title="Simultaneous Segmentation of Ventricles and Normal&#x2F;Abnormal White Matter   Hyperintensities in Clinical MRI using Deep Learning"></a>Simultaneous Segmentation of Ventricles and Normal&#x2F;Abnormal White Matter   Hyperintensities in Clinical MRI using Deep Learning</h2><p><strong>Authors:Mahdi Bashiri Bawil, Mousa Shamsi, Abolhassan Shakeri Bavil</strong></p>
<p>Multiple sclerosis (MS) diagnosis and monitoring rely heavily on accurate assessment of brain MRI biomarkers, particularly white matter hyperintensities (WMHs) and ventricular changes. Current segmentation approaches suffer from several limitations: they typically segment these structures independently despite their pathophysiological relationship, struggle to differentiate between normal and pathological hyperintensities, and are poorly optimized for anisotropic clinical MRI data. We propose a novel 2D pix2pix-based deep learning framework for simultaneous segmentation of ventricles and WMHs with the unique capability to distinguish between normal periventricular hyperintensities and pathological MS lesions. Our method was developed and validated on FLAIR MRI scans from 300 MS patients. Compared to established methods (SynthSeg, Atlas Matching, BIANCA, LST-LPA, LST-LGA, and WMH-SynthSeg), our approach achieved superior performance for both ventricle segmentation (Dice: 0.801+&#x2F;-0.025, HD95: 18.46+&#x2F;-7.1mm) and WMH segmentation (Dice: 0.624+&#x2F;-0.061, precision: 0.755+&#x2F;-0.161). Furthermore, our method successfully differentiated between normal and abnormal hyperintensities with a Dice coefficient of 0.647. Notably, our approach demonstrated exceptional computational efficiency, completing end-to-end processing in approximately 4 seconds per case, up to 36 times faster than baseline methods, while maintaining minimal resource requirements. This combination of improved accuracy, clinically relevant differentiation capability, and computational efficiency addresses critical limitations in current neuroimaging analysis, potentially enabling integration into routine clinical workflows and enhancing MS diagnosis and monitoring. </p>
<blockquote>
<p>å¤šå‘æ€§ç¡¬åŒ–ç—‡ï¼ˆMSï¼‰çš„è¯Šæ–­å’Œç›‘æµ‹ä¸¥é‡ä¾èµ–äºå¯¹å¤§è„‘MRIç”Ÿç‰©æ ‡å¿—ç‰©çš„å‡†ç¡®è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯è„‘ç™½è´¨é«˜ä¿¡å·ï¼ˆWMHsï¼‰å’Œè„‘å®¤å˜åŒ–ã€‚å½“å‰çš„åˆ†å‰²æ–¹æ³•å­˜åœ¨å‡ ä¸ªå±€é™æ€§ï¼šå®ƒä»¬é€šå¸¸ç‹¬ç«‹åœ°åˆ†å‰²è¿™äº›ç»“æ„ï¼Œè€Œå¿½ç•¥äº†å…¶ç—…ç†ç”Ÿç†å…³ç³»ï¼›éš¾ä»¥åŒºåˆ†æ­£å¸¸å’Œç—…ç†æ€§é«˜ä¿¡å·ï¼›å¯¹ä¸´åºŠMRIæ•°æ®çš„ä¼˜åŒ–ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å­¦ä¹ çš„äºŒç»´pix2pixæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶åˆ†å‰²è„‘å®¤å’ŒWMHsï¼Œå¹¶å…·æœ‰åŒºåˆ†æ­£å¸¸è„‘å®¤å‘¨å›´é«˜ä¿¡å·å’Œç—…ç†æ€§MSç—…ç¶çš„ç‹¬ç‰¹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¥è‡ª300åMSæ‚£è€…çš„FLAIR MRIæ‰«æä¸Šè¿›è¡Œäº†å¼€å‘å’ŒéªŒè¯ã€‚ä¸ç°æœ‰æ–¹æ³•ï¼ˆSynthSegã€å›¾è°±åŒ¹é…ã€BIANCAã€LST-LPAã€LST-LGAå’ŒWMH-SynthSegï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— è®ºæ˜¯åœ¨è„‘å®¤åˆ†å‰²ï¼ˆDiceç³»æ•°ï¼š0.801Â±0.025ï¼ŒHD95ï¼šå¹³å‡Â±æ ‡å‡†åå·®ï¼Œä¸‹åŒï¼›åˆ†éš”å‡†ç¡®åº¦çš„è®¡ç®—ä¸­æ¯ä¸ªè¾¹ç¼˜çš„å®Œå…¨ä¸€è‡´çº¿åº¦é‡æ˜¯çº¦ä¸ºé¢„æµ‹çš„æŸç§è¾¹ç¼˜ä¸å®é™…è¾¹ç¼˜ä¹‹é—´è·ç¦»çš„ç™¾åˆ†ä¹‹å‡ çš„å¹³å‡å€¼ã€‚18.46Â±7.1mmï¼‰è¿˜æ˜¯WMHåˆ†å‰²ï¼ˆDiceç³»æ•°ï¼šçº¦ä¸ºä¸¤è€…ç›¸ä¼¼æ€§ç³»æ•°çš„å¹³å‡å€¼+&#x2F;-å·®å¼‚çš„èŒƒå›´ä¸ºæ•´ä½“æœ€å¤§å€¼æœ€å°å€¼çš„ä¸€ä¸ªé‡åº¦è¯„ä¼°æŒ‡æ ‡çš„è¿‘ä¼¼å€¼çš„ç»“æœæ¯”è¾ƒç­‰ç­‰æˆ–äºŒè€…ç²¾ç¡®åº¦å¾—åˆ†å·®å¼‚èŒƒå›´å†…é«˜è¾¾ç™¾åˆ†æ¯”å¹³å‡+&#x2F;-ä¸€ä¸ªèŒƒå›´çš„ç²¾ç¡®åº¦ä¸ºå®é™…è§‚æµ‹å€¼ä¸æ¨¡å‹é¢„æµ‹å€¼ä¹‹é—´çš„ä¸€è‡´ç¨‹åº¦åˆ†æ•°æ˜¯ç™¾åˆ†æ¯”è¾¾åˆ°å€¼æ—¶çš„è®¡ç®—ç²¾åº¦å¾—åˆ†å€¼é«˜ä½é€šå¸¸ä½œä¸ºè¯„ä¼°æ¨¡å‹é¢„æµ‹ç²¾ç¡®åº¦å’Œæœ‰æ•ˆç»Ÿè®¡åˆ†æå’Œæ‹Ÿåˆçš„é‡è¦æ•°æ®æŠ¥å‘Šçš„å› ç´ çš„å€¼å·®è·å¾ˆé‡è¦ä»¥è¾¾åˆ°å‡†ç¡®ç‡ã€å‡†ç¡®åº¦è‰¯å¥½é€‚åº”æ¦‚ç‡ç­‰æƒ…å†µæ—¶è¿›è¡Œçµæ´»ä¸ç«äº‰æ€§è¢«å±•ç°éªŒè¯èƒ½åŠ›æ­¤é¡¹æ›´ä¼˜è¶Šçš„ç»“æœæ˜¾ç¤ºå…¶ä¼˜å¼‚æ€§èƒ½è¡¨ç°åœ¨å‡†ç¡®åº¦å’Œç²¾ç¡®åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼‰æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°åŒºåˆ†äº†æ­£å¸¸å’Œå¼‚å¸¸çš„é«˜ä¿¡å·å¼ºåº¦å€¼é«˜ååº”è¯Šæ–­åˆ¤åˆ«æ•ˆæœå’Œåˆ†ç¦»äºŒè€…ç»†èŠ‚æˆåŠŸä¿ç•™äº†åˆ†éš”ç›®æ ‡é‰´åˆ«ç›®çš„æ¡ä»¶ä¸‹å¸¸è§„ç ”ç©¶æ–¹æ³•åŒé¢„ä¼°åŸºç¡€å¹³å‡ç›¸å·®æœ€å°ææ˜¾è‘—æ•ˆåº”ä¼˜å¼‚æé«˜è¯„ä»·ç‰¹æ€§æŒ‡æ•°æé«˜åº¦æ»¡è¶³ä½æ€§èƒ½æ•ˆç‡ä»·å€¼å¾—åˆ°æœ€ä½³çš„è¡¨ç°é€šè¿‡å‘ˆç°é«˜è´¨é‡çš„é«˜æ•ˆèƒ½æœ‰æ•ˆæ”¯æŒå¤šå…³é”®å› ç´ ç›¸ç»“åˆä¼˜ç§€çš„ç»“æœæˆå°±èƒ½å¤Ÿè¯„ä¼°ç›¸å…³ç ”ç©¶é¢†åŸŸæŠ€æœ¯è¿›æ­¥æˆ–å•†ä¸šäº§å“çš„ä¼˜åŒ–è¯„ä¼°é‰´å®šæœ¬å®éªŒçš„æˆæœè¿˜æ˜¾ç¤ºå…·å¤‡å¼ºå¤§å‡ºè‰²çš„è®¡ç®—èƒ½åŠ›æ¯ä¸ªæ¡ˆä¾‹å¹³å‡å¤§çº¦ä»…è€—æ—¶4ç§’å·¦å³èƒ½å…¨é¢æé«˜æ¯é“å·¥åºæ•ˆç‡çš„å¤æ‚äº‹åŠ¡æ´»åŠ¨åœ¨å®é™…æ‰«æåˆ†æå’Œå¤šé¡¹æ·±åº¦å­¦ä¹ ç›¸å…³çš„é€Ÿåº¦è¿è½¬ä¸Šé¢ç¡®ä¿äº†å·¥ä½œçš„å®ç°é€”å¾„é¢å¯¹å¤§ä½“åŸºäºå…ˆéªŒå‰ææŠ€å·§çš„å…¶ä»–åŠ å¿«ç§‘å­¦ä¸šåŠ¡é¢å‘ç‰¹è‰²ç›¸æ¯”è¾ƒå¤šè¾¾å‡ ä¹é¢†å…ˆäºæœ€å¤šè¾ƒå¿«å¤§å°ºåº¦å’Œç»“åˆè¾¾æˆèƒ½åŠ›çš„ç›¸åŒç»´åº¦çš„é•¿æœŸæ–¹å‘å°±é¿å…äº†åŸºç¡€å¸¸è§„è€—è´¹æ—¶é—´çš„é€Ÿåº¦æ•ˆç‡å¤„ç†æŒ‘æˆ˜æé«˜äº†ç›¸å½“å¿«çš„é€Ÿåº¦å®Œæˆäº†é‡è¦çš„çªç ´æ€§æˆæœå¤„ç†å®Œæˆæ­¤ç±»ä¸´åºŠå·¥ä½œä¸­æ‰€éœ€è¦æ‰¿æ‹…çš„é‡å¤§éœ€æ±‚é‡çš„ç ”ç©¶å·¥ä½œï¼›è¿™é¡¹ç»“åˆæ”¹è¿›å‡†ç¡®æ€§ã€å…·æœ‰ä¸´åºŠæ„ä¹‰åŒºåˆ†èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡çš„çªç ´æ€§ç ”ç©¶è§£å†³äº†å½“å‰ç¥ç»å½±åƒåˆ†æçš„å…³é”®å±€é™æ€§ï¼Œæœ‰æœ›å°†å…¶çº³å…¥å¸¸è§„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œæé«˜MSè¯Šæ–­å’Œç›‘æµ‹æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07123v1">PDF</a> 44 pages, 11 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„äºŒç»´pix2pixæ¡†æ¶ï¼Œç”¨äºåŒæ—¶åˆ†å‰²è„‘å®¤å’Œè„‘ç™½è´¨é«˜ä¿¡å·ç—…å˜ï¼ˆWMHsï¼‰ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåŒºåˆ†æ­£å¸¸ä¸ç—…ç†æ€§ç—…å˜ï¼Œå¯¹FLAIR MRIæ‰«æçš„300åå¤šå‘æ€§ç¡¬åŒ–ç—‡æ‚£è€…è¿›è¡Œå¼€å‘å’ŒéªŒè¯ã€‚ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¿ƒå®¤å’ŒWMHsåˆ†å‰²ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸”è®¡ç®—æ•ˆç‡é«˜ï¼Œå¯å¿«é€Ÿå®Œæˆç—…ä¾‹å¤„ç†ï¼Œæœ‰åŠ©äºæ”¹å–„å¤šå‘æ€§ç¡¬åŒ–ç—‡çš„è¯Šæ–­å’Œç›‘æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰MRIåœ¨MSè¯Šæ–­ä¸­çš„é‡è¦æ€§åœ¨äºå…¶èƒ½å‡†ç¡®è¯„ä¼°å¤§è„‘MRIç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œç‰¹åˆ«æ˜¯è„‘ç™½è´¨é«˜ä¿¡å·ç—…å˜ï¼ˆWMHsï¼‰å’Œè„‘å®¤å˜åŒ–ã€‚</li>
<li>ç‹¬ç«‹åˆ†å‰²WMHså’Œè„‘å®¤å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬ä¹‹é—´å­˜åœ¨ç—…ç†ç”Ÿç†å…³ç³»ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§åŒæ—¶åˆ†å‰²ä¸¤è€…çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å­¦ä¹ çš„pix2pixæ¡†æ¶ï¼Œèƒ½åŒæ—¶åˆ†å‰²è„‘å®¤å’ŒWMHsã€‚æ­¤æ¡†æ¶å…·æœ‰åŒºåˆ†æ­£å¸¸ä¸ç—…ç†æ€§ç—…å˜çš„èƒ½åŠ›ã€‚</li>
<li>æ­¤æ–¹æ³•ç»è¿‡äº†å¤§é‡çš„éªŒè¯æµ‹è¯•ï¼Œä¸å¤šç§å·²çŸ¥æ–¹æ³•ç›¸æ¯”è¡¨ç°å‡ºè¾ƒé«˜çš„åˆ†å‰²å‡†ç¡®æ€§ã€‚å…·ä½“è¡¨ç°ä¸ºå¿ƒå®¤åˆ†å‰²çš„Diceç³»æ•°è¾¾åˆ°0.801ï¼ŒWMHåˆ†å‰²çš„Diceç³»æ•°è¾¾åˆ°0.624ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æˆåŠŸåŒºåˆ†æ­£å¸¸å’Œå¼‚å¸¸çš„è„‘ç™½è´¨é«˜ä¿¡å·ç—…å˜ã€‚</li>
<li>è¯¥æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ï¼Œèƒ½åœ¨çŸ­æ—¶é—´å†…å®Œæˆç—…ä¾‹å¤„ç†ï¼Œæœ‰åŠ©äºå®ç°æ—¥å¸¸ä¸´åºŠå·¥ä½œæµç¨‹çš„é›†æˆã€‚è¿™ä½¿å…¶æˆä¸ºè§£å†³å½“å‰ç¥ç»å½±åƒåˆ†æçš„å…³é”®é™åˆ¶çš„ä¸€ç§æœ‰æ•ˆæ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c660c1675decf73814f16e5febe6d3f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SceneLCM-End-to-End-Layout-Guided-Interactive-Indoor-Scene-Generation-with-Latent-Consistency-Model"><a href="#SceneLCM-End-to-End-Layout-Guided-Interactive-Indoor-Scene-Generation-with-Latent-Consistency-Model" class="headerlink" title="SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation   with Latent Consistency Model"></a>SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation   with Latent Consistency Model</h2><p><strong>Authors:Yangkai Lin, Jiabao Lei, Kui Jia</strong></p>
<p>Our project page: <a target="_blank" rel="noopener" href="https://scutyklin.github.io/SceneLCM/">https://scutyklin.github.io/SceneLCM/</a>. Automated generation of complex, interactive indoor scenes tailored to user prompt remains a formidable challenge. While existing methods achieve indoor scene synthesis, they struggle with rigid editing constraints, physical incoherence, excessive human effort, single-room limitations, and suboptimal material quality. To address these limitations, we propose SceneLCM, an end-to-end framework that synergizes Large Language Model (LLM) for layout design with Latent Consistency Model(LCM) for scene optimization. Our approach decomposes scene generation into four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D spatial reasoning to convert textual descriptions into parametric blueprints(3D layout). And an iterative programmatic validation mechanism iteratively refines layout parameters through LLM-mediated dialogue loops; (2) Furniture Generation. SceneLCM employs Consistency Trajectory Sampling(CTS), a consistency distillation sampling loss guided by LCM, to form fast, semantically rich, and high-quality representations. We also offer two theoretical justification to demonstrate that our CTS loss is equivalent to consistency loss and its distillation error is bounded by the truncation error of the Euler solver; (3) Environment Optimization. We use a multiresolution texture field to encode the appearance of the scene, and optimize via CTS loss. To maintain cross-geometric texture coherence, we introduce a normal-aware cross-attention decoder to predict RGB by cross-attending to the anchors locations in geometrically heterogeneous instance. (4)Physically Editing. SceneLCM supports physically editing by integrating physical simulation, achieved persistent physical realism. Extensive experiments validate SceneLCMâ€™s superiority over state-of-the-art techniques, showing its wide-ranging potential for diverse applications. </p>
<blockquote>
<p>æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://scutyklin.github.io/SceneLCM/">https://scutyklin.github.io/SceneLCM/</a>ã€‚é’ˆå¯¹ç”¨æˆ·æç¤ºè‡ªåŠ¨ç”Ÿæˆå¤æ‚ã€äº¤äº’å¼çš„å®¤å†…åœºæ™¯ä»ç„¶æ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•èƒ½å¤Ÿå®ç°å®¤å†…åœºæ™¯åˆæˆï¼Œä½†å®ƒä»¬é¢ä¸´ç€åˆšæ€§ç¼–è¾‘çº¦æŸã€ç‰©ç†ä¸ä¸€è‡´ã€äººåŠ›æŠ•å…¥è¿‡å¤§ã€å•æˆ¿é—´å±€é™ä»¥åŠææ–™è´¨é‡ä¸ä½³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SceneLCMï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œå®ƒååŒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¸ƒå±€è®¾è®¡ï¼Œä»¥åŠæ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMï¼‰è¿›è¡Œåœºæ™¯ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åœºæ™¯ç”Ÿæˆåˆ†è§£ä¸ºå››ä¸ªæ¨¡å—åŒ–æµç¨‹ï¼šï¼ˆ1ï¼‰å¸ƒå±€ç”Ÿæˆã€‚æˆ‘ä»¬é‡‡ç”¨LLMå¼•å¯¼çš„3Dç©ºé—´æ¨ç†ï¼Œå°†æ–‡æœ¬æè¿°è½¬æ¢ä¸ºå‚æ•°åŒ–è“å›¾ï¼ˆ3Då¸ƒå±€ï¼‰ã€‚é€šè¿‡LLMä»‹å¯¼çš„å¯¹è¯å¾ªç¯ï¼Œä¸€ä¸ªè¿­ä»£ç¨‹åºéªŒè¯æœºåˆ¶ä¼šè¿­ä»£åœ°ä¼˜åŒ–å¸ƒå±€å‚æ•°ï¼›ï¼ˆ2ï¼‰å®¶å…·ç”Ÿæˆã€‚SceneLCMé‡‡ç”¨ç”±LCMå¼•å¯¼çš„ä¸€è‡´æ€§è½¨è¿¹é‡‡æ ·ï¼ˆCTSï¼‰ï¼Œå½¢æˆå¿«é€Ÿã€è¯­ä¹‰ä¸°å¯Œã€é«˜è´¨é‡çš„è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸¤ç§ç†è®ºè¯æ˜ï¼Œè¯æ˜æˆ‘ä»¬çš„CTSæŸå¤±ç›¸å½“äºä¸€è‡´æ€§æŸå¤±ï¼Œå…¶è’¸é¦è¯¯å·®è¢«Euleræ±‚è§£å™¨çš„æˆªæ–­è¯¯å·®æ‰€é™åˆ¶ï¼›ï¼ˆ3ï¼‰ç¯å¢ƒä¼˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šåˆ†è¾¨ç‡çº¹ç†åœºæ¥ç¼–ç åœºæ™¯å¤–è§‚ï¼Œå¹¶é€šè¿‡CTSæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºäº†ä¿æŒè·¨å‡ ä½•çº¹ç†çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ³•çº¿æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›è§£ç å™¨ï¼Œé€šè¿‡äº¤å‰å…³æ³¨å‡ ä½•å¼‚æ„å®ä¾‹ä¸­çš„é”šç‚¹ä½ç½®æ¥é¢„æµ‹RGBã€‚ï¼ˆ4ï¼‰ç‰©ç†ç¼–è¾‘ã€‚SceneLCMé€šè¿‡é›†æˆç‰©ç†æ¨¡æ‹Ÿæ”¯æŒç‰©ç†ç¼–è¾‘ï¼Œå®ç°äº†æŒä¹…çš„ç‰©ç†çœŸå®æ€§ã€‚å¤§é‡å®éªŒéªŒè¯äº†SceneLCMåœ¨å…ˆè¿›æŠ€æœ¯ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§åº”ç”¨ä¸­çš„å¹¿æ³›æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07091v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>SceneLCMæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œå®ƒç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¸ƒå±€è®¾è®¡ä»¥åŠæ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMï¼‰è¿›è¡Œåœºæ™¯ä¼˜åŒ–ï¼Œä»¥è§£å†³å®¤å†…åœºæ™¯åˆæˆçš„å¤šé¡¹æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å››ä¸ªæ¨¡å—åŒ–ç®¡é“ç”Ÿæˆåœºæ™¯ï¼ŒåŒ…æ‹¬å¸ƒå±€ç”Ÿæˆã€å®¶å…·ç”Ÿæˆã€ç¯å¢ƒä¼˜åŒ–å’Œç‰©ç†ç¼–è¾‘ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>SceneLCMæ˜¯ä¸€ä¸ªå…¨æ–°çš„å®¤å†…åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¸ƒå±€è®¾è®¡ï¼Œå¹¶å¼•å…¥æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ï¼ˆLCMï¼‰ä¼˜åŒ–åœºæ™¯ã€‚</li>
<li>å¸ƒå±€ç”Ÿæˆéƒ¨åˆ†ï¼Œé‡‡ç”¨LLMå¼•å¯¼çš„ä¸‰ç»´ç©ºé—´æ¨ç†ï¼Œå°†æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºå‚æ•°åŒ–è“å›¾ï¼Œå¹¶é€šè¿‡è¿­ä»£ç¨‹åºéªŒè¯æœºåˆ¶å®Œå–„å¸ƒå±€å‚æ•°ã€‚</li>
<li>å®¶å…·ç”Ÿæˆéƒ¨åˆ†ï¼Œé‡‡ç”¨ä¸€è‡´æ€§è½¨è¿¹é‡‡æ ·ï¼ˆCTSï¼‰æŠ€æœ¯ï¼Œå¿«é€Ÿç”Ÿæˆä¸°å¯Œè¯­ä¹‰å’Œé«˜è´¨é‡çš„è¡¨ç¤ºã€‚åŒæ—¶ï¼Œæä¾›äº†ä¸¤ç§ç†è®ºè¯æ˜CTSæŸå¤±ä¸ä¸€è‡´æ€§æŸå¤±ç­‰ä»·ï¼Œå¹¶è¯æ˜äº†å…¶è’¸é¦è¯¯å·®è¢«Euleræ±‚è§£å™¨çš„æˆªæ–­è¯¯å·®æ‰€é™åˆ¶ã€‚</li>
<li>ç¯å¢ƒä¼˜åŒ–éƒ¨åˆ†ï¼Œé‡‡ç”¨å¤šåˆ†è¾¨ç‡çº¹ç†åœºæ¥ç¼–ç åœºæ™¯å¤–è§‚ï¼Œå¹¶é€šè¿‡CTSæŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚åŒæ—¶ï¼Œå¼•å…¥æ³•çº¿æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›è§£ç å™¨ï¼Œä»¥é¢„æµ‹RGBå¹¶ç»´æŒè·¨å‡ ä½•çº¹ç†çš„ä¸€è‡´æ€§ã€‚</li>
<li>SceneLCMæ”¯æŒç‰©ç†ç¼–è¾‘ï¼Œé›†æˆç‰©ç†ä»¿çœŸï¼Œå®ç°æŒä¹…çš„ç‰©ç†çœŸå®æ€§ã€‚</li>
<li>å¹¿æ³›å®éªŒéªŒè¯äº†SceneLCMåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå±•ç°å‡ºå…¶åœ¨å¤šç§åº”ç”¨ä¸­çš„å¹¿æ³›æ½œåŠ›ã€‚</li>
<li>SceneLCMæ¡†æ¶è§£å†³äº†å®¤å†…åœºæ™¯åˆæˆä¸­çš„å¤šé¡¹æŒ‘æˆ˜ï¼Œå¦‚åˆšæ€§ç¼–è¾‘çº¦æŸã€ç‰©ç†ä¸ä¸€è‡´æ€§ã€è¿‡åº¦äººå·¥å¹²é¢„ã€å•å®¤é™åˆ¶ä»¥åŠææ–™è´¨é‡ä¸ä¼˜ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-329fc195bb91410216b788057906e4c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6666cde0dc3aca3f9e1398f3b46925a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1834571087f983bc81510e3c2f88487c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74f83f8e2093b2d53811c5a098a13d1c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MoXGATE-Modality-aware-cross-attention-for-multi-omic-gastrointestinal-cancer-sub-type-classification"><a href="#MoXGATE-Modality-aware-cross-attention-for-multi-omic-gastrointestinal-cancer-sub-type-classification" class="headerlink" title="MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal   cancer sub-type classification"></a>MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal   cancer sub-type classification</h2><p><strong>Authors:Sajib Acharjee Dip, Uddip Acharjee Shuvo, Dipanwita Mallick, Abrar Rahman Abir, Liqing Zhang</strong></p>
<p>Cancer subtype classification is crucial for personalized treatment and prognostic assessment. However, effectively integrating multi-omic data remains challenging due to the heterogeneous nature of genomic, epigenomic, and transcriptomic features. In this work, we propose Modality-Aware Cross-Attention MoXGATE, a novel deep-learning framework that leverages cross-attention and learnable modality weights to enhance feature fusion across multiple omics sources. Our approach effectively captures inter-modality dependencies, ensuring robust and interpretable integration. Through experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA) datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods, achieving 95% classification accuracy. Ablation studies validate the effectiveness of cross-attention over simple concatenation and highlight the importance of different omics modalities. Moreover, our model generalizes well to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key contributions include (1) a cross-attention-based multi-omic integration framework, (2) modality-weighted fusion for enhanced interpretability, (3) application of focal loss to mitigate data imbalance, and (4) validation across multiple cancer subtypes. Our results indicate that MoXGATE is a promising approach for multi-omic cancer subtype classification, offering improved performance and biological generalizability. </p>
<blockquote>
<p>ç™Œç—‡äºšå‹åˆ†ç±»å¯¹äºä¸ªæ€§åŒ–æ²»ç–—å’Œé¢„åè¯„ä¼°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºåŸºå› ç»„ã€è¡¨è§‚é—ä¼ ç»„å’Œè½¬å½•ç»„ç‰¹å¾çš„å¼‚è´¨æ€§ï¼Œæœ‰æ•ˆåœ°æ•´åˆå¤šç»„å­¦æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Modality-Aware Cross-Attention MoXGATEï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å’Œå¯å­¦ä¹ çš„æ¨¡æ€æƒé‡æ¥å¢å¼ºè·¨å¤šä¸ªç»„å­¦æ¥æºçš„ç‰¹å¾èåˆçš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æ•è·è·¨æ¨¡æ€ä¾èµ–æ€§ï¼Œç¡®ä¿ç¨³å¥ä¸”å¯è§£é‡Šæ€§çš„é›†æˆã€‚é€šè¿‡å¯¹TCGAçš„èƒƒè‚ é“è…ºç™Œï¼ˆGIACï¼‰å’Œä¹³è…ºç™Œï¼ˆBRCAï¼‰æ•°æ®é›†çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†MoXGATEä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†95%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†äº¤å‰æ³¨æ„åŠ›ç›¸å¯¹äºç®€å•æ‹¼æ¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†ä¸åŒç»„å­¦æ¨¡æ€çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯¹æœªè§è¿‡çš„ç™Œç—‡ç±»å‹ï¼ˆä¾‹å¦‚ä¹³è…ºç™Œï¼‰å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™çªå‡ºäº†å…¶é€‚åº”æ€§ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼ˆ1ï¼‰åŸºäºäº¤å‰æ³¨æ„åŠ›çš„å¤šç»„å­¦æ•´åˆæ¡†æ¶ï¼Œï¼ˆ2ï¼‰ç”¨äºå¢å¼ºå¯è§£é‡Šæ€§çš„æ¨¡æ€åŠ æƒèåˆï¼Œï¼ˆ3ï¼‰åº”ç”¨ç„¦ç‚¹æŸå¤±æ¥ç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œï¼ˆ4ï¼‰è·¨å¤šç§ç™Œç—‡äºšå‹çš„éªŒè¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMoXGATEæ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„å¤šç»„å­¦ç™Œç—‡äºšå‹åˆ†ç±»æ–¹æ³•ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œç”Ÿç‰©å­¦æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06980v1">PDF</a> 9 pages, 1 figure, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè·¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šæ¨¡æ€æ•°æ®èåˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶MoXGATEï¼Œç”¨äºç™Œç—‡äºšå‹åˆ†ç±»ã€‚è¯¥æ¡†æ¶é€šè¿‡èåˆå¤šç»„å­¦æ•°æ®ï¼Œå®ç°æœ‰æ•ˆåˆ†ç±»ï¼Œå‡†ç¡®ç‡é«˜è¾¾95%ã€‚å…¶å…³é”®è´¡çŒ®åŒ…æ‹¬è·¨æ³¨æ„åŠ›å¤šç»„å­¦èåˆæ¡†æ¶ã€æ¨¡æ€åŠ æƒèåˆå¢å¼ºè§£é‡Šæ€§ã€åº”ç”¨ç„¦ç‚¹æŸå¤±ç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶åœ¨å¤šç§ç™Œç—‡äºšå‹ä¸­å¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡äºšå‹åˆ†ç±»å¯¹äºä¸ªæ€§åŒ–æ²»ç–—å’Œé¢„åè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>å¤šç»„å­¦æ•°æ®èåˆæ˜¯ç™Œç—‡äºšå‹åˆ†ç±»çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>MoXGATEæ¡†æ¶åˆ©ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¯å­¦ä¹ çš„æ¨¡æ€æƒé‡ï¼Œå®ç°å¤šç»„å­¦æ•°æ®çš„æœ‰æ•ˆèåˆã€‚</li>
<li>MoXGATEæ¡†æ¶åœ¨èƒƒè‚ é“è…ºç™Œå’Œä¹³è…ºç™Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œåˆ†ç±»å‡†ç¡®ç‡é«˜è¾¾95%ã€‚</li>
<li>è·¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç®€å•æ‹¼æ¥æ–¹æ³•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼ŒéªŒè¯äº†ä¸åŒç»„å­¦æ¨¡æ€çš„é‡è¦æ€§ã€‚</li>
<li>MoXGATEæ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€‚åº”æœªè§è¿‡çš„ç™Œç—‡ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-694ab48ce41e32ab3f41f55b2e9d31d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-188ed319106edb785425793e7ca76cb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-792101f4d4390907a84dfcd6d8f9f6bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008f2d3749def46d46e5386b5635998d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788629f71ffe32bf8077bdc91b2677fd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="NSD-Imagery-A-benchmark-dataset-for-extending-fMRI-vision-decoding-methods-to-mental-imagery"><a href="#NSD-Imagery-A-benchmark-dataset-for-extending-fMRI-vision-decoding-methods-to-mental-imagery" class="headerlink" title="NSD-Imagery: A benchmark dataset for extending fMRI vision decoding   methods to mental imagery"></a>NSD-Imagery: A benchmark dataset for extending fMRI vision decoding   methods to mental imagery</h2><p><strong>Authors:Reese Kneeland, Paul S. Scotti, Ghislain St-Yves, Jesse Breedlove, Kendrick Kay, Thomas Naselaris</strong></p>
<p>We release NSD-Imagery, a benchmark dataset of human fMRI activity paired with mental images, to complement the existing Natural Scenes Dataset (NSD), a large-scale dataset of fMRI activity paired with seen images that enabled unprecedented improvements in fMRI-to-image reconstruction efforts. Recent models trained on NSD have been evaluated only on seen image reconstruction. Using NSD-Imagery, it is possible to assess how well these models perform on mental image reconstruction. This is a challenging generalization requirement because mental images are encoded in human brain activity with relatively lower signal-to-noise and spatial resolution; however, generalization from seen to mental imagery is critical for real-world applications in medical domains and brain-computer interfaces, where the desired information is always internally generated. We provide benchmarks for a suite of recent NSD-trained open-source visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.) on NSD-Imagery, and show that the performance of decoding methods on mental images is largely decoupled from performance on vision reconstruction. We further demonstrate that architectural choices significantly impact cross-decoding performance: models employing simple linear decoding architectures and multimodal feature decoding generalize better to mental imagery, while complex architectures tend to overfit visual training data. Our findings indicate that mental imagery datasets are critical for the development of practical applications, and establish NSD-Imagery as a useful resource for better aligning visual decoding methods with this goal. </p>
<blockquote>
<p>æˆ‘ä»¬å‘å¸ƒäº†NSD-Imageryæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ç»„é…å¯¹äººç±»fMRIæ´»åŠ¨ä¸å¿ƒç†å›¾åƒçš„æ ‡å‡†æ•°æ®é›†ã€‚å®ƒæ—¨åœ¨è¡¥å……ç°æœ‰çš„è‡ªç„¶åœºæ™¯æ•°æ®é›†ï¼ˆNSDï¼‰â€”â€”ä¸€ä¸ªå¤§è§„æ¨¡é…å¯¹fMRIæ´»åŠ¨å’Œè§†è§‰å›¾åƒçš„æ•°æ®é›†ï¼Œæ¨åŠ¨äº†å›¾åƒé‡å»ºä¸­å‰æ‰€æœªæœ‰çš„æ”¹è¿›ã€‚è¿‘æœŸä»…åœ¨è§†è§‰å›¾åƒé‡å»ºä¸Šè¯„ä¼°çš„NSDæ¨¡å‹å¯é€šè¿‡NSD-Imageryæ¥è¯„ä¼°å…¶åœ¨å¿ƒç†å›¾åƒé‡å»ºä¸Šçš„è¡¨ç°ã€‚è¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–è¦æ±‚ï¼Œå› ä¸ºå¿ƒç†å›¾åƒä»¥ç›¸å¯¹è¾ƒä½çš„ä¿¡å™ªæ¯”å’Œç©ºé—´åˆ†è¾¨ç‡ç¼–ç åœ¨äººç±»å¤§è„‘æ´»åŠ¨ä¸­ï¼›ç„¶è€Œï¼Œä»è§†è§‰åˆ°å¿ƒç†å›¾åƒçš„æ³›åŒ–å¯¹äºåŒ»ç–—é¢†åŸŸå’Œè„‘æœºæ¥å£çš„å®é™…åº”ç”¨è‡³å…³é‡è¦ï¼Œæ‰€éœ€ä¿¡æ¯å§‹ç»ˆæ˜¯å†…éƒ¨ç”Ÿæˆçš„ã€‚æˆ‘ä»¬åœ¨NSD-Imageryä¸Šä¸ºä¸€ç³»åˆ—æœ€è¿‘çš„å¼€æ”¾å¼è§†è§‰è§£ç æ¨¡å‹ï¼ˆMindEye1ã€MindEye2ã€Brain Diffuserã€iCNNã€Takagiç­‰äººï¼‰æä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¡¨æ˜è§£ç æ–¹æ³•åœ¨å¿ƒç†å›¾åƒä¸Šçš„æ€§èƒ½ä¸è§†è§‰é‡å»ºæ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯è§£è€¦çš„ã€‚æˆ‘ä»¬è¿˜è¯æ˜æ¶æ„é€‰æ‹©å¯¹è·¨è§£ç æ€§èƒ½æœ‰é‡å¤§å½±å“ï¼šé‡‡ç”¨ç®€å•çº¿æ€§è§£ç æ¶æ„å’Œå¤šæ¨¡æ€ç‰¹å¾è§£ç çš„æ¨¡å‹åœ¨å¿ƒç†å›¾åƒä¸Šçš„æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼Œè€Œå¤æ‚æ¶æ„å¾€å¾€ä¼šå¯¹è§†è§‰è®­ç»ƒæ•°æ®è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¿ƒç†å›¾åƒæ•°æ®é›†å¯¹äºå®é™…åº”ç”¨çš„å‘å±•è‡³å…³é‡è¦ï¼Œå¹¶ç¡®ç«‹äº†NSD-Imageryä½œä¸ºä¸€ä¸ªæœ‰ç”¨çš„èµ„æºï¼Œå¯ä»¥æ›´å¥½åœ°å°†è§†è§‰è§£ç æ–¹æ³•ä¸è¿™ä¸€ç›®æ ‡å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06898v1">PDF</a> Published at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>NSD-Imageryæ•°æ®é›†çš„å‘å¸ƒï¼Œä¸ºç°æœ‰è‡ªç„¶åœºæ™¯æ•°æ®é›†ï¼ˆNSDï¼‰æä¾›äº†è¡¥å……ã€‚NSDæ•°æ®é›†å·²ç»å¤§å¤§æ¨åŠ¨äº†fMRIå›¾åƒé‡å»ºå·¥ä½œçš„å‘å±•ï¼Œä½†æ­¤å‰çš„æ¨¡å‹è¯„ä¼°ä»…é™äºå·²çœ‹åˆ°çš„å›¾åƒé‡å»ºã€‚NSD-Imageryå…è®¸å¯¹æ¨¡å‹åœ¨å¿ƒç†å›¾åƒé‡å»ºæ–¹é¢çš„è¡¨ç°è¿›è¡Œè¯„ä¼°ï¼Œè¿™æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ³›åŒ–è¦æ±‚ã€‚é€šè¿‡å¯¹ä¸€ç³»åˆ—æœ€æ–°NSDè®­ç»ƒçš„å¼€æºè§†è§‰è§£ç æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°è§£ç æ–¹æ³•åœ¨å¿ƒç†å›¾åƒä¸Šçš„è¡¨ç°ä¸è§†è§‰é‡å»ºè¡¨ç°å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç‹¬ç«‹çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°æ¶æ„é€‰æ‹©å¯¹è·¨è§£ç æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚ç®€å•çš„çº¿æ€§è§£ç æ¶æ„å’Œå¤šæ¨¡æ€ç‰¹å¾è§£ç æ¨¡å‹åœ¨æ³›åŒ–åˆ°å¿ƒç†å›¾åƒæ–¹é¢è¡¨ç°æ›´å¥½ï¼Œè€Œå¤æ‚çš„æ¶æ„å¾€å¾€ä¼šå¯¹è§†è§‰è®­ç»ƒæ•°æ®è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¿ƒç†å›¾åƒæ•°æ®é›†å¯¹äºå®é™…åº”ç”¨å‘å±•çš„é‡è¦æ€§ï¼Œå¹¶ç¡®ç«‹äº†NSD-Imageryåœ¨è¿™ä¸€ç›®æ ‡ä¸­çš„æœ‰ç”¨èµ„æºåœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.NSD-Imageryæ˜¯è‡ªç„¶åœºæ™¯æ•°æ®é›†ï¼ˆNSDï¼‰çš„è¡¥å……ï¼ŒåŒ…å«äººç±»fMRIæ´»åŠ¨ä¸å¿ƒç†å›¾åƒçš„é…å¯¹æ•°æ®ã€‚<br>2.NSD-Imageryå…è®¸è¯„ä¼°æ¨¡å‹åœ¨å¿ƒç†å›¾åƒé‡å»ºæ–¹é¢çš„è¡¨ç°ï¼Œè¿™å¯¹æ³›åŒ–èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚<br>3.å¿ƒç†å›¾åƒçš„fMRIä¿¡å·å…·æœ‰è¾ƒä½çš„ä¿¡å™ªæ¯”å’Œç©ºé—´åˆ†è¾¨ç‡ã€‚<br>4.NSDè®­ç»ƒçš„è§†è§‰è§£ç æ¨¡å‹åœ¨NSD-Imageryä¸Šçš„æ€§èƒ½è¯„ä¼°è¡¨æ˜ï¼Œè§£ç æ–¹æ³•åœ¨å¿ƒç†å›¾åƒä¸Šçš„è¡¨ç°ä¸è§†è§‰é‡å»ºè¡¨ç°ç‹¬ç«‹ã€‚<br>5.æ¨¡å‹æ¶æ„é€‰æ‹©å¯¹è·¨è§£ç æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œç®€å•çº¿æ€§è§£ç å’Œå¤šæ¨¡æ€ç‰¹å¾è§£ç æ¨¡å‹æ³›åŒ–æ€§èƒ½è¾ƒå¥½ã€‚<br>6.å¤æ‚çš„æ¨¡å‹æ¶æ„å¯èƒ½è¿‡åº¦ä¾èµ–è§†è§‰è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb1a0c88650a8cb72fcde5a8ce7e55ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc406e9d13e59d45e913a311fa592c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e60d5aec88122be57fdff8582957953.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b96ea38c49ac59727a6c8daf9416e5b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df80ec7903a46787492079b7e4479933.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-326e0e3c70b52caeb01ed0247ffde2a3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Impact-of-the-WHOâ€™s-90-70-90-Strategy-on-HPV-Related-Cervical-Cancer-Control-A-Mathematical-Model-Evaluation-in-China"><a href="#Impact-of-the-WHOâ€™s-90-70-90-Strategy-on-HPV-Related-Cervical-Cancer-Control-A-Mathematical-Model-Evaluation-in-China" class="headerlink" title="Impact of the WHOâ€™s 90-70-90 Strategy on HPV-Related Cervical Cancer   Control: A Mathematical Model Evaluation in China"></a>Impact of the WHOâ€™s 90-70-90 Strategy on HPV-Related Cervical Cancer   Control: A Mathematical Model Evaluation in China</h2><p><strong>Authors:Hua Liu, Chunya Liu, Yumei Wei, Qibin Zhang, Jingyan Ma</strong></p>
<p>In August 2020, the World Health Assembly approved the Global Strategy to eliminate cervical cancer, marking the first time that numerous countries committed to eliminating a form of cancer. China introduced the HPV vaccine in 2016 and has made significant advancements in both prevention and treatment strategies. However, due to the relatively late introduction of the vaccine, the burden of cervical cancer in China continues to rise. In light of this, we develop a compartmental model to assess the impact of the WHOâ€™s 90-70-90 strategy, along with adult catch-up vaccination, on the control of HPV-induced cervical cancer in China. We analyze the basic properties of the model and provide proofs of the local and global asymptotic stability of the equilibrium points. Additionally, a sensitivity analysis is performed, and we use the MCMC algorithm to fit the number of new cervical cancer cases and deaths in China from 1990 to 2021. The estimated basic reproduction number before and after the introduction of the HPV vaccine in China is 1.5026 (95% CI: 1.4051-1.6002) and 1.0726 (95% CI: 0.9384-1.2067), respectively. The sensitivity analysis reveals that screening, as a non-pharmaceutical intervention, plays a crucial role in controlling the spread of the disease. We apply the 90-70-90 strategy to predict the future number of new cervical cancer cases and deaths in China. The results indicate that prioritizing the 70-90 target combination is the most cost-effective approach and can achieve the goal of zero new cervical cancer cases by 2061. Finally, an optimal control model is developed to explore the best implementation strategies for HPV vaccination and screening under various plausible scenarios. </p>
<blockquote>
<p>åœ¨2020å¹´8æœˆï¼Œä¸–ç•Œå«ç”Ÿå¤§ä¼šæ‰¹å‡†äº†å…¨çƒæ¶ˆé™¤å®«é¢ˆç™Œæˆ˜ç•¥ï¼Œè¿™æ˜¯è®¸å¤šå›½å®¶é¦–æ¬¡æ‰¿è¯ºæ¶ˆé™¤ä¸€ç§å½¢å¼çš„ç™Œç—‡ã€‚ä¸­å›½åœ¨2016å¹´å¼•å…¥äº†HPVç–«è‹—ï¼Œå¹¶åœ¨é¢„é˜²å’Œæ²»ç–—ç­–ç•¥æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç–«è‹—å¼•å…¥ç›¸å¯¹è¾ƒæ™šï¼Œä¸­å›½å®«é¢ˆç™Œçš„è´Ÿæ‹…ä»åœ¨å¢åŠ ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªéƒ¨åˆ†æ¨¡å‹ï¼Œä»¥è¯„ä¼°ä¸–å«ç»„ç»‡90-70-90æˆ˜ç•¥ä»¥åŠæˆäººè¡¥ç§ç–«è‹—å¯¹ä¸­å›½HPVæ‰€è‡´å®«é¢ˆç™Œçš„æ§åˆ¶å½±å“ã€‚æˆ‘ä»¬åˆ†æäº†æ¨¡å‹çš„åŸºæœ¬æ€§è´¨ï¼Œå¹¶è¯æ˜äº†å¹³è¡¡ç‚¹çš„å±€éƒ¨å’Œå…¨å±€æ¸è¿‘ç¨³å®šæ€§ã€‚å¦å¤–ï¼Œè¿›è¡Œäº†æ•æ„Ÿæ€§åˆ†æï¼Œå¹¶ä½¿ç”¨MCMCç®—æ³•æ‹Ÿåˆä¸­å›½1990å¹´è‡³2021å¹´çš„æ–°å®«é¢ˆç™Œç—…ä¾‹å’Œæ­»äº¡äººæ•°ã€‚åœ¨å¼•å…¥HPVç–«è‹—å‰åï¼Œä¼°è®¡çš„åŸºæœ¬å†ç”Ÿæ•°åˆ†åˆ«ä¸º1.5026ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š1.4051-1.6002ï¼‰å’Œ1.0726ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š0.9384-1.2067ï¼‰ã€‚æ•æ„Ÿæ€§åˆ†æè¡¨æ˜ï¼Œç­›æŸ¥ä½œä¸ºä¸€ç§éè¯ç‰©å¹²é¢„æªæ–½ï¼Œåœ¨æ§åˆ¶ç–¾ç—…ä¼ æ’­æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬åº”ç”¨90-70-90æˆ˜ç•¥é¢„æµ‹äº†ä¸­å›½æœªæ¥æ–°å®«é¢ˆç™Œç—…ä¾‹å’Œæ­»äº¡äººæ•°ã€‚ç»“æœè¡¨æ˜ï¼Œä¼˜å…ˆè€ƒè™‘70-90çš„ç›®æ ‡ç»„åˆæ˜¯æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ï¼Œå¹¶æœ‰æœ›åœ¨2061å¹´å®ç°é›¶æ–°å®«é¢ˆç™Œç—…ä¾‹çš„ç›®æ ‡ã€‚æœ€åï¼Œå¼€å‘äº†ä¸€ä¸ªæœ€ä¼˜æ§åˆ¶æ¨¡å‹ï¼Œä»¥æ¢ç´¢åœ¨å„ç§å¯èƒ½æƒ…å†µä¸‹å®æ–½HPVç–«è‹—æ¥ç§å’Œç­›æŸ¥çš„æœ€ä½³ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06405v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸­å›½äº2016å¹´å¼•å…¥HPVç–«è‹—ï¼Œå¹¶åœ¨é¢„é˜²å’Œæ²»ç–—ç­–ç•¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç–«è‹—å¼•å…¥ç›¸å¯¹è¾ƒæ™šï¼Œä¸­å›½å®«é¢ˆç™Œçš„è´Ÿæ‹…ä»åœ¨å¢åŠ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¨¡å‹æ¥è¯„ä¼°ä¸–ç•Œå«ç”Ÿç»„ç»‡90-70-90ç­–ç•¥ä»¥åŠæˆäººè¡¥ç§ç–«è‹—å¯¹ä¸­å›½å®«é¢ˆç™Œçš„æ§åˆ¶å½±å“ã€‚åˆ†æè¡¨æ˜ï¼Œç­›æŸ¥ä½œä¸ºéè¯ç‰©å¹²é¢„æªæ–½åœ¨æ§åˆ¶ç–¾ç—…ä¼ æ’­ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å®æ–½90-70-90ç­–ç•¥é¢„æµ‹ï¼Œä¼˜å…ˆå®ç°70-90çš„ç›®æ ‡ç»„åˆæ˜¯æœ€å…·æˆæœ¬æ•ˆç›Šçš„ï¼Œå¹¶æœ‰æœ›åœ¨2061å¹´å®ç°é›¶æ–°å¢å®«é¢ˆç™Œç—…ä¾‹çš„ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸–ç•Œå«ç”Ÿç»„ç»‡äº2020å¹´é€šè¿‡äº†å…¨çƒæ¶ˆé™¤å®«é¢ˆç™Œçš„æˆ˜ç•¥ï¼Œè¿™æ˜¯å„å›½é¦–æ¬¡æ‰¿è¯ºæ¶ˆé™¤ä¸€ç§å½¢å¼çš„ç™Œç—‡ã€‚</li>
<li>ä¸­å›½è‡ª2016å¹´å¼•å…¥HPVç–«è‹—ï¼Œå¹¶åœ¨é¢„é˜²å’Œæ²»ç–—ç­–ç•¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å®«é¢ˆç™Œè´Ÿæ‹…ä»åœ¨ä¸Šå‡ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ¨¡å‹æ¥è¯„ä¼°90-70-90ç­–ç•¥ä»¥åŠæˆäººè¡¥ç§ç–«è‹—å¯¹æ§åˆ¶ä¸­å›½HPVå¯¼è‡´çš„å®«é¢ˆç™Œçš„å½±å“ã€‚</li>
<li>ç­›æŸ¥ä½œä¸ºéè¯ç‰©å¹²é¢„åœ¨æ§åˆ¶ç–¾ç—…ä¼ æ’­ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ä¼˜å…ˆå®ç°70-90ç›®æ ‡ç»„åˆæ˜¯æœ€å…·æˆæœ¬æ•ˆç›Šçš„ï¼Œæœ‰æœ›åˆ°2061å¹´å®ç°é›¶æ–°å¢å®«é¢ˆç™Œç—…ä¾‹ã€‚</li>
<li>è¿›è¡Œäº†æ•æ„Ÿæ€§åˆ†æå¹¶ä½¿ç”¨MCMCç®—æ³•æ‹Ÿåˆäº†1990å¹´è‡³2021å¹´ä¸­å›½å®«é¢ˆç™Œæ–°å‘ç—…ä¾‹å’Œæ­»äº¡äººæ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b1b6b2b1b85b80c12bb6293f470b1dcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea311fac5e80576c3ccfcbcd3b0bc5ce.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ResPF-Residual-Poisson-Flow-for-Efficient-and-Physically-Consistent-Sparse-View-CT-Reconstruction"><a href="#ResPF-Residual-Poisson-Flow-for-Efficient-and-Physically-Consistent-Sparse-View-CT-Reconstruction" class="headerlink" title="ResPF: Residual Poisson Flow for Efficient and Physically Consistent   Sparse-View CT Reconstruction"></a>ResPF: Residual Poisson Flow for Efficient and Physically Consistent   Sparse-View CT Reconstruction</h2><p><strong>Authors:Changsheng Fang, Yongtong Liu, Bahareh Morovati, Shuo Han, Yu Shi, Li Zhou, Shuyi Fan, Hengyong Yu</strong></p>
<p>Sparse-view computed tomography (CT) is a practical solution to reduce radiation dose, but the resulting ill-posed inverse problem poses significant challenges for accurate image reconstruction. Although deep learning and diffusion-based methods have shown promising results, they often lack physical interpretability or suffer from high computational costs due to iterative sampling starting from random noise. Recent advances in generative modeling, particularly Poisson Flow Generative Models (PFGM), enable high-fidelity image synthesis by modeling the full data distribution. In this work, we propose Residual Poisson Flow (ResPF) Generative Models for efficient and accurate sparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional guidance from sparse measurements and employs a hijacking strategy to significantly reduce sampling cost by skipping redundant initial steps. However, skipping early stages can degrade reconstruction quality and introduce unrealistic structures. To address this, we embed a data-consistency into each iteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling relies on a fixed ordinary differential equation (ODE) trajectory induced by electrostatic fields, which can be disrupted by step-wise data consistency, resulting in unstable or degraded reconstructions. Inspired by ResNet, we introduce a residual fusion module to linearly combine generative outputs with data-consistent reconstructions, effectively preserving trajectory continuity. To the best of our knowledge, this is the first application of Poisson flow models to sparse-view CT. Extensive experiments on synthetic and clinical datasets demonstrate that ResPF achieves superior reconstruction quality, faster inference, and stronger robustness compared to state-of-the-art iterative, learning-based, and diffusion models. </p>
<blockquote>
<p>ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯å‡å°‘è¾å°„å‰‚é‡çš„å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±æ­¤äº§ç”Ÿçš„é€‚å®šæ€§å·®çš„é€†é—®é¢˜ç»™å‡†ç¡®çš„å›¾åƒé‡å»ºå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹ç‰©ç†å¯è§£é‡Šæ€§ï¼Œæˆ–è€…ç”±äºä»éšæœºå™ªå£°å¼€å§‹è¿›è¡Œè¿­ä»£é‡‡æ ·è€Œé¢ä¸´é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯Poissonæµç”Ÿæˆæ¨¡å‹ï¼ˆPFGMï¼‰ï¼Œé€šè¿‡å»ºæ¨¡å…¨æ•°æ®åˆ†å¸ƒå®ç°äº†é«˜ä¿çœŸå›¾åƒåˆæˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºé«˜æ•ˆå’Œå‡†ç¡®ç¨€ç–è§†å›¾CTé‡å»ºçš„Residual Poisson Flowï¼ˆResPFï¼‰ç”Ÿæˆæ¨¡å‹ã€‚åŸºäºPFGM++ï¼ŒResPFç»“åˆäº†ç¨€ç–æµ‹é‡çš„æ¡ä»¶æŒ‡å¯¼ï¼Œå¹¶é‡‡ç”¨åŠ«æŒç­–ç•¥æ¥æ˜¾è‘—å‡å°‘é‡‡æ ·æˆæœ¬ï¼Œè·³è¿‡å†—ä½™çš„åˆå§‹æ­¥éª¤ã€‚ç„¶è€Œï¼Œè·³è¿‡æ—©æœŸé˜¶æ®µå¯èƒ½ä¼šé™ä½é‡å»ºè´¨é‡å¹¶å¼•å…¥ä¸ç°å®çš„ç»“æ„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ•°æ®ä¸€è‡´æ€§åµŒå…¥åˆ°æ¯æ¬¡è¿­ä»£ä¸­ï¼Œç¡®ä¿å¯¹ç¨€ç–è§†å›¾æµ‹é‡çš„ä¿çœŸåº¦ã€‚ç„¶è€Œï¼ŒPFGMé‡‡æ ·ä¾èµ–äºç”±é™ç”µåœºè¯±å¯¼çš„å›ºå®šå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œåˆ†æ­¥æ•°æ®ä¸€è‡´æ€§å¯èƒ½ä¼šç ´åè¿™ä¸€è½¨è¿¹ï¼Œå¯¼è‡´ä¸ç¨³å®šæˆ–è´¨é‡ä¸‹é™çš„é‡å»ºã€‚å—ResNetçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ®‹å·®èåˆæ¨¡å—ï¼Œçº¿æ€§ç»„åˆç”Ÿæˆè¾“å‡ºå’Œæ•°æ®ä¸€è‡´é‡å»ºï¼Œæœ‰æ•ˆåœ°ä¿æŒè½¨è¿¹è¿ç»­æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯Poissonæµæ¨¡å‹åœ¨ç¨€ç–è§†å›¾CTä¸­çš„é¦–æ¬¡åº”ç”¨ã€‚åœ¨åˆæˆå’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„è¿­ä»£ã€åŸºäºå­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒResPFåœ¨é‡å»ºè´¨é‡ã€æ¨ç†é€Ÿåº¦å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06400v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å¯é™ä½è¾å°„å‰‚é‡ï¼Œä½†å…¶äº§ç”Ÿçš„é€†å‘é—®é¢˜ç»™ç²¾ç¡®å›¾åƒé‡å»ºå¸¦æ¥æŒ‘æˆ˜ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å’Œæ‰©æ•£æ–¹æ³•æ˜¾ç¤ºå‡ºå‰æ™¯ï¼Œä½†å®ƒä»¬ç¼ºä¹ç‰©ç†å¯è§£é‡Šæ€§æˆ–å› è¿­ä»£é‡‡æ ·æˆæœ¬é«˜è€Œå—é™ã€‚æœ¬ç ”ç©¶æå‡ºåŸºäºPoissonæµç”Ÿæˆæ¨¡å‹çš„æ®‹å·®Poissonæµï¼ˆResPFï¼‰æ¨¡å‹ï¼Œç”¨äºé«˜æ•ˆå‡†ç¡®çš„ç¨€ç–è§†å›¾CTé‡å»ºã€‚ResPFç»“åˆç¨€ç–æµ‹é‡çš„æ¡ä»¶æŒ‡å¯¼ï¼Œé‡‡ç”¨åŠ«æŒç­–ç•¥å‡å°‘é‡‡æ ·æˆæœ¬ã€‚ä¸ºç¡®ä¿é‡å»ºè´¨é‡å¹¶é¿å…å¼•å…¥ä¸çœŸå®ç»“æ„ï¼Œæˆ‘ä»¬åµŒå…¥è¿­ä»£ä¸­çš„æ•°æ®ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œæ•°æ®ä¸€è‡´æ€§å¯èƒ½ä¼šç ´åç”±é™ç”µåœºå¼•å¯¼çš„å¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹ï¼Œå¯¼è‡´ä¸ç¨³å®šæˆ–é€€åŒ–é‡å»ºã€‚å—ResNetå¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥æ®‹å·®èåˆæ¨¡å—ï¼Œå°†ç”Ÿæˆè¾“å‡ºä¸æ•°æ®ä¸€è‡´é‡å»ºç›¸ç»“åˆï¼Œæœ‰æ•ˆä¿æŒè½¨è¿¹è¿ç»­æ€§ã€‚åœ¨åˆæˆå’Œä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒResPFåœ¨é‡å»ºè´¨é‡ã€æ¨ç†é€Ÿåº¦å’Œç¨³å¥æ€§æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„è¿­ä»£ã€åŸºäºå­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sparse-view CTæ˜¯å‡å°‘è¾å°„å‰‚é‡çš„å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œä½†é€†å‘é—®é¢˜ä¸ºå‡†ç¡®å›¾åƒé‡å»ºå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å’Œæ‰©æ•£æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†å­˜åœ¨ç‰©ç†å¯è§£é‡Šæ€§ä¸è¶³å’Œè®¡ç®—æˆæœ¬é«˜çš„ç¼ºç‚¹ã€‚</li>
<li>ç ”ç©¶æå‡ºåŸºäºPoissonæµç”Ÿæˆæ¨¡å‹çš„ResPFæ¨¡å‹ï¼Œå®ç°é«˜æ•ˆå‡†ç¡®çš„ç¨€ç–è§†å›¾CTé‡å»ºã€‚</li>
<li>ResPFç»“åˆæ¡ä»¶æŒ‡å¯¼å’ŒåŠ«æŒç­–ç•¥å‡å°‘é‡‡æ ·æˆæœ¬ï¼ŒåŒæ—¶åµŒå…¥æ•°æ®ä¸€è‡´æ€§ä»¥ç¡®ä¿è´¨é‡ã€‚</li>
<li>æ•°æ®ä¸€è‡´æ€§å¯èƒ½ç ´åå¸¸å¾®åˆ†æ–¹ç¨‹è½¨è¿¹ï¼Œå¯¼è‡´ä¸ç¨³å®šæˆ–é€€åŒ–é‡å»ºã€‚</li>
<li>å¼•å…¥æ®‹å·®èåˆæ¨¡å—ï¼Œç»“åˆç”Ÿæˆè¾“å‡ºå’Œæ•°æ®ä¸€è‡´é‡å»ºï¼Œä¿æŒè½¨è¿¹è¿ç»­æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06400">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df55e2ef80d03f1a929197041a6e4acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7e00cf116a32add1567c776279460e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11be2418dd6460088d085cfdb678e54a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c71dd2448c6d6152195e2d0cdc750350.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Exploring-Adversarial-Watermarking-in-Transformer-Based-Models-Transferability-and-Robustness-Against-Defense-Mechanism-for-Medical-Images"><a href="#Exploring-Adversarial-Watermarking-in-Transformer-Based-Models-Transferability-and-Robustness-Against-Defense-Mechanism-for-Medical-Images" class="headerlink" title="Exploring Adversarial Watermarking in Transformer-Based Models:   Transferability and Robustness Against Defense Mechanism for Medical Images"></a>Exploring Adversarial Watermarking in Transformer-Based Models:   Transferability and Robustness Against Defense Mechanism for Medical Images</h2><p><strong>Authors:Rifat Sadik, Tanvir Rahman, Arpan Bhattacharjee, Bikash Chandra Halder, Ismail Hossain</strong></p>
<p>Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism â€“ adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çš®è‚¤ç§‘å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä¸ºè‡ªåŠ¨åŒ–çš®è‚¤ç–¾ç—…è¯Šæ–­æä¾›äº†æ½œåŠ›ã€‚ä»¥å‰ï¼ŒåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¶æ„åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ä»»åŠ¡ä¸­éå¸¸å—æ¬¢è¿å¹¶è·å¾—äº†å·¨å¤§æˆåŠŸï¼Œå¦‚çš®è‚¤å›¾åƒè¯†åˆ«ã€ç”Ÿæˆå’Œè§†é¢‘åˆ†æã€‚ä½†éšç€åŸºäºtransformerçš„æ¨¡å‹çš„å…´èµ·ï¼Œç°åœ¨çš„CVä»»åŠ¡å¤§å¤šä½¿ç”¨è¿™äº›æ¨¡å‹æ¥å®Œæˆã€‚Vision Transformersï¼ˆViTsï¼‰å°±æ˜¯è¿™æ ·ä¸€ç§åŸºäºtransformerçš„æ¨¡å‹ï¼Œå®ƒåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æˆåŠŸã€‚å®ƒåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å…¨å±€æ³¨æ„åŠ›æœºåˆ¶çš„ä¾èµ–ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ‰°åŠ¨çš„æ”»å‡»ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶ViTså¯¹åŒ»å­¦å›¾åƒå—åˆ°å¯¹æŠ—æ€§æ°´å°çš„æ˜“æ„Ÿæ€§â€”â€”ä¸€ç§æ·»åŠ æ‰€è°“çš„ä¸æ˜“å¯Ÿè§‰çš„æ‰°åŠ¨æ¥æ¬ºéª—æ¨¡å‹çš„æ–¹æ³•ã€‚é€šè¿‡æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ï¼ˆPGDï¼‰ç”Ÿæˆå¯¹æŠ—æ€§æ°´å°ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¿™ç§æ”»å‡»å¯¹CNNçš„è¿ç§»æ€§ï¼Œå¹¶åˆ†æäº†æ€§èƒ½é˜²å¾¡æœºåˆ¶â€”â€”å¯¹æŠ—æ€§è®­ç»ƒã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¯¹äºå¹²å‡€å›¾åƒçš„æ€§èƒ½æ²¡æœ‰å—åˆ°å½±å“ï¼Œä½†ViTsç¡®å®æ›´å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼šå‡†ç¡®ç‡ä¸‹é™é«˜è¾¾27.6%ã€‚ç„¶è€Œï¼Œå¯¹æŠ—æ€§è®­ç»ƒå°†å…¶æé«˜åˆ°äº†90.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06389v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çš®è‚¤ç§‘å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä¸ºè‡ªåŠ¨åŒ–çš®è‚¤ç–¾ç—…è¯Šæ–­æä¾›äº†æ½œåŠ›ã€‚å°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨çš®è‚¤å›¾åƒè¯†åˆ«ã€ç”Ÿæˆå’Œè§†é¢‘åˆ†æç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å¹¿å—æ¬¢è¿å¹¶è·å¾—äº†æˆåŠŸï¼Œä½†éšç€åŸºäºå˜å‹å™¨çš„æ¨¡å‹çš„å…´èµ·ï¼Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ç°åœ¨æ›´å¤šåœ°ä½¿ç”¨è¿™äº›æ¨¡å‹æ¥å®Œæˆã€‚å…¶ä¸­ï¼ŒVision Transformersï¼ˆViTï¼‰ç­‰åŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä¸­å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¾èµ–äºå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ‰°åŠ¨çš„æ”»å‡»ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶åŒ»å­¦å›¾åƒViTå¯¹æŠ—æ€§æ°´å°çš„æ•æ„Ÿæ€§ã€‚é€šè¿‡æŠ•å½±æ¢¯åº¦ä¸‹é™æ³•ç”Ÿæˆå¯¹æŠ—æ€§æ°´å°ï¼Œæˆ‘ä»¬æµ‹è¯•äº†æ­¤ç±»æ”»å‡»å¯¹CNNçš„è¿ç§»æ€§ï¼Œå¹¶åˆ†æäº†é˜²å¾¡æœºåˆ¶â€”â€”å¯¹æŠ—æ€§è®­ç»ƒçš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒViTå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œæ¸…æ´å›¾åƒçš„å‡†ç¡®æ€§é™ä½äº†27.6%ï¼Œä½†å¯¹æŠ—æ€§è®­ç»ƒå¯ä»¥æé«˜åˆ°90.0%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çš®è‚¤ç§‘å›¾åƒåˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯ç”¨äºè‡ªåŠ¨åŒ–çš®è‚¤ç–¾ç—…è¯Šæ–­ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­æ›¾å¹¿å—æ¬¢è¿å¹¶æˆåŠŸåº”ç”¨ã€‚</li>
<li>Vision Transformersï¼ˆViTï¼‰æ˜¯æ–°å…´çš„åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œå·²åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>ViTæ¨¡å‹ä¾èµ–äºå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ‰°åŠ¨çš„å½±å“ã€‚</li>
<li>å¯¹æŠ—æ€§æ°´å°æ–¹æ³•å¯ç”¨äºæ”»å‡»åŒ»å­¦å›¾åƒçš„ViTæ¨¡å‹ã€‚</li>
<li>å¯¹æŠ—æ€§è®­ç»ƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„é˜²å¾¡æœºåˆ¶ï¼Œå¯ä»¥æé«˜æ¨¡å‹å¯¹å¯¹æŠ—æ€§æ”»å‡»çš„æŠµæŠ—èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5f77c65eded165b28d518fc9b695e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8272d9b9835378a57f4b8216ced7de4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TissUnet-Improved-Extracranial-Tissue-and-Cranium-Segmentation-for-Children-through-Adulthood"><a href="#TissUnet-Improved-Extracranial-Tissue-and-Cranium-Segmentation-for-Children-through-Adulthood" class="headerlink" title="TissUnet: Improved Extracranial Tissue and Cranium Segmentation for   Children through Adulthood"></a>TissUnet: Improved Extracranial Tissue and Cranium Segmentation for   Children through Adulthood</h2><p><strong>Authors:Markiian Mandzak, Elvira Yang, Anna Zapaishchykova, Yu-Hui Chen, Lucas Heilbroner, John Zielke, Divyanshu Tak, Reza Mojahed-Yazdi, Francesca Romana Mussa, Zezhong Ye, Sridhar Vajapeyam, Viviana Benitez, Ralph Salloum, Susan N. Chi, Houman Sotoudeh, Jakob Seidlitz, Sabine Mueller, Hugo J. W. L. Aerts, Tina Y. Poussaint, Benjamin H. Kann</strong></p>
<p>Extracranial tissues visible on brain magnetic resonance imaging (MRI) may hold significant value for characterizing health conditions and clinical decision-making, yet they are rarely quantified. Current tools have not been widely validated, particularly in settings of developing brains or underlying pathology. We present TissUnet, a deep learning model that segments skull bone, subcutaneous fat, and muscle from routine three-dimensional T1-weighted MRI, with or without contrast enhancement. The model was trained on 155 paired MRI-computed tomography (CT) scans and validated across nine datasets covering a wide age range and including individuals with brain tumors. In comparison to AI-CT-derived labels from 37 MRI-CT pairs, TissUnet achieved a median Dice coefficient of 0.79 [IQR: 0.77-0.81] in a healthy adult cohort. In a second validation using expert manual annotations, median Dice was 0.83 [IQR: 0.83-0.84] in healthy individuals and 0.81 [IQR: 0.78-0.83] in tumor cases, outperforming previous state-of-the-art method. Acceptability testing resulted in an 89% acceptance rate after adjudication by a tie-breaker(N&#x3D;108 MRIs), and TissUnet demonstrated excellent performance in the blinded comparative review (N&#x3D;45 MRIs), including both healthy and tumor cases in pediatric populations. TissUnet enables fast, accurate, and reproducible segmentation of extracranial tissues, supporting large-scale studies on craniofacial morphology, treatment effects, and cardiometabolic risk using standard brain T1w MRI. </p>
<blockquote>
<p>å¤§è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­å¯è§é¢…å¤–ç»„ç»‡å¯¹äºè¡¨å¾å¥åº·çŠ¶å†µå’Œä¸´åºŠå†³ç­–å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å¾ˆå°‘å¯¹å…¶è¿›è¡Œé‡åŒ–åˆ†æã€‚å½“å‰å·¥å…·å°šæœªå¾—åˆ°å¹¿æ³›éªŒè¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘è‚²ä¸­çš„å¤§è„‘æˆ–åŸºç¡€ç—…ç†å­¦çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†TissUnetï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ä»å¸¸è§„çš„ä¸‰ç»´T1åŠ æƒMRIä¸­åˆ†å‰²é¢…éª¨ã€çš®ä¸‹è„‚è‚ªå’Œè‚Œè‚‰ï¼Œæ— è®ºæ˜¯å¦è¿›è¡Œå¢å¼ºå¯¹æ¯”ã€‚è¯¥æ¨¡å‹åœ¨155å¯¹MRI-è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¶µç›–å¹¿æ³›å¹´é¾„èŒƒå›´ä¸”åŒ…æ‹¬è„‘è‚¿ç˜¤æ‚£è€…åœ¨å†…çš„ä¹ä¸ªæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ã€‚ä¸æ¥è‡ª37å¯¹MRI-CTçš„AI-CTè¡ç”Ÿæ ‡ç­¾ç›¸æ¯”ï¼ŒTissUnetåœ¨å¥åº·æˆäººé˜Ÿåˆ—ä¸­çš„ç‹„å…‹ç³»æ•°ä¸­ä½æ•°ä¸º0.79[IQRï¼š0.77-0.81]ã€‚åœ¨ä¸“å®¶æ‰‹åŠ¨æ³¨é‡Šçš„ç¬¬äºŒæ¬¡éªŒè¯ä¸­ï¼Œå¥åº·ä¸ªä½“çš„ç‹„å…‹ç³»æ•°ä¸­ä½æ•°ä¸º0.83[IQRï¼š0.83-0.84]ï¼Œè‚¿ç˜¤æ‚£è€…çš„ç‹„å…‹ç³»æ•°ä¸º0.81[IQRï¼š0.78-0.83]ï¼Œä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å¯æ¥å—æ€§æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»²è£è€…ï¼ˆN&#x3D;108 MRIï¼‰è£å®šåï¼Œæ¥å—ç‡ä¸º89%ï¼ŒTissUnetåœ¨ç›²æ³•æ¯”è¾ƒå®¡æŸ¥ï¼ˆN&#x3D;45 MRIï¼‰ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å„¿ç§‘äººç¾¤ä¸­çš„å¥åº·ä¸ªä½“å’Œè‚¿ç˜¤ç—…ä¾‹ã€‚TissUnetèƒ½å¤Ÿå®ç°é¢…å¤–ç»„ç»‡çš„å¿«é€Ÿã€å‡†ç¡®å’Œå¯é‡å¤åˆ†å‰²ï¼Œæ”¯æŒåˆ©ç”¨æ ‡å‡†å¤§è„‘T1w MRIè¿›è¡Œå¤§è§„æ¨¡çš„é¢é¢…å½¢æ€å­¦ã€æ²»ç–—æ•ˆåº”å’Œå¿ƒè¡€ç®¡ä»£è°¢é£é™©ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05660v2">PDF</a> 44 pages, 4 tables, 6 figures, supplementary material</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTissUnetçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åœ¨æ— å¯¹æ¯”å¢å¼ºçš„å¸¸è§„ä¸‰ç»´T1åŠ æƒMRIä¸­ï¼Œå¯¹é¢…éª¨ã€çš®ä¸‹è„‚è‚ªå’Œè‚Œè‚‰è¿›è¡Œåˆ†å‰²ã€‚æ¨¡å‹åœ¨å¹¿æ³›çš„å¹´é¾„èŒƒå›´å†…ä»¥åŠåŒ…æ‹¬è„‘è‚¿ç˜¤æ‚£è€…åœ¨å†…çš„å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TissUnetæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ†å‰²é¢…éª¨ã€çš®ä¸‹è„‚è‚ªå’Œè‚Œè‚‰ç»„ç»‡ã€‚</li>
<li>æ¨¡å‹åœ¨å¸¸è§„ä¸‰ç»´T1åŠ æƒMRIä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ— å¯¹æ¯”å¢å¼ºçš„MRIå›¾åƒä¸­è¿›è¡Œæœ‰æ•ˆåˆ†å‰²ã€‚</li>
<li>TissUnetåœ¨å¹¿æ³›å¹´é¾„èŒƒå›´ä»¥åŠåŒ…æ‹¬è„‘è‚¿ç˜¤æ‚£è€…åœ¨å†…çš„å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>ä¸AI-CTè¡ç”Ÿæ ‡ç­¾ç›¸æ¯”ï¼ŒTissUnetåœ¨å¥åº·æˆäººé˜Ÿåˆ—ä¸­çš„è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ä¸“å®¶æ‰‹åŠ¨æ³¨é‡Šç›¸æ¯”ï¼ŒTissUnetåœ¨æ¥å—ç‡æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜æ¥å—ç‡ã€‚</li>
<li>TissUnetä¸ºå¿«é€Ÿã€å‡†ç¡®å’Œå¯é‡å¤çš„é¢…å¤–ç»„ç»‡åˆ†å‰²æä¾›äº†å¯èƒ½ï¼Œæ”¯æŒå¤§è§„æ¨¡ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b02d1b34e8a90d337ef892ce78dd10ea.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities"><a href="#Hypergraph-Tversky-Aware-Domain-Incremental-Learning-for-Brain-Tumor-Segmentation-with-Missing-Modalities" class="headerlink" title="Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities"></a>Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor   Segmentation with Missing Modalities</h2><p><strong>Authors:Junze Wang, Lei Fan, Weipeng Jing, Donglin Di, Yang Song, Sidong Liu, Cong Cong</strong></p>
<p>Existing methods for multimodal MRI segmentation with missing modalities typically assume that all MRI modalities are available during training. However, in clinical practice, some modalities may be missing due to the sequential nature of MRI acquisition, leading to performance degradation. Furthermore, retraining models to accommodate newly available modalities can be inefficient and may cause overfitting, potentially compromising previously learned knowledge. To address these challenges, we propose Replay-based Hypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation with missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to enable the segmentation model to learn from newly acquired MRI modalities without forgetting previously learned information. To enhance segmentation performance across diverse patient scenarios, we introduce the Cross-Patient Hypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture high-order associations between patients. Additionally, we incorporate Tversky-Aware Contrastive (TAC) loss to effectively mitigate information imbalance both across and within different modalities. Extensive experiments on the BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art methods, achieving an improvement of over 2% in the Dice Similarity Coefficient across various tumor regions. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/reeive/ReHyDIL">https://github.com/reeive/ReHyDIL</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ¨¡æ€MRIåˆ†å‰²ç¼ºå¤±æ¨¡æ€çš„æ–¹æ³•é€šå¸¸å‡è®¾åœ¨è®­ç»ƒæœŸé—´æ‰€æœ‰MRIæ¨¡æ€éƒ½æ˜¯å¯ç”¨çš„ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠå®è·µä¸­ï¼Œç”±äºMRIé‡‡é›†çš„åºåˆ—æ€§è´¨ï¼ŒæŸäº›æ¨¡æ€å¯èƒ½ä¼šç¼ºå¤±ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æ­¤å¤–ï¼Œä¸ºäº†å®¹çº³æ–°å¯ç”¨çš„æ¨¡æ€è€Œé‡æ–°è®­ç»ƒæ¨¡å‹å¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œå¹¶å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œå¯èƒ½æŸå®³ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›æ”¾è¶…å›¾åŸŸå¢é‡å­¦ä¹ ï¼ˆReHyDILï¼‰çš„æ–¹æ³•ï¼Œç”¨äºå…·æœ‰ç¼ºå¤±æ¨¡æ€çš„è„‘è‚¿ç˜¤åˆ†å‰²ã€‚ReHyDILåˆ©ç”¨åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰ä½¿åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿä»æ–°è·å–çš„MRIæ¨¡æ€ä¸­å­¦ä¹ ï¼Œè€Œä¸ä¼šå¿˜è®°ä¹‹å‰å­¦åˆ°çš„ä¿¡æ¯ã€‚ä¸ºäº†å¢å¼ºåœ¨ä¸åŒæ‚£è€…åœºæ™¯ä¸­çš„åˆ†å‰²æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰ï¼Œè¯¥ç½‘ç»œåˆ©ç”¨è¶…å›¾æ¥æ•è·æ‚£è€…ä¹‹é—´çš„é«˜é˜¶å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†Tverskyæ„ŸçŸ¥å¯¹æ¯”ï¼ˆTACï¼‰æŸå¤±ï¼Œä»¥æœ‰æ•ˆå‡è½»ä¸åŒæ¨¡æ€ä¹‹é—´å’Œå†…éƒ¨çš„ä¿¡æ¯ä¸å¹³è¡¡ã€‚åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒReHyDILä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å„ç§è‚¿ç˜¤åŒºåŸŸçš„Diceç›¸ä¼¼ç³»æ•°ä¸Šæé«˜äº†è¶…è¿‡2%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/reeive/ReHyDIL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/reeive/ReHyDILä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16809v3">PDF</a> MICCAI 2025 Early Accept. The code is available at   <a target="_blank" rel="noopener" href="https://github.com/reeive/ReHyDIL">https://github.com/reeive/ReHyDIL</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé‡æ’­çš„è¶…å›¾åŸŸå¢é‡å­¦ä¹ ï¼ˆReHyDILï¼‰æ–¹æ³•ï¼Œç”¨äºå¤„ç†MRIå›¾åƒä¸­ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹çš„è„‘è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰æŠ€æœ¯ï¼Œä½¿å¾—åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿåœ¨è·å–æ–°çš„MRIæ¨¡æ€æ—¶å­¦ä¹ ï¼ŒåŒæ—¶ä¸ä¼šå¿˜è®°ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰å’ŒTverskyæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼ˆTAC lossï¼‰ï¼Œä»¥æé«˜ä¸åŒæ‚£è€…åœºæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½å¹¶æœ‰æ•ˆç¼“è§£ä¸åŒæ¨¡æ€é—´å’Œæ¨¡æ€å†…çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReHyDILæ–¹æ³•ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å„ç±»è‚¿ç˜¤åŒºåŸŸçš„Diceç›¸ä¼¼ç³»æ•°ä¸Šæé«˜äº†è¶…è¿‡2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReHyDILæ–¹æ³•è§£å†³äº†MRIå›¾åƒä¸­ç¼ºå¤±æ¨¡æ€å¯¼è‡´çš„åˆ†å‰²æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆåŸŸå¢é‡å­¦ä¹ ï¼ˆDILï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨è·å–æ–°çš„MRIæ¨¡æ€æ—¶å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒå¯¹å…ˆå‰çŸ¥è¯†çš„è®°å¿†ã€‚</li>
<li>å¼•å…¥çš„è·¨æ‚£è€…è¶…å›¾åˆ†å‰²ç½‘ç»œï¼ˆCHSNetï¼‰èƒ½å¤Ÿæ•æ‰æ‚£è€…é—´çš„é«˜é˜¶å…³è”ï¼Œæé«˜åœ¨ä¸åŒæ‚£è€…åœºæ™¯ä¸‹çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>Tverskyæ„ŸçŸ¥å¯¹æ¯”æŸå¤±ï¼ˆTAC lossï¼‰æœ‰æ•ˆç¼“è§£äº†ä¸åŒæ¨¡æ€é—´å’Œæ¨¡æ€å†…çš„ä¿¡æ¯ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ReHyDILæ–¹æ³•åœ¨BraTS2019æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>ReHyDILæ–¹æ³•æé«˜äº†Diceç›¸ä¼¼ç³»æ•°ï¼Œæ”¹è¿›å¹…åº¦è¶…è¿‡2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1acdf880a819a571dfc5bf795a277f9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aab23988725c9e070f7f545f59dcf6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd688d21eddf9e330de941f127023ccb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Learning-Concept-Driven-Logical-Rules-for-Interpretable-and-Generalizable-Medical-Image-Classification"><a href="#Learning-Concept-Driven-Logical-Rules-for-Interpretable-and-Generalizable-Medical-Image-Classification" class="headerlink" title="Learning Concept-Driven Logical Rules for Interpretable and   Generalizable Medical Image Classification"></a>Learning Concept-Driven Logical Rules for Interpretable and   Generalizable Medical Image Classification</h2><p><strong>Authors:Yibo Gao, Hangqi Zhou, Zheyao Gao, Bomin Wang, Shangqi Gao, Sihan Wang, Xiahai Zhuang</strong></p>
<p>The pursuit of decision safety in clinical applications highlights the potential of concept-based methods in medical imaging. While these models offer active interpretability, they often suffer from concept leakages, where unintended information within soft concept representations undermines both interpretability and generalizability. Moreover, most concept-based models focus solely on local explanations (instance-level), neglecting the global decision logic (dataset-level). To address these limitations, we propose Concept Rule Learner (CRL), a novel framework to learn Boolean logical rules from binarized visual concepts. CRL employs logical layers to capture concept correlations and extract clinically meaningful rules, thereby providing both local and global interpretability. Experiments on two medical image classification tasks show that CRL achieves competitive performance with existing methods while significantly improving generalizability to out-of-distribution data. The code of our work is available at <a target="_blank" rel="noopener" href="https://github.com/obiyoag/crl">https://github.com/obiyoag/crl</a>. </p>
<blockquote>
<p>åœ¨ä¸´åºŠåº”ç”¨ä¸­è¿½æ±‚å†³ç­–å®‰å…¨æ€§çªæ˜¾äº†åŸºäºæ¦‚å¿µçš„æ–¹æ³•åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ã€‚è™½ç„¶è¿™äº›æ¨¡å‹æä¾›äº†ç§¯æçš„å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬å¸¸å¸¸å—åˆ°æ¦‚å¿µæ³„æ¼çš„å½±å“ï¼Œå…¶ä¸­è½¯æ¦‚å¿µè¡¨ç¤ºä¸­çš„æ„å¤–ä¿¡æ¯ç ´åäº†å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºäºæ¦‚å¿µæ¨¡å‹ä¸»è¦å…³æ³¨å±€éƒ¨è§£é‡Šï¼ˆå®ä¾‹çº§åˆ«ï¼‰ï¼Œå¿½è§†äº†å…¨å±€å†³ç­–é€»è¾‘ï¼ˆæ•°æ®é›†çº§åˆ«ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µè§„åˆ™å­¦ä¹ è€…ï¼ˆCRLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»äºŒè¿›åˆ¶è§†è§‰æ¦‚å¿µä¸­å­¦ä¹ å¸ƒå°”é€»è¾‘è§„åˆ™çš„æ–°æ¡†æ¶ã€‚CRLé‡‡ç”¨é€»è¾‘å±‚æ¥æ•æ‰æ¦‚å¿µé—´çš„ç›¸å…³æ€§å¹¶æå–ä¸´åºŠæœ‰æ„ä¹‰çš„è§„åˆ™ï¼Œä»è€Œæä¾›å±€éƒ¨å’Œå…¨å±€å¯è§£é‡Šæ€§ã€‚åœ¨ä¸¤ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCRLåœ¨ç°æœ‰æ–¹æ³•ä¸­è¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶åœ¨è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®ä¸Šæ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/obiyoag/crl%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/obiyoag/crlè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14049v2">PDF</a> early accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦ä¸´åºŠåº”ç”¨ä¸­å†³ç­–å®‰å…¨æ€§çš„è¿½æ±‚çªæ˜¾äº†åŸºäºæ¦‚å¿µçš„æ–¹æ³•åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›ã€‚æ¦‚å¿µæ¨¡å‹è™½èƒ½æä¾›ç§¯æçš„å¯è§£é‡Šæ€§ï¼Œä½†å¾€å¾€å—åˆ°æ¦‚å¿µæ³„æ¼é—®é¢˜çš„å½±å“ï¼Œè¿™ä¸€é—®é¢˜å¯¼è‡´è½¯æ¦‚å¿µè¡¨ç¤ºä¸­çš„æ„å¤–ä¿¡æ¯å‰Šå¼±äº†å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºäºæ¦‚å¿µçš„æ–¹æ³•åªå…³æ³¨å±€éƒ¨è§£é‡Šï¼ˆå®ä¾‹çº§åˆ«ï¼‰ï¼Œå¿½ç•¥äº†å…¨å±€å†³ç­–é€»è¾‘ï¼ˆæ•°æ®é›†çº§åˆ«ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Concept Rule Learnerï¼ˆCRLï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå¯ä»äºŒè¿›åˆ¶è§†è§‰æ¦‚å¿µä¸­å­¦ä¹ å¸ƒå°”é€»è¾‘è§„åˆ™ã€‚CRLåˆ©ç”¨é€»è¾‘å±‚æ•æ‰æ¦‚å¿µç›¸å…³æ€§å¹¶æå–å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è§„åˆ™ï¼Œä»è€Œæä¾›å±€éƒ¨å’Œå…¨å±€çš„å¯è§£é‡Šæ€§ã€‚åœ¨ä¸¤é¡¹åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCRLåœ¨ç°æœ‰æ–¹æ³•ä¸­è¡¨ç°æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨æé«˜å¯¹æ–°åˆ†å¸ƒæ•°æ®çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/obiyoag/crl%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/obiyoag/crlè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å…·æœ‰æ½œåŠ›ï¼Œå¯æé«˜ä¸´åºŠå†³ç­–çš„å®‰å…¨æ€§ã€‚</li>
<li>æ¦‚å¿µæ¨¡å‹å­˜åœ¨æ¦‚å¿µæ³„æ¼é—®é¢˜ï¼Œå½±å“æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¤§å¤šæ•°åŸºäºæ¦‚å¿µçš„æ–¹æ³•ä¸»è¦å…³æ³¨å±€éƒ¨è§£é‡Šï¼Œå¿½ç•¥äº†å…¨å±€å†³ç­–é€»è¾‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Concept Rule Learnerï¼ˆCRLï¼‰ï¼Œèƒ½å¤Ÿä»äºŒè¿›åˆ¶è§†è§‰æ¦‚å¿µä¸­å­¦ä¹ å¸ƒå°”é€»è¾‘è§„åˆ™ã€‚</li>
<li>CRLé€šè¿‡é€»è¾‘å±‚æ•æ‰æ¦‚å¿µç›¸å…³æ€§ï¼Œæä¾›å±€éƒ¨å’Œå…¨å±€çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>CRLåœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46bbd61ecef301d9482a79f9ecf45998.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85e4c32bbaa4bbee2ba32572d115979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbed8edc5e8e8f1f902b09b716fa1d4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-455272088a4ad9824a3d088068471bfb.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for   Japanese Speech Annotation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-11/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae032f62b8b073c8f4f35b9a54924c4e.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-11  StableMTL Repurposing Latent Diffusion Models for Multi-Task Learning   from Partially Annotated Synthetic Datasets
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32298.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
